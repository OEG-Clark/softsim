{"home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.__init__": [[154, 171], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# Minimal set of attributes that all data classes should have", "\n", "        ", "self", ".", "dataType", "=", "None", "\n", "self", ".", "device", "=", "None", "\n", "self", ".", "nTrain", "=", "None", "\n", "self", ".", "nValid", "=", "None", "\n", "self", ".", "nTest", "=", "None", "\n", "self", ".", "samples", "=", "{", "}", "\n", "self", ".", "samples", "[", "'train'", "]", "=", "{", "}", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "None", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "None", "\n", "self", ".", "samples", "[", "'valid'", "]", "=", "{", "}", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "=", "None", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "=", "None", "\n", "self", ".", "samples", "[", "'test'", "]", "=", "{", "}", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "=", "None", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.getSamples": [[172, 220], ["len", "len", "type", "numpy.random.choice", "len", "len", "xSelected.unsqueeze", "numpy.expand_dims"], "methods", ["None"], ["", "def", "getSamples", "(", "self", ",", "samplesType", ",", "*", "args", ")", ":", "\n", "# samplesType: train, valid, test", "\n", "# args: 0 args, give back all", "\n", "# args: 1 arg: if int, give that number of samples, chosen at random", "\n", "# args: 1 arg: if list, give those samples precisely.", "\n", "# Check that the type is one of the possible ones", "\n", "        ", "assert", "samplesType", "==", "'train'", "or", "samplesType", "==", "'valid'", "or", "samplesType", "==", "'test'", "\n", "# Check that the number of extra arguments fits", "\n", "assert", "len", "(", "args", ")", "<=", "1", "\n", "# If there are no arguments, just return all the desired samples", "\n", "x", "=", "self", ".", "samples", "[", "samplesType", "]", "[", "'signals'", "]", "\n", "y", "=", "self", ".", "samples", "[", "samplesType", "]", "[", "'targets'", "]", "\n", "# If there's an argument, we have to check whether it is an int or a", "\n", "# list", "\n", "if", "len", "(", "args", ")", "==", "1", ":", "\n", "# If it is an int, just return that number of randomly chosen", "\n", "# samples.", "\n", "            ", "if", "type", "(", "args", "[", "0", "]", ")", "==", "int", ":", "\n", "                ", "nSamples", "=", "x", ".", "shape", "[", "0", "]", "# total number of samples", "\n", "# We can't return more samples than there are available", "\n", "assert", "args", "[", "0", "]", "<=", "nSamples", "\n", "# Randomly choose args[0] indices", "\n", "selectedIndices", "=", "np", ".", "random", ".", "choice", "(", "nSamples", ",", "size", "=", "args", "[", "0", "]", ",", "\n", "replace", "=", "False", ")", "\n", "# Select the corresponding samples", "\n", "xSelected", "=", "x", "[", "selectedIndices", "]", "\n", "y", "=", "y", "[", "selectedIndices", "]", "\n", "", "else", ":", "\n", "# The fact that we put else here instead of elif type()==list", "\n", "# allows for np.array to be used as indices as well. In general,", "\n", "# any variable with the ability to index.", "\n", "                ", "xSelected", "=", "x", "[", "args", "[", "0", "]", "]", "\n", "# And assign the labels", "\n", "y", "=", "y", "[", "args", "[", "0", "]", "]", "\n", "\n", "# If we only selected a single element, then the nDataPoints dim", "\n", "# has been left out. So if we have less dimensions, we have to", "\n", "# put it back", "\n", "", "if", "len", "(", "xSelected", ".", "shape", ")", "<", "len", "(", "x", ".", "shape", ")", ":", "\n", "                ", "if", "'torch'", "in", "self", ".", "dataType", ":", "\n", "                    ", "x", "=", "xSelected", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "                    ", "x", "=", "np", ".", "expand_dims", "(", "xSelected", ",", "axis", "=", "0", ")", "\n", "", "", "else", ":", "\n", "                ", "x", "=", "xSelected", "\n", "\n", "", "", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.expandDims": [[221, 250], ["dataTools._data.samples.keys", "len", "repr", "[].unsqueeze", "numpy.expand_dims", "len", "repr", "[].unsqueeze", "numpy.expand_dims"], "methods", ["None"], ["", "def", "expandDims", "(", "self", ")", ":", "\n", "\n", "# For each data set partition", "\n", "        ", "for", "key", "in", "self", ".", "samples", ".", "keys", "(", ")", ":", "\n", "# If there's something in them", "\n", "            ", "if", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "is", "not", "None", ":", "\n", "# And if it has only two dimensions", "\n", "#   (shape: nDataPoints x nNodes)", "\n", "                ", "if", "len", "(", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", ".", "shape", ")", "==", "2", ":", "\n", "# Then add a third dimension in between so that it ends", "\n", "# up with shape", "\n", "#   nDataPoints x 1 x nNodes", "\n", "# and it respects the 3-dimensional format that is taken", "\n", "# by many of the processing functions", "\n", "                    ", "if", "'torch'", "in", "repr", "(", "self", ".", "dataType", ")", ":", "\n", "                        ", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "=", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "=", "np", ".", "expand_dims", "(", "\n", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", ",", "\n", "axis", "=", "1", ")", "\n", "", "", "elif", "len", "(", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", ".", "shape", ")", "==", "3", ":", "\n", "                    ", "if", "'torch'", "in", "repr", "(", "self", ".", "dataType", ")", ":", "\n", "                        ", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "=", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", ".", "unsqueeze", "(", "2", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "=", "np", ".", "expand_dims", "(", "\n", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", ",", "\n", "axis", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.astype": [[251, 294], ["str", "dataTools._data.samples.keys", "dataTools.changeDataType", "dataTools.changeDataType", "repr", "repr"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType"], ["", "", "", "", "", "def", "astype", "(", "self", ",", "dataType", ")", ":", "\n", "# This changes the type for the minimal attributes (samples). This ", "\n", "# methods should still be initialized within the data classes, if more", "\n", "# attributes are used.", "\n", "\n", "# The labels could be integers as created from the dataset, so if they", "\n", "# are, we need to be sure they are integers also after conversion. ", "\n", "# To do this we need to match the desired dataType to its int ", "\n", "# counterpart. Typical examples are:", "\n", "#   numpy.float64 -> numpy.int64", "\n", "#   numpy.float32 -> numpy.int32", "\n", "#   torch.float64 -> torch.int64", "\n", "#   torch.float32 -> torch.int32", "\n", "\n", "        ", "targetType", "=", "str", "(", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", ".", "dtype", ")", "\n", "if", "'int'", "in", "targetType", ":", "\n", "            ", "if", "'numpy'", "in", "repr", "(", "dataType", ")", ":", "\n", "                ", "if", "'64'", "in", "targetType", ":", "\n", "                    ", "targetType", "=", "np", ".", "int64", "\n", "", "elif", "'32'", "in", "targetType", ":", "\n", "                    ", "targetType", "=", "np", ".", "int32", "\n", "", "", "elif", "'torch'", "in", "repr", "(", "dataType", ")", ":", "\n", "                ", "if", "'64'", "in", "targetType", ":", "\n", "                    ", "targetType", "=", "torch", ".", "int64", "\n", "", "elif", "'32'", "in", "targetType", ":", "\n", "                    ", "targetType", "=", "torch", ".", "int32", "\n", "", "", "", "else", ":", "# If there is no int, just stick with the given dataType", "\n", "            ", "targetType", "=", "dataType", "\n", "\n", "# Now that we have selected the dataType, and the corresponding", "\n", "# labelType, we can proceed to convert the data into the corresponding", "\n", "# type", "\n", "", "for", "key", "in", "self", ".", "samples", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "=", "changeDataType", "(", "\n", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", ",", "\n", "dataType", ")", "\n", "self", ".", "samples", "[", "key", "]", "[", "'targets'", "]", "=", "changeDataType", "(", "\n", "self", ".", "samples", "[", "key", "]", "[", "'targets'", "]", ",", "\n", "targetType", ")", "\n", "\n", "# Update attribute", "\n", "", "if", "dataType", "is", "not", "self", ".", "dataType", ":", "\n", "            ", "self", ".", "dataType", "=", "dataType", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.to": [[295, 309], ["repr", "dataTools._data.samples.keys", "dataTools._data.samples[].keys", "[].to"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# This changes the type for the minimal attributes (samples). This ", "\n", "# methods should still be initialized within the data classes, if more", "\n", "# attributes are used.", "\n", "# This can only be done if they are torch tensors", "\n", "        ", "if", "'torch'", "in", "repr", "(", "self", ".", "dataType", ")", ":", "\n", "            ", "for", "key", "in", "self", ".", "samples", ".", "keys", "(", ")", ":", "\n", "                ", "for", "secondKey", "in", "self", ".", "samples", "[", "key", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "self", ".", "samples", "[", "key", "]", "[", "secondKey", "]", "=", "self", ".", "samples", "[", "key", "]", "[", "secondKey", "]", ".", "to", "(", "device", ")", "\n", "\n", "# If the device changed, save it.", "\n", "", "", "if", "device", "is", "not", "self", ".", "device", ":", "\n", "                ", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._dataForClassification.__init__": [[316, 319], ["dataTools._data.__init__"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._dataForClassification.evaluate": [[321, 342], ["len", "repr", "torch.argmax", "torch.sum", "numpy.array", "numpy.array", "numpy.argmax", "numpy.sum", "numpy.sum.type", "numpy.sum.astype", "torch.abs", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "evaluate", "(", "self", ",", "yHat", ",", "y", ",", "tol", "=", "1e-9", ")", ":", "\n", "        ", "\"\"\"\n        Return the accuracy (ratio of yHat = y)\n        \"\"\"", "\n", "N", "=", "len", "(", "y", ")", "\n", "if", "'torch'", "in", "repr", "(", "self", ".", "dataType", ")", ":", "\n", "#   We compute the target label (hardmax)", "\n", "            ", "yHat", "=", "torch", ".", "argmax", "(", "yHat", ",", "dim", "=", "1", ")", "\n", "#   And compute the error", "\n", "totalErrors", "=", "torch", ".", "sum", "(", "torch", ".", "abs", "(", "yHat", "-", "y", ")", ">", "tol", ")", "\n", "errorRate", "=", "totalErrors", ".", "type", "(", "self", ".", "dataType", ")", "/", "N", "\n", "", "else", ":", "\n", "            ", "yHat", "=", "np", ".", "array", "(", "yHat", ")", "\n", "y", "=", "np", ".", "array", "(", "y", ")", "\n", "#   We compute the target label (hardmax)", "\n", "yHat", "=", "np", ".", "argmax", "(", "yHat", ",", "axis", "=", "1", ")", "\n", "#   And compute the error", "\n", "totalErrors", "=", "np", ".", "sum", "(", "np", ".", "abs", "(", "yHat", "-", "y", ")", ">", "tol", ")", "\n", "errorRate", "=", "totalErrors", ".", "astype", "(", "self", ".", "dataType", ")", "/", "N", "\n", "#   And from that, compute the accuracy", "\n", "", "return", "errorRate", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.FacebookEgo.__init__": [[369, 379], ["dataTools.FacebookEgo.loadData"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.loadData"], ["def", "__init__", "(", "self", ",", "dataDir", ",", "use234", "=", "False", ")", ":", "\n", "\n", "# Dataset directory", "\n", "        ", "self", ".", "dataDir", "=", "dataDir", "\n", "# Empty attributes", "\n", "self", ".", "adjacencyMatrix", "=", "None", "\n", "self", ".", "adjacencyMatrix234", "=", "None", "\n", "\n", "# Load data", "\n", "self", ".", "loadData", "(", "'facebookEgo.pkl'", ",", "use234", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.FacebookEgo.loadData": [[380, 467], ["os.path.join", "os.path.isfile", "os.path.exists", "os.makedirs", "os.path.join", "os.path.isfile", "os.path.isfile", "os.path.isfile", "os.path.join", "os.path.isfile", "numpy.empty", "open", "pickle.load", "os.path.join", "os.path.join", "urllib.request.urlretrieve", "os.path.join", "os.path.join", "open", "open", "pickle.dump", "open", "pickle.load", "gzip.open", "dataLine.rstrip().split", "int", "int", "max", "os.path.join", "open", "shutil.copyfileobj", "max", "numpy.zeros", "numpy.concatenate", "numpy.zeros", "numpy.concatenate", "dataLine.rstrip"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load"], ["", "def", "loadData", "(", "self", ",", "filename", ",", "use234", ")", ":", "\n", "# Check if the dataDir exists, and if not, create it", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "dataDir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "dataDir", ")", "\n", "# Create the filename to save/load", "\n", "", "datasetFilename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataDir", ",", "filename", ")", "\n", "if", "use234", ":", "\n", "            ", "datasetFilename234", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataDir", ",", "'facebookEgo234.pkl'", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "datasetFilename234", ")", ":", "\n", "                ", "with", "open", "(", "datasetFilename234", ",", "'rb'", ")", "as", "datasetFile234", ":", "\n", "                    ", "datasetDict", "=", "pickle", ".", "load", "(", "datasetFile234", ")", "\n", "self", ".", "adjacencyMatrix234", "=", "datasetDict", "[", "'adjacencyMatrix'", "]", "\n", "# Check if the file does exist, load it", "\n", "", "", "", "if", "os", ".", "path", ".", "isfile", "(", "datasetFilename", ")", ":", "\n", "# If it exists, load it", "\n", "            ", "with", "open", "(", "datasetFilename", ",", "'rb'", ")", "as", "datasetFile", ":", "\n", "                ", "datasetDict", "=", "pickle", ".", "load", "(", "datasetFile", ")", "\n", "# And save the corresponding variable", "\n", "self", ".", "adjacencyMatrix", "=", "datasetDict", "[", "'adjacencyMatrix'", "]", "\n", "", "", "else", ":", "# If it doesn't exist, load it", "\n", "# There could be three options here: that we have the raw data ", "\n", "# already there, that we have the zip file and need to unzip it,", "\n", "# or that we do not have nothing and we need to download it.", "\n", "            ", "existsRawData", "=", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "self", ".", "dataDir", ",", "\n", "'facebook_combined.txt'", ")", ")", "\n", "# And the zip file", "\n", "existsZipFile", "=", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "dataDir", ",", "'facebook_combined.txt.gz'", ")", ")", "\n", "if", "not", "existsRawData", "and", "not", "existsZipFile", ":", "# We have to download it", "\n", "                ", "fbURL", "=", "'https://snap.stanford.edu/data/facebook_combined.txt.gz'", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "fbURL", ",", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "dataDir", ",", "'facebook_combined.txt.gz'", ")", ")", "\n", "existsZipFile", "=", "True", "\n", "", "if", "not", "existsRawData", "and", "existsZipFile", ":", "# Unzip it", "\n", "                ", "zipFile", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataDir", ",", "'facebook_combined.txt.gz'", ")", "\n", "txtFile", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataDir", ",", "'facebook_combined.txt'", ")", "\n", "with", "gzip", ".", "open", "(", "zipFile", ",", "'rb'", ")", "as", "f_in", ":", "\n", "                    ", "with", "open", "(", "txtFile", ",", "'wb'", ")", "as", "f_out", ":", "\n", "                        ", "shutil", ".", "copyfileobj", "(", "f_in", ",", "f_out", ")", "\n", "# Now that we have the data, we can get their filenames", "\n", "", "", "", "rawDataFilename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataDir", ",", "'facebook_combined.txt'", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "rawDataFilename", ")", "\n", "# And we can load it and store it.", "\n", "adjacencyMatrix", "=", "np", ".", "empty", "(", "[", "0", ",", "0", "]", ")", "# Start with an empty matrix and", "\n", "# then we slowly add the number of nodes, which we do not assume", "\n", "# to be known beforehand.", "\n", "# Let's start with the data.", "\n", "# Open it.", "\n", "with", "open", "(", "rawDataFilename", ",", "'r'", ")", "as", "rawData", ":", "\n", "# The file consists of a succession of lines, each line", "\n", "# corresponds to an edge", "\n", "                ", "for", "dataLine", "in", "rawData", ":", "\n", "# For each line, we split it in the different fields", "\n", "                    ", "dataLineSplit", "=", "dataLine", ".", "rstrip", "(", "'\\n'", ")", ".", "split", "(", "' '", ")", "\n", "# Keep the ones we care about here", "\n", "node_i", "=", "int", "(", "dataLineSplit", "[", "0", "]", ")", "\n", "node_j", "=", "int", "(", "dataLineSplit", "[", "1", "]", ")", "\n", "node_max", "=", "max", "(", "node_i", ",", "node_j", ")", "# Get the largest node", "\n", "# Now we have to add this information to the adjacency ", "\n", "# matrix.", "\n", "#   We need to check whether we need to add more elements", "\n", "if", "node_max", "+", "1", ">", "max", "(", "adjacencyMatrix", ".", "shape", ")", ":", "\n", "                        ", "colDiff", "=", "node_max", "+", "1", "-", "adjacencyMatrix", ".", "shape", "[", "1", "]", "\n", "zeroPadCols", "=", "np", ".", "zeros", "(", "[", "adjacencyMatrix", ".", "shape", "[", "0", "]", ",", "colDiff", "]", ")", "\n", "adjacencyMatrix", "=", "np", ".", "concatenate", "(", "(", "adjacencyMatrix", ",", "\n", "zeroPadCols", ")", ",", "\n", "axis", "=", "1", ")", "\n", "rowDiff", "=", "node_max", "+", "1", "-", "adjacencyMatrix", ".", "shape", "[", "0", "]", "\n", "zeroPadRows", "=", "np", ".", "zeros", "(", "[", "rowDiff", ",", "adjacencyMatrix", ".", "shape", "[", "1", "]", "]", ")", "\n", "adjacencyMatrix", "=", "np", ".", "concatenate", "(", "(", "adjacencyMatrix", ",", "\n", "zeroPadRows", ")", ",", "\n", "axis", "=", "0", ")", "\n", "# Now that we have assured appropriate dimensions", "\n", "", "adjacencyMatrix", "[", "node_i", ",", "node_j", "]", "=", "1.", "\n", "# And because it is undirected by construction", "\n", "adjacencyMatrix", "[", "node_j", ",", "node_i", "]", "=", "1.", "\n", "# Now that it is loaded, let's store it", "\n", "", "", "self", ".", "adjacencyMatrix", "=", "adjacencyMatrix", "\n", "# And save it in a pickle file for posterity", "\n", "with", "open", "(", "datasetFilename", ",", "'wb'", ")", "as", "datasetFile", ":", "\n", "                ", "pickle", ".", "dump", "(", "\n", "{", "'adjacencyMatrix'", ":", "self", ".", "adjacencyMatrix", "}", ",", "\n", "datasetFile", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.FacebookEgo.getAdjacencyMatrix": [[469, 472], ["None"], "methods", ["None"], ["", "", "", "def", "getAdjacencyMatrix", "(", "self", ",", "use234", "=", "False", ")", ":", "\n", "\n", "        ", "return", "self", ".", "adjacencyMatrix234", "if", "use234", "else", "self", ".", "adjacencyMatrix", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.SourceLocalization.__init__": [[539, 593], ["dataTools._dataForClassification.__init__", "alegnn.computeGFT", "numpy.max", "numpy.random.choice", "numpy.random.choice", "numpy.eye", "numpy.eye.reshape", "range", "range", "numpy.array", "numpy.array", "numpy.array", "dataTools.SourceLocalization.astype", "dataTools.SourceLocalization.to", "numpy.concatenate", "len", "numpy.eye.reshape"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeGFT", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["def", "__init__", "(", "self", ",", "G", ",", "nTrain", ",", "nValid", ",", "nTest", ",", "sourceNodes", ",", "tMax", "=", "None", ",", "\n", "dataType", "=", "np", ".", "float64", ",", "device", "=", "'cpu'", ")", ":", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# store attributes", "\n", "self", ".", "dataType", "=", "dataType", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "nTrain", "=", "nTrain", "\n", "self", ".", "nValid", "=", "nValid", "\n", "self", ".", "nTest", "=", "nTest", "\n", "# If no tMax is specified, set it the maximum possible.", "\n", "if", "tMax", "==", "None", ":", "\n", "            ", "tMax", "=", "G", ".", "N", "\n", "#\\\\\\ Generate the samples", "\n", "# Get the largest eigenvalue of the weighted adjacency matrix", "\n", "", "EW", ",", "VW", "=", "graph", ".", "computeGFT", "(", "G", ".", "W", ",", "order", "=", "'totalVariation'", ")", "\n", "eMax", "=", "np", ".", "max", "(", "EW", ")", "\n", "# Normalize the matrix so that it doesn't explode", "\n", "Wnorm", "=", "G", ".", "W", "/", "eMax", "\n", "# total number of samples", "\n", "nTotal", "=", "nTrain", "+", "nValid", "+", "nTest", "\n", "# sample source nodes", "\n", "sampledSources", "=", "np", ".", "random", ".", "choice", "(", "sourceNodes", ",", "size", "=", "nTotal", ")", "\n", "# sample diffusion times", "\n", "sampledTimes", "=", "np", ".", "random", ".", "choice", "(", "tMax", ",", "size", "=", "nTotal", ")", "\n", "# Since the signals are generated as W^t * delta, this reduces to the", "\n", "# selection of a column of W^t (the column corresponding to the source", "\n", "# node). Therefore, we generate an array of size tMax x N x N with all", "\n", "# the powers of the matrix, and then we just simply select the", "\n", "# corresponding column for the corresponding time", "\n", "lastWt", "=", "np", ".", "eye", "(", "G", ".", "N", ",", "G", ".", "N", ")", "\n", "Wt", "=", "lastWt", ".", "reshape", "(", "[", "1", ",", "G", ".", "N", ",", "G", ".", "N", "]", ")", "\n", "for", "t", "in", "range", "(", "1", ",", "tMax", ")", ":", "\n", "            ", "lastWt", "=", "lastWt", "@", "Wnorm", "\n", "Wt", "=", "np", ".", "concatenate", "(", "(", "Wt", ",", "lastWt", ".", "reshape", "(", "[", "1", ",", "G", ".", "N", ",", "G", ".", "N", "]", ")", ")", ",", "axis", "=", "0", ")", "\n", "", "x", "=", "Wt", "[", "sampledTimes", ",", ":", ",", "sampledSources", "]", "\n", "# Now, we have the signals and the labels", "\n", "signals", "=", "x", "# nTotal x N", "\n", "# Finally, we have to match the source nodes to the corresponding labels", "\n", "# which start at 0 and increase in integers.", "\n", "nodesToLabels", "=", "{", "}", "\n", "for", "it", "in", "range", "(", "len", "(", "sourceNodes", ")", ")", ":", "\n", "            ", "nodesToLabels", "[", "sourceNodes", "[", "it", "]", "]", "=", "it", "\n", "", "labels", "=", "[", "nodesToLabels", "[", "x", "]", "for", "x", "in", "sampledSources", "]", "# nTotal", "\n", "# Split and save them", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "signals", "[", "0", ":", "nTrain", ",", ":", "]", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "np", ".", "array", "(", "labels", "[", "0", ":", "nTrain", "]", ")", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "=", "signals", "[", "nTrain", ":", "nTrain", "+", "nValid", ",", ":", "]", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "=", "np", ".", "array", "(", "labels", "[", "nTrain", ":", "nTrain", "+", "nValid", "]", ")", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "=", "signals", "[", "nTrain", "+", "nValid", ":", "nTotal", ",", ":", "]", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "=", "np", ".", "array", "(", "labels", "[", "nTrain", "+", "nValid", ":", "nTotal", "]", ")", "\n", "# Change data to specified type and device", "\n", "self", ".", "astype", "(", "self", ".", "dataType", ")", "\n", "self", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Authorship.__init__": [[709, 833], ["dataTools._dataForClassification.__init__", "dataTools.Authorship.loadData", "dataTools.Authorship.authorData[].copy", "int", "int", "round", "round", "round", "numpy.random.permutation", "dataTools.Authorship.copy", "xAuthorTrain.copy", "xAuthorValid.copy", "xAuthorTest.copy", "[].copy", "[].copy", "[].copy", "numpy.empty", "dataTools.Authorship.authorData.keys", "numpy.random.permutation", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate.astype", "numpy.concatenate.astype", "numpy.concatenate.astype", "dataTools.Authorship.createGraph", "dataTools.Authorship.astype", "dataTools.Authorship.to", "dataTools.Authorship.authorData.keys", "round", "round", "numpy.concatenate", "numpy.ones", "numpy.zeros", "numpy.ones", "numpy.zeros", "numpy.ones", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.loadData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.createGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["def", "__init__", "(", "self", ",", "authorName", ",", "ratioTrain", ",", "ratioValid", ",", "dataPath", ",", "\n", "graphNormalizationType", ",", "keepIsolatedNodes", ",", "\n", "forceUndirected", ",", "forceConnected", ",", "\n", "dataType", "=", "np", ".", "float64", ",", "device", "=", "'cpu'", ")", ":", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Store", "\n", "self", ".", "authorName", "=", "authorName", "\n", "self", ".", "ratioTrain", "=", "ratioTrain", "\n", "self", ".", "ratioValid", "=", "ratioValid", "\n", "self", ".", "dataPath", "=", "dataPath", "\n", "self", ".", "dataType", "=", "dataType", "\n", "self", ".", "device", "=", "device", "\n", "# Store characteristics of the graph to be created", "\n", "self", ".", "graphNormalizationType", "=", "graphNormalizationType", "\n", "self", ".", "keepIsolatedNodes", "=", "keepIsolatedNodes", "\n", "self", ".", "forceUndirected", "=", "forceUndirected", "\n", "self", ".", "forceConnected", "=", "forceConnected", "\n", "self", ".", "adjacencyMatrix", "=", "None", "\n", "# Other data to save", "\n", "self", ".", "authorData", "=", "None", "\n", "self", ".", "selectedAuthor", "=", "None", "\n", "self", ".", "allFunctionWords", "=", "None", "\n", "self", ".", "functionWords", "=", "None", "\n", "# Load data", "\n", "self", ".", "loadData", "(", "dataPath", ")", "\n", "# Check that the authorName is a valid name", "\n", "assert", "authorName", "in", "self", ".", "authorData", ".", "keys", "(", ")", "\n", "# Get the selected author's data", "\n", "thisAuthorData", "=", "self", ".", "authorData", "[", "authorName", "]", ".", "copy", "(", ")", "\n", "nExcerpts", "=", "thisAuthorData", "[", "'wordFreq'", "]", ".", "shape", "[", "0", "]", "# Number of excerpts", "\n", "# by the selected author", "\n", "nTrainAuthor", "=", "int", "(", "round", "(", "ratioTrain", "*", "nExcerpts", ")", ")", "\n", "nValidAuthor", "=", "int", "(", "round", "(", "ratioValid", "*", "nTrainAuthor", ")", ")", "\n", "nTestAuthor", "=", "nExcerpts", "-", "nTrainAuthor", "\n", "nTrainAuthor", "=", "nTrainAuthor", "-", "nValidAuthor", "\n", "# Now, we know how many training, validation and testing samples from", "\n", "# the required author. But we will also include an equal amount of", "\n", "# other authors, therefore", "\n", "self", ".", "nTrain", "=", "round", "(", "2", "*", "nTrainAuthor", ")", "\n", "self", ".", "nValid", "=", "round", "(", "2", "*", "nValidAuthor", ")", "\n", "self", ".", "nTest", "=", "round", "(", "2", "*", "nTestAuthor", ")", "\n", "\n", "# Now, let's get the corresponding signals for the author", "\n", "xAuthor", "=", "thisAuthorData", "[", "'wordFreq'", "]", "\n", "# Get a random permutation of these works, and split them accordingly", "\n", "randPerm", "=", "np", ".", "random", ".", "permutation", "(", "nExcerpts", ")", "\n", "# Save the indices corresponding to each split", "\n", "randPermTrain", "=", "randPerm", "[", "0", ":", "nTrainAuthor", "]", "\n", "randPermValid", "=", "randPerm", "[", "nTrainAuthor", ":", "nTrainAuthor", "+", "nValidAuthor", "]", "\n", "randPermTest", "=", "randPerm", "[", "nTrainAuthor", "+", "nValidAuthor", ":", "nExcerpts", "]", "\n", "xAuthorTrain", "=", "xAuthor", "[", "randPermTrain", ",", ":", "]", "\n", "xAuthorValid", "=", "xAuthor", "[", "randPermValid", ",", ":", "]", "\n", "xAuthorTest", "=", "xAuthor", "[", "randPermTest", ",", ":", "]", "\n", "# And we will store this split", "\n", "self", ".", "selectedAuthor", "=", "{", "}", "\n", "# Copy all data", "\n", "self", ".", "selectedAuthor", "[", "'all'", "]", "=", "thisAuthorData", ".", "copy", "(", ")", "\n", "# Copy word frequencies", "\n", "self", ".", "selectedAuthor", "[", "'train'", "]", "=", "{", "}", "\n", "self", ".", "selectedAuthor", "[", "'train'", "]", "[", "'wordFreq'", "]", "=", "xAuthorTrain", ".", "copy", "(", ")", "\n", "self", ".", "selectedAuthor", "[", "'valid'", "]", "=", "{", "}", "\n", "self", ".", "selectedAuthor", "[", "'valid'", "]", "[", "'wordFreq'", "]", "=", "xAuthorValid", ".", "copy", "(", ")", "\n", "self", ".", "selectedAuthor", "[", "'test'", "]", "=", "{", "}", "\n", "self", ".", "selectedAuthor", "[", "'test'", "]", "[", "'wordFreq'", "]", "=", "xAuthorTest", ".", "copy", "(", ")", "\n", "# Copy WANs", "\n", "self", ".", "selectedAuthor", "[", "'train'", "]", "[", "'WAN'", "]", "=", "thisAuthorData", "[", "'WAN'", "]", "[", "randPermTrain", ",", ":", ",", ":", "]", ".", "copy", "(", ")", "\n", "self", ".", "selectedAuthor", "[", "'valid'", "]", "[", "'WAN'", "]", "=", "thisAuthorData", "[", "'WAN'", "]", "[", "randPermValid", ",", ":", ",", ":", "]", ".", "copy", "(", ")", "\n", "self", ".", "selectedAuthor", "[", "'test'", "]", "[", "'WAN'", "]", "=", "thisAuthorData", "[", "'WAN'", "]", "[", "randPermTest", ",", ":", ",", ":", "]", ".", "copy", "(", ")", "\n", "# Now we need to get an equal amount of works from the rest of the", "\n", "# authors.", "\n", "xRest", "=", "np", ".", "empty", "(", "[", "0", ",", "xAuthorTrain", ".", "shape", "[", "1", "]", "]", ")", "# Create an empty matrix", "\n", "# to store all the works by the rest of the authors.", "\n", "# Now go author by author gathering all works", "\n", "for", "key", "in", "self", ".", "authorData", ".", "keys", "(", ")", ":", "\n", "# Only for authors that are not the selected author", "\n", "            ", "if", "key", "is", "not", "authorName", ":", "\n", "                ", "thisAuthorTexts", "=", "self", ".", "authorData", "[", "key", "]", "[", "'wordFreq'", "]", "\n", "xRest", "=", "np", ".", "concatenate", "(", "(", "xRest", ",", "thisAuthorTexts", ")", ",", "axis", "=", "0", ")", "\n", "# After obtaining all works, xRest is of shape nRestOfData x nWords", "\n", "# We now need to select at random from this other data, but only up", "\n", "# to nExcerpts. Therefore, we will randperm all the indices, but keep", "\n", "# only the first nExcerpts indices.", "\n", "", "", "randPerm", "=", "np", ".", "random", ".", "permutation", "(", "xRest", ".", "shape", "[", "0", "]", ")", "\n", "randPerm", "=", "randPerm", "[", "0", ":", "nExcerpts", "]", "# nExcerpts x nWords", "\n", "# And now we should just get the appropriate number of texts from these", "\n", "# other authors.", "\n", "# Compute how many samples for each case", "\n", "nTrainRest", "=", "self", ".", "nTrain", "-", "nTrainAuthor", "\n", "nValidRest", "=", "self", ".", "nValid", "-", "nValidAuthor", "\n", "nTestRest", "=", "self", ".", "nTest", "-", "nTestAuthor", "\n", "# And obtain those", "\n", "xRestTrain", "=", "xRest", "[", "randPerm", "[", "0", ":", "nTrainRest", "]", ",", ":", "]", "\n", "xRestValid", "=", "xRest", "[", "randPerm", "[", "nTrainRest", ":", "nTrainRest", "+", "nValidRest", "]", ",", ":", "]", "\n", "xRestTest", "=", "xRest", "[", "randPerm", "[", "nTrainRest", "+", "nValidRest", ":", "nExcerpts", "]", ",", ":", "]", "\n", "# Now construct the signals and labels. Signals is just the ", "\n", "# concatenation of each of these excerpts. Labels is just a bunch of", "\n", "# 1s followed by a bunch of 0s", "\n", "# Obs.: The fact that the dataset is ordered now, it doesn't matter,", "\n", "# since it will be shuffled at each epoch.", "\n", "xTrain", "=", "np", ".", "concatenate", "(", "(", "xAuthorTrain", ",", "xRestTrain", ")", ",", "axis", "=", "0", ")", "\n", "labelsTrain", "=", "np", ".", "concatenate", "(", "(", "np", ".", "ones", "(", "nTrainAuthor", ")", ",", "\n", "np", ".", "zeros", "(", "nTrainRest", ")", ")", ",", "axis", "=", "0", ")", "\n", "xValid", "=", "np", ".", "concatenate", "(", "(", "xAuthorValid", ",", "xRestValid", ")", ",", "axis", "=", "0", ")", "\n", "labelsValid", "=", "np", ".", "concatenate", "(", "(", "np", ".", "ones", "(", "nValidAuthor", ")", ",", "\n", "np", ".", "zeros", "(", "nValidRest", ")", ")", ",", "axis", "=", "0", ")", "\n", "xTest", "=", "np", ".", "concatenate", "(", "(", "xAuthorTest", ",", "xRestTest", ")", ",", "axis", "=", "0", ")", "\n", "labelsTest", "=", "np", ".", "concatenate", "(", "(", "np", ".", "ones", "(", "nTestAuthor", ")", ",", "\n", "np", ".", "zeros", "(", "nTestRest", ")", ")", ",", "axis", "=", "0", ")", "\n", "# And assign them to the required attribute samples", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "xTrain", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "labelsTrain", ".", "astype", "(", "np", ".", "int", ")", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "=", "xValid", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "=", "labelsValid", ".", "astype", "(", "np", ".", "int", ")", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "=", "xTest", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "=", "labelsTest", ".", "astype", "(", "np", ".", "int", ")", "\n", "# Create graph", "\n", "self", ".", "createGraph", "(", ")", "\n", "# Change data to specified type and device", "\n", "self", ".", "astype", "(", "self", ".", "dataType", ")", "\n", "self", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Authorship.loadData": [[834, 886], ["hdf5storage.loadmat", "range", "functionWords.copy", "len", "str", "thisWAN.transpose.transpose.transpose", "functionWords.append", "thisWordFreq.squeeze", "str"], "methods", ["None"], ["", "def", "loadData", "(", "self", ",", "dataPath", ")", ":", "\n", "# Load data (from a .mat file)", "\n", "        ", "rawData", "=", "hdf5storage", ".", "loadmat", "(", "dataPath", ")", "\n", "# rawData is a dictionary with four keys:", "\n", "#   'all_authors': contains the author list", "\n", "#   'all_freqs': contains the word frequency count for each excerpt", "\n", "#   'all_wans': contains the WANS for each excerpt", "\n", "#   'function_words': a list of the functional words", "\n", "# The issue is that hdf5storage, while necessary to load old ", "\n", "# Matlab(R) files, gives the data in a weird format, that we need", "\n", "# to adapt and convert.", "\n", "# The data will be structured as follows. We will have an", "\n", "# authorData dictionary of dictionaries: the first key will be the", "\n", "# author name, the second key will be either freqs or wans to", "\n", "# access either one or another.", "\n", "# We will also clean up and save the functional word list, although", "\n", "# we do not need to use it.", "\n", "authorData", "=", "{", "}", "# Create dictionary", "\n", "for", "it", "in", "range", "(", "len", "(", "rawData", "[", "'all_authors'", "]", ")", ")", ":", "\n", "            ", "thisAuthor", "=", "str", "(", "rawData", "[", "'all_authors'", "]", "[", "it", "]", "[", "0", "]", "[", "0", "]", "[", "0", "]", ")", "\n", "# Each element in rawData['all_authors'] is nested in a couple", "\n", "# of lists, so that's why we need the three indices [0][0][0] ", "\n", "# to reach the string with the actual author name.", "\n", "# Get the word frequency", "\n", "thisWordFreq", "=", "rawData", "[", "'all_freqs'", "]", "[", "0", "]", "[", "it", "]", "# 1 x nWords x nData", "\n", "# Again, the [0] is due to the structure of the data", "\n", "# Let us get rid of that extra 1, and then transpose this to be", "\n", "# stored as nData x nWords (since nWords is the dimension of ", "\n", "# the number of nodes the network will have; CS notation)", "\n", "thisWordFreq", "=", "thisWordFreq", ".", "squeeze", "(", "0", ")", ".", "T", "# nData x nWords", "\n", "# Finally, get the WANs", "\n", "thisWAN", "=", "rawData", "[", "'all_wans'", "]", "[", "0", "]", "[", "it", "]", "# nWords x nWords x nData", "\n", "thisWAN", "=", "thisWAN", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "# nData x nWords x nWords", "\n", "# Obs.: thisWAN is likely not symmetric, so the way this is ", "\n", "# transposed matters. In this case, since thisWAN was intended", "\n", "# to be a tensor in matlab (where the last index is the ", "\n", "# collection of matrices), we just throw that last dimension to", "\n", "# the front (since numpy consider the first index as the ", "\n", "# collection index).", "\n", "# Now we can create the dictionary and save the corresopnding", "\n", "# data.", "\n", "authorData", "[", "thisAuthor", "]", "=", "{", "}", "\n", "authorData", "[", "thisAuthor", "]", "[", "'wordFreq'", "]", "=", "thisWordFreq", "\n", "authorData", "[", "thisAuthor", "]", "[", "'WAN'", "]", "=", "thisWAN", "\n", "# And at last, gather the list of functional words", "\n", "", "functionWords", "=", "[", "]", "# empty list to store the functional words", "\n", "for", "word", "in", "rawData", "[", "'function_words'", "]", ":", "\n", "            ", "functionWords", ".", "append", "(", "str", "(", "word", "[", "0", "]", "[", "0", "]", "[", "0", "]", ")", ")", "\n", "# Store all the data recently collected", "\n", "", "self", ".", "authorData", "=", "authorData", "\n", "self", ".", "allFunctionWords", "=", "functionWords", "\n", "self", ".", "functionWords", "=", "functionWords", ".", "copy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Authorship.getAuthorData": [[887, 937], ["len", "len", "type", "numpy.random.choice", "newShape.extend", "x[].reshape", "len", "len", "newShape.extend", "xNew.reshape", "list", "list"], "methods", ["None"], ["", "def", "getAuthorData", "(", "self", ",", "samplesType", ",", "dataType", ",", "*", "args", ")", ":", "\n", "# dataType: train, valid, test", "\n", "# args: 0 args, give back all", "\n", "# args: 1 arg: if int, give that number of samples, chosen at random", "\n", "# args: 1 arg: if list, give those samples precisely.", "\n", "# Check that the type is one of the possible ones", "\n", "        ", "assert", "samplesType", "==", "'train'", "or", "samplesType", "==", "'valid'", "or", "samplesType", "==", "'test'", "or", "samplesType", "==", "'all'", "\n", "# Check that the dataType is either wordFreq or WAN", "\n", "assert", "dataType", "==", "'WAN'", "or", "dataType", "==", "'wordFreq'", "\n", "# Check that the number of extra arguments fits", "\n", "assert", "len", "(", "args", ")", "<=", "1", "\n", "# If there are no arguments, just return all the desired samples", "\n", "x", "=", "self", ".", "selectedAuthor", "[", "samplesType", "]", "[", "dataType", "]", "\n", "# If there's an argument, we have to check whether it is an int or a", "\n", "# list", "\n", "if", "len", "(", "args", ")", "==", "1", ":", "\n", "# If it is an int, just return that number of randomly chosen", "\n", "# samples.", "\n", "            ", "if", "type", "(", "args", "[", "0", "]", ")", "==", "int", ":", "\n", "                ", "nSamples", "=", "x", ".", "shape", "[", "0", "]", "# total number of samples", "\n", "# We can't return more samples than there are available", "\n", "assert", "args", "[", "0", "]", "<=", "nSamples", "\n", "# Randomly choose args[0] indices", "\n", "selectedIndices", "=", "np", ".", "random", ".", "choice", "(", "nSamples", ",", "size", "=", "args", "[", "0", "]", ",", "\n", "replace", "=", "False", ")", "\n", "# The reshape is to avoid squeezing if only one sample is", "\n", "# requested (because x can have two or three dimension, we", "\n", "# need to take a longer path here, so we will only do it", "\n", "# if args[0] is equal to 1.)", "\n", "if", "args", "[", "0", "]", "==", "1", ":", "\n", "                    ", "newShape", "=", "[", "1", "]", "\n", "newShape", ".", "extend", "(", "list", "(", "x", ".", "shape", "[", "1", ":", "]", ")", ")", "\n", "x", "=", "x", "[", "selectedIndices", "]", ".", "reshape", "(", "newShape", ")", "\n", "", "", "else", ":", "\n", "# The fact that we put else here instead of elif type()==list", "\n", "# allows for np.array to be used as indices as well. In general,", "\n", "# any variable with the ability to index.", "\n", "                ", "xNew", "=", "x", "[", "args", "[", "0", "]", "]", "\n", "# If only one element is selected, avoid squeezing. Given that", "\n", "# the element can be a list (which has property len) or an", "\n", "# np.array (which doesn't have len, but shape), then we can", "\n", "# only avoid squeezing if we check that it has been sequeezed", "\n", "# (or not)", "\n", "if", "len", "(", "xNew", ".", "shape", ")", "<=", "len", "(", "x", ".", "shape", ")", ":", "\n", "                    ", "newShape", "=", "[", "1", "]", "\n", "newShape", ".", "extend", "(", "list", "(", "x", ".", "shape", "[", "1", ":", "]", ")", ")", "\n", "x", "=", "xNew", ".", "reshape", "(", "newShape", ")", "\n", "\n", "", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Authorship.createGraph": [[938, 977], ["alegnn.createGraph", "alegnn.createGraph.astype", "dataTools.Authorship.samples.keys", "len", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.createGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "createGraph", "(", "self", ")", ":", "\n", "\n", "# Save list of nodes to keep to later update the datasets with the", "\n", "# appropriate words", "\n", "        ", "nodesToKeep", "=", "[", "]", "\n", "# Number of nodes (so far) = Number of functional words", "\n", "N", "=", "self", ".", "selectedAuthor", "[", "'all'", "]", "[", "'wordFreq'", "]", ".", "shape", "[", "1", "]", "\n", "# Create graph", "\n", "graphOptions", "=", "{", "}", "\n", "graphOptions", "[", "'adjacencyMatrices'", "]", "=", "self", ".", "selectedAuthor", "[", "'train'", "]", "[", "'WAN'", "]", "\n", "graphOptions", "[", "'nodeList'", "]", "=", "nodesToKeep", "\n", "graphOptions", "[", "'aggregationType'", "]", "=", "'sum'", "\n", "graphOptions", "[", "'normalizationType'", "]", "=", "self", ".", "graphNormalizationType", "\n", "graphOptions", "[", "'isolatedNodes'", "]", "=", "self", ".", "keepIsolatedNodes", "\n", "graphOptions", "[", "'forceUndirected'", "]", "=", "self", ".", "forceUndirected", "\n", "graphOptions", "[", "'forceConnected'", "]", "=", "self", ".", "forceConnected", "\n", "W", "=", "graph", ".", "createGraph", "(", "'fuseEdges'", ",", "N", ",", "graphOptions", ")", "\n", "# Obs.: We do not need to recall graphOptions['nodeList'] as nodesToKeep", "\n", "# since these are all passed as pointers that point to the same list, so", "\n", "# modifying graphOptions also modifies nodesToKeep.", "\n", "# Store adjacency matrix", "\n", "self", ".", "adjacencyMatrix", "=", "W", ".", "astype", "(", "np", ".", "float64", ")", "\n", "# Update data", "\n", "#   For each dataset split", "\n", "for", "key", "in", "self", ".", "samples", ".", "keys", "(", ")", ":", "\n", "#   Check the signals have been loaded", "\n", "            ", "if", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "is", "not", "None", ":", "\n", "#   And check which is the dimension of the nodes (i.e. whether", "\n", "#   it was expanded or not, since we always need to keep the", "\n", "#   entries of the last dimension)", "\n", "                ", "if", "len", "(", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", ".", "shape", ")", "==", "2", ":", "\n", "                    ", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "=", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "[", ":", ",", "nodesToKeep", "]", "\n", "", "elif", "len", "(", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", ".", "shape", ")", "==", "2", ":", "\n", "                    ", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "=", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "[", ":", ",", ":", ",", "nodesToKeep", "]", "\n", "\n", "", "", "", "if", "self", ".", "allFunctionWords", "is", "not", "None", ":", "\n", "            ", "self", ".", "functionWords", "=", "[", "self", ".", "allFunctionWords", "[", "w", "]", "for", "w", "in", "nodesToKeep", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Authorship.getGraph": [[978, 981], ["None"], "methods", ["None"], ["", "", "def", "getGraph", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "adjacencyMatrix", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Authorship.getFunctionWords": [[982, 985], ["None"], "methods", ["None"], ["", "def", "getFunctionWords", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "functionWords", ",", "self", ".", "allFunctionWords", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Authorship.astype": [[986, 998], ["dataTools.Authorship.selectedAuthor.keys", "dataTools.changeDataType", "dataTools._data.astype", "dataTools.Authorship.selectedAuthor[].keys", "dataTools.changeDataType"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType"], ["", "def", "astype", "(", "self", ",", "dataType", ")", ":", "\n", "# This changes the type for the selected author as well as the samples", "\n", "        ", "for", "key", "in", "self", ".", "selectedAuthor", ".", "keys", "(", ")", ":", "\n", "            ", "for", "secondKey", "in", "self", ".", "selectedAuthor", "[", "key", "]", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "selectedAuthor", "[", "key", "]", "[", "secondKey", "]", "=", "changeDataType", "(", "\n", "self", ".", "selectedAuthor", "[", "key", "]", "[", "secondKey", "]", ",", "\n", "dataType", ")", "\n", "", "", "self", ".", "adjacencyMatrix", "=", "changeDataType", "(", "self", ".", "adjacencyMatrix", ",", "dataType", ")", "\n", "\n", "# And now, initialize to change the samples as well (and also save the ", "\n", "# data type)", "\n", "super", "(", ")", ".", "astype", "(", "dataType", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Authorship.to": [[1000, 1013], ["repr", "dataTools.Authorship.selectedAuthor.keys", "dataTools.Authorship.adjacencyMatrix.to", "dataTools._data.to", "dataTools.Authorship.selectedAuthor[].keys", "[].to"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# If the dataType is 'torch'", "\n", "        ", "if", "'torch'", "in", "repr", "(", "self", ".", "dataType", ")", ":", "\n", "# Change the selected author ('test', 'train', 'valid', 'all';", "\n", "# 'WANs', 'wordFreq')", "\n", "            ", "for", "key", "in", "self", ".", "selectedAuthor", ".", "keys", "(", ")", ":", "\n", "                ", "for", "secondKey", "in", "self", ".", "selectedAuthor", "[", "key", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "self", ".", "selectedAuthor", "[", "key", "]", "[", "secondKey", "]", "=", "self", ".", "selectedAuthor", "[", "key", "]", "[", "secondKey", "]", ".", "to", "(", "device", ")", "\n", "", "", "self", ".", "adjacencyMatrix", ".", "to", "(", "device", ")", "\n", "# And call the inherit method to initialize samples (and save to", "\n", "# device)", "\n", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.__init__": [[1131, 1533], ["dataTools._data.__init__", "dataTools.MovieLens.loadData", "numpy.sum", "numpy.random.permutation", "len", "round", "round", "numpy.empty", "numpy.empty", "numpy.empty().astype", "len", "numpy.empty", "numpy.empty", "numpy.empty().astype", "len", "numpy.empty", "numpy.empty", "numpy.empty().astype", "len", "numpy.concatenate", "dataTools.MovieLens.createGraph", "dataTools.MovieLens.astype", "dataTools.MovieLens.to", "numpy.sum", "numpy.sum", "numpy.arange", "numpy.arange", "len", "numpy.empty", "numpy.arange", "numpy.nonzero", "len", "numpy.tile", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "len", "numpy.tile", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "len", "numpy.tile", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "dataTools.MovieLens.interpolateRatings", "type", "numpy.nonzero", "numpy.nonzero", "numpy.sum", "numpy.argsort", "numpy.sum", "numpy.argsort", "numpy.concatenate", "numpy.nonzero", "numpy.empty", "numpy.nonzero", "numpy.empty", "numpy.nonzero", "numpy.empty", "numpy.nonzero", "len", "numpy.argwhere", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.loadData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.createGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.interpolateRatings"], ["def", "__init__", "(", "self", ",", "graphType", ",", "labelID", ",", "ratioTrain", ",", "ratioValid", ",", "dataDir", ",", "\n", "keepIsolatedNodes", ",", "forceUndirected", ",", "forceConnected", ",", "kNN", ",", "\n", "maxNodes", "=", "None", ",", "\n", "maxDataPoints", "=", "None", ",", "\n", "minRatings", "=", "0", ",", "\n", "interpolate", "=", "False", ",", "\n", "dataType", "=", "np", ".", "float64", ",", "device", "=", "'cpu'", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# This creates the attributes: dataType, device, nTrain, nTest, nValid,", "\n", "# and samples, and fills them all with None, and also creates the ", "\n", "# methods: getSamples, astype, and to.", "\n", "self", ".", "dataType", "=", "dataType", "\n", "self", ".", "device", "=", "device", "\n", "\n", "# Store attributes", "\n", "#   GraphType", "\n", "assert", "graphType", "==", "'user'", "or", "graphType", "==", "'movie'", "\n", "# This is because what are the graph signals depends on the graph we", "\n", "# want to use.", "\n", "self", ".", "graphType", "=", "graphType", "\n", "#   Label ID", "\n", "assert", "type", "(", "labelID", ")", "is", "list", "or", "labelID", "==", "'all'", "\n", "# Label ID is the user ID or the movie ID following the MovieLens ", "\n", "# nomenclature. This determines how we build the labels in the", "\n", "# dataset. If it's all, then we want to estimate for all users/movies.", "\n", "#   Dataset partition", "\n", "self", ".", "ratioTrain", "=", "ratioTrain", "\n", "self", ".", "ratioValid", "=", "ratioValid", "\n", "#   Dataset directory", "\n", "self", ".", "dataDir", "=", "dataDir", "# Where the data is, or where it should be saved", "\n", "# to.", "\n", "#   Graph preferences", "\n", "self", ".", "keepIsolatedNodes", "=", "keepIsolatedNodes", "\n", "self", ".", "forceUndirected", "=", "forceUndirected", "\n", "self", ".", "forceConnected", "=", "forceConnected", "\n", "self", ".", "kNN", "=", "kNN", "\n", "#   Reduce the graph to have maxNodes", "\n", "self", ".", "maxNodes", "=", "maxNodes", "\n", "#   Discard samples with less than minRatings ratings", "\n", "self", ".", "minRatings", "=", "minRatings", "\n", "#   Interpolate nonexisting ratings (i.e. get rid of zeros and replace ", "\n", "#   them by the nearest neighbor rating)", "\n", "self", ".", "doInterpolate", "=", "interpolate", "\n", "#   Empty attributes for now", "\n", "self", ".", "incompleteMatrix", "=", "None", "\n", "self", ".", "movieTitles", "=", "{", "}", "\n", "self", ".", "adjacencyMatrix", "=", "None", "\n", "self", ".", "indexDataPoints", "=", "{", "}", "\n", "\n", "# Now, we should be ready to load the data and build the (incomplete) ", "\n", "# matrix", "\n", "self", ".", "loadData", "(", "'movielens100kIncompleteMatrix.pkl'", ")", "\n", "# This has loaded the incompleteMatrix and movieTitles attributes.", "\n", "\n", "# First check if we might need to get rid of columns and rows to get ", "\n", "# the minimum number of ratings requested", "\n", "if", "self", ".", "minRatings", ">", "0", ":", "\n", "            ", "incompleteMatrix", "=", "self", ".", "incompleteMatrix", "\n", "# Get a one where there are ratings, and a 0 where there are not", "\n", "binaryIncompleteMatrix", "=", "(", "incompleteMatrix", ">", "0", ")", ".", "astype", "(", "incompleteMatrix", ".", "dtype", ")", "\n", "# Count the number of ratings in each row", "\n", "nRatingsPerRow", "=", "np", ".", "sum", "(", "binaryIncompleteMatrix", ",", "axis", "=", "1", ")", "\n", "# Count the number of ratings in each column", "\n", "nRatingsPerCol", "=", "np", ".", "sum", "(", "binaryIncompleteMatrix", ",", "axis", "=", "0", ")", "\n", "# Indices of rows and columns to keep", "\n", "indexRowsToKeep", "=", "np", ".", "nonzero", "(", "nRatingsPerRow", ">", "self", ".", "minRatings", ")", "[", "0", "]", "\n", "indexColsToKeep", "=", "np", ".", "nonzero", "(", "nRatingsPerCol", ">", "self", ".", "minRatings", ")", "[", "0", "]", "\n", "# Reduce the size of the matrix", "\n", "incompleteMatrix", "=", "incompleteMatrix", "[", "indexRowsToKeep", "]", "[", ":", ",", "indexColsToKeep", "]", "\n", "# Store it", "\n", "self", ".", "incompleteMatrix", "=", "incompleteMatrix", "\n", "\n", "# Also, we need to consider that, if we have the movie graph, ", "\n", "# then we need to update the movie list as well (all the columns", "\n", "# we lost -the nodes we lost- are part of a movie list that", "\n", "# has a one-to-one correspondence)", "\n", "if", "self", ".", "graphType", "==", "'movie'", ":", "\n", "                ", "if", "len", "(", "self", ".", "movieTitles", ")", ">", "0", ":", "# Non empty movieList", "\n", "# Where to save the new movie list", "\n", "                    ", "movieTitles", "=", "{", "}", "\n", "# Because nodes are now numbered sequentially, we need to", "\n", "# do the same with the movieID to keep them matched (i.e.", "\n", "# node n corresponds to movieList[n] title)", "\n", "newMovieID", "=", "0", "\n", "for", "movieID", "in", "indexColsToKeep", ":", "\n", "                        ", "movieTitles", "[", "newMovieID", "]", "=", "self", ".", "movieTitles", "[", "movieID", "]", "\n", "newMovieID", "=", "newMovieID", "+", "1", "\n", "# Update movieList", "\n", "", "self", ".", "movieTitles", "=", "movieTitles", "\n", "", "", "", "else", ":", "\n", "# If there was no need to reduce the columns or rows", "\n", "            ", "indexRowsToKeep", "=", "np", ".", "arange", "(", "self", ".", "incompleteMatrix", ".", "shape", "[", "0", "]", ")", "\n", "indexColsToKeep", "=", "np", ".", "arange", "(", "self", ".", "incompleteMatrix", ".", "shape", "[", "1", "]", ")", "\n", "\n", "# To simplify code, we will work always with each row being a data", "\n", "# sample. The incompleteMatrix is User x Movies", "\n", "", "if", "graphType", "==", "'user'", ":", "\n", "# If we want to reduce the number of nodes (i.e. is not None), and", "\n", "# we want less nodes than the ones that actually there", "\n", "            ", "if", "maxNodes", "is", "not", "None", "and", "maxNodes", "<", "self", ".", "incompleteMatrix", ".", "shape", "[", "0", "]", ":", "\n", "# The number of columns in the matrix is the number of nodes,", "\n", "# therefore, each column is a node, and the number of nonzero", "\n", "# elements in each node is the number of ratings for each movie", "\n", "                ", "nRatings", "=", "np", ".", "sum", "(", "(", "self", ".", "incompleteMatrix", ">", "zeroTolerance", ")", ",", "\n", "axis", "=", "1", ")", "\n", "# Order the nodes in decreasing order of number of ratings", "\n", "indexRowsToKeep", "=", "np", ".", "argsort", "(", "-", "nRatings", ")", "\n", "# Keep only the first nNodes", "\n", "indexRowsToKeep", "=", "indexRowsToKeep", "[", "0", ":", "maxNodes", "]", "\n", "# And reduce the size of the matrix", "\n", "self", ".", "incompleteMatrix", "=", "self", ".", "incompleteMatrix", "[", "indexRowsToKeep", ",", ":", "]", "\n", "# If the graph type is user-based, then the graph signals are the", "\n", "# movies, scored for every user. This means that each column of the", "\n", "# incompleteMatrix is a graph signal, but since we're working with", "\n", "# rows, we have to transpose it", "\n", "", "workingMatrix", "=", "self", ".", "incompleteMatrix", ".", "T", "# Movies x User", "\n", "# Which one correspond to the nodes", "\n", "indexNodesToKeep", "=", "indexRowsToKeep", "\n", "\n", "# Now, each row is a movie score for all users, so that it is a", "\n", "# graph signal in the user-based graph.", "\n", "", "else", ":", "\n", "            ", "if", "maxNodes", "is", "not", "None", "and", "maxNodes", "<", "self", ".", "incompleteMatrix", ".", "shape", "[", "1", "]", ":", "\n", "                ", "nRatings", "=", "np", ".", "sum", "(", "(", "self", ".", "incompleteMatrix", ">", "zeroTolerance", ")", ",", "\n", "axis", "=", "0", ")", "\n", "indexColsToKeep", "=", "np", ".", "argsort", "(", "-", "nRatings", ")", "\n", "indexColsToKeep", "=", "indexColsToKeep", "[", "0", ":", "maxNodes", "]", "\n", "self", ".", "incompleteMatrix", "=", "self", ".", "incompleteMatrix", "[", ":", ",", "indexColsToKeep", "]", "\n", "", "workingMatrix", "=", "self", ".", "incompleteMatrix", "\n", "# In this case, each row is a user (how that user scored all movies)", "\n", "# and this is the kind of data samples we need for movie-based", "\n", "# graphs", "\n", "indexNodesToKeep", "=", "indexColsToKeep", "\n", "\n", "# Determine the number of nodes", "\n", "", "nNodes", "=", "workingMatrix", ".", "shape", "[", "1", "]", "\n", "\n", "assert", "len", "(", "indexNodesToKeep", ")", "==", "nNodes", "\n", "\n", "# And we need to map the original IDs to the new ones (note that", "\n", "# each column is a node now -each row is a graph signal- so we", "\n", "# care about matching the labels to the corresponding new ones)", "\n", "#   First check, that, unless we wanted all indices (so we don't", "\n", "#   care much about the ones we just dropped), we have them in the", "\n", "#   new indices (i.e. we didn't drop them)        ", "\n", "if", "labelID", "!=", "'all'", ":", "\n", "# For each of the introduced IDs, check:", "\n", "            ", "self", ".", "labelID", "=", "np", ".", "empty", "(", "0", ",", "dtype", "=", "np", ".", "int", ")", "\n", "for", "i", "in", "labelID", ":", "\n", "# Recall that labelID they start with 1, but indexNodesToKeep", "\n", "# starts with zero", "\n", "                ", "assert", "(", "i", "-", "1", ")", "in", "indexNodesToKeep", "\n", "newIndex", "=", "np", ".", "argwhere", "(", "indexNodesToKeep", "==", "(", "i", "-", "1", ")", ")", "[", "0", "]", "\n", "self", ".", "labelID", "=", "np", ".", "concatenate", "(", "(", "self", ".", "labelID", ",", "newIndex", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "labelID", "=", "np", ".", "arange", "(", "nNodes", ")", "\n", "\n", "# Up to this point, we just have an array of IDs of nodes we care about", "\n", "# This could be all, one or a few, but is a numpy.array", "\n", "\n", "# So, now we just select a number of rows (graph signals) at random", "\n", "# to make the train and valid and test set. But we need to keep", "\n", "# track of the ID (the node)", "\n", "# The total number of points is now the number of nonzero elements", "\n", "# of the matrix. The problem is that we cannot get a random number", "\n", "# of nonzero elements of the matrix, because we're risking selecting", "\n", "# all rows (graph signals), and thus not leaving anything for the", "\n", "# train and test set. In other words, the rows determine the graph", "\n", "# signals, and all the nonzero elements of each row will make up", "\n", "# for the points in each training set.", "\n", "\n", "# Next we reduce the size of the matrix to the ones that we are", "\n", "# interested in", "\n", "", "selectedMatrix", "=", "workingMatrix", "[", ":", ",", "self", ".", "labelID", "]", "\n", "\n", "# So far we've got the value of all graph signals only on the nodes", "\n", "# of interest (some of these might just be zero, if the nodes of", "\n", "# interest weren't rated by that given graph signal)", "\n", "\n", "# Get rid of those rows that have no ratings for the labels of", "\n", "# interest", "\n", "#   We sum all the rows: since all the ratings are positive, those", "\n", "#   rows that are zero is because they have no ratings", "\n", "nonzeroRows", "=", "np", ".", "sum", "(", "selectedMatrix", ",", "axis", "=", "1", ")", "\n", "nonzeroRows", "=", "np", ".", "nonzero", "(", "nonzeroRows", ")", "[", "0", "]", "\n", "selectedMatrix", "=", "selectedMatrix", "[", "nonzeroRows", ",", ":", "]", "\n", "\n", "# Now, we move on to count the total number of graph signals that", "\n", "# we have (number of rows)", "\n", "nRows", "=", "selectedMatrix", ".", "shape", "[", "0", "]", "\n", "# Permute the indices at random", "\n", "randPerm", "=", "np", ".", "random", ".", "permutation", "(", "nRows", ")", "\n", "# This gives me a random way of going through all the rows. So we", "\n", "# will do that, going row by row, picking all the nonzero elements", "\n", "# in said row, until we reach the (closest possible) number to the", "\n", "# amount of training samples we want.", "\n", "# The point of this is that each row might have more than one", "\n", "# data point: i.e. some graph signal might have rated more than one", "\n", "# of the nodes of interest; therefore this would amount to having", "\n", "# more than one data point stemming from that graph signal -by ", "\n", "# zero-ing out each of the nodes separately-", "\n", "#   Total number of available samples (whether to take the 0 or the ", "\n", "#   1 element of the set is indistinct, they both have the same len)", "\n", "nDataPoints", "=", "len", "(", "np", ".", "nonzero", "(", "selectedMatrix", ")", "[", "0", "]", ")", "\n", "#   Check if the total number of desired samples has been defined", "\n", "#   (a max number of data points could have been set if we want", "\n", "#   to randomly select a subset of all available datapoints, for", "\n", "#   running a faster training)", "\n", "if", "maxDataPoints", "is", "None", ":", "\n", "            ", "maxDataPoints", "=", "nDataPoints", "\n", "#   and if it was designed, if it is not greater than the total ", "\n", "#   number of data points available", "\n", "", "elif", "maxDataPoints", ">", "nDataPoints", ":", "\n", "            ", "maxDataPoints", "=", "nDataPoints", "\n", "", "self", ".", "maxDataPoints", "=", "maxDataPoints", "\n", "# Target number of train, valid and test samples", "\n", "nTrain", "=", "round", "(", "ratioTrain", "*", "maxDataPoints", ")", "\n", "nValid", "=", "round", "(", "ratioValid", "*", "nTrain", ")", "\n", "nTrain", "=", "nTrain", "-", "nValid", "\n", "nTest", "=", "maxDataPoints", "-", "nTrain", "-", "nValid", "\n", "\n", "# TODO: There has to be a way of accelerating this thing below", "\n", "\n", "# Training count", "\n", "nTrainSoFar", "=", "0", "\n", "rowCounter", "=", "0", "\n", "# Save variables", "\n", "trainSignals", "=", "np", ".", "empty", "(", "[", "0", ",", "nNodes", "]", ")", "\n", "trainLabels", "=", "np", ".", "empty", "(", "0", ")", "\n", "trainIDs", "=", "np", ".", "empty", "(", "0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "while", "nTrainSoFar", "<", "nTrain", "and", "rowCounter", "<", "nRows", ":", "\n", "# Get the corresponding selected row", "\n", "            ", "thisRow", "=", "selectedMatrix", "[", "randPerm", "[", "rowCounter", "]", ",", ":", "]", "\n", "# Get the indices of the nonzero elements of interest (i.e", "\n", "# of all the nodes of interest, which ones have a nonzero", "\n", "# rating on this graph signal)", "\n", "thisNZcols", "=", "np", ".", "nonzero", "(", "thisRow", ")", "[", "0", "]", "# Nonzero Cols", "\n", "# And now we can match this to the corresponding columns in the", "\n", "# original matrix", "\n", "thisIDs", "=", "self", ".", "labelID", "[", "thisNZcols", "]", "\n", "thisNpoints", "=", "len", "(", "thisIDs", ")", "\n", "# Get the labels", "\n", "thisLabels", "=", "thisRow", "[", "thisNZcols", "]", "\n", "# Get the signals", "\n", "thisSignals", "=", "workingMatrix", "[", "nonzeroRows", "[", "randPerm", "[", "rowCounter", "]", "]", ",", ":", "]", "\n", "# From this signal (taken from the original working matrix) we ", "\n", "# will obtain as many signals as nonzero ratings of the nodes of", "\n", "# interest. Therefore, we need to repeat it to that point", "\n", "thisSignals", "=", "np", ".", "tile", "(", "thisSignals", ",", "[", "thisNpoints", ",", "1", "]", ")", "\n", "#   thisNpoints x nNodes", "\n", "#   We need to zero-out those elements that will be part of", "\n", "#   the samples", "\n", "thisSignals", "[", "np", ".", "arange", "(", "thisNpoints", ")", ",", "thisIDs", "]", "=", "0", "\n", "# And now we should be able to concatenate", "\n", "trainSignals", "=", "np", ".", "concatenate", "(", "(", "trainSignals", ",", "thisSignals", ")", ",", "\n", "axis", "=", "0", ")", "\n", "trainLabels", "=", "np", ".", "concatenate", "(", "(", "trainLabels", ",", "thisLabels", ")", ")", "\n", "trainIDs", "=", "np", ".", "concatenate", "(", "(", "trainIDs", ",", "thisIDs", ")", ")", "\n", "# Add how many new data points we have just got", "\n", "nTrainSoFar", "+=", "thisNpoints", "\n", "# And increase the counter", "\n", "rowCounter", "+=", "1", "\n", "# We have finalized the training set. Now, we have to count how", "\n", "# many training samples we actually have", "\n", "", "self", ".", "nTrain", "=", "len", "(", "trainLabels", ")", "\n", "# We also want to know which rows we have selected so far", "\n", "indexTrainPoints", "=", "nonzeroRows", "[", "randPerm", "[", "0", ":", "rowCounter", "]", "]", "\n", "nRowsTrain", "=", "rowCounter", "\n", "\n", "# Now, repeat for validation set:", "\n", "nValidSoFar", "=", "0", "\n", "rowCounter", "=", "nRowsTrain", "# Initialize where the other one left off", "\n", "# Save variables", "\n", "validSignals", "=", "np", ".", "empty", "(", "[", "0", ",", "nNodes", "]", ")", "\n", "validLabels", "=", "np", ".", "empty", "(", "0", ")", "\n", "validIDs", "=", "np", ".", "empty", "(", "0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "while", "nValidSoFar", "<", "nValid", "and", "rowCounter", "<", "nRows", ":", "\n", "# Get the corresponding selected row", "\n", "            ", "thisRow", "=", "selectedMatrix", "[", "randPerm", "[", "rowCounter", "]", ",", ":", "]", "\n", "# Get the indices of the nonzero elements of interest (i.e", "\n", "# of all the nodes of interest, which ones have a nonzero", "\n", "# rating on this graph signal)", "\n", "thisNZcols", "=", "np", ".", "nonzero", "(", "thisRow", ")", "[", "0", "]", "# Nonzero Cols", "\n", "# And now we can match this to the corresponding columns in the", "\n", "# original matrix", "\n", "thisIDs", "=", "self", ".", "labelID", "[", "thisNZcols", "]", "\n", "thisNpoints", "=", "len", "(", "thisIDs", ")", "\n", "# Get the labels", "\n", "thisLabels", "=", "thisRow", "[", "thisNZcols", "]", "\n", "# Get the signals", "\n", "thisSignals", "=", "workingMatrix", "[", "nonzeroRows", "[", "randPerm", "[", "rowCounter", "]", "]", ",", ":", "]", "\n", "# From this signal (taken from the original working matrix) we ", "\n", "# will obtain as many signals as nonzero ratings of the nodes of", "\n", "# interest. Therefore, we need to repeat it to that point", "\n", "thisSignals", "=", "np", ".", "tile", "(", "thisSignals", ",", "[", "thisNpoints", ",", "1", "]", ")", "\n", "#   thisNpoints x nNodes", "\n", "#   We need to zero-out those elements that will be part of", "\n", "#   the samples", "\n", "thisSignals", "[", "np", ".", "arange", "(", "thisNpoints", ")", ",", "thisIDs", "]", "=", "0", "\n", "# And now we should be able to concatenate", "\n", "validSignals", "=", "np", ".", "concatenate", "(", "(", "validSignals", ",", "thisSignals", ")", ",", "\n", "axis", "=", "0", ")", "\n", "validLabels", "=", "np", ".", "concatenate", "(", "(", "validLabels", ",", "thisLabels", ")", ")", "\n", "validIDs", "=", "np", ".", "concatenate", "(", "(", "validIDs", ",", "thisIDs", ")", ")", "\n", "# Add how many new data points we have just got", "\n", "nValidSoFar", "+=", "thisNpoints", "\n", "# And increase the counter", "\n", "rowCounter", "+=", "1", "\n", "# We have finalized the validation set. Now, we have to count how", "\n", "# many validation samples we actually have", "\n", "", "self", ".", "nValid", "=", "len", "(", "validLabels", ")", "\n", "# We also want to know which rows we have selected so far", "\n", "indexValidPoints", "=", "nonzeroRows", "[", "randPerm", "[", "nRowsTrain", ":", "rowCounter", "]", "]", "\n", "nRowsValid", "=", "rowCounter", "-", "nRowsTrain", "\n", "\n", "# And, finally the test set", "\n", "nTestSoFar", "=", "0", "\n", "rowCounter", "=", "nRowsTrain", "+", "nRowsValid", "\n", "# Save variables", "\n", "testSignals", "=", "np", ".", "empty", "(", "[", "0", ",", "nNodes", "]", ")", "\n", "testLabels", "=", "np", ".", "empty", "(", "0", ")", "\n", "testIDs", "=", "np", ".", "empty", "(", "0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "while", "nTestSoFar", "<", "nTest", "and", "rowCounter", "<", "nRows", ":", "\n", "# Get the corresponding selected row", "\n", "            ", "thisRow", "=", "selectedMatrix", "[", "randPerm", "[", "rowCounter", "]", ",", ":", "]", "\n", "# Get the indices of the nonzero elements of interest (i.e", "\n", "# of all the nodes of interest, which ones have a nonzero", "\n", "# rating on this graph signal)", "\n", "thisNZcols", "=", "np", ".", "nonzero", "(", "thisRow", ")", "[", "0", "]", "# Nonzero Cols", "\n", "# And now we can match this to the corresponding columns in the", "\n", "# original matrix", "\n", "thisIDs", "=", "self", ".", "labelID", "[", "thisNZcols", "]", "\n", "thisNpoints", "=", "len", "(", "thisIDs", ")", "\n", "# Get the labels", "\n", "thisLabels", "=", "thisRow", "[", "thisNZcols", "]", "\n", "# Get the signals", "\n", "thisSignals", "=", "workingMatrix", "[", "nonzeroRows", "[", "randPerm", "[", "rowCounter", "]", "]", ",", ":", "]", "\n", "# From this signal (taken from the original working matrix) we ", "\n", "# will obtain as many signals as nonzero ratings of the nodes of", "\n", "# interest. Therefore, we need to repeat it to that point", "\n", "thisSignals", "=", "np", ".", "tile", "(", "thisSignals", ",", "[", "thisNpoints", ",", "1", "]", ")", "\n", "#   thisNpoints x nNodes", "\n", "#   We need to zero-out those elements that will be part of", "\n", "#   the samples", "\n", "thisSignals", "[", "np", ".", "arange", "(", "thisNpoints", ")", ",", "thisIDs", "]", "=", "0", "\n", "# And now we should be able to concatenate", "\n", "testSignals", "=", "np", ".", "concatenate", "(", "(", "testSignals", ",", "thisSignals", ")", ",", "\n", "axis", "=", "0", ")", "\n", "testLabels", "=", "np", ".", "concatenate", "(", "(", "testLabels", ",", "thisLabels", ")", ")", "\n", "testIDs", "=", "np", ".", "concatenate", "(", "(", "testIDs", ",", "thisIDs", ")", ")", "\n", "# Add how many new data points we have just got", "\n", "nTestSoFar", "+=", "thisNpoints", "\n", "# And increase the counter", "\n", "rowCounter", "+=", "1", "\n", "# We have finalized the validation set. Now, we have to count how", "\n", "# many validation samples we actually have", "\n", "", "self", ".", "nTest", "=", "len", "(", "testLabels", ")", "\n", "# We also want to know which rows we have selected so far", "\n", "indexTestPoints", "=", "nonzeroRows", "[", "randPerm", "[", "nRowsTrain", "+", "nRowsValid", ":", "rowCounter", "]", "]", "\n", "\n", "# And we also need all the data points (all the rows), so:", "\n", "indexDataPoints", "=", "np", ".", "concatenate", "(", "(", "indexTrainPoints", ",", "\n", "indexValidPoints", ",", "\n", "indexTestPoints", ")", ")", "\n", "\n", "# Now, this finalizes the data split, now, so we have all we need:", "\n", "# signals, labels, and IDs.", "\n", "\n", "# So far, either by selecting a node, or by selecting all nodes, we", "\n", "# have the variables we need: signals, labels, IDs and index points.", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "trainSignals", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "trainLabels", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "=", "validSignals", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "=", "validLabels", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "=", "testSignals", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "=", "testLabels", "\n", "self", ".", "targetIDs", "=", "{", "}", "\n", "self", ".", "targetIDs", "[", "'train'", "]", "=", "trainIDs", "\n", "self", ".", "targetIDs", "[", "'valid'", "]", "=", "validIDs", "\n", "self", ".", "targetIDs", "[", "'test'", "]", "=", "testIDs", "\n", "# And update the index of the data points (which are the rows selected)", "\n", "self", ".", "indexDataPoints", "[", "'all'", "]", "=", "indexDataPoints", "\n", "self", ".", "indexDataPoints", "[", "'train'", "]", "=", "indexTrainPoints", "\n", "self", ".", "indexDataPoints", "[", "'valid'", "]", "=", "indexValidPoints", "\n", "self", ".", "indexDataPoints", "[", "'test'", "]", "=", "indexTestPoints", "\n", "\n", "# Now the data has been loaded, and the training/test partition has been", "\n", "# made, create the graph", "\n", "self", ".", "createGraph", "(", ")", "\n", "# Observe that this graph also adjusts the signals to reflect any change", "\n", "# in the number of nodes", "\n", "\n", "# Finally, check if we want to interpolate the useless zeros", "\n", "if", "self", ".", "doInterpolate", ":", "\n", "            ", "self", ".", "interpolateRatings", "(", ")", "\n", "\n", "# Change data to specified type and device", "\n", "", "self", ".", "astype", "(", "self", ".", "dataType", ")", "\n", "self", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.loadData": [[1534, 1637], ["os.path.join", "os.path.isfile", "len", "os.path.exists", "os.makedirs", "os.path.isfile", "os.path.join", "os.path.isfile", "os.path.join", "os.path.isfile", "numpy.empty", "open", "pickle.load", "os.path.isfile", "os.path.isfile", "os.path.join", "urllib.request.urlretrieve", "zipfile.ZipFile", "zipfile.ZipFile.extractall", "zipfile.ZipFile.close", "open", "open", "open", "pickle.dump", "os.path.join", "os.path.join", "os.path.join", "dataLine.rstrip().split", "int", "int", "int", "movieLine.rstrip().split", "os.path.join", "numpy.zeros", "numpy.concatenate", "numpy.zeros", "numpy.concatenate", "int", "dataLine.rstrip", "movieLine.rstrip"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.close"], ["", "def", "loadData", "(", "self", ",", "filename", ",", "*", "args", ")", ":", "\n", "# Here we offer the option of including an additional dir, if not, use", "\n", "# the internally stored one.", "\n", "        ", "if", "len", "(", "args", ")", "==", "1", ":", "\n", "            ", "dataDir", "=", "args", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "assert", "self", ".", "dataDir", "is", "not", "None", "\n", "dataDir", "=", "self", ".", "dataDir", "\n", "\n", "# Check if the dataDir exists, and if not, create it", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "dataDir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "dataDir", ")", "\n", "# Create the filename to save/load", "\n", "", "datasetFilename", "=", "os", ".", "path", ".", "join", "(", "dataDir", ",", "filename", ")", "\n", "# Check if the file does exist, load it", "\n", "if", "os", ".", "path", ".", "isfile", "(", "datasetFilename", ")", ":", "\n", "# If it exists, load it", "\n", "            ", "with", "open", "(", "datasetFilename", ",", "'rb'", ")", "as", "datasetFile", ":", "\n", "                ", "datasetDict", "=", "pickle", ".", "load", "(", "datasetFile", ")", "\n", "# And save the corresponding variable", "\n", "self", ".", "incompleteMatrix", "=", "datasetDict", "[", "'incompleteMatrix'", "]", "\n", "self", ".", "movieTitles", "=", "datasetDict", "[", "'movieTitles'", "]", "\n", "", "", "else", ":", "# If it doesn't exist, load it", "\n", "# There could be three options here: that we have the raw data ", "\n", "# already there, that we have the zip file and need to decompress it,", "\n", "# or that we do not have nothing and we need to download it.", "\n", "            ", "existsRawData", "=", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "dataDir", ",", "'ml-100k'", ",", "'u.data'", ")", ")", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "dataDir", ",", "'ml-100k'", ",", "'u.item'", ")", ")", "\n", "# Actually, we're only interested in the ratings, but we're also", "\n", "# getting the movie list, just in case. Other information that we're", "\n", "# not considering at the moment includes: genres, user demographics", "\n", "existsZipFile", "=", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "dataDir", ",", "'ml-100k.zip'", ")", ")", "\n", "if", "not", "existsRawData", "and", "not", "existsZipFile", ":", "# We have to download it", "\n", "                ", "mlURL", "=", "'http://files.grouplens.org/datasets/movielens/ml-100k.zip'", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "mlURL", ",", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "dataDir", ",", "'ml-100k.zip'", ")", ")", "\n", "existsZipFile", "=", "True", "\n", "", "if", "not", "existsRawData", "and", "existsZipFile", ":", "# Unzip it", "\n", "                ", "zipObject", "=", "zipfile", ".", "ZipFile", "(", "os", ".", "path", ".", "join", "(", "dataDir", ",", "'ml-100k.zip'", ")", ")", "\n", "zipObject", ".", "extractall", "(", "dataDir", ")", "\n", "zipObject", ".", "close", "(", ")", "\n", "# Now that we have the data, we can get their filenames", "\n", "", "rawDataFilename", "=", "os", ".", "path", ".", "join", "(", "dataDir", ",", "'ml-100k'", ",", "'u.data'", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "rawDataFilename", ")", "\n", "rawMovieListFilename", "=", "os", ".", "path", ".", "join", "(", "dataDir", ",", "'ml-100k'", ",", "'u.item'", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "rawMovieListFilename", ")", "\n", "# And we can load it and store it.", "\n", "rawMatrix", "=", "np", ".", "empty", "(", "[", "0", ",", "0", "]", ")", "# Start with an empty matrix and then", "\n", "# we slowly add the number of users and movies, which we do not", "\n", "# assume to be known beforehand", "\n", "# Let's start with the data.", "\n", "# Open it.", "\n", "with", "open", "(", "rawDataFilename", ",", "'r'", ")", "as", "rawData", ":", "\n", "# The file consists of a succession of lines, each line", "\n", "# corresponds to a data sample", "\n", "                ", "for", "dataLine", "in", "rawData", ":", "\n", "# For each line, we split it in the different fields", "\n", "                    ", "dataLineSplit", "=", "dataLine", ".", "rstrip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "# Keep the ones we care about here", "\n", "userID", "=", "int", "(", "dataLineSplit", "[", "0", "]", ")", "\n", "movieID", "=", "int", "(", "dataLineSplit", "[", "1", "]", ")", "\n", "rating", "=", "int", "(", "dataLineSplit", "[", "2", "]", ")", "\n", "# Now we have to add this information to the matrix", "\n", "# The matrix is of size Users x Movies (U x M)", "\n", "#   We need to check whether we need to add more rows", "\n", "#   or more columns", "\n", "if", "userID", ">", "rawMatrix", ".", "shape", "[", "0", "]", ":", "\n", "                        ", "rowDiff", "=", "userID", "-", "rawMatrix", ".", "shape", "[", "0", "]", "\n", "zeroPadRows", "=", "np", ".", "zeros", "(", "[", "rowDiff", ",", "rawMatrix", ".", "shape", "[", "1", "]", "]", ")", "\n", "rawMatrix", "=", "np", ".", "concatenate", "(", "(", "rawMatrix", ",", "zeroPadRows", ")", ",", "\n", "axis", "=", "0", ")", "\n", "", "if", "movieID", ">", "rawMatrix", ".", "shape", "[", "1", "]", ":", "\n", "                        ", "colDiff", "=", "movieID", "-", "rawMatrix", ".", "shape", "[", "1", "]", "\n", "zeroPadCols", "=", "np", ".", "zeros", "(", "[", "rawMatrix", ".", "shape", "[", "0", "]", ",", "colDiff", "]", ")", "\n", "rawMatrix", "=", "np", ".", "concatenate", "(", "(", "rawMatrix", ",", "zeroPadCols", ")", ",", "\n", "axis", "=", "1", ")", "\n", "# Now that we have assured appropriate dimensions", "\n", "", "rawMatrix", "[", "userID", "-", "1", ",", "movieID", "-", "1", "]", "=", "rating", "\n", "# Recall that the count of user and movie ID starts at 1", "\n", "# for the movielens dataset, but we need to start indexing", "\n", "# at 0 for Python", "\n", "# Now that we have created the matrix, we store it", "\n", "", "", "self", ".", "incompleteMatrix", "=", "rawMatrix", "\n", "# And we move to load the movie names", "\n", "\n", "with", "open", "(", "rawMovieListFilename", ",", "'r'", ",", "encoding", "=", "\"ISO-8859-1\"", ")", "as", "rawMovieList", ":", "\n", "# Go line by line (each line corresponds to a movie)", "\n", "                ", "for", "movieLine", "in", "rawMovieList", ":", "\n", "                    ", "movieLineSplit", "=", "movieLine", ".", "rstrip", "(", "'\\n'", ")", ".", "split", "(", "'|'", ")", "\n", "movieID", "=", "int", "(", "movieLineSplit", "[", "0", "]", ")", "-", "1", "\n", "# Look that, in this case, we're making the movies ID match", "\n", "# the column indexing (so it starts at zero)", "\n", "movieTitle", "=", "movieLineSplit", "[", "1", "]", "\n", "self", ".", "movieTitles", "[", "movieID", "]", "=", "movieTitle", "\n", "# And now that we're done, we save this in a pickle file for", "\n", "# posterity", "\n", "", "", "with", "open", "(", "datasetFilename", ",", "'wb'", ")", "as", "datasetFile", ":", "\n", "                ", "pickle", ".", "dump", "(", "\n", "{", "'incompleteMatrix'", ":", "self", ".", "incompleteMatrix", ",", "\n", "'movieTitles'", ":", "self", ".", "movieTitles", "}", ",", "\n", "datasetFile", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.createGraph": [[1639, 2018], ["len", "len", "len", "numpy.nonzero", "len", "int", "int", "len", "workingMatrix.copy", "workingMatrix.dot", "binaryTemplate.dot", "numpy.sqrt", "numpy.diag", "numpy.diag", "numpy.nonzero", "normalizedMatrix.reshape.reshape.reshape", "alegnn.createGraph", "alegnn.sparsifyGraph", "dataTools.MovieLens.indexDataPoints.keys", "dataTools.MovieLens.indexDataPoints.keys", "dataTools.MovieLens.indexDataPoints.keys", "dataTools.MovieLens.indexDataPoints.keys", "round", "numpy.array", "numpy.nonzero", "len", "numpy.random.permutation", "numpy.nonzero", "numpy.nonzero", "len", "numpy.diag", "numpy.diag.dot", "numpy.eye", "numpy.diag", "len", "range", "numpy.array", "numpy.array", "range", "numpy.array", "numpy.array", "range", "numpy.array", "numpy.array", "numpy.nonzero", "numpy.nonzero", "numpy.nonzero", "round", "numpy.concatenate", "numpy.concatenate", "correlationMatrix.dot", "numpy.abs", "len", "len", "len", "range", "numpy.nonzero", "numpy.abs", "numpy.array.append", "numpy.array.append", "numpy.array.append", "numpy.array.append", "numpy.array.append", "numpy.array.append", "len", "nodesToKeep.index", "nodesToKeep.index", "nodesToKeep.index"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.createGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.sparsifyGraph"], ["", "", "", "def", "createGraph", "(", "self", ")", ":", "\n", "# Here we can choose to create the movie or the user graph.", "\n", "# Let's start with the incomplete matrix, and get randomly some of the", "\n", "# elements from it to use as training data to build the graph.", "\n", "\n", "# Recall that the datapoints that I have already split following the", "\n", "# user/movie ID selection (or 'all' for all it matters) have to be", "\n", "# taken into account. So, check that this points have been determined", "\n", "        ", "assert", "'all'", "in", "self", ".", "indexDataPoints", ".", "keys", "(", ")", "\n", "assert", "'train'", "in", "self", ".", "indexDataPoints", ".", "keys", "(", ")", "\n", "assert", "'valid'", "in", "self", ".", "indexDataPoints", ".", "keys", "(", ")", "\n", "assert", "'test'", "in", "self", ".", "indexDataPoints", ".", "keys", "(", ")", "\n", "assert", "self", ".", "nTrain", "is", "not", "None", "and", "self", ".", "nValid", "is", "not", "None", "and", "self", ".", "nTest", "is", "not", "None", "\n", "\n", "# To follow the paper by Huang et al., where the data is given by", "\n", "# Y in U x M, and goes into full detail on how to build the U x U", "\n", "# user-based graph, then, we will stick with this formulation", "\n", "if", "self", ".", "graphType", "==", "'user'", ":", "\n", "            ", "workingMatrix", "=", "self", ".", "incompleteMatrix", "# User x Movies", "\n", "", "else", ":", "\n", "            ", "workingMatrix", "=", "self", ".", "incompleteMatrix", ".", "T", "# Movies x User", "\n", "# Note that this is the opposite arrangement that we considered before", "\n", "# when loading the data into samples; back then, we considered samples", "\n", "# to be rows and the data to build the graph was therefore in columns;", "\n", "# in this case, it is the opposite, since we still want to use the data", "\n", "# located in the rows.", "\n", "\n", "# Now, the indices in self.indexDataPoints, essentially determine the", "\n", "# data samples (the graph signals) that we put in each set. Now, these", "\n", "# graph signals are now the columns, because the nodes are the rows.", "\n", "# So, these indexDataPoints are the columns in the new workingMatrix.", "\n", "\n", "# In essence, we need to add more points to complete the train set, but", "\n", "# to be sure that (i) these points are not the ones in the valid and", "\n", "# test sets, and (ii) that the training points are included already.", "\n", "\n", "# Now, out of all possible graph signals (number of columns in this", "\n", "# workingMatrix), we have selected some of those to be part of the", "\n", "# training set. But each  of these graph signals, have a different", "\n", "# number of traning points (because they have a different number of", "\n", "# nonzero elements). And we only care, when building the graph, on the", "\n", "# nonzero elements of the graph signals.", "\n", "\n", "# So, let's count the number of training points that we actually have", "\n", "# To do this, we count the number of nonzero elements in the samples", "\n", "# that we have selected", "\n", "", "trainSamples", "=", "self", ".", "indexDataPoints", "[", "'train'", "]", "\n", "nTrainPointsActual", "=", "len", "(", "np", ".", "nonzero", "(", "workingMatrix", "[", ":", ",", "trainSamples", "]", ")", "[", "0", "]", ")", "\n", "# And the total number of points that we have already partitioned into ", "\n", "# the different sets", "\n", "validSamples", "=", "self", ".", "indexDataPoints", "[", "'valid'", "]", "\n", "nValidPointsActual", "=", "len", "(", "np", ".", "nonzero", "(", "workingMatrix", "[", ":", ",", "validSamples", "]", ")", "[", "0", "]", ")", "\n", "testSamples", "=", "self", ".", "indexDataPoints", "[", "'test'", "]", "\n", "nTestPointsActual", "=", "len", "(", "np", ".", "nonzero", "(", "workingMatrix", "[", ":", ",", "testSamples", "]", ")", "[", "0", "]", ")", "\n", "# Total number of points already considered", "\n", "nPointsActual", "=", "nTrainPointsActual", "+", "nValidPointsActual", "+", "nTestPointsActual", "\n", "\n", "# The total number of data points in the entire dataset is", "\n", "indexDataPoints", "=", "np", ".", "nonzero", "(", "workingMatrix", ")", "\n", "# This is a tuple, where the first element is the place of nonzero", "\n", "# indices in the rows, and the second element is the place of nonzero", "\n", "# indices in the columns.", "\n", "nDataPoints", "=", "len", "(", "indexDataPoints", "[", "0", "]", ")", "# or [1], it doesn't matter", "\n", "# Note that every nonzero point belonging to labelID has already been", "\n", "# assigned to either one or the other dataset, so when we split", "\n", "# these datasets, we cannot consider these.", "\n", "\n", "# The total number of expected training points is", "\n", "nTrainPointsAll", "=", "int", "(", "round", "(", "self", ".", "ratioTrain", "*", "nDataPoints", ")", ")", "\n", "#   Discard the (expected) number of validation points", "\n", "nTrainPointsAll", "=", "int", "(", "nTrainPointsAll", "-", "round", "(", "self", ".", "ratioValid", "*", "nTrainPointsAll", ")", ")", "\n", "\n", "# Now, we only need to add more points if the expected number of ", "\n", "# training points is greater than the ones we still have.", "\n", "\n", "# If we have more training points than what we originally intended, we", "\n", "# just use those (they will be part of the training samples regardless)", "\n", "# This could happen, for instance, if by chances, the graph signals", "\n", "# picked for training set are the more dense ones, giving a lot of", "\n", "# training points: nTrainPointsAll > nTrainPointsActual", "\n", "\n", "# Likewise, if we do not have any more points to take from (because all", "\n", "# the other graph signals have already been taken for validation and", "\n", "# test set), we can proceed to get the remaining needed points:", "\n", "# nPointsActual < nDataPoints", "\n", "\n", "if", "nTrainPointsAll", ">", "nTrainPointsActual", "and", "nPointsActual", "<", "nDataPoints", ":", "\n", "# So, now, the number of points that we still need to get are", "\n", "            ", "nTrainPointsRest", "=", "nTrainPointsAll", "-", "nTrainPointsActual", "\n", "# Next, we need to determine what is the pool of indices where we", "\n", "# can get the samples from (it cannot be samples that have already", "\n", "# been considered in any of the graph signals)", "\n", "nTotalCols", "=", "workingMatrix", ".", "shape", "[", "1", "]", "# Total number of columns", "\n", "# Note that self.indexDataPoints['all'] has all the columns that ", "\n", "# have already been selected. So the remaining columns are the ones", "\n", "# that are not there", "\n", "indexRemainingCols", "=", "[", "i", "for", "i", "in", "range", "(", "nTotalCols", ")", "if", "i", "not", "in", "self", ".", "indexDataPoints", "[", "'all'", "]", "]", "\n", "indexRemainingCols", "=", "np", ".", "array", "(", "indexRemainingCols", ")", "\n", "# So the total number of points left is", "\n", "indexDataPointsRest", "=", "np", ".", "nonzero", "(", "workingMatrix", "[", ":", ",", "indexRemainingCols", "]", ")", "\n", "nDataPointsRest", "=", "len", "(", "indexDataPointsRest", "[", "0", "]", ")", "\n", "# Now, check that we have enough points to complete the total ", "\n", "# desired. If not, just use all of them", "\n", "if", "nDataPointsRest", "<", "nTrainPointsRest", ":", "\n", "                ", "nTrainPointsRest", "=", "nDataPointsRest", "\n", "\n", "# Now, we need to select at random from these points, those that ", "\n", "# will be part of the training set to build the graph.", "\n", "", "randPerm", "=", "np", ".", "random", ".", "permutation", "(", "nDataPointsRest", ")", "\n", "# Pick the needed number of subindices", "\n", "subIndexRandomRest", "=", "randPerm", "[", "0", ":", "nTrainPointsRest", "]", "\n", "# And select the points (both rows and columns)", "\n", "#   Remember that columns indexed by indexDataPointsRest, actually", "\n", "#   refer to the submatrix of remaining columns, so", "\n", "indexDataPointsRestCols", "=", "indexDataPointsRest", "[", "1", "]", "[", "subIndexRandomRest", "]", "\n", "indexDataPointsRestCols", "=", "indexRemainingCols", "[", "indexDataPointsRestCols", "]", "\n", "indexDataPointsRestRows", "=", "indexDataPointsRest", "[", "0", "]", "[", "subIndexRandomRest", "]", "\n", "indexTrainPointsRest", "=", "(", "indexDataPointsRestRows", ",", "\n", "indexDataPointsRestCols", ")", "\n", "# So, so far, we have all the needed training points: (i) those in", "\n", "# the original training set, and (ii) those in the remaining graph", "\n", "# signals to complete the number of desired training points.", "\n", "\n", "# Now, we need to merge these points with the ones already in the", "\n", "# training set of graph signals", "\n", "indexTrainPointsID", "=", "np", ".", "nonzero", "(", "\n", "workingMatrix", "[", ":", ",", "self", ".", "indexDataPoints", "[", "'train'", "]", "]", ")", "\n", "# And put them together with the ones we already had", "\n", "indexTrainPoints", "=", "(", "\n", "np", ".", "concatenate", "(", "(", "indexTrainPointsRest", "[", "0", "]", ",", "indexTrainPointsID", "[", "0", "]", ")", ")", ",", "\n", "np", ".", "concatenate", "(", "(", "indexTrainPointsRest", "[", "1", "]", ",", "\n", "self", ".", "indexDataPoints", "[", "'train'", "]", "[", "indexTrainPointsID", "[", "1", "]", "]", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "# If we already had all the points we wanted, which are those that", "\n", "# were already in the training set, we need to get them, so it's", "\n", "# just a renaming", "\n", "            ", "indexTrainPoints", "=", "np", ".", "nonzero", "(", "\n", "workingMatrix", "[", ":", ",", "self", ".", "indexDataPoints", "[", "'train'", "]", "]", ")", "\n", "# But the columns in this indexTrainPoints, are actually the", "\n", "# columns of the smaller matrix evaluated only on ", "\n", "# self.indexDataPoints['train']. So we need to map it into the", "\n", "# full column numbers", "\n", "indexTrainPoints", "=", "(", "\n", "indexTrainPoints", "[", "0", "]", ",", "\n", "self", ".", "indexDataPoints", "[", "'train'", "]", "[", "indexTrainPoints", "[", "1", "]", "]", "\n", ")", "\n", "# And state that there are no new extra points", "\n", "nTrainPointsRest", "=", "0", "\n", "\n", "# Record the actual number of training points that we are left with", "\n", "", "nTrainPoints", "=", "len", "(", "indexTrainPoints", "[", "0", "]", ")", "\n", "assert", "nTrainPoints", "==", "nTrainPointsRest", "+", "nTrainPointsActual", "\n", "\n", "# And this is it! We got all the necessary training samples, including", "\n", "# those that we were already using.", "\n", "\n", "# Finally, set every other element not in the training set in the ", "\n", "# workingMatrix to zero", "\n", "workingMatrixZeroedTrain", "=", "workingMatrix", ".", "copy", "(", ")", "\n", "workingMatrixZeroedTrain", "[", "indexTrainPoints", "]", "=", "0.", "\n", "workingMatrix", "=", "workingMatrix", "-", "workingMatrixZeroedTrain", "\n", "assert", "len", "(", "np", ".", "nonzero", "(", "workingMatrix", ")", "[", "0", "]", ")", "==", "nTrainPoints", "\n", "# To check that the total number of nonzero elements of the matrix are", "\n", "# the total number of training samples that we're supposed to have.", "\n", "\n", "# Now, we finally have the incompleteMatrix only with the corresponding", "\n", "# elements: a ratioTrain proportion of training samples that, for sure,", "\n", "# include the ones that we will use in the graph signals dataset and, ", "\n", "# for sure, exclude those that are in the validation and test sets.", "\n", "\n", "# Finally, on to compute the correlation matrix.", "\n", "# The mean required for the (u,v)th element of the correlation matrix is", "\n", "# the sum of the ratings for row u, but only in those columns where", "\n", "# there is also a rating for row v. So we care about the values in row", "\n", "# u, but we need to know which nonzero positions coincide between rows", "\n", "# u and v. In order to do this, we create a template that signals", "\n", "# the position of elements.", "\n", "binaryTemplate", "=", "(", "workingMatrix", ">", "0", ")", ".", "astype", "(", "workingMatrix", ".", "dtype", ")", "\n", "# Then, when we multiply the matrix with the actual ratings, with the", "\n", "# transpose of this template, we will be summing the values of one", "\n", "# matrix (in rows) but only for the places where there was an element", "\n", "# in the other row (now a column, because it is transposed). This gives", "\n", "# us the sum part of the mean.", "\n", "sumMatrix", "=", "workingMatrix", ".", "dot", "(", "binaryTemplate", ".", "T", ")", "\n", "# To count the number of elements that are shred by both rows u and v,", "\n", "# we simply multiply the binary template.", "\n", "countMatrix", "=", "binaryTemplate", ".", "dot", "(", "binaryTemplate", ".", "T", ")", "\n", "# Note that there might be elements with zero intersection, then we", "\n", "# need to set this to 1 so division by 0 doesn't create a NaN (the", "\n", "# end result will still be zero, since the sumMatrix will have a", "\n", "# zero in those same positions)", "\n", "countMatrix", "[", "countMatrix", "==", "0", "]", "=", "1", "\n", "# And now we can compute this (u,v) dependent mean", "\n", "avgMatrix", "=", "sumMatrix", "/", "countMatrix", "\n", "# Note that this matrix is not supposed to be symmetric due to the", "\n", "# use of only the sum of the item u over the set uv, instead of using", "\n", "# the sum over u and over v. More specifically, the definition is", "\n", "# mu_{uv} = 1/|S_{uv}| * \\sum_{i \\in S_{uv}} Y_{ui}", "\n", "# Since the sum is of elements u, when we compute mu_{vu} we will get", "\n", "# a different sum", "\n", "\n", "# Now, to compute the correlation, we need to compute the square sum", "\n", "# matrix \\sum_{i \\in S_{uv}} Y_{ui}^{2}", "\n", "sqSumMatrix", "=", "(", "workingMatrix", "**", "2", ")", ".", "dot", "(", "binaryTemplate", ".", "T", ")", "\n", "# And compute the correlation matrix as", "\n", "# 1/|S_{uv}| \\sum_{i \\in S_{uv}} Y_{ui}^{2} - \\mu_{uv}^{2}", "\n", "# where \\mu_{uv} is the mean we computed before", "\n", "correlationMatrix", "=", "sqSumMatrix", "/", "countMatrix", "-", "avgMatrix", "**", "2", "\n", "\n", "# Finally, normalize the individual user variances and get rid of the", "\n", "# identity matrix", "\n", "#   Compute the square root of the diagonal elements", "\n", "sqrtDiagonal", "=", "np", ".", "sqrt", "(", "np", ".", "diag", "(", "correlationMatrix", ")", ")", "\n", "#   Find the place where the nonzero elements are", "\n", "nonzeroSqrtDiagonalIndex", "=", "(", "sqrtDiagonal", ">", "zeroTolerance", ")", ".", "astype", "(", "sqrtDiagonal", ".", "dtype", ")", "\n", "#   Set the zero elements to 1", "\n", "sqrtDiagonal", "[", "sqrtDiagonal", "<", "zeroTolerance", "]", "=", "1.", "\n", "#   Invert safely", "\n", "invSqrtDiagonal", "=", "1", "/", "sqrtDiagonal", "\n", "#   Get rid of the fake 1 inversions", "\n", "invSqrtDiagonal", "=", "invSqrtDiagonal", "*", "nonzeroSqrtDiagonalIndex", "\n", "#   Make it a matrix again", "\n", "normalizationMatrix", "=", "np", ".", "diag", "(", "invSqrtDiagonal", ")", "\n", "#   And normalize", "\n", "normalizedMatrix", "=", "normalizationMatrix", ".", "dot", "(", "\n", "correlationMatrix", ".", "dot", "(", "normalizationMatrix", ")", ")", "-", "np", ".", "eye", "(", "correlationMatrix", ".", "shape", "[", "0", "]", ")", "\n", "#   There could be isolated nodes, which mean that have 0 in the ", "\n", "#   diagonal already, so when subtracting the identity they end up with", "\n", "#   -1 in the diagonal element.", "\n", "#   If this is the case, we just put back a one in those nodes. But the", "\n", "#   real problem, comes if the labelID is within those isolated nodes.", "\n", "#   If that's the case, then we just stop the processing, there's ", "\n", "#   nothing else to do.", "\n", "diagNormalizedMatrix", "=", "np", ".", "diag", "(", "np", ".", "diag", "(", "normalizedMatrix", ")", ")", "\n", "isolatedNodes", "=", "np", ".", "nonzero", "(", "np", ".", "abs", "(", "diagNormalizedMatrix", "+", "1", ")", "<", "zeroTolerance", ")", "\n", "normalizedMatrix", "[", "isolatedNodes", "]", "=", "0.", "\n", "#   Get rid of the \"quasi-zeros\" that could have arrived through ", "\n", "#   division.", "\n", "normalizedMatrix", "[", "np", ".", "abs", "(", "normalizedMatrix", ")", "<", "zeroTolerance", "]", "=", "0.", "\n", "\n", "# Finally, create the graph", "\n", "#   Number of nodes so far", "\n", "N", "=", "normalizedMatrix", ".", "shape", "[", "0", "]", "\n", "#   Add the necessary extra dimension (because it is 'fuseEdges' so it", "\n", "#   expects a set of matrices, instead of just one)", "\n", "normalizedMatrix", "=", "normalizedMatrix", ".", "reshape", "(", "[", "1", ",", "N", ",", "N", "]", ")", "\n", "#   Use 'fuseEdges' to handle several desirable properties that could", "\n", "#   be enforced on the graph", "\n", "nodesToKeep", "=", "[", "]", "# List of nodes to keep after some of them might have", "\n", "#   been removed to satisfy the constraints", "\n", "extraComponents", "=", "[", "]", "# List where we save the rest of the isolated ", "\n", "# components, if there where", "\n", "W", "=", "graph", ".", "createGraph", "(", "'fuseEdges'", ",", "N", ",", "\n", "{", "'adjacencyMatrices'", ":", "normalizedMatrix", ",", "\n", "'aggregationType'", ":", "'sum'", ",", "\n", "'normalizationType'", ":", "'no'", ",", "\n", "'isolatedNodes'", ":", "self", ".", "keepIsolatedNodes", ",", "\n", "'forceUndirected'", ":", "self", ".", "forceUndirected", ",", "\n", "'forceConnected'", ":", "self", ".", "forceConnected", ",", "\n", "'nodeList'", ":", "nodesToKeep", ",", "\n", "'extraComponents'", ":", "extraComponents", "}", ")", "\n", "# So far, the matrix output is the adjacency matrix of the largest ", "\n", "# connected component, and nodesToKeep refer to those nodes.", "\n", "\n", "# At this point, it can happen that some (or all) of the selected nodes", "\n", "# are not in the graph. If none of the selected nodes is there, we", "\n", "# should stop (we have no useful problem anymore)", "\n", "\n", "IDnodesKept", "=", "0", "# How many of the selected ID nodes are we keeping", "\n", "for", "i", "in", "self", ".", "labelID", ":", "\n", "            ", "if", "i", "in", "nodesToKeep", ":", "\n", "                ", "IDnodesKept", "+=", "1", "\n", "\n", "", "", "assert", "IDnodesKept", ">", "0", "\n", "\n", "#   Update samples and labelID, if necessary", "\n", "if", "len", "(", "nodesToKeep", ")", "<", "N", ":", "\n", "# Update the node IDs", "\n", "#   Get signals, IDs and labels", "\n", "            ", "trainSignals", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "\n", "trainIDs", "=", "self", ".", "targetIDs", "[", "'train'", "]", "\n", "trainLabels", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "\n", "validSignals", "=", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "\n", "validIDs", "=", "self", ".", "targetIDs", "[", "'valid'", "]", "\n", "validLabels", "=", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "\n", "testSignals", "=", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "\n", "testIDs", "=", "self", ".", "targetIDs", "[", "'test'", "]", "\n", "testLabels", "=", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "\n", "#   Update the ID", "\n", "#   Train set", "\n", "trainIDsToKeep", "=", "[", "]", "# which samples from the train set we need to ", "\n", "# keep (note that if some of the nodes that were labeled in the", "\n", "# trainIDs have been vanished, then we need to get rid of those", "\n", "# training samples)", "\n", "newTrainIDs", "=", "[", "]", "# Then, we need to match the old node numbering", "\n", "# (with all the nodes), to those of the new numbering", "\n", "for", "i", "in", "range", "(", "len", "(", "trainIDs", ")", ")", ":", "\n", "# If the train ID of the sample is in nodes to keep", "\n", "                ", "if", "trainIDs", "[", "i", "]", "in", "nodesToKeep", ":", "\n", "# We need to add it to the list of nodes to keep (what ", "\n", "# position in the training samples are, because those that", "\n", "# are not there also have to be discarded from the rating)", "\n", "                    ", "trainIDsToKeep", ".", "append", "(", "i", ")", "\n", "# And we have to update the ID to the new one (considering", "\n", "# that not all nodes have been kept)", "\n", "newTrainIDs", ".", "append", "(", "nodesToKeep", ".", "index", "(", "trainIDs", "[", "i", "]", ")", ")", "\n", "", "", "trainIDsToKeep", "=", "np", ".", "array", "(", "trainIDsToKeep", ")", "# Convert to numpy", "\n", "newTrainIDs", "=", "np", ".", "array", "(", "newTrainIDs", ")", "# Conver to numpy", "\n", "#   Valid Set", "\n", "validIDsToKeep", "=", "[", "]", "\n", "newValidIDs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "validIDs", ")", ")", ":", "\n", "                ", "if", "validIDs", "[", "i", "]", "in", "nodesToKeep", ":", "\n", "                    ", "validIDsToKeep", ".", "append", "(", "i", ")", "\n", "newValidIDs", ".", "append", "(", "nodesToKeep", ".", "index", "(", "validIDs", "[", "i", "]", ")", ")", "\n", "", "", "validIDsToKeep", "=", "np", ".", "array", "(", "validIDsToKeep", ")", "# Convert to numpy", "\n", "newValidIDs", "=", "np", ".", "array", "(", "newValidIDs", ")", "# Conver to numpy", "\n", "#   Test Set", "\n", "testIDsToKeep", "=", "[", "]", "\n", "newTestIDs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "testIDs", ")", ")", ":", "\n", "                ", "if", "testIDs", "[", "i", "]", "in", "nodesToKeep", ":", "\n", "                    ", "testIDsToKeep", ".", "append", "(", "i", ")", "\n", "newTestIDs", ".", "append", "(", "nodesToKeep", ".", "index", "(", "testIDs", "[", "i", "]", ")", ")", "\n", "", "", "testIDsToKeep", "=", "np", ".", "array", "(", "testIDsToKeep", ")", "# Convert to numpy", "\n", "newTestIDs", "=", "np", ".", "array", "(", "newTestIDs", ")", "# Conver to numpy", "\n", "\n", "# And, finally, we update the signals", "\n", "trainSignals", "=", "trainSignals", "[", "trainIDsToKeep", "]", "[", ":", ",", "nodesToKeep", "]", "\n", "validSignals", "=", "validSignals", "[", "validIDsToKeep", "]", "[", ":", ",", "nodesToKeep", "]", "\n", "testSignals", "=", "testSignals", "[", "testIDsToKeep", "]", "[", ":", ",", "nodesToKeep", "]", "\n", "# and the IDs", "\n", "trainIDs", "=", "newTrainIDs", "\n", "validIDs", "=", "newValidIDs", "\n", "testIDs", "=", "newTestIDs", "\n", "# Also update the labels (some of the samples are gone)", "\n", "trainLabels", "=", "trainLabels", "[", "trainIDsToKeep", "]", "\n", "validLabels", "=", "validLabels", "[", "validIDsToKeep", "]", "\n", "testLabels", "=", "testLabels", "[", "testIDsToKeep", "]", "\n", "# and store them where they belong", "\n", "self", ".", "nTrain", "=", "trainSignals", ".", "shape", "[", "0", "]", "\n", "self", ".", "nValid", "=", "validSignals", ".", "shape", "[", "0", "]", "\n", "self", ".", "nTest", "=", "testSignals", ".", "shape", "[", "0", "]", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "trainSignals", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "trainLabels", "\n", "self", ".", "targetIDs", "[", "'train'", "]", "=", "trainIDs", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "=", "validSignals", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "=", "validLabels", "\n", "self", ".", "targetIDs", "[", "'valid'", "]", "=", "validIDs", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "=", "testSignals", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "=", "testLabels", "\n", "self", ".", "targetIDs", "[", "'test'", "]", "=", "testIDs", "\n", "# If the graph type is 'movies', then any removed node has a", "\n", "# repercusion in the movie list, and therefore, we need to update", "\n", "# that as well", "\n", "if", "self", ".", "graphType", "==", "'movie'", ":", "\n", "                ", "if", "len", "(", "self", ".", "movieTitles", ")", ">", "0", ":", "# Non empty movieList", "\n", "# Where to save the new movie list", "\n", "                    ", "movieTitles", "=", "{", "}", "\n", "# Because nodes are now numbered sequentially, we need to", "\n", "# do the same with the movieID to keep them matched (i.e.", "\n", "# node n corresponds to movieList[n] title)", "\n", "newMovieID", "=", "0", "\n", "for", "movieID", "in", "nodesToKeep", ":", "\n", "                        ", "movieTitles", "[", "newMovieID", "]", "=", "self", ".", "movieTitles", "[", "movieID", "]", "\n", "newMovieID", "=", "newMovieID", "+", "1", "\n", "# Update movieList", "\n", "", "self", ".", "movieTitles", "=", "movieTitles", "\n", "\n", "#   And finally, sparsify it (nearest neighbors)", "\n", "", "", "", "self", ".", "adjacencyMatrix", "=", "graph", ".", "sparsifyGraph", "(", "W", ",", "'NN'", ",", "self", ".", "kNN", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.interpolateRatings": [[2019, 2109], ["dataTools.MovieLens.samples.keys", "numpy.nonzero", "len", "thisSignal.squeeze.squeeze.squeeze", "len", "numpy.unique", "numpy.array", "numpy.concatenate", "alegnn.computeNeighborhood", "range", "numpy.nonzero", "numpy.abs", "len", "numpy.sum", "numpy.count_nonzero", "numpy.round", "range", "len", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeNeighborhood"], ["", "def", "interpolateRatings", "(", "self", ")", ":", "\n", "# For the nonzero nodes, we will average the value of the closest", "\n", "# nonzero elements.", "\n", "\n", "# So we need to find the neighborhood, iteratively, until we find a", "\n", "# nodes in the neighborhood that have nonzero elements. And then", "\n", "# average those.", "\n", "\n", "# There are three sets of signals, so for each one of them", "\n", "        ", "for", "key", "in", "self", ".", "samples", ".", "keys", "(", ")", ":", "\n", "# Get the signals", "\n", "            ", "thisSignal", "=", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "# B (x F) x N", "\n", "if", "len", "(", "thisSignal", ".", "shape", ")", "==", "3", ":", "# If B x 1 x N", "\n", "                ", "assert", "thisSignal", ".", "shape", "[", "1", "]", "==", "1", "\n", "thisSignal", "=", "thisSignal", ".", "squeeze", "(", "1", ")", "# B x N", "\n", "# Look for the elements in the signal that are zero", "\n", "", "zeroLocations", "=", "np", ".", "nonzero", "(", "np", ".", "abs", "(", "thisSignal", ")", "<", "zeroTolerance", ")", "\n", "# This is a tuple with two elements, each element is a 1-D np.array", "\n", "# with the rows and column indices of the zero elements, respectiv", "\n", "# The columns are the nodes, so we should iterate by nodes. The ", "\n", "# problem is that I do not want to go ahead finding the neighborhood", "\n", "# each time, and I do not want to go ahead element by element, I", "\n", "# want to go node by node, so that I have to do at most N searches", "\n", "\n", "# Not a good idea. Let's do it by neighborhood, since I can get", "\n", "# all neighbors at once", "\n", "# If there are zero locations that need to be interpolated", "\n", "K", "=", "1", "\n", "while", "len", "(", "zeroLocations", "[", "0", "]", ")", ">", "0", ":", "\n", "# Location of nodes with zero value", "\n", "                ", "zeroNodes", "=", "np", ".", "unique", "(", "zeroLocations", "[", "1", "]", ")", "\n", "# If we want to make this faster, we only want the neighborhoods", "\n", "# of the nodes that don't have a value yet.", "\n", "# The problem is that the computeNeighborhood function only", "\n", "# works on the first N values, so we need to reorder the matrix", "\n", "# so that the first elements are the nodes we actually want", "\n", "# To do this, we need to add the rest of the nodes to the list", "\n", "# of zeroNodes and then reorder the matrix", "\n", "#   Full nodes", "\n", "fullNodes", "=", "[", "n", "for", "n", "in", "range", "(", "thisSignal", ".", "shape", "[", "1", "]", ")", "if", "n", "not", "in", "zeroNodes", "]", "\n", "fullNodes", "=", "np", ".", "array", "(", "fullNodes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "#   Complete list of nodes (concatenate them)", "\n", "allNodes", "=", "np", ".", "concatenate", "(", "(", "zeroNodes", ",", "fullNodes", ")", ")", "\n", "#   Reorder the matrix", "\n", "A", "=", "self", ".", "adjacencyMatrix", "[", "allNodes", ",", ":", "]", "[", ":", ",", "allNodes", "]", "\n", "# Get the neighborhood", "\n", "nbList", "=", "graph", ".", "computeNeighborhood", "(", "A", ",", "K", ",", "N", "=", "len", "(", "zeroNodes", ")", ")", "\n", "# This is a list of lists. Each node has associated a list of", "\n", "# neighboring nodes.", "\n", "# But the index of this neighboring nodes is not correct, ", "\n", "# because it belongs to the allNodes ordering and not the ", "\n", "# original one.", "\n", "#   ", "\n", "# Go for each node, and pick up the neighboring values", "\n", "# (It is more likely that we will have more samples than", "\n", "# nodes, so it should be faster to iterate through nodes)", "\n", "\n", "# For each element in the neighborhood list", "\n", "for", "i", "in", "range", "(", "len", "(", "nbList", ")", ")", ":", "\n", "# Get the actual node", "\n", "                    ", "thisNode", "=", "zeroNodes", "[", "i", "]", "\n", "# Get the neighborhood (and map it to the corresponding", "\n", "# nodes in the original ordering)", "\n", "thisNB", "=", "[", "allNodes", "[", "n", "]", "for", "n", "in", "nbList", "[", "i", "]", "]", "\n", "# Now, get the values at the neighborhood (which is now", "\n", "# in the original ordering)", "\n", "nbValues", "=", "thisSignal", "[", ":", ",", "thisNB", "]", "\n", "#   This gives all the neighboring values of each batch", "\n", "# Average the nonzero elements", "\n", "#   Sum of the elements", "\n", "sumSignal", "=", "np", ".", "sum", "(", "nbValues", ",", "axis", "=", "1", ")", "\n", "#   Count of nonzero elements", "\n", "countNZ", "=", "np", ".", "count_nonzero", "(", "nbValues", ",", "axis", "=", "1", ")", "\n", "#   Get rid of the zero elements for division", "\n", "countNZ", "[", "countNZ", "==", "0", "]", "=", "1.", "\n", "#   Compute the average and round to an integer", "\n", "meanSignal", "=", "np", ".", "round", "(", "sumSignal", "/", "countNZ", ")", "\n", "# And now we need to place this newly computed mean ", "\n", "# signal back in the nonzero elements", "\n", "zeroBatches", "=", "zeroLocations", "[", "0", "]", "[", "zeroLocations", "[", "1", "]", "==", "thisNode", "]", "\n", "# Add it to the signal", "\n", "thisSignal", "[", "zeroBatches", ",", "thisNode", "]", "=", "meanSignal", "[", "zeroBatches", "]", "\n", "# Now that we have finished all nodes for the K-hop neighbors", "\n", "# we need to update the zero elements", "\n", "", "zeroLocations", "=", "np", ".", "nonzero", "(", "np", ".", "abs", "(", "thisSignal", ")", "<", "zeroTolerance", ")", "\n", "# and add a new neighborhood", "\n", "K", "+=", "1", "\n", "\n", "# And put it back where it goes", "\n", "", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "=", "thisSignal", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.getIncompleteMatrix": [[2110, 2113], ["None"], "methods", ["None"], ["", "", "def", "getIncompleteMatrix", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "incompleteMatrix", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.getGraph": [[2114, 2117], ["None"], "methods", ["None"], ["", "def", "getGraph", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "adjacencyMatrix", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.getMovieTitles": [[2118, 2121], ["None"], "methods", ["None"], ["", "def", "getMovieTitles", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "movieTitles", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.getLabelID": [[2122, 2163], ["len", "len", "type", "numpy.random.choice"], "methods", ["None"], ["", "def", "getLabelID", "(", "self", ",", "*", "args", ")", ":", "\n", "\n", "# So, here are the options", "\n", "# No arguments: return the list of self.labelID", "\n", "# One argument: it has to be samplesType and then return all labelIDs", "\n", "# for that sample type", "\n", "# Two arguments, can either be list or int, and return at random, like", "\n", "# the getSamples() method", "\n", "\n", "        ", "if", "len", "(", "args", ")", "==", "0", ":", "\n", "            ", "returnID", "=", "self", ".", "labelID", "\n", "", "else", ":", "\n", "# The first argument has to be the sample type", "\n", "            ", "samplesType", "=", "args", "[", "0", "]", "\n", "# Check that is one of the possibilities", "\n", "assert", "samplesType", "==", "'train'", "or", "samplesType", "==", "'valid'", "or", "samplesType", "==", "'test'", "\n", "\n", "returnID", "=", "self", ".", "targetIDs", "[", "samplesType", "]", "\n", "\n", "if", "len", "(", "args", ")", "==", "2", ":", "\n", "# If it is an int, just return that number of randomly chosen", "\n", "# IDs", "\n", "                ", "if", "type", "(", "args", "[", "1", "]", ")", "is", "int", ":", "\n", "# Total number of samples", "\n", "                    ", "nSamples", "=", "returnID", ".", "shape", "\n", "# Check that we are asked to return a number of samples that", "\n", "# we actually have", "\n", "assert", "args", "[", "1", "]", "<=", "nSamples", "\n", "# Randomly choose args[1] indices", "\n", "selectedIndices", "=", "np", ".", "random", ".", "choice", "(", "nSamples", ",", "size", "=", "args", "[", "1", "]", ",", "\n", "replace", "=", "False", ")", "\n", "# Select the corresponding IDs", "\n", "returnID", "=", "returnID", "[", "selectedIndices", "]", "\n", "\n", "", "else", ":", "\n", "# This has to be a list () or an np.array which can serve", "\n", "# as indexing functions", "\n", "                    ", "returnID", "=", "returnID", "[", "args", "[", "1", "]", "]", "\n", "\n", "", "", "", "return", "returnID", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.evaluate": [[2164, 2188], ["y.unsqueeze.unsqueeze.squeeze", "yHat.unsqueeze.unsqueeze.squeeze", "y.unsqueeze.unsqueeze.unsqueeze", "yHat.unsqueeze.unsqueeze.unsqueeze", "repr", "torch.nn.functional.mse_loss", "torch.sqrt", "numpy.mean", "numpy.sqrt"], "methods", ["None"], ["", "def", "evaluate", "(", "self", ",", "yHat", ",", "y", ")", ":", "\n", "# y and yHat should be of the same dimension, where dimension 0 is the", "\n", "# number of samples", "\n", "        ", "N", "=", "y", ".", "shape", "[", "0", "]", "# number of samples", "\n", "assert", "yHat", ".", "shape", "[", "0", "]", "==", "N", "\n", "# And now, get rid of any extra '1' dimension that might appear ", "\n", "# involuntarily from some vectorization issues.", "\n", "y", "=", "y", ".", "squeeze", "(", ")", "\n", "yHat", "=", "yHat", ".", "squeeze", "(", ")", "\n", "# Yet, if there was only one sample, then the sample dimension was", "\n", "# also get rid of during the squeeze, so we need to add it back", "\n", "if", "N", "==", "1", ":", "\n", "            ", "y", "=", "y", ".", "unsqueeze", "(", "0", ")", "\n", "yHat", "=", "yHat", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Now, we compute the RMS", "\n", "", "if", "'torch'", "in", "repr", "(", "self", ".", "dataType", ")", ":", "\n", "            ", "mse", "=", "torch", ".", "nn", ".", "functional", ".", "mse_loss", "(", "yHat", ",", "y", ")", "\n", "rmse", "=", "torch", ".", "sqrt", "(", "mse", ")", "\n", "", "else", ":", "\n", "            ", "mse", "=", "np", ".", "mean", "(", "(", "yHat", "-", "y", ")", "**", "2", ")", "\n", "rmse", "=", "np", ".", "sqrt", "(", "mse", ")", "\n", "\n", "", "return", "rmse", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.astype": [[2189, 2197], ["dataTools.changeDataType", "dataTools.changeDataType", "dataTools._data.astype"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "astype", "(", "self", ",", "dataType", ")", ":", "\n", "# This changes the type for the incomplete and adjacency matrix.", "\n", "        ", "self", ".", "incompleteMatrix", "=", "changeDataType", "(", "self", ".", "incompleteMatrix", ",", "dataType", ")", "\n", "self", ".", "adjacencyMatrix", "=", "changeDataType", "(", "self", ".", "adjacencyMatrix", ",", "dataType", ")", "\n", "\n", "# And now, initialize to change the samples as well (and also save the ", "\n", "# data type)", "\n", "super", "(", ")", ".", "astype", "(", "dataType", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.to": [[2199, 2209], ["repr", "dataTools.MovieLens.incompleteMatrix.to", "dataTools.MovieLens.adjacencyMatrix.to", "dataTools._data.to"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# If the dataType is 'torch'", "\n", "        ", "if", "'torch'", "in", "repr", "(", "self", ".", "dataType", ")", ":", "\n", "# Change the stored attributes that are not handled by the inherited", "\n", "# method to().", "\n", "            ", "self", ".", "incompleteMatrix", ".", "to", "(", "device", ")", "\n", "self", ".", "adjacencyMatrix", ".", "to", "(", "device", ")", "\n", "# And call the inherit method to initialize samples (and save to", "\n", "# device)", "\n", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.__init__": [[2429, 2574], ["dataTools._data.__init__", "float", "dataTools.Flocking.computeInitialPositions", "dataTools.Flocking.computeOptimalTrajectory", "dataTools.Flocking.computeCommunicationGraph", "dataTools.Flocking.computeStates", "stateAll[].copy", "accelAll[].copy", "stateAll[].copy", "accelAll[].copy", "stateAll[].copy", "accelAll[].copy", "dataTools.Flocking.astype", "dataTools.Flocking.to", "print", "print", "print", "print", "print", "print", "print", "print"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeInitialPositions", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeOptimalTrajectory", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeCommunicationGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeStates", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["def", "__init__", "(", "self", ",", "nAgents", ",", "commRadius", ",", "repelDist", ",", "\n", "nTrain", ",", "nValid", ",", "nTest", ",", "\n", "duration", ",", "samplingTime", ",", "\n", "initGeometry", "=", "'circular'", ",", "initVelValue", "=", "3.", ",", "initMinDist", "=", "0.1", ",", "\n", "accelMax", "=", "10.", ",", "\n", "normalizeGraph", "=", "True", ",", "doPrint", "=", "True", ",", "\n", "dataType", "=", "np", ".", "float64", ",", "device", "=", "'cpu'", ")", ":", "\n", "\n", "# Initialize parent class", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save the relevant input information", "\n", "#   Number of nodes", "\n", "self", ".", "nAgents", "=", "nAgents", "\n", "self", ".", "commRadius", "=", "commRadius", "\n", "self", ".", "repelDist", "=", "repelDist", "\n", "#   Number of samples", "\n", "self", ".", "nTrain", "=", "nTrain", "\n", "self", ".", "nValid", "=", "nValid", "\n", "self", ".", "nTest", "=", "nTest", "\n", "nSamples", "=", "nTrain", "+", "nValid", "+", "nTest", "\n", "#   Geometry", "\n", "self", ".", "mapWidth", "=", "None", "\n", "self", ".", "mapHeight", "=", "None", "\n", "#   Agents", "\n", "self", ".", "initGeometry", "=", "initGeometry", "\n", "self", ".", "initVelValue", "=", "initVelValue", "\n", "self", ".", "initMinDist", "=", "initMinDist", "\n", "self", ".", "accelMax", "=", "accelMax", "\n", "#   Duration of the trajectory", "\n", "self", ".", "duration", "=", "float", "(", "duration", ")", "\n", "self", ".", "samplingTime", "=", "samplingTime", "\n", "#   Data", "\n", "self", ".", "normalizeGraph", "=", "normalizeGraph", "\n", "self", ".", "dataType", "=", "dataType", "\n", "self", ".", "device", "=", "device", "\n", "#   Options", "\n", "self", ".", "doPrint", "=", "doPrint", "\n", "\n", "#   Places to store the data", "\n", "self", ".", "initPos", "=", "None", "\n", "self", ".", "initVel", "=", "None", "\n", "self", ".", "pos", "=", "None", "\n", "self", ".", "vel", "=", "None", "\n", "self", ".", "accel", "=", "None", "\n", "self", ".", "commGraph", "=", "None", "\n", "self", ".", "state", "=", "None", "\n", "\n", "if", "self", ".", "doPrint", ":", "\n", "            ", "print", "(", "\"\\tComputing initial conditions...\"", ",", "end", "=", "' '", ",", "flush", "=", "True", ")", "\n", "\n", "# Compute the initial positions", "\n", "", "initPosAll", ",", "initVelAll", "=", "self", ".", "computeInitialPositions", "(", "\n", "self", ".", "nAgents", ",", "nSamples", ",", "self", ".", "commRadius", ",", "\n", "minDist", "=", "self", ".", "initMinDist", ",", "\n", "geometry", "=", "self", ".", "initGeometry", ",", "\n", "xMaxInitVel", "=", "self", ".", "initVelValue", ",", "\n", "yMaxInitVel", "=", "self", ".", "initVelValue", "\n", ")", "\n", "#   Once we have all positions and velocities, we will need to split ", "\n", "#   them in the corresponding datasets (train, valid and test)", "\n", "self", ".", "initPos", "=", "{", "}", "\n", "self", ".", "initVel", "=", "{", "}", "\n", "\n", "if", "self", ".", "doPrint", ":", "\n", "            ", "print", "(", "\"OK\"", ",", "flush", "=", "True", ")", "\n", "# Erase the label first, then print it", "\n", "print", "(", "\"\\tComputing the optimal trajectories...\"", ",", "\n", "end", "=", "' '", ",", "flush", "=", "True", ")", "\n", "\n", "# Compute the optimal trajectory", "\n", "", "posAll", ",", "velAll", ",", "accelAll", "=", "self", ".", "computeOptimalTrajectory", "(", "\n", "initPosAll", ",", "initVelAll", ",", "self", ".", "duration", ",", "\n", "self", ".", "samplingTime", ",", "self", ".", "repelDist", ",", "\n", "accelMax", "=", "self", ".", "accelMax", ")", "\n", "\n", "self", ".", "pos", "=", "{", "}", "\n", "self", ".", "vel", "=", "{", "}", "\n", "self", ".", "accel", "=", "{", "}", "\n", "\n", "if", "self", ".", "doPrint", ":", "\n", "            ", "print", "(", "\"OK\"", ",", "flush", "=", "True", ")", "\n", "# Erase the label first, then print it", "\n", "print", "(", "\"\\tComputing the communication graphs...\"", ",", "\n", "end", "=", "' '", ",", "flush", "=", "True", ")", "\n", "\n", "# Compute communication graph", "\n", "", "commGraphAll", "=", "self", ".", "computeCommunicationGraph", "(", "posAll", ",", "self", ".", "commRadius", ",", "\n", "self", ".", "normalizeGraph", ")", "\n", "\n", "self", ".", "commGraph", "=", "{", "}", "\n", "\n", "if", "self", ".", "doPrint", ":", "\n", "            ", "print", "(", "\"OK\"", ",", "flush", "=", "True", ")", "\n", "# Erase the label first, then print it", "\n", "print", "(", "\"\\tComputing the agent states...\"", ",", "end", "=", "' '", ",", "flush", "=", "True", ")", "\n", "\n", "# Compute the states", "\n", "", "stateAll", "=", "self", ".", "computeStates", "(", "posAll", ",", "velAll", ",", "commGraphAll", ")", "\n", "\n", "self", ".", "state", "=", "{", "}", "\n", "\n", "if", "self", ".", "doPrint", ":", "\n", "# Erase the label", "\n", "            ", "print", "(", "\"OK\"", ",", "flush", "=", "True", ")", "\n", "\n", "# Separate the states into training, validation and testing samples", "\n", "# and save them", "\n", "#   Training set", "\n", "", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "stateAll", "[", "0", ":", "self", ".", "nTrain", "]", ".", "copy", "(", ")", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "accelAll", "[", "0", ":", "self", ".", "nTrain", "]", ".", "copy", "(", ")", "\n", "self", ".", "initPos", "[", "'train'", "]", "=", "initPosAll", "[", "0", ":", "self", ".", "nTrain", "]", "\n", "self", ".", "initVel", "[", "'train'", "]", "=", "initVelAll", "[", "0", ":", "self", ".", "nTrain", "]", "\n", "self", ".", "pos", "[", "'train'", "]", "=", "posAll", "[", "0", ":", "self", ".", "nTrain", "]", "\n", "self", ".", "vel", "[", "'train'", "]", "=", "velAll", "[", "0", ":", "self", ".", "nTrain", "]", "\n", "self", ".", "accel", "[", "'train'", "]", "=", "accelAll", "[", "0", ":", "self", ".", "nTrain", "]", "\n", "self", ".", "commGraph", "[", "'train'", "]", "=", "commGraphAll", "[", "0", ":", "self", ".", "nTrain", "]", "\n", "self", ".", "state", "[", "'train'", "]", "=", "stateAll", "[", "0", ":", "self", ".", "nTrain", "]", "\n", "#   Validation set", "\n", "startSample", "=", "self", ".", "nTrain", "\n", "endSample", "=", "self", ".", "nTrain", "+", "self", ".", "nValid", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "=", "stateAll", "[", "startSample", ":", "endSample", "]", ".", "copy", "(", ")", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "=", "accelAll", "[", "startSample", ":", "endSample", "]", ".", "copy", "(", ")", "\n", "self", ".", "initPos", "[", "'valid'", "]", "=", "initPosAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "initVel", "[", "'valid'", "]", "=", "initVelAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "pos", "[", "'valid'", "]", "=", "posAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "vel", "[", "'valid'", "]", "=", "velAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "accel", "[", "'valid'", "]", "=", "accelAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "commGraph", "[", "'valid'", "]", "=", "commGraphAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "state", "[", "'valid'", "]", "=", "stateAll", "[", "startSample", ":", "endSample", "]", "\n", "#   Testing set", "\n", "startSample", "=", "self", ".", "nTrain", "+", "self", ".", "nValid", "\n", "endSample", "=", "self", ".", "nTrain", "+", "self", ".", "nValid", "+", "self", ".", "nTest", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "=", "stateAll", "[", "startSample", ":", "endSample", "]", ".", "copy", "(", ")", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "=", "accelAll", "[", "startSample", ":", "endSample", "]", ".", "copy", "(", ")", "\n", "self", ".", "initPos", "[", "'test'", "]", "=", "initPosAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "initVel", "[", "'test'", "]", "=", "initVelAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "pos", "[", "'test'", "]", "=", "posAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "vel", "[", "'test'", "]", "=", "velAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "accel", "[", "'test'", "]", "=", "accelAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "commGraph", "[", "'test'", "]", "=", "commGraphAll", "[", "startSample", ":", "endSample", "]", "\n", "self", ".", "state", "[", "'test'", "]", "=", "stateAll", "[", "startSample", ":", "endSample", "]", "\n", "\n", "# Change data to specified type and device", "\n", "self", ".", "astype", "(", "self", ".", "dataType", ")", "\n", "self", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.astype": [[2575, 2590], ["dataTools._data.astype", "dataTools.changeDataType", "dataTools.changeDataType", "dataTools.changeDataType", "dataTools.changeDataType", "dataTools.changeDataType", "dataTools.changeDataType", "dataTools.changeDataType"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType"], ["", "def", "astype", "(", "self", ",", "dataType", ")", ":", "\n", "\n", "# Change all other signals to the correct place", "\n", "        ", "datasetType", "=", "[", "'train'", ",", "'valid'", ",", "'test'", "]", "\n", "for", "key", "in", "datasetType", ":", "\n", "            ", "self", ".", "initPos", "[", "key", "]", "=", "changeDataType", "(", "self", ".", "initPos", "[", "key", "]", ",", "dataType", ")", "\n", "self", ".", "initVel", "[", "key", "]", "=", "changeDataType", "(", "self", ".", "initVel", "[", "key", "]", ",", "dataType", ")", "\n", "self", ".", "pos", "[", "key", "]", "=", "changeDataType", "(", "self", ".", "pos", "[", "key", "]", ",", "dataType", ")", "\n", "self", ".", "vel", "[", "key", "]", "=", "changeDataType", "(", "self", ".", "vel", "[", "key", "]", ",", "dataType", ")", "\n", "self", ".", "accel", "[", "key", "]", "=", "changeDataType", "(", "self", ".", "accel", "[", "key", "]", ",", "dataType", ")", "\n", "self", ".", "commGraph", "[", "key", "]", "=", "changeDataType", "(", "self", ".", "commGraph", "[", "key", "]", ",", "dataType", ")", "\n", "self", ".", "state", "[", "key", "]", "=", "changeDataType", "(", "self", ".", "state", "[", "key", "]", ",", "dataType", ")", "\n", "\n", "# And call the parent", "\n", "", "super", "(", ")", ".", "astype", "(", "dataType", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.to": [[2591, 2607], ["repr", "dataTools._data.to", "dataTools.Flocking.initPos[].to", "dataTools.Flocking.initVel[].to", "dataTools.Flocking.pos[].to", "dataTools.Flocking.vel[].to", "dataTools.Flocking.accel[].to", "dataTools.Flocking.commGraph[].to", "dataTools.Flocking.state[].to"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "\n", "# Check the data is actually torch", "\n", "        ", "if", "'torch'", "in", "repr", "(", "self", ".", "dataType", ")", ":", "\n", "            ", "datasetType", "=", "[", "'train'", ",", "'valid'", ",", "'test'", "]", "\n", "# Move the data", "\n", "for", "key", "in", "datasetType", ":", "\n", "                ", "self", ".", "initPos", "[", "key", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "initVel", "[", "key", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "pos", "[", "key", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "vel", "[", "key", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "accel", "[", "key", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "commGraph", "[", "key", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "state", "[", "key", "]", ".", "to", "(", "device", ")", "\n", "\n", "", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.expandDims": [[2608, 2611], ["None"], "methods", ["None"], ["", "", "def", "expandDims", "(", "self", ")", ":", "\n", "# Just avoid the 'expandDims' method in the parent class", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeStates": [[2612, 2815], ["numpy.cumsum().tolist", "numpy.zeros", "range", "kwargs.keys", "len", "len", "len", "print", "int", "numpy.cumsum", "range", "dataTools.Flocking.computeDifferences", "dataTools.Flocking.computeDifferences", "dataTools.invertTensorEW", "numpy.expand_dims", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.concatenate", "sum", "dataTools.Flocking.computeDifferences", "dataTools.Flocking.computeDifferences", "dataTools.invertTensorEW", "numpy.expand_dims", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.concatenate", "numpy.expand_dims", "int", "numpy.expand_dims", "int", "print", "print", "print", "print", "numpy.abs", "numpy.abs"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW"], ["", "def", "computeStates", "(", "self", ",", "pos", ",", "vel", ",", "graphMatrix", ",", "**", "kwargs", ")", ":", "\n", "\n", "# We get the following inputs.", "\n", "# positions: nSamples x tSamples x 2 x nAgents", "\n", "# velocities: nSamples x tSamples x 2 x nAgents", "\n", "# graphMatrix: nSaples x tSamples x nAgents x nAgents", "\n", "\n", "# And we want to build the state, which is a vector of dimension 6 on ", "\n", "# each node, that is, the output shape is", "\n", "#   nSamples x tSamples x 6 x nAgents", "\n", "\n", "# The print for this one can be settled independently, if not, use the", "\n", "# default of the data object", "\n", "        ", "if", "'doPrint'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "doPrint", "=", "kwargs", "[", "'doPrint'", "]", "\n", "", "else", ":", "\n", "            ", "doPrint", "=", "self", ".", "doPrint", "\n", "\n", "# Check correct dimensions", "\n", "", "assert", "len", "(", "pos", ".", "shape", ")", "==", "len", "(", "vel", ".", "shape", ")", "==", "len", "(", "graphMatrix", ".", "shape", ")", "==", "4", "\n", "nSamples", "=", "pos", ".", "shape", "[", "0", "]", "\n", "tSamples", "=", "pos", ".", "shape", "[", "1", "]", "\n", "assert", "pos", ".", "shape", "[", "2", "]", "==", "2", "\n", "nAgents", "=", "pos", ".", "shape", "[", "3", "]", "\n", "assert", "vel", ".", "shape", "[", "0", "]", "==", "graphMatrix", ".", "shape", "[", "0", "]", "==", "nSamples", "\n", "assert", "vel", ".", "shape", "[", "1", "]", "==", "graphMatrix", ".", "shape", "[", "1", "]", "==", "tSamples", "\n", "assert", "vel", ".", "shape", "[", "2", "]", "==", "2", "\n", "assert", "vel", ".", "shape", "[", "3", "]", "==", "graphMatrix", ".", "shape", "[", "2", "]", "==", "graphMatrix", ".", "shape", "[", "3", "]", "==", "nAgents", "\n", "\n", "# If we have a lot of batches and a particularly long sequence, this", "\n", "# is bound to fail, memory-wise, so let's do it time instant by time", "\n", "# instant if we have a large number of time instants, and split the", "\n", "# batches", "\n", "maxTimeSamples", "=", "200", "# Set the maximum number of t.Samples before", "\n", "# which to start doing this time by time.", "\n", "maxBatchSize", "=", "100", "# Maximum number of samples to process at a given", "\n", "# time", "\n", "\n", "# Compute the number of samples, and split the indices accordingly", "\n", "if", "nSamples", "<", "maxBatchSize", ":", "\n", "            ", "nBatches", "=", "1", "\n", "batchSize", "=", "[", "nSamples", "]", "\n", "", "elif", "nSamples", "%", "maxBatchSize", "!=", "0", ":", "\n", "# If we know it's not divisible, then we do floor division and", "\n", "# add one more batch", "\n", "            ", "nBatches", "=", "nSamples", "//", "maxBatchSize", "+", "1", "\n", "batchSize", "=", "[", "maxBatchSize", "]", "*", "nBatches", "\n", "# But the last batch is actually smaller, so just add the ", "\n", "# remaining ones", "\n", "batchSize", "[", "-", "1", "]", "=", "nSamples", "-", "sum", "(", "batchSize", "[", "0", ":", "-", "1", "]", ")", "\n", "# If they fit evenly, then just do so.", "\n", "", "else", ":", "\n", "            ", "nBatches", "=", "int", "(", "nSamples", "/", "maxBatchSize", ")", "\n", "batchSize", "=", "[", "maxBatchSize", "]", "*", "nBatches", "\n", "# batchIndex is used to determine the first and last element of each", "\n", "# batch. We need to add the 0 because it's the first index.", "\n", "", "batchIndex", "=", "np", ".", "cumsum", "(", "batchSize", ")", ".", "tolist", "(", ")", "\n", "batchIndex", "=", "[", "0", "]", "+", "batchIndex", "\n", "\n", "# Create the output state variable", "\n", "state", "=", "np", ".", "zeros", "(", "(", "nSamples", ",", "tSamples", ",", "6", ",", "nAgents", ")", ")", "\n", "\n", "for", "b", "in", "range", "(", "nBatches", ")", ":", "\n", "\n", "# Pick the batch elements", "\n", "            ", "posBatch", "=", "pos", "[", "batchIndex", "[", "b", "]", ":", "batchIndex", "[", "b", "+", "1", "]", "]", "\n", "velBatch", "=", "vel", "[", "batchIndex", "[", "b", "]", ":", "batchIndex", "[", "b", "+", "1", "]", "]", "\n", "graphMatrixBatch", "=", "graphMatrix", "[", "batchIndex", "[", "b", "]", ":", "batchIndex", "[", "b", "+", "1", "]", "]", "\n", "\n", "if", "tSamples", ">", "maxTimeSamples", ":", "\n", "\n", "# For each time instant", "\n", "                ", "for", "t", "in", "range", "(", "tSamples", ")", ":", "\n", "\n", "# Now, we need to compute the differences, in velocities and in ", "\n", "# positions, for each agent, for each time instant", "\n", "                    ", "posDiff", ",", "posDistSq", "=", "self", ".", "computeDifferences", "(", "posBatch", "[", ":", ",", "t", ",", ":", ",", ":", "]", ")", "\n", "#   posDiff: batchSize[b] x 2 x nAgents x nAgents", "\n", "#   posDistSq: batchSize[b] x nAgents x nAgents", "\n", "velDiff", ",", "_", "=", "self", ".", "computeDifferences", "(", "velBatch", "[", ":", ",", "t", ",", ":", ",", ":", "]", ")", "\n", "#   velDiff: batchSize[b] x 2 x nAgents x nAgents", "\n", "\n", "# Next, we need to get ride of all those places where there are", "\n", "# no neighborhoods. That is given by the nonzero elements of the ", "\n", "# graph matrix.", "\n", "graphMatrixTime", "=", "(", "np", ".", "abs", "(", "graphMatrixBatch", "[", ":", ",", "t", ",", ":", ",", ":", "]", ")", ">", "zeroTolerance", ")", ".", "astype", "(", "pos", ".", "dtype", ")", "\n", "#   graphMatrix: batchSize[b] x nAgents x nAgents", "\n", "# We also need to invert the squares of the distances", "\n", "posDistSqInv", "=", "invertTensorEW", "(", "posDistSq", ")", "\n", "#   posDistSqInv: batchSize[b] x nAgents x nAgents", "\n", "\n", "# Now we add the extra dimensions so that all the ", "\n", "# multiplications are adequate", "\n", "graphMatrixTime", "=", "np", ".", "expand_dims", "(", "graphMatrixTime", ",", "1", ")", "\n", "#   graphMatrix: batchSize[b] x 1 x nAgents x nAgents", "\n", "\n", "# Then, we can get rid of non-neighbors", "\n", "posDiff", "=", "posDiff", "*", "graphMatrixTime", "\n", "posDistSqInv", "=", "np", ".", "expand_dims", "(", "posDistSqInv", ",", "1", ")", "*", "graphMatrixTime", "\n", "velDiff", "=", "velDiff", "*", "graphMatrixTime", "\n", "\n", "# Finally, we can compute the states", "\n", "stateVel", "=", "np", ".", "sum", "(", "velDiff", ",", "axis", "=", "3", ")", "\n", "#   stateVel: batchSize[b] x 2 x nAgents", "\n", "statePosFourth", "=", "np", ".", "sum", "(", "posDiff", "*", "(", "posDistSqInv", "**", "2", ")", ",", "\n", "axis", "=", "3", ")", "\n", "#   statePosFourth: batchSize[b] x 2 x nAgents", "\n", "statePosSq", "=", "np", ".", "sum", "(", "posDiff", "*", "posDistSqInv", ",", "axis", "=", "3", ")", "\n", "#   statePosSq: batchSize[b] x 2 x nAgents", "\n", "\n", "# Concatentate the states and return the result", "\n", "state", "[", "batchIndex", "[", "b", "]", ":", "batchIndex", "[", "b", "+", "1", "]", ",", "t", ",", ":", ",", ":", "]", "=", "np", ".", "concatenate", "(", "(", "stateVel", ",", "\n", "statePosFourth", ",", "\n", "statePosSq", ")", ",", "\n", "axis", "=", "1", ")", "\n", "#   batchSize[b] x 6 x nAgents", "\n", "\n", "if", "doPrint", ":", "\n", "# Sample percentage count", "\n", "                        ", "percentageCount", "=", "int", "(", "100", "*", "(", "t", "+", "1", "+", "b", "*", "tSamples", ")", "/", "(", "nBatches", "*", "tSamples", ")", ")", "\n", "\n", "if", "t", "==", "0", "and", "b", "==", "0", ":", "\n", "# It's the first one, so just print it", "\n", "                            ", "print", "(", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "", "else", ":", "\n", "# Erase the previous characters", "\n", "                            ", "print", "(", "'\\b \\b'", "*", "4", "+", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "", "", "", "", "else", ":", "\n", "\n", "# Now, we need to compute the differences, in velocities and in ", "\n", "# positions, for each agent, for each time instante", "\n", "                ", "posDiff", ",", "posDistSq", "=", "self", ".", "computeDifferences", "(", "posBatch", ")", "\n", "#   posDiff: batchSize[b] x tSamples x 2 x nAgents x nAgents", "\n", "#   posDistSq: batchSize[b] x tSamples x nAgents x nAgents", "\n", "velDiff", ",", "_", "=", "self", ".", "computeDifferences", "(", "velBatch", ")", "\n", "#   velDiff: batchSize[b] x tSamples x 2 x nAgents x nAgents", "\n", "\n", "# Next, we need to get ride of all those places where there are", "\n", "# no neighborhoods. That is given by the nonzero elements of the ", "\n", "# graph matrix.", "\n", "graphMatrixBatch", "=", "(", "np", ".", "abs", "(", "graphMatrixBatch", ")", ">", "zeroTolerance", ")", ".", "astype", "(", "pos", ".", "dtype", ")", "\n", "#   graphMatrix: batchSize[b] x tSamples x nAgents x nAgents", "\n", "# We also need to invert the squares of the distances", "\n", "posDistSqInv", "=", "invertTensorEW", "(", "posDistSq", ")", "\n", "#   posDistSqInv: batchSize[b] x tSamples x nAgents x nAgents", "\n", "\n", "# Now we add the extra dimensions so that all the multiplications", "\n", "# are adequate", "\n", "graphMatrixBatch", "=", "np", ".", "expand_dims", "(", "graphMatrixBatch", ",", "2", ")", "\n", "#   graphMatrix:batchSize[b] x tSamples x 1 x nAgents x nAgents", "\n", "\n", "# Then, we can get rid of non-neighbors", "\n", "posDiff", "=", "posDiff", "*", "graphMatrixBatch", "\n", "posDistSqInv", "=", "np", ".", "expand_dims", "(", "posDistSqInv", ",", "2", ")", "*", "graphMatrixBatch", "\n", "velDiff", "=", "velDiff", "*", "graphMatrixBatch", "\n", "\n", "# Finally, we can compute the states", "\n", "stateVel", "=", "np", ".", "sum", "(", "velDiff", ",", "axis", "=", "4", ")", "\n", "#   stateVel: batchSize[b] x tSamples x 2 x nAgents", "\n", "statePosFourth", "=", "np", ".", "sum", "(", "posDiff", "*", "(", "posDistSqInv", "**", "2", ")", ",", "axis", "=", "4", ")", "\n", "#   statePosFourth: batchSize[b] x tSamples x 2 x nAgents", "\n", "statePosSq", "=", "np", ".", "sum", "(", "posDiff", "*", "posDistSqInv", ",", "axis", "=", "4", ")", "\n", "#   statePosSq: batchSize[b] x tSamples x 2 x nAgents", "\n", "\n", "# Concatentate the states and return the result", "\n", "state", "[", "batchIndex", "[", "b", "]", ":", "batchIndex", "[", "b", "+", "1", "]", "]", "=", "np", ".", "concatenate", "(", "(", "stateVel", ",", "\n", "statePosFourth", ",", "\n", "statePosSq", ")", ",", "\n", "axis", "=", "2", ")", "\n", "#   state: batchSize[b] x tSamples x 6 x nAgents", "\n", "\n", "if", "doPrint", ":", "\n", "# Sample percentage count", "\n", "                    ", "percentageCount", "=", "int", "(", "100", "*", "(", "b", "+", "1", ")", "/", "nBatches", ")", "\n", "\n", "if", "b", "==", "0", ":", "\n", "# It's the first one, so just print it", "\n", "                        ", "print", "(", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "", "else", ":", "\n", "# Erase the previous characters", "\n", "                        ", "print", "(", "'\\b \\b'", "*", "4", "+", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "# Print", "\n", "", "", "", "", "if", "doPrint", ":", "\n", "# Erase the percentage", "\n", "            ", "print", "(", "'\\b \\b'", "*", "4", ",", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeCommunicationGraph": [[2816, 3020], ["numpy.cumsum().tolist", "numpy.zeros", "range", "len", "kwargs.keys", "kwargs.keys", "kwargs.keys", "print", "kwargs.keys", "int", "numpy.cumsum", "range", "dataTools.Flocking.computeDifferences", "sum", "dataTools.Flocking.computeDifferences", "numpy.exp", "numpy.allclose", "numpy.max", "maxEigenvalue.reshape.reshape.reshape", "int", "numpy.exp", "numpy.allclose", "numpy.max", "maxEigenvalue.reshape.reshape.reshape", "int", "numpy.transpose", "numpy.linalg.eigvalsh", "numpy.linalg.eigvals", "numpy.real", "print", "print", "numpy.transpose", "numpy.linalg.eigvalsh", "numpy.linalg.eigvals", "numpy.real", "print", "print", "numpy.arange", "numpy.arange", "numpy.arange", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences"], ["", "def", "computeCommunicationGraph", "(", "self", ",", "pos", ",", "commRadius", ",", "normalizeGraph", ",", "\n", "**", "kwargs", ")", ":", "\n", "\n", "# Take in the position and the communication radius, and return the", "\n", "# trajectory of communication graphs", "\n", "# Input will be of shape", "\n", "#   nSamples x tSamples x 2 x nAgents", "\n", "# Output will be of shape", "\n", "#   nSamples x tSamples x nAgents x nAgents", "\n", "\n", "        ", "assert", "commRadius", ">", "0", "\n", "assert", "len", "(", "pos", ".", "shape", ")", "==", "4", "\n", "nSamples", "=", "pos", ".", "shape", "[", "0", "]", "\n", "tSamples", "=", "pos", ".", "shape", "[", "1", "]", "\n", "assert", "pos", ".", "shape", "[", "2", "]", "==", "2", "\n", "nAgents", "=", "pos", ".", "shape", "[", "3", "]", "\n", "\n", "# Graph type options", "\n", "#   Kernel type (only Gaussian implemented so far)", "\n", "if", "'kernelType'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "kernelType", "=", "kwargs", "[", "'kernelType'", "]", "\n", "", "else", ":", "\n", "            ", "kernelType", "=", "'gaussian'", "\n", "#   Decide if the graph is weighted or not", "\n", "", "if", "'weighted'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "weighted", "=", "kwargs", "[", "'weighted'", "]", "\n", "", "else", ":", "\n", "            ", "weighted", "=", "False", "\n", "\n", "# If it is a Gaussian kernel, we need to determine the scale", "\n", "", "if", "kernelType", "==", "'gaussian'", ":", "\n", "            ", "if", "'kernelScale'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "                ", "kernelScale", "=", "kwargs", "[", "'kernelScale'", "]", "\n", "", "else", ":", "\n", "                ", "kernelScale", "=", "1.", "\n", "\n", "# The print for this one can be settled independently, if not, use the", "\n", "# default of the data object", "\n", "", "", "if", "'doPrint'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "doPrint", "=", "kwargs", "[", "'doPrint'", "]", "\n", "", "else", ":", "\n", "            ", "doPrint", "=", "self", ".", "doPrint", "\n", "\n", "# If we have a lot of batches and a particularly long sequence, this", "\n", "# is bound to fail, memory-wise, so let's do it time instant by time", "\n", "# instant if we have a large number of time instants, and split the", "\n", "# batches", "\n", "", "maxTimeSamples", "=", "200", "# Set the maximum number of t.Samples before", "\n", "# which to start doing this time by time.", "\n", "maxBatchSize", "=", "100", "# Maximum number of samples to process at a given", "\n", "# time", "\n", "\n", "# Compute the number of samples, and split the indices accordingly", "\n", "if", "nSamples", "<", "maxBatchSize", ":", "\n", "            ", "nBatches", "=", "1", "\n", "batchSize", "=", "[", "nSamples", "]", "\n", "", "elif", "nSamples", "%", "maxBatchSize", "!=", "0", ":", "\n", "# If we know it's not divisible, then we do floor division and", "\n", "# add one more batch", "\n", "            ", "nBatches", "=", "nSamples", "//", "maxBatchSize", "+", "1", "\n", "batchSize", "=", "[", "maxBatchSize", "]", "*", "nBatches", "\n", "# But the last batch is actually smaller, so just add the ", "\n", "# remaining ones", "\n", "batchSize", "[", "-", "1", "]", "=", "nSamples", "-", "sum", "(", "batchSize", "[", "0", ":", "-", "1", "]", ")", "\n", "# If they fit evenly, then just do so.", "\n", "", "else", ":", "\n", "            ", "nBatches", "=", "int", "(", "nSamples", "/", "maxBatchSize", ")", "\n", "batchSize", "=", "[", "maxBatchSize", "]", "*", "nBatches", "\n", "# batchIndex is used to determine the first and last element of each", "\n", "# batch. We need to add the 0 because it's the first index.", "\n", "", "batchIndex", "=", "np", ".", "cumsum", "(", "batchSize", ")", ".", "tolist", "(", ")", "\n", "batchIndex", "=", "[", "0", "]", "+", "batchIndex", "\n", "\n", "# Create the output state variable", "\n", "graphMatrix", "=", "np", ".", "zeros", "(", "(", "nSamples", ",", "tSamples", ",", "nAgents", ",", "nAgents", ")", ")", "\n", "\n", "for", "b", "in", "range", "(", "nBatches", ")", ":", "\n", "\n", "# Pick the batch elements", "\n", "            ", "posBatch", "=", "pos", "[", "batchIndex", "[", "b", "]", ":", "batchIndex", "[", "b", "+", "1", "]", "]", "\n", "\n", "if", "tSamples", ">", "maxTimeSamples", ":", "\n", "# If the trajectories are longer than 200 points, then do it ", "\n", "# time by time.", "\n", "\n", "# For each time instant", "\n", "                ", "for", "t", "in", "range", "(", "tSamples", ")", ":", "\n", "\n", "# Let's start by computing the distance squared", "\n", "                    ", "_", ",", "distSq", "=", "self", ".", "computeDifferences", "(", "posBatch", "[", ":", ",", "t", ",", ":", ",", ":", "]", ")", "\n", "# Apply the Kernel", "\n", "if", "kernelType", "==", "'gaussian'", ":", "\n", "                        ", "graphMatrixTime", "=", "np", ".", "exp", "(", "-", "kernelScale", "*", "distSq", ")", "\n", "", "else", ":", "\n", "                        ", "graphMatrixTime", "=", "distSq", "\n", "# Now let's place zeros in all places whose distance is greater", "\n", "# than the radius", "\n", "", "graphMatrixTime", "[", "distSq", ">", "(", "commRadius", "**", "2", ")", "]", "=", "0.", "\n", "# Set the diagonal elements to zero", "\n", "graphMatrixTime", "[", ":", ",", "np", ".", "arange", "(", "0", ",", "nAgents", ")", ",", "np", ".", "arange", "(", "0", ",", "nAgents", ")", "]", "=", "0.", "\n", "# If it is unweighted, force all nonzero values to be 1", "\n", "if", "not", "weighted", ":", "\n", "                        ", "graphMatrixTime", "=", "(", "graphMatrixTime", ">", "zeroTolerance", ")", ".", "astype", "(", "distSq", ".", "dtype", ")", "\n", "\n", "", "if", "normalizeGraph", ":", "\n", "                        ", "isSymmetric", "=", "np", ".", "allclose", "(", "graphMatrixTime", ",", "\n", "np", ".", "transpose", "(", "graphMatrixTime", ",", "\n", "axes", "=", "[", "0", ",", "2", ",", "1", "]", ")", ")", "\n", "# Tries to make the computation faster, only the ", "\n", "# eigenvalues (while there is a cost involved in ", "\n", "# computing whether the matrix is symmetric, ", "\n", "# experiments found that it is still faster to use the", "\n", "# symmetric algorithm for the eigenvalues)", "\n", "if", "isSymmetric", ":", "\n", "                            ", "W", "=", "np", ".", "linalg", ".", "eigvalsh", "(", "graphMatrixTime", ")", "\n", "", "else", ":", "\n", "                            ", "W", "=", "np", ".", "linalg", ".", "eigvals", "(", "graphMatrixTime", ")", "\n", "", "maxEigenvalue", "=", "np", ".", "max", "(", "np", ".", "real", "(", "W", ")", ",", "axis", "=", "1", ")", "\n", "#   batchSize[b]", "\n", "# Reshape to be able to divide by the graph matrix", "\n", "maxEigenvalue", "=", "maxEigenvalue", ".", "reshape", "(", "(", "batchSize", "[", "b", "]", ",", "1", ",", "1", ")", ")", "\n", "# Normalize", "\n", "graphMatrixTime", "=", "graphMatrixTime", "/", "maxEigenvalue", "\n", "\n", "# And put it in the corresponding time instant", "\n", "", "graphMatrix", "[", "batchIndex", "[", "b", "]", ":", "batchIndex", "[", "b", "+", "1", "]", ",", "t", ",", ":", ",", ":", "]", "=", "graphMatrixTime", "\n", "\n", "if", "doPrint", ":", "\n", "# Sample percentage count", "\n", "                        ", "percentageCount", "=", "int", "(", "100", "*", "(", "t", "+", "1", "+", "b", "*", "tSamples", ")", "/", "(", "nBatches", "*", "tSamples", ")", ")", "\n", "\n", "if", "t", "==", "0", "and", "b", "==", "0", ":", "\n", "# It's the first one, so just print it", "\n", "                            ", "print", "(", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "", "else", ":", "\n", "# Erase the previous characters", "\n", "                            ", "print", "(", "'\\b \\b'", "*", "4", "+", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "", "", "", "", "else", ":", "\n", "# Let's start by computing the distance squared", "\n", "                ", "_", ",", "distSq", "=", "self", ".", "computeDifferences", "(", "posBatch", ")", "\n", "# Apply the Kernel", "\n", "if", "kernelType", "==", "'gaussian'", ":", "\n", "                    ", "graphMatrixBatch", "=", "np", ".", "exp", "(", "-", "kernelScale", "*", "distSq", ")", "\n", "", "else", ":", "\n", "                    ", "graphMatrixBatch", "=", "distSq", "\n", "# Now let's place zeros in all places whose distance is greater", "\n", "# than the radius", "\n", "", "graphMatrixBatch", "[", "distSq", ">", "(", "commRadius", "**", "2", ")", "]", "=", "0.", "\n", "# Set the diagonal elements to zero", "\n", "graphMatrixBatch", "[", ":", ",", ":", ",", "\n", "np", ".", "arange", "(", "0", ",", "nAgents", ")", ",", "np", ".", "arange", "(", "0", ",", "nAgents", ")", "]", "=", "0.", "\n", "# If it is unweighted, force all nonzero values to be 1", "\n", "if", "not", "weighted", ":", "\n", "                    ", "graphMatrixBatch", "=", "(", "graphMatrixBatch", ">", "zeroTolerance", ")", ".", "astype", "(", "distSq", ".", "dtype", ")", "\n", "\n", "", "if", "normalizeGraph", ":", "\n", "                    ", "isSymmetric", "=", "np", ".", "allclose", "(", "graphMatrixBatch", ",", "\n", "np", ".", "transpose", "(", "graphMatrixBatch", ",", "\n", "axes", "=", "[", "0", ",", "1", ",", "3", ",", "2", "]", ")", ")", "\n", "# Tries to make the computation faster", "\n", "if", "isSymmetric", ":", "\n", "                        ", "W", "=", "np", ".", "linalg", ".", "eigvalsh", "(", "graphMatrixBatch", ")", "\n", "", "else", ":", "\n", "                        ", "W", "=", "np", ".", "linalg", ".", "eigvals", "(", "graphMatrixBatch", ")", "\n", "", "maxEigenvalue", "=", "np", ".", "max", "(", "np", ".", "real", "(", "W", ")", ",", "axis", "=", "2", ")", "\n", "#   batchSize[b] x tSamples", "\n", "# Reshape to be able to divide by the graph matrix", "\n", "maxEigenvalue", "=", "maxEigenvalue", ".", "reshape", "(", "(", "batchSize", "[", "b", "]", ",", "\n", "tSamples", ",", "\n", "1", ",", "1", ")", ")", "\n", "# Normalize", "\n", "graphMatrixBatch", "=", "graphMatrixBatch", "/", "maxEigenvalue", "\n", "\n", "# Store", "\n", "", "graphMatrix", "[", "batchIndex", "[", "b", "]", ":", "batchIndex", "[", "b", "+", "1", "]", "]", "=", "graphMatrixBatch", "\n", "\n", "if", "doPrint", ":", "\n", "# Sample percentage count", "\n", "                    ", "percentageCount", "=", "int", "(", "100", "*", "(", "b", "+", "1", ")", "/", "nBatches", ")", "\n", "\n", "if", "b", "==", "0", ":", "\n", "# It's the first one, so just print it", "\n", "                        ", "print", "(", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "", "else", ":", "\n", "# Erase the previous characters", "\n", "                        ", "print", "(", "'\\b \\b'", "*", "4", "+", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "# Print", "\n", "", "", "", "", "if", "doPrint", ":", "\n", "# Erase the percentage", "\n", "            ", "print", "(", "'\\b \\b'", "*", "4", ",", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "", "return", "graphMatrix", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.getData": [[3021, 3081], ["getattr", "len", "len", "dir", "type", "getattr.keys", "len", "type", "numpy.random.choice", "len", "repr", "numpy.expand_dims.unsqueeze", "numpy.expand_dims"], "methods", ["None"], ["", "def", "getData", "(", "self", ",", "name", ",", "samplesType", ",", "*", "args", ")", ":", "\n", "\n", "# samplesType: train, valid, test", "\n", "# args: 0 args, give back all", "\n", "# args: 1 arg: if int, give that number of samples, chosen at random", "\n", "# args: 1 arg: if list, give those samples precisely.", "\n", "\n", "# Check that the type is one of the possible ones", "\n", "        ", "assert", "samplesType", "==", "'train'", "or", "samplesType", "==", "'valid'", "or", "samplesType", "==", "'test'", "\n", "# Check that the number of extra arguments fits", "\n", "assert", "len", "(", "args", ")", "<=", "1", "\n", "\n", "# Check that the name is actually an attribute", "\n", "assert", "name", "in", "dir", "(", "self", ")", "\n", "\n", "# Get the desired attribute", "\n", "thisDataDict", "=", "getattr", "(", "self", ",", "name", ")", "\n", "\n", "# Check it's a dictionary and that it has the corresponding key", "\n", "assert", "type", "(", "thisDataDict", ")", "is", "dict", "\n", "assert", "samplesType", "in", "thisDataDict", ".", "keys", "(", ")", "\n", "\n", "# Get the data now", "\n", "thisData", "=", "thisDataDict", "[", "samplesType", "]", "\n", "# Get the dimension length", "\n", "thisDataDims", "=", "len", "(", "thisData", ".", "shape", ")", "\n", "\n", "# Check that it has at least two dimension, where the first one is", "\n", "# always the number of samples", "\n", "assert", "thisDataDims", ">", "1", "\n", "\n", "if", "len", "(", "args", ")", "==", "1", ":", "\n", "# If it is an int, just return that number of randomly chosen", "\n", "# samples.", "\n", "            ", "if", "type", "(", "args", "[", "0", "]", ")", "==", "int", ":", "\n", "                ", "nSamples", "=", "thisData", ".", "shape", "[", "0", "]", "# total number of samples", "\n", "# We can't return more samples than there are available", "\n", "assert", "args", "[", "0", "]", "<=", "nSamples", "\n", "# Randomly choose args[0] indices", "\n", "selectedIndices", "=", "np", ".", "random", ".", "choice", "(", "nSamples", ",", "size", "=", "args", "[", "0", "]", ",", "\n", "replace", "=", "False", ")", "\n", "# Select the corresponding samples", "\n", "thisData", "=", "thisData", "[", "selectedIndices", "]", "\n", "", "else", ":", "\n", "# The fact that we put else here instead of elif type()==list", "\n", "# allows for np.array to be used as indices as well. In general,", "\n", "# any variable with the ability to index.", "\n", "                ", "thisData", "=", "thisData", "[", "args", "[", "0", "]", "]", "\n", "\n", "# If we only selected a single element, then the nDataPoints dim", "\n", "# has been left out. So if we have less dimensions, we have to", "\n", "# put it back", "\n", "", "if", "len", "(", "thisData", ".", "shape", ")", "<", "thisDataDims", ":", "\n", "                ", "if", "'torch'", "in", "repr", "(", "thisData", ".", "dtype", ")", ":", "\n", "                    ", "thisData", "=", "thisData", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "                    ", "thisData", "=", "np", ".", "expand_dims", "(", "thisData", ",", "axis", "=", "0", ")", "\n", "\n", "", "", "", "return", "thisData", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.evaluate": [[3082, 3165], ["repr", "torch.mean", "torch.sum", "torch.mean", "torch.sum", "torch.mean", "numpy.mean", "numpy.sum", "numpy.mean", "numpy.sum", "numpy.mean", "len", "range", "numpy.mean.unsqueeze", "numpy.tile", "repr", "torch.zeros", "initVel.clone().detach", "numpy.zeros", "initVel.copy", "numpy.expand_dims", "len", "len", "repr", "initVel.clone"], "methods", ["None"], ["", "def", "evaluate", "(", "self", ",", "vel", "=", "None", ",", "accel", "=", "None", ",", "initVel", "=", "None", ",", "\n", "samplingTime", "=", "None", ")", ":", "\n", "\n", "# It is optional to add a different sampling time, if not, it uses", "\n", "# the internal one", "\n", "        ", "if", "samplingTime", "is", "None", ":", "\n", "# If there's no argument use the internal sampling time", "\n", "            ", "samplingTime", "=", "self", ".", "samplingTime", "\n", "\n", "# Check whether we have vel, or accel and initVel (i.e. we are either", "\n", "# given the velocities, or we are given the elements to compute them)", "\n", "", "if", "vel", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "vel", ".", "shape", ")", "==", "4", "\n", "nSamples", "=", "vel", ".", "shape", "[", "0", "]", "\n", "tSamples", "=", "vel", ".", "shape", "[", "1", "]", "\n", "assert", "vel", ".", "shape", "[", "2", "]", "==", "2", "\n", "nAgents", "=", "vel", ".", "shape", "[", "3", "]", "\n", "", "elif", "accel", "is", "not", "None", "and", "initVel", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "accel", ".", "shape", ")", "==", "4", "and", "len", "(", "initVel", ".", "shape", ")", "==", "3", "\n", "nSamples", "=", "accel", ".", "shape", "[", "0", "]", "\n", "tSamples", "=", "accel", ".", "shape", "[", "1", "]", "\n", "assert", "accel", ".", "shape", "[", "2", "]", "==", "2", "\n", "nAgents", "=", "accel", ".", "shape", "[", "3", "]", "\n", "assert", "initVel", ".", "shape", "[", "0", "]", "==", "nSamples", "\n", "assert", "initVel", ".", "shape", "[", "1", "]", "==", "2", "\n", "assert", "initVel", ".", "shape", "[", "2", "]", "==", "nAgents", "\n", "\n", "# Now that we know we have a accel and init velocity, compute the", "\n", "# velocity trajectory", "\n", "# Compute the velocity trajectory", "\n", "if", "'torch'", "in", "repr", "(", "accel", ".", "dtype", ")", ":", "\n", "# Check that initVel is also torch", "\n", "                ", "assert", "'torch'", "in", "repr", "(", "initVel", ".", "dtype", ")", "\n", "# Create the tensor to save the velocity trajectory", "\n", "vel", "=", "torch", ".", "zeros", "(", "nSamples", ",", "tSamples", ",", "2", ",", "nAgents", ",", "\n", "dtype", "=", "accel", ".", "dtype", ",", "device", "=", "accel", ".", "device", ")", "\n", "# Add the initial velocity", "\n", "vel", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initVel", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "# Create the space", "\n", "                ", "vel", "=", "np", ".", "zeros", "(", "(", "nSamples", ",", "tSamples", ",", "2", ",", "nAgents", ")", ",", "\n", "dtype", "=", "accel", ".", "dtype", ")", "\n", "# Add the initial velocity", "\n", "vel", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initVel", ".", "copy", "(", ")", "\n", "\n", "# Go over time", "\n", "", "for", "t", "in", "range", "(", "1", ",", "tSamples", ")", ":", "\n", "# Compute velocity", "\n", "                ", "vel", "[", ":", ",", "t", ",", ":", ",", ":", "]", "=", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "*", "samplingTime", "+", "vel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "\n", "\n", "# Check that I did enter one of the if clauses", "\n", "", "", "assert", "vel", "is", "not", "None", "\n", "\n", "# And now that we have the velocities, we can compute the cost", "\n", "if", "'torch'", "in", "repr", "(", "vel", ".", "dtype", ")", ":", "\n", "# Average velocity for time t, averaged across agents", "\n", "            ", "avgVel", "=", "torch", ".", "mean", "(", "vel", ",", "dim", "=", "3", ")", "# nSamples x tSamples x 2", "\n", "# Compute the difference in velocity between each agent and the", "\n", "# mean velocity", "\n", "diffVel", "=", "vel", "-", "avgVel", ".", "unsqueeze", "(", "3", ")", "\n", "#   nSamples x tSamples x 2 x nAgents", "\n", "# Compute the MSE velocity", "\n", "diffVelNorm", "=", "torch", ".", "sum", "(", "diffVel", "**", "2", ",", "dim", "=", "2", ")", "\n", "#   nSamples x tSamples x nAgents", "\n", "# Average over agents", "\n", "diffVelAvg", "=", "torch", ".", "mean", "(", "diffVelNorm", ",", "dim", "=", "2", ")", "# nSamples x tSamples", "\n", "# Sum over time", "\n", "costPerSample", "=", "torch", ".", "sum", "(", "diffVelAvg", ",", "dim", "=", "1", ")", "# nSamples", "\n", "# Final average cost", "\n", "cost", "=", "torch", ".", "mean", "(", "costPerSample", ")", "\n", "", "else", ":", "\n", "# Repeat for numpy", "\n", "            ", "avgVel", "=", "np", ".", "mean", "(", "vel", ",", "axis", "=", "3", ")", "# nSamples x tSamples x 2", "\n", "diffVel", "=", "vel", "-", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "avgVel", ",", "3", ")", ",", "\n", "(", "1", ",", "1", ",", "1", ",", "nAgents", ")", ")", "\n", "#   nSamples x tSamples x 2 x nAgents", "\n", "diffVelNorm", "=", "np", ".", "sum", "(", "diffVel", "**", "2", ",", "axis", "=", "2", ")", "\n", "#   nSamples x tSamples x nAgents", "\n", "diffVelAvg", "=", "np", ".", "mean", "(", "diffVelNorm", ",", "axis", "=", "2", ")", "# nSamples x tSamples", "\n", "costPerSample", "=", "np", ".", "sum", "(", "diffVelAvg", ",", "axis", "=", "1", ")", "# nSamples", "\n", "cost", "=", "np", ".", "mean", "(", "costPerSample", ")", "# scalar", "\n", "\n", "", "return", "cost", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeTrajectory": [[3166, 3340], ["numpy.arange", "len", "numpy.zeros", "numpy.zeros", "range", "numpy.expand_dims", "dataTools.Flocking.computeCommunicationGraph", "dataTools.Flocking.squeeze", "numpy.expand_dims", "dataTools.Flocking.computeStates", "dataTools.Flocking.squeeze", "torch.tensor().to", "torch.tensor().to", "len", "len", "repr", "kwargs.keys", "kwargs.keys", "numpy.zeros", "numpy.zeros", "numpy.zeros", "initPos.cpu().numpy", "initVel.cpu().numpy", "initPos.copy", "initVel.copy", "int", "print", "torch.no_grad", "archit", "archit.cpu().numpy", "print", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "repr", "kwargs.keys", "kwargs.keys", "kwargs.keys", "accel.cpu().numpy.cpu().numpy.cpu().numpy", "numpy.expand_dims", "dataTools.Flocking.computeCommunicationGraph", "dataTools.Flocking.squeeze", "numpy.expand_dims", "dataTools.Flocking.computeStates", "dataTools.Flocking.squeeze", "torch.tensor", "torch.tensor", "int", "print", "torch.tensor", "torch.tensor", "list", "len", "initPos.cpu", "initVel.cpu", "torch.no_grad", "archit", "archit.cpu().numpy", "archit.cpu", "torch.tensor", "torch.tensor", "torch.tensor", "archit.parameters", "repr", "accel.cpu().numpy.cpu().numpy.cpu", "archit.cpu"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeCommunicationGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeStates", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeCommunicationGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeStates"], ["", "def", "computeTrajectory", "(", "self", ",", "initPos", ",", "initVel", ",", "duration", ",", "**", "kwargs", ")", ":", "\n", "\n", "# Check initPos is of shape batchSize x 2 x nAgents", "\n", "        ", "assert", "len", "(", "initPos", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "initPos", ".", "shape", "[", "0", "]", "\n", "assert", "initPos", ".", "shape", "[", "1", "]", "\n", "nAgents", "=", "initPos", ".", "shape", "[", "2", "]", "\n", "\n", "# Check initVel is of shape batchSize x 2 x nAgents", "\n", "assert", "len", "(", "initVel", ".", "shape", ")", "==", "3", "\n", "assert", "initVel", ".", "shape", "[", "0", "]", "==", "batchSize", "\n", "assert", "initVel", ".", "shape", "[", "1", "]", "==", "2", "\n", "assert", "initVel", ".", "shape", "[", "2", "]", "==", "nAgents", "\n", "\n", "# Check what kind of data it is", "\n", "#   This is because all the functions are numpy, but if this was", "\n", "#   torch, we need to return torch, to make it consistent", "\n", "if", "'torch'", "in", "repr", "(", "initPos", ".", "dtype", ")", ":", "\n", "            ", "assert", "'torch'", "in", "repr", "(", "initVel", ".", "dtype", ")", "\n", "useTorch", "=", "True", "\n", "device", "=", "initPos", ".", "device", "\n", "assert", "initVel", ".", "device", "==", "device", "\n", "", "else", ":", "\n", "            ", "useTorch", "=", "False", "\n", "\n", "# Create time line", "\n", "", "time", "=", "np", ".", "arange", "(", "0", ",", "duration", ",", "self", ".", "samplingTime", ")", "\n", "tSamples", "=", "len", "(", "time", ")", "\n", "\n", "# Here, we have two options, or we're given the acceleration or the", "\n", "# architecture", "\n", "assert", "'archit'", "in", "kwargs", ".", "keys", "(", ")", "or", "'accel'", "in", "kwargs", ".", "keys", "(", ")", "\n", "# Flags to determine which method to use", "\n", "useArchit", "=", "False", "\n", "useAccel", "=", "False", "\n", "\n", "if", "'archit'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "archit", "=", "kwargs", "[", "'archit'", "]", "# This is a torch.nn.Module architecture", "\n", "architDevice", "=", "list", "(", "archit", ".", "parameters", "(", ")", ")", "[", "0", "]", ".", "device", "\n", "useArchit", "=", "True", "\n", "", "elif", "'accel'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "accel", "=", "kwargs", "[", "'accel'", "]", "\n", "# accel has to be of shape batchSize x tSamples x 2 x nAgents", "\n", "assert", "len", "(", "accel", ".", "shape", ")", "==", "4", "\n", "assert", "accel", ".", "shape", "[", "0", "]", "==", "batchSize", "\n", "assert", "accel", ".", "shape", "[", "1", "]", "==", "tSamples", "\n", "assert", "accel", ".", "shape", "[", "2", "]", "==", "2", "\n", "assert", "accel", ".", "shape", "[", "3", "]", "==", "nAgents", "\n", "if", "useTorch", ":", "\n", "                ", "assert", "'torch'", "in", "repr", "(", "accel", ".", "dtype", ")", "\n", "", "useAccel", "=", "True", "\n", "\n", "# Decide on printing or not:", "\n", "", "if", "'doPrint'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "doPrint", "=", "kwargs", "[", "'doPrint'", "]", "\n", "", "else", ":", "\n", "            ", "doPrint", "=", "self", ".", "doPrint", "# Use default", "\n", "\n", "# Now create the outputs that will be filled afterwards", "\n", "", "pos", "=", "np", ".", "zeros", "(", "(", "batchSize", ",", "tSamples", ",", "2", ",", "nAgents", ")", ",", "dtype", "=", "np", ".", "float", ")", "\n", "vel", "=", "np", ".", "zeros", "(", "(", "batchSize", ",", "tSamples", ",", "2", ",", "nAgents", ")", ",", "dtype", "=", "np", ".", "float", ")", "\n", "if", "useArchit", ":", "\n", "            ", "accel", "=", "np", ".", "zeros", "(", "(", "batchSize", ",", "tSamples", ",", "2", ",", "nAgents", ")", ",", "dtype", "=", "np", ".", "float", ")", "\n", "state", "=", "np", ".", "zeros", "(", "(", "batchSize", ",", "tSamples", ",", "6", ",", "nAgents", ")", ",", "dtype", "=", "np", ".", "float", ")", "\n", "graph", "=", "np", ".", "zeros", "(", "(", "batchSize", ",", "tSamples", ",", "nAgents", ",", "nAgents", ")", ",", "\n", "dtype", "=", "np", ".", "float", ")", "\n", "\n", "# Assign the initial positions and velocities", "\n", "", "if", "useTorch", ":", "\n", "            ", "pos", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initPos", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "vel", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initVel", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "useAccel", ":", "\n", "                ", "accel", "=", "accel", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "pos", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initPos", ".", "copy", "(", ")", "\n", "vel", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initVel", ".", "copy", "(", ")", "\n", "\n", "", "if", "doPrint", ":", "\n", "# Sample percentage count", "\n", "            ", "percentageCount", "=", "int", "(", "100", "/", "tSamples", ")", "\n", "# Print new value", "\n", "print", "(", "\"%3d%%\"", "%", "percentageCount", ",", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "# Now, let's get started:", "\n", "", "for", "t", "in", "range", "(", "1", ",", "tSamples", ")", ":", "\n", "\n", "# If it is architecture-based, we need to compute the state, and", "\n", "# for that, we need to compute the graph", "\n", "            ", "if", "useArchit", ":", "\n", "# Adjust pos value for graph computation", "\n", "                ", "thisPos", "=", "np", ".", "expand_dims", "(", "pos", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", ",", "1", ")", "\n", "# Compute graph", "\n", "thisGraph", "=", "self", ".", "computeCommunicationGraph", "(", "thisPos", ",", "\n", "self", ".", "commRadius", ",", "\n", "True", ",", "\n", "doPrint", "=", "False", ")", "\n", "# Save graph", "\n", "graph", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "=", "thisGraph", ".", "squeeze", "(", "1", ")", "\n", "# Adjust vel value for state computation", "\n", "thisVel", "=", "np", ".", "expand_dims", "(", "vel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", ",", "1", ")", "\n", "# Compute state", "\n", "thisState", "=", "self", ".", "computeStates", "(", "thisPos", ",", "thisVel", ",", "thisGraph", ",", "\n", "doPrint", "=", "False", ")", "\n", "# Save state", "\n", "state", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "=", "thisState", ".", "squeeze", "(", "1", ")", "\n", "\n", "# Compute the output of the architecture", "\n", "#   Note that we need the collection of all time instants up", "\n", "#   to now, because when we do the communication exchanges,", "\n", "#   it involves past times.", "\n", "x", "=", "torch", ".", "tensor", "(", "state", "[", ":", ",", "0", ":", "t", ",", ":", ",", ":", "]", ",", "device", "=", "architDevice", ")", "\n", "S", "=", "torch", ".", "tensor", "(", "graph", "[", ":", ",", "0", ":", "t", ",", ":", ",", ":", "]", ",", "device", "=", "architDevice", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "thisAccel", "=", "archit", "(", "x", ",", "S", ")", "\n", "# Now that we have computed the acceleration, we only care ", "\n", "# about the last element in time", "\n", "", "thisAccel", "=", "thisAccel", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "-", "1", ",", ":", ",", ":", "]", "\n", "thisAccel", "[", "thisAccel", ">", "self", ".", "accelMax", "]", "=", "self", ".", "accelMax", "\n", "thisAccel", "[", "thisAccel", "<", "-", "self", ".", "accelMax", "]", "=", "self", ".", "accelMax", "\n", "# And save it", "\n", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "=", "thisAccel", "\n", "\n", "# Now that we have the acceleration, we can update position and", "\n", "# velocity", "\n", "", "vel", "[", ":", ",", "t", ",", ":", ",", ":", "]", "=", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "*", "self", ".", "samplingTime", "+", "vel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "\n", "pos", "[", ":", ",", "t", ",", ":", ",", ":", "]", "=", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "*", "(", "self", ".", "samplingTime", "**", "2", ")", "/", "2", "+", "vel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "*", "self", ".", "samplingTime", "+", "pos", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "\n", "\n", "if", "doPrint", ":", "\n", "# Sample percentage count", "\n", "                ", "percentageCount", "=", "int", "(", "100", "*", "(", "t", "+", "1", ")", "/", "tSamples", ")", "\n", "# Erase previous value and print new value", "\n", "print", "(", "'\\b \\b'", "*", "4", "+", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "# And we're missing the last values of graph, state and accel, so", "\n", "# let's compute them for completeness", "\n", "#   Graph", "\n", "", "", "thisPos", "=", "np", ".", "expand_dims", "(", "pos", "[", ":", ",", "-", "1", ",", ":", ",", ":", "]", ",", "1", ")", "\n", "thisGraph", "=", "self", ".", "computeCommunicationGraph", "(", "thisPos", ",", "self", ".", "commRadius", ",", "\n", "True", ",", "doPrint", "=", "False", ")", "\n", "graph", "[", ":", ",", "-", "1", ",", ":", ",", ":", "]", "=", "thisGraph", ".", "squeeze", "(", "1", ")", "\n", "#   State", "\n", "thisVel", "=", "np", ".", "expand_dims", "(", "vel", "[", ":", ",", "-", "1", ",", ":", ",", ":", "]", ",", "1", ")", "\n", "thisState", "=", "self", ".", "computeStates", "(", "thisPos", ",", "thisVel", ",", "thisGraph", ",", "\n", "doPrint", "=", "False", ")", "\n", "state", "[", ":", ",", "-", "1", ",", ":", ",", ":", "]", "=", "thisState", ".", "squeeze", "(", "1", ")", "\n", "#   Accel", "\n", "x", "=", "torch", ".", "tensor", "(", "state", ")", ".", "to", "(", "architDevice", ")", "\n", "S", "=", "torch", ".", "tensor", "(", "graph", ")", ".", "to", "(", "architDevice", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "thisAccel", "=", "archit", "(", "x", ",", "S", ")", "\n", "", "thisAccel", "=", "thisAccel", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "-", "1", ",", ":", ",", ":", "]", "\n", "thisAccel", "[", "thisAccel", ">", "self", ".", "accelMax", "]", "=", "self", ".", "accelMax", "\n", "thisAccel", "[", "thisAccel", "<", "-", "self", ".", "accelMax", "]", "=", "self", ".", "accelMax", "\n", "# And save it", "\n", "accel", "[", ":", ",", "-", "1", ",", ":", ",", ":", "]", "=", "thisAccel", "\n", "\n", "# Print", "\n", "if", "doPrint", ":", "\n", "# Erase the percentage", "\n", "            ", "print", "(", "'\\b \\b'", "*", "4", ",", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "# After we have finished, turn it back into tensor, if required", "\n", "", "if", "useTorch", ":", "\n", "            ", "pos", "=", "torch", ".", "tensor", "(", "pos", ")", ".", "to", "(", "device", ")", "\n", "vel", "=", "torch", ".", "tensor", "(", "vel", ")", ".", "to", "(", "device", ")", "\n", "accel", "=", "torch", ".", "tensor", "(", "accel", ")", ".", "to", "(", "device", ")", "\n", "\n", "# And return it", "\n", "", "if", "useArchit", ":", "\n", "            ", "return", "pos", ",", "vel", ",", "accel", ",", "state", ",", "graph", "\n", "", "elif", "useAccel", ":", "\n", "            ", "return", "pos", ",", "vel", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences": [[3341, 3405], ["u[].reshape", "u[].reshape", "u[].reshape", "u[].reshape", "numpy.expand_dims", "numpy.expand_dims", "numpy.concatenate", "len", "numpy.expand_dims", "uDistSq.squeeze.squeeze.squeeze", "uDiff.squeeze.squeeze.squeeze", "len", "len"], "methods", ["None"], ["", "", "def", "computeDifferences", "(", "self", ",", "u", ")", ":", "\n", "\n", "# Takes as input a tensor of shape", "\n", "#   nSamples x tSamples x 2 x nAgents", "\n", "# or of shape", "\n", "#   nSamples x 2 x nAgents", "\n", "# And returns the elementwise difference u_i - u_j of shape", "\n", "#   nSamples (x tSamples) x 2 x nAgents x nAgents", "\n", "# And the distance squared ||u_i - u_j||^2 of shape", "\n", "#   nSamples (x tSamples) x nAgents x nAgents", "\n", "\n", "# Check dimensions", "\n", "        ", "assert", "len", "(", "u", ".", "shape", ")", "==", "3", "or", "len", "(", "u", ".", "shape", ")", "==", "4", "\n", "# If it has shape 3, which means it's only a single time instant, then", "\n", "# add the extra dimension so we move along assuming we have multiple", "\n", "# time instants", "\n", "if", "len", "(", "u", ".", "shape", ")", "==", "3", ":", "\n", "            ", "u", "=", "np", ".", "expand_dims", "(", "u", ",", "1", ")", "\n", "hasTimeDim", "=", "False", "\n", "", "else", ":", "\n", "            ", "hasTimeDim", "=", "True", "\n", "\n", "# Now we have that pos always has shape", "\n", "#   nSamples x tSamples x 2 x nAgents", "\n", "", "nSamples", "=", "u", ".", "shape", "[", "0", "]", "\n", "tSamples", "=", "u", ".", "shape", "[", "1", "]", "\n", "assert", "u", ".", "shape", "[", "2", "]", "==", "2", "\n", "nAgents", "=", "u", ".", "shape", "[", "3", "]", "\n", "\n", "# Compute the difference along each axis. For this, we subtract a", "\n", "# column vector from a row vector. The difference tensor on each", "\n", "# position will have shape nSamples x tSamples x nAgents x nAgents", "\n", "# and then we add the extra dimension to concatenate and obtain a final", "\n", "# tensor of shape nSamples x tSamples x 2 x nAgents x nAgents", "\n", "# First, axis x", "\n", "#   Reshape as column and row vector, respectively", "\n", "uCol_x", "=", "u", "[", ":", ",", ":", ",", "0", ",", ":", "]", ".", "reshape", "(", "(", "nSamples", ",", "tSamples", ",", "nAgents", ",", "1", ")", ")", "\n", "uRow_x", "=", "u", "[", ":", ",", ":", ",", "0", ",", ":", "]", ".", "reshape", "(", "(", "nSamples", ",", "tSamples", ",", "1", ",", "nAgents", ")", ")", "\n", "#   Subtract them", "\n", "uDiff_x", "=", "uCol_x", "-", "uRow_x", "# nSamples x tSamples x nAgents x nAgents", "\n", "# Second, for axis y", "\n", "uCol_y", "=", "u", "[", ":", ",", ":", ",", "1", ",", ":", "]", ".", "reshape", "(", "(", "nSamples", ",", "tSamples", ",", "nAgents", ",", "1", ")", ")", "\n", "uRow_y", "=", "u", "[", ":", ",", ":", ",", "1", ",", ":", "]", ".", "reshape", "(", "(", "nSamples", ",", "tSamples", ",", "1", ",", "nAgents", ")", ")", "\n", "uDiff_y", "=", "uCol_y", "-", "uRow_y", "# nSamples x tSamples x nAgents x nAgents", "\n", "# Third, compute the distance tensor of shape", "\n", "#   nSamples x tSamples x nAgents x nAgents", "\n", "uDistSq", "=", "uDiff_x", "**", "2", "+", "uDiff_y", "**", "2", "\n", "# Finally, concatenate to obtain the tensor of differences", "\n", "#   Add the extra dimension in the position", "\n", "uDiff_x", "=", "np", ".", "expand_dims", "(", "uDiff_x", ",", "2", ")", "\n", "uDiff_y", "=", "np", ".", "expand_dims", "(", "uDiff_y", ",", "2", ")", "\n", "#   And concatenate them", "\n", "uDiff", "=", "np", ".", "concatenate", "(", "(", "uDiff_x", ",", "uDiff_y", ")", ",", "2", ")", "\n", "#   nSamples x tSamples x 2 x nAgents x nAgents", "\n", "\n", "# Get rid of the time dimension if we don't need it", "\n", "if", "not", "hasTimeDim", ":", "\n", "# (This fails if tSamples > 1)", "\n", "            ", "uDistSq", "=", "uDistSq", ".", "squeeze", "(", "1", ")", "\n", "#   nSamples x nAgents x nAgents", "\n", "uDiff", "=", "uDiff", ".", "squeeze", "(", "1", ")", "\n", "#   nSamples x 2 x nAgents x nAgents", "\n", "\n", "", "return", "uDiff", ",", "uDistSq", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeOptimalTrajectory": [[3406, 3507], ["numpy.arange", "len", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "len", "len", "int", "print", "dataTools.Flocking.computeDifferences", "dataTools.Flocking.computeDifferences", "dataTools.invertTensorEW", "numpy.expand_dims", "accel[].copy", "print", "numpy.expand_dims", "int", "print", "numpy.sum", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW"], ["", "def", "computeOptimalTrajectory", "(", "self", ",", "initPos", ",", "initVel", ",", "duration", ",", "\n", "samplingTime", ",", "repelDist", ",", "\n", "accelMax", "=", "100.", ")", ":", "\n", "\n", "# The optimal trajectory is given by", "\n", "# u_{i} = - \\sum_{j=1}^{N} (v_{i} - v_{j})", "\n", "#         + 2 \\sum_{j=1}^{N} (r_{i} - r_{j}) *", "\n", "#                                 (1/\\|r_{i}\\|^{4} + 1/\\|r_{j}\\|^{2}) *", "\n", "#                                 1{\\|r_{ij}\\| < R}", "\n", "# for each agent i=1,...,N, where v_{i} is the velocity and r_{i} the", "\n", "# position.", "\n", "\n", "# Check that initPos and initVel as nSamples x 2 x nAgents arrays", "\n", "        ", "assert", "len", "(", "initPos", ".", "shape", ")", "==", "len", "(", "initVel", ".", "shape", ")", "==", "3", "\n", "nSamples", "=", "initPos", ".", "shape", "[", "0", "]", "\n", "assert", "initPos", ".", "shape", "[", "1", "]", "==", "initVel", ".", "shape", "[", "1", "]", "==", "2", "\n", "nAgents", "=", "initPos", ".", "shape", "[", "2", "]", "\n", "assert", "initVel", ".", "shape", "[", "0", "]", "==", "nSamples", "\n", "assert", "initVel", ".", "shape", "[", "2", "]", "==", "nAgents", "\n", "\n", "# time", "\n", "time", "=", "np", ".", "arange", "(", "0", ",", "duration", ",", "samplingTime", ")", "\n", "tSamples", "=", "len", "(", "time", ")", "# number of time samples", "\n", "\n", "# Create arrays to store the trajectory", "\n", "pos", "=", "np", ".", "zeros", "(", "(", "nSamples", ",", "tSamples", ",", "2", ",", "nAgents", ")", ")", "\n", "vel", "=", "np", ".", "zeros", "(", "(", "nSamples", ",", "tSamples", ",", "2", ",", "nAgents", ")", ")", "\n", "accel", "=", "np", ".", "zeros", "(", "(", "nSamples", ",", "tSamples", ",", "2", ",", "nAgents", ")", ")", "\n", "\n", "# Initial settings", "\n", "pos", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initPos", "\n", "vel", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initVel", "\n", "\n", "if", "self", ".", "doPrint", ":", "\n", "# Sample percentage count", "\n", "            ", "percentageCount", "=", "int", "(", "100", "/", "tSamples", ")", "\n", "# Print new value", "\n", "print", "(", "\"%3d%%\"", "%", "percentageCount", ",", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "# For each time instant", "\n", "", "for", "t", "in", "range", "(", "1", ",", "tSamples", ")", ":", "\n", "\n", "# Compute the optimal acceleration", "\n", "#   Compute the distance between all elements (positions)", "\n", "            ", "ijDiffPos", ",", "ijDistSq", "=", "self", ".", "computeDifferences", "(", "pos", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", ")", "\n", "#       ijDiffPos: nSamples x 2 x nAgents x nAgents", "\n", "#       ijDistSq:  nSamples x nAgents x nAgents", "\n", "#   And also the difference in velocities", "\n", "ijDiffVel", ",", "_", "=", "self", ".", "computeDifferences", "(", "vel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", ")", "\n", "#       ijDiffVel: nSamples x 2 x nAgents x nAgents", "\n", "#   The last element we need to compute the acceleration is the", "\n", "#   gradient. Note that the gradient only counts when the distance ", "\n", "#   is smaller than the repel distance", "\n", "#       This is the mask to consider each of the differences", "\n", "repelMask", "=", "(", "ijDistSq", "<", "(", "repelDist", "**", "2", ")", ")", ".", "astype", "(", "ijDiffPos", ".", "dtype", ")", "\n", "#       Apply the mask to the relevant differences", "\n", "ijDiffPos", "=", "ijDiffPos", "*", "np", ".", "expand_dims", "(", "repelMask", ",", "1", ")", "\n", "#       Compute the constant (1/||r_ij||^4 + 1/||r_ij||^2)", "\n", "ijDistSqInv", "=", "invertTensorEW", "(", "ijDistSq", ")", "\n", "#       Add the extra dimension", "\n", "ijDistSqInv", "=", "np", ".", "expand_dims", "(", "ijDistSqInv", ",", "1", ")", "\n", "#   Compute the acceleration", "\n", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "=", "-", "np", ".", "sum", "(", "ijDiffVel", ",", "axis", "=", "3", ")", "+", "2", "*", "np", ".", "sum", "(", "ijDiffPos", "*", "(", "ijDistSqInv", "**", "2", "+", "ijDistSqInv", ")", ",", "\n", "axis", "=", "3", ")", "\n", "\n", "# Finally, note that if the agents are too close together, the", "\n", "# acceleration will be very big to get them as far apart as", "\n", "# possible, and this is physically impossible.", "\n", "# So let's add a limitation to the maximum aceleration", "\n", "\n", "# Find the places where the acceleration is big", "\n", "thisAccel", "=", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", ".", "copy", "(", ")", "\n", "# Values that exceed accelMax, force them to be accelMax", "\n", "thisAccel", "[", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", ">", "accelMax", "]", "=", "accelMax", "\n", "# Values that are smaller than -accelMax, force them to be accelMax", "\n", "thisAccel", "[", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "<", "-", "accelMax", "]", "=", "-", "accelMax", "\n", "# And put it back", "\n", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "=", "thisAccel", "\n", "\n", "# Update the values", "\n", "#   Update velocity", "\n", "vel", "[", ":", ",", "t", ",", ":", ",", ":", "]", "=", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "*", "samplingTime", "+", "vel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "\n", "#   Update the position", "\n", "pos", "[", ":", ",", "t", ",", ":", ",", ":", "]", "=", "accel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "*", "(", "samplingTime", "**", "2", ")", "/", "2", "+", "vel", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "*", "samplingTime", "+", "pos", "[", ":", ",", "t", "-", "1", ",", ":", ",", ":", "]", "\n", "\n", "if", "self", ".", "doPrint", ":", "\n", "# Sample percentage count", "\n", "                ", "percentageCount", "=", "int", "(", "100", "*", "(", "t", "+", "1", ")", "/", "tSamples", ")", "\n", "# Erase previous pecentage and print new value", "\n", "print", "(", "'\\b \\b'", "*", "4", "+", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "# Print", "\n", "", "", "if", "self", ".", "doPrint", ":", "\n", "# Erase the percentage", "\n", "            ", "print", "(", "'\\b \\b'", "*", "4", ",", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "", "return", "pos", ",", "vel", ",", "accel", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeInitialPositions": [[3508, 3699], ["dataTools.Flocking.computeDifferences", "distSq.squeeze.squeeze.squeeze", "numpy.min", "dataTools.Flocking.computeCommunicationGraph", "graphMatrix.squeeze.squeeze.squeeze", "range", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.concatenate().reshape", "int", "numpy.arange", "numpy.tile", "numpy.repeat", "numpy.concatenate", "numpy.repeat", "numpy.random.uniform", "numpy.expand_dims", "numpy.expand_dims", "alegnn.isConnected", "kwargs.keys", "kwargs.keys", "numpy.concatenate", "numpy.ceil", "numpy.expand_dims", "range", "numpy.empty", "numpy.empty", "numpy.repeat", "numpy.repeat", "range", "numpy.random.uniform", "numpy.zeros", "numpy.concatenate", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.expand_dims", "numpy.expand_dims", "numpy.arange", "len", "numpy.arange", "len", "numpy.concatenate", "numpy.concatenate", "numpy.expand_dims", "numpy.expand_dims", "numpy.random.uniform", "numpy.cos", "numpy.sin", "numpy.eye().reshape", "numpy.abs", "len", "numpy.repeat", "numpy.eye", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeCommunicationGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected"], ["", "def", "computeInitialPositions", "(", "self", ",", "nAgents", ",", "nSamples", ",", "commRadius", ",", "\n", "minDist", "=", "0.1", ",", "geometry", "=", "'rectangular'", ",", "\n", "**", "kwargs", ")", ":", "\n", "\n", "# It will always be uniform. We can select whether it is rectangular", "\n", "# or circular (or some other shape) and the parameters respecting", "\n", "# that", "\n", "        ", "assert", "geometry", "==", "'rectangular'", "or", "geometry", "==", "'circular'", "\n", "assert", "minDist", "*", "(", "1.", "+", "zeroTolerance", ")", "<=", "commRadius", "*", "(", "1.", "-", "zeroTolerance", ")", "\n", "# We use a zeroTolerance buffer zone, just in case", "\n", "minDist", "=", "minDist", "*", "(", "1.", "+", "zeroTolerance", ")", "\n", "commRadius", "=", "commRadius", "*", "(", "1.", "-", "zeroTolerance", ")", "\n", "\n", "# If there are other keys in the kwargs argument, they will just be", "\n", "# ignored", "\n", "\n", "# We will first create the grid, whether it is rectangular or", "\n", "# circular.", "\n", "\n", "# Let's start by setting the fixed position", "\n", "if", "geometry", "==", "'rectangular'", ":", "\n", "\n", "# This grid has a distance that depends on the desired minDist and", "\n", "# the commRadius", "\n", "            ", "distFixed", "=", "(", "commRadius", "+", "minDist", ")", "/", "(", "2.", "*", "np", ".", "sqrt", "(", "2", ")", ")", "\n", "#   This is the fixed distance between points in the grid", "\n", "distPerturb", "=", "(", "commRadius", "-", "minDist", ")", "/", "(", "4.", "*", "np", ".", "sqrt", "(", "2", ")", ")", "\n", "#   This is the standard deviation of a uniform perturbation around", "\n", "#   the fixed point.", "\n", "# This should guarantee that, even after the perturbations, there", "\n", "# are no agents below minDist, and that all agents have at least", "\n", "# one other agent within commRadius.", "\n", "\n", "# How many agents per axis", "\n", "nAgentsPerAxis", "=", "int", "(", "np", ".", "ceil", "(", "np", ".", "sqrt", "(", "nAgents", ")", ")", ")", "\n", "\n", "axisFixedPos", "=", "np", ".", "arange", "(", "-", "(", "nAgentsPerAxis", "*", "distFixed", ")", "/", "2", ",", "\n", "(", "nAgentsPerAxis", "*", "distFixed", ")", "/", "2", ",", "\n", "step", "=", "distFixed", ")", "\n", "\n", "# Repeat the positions in the same order (x coordinate)", "\n", "xFixedPos", "=", "np", ".", "tile", "(", "axisFixedPos", ",", "nAgentsPerAxis", ")", "\n", "# Repeat each element (y coordinate)", "\n", "yFixedPos", "=", "np", ".", "repeat", "(", "axisFixedPos", ",", "nAgentsPerAxis", ")", "\n", "\n", "# Concatenate this to obtain the positions", "\n", "fixedPos", "=", "np", ".", "concatenate", "(", "(", "np", ".", "expand_dims", "(", "xFixedPos", ",", "0", ")", ",", "\n", "np", ".", "expand_dims", "(", "yFixedPos", ",", "0", ")", ")", ",", "\n", "axis", "=", "0", ")", "\n", "\n", "# Get rid of unnecessary agents", "\n", "fixedPos", "=", "fixedPos", "[", ":", ",", "0", ":", "nAgents", "]", "\n", "# And repeat for the number of samples we want to generate", "\n", "fixedPos", "=", "np", ".", "repeat", "(", "np", ".", "expand_dims", "(", "fixedPos", ",", "0", ")", ",", "nSamples", ",", "\n", "axis", "=", "0", ")", "\n", "#   nSamples x 2 x nAgents", "\n", "\n", "# Now generate the noise", "\n", "perturbPos", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "distPerturb", ",", "\n", "high", "=", "distPerturb", ",", "\n", "size", "=", "(", "nSamples", ",", "2", ",", "nAgents", ")", ")", "\n", "\n", "# Initial positions", "\n", "initPos", "=", "fixedPos", "+", "perturbPos", "\n", "\n", "", "elif", "geometry", "==", "'circular'", ":", "\n", "\n", "# Radius for the grid", "\n", "            ", "rFixed", "=", "(", "commRadius", "+", "minDist", ")", "/", "2.", "\n", "rPerturb", "=", "(", "commRadius", "-", "minDist", ")", "/", "4.", "\n", "fixedRadius", "=", "np", ".", "arange", "(", "0", ",", "rFixed", "*", "nAgents", ",", "step", "=", "rFixed", ")", "+", "rFixed", "\n", "\n", "# Angles for the grid", "\n", "aFixed", "=", "(", "commRadius", "/", "fixedRadius", "+", "minDist", "/", "fixedRadius", ")", "/", "2.", "\n", "for", "a", "in", "range", "(", "len", "(", "aFixed", ")", ")", ":", "\n", "# How many times does aFixed[a] fits within 2pi?", "\n", "                ", "nAgentsPerCircle", "=", "2", "*", "np", ".", "pi", "//", "aFixed", "[", "a", "]", "\n", "# And now divide 2*np.pi by this number", "\n", "aFixed", "[", "a", "]", "=", "2", "*", "np", ".", "pi", "/", "nAgentsPerCircle", "\n", "#   Fixed angle difference for each value of fixedRadius", "\n", "\n", "# Now, let's get the radius, angle coordinates for each agents", "\n", "", "initRadius", "=", "np", ".", "empty", "(", "(", "0", ")", ")", "\n", "initAngles", "=", "np", ".", "empty", "(", "(", "0", ")", ")", "\n", "agentsSoFar", "=", "0", "# Number of agents located so far", "\n", "n", "=", "0", "# Index for radius", "\n", "while", "agentsSoFar", "<", "nAgents", ":", "\n", "                ", "thisRadius", "=", "fixedRadius", "[", "n", "]", "\n", "thisAngles", "=", "np", ".", "arange", "(", "0", ",", "2", "*", "np", ".", "pi", ",", "step", "=", "aFixed", "[", "n", "]", ")", "\n", "agentsSoFar", "+=", "len", "(", "thisAngles", ")", "\n", "initRadius", "=", "np", ".", "concatenate", "(", "(", "initRadius", ",", "\n", "np", ".", "repeat", "(", "thisRadius", ",", "\n", "len", "(", "thisAngles", ")", ")", ")", ")", "\n", "initAngles", "=", "np", ".", "concatenate", "(", "(", "initAngles", ",", "thisAngles", ")", ")", "\n", "n", "+=", "1", "\n", "assert", "len", "(", "initRadius", ")", "==", "agentsSoFar", "\n", "\n", "# Restrict to the number of agents we need", "\n", "", "initRadius", "=", "initRadius", "[", "0", ":", "nAgents", "]", "\n", "initAngles", "=", "initAngles", "[", "0", ":", "nAgents", "]", "\n", "\n", "# Add the number of samples", "\n", "initRadius", "=", "np", ".", "repeat", "(", "np", ".", "expand_dims", "(", "initRadius", ",", "0", ")", ",", "nSamples", ",", "\n", "axis", "=", "0", ")", "\n", "initAngles", "=", "np", ".", "repeat", "(", "np", ".", "expand_dims", "(", "initAngles", ",", "0", ")", ",", "nSamples", ",", "\n", "axis", "=", "0", ")", "\n", "\n", "# Add the noise", "\n", "#   First, to the angles", "\n", "for", "n", "in", "range", "(", "nAgents", ")", ":", "\n", "# Get the radius (the angle noise depends on the radius); so", "\n", "# far the radius is the same for all samples", "\n", "                ", "thisRadius", "=", "initRadius", "[", "0", ",", "n", "]", "\n", "aPerturb", "=", "(", "commRadius", "/", "thisRadius", "-", "minDist", "/", "thisRadius", ")", "/", "4.", "\n", "# Add the noise to the angles", "\n", "initAngles", "[", ":", ",", "n", "]", "+=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "aPerturb", ",", "\n", "high", "=", "aPerturb", ",", "\n", "size", "=", "(", "nSamples", ")", ")", "\n", "#   Then, to the radius", "\n", "", "initRadius", "+=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "rPerturb", ",", "\n", "high", "=", "rPerturb", ",", "\n", "size", "=", "(", "nSamples", ",", "nAgents", ")", ")", "\n", "\n", "# And finally, get the positions in the cartesian coordinates", "\n", "initPos", "=", "np", ".", "zeros", "(", "(", "nSamples", ",", "2", ",", "nAgents", ")", ")", "\n", "initPos", "[", ":", ",", "0", ",", ":", "]", "=", "initRadius", "*", "np", ".", "cos", "(", "initAngles", ")", "\n", "initPos", "[", ":", ",", "1", ",", ":", "]", "=", "initRadius", "*", "np", ".", "sin", "(", "initAngles", ")", "\n", "\n", "# Now, check that the conditions are met:", "\n", "#   Compute square distances", "\n", "", "_", ",", "distSq", "=", "self", ".", "computeDifferences", "(", "np", ".", "expand_dims", "(", "initPos", ",", "1", ")", ")", "\n", "#   Get rid of the \"time\" dimension that arises from using the ", "\n", "#   method to compute distances", "\n", "distSq", "=", "distSq", ".", "squeeze", "(", "1", ")", "\n", "#   Compute the minimum distance (don't forget to add something in", "\n", "#   the diagonal, which otherwise is zero)", "\n", "minDistSq", "=", "np", ".", "min", "(", "distSq", "+", "2", "*", "commRadius", "*", "np", ".", "eye", "(", "distSq", ".", "shape", "[", "1", "]", ")", ".", "reshape", "(", "1", ",", "\n", "distSq", ".", "shape", "[", "1", "]", ",", "\n", "distSq", ".", "shape", "[", "2", "]", ")", "\n", ")", "\n", "\n", "assert", "minDistSq", ">=", "minDist", "**", "2", "\n", "\n", "#   Now the number of neighbors", "\n", "graphMatrix", "=", "self", ".", "computeCommunicationGraph", "(", "np", ".", "expand_dims", "(", "initPos", ",", "1", ")", ",", "\n", "self", ".", "commRadius", ",", "\n", "False", ",", "\n", "doPrint", "=", "False", ")", "\n", "graphMatrix", "=", "graphMatrix", ".", "squeeze", "(", "1", ")", "# nSamples x nAgents x nAgents  ", "\n", "\n", "#   Binarize the matrix", "\n", "graphMatrix", "=", "(", "np", ".", "abs", "(", "graphMatrix", ")", ">", "zeroTolerance", ")", ".", "astype", "(", "initPos", ".", "dtype", ")", "\n", "\n", "#   And check that we always have initially connected graphs", "\n", "for", "n", "in", "range", "(", "nSamples", ")", ":", "\n", "            ", "assert", "graph", ".", "isConnected", "(", "graphMatrix", "[", "n", ",", ":", ",", ":", "]", ")", "\n", "\n", "# We move to compute the initial velocities. Velocities can be", "\n", "# either positive or negative, so we do not need to determine", "\n", "# the lower and higher, just around zero", "\n", "", "if", "'xMaxInitVel'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "xMaxInitVel", "=", "kwargs", "[", "'xMaxInitVel'", "]", "\n", "", "else", ":", "\n", "            ", "xMaxInitVel", "=", "3.", "\n", "#   Takes five seconds to traverse half the map", "\n", "# Same for the other axis", "\n", "", "if", "'yMaxInitVel'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "yMaxInitVel", "=", "kwargs", "[", "'yMaxInitVel'", "]", "\n", "", "else", ":", "\n", "            ", "yMaxInitVel", "=", "3.", "\n", "\n", "# And sample the velocities", "\n", "", "xInitVel", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "xMaxInitVel", ",", "high", "=", "xMaxInitVel", ",", "\n", "size", "=", "(", "nSamples", ",", "1", ",", "nAgents", ")", ")", "\n", "yInitVel", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "yMaxInitVel", ",", "high", "=", "yMaxInitVel", ",", "\n", "size", "=", "(", "nSamples", ",", "1", ",", "nAgents", ")", ")", "\n", "# Add bias", "\n", "xVelBias", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "xMaxInitVel", ",", "high", "=", "xMaxInitVel", ",", "\n", "size", "=", "(", "nSamples", ")", ")", "\n", "yVelBias", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "yMaxInitVel", ",", "high", "=", "yMaxInitVel", ",", "\n", "size", "=", "(", "nSamples", ")", ")", "\n", "\n", "# And concatenate them", "\n", "velBias", "=", "np", ".", "concatenate", "(", "(", "xVelBias", ",", "yVelBias", ")", ")", ".", "reshape", "(", "(", "nSamples", ",", "2", ",", "1", ")", ")", "\n", "initVel", "=", "np", ".", "concatenate", "(", "(", "xInitVel", ",", "yInitVel", ")", ",", "axis", "=", "1", ")", "+", "velBias", "\n", "#   nSamples x 2 x nAgents", "\n", "\n", "return", "initPos", ",", "initVel", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.saveVideo": [[3701, 4005], ["dict", "matplotlib.animation.FFMpegWriter", "matplotlib.animation.FFMpegWriter", "range", "len", "repr", "numpy.expand_dims.cpu().numpy", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "len", "len", "os.path.exists", "os.mkdir", "numpy.min", "numpy.max", "numpy.min", "numpy.max", "print", "matplotlib.figure", "matplotlib.figure", "matplotlib.xlim", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylim", "matplotlib.axis", "matplotlib.axis", "matplotlib.plot", "matplotlib.plot", "matplotlib.close", "matplotlib.close", "print", "numpy.arange", "numpy.linspace", "numpy.unique().astype", "os.path.join", "range", "len", "repr", "numpy.expand_dims.cpu().numpy", "kwargs.keys", "kwargs.keys", "type", "numpy.random.choice", "len", "numpy.expand_dims", "matplotlib.text", "matplotlib.text", "matplotlib.animation.FFMpegWriter.saving", "matplotlib.animation.FFMpegWriter.saving", "range", "numpy.sum", "numpy.max", "maxVelNormSq.reshape.reshape.reshape", "print", "len", "print", "numpy.expand_dims.cpu", "len", "numpy.expand_dims", "os.path.join", "plotAgents.set_data", "matplotlib.animation.FFMpegWriter.grab_frame", "matplotlib.animation.FFMpegWriter.grab_frame", "numpy.max", "numpy.sqrt", "numpy.unique", "os.path.exists", "os.mkdir", "numpy.allclose", "numpy.nonzero", "matplotlib.figure", "matplotlib.figure", "matplotlib.xlim", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylim", "matplotlib.axis", "matplotlib.axis", "matplotlib.plot", "matplotlib.plot", "matplotlib.plot", "matplotlib.plot", "matplotlib.title", "matplotlib.title", "matplotlib.figure.savefig", "matplotlib.close", "matplotlib.close", "numpy.expand_dims.cpu", "int", "numpy.unique().astype.astype", "numpy.triu", "range", "dataTools.Flocking.evaluate", "matplotlib.text", "matplotlib.text", "os.path.join", "int", "print", "print", "numpy.abs", "matplotlib.arrow", "matplotlib.arrow", "print", "print"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.close", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.close", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.close", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.close", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate"], ["", "def", "saveVideo", "(", "self", ",", "saveDir", ",", "pos", ",", "*", "args", ",", "\n", "commGraph", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "\n", "# Check that pos is a position of shape nSamples x tSamples x 2 x nAgents", "\n", "        ", "assert", "len", "(", "pos", ".", "shape", ")", "==", "4", "\n", "nSamples", "=", "pos", ".", "shape", "[", "0", "]", "\n", "tSamples", "=", "pos", ".", "shape", "[", "1", "]", "\n", "assert", "pos", ".", "shape", "[", "2", "]", "==", "2", "\n", "nAgents", "=", "pos", ".", "shape", "[", "3", "]", "\n", "if", "'torch'", "in", "repr", "(", "pos", ".", "dtype", ")", ":", "\n", "            ", "pos", "=", "pos", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Check if there's the need to plot a graph", "\n", "", "if", "commGraph", "is", "not", "None", ":", "\n", "# If there's a communication graph, then it has to have shape", "\n", "#   nSamples x tSamples x nAgents x nAgents", "\n", "            ", "assert", "len", "(", "commGraph", ".", "shape", ")", "==", "4", "\n", "assert", "commGraph", ".", "shape", "[", "0", "]", "==", "nSamples", "\n", "assert", "commGraph", ".", "shape", "[", "1", "]", "==", "tSamples", "\n", "assert", "commGraph", ".", "shape", "[", "2", "]", "==", "commGraph", ".", "shape", "[", "3", "]", "==", "nAgents", "\n", "if", "'torch'", "in", "repr", "(", "commGraph", ".", "dtype", ")", ":", "\n", "                ", "commGraph", "=", "commGraph", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "showGraph", "=", "True", "\n", "", "else", ":", "\n", "            ", "showGraph", "=", "False", "\n", "\n", "", "if", "'doPrint'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "doPrint", "=", "kwargs", "[", "'doPrint'", "]", "\n", "", "else", ":", "\n", "            ", "doPrint", "=", "self", ".", "doPrint", "\n", "\n", "# This number determines how faster or slower to reproduce the video", "\n", "", "if", "'videoSpeed'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "videoSpeed", "=", "kwargs", "[", "'videoSpeed'", "]", "\n", "", "else", ":", "\n", "            ", "videoSpeed", "=", "1.", "\n", "\n", "", "if", "'showVideoSpeed'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "showVideoSpeed", "=", "kwargs", "[", "'showVideoSpeed'", "]", "\n", "", "else", ":", "\n", "            ", "if", "videoSpeed", "!=", "1", ":", "\n", "                ", "showVideoSpeed", "=", "True", "\n", "", "else", ":", "\n", "                ", "showVideoSpeed", "=", "False", "\n", "\n", "", "", "if", "'vel'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "vel", "=", "kwargs", "[", "'vel'", "]", "\n", "if", "'showCost'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "                ", "showCost", "=", "kwargs", "[", "'showCost'", "]", "\n", "", "else", ":", "\n", "                ", "showCost", "=", "True", "\n", "", "if", "'showArrows'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "                ", "showArrows", "=", "kwargs", "[", "'showArrows'", "]", "\n", "", "else", ":", "\n", "                ", "showArrows", "=", "True", "\n", "", "", "else", ":", "\n", "            ", "showCost", "=", "False", "\n", "showArrows", "=", "False", "\n", "\n", "# Check that the number of extra arguments fits", "\n", "", "assert", "len", "(", "args", ")", "<=", "1", "\n", "# If there's an argument, we have to check whether it is an int or a", "\n", "# list", "\n", "if", "len", "(", "args", ")", "==", "1", ":", "\n", "# If it is an int, just return that number of randomly chosen", "\n", "# samples.", "\n", "            ", "if", "type", "(", "args", "[", "0", "]", ")", "==", "int", ":", "\n", "# We can't return more samples than there are available", "\n", "                ", "assert", "args", "[", "0", "]", "<=", "nSamples", "\n", "# Randomly choose args[0] indices", "\n", "selectedIndices", "=", "np", ".", "random", ".", "choice", "(", "nSamples", ",", "size", "=", "args", "[", "0", "]", ",", "\n", "replace", "=", "False", ")", "\n", "", "else", ":", "\n", "# The fact that we put else here instead of elif type()==list", "\n", "# allows for np.array to be used as indices as well. In general,", "\n", "# any variable with the ability to index.", "\n", "                ", "selectedIndices", "=", "args", "[", "0", "]", "\n", "\n", "# Select the corresponding samples", "\n", "", "pos", "=", "pos", "[", "selectedIndices", "]", "\n", "\n", "# Finally, observe that if pos has shape only 3, then that's ", "\n", "# because we selected a single sample, so we need to add the extra", "\n", "# dimension back again", "\n", "if", "len", "(", "pos", ".", "shape", ")", "<", "4", ":", "\n", "                ", "pos", "=", "np", ".", "expand_dims", "(", "pos", ",", "0", ")", "\n", "\n", "", "if", "showGraph", ":", "\n", "                ", "commGraph", "=", "commGraph", "[", "selectedIndices", "]", "\n", "if", "len", "(", "commGraph", ".", "shape", ")", "<", "4", ":", "\n", "                    ", "commGraph", "=", "np", ".", "expand_dims", "(", "commGraph", ",", "0", ")", "\n", "\n", "# Where to save the video", "\n", "", "", "", "if", "not", "os", ".", "path", ".", "exists", "(", "saveDir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "saveDir", ")", "\n", "\n", "", "videoName", "=", "'sampleTrajectory'", "\n", "\n", "xMinMap", "=", "np", ".", "min", "(", "pos", "[", ":", ",", ":", ",", "0", ",", ":", "]", ")", "*", "1.2", "\n", "xMaxMap", "=", "np", ".", "max", "(", "pos", "[", ":", ",", ":", ",", "0", ",", ":", "]", ")", "*", "1.2", "\n", "yMinMap", "=", "np", ".", "min", "(", "pos", "[", ":", ",", ":", ",", "1", ",", ":", "]", ")", "*", "1.2", "\n", "yMaxMap", "=", "np", ".", "max", "(", "pos", "[", ":", ",", ":", ",", "1", ",", ":", "]", ")", "*", "1.2", "\n", "\n", "# Create video object", "\n", "\n", "videoMetadata", "=", "dict", "(", "title", "=", "'Sample Trajectory'", ",", "artist", "=", "'Flocking'", ",", "\n", "comment", "=", "'Flocking example'", ")", "\n", "videoWriter", "=", "FFMpegWriter", "(", "fps", "=", "videoSpeed", "/", "self", ".", "samplingTime", ",", "\n", "metadata", "=", "videoMetadata", ")", "\n", "\n", "if", "doPrint", ":", "\n", "            ", "print", "(", "\"\\tSaving video(s)...\"", ",", "end", "=", "' '", ",", "flush", "=", "True", ")", "\n", "\n", "# For each sample now", "\n", "", "for", "n", "in", "range", "(", "pos", ".", "shape", "[", "0", "]", ")", ":", "\n", "\n", "# If there's more than one video to create, enumerate them", "\n", "            ", "if", "pos", ".", "shape", "[", "0", "]", ">", "1", ":", "\n", "                ", "thisVideoName", "=", "videoName", "+", "'%03d.mp4'", "%", "n", "\n", "", "else", ":", "\n", "                ", "thisVideoName", "=", "videoName", "+", "'.mp4'", "\n", "\n", "# Select the corresponding position trajectory", "\n", "", "thisPos", "=", "pos", "[", "n", "]", "\n", "\n", "# Create figure", "\n", "videoFig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "5", ",", "5", ")", ")", "\n", "\n", "# Set limits", "\n", "plt", ".", "xlim", "(", "(", "xMinMap", ",", "xMaxMap", ")", ")", "\n", "plt", ".", "ylim", "(", "(", "yMinMap", ",", "yMaxMap", ")", ")", "\n", "plt", ".", "axis", "(", "'equal'", ")", "\n", "\n", "if", "showVideoSpeed", ":", "\n", "                ", "plt", ".", "text", "(", "xMinMap", ",", "yMinMap", ",", "r'Speed: $%.2f$'", "%", "videoSpeed", ")", "\n", "\n", "# Create plot handle", "\n", "", "plotAgents", ",", "=", "plt", ".", "plot", "(", "[", "]", ",", "[", "]", ",", "\n", "marker", "=", "'o'", ",", "\n", "markersize", "=", "3", ",", "\n", "linewidth", "=", "0", ",", "\n", "color", "=", "'#01256E'", ",", "\n", "scalex", "=", "False", ",", "\n", "scaley", "=", "False", ")", "\n", "\n", "# Create the video", "\n", "with", "videoWriter", ".", "saving", "(", "videoFig", ",", "\n", "os", ".", "path", ".", "join", "(", "saveDir", ",", "thisVideoName", ")", ",", "\n", "tSamples", ")", ":", "\n", "\n", "                ", "for", "t", "in", "range", "(", "tSamples", ")", ":", "\n", "\n", "# Plot the agents", "\n", "                    ", "plotAgents", ".", "set_data", "(", "thisPos", "[", "t", ",", "0", ",", ":", "]", ",", "thisPos", "[", "t", ",", "1", ",", ":", "]", ")", "\n", "videoWriter", ".", "grab_frame", "(", ")", "\n", "\n", "# Print", "\n", "if", "doPrint", ":", "\n", "# Sample percentage count", "\n", "                        ", "percentageCount", "=", "int", "(", "\n", "100", "*", "(", "t", "+", "1", "+", "n", "*", "tSamples", ")", "/", "(", "tSamples", "*", "pos", ".", "shape", "[", "0", "]", ")", "\n", ")", "\n", "\n", "if", "n", "==", "0", "and", "t", "==", "0", ":", "\n", "                            ", "print", "(", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "", "else", ":", "\n", "                            ", "print", "(", "'\\b \\b'", "*", "4", "+", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "", "", "", "", "plt", ".", "close", "(", "fig", "=", "videoFig", ")", "\n", "\n", "# Print", "\n", "", "if", "doPrint", ":", "\n", "# Erase the percentage and the label", "\n", "            ", "print", "(", "'\\b \\b'", "*", "4", "+", "\"OK\"", ",", "flush", "=", "True", ")", "\n", "\n", "", "if", "showGraph", ":", "\n", "\n", "# Normalize velocity", "\n", "            ", "if", "showArrows", ":", "\n", "# vel is of shape nSamples x tSamples x 2 x nAgents", "\n", "                ", "velNormSq", "=", "np", ".", "sum", "(", "vel", "**", "2", ",", "axis", "=", "2", ")", "\n", "#   nSamples x tSamples x nAgents", "\n", "maxVelNormSq", "=", "np", ".", "max", "(", "np", ".", "max", "(", "velNormSq", ",", "axis", "=", "2", ")", ",", "axis", "=", "1", ")", "\n", "#   nSamples", "\n", "maxVelNormSq", "=", "maxVelNormSq", ".", "reshape", "(", "(", "nSamples", ",", "1", ",", "1", ",", "1", ")", ")", "\n", "#   nSamples x 1 x 1 x 1", "\n", "normVel", "=", "2", "*", "vel", "/", "np", ".", "sqrt", "(", "maxVelNormSq", ")", "\n", "\n", "", "if", "doPrint", ":", "\n", "                ", "print", "(", "\"\\tSaving graph snapshots...\"", ",", "end", "=", "' '", ",", "flush", "=", "True", ")", "\n", "\n", "# Essentially, we will print nGraphs snapshots and save them", "\n", "# as images with the graph. This is the best we can do in a", "\n", "# reasonable processing time (adding the graph to the video takes", "\n", "# forever).", "\n", "", "time", "=", "np", ".", "arange", "(", "0", ",", "self", ".", "duration", ",", "step", "=", "self", ".", "samplingTime", ")", "\n", "assert", "len", "(", "time", ")", "==", "tSamples", "\n", "\n", "nSnapshots", "=", "5", "# The number of snapshots we will consider", "\n", "tSnapshots", "=", "np", ".", "linspace", "(", "0", ",", "tSamples", "-", "1", ",", "num", "=", "nSnapshots", ")", "\n", "#   This gives us nSnapshots equally spaced in time. Now, we need", "\n", "#   to be sure these are integers", "\n", "tSnapshots", "=", "np", ".", "unique", "(", "tSnapshots", ".", "astype", "(", "np", ".", "int", ")", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "# Directory to save the snapshots", "\n", "snapshotDir", "=", "os", ".", "path", ".", "join", "(", "saveDir", ",", "'graphSnapshots'", ")", "\n", "# Base name of the snapshots", "\n", "snapshotName", "=", "'graphSnapshot'", "\n", "\n", "for", "n", "in", "range", "(", "pos", ".", "shape", "[", "0", "]", ")", ":", "\n", "\n", "                ", "if", "pos", ".", "shape", "[", "0", "]", ">", "1", ":", "\n", "                    ", "thisSnapshotDir", "=", "snapshotDir", "+", "'%03d'", "%", "n", "\n", "thisSnapshotName", "=", "snapshotName", "+", "'%03d'", "%", "n", "\n", "", "else", ":", "\n", "                    ", "thisSnapshotDir", "=", "snapshotDir", "\n", "thisSnapshotName", "=", "snapshotName", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "thisSnapshotDir", ")", ":", "\n", "                    ", "os", ".", "mkdir", "(", "thisSnapshotDir", ")", "\n", "\n", "# Get the corresponding positions", "\n", "", "thisPos", "=", "pos", "[", "n", "]", "\n", "thisCommGraph", "=", "commGraph", "[", "n", "]", "\n", "\n", "for", "t", "in", "tSnapshots", ":", "\n", "\n", "# Get the edge pairs", "\n", "#   Get the graph for this time instant", "\n", "                    ", "thisCommGraphTime", "=", "thisCommGraph", "[", "t", "]", "\n", "#   Check if it is symmetric", "\n", "isSymmetric", "=", "np", ".", "allclose", "(", "thisCommGraphTime", ",", "\n", "thisCommGraphTime", ".", "T", ")", "\n", "if", "isSymmetric", ":", "\n", "#   Use only half of the matrix", "\n", "                        ", "thisCommGraphTime", "=", "np", ".", "triu", "(", "thisCommGraphTime", ")", "\n", "\n", "#   Find the position of all edges", "\n", "", "outEdge", ",", "inEdge", "=", "np", ".", "nonzero", "(", "np", ".", "abs", "(", "thisCommGraphTime", ")", ">", "zeroTolerance", ")", "\n", "\n", "# Create the figure", "\n", "thisGraphSnapshotFig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "5", ",", "5", ")", ")", "\n", "\n", "# Set limits (to be the same as the video)", "\n", "plt", ".", "xlim", "(", "(", "xMinMap", ",", "xMaxMap", ")", ")", "\n", "plt", ".", "ylim", "(", "(", "yMinMap", ",", "yMaxMap", ")", ")", "\n", "plt", ".", "axis", "(", "'equal'", ")", "\n", "\n", "# Plot the edges", "\n", "plt", ".", "plot", "(", "[", "thisPos", "[", "t", ",", "0", ",", "outEdge", "]", ",", "thisPos", "[", "t", ",", "0", ",", "inEdge", "]", "]", ",", "\n", "[", "thisPos", "[", "t", ",", "1", ",", "outEdge", "]", ",", "thisPos", "[", "t", ",", "1", ",", "inEdge", "]", "]", ",", "\n", "color", "=", "'#A8AAAF'", ",", "linewidth", "=", "0.75", ",", "\n", "scalex", "=", "False", ",", "scaley", "=", "False", ")", "\n", "\n", "# Plot the arrows", "\n", "if", "showArrows", ":", "\n", "                        ", "for", "i", "in", "range", "(", "nAgents", ")", ":", "\n", "                            ", "plt", ".", "arrow", "(", "thisPos", "[", "t", ",", "0", ",", "i", "]", ",", "thisPos", "[", "t", ",", "1", ",", "i", "]", ",", "\n", "normVel", "[", "n", ",", "t", ",", "0", ",", "i", "]", ",", "normVel", "[", "n", ",", "t", ",", "1", ",", "i", "]", ")", "\n", "\n", "# Plot the nodes", "\n", "", "", "plt", ".", "plot", "(", "thisPos", "[", "t", ",", "0", ",", ":", "]", ",", "thisPos", "[", "t", ",", "1", ",", ":", "]", ",", "\n", "marker", "=", "'o'", ",", "markersize", "=", "3", ",", "linewidth", "=", "0", ",", "\n", "color", "=", "'#01256E'", ",", "scalex", "=", "False", ",", "scaley", "=", "False", ")", "\n", "\n", "# Add the cost value", "\n", "if", "showCost", ":", "\n", "                        ", "totalCost", "=", "self", ".", "evaluate", "(", "vel", "=", "vel", "[", ":", ",", "t", ":", "t", "+", "1", ",", ":", ",", ":", "]", ")", "\n", "plt", ".", "text", "(", "xMinMap", ",", "yMinMap", ",", "r'Cost: $%.4f$'", "%", "totalCost", ")", "\n", "\n", "# Add title", "\n", "", "plt", ".", "title", "(", "\"Time $t=%.4f$s\"", "%", "time", "[", "t", "]", ")", "\n", "\n", "# Save figure", "\n", "thisGraphSnapshotFig", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "thisSnapshotDir", ",", "\n", "thisSnapshotName", "+", "'%03d.pdf'", "%", "t", ")", ")", "\n", "\n", "# Close figure", "\n", "plt", ".", "close", "(", "fig", "=", "thisGraphSnapshotFig", ")", "\n", "\n", "# Print percentage completion", "\n", "if", "doPrint", ":", "\n", "# Sample percentage count", "\n", "                        ", "percentageCount", "=", "int", "(", "\n", "100", "*", "(", "t", "+", "1", "+", "n", "*", "tSamples", ")", "/", "(", "tSamples", "*", "pos", ".", "shape", "[", "0", "]", ")", "\n", ")", "\n", "if", "n", "==", "0", "and", "t", "==", "0", ":", "\n", "# Print new value", "\n", "                            ", "print", "(", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "", "else", ":", "\n", "# Erase the previous characters", "\n", "                            ", "print", "(", "'\\b \\b'", "*", "4", "+", "\"%3d%%\"", "%", "percentageCount", ",", "\n", "end", "=", "''", ",", "flush", "=", "True", ")", "\n", "\n", "\n", "\n", "# Print", "\n", "", "", "", "", "if", "doPrint", ":", "\n", "# Erase the percentage and the label", "\n", "                ", "print", "(", "'\\b \\b'", "*", "4", "+", "\"OK\"", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.__init__": [[4118, 4164], ["dataTools._dataForClassification.__init__", "dataTools.TwentyNews.getData", "dataTools.TwentyNews.embedData", "dataTools.TwentyNews.getData", "dataTools.TwentyNews.normalizeData", "dataTools.TwentyNews.createValidationSet", "dataTools.TwentyNews.createGraph", "dataTools.TwentyNews.astype", "dataTools.TwentyNews.to"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.embedData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.normalizeData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.createValidationSet", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.createGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["def", "__init__", "(", "self", ",", "ratioValid", ",", "nWords", ",", "nWordsShortDocs", ",", "nEdges", ",", "distMetric", ",", "\n", "dataDir", ",", "dataType", "=", "np", ".", "float64", ",", "device", "=", "'cpu'", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# This creates the attributes: dataType, device, nTrain, nTest, nValid,", "\n", "# and samples, and fills them all with None, and also creates the ", "\n", "# methods: getSamples, astype, to, and evaluate.", "\n", "self", ".", "dataType", "=", "dataType", "\n", "self", ".", "device", "=", "device", "\n", "\n", "# Other relevant information we need to store:", "\n", "self", ".", "dataDir", "=", "dataDir", "# Where the data is", "\n", "self", ".", "N", "=", "nWords", "# Number of nodes", "\n", "self", ".", "nWordsShortDocs", "=", "nWordsShortDocs", "# Number of words under which", "\n", "# a document is too short to be taken into consideration", "\n", "self", ".", "M", "=", "nEdges", "# Number of edges", "\n", "self", ".", "distMetric", "=", "distMetric", "# Distance metric to use", "\n", "self", ".", "dataset", "=", "{", "}", "# Here we save the dataset classes as they are", "\n", "# handled by mdeff's code", "\n", "self", ".", "nClasses", "=", "None", "# Number of classes", "\n", "self", ".", "vocab", "=", "None", "# Words considered", "\n", "self", ".", "graphData", "=", "None", "# Store the data (word2vec embeddings) required", "\n", "# to build the graph", "\n", "self", ".", "adjacencyMatrix", "=", "None", "# Store the graph built from the loaded", "\n", "# data", "\n", "\n", "# Get the training dataset. Saves vocab, dataset, and samples", "\n", "self", ".", "getData", "(", "'train'", ")", "\n", "# Embeds the data following the N words and a word2vec approach, saves", "\n", "# the embedded vectors in graphData, and updates vocab to keep only", "\n", "# the N words selected", "\n", "self", ".", "embedData", "(", ")", "\n", "# Get the testing dataset, only for the words stored in vocab.", "\n", "self", ".", "getData", "(", "'test'", ")", "\n", "# Normalize", "\n", "self", ".", "normalizeData", "(", ")", "\n", "# Save number of samples", "\n", "self", ".", "nTrain", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", ".", "shape", "[", "0", "]", "\n", "self", ".", "nTest", "=", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", ".", "shape", "[", "0", "]", "\n", "# Create validation set", "\n", "self", ".", "createValidationSet", "(", "ratioValid", ")", "\n", "# Create graph", "\n", "self", ".", "createGraph", "(", ")", "# Only after data has been embedded", "\n", "# Change data to specified type and device", "\n", "self", ".", "astype", "(", "self", ".", "dataType", ")", "\n", "self", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData": [[4165, 4197], ["dataTools.Text20News", "dataTools.TextDataset.clean_text", "Text20News.data.toarray", "dataTools.TextDataset.vectorize", "dataTools.TextDataset.vectorize", "dataTools.TextDataset.remove_short_documents", "dataTools.TextDataset.remove_encoded_images", "len", "dataTools.TextDataset.remove_short_documents"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.clean_text", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.vectorize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.vectorize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.remove_short_documents", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.remove_encoded_images", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.remove_short_documents"], ["", "def", "getData", "(", "self", ",", "dataSubset", ")", ":", "\n", "\n", "# Load dataset", "\n", "        ", "dataset", "=", "Text20News", "(", "data_home", "=", "self", ".", "dataDir", ",", "\n", "subset", "=", "dataSubset", ",", "\n", "remove", "=", "(", "'headers'", ",", "'footers'", ",", "'quotes'", ")", ",", "\n", "shuffle", "=", "True", ")", "\n", "# Get rid of numbers and other stuff", "\n", "dataset", ".", "clean_text", "(", "num", "=", "'substitute'", ")", "\n", "# If there's some vocabulary already defined, vectorize (count the", "\n", "# frequencies) of the words in vocab, if not, count all of them", "\n", "if", "self", ".", "vocab", "is", "None", ":", "\n", "            ", "dataset", ".", "vectorize", "(", "stop_words", "=", "'english'", ")", "\n", "self", ".", "vocab", "=", "dataset", ".", "vocab", "\n", "", "else", ":", "\n", "            ", "dataset", ".", "vectorize", "(", "vocabulary", "=", "self", ".", "vocab", ")", "\n", "\n", "# Get rid of short documents", "\n", "", "if", "dataSubset", "==", "'train'", ":", "\n", "            ", "dataset", ".", "remove_short_documents", "(", "nwords", "=", "self", ".", "nWordsShortDocs", ",", "\n", "vocab", "=", "'full'", ")", "\n", "# Get rid of images", "\n", "dataset", ".", "remove_encoded_images", "(", ")", "\n", "self", ".", "nClasses", "=", "len", "(", "dataset", ".", "class_names", ")", "\n", "", "else", ":", "\n", "            ", "dataset", ".", "remove_short_documents", "(", "nwords", "=", "self", ".", "nWordsShortDocs", ",", "\n", "vocab", "=", "'selected'", ")", "\n", "\n", "# Save them in the corresponding places", "\n", "", "self", ".", "samples", "[", "dataSubset", "]", "[", "'signals'", "]", "=", "dataset", ".", "data", ".", "toarray", "(", ")", "\n", "self", ".", "samples", "[", "dataSubset", "]", "[", "'targets'", "]", "=", "dataset", ".", "labels", "\n", "self", ".", "dataset", "[", "dataSubset", "]", "=", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.embedData": [[4198, 4224], ["dataTools.TwentyNews.dataset[].embed", "dataTools.TwentyNews.dataset[].keep_top_words", "dataTools.TwentyNews.dataset[].remove_short_documents", "dataTools.TwentyNews.dataset[].data.toarray", "dataTools.TwentyNews.dataset.keys", "dataTools.TwentyNews.dataset.keys", "dataTools.TwentyNews.dataset[].vectorize", "dataTools.TwentyNews.dataset[].data.toarray"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.embed", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.keep_top_words", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.remove_short_documents", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.vectorize"], ["", "def", "embedData", "(", "self", ")", ":", "\n", "\n", "# We need to have loaded the training dataset first.", "\n", "        ", "assert", "'train'", "in", "self", ".", "dataset", ".", "keys", "(", ")", "\n", "# Embed them (word2vec embedding)", "\n", "self", ".", "dataset", "[", "'train'", "]", ".", "embed", "(", ")", "\n", "# Keep only the top words (which determine the number of nodes)", "\n", "self", ".", "dataset", "[", "'train'", "]", ".", "keep_top_words", "(", "self", ".", "N", ")", "\n", "# Update the vocabulary", "\n", "self", ".", "vocab", "=", "self", ".", "dataset", "[", "'train'", "]", ".", "vocab", "\n", "# Get rid of short documents when considering only the specific ", "\n", "# vocabulary", "\n", "self", ".", "dataset", "[", "'train'", "]", ".", "remove_short_documents", "(", "\n", "nwords", "=", "self", ".", "nWordsShortDocs", ",", "\n", "vocab", "=", "'selected'", ")", "\n", "# Save the embeddings, which are necessary to build a graph", "\n", "self", ".", "graphData", "=", "self", ".", "dataset", "[", "'train'", "]", ".", "embeddings", "\n", "# Update the samples", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "self", ".", "dataset", "[", "'train'", "]", ".", "data", ".", "toarray", "(", ")", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "self", ".", "dataset", "[", "'train'", "]", ".", "labels", "\n", "# If there's an existing dataset, update it to the new vocabulary", "\n", "if", "'test'", "in", "self", ".", "dataset", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "dataset", "[", "'test'", "]", ".", "vectorize", "(", "vocabulary", "=", "self", ".", "vocab", ")", "\n", "# Update the samples", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "=", "self", ".", "dataset", "[", "'test'", "]", ".", "data", ".", "toarray", "(", ")", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "=", "self", ".", "dataset", "[", "'test'", "]", ".", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.normalizeData": [[4225, 4233], ["dataTools.TwentyNews.dataset.keys", "dataTools.TwentyNews.dataset[].normalize", "dataTools.TwentyNews.dataset[].data.toarray"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize"], ["", "", "def", "normalizeData", "(", "self", ",", "normType", "=", "'l1'", ")", ":", "\n", "\n", "        ", "for", "key", "in", "self", ".", "dataset", ".", "keys", "(", ")", ":", "\n", "# Normalize the frequencies on the l1 norm.", "\n", "            ", "self", ".", "dataset", "[", "key", "]", ".", "normalize", "(", "norm", "=", "normType", ")", "\n", "# And save it", "\n", "self", ".", "samples", "[", "key", "]", "[", "'signals'", "]", "=", "self", ".", "dataset", "[", "key", "]", ".", "data", ".", "toarray", "(", ")", "\n", "self", ".", "samples", "[", "key", "]", "[", "'targets'", "]", "=", "self", ".", "dataset", "[", "key", "]", ".", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.createValidationSet": [[4234, 4254], ["int", "numpy.random.permutation"], "methods", ["None"], ["", "", "def", "createValidationSet", "(", "self", ",", "ratio", ")", ":", "\n", "# How many valid samples", "\n", "        ", "self", ".", "nValid", "=", "int", "(", "ratio", "*", "self", ".", "nTrain", ")", "\n", "# Shuffle indices", "\n", "randomIndices", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "nTrain", ")", "\n", "validationIndices", "=", "randomIndices", "[", "0", ":", "self", ".", "nValid", "]", "\n", "trainIndices", "=", "randomIndices", "[", "self", ".", "nValid", ":", "]", "\n", "# Fetch those samples and put them in the validation set", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "[", "validationIndices", ",", ":", "]", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "[", "validationIndices", "]", "\n", "# And update the training set", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "[", "trainIndices", ",", ":", "]", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "[", "trainIndices", "]", "\n", "# Update the numbers", "\n", "self", ".", "nValid", "=", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", ".", "shape", "[", "0", "]", "\n", "self", ".", "nTrain", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.createGraph": [[4255, 4265], ["dataTools.distance_sklearn_metrics", "adjacency().toarray", "len", "len", "len", "dataTools.adjacency"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.distance_sklearn_metrics", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.adjacency"], ["", "def", "createGraph", "(", "self", ",", "*", "args", ")", ":", "\n", "\n", "        ", "assert", "self", ".", "graphData", "is", "not", "None", "\n", "assert", "len", "(", "args", ")", "==", "0", "or", "len", "(", "args", ")", "==", "2", "\n", "if", "len", "(", "args", ")", "==", "2", ":", "\n", "            ", "self", ".", "M", "=", "args", "[", "0", "]", "# Number of edges", "\n", "self", ".", "distMetric", "=", "args", "[", "1", "]", "# Distance metric", "\n", "", "dist", ",", "idx", "=", "distance_sklearn_metrics", "(", "self", ".", "graphData", ",", "k", "=", "self", ".", "M", ",", "\n", "metric", "=", "self", ".", "distMetric", ")", "\n", "self", ".", "adjacencyMatrix", "=", "adjacency", "(", "dist", ",", "idx", ")", ".", "toarray", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getGraph": [[4266, 4269], ["None"], "methods", ["None"], ["", "def", "getGraph", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "adjacencyMatrix", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getNumberOfClasses": [[4270, 4273], ["None"], "methods", ["None"], ["", "def", "getNumberOfClasses", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "nClasses", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.reduceDataset": [[4274, 4308], ["numpy.random.permutation", "numpy.random.permutation", "numpy.random.permutation"], "methods", ["None"], ["", "def", "reduceDataset", "(", "self", ",", "nTrain", ",", "nValid", ",", "nTest", ")", ":", "\n", "        ", "if", "nTrain", "<", "self", ".", "nTrain", ":", "\n", "            ", "randomIndices", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "nTrain", ")", "\n", "trainIndices", "=", "randomIndices", "[", "0", ":", "nTrain", "]", "\n", "# And update the training set", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "[", "trainIndices", ",", ":", "]", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "[", "trainIndices", "]", "\n", "self", ".", "nTrain", "=", "nTrain", "\n", "", "if", "nValid", "<", "self", ".", "nValid", ":", "\n", "            ", "randomIndices", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "nValid", ")", "\n", "validIndices", "=", "randomIndices", "[", "0", ":", "nValid", "]", "\n", "# And update the training set", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "=", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "[", "validIndices", ",", ":", "]", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "=", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "[", "validIndices", "]", "\n", "self", ".", "nValid", "=", "nValid", "\n", "", "if", "nTest", "<", "self", ".", "nTest", ":", "\n", "            ", "randomIndices", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "nTest", ")", "\n", "testIndices", "=", "randomIndices", "[", "0", ":", "nTest", "]", "\n", "# And update the training set", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "=", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "[", "testIndices", ",", ":", "]", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "=", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "[", "testIndices", "]", "\n", "self", ".", "nTest", "=", "nTest", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype": [[4309, 4319], ["dataTools.changeDataType", "dataTools.changeDataType", "dataTools._data.astype"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "", "def", "astype", "(", "self", ",", "dataType", ")", ":", "\n", "# This changes the type for the graph data, as well as the adjacency", "\n", "# matrix. We are going to leave the dataset attribute as it is, since", "\n", "# this is the most accurate reflection of mdeff's code.", "\n", "        ", "self", ".", "graphData", "=", "changeDataType", "(", "self", ".", "graphData", ",", "dataType", ")", "\n", "self", ".", "adjacencyMatrix", "=", "changeDataType", "(", "self", ".", "adjacencyMatrix", ",", "dataType", ")", "\n", "\n", "# And now, initialize to change the samples as well (and also save the ", "\n", "# data type)", "\n", "super", "(", ")", ".", "astype", "(", "dataType", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.to": [[4321, 4331], ["repr().find", "dataTools.TwentyNews.graphData.to", "dataTools.TwentyNews.adjacencyMatrix.to", "dataTools._data.to", "repr"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# If the dataType is 'torch'", "\n", "        ", "if", "repr", "(", "self", ".", "dataType", ")", ".", "find", "(", "'torch'", ")", ">=", "0", ":", "\n", "# Change the stored attributes that are not handled by the inherited", "\n", "# method to().", "\n", "            ", "self", ".", "graphData", ".", "to", "(", "device", ")", "\n", "self", ".", "adjacencyMatrix", ".", "to", "(", "device", ")", "\n", "# And call the inherit method to initialize samples (and save to", "\n", "# device)", "\n", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.clean_text": [[4410, 4444], ["enumerate", "re.sub.replace", "re.sub.lower", "re.sub", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.replace", "re.sub.split", "re.sub", "re.sub"], "methods", ["None"], ["    ", "def", "clean_text", "(", "self", ",", "num", "=", "'substitute'", ")", ":", "\n", "# TODO: stemming, lemmatisation", "\n", "        ", "for", "i", ",", "doc", "in", "enumerate", "(", "self", ".", "documents", ")", ":", "\n", "# Digits.", "\n", "            ", "if", "num", "==", "'spell'", ":", "\n", "                ", "doc", "=", "doc", ".", "replace", "(", "'0'", ",", "' zero '", ")", "\n", "doc", "=", "doc", ".", "replace", "(", "'1'", ",", "' one '", ")", "\n", "doc", "=", "doc", ".", "replace", "(", "'2'", ",", "' two '", ")", "\n", "doc", "=", "doc", ".", "replace", "(", "'3'", ",", "' three '", ")", "\n", "doc", "=", "doc", ".", "replace", "(", "'4'", ",", "' four '", ")", "\n", "doc", "=", "doc", ".", "replace", "(", "'5'", ",", "' five '", ")", "\n", "doc", "=", "doc", ".", "replace", "(", "'6'", ",", "' six '", ")", "\n", "doc", "=", "doc", ".", "replace", "(", "'7'", ",", "' seven '", ")", "\n", "doc", "=", "doc", ".", "replace", "(", "'8'", ",", "' eight '", ")", "\n", "doc", "=", "doc", ".", "replace", "(", "'9'", ",", "' nine '", ")", "\n", "", "elif", "num", "==", "'substitute'", ":", "\n", "# All numbers are equal. Useful for embedding", "\n", "# (countable words) ?", "\n", "                ", "doc", "=", "re", ".", "sub", "(", "'(\\\\d+)'", ",", "' NUM '", ",", "doc", ")", "\n", "", "elif", "num", "==", "'remove'", ":", "\n", "# Numbers are uninformative (they are all over the place).", "\n", "# Useful for bag-of-words ?", "\n", "# But maybe some kind of documents contain more numbers,", "\n", "# e.g. finance.", "\n", "# Some documents are indeed full of numbers. At least", "\n", "# in 20NEWS.", "\n", "                ", "doc", "=", "re", ".", "sub", "(", "'[0-9]'", ",", "' '", ",", "doc", ")", "\n", "# Remove everything except a-z characters and single space.", "\n", "", "doc", "=", "doc", ".", "replace", "(", "'$'", ",", "' dollar '", ")", "\n", "doc", "=", "doc", ".", "lower", "(", ")", "\n", "doc", "=", "re", ".", "sub", "(", "'[^a-z]'", ",", "' '", ",", "doc", ")", "\n", "doc", "=", "' '", ".", "join", "(", "doc", ".", "split", "(", ")", ")", "# same as ", "\n", "# doc = re.sub('\\s{2,}', ' ', doc)", "\n", "self", ".", "documents", "[", "i", "]", "=", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.vectorize": [[4445, 4451], ["sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "sklearn.feature_extraction.text.CountVectorizer.fit_transform", "sklearn.feature_extraction.text.CountVectorizer.get_feature_names", "sklearn.feature_extraction.text.CountVectorizer.get_feature_names", "sklearn.feature_extraction.text.CountVectorizer.get_feature_names", "len"], "methods", ["None"], ["", "", "def", "vectorize", "(", "self", ",", "**", "params", ")", ":", "\n", "# TODO: count or tf-idf. Or in normalize ?", "\n", "        ", "vectorizer", "=", "sklearn", ".", "feature_extraction", ".", "text", ".", "CountVectorizer", "(", "**", "params", ")", "\n", "self", ".", "data", "=", "vectorizer", ".", "fit_transform", "(", "self", ".", "documents", ")", "\n", "self", ".", "vocab", "=", "vectorizer", ".", "get_feature_names", "(", ")", "\n", "assert", "len", "(", "self", ".", "vocab", ")", "==", "self", ".", "data", ".", "shape", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.keep_documents": [[4452, 4457], ["None"], "methods", ["None"], ["", "def", "keep_documents", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Keep the documents given by the index, discard the others.\"\"\"", "\n", "self", ".", "documents", "=", "[", "self", ".", "documents", "[", "i", "]", "for", "i", "in", "idx", "]", "\n", "self", ".", "labels", "=", "self", ".", "labels", "[", "idx", "]", "\n", "self", ".", "data", "=", "self", ".", "data", "[", "idx", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.keep_words": [[4458, 4466], ["None"], "methods", ["None"], ["", "def", "keep_words", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Keep the documents given by the index, discard the others.\"\"\"", "\n", "self", ".", "data", "=", "self", ".", "data", "[", ":", ",", "idx", "]", "\n", "self", ".", "vocab", "=", "[", "self", ".", "vocab", "[", "i", "]", "for", "i", "in", "idx", "]", "\n", "try", ":", "\n", "            ", "self", ".", "embeddings", "=", "self", ".", "embeddings", "[", "idx", ",", ":", "]", "\n", "", "except", "AttributeError", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.remove_short_documents": [[4467, 4480], ["numpy.argwhere().squeeze", "dataTools.TextDataset.keep_documents", "dataTools.TextDataset.data.sum", "numpy.squeeze", "numpy.asarray", "numpy.empty", "enumerate", "numpy.argwhere", "len", "len", "doc.split"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.keep_documents"], ["", "", "def", "remove_short_documents", "(", "self", ",", "nwords", ",", "vocab", "=", "'selected'", ")", ":", "\n", "        ", "\"\"\"Remove a document if it contains less than nwords.\"\"\"", "\n", "if", "vocab", "==", "'selected'", ":", "\n", "# Word count with selected vocabulary.", "\n", "            ", "wc", "=", "self", ".", "data", ".", "sum", "(", "axis", "=", "1", ")", "\n", "wc", "=", "np", ".", "squeeze", "(", "np", ".", "asarray", "(", "wc", ")", ")", "\n", "", "elif", "vocab", "==", "'full'", ":", "\n", "# Word count with full vocabulary.", "\n", "            ", "wc", "=", "np", ".", "empty", "(", "len", "(", "self", ".", "documents", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "self", ".", "documents", ")", ":", "\n", "                ", "wc", "[", "i", "]", "=", "len", "(", "doc", ".", "split", "(", ")", ")", "\n", "", "", "idx", "=", "np", ".", "argwhere", "(", "wc", ">=", "nwords", ")", ".", "squeeze", "(", ")", "\n", "self", ".", "keep_documents", "(", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.keep_top_words": [[4481, 4488], ["dataTools.TextDataset.data.sum", "numpy.squeeze", "dataTools.TextDataset.keep_words", "numpy.asarray", "numpy.argsort"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.keep_words"], ["", "def", "keep_top_words", "(", "self", ",", "M", ")", ":", "\n", "        ", "\"\"\"Keep in the vocaluary the M words who appear most often.\"\"\"", "\n", "freq", "=", "self", ".", "data", ".", "sum", "(", "axis", "=", "0", ")", "\n", "freq", "=", "np", ".", "squeeze", "(", "np", ".", "asarray", "(", "freq", ")", ")", "\n", "idx", "=", "np", ".", "argsort", "(", "freq", ")", "[", ":", ":", "-", "1", "]", "\n", "idx", "=", "idx", "[", ":", "M", "]", "\n", "self", ".", "keep_words", "(", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize": [[4489, 4494], ["dataTools.TextDataset.data.astype", "sklearn.preprocessing.normalize", "sklearn.preprocessing.normalize", "sklearn.preprocessing.normalize", "sklearn.preprocessing.normalize", "sklearn.preprocessing.normalize", "sklearn.preprocessing.normalize", "sklearn.preprocessing.normalize", "sklearn.preprocessing.normalize", "sklearn.preprocessing.normalize"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.normalize"], ["", "def", "normalize", "(", "self", ",", "norm", "=", "'l1'", ")", ":", "\n", "        ", "\"\"\"Normalize data to unit length.\"\"\"", "\n", "# TODO: TF-IDF.", "\n", "data", "=", "self", ".", "data", ".", "astype", "(", "np", ".", "float64", ")", "\n", "self", ".", "data", "=", "sklearn", ".", "preprocessing", ".", "normalize", "(", "data", ",", "axis", "=", "1", ",", "norm", "=", "norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.embed": [[4495, 4519], ["numpy.empty", "enumerate", "dataTools.TextDataset.keep_words", "gensim.models.Word2Vec.load_word2vec_format", "gensim.models.Word2Vec", "Sentences", "len", "keep.append", "document.split"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.keep_words"], ["", "def", "embed", "(", "self", ",", "filename", "=", "None", ",", "size", "=", "100", ")", ":", "\n", "        ", "\"\"\"Embed the vocabulary using pre-trained vectors.\"\"\"", "\n", "if", "filename", ":", "\n", "            ", "model", "=", "gensim", ".", "models", ".", "Word2Vec", ".", "load_word2vec_format", "(", "filename", ",", "\n", "binary", "=", "True", ")", "\n", "size", "=", "model", ".", "vector_size", "\n", "", "else", ":", "\n", "            ", "class", "Sentences", "(", "object", ")", ":", "\n", "                ", "def", "__init__", "(", "self", ",", "documents", ")", ":", "\n", "                    ", "self", ".", "documents", "=", "documents", "\n", "", "def", "__iter__", "(", "self", ")", ":", "\n", "                    ", "for", "document", "in", "self", ".", "documents", ":", "\n", "                        ", "yield", "document", ".", "split", "(", ")", "\n", "", "", "", "model", "=", "gensim", ".", "models", ".", "Word2Vec", "(", "Sentences", "(", "self", ".", "documents", ")", ",", "size", "=", "size", ")", "\n", "", "self", ".", "embeddings", "=", "np", ".", "empty", "(", "(", "len", "(", "self", ".", "vocab", ")", ",", "size", ")", ")", "\n", "keep", "=", "[", "]", "\n", "not_found", "=", "0", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "self", ".", "vocab", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "self", ".", "embeddings", "[", "i", ",", ":", "]", "=", "model", "[", "word", "]", "\n", "keep", ".", "append", "(", "i", ")", "\n", "", "except", "KeyError", ":", "\n", "                ", "not_found", "+=", "1", "\n", "", "", "self", ".", "keep_words", "(", "keep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.remove_encoded_images": [[4520, 4525], ["dataTools.TextDataset.vocab.index", "dataTools.TextDataset.data[].toarray().squeeze", "numpy.argwhere().squeeze", "dataTools.TextDataset.keep_documents", "dataTools.TextDataset.data[].toarray", "numpy.argwhere"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TextDataset.keep_documents"], ["", "def", "remove_encoded_images", "(", "self", ",", "freq", "=", "1e3", ")", ":", "\n", "        ", "widx", "=", "self", ".", "vocab", ".", "index", "(", "'ax'", ")", "\n", "wc", "=", "self", ".", "data", "[", ":", ",", "widx", "]", ".", "toarray", "(", ")", ".", "squeeze", "(", ")", "\n", "idx", "=", "np", ".", "argwhere", "(", "wc", "<", "freq", ")", ".", "squeeze", "(", ")", "\n", "self", ".", "keep_documents", "(", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Text20News.__init__": [[4527, 4533], ["sklearn.datasets.fetch_20newsgroups", "sklearn.datasets.fetch_20newsgroups", "sklearn.datasets.fetch_20newsgroups", "sklearn.datasets.fetch_20newsgroups", "sklearn.datasets.fetch_20newsgroups", "sklearn.datasets.fetch_20newsgroups", "sklearn.datasets.fetch_20newsgroups", "sklearn.datasets.fetch_20newsgroups", "sklearn.datasets.fetch_20newsgroups", "len", "max"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "**", "params", ")", ":", "\n", "        ", "dataset", "=", "sklearn", ".", "datasets", ".", "fetch_20newsgroups", "(", "**", "params", ")", "\n", "self", ".", "documents", "=", "dataset", ".", "data", "\n", "self", ".", "labels", "=", "dataset", ".", "target", "\n", "self", ".", "class_names", "=", "dataset", ".", "target_names", "\n", "assert", "max", "(", "self", ".", "labels", ")", "+", "1", "==", "len", "(", "self", ".", "class_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Epidemics.__init__": [[4536, 4591], ["dataTools._data.__init__", "dataTools.Epidemics.createGraph", "numpy.expand_dims", "numpy.zeros", "range", "numpy.random.binomial", "range", "numpy.concatenate", "numpy.sum", "numpy.random.binomial", "range", "numpy.expand_dims", "numpy.sum", "list", "numpy.argwhere", "numpy.random.binomial"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.createGraph"], ["        ", "def", "__init__", "(", "self", ",", "seqLen", ",", "seedProb", ",", "infectionProb", ",", "recoveryTime", ",", "\n", "nTrain", ",", "nValid", ",", "nTest", ",", "x0", "=", "None", ",", "dataType", "=", "np", ".", "float64", ",", "\n", "device", "=", "'cpu'", ")", ":", "\n", "\n", "            ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seqLen", "=", "seqLen", "\n", "self", ".", "seedProb", "=", "seedProb", "\n", "self", ".", "infectionProb", "=", "infectionProb", "\n", "self", ".", "recoveryTime", "=", "recoveryTime", "\n", "self", ".", "nTrain", "=", "nTrain", "\n", "self", ".", "nValid", "=", "nValid", "\n", "self", ".", "nTest", "=", "nTest", "\n", "nSamples", "=", "nTrain", "+", "nValid", "+", "nTest", "\n", "self", ".", "dataType", "=", "dataType", "\n", "self", ".", "device", "=", "device", "\n", "\n", "self", ".", "Adj", "=", "self", ".", "createGraph", "(", ")", "\n", "self", ".", "N", "=", "self", ".", "Adj", ".", "shape", "[", "0", "]", "\n", "\n", "if", "x0", "is", "not", "None", ":", "\n", "                ", "self", ".", "x0", "=", "x0", "\n", "", "else", ":", "\n", "                ", "x0", "=", "np", ".", "random", ".", "binomial", "(", "1", ",", "self", ".", "seedProb", ",", "(", "nSamples", ",", "self", ".", "N", ")", ")", "\n", "while", "np", ".", "sum", "(", "np", ".", "sum", "(", "x0", ",", "axis", "=", "1", ")", ">", "0", ")", "<", "nSamples", ":", "\n", "                    ", "x0", "=", "np", ".", "random", ".", "binomial", "(", "1", ",", "self", ".", "seedProb", ",", "(", "nSamples", ",", "self", ".", "N", ")", ")", "\n", "", "self", ".", "x0", "=", "x0", "\n", "\n", "", "horizon", "=", "2", "*", "seqLen", "\n", "x_t", "=", "x0", "\n", "x", "=", "np", ".", "expand_dims", "(", "x_t", ",", "axis", "=", "1", ")", "\n", "timeInfection", "=", "np", ".", "zeros", "(", "(", "self", ".", "N", ",", "nSamples", ")", ")", "\n", "for", "t", "in", "range", "(", "1", ",", "horizon", ")", ":", "\n", "                ", "x_tplus1", "=", "x_t", "\n", "for", "n", "in", "range", "(", "nSamples", ")", ":", "\n", "                    ", "for", "i", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "                        ", "if", "x_t", "[", "n", ",", "i", "]", "==", "1", ":", "\n", "                            ", "for", "j", "in", "list", "(", "np", ".", "argwhere", "(", "self", ".", "Adj", "[", "i", ",", "i", ":", "]", ">", "0", ")", ")", ":", "\n", "                                ", "if", "x_t", "[", "n", ",", "j", "]", "==", "0", ":", "\n", "                                    ", "x_tplus1", "[", "n", ",", "j", "]", "==", "np", ".", "random", ".", "binomial", "(", "1", ",", "\n", "infectionProb", "*", "t", "/", "horizon", ")", "\n", "timeInfection", "[", "j", ",", "n", "]", "=", "t", "\n", "", "", "if", "timeInfection", "[", "i", ",", "n", "]", "-", "t", ">=", "recoveryTime", ":", "\n", "                                ", "x_tplus1", "[", "n", ",", "i", "]", "=", "2", "\n", "", "", "", "", "x_t", "=", "x_tplus1", "\n", "x", "=", "np", ".", "concatenate", "(", "(", "x", ",", "np", ".", "expand_dims", "(", "x_t", ",", "axis", "=", "1", ")", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "y", "=", "x", "[", ":", ",", "seqLen", ":", "horizon", ",", ":", "]", "==", "1", "\n", "x", "=", "x", "[", ":", ",", ":", "seqLen", ",", ":", "]", "\n", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'signals'", "]", "=", "x", "[", "0", ":", "nTrain", ",", ":", ",", ":", "]", "\n", "self", ".", "samples", "[", "'train'", "]", "[", "'targets'", "]", "=", "y", "[", "0", ":", "nTrain", ",", ":", ",", ":", "]", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'signals'", "]", "=", "x", "[", "nTrain", ":", "nTrain", "+", "nValid", ",", ":", ",", ":", "]", "\n", "self", ".", "samples", "[", "'valid'", "]", "[", "'targets'", "]", "=", "y", "[", "nTrain", ":", "nTrain", "+", "nValid", ",", ":", ",", ":", "]", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'signals'", "]", "=", "x", "[", "nTrain", "+", "nValid", ":", "nSamples", ",", ":", ",", ":", "]", "\n", "self", ".", "samples", "[", "'test'", "]", "[", "'targets'", "]", "=", "y", "[", "nTrain", "+", "nValid", ":", "nSamples", ",", ":", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Epidemics.createGraph": [[4592, 4614], ["numpy.array", "numpy.argwhere().squeeze", "open", "csv.reader", "max", "aux_list.append", "aux_list.append", "edge_list.append", "max", "range", "numpy.transpose", "numpy.argwhere", "int", "int", "numpy.matmul", "numpy.ones"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "createGraph", "(", ")", ":", "\n", "\n", "            ", "edge_list", "=", "[", "]", "\n", "with", "open", "(", "'datasets/epidemics/edge_list.txt'", ")", "as", "csv_file", ":", "\n", "                ", "csv_reader", "=", "csv", ".", "reader", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "for", "row", "in", "csv_reader", ":", "\n", "                    ", "aux_list", "=", "[", "]", "\n", "aux_list", ".", "append", "(", "int", "(", "row", "[", "0", "]", ")", "-", "1", ")", "\n", "aux_list", ".", "append", "(", "int", "(", "row", "[", "1", "]", ")", "-", "1", ")", "\n", "edge_list", ".", "append", "(", "aux_list", ")", "\n", "", "", "nNodes", "=", "max", "(", "max", "(", "edge_list", ")", ")", "+", "1", "\n", "Adj", "=", "[", "[", "0", "]", "*", "nNodes", "for", "_", "in", "range", "(", "nNodes", ")", "]", "\n", "for", "sink", ",", "source", "in", "edge_list", ":", "\n", "                ", "Adj", "[", "sink", "]", "[", "source", "]", "=", "1", "\n", "", "Adj", "=", "np", ".", "array", "(", "Adj", ")", "\n", "Adj", "=", "Adj", "+", "np", ".", "transpose", "(", "Adj", ")", ">", "0", "\n", "idx_0", "=", "np", ".", "argwhere", "(", "np", ".", "matmul", "(", "Adj", ",", "np", ".", "ones", "(", "nNodes", ")", ")", ">", "0", ")", ".", "squeeze", "(", ")", "\n", "Adj", "=", "Adj", "[", "idx_0", ",", ":", "]", "\n", "Adj", "=", "Adj", "[", ":", ",", "idx_0", "]", "\n", "\n", "return", "Adj", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Epidemics.evaluate": [[4615, 4649], ["len", "yHat.double.double.reshape", "torch.nn.functional.log_softmax", "torch.exp", "torch.argmax", "yHat.double.double.double", "y.reshape.reshape.reshape", "torch.sum", "torch.sum", "torch.sum", "torch.mean"], "methods", ["None"], ["", "def", "evaluate", "(", "self", ",", "yHat", ",", "y", ",", "tol", "=", "1e-9", ")", ":", "\n", "\n", "            ", "dimensions", "=", "len", "(", "yHat", ".", "shape", ")", "\n", "C", "=", "yHat", ".", "shape", "[", "dimensions", "-", "2", "]", "\n", "N", "=", "yHat", ".", "shape", "[", "dimensions", "-", "1", "]", "\n", "yHat", "=", "yHat", ".", "reshape", "(", "(", "-", "1", ",", "C", ",", "N", ")", ")", "\n", "yHat", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "yHat", ",", "dim", "=", "1", ")", "\n", "yHat", "=", "torch", ".", "exp", "(", "yHat", ")", "\n", "yHat", "=", "torch", ".", "argmax", "(", "yHat", ",", "dim", "=", "1", ")", "\n", "yHat", "=", "yHat", ".", "double", "(", ")", "\n", "y", "=", "y", ".", "reshape", "(", "(", "-", "1", ",", "N", ")", ")", "\n", "\n", "tp", "=", "torch", ".", "sum", "(", "y", "*", "yHat", ",", "1", ")", "\n", "#tn = torch.sum((1-y)*(1-yHat),1)", "\n", "fp", "=", "torch", ".", "sum", "(", "(", "1", "-", "y", ")", "*", "yHat", ",", "1", ")", "\n", "fn", "=", "torch", ".", "sum", "(", "y", "*", "(", "1", "-", "yHat", ")", ",", "1", ")", "\n", "\n", "p", "=", "tp", "/", "(", "tp", "+", "fp", ")", "\n", "r", "=", "tp", "/", "(", "tp", "+", "fn", ")", "\n", "\n", "idx_p", "=", "p", "!=", "p", "\n", "idx_tp", "=", "tp", "<", "tol", "\n", "idx_p1", "=", "idx_p", "*", "idx_tp", "\n", "p", "[", "idx_p", "]", "=", "0", "\n", "p", "[", "idx_p1", "]", "=", "1", "\n", "idx_r", "=", "r", "!=", "r", "\n", "idx_r1", "=", "idx_r", "*", "idx_tp", "\n", "r", "[", "idx_r", "]", "=", "0", "\n", "r", "[", "idx_r1", "]", "=", "1", "\n", "\n", "f1", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "\n", "f1", "[", "f1", "!=", "f1", "]", "=", "0", "\n", "\n", "return", "1", "-", "torch", ".", "mean", "(", "f1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.normalizeData": [[52, 85], ["type", "len", "repr", "numpy.mean", "numpy.std", "numpy.expand_dims", "numpy.expand_dims", "repr", "torch.mean", "torch.std", "xMean.unsqueeze.unsqueeze", "xDev.unsqueeze.unsqueeze"], "function", ["None"], ["def", "normalizeData", "(", "x", ",", "ax", ")", ":", "\n", "    ", "\"\"\"\n    normalizeData(x, ax): normalize data x (subtract mean and divide by standard \n    deviation) along the specified axis ax\n    \"\"\"", "\n", "\n", "thisShape", "=", "x", ".", "shape", "# get the shape", "\n", "assert", "ax", "<", "len", "(", "thisShape", ")", "# check that the axis that we want to normalize", "\n", "# is there", "\n", "dataType", "=", "type", "(", "x", ")", "# get data type so that we don't have to convert", "\n", "\n", "if", "'numpy'", "in", "repr", "(", "dataType", ")", ":", "\n", "\n", "# Compute the statistics", "\n", "        ", "xMean", "=", "np", ".", "mean", "(", "x", ",", "axis", "=", "ax", ")", "\n", "xDev", "=", "np", ".", "std", "(", "x", ",", "axis", "=", "ax", ")", "\n", "# Add back the dimension we just took out", "\n", "xMean", "=", "np", ".", "expand_dims", "(", "xMean", ",", "ax", ")", "\n", "xDev", "=", "np", ".", "expand_dims", "(", "xDev", ",", "ax", ")", "\n", "\n", "", "elif", "'torch'", "in", "repr", "(", "dataType", ")", ":", "\n", "\n", "# Compute the statistics", "\n", "        ", "xMean", "=", "torch", ".", "mean", "(", "x", ",", "dim", "=", "ax", ")", "\n", "xDev", "=", "torch", ".", "std", "(", "x", ",", "dim", "=", "ax", ")", "\n", "# Add back the dimension we just took out", "\n", "xMean", "=", "xMean", ".", "unsqueeze", "(", "ax", ")", "\n", "xDev", "=", "xDev", ".", "unsqueeze", "(", "ax", ")", "\n", "\n", "# Subtract mean and divide by standard deviation", "\n", "", "x", "=", "(", "x", "-", "xMean", ")", "/", "xDev", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType": [[86, 128], ["dir", "repr", "repr", "torch.tensor.cpu().numpy().astype", "repr", "repr", "torch.tensor.astype", "repr", "torch.tensor.type", "torch.tensor.cpu().numpy", "type", "repr", "torch.tensor", "type", "torch.tensor.cpu"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "changeDataType", "(", "x", ",", "dataType", ")", ":", "\n", "    ", "\"\"\"\n    changeDataType(x, dataType): change the dataType of variable x into dataType\n    \"\"\"", "\n", "\n", "# So this is the thing: To change data type it depends on both, what dtype", "\n", "# the variable already is, and what dtype we want to make it.", "\n", "# Torch changes type by .type(), but numpy by .astype()", "\n", "# If we have already a torch defined, and we apply a torch.tensor() to it,", "\n", "# then there will be warnings because of gradient accounting.", "\n", "\n", "# All of these facts make changing types considerably cumbersome. So we", "\n", "# create a function that just changes type and handles all this issues", "\n", "# inside.", "\n", "\n", "# If we can't recognize the type, we just make everything numpy.", "\n", "\n", "# Check if the variable has an argument called 'dtype' so that we can now", "\n", "# what type of data type the variable is", "\n", "if", "'dtype'", "in", "dir", "(", "x", ")", ":", "\n", "        ", "varType", "=", "x", ".", "dtype", "\n", "\n", "# So, let's start assuming we want to convert to numpy", "\n", "", "if", "'numpy'", "in", "repr", "(", "dataType", ")", ":", "\n", "# Then, the variable con be torch, in which case we move it to cpu, to", "\n", "# numpy, and convert it to the right type.", "\n", "        ", "if", "'torch'", "in", "repr", "(", "varType", ")", ":", "\n", "            ", "x", "=", "x", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "astype", "(", "dataType", ")", "\n", "# Or it could be numpy, in which case we just use .astype", "\n", "", "elif", "'numpy'", "in", "repr", "(", "type", "(", "x", ")", ")", ":", "\n", "            ", "x", "=", "x", ".", "astype", "(", "dataType", ")", "\n", "# Now, we want to convert to torch", "\n", "", "", "elif", "'torch'", "in", "repr", "(", "dataType", ")", ":", "\n", "# If the variable is torch in itself", "\n", "        ", "if", "'torch'", "in", "repr", "(", "varType", ")", ":", "\n", "            ", "x", "=", "x", ".", "type", "(", "dataType", ")", "\n", "# But, if it's numpy", "\n", "", "elif", "'numpy'", "in", "repr", "(", "type", "(", "x", ")", ")", ":", "\n", "            ", "x", "=", "torch", ".", "tensor", "(", "x", ",", "dtype", "=", "dataType", ")", "\n", "\n", "# This only converts between numpy and torch. Any other thing is ignored", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW": [[129, 140], ["x.copy"], "function", ["None"], ["", "def", "invertTensorEW", "(", "x", ")", ":", "\n", "\n", "# Elementwise inversion of a tensor where the 0 elements are kept as zero.", "\n", "# Warning: Creates a copy of the tensor", "\n", "    ", "xInv", "=", "x", ".", "copy", "(", ")", "# Copy the matrix to invert", "\n", "# Replace zeros for ones.", "\n", "xInv", "[", "x", "<", "zeroTolerance", "]", "=", "1.", "# Replace zeros for ones", "\n", "xInv", "=", "1.", "/", "xInv", "# Now we can invert safely", "\n", "xInv", "[", "x", "<", "zeroTolerance", "]", "=", "0.", "# Put back the zeros", "\n", "\n", "return", "xInv", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.distance_sklearn_metrics": [[4341, 4350], ["sklearn.metrics.pairwise.pairwise_distances", "sklearn.metrics.pairwise.pairwise_distances", "sklearn.metrics.pairwise.pairwise_distances", "sklearn.metrics.pairwise.pairwise_distances.sort", "numpy.argsort"], "function", ["None"], ["def", "distance_sklearn_metrics", "(", "z", ",", "k", "=", "4", ",", "metric", "=", "'euclidean'", ")", ":", "\n", "\t", "\"\"\"Compute exact pairwise distances.\"\"\"", "\n", "d", "=", "sklearn", ".", "metrics", ".", "pairwise", ".", "pairwise_distances", "(", "\n", "z", ",", "metric", "=", "metric", ")", "\n", "# k-NN graph.", "\n", "idx", "=", "np", ".", "argsort", "(", "d", ")", "[", ":", ",", "1", ":", "k", "+", "1", "]", "\n", "d", ".", "sort", "(", ")", "\n", "d", "=", "d", "[", ":", ",", "1", ":", "k", "+", "1", "]", "\n", "return", "d", ",", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.adjacency": [[4351, 4378], ["numpy.exp", "numpy.arange().repeat", "idx.reshape", "np.exp.reshape", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix.setdiag", "np.exp.min", "numpy.mean", "scipy.sparse.coo_matrix.T.multiply", "numpy.abs().mean", "type", "numpy.arange", "scipy.sparse.coo_matrix.multiply", "numpy.abs"], "function", ["None"], ["", "def", "adjacency", "(", "dist", ",", "idx", ")", ":", "\n", "\t", "\"\"\"Return the adjacency matrix of a kNN graph.\"\"\"", "\n", "M", ",", "k", "=", "dist", ".", "shape", "\n", "assert", "M", ",", "k", "==", "idx", ".", "shape", "\n", "assert", "dist", ".", "min", "(", ")", ">=", "0", "\n", "\n", "# Weights.", "\n", "sigma2", "=", "np", ".", "mean", "(", "dist", "[", ":", ",", "-", "1", "]", ")", "**", "2", "\n", "dist", "=", "np", ".", "exp", "(", "-", "dist", "**", "2", "/", "sigma2", ")", "\n", "\n", "# Weight matrix.", "\n", "I", "=", "np", ".", "arange", "(", "0", ",", "M", ")", ".", "repeat", "(", "k", ")", "\n", "J", "=", "idx", ".", "reshape", "(", "M", "*", "k", ")", "\n", "V", "=", "dist", ".", "reshape", "(", "M", "*", "k", ")", "\n", "W", "=", "scipy", ".", "sparse", ".", "coo_matrix", "(", "(", "V", ",", "(", "I", ",", "J", ")", ")", ",", "shape", "=", "(", "M", ",", "M", ")", ")", "\n", "\n", "# No self-connections.", "\n", "W", ".", "setdiag", "(", "0", ")", "\n", "\n", "# Non-directed graph.", "\n", "bigger", "=", "W", ".", "T", ">", "W", "\n", "W", "=", "W", "-", "W", ".", "multiply", "(", "bigger", ")", "+", "W", ".", "T", ".", "multiply", "(", "bigger", ")", "\n", "\n", "assert", "W", ".", "nnz", "%", "2", "==", "0", "\n", "assert", "np", ".", "abs", "(", "W", "-", "W", ".", "T", ")", ".", "mean", "(", ")", "<", "1e-10", "\n", "assert", "type", "(", "W", ")", "is", "scipy", ".", "sparse", ".", "csr", ".", "csr_matrix", "\n", "return", "W", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.replace_random_edges": [[4379, 4408], ["int", "numpy.random.randint", "numpy.random.randint", "numpy.random.uniform", "scipy.sparse.triu", "A.tocsr.tolil", "zip", "A.tocsr.setdiag", "A.tocsr.tocsr", "A.tocsr.eliminate_zeros", "numpy.random.permutation", "len", "len", "len", "len"], "function", ["None"], ["", "def", "replace_random_edges", "(", "A", ",", "noise_level", ")", ":", "\n", "\t", "\"\"\"Replace randomly chosen edges by random edges.\"\"\"", "\n", "M", ",", "M", "=", "A", ".", "shape", "\n", "n", "=", "int", "(", "noise_level", "*", "A", ".", "nnz", "//", "2", ")", "\n", "\n", "indices", "=", "np", ".", "random", ".", "permutation", "(", "A", ".", "nnz", "//", "2", ")", "[", ":", "n", "]", "\n", "rows", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "M", ",", "n", ")", "\n", "cols", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "M", ",", "n", ")", "\n", "vals", "=", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ",", "n", ")", "\n", "assert", "len", "(", "indices", ")", "==", "len", "(", "rows", ")", "==", "len", "(", "cols", ")", "==", "len", "(", "vals", ")", "\n", "\n", "A_coo", "=", "scipy", ".", "sparse", ".", "triu", "(", "A", ",", "format", "=", "'coo'", ")", "\n", "assert", "A_coo", ".", "nnz", "==", "A", ".", "nnz", "//", "2", "\n", "assert", "A_coo", ".", "nnz", ">=", "n", "\n", "A", "=", "A", ".", "tolil", "(", ")", "\n", "\n", "for", "idx", ",", "row", ",", "col", ",", "val", "in", "zip", "(", "indices", ",", "rows", ",", "cols", ",", "vals", ")", ":", "\n", "\t\t", "old_row", "=", "A_coo", ".", "row", "[", "idx", "]", "\n", "old_col", "=", "A_coo", ".", "col", "[", "idx", "]", "\n", "\n", "A", "[", "old_row", ",", "old_col", "]", "=", "0", "\n", "A", "[", "old_col", ",", "old_row", "]", "=", "0", "\n", "A", "[", "row", ",", "col", "]", "=", "1", "\n", "A", "[", "col", ",", "row", "]", "=", "1", "\n", "\n", "", "A", ".", "setdiag", "(", "0", ")", "\n", "A", "=", "A", ".", "tocsr", "(", ")", "\n", "A", ".", "eliminate_zeros", "(", ")", "\n", "return", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.miscTools.num2filename": [[18, 47], ["int", "str", "str().replace", "int", "str"], "function", ["None"], ["def", "num2filename", "(", "x", ",", "d", ")", ":", "\n", "    ", "\"\"\"\n    Takes a number and returns a string with the value of the number, but in a\n    format that is writable into a filename.\n\n    s = num2filename(x,d) Gets rid of decimal points which are usually\n        inconvenient to have in a filename.\n        If the number x is an integer, then s = str(int(x)).\n        If the number x is a decimal number, then it replaces the '.' by the\n        character specified by d. Setting d = '' erases the decimal point,\n        setting d = '.' simply returns a string with the exact same number.\n\n    Example:\n        >> num2filename(2,'d')\n        >> '2'\n\n        >> num2filename(3.1415,'d')\n        >> '3d1415'\n\n        >> num2filename(3.1415,'')\n        >> '31415'\n\n        >> num2filename(3.1415,'.')\n        >> '3.1415'\n    \"\"\"", "\n", "if", "x", "==", "int", "(", "x", ")", ":", "\n", "        ", "return", "str", "(", "int", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "str", "(", "x", ")", ".", "replace", "(", "'.'", ",", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.miscTools.saveSeed": [[48, 67], ["os.path.join", "open", "pickle.dump"], "function", ["None"], ["", "", "def", "saveSeed", "(", "randomStates", ",", "saveDir", ")", ":", "\n", "    ", "\"\"\"\n    Takes a list of dictionaries of random generator states of different modules\n    and saves them in a .pkl format.\n    \n    Inputs:\n        randomStates (list): The length of this list is equal to the number of\n            modules whose states want to be saved (torch, numpy, etc.). Each\n            element in this list is a dictionary. The dictionary has three keys:\n            'module' with the name of the module in string format ('numpy' or\n            'torch', for example), 'state' with the saved generator state and,\n            if corresponds, 'seed' with the specific seed for the generator\n            (note that torch has both state and seed, but numpy only has state)\n        saveDir (path): where to save the seed, it will be saved under the \n            filename 'randomSeedUsed.pkl'\n    \"\"\"", "\n", "pathToSeed", "=", "os", ".", "path", ".", "join", "(", "saveDir", ",", "'randomSeedUsed.pkl'", ")", "\n", "with", "open", "(", "pathToSeed", ",", "'wb'", ")", "as", "seedFile", ":", "\n", "        ", "pickle", ".", "dump", "(", "{", "'randomStates'", ":", "randomStates", "}", ",", "seedFile", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.miscTools.loadSeed": [[68, 96], ["os.path.join", "open", "pickle.load", "numpy.random.RandomState().set_state", "torch.set_rng_state", "torch.manual_seed", "numpy.random.RandomState"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load"], ["", "", "def", "loadSeed", "(", "loadDir", ")", ":", "\n", "    ", "\"\"\"\n    Loads the states and seed saved in a specified path\n    \n    Inputs:\n        loadDir (path): where to look for thee seed to load; it is expected that\n            the appropriate file within loadDir is named 'randomSeedUsed.pkl'\n    \n    Obs.: The file 'randomSeedUsed.pkl' should contain a list structured as\n        follows. The length of this list is equal to the number of modules whose\n        states were saved (torch, numpy, etc.). Each element in this list is a\n        dictionary. The dictionary has three keys: 'module' with the name of \n        the module in string format ('numpy' or 'torch', for example), 'state' \n        with the saved generator state and, if corresponds, 'seed' with the \n        specific seed for the generator (note that torch has both state and \n        seed, but numpy only has state)\n    \"\"\"", "\n", "pathToSeed", "=", "os", ".", "path", ".", "join", "(", "loadDir", ",", "'randomSeedUsed.pkl'", ")", "\n", "with", "open", "(", "pathToSeed", ",", "'rb'", ")", "as", "seedFile", ":", "\n", "        ", "randomStates", "=", "pickle", ".", "load", "(", "seedFile", ")", "\n", "randomStates", "=", "randomStates", "[", "'randomStates'", "]", "\n", "", "for", "module", "in", "randomStates", ":", "\n", "        ", "thisModule", "=", "module", "[", "'module'", "]", "\n", "if", "thisModule", "==", "'numpy'", ":", "\n", "            ", "np", ".", "random", ".", "RandomState", "(", ")", ".", "set_state", "(", "module", "[", "'state'", "]", ")", "\n", "", "elif", "thisModule", "==", "'torch'", ":", "\n", "            ", "torch", ".", "set_rng_state", "(", "module", "[", "'state'", "]", ")", "\n", "torch", ".", "manual_seed", "(", "module", "[", "'seed'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.miscTools.writeVarValues": [[98, 112], ["open", "varValues.keys", "file.write", "file.write"], "function", ["None"], ["", "", "", "def", "writeVarValues", "(", "fileToWrite", ",", "varValues", ")", ":", "\n", "    ", "\"\"\"\n    Write the value of several string variables specified by a dictionary into\n    the designated .txt file.\n    \n    Input:\n        fileToWrite (os.path): text file to save the specified variables\n        varValues (dictionary): values to save in the text file. They are\n            saved in the format \"key = value\".\n    \"\"\"", "\n", "with", "open", "(", "fileToWrite", ",", "'a+'", ")", "as", "file", ":", "\n", "        ", "for", "key", "in", "varValues", ".", "keys", "(", ")", ":", "\n", "            ", "file", ".", "write", "(", "'%s = %s\\n'", "%", "(", "key", ",", "varValues", "[", "key", "]", ")", ")", "\n", "", "file", ".", "write", "(", "'\\n'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.__init__": [[12, 23], ["glob.glob", "tensorboardX.SummaryWriter", "os.remove"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "checkpoints_dir", ",", "name", ")", ":", "\n", "        ", "self", ".", "win_size", "=", "256", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "saved", "=", "False", "\n", "self", ".", "checkpoints_dir", "=", "checkpoints_dir", "\n", "self", ".", "ncols", "=", "4", "\n", "\n", "# remove existing", "\n", "for", "filename", "in", "glob", ".", "glob", "(", "self", ".", "checkpoints_dir", "+", "\"/events*\"", ")", ":", "\n", "            ", "os", ".", "remove", "(", "filename", ")", "\n", "", "self", ".", "writer", "=", "SummaryWriter", "(", "checkpoints_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.reset": [[24, 26], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "saved", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.image_summary": [[28, 31], ["torchvision.make_grid", "visualTools.Visualizer.writer.add_image"], "methods", ["None"], ["", "def", "image_summary", "(", "self", ",", "mode", ",", "epoch", ",", "images", ")", ":", "\n", "        ", "images", "=", "vutils", ".", "make_grid", "(", "images", ",", "normalize", "=", "True", ",", "scale_each", "=", "True", ")", "\n", "self", ".", "writer", ".", "add_image", "(", "'{}/Image'", ".", "format", "(", "mode", ")", ",", "images", ",", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.figure_summary": [[33, 35], ["visualTools.Visualizer.writer.add_figure"], "methods", ["None"], ["", "def", "figure_summary", "(", "self", ",", "mode", ",", "epoch", ",", "fig", ")", ":", "\n", "        ", "self", ".", "writer", ".", "add_figure", "(", "'{}/Figure'", ".", "format", "(", "mode", ")", ",", "fig", ",", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.text_summary": [[37, 50], ["enumerate", "operator.itemgetter", "len", "visualTools.Visualizer.writer.add_text", "visualTools.Visualizer.writer.add_text", "el.nonzero().squeeze", "filter", "len", "el.nonzero"], "methods", ["None"], ["", "def", "text_summary", "(", "self", ",", "mode", ",", "epoch", ",", "type", ",", "text", ",", "vocabulary", ",", "gt", "=", "True", ",", "max_length", "=", "20", ")", ":", "\n", "        ", "for", "i", ",", "el", "in", "enumerate", "(", "text", ")", ":", "# text_list", "\n", "            ", "if", "not", "gt", ":", "# we are printing a sample", "\n", "                ", "idx", "=", "el", ".", "nonzero", "(", ")", ".", "squeeze", "(", ")", "+", "1", "\n", "", "else", ":", "\n", "                ", "idx", "=", "el", "# we are printing the ground truth", "\n", "\n", "", "words_list", "=", "itemgetter", "(", "*", "idx", ")", "(", "vocabulary", ")", "\n", "\n", "if", "len", "(", "words_list", ")", "<=", "max_length", ":", "\n", "                ", "self", ".", "writer", ".", "add_text", "(", "'{}/{}_{}_{}'", ".", "format", "(", "mode", ",", "type", ",", "i", ",", "'gt'", "if", "gt", "else", "'prediction'", ")", ",", "', '", ".", "join", "(", "filter", "(", "lambda", "x", ":", "x", "!=", "'<pad>'", ",", "words_list", ")", ")", ",", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "writer", ".", "add_text", "(", "'{}/{}_{}_{}'", ".", "format", "(", "mode", ",", "type", ",", "i", ",", "'gt'", "if", "gt", "else", "'prediction'", ")", ",", "'Number of sampled ingredients is too big: {}'", ".", "format", "(", "len", "(", "words_list", ")", ")", ",", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.scalar_summary": [[52, 57], ["args.items", "visualTools.Visualizer.writer.export_scalars_to_json", "visualTools.Visualizer.writer.add_scalar"], "methods", ["None"], ["", "", "", "def", "scalar_summary", "(", "self", ",", "mode", ",", "epoch", ",", "**", "args", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "args", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "writer", ".", "add_scalar", "(", "'{}/{}'", ".", "format", "(", "mode", ",", "k", ")", ",", "v", ",", "epoch", ")", "\n", "\n", "", "self", ".", "writer", ".", "export_scalars_to_json", "(", "\"{}/tensorboard_all_scalars.json\"", ".", "format", "(", "self", ".", "checkpoints_dir", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.histo_summary": [[58, 63], ["model.named_parameters", "visualTools.Visualizer.writer.add_histogram"], "methods", ["None"], ["", "def", "histo_summary", "(", "self", ",", "model", ",", "step", ")", ":", "\n", "        ", "\"\"\"Log a histogram of the tensor of values.\"\"\"", "\n", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "self", ".", "writer", ".", "add_histogram", "(", "name", ",", "param", ",", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.close": [[64, 66], ["visualTools.Visualizer.writer.close"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.close"], ["", "", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "writer", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.Graph.__init__": [[1236, 1272], ["graphTools.createGraph", "numpy.allclose", "numpy.diag", "int", "numpy.sum", "graphTools.adjacencyToLaplacian", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.triu", "numpy.abs", "numpy.abs", "numpy.diag"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.createGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.adjacencyToLaplacian"], ["def", "__init__", "(", "self", ",", "graphType", ",", "N", ",", "graphOptions", ")", ":", "\n", "        ", "assert", "N", ">", "0", "\n", "#\\\\\\ Create the graph (Outputs adjacency matrix):", "\n", "self", ".", "W", "=", "createGraph", "(", "graphType", ",", "N", ",", "graphOptions", ")", "\n", "# TODO: Let's start easy: make it just an N x N matrix. We'll see later", "\n", "# the rest of the things just as handling multiple features and stuff.", "\n", "#\\\\\\ Number of nodes:", "\n", "self", ".", "N", "=", "(", "self", ".", "W", ")", ".", "shape", "[", "0", "]", "\n", "#\\\\\\ Bool for graph being undirected:", "\n", "self", ".", "undirected", "=", "np", ".", "allclose", "(", "self", ".", "W", ",", "(", "self", ".", "W", ")", ".", "T", ",", "atol", "=", "zeroTolerance", ")", "\n", "#   np.allclose() gives true if matrices W and W.T are the same up to", "\n", "#   atol.", "\n", "#\\\\\\ Bool for graph having self-loops:", "\n", "self", ".", "selfLoops", "=", "True", "if", "np", ".", "sum", "(", "np", ".", "abs", "(", "np", ".", "diag", "(", "self", ".", "W", ")", ")", ">", "zeroTolerance", ")", ">", "0", "else", "False", "\n", "#\\\\\\ Degree matrix:", "\n", "self", ".", "D", "=", "np", ".", "diag", "(", "np", ".", "sum", "(", "self", ".", "W", ",", "axis", "=", "1", ")", ")", "\n", "#\\\\\\ Number of edges:", "\n", "self", ".", "M", "=", "int", "(", "np", ".", "sum", "(", "np", ".", "triu", "(", "self", ".", "W", ")", ")", "if", "self", ".", "undirected", "else", "np", ".", "sum", "(", "self", ".", "W", ")", ")", "\n", "#\\\\\\ Unweighted adjacency:", "\n", "self", ".", "A", "=", "(", "np", ".", "abs", "(", "self", ".", "W", ")", ">", "0", ")", ".", "astype", "(", "self", ".", "W", ".", "dtype", ")", "\n", "#\\\\\\ Laplacian matrix:", "\n", "#   Only if the graph is undirected and has no self-loops", "\n", "if", "self", ".", "undirected", "and", "not", "self", ".", "selfLoops", ":", "\n", "            ", "self", ".", "L", "=", "adjacencyToLaplacian", "(", "self", ".", "W", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "L", "=", "None", "\n", "#\\\\\\ GSO (Graph Shift Operator):", "\n", "#   The weighted adjacency matrix by default", "\n", "", "self", ".", "S", "=", "self", ".", "W", "\n", "#\\\\\\ GFT: Declare variables but do not compute it unless specifically", "\n", "# requested", "\n", "self", ".", "E", "=", "None", "# Eigenvalues", "\n", "self", ".", "V", "=", "None", "# Eigenvectors", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.Graph.computeGFT": [[1273, 1279], ["graphTools.Graph.computeGFT"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeGFT"], ["", "def", "computeGFT", "(", "self", ")", ":", "\n", "# Compute the GFT of the stored GSO", "\n", "        ", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "#\\\\ GFT:", "\n", "#   Compute the eigenvalues (E) and eigenvectors (V)", "\n", "            ", "self", ".", "E", ",", "self", ".", "V", "=", "computeGFT", "(", "self", ".", "S", ",", "order", "=", "'totalVariation'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.Graph.setGSO": [[1280, 1293], ["graphTools.Graph.computeGFT"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeGFT"], ["", "", "def", "setGSO", "(", "self", ",", "S", ",", "GFT", "=", "'no'", ")", ":", "\n", "# This simply sets a matrix as a new GSO. It has to have the same number", "\n", "# of nodes (otherwise, it's a different graph!) and it can or cannot", "\n", "# compute the GFT, depending on the options for GFT", "\n", "        ", "assert", "S", ".", "shape", "[", "0", "]", "==", "S", ".", "shape", "[", "1", "]", "==", "self", ".", "N", "\n", "assert", "GFT", "==", "'no'", "or", "GFT", "==", "'increasing'", "or", "GFT", "==", "'totalVariation'", "\n", "# Set the new GSO", "\n", "self", ".", "S", "=", "S", "\n", "if", "GFT", "==", "'no'", ":", "\n", "            ", "self", ".", "E", "=", "None", "\n", "self", ".", "V", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "E", ",", "self", ".", "V", "=", "computeGFT", "(", "self", ".", "S", ",", "order", "=", "GFT", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.plotGraph": [[52, 136], ["matplotlib.figure", "range", "range", "kwargs.keys", "numpy.linspace", "numpy.array", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "range", "matplotlib.plot", "len", "matplotlib.text", "matplotlib.plot", "numpy.sin", "numpy.cos"], "function", ["None"], ["def", "plotGraph", "(", "adjacencyMatrix", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    plotGraph(A): plots a graph from adjacency matrix A of size N x N\n    \n    Optional keyword arguments:\n        'positions' (np.array, default: points in a circle of radius 1):\n                size N x 2 of positions for each node\n        'figSize' (int, default: 5): size of the figure\n        'linewidth' (int, default: 1): edge width\n        'markerSize' (int, default: 15): node size\n        'markerShape' (string, default: 'o'): node shape\n        'color' (hex code string, default: '#01256E'): color of the nodes\n        'nodeLabel' (list, default: None): list of length N where each element\n            corresponds to the label of each node\n    \"\"\"", "\n", "\n", "# Data", "\n", "#   Adjacency matrix", "\n", "W", "=", "adjacencyMatrix", "\n", "assert", "W", ".", "shape", "[", "0", "]", "==", "W", ".", "shape", "[", "1", "]", "\n", "N", "=", "W", ".", "shape", "[", "0", "]", "\n", "#   Positions (optional)", "\n", "if", "'positions'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "pos", "=", "kwargs", "[", "'positions'", "]", "\n", "", "else", ":", "\n", "        ", "angle", "=", "np", ".", "linspace", "(", "0", ",", "2", "*", "np", ".", "pi", "*", "(", "1", "-", "1", "/", "N", ")", ",", "num", "=", "N", ")", "\n", "radius", "=", "1", "\n", "pos", "=", "np", ".", "array", "(", "[", "\n", "radius", "*", "np", ".", "sin", "(", "angle", ")", ",", "\n", "radius", "*", "np", ".", "cos", "(", "angle", ")", "\n", "]", ")", "\n", "\n", "# Create figure", "\n", "#   Figure size", "\n", "", "if", "'figSize'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "figSize", "=", "kwargs", "[", "'figSize'", "]", "\n", "", "else", ":", "\n", "        ", "figSize", "=", "5", "\n", "#   Line width", "\n", "", "if", "'lineWidth'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "lineWidth", "=", "kwargs", "[", "'lineWidth'", "]", "\n", "", "else", ":", "\n", "        ", "lineWidth", "=", "1", "\n", "#   Marker Size", "\n", "", "if", "'markerSize'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "markerSize", "=", "kwargs", "[", "'markerSize'", "]", "\n", "", "else", ":", "\n", "        ", "markerSize", "=", "15", "\n", "#   Marker shape", "\n", "", "if", "'markerShape'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "markerShape", "=", "kwargs", "[", "'markerShape'", "]", "\n", "", "else", ":", "\n", "        ", "markerShape", "=", "'o'", "\n", "#   Marker color", "\n", "", "if", "'color'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "markerColor", "=", "kwargs", "[", "'color'", "]", "\n", "", "else", ":", "\n", "        ", "markerColor", "=", "'#01256E'", "\n", "#   Node labeling", "\n", "", "if", "'nodeLabel'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "doText", "=", "True", "\n", "nodeLabel", "=", "kwargs", "[", "'nodeLabel'", "]", "\n", "assert", "len", "(", "nodeLabel", ")", "==", "N", "\n", "", "else", ":", "\n", "        ", "doText", "=", "False", "\n", "\n", "# Plot", "\n", "", "figGraph", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "1", "*", "figSize", ",", "1", "*", "figSize", ")", ")", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "N", ")", ":", "\n", "            ", "if", "W", "[", "i", ",", "j", "]", ">", "0", ":", "\n", "                ", "plt", ".", "plot", "(", "[", "pos", "[", "0", ",", "i", "]", ",", "pos", "[", "0", ",", "j", "]", "]", ",", "[", "pos", "[", "1", ",", "i", "]", ",", "pos", "[", "1", ",", "j", "]", "]", ",", "\n", "linewidth", "=", "W", "[", "i", ",", "j", "]", "*", "lineWidth", ",", "\n", "color", "=", "'#A8AAAF'", ")", "\n", "", "", "", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "        ", "plt", ".", "plot", "(", "pos", "[", "0", ",", "i", "]", ",", "pos", "[", "1", ",", "i", "]", ",", "color", "=", "markerColor", ",", "\n", "marker", "=", "markerShape", ",", "markerSize", "=", "markerSize", ")", "\n", "if", "doText", ":", "\n", "            ", "plt", ".", "text", "(", "pos", "[", "0", ",", "i", "]", ",", "pos", "[", "1", ",", "i", "]", ",", "nodeLabel", "[", "i", "]", ",", "\n", "verticalalignment", "=", "'center'", ",", "\n", "horizontalalignment", "=", "'center'", ",", "\n", "color", "=", "'#F2F2F3'", ")", "\n", "\n", "", "", "return", "figGraph", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.printGraph": [[137, 202], ["graphTools.plotGraph", "matplotlib.axis", "plotGraph.savefig", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "matplotlib.xlabel", "matplotlib.yLabel", "matplotlib.legend", "os.path.join"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.plotGraph"], ["", "def", "printGraph", "(", "adjacencyMatrix", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    printGraph(A): Wrapper for plot graph to directly save it as a graph (with \n        no axis, nor anything else like that, more aesthetic, less changes)\n    \n    Optional keyword arguments:\n        'saveDir' (os.path, default: '.'): directory where to save the graph\n        'legend' (default: None): Text for a legend\n        'xLabel' (str, default: None): Text for the x axis\n        'yLabel' (str, default: None): Text for the y axis\n        'graphName' (str, default: 'graph'): name to save the file\n        'positions' (np.array, default: points in a circle of radius 1):\n                size N x 2 of positions for each node\n        'figSize' (int, default: 5): size of the figure\n        'linewidth' (int, default: 1): edge width\n        'markerSize' (int, default: 15): node size\n        'markerShape' (string, default: 'o'): node shape\n        'color' (hex code string, default: '#01256E'): color of the nodes\n        'nodeLabel' (list, default: None): list of length N where each element\n            corresponds to the label of each node\n    \"\"\"", "\n", "\n", "# Wrapper for plot graph to directly save it as a graph (with no axis,", "\n", "# nor anything else like that, more aesthetic, less changes)", "\n", "\n", "W", "=", "adjacencyMatrix", "\n", "assert", "W", ".", "shape", "[", "0", "]", "==", "W", ".", "shape", "[", "1", "]", "\n", "\n", "# Printing options", "\n", "if", "'saveDir'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "saveDir", "=", "kwargs", "[", "'saveDir'", "]", "\n", "", "else", ":", "\n", "        ", "saveDir", "=", "'.'", "\n", "", "if", "'legend'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "doLegend", "=", "True", "\n", "legendText", "=", "kwargs", "[", "'legend'", "]", "\n", "", "else", ":", "\n", "        ", "doLegend", "=", "False", "\n", "", "if", "'xLabel'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "doXlabel", "=", "True", "\n", "xLabelText", "=", "kwargs", "[", "'xLabel'", "]", "\n", "", "else", ":", "\n", "        ", "doXlabel", "=", "False", "\n", "", "if", "'yLabel'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "doYlabel", "=", "True", "\n", "yLabelText", "=", "kwargs", "[", "'yLabel'", "]", "\n", "", "else", ":", "\n", "        ", "doYlabel", "=", "False", "\n", "", "if", "'graphName'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "graphName", "=", "kwargs", "[", "'graphName'", "]", "\n", "", "else", ":", "\n", "        ", "graphName", "=", "'graph'", "\n", "\n", "", "figGraph", "=", "plotGraph", "(", "adjacencyMatrix", ",", "**", "kwargs", ")", "\n", "\n", "plt", ".", "axis", "(", "'off'", ")", "\n", "if", "doXlabel", ":", "\n", "        ", "plt", ".", "xlabel", "(", "xLabelText", ")", "\n", "", "if", "doYlabel", ":", "\n", "        ", "plt", ".", "yLabel", "(", "yLabelText", ")", "\n", "", "if", "doLegend", ":", "\n", "        ", "plt", ".", "legend", "(", "legendText", ")", "\n", "\n", "", "figGraph", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "saveDir", ",", "'%s.pdf'", "%", "graphName", ")", ",", "\n", "bbox_inches", "=", "'tight'", ",", "transparent", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.adjacencyToLaplacian": [[203, 223], ["numpy.sum", "numpy.diag"], "function", ["None"], ["", "def", "adjacencyToLaplacian", "(", "W", ")", ":", "\n", "    ", "\"\"\"\n    adjacencyToLaplacian: Computes the Laplacian from an Adjacency matrix\n\n    Input:\n\n        W (np.array): adjacency matrix\n\n    Output:\n\n        L (np.array): Laplacian matrix\n    \"\"\"", "\n", "# Check that the matrix is square", "\n", "assert", "W", ".", "shape", "[", "0", "]", "==", "W", ".", "shape", "[", "1", "]", "\n", "# Compute the degree vector", "\n", "d", "=", "np", ".", "sum", "(", "W", ",", "axis", "=", "1", ")", "\n", "# And build the degree matrix", "\n", "D", "=", "np", ".", "diag", "(", "d", ")", "\n", "# Return the Laplacian", "\n", "return", "D", "-", "W", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.normalizeAdjacency": [[224, 246], ["numpy.sum", "numpy.diag", "numpy.sqrt"], "function", ["None"], ["", "def", "normalizeAdjacency", "(", "W", ")", ":", "\n", "    ", "\"\"\"\n    NormalizeAdjacency: Computes the degree-normalized adjacency matrix\n\n    Input:\n\n        W (np.array): adjacency matrix\n\n    Output:\n\n        A (np.array): degree-normalized adjacency matrix\n    \"\"\"", "\n", "# Check that the matrix is square", "\n", "assert", "W", ".", "shape", "[", "0", "]", "==", "W", ".", "shape", "[", "1", "]", "\n", "# Compute the degree vector", "\n", "d", "=", "np", ".", "sum", "(", "W", ",", "axis", "=", "1", ")", "\n", "# Invert the square root of the degree", "\n", "d", "=", "1", "/", "np", ".", "sqrt", "(", "d", ")", "\n", "# And build the square root inverse degree matrix", "\n", "D", "=", "np", ".", "diag", "(", "d", ")", "\n", "# Return the Normalized Adjacency", "\n", "return", "D", "@", "W", "@", "D", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.normalizeLaplacian": [[247, 269], ["numpy.diag", "numpy.diag", "numpy.sqrt"], "function", ["None"], ["", "def", "normalizeLaplacian", "(", "L", ")", ":", "\n", "    ", "\"\"\"\n    NormalizeLaplacian: Computes the degree-normalized Laplacian matrix\n\n    Input:\n\n        L (np.array): Laplacian matrix\n\n    Output:\n\n        normL (np.array): degree-normalized Laplacian matrix\n    \"\"\"", "\n", "# Check that the matrix is square", "\n", "assert", "L", ".", "shape", "[", "0", "]", "==", "L", ".", "shape", "[", "1", "]", "\n", "# Compute the degree vector (diagonal elements of L)", "\n", "d", "=", "np", ".", "diag", "(", "L", ")", "\n", "# Invert the square root of the degree", "\n", "d", "=", "1", "/", "np", ".", "sqrt", "(", "d", ")", "\n", "# And build the square root inverse degree matrix", "\n", "D", "=", "np", ".", "diag", "(", "d", ")", "\n", "# Return the Normalized Laplacian", "\n", "return", "D", "@", "L", "@", "D", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeGFT": [[270, 310], ["numpy.allclose", "numpy.diag", "numpy.linalg.eigh", "numpy.linalg.eig", "numpy.max", "numpy.argsort", "numpy.abs", "numpy.argsort", "numpy.arange", "numpy.abs"], "function", ["None"], ["", "def", "computeGFT", "(", "S", ",", "order", "=", "'no'", ")", ":", "\n", "    ", "\"\"\"\n    computeGFT: Computes the frequency basis (eigenvectors) and frequency\n        coefficients (eigenvalues) of a given GSO\n\n    Input:\n\n        S (np.array): graph shift operator matrix\n        order (string): 'no', 'increasing', 'totalVariation' chosen order of\n            frequency coefficients (default: 'no')\n\n    Output:\n\n        E (np.array): diagonal matrix with the frequency coefficients\n            (eigenvalues) in the diagonal\n        V (np.array): matrix with frequency basis (eigenvectors)\n    \"\"\"", "\n", "# Check the correct order input", "\n", "assert", "order", "==", "'totalVariation'", "or", "order", "==", "'no'", "or", "order", "==", "'increasing'", "\n", "# Check the matrix is square", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "S", ".", "shape", "[", "1", "]", "\n", "# Check if it is symmetric", "\n", "symmetric", "=", "np", ".", "allclose", "(", "S", ",", "S", ".", "T", ",", "atol", "=", "zeroTolerance", ")", "\n", "# Then, compute eigenvalues and eigenvectors", "\n", "if", "symmetric", ":", "\n", "        ", "e", ",", "V", "=", "np", ".", "linalg", ".", "eigh", "(", "S", ")", "\n", "", "else", ":", "\n", "        ", "e", ",", "V", "=", "np", ".", "linalg", ".", "eig", "(", "S", ")", "\n", "# Sort the eigenvalues by the desired error:", "\n", "", "if", "order", "==", "'totalVariation'", ":", "\n", "        ", "eMax", "=", "np", ".", "max", "(", "e", ")", "\n", "sortIndex", "=", "np", ".", "argsort", "(", "np", ".", "abs", "(", "e", "-", "eMax", ")", ")", "\n", "", "elif", "order", "==", "'increasing'", ":", "\n", "        ", "sortIndex", "=", "np", ".", "argsort", "(", "np", ".", "abs", "(", "e", ")", ")", "\n", "", "else", ":", "\n", "        ", "sortIndex", "=", "np", ".", "arange", "(", "0", ",", "S", ".", "shape", "[", "0", "]", ")", "\n", "", "e", "=", "e", "[", "sortIndex", "]", "\n", "V", "=", "V", "[", ":", ",", "sortIndex", "]", "\n", "E", "=", "np", ".", "diag", "(", "e", ")", "\n", "return", "E", ",", "V", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.matrixPowers": [[311, 348], ["numpy.tile", "np.tile.reshape", "range", "len", "S.reshape.reshape", "numpy.eye().reshape", "numpy.concatenate", "SK.reshape.reshape", "len", "numpy.eye", "np.tile.reshape"], "function", ["None"], ["", "def", "matrixPowers", "(", "S", ",", "K", ")", ":", "\n", "    ", "\"\"\"\n    matrixPowers(A, K) Computes the matrix powers A^k for k = 0, ..., K-1\n\n    Inputs:\n        A: either a single N x N matrix or a collection E x N x N of E matrices.\n        K: integer, maximum power to be computed (up to K-1)\n\n    Outputs:\n        AK: either a collection of K matrices K x N x N (if the input was a\n            single matrix) or a collection E x K x N x N (if the input was a\n            collection of E matrices).\n    \"\"\"", "\n", "# S can be either a single GSO (N x N) or a collection of GSOs (E x N x N)", "\n", "if", "len", "(", "S", ".", "shape", ")", "==", "2", ":", "\n", "        ", "N", "=", "S", ".", "shape", "[", "0", "]", "\n", "assert", "S", ".", "shape", "[", "1", "]", "==", "N", "\n", "E", "=", "1", "\n", "S", "=", "S", ".", "reshape", "(", "1", ",", "N", ",", "N", ")", "\n", "scalarWeights", "=", "True", "\n", "", "elif", "len", "(", "S", ".", "shape", ")", "==", "3", ":", "\n", "        ", "E", "=", "S", ".", "shape", "[", "0", "]", "\n", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "scalarWeights", "=", "False", "\n", "\n", "# Now, let's build the powers of S:", "\n", "", "thisSK", "=", "np", ".", "tile", "(", "np", ".", "eye", "(", "N", ",", "N", ")", ".", "reshape", "(", "1", ",", "N", ",", "N", ")", ",", "[", "E", ",", "1", ",", "1", "]", ")", "\n", "SK", "=", "thisSK", ".", "reshape", "(", "E", ",", "1", ",", "N", ",", "N", ")", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "        ", "thisSK", "=", "thisSK", "@", "S", "\n", "SK", "=", "np", ".", "concatenate", "(", "(", "SK", ",", "thisSK", ".", "reshape", "(", "E", ",", "1", ",", "N", ",", "N", ")", ")", ",", "axis", "=", "1", ")", "\n", "# Take out the first dimension if it was a single GSO", "\n", "", "if", "scalarWeights", ":", "\n", "        ", "SK", "=", "SK", ".", "reshape", "(", "K", ",", "N", ",", "N", ")", "\n", "\n", "", "return", "SK", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeNonzeroRows": [[349, 377], ["range", "numpy.flatnonzero"], "function", ["None"], ["", "def", "computeNonzeroRows", "(", "S", ",", "Nl", "=", "'all'", ")", ":", "\n", "    ", "\"\"\"\n    computeNonzeroRows: Find the position of the nonzero elements of each\n        row of a matrix\n\n    Input:\n\n        S (np.array): matrix\n        Nl (int or 'all'): number of rows to compute the nonzero elements; if\n            'all', then Nl = S.shape[0]. Rows are counted from the top.\n\n    Output:\n\n        nonzeroElements (list): list of size Nl where each element is an array\n            of the indices of the nonzero elements of the corresponding row.\n    \"\"\"", "\n", "# Find the position of the nonzero elements of each row of the matrix S.", "\n", "# Nl = 'all' means for all rows, otherwise, it will be an int.", "\n", "if", "Nl", "==", "'all'", ":", "\n", "        ", "Nl", "=", "S", ".", "shape", "[", "0", "]", "\n", "", "assert", "Nl", "<=", "S", ".", "shape", "[", "0", "]", "\n", "# Save neighborhood variable", "\n", "neighborhood", "=", "[", "]", "\n", "# For each of the selected nodes", "\n", "for", "n", "in", "range", "(", "Nl", ")", ":", "\n", "        ", "neighborhood", "+=", "[", "np", ".", "flatnonzero", "(", "S", "[", "n", ",", ":", "]", ")", "]", "\n", "\n", "", "return", "neighborhood", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeNeighborhood": [[378, 528], ["isinstance", "range", "len", "list", "range", "allNeighbors.copy", "range", "neighbors[].copy", "max", "range", "numpy.array", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix", "len", "numpy.sum", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix", "range", "range", "scipy.sparse.coo_matrix.nonzero", "len", "allNeighbors[].append", "list", "range", "len", "len", "len", "len", "numpy.abs", "range", "set", "range", "neighbors[].extend", "list", "numpy.concatenate", "set", "newNeighbors.extend", "visitedNodes[].append", "numpy.abs", "numpy.ones"], "function", ["None"], ["", "def", "computeNeighborhood", "(", "S", ",", "K", ",", "N", "=", "'all'", ",", "nb", "=", "'all'", ",", "outputType", "=", "'list'", ")", ":", "\n", "    ", "\"\"\"\n    computeNeighborhood: compute the set of nodes within the K-hop neighborhood\n        of a graph (i.e. all nodes that can be reached within K-hops of each\n        node)\n\n        computeNeighborhood(W, K, N = 'all', nb = 'all', outputType = 'list')\n\n    Input:\n        W (np.array): adjacency matrix\n        K (int): K-hop neighborhood to compute the neighbors\n        N (int or 'all'): how many nodes (from top) to compute the neighbors\n            from (default: 'all').\n        nb (int or 'all'): how many nodes to consider valid when computing the\n            neighborhood (i.e. nodes beyond nb are not trimmed out of the\n            neighborhood; note that nodes smaller than nb that can be reached\n            by nodes greater than nb, are included. default: 'all')\n        outputType ('list' or 'matrix'): choose if the output is given in the\n            form of a list of arrays, or a matrix with zero-padding of neighbors\n            with neighborhoods smaller than the maximum neighborhood\n            (default: 'list')\n\n    Output:\n        neighborhood (np.array or list): contains the indices of the neighboring\n            nodes following the order established by the adjacency matrix.\n    \"\"\"", "\n", "# outputType is either a list (a list of np.arrays) or a matrix.", "\n", "assert", "outputType", "==", "'list'", "or", "outputType", "==", "'matrix'", "\n", "# Here, we can assume S is already sparse, in which case is a list of", "\n", "# sparse matrices, or that S is full, in which case it is a 3-D array.", "\n", "if", "isinstance", "(", "S", ",", "list", ")", ":", "\n", "# If it is a list, it has to be a list of matrices, where the length", "\n", "# of the list has to be the number of edge weights. But we actually need", "\n", "# to sum over all edges to be sure we consider all reachable nodes on", "\n", "# at least one of the edge dimensions", "\n", "        ", "newS", "=", "0.", "\n", "for", "e", "in", "len", "(", "S", ")", ":", "\n", "# First check it's a matrix, and a square one", "\n", "            ", "assert", "len", "(", "S", "[", "e", "]", ")", "==", "2", "\n", "assert", "S", "[", "e", "]", ".", "shape", "[", "0", "]", "==", "S", "[", "e", "]", ".", "shape", "[", "1", "]", "\n", "# For each edge, convert to sparse (in COO because we care about", "\n", "# coordinates to find the neighborhoods)", "\n", "newS", "+=", "scipy", ".", "sparse", ".", "coo_matrix", "(", "\n", "(", "np", ".", "abs", "(", "S", "[", "e", "]", ")", ">", "zeroTolerance", ")", ".", "astype", "(", "S", "[", "e", "]", ".", "dtype", ")", ")", "\n", "", "S", "=", "(", "newS", ">", "zeroTolerance", ")", ".", "astype", "(", "newS", ".", "dtype", ")", "\n", "", "else", ":", "\n", "# if S is not a list, check that it is either a E x N x N or a N x N", "\n", "# array.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "2", "or", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "S", ".", "shape", ")", "==", "3", ":", "\n", "            ", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "\n", "# If it has an edge feature dimension, just add over that dimension.", "\n", "# We only need one non-zero value along the vector to have an edge", "\n", "# there. (Obs.: While normally assume that all weights are positive,", "\n", "# let's just add on abs() value to avoid any cancellations).", "\n", "S", "=", "np", ".", "sum", "(", "np", ".", "abs", "(", "S", ")", ",", "axis", "=", "0", ")", "\n", "S", "=", "scipy", ".", "sparse", ".", "coo_matrix", "(", "(", "S", ">", "zeroTolerance", ")", ".", "astype", "(", "S", ".", "dtype", ")", ")", "\n", "", "else", ":", "\n", "# In this case, if it is a 2-D array, we do not need to add over the", "\n", "# edge dimension, so we just sparsify it", "\n", "            ", "assert", "S", ".", "shape", "[", "0", "]", "==", "S", ".", "shape", "[", "1", "]", "\n", "S", "=", "scipy", ".", "sparse", ".", "coo_matrix", "(", "(", "S", ">", "zeroTolerance", ")", ".", "astype", "(", "S", ".", "dtype", ")", ")", "\n", "# Now, we finally have a sparse, binary matrix, with the connections.", "\n", "# Now check that K and N are correct inputs.", "\n", "# K is an int (target K-hop neighborhood)", "\n", "# N is either 'all' or an int determining how many rows", "\n", "", "", "assert", "K", ">=", "0", "# K = 0 is just the identity", "\n", "# Check how many nodes we want to obtain", "\n", "if", "N", "==", "'all'", ":", "\n", "        ", "N", "=", "S", ".", "shape", "[", "0", "]", "\n", "", "if", "nb", "==", "'all'", ":", "\n", "        ", "nb", "=", "S", ".", "shape", "[", "0", "]", "\n", "", "assert", "N", ">=", "0", "and", "N", "<=", "S", ".", "shape", "[", "0", "]", "# Cannot return more nodes than there are", "\n", "assert", "nb", ">=", "0", "and", "nb", "<=", "S", ".", "shape", "[", "0", "]", "\n", "\n", "# All nodes are in their own neighborhood, so", "\n", "allNeighbors", "=", "[", "[", "n", "]", "for", "n", "in", "range", "(", "S", ".", "shape", "[", "0", "]", ")", "]", "\n", "# Now, if K = 0, then these are all the neighborhoods we need.", "\n", "# And also keep track only about the nodes we care about", "\n", "neighbors", "=", "[", "[", "n", "]", "for", "n", "in", "range", "(", "N", ")", "]", "\n", "# But if K > 0", "\n", "if", "K", ">", "0", ":", "\n", "# Let's start with the one-hop neighborhood of all nodes (we need this)", "\n", "        ", "nonzeroS", "=", "list", "(", "S", ".", "nonzero", "(", ")", ")", "\n", "# This is a tuple with two arrays, the first one containing the row", "\n", "# index of the nonzero elements, and the second one containing the", "\n", "# column index of the nonzero elements.", "\n", "# Now, we want the one-hop neighborhood of all nodes (and all nodes have", "\n", "# a one-hop neighborhood, since the graphs are connected)", "\n", "for", "n", "in", "range", "(", "len", "(", "nonzeroS", "[", "0", "]", ")", ")", ":", "\n", "# The list in index 0 is the nodes, the list in index 1 is the", "\n", "# corresponding neighbor", "\n", "            ", "allNeighbors", "[", "nonzeroS", "[", "0", "]", "[", "n", "]", "]", ".", "append", "(", "nonzeroS", "[", "1", "]", "[", "n", "]", ")", "\n", "# Now that we have the one-hop neighbors, we just need to do a depth", "\n", "# first search looking for the one-hop neighborhood of each neighbor", "\n", "# and so on.", "\n", "", "oneHopNeighbors", "=", "allNeighbors", ".", "copy", "(", ")", "\n", "# We have already visited the nodes themselves, since we already", "\n", "# gathered the one-hop neighbors.", "\n", "visitedNodes", "=", "[", "[", "n", "]", "for", "n", "in", "range", "(", "N", ")", "]", "\n", "# Keep only the one-hop neighborhood of the ones we're interested in", "\n", "neighbors", "=", "[", "list", "(", "set", "(", "allNeighbors", "[", "n", "]", ")", ")", "for", "n", "in", "range", "(", "N", ")", "]", "\n", "# For each hop", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "# For each of the nodes we care about", "\n", "            ", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "# Store the new neighbors to be included for node i", "\n", "                ", "newNeighbors", "=", "[", "]", "\n", "# Take each of the neighbors we already have", "\n", "for", "j", "in", "neighbors", "[", "i", "]", ":", "\n", "# and if we haven't visited those neighbors yet", "\n", "                    ", "if", "j", "not", "in", "visitedNodes", "[", "i", "]", ":", "\n", "# Just look for our neighbor's one-hop neighbors and", "\n", "# add them to the neighborhood list", "\n", "                        ", "newNeighbors", ".", "extend", "(", "oneHopNeighbors", "[", "j", "]", ")", "\n", "# And don't forget to add the node to the visited ones", "\n", "# (we already have its one-hope neighborhood)", "\n", "visitedNodes", "[", "i", "]", ".", "append", "(", "j", ")", "\n", "# And now that we have added all the new neighbors, we add them", "\n", "# to the old neighbors", "\n", "", "", "neighbors", "[", "i", "]", ".", "extend", "(", "newNeighbors", ")", "\n", "# And get rid of those that appear more than once", "\n", "neighbors", "[", "i", "]", "=", "list", "(", "set", "(", "neighbors", "[", "i", "]", ")", ")", "\n", "\n", "# Now that all nodes have been collected, get rid of those beyond nb", "\n", "", "", "", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "# Get the neighborhood", "\n", "        ", "thisNeighborhood", "=", "neighbors", "[", "i", "]", ".", "copy", "(", ")", "\n", "# And get rid of the excess nodes", "\n", "neighbors", "[", "i", "]", "=", "[", "j", "for", "j", "in", "thisNeighborhood", "if", "j", "<", "nb", "]", "\n", "\n", "\n", "", "if", "outputType", "==", "'matrix'", ":", "\n", "# List containing all the neighborhood sizes", "\n", "        ", "neighborhoodSizes", "=", "[", "len", "(", "x", ")", "for", "x", "in", "neighbors", "]", "\n", "# Obtain max number of neighbors", "\n", "maxNeighborhoodSize", "=", "max", "(", "neighborhoodSizes", ")", "\n", "# then we have to check each neighborhood and find if we need to add", "\n", "# more nodes (itself) to pad it so we can build a matrix", "\n", "paddedNeighbors", "=", "[", "]", "\n", "for", "n", "in", "range", "(", "N", ")", ":", "\n", "            ", "paddedNeighbors", "+=", "[", "np", ".", "concatenate", "(", "\n", "(", "neighbors", "[", "n", "]", ",", "\n", "n", "*", "np", ".", "ones", "(", "maxNeighborhoodSize", "-", "neighborhoodSizes", "[", "n", "]", ")", ")", "\n", ")", "]", "\n", "# And now that every element in the list paddedNeighbors has the same", "\n", "# length, we can make it a matrix", "\n", "", "neighbors", "=", "np", ".", "array", "(", "paddedNeighbors", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "", "return", "neighbors", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeSourceNodes": [[529, 559], ["numpy.sum", "sklearn.cluster.SpectralClustering", "communityClusters.fit.fit", "range", "numpy.argsort", "numpy.nonzero"], "function", ["None"], ["", "def", "computeSourceNodes", "(", "A", ",", "C", ")", ":", "\n", "    ", "\"\"\"\n    computeSourceNodes: compute source nodes for the source localization problem\n    \n    Input:\n        A (np.array): adjacency matrix of shape N x N\n        C (int): number of classes\n        \n    Output:\n        sourceNodes (list): contains the indices of the C source nodes\n        \n    Uses the adjacency matrix to compute C communities by means of spectral \n    clustering, and then selects the node with largest degree within each \n    community\n    \"\"\"", "\n", "sourceNodes", "=", "[", "]", "\n", "degree", "=", "np", ".", "sum", "(", "A", ",", "axis", "=", "0", ")", "# degree of each vector", "\n", "# Compute communities", "\n", "communityClusters", "=", "SpectralClustering", "(", "n_clusters", "=", "C", ",", "\n", "affinity", "=", "'precomputed'", ",", "\n", "assign_labels", "=", "'discretize'", ")", "\n", "communityClusters", "=", "communityClusters", ".", "fit", "(", "A", ")", "\n", "communityLabels", "=", "communityClusters", ".", "labels_", "\n", "# For each community", "\n", "for", "c", "in", "range", "(", "C", ")", ":", "\n", "        ", "communityNodes", "=", "np", ".", "nonzero", "(", "communityLabels", "==", "c", ")", "[", "0", "]", "\n", "degreeSorted", "=", "np", ".", "argsort", "(", "degree", "[", "communityNodes", "]", ")", "\n", "sourceNodes", "=", "sourceNodes", "+", "[", "communityNodes", "[", "degreeSorted", "[", "-", "1", "]", "]", "]", "\n", "\n", "", "return", "sourceNodes", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected": [[562, 590], ["numpy.allclose", "graphTools.adjacencyToLaplacian", "graphTools.computeGFT", "numpy.diag", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.adjacencyToLaplacian", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeGFT"], ["", "def", "isConnected", "(", "W", ")", ":", "\n", "    ", "\"\"\"\n    isConnected: determine if a graph is connected\n\n    Input:\n        W (np.array): adjacency matrix\n\n    Output:\n        connected (bool): True if the graph is connected, False otherwise\n    \n    Obs.: If the graph is directed, we consider it is connected when there is\n    at least one edge that would make it connected (i.e. if we drop the \n    direction of all edges, and just keep them as undirected, then the resulting\n    graph would be connected).\n    \"\"\"", "\n", "undirected", "=", "np", ".", "allclose", "(", "W", ",", "W", ".", "T", ",", "atol", "=", "zeroTolerance", ")", "\n", "if", "not", "undirected", ":", "\n", "        ", "W", "=", "0.5", "*", "(", "W", "+", "W", ".", "T", ")", "\n", "", "L", "=", "adjacencyToLaplacian", "(", "W", ")", "\n", "E", ",", "V", "=", "computeGFT", "(", "L", ")", "\n", "e", "=", "np", ".", "diag", "(", "E", ")", "# only eigenvavlues", "\n", "# Check how many values are greater than zero:", "\n", "nComponents", "=", "np", ".", "sum", "(", "e", "<", "zeroTolerance", ")", "# Number of connected components", "\n", "if", "nComponents", "==", "1", ":", "\n", "        ", "connected", "=", "True", "\n", "", "else", ":", "\n", "        ", "connected", "=", "False", "\n", "", "return", "connected", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.sparsifyGraph": [[591, 681], ["graphTools.isConnected", "numpy.allclose", "W.copy", "graphTools.isConnected", "numpy.sort", "W.copy", "graphTools.isConnected", "graphTools.isConnected", "numpy.abs", "graphTools.isConnected", "kthLargest.reshape", "numpy.abs", "kthLargest.reshape"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected"], ["", "def", "sparsifyGraph", "(", "W", ",", "sparsificationType", ",", "p", ")", ":", "\n", "    ", "\"\"\"\n    sparsifyGraph: sparsifies a given graph matrix\n    \n    Input:\n        W (np.array): adjacency matrix\n        sparsificationType ('threshold' or 'NN'): threshold or nearest-neighbor\n        sparsificationParameter (float): sparsification parameter (value of the\n            threshold under which edges are deleted or the number of NN to keep)\n        \n    Output:\n        W (np.array): adjacency matrix of sparsified graph\n    \n    Observation:\n        - If it is an undirected graph, when computing the kNN edges, the\n    resulting graph might be directed. Then, the graph is converted into an\n    undirected one by taking the average of incoming and outgoing edges (this\n    might result in a graph where some nodes have more than kNN neighbors).\n        - If it is a directed graph, remember that element (i,j) of the \n    adjacency matrix corresponds to edge (j,i). This means that each row of the\n    matrix has nonzero elements on all the incoming edges. In the directed case,\n    the number of nearest neighbors is with respect to the incoming edges (i.e.\n    kNN incoming edges are kept).\n        - If the original graph is connected, then thresholding might\n    lead to a disconnected graph. If this is the case, the threshold will be\n    increased in small increments until the resulting graph is connected.\n    To recover the actual treshold used (higher than the one specified) do\n    np.min(W[np.nonzero(W)]). In the case of kNN, if the resulting graph is\n    disconnected, the parameter k is increased in 1 until the resultin graph\n    is connected.\n    \"\"\"", "\n", "# Check input arguments", "\n", "N", "=", "W", ".", "shape", "[", "0", "]", "\n", "assert", "W", ".", "shape", "[", "1", "]", "==", "N", "\n", "assert", "sparsificationType", "==", "'threshold'", "or", "sparsificationType", "==", "'NN'", "\n", "\n", "connected", "=", "isConnected", "(", "W", ")", "\n", "undirected", "=", "np", ".", "allclose", "(", "W", ",", "W", ".", "T", ",", "atol", "=", "zeroTolerance", ")", "\n", "#   np.allclose() gives true if matrices W and W.T are the same up to", "\n", "#   atol.", "\n", "\n", "# Start with thresholding", "\n", "if", "sparsificationType", "==", "'threshold'", ":", "\n", "        ", "Wnew", "=", "W", ".", "copy", "(", ")", "\n", "Wnew", "[", "np", ".", "abs", "(", "Wnew", ")", "<", "p", "]", "=", "0.", "\n", "# If the original graph was connected, we need to be sure this one is", "\n", "# connected as well", "\n", "if", "connected", ":", "\n", "# Check if the new graph is connected", "\n", "            ", "newGraphIsConnected", "=", "isConnected", "(", "Wnew", ")", "\n", "# While it is not connected", "\n", "while", "not", "newGraphIsConnected", ":", "\n", "# We need to reduce the size of p until we get it connected", "\n", "                ", "p", "=", "p", "/", "2.", "\n", "Wnew", "=", "W", ".", "copy", "(", ")", "\n", "Wnew", "[", "np", ".", "abs", "(", "Wnew", ")", "<", "p", "]", "=", "0.", "\n", "# Check if it is connected now", "\n", "newGraphIsConnected", "=", "isConnected", "(", "Wnew", ")", "\n", "# Now, let's move to k nearest neighbors", "\n", "", "", "", "elif", "sparsificationType", "==", "'NN'", ":", "\n", "# We sort the values of each row (in increasing order)", "\n", "        ", "Wsorted", "=", "np", ".", "sort", "(", "W", ",", "axis", "=", "1", ")", "\n", "# Pick the kth largest", "\n", "kthLargest", "=", "Wsorted", "[", ":", ",", "-", "p", "]", "# array of size N", "\n", "# Find the elements that are greater or equal that these values", "\n", "maskOfEdgesToKeep", "=", "(", "W", ">=", "kthLargest", ".", "reshape", "(", "[", "N", ",", "1", "]", ")", ")", ".", "astype", "(", "W", ".", "dtype", ")", "\n", "# And keep those edges", "\n", "Wnew", "=", "W", "*", "maskOfEdgesToKeep", "\n", "# If the original graph was connected", "\n", "if", "connected", ":", "\n", "# Check if the new graph is connected", "\n", "            ", "newGraphIsConnected", "=", "isConnected", "(", "Wnew", ")", "\n", "# While it is not connected", "\n", "while", "not", "newGraphIsConnected", ":", "\n", "# Increase the number of k-NN by 1", "\n", "                ", "p", "=", "p", "+", "1", "\n", "# Compute the new kth Largest", "\n", "kthLargest", "=", "Wsorted", "[", ":", ",", "-", "p", "]", "# array of size N", "\n", "# Find the elements that are greater or equal that these values", "\n", "maskOfEdgesToKeep", "=", "(", "W", ">=", "kthLargest", ".", "reshape", "(", "[", "N", ",", "1", "]", ")", ")", ".", "astype", "(", "W", ".", "dtype", ")", "\n", "# And keep those edges", "\n", "Wnew", "=", "W", "*", "maskOfEdgesToKeep", "\n", "# Check if it is connected now", "\n", "newGraphIsConnected", "=", "isConnected", "(", "Wnew", ")", "\n", "# if it's undirected, this is the moment to reconvert it as undirected", "\n", "", "", "if", "undirected", ":", "\n", "            ", "Wnew", "=", "0.5", "*", "(", "Wnew", "+", "Wnew", ".", "T", ")", "\n", "\n", "", "", "return", "Wnew", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.createGraph": [[682, 987], ["numpy.zeros", "range", "len", "int", "sum", "numpy.cumsum().tolist", "numpy.ones", "numpy.random.rand", "numpy.triu", "graphTools.isConnected", "numpy.cos().reshape", "numpy.sin().reshape", "numpy.concatenate", "numpy.zeros", "graphOptions.keys", "scipy.distance.squareform", "range", "range", "numpy.triu", "graphTools.isConnected", "numpy.arange", "nodeList.extend", "numpy.cumsum", "numpy.cos", "numpy.sin", "scipy.distance.pdist", "numpy.random.binomial", "numpy.concatenate", "numpy.argsort", "range", "len", "len", "len", "numpy.sum", "numpy.sum().reshape", "numpy.sum", "numpy.nonzero", "graphTools.isConnected", "eachNodeList.pop.tolist", "numpy.nonzero", "len", "numpy.random.permutation().astype", "numpy.nonzero", "len", "graphOptions.keys", "graphOptions.keys", "numpy.mean", "numpy.tile", "numpy.sum().reshape", "numpy.abs", "len", "scipy.sparse.csgraph.connected_components", "scipy.sparse.csgraph.connected_components", "numpy.arange", "range", "eachAdjacency.pop", "eachNodeList.pop", "graphTools.isConnected", "graphOptions.keys", "numpy.arange", "numpy.arange", "numpy.random.rand", "numpy.floor().astype", "numpy.sum", "numpy.tile", "len", "extraComponents.append", "extraComponents.append", "numpy.arange", "numpy.arange", "numpy.random.permutation", "numpy.nonzero", "numpy.abs", "numpy.sum", "len", "numpy.floor", "numpy.abs", "numpy.random.rand", "len", "numpy.min", "numpy.min", "numpy.min"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.isConnected", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "createGraph", "(", "graphType", ",", "N", ",", "graphOptions", ")", ":", "\n", "    ", "\"\"\"\n    createGraph: creates a graph of a specified type\n    \n    Input:\n        graphType (string): 'SBM', 'SmallWorld', 'fuseEdges', and 'adjacency'\n        N (int): Number of nodes\n        graphOptions (dict): Depends on the type selected.\n        Obs.: More types to come.\n        \n    Output:\n        W (np.array): adjacency matrix of shape N x N\n    \n    Optional inputs (by keyword):\n        graphType: 'SBM'\n            'nCommunities': (int) number of communities\n            'probIntra': (float) probability of drawing an edge between nodes\n                inside the same community\n            'probInter': (float) probability of drawing an edge between nodes\n                of different communities\n            Obs.: This always results in a connected graph.\n        graphType: 'SmallWorld'\n            'probEdge': probability of drawing an edge between nodes\n            'probRewiring': probability of rewiring an edge\n            Obs.: This always results in a connected graph.\n        graphType: 'fuseEdges'\n            (Given a collection of adjacency matrices of graphs with the same\n            number of nodes, this graph type is a fusion of the edges of the \n            collection of graphs, following different desirable properties)\n            'adjacencyMatrices' (np.array): collection of matrices in a tensor\n                np.array of dimension nGraphs x N x N\n            'aggregationType' ('sum' or 'avg'): if 'sum', edges are summed\n                across the collection of matrices, if 'avg' they are averaged\n            'normalizationType' ('rows', 'cols' or 'no'): if 'rows', the values\n                of the rows (after aggregated) are normalized to sum to one, if\n                'cols', it is for the columns, if it is 'no' there is no \n                normalization.\n            'isolatedNodes' (bool): if True, keep isolated nodes should there\n                be any\n            'forceUndirected' (bool): if True, make the resulting graph \n                undirected by replacing directed edges by the average of the \n                outgoing and incoming edges between each pair of nodes\n            'forceConnected' (bool): if True, make the graph connected by taking\n                the largest connected component\n            'nodeList' (list): this is an empty list that, after calling the\n                function, will contain a list of the nodes that were kept when\n                creating the adjacency matrix out of fusing the given ones with\n                the desired options\n            'extraComponents' (list, optional): if the resulting fused adjacency\n                matrix is not connected, and then forceConnected = True, then\n                this list will contain two lists, the first one with the \n                adjacency matrices of the smaller connected components, and\n                the second one a corresponding list with the index of the nodes\n                that were kept for each of the smaller connected components\n            (Obs.: If a given single graph is required to be adapted with any\n            of the options in this function, then it can just be expanded to\n            have one dimension along axis = 0 and fed to this function to\n            obtain the corresponding graph with the desired properties)\n        graphType: 'adjacency'\n            'adjacencyMatrix' (np.array): just return the given adjacency\n                matrix (after checking it has N nodes)\n    \"\"\"", "\n", "# Check", "\n", "assert", "N", ">=", "0", "\n", "\n", "if", "graphType", "==", "'SBM'", ":", "\n", "        ", "assert", "(", "len", "(", "graphOptions", ".", "keys", "(", ")", ")", ")", "==", "3", "\n", "C", "=", "graphOptions", "[", "'nCommunities'", "]", "# Number of communities", "\n", "assert", "int", "(", "C", ")", "==", "C", "# Check that the number of communities is an integer", "\n", "pii", "=", "graphOptions", "[", "'probIntra'", "]", "# Intracommunity probability", "\n", "pij", "=", "graphOptions", "[", "'probInter'", "]", "# Intercommunity probability", "\n", "assert", "0", "<=", "pii", "<=", "1", "# Check that they are valid probabilities", "\n", "assert", "0", "<=", "pij", "<=", "1", "\n", "# We create the SBM as follows: we generate random numbers between", "\n", "# 0 and 1 and then we compare them elementwise to a matrix of the", "\n", "# same size of pii and pij to set some of them to one and other to", "\n", "# zero.", "\n", "# Let's start by creating the matrix of pii and pij.", "\n", "# First, we need to know how many numbers on each community.", "\n", "nNodesC", "=", "[", "N", "//", "C", "]", "*", "C", "# Number of nodes per community: floor division", "\n", "c", "=", "0", "# counter for community", "\n", "while", "sum", "(", "nNodesC", ")", "<", "N", ":", "# If there are still nodes to put in communities", "\n", "# do it one for each (balanced communities)", "\n", "            ", "nNodesC", "[", "c", "]", "=", "nNodesC", "[", "c", "]", "+", "1", "\n", "c", "+=", "1", "\n", "# So now, the list nNodesC has how many nodes are on each community.", "\n", "# We proceed to build the probability matrix.", "\n", "# We create a zero matrix", "\n", "", "probMatrix", "=", "np", ".", "zeros", "(", "[", "N", ",", "N", "]", ")", "\n", "# And fill ones on the block diagonals following the number of nodes.", "\n", "# For this, we need the cumulative sum of the number of nodes", "\n", "nNodesCIndex", "=", "[", "0", "]", "+", "np", ".", "cumsum", "(", "nNodesC", ")", ".", "tolist", "(", ")", "\n", "# The zero is added because it is the first index", "\n", "for", "c", "in", "range", "(", "C", ")", ":", "\n", "            ", "probMatrix", "[", "nNodesCIndex", "[", "c", "]", ":", "nNodesCIndex", "[", "c", "+", "1", "]", ",", "nNodesCIndex", "[", "c", "]", ":", "nNodesCIndex", "[", "c", "+", "1", "]", "]", "=", "np", ".", "ones", "(", "[", "nNodesC", "[", "c", "]", ",", "nNodesC", "[", "c", "]", "]", ")", "\n", "# The matrix probMatrix has one in the block diagonal, which should", "\n", "# have probabilities p_ii and 0 in the offdiagonal that should have", "\n", "# probabilities p_ij. So that", "\n", "", "probMatrix", "=", "pii", "*", "probMatrix", "+", "pij", "*", "(", "1", "-", "probMatrix", ")", "\n", "# has pii in the intracommunity blocks and pij in the intercommunity", "\n", "# blocks.", "\n", "# Now we're finally ready to generate a connected graph", "\n", "connectedGraph", "=", "False", "\n", "while", "not", "connectedGraph", ":", "\n", "# Generate random matrix", "\n", "            ", "W", "=", "np", ".", "random", ".", "rand", "(", "N", ",", "N", ")", "\n", "W", "=", "(", "W", "<", "probMatrix", ")", ".", "astype", "(", "np", ".", "float64", ")", "\n", "# This matrix will have a 1 if the element ij is less or equal than", "\n", "# p_ij, so that if p_ij = 0.8, then it will be 1 80% of the times", "\n", "# (on average).", "\n", "# We need to make it undirected and without self-loops, so keep the", "\n", "# upper triangular part after the main diagonal", "\n", "W", "=", "np", ".", "triu", "(", "W", ",", "1", ")", "\n", "# And add it to the lower triangular part", "\n", "W", "=", "W", "+", "W", ".", "T", "\n", "# Now let's check that it is connected", "\n", "connectedGraph", "=", "isConnected", "(", "W", ")", "\n", "", "", "elif", "graphType", "==", "'SmallWorld'", ":", "\n", "# Function provided by Tuomo M\u00e4ki-Marttunen", "\n", "# Connectedness introduced by Dr. S. Segarra.", "\n", "# Adapted to numpy by Fernando Gama.", "\n", "        ", "p", "=", "graphOptions", "[", "'probEdge'", "]", "# Edge probability", "\n", "q", "=", "graphOptions", "[", "'probRewiring'", "]", "# Rewiring probability", "\n", "# Positions on a circle", "\n", "posX", "=", "np", ".", "cos", "(", "2", "*", "np", ".", "pi", "*", "np", ".", "arange", "(", "0", ",", "N", ")", "/", "N", ")", ".", "reshape", "(", "[", "N", ",", "1", "]", ")", "# x axis", "\n", "posY", "=", "np", ".", "sin", "(", "2", "*", "np", ".", "pi", "*", "np", ".", "arange", "(", "0", ",", "N", ")", "/", "N", ")", ".", "reshape", "(", "[", "N", ",", "1", "]", ")", "# y axis", "\n", "pos", "=", "np", ".", "concatenate", "(", "(", "posX", ",", "posY", ")", ",", "axis", "=", "1", ")", "# N x 2 position matrix", "\n", "connectedGraph", "=", "False", "\n", "W", "=", "np", ".", "zeros", "(", "[", "N", ",", "N", "]", ",", "dtype", "=", "pos", ".", "dtype", ")", "# Empty adjacency matrix", "\n", "D", "=", "sp", ".", "distance", ".", "squareform", "(", "sp", ".", "distance", ".", "pdist", "(", "pos", ")", ")", "**", "2", "# Squared", "\n", "# distance matrix", "\n", "\n", "while", "not", "connectedGraph", ":", "\n", "# 1. The generation of locally connected network with given", "\n", "# in-degree:", "\n", "            ", "for", "n", "in", "range", "(", "N", ")", ":", "# Go through all nodes in order", "\n", "                ", "nn", "=", "np", ".", "random", ".", "binomial", "(", "N", ",", "p", ")", "\n", "# Possible inputs are all but the node itself:", "\n", "pind", "=", "np", ".", "concatenate", "(", "(", "np", ".", "arange", "(", "0", ",", "n", ")", ",", "np", ".", "arange", "(", "n", "+", "1", ",", "N", ")", ")", ")", "\n", "sortedIndices", "=", "np", ".", "argsort", "(", "D", "[", "n", ",", "pind", "]", ")", "\n", "dists", "=", "D", "[", "n", ",", "pind", "[", "sortedIndices", "]", "]", "\n", "inds_equallyfar", "=", "np", ".", "nonzero", "(", "dists", "==", "dists", "[", "nn", "]", ")", "[", "0", "]", "\n", "if", "len", "(", "inds_equallyfar", ")", "==", "1", ":", "# if a unique farthest node to", "\n", "# be chosen as input", "\n", "                    ", "W", "[", "pind", "[", "sortedIndices", "[", "0", ":", "nn", "]", "]", ",", "n", "]", "=", "1", "# choose as inputs all", "\n", "# from closest to the farthest-to-be-chosen", "\n", "", "else", ":", "\n", "                    ", "W", "[", "pind", "[", "sortedIndices", "[", "0", ":", "np", ".", "min", "(", "inds_equallyfar", ")", "]", "]", ",", "n", "]", "=", "1", "\n", "# choose each nearer than farthest-to-be-chosen", "\n", "r", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "inds_equallyfar", ")", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "# choose randomly between the ones that are as far as ", "\n", "# be-chosen", "\n", "\n", "W", "[", "pind", "[", "sortedIndices", "[", "np", ".", "min", "(", "inds_equallyfar", ")", "+", "r", "[", "0", ":", "nn", "-", "np", ".", "min", "(", "inds_equallyfar", ")", "+", "1", "]", "]", "]", ",", "n", "]", "=", "1", ";", "\n", "# 2. Watts-Strogatz perturbation:", "\n", "", "", "for", "n", "in", "range", "(", "N", ")", ":", "\n", "                ", "A", "=", "np", ".", "nonzero", "(", "W", "[", ":", ",", "n", "]", ")", "[", "0", "]", "# find the in-neighbours of n", "\n", "for", "j", "in", "range", "(", "len", "(", "A", ")", ")", ":", "\n", "                    ", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "q", ":", "\n", "                        ", "freeind", "=", "1", "-", "W", "[", ":", ",", "n", "]", "# possible new candidates are", "\n", "# all the ones not yet outputting to n", "\n", "# (excluding n itself)", "\n", "freeind", "[", "n", "]", "=", "0", "\n", "freeind", "[", "A", "[", "j", "]", "]", "=", "1", "\n", "B", "=", "np", ".", "nonzero", "(", "freeind", ")", "[", "0", "]", "\n", "r", "=", "np", ".", "floor", "(", "np", ".", "random", ".", "rand", "(", ")", "*", "len", "(", "B", ")", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "W", "[", "A", "[", "j", "]", ",", "n", "]", "=", "0", "\n", "W", "[", "B", "[", "r", "]", ",", "n", "]", "=", "1", ";", "\n", "\n", "# symmetrize M", "\n", "", "", "", "W", "=", "np", ".", "triu", "(", "W", ")", "\n", "W", "=", "W", "+", "W", ".", "T", "\n", "# Check that graph is connected", "\n", "connectedGraph", "=", "isConnected", "(", "W", ")", "\n", "", "", "elif", "graphType", "==", "'fuseEdges'", ":", "\n", "# This alternative assumes that there are multiple graphs that have to", "\n", "# be fused into one.", "\n", "# This will be done in two ways: average or sum.", "\n", "# On top, options will include: to symmetrize it or not, to make it", "\n", "# connected or not.", "\n", "# The input data is a tensor E x N x N where E are the multiple edge", "\n", "# features that we want to fuse.", "\n", "# Argument N is ignored", "\n", "# Data", "\n", "        ", "assert", "7", "<=", "len", "(", "graphOptions", ".", "keys", "(", ")", ")", "<=", "8", "\n", "W", "=", "graphOptions", "[", "'adjacencyMatrices'", "]", "# Data in format E x N x N", "\n", "assert", "len", "(", "W", ".", "shape", ")", "==", "3", "\n", "N", "=", "W", ".", "shape", "[", "1", "]", "# Number of nodes", "\n", "assert", "W", ".", "shape", "[", "1", "]", "==", "W", ".", "shape", "[", "2", "]", "\n", "# Name the list with all nodes to keep", "\n", "nodeList", "=", "graphOptions", "[", "'nodeList'", "]", "# This should be an empty list", "\n", "# If there is an 8th argument, this is where we are going to save the", "\n", "# extra components which are not the largest", "\n", "if", "len", "(", "graphOptions", ".", "keys", "(", ")", ")", "==", "8", ":", "\n", "            ", "logExtraComponents", "=", "True", "\n", "extraComponents", "=", "graphOptions", "[", "'extraComponents'", "]", "\n", "# This will be a list with two elements, the first elements will be", "\n", "# the adjacency matrix of the other (smaller) components, whereas", "\n", "# the second elements will be a list of the same size, where each", "\n", "# elements is yet another list of nodes to keep from the original ", "\n", "# graph to build such an adjacency matrix (akin to nodeList)", "\n", "", "else", ":", "\n", "            ", "logExtraComponents", "=", "False", "# Flag to know if we need to log the", "\n", "# extra components or not", "\n", "", "allNodes", "=", "np", ".", "arange", "(", "N", ")", "\n", "# What type of node aggregation", "\n", "aggregationType", "=", "graphOptions", "[", "'aggregationType'", "]", "\n", "assert", "aggregationType", "==", "'sum'", "or", "aggregationType", "==", "'avg'", "\n", "if", "aggregationType", "==", "'sum'", ":", "\n", "            ", "W", "=", "np", ".", "sum", "(", "W", ",", "axis", "=", "0", ")", "\n", "", "elif", "aggregationType", "==", "'avg'", ":", "\n", "            ", "W", "=", "np", ".", "mean", "(", "W", ",", "axis", "=", "0", ")", "\n", "# Normalization (sum of rows or columns is equal to 1)", "\n", "", "normalizationType", "=", "graphOptions", "[", "'normalizationType'", "]", "\n", "if", "normalizationType", "==", "'rows'", ":", "\n", "            ", "rowSum", "=", "np", ".", "sum", "(", "W", ",", "axis", "=", "1", ")", ".", "reshape", "(", "[", "N", ",", "1", "]", ")", "\n", "rowSum", "[", "np", ".", "abs", "(", "rowSum", ")", "<", "zeroTolerance", "]", "=", "1.", "\n", "W", "=", "W", "/", "np", ".", "tile", "(", "rowSum", ",", "[", "1", ",", "N", "]", ")", "\n", "", "elif", "normalizationType", "==", "'cols'", ":", "\n", "            ", "colSum", "=", "np", ".", "sum", "(", "W", ",", "axis", "=", "0", ")", ".", "reshape", "(", "[", "1", ",", "N", "]", ")", "\n", "colSum", "[", "np", ".", "abs", "(", "colSum", ")", "<", "zeroTolerance", "]", "=", "1.", "\n", "W", "=", "W", "/", "np", ".", "tile", "(", "colSum", ",", "[", "N", ",", "1", "]", ")", "\n", "# Discarding isolated nodes", "\n", "", "isolatedNodes", "=", "graphOptions", "[", "'isolatedNodes'", "]", "# if True, isolated nodes", "\n", "# are allowed, if not, discard them", "\n", "if", "isolatedNodes", "==", "False", ":", "\n", "# A Node is isolated when it's degree is zero", "\n", "            ", "degVector", "=", "np", ".", "sum", "(", "np", ".", "abs", "(", "W", ")", ",", "axis", "=", "0", ")", "\n", "# Keep nodes whose degree is not zero", "\n", "keepNodes", "=", "np", ".", "nonzero", "(", "degVector", ">", "zeroTolerance", ")", "\n", "# Get the first element of the output tuple, for some reason if", "\n", "# we take keepNodes, _ as the output it says it cannot unpack it.", "\n", "keepNodes", "=", "keepNodes", "[", "0", "]", "\n", "if", "len", "(", "keepNodes", ")", "<", "N", ":", "\n", "                ", "W", "=", "W", "[", "keepNodes", "]", "[", ":", ",", "keepNodes", "]", "\n", "# Update the nodes kept", "\n", "allNodes", "=", "allNodes", "[", "keepNodes", "]", "\n", "# Check if we need to make it undirected or not", "\n", "", "", "forceUndirected", "=", "graphOptions", "[", "'forceUndirected'", "]", "# if True, make it", "\n", "# undirected by using the average between nodes (careful, some ", "\n", "# edges might cancel)", "\n", "if", "forceUndirected", "==", "True", ":", "\n", "            ", "W", "=", "0.5", "*", "(", "W", "+", "W", ".", "T", ")", "\n", "# Finally, making it a connected graph", "\n", "", "forceConnected", "=", "graphOptions", "[", "'forceConnected'", "]", "# if True, make the", "\n", "# graph connected", "\n", "if", "forceConnected", "==", "True", ":", "\n", "# Check if the given graph is already connected", "\n", "            ", "connectedFlag", "=", "isConnected", "(", "W", ")", "\n", "# If it is not connected", "\n", "if", "not", "connectedFlag", ":", "\n", "# Find all connected components", "\n", "                ", "nComponents", ",", "nodeLabels", "=", "scipy", ".", "sparse", ".", "csgraph", ".", "connected_components", "(", "W", ")", "\n", "# Now, we have to pick the connected component with the largest", "\n", "# number of nodes, because that's the one to output.", "\n", "# Momentarily store the rest.", "\n", "# Let's get the list of nodes we have so far", "\n", "partialNodes", "=", "np", ".", "arange", "(", "W", ".", "shape", "[", "0", "]", ")", "\n", "# Create the lists to store the adjacency matrices and", "\n", "# the official lists of nodes to keep", "\n", "eachAdjacency", "=", "[", "None", "]", "*", "nComponents", "\n", "eachNodeList", "=", "[", "None", "]", "*", "nComponents", "\n", "# And we want to keep the one with largest number of nodes, but", "\n", "# we will do only one for, so we need to be checking which one", "\n", "# is, so we will compare against the maximum number of nodes", "\n", "# registered so far", "\n", "nNodesMax", "=", "0", "# To start", "\n", "for", "l", "in", "range", "(", "nComponents", ")", ":", "\n", "# Find the nodes belonging to the lth connected component", "\n", "                    ", "thisNodesToKeep", "=", "partialNodes", "[", "nodeLabels", "==", "l", "]", "\n", "# This adjacency matrix", "\n", "eachAdjacency", "[", "l", "]", "=", "W", "[", "thisNodesToKeep", "]", "[", ":", ",", "thisNodesToKeep", "]", "\n", "# The actual list", "\n", "eachNodeList", "[", "l", "]", "=", "allNodes", "[", "thisNodesToKeep", "]", "\n", "# Check the number of nodes", "\n", "thisNumberOfNodes", "=", "len", "(", "thisNodesToKeep", ")", "\n", "# And see if this is the largest", "\n", "if", "thisNumberOfNodes", ">", "nNodesMax", ":", "\n", "# Store the new number of maximum nodes", "\n", "                        ", "nNodesMax", "=", "thisNumberOfNodes", "\n", "# Store the element of the list that satisfies it", "\n", "indexLargestComponent", "=", "l", "\n", "# Once we have been over all the connected components, just", "\n", "# output the one with largest number of nodes", "\n", "", "", "W", "=", "eachAdjacency", ".", "pop", "(", "indexLargestComponent", ")", "\n", "allNodes", "=", "eachNodeList", ".", "pop", "(", "indexLargestComponent", ")", "\n", "# Check that it is effectively connected", "\n", "assert", "isConnected", "(", "W", ")", "\n", "# And, if we have the extra argument, return all the other", "\n", "# connected components", "\n", "if", "logExtraComponents", "==", "True", ":", "\n", "                    ", "extraComponents", ".", "append", "(", "eachAdjacency", ")", "\n", "extraComponents", ".", "append", "(", "eachNodeList", ")", "\n", "# To end, update the node list, so that it is returned through argument", "\n", "", "", "", "nodeList", ".", "extend", "(", "allNodes", ".", "tolist", "(", ")", ")", "\n", "", "elif", "graphType", "==", "'adjacency'", ":", "\n", "        ", "assert", "'adjacencyMatrix'", "in", "graphOptions", ".", "keys", "(", ")", "\n", "W", "=", "graphOptions", "[", "'adjacencyMatrix'", "]", "\n", "assert", "W", ".", "shape", "[", "0", "]", "==", "W", ".", "shape", "[", "1", "]", "==", "N", "\n", "\n", "", "return", "W", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.permIdentity": [[990, 1019], ["numpy.arange", "len", "S.reshape.reshape", "S.reshape.reshape", "np.arange.tolist", "len", "len"], "function", ["None"], ["", "def", "permIdentity", "(", "S", ")", ":", "\n", "    ", "\"\"\"\n    permIdentity: determines the identity permnutation\n\n    Input:\n        S (np.array): matrix\n\n    Output:\n        permS (np.array): matrix permuted (since, there's no permutation, it's\n              the same input matrix)\n        order (list): list of indices to make S become permS.\n    \"\"\"", "\n", "assert", "len", "(", "S", ".", "shape", ")", "==", "2", "or", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "S", ".", "shape", ")", "==", "2", ":", "\n", "        ", "assert", "S", ".", "shape", "[", "0", "]", "==", "S", ".", "shape", "[", "1", "]", "\n", "S", "=", "S", ".", "reshape", "(", "[", "1", ",", "S", ".", "shape", "[", "0", "]", ",", "S", ".", "shape", "[", "1", "]", "]", ")", "\n", "scalarWeights", "=", "True", "\n", "", "else", ":", "\n", "        ", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "\n", "scalarWeights", "=", "False", "\n", "# Number of nodes", "\n", "", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "# Identity order", "\n", "order", "=", "np", ".", "arange", "(", "N", ")", "\n", "# If the original GSO assumed scalar weights, get rid of the extra dimension", "\n", "if", "scalarWeights", ":", "\n", "        ", "S", "=", "S", ".", "reshape", "(", "[", "N", ",", "N", "]", ")", "\n", "\n", "", "return", "S", ",", "order", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.permDegree": [[1020, 1053], ["numpy.sum", "numpy.argsort", "numpy.flip", "len", "S.reshape.reshape", "numpy.sum", "S.reshape.reshape", "np.flip.tolist", "len", "len"], "function", ["None"], ["", "def", "permDegree", "(", "S", ")", ":", "\n", "    ", "\"\"\"\n    permDegree: determines the permutation by degree (nodes ordered from highest\n        degree to lowest)\n\n    Input:\n        S (np.array): matrix\n\n    Output:\n        permS (np.array): matrix permuted\n        order (list): list of indices to permute S to turn into permS.\n    \"\"\"", "\n", "assert", "len", "(", "S", ".", "shape", ")", "==", "2", "or", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "S", ".", "shape", ")", "==", "2", ":", "\n", "        ", "assert", "S", ".", "shape", "[", "0", "]", "==", "S", ".", "shape", "[", "1", "]", "\n", "S", "=", "S", ".", "reshape", "(", "[", "1", ",", "S", ".", "shape", "[", "0", "]", ",", "S", ".", "shape", "[", "1", "]", "]", ")", "\n", "scalarWeights", "=", "True", "\n", "", "else", ":", "\n", "        ", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "\n", "scalarWeights", "=", "False", "\n", "# Compute the degree", "\n", "", "d", "=", "np", ".", "sum", "(", "np", ".", "sum", "(", "S", ",", "axis", "=", "1", ")", ",", "axis", "=", "0", ")", "\n", "# Sort ascending order (from min degree to max degree)", "\n", "order", "=", "np", ".", "argsort", "(", "d", ")", "\n", "# Reverse sorting", "\n", "order", "=", "np", ".", "flip", "(", "order", ",", "0", ")", "\n", "# And update S", "\n", "S", "=", "S", "[", ":", ",", "order", ",", ":", "]", "[", ":", ",", ":", ",", "order", "]", "\n", "# If the original GSO assumed scalar weights, get rid of the extra dimension", "\n", "if", "scalarWeights", ":", "\n", "        ", "S", "=", "S", ".", "reshape", "(", "[", "S", ".", "shape", "[", "1", "]", ",", "S", ".", "shape", "[", "2", "]", "]", ")", "\n", "\n", "", "return", "S", ",", "order", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.permSpectralProxies": [[1054, 1113], ["numpy.linalg.matrix_power", "numpy.linalg.matrix_power", "len", "S.copy", "numpy.mean", "np.mean.conj", "len", "numpy.linalg.eig", "numpy.square", "numpy.argmax", "nodes.append", "len", "len", "numpy.absolute", "range", "numpy.argmin"], "function", ["None"], ["", "def", "permSpectralProxies", "(", "S", ")", ":", "\n", "    ", "\"\"\"\n    permSpectralProxies: determines the permutation by the spectral proxies\n        score (from highest to lowest)\n\n    Input:\n        S (np.array): matrix\n\n    Output:\n        permS (np.array): matrix permuted\n        order (list): list of indices to permute S to turn into permS.\n    \"\"\"", "\n", "# Design decisions:", "\n", "k", "=", "8", "# Parameter of the spectral proxies method. This is fixed for", "\n", "# consistency with the calls of the other permutation functions.", "\n", "# Design decisions: If we are given a multi-edge GSO, we're just going to", "\n", "# average all the edge dimensions and use that to compute the spectral", "\n", "# proxies.", "\n", "# Check S is of correct shape", "\n", "assert", "len", "(", "S", ".", "shape", ")", "==", "2", "or", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# If it is a matrix, just use it", "\n", "if", "len", "(", "S", ".", "shape", ")", "==", "2", ":", "\n", "        ", "assert", "S", ".", "shape", "[", "0", "]", "==", "S", ".", "shape", "[", "1", "]", "\n", "scalarWeights", "=", "True", "\n", "simpleS", "=", "S", ".", "copy", "(", ")", "\n", "# If it is a tensor of shape E x N x N, average over dimension E.", "\n", "", "else", ":", "\n", "        ", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "\n", "scalarWeights", "=", "False", "\n", "# Average over dimension E", "\n", "simpleS", "=", "np", ".", "mean", "(", "S", ",", "axis", "=", "0", ")", "\n", "\n", "", "N", "=", "simpleS", ".", "shape", "[", "0", "]", "# Number of nodes", "\n", "ST", "=", "simpleS", ".", "conj", "(", ")", ".", "T", "# Transpose of S, needed for the method", "\n", "Sk", "=", "np", ".", "linalg", ".", "matrix_power", "(", "simpleS", ",", "k", ")", "# S^k", "\n", "STk", "=", "np", ".", "linalg", ".", "matrix_power", "(", "ST", ",", "k", ")", "# (S^T)^k", "\n", "STkSk", "=", "STk", "@", "Sk", "# (S^T)^k * S^k, needed for the method", "\n", "\n", "nodes", "=", "[", "]", "# Where to save the nodes, order according the criteria", "\n", "it", "=", "1", "\n", "M", "=", "N", "# This opens up the door if we want to use this code for the actual", "\n", "# selection of nodes, instead of just ordering", "\n", "\n", "while", "len", "(", "nodes", ")", "<", "M", ":", "\n", "        ", "remainingNodes", "=", "[", "n", "for", "n", "in", "range", "(", "N", ")", "if", "n", "not", "in", "nodes", "]", "\n", "# Computes the eigenvalue decomposition", "\n", "phi_eig", ",", "phi_ast_k", "=", "np", ".", "linalg", ".", "eig", "(", "\n", "STkSk", "[", "remainingNodes", "]", "[", ":", ",", "remainingNodes", "]", ")", "\n", "phi_ast_k", "=", "phi_ast_k", "[", ":", "]", "[", ":", ",", "np", ".", "argmin", "(", "phi_eig", ".", "real", ")", "]", "\n", "abs_phi_ast_k_2", "=", "np", ".", "square", "(", "np", ".", "absolute", "(", "phi_ast_k", ")", ")", "\n", "newNodePos", "=", "np", ".", "argmax", "(", "abs_phi_ast_k_2", ")", "\n", "nodes", ".", "append", "(", "remainingNodes", "[", "newNodePos", "]", ")", "\n", "it", "+=", "1", "\n", "\n", "", "if", "scalarWeights", ":", "\n", "        ", "S", "=", "S", "[", "nodes", ",", ":", "]", "[", ":", ",", "nodes", "]", "\n", "", "else", ":", "\n", "        ", "S", "=", "S", "[", ":", ",", "nodes", ",", ":", "]", "[", ":", ",", ":", ",", "nodes", "]", "\n", "", "return", "S", ",", "nodes", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.permEDS": [[1114, 1162], ["numpy.linalg.eig", "numpy.max", "numpy.square", "numpy.argsort", "numpy.flip", "len", "S.copy", "numpy.mean", "numpy.absolute", "np.flip.tolist", "len", "len"], "function", ["None"], ["", "def", "permEDS", "(", "S", ")", ":", "\n", "    ", "\"\"\"\n    permEDS: determines the permutation by the experimentally designed sampling\n        score (from highest to lowest)\n\n    Input:\n        S (np.array): matrix\n\n    Output:\n        permS (np.array): matrix permuted\n        order (list): list of indices to permute S to turn into permS.\n    \"\"\"", "\n", "# Design decisions: If we are given a multi-edge GSO, we're just going to", "\n", "# average all the edge dimensions and use that to compute the spectral", "\n", "# proxies.", "\n", "# Check S is of correct shape", "\n", "assert", "len", "(", "S", ".", "shape", ")", "==", "2", "or", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# If it is a matrix, just use it", "\n", "if", "len", "(", "S", ".", "shape", ")", "==", "2", ":", "\n", "        ", "assert", "S", ".", "shape", "[", "0", "]", "==", "S", ".", "shape", "[", "1", "]", "\n", "scalarWeights", "=", "True", "\n", "simpleS", "=", "S", ".", "copy", "(", ")", "\n", "# If it is a tensor of shape E x N x N, average over dimension E.", "\n", "", "else", ":", "\n", "        ", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "\n", "scalarWeights", "=", "False", "\n", "# Average over dimension E", "\n", "simpleS", "=", "np", ".", "mean", "(", "S", ",", "axis", "=", "0", ")", "\n", "\n", "", "E", ",", "V", "=", "np", ".", "linalg", ".", "eig", "(", "simpleS", ")", "# Eigendecomposition of S", "\n", "kappa", "=", "np", ".", "max", "(", "np", ".", "absolute", "(", "V", ")", ",", "axis", "=", "1", ")", "\n", "\n", "kappa2", "=", "np", ".", "square", "(", "kappa", ")", "# The probabilities assigned to each node are", "\n", "# proportional to kappa2, so in the mean, the ones with largest kappa^2", "\n", "# would be \"sampled\" more often, and as suche are more important (i.e.", "\n", "# they have a higher score)", "\n", "\n", "# Sort ascending order (from min degree to max degree)", "\n", "order", "=", "np", ".", "argsort", "(", "kappa2", ")", "\n", "# Reverse sorting", "\n", "order", "=", "np", ".", "flip", "(", "order", ",", "0", ")", "\n", "\n", "if", "scalarWeights", ":", "\n", "        ", "S", "=", "S", "[", "order", ",", ":", "]", "[", ":", ",", "order", "]", "\n", "", "else", ":", "\n", "        ", "S", "=", "S", "[", ":", ",", "order", ",", ":", "]", "[", ":", ",", ":", ",", "order", "]", "\n", "\n", "", "return", "S", ",", "order", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.edgeFailSampling": [[1163, 1191], ["numpy.allclose", "numpy.random.rand", "numpy.triu"], "function", ["None"], ["", "def", "edgeFailSampling", "(", "W", ",", "p", ")", ":", "\n", "    ", "\"\"\"\n    edgeFailSampling: randomly delete the edges of a given graph\n    \n    Input:\n        W (np.array): adjacency matrix\n        p (float): probability of deleting an edge\n    \n    Output:\n        W (np.array): adjacency matrix with some edges randomly deleted\n        \n    Obs.: The resulting graph need not be connected (even if the input graph is)\n    \"\"\"", "\n", "\n", "assert", "0", "<=", "p", "<=", "1", "\n", "N", "=", "W", ".", "shape", "[", "0", "]", "\n", "assert", "W", ".", "shape", "[", "1", "]", "==", "N", "\n", "undirected", "=", "np", ".", "allclose", "(", "W", ",", "W", ".", "T", ",", "atol", "=", "zeroTolerance", ")", "\n", "\n", "maskEdges", "=", "np", ".", "random", ".", "rand", "(", "N", ",", "N", ")", "\n", "maskEdges", "=", "(", "maskEdges", ">", "p", ")", ".", "astype", "(", "W", ".", "dtype", ")", "# Put a 1 with probability 1-p", "\n", "\n", "W", "=", "maskEdges", "*", "W", "\n", "if", "undirected", ":", "\n", "        ", "W", "=", "np", ".", "triu", "(", "W", ")", "\n", "W", "=", "W", "+", "W", ".", "T", "\n", "\n", "", "return", "W", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.splineBasis": [[1294, 1336], ["numpy.isscalar", "numpy.linspace", "numpy.concatenate", "numpy.column_stack", "numpy.linspace", "np.linspace.min", "numpy.ones", "np.linspace.min", "np.linspace.max", "np.linspace.max", "numpy.ones", "graphTools.splineBasis.cox_deboor"], "function", ["None"], ["", "", "", "def", "splineBasis", "(", "K", ",", "x", ",", "degree", "=", "3", ")", ":", "\n", "# Function written by M. Defferrard, taken verbatim (except for function", "\n", "# name), from ", "\n", "# https://github.com/mdeff/cnn_graph/blob/master/lib/models.py#L662", "\n", "    ", "\"\"\"\n    Return the B-spline basis.\n    K: number of control points.\n    x: evaluation points\n       or number of evenly distributed evaluation points.\n    degree: degree of the spline. Cubic spline by default.\n    \"\"\"", "\n", "if", "np", ".", "isscalar", "(", "x", ")", ":", "\n", "        ", "x", "=", "np", ".", "linspace", "(", "0", ",", "1", ",", "x", ")", "\n", "\n", "# Evenly distributed knot vectors.", "\n", "", "kv1", "=", "x", ".", "min", "(", ")", "*", "np", ".", "ones", "(", "degree", ")", "\n", "kv2", "=", "np", ".", "linspace", "(", "x", ".", "min", "(", ")", ",", "x", ".", "max", "(", ")", ",", "K", "-", "degree", "+", "1", ")", "\n", "kv3", "=", "x", ".", "max", "(", ")", "*", "np", ".", "ones", "(", "degree", ")", "\n", "kv", "=", "np", ".", "concatenate", "(", "(", "kv1", ",", "kv2", ",", "kv3", ")", ")", "\n", "\n", "# Cox - DeBoor recursive function to compute one spline over x.", "\n", "def", "cox_deboor", "(", "k", ",", "d", ")", ":", "\n", "# Test for end conditions, the rectangular degree zero spline.", "\n", "        ", "if", "(", "d", "==", "0", ")", ":", "\n", "            ", "return", "(", "(", "x", "-", "kv", "[", "k", "]", ">=", "0", ")", "&", "(", "x", "-", "kv", "[", "k", "+", "1", "]", "<", "0", ")", ")", ".", "astype", "(", "int", ")", "\n", "\n", "", "denom1", "=", "kv", "[", "k", "+", "d", "]", "-", "kv", "[", "k", "]", "\n", "term1", "=", "0", "\n", "if", "denom1", ">", "0", ":", "\n", "            ", "term1", "=", "(", "(", "x", "-", "kv", "[", "k", "]", ")", "/", "denom1", ")", "*", "cox_deboor", "(", "k", ",", "d", "-", "1", ")", "\n", "\n", "", "denom2", "=", "kv", "[", "k", "+", "d", "+", "1", "]", "-", "kv", "[", "k", "+", "1", "]", "\n", "term2", "=", "0", "\n", "if", "denom2", ">", "0", ":", "\n", "            ", "term2", "=", "(", "(", "-", "(", "x", "-", "kv", "[", "k", "+", "d", "+", "1", "]", ")", "/", "denom2", ")", "*", "cox_deboor", "(", "k", "+", "1", ",", "d", "-", "1", ")", ")", "\n", "\n", "", "return", "term1", "+", "term2", "\n", "\n", "# Compute basis for each point", "\n", "", "basis", "=", "np", ".", "column_stack", "(", "[", "cox_deboor", "(", "k", ",", "degree", ")", "for", "k", "in", "range", "(", "K", ")", "]", ")", "\n", "basis", "[", "-", "1", ",", "-", "1", "]", "=", "1", "\n", "return", "basis", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.coarsen": [[1337, 1367], ["graphTools.metis", "graphTools.compute_perm", "enumerate", "perm_adjacency.tocsr", "perm_adjacency.eliminate_zeros", "perm_adjacency.tocoo", "perm_adjacency.setdiag", "graphTools.perm_adjacency"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.metis", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.compute_perm", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.perm_adjacency"], ["", "def", "coarsen", "(", "A", ",", "levels", ",", "self_connections", "=", "False", ")", ":", "\n", "# Function written by M. Defferrard, taken (almost) verbatim, from ", "\n", "# https://github.com/mdeff/cnn_graph/blob/master/lib/coarsening.py#L5", "\n", "    ", "\"\"\"\n    Coarsen a graph, represented by its adjacency matrix A, at multiple\n    levels.\n    \"\"\"", "\n", "graphs", ",", "parents", "=", "metis", "(", "A", ",", "levels", ")", "\n", "perms", "=", "compute_perm", "(", "parents", ")", "\n", "\n", "for", "i", ",", "A", "in", "enumerate", "(", "graphs", ")", ":", "\n", "        ", "M", ",", "M", "=", "A", ".", "shape", "\n", "\n", "if", "not", "self_connections", ":", "\n", "            ", "A", "=", "A", ".", "tocoo", "(", ")", "\n", "A", ".", "setdiag", "(", "0", ")", "\n", "\n", "", "if", "i", "<", "levels", ":", "\n", "            ", "A", "=", "perm_adjacency", "(", "A", ",", "perms", "[", "i", "]", ")", "\n", "\n", "", "A", "=", "A", ".", "tocsr", "(", ")", "\n", "A", ".", "eliminate_zeros", "(", ")", "\n", "graphs", "[", "i", "]", "=", "A", "\n", "\n", "#        Mnew, Mnew = A.shape", "\n", "#        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added),'", "\n", "#              '|E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz//2))", "\n", "\n", "\n", "", "return", "graphs", ",", "perms", "[", "0", "]", "if", "levels", ">", "0", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.metis": [[1368, 1452], ["graphs.append", "range", "numpy.random.permutation", "scipy.sparse.csr_matrix.sum", "scipy.sparse.csr_matrix.diagonal", "numpy.array().squeeze", "scipy.sparse.find", "scipy.sparse.find", "numpy.argsort", "graphTools.metis_one_level", "parents.append", "scipy.sparse.csr_matrix", "scipy.sparse.csr_matrix", "scipy.sparse.csr_matrix.eliminate_zeros", "graphs.append", "scipy.sparse.csr_matrix.sum", "numpy.array().squeeze", "numpy.argsort", "range", "metis_one_level.max", "numpy.array", "numpy.array", "scipy.sparse.csr_matrix.sum"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.metis_one_level"], ["", "def", "metis", "(", "W", ",", "levels", ",", "rid", "=", "None", ")", ":", "\n", "# Function written by M. Defferrard, taken verbatim, from ", "\n", "# https://github.com/mdeff/cnn_graph/blob/master/lib/coarsening.py#L34", "\n", "    ", "\"\"\"\n    Coarsen a graph multiple times using the METIS algorithm.\n    INPUT\n    W: symmetric sparse weight (adjacency) matrix\n    levels: the number of coarsened graphs\n    OUTPUT\n    graph[0]: original graph of size N_1\n    graph[2]: coarser graph of size N_2 < N_1\n    graph[levels]: coarsest graph of Size N_levels < ... < N_2 < N_1\n    parents[i] is a vector of size N_i with entries ranging from 1 to N_{i+1}\n        which indicate the parents in the coarser graph[i+1]\n    nd_sz{i} is a vector of size N_i that contains the size of the supernode\n        in the graph{i}\n    NOTE\n        if \"graph\" is a list of length k, then \"parents\" will be a list of\n        length k-1\n    \"\"\"", "\n", "\n", "N", ",", "N", "=", "W", ".", "shape", "\n", "if", "rid", "is", "None", ":", "\n", "        ", "rid", "=", "np", ".", "random", ".", "permutation", "(", "range", "(", "N", ")", ")", "\n", "", "parents", "=", "[", "]", "\n", "degree", "=", "W", ".", "sum", "(", "axis", "=", "0", ")", "-", "W", ".", "diagonal", "(", ")", "\n", "graphs", "=", "[", "]", "\n", "graphs", ".", "append", "(", "W", ")", "\n", "#supernode_size = np.ones(N)", "\n", "#nd_sz = [supernode_size]", "\n", "#count = 0", "\n", "\n", "#while N > maxsize:", "\n", "for", "_", "in", "range", "(", "levels", ")", ":", "\n", "\n", "#count += 1", "\n", "\n", "# CHOOSE THE WEIGHTS FOR THE PAIRING", "\n", "# weights = ones(N,1)       # metis weights", "\n", "        ", "weights", "=", "degree", "# graclus weights", "\n", "# weights = supernode_size  # other possibility", "\n", "weights", "=", "np", ".", "array", "(", "weights", ")", ".", "squeeze", "(", ")", "\n", "\n", "# PAIR THE VERTICES AND CONSTRUCT THE ROOT VECTOR", "\n", "idx_row", ",", "idx_col", ",", "val", "=", "scipy", ".", "sparse", ".", "find", "(", "W", ")", "\n", "perm", "=", "np", ".", "argsort", "(", "idx_row", ")", "\n", "rr", "=", "idx_row", "[", "perm", "]", "\n", "cc", "=", "idx_col", "[", "perm", "]", "\n", "vv", "=", "val", "[", "perm", "]", "\n", "cluster_id", "=", "metis_one_level", "(", "rr", ",", "cc", ",", "vv", ",", "rid", ",", "weights", ")", "# rr is ordered", "\n", "parents", ".", "append", "(", "cluster_id", ")", "\n", "\n", "# TO DO", "\n", "# COMPUTE THE SIZE OF THE SUPERNODES AND THEIR DEGREE ", "\n", "#supernode_size = full(   sparse(cluster_id,  ones(N,1) ,", "\n", "#\tsupernode_size )     )", "\n", "#print(cluster_id)", "\n", "#print(supernode_size)", "\n", "#nd_sz{count+1}=supernode_size;", "\n", "\n", "# COMPUTE THE EDGES WEIGHTS FOR THE NEW GRAPH", "\n", "nrr", "=", "cluster_id", "[", "rr", "]", "\n", "ncc", "=", "cluster_id", "[", "cc", "]", "\n", "nvv", "=", "vv", "\n", "Nnew", "=", "cluster_id", ".", "max", "(", ")", "+", "1", "\n", "# CSR is more appropriate: row,val pairs appear multiple times", "\n", "W", "=", "scipy", ".", "sparse", ".", "csr_matrix", "(", "(", "nvv", ",", "(", "nrr", ",", "ncc", ")", ")", ",", "shape", "=", "(", "Nnew", ",", "Nnew", ")", ")", "\n", "W", ".", "eliminate_zeros", "(", ")", "\n", "# Add new graph to the list of all coarsened graphs", "\n", "graphs", ".", "append", "(", "W", ")", "\n", "N", ",", "N", "=", "W", ".", "shape", "\n", "\n", "# COMPUTE THE DEGREE (OMIT OR NOT SELF LOOPS)", "\n", "degree", "=", "W", ".", "sum", "(", "axis", "=", "0", ")", "\n", "#degree = W.sum(axis=0) - W.diagonal()", "\n", "\n", "# CHOOSE THE ORDER IN WHICH VERTICES WILL BE VISTED AT THE NEXT PASS", "\n", "#[~, rid]=sort(ss);     # arthur strategy", "\n", "#[~, rid]=sort(supernode_size);    #  thomas strategy", "\n", "#rid=randperm(N);                  #  metis/graclus strategy", "\n", "ss", "=", "np", ".", "array", "(", "W", ".", "sum", "(", "axis", "=", "0", ")", ")", ".", "squeeze", "(", ")", "\n", "rid", "=", "np", ".", "argsort", "(", "ss", ")", "\n", "\n", "", "return", "graphs", ",", "parents", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.metis_one_level": [[1454, 1503], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "range", "range"], "function", ["None"], ["", "def", "metis_one_level", "(", "rr", ",", "cc", ",", "vv", ",", "rid", ",", "weights", ")", ":", "\n", "# Function written by M. Defferrard, taken verbatim, from ", "\n", "# https://github.com/mdeff/cnn_graph/blob/master/lib/coarsening.py#L119", "\n", "\n", "    ", "nnz", "=", "rr", ".", "shape", "[", "0", "]", "\n", "N", "=", "rr", "[", "nnz", "-", "1", "]", "+", "1", "\n", "\n", "marked", "=", "np", ".", "zeros", "(", "N", ",", "np", ".", "bool", ")", "\n", "rowstart", "=", "np", ".", "zeros", "(", "N", ",", "np", ".", "int32", ")", "\n", "rowlength", "=", "np", ".", "zeros", "(", "N", ",", "np", ".", "int32", ")", "\n", "cluster_id", "=", "np", ".", "zeros", "(", "N", ",", "np", ".", "int32", ")", "\n", "\n", "oldval", "=", "rr", "[", "0", "]", "\n", "count", "=", "0", "\n", "clustercount", "=", "0", "\n", "\n", "for", "ii", "in", "range", "(", "nnz", ")", ":", "\n", "        ", "rowlength", "[", "count", "]", "=", "rowlength", "[", "count", "]", "+", "1", "\n", "if", "rr", "[", "ii", "]", ">", "oldval", ":", "\n", "            ", "oldval", "=", "rr", "[", "ii", "]", "\n", "rowstart", "[", "count", "+", "1", "]", "=", "ii", "\n", "count", "=", "count", "+", "1", "\n", "\n", "", "", "for", "ii", "in", "range", "(", "N", ")", ":", "\n", "        ", "tid", "=", "rid", "[", "ii", "]", "\n", "if", "not", "marked", "[", "tid", "]", ":", "\n", "            ", "wmax", "=", "0.0", "\n", "rs", "=", "rowstart", "[", "tid", "]", "\n", "marked", "[", "tid", "]", "=", "True", "\n", "bestneighbor", "=", "-", "1", "\n", "for", "jj", "in", "range", "(", "rowlength", "[", "tid", "]", ")", ":", "\n", "                ", "nid", "=", "cc", "[", "rs", "+", "jj", "]", "\n", "if", "marked", "[", "nid", "]", ":", "\n", "                    ", "tval", "=", "0.0", "\n", "", "else", ":", "\n", "                    ", "tval", "=", "vv", "[", "rs", "+", "jj", "]", "*", "(", "1.0", "/", "weights", "[", "tid", "]", "+", "1.0", "/", "weights", "[", "nid", "]", ")", "\n", "", "if", "tval", ">", "wmax", ":", "\n", "                    ", "wmax", "=", "tval", "\n", "bestneighbor", "=", "nid", "\n", "\n", "", "", "cluster_id", "[", "tid", "]", "=", "clustercount", "\n", "\n", "if", "bestneighbor", ">", "-", "1", ":", "\n", "                ", "cluster_id", "[", "bestneighbor", "]", "=", "clustercount", "\n", "marked", "[", "bestneighbor", "]", "=", "True", "\n", "\n", "", "clustercount", "+=", "1", "\n", "\n", "", "", "return", "cluster_id", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.compute_perm": [[1504, 1554], ["enumerate", "len", "indices.append", "len", "indices.append", "len", "max", "list", "list", "indices_layer.extend", "sorted", "list", "range", "len", "len", "list.append", "range", "numpy.where", "len", "list.append", "list.append"], "function", ["None"], ["", "def", "compute_perm", "(", "parents", ")", ":", "\n", "# Function written by M. Defferrard, taken verbatim, from ", "\n", "# https://github.com/mdeff/cnn_graph/blob/master/lib/coarsening.py#L167", "\n", "    ", "\"\"\"\n    Return a list of indices to reorder the adjacency and data matrices so\n    that the union of two neighbors from layer to layer forms a binary tree.\n    \"\"\"", "\n", "\n", "# Order of last layer is random (chosen by the clustering algorithm).", "\n", "indices", "=", "[", "]", "\n", "if", "len", "(", "parents", ")", ">", "0", ":", "\n", "        ", "M_last", "=", "max", "(", "parents", "[", "-", "1", "]", ")", "+", "1", "\n", "indices", ".", "append", "(", "list", "(", "range", "(", "M_last", ")", ")", ")", "\n", "\n", "", "for", "parent", "in", "parents", "[", ":", ":", "-", "1", "]", ":", "\n", "#print('parent: {}'.format(parent))", "\n", "\n", "# Fake nodes go after real ones.", "\n", "        ", "pool_singeltons", "=", "len", "(", "parent", ")", "\n", "\n", "indices_layer", "=", "[", "]", "\n", "for", "i", "in", "indices", "[", "-", "1", "]", ":", "\n", "            ", "indices_node", "=", "list", "(", "np", ".", "where", "(", "parent", "==", "i", ")", "[", "0", "]", ")", "\n", "assert", "0", "<=", "len", "(", "indices_node", ")", "<=", "2", "\n", "#print('indices_node: {}'.format(indices_node))", "\n", "\n", "# Add a node to go with a singelton.", "\n", "if", "len", "(", "indices_node", ")", "==", "1", ":", "\n", "                ", "indices_node", ".", "append", "(", "pool_singeltons", ")", "\n", "pool_singeltons", "+=", "1", "\n", "#print('new singelton: {}'.format(indices_node))", "\n", "# Add two nodes as children of a singelton in the parent.", "\n", "", "elif", "len", "(", "indices_node", ")", "==", "0", ":", "\n", "                ", "indices_node", ".", "append", "(", "pool_singeltons", "+", "0", ")", "\n", "indices_node", ".", "append", "(", "pool_singeltons", "+", "1", ")", "\n", "pool_singeltons", "+=", "2", "\n", "#print('singelton childrens: {}'.format(indices_node))", "\n", "\n", "", "indices_layer", ".", "extend", "(", "indices_node", ")", "\n", "", "indices", ".", "append", "(", "indices_layer", ")", "\n", "\n", "# Sanity checks.", "\n", "", "for", "i", ",", "indices_layer", "in", "enumerate", "(", "indices", ")", ":", "\n", "        ", "M", "=", "M_last", "*", "2", "**", "i", "\n", "# Reduction by 2 at each layer (binary tree).", "\n", "assert", "len", "(", "indices", "[", "0", "]", "==", "M", ")", "\n", "# The new ordering does not omit an indice.", "\n", "assert", "sorted", "(", "indices_layer", ")", "==", "list", "(", "range", "(", "M", ")", ")", "\n", "\n", "", "return", "indices", "[", ":", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.perm_adjacency": [[1555, 1585], ["len", "scipy.sparse.hstack.tocoo", "numpy.argsort", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix", "scipy.sparse.vstack", "scipy.sparse.vstack", "scipy.sparse.hstack", "scipy.sparse.hstack", "numpy.array", "numpy.array", "type"], "function", ["None"], ["", "def", "perm_adjacency", "(", "A", ",", "indices", ")", ":", "\n", "# Function written by M. Defferrard, taken verbatim, from ", "\n", "# https://github.com/mdeff/cnn_graph/blob/master/lib/coarsening.py#L242", "\n", "    ", "\"\"\"\n    Permute adjacency matrix, i.e. exchange node ids,\n    so that binary unions form the clustering tree.\n    \"\"\"", "\n", "if", "indices", "is", "None", ":", "\n", "        ", "return", "A", "\n", "\n", "", "M", ",", "M", "=", "A", ".", "shape", "\n", "Mnew", "=", "len", "(", "indices", ")", "\n", "assert", "Mnew", ">=", "M", "\n", "A", "=", "A", ".", "tocoo", "(", ")", "\n", "\n", "# Add Mnew - M isolated vertices.", "\n", "if", "Mnew", ">", "M", ":", "\n", "        ", "rows", "=", "scipy", ".", "sparse", ".", "coo_matrix", "(", "(", "Mnew", "-", "M", ",", "M", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "cols", "=", "scipy", ".", "sparse", ".", "coo_matrix", "(", "(", "Mnew", ",", "Mnew", "-", "M", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "A", "=", "scipy", ".", "sparse", ".", "vstack", "(", "[", "A", ",", "rows", "]", ")", "\n", "A", "=", "scipy", ".", "sparse", ".", "hstack", "(", "[", "A", ",", "cols", "]", ")", "\n", "\n", "# Permute the rows and the columns.", "\n", "", "perm", "=", "np", ".", "argsort", "(", "indices", ")", "\n", "A", ".", "row", "=", "np", ".", "array", "(", "perm", ")", "[", "A", ".", "row", "]", "\n", "A", ".", "col", "=", "np", ".", "array", "(", "perm", ")", "[", "A", ".", "col", "]", "\n", "\n", "# assert np.abs(A - A.T).mean() < 1e-9", "\n", "assert", "type", "(", "A", ")", "is", "scipy", ".", "sparse", ".", "coo", ".", "coo_matrix", "\n", "return", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.permCoarsening": [[1586, 1615], ["len", "numpy.empty", "enumerate", "numpy.zeros"], "function", ["None"], ["", "def", "permCoarsening", "(", "x", ",", "indices", ")", ":", "\n", "# Original function written by M. Defferrard, found in", "\n", "# https://github.com/mdeff/cnn_graph/blob/master/lib/coarsening.py#L219", "\n", "# Function name has been changed, and it has been further adapted to handle", "\n", "# multiple features as", "\n", "#   number_data_points x number_features x number_nodes", "\n", "# instead of the original", "\n", "#   number_data_points x number_nodes", "\n", "    ", "\"\"\"\n    Permute data matrix, i.e. exchange node ids,\n    so that binary unions form the clustering tree.\n    \"\"\"", "\n", "if", "indices", "is", "None", ":", "\n", "        ", "return", "x", "\n", "\n", "", "B", ",", "F", ",", "N", "=", "x", ".", "shape", "\n", "Nnew", "=", "len", "(", "indices", ")", "\n", "assert", "Nnew", ">=", "N", "\n", "xnew", "=", "np", ".", "empty", "(", "(", "B", ",", "F", ",", "Nnew", ")", ")", "\n", "for", "i", ",", "j", "in", "enumerate", "(", "indices", ")", ":", "\n", "# Existing vertex, i.e. real data.", "\n", "        ", "if", "j", "<", "N", ":", "\n", "            ", "xnew", "[", ":", ",", ":", ",", "i", "]", "=", "x", "[", ":", ",", ":", ",", "j", "]", "\n", "# Fake vertex because of singeltons.", "\n", "# They will stay 0 so that max pooling chooses the singelton.", "\n", "# Or -infty ?", "\n", "", "else", ":", "\n", "            ", "xnew", "[", ":", ",", ":", ",", "i", "]", "=", "np", ".", "zeros", "(", "[", "B", ",", "F", "]", ")", "\n", "", "", "return", "xnew", "\n", "", ""]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MaxLocalActivation.__init__": [[1574, 1586], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.MaxLocalActivation.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "K", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "K", ">", "0", "# range has to be greater than 0", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "S", "=", "None", "# no GSO assigned yet", "\n", "self", ".", "N", "=", "None", "# no GSO assigned yet (N learned from the GSO)", "\n", "self", ".", "neighborhood", "=", "'None'", "# no neighborhoods calculated yet", "\n", "# Create parameters:", "\n", "self", ".", "weight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "self", ".", "K", "+", "1", ")", ")", "\n", "# Initialize parameters", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MaxLocalActivation.addGSO": [[1587, 1609], ["S.cpu().numpy.cpu().numpy.cpu().numpy", "range", "len", "alegnn.computeNeighborhood", "neighborhood.append", "maxNeighborhoodSizes.append", "S.cpu().numpy.cpu().numpy.cpu", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeNeighborhood", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "# Change tensor S to numpy now that we have saved it as tensor in self.S", "\n", "S", "=", "S", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# The neighborhood matrix has to be a tensor of shape", "\n", "#   nOutputNodes x maxNeighborhoodSize", "\n", "neighborhood", "=", "[", "]", "\n", "maxNeighborhoodSizes", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "1", ",", "self", ".", "K", "+", "1", ")", ":", "\n", "# For each hop (0,1,...) in the range K", "\n", "            ", "thisNeighborhood", "=", "graphTools", ".", "computeNeighborhood", "(", "S", ",", "k", ",", "\n", "outputType", "=", "'matrix'", ")", "\n", "# compute the k-hop neighborhood", "\n", "neighborhood", ".", "append", "(", "torch", ".", "tensor", "(", "thisNeighborhood", ")", ".", "to", "(", "self", ".", "S", ".", "device", ")", ")", "\n", "maxNeighborhoodSizes", ".", "append", "(", "thisNeighborhood", ".", "shape", "[", "1", "]", ")", "\n", "", "self", ".", "maxNeighborhoodSizes", "=", "maxNeighborhoodSizes", "\n", "self", ".", "neighborhood", "=", "neighborhood", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MaxLocalActivation.forward": [[1610, 1673], ["torch.cat.unsqueeze", "torch.cat.unsqueeze", "x.unsqueeze.unsqueeze.unsqueeze", "range", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "out.reshape.reshape.reshape", "x.unsqueeze.unsqueeze.repeat", "graphML.MaxLocalActivation.neighborhood[].reshape", "gatherNeighbor.repeat.repeat.repeat", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.max", "torch.max", "torch.max", "torch.max", "v.unsqueeze.unsqueeze.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "graphML.MaxLocalActivation.weight.unsqueeze", "gatherNeighbor.repeat.repeat.long().to", "gatherNeighbor.repeat.repeat.long"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x should be of shape batchSize x dimNodeSignals x N", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "dimNodeSignals", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "# And given that the self.neighborhood is already a torch.tensor matrix", "\n", "# we can just go ahead and get it.", "\n", "# So, x is of shape B x F x N. But we need it to be of shape", "\n", "# B x F x N x maxNeighbor. Why? Well, because we need to compute the", "\n", "# maximum between the value of each node and those of its neighbors.", "\n", "# And we do this by applying a torch.max across the rows (dim = 3) so", "\n", "# that we end up again with a B x F x N, but having computed the max.", "\n", "# How to fill those extra dimensions? Well, what we have is neighborhood", "\n", "# matrix, and we are going to use torch.gather to bring the right", "\n", "# values (torch.index_select, while more straightforward, only works", "\n", "# along a single dimension).", "\n", "# Each row of the matrix neighborhood determines all the neighbors of", "\n", "# each node: the first row contains all the neighbors of the first node,", "\n", "# etc.", "\n", "# The values of the signal at those nodes are contained in the dim = 2", "\n", "# of x. So, just for now, let's ignore the batch and feature dimensions", "\n", "# and imagine we have a column vector: N x 1. We have to pick some of", "\n", "# the elements of this vector and line them up alongside each row", "\n", "# so that then we can compute the maximum along these rows.", "\n", "# When we torch.gather along dimension 0, we are selecting which row to", "\n", "# pick according to each column. Thus, if we have that the first row", "\n", "# of the neighborhood matrix is [1, 2, 0] means that we want to pick", "\n", "# the value at row 1 of x, at row 2 of x in the next column, and at row", "\n", "# 0 of the last column. For these values to be the appropriate ones, we", "\n", "# have to repeat x as columns to build our b x F x N x maxNeighbor", "\n", "# matrix.", "\n", "xK", "=", "x", "# xK is a tensor aggregating the 0-hop (x), 1-hop, ..., K-hop", "\n", "# max's it is initialized with the 0-hop neigh. (x itself)", "\n", "xK", "=", "xK", ".", "unsqueeze", "(", "3", ")", "# extra dimension added for concatenation ahead", "\n", "x", "=", "x", ".", "unsqueeze", "(", "3", ")", "# B x F x N x 1", "\n", "# And the neighbors that we need to gather are the same across the batch", "\n", "# and feature dimensions, so we need to repeat the matrix along those", "\n", "# dimensions", "\n", "for", "k", "in", "range", "(", "1", ",", "self", ".", "K", "+", "1", ")", ":", "\n", "            ", "x_aux", "=", "x", ".", "repeat", "(", "[", "1", ",", "1", ",", "1", ",", "self", ".", "maxNeighborhoodSizes", "[", "k", "-", "1", "]", "]", ")", "\n", "gatherNeighbor", "=", "self", ".", "neighborhood", "[", "k", "-", "1", "]", ".", "reshape", "(", "\n", "[", "1", ",", "\n", "1", ",", "\n", "self", ".", "N", ",", "\n", "self", ".", "maxNeighborhoodSizes", "[", "k", "-", "1", "]", "]", "\n", ")", "\n", "gatherNeighbor", "=", "gatherNeighbor", ".", "repeat", "(", "[", "batchSize", ",", "\n", "dimNodeSignals", ",", "\n", "1", ",", "\n", "1", "]", ")", "\n", "# And finally we're in position of getting all the neighbors in line", "\n", "xNeighbors", "=", "torch", ".", "gather", "(", "x_aux", ",", "2", ",", "gatherNeighbor", ".", "long", "(", ")", ".", "to", "(", "x", ".", "device", ")", ")", "\n", "#   B x F x nOutput x maxNeighbor", "\n", "# Note that this gather function already reduces the dimension to", "\n", "# nOutputNodes.", "\n", "# And proceed to compute the maximum along this dimension", "\n", "v", ",", "_", "=", "torch", ".", "max", "(", "xNeighbors", ",", "dim", "=", "3", ")", "\n", "v", "=", "v", ".", "unsqueeze", "(", "3", ")", "# to concatenate with xK", "\n", "xK", "=", "torch", ".", "cat", "(", "(", "xK", ",", "v", ")", ",", "3", ")", "\n", "", "out", "=", "torch", ".", "matmul", "(", "xK", ",", "self", ".", "weight", ".", "unsqueeze", "(", "2", ")", ")", "\n", "# multiply each k-hop max by corresponding weight", "\n", "out", "=", "out", ".", "reshape", "(", "[", "batchSize", ",", "dimNodeSignals", ",", "self", ".", "N", "]", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MaxLocalActivation.reset_parameters": [[1674, 1678], ["graphML.MaxLocalActivation.weight.data.uniform_", "math.sqrt"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "K", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MaxLocalActivation.extra_repr": [[1679, 1685], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "neighborhood", "is", "not", "None", ":", "\n", "            ", "reprString", "=", "\"neighborhood stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "=", "\"NO neighborhood stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MedianLocalActivation.__init__": [[1728, 1741], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.MedianLocalActivation.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "K", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "K", ">", "0", "# range has to be greater than 0", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "S", "=", "None", "# no GSO assigned yet", "\n", "self", ".", "N", "=", "None", "# no GSO assigned yet (N learned from the GSO)", "\n", "self", ".", "neighborhood", "=", "'None'", "# no neighborhoods calculated yet", "\n", "self", ".", "masks", "=", "'None'", "# no mask yet", "\n", "# Create parameters:", "\n", "self", ".", "weight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "self", ".", "K", "+", "1", ")", ")", "\n", "# Initialize parameters", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MedianLocalActivation.addGSO": [[1742, 1761], ["S.cpu().numpy.cpu().numpy.cpu().numpy", "range", "len", "alegnn.computeNeighborhood", "neighborhood.append", "S.cpu().numpy.cpu().numpy.cpu"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeNeighborhood"], ["", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "# Change tensor S to numpy now that we have saved it as tensor in self.S", "\n", "S", "=", "S", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# The neighborhood matrix has to be a tensor of shape", "\n", "#   nOutputNodes x maxNeighborhoodSize", "\n", "neighborhood", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "1", ",", "self", ".", "K", "+", "1", ")", ":", "\n", "# For each hop (0,1,...) in the range K", "\n", "            ", "thisNeighborhood", "=", "graphTools", ".", "computeNeighborhood", "(", "S", ",", "k", ",", "\n", "outputType", "=", "'list'", ")", "\n", "# compute the k-hop neighborhood", "\n", "neighborhood", ".", "append", "(", "thisNeighborhood", ")", "\n", "", "self", ".", "neighborhood", "=", "neighborhood", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MedianLocalActivation.forward": [[1762, 1803], ["torch.cat.unsqueeze", "torch.cat.unsqueeze", "range", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "out.reshape.reshape.reshape", "torch.empty().to", "torch.empty().to", "torch.empty().to", "torch.empty().to", "range", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "graphML.MedianLocalActivation.weight.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "torch.tensor.reshape", "torch.tensor.reshape", "gatherNode.repeat.repeat.repeat", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.median", "torch.median", "torch.median", "torch.median", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "numpy.array", "gatherNode.repeat.repeat.long().to", "gatherNode.repeat.repeat.long"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x should be of shape batchSize x dimNodeSignals x N", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "dimNodeSignals", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "xK", "=", "x", "# xK is a tensor aggregating the 0-hop (x), 1-hop, ..., K-hop", "\n", "# max's", "\n", "# It is initialized with the 0-hop neigh. (x itself)", "\n", "xK", "=", "xK", ".", "unsqueeze", "(", "3", ")", "# extra dimension added for concatenation ahead", "\n", "#x = x.unsqueeze(3) # B x F x N x 1", "\n", "for", "k", "in", "range", "(", "1", ",", "self", ".", "K", "+", "1", ")", ":", "\n", "            ", "kHopNeighborhood", "=", "self", ".", "neighborhood", "[", "k", "-", "1", "]", "\n", "# Fetching k-hop neighborhoods of all nodes", "\n", "kHopMedian", "=", "torch", ".", "empty", "(", "0", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "# Initializing the vector that will contain the k-hop median for", "\n", "# every node", "\n", "for", "n", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "# Iterating over the nodes", "\n", "# This step is necessary because here the neighborhoods are", "\n", "# lists of lists. It is impossible to pad them and feed them as", "\n", "# a matrix, as this would impact the outcome of the median", "\n", "# operation", "\n", "                ", "nodeNeighborhood", "=", "torch", ".", "tensor", "(", "np", ".", "array", "(", "kHopNeighborhood", "[", "n", "]", ")", ")", "\n", "neighborhoodLen", "=", "len", "(", "nodeNeighborhood", ")", "\n", "gatherNode", "=", "nodeNeighborhood", ".", "reshape", "(", "[", "1", ",", "1", ",", "neighborhoodLen", "]", ")", "\n", "gatherNode", "=", "gatherNode", ".", "repeat", "(", "[", "batchSize", ",", "dimNodeSignals", ",", "1", "]", ")", "\n", "# Reshaping the node neighborhood for the gather operation", "\n", "xNodeNeighbors", "=", "torch", ".", "gather", "(", "x", ",", "2", ",", "gatherNode", ".", "long", "(", ")", ".", "to", "(", "x", ".", "device", ")", ")", "\n", "# Gathering signal values in the node neighborhood", "\n", "nodeMedian", ",", "_", "=", "torch", ".", "median", "(", "xNodeNeighbors", ",", "dim", "=", "2", ",", "\n", "keepdim", "=", "True", ")", "\n", "# Computing the median in the neighborhood", "\n", "kHopMedian", "=", "torch", ".", "cat", "(", "[", "kHopMedian", ",", "nodeMedian", "]", ",", "2", ")", "\n", "# Concatenating k-hop medians node by node", "\n", "", "kHopMedian", "=", "kHopMedian", ".", "unsqueeze", "(", "3", ")", "# Extra dimension for", "\n", "# concatenation with the previous (k-1)-hop median tensor ", "\n", "xK", "=", "torch", ".", "cat", "(", "[", "xK", ",", "kHopMedian", "]", ",", "3", ")", "\n", "", "out", "=", "torch", ".", "matmul", "(", "xK", ",", "self", ".", "weight", ".", "unsqueeze", "(", "2", ")", ")", "\n", "# Multiplying each k-hop median by corresponding trainable weight", "\n", "out", "=", "out", ".", "reshape", "(", "[", "batchSize", ",", "dimNodeSignals", ",", "self", ".", "N", "]", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MedianLocalActivation.reset_parameters": [[1804, 1808], ["graphML.MedianLocalActivation.weight.data.uniform_", "math.sqrt"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "K", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MedianLocalActivation.extra_repr": [[1809, 1815], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "neighborhood", "is", "not", "None", ":", "\n", "            ", "reprString", "=", "\"neighborhood stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "=", "\"NO neighborhood stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NoActivation.__init__": [[1834, 1836], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NoActivation.forward": [[1836, 1839], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NoActivation.extra_repr": [[1840, 1843], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"No Activation Function\"", "\n", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NoPool.__init__": [[1861, 1868], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "nInputNodes", ",", "nOutputNodes", ",", "nHops", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nInputNodes", "=", "nInputNodes", "\n", "self", ".", "nOutputNodes", "=", "nOutputNodes", "\n", "self", ".", "nHops", "=", "nHops", "\n", "self", ".", "neighborhood", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NoPool.addGSO": [[1869, 1873], ["None"], "methods", ["None"], ["", "def", "addGSO", "(", "self", ",", "GSO", ")", ":", "\n", "# This is necessary to keep the form of the other pooling strategies", "\n", "# within the SelectionGNN framework. But we do not care about any GSO.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NoPool.forward": [[1874, 1883], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x should be of shape batchSize x dimNodeSignals x nInputNodes", "\n", "        ", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "nInputNodes", "\n", "# Check that there are at least the same number of nodes that", "\n", "# we will keep (otherwise, it would be unpooling, instead of", "\n", "# pooling)", "\n", "assert", "x", ".", "shape", "[", "2", "]", ">=", "self", ".", "nOutputNodes", "\n", "# And do not do anything", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NoPool.extra_repr": [[1884, 1889], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_dim=%d, out_dim=%d, number_hops = %d, \"", "%", "(", "\n", "self", ".", "nInputNodes", ",", "self", ".", "nOutputNodes", ",", "self", ".", "nHops", ")", "\n", "reprString", "+=", "\"no neighborhood needed\"", "\n", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MaxPoolLocal.__init__": [[1932, 1939], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "nInputNodes", ",", "nOutputNodes", ",", "nHops", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nInputNodes", "=", "nInputNodes", "\n", "self", ".", "nOutputNodes", "=", "nOutputNodes", "\n", "self", ".", "nHops", "=", "nHops", "\n", "self", ".", "neighborhood", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MaxPoolLocal.addGSO": [[1940, 1967], ["numpy.array", "alegnn.computeNeighborhood", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "len", "numpy.array.cpu", "torch.tensor().to.max", "torch.tensor().to.max", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeNeighborhood", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N (And I don't care about E, because the", "\n", "# computeNeighborhood function takes care of it)", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "# Get the device (before operating with S and losing it, it's cheaper", "\n", "# to store the device now, than to duplicate S -i.e. keep a numpy and a", "\n", "# tensor copy of S)", "\n", "device", "=", "S", ".", "device", "\n", "# Move the GSO to cpu and to np.array so it can be handled by the", "\n", "# computeNeighborhood function", "\n", "S", "=", "np", ".", "array", "(", "S", ".", "cpu", "(", ")", ")", "\n", "# Compute neighborhood", "\n", "neighborhood", "=", "graphTools", ".", "computeNeighborhood", "(", "S", ",", "self", ".", "nHops", ",", "\n", "self", ".", "nOutputNodes", ",", "\n", "self", ".", "nInputNodes", ",", "'matrix'", ")", "\n", "# And move the neighborhood back to a tensor", "\n", "neighborhood", "=", "torch", ".", "tensor", "(", "neighborhood", ")", ".", "to", "(", "device", ")", "\n", "# The neighborhood matrix has to be a tensor of shape", "\n", "#   nOutputNodes x maxNeighborhoodSize", "\n", "assert", "neighborhood", ".", "shape", "[", "0", "]", "==", "self", ".", "nOutputNodes", "\n", "assert", "neighborhood", ".", "max", "(", ")", "<=", "self", ".", "nInputNodes", "\n", "# Store all the relevant information", "\n", "self", ".", "maxNeighborhoodSize", "=", "neighborhood", ".", "shape", "[", "1", "]", "\n", "self", ".", "neighborhood", "=", "neighborhood", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MaxPoolLocal.forward": [[1968, 2020], ["x.repeat.repeat.unsqueeze", "x.repeat.repeat.repeat", "graphML.MaxPoolLocal.neighborhood.reshape", "gatherNeighbor.repeat.repeat.repeat", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.max", "torch.max", "torch.max", "torch.max"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x should be of shape batchSize x dimNodeSignals x nInputNodes", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "dimNodeSignals", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "nInputNodes", "\n", "# Check that there are at least the same number of nodes that", "\n", "# we will keep (otherwise, it would be unpooling, instead of", "\n", "# pooling)", "\n", "assert", "x", ".", "shape", "[", "2", "]", ">=", "self", ".", "nOutputNodes", "\n", "# And given that the self.neighborhood is already a torch.tensor matrix", "\n", "# we can just go ahead and get it.", "\n", "# So, x is of shape B x F x N. But we need it to be of shape", "\n", "# B x F x N x maxNeighbor. Why? Well, because we need to compute the", "\n", "# maximum between the value of each node and those of its neighbors.", "\n", "# And we do this by applying a torch.max across the rows (dim = 3) so", "\n", "# that we end up again with a B x F x N, but having computed the max.", "\n", "# How to fill those extra dimensions? Well, what we have is neighborhood", "\n", "# matrix, and we are going to use torch.gather to bring the right", "\n", "# values (torch.index_select, while more straightforward, only works", "\n", "# along a single dimension).", "\n", "# Each row of the matrix neighborhood determines all the neighbors of", "\n", "# each node: the first row contains all the neighbors of the first node,", "\n", "# etc.", "\n", "# The values of the signal at those nodes are contained in the dim = 2", "\n", "# of x. So, just for now, let's ignore the batch and feature dimensions", "\n", "# and imagine we have a column vector: N x 1. We have to pick some of", "\n", "# the elements of this vector and line them up alongside each row", "\n", "# so that then we can compute the maximum along these rows.", "\n", "# When we torch.gather along dimension 0, we are selecting which row to", "\n", "# pick according to each column. Thus, if we have that the first row", "\n", "# of the neighborhood matrix is [1, 2, 0] means that we want to pick", "\n", "# the value at row 1 of x, at row 2 of x in the next column, and at row", "\n", "# 0 of the last column. For these values to be the appropriate ones, we", "\n", "# have to repeat x as columns to build our b x F x N x maxNeighbor", "\n", "# matrix.", "\n", "x", "=", "x", ".", "unsqueeze", "(", "3", ")", "# B x F x N x 1", "\n", "x", "=", "x", ".", "repeat", "(", "[", "1", ",", "1", ",", "1", ",", "self", ".", "maxNeighborhoodSize", "]", ")", "# BxFxNxmaxNeighbor", "\n", "# And the neighbors that we need to gather are the same across the batch", "\n", "# and feature dimensions, so we need to repeat the matrix along those", "\n", "# dimensions", "\n", "gatherNeighbor", "=", "self", ".", "neighborhood", ".", "reshape", "(", "[", "1", ",", "1", ",", "\n", "self", ".", "nOutputNodes", ",", "\n", "self", ".", "maxNeighborhoodSize", "]", ")", "\n", "gatherNeighbor", "=", "gatherNeighbor", ".", "repeat", "(", "[", "batchSize", ",", "dimNodeSignals", ",", "1", ",", "1", "]", ")", "\n", "# And finally we're in position of getting all the neighbors in line", "\n", "xNeighbors", "=", "torch", ".", "gather", "(", "x", ",", "2", ",", "gatherNeighbor", ")", "\n", "#   B x F x nOutput x maxNeighbor", "\n", "# Note that this gather function already reduces the dimension to", "\n", "# nOutputNodes.", "\n", "# And proceed to compute the maximum along this dimension", "\n", "v", ",", "_", "=", "torch", ".", "max", "(", "xNeighbors", ",", "dim", "=", "3", ")", "\n", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.MaxPoolLocal.extra_repr": [[2021, 2029], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_dim=%d, out_dim=%d, number_hops = %d, \"", "%", "(", "\n", "self", ".", "nInputNodes", ",", "self", ".", "nOutputNodes", ",", "self", ".", "nHops", ")", "\n", "if", "self", ".", "neighborhood", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"neighborhood stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"NO neighborhood stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter.__init__": [[2086, 2108], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.GraphFilter.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.GraphFilter.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "G", ",", "F", ",", "K", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# K: Number of filter taps", "\n", "# GSOs will be added later.", "\n", "# This combines both weight scalars and weight vectors.", "\n", "# Bias will always be shared and scalar.", "\n", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save parameters:", "\n", "self", ".", "G", "=", "G", "\n", "self", ".", "F", "=", "F", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "E", "=", "E", "\n", "self", ".", "S", "=", "None", "# No GSO assigned yet", "\n", "# Create parameters:", "\n", "self", ".", "weight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "E", ",", "K", ",", "G", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter.reset_parameters": [[2109, 2115], ["graphML.GraphFilter.weight.data.uniform_", "math.sqrt", "graphML.GraphFilter.bias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "G", "*", "self", ".", "K", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "self", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter.addGSO": [[2116, 2124], ["len"], "methods", ["None"], ["", "", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter.forward": [[2125, 2145], ["graphML.LSIGF", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.LSIGF", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x is of shape: batchSize x dimInFeatures x numberNodesIn", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "F", "=", "x", ".", "shape", "[", "1", "]", "\n", "Nin", "=", "x", ".", "shape", "[", "2", "]", "\n", "# And now we add the zero padding", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "\n", "torch", ".", "zeros", "(", "B", ",", "F", ",", "self", ".", "N", "-", "Nin", ")", ".", "type", "(", "x", ".", "dtype", ")", ".", "to", "(", "x", ".", "device", ")", "\n", ")", ",", "dim", "=", "2", ")", "\n", "# Compute the filter output", "\n", "", "u", "=", "LSIGF", "(", "self", ".", "weight", ",", "self", ".", "S", ",", "x", ",", "self", ".", "bias", ")", "\n", "# So far, u is of shape batchSize x dimOutFeatures x numberNodes", "\n", "# And we want to return a tensor of shape", "\n", "# batchSize x dimOutFeatures x numberNodesIn", "\n", "# since the nodes between numberNodesIn and numberNodes are not required", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "u", "=", "torch", ".", "index_select", "(", "u", ",", "2", ",", "torch", ".", "arange", "(", "Nin", ")", ".", "to", "(", "u", ".", "device", ")", ")", "\n", "", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter.extra_repr": [[2146, 2156], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, out_features=%d, \"", "%", "(", "\n", "self", ".", "G", ",", "self", ".", "F", ")", "+", "\"filter_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", "is", "not", "None", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.SpectralGF.__init__": [[2208, 2228], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.SpectralGF.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.SpectralGF.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "G", ",", "F", ",", "M", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# GSOs will be added later.", "\n", "# Bias will always be shared and scalar.", "\n", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save parameters:", "\n", "self", ".", "G", "=", "G", "\n", "self", ".", "F", "=", "F", "\n", "self", ".", "M", "=", "M", "\n", "self", ".", "E", "=", "E", "\n", "self", ".", "S", "=", "None", "# No GSO assigned yet", "\n", "# Create parameters:", "\n", "self", ".", "weight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "E", ",", "G", ",", "M", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.SpectralGF.reset_parameters": [[2229, 2235], ["graphML.SpectralGF.weight.data.uniform_", "math.sqrt", "graphML.SpectralGF.bias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "G", "*", "self", ".", "M", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "self", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.SpectralGF.addGSO": [[2236, 2269], ["numpy.array", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "torch.tensor().type().to", "len", "S.data.cpu", "numpy.linalg.eig", "alegnn.splineBasis", "V[].conj", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor().type", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.splineBasis"], ["", "", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has to have 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "# Save S", "\n", "# Now we need to compute the eigendecomposition and save it", "\n", "# To compute the eigendecomposition, we use numpy.", "\n", "# So, first, get S in numpy format.", "\n", "Snp", "=", "np", ".", "array", "(", "S", ".", "data", ".", "cpu", "(", ")", ")", "\n", "# We will compute the eigendecomposition for each edge feature, so we", "\n", "# create the E x N x N space for V, VH and Lambda (we need lambda for", "\n", "# the spline kernel)", "\n", "V", "=", "np", ".", "zeros", "(", "[", "self", ".", "E", ",", "self", ".", "N", ",", "self", ".", "N", "]", ")", "\n", "VH", "=", "np", ".", "zeros", "(", "[", "self", ".", "E", ",", "self", ".", "N", ",", "self", ".", "N", "]", ")", "\n", "Lambda", "=", "np", ".", "zeros", "(", "[", "self", ".", "E", ",", "self", ".", "N", "]", ")", "\n", "# Here we save the resulting spline kernel matrix", "\n", "splineKernel", "=", "np", ".", "zeros", "(", "[", "self", ".", "E", ",", "self", ".", "N", ",", "self", ".", "M", "]", ")", "\n", "for", "e", "in", "range", "(", "self", ".", "E", ")", ":", "\n", "# Compute the eigendecomposition", "\n", "            ", "Lambda", "[", "e", ",", ":", "]", ",", "V", "[", "e", ",", ":", ",", ":", "]", "=", "np", ".", "linalg", ".", "eig", "(", "Snp", "[", "e", ",", ":", ",", ":", "]", ")", "\n", "# Compute the hermitian", "\n", "VH", "[", "e", ",", ":", ",", ":", "]", "=", "V", "[", "e", ",", ":", ",", ":", "]", ".", "conj", "(", ")", ".", "T", "\n", "# Compute the splineKernel basis matrix", "\n", "splineKernel", "[", "e", ",", ":", ",", ":", "]", "=", "graphTools", ".", "splineBasis", "(", "self", ".", "M", ",", "Lambda", "[", "e", ",", ":", "]", ")", "\n", "# Transform everything to tensors of appropriate type on appropriate", "\n", "# device, and store them.", "\n", "", "self", ".", "V", "=", "torch", ".", "tensor", "(", "V", ")", ".", "type", "(", "S", ".", "dtype", ")", ".", "to", "(", "S", ".", "device", ")", "# E x N x N", "\n", "self", ".", "VH", "=", "torch", ".", "tensor", "(", "VH", ")", ".", "type", "(", "S", ".", "dtype", ")", ".", "to", "(", "S", ".", "device", ")", "# E x N x N", "\n", "self", ".", "splineKernel", "=", "torch", ".", "tensor", "(", "splineKernel", ")", ".", "type", "(", "S", ".", "dtype", ")", ".", "to", "(", "S", ".", "device", ")", "\n", "# E x N x M", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.SpectralGF.forward": [[2273, 2305], ["graphML.spectralGF", "graphML.SpectralGF.splineKernel.reshape", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "graphML.SpectralGF.h.permute", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "graphML.SpectralGF.weight.permute", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.spectralGF", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x is of shape: batchSize x dimInFeatures x numberNodesIn", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "F", "=", "x", ".", "shape", "[", "1", "]", "\n", "Nin", "=", "x", ".", "shape", "[", "2", "]", "\n", "\n", "# Check if we have enough spectral filter coefficients as needed, or if", "\n", "# we need to fill out the rest using the spline kernel.", "\n", "if", "self", ".", "M", "==", "self", ".", "N", ":", "\n", "            ", "self", ".", "h", "=", "self", ".", "weight", "# F x E x G x N (because N = M)", "\n", "", "else", ":", "\n", "# Adjust dimensions for proper algebraic matrix multiplication", "\n", "            ", "splineKernel", "=", "self", ".", "splineKernel", ".", "reshape", "(", "[", "1", ",", "self", ".", "E", ",", "self", ".", "N", ",", "self", ".", "M", "]", ")", "\n", "# We will multiply a 1 x E x N x M matrix with a F x E x M x G", "\n", "# matrix to get the proper F x E x N x G coefficients", "\n", "self", ".", "h", "=", "torch", ".", "matmul", "(", "splineKernel", ",", "self", ".", "weight", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", ")", "\n", "# And now we rearrange it to the same shape that the function takes", "\n", "self", ".", "h", "=", "self", ".", "h", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# F x E x G x N", "\n", "# And now we add the zero padding (if this comes from a pooling", "\n", "# operation)", "\n", "", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "zeroPad", "=", "torch", ".", "zeros", "(", "B", ",", "F", ",", "self", ".", "N", "-", "Nin", ")", ".", "type", "(", "x", ".", "dtype", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "zeroPad", ")", ",", "dim", "=", "2", ")", "\n", "# Compute the filter output", "\n", "", "u", "=", "spectralGF", "(", "self", ".", "h", ",", "self", ".", "V", ",", "self", ".", "VH", ",", "x", ",", "self", ".", "bias", ")", "\n", "# So far, u is of shape batchSize x dimOutFeatures x numberNodes", "\n", "# And we want to return a tensor of shape", "\n", "# batchSize x dimOutFeatures x numberNodesIn", "\n", "# since the nodes between numberNodesIn and numberNodes are not required", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "u", "=", "torch", ".", "index_select", "(", "u", ",", "2", ",", "torch", ".", "arange", "(", "Nin", ")", ".", "to", "(", "u", ".", "device", ")", ")", "\n", "", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.SpectralGF.extra_repr": [[2306, 2316], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, out_features=%d, \"", "%", "(", "\n", "self", ".", "G", ",", "self", ".", "F", ")", "+", "\"filter_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", "is", "not", "None", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeVariantGF.__init__": [[2369, 2394], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.NodeVariantGF.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.NodeVariantGF.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "G", ",", "F", ",", "K", ",", "M", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# G: Number of input features", "\n", "# F: Number of output features", "\n", "# K: Number of filter shift taps", "\n", "# M: Number of filter node taps", "\n", "# GSOs will be added later.", "\n", "# Bias will always be shared and scalar.", "\n", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save parameters:", "\n", "self", ".", "G", "=", "G", "\n", "self", ".", "F", "=", "F", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "M", "=", "M", "\n", "self", ".", "E", "=", "E", "\n", "self", ".", "S", "=", "None", "# No GSO assigned yet", "\n", "# Create parameters:", "\n", "self", ".", "weight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "E", ",", "K", ",", "G", ",", "M", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeVariantGF.reset_parameters": [[2395, 2401], ["graphML.NodeVariantGF.weight.data.uniform_", "math.sqrt", "graphML.NodeVariantGF.bias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "G", "*", "self", ".", "K", "*", "self", ".", "M", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "self", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeVariantGF.addGSO": [[2402, 2469], ["numpy.array", "len", "S.data.cpu", "alegnn.computeNeighborhood", "range", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "len", "alegnn.computeNeighborhood", "copyNodes.append", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "range", "range", "min", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len", "range", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeNeighborhood", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.computeNeighborhood", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "npS", "=", "np", ".", "array", "(", "S", ".", "data", ".", "cpu", "(", ")", ")", "# Save the GSO as a numpy array because we", "\n", "# are going to compute the neighbors.", "\n", "# And now we have to fill up the parameter vector, from M to N", "\n", "if", "self", ".", "M", "<", "self", ".", "N", ":", "\n", "# The first elements of M (ordered with whatever order we want)", "\n", "# are the ones associated to independent node taps.", "\n", "            ", "copyNodes", "=", "[", "m", "for", "m", "in", "range", "(", "self", ".", "M", ")", "]", "\n", "# The rest of the nodes will copy one of these M node taps.", "\n", "# The way we do this is: if they are connected to one of the M", "\n", "# indepdendent nodes, just copy it. If they are not connected,", "\n", "# look at the neighbors, neighbors, and so on, until we reach one", "\n", "# of the independent nodes.", "\n", "# Ties are broken by selecting the node with the smallest index", "\n", "# (which, due to the ordering, is the most important node of all", "\n", "# the available ones)", "\n", "neighborList", "=", "graphTools", ".", "computeNeighborhood", "(", "npS", ",", "1", ",", "\n", "nb", "=", "self", ".", "M", ")", "\n", "# This gets the list of 1-hop neighbors for all nodes.", "\n", "# Find the nodes that have no neighbors", "\n", "nodesWithNoNeighbors", "=", "[", "n", "for", "n", "in", "range", "(", "self", ".", "N", ")", "if", "len", "(", "neighborList", "[", "n", "]", ")", "==", "0", "]", "\n", "# If there are still nodes that didn't find a neighbor", "\n", "K", "=", "1", "# K-hop neighbor we have looked so far", "\n", "while", "len", "(", "nodesWithNoNeighbors", ")", ">", "0", ":", "\n", "# Looks for the next hop", "\n", "                ", "K", "+=", "1", "\n", "# Get the neigbors one further hop away", "\n", "thisNeighborList", "=", "graphTools", ".", "computeNeighborhood", "(", "npS", ",", "\n", "K", ",", "\n", "nb", "=", "self", ".", "M", ")", "\n", "# Check if we now have neighbors for those that didn't have", "\n", "# before", "\n", "for", "n", "in", "nodesWithNoNeighbors", ":", "\n", "# Get the neighbors of the node", "\n", "                    ", "thisNodeList", "=", "thisNeighborList", "[", "n", "]", "\n", "# If there are neighbors", "\n", "if", "len", "(", "thisNodeList", ")", ">", "0", ":", "\n", "# Add them to the list", "\n", "                        ", "neighborList", "[", "n", "]", "=", "thisNodeList", "\n", "# Recheck if all nodes have non-empty neighbors", "\n", "", "", "nodesWithNoNeighbors", "=", "[", "n", "for", "n", "in", "range", "(", "self", ".", "N", ")", "if", "len", "(", "neighborList", "[", "n", "]", ")", "==", "0", "]", "\n", "# Now we have obtained the list of independent nodes connected to", "\n", "# all nodes, we keep the one with highest score. And since the", "\n", "# matrix is already properly ordered, this means keeping the", "\n", "# smallest index in the neighborList.", "\n", "", "for", "m", "in", "range", "(", "self", ".", "M", ",", "self", ".", "N", ")", ":", "\n", "                ", "copyNodes", ".", "append", "(", "min", "(", "neighborList", "[", "m", "]", ")", ")", "\n", "# And, finally create the indices of nodes to copy", "\n", "", "self", ".", "copyNodes", "=", "torch", ".", "tensor", "(", "copyNodes", ")", ".", "to", "(", "S", ".", "device", ")", "\n", "", "elif", "self", ".", "M", "==", "self", ".", "N", ":", "\n", "# In this case, all parameters go into the vector h", "\n", "            ", "self", ".", "copyNodes", "=", "torch", ".", "arange", "(", "self", ".", "M", ")", ".", "to", "(", "S", ".", "device", ")", "\n", "", "else", ":", "\n", "# This is the rare case in which self.M < self.N, for example, if", "\n", "# we train in a larger network and deploy in a smaller one. Since", "\n", "# the matrix is ordered by score, we just keep the first N", "\n", "# weights", "\n", "            ", "self", ".", "copyNodes", "=", "torch", ".", "arange", "(", "self", ".", "N", ")", ".", "to", "(", "S", ".", "device", ")", "\n", "# OBS.: self.weight is updated on each training step, so we cannot", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeVariantGF.forward": [[2475, 2499], ["graphML.NVGF", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NVGF", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x is of shape: batchSize x dimInFeatures x numberNodesIn", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "F", "=", "x", ".", "shape", "[", "1", "]", "\n", "Nin", "=", "x", ".", "shape", "[", "2", "]", "\n", "# If we have less filter coefficients than the required ones, we need", "\n", "# to use the copying scheme", "\n", "if", "self", ".", "M", "==", "self", ".", "N", ":", "\n", "            ", "self", ".", "h", "=", "self", ".", "weight", "\n", "", "else", ":", "\n", "            ", "self", ".", "h", "=", "torch", ".", "index_select", "(", "self", ".", "weight", ",", "4", ",", "self", ".", "copyNodes", ")", "\n", "# And now we add the zero padding", "\n", "", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "zeroPad", "=", "torch", ".", "zeros", "(", "B", ",", "F", ",", "self", ".", "N", "-", "Nin", ")", ".", "type", "(", "x", ".", "dtype", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "zeroPad", ")", ",", "dim", "=", "2", ")", "\n", "# Compute the filter output", "\n", "", "u", "=", "NVGF", "(", "self", ".", "h", ",", "self", ".", "S", ",", "x", ",", "self", ".", "bias", ")", "\n", "# So far, u is of shape batchSize x dimOutFeatures x numberNodes", "\n", "# And we want to return a tensor of shape", "\n", "# batchSize x dimOutFeatures x numberNodesIn", "\n", "# since the nodes between numberNodesIn and numberNodes are not required", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "u", "=", "torch", ".", "index_select", "(", "u", ",", "2", ",", "torch", ".", "arange", "(", "Nin", ")", ".", "to", "(", "u", ".", "device", ")", ")", "\n", "", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeVariantGF.extra_repr": [[2500, 2510], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, out_features=%d, \"", "%", "(", "\n", "self", ".", "G", ",", "self", ".", "F", ")", "+", "\"shift_taps=%d, node_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ",", "self", ".", "M", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", "is", "not", "None", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantGF.__init__": [[2574, 2598], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.EdgeVariantGF.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.EdgeVariantGF.register_parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.EdgeVariantGF.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "G", ",", "F", ",", "K", ",", "M", ",", "N", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save parameters:", "\n", "self", ".", "G", "=", "G", "\n", "self", ".", "F", "=", "F", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "E", "=", "E", "\n", "self", ".", "M", "=", "M", "# Number of selected nodes", "\n", "self", ".", "N", "=", "N", "# Total number of nodes", "\n", "self", ".", "S", "=", "None", "\n", "# Create parameters for the Edge-Variant part:", "\n", "self", ".", "weightEV", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "E", ",", "K", ",", "G", ",", "N", ",", "N", ")", ")", "\n", "# If we want a hybrid, create parameters", "\n", "if", "self", ".", "M", "<", "self", ".", "N", ":", "\n", "            ", "self", ".", "weightLSI", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "E", ",", "K", ",", "G", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'weightLSI'", ",", "None", ")", "\n", "", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantGF.reset_parameters": [[2599, 2607], ["graphML.EdgeVariantGF.weightEV.data.uniform_", "math.sqrt", "graphML.EdgeVariantGF.weightLSI.data.uniform_", "graphML.EdgeVariantGF.bias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "G", "*", "self", ".", "K", "*", "self", ".", "N", ")", "\n", "self", ".", "weightEV", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "weightLSI", "is", "not", "None", ":", "\n", "            ", "self", ".", "weightLSI", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "self", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantGF.addGSO": [[2608, 2669], ["torch.eye().reshape().repeat().to", "torch.eye().reshape().repeat().to", "torch.eye().reshape().repeat().to", "torch.eye().reshape().repeat().to", "torch.cat.type", "torch.cat.type", "torch.ones.reshape().to", "torch.ones.reshape().to", "torch.cat.to", "torch.cat.to", "torch.cat.type().to", "torch.cat.type().to", "len", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.cat.reshape", "torch.cat.reshape", "torch.cat.repeat", "torch.cat.repeat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.eye().reshape().repeat", "torch.eye().reshape().repeat", "torch.eye().reshape().repeat", "torch.eye().reshape().repeat", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.ones.reshape", "torch.ones.reshape", "torch.cat.type", "torch.cat.type", "torch.eye().reshape", "torch.eye().reshape", "torch.eye().reshape", "torch.eye().reshape", "torch.eye", "torch.eye", "torch.eye", "torch.eye"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "# Save the GSO", "\n", "# Get the identity matrix across all edge features", "\n", "multipleIdentity", "=", "torch", ".", "eye", "(", "self", ".", "N", ")", ".", "reshape", "(", "[", "1", ",", "self", ".", "N", ",", "self", ".", "N", "]", ")", ".", "repeat", "(", "self", ".", "E", ",", "1", ",", "1", ")", ".", "to", "(", "S", ".", "device", ")", "\n", "# Compute the nonzero elements of S+I_{N}", "\n", "sparsityPattern", "=", "(", "(", "torch", ".", "abs", "(", "S", ")", "+", "multipleIdentity", ")", ">", "zeroTolerance", ")", "\n", "# Change from byte tensors to float tensors (or the same type of data as", "\n", "# the GSO)", "\n", "sparsityPattern", "=", "sparsityPattern", ".", "type", "(", "S", ".", "dtype", ")", "\n", "# But now we need to kill everything that is between elements M and N", "\n", "# (only if M < N)", "\n", "if", "self", ".", "M", "<", "self", ".", "N", ":", "\n", "# Create the ones in the row", "\n", "            ", "hybridMaskOnesRows", "=", "torch", ".", "ones", "(", "[", "self", ".", "M", ",", "self", ".", "N", "]", ")", "\n", "# Create the ones int he columns", "\n", "hybridMaskOnesCols", "=", "torch", ".", "ones", "(", "[", "self", ".", "N", "-", "self", ".", "M", ",", "self", ".", "M", "]", ")", "\n", "# Create the zeros", "\n", "hybridMaskZeros", "=", "torch", ".", "zeros", "(", "[", "self", ".", "N", "-", "self", ".", "M", ",", "self", ".", "N", "-", "self", ".", "M", "]", ")", "\n", "# Concatenate the columns", "\n", "hybridMask", "=", "torch", ".", "cat", "(", "(", "hybridMaskOnesCols", ",", "hybridMaskZeros", ")", ",", "dim", "=", "1", ")", "\n", "# Concatenate the rows", "\n", "hybridMask", "=", "torch", ".", "cat", "(", "(", "hybridMaskOnesRows", ",", "hybridMask", ")", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "hybridMask", "=", "torch", ".", "ones", "(", "[", "self", ".", "N", ",", "self", ".", "N", "]", ")", "\n", "# Now that we have the hybrid mask, we need to mask the sparsityPattern", "\n", "# we got so far", "\n", "", "hybridMask", "=", "hybridMask", ".", "reshape", "(", "[", "1", ",", "self", ".", "N", ",", "self", ".", "N", "]", ")", ".", "to", "(", "S", ".", "device", ")", "\n", "#   1 x N x N", "\n", "sparsityPattern", "=", "sparsityPattern", "*", "hybridMask", "\n", "self", ".", "sparsityPattern", "=", "sparsityPattern", ".", "to", "(", "S", ".", "device", ")", "\n", "#   E x N x N", "\n", "# This gives the sparsity pattern for each edge feature", "\n", "# Now, let's create it of the right shape, so we do not have to go", "\n", "# around wasting time with reshapes when called in the forward", "\n", "# The weights have shape F x E x K x G x N x N", "\n", "# The sparsity pattern has shape E x N x N. And we want to make it", "\n", "# 1 x E x K x 1 x N x N. The K dimension is to guarantee that for k=0", "\n", "# we have the identity", "\n", "multipleIdentity", "=", "(", "multipleIdentity", "*", "hybridMask", ")", ".", "reshape", "(", "[", "1", ",", "self", ".", "E", ",", "1", ",", "1", ",", "self", ".", "N", ",", "self", ".", "N", "]", ")", "\n", "if", "self", ".", "K", ">", "1", ":", "\n", "# This gives a 1 x E x 1 x 1 x N x N identity matrix", "\n", "            ", "sparsityPattern", "=", "sparsityPattern", ".", "reshape", "(", "[", "1", ",", "self", ".", "E", ",", "1", ",", "1", ",", "self", ".", "N", ",", "self", ".", "N", "]", ")", "\n", "# This gives a 1 x E x 1 x 1 x N x N sparsity pattern matrix", "\n", "sparsityPattern", "=", "sparsityPattern", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "K", "-", "1", ",", "1", ",", "1", ",", "1", ")", "\n", "# This repeats the sparsity pattern K-1 times giving a matrix of shape", "\n", "#   1 x E x (K-1) x 1 x N x N", "\n", "sparsityPattern", "=", "torch", ".", "cat", "(", "(", "multipleIdentity", ",", "sparsityPattern", ")", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "sparsityPattern", "=", "multipleIdentity", "\n", "# This sholud give me a 1 x E x K x 1 x N x N matrix with the identity", "\n", "# in the first element", "\n", "", "self", ".", "sparsityPatternFull", "=", "sparsityPattern", ".", "type", "(", "S", ".", "dtype", ")", ".", "to", "(", "S", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantGF.forward": [[2670, 2699], ["graphML.EVGF", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "graphML.LSIGF", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EVGF", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.LSIGF", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x is of shape: batchSize x dimInFeatures x numberNodesIn", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "F", "=", "x", ".", "shape", "[", "1", "]", "\n", "Nin", "=", "x", ".", "shape", "[", "2", "]", "\n", "# Mask the parameters", "\n", "self", ".", "Phi", "=", "self", ".", "weightEV", "*", "self", ".", "sparsityPatternFull", "\n", "# And now we add the zero padding", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "zeroPad", "=", "torch", ".", "zeros", "(", "B", ",", "F", ",", "self", ".", "N", "-", "Nin", ")", ".", "type", "(", "x", ".", "dtype", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "zeroPad", ")", ",", "dim", "=", "2", ")", "\n", "# Compute the filter output for the EV part", "\n", "", "uEV", "=", "EVGF", "(", "self", ".", "Phi", ",", "x", ",", "self", ".", "bias", ")", "\n", "# Check if we need an LSI part", "\n", "if", "self", ".", "M", "<", "self", ".", "N", ":", "\n", "# Compute the filter output for the LSI part", "\n", "            ", "uLSI", "=", "LSIGF", "(", "self", ".", "weightLSI", ",", "self", ".", "S", ",", "x", ",", "self", ".", "bias", ")", "\n", "", "else", ":", "\n", "# If we don't, just add zero", "\n", "            ", "uLSI", "=", "torch", ".", "tensor", "(", "0.", ",", "dtype", "=", "uEV", ".", "dtype", ")", ".", "to", "(", "uEV", ".", "device", ")", "\n", "# Add both", "\n", "", "u", "=", "uEV", "+", "uLSI", "\n", "# So far, u is of shape batchSize x dimOutFeatures x numberNodes", "\n", "# And we want to return a tensor of shape", "\n", "# batchSize x dimOutFeatures x numberNodesIn", "\n", "# since the nodes between numberNodesIn and numberNodes are not required", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "u", "=", "torch", ".", "index_select", "(", "u", ",", "2", ",", "torch", ".", "arange", "(", "Nin", ")", ".", "to", "(", "u", ".", "device", ")", ")", "\n", "", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantGF.extra_repr": [[2700, 2713], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, out_features=%d, \"", "%", "(", "\n", "self", ".", "G", ",", "self", ".", "F", ")", "+", "\"shift_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"selected_nodes=%d, \"", "%", "(", "self", ".", "M", ")", "+", "\"number_nodes=%d, \"", "%", "(", "self", ".", "N", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", "is", "not", "None", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterARMA.__init__": [[2769, 2795], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.GraphFilterARMA.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.GraphFilterARMA.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "G", ",", "F", ",", "P", ",", "K", ",", "E", "=", "1", ",", "bias", "=", "True", ",", "tMax", "=", "5", ")", ":", "\n", "# K: Number of filter taps", "\n", "# GSOs will be added later.", "\n", "# This combines both weight scalars and weight vectors.", "\n", "# Bias will always be shared and scalar.", "\n", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save parameters:", "\n", "self", ".", "G", "=", "G", "\n", "self", ".", "F", "=", "F", "\n", "self", ".", "P", "=", "P", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "E", "=", "E", "\n", "self", ".", "tMax", "=", "tMax", "\n", "self", ".", "S", "=", "None", "# No GSO assigned yet", "\n", "# Create parameters:", "\n", "self", ".", "inverseWeight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "E", ",", "P", ",", "G", ")", ")", "\n", "self", ".", "directWeight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "E", ",", "P", ",", "G", ")", ")", "\n", "self", ".", "filterWeight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "E", ",", "K", ",", "G", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterARMA.reset_parameters": [[2796, 2804], ["graphML.GraphFilterARMA.inverseWeight.data.uniform_", "graphML.GraphFilterARMA.directWeight.data.uniform_", "graphML.GraphFilterARMA.filterWeight.data.uniform_", "math.sqrt", "graphML.GraphFilterARMA.bias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "G", "*", "self", ".", "P", ")", "\n", "self", ".", "inverseWeight", ".", "data", ".", "uniform_", "(", "1.", "+", "1.", "/", "stdv", ",", "1.", "+", "2.", "/", "stdv", ")", "\n", "self", ".", "directWeight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "filterWeight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "self", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterARMA.addGSO": [[2805, 2813], ["len"], "methods", ["None"], ["", "", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterARMA.forward": [[2814, 2835], ["graphML.jARMA", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.jARMA", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x is of shape: batchSize x dimInFeatures x numberNodesIn", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "F", "=", "x", ".", "shape", "[", "1", "]", "\n", "Nin", "=", "x", ".", "shape", "[", "2", "]", "\n", "# And now we add the zero padding", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "\n", "torch", ".", "zeros", "(", "B", ",", "F", ",", "self", ".", "N", "-", "Nin", ")", ".", "type", "(", "x", ".", "dtype", ")", ".", "to", "(", "x", ".", "device", ")", "\n", ")", ",", "dim", "=", "2", ")", "\n", "# Compute the filter output", "\n", "", "u", "=", "jARMA", "(", "self", ".", "inverseWeight", ",", "self", ".", "directWeight", ",", "self", ".", "filterWeight", ",", "\n", "self", ".", "S", ",", "x", ",", "b", "=", "self", ".", "bias", ",", "tMax", "=", "self", ".", "tMax", ")", "\n", "# So far, u is of shape batchSize x dimOutFeatures x numberNodes", "\n", "# And we want to return a tensor of shape", "\n", "# batchSize x dimOutFeatures x numberNodesIn", "\n", "# since the nodes between numberNodesIn and numberNodes are not required", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "u", "=", "torch", ".", "index_select", "(", "u", ",", "2", ",", "torch", ".", "arange", "(", "Nin", ")", ".", "to", "(", "u", ".", "device", ")", ")", "\n", "", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterARMA.extra_repr": [[2836, 2848], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, \"", "%", "self", ".", "G", "\n", "reprString", "+=", "\"out_features=%d, \"", "%", "self", ".", "F", "\n", "reprString", "+=", "\"denominator_taps=%d, \"", "%", "self", ".", "P", "\n", "reprString", "+=", "\"residue_taps=%d, \"", "%", "self", ".", "K", "\n", "reprString", "+=", "\"edge_features=%d, \"", "%", "self", ".", "E", "\n", "reprString", "+=", "\"bias=%s, \"", "%", "(", "self", ".", "bias", "is", "not", "None", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphAttentional.__init__": [[2898, 2919], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.GraphAttentional.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "G", ",", "F", ",", "K", ",", "E", "=", "1", ",", "\n", "nonlinearity", "=", "nn", ".", "functional", ".", "relu", ",", "concatenate", "=", "True", ")", ":", "\n", "# K: Number of filter taps", "\n", "# GSOs will be added later.", "\n", "# This combines both weight scalars and weight vectors.", "\n", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save parameters:", "\n", "self", ".", "G", "=", "G", "\n", "self", ".", "F", "=", "F", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "E", "=", "E", "\n", "self", ".", "S", "=", "None", "# No GSO assigned yet", "\n", "self", ".", "nonlinearity", "=", "nonlinearity", "\n", "self", ".", "concatenate", "=", "concatenate", "\n", "# Create parameters:", "\n", "self", ".", "mixer", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "K", ",", "E", ",", "2", "*", "F", ")", ")", "\n", "self", ".", "weight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "K", ",", "E", ",", "F", ",", "G", ")", ")", "\n", "# Initialize parameters", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphAttentional.reset_parameters": [[2920, 2925], ["graphML.GraphAttentional.weight.data.uniform_", "graphML.GraphAttentional.mixer.data.uniform_", "math.sqrt"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "G", "*", "self", ".", "K", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "mixer", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphAttentional.addGSO": [[2926, 2934], ["len"], "methods", ["None"], ["", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphAttentional.forward": [[2935, 2968], ["graphML.graphAttention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "graphML.GraphAttentional.nonlinearity", "torch.index_select.permute().reshape().permute", "torch.index_select.permute().reshape().permute", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "graphML.GraphAttentional.nonlinearity", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.index_select.permute().reshape", "torch.index_select.permute().reshape", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.index_select.permute", "torch.index_select.permute", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.graphAttention", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x is of shape: batchSize x dimInFeatures x numberNodesIn", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "F", "=", "x", ".", "shape", "[", "1", "]", "\n", "Nin", "=", "x", ".", "shape", "[", "2", "]", "\n", "# And now we add the zero padding", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "\n", "torch", ".", "zeros", "(", "B", ",", "F", ",", "self", ".", "N", "-", "Nin", ")", ".", "type", "(", "x", ".", "dtype", ")", ".", "to", "(", "x", ".", "device", ")", "\n", ")", ",", "dim", "=", "2", ")", "\n", "# And get the graph attention output", "\n", "", "y", "=", "graphAttention", "(", "x", ",", "self", ".", "mixer", ",", "self", ".", "weight", ",", "self", ".", "S", ")", "\n", "# This output is of size B x K x F x N. Now, we can either concatenate", "\n", "# them (inner layers) or average them (outer layer)", "\n", "if", "self", ".", "concatenate", ":", "\n", "# When we concatenate we first apply the nonlinearity", "\n", "            ", "y", "=", "self", ".", "nonlinearity", "(", "y", ")", "\n", "# Concatenate: Make it B x KF x N such that first iterates over f", "\n", "# and then over k: (k=0,f=0), (k=0,f=1), ..., (k=0,f=F-1), (k=1,f=0),", "\n", "# (k=1,f=1), ..., etc.", "\n", "y", "=", "y", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "reshape", "(", "[", "B", ",", "self", ".", "N", ",", "self", ".", "K", "*", "self", ".", "F", "]", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "", "else", ":", "\n", "# When we don't, we first average", "\n", "            ", "y", "=", "torch", ".", "mean", "(", "y", ",", "dim", "=", "1", ")", "# B x F x N", "\n", "# And then we apply the nonlinearity", "\n", "y", "=", "self", ".", "nonlinearity", "(", "y", ")", "\n", "\n", "", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "y", "=", "torch", ".", "index_select", "(", "y", ",", "2", ",", "torch", ".", "arange", "(", "Nin", ")", ".", "to", "(", "y", ".", "device", ")", ")", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphAttentional.extra_repr": [[2969, 2978], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, out_features=%d, \"", "%", "(", "\n", "self", ".", "G", ",", "self", ".", "F", ")", "+", "\"attention_heads=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored: number_nodes=%d\"", "%", "(", "self", ".", "N", ")", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterAttentional.__init__": [[3032, 3059], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.GraphFilterAttentional.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.GraphFilterAttentional.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "G", ",", "F", ",", "K", ",", "P", ",", "E", "=", "1", ",", "bias", "=", "True", ",", "\n", "nonlinearity", "=", "nn", ".", "functional", ".", "relu", ",", "concatenate", "=", "True", ")", ":", "\n", "# P: Number of heads", "\n", "# GSOs will be added later.", "\n", "# This combines both weight scalars and weight vectors.", "\n", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save parameters:", "\n", "self", ".", "G", "=", "G", "# in_features", "\n", "self", ".", "F", "=", "F", "# out_features", "\n", "self", ".", "K", "=", "K", "# filter_taps", "\n", "self", ".", "P", "=", "P", "# attention_heads", "\n", "self", ".", "E", "=", "E", "# edge_features", "\n", "self", ".", "S", "=", "None", "# No GSO assigned yet", "\n", "self", ".", "nonlinearity", "=", "nonlinearity", "\n", "self", ".", "concatenate", "=", "concatenate", "\n", "# Create parameters:", "\n", "self", ".", "mixer", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "P", ",", "E", ",", "2", "*", "F", ")", ")", "\n", "self", ".", "weight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "P", ",", "E", ",", "F", ",", "G", ")", ")", "\n", "self", ".", "filterWeight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "E", ",", "K", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterAttentional.reset_parameters": [[3060, 3068], ["graphML.GraphFilterAttentional.weight.data.uniform_", "graphML.GraphFilterAttentional.mixer.data.uniform_", "graphML.GraphFilterAttentional.filterWeight.data.uniform_", "math.sqrt", "graphML.GraphFilterAttentional.bias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "G", "*", "self", ".", "P", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "mixer", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "filterWeight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "self", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterAttentional.addGSO": [[3069, 3077], ["len"], "methods", ["None"], ["", "", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterAttentional.forward": [[3078, 3112], ["graphML.graphAttentionLSIGF", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "graphML.GraphFilterAttentional.nonlinearity", "torch.index_select.permute().reshape().permute", "torch.index_select.permute().reshape().permute", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "graphML.GraphFilterAttentional.nonlinearity", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.index_select.permute().reshape", "torch.index_select.permute().reshape", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.index_select.permute", "torch.index_select.permute", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.graphAttentionLSIGF", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x is of shape: batchSize x dimInFeatures x numberNodesIn", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "F", "=", "x", ".", "shape", "[", "1", "]", "\n", "Nin", "=", "x", ".", "shape", "[", "2", "]", "\n", "# And now we add the zero padding", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "\n", "torch", ".", "zeros", "(", "B", ",", "F", ",", "self", ".", "N", "-", "Nin", ")", ".", "type", "(", "x", ".", "dtype", ")", ".", "to", "(", "x", ".", "device", ")", "\n", ")", ",", "dim", "=", "2", ")", "\n", "# And get the graph attention output", "\n", "", "y", "=", "graphAttentionLSIGF", "(", "self", ".", "filterWeight", ",", "x", ",", "self", ".", "mixer", ",", "self", ".", "weight", ",", "\n", "self", ".", "S", ",", "b", "=", "self", ".", "bias", ")", "\n", "# This output is of size B x P x F x N. Now, we can either concatenate", "\n", "# them (inner layers) or average them (outer layer)", "\n", "if", "self", ".", "concatenate", ":", "\n", "# When we concatenate we first apply the nonlinearity", "\n", "            ", "y", "=", "self", ".", "nonlinearity", "(", "y", ")", "\n", "# Concatenate: Make it B x PF x N such that first iterates over f", "\n", "# and then over p: (p=0,f=0), (p=0,f=1), ..., (p=0,f=F-1), (p=1,f=0),", "\n", "# (p=1,f=1), ..., etc.", "\n", "y", "=", "y", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "reshape", "(", "[", "B", ",", "self", ".", "N", ",", "self", ".", "P", "*", "self", ".", "F", "]", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "", "else", ":", "\n", "# When we don't, we first average", "\n", "            ", "y", "=", "torch", ".", "mean", "(", "y", ",", "dim", "=", "1", ")", "# B x F x N", "\n", "# And then we apply the nonlinearity", "\n", "y", "=", "self", ".", "nonlinearity", "(", "y", ")", "\n", "\n", "", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "y", "=", "torch", ".", "index_select", "(", "y", ",", "2", ",", "torch", ".", "arange", "(", "Nin", ")", ".", "to", "(", "y", ".", "device", ")", ")", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilterAttentional.extra_repr": [[3113, 3125], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, \"", "%", "self", ".", "G", "\n", "reprString", "+=", "\"out_features=%d, \"", "%", "self", ".", "F", "\n", "reprString", "+=", "\"filter_taps=%d, \"", "%", "self", ".", "K", "\n", "reprString", "+=", "\"attention_heads=%d, \"", "%", "self", ".", "P", "\n", "reprString", "+=", "\"edge_features=%d, \"", "%", "self", ".", "E", "\n", "reprString", "+=", "\"bias=%s, \"", "%", "(", "self", ".", "bias", "is", "not", "None", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored: number_nodes=%d\"", "%", "(", "self", ".", "N", ")", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantAttentional.__init__": [[3180, 3207], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.EdgeVariantAttentional.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.EdgeVariantAttentional.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "G", ",", "F", ",", "K", ",", "P", ",", "E", "=", "1", ",", "bias", "=", "True", ",", "\n", "nonlinearity", "=", "nn", ".", "functional", ".", "relu", ",", "concatenate", "=", "True", ")", ":", "\n", "# K: Number of filter taps", "\n", "# P: Number of attention heads", "\n", "# GSOs will be added later.", "\n", "# This combines both weight scalars and weight vectors.", "\n", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save parameters:", "\n", "self", ".", "G", "=", "G", "# in_features", "\n", "self", ".", "F", "=", "F", "# out_features", "\n", "self", ".", "K", "=", "K", "# filter_taps", "\n", "self", ".", "P", "=", "P", "# attention_heads", "\n", "self", ".", "E", "=", "E", "# edge_features", "\n", "self", ".", "S", "=", "None", "# No GSO assigned yet", "\n", "self", ".", "nonlinearity", "=", "nonlinearity", "\n", "self", ".", "concatenate", "=", "concatenate", "\n", "# Create parameters:", "\n", "self", ".", "mixer", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "P", ",", "K", ",", "E", ",", "2", "*", "F", ")", ")", "\n", "self", ".", "weight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "P", ",", "K", ",", "E", ",", "F", ",", "G", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantAttentional.reset_parameters": [[3208, 3215], ["graphML.EdgeVariantAttentional.weight.data.uniform_", "graphML.EdgeVariantAttentional.mixer.data.uniform_", "math.sqrt", "graphML.EdgeVariantAttentional.bias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "G", "*", "self", ".", "K", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "mixer", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "self", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantAttentional.addGSO": [[3216, 3224], ["len"], "methods", ["None"], ["", "", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantAttentional.forward": [[3225, 3258], ["graphML.graphAttentionEVGF", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "graphML.EdgeVariantAttentional.nonlinearity", "torch.index_select.permute().reshape().permute", "torch.index_select.permute().reshape().permute", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "graphML.EdgeVariantAttentional.nonlinearity", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.zeros().type().to", "torch.index_select.permute().reshape", "torch.index_select.permute().reshape", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.index_select.permute", "torch.index_select.permute", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.graphAttentionEVGF", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x is of shape: batchSize x dimInFeatures x numberNodesIn", "\n", "        ", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "F", "=", "x", ".", "shape", "[", "1", "]", "\n", "Nin", "=", "x", ".", "shape", "[", "2", "]", "\n", "# And now we add the zero padding", "\n", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "\n", "torch", ".", "zeros", "(", "B", ",", "F", ",", "self", ".", "N", "-", "Nin", ")", ".", "type", "(", "x", ".", "dtype", ")", ".", "to", "(", "x", ".", "device", ")", "\n", ")", ",", "dim", "=", "2", ")", "\n", "# And get the graph attention output", "\n", "", "y", "=", "graphAttentionEVGF", "(", "x", ",", "self", ".", "mixer", ",", "self", ".", "weight", ",", "self", ".", "S", ",", "b", "=", "self", ".", "bias", ")", "\n", "# This output is of size B x K x F x N. Now, we can either concatenate", "\n", "# them (inner layers) or average them (outer layer)", "\n", "if", "self", ".", "concatenate", ":", "\n", "# When we concatenate we first apply the nonlinearity", "\n", "            ", "y", "=", "self", ".", "nonlinearity", "(", "y", ")", "\n", "# Concatenate: Make it B x KF x N such that first iterates over f", "\n", "# and then over k: (k=0,f=0), (k=0,f=1), ..., (k=0,f=F-1), (k=1,f=0),", "\n", "# (k=1,f=1), ..., etc.", "\n", "y", "=", "y", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "reshape", "(", "[", "B", ",", "self", ".", "N", ",", "self", ".", "K", "*", "self", ".", "F", "]", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "", "else", ":", "\n", "# When we don't, we first average", "\n", "            ", "y", "=", "torch", ".", "mean", "(", "y", ",", "dim", "=", "1", ")", "# B x F x N", "\n", "# And then we apply the nonlinearity", "\n", "y", "=", "self", ".", "nonlinearity", "(", "y", ")", "\n", "\n", "", "if", "Nin", "<", "self", ".", "N", ":", "\n", "            ", "y", "=", "torch", ".", "index_select", "(", "y", ",", "2", ",", "torch", ".", "arange", "(", "Nin", ")", ".", "to", "(", "y", ".", "device", ")", ")", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeVariantAttentional.extra_repr": [[3259, 3271], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, \"", "%", "self", ".", "G", "\n", "reprString", "+=", "\"out_features=%d, \"", "%", "self", ".", "F", "\n", "reprString", "+=", "\"filter_taps=%d, \"", "%", "self", ".", "K", "\n", "reprString", "+=", "\"attention_heads=%d, \"", "%", "self", ".", "P", "\n", "reprString", "+=", "\"edge_features=%d, \"", "%", "self", ".", "E", "\n", "reprString", "+=", "\"bias=%s, \"", "%", "(", "self", ".", "bias", "is", "not", "None", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored: number_nodes=%d\"", "%", "(", "self", ".", "N", ")", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter_DB.__init__": [[3331, 3353], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.GraphFilter_DB.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.GraphFilter_DB.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "G", ",", "F", ",", "K", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# K: Number of filter taps", "\n", "# GSOs will be added later.", "\n", "# This combines both weight scalars and weight vectors.", "\n", "# Bias will always be shared and scalar.", "\n", "\n", "# Initialize parent", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Save parameters:", "\n", "self", ".", "G", "=", "G", "\n", "self", ".", "F", "=", "F", "\n", "self", ".", "K", "=", "K", "\n", "self", ".", "E", "=", "E", "\n", "self", ".", "S", "=", "None", "# No GSO assigned yet", "\n", "# Create parameters:", "\n", "self", ".", "weight", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "E", ",", "K", ",", "G", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "F", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter_DB.reset_parameters": [[3354, 3360], ["graphML.GraphFilter_DB.weight.data.uniform_", "math.sqrt", "graphML.GraphFilter_DB.bias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "G", "*", "self", ".", "K", ")", "\n", "self", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "self", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter_DB.addGSO": [[3361, 3369], ["len"], "methods", ["None"], ["", "", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 5 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "5", "\n", "# S is of shape B x T x E x N x N", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "3", "]", "\n", "assert", "S", ".", "shape", "[", "4", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter_DB.forward": [[3370, 3383], ["graphML.LSIGF_DB", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.LSIGF_DB"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# x is of shape: batchSize x time x dimInFeatures x numberNodesIn", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "self", ".", "S", ".", "shape", "[", "0", "]", "==", "B", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "self", ".", "S", ".", "shape", "[", "1", "]", "==", "T", "\n", "#F = x.shape[2]", "\n", "assert", "x", ".", "shape", "[", "3", "]", "==", "self", ".", "N", "\n", "# Compute the filter output", "\n", "u", "=", "LSIGF_DB", "(", "self", ".", "weight", ",", "self", ".", "S", ",", "x", ",", "self", ".", "bias", ")", "\n", "# u is of shape batchSize x time x dimOutFeatures x numberNodes", "\n", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GraphFilter_DB.extra_repr": [[3384, 3394], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, out_features=%d, \"", "%", "(", "\n", "self", ".", "G", ",", "self", ".", "F", ")", "+", "\"filter_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", "is", "not", "None", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState_DB.__init__": [[3450, 3474], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.HiddenState_DB.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.HiddenState_DB.register_parameter", "graphML.HiddenState_DB.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "F", ",", "H", ",", "K", ",", "nonlinearity", "=", "torch", ".", "tanh", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "F", "=", "F", "# Input Features", "\n", "self", ".", "H", "=", "H", "# Hidden Features", "\n", "self", ".", "K", "=", "K", "# Filter taps", "\n", "self", ".", "E", "=", "E", "# Number of edge features", "\n", "self", ".", "S", "=", "None", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "self", ".", "sigma", "=", "nonlinearity", "# torch.nn.functional", "\n", "\n", "# Create parameters:", "\n", "self", ".", "aWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "F", ")", ")", "\n", "self", ".", "bWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "H", ")", ")", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "self", ".", "zBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'xBias'", ",", "None", ")", "\n", "self", ".", "register_parameter", "(", "'zBias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState_DB.reset_parameters": [[3475, 3483], ["graphML.HiddenState_DB.aWeights.data.uniform_", "graphML.HiddenState_DB.bWeights.data.uniform_", "math.sqrt", "graphML.HiddenState_DB.xBias.data.uniform_", "graphML.HiddenState_DB.zBias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "F", "*", "self", ".", "K", ")", "\n", "self", ".", "aWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "bWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "zBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState_DB.forward": [[3484, 3518], ["graphML.GRNN_DB", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select.unsqueeze", "torch.index_select.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GRNN_DB"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "z0", ")", ":", "\n", "\n", "        ", "assert", "self", ".", "S", "is", "not", "None", "\n", "\n", "# Input", "\n", "#   S: B x T (x E) x N x N", "\n", "#   x: B x T x F x N", "\n", "#   z0: B x H x N", "\n", "# Output", "\n", "#   z: B x T x H x N", "\n", "\n", "# Check dimensions", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "self", ".", "S", ".", "shape", "[", "0", "]", "==", "B", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "self", ".", "S", ".", "shape", "[", "1", "]", "==", "T", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "\n", "N", "=", "x", ".", "shape", "[", "3", "]", "\n", "\n", "assert", "len", "(", "z0", ".", "shape", ")", "==", "3", "\n", "assert", "z0", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "z0", ".", "shape", "[", "1", "]", "==", "self", ".", "H", "\n", "assert", "z0", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "z", "=", "GRNN_DB", "(", "self", ".", "aWeights", ",", "self", ".", "bWeights", ",", "\n", "self", ".", "S", ",", "x", ",", "z0", ",", "self", ".", "sigma", ",", "\n", "xBias", "=", "self", ".", "xBias", ",", "zBias", "=", "self", ".", "zBias", ")", "\n", "\n", "zT", "=", "torch", ".", "index_select", "(", "z", ",", "1", ",", "torch", ".", "tensor", "(", "T", "-", "1", ",", "device", "=", "z", ".", "device", ")", ")", "\n", "# Give out the last one, to be used as starting point if used in", "\n", "# succession", "\n", "\n", "return", "z", ",", "zT", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState_DB.addGSO": [[3519, 3527], ["len"], "methods", ["None"], ["", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 5 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "5", "\n", "# S is of shape B x T x E x N x N", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "3", "]", "\n", "assert", "S", ".", "shape", "[", "4", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState_DB.extra_repr": [[3528, 3539], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, hidden_features=%d, \"", "%", "(", "\n", "self", ".", "F", ",", "self", ".", "H", ")", "+", "\"filter_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", ")", "+", "\"nonlinearity=%s\"", "%", "(", "self", ".", "sigma", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState.__init__": [[3595, 3619], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.HiddenState.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.HiddenState.register_parameter", "graphML.HiddenState.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "F", ",", "H", ",", "K", ",", "nonlinearity", "=", "torch", ".", "tanh", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "F", "=", "F", "# Input Features", "\n", "self", ".", "H", "=", "H", "# Hidden Features", "\n", "self", ".", "K", "=", "K", "# Filter taps", "\n", "self", ".", "E", "=", "E", "# Number of edge features", "\n", "self", ".", "S", "=", "None", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "self", ".", "sigma", "=", "nonlinearity", "# torch.nn.functional", "\n", "\n", "# Create parameters:", "\n", "self", ".", "aWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "F", ")", ")", "\n", "self", ".", "bWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "H", ")", ")", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "self", ".", "zBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'xBias'", ",", "None", ")", "\n", "self", ".", "register_parameter", "(", "'zBias'", ",", "None", ")", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState.reset_parameters": [[3620, 3628], ["graphML.HiddenState.aWeights.data.uniform_", "graphML.HiddenState.bWeights.data.uniform_", "math.sqrt", "graphML.HiddenState.xBias.data.uniform_", "graphML.HiddenState.zBias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "F", "*", "self", ".", "K", ")", "\n", "self", ".", "aWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "bWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "zBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState.forward": [[3629, 3661], ["graphML.GatedGRNN", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select.unsqueeze", "torch.index_select.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GatedGRNN"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "z0", ")", ":", "\n", "\n", "        ", "assert", "self", ".", "S", "is", "not", "None", "\n", "\n", "# Input", "\n", "#   S: (E) x N x N", "\n", "#   x: B x T x F x N", "\n", "#   z0: B x H x N", "\n", "# Output", "\n", "#   z: B x T x H x N", "\n", "\n", "# Check dimensions", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "\n", "N", "=", "x", ".", "shape", "[", "3", "]", "\n", "\n", "assert", "len", "(", "z0", ".", "shape", ")", "==", "3", "\n", "assert", "z0", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "z0", ".", "shape", "[", "1", "]", "==", "self", ".", "H", "\n", "assert", "z0", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "z", "=", "GatedGRNN", "(", "self", ".", "aWeights", ",", "self", ".", "bWeights", ",", "\n", "self", ".", "S", ",", "x", ",", "z0", ",", "self", ".", "sigma", ",", "\n", "xBias", "=", "self", ".", "xBias", ",", "zBias", "=", "self", ".", "zBias", ")", "\n", "\n", "zT", "=", "torch", ".", "index_select", "(", "z", ",", "1", ",", "torch", ".", "tensor", "(", "T", "-", "1", ",", "device", "=", "z", ".", "device", ")", ")", "\n", "# Give out the last one, to be used as starting point if used in", "\n", "# succession", "\n", "\n", "return", "z", ",", "zT", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState.addGSO": [[3662, 3670], ["len"], "methods", ["None"], ["", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.HiddenState.extra_repr": [[3671, 3682], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, hidden_features=%d, \"", "%", "(", "\n", "self", ".", "F", ",", "self", ".", "H", ")", "+", "\"filter_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", ")", "+", "\"nonlinearity=%s\"", "%", "(", "self", ".", "sigma", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.TimeGatedHiddenState.__init__": [[3739, 3771], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.HiddenState", "graphML.HiddenState", "graphML.TimeGatedHiddenState.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.TimeGatedHiddenState.register_parameter", "graphML.TimeGatedHiddenState.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "F", ",", "H", ",", "K", ",", "nonlinearity", "=", "torch", ".", "tanh", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "F", "=", "F", "# Input Features", "\n", "self", ".", "H", "=", "H", "# Hidden Features", "\n", "self", ".", "K", "=", "K", "# Filter taps", "\n", "self", ".", "E", "=", "E", "# Number of edge features", "\n", "self", ".", "S", "=", "None", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "self", ".", "sigma", "=", "nonlinearity", "# torch.nn.functional", "\n", "\n", "# Create parameters of the main GRNN", "\n", "self", ".", "aWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "F", ")", ")", "\n", "self", ".", "bWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "H", ")", ")", "\n", "\n", "# Create input gate GRNN ", "\n", "self", ".", "inputGateGRNN", "=", "HiddenState", "(", "F", ",", "H", ",", "K", ",", "bias", "=", "bias", ")", "\n", "\n", "# Create forget gate GRNN", "\n", "self", ".", "forgetGateGRNN", "=", "HiddenState", "(", "F", ",", "H", ",", "K", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "self", ".", "zBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'xBias'", ",", "None", ")", "\n", "self", ".", "register_parameter", "(", "'zBias'", ",", "None", ")", "\n", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.TimeGatedHiddenState.reset_parameters": [[3774, 3782], ["graphML.TimeGatedHiddenState.aWeights.data.uniform_", "graphML.TimeGatedHiddenState.bWeights.data.uniform_", "math.sqrt", "graphML.TimeGatedHiddenState.xBias.data.uniform_", "graphML.TimeGatedHiddenState.zBias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "F", "*", "self", ".", "K", ")", "\n", "self", ".", "aWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "bWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "zBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.TimeGatedHiddenState.forward": [[3783, 3827], ["graphML.TimeGatedHiddenState.inputGateGRNN", "zHat.reshape.reshape.reshape", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "qHat.unsqueeze.unsqueeze.unsqueeze", "graphML.TimeGatedHiddenState.forgetGateGRNN", "zCheck.reshape.reshape.reshape", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "qCheck.unsqueeze.unsqueeze.unsqueeze", "graphML.GatedGRNN", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "len", "len", "graphML.TimeGatedHiddenState.inputGateFC", "graphML.TimeGatedHiddenState.forgetGateFC", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select.unsqueeze", "torch.index_select.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GatedGRNN"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "z0", ")", ":", "\n", "\n", "        ", "assert", "self", ".", "S", "is", "not", "None", "\n", "\n", "# Input", "\n", "#   S: (E) x N x N", "\n", "#   x: B x T x F x N", "\n", "#   z0: B x H x N", "\n", "# Output", "\n", "#   z: B x T x H x N", "\n", "\n", "# Check dimensions", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "\n", "N", "=", "x", ".", "shape", "[", "3", "]", "\n", "\n", "assert", "len", "(", "z0", ".", "shape", ")", "==", "3", "\n", "assert", "z0", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "z0", ".", "shape", "[", "1", "]", "==", "self", ".", "H", "\n", "assert", "z0", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "# Calculating input gate", "\n", "zHat", ",", "_", "=", "self", ".", "inputGateGRNN", "(", "x", ",", "z0", ")", "\n", "zHat", "=", "zHat", ".", "reshape", "(", "(", "B", ",", "T", ",", "self", ".", "H", "*", "N", ")", ")", "\n", "qHat", "=", "torch", ".", "sigmoid", "(", "self", ".", "inputGateFC", "(", "zHat", ")", ")", "\n", "qHat", "=", "qHat", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Calculating forget gate", "\n", "zCheck", ",", "_", "=", "self", ".", "forgetGateGRNN", "(", "x", ",", "z0", ")", "\n", "zCheck", "=", "zCheck", ".", "reshape", "(", "(", "B", ",", "T", ",", "self", ".", "H", "*", "N", ")", ")", "\n", "qCheck", "=", "torch", ".", "sigmoid", "(", "self", ".", "forgetGateFC", "(", "zCheck", ")", ")", "\n", "qCheck", "=", "qCheck", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "z", "=", "GatedGRNN", "(", "self", ".", "aWeights", ",", "self", ".", "bWeights", ",", "\n", "self", ".", "S", ",", "x", ",", "z0", ",", "self", ".", "sigma", ",", "qHat", ",", "qCheck", ",", "\n", "xBias", "=", "self", ".", "xBias", ",", "zBias", "=", "self", ".", "zBias", ")", "\n", "\n", "zT", "=", "torch", ".", "index_select", "(", "z", ",", "1", ",", "torch", ".", "tensor", "(", "T", "-", "1", ",", "device", "=", "z", ".", "device", ")", ")", "\n", "# Give out the last one, to be used as starting point if used in", "\n", "# succession", "\n", "\n", "return", "z", ",", "zT", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.TimeGatedHiddenState.addGSO": [[3828, 3844], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "graphML.TimeGatedHiddenState.inputGateGRNN.addGSO", "graphML.TimeGatedHiddenState.forgetGateGRNN.addGSO", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n", "# Create fully connected layers mapping hidden states to gates", "\n", "self", ".", "inputGateFC", "=", "nn", ".", "Linear", "(", "self", ".", "H", "*", "self", ".", "N", ",", "1", ",", "self", ".", "bias", ")", "\n", "self", ".", "forgetGateFC", "=", "nn", ".", "Linear", "(", "self", ".", "H", "*", "self", ".", "N", ",", "1", ",", "self", ".", "bias", ")", "\n", "\n", "# Add GSO to gate GRNNs", "\n", "self", ".", "inputGateGRNN", ".", "addGSO", "(", "S", ")", "\n", "self", ".", "forgetGateGRNN", ".", "addGSO", "(", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.TimeGatedHiddenState.extra_repr": [[3845, 3856], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, hidden_features=%d, \"", "%", "(", "\n", "self", ".", "F", ",", "self", ".", "H", ")", "+", "\"filter_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", ")", "+", "\"nonlinearity=%s\"", "%", "(", "self", ".", "sigma", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeGatedHiddenState.__init__": [[3913, 3945], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.HiddenState", "graphML.HiddenState", "graphML.NodeGatedHiddenState.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.NodeGatedHiddenState.register_parameter", "graphML.NodeGatedHiddenState.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "F", ",", "H", ",", "K", ",", "nonlinearity", "=", "torch", ".", "tanh", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "F", "=", "F", "# Input Features", "\n", "self", ".", "H", "=", "H", "# Hidden Features", "\n", "self", ".", "K", "=", "K", "# Filter taps", "\n", "self", ".", "E", "=", "E", "# Number of edge features", "\n", "self", ".", "S", "=", "None", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "self", ".", "sigma", "=", "nonlinearity", "# torch.nn.functional", "\n", "\n", "# Create parameters of the main GRNN", "\n", "self", ".", "aWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "F", ")", ")", "\n", "self", ".", "bWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "H", ")", ")", "\n", "\n", "# Create input gate GRNN ", "\n", "self", ".", "inputGateGRNN", "=", "HiddenState", "(", "F", ",", "H", ",", "K", ",", "bias", "=", "bias", ")", "\n", "\n", "# Create forget gate GRNN", "\n", "self", ".", "forgetGateGRNN", "=", "HiddenState", "(", "F", ",", "H", ",", "K", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "self", ".", "zBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'xBias'", ",", "None", ")", "\n", "self", ".", "register_parameter", "(", "'zBias'", ",", "None", ")", "\n", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeGatedHiddenState.reset_parameters": [[3948, 3956], ["graphML.NodeGatedHiddenState.aWeights.data.uniform_", "graphML.NodeGatedHiddenState.bWeights.data.uniform_", "math.sqrt", "graphML.NodeGatedHiddenState.xBias.data.uniform_", "graphML.NodeGatedHiddenState.zBias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "F", "*", "self", ".", "K", ")", "\n", "self", ".", "aWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "bWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "zBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeGatedHiddenState.forward": [[3957, 4001], ["graphML.NodeGatedHiddenState.inputGateGRNN", "zHat.reshape.reshape.reshape", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "qHat.reshape.reshape.reshape", "graphML.NodeGatedHiddenState.forgetGateGRNN", "zCheck.reshape.reshape.reshape", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "qCheck.reshape.reshape.reshape", "graphML.GatedGRNN", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "len", "len", "graphML.NodeGatedHiddenState.inputGateGraphFilter", "graphML.NodeGatedHiddenState.forgetGateGraphFilter", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select.unsqueeze", "torch.index_select.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GatedGRNN"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "z0", ")", ":", "\n", "\n", "        ", "assert", "self", ".", "S", "is", "not", "None", "\n", "\n", "# Input", "\n", "#   S: (E) x N x N", "\n", "#   x: B x T x F x N", "\n", "#   z0: B x H x N", "\n", "# Output", "\n", "#   z: B x T x H x N", "\n", "\n", "# Check dimensions", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "\n", "N", "=", "x", ".", "shape", "[", "3", "]", "\n", "\n", "assert", "len", "(", "z0", ".", "shape", ")", "==", "3", "\n", "assert", "z0", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "z0", ".", "shape", "[", "1", "]", "==", "self", ".", "H", "\n", "assert", "z0", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "# Calculating input gate", "\n", "zHat", ",", "_", "=", "self", ".", "inputGateGRNN", "(", "x", ",", "z0", ")", "\n", "zHat", "=", "zHat", ".", "reshape", "(", "(", "B", "*", "T", ",", "self", ".", "H", ",", "N", ")", ")", "\n", "qHat", "=", "torch", ".", "sigmoid", "(", "self", ".", "inputGateGraphFilter", "(", "zHat", ")", ")", "\n", "qHat", "=", "qHat", ".", "reshape", "(", "(", "B", ",", "T", ",", "1", ",", "N", ")", ")", "\n", "\n", "# Calculating forget gate", "\n", "zCheck", ",", "_", "=", "self", ".", "forgetGateGRNN", "(", "x", ",", "z0", ")", "\n", "zCheck", "=", "zCheck", ".", "reshape", "(", "(", "B", "*", "T", ",", "self", ".", "H", ",", "N", ")", ")", "\n", "qCheck", "=", "torch", ".", "sigmoid", "(", "self", ".", "forgetGateGraphFilter", "(", "zCheck", ")", ")", "\n", "qCheck", "=", "qCheck", ".", "reshape", "(", "(", "B", ",", "T", ",", "1", ",", "N", ")", ")", "\n", "\n", "z", "=", "GatedGRNN", "(", "self", ".", "aWeights", ",", "self", ".", "bWeights", ",", "\n", "self", ".", "S", ",", "x", ",", "z0", ",", "self", ".", "sigma", ",", "qHat", ",", "qCheck", ",", "\n", "xBias", "=", "self", ".", "xBias", ",", "zBias", "=", "self", ".", "zBias", ")", "\n", "\n", "zT", "=", "torch", ".", "index_select", "(", "z", ",", "1", ",", "torch", ".", "tensor", "(", "T", "-", "1", ",", "device", "=", "z", ".", "device", ")", ")", "\n", "# Give out the last one, to be used as starting point if used in", "\n", "# succession", "\n", "\n", "return", "z", ",", "zT", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeGatedHiddenState.addGSO": [[4002, 4020], ["graphML.GraphFilter", "graphML.GraphFilter", "graphML.NodeGatedHiddenState.inputGateGRNN.addGSO", "graphML.NodeGatedHiddenState.forgetGateGRNN.addGSO", "graphML.NodeGatedHiddenState.inputGateGraphFilter.addGSO", "graphML.NodeGatedHiddenState.forgetGateGraphFilter.addGSO", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n", "# Create linear filters mapping hidden states to gates", "\n", "self", ".", "inputGateGraphFilter", "=", "GraphFilter", "(", "self", ".", "H", ",", "1", ",", "self", ".", "K", ",", "bias", "=", "self", ".", "bias", ")", "\n", "self", ".", "forgetGateGraphFilter", "=", "GraphFilter", "(", "self", ".", "H", ",", "1", ",", "self", ".", "K", ",", "bias", "=", "self", ".", "bias", ")", "\n", "\n", "# Add GSO to gate GRNNs", "\n", "self", ".", "inputGateGRNN", ".", "addGSO", "(", "S", ")", "\n", "self", ".", "forgetGateGRNN", ".", "addGSO", "(", "S", ")", "\n", "self", ".", "inputGateGraphFilter", ".", "addGSO", "(", "S", ")", "\n", "self", ".", "forgetGateGraphFilter", ".", "addGSO", "(", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NodeGatedHiddenState.extra_repr": [[4021, 4032], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, hidden_features=%d, \"", "%", "(", "\n", "self", ".", "F", ",", "self", ".", "H", ")", "+", "\"filter_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", ")", "+", "\"nonlinearity=%s\"", "%", "(", "self", ".", "sigma", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.__init__": [[4089, 4121], ["torch.Module.__init__", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.HiddenState", "graphML.HiddenState", "graphML.EdgeGatedHiddenState.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "torch.parameter.Parameter", "graphML.EdgeGatedHiddenState.register_parameter", "graphML.EdgeGatedHiddenState.register_parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters"], ["def", "__init__", "(", "self", ",", "F", ",", "H", ",", "K", ",", "nonlinearity", "=", "torch", ".", "tanh", ",", "E", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "F", "=", "F", "# Input Features", "\n", "self", ".", "H", "=", "H", "# Hidden Features", "\n", "self", ".", "K", "=", "K", "# Filter taps", "\n", "self", ".", "E", "=", "E", "# Number of edge features", "\n", "self", ".", "S", "=", "None", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "self", ".", "sigma", "=", "nonlinearity", "# torch.nn.functional", "\n", "\n", "# Create parameters of the main GRNN", "\n", "self", ".", "aWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "F", ")", ")", "\n", "self", ".", "bWeights", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "E", ",", "K", ",", "H", ")", ")", "\n", "\n", "# Create input gate GRNN ", "\n", "self", ".", "inputGateGRNN", "=", "HiddenState", "(", "F", ",", "H", ",", "K", ",", "bias", "=", "bias", ")", "\n", "\n", "# Create forget gate GRNN", "\n", "self", ".", "forgetGateGRNN", "=", "HiddenState", "(", "F", ",", "H", ",", "K", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "self", ".", "zBias", "=", "nn", ".", "parameter", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "H", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'xBias'", ",", "None", ")", "\n", "self", ".", "register_parameter", "(", "'zBias'", ",", "None", ")", "\n", "\n", "# Initialize parameters", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.reset_parameters": [[4124, 4132], ["graphML.EdgeGatedHiddenState.aWeights.data.uniform_", "graphML.EdgeGatedHiddenState.bWeights.data.uniform_", "math.sqrt", "graphML.EdgeGatedHiddenState.xBias.data.uniform_", "graphML.EdgeGatedHiddenState.zBias.data.uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Taken from _ConvNd initialization of parameters:", "\n", "        ", "stdv", "=", "1.", "/", "math", ".", "sqrt", "(", "self", ".", "F", "*", "self", ".", "K", ")", "\n", "self", ".", "aWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "bWeights", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "if", "self", ".", "bias", ":", "\n", "            ", "self", ".", "xBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "self", ".", "zBias", ".", "data", ".", "uniform_", "(", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.forward": [[4133, 4179], ["graphML.EdgeGatedHiddenState.inputGateGRNN", "zHat.reshape.reshape.reshape", "graphML.learnAttentionGSO", "qHat.reshape.reshape.reshape", "graphML.EdgeGatedHiddenState.forgetGateGRNN", "zCheck.reshape.reshape.reshape", "graphML.learnAttentionGSO", "qCheck.reshape.reshape.reshape", "graphML.GatedGRNN", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select.unsqueeze", "torch.index_select.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.learnAttentionGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.learnAttentionGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GatedGRNN"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "z0", ")", ":", "\n", "\n", "        ", "assert", "self", ".", "S", "is", "not", "None", "\n", "\n", "# Input", "\n", "#   S: (E) x N x N", "\n", "#   x: B x T x F x N", "\n", "#   z0: B x H x N", "\n", "# Output", "\n", "#   z: B x T x H x N", "\n", "\n", "# Check dimensions", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "\n", "N", "=", "x", ".", "shape", "[", "3", "]", "\n", "\n", "assert", "len", "(", "z0", ".", "shape", ")", "==", "3", "\n", "assert", "z0", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "z0", ".", "shape", "[", "1", "]", "==", "self", ".", "H", "\n", "assert", "z0", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "# Calculating input gate", "\n", "zHat", ",", "_", "=", "self", ".", "inputGateGRNN", "(", "x", ",", "z0", ")", "\n", "zHat", "=", "zHat", ".", "reshape", "(", "(", "B", "*", "T", ",", "self", ".", "H", ",", "N", ")", ")", "\n", "qHat", "=", "learnAttentionGSO", "(", "zHat", ",", "self", ".", "inputGateGAT", ".", "mixer", ",", "\n", "self", ".", "inputGateGAT", ".", "weight", ",", "self", ".", "inputGateGAT", ".", "S", ")", "\n", "qHat", "=", "qHat", ".", "reshape", "(", "(", "B", ",", "T", ",", "1", ",", "N", ",", "N", ")", ")", "\n", "\n", "# Calculating forget gate", "\n", "zCheck", ",", "_", "=", "self", ".", "forgetGateGRNN", "(", "x", ",", "z0", ")", "\n", "zCheck", "=", "zCheck", ".", "reshape", "(", "(", "B", "*", "T", ",", "self", ".", "H", ",", "N", ")", ")", "\n", "qCheck", "=", "learnAttentionGSO", "(", "zCheck", ",", "self", ".", "forgetGateGAT", ".", "mixer", ",", "\n", "self", ".", "forgetGateGAT", ".", "weight", ",", "self", ".", "forgetGateGAT", ".", "S", ")", "\n", "qCheck", "=", "qCheck", ".", "reshape", "(", "(", "B", ",", "T", ",", "1", ",", "N", ",", "N", ")", ")", "\n", "\n", "z", "=", "GatedGRNN", "(", "self", ".", "aWeights", ",", "self", ".", "bWeights", ",", "\n", "self", ".", "S", ",", "x", ",", "z0", ",", "self", ".", "sigma", ",", "qHat", ",", "qCheck", ",", "\n", "xBias", "=", "self", ".", "xBias", ",", "zBias", "=", "self", ".", "zBias", ")", "\n", "\n", "zT", "=", "torch", ".", "index_select", "(", "z", ",", "1", ",", "torch", ".", "tensor", "(", "T", "-", "1", ",", "device", "=", "z", ".", "device", ")", ")", "\n", "# Give out the last one, to be used as starting point if used in", "\n", "# succession", "\n", "\n", "return", "z", ",", "zT", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO": [[4180, 4198], ["graphML.GraphAttentional", "graphML.GraphAttentional", "graphML.EdgeGatedHiddenState.inputGateGRNN.addGSO", "graphML.EdgeGatedHiddenState.forgetGateGRNN.addGSO", "graphML.EdgeGatedHiddenState.inputGateGAT.addGSO", "graphML.EdgeGatedHiddenState.forgetGateGAT.addGSO", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "addGSO", "(", "self", ",", "S", ")", ":", "\n", "# Every S has 3 dimensions.", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "3", "\n", "# S is of shape E x N x N", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "self", ".", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "self", ".", "S", "=", "S", "\n", "\n", "# Create GATs", "\n", "self", ".", "inputGateGAT", "=", "GraphAttentional", "(", "self", ".", "H", ",", "1", ",", "1", ")", "\n", "self", ".", "forgetGateGAT", "=", "GraphAttentional", "(", "self", ".", "H", ",", "1", ",", "1", ")", "\n", "\n", "# Add GSO to gate GRNNs", "\n", "self", ".", "inputGateGRNN", ".", "addGSO", "(", "S", ")", "\n", "self", ".", "forgetGateGRNN", ".", "addGSO", "(", "S", ")", "\n", "self", ".", "inputGateGAT", ".", "addGSO", "(", "S", ")", "\n", "self", ".", "forgetGateGAT", ".", "addGSO", "(", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.extra_repr": [[4199, 4210], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"in_features=%d, hidden_features=%d, \"", "%", "(", "\n", "self", ".", "F", ",", "self", ".", "H", ")", "+", "\"filter_taps=%d, \"", "%", "(", "\n", "self", ".", "K", ")", "+", "\"edge_features=%d, \"", "%", "(", "self", ".", "E", ")", "+", "\"bias=%s, \"", "%", "(", "self", ".", "bias", ")", "+", "\"nonlinearity=%s\"", "%", "(", "self", ".", "sigma", ")", "\n", "if", "self", ".", "S", "is", "not", "None", ":", "\n", "            ", "reprString", "+=", "\"GSO stored\"", "\n", "", "else", ":", "\n", "            ", "reprString", "+=", "\"no GSO stored\"", "\n", "", "return", "reprString", "", "", "", ""]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.LSIGF": [[83, 177], ["torch.matmul.reshape", "S.reshape.reshape", "torch.matmul.reshape().repeat", "range", "torch.matmul().permute", "torch.matmul().permute", "torch.matmul", "torch.matmul", "torch.matmul.reshape", "torch.cat", "torch.cat", "torch.matmul.reshape", "torch.matmul", "torch.matmul", "torch.cat.permute().reshape", "h.reshape().permute", "torch.cat.permute", "h.reshape"], "function", ["None"], ["def", "LSIGF", "(", "h", ",", "S", ",", "x", ",", "b", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    LSIGF(filter_taps, GSO, input, bias=None) Computes the output of a linear\n        shift-invariant graph filter on input and then adds bias.\n\n    Denote as G the number of input features, F the number of output features,\n    E the number of edge features, K the number of filter taps, N the number of\n    nodes, S_{e} in R^{N x N} the GSO for edge feature e, x in R^{G x N} the\n    input data where x_{g} in R^{N} is the graph signal representing feature\n    g, and b in R^{F x N} the bias vector, with b_{f} in R^{N} representing the\n    bias for feature f.\n\n    Then, the LSI-GF is computed as\n        y_{f} = \\sum_{e=1}^{E}\n                    \\sum_{k=0}^{K-1}\n                    \\sum_{g=1}^{G}\n                        [h_{f,g,e}]_{k} S_{e}^{k} x_{g}\n                + b_{f}\n    for f = 1, ..., F.\n\n    Inputs:\n        filter_taps (torch.tensor): array of filter taps; shape:\n            output_features x edge_features x filter_taps x input_features\n        GSO (torch.tensor): graph shift operator; shape:\n            edge_features x number_nodes x number_nodes\n        input (torch.tensor): input signal; shape:\n            batch_size x input_features x number_nodes\n        bias (torch.tensor): shape: output_features x number_nodes\n            if the same bias is to be applied to all nodes, set number_nodes = 1\n            so that b_{f} vector becomes b_{f} \\mathbf{1}_{N}\n\n    Outputs:\n        output: filtered signals; shape:\n            batch_size x output_features x number_nodes\n    \"\"\"", "\n", "# The basic idea of what follows is to start reshaping the input and the", "\n", "# GSO so the filter coefficients go just as a very plain and simple", "\n", "# linear operation, so that all the derivatives and stuff on them can be", "\n", "# easily computed.", "\n", "\n", "# h is output_features x edge_weights x filter_taps x input_features", "\n", "# S is edge_weighs x number_nodes x number_nodes", "\n", "# x is batch_size x input_features x number_nodes", "\n", "# b is output_features x number_nodes", "\n", "# Output:", "\n", "# y is batch_size x output_features x number_nodes", "\n", "\n", "# Get the parameter numbers:", "\n", "F", "=", "h", ".", "shape", "[", "0", "]", "\n", "E", "=", "h", ".", "shape", "[", "1", "]", "\n", "K", "=", "h", ".", "shape", "[", "2", "]", "\n", "G", "=", "h", ".", "shape", "[", "3", "]", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "E", "\n", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "G", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "N", "\n", "# Or, in the notation we've been using:", "\n", "# h in F x E x K x G", "\n", "# S in E x N x N", "\n", "# x in B x G x N", "\n", "# b in F x N", "\n", "# y in B x F x N", "\n", "\n", "# Now, we have x in B x G x N and S in E x N x N, and we want to come up", "\n", "# with matrix multiplication that yields z = x * S with shape", "\n", "# B x E x K x G x N.", "\n", "# For this, we first add the corresponding dimensions", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "G", ",", "N", "]", ")", "\n", "S", "=", "S", ".", "reshape", "(", "[", "1", ",", "E", ",", "N", ",", "N", "]", ")", "\n", "z", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "G", ",", "N", "]", ")", ".", "repeat", "(", "1", ",", "E", ",", "1", ",", "1", ",", "1", ")", "# This is for k = 0", "\n", "# We need to repeat along the E dimension, because for k=0, S_{e} = I for", "\n", "# all e, and therefore, the same signal values have to be used along all", "\n", "# edge feature dimensions.", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "        ", "x", "=", "torch", ".", "matmul", "(", "x", ",", "S", ")", "# B x E x G x N", "\n", "xS", "=", "x", ".", "reshape", "(", "[", "B", ",", "E", ",", "1", ",", "G", ",", "N", "]", ")", "# B x E x 1 x G x N", "\n", "z", "=", "torch", ".", "cat", "(", "(", "z", ",", "xS", ")", ",", "dim", "=", "2", ")", "# B x E x k x G x N", "\n", "# This output z is of size B x E x K x G x N", "\n", "# Now we have the x*S_{e}^{k} product, and we need to multiply with the", "\n", "# filter taps.", "\n", "# We multiply z on the left, and h on the right, the output is to be", "\n", "# B x N x F (the multiplication is not along the N dimension), so we reshape", "\n", "# z to be B x N x E x K x G and reshape it to B x N x EKG (remember we", "\n", "# always reshape the last dimensions), and then make h be E x K x G x F and", "\n", "# reshape it to EKG x F, and then multiply", "\n", "", "y", "=", "torch", ".", "matmul", "(", "z", ".", "permute", "(", "0", ",", "4", ",", "1", ",", "2", ",", "3", ")", ".", "reshape", "(", "[", "B", ",", "N", ",", "E", "*", "K", "*", "G", "]", ")", ",", "\n", "h", ".", "reshape", "(", "[", "F", ",", "E", "*", "K", "*", "G", "]", ")", ".", "permute", "(", "1", ",", "0", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "# And permute againt to bring it from B x N x F to B x F x N.", "\n", "# Finally, add the bias", "\n", "if", "b", "is", "not", "None", ":", "\n", "        ", "y", "=", "y", "+", "b", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.spectralGF": [[178, 292], ["x.reshape().permute.reshape().permute", "VH.reshape.reshape", "torch.matmul().permute", "torch.matmul().permute", "V.reshape.reshape", "h.reshape().repeat", "Vdiagh.reshape.reshape", "VHx.reshape.reshape", "torch.matmul", "torch.matmul", "torch.matmul.squeeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "x.reshape().permute.reshape", "torch.matmul", "torch.matmul", "h.reshape"], "function", ["None"], ["", "def", "spectralGF", "(", "h", ",", "V", ",", "VH", ",", "x", ",", "b", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    spectralGF(filter_coeff, eigenbasis, eigenbasis_hermitian, input, bias=None)\n        Computes the output of a linear shift-invariant graph filter in spectral\n        form applying filter_coefficients on the graph fourier transform of the\n        input .\n\n    Denote as G the number of input features, F the number of output features,\n    E the number of edge features, N the number of nodes, S_{e} in R^{N x N}\n    the GSO for edge feature e with S_{e} = V_{e} Lambda_{e} V_{e}^{H} as\n    eigendecomposition, x in R^{G x N} the input data where x_{g} in R^{N} is\n    the graph signal representing feature g, and b in R^{F x N} the bias vector,\n    with b_{f} in R^{N} representing the bias for feature f.\n\n    Then, the LSI-GF in spectral form is computed as\n        y_{f} = \\sum_{e=1}^{E}\n                    \\sum_{g=1}^{G}\n                        V_{e} diag(h_{f,g,e}) V_{e}^{H} x_{g}\n                + b_{f}\n    for f = 1, ..., F, with h_{f,g,e} in R^{N} the filter coefficients for\n    output feature f, input feature g and edge feature e.\n\n    Inputs:\n        filter_coeff (torch.tensor): array of filter coefficients; shape:\n            output_features x edge_features x input_features x number_nodes\n        eigenbasis (torch.tensor): eigenbasis of the graph shift operator;shape:\n            edge_features x number_nodes x number_nodes\n        eigenbasis_hermitian (torch.tensor): hermitian of the eigenbasis; shape:\n            edge_features x number_nodes x number_nodes\n        input (torch.tensor): input signal; shape:\n            batch_size x input_features x number_nodes\n        bias (torch.tensor): shape: output_features x number_nodes\n            if the same bias is to be applied to all nodes, set number_nodes = 1\n            so that b_{f} vector becomes b_{f} \\mathbf{1}_{N}\n\n    Outputs:\n        output: filtered signals; shape:\n            batch_size x output_features x number_nodes\n\n    Obs.: While we consider most GSOs to be normal (so that the eigenbasis is\n    an orthonormal basis), this function would also work if V^{-1} is used as\n    input instead of V^{H}\n    \"\"\"", "\n", "# The decision to input both V and V_H is to avoid any time spent in", "\n", "# permuting/inverting the matrix. Because this depends on the graph and not", "\n", "# the data, it can be done faster if we just input it.", "\n", "\n", "# h is output_features x edge_weights x input_features x number_nodes", "\n", "# V is edge_weighs x number_nodes x number_nodes", "\n", "# VH is edge_weighs x number_nodes x number_nodes", "\n", "# x is batch_size x input_features x number_nodes", "\n", "# b is output_features x number_nodes", "\n", "# Output:", "\n", "# y is batch_size x output_features x number_nodes", "\n", "\n", "# Get the parameter numbers:", "\n", "F", "=", "h", ".", "shape", "[", "0", "]", "\n", "E", "=", "h", ".", "shape", "[", "1", "]", "\n", "G", "=", "h", ".", "shape", "[", "2", "]", "\n", "N", "=", "h", ".", "shape", "[", "3", "]", "\n", "assert", "V", ".", "shape", "[", "0", "]", "==", "VH", ".", "shape", "[", "0", "]", "==", "E", "\n", "assert", "V", ".", "shape", "[", "1", "]", "==", "VH", ".", "shape", "[", "1", "]", "==", "V", ".", "shape", "[", "2", "]", "==", "VH", ".", "shape", "[", "2", "]", "==", "N", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "G", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "N", "\n", "# Or, in the notation I've been using:", "\n", "# h in F x E x G x N", "\n", "# V in E x N x N", "\n", "# VH in E x N x N", "\n", "# x in B x G x N", "\n", "# b in F x N", "\n", "# y in B x F x N", "\n", "\n", "# We will do proper matrix multiplication in this case (algebraic", "\n", "# multiplication using column vectors instead of CS notation using row", "\n", "# vectors).", "\n", "# We will multiply separate VH with x, and V with diag(h).", "\n", "# First, to multiply VH with x, we need to add one dimension for each one", "\n", "# of them (dimension E for x and dimension B for VH)", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "G", ",", "N", "]", ")", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# B x 1 x N x G", "\n", "VH", "=", "VH", ".", "reshape", "(", "[", "1", ",", "E", ",", "N", ",", "N", "]", ")", "# 1 x E x N x N", "\n", "# Now we multiply. Note that we also permute to make it B x E x G x N", "\n", "# instead of B x E x N x G because we want to multiply for a specific e and", "\n", "# g, there we do not want to sum (yet) over G.", "\n", "VHx", "=", "torch", ".", "matmul", "(", "VH", ",", "x", ")", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# B x E x G x N", "\n", "\n", "# Now we want to multiply V * diag(h), both are matrices. So first, we", "\n", "# add the necessary dimensions (B and G for V and an extra N for h to make", "\n", "# it a matrix from a vector)", "\n", "V", "=", "V", ".", "reshape", "(", "[", "1", ",", "E", ",", "1", ",", "N", ",", "N", "]", ")", "# 1 x E x 1 x N x N", "\n", "# We note that multiplying by a diagonal matrix to the right is equivalent", "\n", "# to an elementwise multiplication in which each column is multiplied by", "\n", "# a different number, so we will do this to make it faster (elementwise", "\n", "# multiplication is faster than matrix multiplication). We need to repeat", "\n", "# the vector we have columnwise.", "\n", "diagh", "=", "h", ".", "reshape", "(", "[", "F", ",", "E", ",", "G", ",", "1", ",", "N", "]", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "N", ",", "1", ")", "# F x E x G x N x N", "\n", "# And now we do elementwise multiplication", "\n", "Vdiagh", "=", "V", "*", "diagh", "# F x E x G x N x N", "\n", "# Finally, we make the multiplication of these two matrices. First, we add", "\n", "# the corresponding dimensions", "\n", "Vdiagh", "=", "Vdiagh", ".", "reshape", "(", "[", "1", ",", "F", ",", "E", ",", "G", ",", "N", ",", "N", "]", ")", "# 1 x F x E x G x N x N", "\n", "VHx", "=", "VHx", ".", "reshape", "(", "[", "B", ",", "1", ",", "E", ",", "G", ",", "N", ",", "1", "]", ")", "# B x 1 x E x G x N x 1", "\n", "# And do matrix multiplication to get all the corresponding B,F,E,G vectors", "\n", "VdiaghVHx", "=", "torch", ".", "matmul", "(", "Vdiagh", ",", "VHx", ")", "# B x F x E x G x N x 1", "\n", "# Get rid of the last dimension which we do not need anymore", "\n", "y", "=", "VdiaghVHx", ".", "squeeze", "(", "5", ")", "# B x F x E x G x N", "\n", "# Sum over G", "\n", "y", "=", "torch", ".", "sum", "(", "y", ",", "dim", "=", "3", ")", "# B x F x E x N", "\n", "# Sum over E", "\n", "y", "=", "torch", ".", "sum", "(", "y", ",", "dim", "=", "2", ")", "# B x F x N", "\n", "# Finally, add the bias", "\n", "if", "b", "is", "not", "None", ":", "\n", "        ", "y", "=", "y", "+", "b", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.NVGF": [[293, 388], ["x.reshape", "S.reshape", "torch.matmul.reshape().repeat", "range", "torch.cat.reshape", "h.reshape.reshape", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.matmul", "torch.matmul", "torch.matmul.reshape", "torch.cat", "torch.cat", "torch.matmul.reshape"], "function", ["None"], ["", "def", "NVGF", "(", "h", ",", "S", ",", "x", ",", "b", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    NVGF(filter_taps, GSO, input, bias=None) Computes the output of a\n    node-variant graph filter on input and then adds bias.\n\n    Denote as G the number of input features, F the number of output features,\n    E the number of edge features, K the number of shifts, N the number of\n    nodes, S_{e} in R^{N x N} the GSO for edge feature e, x in R^{G x N} the\n    input data where x_{g} in R^{N} is the graph signal representing feature\n    g, and b in R^{F x N} the bias vector, with b_{f} in R^{N} representing the\n    bias for feature f. Denote as h_{k}^{efg} in R^{N} the vector with the N\n    filter taps corresponding to the efg filter for shift k.\n\n    Then, the NV-GF is computed as\n        y_{f} = \\sum_{e=1}^{E}\n                    \\sum_{k=0}^{K-1}\n                    \\sum_{g=1}^{G}\n                        diag(h_{k}^{efg}) S_{e}^{k} x_{g}\n                + b_{f}\n    for f = 1, ..., F.\n\n    Inputs:\n        filter_taps (torch.tensor): array of filter taps; shape:\n            output_features x edge_features x filter_taps x input_features\n                x number_nodes\n        GSO (torch.tensor): graph shift operator; shape:\n            edge_features x number_nodes x number_nodes\n        input (torch.tensor): input signal; shape:\n            batch_size x input_features x number_nodes\n        bias (torch.tensor): shape: output_features x number_nodes\n            if the same bias is to be applied to all nodes, set number_nodes = 1\n            so that b_{f} vector becomes b_{f} \\mathbf{1}_{N}\n\n    Outputs:\n        output: filtered signals; shape:\n            batch_size x output_features x number_nodes\n    \"\"\"", "\n", "# h is output_features x edge_weights x filter_taps x input_features", "\n", "#                                                             x number_nodes", "\n", "# S is edge_weighs x number_nodes x number_nodes", "\n", "# x is batch_size x input_features x number_nodes", "\n", "# b is output_features x number_nodes", "\n", "# Output:", "\n", "# y is batch_size x output_features x number_nodes", "\n", "\n", "# Get the parameter numbers:", "\n", "F", "=", "h", ".", "shape", "[", "0", "]", "\n", "E", "=", "h", ".", "shape", "[", "1", "]", "\n", "K", "=", "h", ".", "shape", "[", "2", "]", "\n", "G", "=", "h", ".", "shape", "[", "3", "]", "\n", "N", "=", "h", ".", "shape", "[", "4", "]", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "E", "\n", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "G", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "N", "\n", "# Or, in the notation I've been using:", "\n", "# h in F x E x K x G x N", "\n", "# S in E x N x N", "\n", "# x in B x G x N", "\n", "# b in F x N", "\n", "# y in B x F x N", "\n", "\n", "# Now, we have x in B x G x N and S in E x N x N, and we want to come up", "\n", "# with matrix multiplication that yields z = x * S with shape", "\n", "# B x E x K x G x N.", "\n", "# For this, we first add the corresponding dimensions", "\n", "xr", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "G", ",", "N", "]", ")", "\n", "Sr", "=", "S", ".", "reshape", "(", "[", "1", ",", "E", ",", "N", ",", "N", "]", ")", "\n", "z", "=", "xr", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "G", ",", "N", "]", ")", ".", "repeat", "(", "1", ",", "E", ",", "1", ",", "1", ",", "1", ")", "# This is for k = 0", "\n", "# We need to repeat along the E dimension, because for k=0, S_{e} = I for", "\n", "# all e, and therefore, the same signal values have to be used along all", "\n", "# edge feature dimensions.", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "        ", "xr", "=", "torch", ".", "matmul", "(", "xr", ",", "Sr", ")", "# B x E x G x N", "\n", "xS", "=", "xr", ".", "reshape", "(", "[", "B", ",", "E", ",", "1", ",", "G", ",", "N", "]", ")", "# B x E x 1 x G x N", "\n", "z", "=", "torch", ".", "cat", "(", "(", "z", ",", "xS", ")", ",", "dim", "=", "2", ")", "# B x E x k x G x N", "\n", "# This output z is of size B x E x K x G x N", "\n", "# Now we have the x*S_{e}^{k} product, and we need to multiply with the", "\n", "# filter taps.", "\n", "# This multiplication with filter taps is ``element wise'' on N since for", "\n", "# each node we have a different element", "\n", "# First, add the extra dimension (F for z, and B for h)", "\n", "", "z", "=", "z", ".", "reshape", "(", "[", "B", ",", "1", ",", "E", ",", "K", ",", "G", ",", "N", "]", ")", "\n", "h", "=", "h", ".", "reshape", "(", "[", "1", ",", "F", ",", "E", ",", "K", ",", "G", ",", "N", "]", ")", "\n", "# Now let's do elementwise multiplication", "\n", "zh", "=", "z", "*", "h", "\n", "# And sum over the dimensions E, K, G to get B x F x N", "\n", "y", "=", "torch", ".", "sum", "(", "zh", ",", "dim", "=", "4", ")", "# Sum over G", "\n", "y", "=", "torch", ".", "sum", "(", "y", ",", "dim", "=", "3", ")", "# Sum over K", "\n", "y", "=", "torch", ".", "sum", "(", "y", ",", "dim", "=", "2", ")", "# Sum over E", "\n", "# Finally, add the bias", "\n", "if", "b", "is", "not", "None", ":", "\n", "        ", "y", "=", "y", "+", "b", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EVGF": [[389, 489], ["torch.matmul.reshape", "torch.index_select().squeeze", "torch.index_select().squeeze", "Sk.unsqueeze.unsqueeze", "torch.matmul", "torch.matmul", "torch.matmul.reshape().squeeze", "range", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.index_select().squeeze", "torch.index_select().squeeze", "Sk.unsqueeze.unsqueeze", "torch.matmul", "torch.matmul", "torch.matmul.reshape().squeeze", "torch.cat", "torch.cat", "torch.index_select", "torch.index_select", "torch.matmul.reshape", "torch.tensor().to", "torch.tensor().to", "torch.index_select", "torch.index_select", "torch.matmul.reshape", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "EVGF", "(", "S", ",", "x", ",", "b", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    EVGF(filter_matrices, input, bias=None) Computes the output of an\n    edge-variant graph filter on input and then adds bias.\n\n    Denote as G the number of input features, F the number of output features,\n    E the number of edge features, K the number of shifts, N the number of\n    nodes, Phi_{efg} in R^{N x N} the filter matrix for edge feature e, output\n    feature f and input feature g (recall that Phi_{efg}^{k} has the same\n    sparsity pattern as the graph, except for Phi_{efg}^{0} which is expected to\n    be a diagonal matrix), x in R^{G x N} the input data where x_{g} in R^{N} is\n    the graph signal representing feature g, and b in R^{F x N} the bias vector,\n    with b_{f} in R^{N} representing the bias for feature f.\n\n    Then, the EV-GF is computed as\n        y_{f} = \\sum_{e=1}^{E}\n                    \\sum_{k=0}^{K-1}\n                    \\sum_{g=1}^{G}\n                        Phi_{efg}^{k:0} x_{g}\n                + b_{f}\n    for f = 1, ..., F, with Phi_{efg}^{k:0} = Phi_{efg}^{k} Phi_{efg}^{k-1} ...\n    Phi_{efg}^{0}.\n\n    Inputs:\n        filter_matrices (torch.tensor): array of filter matrices; shape:\n            output_features x edge_features x filter_taps x input_features\n                x number_nodes x number_nodes\n        input (torch.tensor): input signal; shape:\n            batch_size x input_features x number_nodes\n        bias (torch.tensor): shape: output_features x number_nodes\n            if the same bias is to be applied to all nodes, set number_nodes = 1\n            so that b_{f} vector becomes b_{f} \\mathbf{1}_{N}\n\n    Outputs:\n        output: filtered signals; shape:\n            batch_size x output_features x number_nodes\n    \"\"\"", "\n", "# We just need to multiply by the filter_matrix recursively, and then", "\n", "# add for all E, G, and K features.", "\n", "\n", "# S is output_features x edge_features x filter_taps x input_features", "\n", "#   x number_nodes x number_nodes", "\n", "# x is batch_size x input_features x number_nodes", "\n", "# b is output_features x number_nodes", "\n", "# Output:", "\n", "# y is batch_size x output_features x number_nodes", "\n", "\n", "# Get the parameter numbers:", "\n", "F", "=", "S", ".", "shape", "[", "0", "]", "\n", "E", "=", "S", ".", "shape", "[", "1", "]", "\n", "K", "=", "S", ".", "shape", "[", "2", "]", "\n", "G", "=", "S", ".", "shape", "[", "3", "]", "\n", "N", "=", "S", ".", "shape", "[", "4", "]", "\n", "assert", "S", ".", "shape", "[", "5", "]", "==", "N", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "G", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "N", "\n", "# Or, in the notation I've been using:", "\n", "# S in F x E x K x G x N x N", "\n", "# x in B x G x N", "\n", "# b in F x N", "\n", "# y in B x F x N", "\n", "\n", "# We will be doing matrix multiplications in the algebraic way, trying to", "\n", "# multiply the N x N matrix corresponding to the appropriate e, f, k and g", "\n", "# dimensions, with the respective x vector (N x 1 column vector)", "\n", "# For this, we first add the corresponding dimensions (for x we add", "\n", "# dimensions F, E and the last dimension for column vector)", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "G", ",", "N", ",", "1", "]", ")", "\n", "# When we do index_select along dimension K we get rid of this dimension", "\n", "Sk", "=", "torch", ".", "index_select", "(", "S", ",", "2", ",", "torch", ".", "tensor", "(", "0", ")", ".", "to", "(", "S", ".", "device", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "# Sk in F x E x G x N x N", "\n", "# And we add one further dimension for the batch size B", "\n", "Sk", "=", "Sk", ".", "unsqueeze", "(", "0", ")", "# 1 x F x E x G x N x N", "\n", "# Matrix multiplication", "\n", "x", "=", "torch", ".", "matmul", "(", "Sk", ",", "x", ")", "# B x F x E x G x N x 1", "\n", "# And we collect this for every k in a vector z, along the K dimension", "\n", "z", "=", "x", ".", "reshape", "(", "[", "B", ",", "F", ",", "E", ",", "1", ",", "G", ",", "N", ",", "1", "]", ")", ".", "squeeze", "(", "6", ")", "# B x F x E x 1 x G x N", "\n", "# Now we do all the matrix multiplication", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "# Extract the following k", "\n", "        ", "Sk", "=", "torch", ".", "index_select", "(", "S", ",", "2", ",", "torch", ".", "tensor", "(", "k", ")", ".", "to", "(", "S", ".", "device", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "# Sk in F x E x G x N x N", "\n", "# Give space for the batch dimension B", "\n", "Sk", "=", "Sk", ".", "unsqueeze", "(", "0", ")", "# 1 x F x E x G x N x N", "\n", "# Multiply with the previously cumulative Sk * x", "\n", "x", "=", "torch", ".", "matmul", "(", "Sk", ",", "x", ")", "# B x F x E x G x N x 1", "\n", "# Get rid of the last dimension (of a column vector)", "\n", "Sx", "=", "x", ".", "reshape", "(", "[", "B", ",", "F", ",", "E", ",", "1", ",", "G", ",", "N", ",", "1", "]", ")", ".", "squeeze", "(", "6", ")", "# B x F x E x 1 x G x N", "\n", "# Add to the z", "\n", "z", "=", "torch", ".", "cat", "(", "(", "z", ",", "Sx", ")", ",", "dim", "=", "2", ")", "# B x F x E x k x G x N", "\n", "# Sum over G", "\n", "", "z", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "4", ")", "\n", "# Sum over K", "\n", "z", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "3", ")", "\n", "# Sum over E", "\n", "y", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "2", ")", "\n", "if", "b", "is", "not", "None", ":", "\n", "        ", "y", "=", "y", "+", "b", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.jARMA": [[490, 639], ["torch.empty().to", "torch.empty().to", "torch.empty().to", "torch.empty().to", "range", "torch.eye().reshape().to", "torch.eye().reshape().to", "torch.cat.reshape", "torch.matmul().squeeze", "torch.matmul().squeeze", "torch.matmul", "torch.matmul", "graphML.LSIGF", "torch.matmul().squeeze.reshape", "x.reshape", "torch.matmul().squeeze.unsqueeze", "range", "torch.tensor().to", "torch.tensor().to", "thisCoeffs.reshape().unsqueeze.permute", "torch.cat.permute", "thisCoeffs.reshape().unsqueeze.reshape().unsqueeze", "torch.cat.reshape", "torch.matmul().squeeze", "torch.matmul().squeeze", "torch.matmul().squeeze", "torch.matmul().squeeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "H2x.permute.permute", "torch.index_select().squeeze", "torch.index_select().squeeze", "torch.diag", "torch.diag", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "psi.reshape", "torch.eye().to", "torch.eye().to", "torch.cat.reshape", "torch.matmul", "torch.matmul", "torch.cat", "torch.cat", "torch.matmul", "torch.matmul", "thisCoeffs.reshape().unsqueeze.reshape", "varphi.reshape().repeat", "torch.sum", "torch.sum", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.diag", "torch.diag", "torch.eye().reshape", "torch.eye().reshape", "torch.matmul", "torch.matmul", "torch.matmul.unsqueeze", "torch.matmul.unsqueeze", "torch.tensor", "torch.tensor", "thisCoeffs.reshape().unsqueeze.reshape", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "numpy.mod", "torch.index_select", "torch.index_select", "torch.diag.unsqueeze", "torch.ones", "torch.ones", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "SbarInv.reshape", "x.reshape", "torch.matmul.squeeze().unsqueeze", "varphi.reshape", "torch.matmul.unsqueeze", "torch.tensor().to", "torch.tensor().to", "torch.eye", "torch.eye", "numpy.arange", "torch.matmul.squeeze", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.LSIGF", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "jARMA", "(", "psi", ",", "varphi", ",", "phi", ",", "S", ",", "x", ",", "b", "=", "None", ",", "tMax", "=", "5", ")", ":", "\n", "    ", "\"\"\"\n    jARMA(inverse_taps, direct_taps, filter_taps, GSO, input, bias = None,\n        tMax = 5) Computes the output of an ARMA filter using Jacobi\n        iterations.\n        \n    The output of an ARMA computed by means of tMax Jacobi iterations is given\n    as follows\n        y^{f} = \\sum_{e=1}^{E} \\sum_{g=1}^{G}\n                    \\sum_{p=0}^{P-1}\n                        H_{p}^{1}(S) (\\bar{S}_{p}^{fge})^{-1} x\n                        + H_{p}^{2}(S) x\n                    + H^{3}(S) x\n    where E is the total number of edge features, G is the total number of input\n    features, and P is the order of the denominator polynomial. The filters are\n        H_{p}^{1}(S) \n            = \\sum_{tau=0}^{t} \n                (-1)^{tau} varphi_{p}^{fge} (\\barS_{p}^{-1} \\tilde{S})^{tau}\n        H_{p}^{2}(S)\n            = (-1)^{t+1} ((\\bar{S}_{p}^{fge})^{-1} \\tilde{S})^{t+1}\n        H^{2}(S) = \\sum_{k=0}^{K-1} phi_{k}^{fge} S^{k}\n    where varphi_{p}^{fge} are the direct filter taps of the rational one ARMA \n    filter, phi_{k}^{fge} are the filter taps of the residue LSIGF filter, and\n    the GSOs used derive from GSO S and are\n        \\bar{S}_{p}^{fge} = Diag(S) - psi_{p}^{fge} I_{N}\n        \\tilde{S} = DiagOff(S)\n    with psi_{p}^{fge} the inverse filter taps of the rational one ARMA filter.\n    \n    Inputs:\n        inverse_taps (torch.tensor): array of filter taps psi_{p}^{fge}; shape:\n            out_features x edge_features x denominator_order x in_features\n        direct_taps (torch.tensor): array of taps varphi_{p}^{fge}; shape:\n            out_features x edge_features x denominator_order x in_features\n        filter_taps (torch.tensor): array of filter taps phi_{p}^{fge}; shape:\n            out_features x edge_features x residue_order x in_features\n        GSO (torch.tensor): graph shift operator; shape:\n            edge_features x number_nodes x number_nodes\n        input (torch.tensor): input signal; shape:\n            batch_size x input_features x number_nodes\n        bias (torch.tensor): shape: output_features x number_nodes\n            if the same bias is to be applied to all nodes, set number_nodes = 1\n            so that b_{f} vector becomes b_{f} \\mathbf{1}_{N} (default: None)\n        tMax (int): value of t for computing the Jacobi approximation \n            (default: 5)\n            \n    Outputs:\n        output: filtered signals; shape:\n            batch_size x output_features x number_nodes\n    \"\"\"", "\n", "# The inputs are:", "\n", "#   psi in F x E x P x G (inverse coefficient in order-one rational)", "\n", "#   varphi in F x E x P x G (direct coefficient in order-one rational)", "\n", "#   phi in F x E x K x G (direct filter coefficients)", "\n", "#   x in B x G x N", "\n", "#   S in E x N x N", "\n", "F", "=", "psi", ".", "shape", "[", "0", "]", "# out_features", "\n", "E", "=", "psi", ".", "shape", "[", "1", "]", "# edge_features", "\n", "P", "=", "psi", ".", "shape", "[", "2", "]", "# inverse polynomial order", "\n", "G", "=", "psi", ".", "shape", "[", "3", "]", "# in_features", "\n", "assert", "varphi", ".", "shape", "[", "0", "]", "==", "F", "\n", "assert", "varphi", ".", "shape", "[", "1", "]", "==", "E", "\n", "assert", "varphi", ".", "shape", "[", "2", "]", "==", "P", "\n", "assert", "varphi", ".", "shape", "[", "3", "]", "==", "G", "\n", "assert", "phi", ".", "shape", "[", "0", "]", "==", "F", "\n", "assert", "phi", ".", "shape", "[", "1", "]", "==", "E", "\n", "assert", "phi", ".", "shape", "[", "3", "]", "==", "G", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "# batch_size", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "G", "\n", "N", "=", "x", ".", "shape", "[", "2", "]", "# number_nodes", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "E", "\n", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "# First, let's build Stilde and Sbar", "\n", "Stilde", "=", "torch", ".", "empty", "(", "0", ")", ".", "to", "(", "S", ".", "device", ")", "# Will be of shape E x N x N", "\n", "DiagS", "=", "torch", ".", "empty", "(", "0", ")", ".", "to", "(", "S", ".", "device", ")", "# Will be of shape E x N x N", "\n", "for", "e", "in", "range", "(", "E", ")", ":", "\n", "        ", "thisS", "=", "torch", ".", "index_select", "(", "S", ",", "0", ",", "torch", ".", "tensor", "(", "e", ")", ".", "to", "(", "S", ".", "device", ")", ")", ".", "squeeze", "(", "0", ")", "\n", "thisDiagS", "=", "torch", ".", "diag", "(", "torch", ".", "diag", "(", "thisS", ")", ")", "\n", "DiagOffS", "=", "(", "thisS", "-", "thisDiagS", ")", ".", "unsqueeze", "(", "0", ")", "# E x N x N", "\n", "Stilde", "=", "torch", ".", "cat", "(", "(", "Stilde", ",", "DiagOffS", ")", ",", "dim", "=", "0", ")", "\n", "DiagS", "=", "torch", ".", "cat", "(", "(", "DiagS", ",", "thisDiagS", ".", "unsqueeze", "(", "0", ")", ")", ",", "dim", "=", "0", ")", "\n", "", "I", "=", "torch", ".", "eye", "(", "N", ")", ".", "reshape", "(", "[", "1", ",", "1", ",", "1", ",", "1", ",", "N", ",", "N", "]", ")", ".", "to", "(", "S", ".", "device", ")", "# (FxExPxGxNxN)", "\n", "psiI", "=", "psi", ".", "reshape", "(", "[", "F", ",", "E", ",", "P", ",", "G", ",", "1", ",", "1", "]", ")", "*", "I", "\n", "DiagS", "=", "DiagS", ".", "reshape", "(", "[", "1", ",", "E", ",", "1", ",", "1", ",", "N", ",", "N", "]", ")", "\n", "Sbar", "=", "DiagS", "-", "psiI", "# F x E x P x G x N x N", "\n", "\n", "# Now, invert Sbar, that doesn't depend on t either, and multiply it by x", "\n", "# Obs.: We cannot just do 1/Sbar, because all the nonzero elements will", "\n", "# give inf, ruining everything. So we will force the off-diagonal elements", "\n", "# to be one, and then get rid of them", "\n", "offDiagonalOnes", "=", "(", "torch", ".", "ones", "(", "N", ",", "N", ")", "-", "torch", ".", "eye", "(", "N", ")", ")", ".", "to", "(", "Sbar", ".", "device", ")", "\n", "SbarInv", "=", "1", "/", "(", "Sbar", "+", "offDiagonalOnes", ")", "# F x E x P x G x N x N", "\n", "SbarInv", "=", "SbarInv", "*", "torch", ".", "eye", "(", "N", ")", ".", "to", "(", "Sbar", ".", "device", ")", "\n", "SbarInvX", "=", "torch", ".", "matmul", "(", "SbarInv", ".", "reshape", "(", "[", "1", ",", "F", ",", "E", ",", "P", ",", "G", ",", "N", ",", "N", "]", ")", ",", "\n", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "1", ",", "G", ",", "N", ",", "1", "]", ")", ")", ".", "squeeze", "(", "6", ")", "\n", "#   B x F x E x P x G x N", "\n", "# And also multiply SbarInv with Stilde which is also used in H1 and H2", "\n", "SbarInvStilde", "=", "torch", ".", "matmul", "(", "SbarInv", ",", "\n", "Stilde", ".", "reshape", "(", "[", "1", ",", "E", ",", "1", ",", "1", ",", "N", ",", "N", "]", ")", ")", "\n", "#   B x F x E x P x G x N x N", "\n", "\n", "# Next, filtering through H^{3}(S) also doesn't depend on t or p, so", "\n", "H3x", "=", "LSIGF", "(", "phi", ",", "S", ",", "x", ")", "\n", "\n", "# Last, build the output from combining all filters H1, H2 and H3", "\n", "\n", "# Compute H1 SbarInvX", "\n", "z", "=", "SbarInvX", ".", "reshape", "(", "[", "B", ",", "F", ",", "E", ",", "1", ",", "P", ",", "G", ",", "N", "]", ")", "\n", "y", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "1", ",", "G", ",", "N", ",", "1", "]", ")", "# (B x F x E x P x G x N x 1)", "\n", "x1", "=", "SbarInvX", ".", "unsqueeze", "(", "6", ")", "# B x F x E x P x G x N x 1", "\n", "#   (B x F x E x tau x P x G x N)", "\n", "for", "tau", "in", "range", "(", "1", ",", "tMax", "+", "1", ")", ":", "\n", "        ", "x1", "=", "torch", ".", "matmul", "(", "SbarInvStilde", ".", "unsqueeze", "(", "0", ")", ",", "# 1 x F x E x P x G x N x N", "\n", "x1", ")", "\n", "#   B x F x E x P x G x N x 1", "\n", "z", "=", "torch", ".", "cat", "(", "(", "z", ",", "x1", ".", "squeeze", "(", "6", ")", ".", "unsqueeze", "(", "3", ")", ")", ",", "dim", "=", "3", ")", "\n", "#   B x F x E x tau x P x G x N", "\n", "y", "=", "torch", ".", "matmul", "(", "SbarInvStilde", ".", "unsqueeze", "(", "0", ")", ",", "# 1 x F x E x P x G x N x N", "\n", "y", ")", "\n", "# B x F x E x P x G x N x 1", "\n", "", "thisCoeffs", "=", "torch", ".", "tensor", "(", "(", "-", "1.", ")", "**", "np", ".", "arange", "(", "0", ",", "tMax", "+", "1", ")", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "thisCoeffs", "=", "thisCoeffs", ".", "reshape", "(", "[", "1", ",", "1", ",", "1", ",", "tMax", "+", "1", ",", "1", ",", "1", "]", ")", "*", "varphi", ".", "reshape", "(", "[", "1", ",", "F", ",", "E", ",", "1", ",", "P", ",", "G", "]", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "tMax", "+", "1", ",", "1", ",", "1", ")", "\n", "#   1 x F x E x (tMax+1) x P x G", "\n", "thisCoeffs", "=", "thisCoeffs", ".", "permute", "(", "0", ",", "4", ",", "1", ",", "2", ",", "3", ",", "5", ")", "\n", "#   1 x P x F x E x (tMax+1) x G", "\n", "z", "=", "z", ".", "permute", "(", "0", ",", "4", ",", "1", ",", "6", ",", "2", ",", "3", ",", "5", ")", "# B x P x F x N x E x (tMax+1) x G", "\n", "thisCoeffs", "=", "thisCoeffs", ".", "reshape", "(", "[", "1", ",", "P", ",", "F", ",", "E", "*", "(", "tMax", "+", "1", ")", "*", "G", "]", ")", ".", "unsqueeze", "(", "4", ")", "\n", "#   1 x P x F x E(tMax+1)G x 1", "\n", "z", "=", "z", ".", "reshape", "(", "B", ",", "P", ",", "F", ",", "N", ",", "E", "*", "(", "tMax", "+", "1", ")", "*", "G", ")", "\n", "#   B x P x F x N x E*(tMax+1)*G", "\n", "H1x", "=", "torch", ".", "matmul", "(", "z", ",", "thisCoeffs", ")", ".", "squeeze", "(", "4", ")", "\n", "#   B x P x F x N", "\n", "# Now, to compute H2x we need y, but y went only up to value tMax, and", "\n", "# we need to go to tMax+1, so we need to multiply it once more", "\n", "y", "=", "torch", ".", "matmul", "(", "SbarInvStilde", ".", "unsqueeze", "(", "0", ")", ",", "y", ")", ".", "squeeze", "(", "6", ")", "\n", "#   B x F x E x P x G x N", "\n", "H2x", "=", "-", "y", "if", "np", ".", "mod", "(", "tMax", ",", "2", ")", "==", "0", "else", "y", "\n", "H2x", "=", "torch", ".", "sum", "(", "H2x", ",", "dim", "=", "4", ")", "# sum over G, shape: B x F x E x P x N", "\n", "H2x", "=", "torch", ".", "sum", "(", "H2x", ",", "dim", "=", "2", ")", "# sume over E, shape: B x F x P x N", "\n", "H2x", "=", "H2x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "# B x P x F x N", "\n", "# Finally, we add up H1x and H2x and sum over all p, and add to H3 to", "\n", "# update u", "\n", "u", "=", "torch", ".", "sum", "(", "H1x", "+", "H2x", ",", "dim", "=", "1", ")", "+", "H3x", "\n", "\n", "if", "b", "is", "not", "None", ":", "\n", "        ", "u", "=", "u", "+", "b", "\n", "", "return", "u", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.learnAttentionGSO": [[640, 738], ["x.reshape.reshape", "W.reshape.reshape", "torch.matmul", "torch.matmul", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.functional.leaky_relu", "torch.sum", "torch.sum", "torch.functional.softmax", "int", "torch.eye().reshape().repeat().to", "torch.eye().reshape().repeat().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.index_select.reshape", "torch.index_select.reshape", "torch.matmul.permute", "torch.abs", "torch.abs", "torch.eye().reshape().repeat", "torch.eye().reshape().repeat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.eye().reshape", "torch.eye().reshape", "torch.eye", "torch.eye"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "learnAttentionGSO", "(", "x", ",", "a", ",", "W", ",", "S", ",", "negative_slope", "=", "0.2", ")", ":", "\n", "    ", "\"\"\"\n    learnAttentionGSO(x, a, W, S) Computes the GSO following the attention\n        mechanism\n\n    Denote as G the number of input features, F the number of output features,\n    E the number of edge features, P the number of attention heads, Ji the\n    number of nodes in N_{i}, the neighborhood of node i, and N the number of\n    nodes. Let x_{i} in R^{G} be the feature associated to node i,\n    W^{ep} in R^{F x G} the weight marix associated to edge feature e and\n    attention head p, and a^{ep} in R^{2F} the mixing vector. Let\n    alpha_{ij}^{ep} in R the attention coefficient between nodes i and j, for\n    edge feature e and attention head p, and let s_{ij}^{e} be the value of\n    feature e of the edge connecting nodes i and j.\n    \n    Each elements of the new GSO is alpha_{ij}^{ep} computed as\n        alpha_{ij}^{ep} = softmax_{j} ( LeakyReLU_{beta} (\n                (a^{ep})^T [cat(W^{ep}x_{i}, W^{ep} x_{j})]\n        ))\n    for all j in N_{i}, and where beta is the negative slope of the leaky ReLU.\n\n    Inputs:\n        x (torch.tensor): input;\n            shape: batch_size x input_features x number_nodes\n        a (torch.tensor): mixing parameter; shape:\n            number_heads x edge_features x 2 * output_features\n        W (torch.tensor): linear parameter; shape:\n            number_heads x edge_features x output_features x input_features\n        S (torch.tensor): graph shift operator; shape:\n            edge_features x number_nodes x number_nodes\n        negative_slope (float): negative slope of the leaky relu (default: 0.2)\n\n    Outputs:\n        aij: output GSO; shape:\n         batch_size x number_heads x edge_features x number_nodes x number_nodes\n    \"\"\"", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "# batch_size", "\n", "G", "=", "x", ".", "shape", "[", "1", "]", "# input_features", "\n", "N", "=", "x", ".", "shape", "[", "2", "]", "# number_nodes", "\n", "P", "=", "a", ".", "shape", "[", "0", "]", "# number_heads", "\n", "E", "=", "a", ".", "shape", "[", "1", "]", "# edge_features", "\n", "assert", "W", ".", "shape", "[", "0", "]", "==", "P", "\n", "assert", "W", ".", "shape", "[", "1", "]", "==", "E", "\n", "F", "=", "W", ".", "shape", "[", "2", "]", "# output_features", "\n", "assert", "a", ".", "shape", "[", "2", "]", "==", "int", "(", "2", "*", "F", ")", "\n", "G", "=", "W", ".", "shape", "[", "3", "]", "# input_features", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "E", "\n", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "# Add ones of the GSO at all edge feature levels so that the node always", "\n", "# has access to itself. The fact that it's one is not so relevant, because", "\n", "# the attention coefficient that is learned would compensate for this", "\n", "S", "=", "S", "+", "torch", ".", "eye", "(", "N", ")", ".", "reshape", "(", "[", "1", ",", "N", ",", "N", "]", ")", ".", "repeat", "(", "E", ",", "1", ",", "1", ")", ".", "to", "(", "S", ".", "device", ")", "\n", "# WARNING:", "\n", "# (If the GSOs already have self-connections, then these will be added a 1,", "\n", "# which might be a problem if the self-connection is a -1. I will have to", "\n", "# think of this more carefully)", "\n", "\n", "# W is of size P x E x F x G", "\n", "# a is of size P x E x 2F", "\n", "# Compute Wx for all nodes", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "G", ",", "N", "]", ")", "\n", "W", "=", "W", ".", "reshape", "(", "[", "1", ",", "P", ",", "E", ",", "F", ",", "G", "]", ")", "\n", "Wx", "=", "torch", ".", "matmul", "(", "W", ",", "x", ")", "# B x P x E x F x N", "\n", "# Now, do a_1^T Wx, and a_2^T Wx to get a tensor of shape B x P x E x 1 x N", "\n", "# because we're applying the inner product on the F dimension.", "\n", "a1", "=", "torch", ".", "index_select", "(", "a", ",", "2", ",", "torch", ".", "arange", "(", "F", ")", ".", "to", "(", "x", ".", "device", ")", ")", "# K x E x F", "\n", "a2", "=", "torch", ".", "index_select", "(", "a", ",", "2", ",", "torch", ".", "arange", "(", "F", ",", "2", "*", "F", ")", ".", "to", "(", "x", ".", "device", ")", ")", "# K x E x F", "\n", "a1Wx", "=", "torch", ".", "matmul", "(", "a1", ".", "reshape", "(", "[", "1", ",", "P", ",", "E", ",", "1", ",", "F", "]", ")", ",", "Wx", ")", "# B x P x E x 1 x N", "\n", "a2Wx", "=", "torch", ".", "matmul", "(", "a2", ".", "reshape", "(", "[", "1", ",", "P", ",", "E", ",", "1", ",", "F", "]", ")", ",", "Wx", ")", "# B x P x E x 1 x N", "\n", "# And then, use this to sum them accordingly and create a B x P x E x N x N", "\n", "# matrix.", "\n", "aWx", "=", "a1Wx", "+", "a2Wx", ".", "permute", "(", "0", ",", "1", ",", "2", ",", "4", ",", "3", ")", "# B x P x E x N x N", "\n", "#   Obs.: In this case, we have one column vector and one row vector; then,", "\n", "# what the sum does, is to repeat the column and the row, respectively,", "\n", "# until both matrices are of the same size, and then adds up, which is", "\n", "# precisely what we want to do", "\n", "# Apply the LeakyRelu", "\n", "eij", "=", "nn", ".", "functional", ".", "leaky_relu", "(", "aWx", ",", "negative_slope", "=", "negative_slope", ")", "\n", "#   B x P x E x N x N", "\n", "# Each element of this N x N matrix is, precisely, e_ij (eq. 1) in the GAT", "\n", "# paper.", "\n", "# And apply the softmax. For the softmax, we do not want to consider", "\n", "# the places where there are no neighbors, so we need to set them to -infty", "\n", "# so that they will be assigned a zero.", "\n", "#   First, get places where we have edges", "\n", "maskEdges", "=", "torch", ".", "sum", "(", "torch", ".", "abs", "(", "S", ".", "data", ")", ",", "dim", "=", "0", ")", "\n", "#   Make it a binary matrix", "\n", "maskEdges", "=", "(", "maskEdges", ">", "zeroTolerance", ")", ".", "type", "(", "x", ".", "dtype", ")", "\n", "#   Make it -infinity where there are zeros", "\n", "infinityMask", "=", "(", "1", "-", "maskEdges", ")", "*", "infiniteNumber", "\n", "#   Compute the softmax plus the -infinity (we first force the places where", "\n", "# there is no edge to be zero, and then we add -infinity to them)", "\n", "aij", "=", "nn", ".", "functional", ".", "softmax", "(", "eij", "*", "maskEdges", "-", "infinityMask", ",", "dim", "=", "4", ")", "\n", "#   B x P x E x N x N", "\n", "# This will give me a matrix of all the alpha_ij coefficients.", "\n", "# Re-inforce the zeros just to be sure", "\n", "return", "aij", "*", "maskEdges", "# B x P x E x N x N", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.graphAttention": [[739, 810], ["graphML.learnAttentionGSO", "x.reshape.reshape", "W.reshape.reshape", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.sum", "torch.sum", "int", "S.reshape"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.learnAttentionGSO"], ["", "def", "graphAttention", "(", "x", ",", "a", ",", "W", ",", "S", ",", "negative_slope", "=", "0.2", ")", ":", "\n", "    ", "\"\"\"\n    graphAttention(x, a, W, S) Computes attention following GAT layer taking\n        into account multiple edge features.\n\n    Denote as G the number of input features, F the number of output features,\n    E the number of edge features, P the number of attention heads, Ji the\n    number of nodes in N_{i}, the neighborhood of node i, and N the number of\n    nodes. Let x_{i} in R^{G} be the feature associated to node i,\n    W^{ep} in R^{F x G} the weight marix associated to edge feature e and\n    attention head p, and a^{ep} in R^{2F} the mixing vector. Let\n    alpha_{ij}^{ep} in R the attention coefficient between nodes i and j, for\n    edge feature e and attention head p, and let s_{ij}^{e} be the value of\n    feature e of the edge connecting nodes i and j.\n\n    Let y_{i}^{p} in R^{F} be the output of the graph attention at node i for\n    attention head p. It is computed as\n        y_{i}^{p} = \\sum_{e=1}^{E}\n                        \\sum_{j in N_{i}}\n                            s_{ij}^{e} alpha_{ij}^{ep} W^{ep} x_{j}\n    with\n        alpha_{ij}^{ep} = softmax_{j} ( LeakyReLU_{beta} (\n                (a^{ep})^T [cat(W^{ep}x_{i}, W^{ep} x_{j})]\n        ))\n    for all j in N_{i}, and where beta is the negative slope of the leaky ReLU.\n\n    Inputs:\n        x (torch.tensor): input;\n            shape: batch_size x input_features x number_nodes\n        a (torch.tensor): mixing parameter; shape:\n            number_heads x edge_features x 2 * output_features\n        W (torch.tensor): linear parameter; shape:\n            number_heads x edge_features x output_features x input_features\n        S (torch.tensor): graph shift operator; shape:\n            edge_features x number_nodes x number_nodes\n        negative_slope (float): negative slope of the leaky relu (default: 0.2)\n\n    Outputs:\n        y: output; shape:\n            batch_size x number_heads x output_features x number_nodes\n    \"\"\"", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "# batch_size", "\n", "G", "=", "x", ".", "shape", "[", "1", "]", "# input_features", "\n", "N", "=", "x", ".", "shape", "[", "2", "]", "# number_nodes", "\n", "P", "=", "a", ".", "shape", "[", "0", "]", "# number_heads", "\n", "E", "=", "a", ".", "shape", "[", "1", "]", "# edge_features", "\n", "assert", "W", ".", "shape", "[", "0", "]", "==", "P", "\n", "assert", "W", ".", "shape", "[", "1", "]", "==", "E", "\n", "F", "=", "W", ".", "shape", "[", "2", "]", "# output_features", "\n", "assert", "a", ".", "shape", "[", "2", "]", "==", "int", "(", "2", "*", "F", ")", "\n", "G", "=", "W", ".", "shape", "[", "3", "]", "# input_features", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "E", "\n", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "# First, we need to learn the attention GSO", "\n", "aij", "=", "learnAttentionGSO", "(", "x", ",", "a", ",", "W", ",", "S", ",", "negative_slope", "=", "negative_slope", ")", "\n", "# B x P x E x N x N", "\n", "\n", "# Then, we need to compute the high-level features", "\n", "# W is of size P x E x F x G", "\n", "# a is of size P x E x 2F", "\n", "# Compute Wx for all nodes", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "G", ",", "N", "]", ")", "\n", "W", "=", "W", ".", "reshape", "(", "[", "1", ",", "P", ",", "E", ",", "F", ",", "G", "]", ")", "\n", "Wx", "=", "torch", ".", "matmul", "(", "W", ",", "x", ")", "# B x P x E x F x N", "\n", "\n", "# Finally, we just need to apply this matrix to the Wx which we have already", "\n", "# computed, and done.", "\n", "y", "=", "torch", ".", "matmul", "(", "Wx", ",", "S", ".", "reshape", "(", "[", "1", ",", "1", ",", "E", ",", "N", ",", "N", "]", ")", "*", "aij", ")", "# B x P x E x F x N", "\n", "# And sum over all edges", "\n", "return", "torch", ".", "sum", "(", "y", ",", "dim", "=", "2", ")", "# B x P x F x N", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.graphAttentionLSIGF": [[811, 896], ["graphML.learnAttentionGSO", "h.permute.reshape", "W.reshape.permute", "W.reshape.reshape", "torch.matmul.reshape", "torch.matmul.reshape().repeat", "range", "h.permute.reshape", "h.permute.permute", "torch.cat.permute().reshape", "torch.matmul", "torch.matmul", "y.permute.permute", "int", "torch.matmul", "torch.matmul", "torch.matmul.reshape", "torch.cat", "torch.cat", "torch.matmul.reshape", "torch.cat.permute"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.learnAttentionGSO"], ["", "def", "graphAttentionLSIGF", "(", "h", ",", "x", ",", "a", ",", "W", ",", "S", ",", "b", "=", "None", ",", "negative_slope", "=", "0.2", ")", ":", "\n", "    ", "\"\"\"\n    graphAttentionLSIGF(h, x, a, W, S) Computes a graph convolution \n        (LSIGF) over a graph shift operator learned through the attention\n        mechanism\n\n    Inputs:\n        h (torch.tensor): array of filter taps; shape:\n            edge_features x filter_taps\n        x (torch.tensor): input; shape:\n            batch_size x input_features x number_nodes\n        a (torch.tensor): mixing parameter; shape:\n            number_heads x edge_features x 2 * out_features\n        W (torch.tensor): linear parameter; shape:\n            number_heads x edge_features x out_features x input_features\n        S (torch.tensor): graph shift operator; shape:\n            edge_features x number_nodes x number_nodes\n        bias (torch.tensor): shape: output_features x number_nodes\n        negative_slope (float): negative slope of the leaky relu (default: 0.2)\n\n    Outputs:\n        y: output; shape:\n            batch_size x number_heads x output_features x number_nodes\n    \"\"\"", "\n", "E", "=", "h", ".", "shape", "[", "0", "]", "# edge_features", "\n", "K", "=", "h", ".", "shape", "[", "1", "]", "# filter_taps", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "# batch_size", "\n", "G", "=", "x", ".", "shape", "[", "1", "]", "# input_features", "\n", "N", "=", "x", ".", "shape", "[", "2", "]", "# number_nodes", "\n", "P", "=", "a", ".", "shape", "[", "0", "]", "# number_heads", "\n", "E", "=", "a", ".", "shape", "[", "1", "]", "# edge_features", "\n", "assert", "W", ".", "shape", "[", "0", "]", "==", "P", "\n", "assert", "W", ".", "shape", "[", "1", "]", "==", "E", "\n", "F", "=", "W", ".", "shape", "[", "2", "]", "# out_features", "\n", "assert", "W", ".", "shape", "[", "3", "]", "==", "G", "\n", "assert", "a", ".", "shape", "[", "2", "]", "==", "int", "(", "2", "*", "F", ")", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "E", "\n", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "# First, we need to learn the attention GSO", "\n", "aij", "=", "learnAttentionGSO", "(", "x", ",", "a", ",", "W", ",", "S", ",", "negative_slope", "=", "negative_slope", ")", "\n", "# B x P x E x N x N", "\n", "\n", "# And now we need to compute an LSIGF with this learned GSO, but the filter", "\n", "# taps of the LSIGF are a combination of h (along K), and W (along F and G)", "\n", "# So, we have", "\n", "#   h in E x K", "\n", "#   W in P x E x G x F", "\n", "# The filter taps, will thus have shape", "\n", "#   h in P x F x E x K x G", "\n", "h", "=", "h", ".", "reshape", "(", "[", "1", ",", "1", ",", "E", ",", "K", ",", "1", "]", ")", "# (P x F x E x K x G)", "\n", "W", "=", "W", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "# P x F x E x G", "\n", "W", "=", "W", ".", "reshape", "(", "[", "P", ",", "F", ",", "E", ",", "1", ",", "G", "]", ")", "# (P x F x E x K x G)", "\n", "h", "=", "h", "*", "W", "# P x F x E x K x G (We hope, if not, we need to repeat on the ", "\n", "# corresponding dimensions)", "\n", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "G", ",", "N", "]", ")", "# (B x P x E x G x N)", "\n", "# The easiest would be to use the LSIGF function, but that takes as input", "\n", "# a B x F x N input, and while we could join together B and P into a single", "\n", "# dimension, we would still be unable to handle the E features this way.", "\n", "# So we basically need to copy the code from LSIGF but accounting the ", "\n", "# matrix multiplications with multiple edge features as Wx has", "\n", "z", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "1", ",", "G", ",", "N", "]", ")", ".", "repeat", "(", "1", ",", "P", ",", "E", ",", "1", ",", "1", ",", "1", ")", "\n", "# add the k=0 dimension (B x P x E x K x G x N)", "\n", "# And now do the repeated multiplication with S", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "        ", "x", "=", "torch", ".", "matmul", "(", "x", ",", "aij", ")", "# B x P x E x G x N", "\n", "xAij", "=", "x", ".", "reshape", "(", "[", "B", ",", "P", ",", "E", ",", "1", ",", "G", ",", "N", "]", ")", "# add the k dimension", "\n", "z", "=", "torch", ".", "cat", "(", "(", "z", ",", "xAij", ")", ",", "dim", "=", "3", ")", "# B x P x E x k x G x N", "\n", "# This output z is of shape B x P x E x K x M x N and represents the product", "\n", "# x * aij_{e}^{k} (i.e. the multiplication between x and the kth power of", "\n", "# the learned GSO).", "\n", "# Now, we need to multiply this by the filter coefficients", "\n", "# Convert h, from F x E x K x M to EKM x F to multiply from the right", "\n", "", "h", "=", "h", ".", "reshape", "(", "[", "1", ",", "P", ",", "F", ",", "E", "*", "K", "*", "G", "]", ")", "# (B x P x F x (EKG))", "\n", "h", "=", "h", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# (B x P x EKG x F)", "\n", "# And z from B x P x E x K x G x N to B x P x N x EKG to left multiply", "\n", "z", "=", "z", ".", "permute", "(", "0", ",", "1", ",", "5", ",", "2", ",", "3", ",", "4", ")", ".", "reshape", "(", "[", "B", ",", "P", ",", "N", ",", "E", "*", "K", "*", "G", "]", ")", "\n", "# And multiply", "\n", "y", "=", "torch", ".", "matmul", "(", "z", ",", "h", ")", "# B x P x N x F", "\n", "y", "=", "y", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# The output needs to be B x P x F x N", "\n", "# Finally, add the bias", "\n", "if", "b", "is", "not", "None", ":", "\n", "        ", "y", "=", "y", "+", "b", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.graphAttentionEVGF": [[897, 970], ["torch.index_select().squeeze", "torch.index_select().squeeze", "W0.reshape.reshape", "torch.matmul", "torch.matmul", "torch.index_select().squeeze", "torch.index_select().squeeze", "torch.index_select().squeeze", "torch.index_select().squeeze", "graphML.learnAttentionGSO", "torch.matmul", "torch.matmul", "range", "torch.sum", "torch.sum", "int", "x.reshape", "torch.index_select().squeeze", "torch.index_select().squeeze", "torch.index_select().squeeze", "torch.index_select().squeeze", "graphML.learnAttentionGSO", "torch.matmul", "torch.matmul", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "S.reshape", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "S.reshape", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.learnAttentionGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.learnAttentionGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "graphAttentionEVGF", "(", "x", ",", "a", ",", "W", ",", "S", ",", "b", "=", "None", ",", "negative_slope", "=", "0.2", ")", ":", "\n", "    ", "\"\"\"\n    graphAttentionEVGF(h, x, a, W, S) Computes an edge varying graph filter\n        (EVGF) where each EVGF is learned by an attention mechanism\n\n    Inputs:\n        x (torch.tensor): input; shape:\n            batch_size x input_features x number_nodes\n        a (torch.tensor): mixing parameter; shape:\n            number_heads x filter_taps x edge_features x 2 * out_features\n        W (torch.tensor): linear parameter; shape:\n            number_heads x filter_taps x edge_features x out_features x input_features\n        S (torch.tensor): graph shift operator; shape:\n            edge_features x number_nodes x number_nodes\n        bias (torch.tensor): shape: output_features x number_nodes\n        negative_slope (float): negative slope of the leaky relu (default: 0.2)\n\n    Outputs:\n        y: output; shape:\n            batch_size x number_heads x output_features x number_nodes\n    \"\"\"", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "# batch_size", "\n", "G", "=", "x", ".", "shape", "[", "1", "]", "# input_features", "\n", "N", "=", "x", ".", "shape", "[", "2", "]", "# number_nodes", "\n", "P", "=", "a", ".", "shape", "[", "0", "]", "# number_heads", "\n", "K", "=", "a", ".", "shape", "[", "1", "]", "# filter_taps", "\n", "E", "=", "a", ".", "shape", "[", "2", "]", "# edge_features", "\n", "assert", "W", ".", "shape", "[", "0", "]", "==", "P", "\n", "assert", "W", ".", "shape", "[", "1", "]", "==", "K", "\n", "assert", "W", ".", "shape", "[", "2", "]", "==", "E", "\n", "F", "=", "W", ".", "shape", "[", "3", "]", "# output_features", "\n", "assert", "W", ".", "shape", "[", "4", "]", "==", "G", "\n", "assert", "a", ".", "shape", "[", "3", "]", "==", "int", "(", "2", "*", "F", ")", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "E", "\n", "assert", "S", ".", "shape", "[", "1", "]", "==", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "# First, we need to compute the high-level features", "\n", "# W is of size P x K x E x F x G", "\n", "# a is of size P x K x E x 2F", "\n", "# To compute Wx, we need the first element (K = 0)", "\n", "W0", "=", "torch", ".", "index_select", "(", "W", ",", "1", ",", "torch", ".", "tensor", "(", "0", ")", ".", "to", "(", "S", ".", "device", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "#   P x E x F x G", "\n", "W0", "=", "W0", ".", "reshape", "(", "[", "1", ",", "P", ",", "E", ",", "F", ",", "G", "]", ")", "\n", "W0x", "=", "torch", ".", "matmul", "(", "W0", ",", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "G", ",", "N", "]", ")", ")", "# B x P x E x F x N", "\n", "\n", "# Now we proceed to learn the rest of the EVGF.", "\n", "# That first filter coefficient (for the one-hop neighborhood) is learned", "\n", "# from the first element along the K dimension (dim = 1)", "\n", "thisa", "=", "torch", ".", "index_select", "(", "a", ",", "1", ",", "torch", ".", "tensor", "(", "0", ")", ".", "to", "(", "S", ".", "device", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "thisW", "=", "torch", ".", "index_select", "(", "W", ",", "1", ",", "torch", ".", "tensor", "(", "0", ")", ".", "to", "(", "S", ".", "device", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "aij", "=", "learnAttentionGSO", "(", "x", ",", "thisa", ",", "thisW", ",", "S", ",", "negative_slope", "=", "negative_slope", ")", "\n", "# B x P x E x N x N (repesents k=0,1)", "\n", "W0x", "=", "torch", ".", "matmul", "(", "W0x", ",", "S", ".", "reshape", "(", "[", "1", ",", "1", ",", "E", ",", "N", ",", "N", "]", ")", "*", "aij", ")", "# B x P x E x F x N", "\n", "y", "=", "W0x", "# This is the first multiplication between Wx and Aij corresponding", "\n", "# to the first-hop neighborhood", "\n", "\n", "# Now, we move on to the rest of the coefficients", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "        ", "thisa", "=", "torch", ".", "index_select", "(", "a", ",", "1", ",", "torch", ".", "tensor", "(", "k", ")", ".", "to", "(", "S", ".", "device", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "thisW", "=", "torch", ".", "index_select", "(", "W", ",", "1", ",", "torch", ".", "tensor", "(", "k", ")", ".", "to", "(", "S", ".", "device", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "aij", "=", "learnAttentionGSO", "(", "x", ",", "thisa", ",", "thisW", ",", "S", ",", "\n", "negative_slope", "=", "negative_slope", ")", "\n", "W0x", "=", "torch", ".", "matmul", "(", "W0x", ",", "S", ".", "reshape", "(", "[", "1", ",", "1", ",", "E", ",", "N", ",", "N", "]", ")", "*", "aij", ")", "\n", "# This multiplies the previous W0x Aij^{1:k-1} with A_ij^{(k)}", "\n", "y", "=", "y", "+", "W0x", "# Adds that multiplication to the running sum for all other", "\n", "# ks, shape: B x P x E x F x N", "\n", "\n", "# Sum over all edge features", "\n", "", "y", "=", "torch", ".", "sum", "(", "y", ",", "dim", "=", "2", ")", "# B x P x F x N", "\n", "# Finally, add the bias", "\n", "if", "b", "is", "not", "None", ":", "\n", "        ", "y", "=", "y", "+", "b", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.LSIGF_DB": [[977, 1095], ["torch.matmul.reshape().repeat", "torch.matmul.reshape", "range", "torch.cat.permute", "torch.cat.reshape", "h.permute.reshape", "h.permute.permute", "torch.matmul", "torch.matmul", "y.permute.permute", "len", "len", "len", "torch.split", "torch.split", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.matmul", "torch.matmul", "torch.matmul.reshape", "torch.cat", "torch.cat", "torch.matmul.reshape"], "function", ["None"], ["", "def", "LSIGF_DB", "(", "h", ",", "S", ",", "x", ",", "b", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    LSIGF_DB(filter_taps, GSO, input, bias=None) Computes the output of a \n        linear shift-invariant graph filter (graph convolution) on delayed \n        input and then adds bias.\n\n    Denote as G the number of input features, F the number of output features,\n    E the number of edge features, K the number of filter taps, N the number of\n    nodes, S_{e}(t) in R^{N x N} the GSO for edge feature e at time t, \n    x(t) in R^{G x N} the input data at time t where x_{g}(t) in R^{N} is the\n    graph signal representing feature g, and b in R^{F x N} the bias vector,\n    with b_{f} in R^{N} representing the bias for feature f.\n\n    Then, the LSI-GF is computed as\n        y_{f} = \\sum_{e=1}^{E}\n                    \\sum_{k=0}^{K-1}\n                    \\sum_{g=1}^{G}\n                        [h_{f,g,e}]_{k} S_{e}(t)S_{e}(t-1)...S_{e}(t-(k-1))\n                                        x_{g}(t-k)\n                + b_{f}\n    for f = 1, ..., F.\n\n    Inputs:\n        filter_taps (torch.tensor): array of filter taps; shape:\n            output_features x edge_features x filter_taps x input_features\n        GSO (torch.tensor): graph shift operator; shape:\n            batch_size x time_samples x edge_features \n                                                  x number_nodes x number_nodes\n        input (torch.tensor): input signal; shape:\n            batch_size x time_samples x input_features x number_nodes\n        bias (torch.tensor): shape: output_features x number_nodes\n            if the same bias is to be applied to all nodes, set number_nodes = 1\n            so that b_{f} vector becomes b_{f} \\mathbf{1}_{N}\n\n    Outputs:\n        output: filtered signals; shape:\n            batch_size x time_samples x output_features x number_nodes\n    \"\"\"", "\n", "# This is the LSIGF with (B)atch and (D)elay capabilities (i.e. there is", "\n", "# a different GSO for each element in the batch, and it handles time", "\n", "# sequences, both in the GSO as in the input signal. The GSO should be", "\n", "# transparent).", "\n", "\n", "# So, the input ", "\n", "#   h: F x E x K x G", "\n", "#   S: B x T x E x N x N", "\n", "#   x: B x T x G x N", "\n", "#   b: F x N", "\n", "# And the output has to be", "\n", "#   y: B x T x F x N", "\n", "\n", "# Check dimensions", "\n", "assert", "len", "(", "h", ".", "shape", ")", "==", "4", "\n", "F", "=", "h", ".", "shape", "[", "0", "]", "\n", "E", "=", "h", ".", "shape", "[", "1", "]", "\n", "K", "=", "h", ".", "shape", "[", "2", "]", "\n", "G", "=", "h", ".", "shape", "[", "3", "]", "\n", "assert", "len", "(", "S", ".", "shape", ")", "==", "5", "\n", "B", "=", "S", ".", "shape", "[", "0", "]", "\n", "T", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "E", "\n", "N", "=", "S", ".", "shape", "[", "3", "]", "\n", "assert", "S", ".", "shape", "[", "4", "]", "==", "N", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "assert", "x", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "T", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "G", "\n", "assert", "x", ".", "shape", "[", "3", "]", "==", "N", "\n", "\n", "# We would like a z of shape B x T x K x E x G x N that represents, for", "\n", "# each t, x_t, S_t x_{t-1}, S_t S_{t-1} x_{t-2}, ...,", "\n", "#         S_{t} ... S_{t-(k-1)} x_{t-k}, ..., S_{t} S_{t-1} ... x_{t-(K-1)}", "\n", "# But we don't want to do \"for each t\". We just want to do \"for each k\".", "\n", "\n", "# Let's start by reshaping x so it can be multiplied by S", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", ",", "T", ",", "1", ",", "G", ",", "N", "]", ")", ".", "repeat", "(", "1", ",", "1", ",", "E", ",", "1", ",", "1", ")", "\n", "\n", "# Now, for the first value of k, we just have the same signal", "\n", "z", "=", "x", ".", "reshape", "(", "[", "B", ",", "T", ",", "1", ",", "E", ",", "G", ",", "N", "]", ")", "\n", "# For k = 0, k is counted in dim = 2", "\n", "\n", "# Now we need to start multiplying with S, but displacing the entire thing", "\n", "# once across time", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "# Across dim = 1 we need to \"displace the dimension down\", i.e. where", "\n", "# it used to be t = 1 we now need it to be t=0 and so on. For t=0", "\n", "# we add a \"row\" of zeros.", "\n", "        ", "x", ",", "_", "=", "torch", ".", "split", "(", "x", ",", "[", "T", "-", "1", ",", "1", "]", ",", "dim", "=", "1", ")", "\n", "#   The second part is the most recent time instant which we do not need", "\n", "#   anymore (it's used only once for the first value of K)", "\n", "# Now, we need to add a \"row\" of zeros at the beginning (for t = 0)", "\n", "zeroRow", "=", "torch", ".", "zeros", "(", "B", ",", "1", ",", "E", ",", "G", ",", "N", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "zeroRow", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "# And now we multiply with S", "\n", "x", "=", "torch", ".", "matmul", "(", "x", ",", "S", ")", "\n", "# Add the dimension along K", "\n", "xS", "=", "x", ".", "reshape", "(", "B", ",", "T", ",", "1", ",", "E", ",", "G", ",", "N", ")", "\n", "# And concatenate it with z", "\n", "z", "=", "torch", ".", "cat", "(", "(", "z", ",", "xS", ")", ",", "dim", "=", "2", ")", "\n", "\n", "# Now, we finally made it to a vector z of shape B x T x K x E x G x N", "\n", "# To finally multiply with the filter taps, we need to swap the sizes", "\n", "# and reshape", "\n", "", "z", "=", "z", ".", "permute", "(", "0", ",", "1", ",", "5", ",", "3", ",", "2", ",", "4", ")", "# B x T x N x E x K x G", "\n", "z", "=", "z", ".", "reshape", "(", "B", ",", "T", ",", "N", ",", "E", "*", "K", "*", "G", ")", "\n", "# And the same with the filter taps", "\n", "h", "=", "h", ".", "reshape", "(", "F", ",", "E", "*", "K", "*", "G", ")", "\n", "h", "=", "h", ".", "permute", "(", "1", ",", "0", ")", "# E*K*G x F", "\n", "\n", "# Multiply", "\n", "y", "=", "torch", ".", "matmul", "(", "z", ",", "h", ")", "# B x T x N x F", "\n", "# And permute", "\n", "y", "=", "y", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# B x T x F x N", "\n", "# Finally, add the bias", "\n", "if", "b", "is", "not", "None", ":", "\n", "        ", "y", "=", "y", "+", "b", "\n", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GRNN_DB": [[1096, 1291], ["graphML.LSIGF_DB", "b.unsqueeze().reshape.unsqueeze().reshape", "torch.eye", "torch.eye", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "CK.reshape.reshape", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "zK.permute().reshape().permute.reshape().repeat", "zK.permute().reshape().permute.permute().reshape().permute", "torch.matmul", "torch.matmul", "torch.index_select().reshape", "torch.index_select().reshape", "sigma", "sigma().unsqueeze.unsqueeze", "sigma().unsqueeze.unsqueeze", "z0.reshape().repeat", "range", "Sz.permute().reshape.permute", "torch.matmul", "torch.matmul", "torch.index_select", "torch.index_select", "Axt.reshape.reshape", "sigma().unsqueeze", "torch.cat", "torch.cat", "b.unsqueeze().reshape.unsqueeze", "z0.unsqueeze", "zK.permute().reshape().permute.reshape", "zK.permute().reshape().permute.permute().reshape", "torch.index_select", "torch.index_select", "z0.reshape", "torch.index_select", "torch.index_select", "St.repeat.repeat", "torch.matmul", "torch.matmul", "sigma().unsqueeze.unsqueeze().repeat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat.permute().reshape", "torch.cat.permute", "torch.matmul", "torch.matmul", "torch.cat.permute", "torch.index_select", "torch.index_select", "St.repeat.repeat", "torch.matmul", "torch.matmul", "sigma().unsqueeze.unsqueeze().repeat", "torch.cat", "torch.cat", "torch.cat.permute().reshape", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "sigma", "zK.permute().reshape().permute.permute", "sigma().unsqueeze.unsqueeze", "torch.cat.permute", "sigma().unsqueeze.unsqueeze", "torch.cat.permute"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.LSIGF_DB"], ["", "def", "GRNN_DB", "(", "a", ",", "b", ",", "S", ",", "x", ",", "z0", ",", "sigma", ",", "\n", "xBias", "=", "None", ",", "zBias", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    GRNN_DB(signal_to_hidden_taps, hidden_to_hidden_taps, GSO, input,\n            initial_hidden, nonlinearity, signal_bias, hidden_bias)\n    Computes the sequence of hidden states for the input sequence x following \n    the equation z_{t} = sigma(A(S) x_{t} + B(S) z_{t-1}) with initial state z0\n    and where sigma is the nonlinearity, and A(S) and B(S) are the \n    Input-to-Hidden filters and the Hidden-to-Hidden filters with the \n    corresponding taps.\n    \n    Inputs:\n        signal_to_hidden_taps (torch.tensor): shape\n            hidden_features x edge_features x filter_taps x signal_features\n        hidden_to_hidden_taps (torch.tensor): shape\n            hidden_features x edge_features x filter_taps x hidden_features\n        GSO (torch.tensor): shape\n            batch_size x time x edge_features x number_nodes x number_nodes\n        input (torch.tensor): shape\n            batch_size x time x signal_features x number_nodes\n        initial_hidden: shape\n            batch_size x hidden_features x number_nodes\n        signal_bias (torch.tensor): shape\n            1 x 1 x hidden_features x 1\n        hidden_bias (torch.tensor): shape\n            1 x 1 x hidden_features x 1\n    \n    Outputs:\n        hidden_state: shape\n            batch_size x time x hidden_features x number_nodes\n            \n    \"\"\"", "\n", "# We will compute the hidden state for a delayed and batch data.", "\n", "\n", "# So, the input", "\n", "#   a: H x E x K x F (Input to Hidden filters)", "\n", "#   b: H x E x K x H (Hidden to Hidden filters)", "\n", "#   S: B x T x E x N x N (GSO)", "\n", "#   x: B x T x F x N (Input signal)", "\n", "#   z0: B x H x N (Initial state)", "\n", "#   xBias: 1 x 1 x H x 1 (bias on the Input to Hidden features)", "\n", "#   zBias: 1 x 1 x H x 1 (bias on the Hidden to Hidden features)", "\n", "# And the output has to be", "\n", "#   z: B x T x H x N (Hidden state signal)", "\n", "\n", "# Check dimensions", "\n", "H", "=", "a", ".", "shape", "[", "0", "]", "# Number of hidden state features", "\n", "E", "=", "a", ".", "shape", "[", "1", "]", "# Number of edge features", "\n", "K", "=", "a", ".", "shape", "[", "2", "]", "# Number of filter taps", "\n", "F", "=", "a", ".", "shape", "[", "3", "]", "# Number of input features", "\n", "assert", "b", ".", "shape", "[", "0", "]", "==", "H", "\n", "assert", "b", ".", "shape", "[", "1", "]", "==", "E", "\n", "assert", "b", ".", "shape", "[", "2", "]", "==", "K", "\n", "assert", "b", ".", "shape", "[", "3", "]", "==", "H", "\n", "B", "=", "S", ".", "shape", "[", "0", "]", "\n", "T", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "E", "\n", "N", "=", "S", ".", "shape", "[", "3", "]", "\n", "assert", "S", ".", "shape", "[", "4", "]", "==", "N", "\n", "assert", "x", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "T", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "F", "\n", "assert", "x", ".", "shape", "[", "3", "]", "==", "N", "\n", "assert", "z0", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "z0", ".", "shape", "[", "1", "]", "==", "H", "\n", "assert", "z0", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "# The application of A(S) x(t) doesn't change (it does not depend on z(t))", "\n", "Ax", "=", "LSIGF_DB", "(", "a", ",", "S", ",", "x", ",", "b", "=", "xBias", ")", "# B x T x H x N", "\n", "# This is the filtered signal for all time instants.", "\n", "# This also doesn't split S, it only splits x.", "\n", "\n", "# The b parameters we will always need them in this shape", "\n", "b", "=", "b", ".", "unsqueeze", "(", "0", ")", ".", "reshape", "(", "1", ",", "H", ",", "E", "*", "K", "*", "H", ")", "# 1 x H x EKH", "\n", "# so that we can multiply them with the product Sz that should be of shape", "\n", "# B x EKH x N", "\n", "\n", "# We will also need a selection matrix that selects the first K-1 elements", "\n", "# out of the original K (to avoid torch.split and torch.index_select with", "\n", "# more than one index)", "\n", "CK", "=", "torch", ".", "eye", "(", "K", "-", "1", ",", "device", "=", "S", ".", "device", ")", "# (K-1) x (K-1)", "\n", "zeroRow", "=", "torch", ".", "zeros", "(", "(", "1", ",", "K", "-", "1", ")", ",", "device", "=", "CK", ".", "device", ")", "\n", "CK", "=", "torch", ".", "cat", "(", "(", "CK", ",", "zeroRow", ")", ",", "dim", "=", "0", ")", "# K x (K-1)", "\n", "# This matrix discards the last column when multiplying on the left", "\n", "CK", "=", "CK", ".", "reshape", "(", "1", ",", "1", ",", "1", ",", "K", ",", "K", "-", "1", ")", "# 1(B) x 1(E) x 1(H) x K x K-1", "\n", "\n", "#\\\\\\ Now compute the first time instant", "\n", "\n", "# We just need to multiplicate z0 = z(-1) by b(0) to get z(0)", "\n", "#   Create the zeros that will multiply the values of b(1), b(2), ... b(K-1)", "\n", "#   since we only need b(0)", "\n", "zerosK", "=", "torch", ".", "zeros", "(", "(", "B", ",", "K", "-", "1", ",", "H", ",", "N", ")", ",", "device", "=", "z0", ".", "device", ")", "\n", "#   Concatenate them after z", "\n", "zK", "=", "torch", ".", "cat", "(", "(", "z0", ".", "unsqueeze", "(", "1", ")", ",", "zerosK", ")", ",", "dim", "=", "1", ")", "# B x K x H x N", "\n", "#   Now we have a signal that has only the z(-1) and the rest are zeros, so", "\n", "#   now we can go ahead and multiply it by b. For this to happen, we need", "\n", "#   to reshape it as B x EKH x N, but since we are always reshaping the last", "\n", "#   dimensions we will bring EKH to the end, reshape, and then put them back", "\n", "#   in the middle.", "\n", "zK", "=", "zK", ".", "reshape", "(", "B", ",", "1", ",", "K", ",", "H", ",", "N", ")", ".", "repeat", "(", "1", ",", "E", ",", "1", ",", "1", ",", "1", ")", "# B x E x K x H x N", "\n", "zK", "=", "zK", ".", "permute", "(", "0", ",", "4", ",", "1", ",", "2", ",", "3", ")", ".", "reshape", "(", "B", ",", "N", ",", "E", "*", "K", "*", "H", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "#   B x EKH x N", "\n", "#   Finally, we can go ahead an multiply with b", "\n", "zt", "=", "torch", ".", "matmul", "(", "b", ",", "zK", ")", "# B x H x N", "\n", "# Now that we have b(0) z(0) we can add the bias, if necessary", "\n", "if", "zBias", "is", "not", "None", ":", "\n", "        ", "zt", "=", "zt", "+", "zBias", "\n", "# And we need to add it to a(0)x(0) which is the first element of Ax in the", "\n", "# T dimension", "\n", "# Let's do a torch.index_select; not so sure a selection matrix isn't better", "\n", "", "a0x0", "=", "torch", ".", "index_select", "(", "Ax", ",", "1", ",", "torch", ".", "tensor", "(", "0", ",", "device", "=", "Ax", ".", "device", ")", ")", ".", "reshape", "(", "B", ",", "H", ",", "N", ")", "\n", "#   B x H x N", "\n", "# Recall that a0x0 already has the bias, so now we just need to add up and", "\n", "# apply the nonlinearity", "\n", "zt", "=", "sigma", "(", "a0x0", "+", "zt", ")", "# B x H x N", "\n", "z", "=", "zt", ".", "unsqueeze", "(", "1", ")", "# B x 1 x H x N", "\n", "zt", "=", "zt", ".", "unsqueeze", "(", "1", ")", "# B x 1 x H x N", "\n", "# This is where we will keep track of the product Sz", "\n", "Sz", "=", "z0", ".", "reshape", "(", "B", ",", "1", ",", "1", ",", "H", ",", "N", ")", ".", "repeat", "(", "1", ",", "1", ",", "E", ",", "1", ",", "1", ")", "# B x 1 x E x H x N", "\n", "\n", "# Starting now, we need to multiply this by S every time", "\n", "for", "t", "in", "range", "(", "1", ",", "T", ")", ":", "\n", "        ", "if", "t", "<", "K", ":", "\n", "# Get the current time instant", "\n", "            ", "St", "=", "torch", ".", "index_select", "(", "S", ",", "1", ",", "torch", ".", "tensor", "(", "t", ",", "device", "=", "S", ".", "device", ")", ")", "\n", "#   B x 1 x E x N x N", "\n", "# We need to multiply this time instant by all the elements in Sz", "\n", "# now, and there are t of those", "\n", "St", "=", "St", ".", "repeat", "(", "1", ",", "t", ",", "1", ",", "1", ",", "1", ")", "# B x t x E x N x N", "\n", "# Multiply by the newly acquired St to do one more delay", "\n", "Sz", "=", "torch", ".", "matmul", "(", "Sz", ",", "St", ")", "# B x t x E x H x N", "\n", "# Observe that these delays are backward: the last element in the", "\n", "# T dimension (dim = 1) is the latest element, this makes sense ", "\n", "# since that is the element we want to multiply by the last element", "\n", "# in b.", "\n", "\n", "# Now that we have delayed, add the newest value (which requires", "\n", "# no delay)", "\n", "ztThis", "=", "zt", ".", "unsqueeze", "(", "2", ")", ".", "repeat", "(", "1", ",", "1", ",", "E", ",", "1", ",", "1", ")", "# B x 1 x E x H x N", "\n", "Sz", "=", "torch", ".", "cat", "(", "(", "ztThis", ",", "Sz", ")", ",", "dim", "=", "1", ")", "# B x (t+1) x E x H x N", "\n", "\n", "# Pad all those values that are not there yet (will multiply b", "\n", "# by zero)", "\n", "zeroRow", "=", "torch", ".", "zeros", "(", "(", "B", ",", "K", "-", "(", "t", "+", "1", ")", ",", "E", ",", "H", ",", "N", ")", ",", "device", "=", "Sz", ".", "device", ")", "\n", "SzPad", "=", "torch", ".", "cat", "(", "(", "Sz", ",", "zeroRow", ")", ",", "dim", "=", "1", ")", "# B x K x E x H x N", "\n", "\n", "# Reshape and permute to adapt to multiplication with b (happens", "\n", "# outside the if)", "\n", "bSz", "=", "SzPad", ".", "permute", "(", "0", ",", "4", ",", "2", ",", "1", ",", "3", ")", ".", "reshape", "(", "B", ",", "N", ",", "E", "*", "K", "*", "H", ")", "\n", "", "else", ":", "\n", "# Now, we have t>=K which means that Sz is of shape", "\n", "#   B x K x E x H x N", "\n", "# and thus is full, so we need to get rid of the last element in Sz", "\n", "# before adding the new element and multiplying by St.", "\n", "\n", "# We can always get rid of the last element by multiplying by a", "\n", "# K x (K-1) selection matrix. So we do that (first we need to", "\n", "# permute to have the dimensions ready for multiplication)", "\n", "            ", "Sz", "=", "Sz", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "4", ",", "1", ")", "# B x E x H x N x K", "\n", "Sz", "=", "torch", ".", "matmul", "(", "Sz", ",", "CK", ")", "# B x E x H x N x (K-1)", "\n", "Sz", "=", "Sz", ".", "permute", "(", "0", ",", "4", ",", "1", ",", "2", ",", "3", ")", "# B x (K-1) x E x H x N", "\n", "\n", "# Get the current time instant", "\n", "St", "=", "torch", ".", "index_select", "(", "S", ",", "1", ",", "torch", ".", "tensor", "(", "t", ",", "device", "=", "S", ".", "device", ")", ")", "\n", "#   B x 1 x E x N x N", "\n", "# We need to multiply this time instant by all the elements in Sz", "\n", "# now, and there are K-1 of those", "\n", "St", "=", "St", ".", "repeat", "(", "1", ",", "K", "-", "1", ",", "1", ",", "1", ",", "1", ")", "# B x (K-1) x E x N x N", "\n", "# Multiply by the newly acquired St to do one more delay", "\n", "Sz", "=", "torch", ".", "matmul", "(", "Sz", ",", "St", ")", "# B x (K-1) x E x H x N", "\n", "\n", "# Now that we have delayed, add the newest value (which requires", "\n", "# no delay)", "\n", "ztThis", "=", "zt", ".", "unsqueeze", "(", "2", ")", ".", "repeat", "(", "1", ",", "1", ",", "E", ",", "1", ",", "1", ")", "# B x 1 x E x H x N", "\n", "Sz", "=", "torch", ".", "cat", "(", "(", "ztThis", ",", "Sz", ")", ",", "dim", "=", "1", ")", "# B x K x E x H x N", "\n", "\n", "# Reshape and permute to adapt to multiplication with b (happens", "\n", "# outside the if)", "\n", "bSz", "=", "Sz", ".", "permute", "(", "0", ",", "4", ",", "2", ",", "1", ",", "3", ")", ".", "reshape", "(", "B", ",", "N", ",", "E", "*", "K", "*", "H", ")", "\n", "\n", "# Get back to proper order", "\n", "", "bSz", "=", "bSz", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# B x EKH x N", "\n", "#   And multiply with the coefficients", "\n", "Bzt", "=", "torch", ".", "matmul", "(", "b", ",", "bSz", ")", "# B x H x N", "\n", "# Now that we have the Bz for this time instant, add the bias", "\n", "if", "zBias", "is", "not", "None", ":", "\n", "            ", "Bzt", "=", "Bzt", "+", "zBias", "\n", "# Get the corresponding value of Ax", "\n", "", "Axt", "=", "torch", ".", "index_select", "(", "Ax", ",", "1", ",", "torch", ".", "tensor", "(", "t", ",", "device", "=", "Ax", ".", "device", ")", ")", "\n", "Axt", "=", "Axt", ".", "reshape", "(", "B", ",", "H", ",", "N", ")", "\n", "# Sum and apply the nonlinearity", "\n", "zt", "=", "sigma", "(", "Axt", "+", "Bzt", ")", ".", "unsqueeze", "(", "1", ")", "# B x 1 x H x N", "\n", "z", "=", "torch", ".", "cat", "(", "(", "z", ",", "zt", ")", ",", "dim", "=", "1", ")", "# B x (t+1) x H x N", "\n", "\n", "", "return", "z", "# B x T x H x N", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.GatedGRNN": [[1292, 1528], ["torch.ones", "torch.ones", "torch.ones", "torch.ones", "range", "len", "len", "xBias.reshape.reshape", "zBias.reshape.reshape", "len", "graphML.LSIGF", "Ax.reshape.reshape", "S.unsqueeze", "edgeS.repeat.repeat", "x.permute.reshape", "x.permute.unsqueeze", "x.permute.reshape().repeat", "range", "torch.matmul().permute", "torch.matmul().permute", "Ax.reshape.reshape", "torch.index_select", "torch.index_select", "Axt.reshape.reshape", "sigma().unsqueeze", "len", "len", "x.permute.reshape", "q_hat.reshape", "x.permute.reshape", "torch.matmul", "torch.matmul", "x.permute.reshape", "torch.diagonal", "torch.diagonal", "x.permute.permute", "x.permute.reshape", "torch.cat", "torch.cat", "len", "graphML.LSIGF", "torch.index_select", "torch.index_select", "this_q_check.squeeze.squeeze", "S.unsqueeze", "edgeS.repeat.repeat", "zt.permute.reshape", "zt.permute.unsqueeze", "zt.permute.reshape().repeat", "range", "torch.matmul().permute", "torch.matmul().permute", "torch.tensor", "torch.tensor", "torch.cat", "torch.cat", "x.permute.reshape", "torch.matmul", "torch.matmul", "zt.permute.reshape", "len", "torch.index_select", "torch.index_select", "this_q_check.squeeze.squeeze", "torch.tensor", "torch.tensor", "zt.permute.reshape", "torch.matmul", "torch.matmul", "zt.permute.reshape", "torch.diagonal", "torch.diagonal", "zt.permute.permute", "zt.permute.reshape", "torch.cat", "torch.cat", "sigma", "torch.cat.permute().reshape", "a.reshape().permute", "torch.tensor", "torch.tensor", "zt.permute.reshape", "torch.matmul", "torch.matmul", "torch.cat.permute().reshape", "b.reshape().permute", "torch.cat.permute", "a.reshape", "torch.cat.permute", "b.reshape"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.LSIGF", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.LSIGF"], ["", "def", "GatedGRNN", "(", "a", ",", "b", ",", "S", ",", "x", ",", "z0", ",", "sigma", ",", "q_hat", "=", "torch", ".", "ones", "(", "1", ")", ",", "q_check", "=", "torch", ".", "ones", "(", "1", ")", ",", "\n", "xBias", "=", "None", ",", "zBias", "=", "None", ")", ":", "\n", "# Luana R. Ruiz, rubruiz@seas.upenn.edu, 2021/01/28", "\n", "    ", "\"\"\"\n    GatedGRNN(signal_to_hidden_taps, hidden_to_hidden_taps, GSO, input,\n            initial_hidden, nonlinearity, input_gate, forget_gate, signal_bias,\n            hidden_bias)\n    Computes the sequence of hidden states for the input sequence x following \n    the equation z_{t} = sigma(Q_hat{A(S) x_{t}} + Q_check{B(S) z_{t-1}}) with \n    initial state z0 and where sigma is the nonlinearity, A(S) and B(S) are the \n    Input-to-Hidden filters and the Hidden-to-Hidden filters with the \n    corresponding taps, Q_hat is the input gate operator and Q_check the forget\n    gate operator\n    \n    Inputs:\n        signal_to_hidden_taps (torch.tensor): shape\n            hidden_features x edge_features x filter_taps x signal_features\n        hidden_to_hidden_taps (torch.tensor): shape\n            hidden_features x edge_features x filter_taps x hidden_features\n        GSO (torch.tensor): shape\n            batch_size x edge_features x number_nodes x number_nodes\n        input (torch.tensor): shape\n            batch_size x time x signal_features x number_nodes\n        initial_hidden: shape\n            batch_size x hidden_features x number_nodes\n        input_gate: shape depends on the type of gating\n            > no gating \n                torch.ones(1)\n            > time gating:\n                batch_size x time x 1 x 1\n            > node gating:\n                batch_size x time x 1 x number_nodes\n            > edge gating:\n                batch_size x time x 1 x number_nodes x number_nodes\n        forget_gate: shape depends on the type of gating\n            > no gating \n                torch.ones(1)\n            > time gating:\n                batch_size x time x 1 x 1\n            > node gating:\n                batch_size x time x 1 x number_nodes\n            > edge gating:\n                batch_size x time x 1 x number_nodes x number_nodes              \n        signal_bias (torch.tensor): shape\n            1 x 1 x hidden_features x 1\n        hidden_bias (torch.tensor): shape\n            1 x 1 x hidden_features x 1\n    \n    Outputs:\n        hidden_state: shape\n            batch_size x time x hidden_features x number_nodes\n            \n    \"\"\"", "\n", "# We will compute the hidden state for a delayed and batch data.", "\n", "\n", "# So, the input", "\n", "#   a: H x E x K x F (Input to Hidden filters)", "\n", "#   b: H x E x K x H (Hidden to Hidden filters)", "\n", "#   S: E x N x N (GSO)", "\n", "#   x: B x T x F x N (Input signal)", "\n", "#   z0: B x H x N (Initial state)", "\n", "#   xBias: 1 x 1 x H x 1 (bias on the Input to Hidden features)", "\n", "#   zBias: 1 x 1 x H x 1 (bias on the Hidden to Hidden features)", "\n", "# And the output has to be", "\n", "#   z: B x T x H x N (Hidden state signal)", "\n", "# q_hat and q_check depend on type of gating", "\n", "\n", "# Check dimensions", "\n", "H", "=", "a", ".", "shape", "[", "0", "]", "# Number of hidden state features", "\n", "E", "=", "a", ".", "shape", "[", "1", "]", "# Number of edge features", "\n", "K", "=", "a", ".", "shape", "[", "2", "]", "# Number of filter taps", "\n", "F", "=", "a", ".", "shape", "[", "3", "]", "# Number of input features", "\n", "assert", "b", ".", "shape", "[", "0", "]", "==", "H", "\n", "assert", "b", ".", "shape", "[", "1", "]", "==", "E", "\n", "assert", "b", ".", "shape", "[", "2", "]", "==", "K", "\n", "assert", "b", ".", "shape", "[", "3", "]", "==", "H", "\n", "assert", "S", ".", "shape", "[", "0", "]", "==", "E", "\n", "N", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "F", "\n", "assert", "x", ".", "shape", "[", "3", "]", "==", "N", "\n", "assert", "z0", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "z0", ".", "shape", "[", "1", "]", "==", "H", "\n", "assert", "z0", ".", "shape", "[", "2", "]", "==", "N", "\n", "assert", "q_hat", ".", "shape", "[", "0", "]", "==", "B", "or", "q_hat", ".", "shape", "[", "0", "]", "==", "1", "\n", "if", "len", "(", "q_hat", ".", "shape", ")", ">", "1", ":", "\n", "        ", "assert", "q_hat", ".", "shape", "[", "1", "]", "==", "T", "\n", "assert", "q_hat", ".", "shape", "[", "2", "]", "==", "1", "\n", "assert", "q_hat", ".", "shape", "[", "3", "]", "==", "1", "or", "q_hat", ".", "shape", "[", "3", "]", "==", "N", "\n", "if", "len", "(", "q_hat", ".", "shape", ")", ">", "4", ":", "\n", "            ", "assert", "q_hat", ".", "shape", "[", "4", "]", "==", "N", "\n", "", "", "assert", "q_check", ".", "shape", "[", "0", "]", "==", "B", "or", "q_check", ".", "shape", "[", "0", "]", "==", "1", "\n", "if", "len", "(", "q_check", ".", "shape", ")", ">", "1", ":", "\n", "        ", "assert", "q_check", ".", "shape", "[", "1", "]", "==", "T", "\n", "assert", "q_check", ".", "shape", "[", "2", "]", "==", "1", "\n", "assert", "q_check", ".", "shape", "[", "3", "]", "==", "1", "or", "q_check", ".", "shape", "[", "3", "]", "==", "N", "\n", "if", "len", "(", "q_check", ".", "shape", ")", ">", "4", ":", "\n", "            ", "assert", "q_check", ".", "shape", "[", "4", "]", "==", "N", "\n", "\n", "# Checking if there is bias", "\n", "", "", "if", "xBias", "is", "not", "None", ":", "\n", "        ", "xBias", "=", "xBias", ".", "reshape", "(", "1", ",", "H", ",", "1", ")", "\n", "", "if", "zBias", "is", "not", "None", ":", "\n", "        ", "zBias", "=", "zBias", ".", "reshape", "(", "1", ",", "H", ",", "1", ")", "\n", "\n", "# We start by handling the input to state transformation Ax", "\n", "# First, we have to check if we are NOT doing edge gating", "\n", "", "if", "len", "(", "q_hat", ".", "shape", ")", "<", "5", ":", "\n", "# The application of A(S) x(t) doesn't change (it does not depend on z(t))", "\n", "        ", "Ax", "=", "LSIGF", "(", "a", ",", "S", ",", "x", ".", "reshape", "(", "(", "B", "*", "T", ",", "F", ",", "N", ")", ")", ",", "\n", "b", "=", "xBias", ")", "# BT x H x N", "\n", "# We merge the batch and time dimensions of x to apply the linear shift-", "\n", "# invariant graph filter. Then, we re-add the time dimension", "\n", "Ax", "=", "Ax", ".", "reshape", "(", "(", "B", ",", "T", ",", "H", ",", "N", ")", ")", "\n", "# This is the filtered signal for all time instants. Finally, we \"gate\"", "\n", "Ax", "=", "q_hat", "*", "Ax", "\n", "", "else", ":", "\n", "# If we have edge gating, we have to add a batch and a time dimension to S", "\n", "        ", "edgeS", "=", "S", ".", "unsqueeze", "(", "0", ")", "# 1 x E x N x N", "\n", "edgeS", "=", "edgeS", ".", "repeat", "(", "B", "*", "T", ",", "1", ",", "1", ",", "1", ")", "# BT x E x N x N", "\n", "# The first step is to gate the GSO", "\n", "edgeS", "=", "q_hat", ".", "reshape", "(", "[", "B", "*", "T", ",", "E", ",", "N", ",", "N", "]", ")", "*", "edgeS", "\n", "# Then we reshape x to multiply each batch and sequence element by the ", "\n", "# corresponding gated GSO...", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", "*", "T", ",", "F", ",", "N", "]", ")", "\n", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "# B*T x 1 x F x N", "\n", "# ... and follow a similar filtering procedure as in the LSI-GF", "\n", "# u is the tensor used to store S^0x, S^1x, ..., S^{K-1}x", "\n", "u", "=", "x", ".", "reshape", "(", "[", "B", "*", "T", ",", "1", ",", "1", ",", "F", ",", "N", "]", ")", ".", "repeat", "(", "1", ",", "E", ",", "1", ",", "1", ",", "1", ")", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "            ", "x", "=", "x", ".", "reshape", "(", "(", "B", "*", "T", ",", "F", ",", "N", ")", ")", "\n", "x", "=", "torch", ".", "matmul", "(", "x", ",", "edgeS", ")", "# BT x BT x E x F x N", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", "*", "T", ",", "B", "*", "T", ",", "E", ",", "F", ",", "N", "]", ")", "# BT x BT x E x F x N", "\n", "# We only care about the elements for which the batch-time indices ", "\n", "# of x match the batch-time indices of S, therefore we take the", "\n", "# diagonal along dimensions 0 and 1", "\n", "x", "=", "torch", ".", "diagonal", "(", "x", ")", "# E x F x N x BT", "\n", "x", "=", "x", ".", "permute", "(", "3", ",", "0", ",", "1", ",", "2", ")", "# BT x E x F x N", "\n", "xS", "=", "x", ".", "reshape", "(", "[", "B", "*", "T", ",", "E", ",", "1", ",", "F", ",", "N", "]", ")", "# BT x E x 1 x F x N", "\n", "u", "=", "torch", ".", "cat", "(", "(", "u", ",", "xS", ")", ",", "dim", "=", "2", ")", "# BT x E x k x F x N", "\n", "# This output u is of size BT x E x K x F x N", "\n", "# Now we have the x*S_{e}^{k} product, and we need to multiply with the", "\n", "# filter taps.", "\n", "# We multiply u on the left, and a on the right, the output is to be", "\n", "# B x N x H (the multiplication is not along the N dimension), so we reshape", "\n", "# u to be B x N x E x K x F and reshape it to B x N x EKF (remember we", "\n", "# always reshape the last dimensions), and then make a be E x K x F x H and", "\n", "# reshape it to EKF x H, and then multiply", "\n", "", "Ax", "=", "torch", ".", "matmul", "(", "u", ".", "permute", "(", "0", ",", "4", ",", "1", ",", "2", ",", "3", ")", ".", "reshape", "(", "[", "B", "*", "T", ",", "N", ",", "E", "*", "K", "*", "F", "]", ")", ",", "\n", "a", ".", "reshape", "(", "[", "H", ",", "E", "*", "K", "*", "F", "]", ")", ".", "permute", "(", "1", ",", "0", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "# And permute againt to bring it from B x N x H to B x H x N.", "\n", "# Finally, add the bias", "\n", "if", "xBias", "is", "not", "None", ":", "\n", "            ", "Ax", "=", "Ax", "+", "xBias", "\n", "# We have merged the batch and time dimensions of x and S to apply the ", "\n", "# linear shift-invariant graph filter. Now, we re-add the time dimension", "\n", "", "Ax", "=", "Ax", ".", "reshape", "(", "(", "B", ",", "T", ",", "H", ",", "N", ")", ")", "\n", "# This is the filtered signal for all time instants", "\n", "\n", "# The second step is to handle the state-to-state transformation Bz", "\n", "# Assign first state as initial state", "\n", "", "zt", "=", "z0", "\n", "# Calculate the hidden states for t=1,...,T", "\n", "for", "t", "in", "range", "(", "1", ",", "T", "+", "1", ")", ":", "\n", "# First, we have to check if we are NOT doing edge gating", "\n", "        ", "if", "len", "(", "q_check", ".", "shape", ")", "<", "5", ":", "\n", "# We apply the filter B(S) to the hidden state at time t-1", "\n", "            ", "Bzt", "=", "LSIGF", "(", "b", ",", "S", ",", "zt", ".", "reshape", "(", "(", "B", ",", "H", ",", "N", ")", ")", ",", "\n", "b", "=", "zBias", ")", "# B x H x N", "\n", "# Then, if there is a gate, we select the gate corresponding to ", "\n", "# instant t...", "\n", "if", "len", "(", "q_check", ".", "shape", ")", ">", "1", ":", "\n", "                ", "this_q_check", "=", "torch", ".", "index_select", "(", "q_check", ",", "1", ",", "\n", "torch", ".", "tensor", "(", "t", "-", "1", ",", "device", "=", "q_check", ".", "device", ")", ")", "\n", "this_q_check", "=", "this_q_check", ".", "squeeze", "(", "1", ")", "# B x 1 x (1 for time) or (N for node)", "\n", "# ... and if there is no gate, this gate is simply equal to the default gate", "\n", "", "else", ":", "\n", "                ", "this_q_check", "=", "q_check", "\n", "# and apply it", "\n", "", "Bzt", "=", "this_q_check", "*", "Bzt", "\n", "", "else", ":", "\n", "# If we have edge gating, we first have to select the gate corresponding ", "\n", "# to instant t", "\n", "            ", "this_q_check", "=", "torch", ".", "index_select", "(", "q_check", ",", "1", ",", "\n", "torch", ".", "tensor", "(", "t", "-", "1", ",", "device", "=", "q_check", ".", "device", ")", ")", "\n", "this_q_check", "=", "this_q_check", ".", "squeeze", "(", "1", ")", "# B x 1 x N x N", "\n", "# We also have to add a batch dimension to S", "\n", "edgeS", "=", "S", ".", "unsqueeze", "(", "0", ")", "# 1 x E x N x N", "\n", "edgeS", "=", "edgeS", ".", "repeat", "(", "B", ",", "1", ",", "1", ",", "1", ")", "# B x E x N x N", "\n", "# Then we're ready to gate the GSO, ...", "\n", "edgeS", "=", "this_q_check", "*", "edgeS", "\n", "# ... and follow a similar filtering procedure as in the LSI-GF", "\n", "zt", "=", "zt", ".", "reshape", "(", "[", "B", ",", "H", ",", "N", "]", ")", "\n", "zt", "=", "zt", ".", "unsqueeze", "(", "1", ")", "# B x 1 x F x N", "\n", "# u is the tensor used to store S^0zt, S^1zt, ..., S^{K-1}zt", "\n", "u", "=", "zt", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "H", ",", "N", "]", ")", ".", "repeat", "(", "1", ",", "E", ",", "1", ",", "1", ",", "1", ")", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "                ", "zt", "=", "zt", ".", "reshape", "(", "(", "B", ",", "H", ",", "N", ")", ")", "\n", "zt", "=", "torch", ".", "matmul", "(", "zt", ",", "edgeS", ")", "\n", "zt", "=", "zt", ".", "reshape", "(", "[", "B", ",", "B", ",", "E", ",", "H", ",", "N", "]", ")", "# B x B x E x H x N", "\n", "# We only care about the elements for which the batch-time indices ", "\n", "# of zt match the batch-time indices of S, therefore we take the", "\n", "# diagonal along dimensions 0 and 1", "\n", "zt", "=", "torch", ".", "diagonal", "(", "zt", ")", "# E x H x N x B", "\n", "zt", "=", "zt", ".", "permute", "(", "3", ",", "0", ",", "1", ",", "2", ")", "# B x E x H x N", "\n", "ztS", "=", "zt", ".", "reshape", "(", "[", "B", ",", "E", ",", "1", ",", "H", ",", "N", "]", ")", "# B x E x 1 x H x N", "\n", "u", "=", "torch", ".", "cat", "(", "(", "u", ",", "ztS", ")", ",", "dim", "=", "2", ")", "# B x E x k x H x N", "\n", "# This output u is of size B x E x K x H x N", "\n", "# Now we have the zt*S_{e}^{k} product, and we need to multiply with the", "\n", "# filter taps.", "\n", "# We multiply u on the left, and b on the right, the output is to be", "\n", "# B x N x H (the multiplication is not along the N dimension), so we reshape", "\n", "# u to be B x N x E x K x H and reshape it to B x N x EKH (remember we", "\n", "# always reshape the last dimensions), and then make b be E x K x H x H and", "\n", "# reshape it to EKH x H, and then multiply", "\n", "", "Bzt", "=", "torch", ".", "matmul", "(", "u", ".", "permute", "(", "0", ",", "4", ",", "1", ",", "2", ",", "3", ")", ".", "reshape", "(", "[", "B", ",", "N", ",", "E", "*", "K", "*", "H", "]", ")", ",", "\n", "b", ".", "reshape", "(", "[", "H", ",", "E", "*", "K", "*", "H", "]", ")", ".", "permute", "(", "1", ",", "0", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "# And permute againt to bring it from B x N x H to B x H x N.", "\n", "# Finally, add the bias", "\n", "if", "zBias", "is", "not", "None", ":", "\n", "                ", "Bzt", "=", "Bzt", "+", "zBias", "\n", "\n", "# Now we are able to compute the current state from Ax and Bzt", "\n", "# Get the corresponding value of Ax", "\n", "", "", "Axt", "=", "torch", ".", "index_select", "(", "Ax", ",", "1", ",", "torch", ".", "tensor", "(", "t", "-", "1", ",", "device", "=", "Ax", ".", "device", ")", ")", "\n", "Axt", "=", "Axt", ".", "reshape", "(", "B", ",", "H", ",", "N", ")", "\n", "# Sum and apply the nonlinearity", "\n", "zt", "=", "sigma", "(", "Axt", "+", "Bzt", ")", ".", "unsqueeze", "(", "1", ")", "# B x 1 x H x N", "\n", "if", "t", "==", "1", ":", "\n", "            ", "z", "=", "zt", "# initialize hidden state tensor", "\n", "", "else", ":", "\n", "            ", "z", "=", "torch", ".", "cat", "(", "(", "z", ",", "zt", ")", ",", "dim", "=", "1", ")", "# B x (t+1) x H x N", "\n", "\n", "", "", "return", "z", "# B x T x H x N", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.loss.adaptExtraDimensionLoss.__init__": [[62, 72], ["super().__init__", "len", "lossFunction", "lossFunction"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "lossFunction", ",", "*", "args", ")", ":", "\n", "# The second argument is optional and it is if there are any extra ", "\n", "# arguments with which we want to initialize the loss", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "len", "(", "args", ")", ">", "0", ":", "\n", "            ", "self", ".", "loss", "=", "lossFunction", "(", "*", "args", ")", "# Initialize loss function", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss", "=", "lossFunction", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.loss.adaptExtraDimensionLoss.forward": [[73, 92], ["loss.adaptExtraDimensionLoss.loss", "repr", "len", "repr", "repr", "repr", "len", "estimate.squeeze.squeeze.squeeze", "len"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "estimate", ",", "target", ")", ":", "\n", "\n", "# What we're doing here is checking what kind of loss it is and", "\n", "# what kind of reshape we have to do on the estimate", "\n", "\n", "        ", "if", "'CrossEntropyLoss'", "in", "repr", "(", "self", ".", "loss", ")", ":", "\n", "# This is supposed to be a one-hot vector batchSize x nClasses", "\n", "            ", "assert", "len", "(", "estimate", ".", "shape", ")", "==", "2", "\n", "", "elif", "'SmoothL1Loss'", "in", "repr", "(", "self", ".", "loss", ")", "or", "'MSELoss'", "in", "repr", "(", "self", ".", "loss", ")", "or", "'L1Loss'", "in", "repr", "(", "self", ".", "loss", ")", ":", "\n", "# In this case, the estimate has to be a batchSize tensor, so if", "\n", "# it has two dimensions, the second dimension has to be 1", "\n", "            ", "if", "len", "(", "estimate", ".", "shape", ")", "==", "2", ":", "\n", "                ", "assert", "estimate", ".", "shape", "[", "1", "]", "==", "1", "\n", "estimate", "=", "estimate", ".", "squeeze", "(", "1", ")", "\n", "", "assert", "len", "(", "estimate", ".", "shape", ")", "==", "1", "\n", "\n", "", "return", "self", ".", "loss", "(", "estimate", ",", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.loss.F1Score": [[93, 126], ["len", "torch.exp.reshape", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.exp", "torch.exp", "y.reshape.reshape", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.mean", "torch.mean"], "function", ["None"], ["", "", "def", "F1Score", "(", "yHat", ",", "y", ")", ":", "\n", "# Luana R. Ruiz, rubruiz@seas.upenn.edu, 2021/03/04", "\n", "    ", "dimensions", "=", "len", "(", "yHat", ".", "shape", ")", "\n", "C", "=", "yHat", ".", "shape", "[", "dimensions", "-", "2", "]", "\n", "N", "=", "yHat", ".", "shape", "[", "dimensions", "-", "1", "]", "\n", "yHat", "=", "yHat", ".", "reshape", "(", "(", "-", "1", ",", "C", ",", "N", ")", ")", "\n", "yHat", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "yHat", ",", "dim", "=", "1", ")", "\n", "yHat", "=", "torch", ".", "exp", "(", "yHat", ")", "\n", "yHat", "=", "yHat", "[", ":", ",", "1", ",", ":", "]", "\n", "y", "=", "y", ".", "reshape", "(", "(", "-", "1", ",", "N", ")", ")", "\n", "\n", "tp", "=", "torch", ".", "sum", "(", "y", "*", "yHat", ",", "1", ")", "\n", "#tn = torch.sum((1-y)*(1-yHat),1)", "\n", "fp", "=", "torch", ".", "sum", "(", "(", "1", "-", "y", ")", "*", "yHat", ",", "1", ")", "\n", "fn", "=", "torch", ".", "sum", "(", "y", "*", "(", "1", "-", "yHat", ")", ",", "1", ")", "\n", "\n", "p", "=", "tp", "/", "(", "tp", "+", "fp", ")", "\n", "r", "=", "tp", "/", "(", "tp", "+", "fn", ")", "\n", "\n", "idx_p", "=", "p", "!=", "p", "\n", "idx_tp", "=", "tp", "==", "0", "\n", "idx_p1", "=", "idx_p", "*", "idx_tp", "\n", "p", "[", "idx_p", "]", "=", "0", "\n", "p", "[", "idx_p1", "]", "=", "1", "\n", "idx_r", "=", "r", "!=", "r", "\n", "idx_r1", "=", "idx_r", "*", "idx_tp", "\n", "r", "[", "idx_r", "]", "=", "0", "\n", "r", "[", "idx_r1", "]", "=", "1", "\n", "\n", "f1", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "\n", "f1", "[", "f1", "!=", "f1", "]", "=", "0", "\n", "\n", "return", "1", "-", "torch", ".", "mean", "(", "f1", ")", "", "", ""]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.LocalGNN_DB.__init__": [[114, 170], ["torch.Module.__init__", "len", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "gfl.append", "gfl.append", "len", "fc.append", "range", "len", "alegnn.GraphFilter_DB", "architecturesTime.LocalGNN_DB.sigma", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architecturesTime.LocalGNN_DB.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nFilterTaps", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# MLP in the end", "\n", "dimReadout", ",", "\n", "# Structure", "\n", "dimEdgeFeatures", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nFilterTaps", ")", "+", "1", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "L", "=", "len", "(", "nFilterTaps", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "E", "=", "dimEdgeFeatures", "# Number of edge features", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "# Store the rest of the variables", "\n", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "dimReadout", "=", "dimReadout", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "gfl", "=", "[", "]", "# Graph Filtering Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "gfl", ".", "append", "(", "gml", ".", "GraphFilter_DB", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "self", ".", "K", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ")", ")", "\n", "#\\\\ Nonlinearity", "\n", "gfl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "GFL", "=", "nn", ".", "Sequential", "(", "*", "gfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimReadout", ")", ">", "0", ":", "# Maybe we don't want to readout anything", "\n", "# The first layer has to connect whatever was left of the graph ", "\n", "# filtering stage to create the number of features required by", "\n", "# the readout layer", "\n", "            ", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "F", "[", "-", "1", "]", ",", "dimReadout", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimReadout", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimReadout", "[", "l", "]", ",", "dimReadout", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "Readout", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.LocalGNN_DB.splitForward": [[172, 203], ["range", "architecturesTime.LocalGNN_DB.GFL", "architecturesTime.LocalGNN_DB.permute", "architecturesTime.LocalGNN_DB.Readout", "len", "S.unsqueeze.unsqueeze.unsqueeze", "len", "architecturesTime.LocalGNN_DB.GFL[].addGSO", "architecturesTime.LocalGNN_DB.permute", "len", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "splitForward", "(", "self", ",", "x", ",", "S", ")", ":", "\n", "\n", "# Check the dimensions of the input", "\n", "#   S: B x T (x E) x N x N", "\n", "#   x: B x T x F[0] x N", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "4", "or", "len", "(", "S", ".", "shape", ")", "==", "5", "\n", "if", "len", "(", "S", ".", "shape", ")", "==", "4", ":", "\n", "            ", "S", "=", "S", ".", "unsqueeze", "(", "2", ")", "\n", "", "B", "=", "S", ".", "shape", "[", "0", "]", "\n", "T", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "E", "\n", "N", "=", "S", ".", "shape", "[", "3", "]", "\n", "assert", "S", ".", "shape", "[", "4", "]", "==", "N", "\n", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "assert", "x", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "T", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "3", "]", "==", "N", "\n", "\n", "# Add the GSO at each layer", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "GFL", "[", "2", "*", "l", "]", ".", "addGSO", "(", "S", ")", "\n", "# Let's call the graph filtering layer", "\n", "", "yGFL", "=", "self", ".", "GFL", "(", "x", ")", "\n", "# Change the order, for the readout", "\n", "y", "=", "yGFL", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# B x T x N x F[-1]", "\n", "# And, feed it into the Readout layer", "\n", "y", "=", "self", ".", "Readout", "(", "y", ")", "# B x T x N x dimReadout[-1]", "\n", "# Reshape and return", "\n", "return", "y", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", ",", "yGFL", "\n", "# B x T x dimReadout[-1] x N, B x T x dimFeatures[-1] x N", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.LocalGNN_DB.forward": [[205, 215], ["architecturesTime.LocalGNN_DB.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ",", "S", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ",", "S", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.LocalGNN_DB.singleNodeForward": [[216, 272], ["numpy.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "architecturesTime.LocalGNN_DB.forward", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.squeeze", "torch.matmul.squeeze", "type", "architecturesTime.LocalGNN_DB.order.index", "numpy.array", "numpy.tile", "type", "numpy.array", "type", "type", "type", "architecturesTime.LocalGNN_DB.order.index", "type", "numpy.array", "numpy.array.astype", "numpy.arange", "numpy.where", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.forward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "singleNodeForward", "(", "self", ",", "x", ",", "S", ",", "nodes", ")", ":", "\n", "\n", "# x is of shape B x T x F[0] x N", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "N", "=", "x", ".", "shape", "[", "3", "]", "\n", "\n", "# nodes is either an int, or a list/np.array of ints of size B", "\n", "assert", "type", "(", "nodes", ")", "is", "int", "or", "type", "(", "nodes", ")", "is", "list", "or", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", "\n", "\n", "# Let us start by building the selection matrix", "\n", "# This selection matrix has to be a matrix of shape", "\n", "#   B x 1 x N[-1] x 1", "\n", "# so that when multiplying with the output of the forward, we get a", "\n", "#   B x T x dimRedout[-1] x 1", "\n", "# and we just squeeze the last dimension", "\n", "\n", "# TODO: The big question here is if multiplying by a matrix is faster", "\n", "# than doing torch.index_select", "\n", "\n", "# Let's always work with numpy arrays to make it easier.", "\n", "if", "type", "(", "nodes", ")", "is", "int", ":", "\n", "# Change the node number to accommodate the new order", "\n", "            ", "nodes", "=", "self", ".", "order", ".", "index", "(", "nodes", ")", "\n", "# If it's int, make it a list and an array", "\n", "nodes", "=", "np", ".", "array", "(", "[", "nodes", "]", ",", "dtype", "=", "np", ".", "int", ")", "\n", "# And repeat for the number of batches", "\n", "nodes", "=", "np", ".", "tile", "(", "nodes", ",", "batchSize", ")", "\n", "", "if", "type", "(", "nodes", ")", "is", "list", ":", "\n", "            ", "newNodes", "=", "[", "self", ".", "order", ".", "index", "(", "n", ")", "for", "n", "in", "nodes", "]", "\n", "nodes", "=", "np", ".", "array", "(", "newNodes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "", "elif", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", ":", "\n", "            ", "newNodes", "=", "np", ".", "array", "(", "[", "np", ".", "where", "(", "np", ".", "array", "(", "self", ".", "order", ")", "==", "n", ")", "[", "0", "]", "[", "0", "]", "for", "n", "in", "nodes", "]", ")", "\n", "nodes", "=", "newNodes", ".", "astype", "(", "np", ".", "int", ")", "\n", "# Now, nodes is an np.int np.ndarray with shape batchSize", "\n", "\n", "# Build the selection matrix", "\n", "", "selectionMatrix", "=", "np", ".", "zeros", "(", "[", "batchSize", ",", "1", ",", "N", ",", "1", "]", ")", "\n", "selectionMatrix", "[", "np", ".", "arange", "(", "batchSize", ")", ",", "nodes", ",", "0", "]", "=", "1.", "\n", "# And convert it to a tensor", "\n", "selectionMatrix", "=", "torch", ".", "tensor", "(", "selectionMatrix", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Now compute the output", "\n", "y", "=", "self", ".", "forward", "(", "x", ",", "S", ")", "\n", "# This output is of size B x T x dimReadout[-1] x N", "\n", "\n", "# Multiply the output", "\n", "y", "=", "torch", ".", "matmul", "(", "y", ",", "selectionMatrix", ")", "\n", "#   B x T x dimReadout[-1] x 1", "\n", "\n", "# Squeeze the last dimension and return", "\n", "return", "y", ".", "squeeze", "(", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.GraphRecurrentNN_DB.__init__": [[356, 417], ["torch.Module.__init__", "alegnn.HiddenState_DB", "alegnn.GraphFilter_DB", "torch.Sequential", "torch.Sequential", "len", "len", "fc.append", "range", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architecturesTime.GraphRecurrentNN_DB.nonlinearityReadout", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimInputSignals", ",", "\n", "dimOutputSignals", ",", "\n", "dimHiddenSignals", ",", "\n", "nFilterTaps", ",", "bias", ",", "\n", "# Nonlinearities", "\n", "nonlinearityHidden", ",", "\n", "nonlinearityOutput", ",", "\n", "nonlinearityReadout", ",", "# nn.Module", "\n", "# Local MLP in the end", "\n", "dimReadout", ",", "\n", "# Structure", "\n", "dimEdgeFeatures", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# A list of two int, one for the number of filter taps (the computation", "\n", "# of the hidden state has the same number of filter taps)", "\n", "assert", "len", "(", "nFilterTaps", ")", "==", "2", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "F", "=", "dimInputSignals", "# Number of input features", "\n", "self", ".", "G", "=", "dimOutputSignals", "# Number of output features", "\n", "self", ".", "H", "=", "dimHiddenSignals", "# NUmber of hidden features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "E", "=", "dimEdgeFeatures", "# Number of edge features", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "# Store the rest of the variables", "\n", "self", ".", "sigma", "=", "nonlinearityHidden", "\n", "self", ".", "rho", "=", "nonlinearityOutput", "\n", "self", ".", "nonlinearityReadout", "=", "nonlinearityReadout", "\n", "self", ".", "dimReadout", "=", "dimReadout", "\n", "#\\\\\\ Hidden State RNN \\\\\\", "\n", "# Create the layer that generates the hidden state, and generate z0", "\n", "self", ".", "hiddenState", "=", "gml", ".", "HiddenState_DB", "(", "self", ".", "F", ",", "self", ".", "H", ",", "self", ".", "K", "[", "0", "]", ",", "\n", "nonlinearity", "=", "self", ".", "sigma", ",", "E", "=", "self", ".", "E", ",", "\n", "bias", "=", "self", ".", "bias", ")", "\n", "#\\\\\\ Output Graph Filters \\\\\\", "\n", "self", ".", "outputState", "=", "gml", ".", "GraphFilter_DB", "(", "self", ".", "H", ",", "self", ".", "G", ",", "self", ".", "K", "[", "1", "]", ",", "\n", "E", "=", "self", ".", "E", ",", "bias", "=", "self", ".", "bias", ")", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimReadout", ")", ">", "0", ":", "# Maybe we don't want to readout anything", "\n", "# The first layer has to connect whatever was left of the graph ", "\n", "# filtering stage to create the number of features required by", "\n", "# the readout layer", "\n", "            ", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "G", ",", "dimReadout", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimReadout", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "nonlinearityReadout", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimReadout", "[", "l", "]", ",", "dimReadout", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "Readout", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.GraphRecurrentNN_DB.splitForward": [[419, 459], ["torch.randn", "torch.randn", "torch.randn", "torch.randn", "architecturesTime.GraphRecurrentNN_DB.hiddenState.addGSO", "architecturesTime.GraphRecurrentNN_DB.outputState.addGSO", "architecturesTime.GraphRecurrentNN_DB.hiddenState", "architecturesTime.GraphRecurrentNN_DB.outputState", "architecturesTime.GraphRecurrentNN_DB.rho", "architecturesTime.GraphRecurrentNN_DB.permute", "architecturesTime.GraphRecurrentNN_DB.Readout", "len", "S.unsqueeze.unsqueeze.unsqueeze", "len", "architecturesTime.GraphRecurrentNN_DB.permute", "len", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "splitForward", "(", "self", ",", "x", ",", "S", ")", ":", "\n", "\n", "# Check the dimensions of the input", "\n", "#   S: B x T (x E) x N x N", "\n", "#   x: B x T x F[0] x N", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "4", "or", "len", "(", "S", ".", "shape", ")", "==", "5", "\n", "if", "len", "(", "S", ".", "shape", ")", "==", "4", ":", "\n", "            ", "S", "=", "S", ".", "unsqueeze", "(", "2", ")", "\n", "", "B", "=", "S", ".", "shape", "[", "0", "]", "\n", "T", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "E", "\n", "N", "=", "S", ".", "shape", "[", "3", "]", "\n", "assert", "S", ".", "shape", "[", "4", "]", "==", "N", "\n", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "assert", "x", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "T", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "\n", "assert", "x", ".", "shape", "[", "3", "]", "==", "N", "\n", "\n", "# This can be generated here or generated outside of here, not clear yet", "\n", "# what's the most coherent option", "\n", "z0", "=", "torch", ".", "randn", "(", "(", "B", ",", "self", ".", "H", ",", "N", ")", ",", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Add the GSO for each graph filter", "\n", "self", ".", "hiddenState", ".", "addGSO", "(", "S", ")", "\n", "self", ".", "outputState", ".", "addGSO", "(", "S", ")", "\n", "\n", "# Compute the trajectory of hidden states", "\n", "z", ",", "_", "=", "self", ".", "hiddenState", "(", "x", ",", "z0", ")", "\n", "# Compute the output trajectory from the hidden states", "\n", "yOut", "=", "self", ".", "outputState", "(", "z", ")", "\n", "yOut", "=", "self", ".", "rho", "(", "yOut", ")", "# Don't forget the nonlinearity!", "\n", "#   B x T x G x N", "\n", "# Change the order, for the readout", "\n", "y", "=", "yOut", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# B x T x N x G", "\n", "# And, feed it into the Readout layer", "\n", "y", "=", "self", ".", "Readout", "(", "y", ")", "# B x T x N x dimReadout[-1]", "\n", "# Reshape and return", "\n", "return", "y", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", ",", "yOut", "\n", "# B x T x dimReadout[-1] x N, B x T x dimFeatures[-1] x N", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.GraphRecurrentNN_DB.forward": [[461, 471], ["architecturesTime.GraphRecurrentNN_DB.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ",", "S", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ",", "S", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.GraphRecurrentNN_DB.singleNodeForward": [[472, 528], ["numpy.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "architecturesTime.GraphRecurrentNN_DB.forward", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.squeeze", "torch.matmul.squeeze", "type", "architecturesTime.GraphRecurrentNN_DB.order.index", "numpy.array", "numpy.tile", "type", "numpy.array", "type", "type", "type", "architecturesTime.GraphRecurrentNN_DB.order.index", "type", "numpy.array", "numpy.array.astype", "numpy.arange", "numpy.where", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.forward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "singleNodeForward", "(", "self", ",", "x", ",", "S", ",", "nodes", ")", ":", "\n", "\n", "# x is of shape B x T x F[0] x N", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "N", "=", "x", ".", "shape", "[", "3", "]", "\n", "\n", "# nodes is either an int, or a list/np.array of ints of size B", "\n", "assert", "type", "(", "nodes", ")", "is", "int", "or", "type", "(", "nodes", ")", "is", "list", "or", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", "\n", "\n", "# Let us start by building the selection matrix", "\n", "# This selection matrix has to be a matrix of shape", "\n", "#   B x 1 x N[-1] x 1", "\n", "# so that when multiplying with the output of the forward, we get a", "\n", "#   B x T x dimRedout[-1] x 1", "\n", "# and we just squeeze the last dimension", "\n", "\n", "# TODO: The big question here is if multiplying by a matrix is faster", "\n", "# than doing torch.index_select", "\n", "\n", "# Let's always work with numpy arrays to make it easier.", "\n", "if", "type", "(", "nodes", ")", "is", "int", ":", "\n", "# Change the node number to accommodate the new order", "\n", "            ", "nodes", "=", "self", ".", "order", ".", "index", "(", "nodes", ")", "\n", "# If it's int, make it a list and an array", "\n", "nodes", "=", "np", ".", "array", "(", "[", "nodes", "]", ",", "dtype", "=", "np", ".", "int", ")", "\n", "# And repeat for the number of batches", "\n", "nodes", "=", "np", ".", "tile", "(", "nodes", ",", "batchSize", ")", "\n", "", "if", "type", "(", "nodes", ")", "is", "list", ":", "\n", "            ", "newNodes", "=", "[", "self", ".", "order", ".", "index", "(", "n", ")", "for", "n", "in", "nodes", "]", "\n", "nodes", "=", "np", ".", "array", "(", "newNodes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "", "elif", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", ":", "\n", "            ", "newNodes", "=", "np", ".", "array", "(", "[", "np", ".", "where", "(", "np", ".", "array", "(", "self", ".", "order", ")", "==", "n", ")", "[", "0", "]", "[", "0", "]", "for", "n", "in", "nodes", "]", ")", "\n", "nodes", "=", "newNodes", ".", "astype", "(", "np", ".", "int", ")", "\n", "# Now, nodes is an np.int np.ndarray with shape batchSize", "\n", "\n", "# Build the selection matrix", "\n", "", "selectionMatrix", "=", "np", ".", "zeros", "(", "[", "batchSize", ",", "1", ",", "N", ",", "1", "]", ")", "\n", "selectionMatrix", "[", "np", ".", "arange", "(", "batchSize", ")", ",", "nodes", ",", "0", "]", "=", "1.", "\n", "# And convert it to a tensor", "\n", "selectionMatrix", "=", "torch", ".", "tensor", "(", "selectionMatrix", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Now compute the output", "\n", "y", "=", "self", ".", "forward", "(", "x", ",", "S", ")", "\n", "# This output is of size B x T x dimReadout[-1] x N", "\n", "\n", "# Multiply the output", "\n", "y", "=", "torch", ".", "matmul", "(", "y", ",", "selectionMatrix", ")", "\n", "#   B x T x dimReadout[-1] x 1", "\n", "\n", "# Squeeze the last dimension and return", "\n", "return", "y", ".", "squeeze", "(", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.AggregationGNN_DB.__init__": [[582, 670], ["torch.Module.__init__", "len", "range", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "convl.append", "convl.append", "convl.append", "len", "fc.append", "range", "len", "int", "torch.Conv1d", "torch.Conv1d", "architecturesTime.AggregationGNN_DB.sigma", "architecturesTime.AggregationGNN_DB.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architecturesTime.AggregationGNN_DB.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimFeatures", ",", "nFilterTaps", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimReadout", ",", "\n", "# Structure", "\n", "dimEdgeFeatures", ",", "nExchanges", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimFeatures", ")", "==", "len", "(", "nFilterTaps", ")", "+", "1", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nFilterTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "L", "=", "len", "(", "nFilterTaps", ")", "# Number of convolutional layers", "\n", "self", ".", "F", "=", "dimFeatures", "# Features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "E", "=", "dimEdgeFeatures", "# Dimension of edge features", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "# This acts as both the kernel_size and the", "\n", "# stride, so there is no overlap on the elements over which we take", "\n", "# the maximum (this is how it works as default)", "\n", "self", ".", "dimReadout", "=", "dimReadout", "\n", "self", ".", "nExchanges", "=", "nExchanges", "# Number of exchanges", "\n", "# Let's also record the number of nodes on each layer (L+1, actually)", "\n", "self", ".", "N", "=", "[", "self", ".", "nExchanges", "+", "1", "]", "# If we have one exchange, then we have", "\n", "#   two entries in the collected vector (the zeroth-exchange the", "\n", "#   first exchange)", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "# In pyTorch, the convolution is a valid correlation, instead of a", "\n", "# full one, which means that the output is smaller than the input.", "\n", "# Precisely, this smaller (check documentation for nn.conv1d)", "\n", "            ", "outConvN", "=", "self", ".", "N", "[", "l", "]", "-", "(", "self", ".", "K", "[", "l", "]", "-", "1", ")", "# Size of the conv output", "\n", "# The next equation to compute the number of nodes is obtained from", "\n", "# the maxPool1d help in the pytorch documentation", "\n", "self", ".", "N", "+=", "[", "int", "(", "\n", "(", "outConvN", "-", "(", "self", ".", "alpha", "[", "l", "]", "-", "1", ")", "-", "1", ")", "/", "self", ".", "alpha", "[", "l", "]", "+", "1", "\n", ")", "]", "\n", "# int() on a float always applies floor()", "\n", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "", "convl", "=", "[", "]", "# Convolutional Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "convl", ".", "append", "(", "nn", ".", "Conv1d", "(", "self", ".", "F", "[", "l", "]", "*", "self", ".", "E", ",", "\n", "self", ".", "F", "[", "l", "+", "1", "]", "*", "self", ".", "E", ",", "\n", "self", ".", "K", "[", "l", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "#\\\\ Nonlinearity", "\n", "convl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "convl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "ConvLayers", "=", "nn", ".", "Sequential", "(", "*", "convl", ")", "# Convolutional layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimReadout", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "            ", "dimInputReadout", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "E", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputReadout", ",", "dimReadout", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimReadout", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimReadout", "[", "l", "]", ",", "dimReadout", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done within each node", "\n", "", "", "self", ".", "Readout", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.AggregationGNN_DB.forward": [[671, 776], ["torch.matmul.reshape().repeat", "torch.matmul.reshape().repeat", "torch.matmul.reshape", "torch.matmul.reshape", "range", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cat.permute", "torch.cat.permute", "torch.cat.reshape", "torch.cat.reshape", "torch.cat.permute", "torch.cat.permute", "architecturesTime.AggregationGNN_DB.ConvLayers", "y.permute.permute.reshape", "architecturesTime.AggregationGNN_DB.Readout", "y.permute.permute.permute", "y.permute.permute.reshape", "y.permute.permute.permute", "len", "S.unsqueeze.unsqueeze.unsqueeze", "len", "torch.split", "torch.split", "torch.split", "torch.split", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.reshape", "torch.matmul.reshape", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "len", "torch.matmul.reshape", "torch.matmul.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "S", ")", ":", "\n", "\n", "# Check the dimensions of the input first", "\n", "#   S: B x T (x E) x N x N", "\n", "#   x: B x T x F[0] x N", "\n", "        ", "assert", "len", "(", "S", ".", "shape", ")", "==", "4", "or", "len", "(", "S", ".", "shape", ")", "==", "5", "\n", "if", "len", "(", "S", ".", "shape", ")", "==", "4", ":", "\n", "# Then S is B x T x N x N", "\n", "            ", "S", "=", "S", ".", "unsqueeze", "(", "2", ")", "# And we want it B x T x 1 x N x N", "\n", "", "B", "=", "S", ".", "shape", "[", "0", "]", "\n", "T", "=", "S", ".", "shape", "[", "1", "]", "\n", "assert", "S", ".", "shape", "[", "2", "]", "==", "self", ".", "E", "\n", "N", "=", "S", ".", "shape", "[", "3", "]", "\n", "assert", "S", ".", "shape", "[", "4", "]", "==", "N", "\n", "#   Check the dimensions of x", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "assert", "x", ".", "shape", "[", "0", "]", "==", "B", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "T", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "3", "]", "==", "N", "\n", "\n", "# Now we need to do the exchange to build the aggregation vector at", "\n", "# every node", "\n", "# z has to be of shape: B x T x F[0] x (nExchanges+1) x N", "\n", "# to be fed into conv1d it has to be (B*T*N) x F[0] x (nExchanges+1)", "\n", "\n", "# This vector is built by multiplying x with S, so we need to adapt x", "\n", "# to have a dimension that can be multiplied by S (we need to add the", "\n", "# E dimension)", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", ",", "T", ",", "1", ",", "self", ".", "F", "[", "0", "]", ",", "N", "]", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "E", ",", "1", ",", "1", ")", "\n", "\n", "# The first element of z is, precisely, this element (no exchanges)", "\n", "z", "=", "x", ".", "reshape", "(", "[", "B", ",", "T", ",", "1", ",", "self", ".", "E", ",", "self", ".", "F", "[", "0", "]", ",", "N", "]", ")", "# The new dimension is", "\n", "#   the one that accumulates the nExchanges", "\n", "\n", "# Now we start with the exchanges (multiplying by S)", "\n", "for", "k", "in", "range", "(", "1", ",", "self", ".", "nExchanges", "+", "1", ")", ":", "\n", "# Across dim = 1 (time) we need to \"displace the dimension down\", ", "\n", "# i.e. where it used to be t = 1 we now need it to be t=0 and so", "\n", "# on. For t=0 we add a \"row\" of zeros.", "\n", "            ", "x", ",", "_", "=", "torch", ".", "split", "(", "x", ",", "[", "T", "-", "1", ",", "1", "]", ",", "dim", "=", "1", ")", "\n", "#   The second part is the most recent time instant which we do ", "\n", "#   not need anymore (it's used only once for the first value of K)", "\n", "# Now, we need to add a \"row\" of zeros at the beginning (for t = 0)", "\n", "zeroRow", "=", "torch", ".", "zeros", "(", "B", ",", "1", ",", "self", ".", "E", ",", "self", ".", "F", "[", "0", "]", ",", "N", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "zeroRow", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "# And now we multiply with S", "\n", "x", "=", "torch", ".", "matmul", "(", "x", ",", "S", ")", "\n", "# Add the dimension along K", "\n", "xS", "=", "x", ".", "reshape", "(", "B", ",", "T", ",", "1", ",", "self", ".", "E", ",", "self", ".", "F", "[", "0", "]", ",", "N", ")", "\n", "# And concatenate it with z", "\n", "z", "=", "torch", ".", "cat", "(", "(", "z", ",", "xS", ")", ",", "dim", "=", "2", ")", "\n", "\n", "# Now, we have finally built the vector of delayed aggregations. This", "\n", "# vector has shape B x T x (nExchanges+1) x E x F[0] x N", "\n", "# To get rid of the edge features (dim E) we just sum through that", "\n", "# dimension", "\n", "", "z", "=", "torch", ".", "sum", "(", "z", ",", "dim", "=", "3", ")", "# B x T x (nExchanges+1) x F[0] x N", "\n", "# It is, essentially, a matrix of N x (nExchanges+1) for each feature,", "\n", "# for each time instant, for each batch.", "\n", "# NOTE1: This is inconsequential if self.E = 1 (most of the cases)", "\n", "# NOTE2: Alternatively, not to lose information, we could contatenate", "\n", "# dim E after dim F[0] to get E*F[0] features; this increases the", "\n", "# dimensionsonality of the data (which could be fine) but need to be", "\n", "# adapted so that the first input in the conv1d takes self.E*self.F[0]", "\n", "# features instead of just self.F[0]", "\n", "\n", "# The operation conv1d takes tensors of shape ", "\n", "#   batchSize x nFeatures x nEntries", "\n", "# This means that the convolution takes place along nEntries with", "\n", "# a summation along nFeatures, for each of the elements along", "\n", "# batchSize. So we need to put (nExchanges+1) last since it is along", "\n", "# those elements that we want the convolution to be performed, and", "\n", "# we need to put F[0] as nFeatures since there is where we want the", "\n", "# features to be combined. The other three dimensions are different", "\n", "# elements (agents, time, batch) to which the convolution needs to be", "\n", "# applied.", "\n", "# Therefore, we want a vector z of shape", "\n", "#   (B*T*N) x F[0] x (nExchanges+1)", "\n", "\n", "# Let's get started with this reorganization", "\n", "#   First, we join B*T*N. Because we always join the last dimensions,", "\n", "#   we need to permute first to put B, T, N as the last dimensions.", "\n", "#   z: B x T x (nExchanges+1) x F[0] x N", "\n", "z", "=", "z", ".", "permute", "(", "3", ",", "2", ",", "0", ",", "1", ",", "4", ")", "# F[0] x (nExchanges+1) x B x T x N", "\n", "z", "=", "z", ".", "reshape", "(", "[", "self", ".", "F", "[", "0", "]", ",", "self", ".", "nExchanges", "+", "1", ",", "B", "*", "T", "*", "N", "]", ")", "\n", "#   F[0] x (nExchanges+1) x B*T*N", "\n", "#   Second, we put it back at the beginning", "\n", "z", "=", "z", ".", "permute", "(", "2", ",", "0", ",", "1", ")", "# B*T*N x F[0] x (nExchanges+1)", "\n", "\n", "# Let's call the convolutional layers", "\n", "y", "=", "self", ".", "ConvLayers", "(", "z", ")", "\n", "#   B*T*N x F[-1] x N[-1]", "\n", "# Flatten the output", "\n", "y", "=", "y", ".", "reshape", "(", "[", "B", "*", "T", "*", "N", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", "]", ")", "\n", "# And, feed it into the per node readout layers", "\n", "y", "=", "self", ".", "Readout", "(", "y", ")", "# (B*T*N) x dimReadout[-1]", "\n", "# And now we have to unpack it back for every node, i.e. to get it", "\n", "# back to shape B x T x N x dimReadout[-1]", "\n", "y", "=", "y", ".", "permute", "(", "1", ",", "0", ")", "# dimReadout[-1] x (B*T*N)", "\n", "y", "=", "y", ".", "reshape", "(", "self", ".", "dimReadout", "[", "-", "1", "]", ",", "B", ",", "T", ",", "N", ")", "\n", "# And finally put it back to the usual B x T x F x N", "\n", "y", "=", "y", ".", "permute", "(", "1", ",", "2", ",", "0", ",", "3", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architecturesTime.AggregationGNN_DB.to": [[777, 783], ["super().to"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.__init__": [[51, 95], ["model.Model.archit.to", "list", "model.Model.archit.parameters", "len", "range", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["def", "__init__", "(", "self", ",", "\n", "# Architecture (nn.Module)", "\n", "architecture", ",", "\n", "# Loss Function (nn.modules.loss._Loss)", "\n", "loss", ",", "\n", "# Optimization Algorithm (nn.optim)", "\n", "optimizer", ",", "\n", "# Training Algorithm (Modules.training)", "\n", "trainer", ",", "\n", "# Evaluating Algorithm (Modules.evaluation)", "\n", "evaluator", ",", "\n", "# Other", "\n", "device", ",", "name", ",", "saveDir", ")", ":", "\n", "\n", "#\\\\\\ ARCHITECTURE", "\n", "# Store", "\n", "        ", "self", ".", "archit", "=", "architecture", "\n", "# Move it to device", "\n", "self", ".", "archit", ".", "to", "(", "device", ")", "\n", "# Count parameters (doesn't work for EdgeVarying)", "\n", "self", ".", "nParameters", "=", "0", "\n", "for", "param", "in", "list", "(", "self", ".", "archit", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "if", "len", "(", "param", ".", "shape", ")", ">", "0", ":", "\n", "                ", "thisNParam", "=", "1", "\n", "for", "p", "in", "range", "(", "len", "(", "param", ".", "shape", ")", ")", ":", "\n", "                    ", "thisNParam", "*=", "param", ".", "shape", "[", "p", "]", "\n", "", "self", ".", "nParameters", "+=", "thisNParam", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "#\\\\\\ LOSS FUNCTION", "\n", "", "", "self", ".", "loss", "=", "loss", "\n", "#\\\\\\ OPTIMIZATION ALGORITHM", "\n", "self", ".", "optim", "=", "optimizer", "\n", "#\\\\\\ TRAINING ALGORITHM", "\n", "self", ".", "trainer", "=", "trainer", "\n", "#\\\\\\ EVALUATING ALGORITHM", "\n", "self", ".", "evaluator", "=", "evaluator", "\n", "#\\\\\\ OTHER", "\n", "# Device", "\n", "self", ".", "device", "=", "device", "\n", "# Model name", "\n", "self", ".", "name", "=", "name", "\n", "# Saving directory", "\n", "self", ".", "saveDir", "=", "saveDir", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.train": [[96, 101], ["model.Model.trainer", "model.Model.trainer.train"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerFlocking.train"], ["", "def", "train", "(", "self", ",", "data", ",", "nEpochs", ",", "batchSize", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "self", ".", "trainer", "=", "self", ".", "trainer", "(", "self", ",", "data", ",", "nEpochs", ",", "batchSize", ",", "**", "kwargs", ")", "\n", "\n", "return", "self", ".", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.evaluate": [[102, 105], ["model.Model.evaluator"], "methods", ["None"], ["", "def", "evaluate", "(", "self", ",", "data", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "return", "self", ".", "evaluator", "(", "self", ",", "data", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save": [[106, 118], ["os.path.join", "os.path.join", "torch.save", "torch.save", "kwargs.keys", "os.path.exists", "os.makedirs", "model.Model.archit.state_dict", "model.Model.optim.state_dict"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save"], ["", "def", "save", "(", "self", ",", "label", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "'saveDir'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "saveDir", "=", "kwargs", "[", "'saveDir'", "]", "\n", "", "else", ":", "\n", "            ", "saveDir", "=", "self", ".", "saveDir", "\n", "", "saveModelDir", "=", "os", ".", "path", ".", "join", "(", "saveDir", ",", "'savedModels'", ")", "\n", "# Create directory savedModels if it doesn't exist yet:", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "saveModelDir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "saveModelDir", ")", "\n", "", "saveFile", "=", "os", ".", "path", ".", "join", "(", "saveModelDir", ",", "self", ".", "name", ")", "\n", "torch", ".", "save", "(", "self", ".", "archit", ".", "state_dict", "(", ")", ",", "saveFile", "+", "'Archit'", "+", "label", "+", "'.ckpt'", ")", "\n", "torch", ".", "save", "(", "self", ".", "optim", ".", "state_dict", "(", ")", ",", "saveFile", "+", "'Optim'", "+", "label", "+", "'.ckpt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load": [[119, 130], ["model.Model.archit.load_state_dict", "model.Model.optim.load_state_dict", "kwargs.keys", "os.path.join", "os.path.join", "os.path.join", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load"], ["", "def", "load", "(", "self", ",", "label", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "'loadFiles'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "(", "architLoadFile", ",", "optimLoadFile", ")", "=", "kwargs", "[", "'loadFiles'", "]", "\n", "", "else", ":", "\n", "            ", "saveModelDir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "saveDir", ",", "'savedModels'", ")", "\n", "architLoadFile", "=", "os", ".", "path", ".", "join", "(", "saveModelDir", ",", "\n", "self", ".", "name", "+", "'Archit'", "+", "label", "+", "'.ckpt'", ")", "\n", "optimLoadFile", "=", "os", ".", "path", ".", "join", "(", "saveModelDir", ",", "\n", "self", ".", "name", "+", "'Optim'", "+", "label", "+", "'.ckpt'", ")", "\n", "", "self", ".", "archit", ".", "load_state_dict", "(", "torch", ".", "load", "(", "architLoadFile", ")", ")", "\n", "self", ".", "optim", ".", "load_state_dict", "(", "torch", ".", "load", "(", "optimLoadFile", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.getTrainingOptions": [[131, 136], ["dir"], "methods", ["None"], ["", "def", "getTrainingOptions", "(", "self", ")", ":", "\n", "\n", "        ", "return", "self", ".", "trainer", ".", "trainingOptions", "if", "'trainingOptions'", "in", "dir", "(", "self", ".", "trainer", ")", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.__repr__": [[137, 164], ["repr", "repr", "repr", "repr", "repr"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "reprString", "=", "\"Name: %s\\n\"", "%", "(", "self", ".", "name", ")", "\n", "reprString", "+=", "\"Number of learnable parameters: %d\\n\"", "%", "(", "self", ".", "nParameters", ")", "\n", "reprString", "+=", "\"\\n\"", "\n", "reprString", "+=", "\"Model architecture:\\n\"", "\n", "reprString", "+=", "\"----- -------------\\n\"", "\n", "reprString", "+=", "\"\\n\"", "\n", "reprString", "+=", "repr", "(", "self", ".", "archit", ")", "+", "\"\\n\"", "\n", "reprString", "+=", "\"\\n\"", "\n", "reprString", "+=", "\"Loss function:\\n\"", "\n", "reprString", "+=", "\"---- ---------\\n\"", "\n", "reprString", "+=", "\"\\n\"", "\n", "reprString", "+=", "repr", "(", "self", ".", "loss", ")", "+", "\"\\n\"", "\n", "reprString", "+=", "\"\\n\"", "\n", "reprString", "+=", "\"Optimizer:\\n\"", "\n", "reprString", "+=", "\"----------\\n\"", "\n", "reprString", "+=", "\"\\n\"", "\n", "reprString", "+=", "repr", "(", "self", ".", "optim", ")", "+", "\"\\n\"", "\n", "reprString", "+=", "\"Training algorithm:\\n\"", "\n", "reprString", "+=", "\"-------- ----------\\n\"", "\n", "reprString", "+=", "\"\\n\"", "\n", "reprString", "+=", "repr", "(", "self", ".", "trainer", ")", "+", "\"\\n\"", "\n", "reprString", "+=", "\"Evaluation algorithm:\\n\"", "\n", "reprString", "+=", "\"---------- ----------\\n\"", "\n", "reprString", "+=", "\"\\n\"", "\n", "reprString", "+=", "repr", "(", "self", ".", "evaluator", ")", "+", "\"\\n\"", "\n", "return", "reprString", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.Trainer.__init__": [[84, 227], ["numpy.cumsum().tolist", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "os.path.join", "Visualizer", "kwargs.keys", "kwargs.keys", "kwargs.keys", "numpy.ceil().astype", "numpy.int", "numpy.cumsum", "sum", "numpy.ceil"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["def", "__init__", "(", "self", ",", "model", ",", "data", ",", "nEpochs", ",", "batchSize", ",", "**", "kwargs", ")", ":", "\n", "\n", "#\\\\\\ Store model", "\n", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "data", "=", "data", "\n", "\n", "####################################", "\n", "# ARGUMENTS (Store chosen options) #", "\n", "####################################", "\n", "\n", "# Training Options:", "\n", "if", "'doLogging'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "doLogging", "=", "kwargs", "[", "'doLogging'", "]", "\n", "", "else", ":", "\n", "            ", "doLogging", "=", "False", "\n", "\n", "", "if", "'doSaveVars'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "doSaveVars", "=", "kwargs", "[", "'doSaveVars'", "]", "\n", "", "else", ":", "\n", "            ", "doSaveVars", "=", "True", "\n", "\n", "", "if", "'printInterval'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "printInterval", "=", "kwargs", "[", "'printInterval'", "]", "\n", "if", "printInterval", ">", "0", ":", "\n", "                ", "doPrint", "=", "True", "\n", "", "else", ":", "\n", "                ", "doPrint", "=", "False", "\n", "", "", "else", ":", "\n", "            ", "doPrint", "=", "True", "\n", "printInterval", "=", "(", "data", ".", "nTrain", "//", "batchSize", ")", "//", "5", "\n", "\n", "", "if", "'learningRateDecayRate'", "in", "kwargs", ".", "keys", "(", ")", "and", "'learningRateDecayPeriod'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "doLearningRateDecay", "=", "True", "\n", "learningRateDecayRate", "=", "kwargs", "[", "'learningRateDecayRate'", "]", "\n", "learningRateDecayPeriod", "=", "kwargs", "[", "'learningRateDecayPeriod'", "]", "\n", "", "else", ":", "\n", "            ", "doLearningRateDecay", "=", "False", "\n", "\n", "", "if", "'validationInterval'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "validationInterval", "=", "kwargs", "[", "'validationInterval'", "]", "\n", "", "else", ":", "\n", "            ", "validationInterval", "=", "data", ".", "nTrain", "//", "batchSize", "\n", "\n", "", "if", "'earlyStoppingLag'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "doEarlyStopping", "=", "True", "\n", "earlyStoppingLag", "=", "kwargs", "[", "'earlyStoppingLag'", "]", "\n", "", "else", ":", "\n", "            ", "doEarlyStopping", "=", "False", "\n", "earlyStoppingLag", "=", "0", "\n", "\n", "", "if", "'graphNo'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "graphNo", "=", "kwargs", "[", "'graphNo'", "]", "\n", "", "else", ":", "\n", "            ", "graphNo", "=", "-", "1", "\n", "\n", "", "if", "'realizationNo'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "if", "'graphNo'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "                ", "realizationNo", "=", "kwargs", "[", "'realizationNo'", "]", "\n", "", "else", ":", "\n", "                ", "graphNo", "=", "kwargs", "[", "'realizationNo'", "]", "\n", "realizationNo", "=", "-", "1", "\n", "", "", "else", ":", "\n", "            ", "realizationNo", "=", "-", "1", "\n", "\n", "", "if", "doLogging", ":", "\n", "            ", "from", "alegnn", ".", "utils", ".", "visualTools", "import", "Visualizer", "\n", "logsTB", "=", "os", ".", "path", ".", "join", "(", "self", ".", "saveDir", ",", "self", ".", "name", "+", "'-logsTB'", ")", "\n", "logger", "=", "Visualizer", "(", "logsTB", ",", "name", "=", "'visualResults'", ")", "\n", "", "else", ":", "\n", "            ", "logger", "=", "None", "\n", "\n", "# No training case:", "\n", "", "if", "nEpochs", "==", "0", ":", "\n", "            ", "doSaveVars", "=", "False", "\n", "doLogging", "=", "False", "\n", "# If there's no training happening, there's nothing to report about", "\n", "# training losses and stuff.", "\n", "\n", "###########################################", "\n", "# DATA INPUT (pick up on data parameters) #", "\n", "###########################################", "\n", "\n", "", "nTrain", "=", "data", ".", "nTrain", "# size of the training set", "\n", "\n", "# Number of batches: If the desired number of batches does not split the", "\n", "# dataset evenly, we reduce the size of the last batch (the number of", "\n", "# samples in the last batch).", "\n", "# The variable batchSize is a list of length nBatches (number of", "\n", "# batches), where each element of the list is a number indicating the", "\n", "# size of the corresponding batch.", "\n", "if", "nTrain", "<", "batchSize", ":", "\n", "            ", "nBatches", "=", "1", "\n", "batchSize", "=", "[", "nTrain", "]", "\n", "", "elif", "nTrain", "%", "batchSize", "!=", "0", ":", "\n", "            ", "nBatches", "=", "np", ".", "ceil", "(", "nTrain", "/", "batchSize", ")", ".", "astype", "(", "np", ".", "int64", ")", "\n", "batchSize", "=", "[", "batchSize", "]", "*", "nBatches", "\n", "# If the sum of all batches so far is not the total number of", "\n", "# graphs, start taking away samples from the last batch (remember", "\n", "# that we used ceiling, so we are overshooting with the estimated", "\n", "# number of batches)", "\n", "while", "sum", "(", "batchSize", ")", "!=", "nTrain", ":", "\n", "                ", "batchSize", "[", "-", "1", "]", "-=", "1", "\n", "# If they fit evenly, then just do so.", "\n", "", "", "else", ":", "\n", "            ", "nBatches", "=", "np", ".", "int", "(", "nTrain", "/", "batchSize", ")", "\n", "batchSize", "=", "[", "batchSize", "]", "*", "nBatches", "\n", "# batchIndex is used to determine the first and last element of each", "\n", "# batch.", "\n", "# If batchSize is, for example [20,20,20] meaning that there are three", "\n", "# batches of size 20 each, then cumsum will give [20,40,60] which", "\n", "# determines the last index of each batch: up to 20, from 20 to 40, and", "\n", "# from 40 to 60. We add the 0 at the beginning so that", "\n", "# batchIndex[b]:batchIndex[b+1] gives the right samples for batch b.", "\n", "", "batchIndex", "=", "np", ".", "cumsum", "(", "batchSize", ")", ".", "tolist", "(", ")", "\n", "batchIndex", "=", "[", "0", "]", "+", "batchIndex", "\n", "\n", "###################", "\n", "# SAVE ATTRIBUTES #", "\n", "###################", "\n", "\n", "self", ".", "trainingOptions", "=", "{", "}", "\n", "self", ".", "trainingOptions", "[", "'doLogging'", "]", "=", "doLogging", "\n", "self", ".", "trainingOptions", "[", "'logger'", "]", "=", "logger", "\n", "self", ".", "trainingOptions", "[", "'doSaveVars'", "]", "=", "doSaveVars", "\n", "self", ".", "trainingOptions", "[", "'doPrint'", "]", "=", "doPrint", "\n", "self", ".", "trainingOptions", "[", "'printInterval'", "]", "=", "printInterval", "\n", "self", ".", "trainingOptions", "[", "'doLearningRateDecay'", "]", "=", "doLearningRateDecay", "\n", "if", "doLearningRateDecay", ":", "\n", "            ", "self", ".", "trainingOptions", "[", "'learningRateDecayRate'", "]", "=", "learningRateDecayRate", "\n", "self", ".", "trainingOptions", "[", "'learningRateDecayPeriod'", "]", "=", "learningRateDecayPeriod", "\n", "", "self", ".", "trainingOptions", "[", "'validationInterval'", "]", "=", "validationInterval", "\n", "self", ".", "trainingOptions", "[", "'doEarlyStopping'", "]", "=", "doEarlyStopping", "\n", "self", ".", "trainingOptions", "[", "'earlyStoppingLag'", "]", "=", "earlyStoppingLag", "\n", "self", ".", "trainingOptions", "[", "'batchIndex'", "]", "=", "batchIndex", "\n", "self", ".", "trainingOptions", "[", "'batchSize'", "]", "=", "batchSize", "\n", "self", ".", "trainingOptions", "[", "'nEpochs'", "]", "=", "nEpochs", "\n", "self", ".", "trainingOptions", "[", "'nBatches'", "]", "=", "nBatches", "\n", "self", ".", "trainingOptions", "[", "'graphNo'", "]", "=", "graphNo", "\n", "self", ".", "trainingOptions", "[", "'realizationNo'", "]", "=", "realizationNo", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.Trainer.trainBatch": [[228, 266], ["training.Trainer.data.getSamples", "xTrain.to.to.to", "yTrain.to.to.to", "datetime.datetime.now", "training.Trainer.model.archit.zero_grad", "training.Trainer.model.archit", "training.Trainer.model.loss", "training.Trainer.backward", "training.Trainer.model.optim.step", "datetime.datetime.now", "abs().total_seconds", "training.Trainer.data.evaluate", "training.Trainer.item", "training.Trainer.item", "abs"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.getSamples", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate"], ["", "def", "trainBatch", "(", "self", ",", "thisBatchIndices", ")", ":", "\n", "\n", "# Get the samples", "\n", "        ", "xTrain", ",", "yTrain", "=", "self", ".", "data", ".", "getSamples", "(", "'train'", ",", "thisBatchIndices", ")", "\n", "xTrain", "=", "xTrain", ".", "to", "(", "self", ".", "model", ".", "device", ")", "\n", "yTrain", "=", "yTrain", ".", "to", "(", "self", ".", "model", ".", "device", ")", "\n", "\n", "# Start measuring time", "\n", "startTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "# Reset gradients", "\n", "self", ".", "model", ".", "archit", ".", "zero_grad", "(", ")", "\n", "\n", "# Obtain the output of the GNN", "\n", "yHatTrain", "=", "self", ".", "model", ".", "archit", "(", "xTrain", ")", "\n", "\n", "# Compute loss", "\n", "lossValueTrain", "=", "self", ".", "model", ".", "loss", "(", "yHatTrain", ",", "yTrain", ")", "\n", "\n", "# Compute gradients", "\n", "lossValueTrain", ".", "backward", "(", ")", "\n", "\n", "# Optimize", "\n", "self", ".", "model", ".", "optim", ".", "step", "(", ")", "\n", "\n", "# Finish measuring time", "\n", "endTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "timeElapsed", "=", "abs", "(", "endTime", "-", "startTime", ")", ".", "total_seconds", "(", ")", "\n", "\n", "# Compute the accuracy", "\n", "#   Note: Using yHatTrain.data creates a new tensor with the", "\n", "#   same value, but detaches it from the gradient, so that no", "\n", "#   gradient operation is taken into account here.", "\n", "#   (Alternatively, we could use a with torch.no_grad():)", "\n", "costTrain", "=", "self", ".", "data", ".", "evaluate", "(", "yHatTrain", ".", "data", ",", "yTrain", ")", "\n", "\n", "return", "lossValueTrain", ".", "item", "(", ")", ",", "costTrain", ".", "item", "(", ")", ",", "timeElapsed", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.Trainer.validationStep": [[267, 296], ["training.Trainer.data.getSamples", "xValid.to.to.to", "yValid.to.to.to", "datetime.datetime.now", "torch.no_grad", "training.Trainer.model.archit", "training.Trainer.model.loss", "datetime.datetime.now", "abs().total_seconds", "training.Trainer.data.evaluate", "training.Trainer.item", "training.Trainer.item", "abs"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.getSamples", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate"], ["", "def", "validationStep", "(", "self", ")", ":", "\n", "\n", "# Validation:", "\n", "        ", "xValid", ",", "yValid", "=", "self", ".", "data", ".", "getSamples", "(", "'valid'", ")", "\n", "xValid", "=", "xValid", ".", "to", "(", "self", ".", "model", ".", "device", ")", "\n", "yValid", "=", "yValid", ".", "to", "(", "self", ".", "model", ".", "device", ")", "\n", "\n", "# Start measuring time", "\n", "startTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "# Under torch.no_grad() so that the computations carried out", "\n", "# to obtain the validation accuracy are not taken into", "\n", "# account to update the learnable parameters.", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Obtain the output of the GNN", "\n", "            ", "yHatValid", "=", "self", ".", "model", ".", "archit", "(", "xValid", ")", "\n", "\n", "# Compute loss", "\n", "lossValueValid", "=", "self", ".", "model", ".", "loss", "(", "yHatValid", ",", "yValid", ")", "\n", "\n", "# Finish measuring time", "\n", "endTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "timeElapsed", "=", "abs", "(", "endTime", "-", "startTime", ")", ".", "total_seconds", "(", ")", "\n", "\n", "# Compute accuracy:", "\n", "costValid", "=", "self", ".", "data", ".", "evaluate", "(", "yHatValid", ",", "yValid", ")", "\n", "\n", "", "return", "lossValueValid", ".", "item", "(", ")", ",", "costValid", ".", "item", "(", ")", ",", "timeElapsed", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.Trainer.train": [[297, 579], ["training.Trainer.model.save", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "training.Trainer.model.load", "dir", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "torch.optim.lr_scheduler.StepLR", "numpy.random.permutation", "numpy.array", "numpy.array", "os.path.join", "os.path.join", "training.Trainer.model.save", "training.Trainer.model.save", "print", "training.Trainer.trainingOptions.keys", "training.Trainer.trainingOptions.keys", "int", "torch.optim.lr_scheduler.StepLR.step", "training.Trainer.trainBatch", "os.path.exists", "os.makedirs", "open", "pickle.dump", "print", "print", "logger.scalar_summary", "training.Trainer.validationStep", "print", "print", "print", "print", "print", "logger.scalar_summary", "training.Trainer.model.save", "print", "print", "print", "training.Trainer.model.save", "print", "print", "print"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerSingleNode.trainBatch", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.scalar_summary", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerSingleNode.validationStep", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.scalar_summary", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save"], ["", "def", "train", "(", "self", ")", ":", "\n", "\n", "# Get back the training options", "\n", "        ", "assert", "'trainingOptions'", "in", "dir", "(", "self", ")", "\n", "assert", "'doLogging'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doLogging", "=", "self", ".", "trainingOptions", "[", "'doLogging'", "]", "\n", "assert", "'logger'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "logger", "=", "self", ".", "trainingOptions", "[", "'logger'", "]", "\n", "assert", "'doSaveVars'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doSaveVars", "=", "self", ".", "trainingOptions", "[", "'doSaveVars'", "]", "\n", "assert", "'doPrint'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doPrint", "=", "self", ".", "trainingOptions", "[", "'doPrint'", "]", "\n", "assert", "'printInterval'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "printInterval", "=", "self", ".", "trainingOptions", "[", "'printInterval'", "]", "\n", "assert", "'doLearningRateDecay'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doLearningRateDecay", "=", "self", ".", "trainingOptions", "[", "'doLearningRateDecay'", "]", "\n", "if", "doLearningRateDecay", ":", "\n", "            ", "assert", "'learningRateDecayRate'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "learningRateDecayRate", "=", "self", ".", "trainingOptions", "[", "'learningRateDecayRate'", "]", "\n", "assert", "'learningRateDecayPeriod'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "learningRateDecayPeriod", "=", "self", ".", "trainingOptions", "[", "'learningRateDecayPeriod'", "]", "\n", "", "assert", "'validationInterval'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "validationInterval", "=", "self", ".", "trainingOptions", "[", "'validationInterval'", "]", "\n", "assert", "'doEarlyStopping'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doEarlyStopping", "=", "self", ".", "trainingOptions", "[", "'doEarlyStopping'", "]", "\n", "assert", "'earlyStoppingLag'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "earlyStoppingLag", "=", "self", ".", "trainingOptions", "[", "'earlyStoppingLag'", "]", "\n", "assert", "'batchIndex'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "batchIndex", "=", "self", ".", "trainingOptions", "[", "'batchIndex'", "]", "\n", "assert", "'batchSize'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "batchSize", "=", "self", ".", "trainingOptions", "[", "'batchSize'", "]", "\n", "assert", "'nEpochs'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "nEpochs", "=", "self", ".", "trainingOptions", "[", "'nEpochs'", "]", "\n", "assert", "'nBatches'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "nBatches", "=", "self", ".", "trainingOptions", "[", "'nBatches'", "]", "\n", "assert", "'graphNo'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "graphNo", "=", "self", ".", "trainingOptions", "[", "'graphNo'", "]", "\n", "assert", "'realizationNo'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "realizationNo", "=", "self", ".", "trainingOptions", "[", "'realizationNo'", "]", "\n", "\n", "# Learning rate scheduler:", "\n", "if", "doLearningRateDecay", ":", "\n", "            ", "learningRateScheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "\n", "self", ".", "model", ".", "optim", ",", "learningRateDecayPeriod", ",", "learningRateDecayRate", ")", "\n", "\n", "# Initialize counters (since we give the possibility of early stopping,", "\n", "# we had to drop the 'for' and use a 'while' instead):", "\n", "", "epoch", "=", "0", "# epoch counter", "\n", "lagCount", "=", "0", "# lag counter for early stopping", "\n", "\n", "# Store the training variables", "\n", "lossTrain", "=", "[", "]", "\n", "costTrain", "=", "[", "]", "\n", "lossValid", "=", "[", "]", "\n", "costValid", "=", "[", "]", "\n", "timeTrain", "=", "[", "]", "\n", "timeValid", "=", "[", "]", "\n", "\n", "while", "epoch", "<", "nEpochs", "and", "(", "lagCount", "<", "earlyStoppingLag", "or", "(", "not", "doEarlyStopping", ")", ")", ":", "\n", "# The condition will be zero (stop), whenever one of the items of", "\n", "# the 'and' is zero. Therefore, we want this to stop only for epoch", "\n", "# counting when we are NOT doing early stopping. This can be", "\n", "# achieved if the second element of the 'and' is always 1 (so that", "\n", "# the first element, the epoch counting, decides). In order to", "\n", "# force the second element to be one whenever there is not early", "\n", "# stopping, we have an or, and force it to one. So, when we are not", "\n", "# doing early stopping, the variable 'not doEarlyStopping' is 1,", "\n", "# and the result of the 'or' is 1 regardless of the lagCount. When", "\n", "# we do early stopping, then the variable 'not doEarlyStopping' is", "\n", "# 0, and the value 1 for the 'or' gate is determined by the lag", "\n", "# count.", "\n", "# ALTERNATIVELY, we could just keep 'and lagCount<earlyStoppingLag'", "\n", "# and be sure that lagCount can only be increased whenever", "\n", "# doEarlyStopping is True. But I somehow figured out that would be", "\n", "# harder to maintain (more parts of the code to check if we are", "\n", "# accidentally increasing lagCount).", "\n", "\n", "# Randomize dataset for each epoch", "\n", "            ", "randomPermutation", "=", "np", ".", "random", ".", "permutation", "(", "self", ".", "data", ".", "nTrain", ")", "\n", "# Convert a numpy.array of numpy.int into a list of actual int.", "\n", "idxEpoch", "=", "[", "int", "(", "i", ")", "for", "i", "in", "randomPermutation", "]", "\n", "\n", "# Learning decay", "\n", "if", "doLearningRateDecay", ":", "\n", "                ", "learningRateScheduler", ".", "step", "(", ")", "\n", "\n", "if", "doPrint", ":", "\n", "# All the optimization have the same learning rate, so just", "\n", "# print one of them", "\n", "# TODO: Actually, they might be different, so I will need to", "\n", "# print all of them.", "\n", "                    ", "print", "(", "\"Epoch %d, learning rate = %.8f\"", "%", "(", "epoch", "+", "1", ",", "\n", "learningRateScheduler", ".", "optim", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "\n", "# Initialize counter", "\n", "", "", "batch", "=", "0", "# batch counter", "\n", "while", "batch", "<", "nBatches", "and", "(", "lagCount", "<", "earlyStoppingLag", "or", "(", "not", "doEarlyStopping", ")", ")", ":", "\n", "\n", "# Extract the adequate batch", "\n", "                ", "thisBatchIndices", "=", "idxEpoch", "[", "batchIndex", "[", "batch", "]", "\n", ":", "batchIndex", "[", "batch", "+", "1", "]", "]", "\n", "\n", "lossValueTrain", ",", "costValueTrain", ",", "timeElapsed", "=", "self", ".", "trainBatch", "(", "thisBatchIndices", ")", "\n", "\n", "\n", "# Logging values", "\n", "if", "doLogging", ":", "\n", "                    ", "lossTrainTB", "=", "lossValueTrain", "\n", "costTrainTB", "=", "costValueTrain", "\n", "# Save values", "\n", "", "lossTrain", "+=", "[", "lossValueTrain", "]", "\n", "costTrain", "+=", "[", "costValueTrain", "]", "\n", "timeTrain", "+=", "[", "timeElapsed", "]", "\n", "\n", "# Print:", "\n", "if", "doPrint", ":", "\n", "                    ", "if", "(", "epoch", "*", "nBatches", "+", "batch", ")", "%", "printInterval", "==", "0", ":", "\n", "                        ", "print", "(", "\"\\t(E: %2d, B: %3d) %6.4f / %7.4f - %6.4fs\"", "%", "(", "\n", "epoch", "+", "1", ",", "batch", "+", "1", ",", "costValueTrain", ",", "\n", "lossValueTrain", ",", "timeElapsed", ")", ",", "\n", "end", "=", "' '", ")", "\n", "if", "graphNo", ">", "-", "1", ":", "\n", "                            ", "print", "(", "\"[%d\"", "%", "graphNo", ",", "end", "=", "''", ")", "\n", "if", "realizationNo", ">", "-", "1", ":", "\n", "                                ", "print", "(", "\"/%d\"", "%", "realizationNo", ",", "\n", "end", "=", "''", ")", "\n", "", "print", "(", "\"]\"", ",", "end", "=", "''", ")", "\n", "", "print", "(", "\"\"", ")", "\n", "\n", "#\\\\\\\\\\\\\\", "\n", "#\\\\\\ TB LOGGING (for each batch)", "\n", "#\\\\\\\\\\\\\\", "\n", "\n", "", "", "if", "doLogging", ":", "\n", "                    ", "logger", ".", "scalar_summary", "(", "mode", "=", "'Training'", ",", "\n", "epoch", "=", "epoch", "*", "nBatches", "+", "batch", ",", "\n", "**", "{", "'lossTrain'", ":", "lossTrainTB", ",", "\n", "'costTrain'", ":", "costTrainTB", "}", ")", "\n", "\n", "#\\\\\\\\\\\\\\", "\n", "#\\\\\\ VALIDATION", "\n", "#\\\\\\\\\\\\\\", "\n", "\n", "", "if", "(", "epoch", "*", "nBatches", "+", "batch", ")", "%", "validationInterval", "==", "0", ":", "\n", "\n", "                    ", "lossValueValid", ",", "costValueValid", ",", "timeElapsed", "=", "self", ".", "validationStep", "(", ")", "\n", "\n", "# Logging values", "\n", "if", "doLogging", ":", "\n", "                        ", "lossValidTB", "=", "lossValueValid", "\n", "costValidTB", "=", "costValueValid", "\n", "# Save values", "\n", "", "lossValid", "+=", "[", "lossValueValid", "]", "\n", "costValid", "+=", "[", "costValueValid", "]", "\n", "timeValid", "+=", "[", "timeElapsed", "]", "\n", "\n", "# Print:", "\n", "if", "doPrint", ":", "\n", "                        ", "print", "(", "\"\\t(E: %2d, B: %3d) %6.4f / %7.4f - %6.4fs\"", "%", "(", "\n", "epoch", "+", "1", ",", "batch", "+", "1", ",", "\n", "costValueValid", ",", "\n", "lossValueValid", ",", "\n", "timeElapsed", ")", ",", "end", "=", "' '", ")", "\n", "print", "(", "\"[VALIDATION\"", ",", "end", "=", "''", ")", "\n", "if", "graphNo", ">", "-", "1", ":", "\n", "                            ", "print", "(", "\".%d\"", "%", "graphNo", ",", "end", "=", "''", ")", "\n", "if", "realizationNo", ">", "-", "1", ":", "\n", "                                ", "print", "(", "\"/%d\"", "%", "realizationNo", ",", "end", "=", "''", ")", "\n", "", "", "print", "(", "\" (%s)]\"", "%", "self", ".", "model", ".", "name", ")", "\n", "\n", "\n", "", "if", "doLogging", ":", "\n", "                        ", "logger", ".", "scalar_summary", "(", "mode", "=", "'Validation'", ",", "\n", "epoch", "=", "epoch", "*", "nBatches", "+", "batch", ",", "\n", "**", "{", "'lossValid'", ":", "lossValidTB", ",", "\n", "'costValid'", ":", "costValidTB", "}", ")", "\n", "\n", "# No previous best option, so let's record the first trial", "\n", "# as the best option", "\n", "", "if", "epoch", "==", "0", "and", "batch", "==", "0", ":", "\n", "                        ", "bestScore", "=", "costValueValid", "\n", "bestEpoch", ",", "bestBatch", "=", "epoch", ",", "batch", "\n", "# Save this model as the best (so far)", "\n", "self", ".", "model", ".", "save", "(", "label", "=", "'Best'", ")", "\n", "# Start the counter", "\n", "if", "doEarlyStopping", ":", "\n", "                            ", "initialBest", "=", "True", "\n", "", "", "else", ":", "\n", "                        ", "thisValidScore", "=", "costValueValid", "\n", "if", "thisValidScore", "<", "bestScore", ":", "\n", "                            ", "bestScore", "=", "thisValidScore", "\n", "bestEpoch", ",", "bestBatch", "=", "epoch", ",", "batch", "\n", "if", "doPrint", ":", "\n", "                                ", "print", "(", "\"\\t=> New best achieved: %.4f\"", "%", "(", "bestScore", ")", ")", "\n", "", "self", ".", "model", ".", "save", "(", "label", "=", "'Best'", ")", "\n", "# Now that we have found a best that is not the", "\n", "# initial one, we can start counting the lag (if", "\n", "# needed)", "\n", "initialBest", "=", "False", "\n", "# If we achieved a new best, then we need to reset", "\n", "# the lag count.", "\n", "if", "doEarlyStopping", ":", "\n", "                                ", "lagCount", "=", "0", "\n", "# If we didn't achieve a new best, increase the lag", "\n", "# count.", "\n", "# Unless it was the initial best, in which case we", "\n", "# haven't found any best yet, so we shouldn't be doing", "\n", "# the early stopping count.", "\n", "", "", "elif", "doEarlyStopping", "and", "not", "initialBest", ":", "\n", "                            ", "lagCount", "+=", "1", "\n", "\n", "#\\\\\\\\\\\\\\", "\n", "#\\\\\\ END OF BATCH:", "\n", "#\\\\\\\\\\\\\\", "\n", "\n", "#\\\\\\ Increase batch count:", "\n", "", "", "", "batch", "+=", "1", "\n", "\n", "#\\\\\\\\\\\\\\", "\n", "#\\\\\\ END OF EPOCH:", "\n", "#\\\\\\\\\\\\\\", "\n", "\n", "#\\\\\\ Increase epoch count:", "\n", "", "epoch", "+=", "1", "\n", "\n", "#\\\\\\ Save models:", "\n", "", "self", ".", "model", ".", "save", "(", "label", "=", "'Last'", ")", "\n", "\n", "#################", "\n", "# TRAINING OVER #", "\n", "#################", "\n", "\n", "# We convert the lists into np.arrays", "\n", "lossTrain", "=", "np", ".", "array", "(", "lossTrain", ")", "\n", "costTrain", "=", "np", ".", "array", "(", "costTrain", ")", "\n", "lossValid", "=", "np", ".", "array", "(", "lossValid", ")", "\n", "costValid", "=", "np", ".", "array", "(", "costValid", ")", "\n", "# And we would like to save all the relevant information from", "\n", "# training", "\n", "trainVars", "=", "{", "'nEpochs'", ":", "nEpochs", ",", "\n", "'nBatches'", ":", "nBatches", ",", "\n", "'validationInterval'", ":", "validationInterval", ",", "\n", "'batchSize'", ":", "np", ".", "array", "(", "batchSize", ")", ",", "\n", "'batchIndex'", ":", "np", ".", "array", "(", "batchIndex", ")", ",", "\n", "'lossTrain'", ":", "lossTrain", ",", "\n", "'costTrain'", ":", "costTrain", ",", "\n", "'lossValid'", ":", "lossValid", ",", "\n", "'costValid'", ":", "costValid", "\n", "}", "\n", "\n", "if", "doSaveVars", ":", "\n", "            ", "saveDirVars", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model", ".", "saveDir", ",", "'trainVars'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "saveDirVars", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "saveDirVars", ")", "\n", "", "pathToFile", "=", "os", ".", "path", ".", "join", "(", "saveDirVars", ",", "\n", "self", ".", "model", ".", "name", "+", "'trainVars.pkl'", ")", "\n", "with", "open", "(", "pathToFile", ",", "'wb'", ")", "as", "trainVarsFile", ":", "\n", "                ", "pickle", ".", "dump", "(", "trainVars", ",", "trainVarsFile", ")", "\n", "\n", "# Now, if we didn't do any training (i.e. nEpochs = 0), then the last is", "\n", "# also the best.", "\n", "", "", "if", "nEpochs", "==", "0", ":", "\n", "            ", "self", ".", "model", ".", "save", "(", "label", "=", "'Best'", ")", "\n", "self", ".", "model", ".", "save", "(", "label", "=", "'Last'", ")", "\n", "if", "doPrint", ":", "\n", "                ", "print", "(", "\"WARNING: No training. Best and Last models are the same.\"", ")", "\n", "\n", "# After training is done, reload best model before proceeding to", "\n", "# evaluation:", "\n", "", "", "self", ".", "model", ".", "load", "(", "label", "=", "'Best'", ")", "\n", "\n", "#\\\\\\ Print out best:", "\n", "if", "doPrint", "and", "nEpochs", ">", "0", ":", "\n", "            ", "print", "(", "\"=> Best validation achieved (E: %d, B: %d): %.4f\"", "%", "(", "\n", "bestEpoch", "+", "1", ",", "bestBatch", "+", "1", ",", "bestScore", ")", ")", "\n", "\n", "", "return", "trainVars", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerSingleNode.__init__": [[637, 644], ["training.Trainer.__init__", "dir", "dir"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "data", ",", "nEpochs", ",", "batchSize", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "assert", "'singleNodeForward'", "in", "dir", "(", "model", ".", "archit", ")", "\n", "assert", "'getLabelID'", "in", "dir", "(", "data", ")", "\n", "\n", "# Initialize supraclass", "\n", "super", "(", ")", ".", "__init__", "(", "model", ",", "data", ",", "nEpochs", ",", "batchSize", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerSingleNode.trainBatch": [[645, 684], ["training.TrainerSingleNode.data.getSamples", "xTrain.to.to.to", "yTrain.to.to.to", "training.TrainerSingleNode.data.getLabelID", "datetime.datetime.now", "training.TrainerSingleNode.model.archit.zero_grad", "training.TrainerSingleNode.model.archit.singleNodeForward", "training.TrainerSingleNode.model.loss", "training.TrainerSingleNode.backward", "training.TrainerSingleNode.model.optim.step", "datetime.datetime.now", "abs().total_seconds", "training.TrainerSingleNode.data.evaluate", "training.TrainerSingleNode.item", "training.TrainerSingleNode.item", "abs"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.getSamples", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.getLabelID", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.singleNodeForward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate"], ["", "def", "trainBatch", "(", "self", ",", "thisBatchIndices", ")", ":", "\n", "\n", "# Get the samples", "\n", "        ", "xTrain", ",", "yTrain", "=", "self", ".", "data", ".", "getSamples", "(", "'train'", ",", "thisBatchIndices", ")", "\n", "xTrain", "=", "xTrain", ".", "to", "(", "self", ".", "model", ".", "device", ")", "\n", "yTrain", "=", "yTrain", ".", "to", "(", "self", ".", "model", ".", "device", ")", "\n", "targetIDs", "=", "self", ".", "data", ".", "getLabelID", "(", "'train'", ",", "thisBatchIndices", ")", "\n", "\n", "# Start measuring time", "\n", "startTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "# Reset gradients", "\n", "self", ".", "model", ".", "archit", ".", "zero_grad", "(", ")", "\n", "\n", "# Obtain the output of the GNN", "\n", "yHatTrain", "=", "self", ".", "model", ".", "archit", ".", "singleNodeForward", "(", "xTrain", ",", "targetIDs", ")", "\n", "\n", "# Compute loss", "\n", "lossValueTrain", "=", "self", ".", "model", ".", "loss", "(", "yHatTrain", ",", "yTrain", ")", "\n", "\n", "# Compute gradients", "\n", "lossValueTrain", ".", "backward", "(", ")", "\n", "\n", "# Optimize", "\n", "self", ".", "model", ".", "optim", ".", "step", "(", ")", "\n", "\n", "# Finish measuring time", "\n", "endTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "timeElapsed", "=", "abs", "(", "endTime", "-", "startTime", ")", ".", "total_seconds", "(", ")", "\n", "\n", "# Compute the accuracy", "\n", "#   Note: Using yHatTrain.data creates a new tensor with the", "\n", "#   same value, but detaches it from the gradient, so that no", "\n", "#   gradient operation is taken into account here.", "\n", "#   (Alternatively, we could use a with torch.no_grad():)", "\n", "costTrain", "=", "self", ".", "data", ".", "evaluate", "(", "yHatTrain", ".", "data", ",", "yTrain", ")", "\n", "\n", "return", "lossValueTrain", ".", "item", "(", ")", ",", "costTrain", ".", "item", "(", ")", ",", "timeElapsed", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerSingleNode.validationStep": [[685, 715], ["training.TrainerSingleNode.data.getSamples", "xValid.to.to.to", "yValid.to.to.to", "training.TrainerSingleNode.data.getLabelID", "datetime.datetime.now", "torch.no_grad", "training.TrainerSingleNode.model.archit.singleNodeForward", "training.TrainerSingleNode.model.loss", "datetime.datetime.now", "abs().total_seconds", "training.TrainerSingleNode.data.evaluate", "training.TrainerSingleNode.item", "training.TrainerSingleNode.item", "abs"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.getSamples", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.getLabelID", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.singleNodeForward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate"], ["", "def", "validationStep", "(", "self", ")", ":", "\n", "\n", "# Validation:", "\n", "        ", "xValid", ",", "yValid", "=", "self", ".", "data", ".", "getSamples", "(", "'valid'", ")", "\n", "xValid", "=", "xValid", ".", "to", "(", "self", ".", "model", ".", "device", ")", "\n", "yValid", "=", "yValid", ".", "to", "(", "self", ".", "model", ".", "device", ")", "\n", "targetIDs", "=", "self", ".", "data", ".", "getLabelID", "(", "'valid'", ")", "\n", "\n", "# Start measuring time", "\n", "startTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "# Under torch.no_grad() so that the computations carried out", "\n", "# to obtain the validation accuracy are not taken into", "\n", "# account to update the learnable parameters.", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Obtain the output of the GNN", "\n", "            ", "yHatValid", "=", "self", ".", "model", ".", "archit", ".", "singleNodeForward", "(", "xValid", ",", "targetIDs", ")", "\n", "\n", "# Compute loss", "\n", "lossValueValid", "=", "self", ".", "model", ".", "loss", "(", "yHatValid", ",", "yValid", ")", "\n", "\n", "# Finish measuring time", "\n", "endTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "timeElapsed", "=", "abs", "(", "endTime", "-", "startTime", ")", ".", "total_seconds", "(", ")", "\n", "\n", "# Compute accuracy:", "\n", "costValid", "=", "self", ".", "data", ".", "evaluate", "(", "yHatValid", ",", "yValid", ")", "\n", "\n", "", "return", "lossValueValid", ".", "item", "(", ")", ",", "costValid", ".", "item", "(", ")", ",", "timeElapsed", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerFlocking.__init__": [[794, 816], ["training.Trainer.__init__", "kwargs.keys", "kwargs.keys"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "data", ",", "nEpochs", ",", "batchSize", ",", "**", "kwargs", ")", ":", "\n", "\n", "# Initialize supraclass", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "data", ",", "nEpochs", ",", "batchSize", ",", "**", "kwargs", ")", "\n", "\n", "# Add the specific options", "\n", "\n", "if", "'probExpert'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "doDAGger", "=", "True", "\n", "probExpert", "=", "kwargs", "[", "'probExpert'", "]", "\n", "", "else", ":", "\n", "            ", "doDAGger", "=", "False", "\n", "\n", "", "if", "'DAGgerType'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "DAGgerType", "=", "kwargs", "[", "'DAGgerType'", "]", "\n", "", "else", ":", "\n", "            ", "DAGgerType", "=", "'fixedBatch'", "\n", "\n", "", "self", ".", "trainingOptions", "[", "'doDAGger'", "]", "=", "doDAGger", "\n", "if", "doDAGger", ":", "\n", "            ", "self", ".", "trainingOptions", "[", "'probExpert'", "]", "=", "probExpert", "\n", "self", ".", "trainingOptions", "[", "'DAGgerType'", "]", "=", "DAGgerType", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerFlocking.train": [[817, 1257], ["training.TrainerFlocking.data.getSamples", "training.TrainerFlocking.data.getData", "training.TrainerFlocking.data.getData", "training.TrainerFlocking.model.save", "training.TrainerFlocking.model.load", "dir", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "torch.optim.lr_scheduler.StepLR", "training.TrainerFlocking.data.getData", "numpy.random.permutation", "numpy.array", "numpy.array", "os.path.join", "os.path.join", "training.TrainerFlocking.model.save", "training.TrainerFlocking.model.save", "print", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "training.TrainerFlocking.trainingOptions.keys", "int", "torch.optim.lr_scheduler.StepLR.step", "training.TrainerFlocking.randomEpochDAGger", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "datetime.datetime.now", "thisArchit.zero_grad", "thisArchit", "thisLoss", "thisLoss.backward", "thisOptim.step", "datetime.datetime.now", "abs().total_seconds", "numpy.array", "numpy.array", "os.path.exists", "os.makedirs", "open", "pickle.dump", "print", "print", "training.TrainerFlocking.replaceTimeBatchDAGger", "training.TrainerFlocking.fixedBatchDAGger", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.tile", "thisLoss.item", "logger.scalar_summary", "datetime.datetime.now", "training.TrainerFlocking.data.getData", "training.TrainerFlocking.data.getData", "training.TrainerFlocking.data.computeTrajectory", "training.TrainerFlocking.data.evaluate", "datetime.datetime.now", "abs().total_seconds", "abs", "thisLoss.item", "print", "print", "print", "print", "print", "logger.scalar_summary", "training.TrainerFlocking.model.save", "print", "print", "abs", "print", "training.TrainerFlocking.model.save", "print", "print", "print", "thisLoss.item"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.getSamples", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerFlocking.randomEpochDAGger", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerFlocking.replaceTimeBatchDAGger", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerFlocking.fixedBatchDAGger", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.scalar_summary", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeTrajectory", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.visualTools.Visualizer.scalar_summary", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.save"], ["", "", "def", "train", "(", "self", ")", ":", "\n", "\n", "# Get back the training options", "\n", "        ", "assert", "'trainingOptions'", "in", "dir", "(", "self", ")", "\n", "assert", "'doLogging'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doLogging", "=", "self", ".", "trainingOptions", "[", "'doLogging'", "]", "\n", "assert", "'logger'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "logger", "=", "self", ".", "trainingOptions", "[", "'logger'", "]", "\n", "assert", "'doSaveVars'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doSaveVars", "=", "self", ".", "trainingOptions", "[", "'doSaveVars'", "]", "\n", "assert", "'doPrint'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doPrint", "=", "self", ".", "trainingOptions", "[", "'doPrint'", "]", "\n", "assert", "'printInterval'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "printInterval", "=", "self", ".", "trainingOptions", "[", "'printInterval'", "]", "\n", "assert", "'doLearningRateDecay'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doLearningRateDecay", "=", "self", ".", "trainingOptions", "[", "'doLearningRateDecay'", "]", "\n", "if", "doLearningRateDecay", ":", "\n", "            ", "assert", "'learningRateDecayRate'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "learningRateDecayRate", "=", "self", ".", "trainingOptions", "[", "'learningRateDecayRate'", "]", "\n", "assert", "'learningRateDecayPeriod'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "learningRateDecayPeriod", "=", "self", ".", "trainingOptions", "[", "'learningRateDecayPeriod'", "]", "\n", "", "assert", "'validationInterval'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "validationInterval", "=", "self", ".", "trainingOptions", "[", "'validationInterval'", "]", "\n", "assert", "'doEarlyStopping'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doEarlyStopping", "=", "self", ".", "trainingOptions", "[", "'doEarlyStopping'", "]", "\n", "assert", "'earlyStoppingLag'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "earlyStoppingLag", "=", "self", ".", "trainingOptions", "[", "'earlyStoppingLag'", "]", "\n", "assert", "'batchIndex'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "batchIndex", "=", "self", ".", "trainingOptions", "[", "'batchIndex'", "]", "\n", "assert", "'batchSize'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "batchSize", "=", "self", ".", "trainingOptions", "[", "'batchSize'", "]", "\n", "assert", "'nEpochs'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "nEpochs", "=", "self", ".", "trainingOptions", "[", "'nEpochs'", "]", "\n", "assert", "'nBatches'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "nBatches", "=", "self", ".", "trainingOptions", "[", "'nBatches'", "]", "\n", "assert", "'graphNo'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "graphNo", "=", "self", ".", "trainingOptions", "[", "'graphNo'", "]", "\n", "assert", "'realizationNo'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "realizationNo", "=", "self", ".", "trainingOptions", "[", "'realizationNo'", "]", "\n", "assert", "'doDAGger'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "doDAGger", "=", "self", ".", "trainingOptions", "[", "'doDAGger'", "]", "\n", "if", "doDAGger", ":", "\n", "            ", "assert", "'DAGgerType'", "in", "self", ".", "trainingOptions", ".", "keys", "(", ")", "\n", "DAGgerType", "=", "self", ".", "trainingOptions", "[", "'DAGgerType'", "]", "\n", "\n", "# Get the values we need", "\n", "", "nTrain", "=", "self", ".", "data", ".", "nTrain", "\n", "thisArchit", "=", "self", ".", "model", ".", "archit", "\n", "thisLoss", "=", "self", ".", "model", ".", "loss", "\n", "thisOptim", "=", "self", ".", "model", ".", "optim", "\n", "thisDevice", "=", "self", ".", "model", ".", "device", "\n", "\n", "# Learning rate scheduler:", "\n", "if", "doLearningRateDecay", ":", "\n", "            ", "learningRateScheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "self", ".", "optim", ",", "\n", "learningRateDecayPeriod", ",", "learningRateDecayRate", ")", "\n", "\n", "# Initialize counters (since we give the possibility of early stopping,", "\n", "# we had to drop the 'for' and use a 'while' instead):", "\n", "", "epoch", "=", "0", "# epoch counter", "\n", "lagCount", "=", "0", "# lag counter for early stopping", "\n", "\n", "if", "doSaveVars", ":", "\n", "            ", "lossTrain", "=", "[", "]", "\n", "evalValid", "=", "[", "]", "\n", "timeTrain", "=", "[", "]", "\n", "timeValid", "=", "[", "]", "\n", "\n", "# Get original dataset", "\n", "", "xTrainOrig", ",", "yTrainOrig", "=", "self", ".", "data", ".", "getSamples", "(", "'train'", ")", "\n", "StrainOrig", "=", "self", ".", "data", ".", "getData", "(", "'commGraph'", ",", "'train'", ")", "\n", "initVelTrainAll", "=", "self", ".", "data", ".", "getData", "(", "'initVel'", ",", "'train'", ")", "\n", "if", "doDAGger", ":", "\n", "            ", "initPosTrainAll", "=", "self", ".", "data", ".", "getData", "(", "'initPos'", ",", "'train'", ")", "\n", "\n", "# And save it as the original \"all samples\"", "\n", "", "xTrainAll", "=", "xTrainOrig", "\n", "yTrainAll", "=", "yTrainOrig", "\n", "StrainAll", "=", "StrainOrig", "\n", "\n", "# If it is:", "\n", "#   'randomEpoch' assigns always the original training set at the", "\n", "#       beginning of each epoch, so it is reset by using the variable", "\n", "#       Orig, instead of the variable all", "\n", "#   'replaceTimeBatch' keeps working only in the All variables, so", "\n", "#       every epoch updates the previous dataset, and never goes back", "\n", "#       to the original dataset (i.e. there is no Orig involved in", "\n", "#       the 'replaceTimeBatch' DAGger)", "\n", "#   'fixedBatch': it takes All = Orig from the beginning and then it", "\n", "#       doesn't matter becuase it always acts by creating a new", "\n", "#       batch with \"corrected\" trajectories for the learned policies", "\n", "\n", "while", "epoch", "<", "nEpochs", "and", "(", "lagCount", "<", "earlyStoppingLag", "or", "(", "not", "doEarlyStopping", ")", ")", ":", "\n", "# The condition will be zero (stop), whenever one of the items of", "\n", "# the 'and' is zero. Therefore, we want this to stop only for epoch", "\n", "# counting when we are NOT doing early stopping. This can be", "\n", "# achieved if the second element of the 'and' is always 1 (so that", "\n", "# the first element, the epoch counting, decides). In order to", "\n", "# force the second element to be one whenever there is not early", "\n", "# stopping, we have an or, and force it to one. So, when we are not", "\n", "# doing early stopping, the variable 'not doEarlyStopping' is 1,", "\n", "# and the result of the 'or' is 1 regardless of the lagCount. When", "\n", "# we do early stopping, then the variable 'not doEarlyStopping' is", "\n", "# 0, and the value 1 for the 'or' gate is determined by the lag", "\n", "# count.", "\n", "# ALTERNATIVELY, we could just keep 'and lagCount<earlyStoppingLag'", "\n", "# and be sure that lagCount can only be increased whenever", "\n", "# doEarlyStopping is True. But I somehow figured out that would be", "\n", "# harder to maintain (more parts of the code to check if we are", "\n", "# accidentally increasing lagCount).", "\n", "\n", "# Randomize dataset for each epoch", "\n", "            ", "randomPermutation", "=", "np", ".", "random", ".", "permutation", "(", "nTrain", ")", "\n", "# Convert a numpy.array of numpy.int into a list of actual int.", "\n", "idxEpoch", "=", "[", "int", "(", "i", ")", "for", "i", "in", "randomPermutation", "]", "\n", "\n", "# Learning decay", "\n", "if", "doLearningRateDecay", ":", "\n", "                ", "learningRateScheduler", ".", "step", "(", ")", "\n", "\n", "if", "doPrint", ":", "\n", "# All the optimization have the same learning rate, so just", "\n", "# print one of them", "\n", "# TODO: Actually, they might be different, so I will need to", "\n", "# print all of them.", "\n", "                    ", "print", "(", "\"Epoch %d, learning rate = %.8f\"", "%", "(", "epoch", "+", "1", ",", "\n", "learningRateScheduler", ".", "optim", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "#\\\\\\ Start DAGGER: randomEpoch", "\n", "#\\\\\\", "\n", "", "", "if", "doDAGger", "and", "epoch", ">", "0", "and", "DAGgerType", "==", "'randomEpoch'", ":", "\n", "\n", "# The 'randomEpoch' option forms a new training set for each", "\n", "# epoch consisting, with probability probExpert, of samples", "\n", "# of the original dataset (optimal trajectories) and with", "\n", "# probability 1-probExpert, with trajectories following the", "\n", "# latest trained dataset.", "\n", "\n", "                ", "xTrainAll", ",", "yTrainAll", ",", "StrainAll", "=", "self", ".", "randomEpochDAGger", "(", "epoch", ",", "xTrainOrig", ",", "yTrainOrig", ",", "\n", "StrainOrig", ",", "initPosTrainAll", ",", "\n", "initVelTrainAll", ")", "\n", "#\\\\\\", "\n", "#\\\\\\ Finished DAGGER", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "\n", "# Initialize counter", "\n", "", "batch", "=", "0", "# batch counter", "\n", "while", "batch", "<", "nBatches", "and", "(", "lagCount", "<", "earlyStoppingLag", "or", "(", "not", "doEarlyStopping", ")", ")", ":", "\n", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "#\\\\\\ Start DAGGER: replaceTimeBatch", "\n", "#\\\\\\", "\n", "                ", "if", "doDAGger", "and", "(", "batch", ">", "0", "or", "epoch", ">", "0", ")", "and", "DAGgerType", "==", "'replaceTimeBatch'", ":", "\n", "\n", "# The option 'replaceTimeBatch' creates a fixed number of", "\n", "# new trajectories following randomly at each time step", "\n", "# either the optimal control or the learned control", "\n", "# Then, replaces this fixed number of new trajectores into", "\n", "# the training set (then these might, or might not get", "\n", "# selected by the next batch)", "\n", "\n", "                    ", "xTrainAll", ",", "yTrainAll", ",", "StrainAll", "=", "self", ".", "replaceTimeBatchDAGger", "(", "epoch", ",", "xTrainAll", ",", "yTrainAll", ",", "\n", "StrainAll", ",", "initPosTrainAll", ",", "\n", "initVelTrainAll", ")", "\n", "#\\\\\\", "\n", "#\\\\\\ Finished DAGGER", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "\n", "# Extract the adequate batch", "\n", "", "thisBatchIndices", "=", "idxEpoch", "[", "batchIndex", "[", "batch", "]", "\n", ":", "batchIndex", "[", "batch", "+", "1", "]", "]", "\n", "# Get the samples", "\n", "xTrain", "=", "xTrainAll", "[", "thisBatchIndices", "]", "\n", "yTrain", "=", "yTrainAll", "[", "thisBatchIndices", "]", "\n", "Strain", "=", "StrainAll", "[", "thisBatchIndices", "]", "\n", "initVelTrain", "=", "initVelTrainAll", "[", "thisBatchIndices", "]", "\n", "if", "doDAGger", "and", "DAGgerType", "==", "'fixedBatch'", ":", "\n", "                    ", "initPosTrain", "=", "initPosTrainAll", "[", "thisBatchIndices", "]", "\n", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "#\\\\\\ Start DAGGER: fixedBatch", "\n", "#\\\\\\", "\n", "", "if", "doDAGger", "and", "(", "batch", ">", "0", "or", "epoch", ">", "0", ")", "and", "DAGgerType", "==", "'fixedBatch'", ":", "\n", "\n", "# The 'fixedBatch' option, doubles the batch samples", "\n", "# by considering the same initial velocities and", "\n", "# positions, a trajectory given by the latest trained", "\n", "# architecture, and the corresponding correction", "\n", "# given by the optimal acceleration (i.e. for each", "\n", "# position and velocity we give what would be the", "\n", "# optimal acceleration, even though the next position", "\n", "# and velocity won't reflect this decision, but the", "\n", "# one taken by the learned policy)", "\n", "\n", "                    ", "xDAG", ",", "yDAG", ",", "SDAG", "=", "self", ".", "fixedBatchDAGger", "(", "initPosTrain", ",", "\n", "initVelTrain", ")", "\n", "\n", "xTrain", "=", "np", ".", "concatenate", "(", "(", "xTrain", ",", "xDAG", ")", ",", "axis", "=", "0", ")", "\n", "Strain", "=", "np", ".", "concatenate", "(", "(", "Strain", ",", "SDAG", ")", ",", "axis", "=", "0", ")", "\n", "yTrain", "=", "np", ".", "concatenate", "(", "(", "yTrain", ",", "yDAG", ")", ",", "axis", "=", "0", ")", "\n", "initVelTrain", "=", "np", ".", "tile", "(", "initVelTrain", ",", "(", "2", ",", "1", ",", "1", ")", ")", "\n", "#\\\\\\", "\n", "#\\\\\\ Finished DAGGER", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "\n", "# Now that we have our dataset, move it to tensor and device", "\n", "# so we can use it", "\n", "", "xTrain", "=", "torch", ".", "tensor", "(", "xTrain", ",", "device", "=", "thisDevice", ")", "\n", "Strain", "=", "torch", ".", "tensor", "(", "Strain", ",", "device", "=", "thisDevice", ")", "\n", "yTrain", "=", "torch", ".", "tensor", "(", "yTrain", ",", "device", "=", "thisDevice", ")", "\n", "initVelTrain", "=", "torch", ".", "tensor", "(", "initVelTrain", ",", "device", "=", "thisDevice", ")", "\n", "\n", "# Start measuring time", "\n", "startTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "# Reset gradients", "\n", "thisArchit", ".", "zero_grad", "(", ")", "\n", "\n", "# Obtain the output of the GNN", "\n", "yHatTrain", "=", "thisArchit", "(", "xTrain", ",", "Strain", ")", "\n", "\n", "# Compute loss", "\n", "lossValueTrain", "=", "thisLoss", "(", "yHatTrain", ",", "yTrain", ")", "\n", "\n", "# Compute gradients", "\n", "lossValueTrain", ".", "backward", "(", ")", "\n", "\n", "# Optimize", "\n", "thisOptim", ".", "step", "(", ")", "\n", "\n", "# Finish measuring time", "\n", "endTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "timeElapsed", "=", "abs", "(", "endTime", "-", "startTime", ")", ".", "total_seconds", "(", ")", "\n", "\n", "# Logging values", "\n", "if", "doLogging", ":", "\n", "                    ", "lossTrainTB", "=", "lossValueTrain", ".", "item", "(", ")", "\n", "# Save values", "\n", "", "if", "doSaveVars", ":", "\n", "                    ", "lossTrain", "+=", "[", "lossValueTrain", ".", "item", "(", ")", "]", "\n", "timeTrain", "+=", "[", "timeElapsed", "]", "\n", "\n", "# Print:", "\n", "", "if", "doPrint", "and", "printInterval", ">", "0", ":", "\n", "                    ", "if", "(", "epoch", "*", "nBatches", "+", "batch", ")", "%", "printInterval", "==", "0", ":", "\n", "                        ", "print", "(", "\"\\t(E: %2d, B: %3d) %7.4f - %6.4fs\"", "%", "(", "\n", "epoch", "+", "1", ",", "batch", "+", "1", ",", "\n", "lossValueTrain", ".", "item", "(", ")", ",", "timeElapsed", ")", ",", "\n", "end", "=", "' '", ")", "\n", "if", "graphNo", ">", "-", "1", ":", "\n", "                            ", "print", "(", "\"[%d\"", "%", "graphNo", ",", "end", "=", "''", ")", "\n", "if", "realizationNo", ">", "-", "1", ":", "\n", "                                ", "print", "(", "\"/%d\"", "%", "realizationNo", ",", "\n", "end", "=", "''", ")", "\n", "", "print", "(", "\"]\"", ",", "end", "=", "''", ")", "\n", "", "print", "(", "\"\"", ")", "\n", "\n", "\n", "# Delete variables to free space in CUDA memory", "\n", "", "", "del", "xTrain", "\n", "del", "Strain", "\n", "del", "yTrain", "\n", "del", "initVelTrain", "\n", "del", "lossValueTrain", "\n", "\n", "#\\\\\\\\\\\\\\", "\n", "#\\\\\\ TB LOGGING (for each batch)", "\n", "#\\\\\\\\\\\\\\", "\n", "\n", "if", "doLogging", ":", "\n", "                    ", "logger", ".", "scalar_summary", "(", "mode", "=", "'Training'", ",", "\n", "epoch", "=", "epoch", "*", "nBatches", "+", "batch", ",", "\n", "**", "{", "'lossTrain'", ":", "lossTrainTB", "}", ")", "\n", "\n", "#\\\\\\\\\\\\\\", "\n", "#\\\\\\ VALIDATION", "\n", "#\\\\\\\\\\\\\\", "\n", "\n", "", "if", "(", "epoch", "*", "nBatches", "+", "batch", ")", "%", "validationInterval", "==", "0", ":", "\n", "\n", "# Start measuring time", "\n", "                    ", "startTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "# Create trajectories", "\n", "\n", "# Initial data", "\n", "initPosValid", "=", "self", ".", "data", ".", "getData", "(", "'initPos'", ",", "'valid'", ")", "\n", "initVelValid", "=", "self", ".", "data", ".", "getData", "(", "'initVel'", ",", "'valid'", ")", "\n", "\n", "# Compute trajectories", "\n", "_", ",", "velTestValid", ",", "_", ",", "_", ",", "_", "=", "self", ".", "data", ".", "computeTrajectory", "(", "\n", "initPosValid", ",", "initVelValid", ",", "self", ".", "data", ".", "duration", ",", "\n", "archit", "=", "thisArchit", ",", "doPrint", "=", "False", ")", "\n", "\n", "# Compute evaluation", "\n", "accValid", "=", "self", ".", "data", ".", "evaluate", "(", "vel", "=", "velTestValid", ")", "\n", "\n", "# Finish measuring time", "\n", "endTime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "\n", "timeElapsed", "=", "abs", "(", "endTime", "-", "startTime", ")", ".", "total_seconds", "(", ")", "\n", "\n", "# Logging values", "\n", "if", "doLogging", ":", "\n", "                        ", "evalValidTB", "=", "accValid", "\n", "# Save values", "\n", "", "if", "doSaveVars", ":", "\n", "                        ", "evalValid", "+=", "[", "accValid", "]", "\n", "timeValid", "+=", "[", "timeElapsed", "]", "\n", "\n", "# Print:", "\n", "", "if", "doPrint", ":", "\n", "                        ", "print", "(", "\"\\t(E: %2d, B: %3d) %8.4f - %6.4fs\"", "%", "(", "\n", "epoch", "+", "1", ",", "batch", "+", "1", ",", "\n", "accValid", ",", "\n", "timeElapsed", ")", ",", "end", "=", "' '", ")", "\n", "print", "(", "\"[VALIDATION\"", ",", "end", "=", "''", ")", "\n", "if", "graphNo", ">", "-", "1", ":", "\n", "                            ", "print", "(", "\".%d\"", "%", "graphNo", ",", "end", "=", "''", ")", "\n", "if", "realizationNo", ">", "-", "1", ":", "\n", "                                ", "print", "(", "\"/%d\"", "%", "realizationNo", ",", "end", "=", "''", ")", "\n", "", "", "print", "(", "\" (%s)]\"", "%", "self", ".", "model", ".", "name", ")", "\n", "\n", "", "if", "doLogging", ":", "\n", "                        ", "logger", ".", "scalar_summary", "(", "mode", "=", "'Validation'", ",", "\n", "epoch", "=", "epoch", "*", "nBatches", "+", "batch", ",", "\n", "**", "{", "'evalValid'", ":", "evalValidTB", "}", ")", "\n", "\n", "# No previous best option, so let's record the first trial", "\n", "# as the best option", "\n", "", "if", "epoch", "==", "0", "and", "batch", "==", "0", ":", "\n", "                        ", "bestScore", "=", "accValid", "\n", "bestEpoch", ",", "bestBatch", "=", "epoch", ",", "batch", "\n", "# Save this model as the best (so far)", "\n", "self", ".", "model", ".", "save", "(", "label", "=", "'Best'", ")", "\n", "# Start the counter", "\n", "if", "doEarlyStopping", ":", "\n", "                            ", "initialBest", "=", "True", "\n", "", "", "else", ":", "\n", "                        ", "thisValidScore", "=", "accValid", "\n", "if", "thisValidScore", "<", "bestScore", ":", "\n", "                            ", "bestScore", "=", "thisValidScore", "\n", "bestEpoch", ",", "bestBatch", "=", "epoch", ",", "batch", "\n", "if", "doPrint", ":", "\n", "                                ", "print", "(", "\"\\t=> New best achieved: %.4f\"", "%", "(", "bestScore", ")", ")", "\n", "", "self", ".", "model", ".", "save", "(", "label", "=", "'Best'", ")", "\n", "# Now that we have found a best that is not the", "\n", "# initial one, we can start counting the lag (if", "\n", "# needed)", "\n", "initialBest", "=", "False", "\n", "# If we achieved a new best, then we need to reset", "\n", "# the lag count.", "\n", "if", "doEarlyStopping", ":", "\n", "                                ", "lagCount", "=", "0", "\n", "# If we didn't achieve a new best, increase the lag", "\n", "# count.", "\n", "# Unless it was the initial best, in which case we", "\n", "# haven't found any best yet, so we shouldn't be doing", "\n", "# the early stopping count.", "\n", "", "", "elif", "doEarlyStopping", "and", "not", "initialBest", ":", "\n", "                            ", "lagCount", "+=", "1", "\n", "\n", "# Delete variables to free space in CUDA memory", "\n", "", "", "del", "initVelValid", "\n", "del", "initPosValid", "\n", "\n", "#\\\\\\\\\\\\\\", "\n", "#\\\\\\ END OF BATCH:", "\n", "#\\\\\\\\\\\\\\", "\n", "\n", "#\\\\\\ Increase batch count:", "\n", "", "batch", "+=", "1", "\n", "\n", "#\\\\\\\\\\\\\\", "\n", "#\\\\\\ END OF EPOCH:", "\n", "#\\\\\\\\\\\\\\", "\n", "\n", "#\\\\\\ Increase epoch count:", "\n", "", "epoch", "+=", "1", "\n", "\n", "#\\\\\\ Save models:", "\n", "", "self", ".", "model", ".", "save", "(", "label", "=", "'Last'", ")", "\n", "\n", "#################", "\n", "# TRAINING OVER #", "\n", "#################", "\n", "\n", "if", "doSaveVars", ":", "\n", "# We convert the lists into np.arrays", "\n", "            ", "lossTrain", "=", "np", ".", "array", "(", "lossTrain", ")", "\n", "evalValid", "=", "np", ".", "array", "(", "evalValid", ")", "\n", "# And we would like to save all the relevant information from", "\n", "# training", "\n", "trainVars", "=", "{", "'nEpochs'", ":", "nEpochs", ",", "\n", "'nBatches'", ":", "nBatches", ",", "\n", "'validationInterval'", ":", "validationInterval", ",", "\n", "'batchSize'", ":", "np", ".", "array", "(", "batchSize", ")", ",", "\n", "'batchIndex'", ":", "np", ".", "array", "(", "batchIndex", ")", ",", "\n", "'bestBatch'", ":", "bestBatch", ",", "\n", "'bestEpoch'", ":", "bestEpoch", ",", "\n", "'bestScore'", ":", "bestScore", ",", "\n", "'lossTrain'", ":", "lossTrain", ",", "\n", "'timeTrain'", ":", "timeTrain", ",", "\n", "'evalValid'", ":", "evalValid", ",", "\n", "'timeValid'", ":", "timeValid", "\n", "}", "\n", "saveDirVars", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model", ".", "saveDir", ",", "'trainVars'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "saveDirVars", ")", ":", "\n", "                ", "os", ".", "makedirs", "(", "saveDirVars", ")", "\n", "", "pathToFile", "=", "os", ".", "path", ".", "join", "(", "saveDirVars", ",", "self", ".", "model", ".", "name", "+", "'trainVars.pkl'", ")", "\n", "with", "open", "(", "pathToFile", ",", "'wb'", ")", "as", "trainVarsFile", ":", "\n", "                ", "pickle", ".", "dump", "(", "trainVars", ",", "trainVarsFile", ")", "\n", "\n", "# Now, if we didn't do any training (i.e. nEpochs = 0), then the last is", "\n", "# also the best.", "\n", "", "", "if", "nEpochs", "==", "0", ":", "\n", "            ", "self", ".", "model", ".", "save", "(", "label", "=", "'Best'", ")", "\n", "self", ".", "model", ".", "save", "(", "label", "=", "'Last'", ")", "\n", "if", "doPrint", ":", "\n", "                ", "print", "(", "\"\\nWARNING: No training. Best and Last models are the same.\\n\"", ")", "\n", "\n", "# After training is done, reload best model before proceeding to", "\n", "# evaluation:", "\n", "", "", "self", ".", "model", ".", "load", "(", "label", "=", "'Best'", ")", "\n", "\n", "#\\\\\\ Print out best:", "\n", "if", "doPrint", "and", "nEpochs", ">", "0", ":", "\n", "            ", "print", "(", "\"\\t=> Best validation achieved (E: %d, B: %d): %.4f\"", "%", "(", "\n", "bestEpoch", "+", "1", ",", "bestBatch", "+", "1", ",", "bestScore", ")", ")", "\n", "\n", "", "return", "trainVars", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerFlocking.randomEpochDAGger": [[1258, 1414], ["numpy.max", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "training.TrainerFlocking.trainingOptions.kwargs", "numpy.random.binomial", "training.TrainerFlocking.data.computeTrajectory", "training.TrainerFlocking.data.computeCommunicationGraph", "training.TrainerFlocking.data.computeStates", "numpy.zeros", "range", "training.TrainerFlocking.data.computeDifferences", "training.TrainerFlocking.data.computeDifferences", "alegnn.utils.dataTools.invertTensorEW", "numpy.expand_dims", "training.TrainerFlocking.data.computeDifferences", "training.TrainerFlocking.data.computeDifferences", "alegnn.utils.dataTools.invertTensorEW", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeTrajectory", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeCommunicationGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeStates", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW"], ["", "def", "randomEpochDAGger", "(", "self", ",", "epoch", ",", "xTrainOrig", ",", "yTrainOrig", ",", "StrainOrig", ",", "\n", "initPosTrainAll", ",", "initVelTrainAll", ")", ":", "\n", "\n", "# The 'randomEpoch' option forms a new training set for each", "\n", "# epoch consisting, with probability probExpert, of samples", "\n", "# of the original dataset (optimal trajectories) and with", "\n", "# probability 1-probExpert, with trajectories following the", "\n", "# latest trained dataset.", "\n", "\n", "        ", "assert", "'probExpert'", "in", "self", ".", "trainingOptions", ".", "kwargs", "(", ")", "\n", "probExpert", "=", "self", ".", "trainingOptions", "[", "'probExpert'", "]", "\n", "nTrain", "=", "xTrainOrig", ".", "shape", "[", "0", "]", "\n", "\n", "# Compute the prob expert", "\n", "chooseExpertProb", "=", "np", ".", "max", "(", "(", "probExpert", "**", "epoch", ",", "0.5", ")", ")", "\n", "\n", "# What we will pass to the actual training epoch are:", "\n", "# xTrain, Strain and yTrain for computation", "\n", "xDAG", "=", "np", ".", "zeros", "(", "xTrainOrig", ".", "shape", ")", "\n", "yDAG", "=", "np", ".", "zeros", "(", "yTrainOrig", ".", "shape", ")", "\n", "SDAG", "=", "np", ".", "zeros", "(", "StrainOrig", ".", "shape", ")", "\n", "# initVelTrain is needed for evaluation, but doesn't change", "\n", "\n", "# For each sample, choose whether we keep the optimal", "\n", "# trajectory or we add the learned trajectory", "\n", "for", "s", "in", "range", "(", "nTrain", ")", ":", "\n", "\n", "            ", "if", "np", ".", "random", ".", "binomial", "(", "1", ",", "chooseExpertProb", ")", "==", "1", ":", "\n", "\n", "# If we choose the expert, we just get the values of", "\n", "# the optimal trajectory", "\n", "\n", "                ", "xDAG", "[", "s", "]", "=", "xTrainOrig", "[", "s", "]", "\n", "yDAG", "[", "s", "]", "=", "yTrainOrig", "[", "s", "]", "\n", "SDAG", "[", "s", "]", "=", "StrainOrig", "[", "s", "]", "\n", "\n", "", "else", ":", "\n", "\n", "# If not, we compute a new trajectory based on the", "\n", "# given architecture", "\n", "                ", "posDAG", ",", "velDAG", ",", "_", ",", "_", ",", "_", "=", "self", ".", "data", ".", "computeTrajectory", "(", "\n", "initPosTrainAll", "[", "s", ":", "s", "+", "1", "]", ",", "initVelTrainAll", "[", "s", ":", "s", "+", "1", "]", ",", "\n", "self", ".", "data", ".", "duration", ",", "archit", "=", "self", ".", "model", ".", "archit", ",", "\n", "doPrint", "=", "False", ")", "\n", "\n", "# Now that we have the position and velocity trajectory", "\n", "# that we would get based on the learned controller,", "\n", "# we need to compute what the optimal acceleration", "\n", "# would actually be in each case.", "\n", "# And since this could be a large trajectory, we need", "\n", "# to split it based on how many samples", "\n", "\n", "maxTimeSamples", "=", "200", "\n", "\n", "if", "posDAG", ".", "shape", "[", "1", "]", ">", "maxTimeSamples", ":", "\n", "\n", "# Create the space", "\n", "                    ", "yDAGaux", "=", "np", ".", "zeros", "(", "(", "1", ",", "# batchSize", "\n", "posDAG", ".", "shape", "[", "1", "]", ",", "# tSamples", "\n", "2", ",", "\n", "posDAG", ".", "shape", "[", "3", "]", ")", ")", "# nAgents", "\n", "\n", "for", "t", "in", "range", "(", "posDAG", ".", "shape", "[", "1", "]", ")", ":", "\n", "\n", "# Compute the expert on the corresponding", "\n", "# trajectory", "\n", "#   First, we need the difference in positions", "\n", "                        ", "ijDiffPos", ",", "ijDistSq", "=", "self", ".", "data", ".", "computeDifferences", "(", "posDAG", "[", ":", ",", "t", ",", ":", ",", ":", "]", ")", "\n", "#   And in velocities", "\n", "ijDiffVel", ",", "_", "=", "self", ".", "data", ".", "computeDifferences", "(", "velDAG", "[", ":", ",", "t", ",", ":", ",", ":", "]", ")", "\n", "#   Now, the second term (the one that depends", "\n", "#   on the positions) only needs to be computed", "\n", "#   for nodes thatare within repel distance, so", "\n", "#   let's compute a mask to find these nodes.", "\n", "repelMask", "=", "(", "ijDistSq", "<", "(", "self", ".", "data", ".", "repelDist", "**", "2", ")", ")", ".", "astype", "(", "ijDiffPos", ".", "dtype", ")", "\n", "#   Apply this mask to the position difference", "\n", "#   (we need not apply it to the square", "\n", "#   differences since these will be multiplied", "\n", "#   by the position differences which already", "\n", "#   will be zero)", "\n", "#   Note that we need to add the dimension of axis", "\n", "#   to properly multiply it", "\n", "ijDiffPos", "=", "ijDiffPos", "*", "np", ".", "expand_dims", "(", "repelMask", ",", "1", ")", "\n", "#   Invert the tensor elementwise (avoiding the", "\n", "#   zeros)", "\n", "ijDistSqInv", "=", "invertTensorEW", "(", "ijDistSq", ")", "\n", "#   Add an extra dimension, also across the", "\n", "#   axis", "\n", "ijDistSqInv", "=", "np", ".", "expand_dims", "(", "ijDistSqInv", ",", "1", ")", "\n", "#   Compute the optimal solution", "\n", "thisAccel", "=", "-", "np", ".", "sum", "(", "ijDiffVel", ",", "axis", "=", "3", ")", "+", "2", "*", "np", ".", "sum", "(", "ijDiffPos", "*", "(", "ijDistSqInv", "**", "2", "+", "ijDistSqInv", ")", ",", "\n", "axis", "=", "3", ")", "\n", "# And cap it", "\n", "thisAccel", "[", "thisAccel", ">", "self", ".", "data", ".", "accelMax", "]", "=", "self", ".", "data", ".", "accelMax", "\n", "thisAccel", "[", "thisAccel", "<", "-", "self", ".", "data", ".", "accelMax", "]", "=", "-", "self", ".", "data", ".", "accelMax", "\n", "\n", "# Store it", "\n", "yDAGaux", "[", ":", ",", "t", ",", ":", ",", ":", "]", "=", "thisAccel", "\n", "\n", "", "", "else", ":", "\n", "# Compute the expert on the corresponding", "\n", "# trajectory", "\n", "#   First, we need the difference in positions", "\n", "                    ", "ijDiffPos", ",", "ijDistSq", "=", "self", ".", "data", ".", "computeDifferences", "(", "posDAG", ")", "\n", "#   And in velocities", "\n", "ijDiffVel", ",", "_", "=", "self", ".", "data", ".", "computeDifferences", "(", "velDAG", ")", "\n", "#   Now, the second term (the one that depends on", "\n", "#   the positions) only needs to be computed for", "\n", "#   nodes that are within repel distance, so let's", "\n", "#   compute a mask to find these nodes.", "\n", "repelMask", "=", "(", "ijDistSq", "<", "(", "self", ".", "data", ".", "repelDist", "**", "2", ")", ")", ".", "astype", "(", "ijDiffPos", ".", "dtype", ")", "\n", "#   Apply this mask to the position difference (we", "\n", "#   need not apply it to the square differences,", "\n", "#   since these will be multiplied by the position", "\n", "#   differences, which already will be zero)", "\n", "#   Note that we need to add the dimension of axis", "\n", "#   to properly multiply it", "\n", "ijDiffPos", "=", "ijDiffPos", "*", "np", ".", "expand_dims", "(", "repelMask", ",", "2", ")", "\n", "#   Invert the tensor elementwise (avoiding the", "\n", "#   zeros)", "\n", "ijDistSqInv", "=", "invertTensorEW", "(", "ijDistSq", ")", "\n", "#   Add an extra dimension, also across the axis", "\n", "ijDistSqInv", "=", "np", ".", "expand_dims", "(", "ijDistSqInv", ",", "2", ")", "\n", "#   Compute the optimal solution", "\n", "yDAGaux", "=", "-", "np", ".", "sum", "(", "ijDiffVel", ",", "axis", "=", "4", ")", "+", "2", "*", "np", ".", "sum", "(", "ijDiffPos", "*", "(", "ijDistSqInv", "**", "2", "+", "ijDistSqInv", ")", ",", "\n", "axis", "=", "4", ")", "\n", "# And cap it", "\n", "yDAGaux", "[", "yDAGaux", ">", "self", ".", "data", ".", "accelMax", "]", "=", "self", ".", "data", ".", "accelMax", "\n", "yDAGaux", "[", "yDAGaux", "<", "-", "self", ".", "data", ".", "accelMax", "]", "=", "-", "self", ".", "data", ".", "accelMax", "\n", "\n", "# Finally, compute the corresponding graph of states", "\n", "# (pos) visited by the policy", "\n", "", "SDAGaux", "=", "self", ".", "data", ".", "computeCommunicationGraph", "(", "\n", "posDAG", ",", "self", ".", "data", ".", "commRadius", ",", "True", ",", "doPrint", "=", "False", ")", "\n", "xDAGaux", "=", "self", ".", "data", ".", "computeStates", "(", "posDAG", ",", "velDAG", ",", "SDAGaux", ",", "\n", "doPrint", "=", "False", ")", "\n", "\n", "# And save them", "\n", "xDAG", "[", "s", "]", "=", "xDAGaux", "[", "0", "]", "\n", "yDAG", "[", "s", "]", "=", "yDAGaux", "[", "0", "]", "\n", "SDAG", "[", "s", "]", "=", "SDAGaux", "[", "0", "]", "\n", "\n", "# And now that we have created the DAGger alternatives, we", "\n", "# just need to consider them as the basic training variables", "\n", "", "", "return", "xDAG", ",", "yDAG", ",", "SDAG", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerFlocking.replaceTimeBatchDAGger": [[1415, 1577], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.max", "range", "training.TrainerFlocking.trainingOptions.kwargs", "numpy.random.permutation", "range", "training.TrainerFlocking.squeeze", "training.TrainerFlocking.squeeze", "training.TrainerFlocking.data.computeCommunicationGraph", "training.TrainerFlocking.squeeze().squeeze", "training.TrainerFlocking.data.computeStates", "training.TrainerFlocking.squeeze().squeeze", "numpy.random.binomial", "training.TrainerFlocking.data.computeDifferences", "training.TrainerFlocking.data.computeDifferences", "alegnn.utils.dataTools.invertTensorEW", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims", "torch.tensor", "torch.tensor", "numpy.random.binomial", "training.TrainerFlocking.data.computeDifferences", "training.TrainerFlocking.data.computeDifferences", "alegnn.utils.dataTools.invertTensorEW", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims", "torch.tensor", "torch.tensor", "numpy.expand_dims", "torch.no_grad", "training.TrainerFlocking.model.archit", "training.TrainerFlocking.cpu().numpy", "numpy.expand_dims", "torch.no_grad", "training.TrainerFlocking.model.archit", "training.TrainerFlocking.cpu().numpy", "training.TrainerFlocking.squeeze", "training.TrainerFlocking.squeeze", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "training.TrainerFlocking.cpu", "training.TrainerFlocking.cpu"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeCommunicationGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeStates", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW"], ["", "def", "replaceTimeBatchDAGger", "(", "self", ",", "epoch", ",", "xTrainAll", ",", "yTrainAll", ",", "StrainAll", ",", "\n", "initPosTrainAll", ",", "initVelTrainAll", ",", "nReplace", "=", "10", ")", ":", "\n", "\n", "# The option 'replaceTimeBatch' creates a fixed number of", "\n", "# new trajectories following randomly at each time step", "\n", "# either the optimal control or the learned control", "\n", "# Then, replaces this fixed number of new trajectores into", "\n", "# the training set (then these might, or might not get", "\n", "# selected by the next batch)", "\n", "\n", "        ", "assert", "'probExpert'", "in", "self", ".", "trainingOptions", ".", "kwargs", "(", ")", "\n", "probExpert", "=", "self", ".", "trainingOptions", "[", "'probExpert'", "]", "\n", "nTrain", "=", "xTrainAll", ".", "shape", "[", "0", "]", "\n", "\n", "if", "nReplace", ">", "nTrain", ":", "\n", "            ", "nReplace", "=", "nTrain", "\n", "\n", "# Select the indices of the samples to replace", "\n", "", "replaceIndices", "=", "np", ".", "random", ".", "permutation", "(", "nTrain", ")", "[", "0", ":", "nReplace", "]", "\n", "\n", "# Get the corresponding initial velocities and positions", "\n", "initPosTrainThis", "=", "initPosTrainAll", "[", "replaceIndices", "]", "\n", "initVelTrainThis", "=", "initVelTrainAll", "[", "replaceIndices", "]", "\n", "\n", "# Save the resulting trajectories", "\n", "xDAG", "=", "np", ".", "zeros", "(", "(", "nReplace", ",", "\n", "xTrainAll", ".", "shape", "[", "1", "]", ",", "\n", "6", ",", "\n", "xTrainAll", ".", "shape", "[", "3", "]", ")", ")", "\n", "yDAG", "=", "np", ".", "zeros", "(", "(", "nReplace", ",", "\n", "yTrainAll", ".", "shape", "[", "1", "]", ",", "\n", "2", ",", "\n", "yTrainAll", ".", "shape", "[", "3", "]", ")", ")", "\n", "SDAG", "=", "np", ".", "zeros", "(", "(", "nReplace", ",", "\n", "StrainAll", ".", "shape", "[", "1", "]", ",", "\n", "StrainAll", ".", "shape", "[", "2", "]", ",", "\n", "StrainAll", ".", "shape", "[", "3", "]", ")", ")", "\n", "posDAG", "=", "np", ".", "zeros", "(", "yDAG", ".", "shape", ")", "\n", "velDAG", "=", "np", ".", "zeros", "(", "yDAG", ".", "shape", ")", "\n", "\n", "# Initialize first elements", "\n", "posDAG", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initPosTrainThis", "\n", "velDAG", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "initVelTrainThis", "\n", "SDAG", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "StrainAll", "[", "replaceIndices", ",", "0", "]", "\n", "xDAG", "[", ":", ",", "0", ",", ":", ",", ":", "]", "=", "xTrainAll", "[", "replaceIndices", ",", "0", "]", "\n", "\n", "# Compute the prob expert", "\n", "chooseExpertProb", "=", "np", ".", "max", "(", "(", "probExpert", "**", "(", "epoch", "+", "1", ")", ",", "0.5", ")", ")", "\n", "\n", "# Now, for each sample", "\n", "for", "s", "in", "range", "(", "nReplace", ")", ":", "\n", "\n", "# For each time instant", "\n", "            ", "for", "t", "in", "range", "(", "1", ",", "xTrainAll", ".", "shape", "[", "1", "]", ")", ":", "\n", "\n", "# Decide whether we apply the learned or the", "\n", "# optimal controller", "\n", "                ", "if", "np", ".", "random", ".", "binomial", "(", "1", ",", "chooseExpertProb", ")", "==", "1", ":", "\n", "\n", "# Compute the optimal acceleration", "\n", "                    ", "ijDiffPos", ",", "ijDistSq", "=", "self", ".", "data", ".", "computeDifferences", "(", "posDAG", "[", "s", ":", "s", "+", "1", ",", "t", "-", "1", ",", ":", ",", ":", "]", ")", "\n", "ijDiffVel", ",", "_", "=", "self", ".", "data", ".", "computeDifferences", "(", "velDAG", "[", "s", ":", "s", "+", "1", ",", "t", "-", "1", ",", ":", ",", ":", "]", ")", "\n", "repelMask", "=", "(", "ijDistSq", "<", "(", "self", ".", "data", ".", "repelDist", "**", "2", ")", ")", ".", "astype", "(", "ijDiffPos", ".", "dtype", ")", "\n", "ijDiffPos", "=", "ijDiffPos", "*", "np", ".", "expand_dims", "(", "repelMask", ",", "1", ")", "\n", "ijDistSqInv", "=", "invertTensorEW", "(", "ijDistSq", ")", "\n", "ijDistSqInv", "=", "np", ".", "expand_dims", "(", "ijDistSqInv", ",", "1", ")", "\n", "thisAccel", "=", "-", "np", ".", "sum", "(", "ijDiffVel", ",", "axis", "=", "3", ")", "+", "2", "*", "np", ".", "sum", "(", "ijDiffPos", "*", "(", "ijDistSqInv", "**", "2", "+", "ijDistSqInv", ")", ",", "\n", "axis", "=", "3", ")", "\n", "", "else", ":", "\n", "\n", "# Compute the learned acceleration", "\n", "#   Add the sample dimension", "\n", "                    ", "xThis", "=", "np", ".", "expand_dims", "(", "xDAG", "[", "s", ",", "0", ":", "t", ",", ":", ",", ":", "]", ",", "0", ")", "\n", "Sthis", "=", "np", ".", "expand_dims", "(", "SDAG", "[", "s", ",", "0", ":", "t", ",", ":", ",", ":", "]", ",", "0", ")", "\n", "#   Convert to tensor", "\n", "xThis", "=", "torch", ".", "tensor", "(", "xThis", ",", "device", "=", "self", ".", "model", ".", "device", ")", "\n", "Sthis", "=", "torch", ".", "tensor", "(", "Sthis", ",", "device", "=", "self", ".", "model", ".", "device", ")", "\n", "#   Compute the acceleration", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "thisAccel", "=", "self", ".", "model", ".", "archit", "(", "xThis", ",", "Sthis", ")", "\n", "#   Get only the last acceleration", "\n", "", "thisAccel", "=", "thisAccel", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "-", "1", ",", ":", ",", ":", "]", "\n", "\n", "# Cap the acceleration", "\n", "", "thisAccel", "[", "thisAccel", ">", "self", ".", "data", ".", "accelMax", "]", "=", "self", ".", "data", ".", "accelMax", "\n", "thisAccel", "[", "thisAccel", "<", "-", "self", ".", "data", ".", "accelMax", "]", "=", "-", "self", ".", "data", ".", "accelMax", "\n", "# Save it", "\n", "yDAG", "[", "s", ",", "t", "-", "1", ",", ":", ",", ":", "]", "=", "thisAccel", ".", "squeeze", "(", "0", ")", "\n", "\n", "# Update the position and velocity", "\n", "velDAG", "[", "s", ",", "t", ",", ":", ",", ":", "]", "=", "yDAG", "[", "s", ",", "t", "-", "1", ",", ":", ",", ":", "]", "*", "self", ".", "data", ".", "samplingTime", "+", "velDAG", "[", "s", ",", "t", "-", "1", ",", ":", ",", ":", "]", "\n", "posDAG", "[", "s", ",", "t", ",", ":", ",", ":", "]", "=", "velDAG", "[", "s", ",", "t", "-", "1", ",", ":", ",", ":", "]", "*", "self", ".", "data", ".", "samplingTime", "+", "posDAG", "[", "s", ",", "t", "-", "1", ",", ":", ",", ":", "]", "\n", "# Update the state and the graph", "\n", "thisGraph", "=", "self", ".", "data", ".", "computeCommunicationGraph", "(", "\n", "posDAG", "[", "s", ":", "s", "+", "1", ",", "t", ":", "t", "+", "1", ",", ":", ",", ":", "]", ",", "self", ".", "data", ".", "commRadius", ",", "\n", "True", ",", "doPrint", "=", "False", ")", "\n", "SDAG", "[", "s", ",", "t", ",", ":", ",", ":", "]", "=", "thisGraph", ".", "squeeze", "(", "1", ")", ".", "squeeze", "(", "0", ")", "\n", "thisState", "=", "self", ".", "data", ".", "computeStates", "(", "\n", "posDAG", "[", "s", ":", "s", "+", "1", ",", "t", ":", "t", "+", "1", ",", ":", ",", ":", "]", ",", "\n", "velDAG", "[", "s", ":", "s", "+", "1", ",", "t", ":", "t", "+", "1", ",", ":", ",", ":", "]", ",", "\n", "SDAG", "[", "s", ":", "s", "+", "1", ",", "t", ":", "t", "+", "1", ",", ":", ",", ":", "]", ",", "\n", "doPrint", "=", "False", ")", "\n", "xDAG", "[", "s", ",", "t", ",", ":", ",", ":", "]", "=", "thisState", ".", "squeeze", "(", "1", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "# And now compute the last acceleration step", "\n", "\n", "", "if", "np", ".", "random", ".", "binomial", "(", "1", ",", "chooseExpertProb", ")", "==", "1", ":", "\n", "\n", "# Compute the optimal acceleration", "\n", "                ", "ijDiffPos", ",", "ijDistSq", "=", "self", ".", "data", ".", "computeDifferences", "(", "posDAG", "[", "s", ":", "s", "+", "1", ",", "-", "1", ",", ":", ",", ":", "]", ")", "\n", "ijDiffVel", ",", "_", "=", "self", ".", "data", ".", "computeDifferences", "(", "velDAG", "[", "s", ":", "s", "+", "1", ",", "-", "1", ",", ":", ",", ":", "]", ")", "\n", "repelMask", "=", "(", "ijDistSq", "<", "(", "self", ".", "data", ".", "repelDist", "**", "2", ")", ")", ".", "astype", "(", "ijDiffPos", ".", "dtype", ")", "\n", "ijDiffPos", "=", "ijDiffPos", "*", "np", ".", "expand_dims", "(", "repelMask", ",", "1", ")", "\n", "ijDistSqInv", "=", "invertTensorEW", "(", "ijDistSq", ")", "\n", "ijDistSqInv", "=", "np", ".", "expand_dims", "(", "ijDistSqInv", ",", "1", ")", "\n", "thisAccel", "=", "-", "np", ".", "sum", "(", "ijDiffVel", ",", "axis", "=", "3", ")", "+", "2", "*", "np", ".", "sum", "(", "ijDiffPos", "*", "(", "ijDistSqInv", "**", "2", "+", "ijDistSqInv", ")", ",", "\n", "axis", "=", "3", ")", "\n", "", "else", ":", "\n", "\n", "# Compute the learned acceleration", "\n", "#   Add the sample dimension", "\n", "                ", "xThis", "=", "np", ".", "expand_dims", "(", "xDAG", "[", "s", "]", ",", "0", ")", "\n", "Sthis", "=", "np", ".", "expand_dims", "(", "SDAG", "[", "s", "]", ",", "0", ")", "\n", "#   Convert to tensor", "\n", "xThis", "=", "torch", ".", "tensor", "(", "xThis", ",", "device", "=", "self", ".", "model", ".", "device", ")", "\n", "Sthis", "=", "torch", ".", "tensor", "(", "Sthis", ",", "device", "=", "self", ".", "model", ".", "device", ")", "\n", "#   Compute the acceleration", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "thisAccel", "=", "self", ".", "model", ".", "archit", "(", "xThis", ",", "Sthis", ")", "\n", "#   Get only the last acceleration", "\n", "", "thisAccel", "=", "thisAccel", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "-", "1", ",", ":", ",", ":", "]", "\n", "\n", "# Cap the acceleration", "\n", "", "thisAccel", "[", "thisAccel", ">", "self", ".", "data", ".", "accelMax", "]", "=", "self", ".", "data", ".", "accelMax", "\n", "thisAccel", "[", "thisAccel", "<", "-", "self", ".", "data", ".", "accelMax", "]", "=", "-", "self", ".", "data", ".", "accelMax", "\n", "# Save it", "\n", "yDAG", "[", "s", ",", "-", "1", ",", ":", ",", ":", "]", "=", "thisAccel", ".", "squeeze", "(", "0", ")", "\n", "\n", "# And now that we have done this for all the samples in", "\n", "# the replacement set, just replace them", "\n", "\n", "", "xTrainAll", "[", "replaceIndices", "]", "=", "xDAG", "\n", "yTrainAll", "[", "replaceIndices", "]", "=", "yDAG", "\n", "StrainAll", "[", "replaceIndices", "]", "=", "SDAG", "\n", "\n", "return", "xTrainAll", ",", "yTrainAll", ",", "StrainAll", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.training.TrainerFlocking.fixedBatchDAGger": [[1578, 1698], ["training.TrainerFlocking.data.computeTrajectory", "training.TrainerFlocking.data.computeCommunicationGraph", "training.TrainerFlocking.data.computeStates", "numpy.zeros", "range", "training.TrainerFlocking.data.computeDifferences", "training.TrainerFlocking.data.computeDifferences", "alegnn.utils.dataTools.invertTensorEW", "numpy.expand_dims", "training.TrainerFlocking.data.computeDifferences", "training.TrainerFlocking.data.computeDifferences", "alegnn.utils.dataTools.invertTensorEW", "numpy.expand_dims", "numpy.expand_dims", "numpy.expand_dims", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeTrajectory", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeCommunicationGraph", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeStates", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeDifferences", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.invertTensorEW"], ["", "def", "fixedBatchDAGger", "(", "self", ",", "initPosTrain", ",", "initVelTrain", ")", ":", "\n", "\n", "# The 'fixedBatch' option, doubles the batch samples", "\n", "# by considering the same initial velocities and", "\n", "# positions, a trajectory given by the latest trained", "\n", "# architecture, and the corresponding correction", "\n", "# given by the optimal acceleration (i.e. for each", "\n", "# position and velocity we give what would be the", "\n", "# optimal acceleration, even though the next position", "\n", "# and velocity won't reflect this decision, but the", "\n", "# one taken by the learned policy)", "\n", "\n", "# Note that there's no point on doing it randomly here,", "\n", "# since the optimal trajectory is already considered in", "\n", "# the batch anyways.", "\n", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "#\\\\\\ Start DAGGER", "\n", "\n", "# Always apply DAGger on the trained policy", "\n", "        ", "posPol", ",", "velPol", ",", "_", ",", "_", ",", "_", "=", "self", ".", "data", ".", "computeTrajectory", "(", "initPosTrain", ",", "\n", "initVelTrain", ",", "\n", "self", ".", "data", ".", "duration", ",", "\n", "archit", "=", "self", ".", "model", ".", "archit", ",", "\n", "doPrint", "=", "False", ")", "\n", "\n", "# Compute the optimal acceleration on the trajectory given", "\n", "# by the trained policy", "\n", "\n", "maxTimeSamples", "=", "200", "\n", "\n", "if", "posPol", ".", "shape", "[", "1", "]", ">", "maxTimeSamples", ":", "\n", "\n", "# Create the space to store this", "\n", "            ", "yDAG", "=", "np", ".", "zeros", "(", "posPol", ".", "shape", ")", "\n", "\n", "for", "t", "in", "range", "(", "posPol", ".", "shape", "[", "1", "]", ")", ":", "\n", "\n", "# Compute the expert on the corresponding trajectory", "\n", "#   First, we need the difference in positions", "\n", "                ", "ijDiffPos", ",", "ijDistSq", "=", "self", ".", "data", ".", "computeDifferences", "(", "posPol", "[", ":", ",", "t", ",", ":", ",", ":", "]", ")", "\n", "#   And in velocities", "\n", "ijDiffVel", ",", "_", "=", "self", ".", "data", ".", "computeDifferences", "(", "velPol", "[", ":", ",", "t", ",", ":", ",", ":", "]", ")", "\n", "#   Now, the second term (the one that depends on", "\n", "#   the positions) only needs to be computed for", "\n", "#   nodes thatare within repel distance, so let's", "\n", "#   compute a mask to find these nodes.", "\n", "repelMask", "=", "(", "ijDistSq", "<", "(", "self", ".", "data", ".", "repelDist", "**", "2", ")", ")", ".", "astype", "(", "ijDiffPos", ".", "dtype", ")", "\n", "#   Apply this mask to the position difference (we", "\n", "#   need not apply it to the square differences,", "\n", "#   since these will be multiplied by the position", "\n", "#   differences which already will be zero)", "\n", "#   Note that we need to add the dimension of axis", "\n", "#   to properly multiply it", "\n", "ijDiffPos", "=", "ijDiffPos", "*", "np", ".", "expand_dims", "(", "repelMask", ",", "1", ")", "\n", "#   Invert the tensor elementwise (avoiding the", "\n", "#   zeros)", "\n", "ijDistSqInv", "=", "invertTensorEW", "(", "ijDistSq", ")", "\n", "#   Add an extra dimension, also across the axis", "\n", "ijDistSqInv", "=", "np", ".", "expand_dims", "(", "ijDistSqInv", ",", "1", ")", "\n", "#   Compute the optimal solution", "\n", "thisAccel", "=", "-", "np", ".", "sum", "(", "ijDiffVel", ",", "axis", "=", "3", ")", "+", "2", "*", "np", ".", "sum", "(", "ijDiffPos", "*", "(", "ijDistSqInv", "**", "2", "+", "ijDistSqInv", ")", ",", "\n", "axis", "=", "3", ")", "\n", "# And cap it", "\n", "thisAccel", "[", "thisAccel", ">", "self", ".", "data", ".", "accelMax", "]", "=", "self", ".", "data", ".", "accelMax", "\n", "thisAccel", "[", "thisAccel", "<", "-", "self", ".", "data", ".", "accelMax", "]", "=", "-", "self", ".", "data", ".", "accelMax", "\n", "\n", "# Store it", "\n", "yDAG", "[", ":", ",", "t", ",", ":", ",", ":", "]", "=", "thisAccel", "\n", "\n", "", "", "else", ":", "\n", "# Compute the expert on the corresponding trajectory", "\n", "#   First, we need the difference in positions", "\n", "            ", "ijDiffPos", ",", "ijDistSq", "=", "self", ".", "data", ".", "computeDifferences", "(", "posPol", ")", "\n", "#   And in velocities", "\n", "ijDiffVel", ",", "_", "=", "self", ".", "data", ".", "computeDifferences", "(", "velPol", ")", "\n", "#   Now, the second term (the one that depends on the", "\n", "#   positions) only needs to be computed for nodes that", "\n", "#   are within repel distance, so let's compute a mask", "\n", "#   to find these nodes.", "\n", "repelMask", "=", "(", "ijDistSq", "<", "(", "self", ".", "data", ".", "repelDist", "**", "2", ")", ")", ".", "astype", "(", "ijDiffPos", ".", "dtype", ")", "\n", "#   Apply this mask to the position difference (we need", "\n", "#   not apply it to the square differences, since these", "\n", "#   will be multiplied by the position differences,", "\n", "#   which already will be zero)", "\n", "#   Note that we need to add the dimension of axis to", "\n", "#   properly multiply it", "\n", "ijDiffPos", "=", "ijDiffPos", "*", "np", ".", "expand_dims", "(", "repelMask", ",", "2", ")", "\n", "#   Invert the tensor elementwise (avoiding the zeros)", "\n", "ijDistSqInv", "=", "invertTensorEW", "(", "ijDistSq", ")", "\n", "#   Add an extra dimension, also across the axis", "\n", "ijDistSqInv", "=", "np", ".", "expand_dims", "(", "ijDistSqInv", ",", "2", ")", "\n", "#   Compute the optimal solution", "\n", "yDAG", "=", "-", "np", ".", "sum", "(", "ijDiffVel", ",", "axis", "=", "4", ")", "+", "2", "*", "np", ".", "sum", "(", "ijDiffPos", "*", "(", "ijDistSqInv", "**", "2", "+", "ijDistSqInv", ")", ",", "\n", "axis", "=", "4", ")", "\n", "# And cap it", "\n", "yDAG", "[", "yDAG", ">", "self", ".", "data", ".", "accelMax", "]", "=", "self", ".", "data", ".", "accelMax", "\n", "yDAG", "[", "yDAG", "<", "-", "self", ".", "data", ".", "accelMax", "]", "=", "-", "self", ".", "data", ".", "accelMax", "\n", "\n", "# Finally, compute the corresponding graph of states", "\n", "# (pos) visited by the policy", "\n", "", "graphDAG", "=", "self", ".", "data", ".", "computeCommunicationGraph", "(", "posPol", ",", "\n", "self", ".", "data", ".", "commRadius", ",", "\n", "True", ",", "\n", "doPrint", "=", "False", ")", "\n", "xDAG", "=", "self", ".", "data", ".", "computeStates", "(", "posPol", ",", "velPol", ",", "graphDAG", ",", "\n", "doPrint", "=", "False", ")", "\n", "\n", "# Add it to the existing batch", "\n", "\n", "return", "xDAG", ",", "yDAG", ",", "graphDAG", "", "", "", ""]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SelectionGNN.__init__": [[166, 320], ["torch.Module.__init__", "len", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "scipy.sparse.csr_matrix.reshape", "eval", "scipy.sparse.csr_matrix", "alegnn.utils.graphTools.coarsen", "alegnn.utils.graphTools.coarsen", "alegnn.utils.graphTools.coarsen", "alegnn.utils.graphTools.coarsen", "architectures.SelectionGNN.permFunction", "gfl.append", "gfl.append", "len", "fc.append", "range", "len", "len", "len", "S.todense().A.reshape.todense().A.reshape.todense().A.reshape", "architectures.SelectionGNN.S.append", "architectures.SelectionGNN.N.append", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "alegnn.GraphFilter", "alegnn.GraphFilter", "gfl[].addGSO", "gfl[].addGSO", "architectures.SelectionGNN.sigma", "gfl.append", "gfl.append", "gfl[].addGSO", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "architectures.SelectionGNN.rho", "architectures.SelectionGNN.rho", "len", "architectures.SelectionGNN.sigma", "torch.Linear", "torch.Linear", "S.todense().A.reshape.todense().A.reshape.todense"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.coarsen", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.coarsen", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.coarsen", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.coarsen", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nFilterTaps", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Structure", "\n", "GSO", ",", "\n", "# Ordering", "\n", "order", "=", "None", ",", "\n", "# Coarsening", "\n", "coarsening", "=", "False", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nFilterTaps", ")", "+", "1", "\n", "# nSelectedNodes should be a list of size nFilterTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nFilterTaps", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nFilterTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nFilterTaps", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "\n", "", "self", ".", "coarsening", "=", "coarsening", "# Whether to do coarsening or not", "\n", "# If we have to do coarsening, then note that it can only be done if", "\n", "# we have a single edge feature, otherwise, each edge feature could be", "\n", "# coarsed (and thus, ordered) in a different way, and there is no", "\n", "# sensible way of merging back this different orderings. So, we will", "\n", "# only do coarsening if we have a single edge feature; otherwise, we", "\n", "# will default to selection sampling (therefore, always specify", "\n", "# nSelectedNodes)", "\n", "if", "self", ".", "coarsening", "and", "self", ".", "E", "==", "1", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permCoarsening", "# Override", "\n", "# permutation function for the one corresponding to coarsening", "\n", "GSO", "=", "scipy", ".", "sparse", ".", "csr_matrix", "(", "GSO", "[", "0", "]", ")", "\n", "GSO", ",", "self", ".", "order", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "coarsen", "(", "GSO", ",", "levels", "=", "self", ".", "L", ",", "\n", "self_connections", "=", "False", ")", "\n", "# Now, GSO is a list of csr_matrix with self.L+1 coarsened GSOs,", "\n", "# we need to torch.tensor them and put them in a list.", "\n", "# order is just a list of indices to reorder the nodes.", "\n", "self", ".", "S", "=", "[", "]", "\n", "self", ".", "N", "=", "[", "]", "# It has to be reset, because now the number of", "\n", "# nodes is determined by the coarsening scheme", "\n", "for", "S", "in", "GSO", ":", "\n", "                ", "S", "=", "S", ".", "todense", "(", ")", ".", "A", ".", "reshape", "(", "[", "self", ".", "E", ",", "S", ".", "shape", "[", "0", "]", ",", "S", ".", "shape", "[", "1", "]", "]", ")", "\n", "# So, S.todense() returns a numpy.matrix object; a numpy", "\n", "# matrix cannot be converted into a tensor (i.e., added", "\n", "# the third dimension), therefore we need to convert it to", "\n", "# a numpy.array. According to the documentation, the ", "\n", "# attribute .A in a numpy.matrix returns self as an ndarray", "\n", "# object. So that's why the .A is there.", "\n", "self", ".", "S", ".", "append", "(", "torch", ".", "tensor", "(", "S", ")", ")", "\n", "self", ".", "N", ".", "append", "(", "S", ".", "shape", "[", "1", "]", ")", "\n", "# Finally, because the graph coarsening algorithm is a binary tree", "\n", "# pooling, we always need to force a pooling size of 2", "\n", "", "self", ".", "alpha", "=", "[", "2", "]", "*", "self", ".", "L", "\n", "", "else", ":", "\n", "# Call the corresponding ordering function. Recall that if no", "\n", "# order was selected, then this is permIdentity, so that nothing", "\n", "# changes.", "\n", "            ", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "                ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "coarsening", "=", "False", "# If it failed because there are more than", "\n", "# one edge feature, then just set this to false, so we do not", "\n", "# need to keep checking whether self.E == 1 or not, just this", "\n", "# one", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "# Store the rest of the variables", "\n", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "gfl", "=", "[", "]", "# Graph Filtering Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "gfl", ".", "append", "(", "gml", ".", "GraphFilter", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "self", ".", "K", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ")", ")", "\n", "# There is a 3*l below here, because we have three elements per", "\n", "# layer: graph filter, nonlinearity and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "if", "self", ".", "coarsening", ":", "\n", "                ", "gfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", "[", "l", "]", ")", "\n", "", "else", ":", "\n", "                ", "gfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Nonlinearity", "\n", "", "gfl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "if", "self", ".", "coarsening", ":", "\n", "                ", "gfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "gfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 3*l+2", "\n", "gfl", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "", "self", ".", "GFL", "=", "nn", ".", "Sequential", "(", "*", "gfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "            ", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SelectionGNN.changeGSO": [[322, 421], ["len", "scipy.sparse.csr_matrix.reshape", "dir", "architectures.SelectionGNN.permFunction", "alegnn.utils.dataTools.changeDataType", "alegnn.utils.dataTools.changeDataType", "range", "scipy.sparse.csr_matrix", "alegnn.utils.graphTools.coarsen", "alegnn.utils.graphTools.coarsen", "alegnn.utils.graphTools.coarsen", "alegnn.utils.graphTools.coarsen", "range", "range", "len", "len", "architectures.SelectionGNN.S.to", "len", "len", "len", "len", "architectures.SelectionGNN.rho", "architectures.SelectionGNN.GFL[].addGSO", "range", "S.todense().A.reshape.todense().A.reshape.todense().A.reshape", "architectures.SelectionGNN.S.append", "architectures.SelectionGNN.N.append", "architectures.SelectionGNN.GFL[].addGSO", "architectures.SelectionGNN.GFL[].addGSO", "len", "architectures.SelectionGNN.GFL[].addGSO", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "S.todense().A.reshape.todense().A.reshape.todense", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.coarsen", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.coarsen", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.coarsen", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.coarsen", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "changeGSO", "(", "self", ",", "GSO", ",", "nSelectedNodes", "=", "[", "]", ",", "poolingSize", "=", "[", "]", ")", ":", "\n", "\n", "# We use this to change the GSO, using the same graph filters.", "\n", "\n", "# Check that the new GSO has the correct", "\n", "        ", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "\n", "# Get dataType and device of the current GSO, so when we replace it, it", "\n", "# is still located in the same type and the same device.", "\n", "", "dataType", "=", "self", ".", "S", ".", "dtype", "\n", "if", "'device'", "in", "dir", "(", "self", ".", "S", ")", ":", "\n", "            ", "device", "=", "self", ".", "S", ".", "device", "\n", "", "else", ":", "\n", "            ", "device", "=", "None", "\n", "\n", "# Now, if we don't have coarsening, then we need to reorder the GSO,", "\n", "# and since this GSO reordering will affect several parts of the non", "\n", "# coarsening algorithm, then we will do it now", "\n", "# Reorder the GSO", "\n", "", "if", "not", "self", ".", "coarsening", ":", "\n", "            ", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "# Change data type and device as required", "\n", "self", ".", "S", "=", "changeDataType", "(", "self", ".", "S", ",", "dataType", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "                ", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "\n", "# Before making decisions, check if there is a new poolingSize list", "\n", "", "", "if", "len", "(", "poolingSize", ")", ">", "0", "and", "not", "self", ".", "coarsening", ":", "\n", "# (If it's coarsening, then the pooling size cannot change)", "\n", "# Check it has the right length", "\n", "            ", "assert", "len", "(", "poolingSize", ")", "==", "self", ".", "L", "\n", "# And update it", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "\n", "# Now, check if we have a new list of nodes (this only makes sense", "\n", "# if there is no coarsening, because if it is coarsening, the list with", "\n", "# the number of nodes to be considered is ignored.)", "\n", "", "if", "len", "(", "nSelectedNodes", ")", ">", "0", "and", "not", "self", ".", "coarsening", ":", "\n", "# If we do, then we need to change the pooling functions to select", "\n", "# less nodes. This would allow to use graphs of different size.", "\n", "# Note that the pooling function, there is nothing learnable, so", "\n", "# they can easily be re-made, re-initialized.", "\n", "# The first thing we need to check, is that the length of the", "\n", "# number of nodes is equal to the number of layers (this list ", "\n", "# indicates the number of nodes selected at the output of each", "\n", "# layer)", "\n", "            ", "assert", "len", "(", "nSelectedNodes", ")", "==", "self", ".", "L", "\n", "# Then, update the N that we have stored", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "\n", "# And get the new pooling functions", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "# For each layer, add the pooling function", "\n", "                ", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", "=", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "alpha", "[", "l", "]", ")", "\n", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "", "elif", "len", "(", "nSelectedNodes", ")", "==", "0", "and", "not", "self", ".", "coarsening", ":", "\n", "# Just update the GSO", "\n", "            ", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n", "# If it's coarsening, then we need to compute the new coarsening", "\n", "# scheme", "\n", "", "", "if", "self", ".", "coarsening", "and", "self", ".", "E", "==", "1", ":", "\n", "            ", "device", "=", "self", ".", "S", "[", "0", "]", ".", "device", "\n", "GSO", "=", "scipy", ".", "sparse", ".", "csr_matrix", "(", "GSO", "[", "0", "]", ")", "\n", "GSO", ",", "self", ".", "order", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "coarsen", "(", "GSO", ",", "levels", "=", "self", ".", "L", ",", "\n", "self_connections", "=", "False", ")", "\n", "# Now, GSO is a list of csr_matrix with self.L+1 coarsened GSOs,", "\n", "# we need to torch.tensor them and put them in a list.", "\n", "# order is just a list of indices to reorder the nodes.", "\n", "self", ".", "S", "=", "[", "]", "\n", "self", ".", "N", "=", "[", "]", "# It has to be reset, because now the number of", "\n", "# nodes is determined by the coarsening scheme", "\n", "for", "S", "in", "GSO", ":", "\n", "                ", "S", "=", "S", ".", "todense", "(", ")", ".", "A", ".", "reshape", "(", "[", "self", ".", "E", ",", "S", ".", "shape", "[", "0", "]", ",", "S", ".", "shape", "[", "1", "]", "]", ")", "\n", "# So, S.todense() returns a numpy.matrix object; a numpy", "\n", "# matrix cannot be converted into a tensor (i.e., added", "\n", "# the third dimension), therefore we need to convert it to", "\n", "# a numpy.array. According to the documentation, the ", "\n", "# attribute .A in a numpy.matrix returns self as an ndarray", "\n", "# object. So that's why the .A is there.", "\n", "self", ".", "S", ".", "append", "(", "torch", ".", "tensor", "(", "S", ")", ".", "to", "(", "device", ")", ")", "\n", "self", ".", "N", ".", "append", "(", "S", ".", "shape", "[", "1", "]", ")", "\n", "# And we need to update the GSO in all the places.", "\n", "#   Note that we do not need to change the pooling function, because", "\n", "#   it is the standard pooling function that doesn't care about the", "\n", "#   number of nodes: it still takes one every two of them.", "\n", "", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "GFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", "[", "l", "]", ")", "# Graph convolutional layer", "\n", "", "", "else", ":", "\n", "# And update in the LSIGF that is still missing (recall that the", "\n", "# ordering for the non-coarsening case has already been done)", "\n", "            ", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "GFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "# Graph convolutional layer", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SelectionGNN.splitForward": [[422, 450], ["architectures.SelectionGNN.GFL", "architectures.SelectionGNN.reshape", "torch.tensor().to.cpu().numpy", "torch.tensor().to.cpu().numpy", "alegnn.utils.graphTools.permCoarsening", "alegnn.utils.graphTools.permCoarsening", "alegnn.utils.graphTools.permCoarsening", "alegnn.utils.graphTools.permCoarsening", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "len", "architectures.SelectionGNN.MLP", "torch.tensor().to.cpu", "torch.tensor().to.cpu", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.permCoarsening", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.permCoarsening", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.permCoarsening", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphTools.permCoarsening", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "", "", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Reorder the nodes from the data", "\n", "# If we have added dummy nodes (which, has to happen when the size", "\n", "# is different and we chose coarsening), then we need to use the", "\n", "# provided permCoarsening function (which acts on data to add dummy", "\n", "# variables)", "\n", "        ", "if", "x", ".", "shape", "[", "2", "]", "!=", "self", ".", "N", "[", "0", "]", "and", "self", ".", "coarsening", ":", "\n", "            ", "thisDevice", "=", "x", ".", "device", "# Save the device we where operating on", "\n", "x", "=", "x", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "# Convert to numpy", "\n", "x", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permCoarsening", "(", "x", ",", "self", ".", "order", ")", "\n", "# Re order and add dummy values", "\n", "x", "=", "torch", ".", "tensor", "(", "x", ")", ".", "to", "(", "thisDevice", ")", "\n", "", "else", ":", "\n", "# If not, simply reorder the nodes", "\n", "            ", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "\n", "\n", "# Now we compute the forward call", "\n", "", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Let's call the graph filtering layer", "\n", "y", "=", "self", ".", "GFL", "(", "x", ")", "\n", "# Flatten the output", "\n", "yFlat", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "yFlat", ")", ",", "y", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SelectionGNN.forward": [[452, 462], ["architectures.SelectionGNN.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SelectionGNN.to": [[463, 480], ["super().to", "range", "architectures.SelectionGNN.S.to", "range", "architectures.SelectionGNN.S[].to", "architectures.SelectionGNN.GFL[].addGSO", "architectures.SelectionGNN.GFL[].addGSO", "architectures.SelectionGNN.GFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "if", "self", ".", "coarsening", ":", "\n", "            ", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "S", "[", "l", "]", "=", "self", ".", "S", "[", "l", "]", ".", "to", "(", "device", ")", "\n", "self", ".", "GFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", "[", "l", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "GFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalActivationGNN.__init__": [[591, 704], ["torch.Module.__init__", "len", "architectures.LocalActivationGNN.permFunction", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "gfl.append", "gfl[].addGSO", "gfl.append", "gfl[].addGSO", "gfl.append", "gfl[].addGSO", "len", "fc.append", "range", "len", "len", "len", "alegnn.GraphFilter", "alegnn.GraphFilter", "architectures.LocalActivationGNN.sigma", "architectures.LocalActivationGNN.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architectures.LocalActivationGNN.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nFilterTaps", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "kHopActivation", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nFilterTaps", ")", "+", "1", "\n", "# kHopActivation is a list with the same number of elements as ", "\n", "# nFilterTaps (number of layers)", "\n", "assert", "len", "(", "kHopActivation", ")", "==", "len", "(", "nFilterTaps", ")", "\n", "# nSelectedNodes should be a list of size nFilterTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nFilterTaps", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nFilterTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nFilterTaps", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "kHop", "=", "kHopActivation", "# k-hop neighborhood for local activation", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "\n", "# Call the corresponding ordering function. Recall that if no", "\n", "# order was selected, then this is permIdentity, so that nothing", "\n", "# changes.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "# Store the rest of the variables", "\n", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "gfl", "=", "[", "]", "# Graph Filtering Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "gfl", ".", "append", "(", "gml", ".", "GraphFilter", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "self", ".", "K", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ")", ")", "\n", "# There is a 3*l below here, because we have three elements per", "\n", "# layer: graph filter, nonlinearity and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "gfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Nonlinearity", "\n", "gfl", ".", "append", "(", "self", ".", "sigma", "(", "self", ".", "kHop", "[", "l", "]", ")", ")", "\n", "# Add GSO for this layer", "\n", "gfl", "[", "3", "*", "l", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 3*l+2", "\n", "gfl", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "GFL", "=", "nn", ".", "Sequential", "(", "*", "gfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "            ", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalActivationGNN.changeGSO": [[706, 773], ["architectures.LocalActivationGNN.permFunction", "alegnn.utils.dataTools.changeDataType", "alegnn.utils.dataTools.changeDataType", "range", "len", "GSO.reshape.reshape.reshape", "dir", "architectures.LocalActivationGNN.S.to", "len", "len", "range", "architectures.LocalActivationGNN.GFL[].addGSO", "architectures.LocalActivationGNN.GFL[].addGSO", "len", "len", "len", "len", "architectures.LocalActivationGNN.rho", "architectures.LocalActivationGNN.GFL[].addGSO", "range", "len", "architectures.LocalActivationGNN.GFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "changeGSO", "(", "self", ",", "GSO", ",", "nSelectedNodes", "=", "[", "]", ",", "poolingSize", "=", "[", "]", ")", ":", "\n", "\n", "# We use this to change the GSO, using the same graph filters.", "\n", "\n", "# Check that the new GSO has the correct", "\n", "        ", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "\n", "# Get dataType and device of the current GSO, so when we replace it, it", "\n", "# is still located in the same type and the same device.", "\n", "", "dataType", "=", "self", ".", "S", ".", "dtype", "\n", "if", "'device'", "in", "dir", "(", "self", ".", "S", ")", ":", "\n", "            ", "device", "=", "self", ".", "S", ".", "device", "\n", "", "else", ":", "\n", "            ", "device", "=", "None", "\n", "\n", "# Reorder the new GSO", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "# Change data type and device as required", "\n", "self", ".", "S", "=", "changeDataType", "(", "self", ".", "S", ",", "dataType", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "\n", "# Before making decisions, check if there is a new poolingSize list", "\n", "", "if", "len", "(", "poolingSize", ")", ">", "0", ":", "\n", "# (If it's coarsening, then the pooling size cannot change)", "\n", "# Check it has the right length", "\n", "            ", "assert", "len", "(", "poolingSize", ")", "==", "self", ".", "L", "\n", "# And update it", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "\n", "# Now, check if we have a new list of nodes (this only makes sense", "\n", "# if there is no coarsening, because if it is coarsening, the list with", "\n", "# the number of nodes to be considered is ignored.)", "\n", "", "if", "len", "(", "nSelectedNodes", ")", ">", "0", ":", "\n", "# If we do, then we need to change the pooling functions to select", "\n", "# less nodes. This would allow to use graphs of different size.", "\n", "# Note that the pooling function, there is nothing learnable, so", "\n", "# they can easily be re-made, re-initialized.", "\n", "# The first thing we need to check, is that the length of the", "\n", "# number of nodes is equal to the number of layers (this list ", "\n", "# indicates the number of nodes selected at the output of each", "\n", "# layer)", "\n", "            ", "assert", "len", "(", "nSelectedNodes", ")", "==", "self", ".", "L", "\n", "# Then, update the N that we have stored", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "\n", "# And get the new pooling functions", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "# For each layer, add the pooling function", "\n", "                ", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", "=", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "alpha", "[", "l", "]", ")", "\n", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "", "elif", "len", "(", "nSelectedNodes", ")", "==", "0", "and", "not", "self", ".", "coarsening", ":", "\n", "# Just update the GSO", "\n", "            ", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n", "\n", "# And update in the LSIGF that is still missing (recall that the", "\n", "# ordering for the non-coarsening case has already been done)", "\n", "", "", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "GFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "# Graph convolutional layer", "\n", "self", ".", "GFL", "[", "3", "*", "l", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "# Local Activation function", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalActivationGNN.splitForward": [[774, 789], ["architectures.LocalActivationGNN.GFL", "architectures.LocalActivationGNN.reshape", "len", "architectures.LocalActivationGNN.MLP"], "methods", ["None"], ["", "", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph filtering layer", "\n", "y", "=", "self", ".", "GFL", "(", "x", ")", "\n", "# Flatten the output", "\n", "yFlat", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "yFlat", ")", ",", "y", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalActivationGNN.forward": [[791, 801], ["architectures.LocalActivationGNN.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalActivationGNN.to": [[802, 815], ["super().to", "architectures.LocalActivationGNN.S.to", "range", "architectures.LocalActivationGNN.GFL[].addGSO", "architectures.LocalActivationGNN.GFL[].addGSO", "architectures.LocalActivationGNN.GFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "GFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "GFL", "[", "3", "*", "l", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalGNN.__init__": [[925, 1024], ["torch.Module.__init__", "len", "architectures.LocalGNN.permFunction", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "gfl.append", "gfl[].addGSO", "gfl.append", "gfl.append", "gfl[].addGSO", "len", "fc.append", "range", "len", "len", "len", "alegnn.GraphFilter", "alegnn.GraphFilter", "architectures.LocalGNN.sigma", "architectures.LocalGNN.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architectures.LocalGNN.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nFilterTaps", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimReadout", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nFilterTaps", ")", "+", "1", "\n", "# nSelectedNodes should be a list of size nFilterTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nFilterTaps", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nFilterTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nFilterTaps", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "# Store the rest of the variables", "\n", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "dimReadout", "=", "dimReadout", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "gfl", "=", "[", "]", "# Graph Filtering Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "gfl", ".", "append", "(", "gml", ".", "GraphFilter", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "self", ".", "K", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ")", ")", "\n", "# There is a 3*l below here, because we have three elements per", "\n", "# layer: graph filter, nonlinearity and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "gfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Nonlinearity", "\n", "gfl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "gfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 3*l+2", "\n", "gfl", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "GFL", "=", "nn", ".", "Sequential", "(", "*", "gfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimReadout", ")", ">", "0", ":", "# Maybe we don't want to readout anything", "\n", "# The first layer has to connect whatever was left of the graph ", "\n", "# filtering stage to create the number of features required by", "\n", "# the readout layer", "\n", "            ", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "F", "[", "-", "1", "]", ",", "dimReadout", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimReadout", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimReadout", "[", "l", "]", ",", "dimReadout", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "Readout", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalGNN.changeGSO": [[1026, 1087], ["architectures.LocalGNN.permFunction", "alegnn.utils.dataTools.changeDataType", "alegnn.utils.dataTools.changeDataType", "range", "len", "GSO.reshape.reshape.reshape", "dir", "architectures.LocalGNN.S.to", "len", "len", "range", "range", "architectures.LocalGNN.GFL[].addGSO", "len", "len", "len", "len", "architectures.LocalGNN.rho", "architectures.LocalGNN.GFL[].addGSO", "architectures.LocalGNN.GFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "changeGSO", "(", "self", ",", "GSO", ",", "nSelectedNodes", "=", "[", "]", ",", "poolingSize", "=", "[", "]", ")", ":", "\n", "\n", "# We use this to change the GSO, using the same graph filters.", "\n", "\n", "# Check that the new GSO has the correct", "\n", "        ", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "\n", "# Get dataType and device of the current GSO, so when we replace it, it", "\n", "# is still located in the same type and the same device.", "\n", "", "dataType", "=", "self", ".", "S", ".", "dtype", "\n", "if", "'device'", "in", "dir", "(", "self", ".", "S", ")", ":", "\n", "            ", "device", "=", "self", ".", "S", ".", "device", "\n", "", "else", ":", "\n", "            ", "device", "=", "None", "\n", "\n", "# Reorder the new GSO", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "# Change data type and device as required", "\n", "self", ".", "S", "=", "changeDataType", "(", "self", ".", "S", ",", "dataType", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "\n", "# Before making decisions, check if there is a new poolingSize list", "\n", "", "if", "len", "(", "poolingSize", ")", ">", "0", ":", "\n", "# Check it has the right length", "\n", "            ", "assert", "len", "(", "poolingSize", ")", "==", "self", ".", "L", "\n", "# And update it", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "\n", "# Now, check if we have a new list of nodes", "\n", "", "if", "len", "(", "nSelectedNodes", ")", ">", "0", ":", "\n", "# If we do, then we need to change the pooling functions to select", "\n", "# less nodes. This would allow to use graphs of different size.", "\n", "# Note that the pooling function, there is nothing learnable, so", "\n", "# they can easily be re-made, re-initialized.", "\n", "# The first thing we need to check, is that the length of the", "\n", "# number of nodes is equal to the number of layers (this list ", "\n", "# indicates the number of nodes selected at the output of each", "\n", "# layer)", "\n", "            ", "assert", "len", "(", "nSelectedNodes", ")", "==", "self", ".", "L", "\n", "# Then, update the N that we have stored", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "\n", "# And get the new pooling functions", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "# For each layer, add the pooling function", "\n", "                ", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", "=", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "alpha", "[", "l", "]", ")", "\n", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "", "else", ":", "\n", "# Just update the GSO", "\n", "            ", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n", "# And update in the LSIGF that is still missing", "\n", "", "", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "GFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "# Graph convolutional layer", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalGNN.splitForward": [[1088, 1104], ["architectures.LocalGNN.GFL", "architectures.LocalGNN.permute", "architectures.LocalGNN.Readout", "len", "architectures.LocalGNN.permute"], "methods", ["None"], ["", "", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph filtering layer", "\n", "yGFL", "=", "self", ".", "GFL", "(", "x", ")", "\n", "# Change the order, for the readout", "\n", "y", "=", "yGFL", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# B x N[-1] x F[-1]", "\n", "# And, feed it into the Readout layer", "\n", "y", "=", "self", ".", "Readout", "(", "y", ")", "# B x N[-1] x dimReadout[-1]", "\n", "# Reshape and return", "\n", "return", "y", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "yGFL", "\n", "# B x dimReadout[-1] x N[-1], B x dimFeatures[-1] x N[-1]", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalGNN.forward": [[1106, 1116], ["architectures.LocalGNN.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalGNN.singleNodeForward": [[1117, 1171], ["numpy.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "architectures.LocalGNN.forward", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.squeeze", "torch.matmul.squeeze", "type", "architectures.LocalGNN.order.index", "numpy.array", "numpy.tile", "type", "numpy.array", "type", "type", "type", "architectures.LocalGNN.order.index", "type", "numpy.array", "numpy.array.astype", "numpy.arange", "numpy.where", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.forward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "singleNodeForward", "(", "self", ",", "x", ",", "nodes", ")", ":", "\n", "\n", "# x is of shape B x F[0] x N[-1]", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "# nodes is either an int, or a list/np.array of ints of size B", "\n", "assert", "type", "(", "nodes", ")", "is", "int", "or", "type", "(", "nodes", ")", "is", "list", "or", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", "\n", "\n", "# Let us start by building the selection matrix", "\n", "# This selection matrix has to be a matrix of shape", "\n", "#   B x N[-1] x 1", "\n", "# so that when multiplying with the output of the forward, we get a", "\n", "#   B x dimRedout[-1] x 1", "\n", "# and we just squeeze the last dimension", "\n", "\n", "# TODO: The big question here is if multiplying by a matrix is faster", "\n", "# than doing torch.index_select", "\n", "\n", "# Let's always work with numpy arrays to make it easier.", "\n", "if", "type", "(", "nodes", ")", "is", "int", ":", "\n", "# Change the node number to accommodate the new order", "\n", "            ", "nodes", "=", "self", ".", "order", ".", "index", "(", "nodes", ")", "\n", "# If it's int, make it a list and an array", "\n", "nodes", "=", "np", ".", "array", "(", "[", "nodes", "]", ",", "dtype", "=", "np", ".", "int", ")", "\n", "# And repeat for the number of batches", "\n", "nodes", "=", "np", ".", "tile", "(", "nodes", ",", "batchSize", ")", "\n", "", "if", "type", "(", "nodes", ")", "is", "list", ":", "\n", "            ", "newNodes", "=", "[", "self", ".", "order", ".", "index", "(", "n", ")", "for", "n", "in", "nodes", "]", "\n", "nodes", "=", "np", ".", "array", "(", "newNodes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "", "elif", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", ":", "\n", "            ", "newNodes", "=", "np", ".", "array", "(", "[", "np", ".", "where", "(", "np", ".", "array", "(", "self", ".", "order", ")", "==", "n", ")", "[", "0", "]", "[", "0", "]", "for", "n", "in", "nodes", "]", ")", "\n", "nodes", "=", "newNodes", ".", "astype", "(", "np", ".", "int", ")", "\n", "# Now, nodes is an np.int np.ndarray with shape batchSize", "\n", "\n", "# Build the selection matrix", "\n", "", "selectionMatrix", "=", "np", ".", "zeros", "(", "[", "batchSize", ",", "self", ".", "N", "[", "-", "1", "]", ",", "1", "]", ")", "\n", "selectionMatrix", "[", "np", ".", "arange", "(", "batchSize", ")", ",", "nodes", ",", "0", "]", "=", "1.", "\n", "# And convert it to a tensor", "\n", "selectionMatrix", "=", "torch", ".", "tensor", "(", "selectionMatrix", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Now compute the output", "\n", "y", "=", "self", ".", "forward", "(", "x", ")", "\n", "# This output is of size B x dimReadout[-1] x N[-1]", "\n", "\n", "# Multiply the output", "\n", "y", "=", "torch", ".", "matmul", "(", "y", ",", "selectionMatrix", ")", "\n", "#   B x dimReadout[-1] x 1", "\n", "\n", "# Squeeze the last dimension and return", "\n", "return", "y", ".", "squeeze", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalGNN.to": [[1172, 1184], ["super().to", "architectures.LocalGNN.S.to", "range", "architectures.LocalGNN.GFL[].addGSO", "architectures.LocalGNN.GFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "GFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "GFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SpectralGNN.__init__": [[1280, 1381], ["torch.Module.__init__", "len", "architectures.SpectralGNN.permFunction", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "sgfl.append", "sgfl[].addGSO", "sgfl.append", "sgfl.append", "sgfl[].addGSO", "len", "fc.append", "range", "len", "len", "len", "alegnn.SpectralGF", "alegnn.SpectralGF", "architectures.SpectralGNN.sigma", "architectures.SpectralGNN.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architectures.SpectralGNN.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nCoeff", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nCoeff", ")", "+", "1", "\n", "# nSelectedNodes should be a list of size nFilterTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nCoeff", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nCoeff", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nCoeff", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "M", "=", "nCoeff", "# Filter taps", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "sgfl", "=", "[", "]", "# Graph Filtering Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "sgfl", ".", "append", "(", "gml", ".", "SpectralGF", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "self", ".", "M", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ")", ")", "\n", "# There is a 3*l below here, because we have three elements per", "\n", "# layer: graph filter, nonlinearity and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "sgfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Nonlinearity", "\n", "sgfl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "sgfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 3*l+2", "\n", "sgfl", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "SGFL", "=", "nn", ".", "Sequential", "(", "*", "sgfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "            ", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SpectralGNN.changeGSO": [[1383, 1444], ["architectures.SpectralGNN.permFunction", "alegnn.utils.dataTools.changeDataType", "alegnn.utils.dataTools.changeDataType", "range", "len", "GSO.reshape.reshape.reshape", "dir", "architectures.SpectralGNN.S.to", "len", "len", "range", "range", "architectures.SpectralGNN.SGFL[].addGSO", "len", "len", "len", "len", "architectures.SpectralGNN.rho", "architectures.SpectralGNN.SGFL[].addGSO", "architectures.SpectralGNN.SGFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "changeGSO", "(", "self", ",", "GSO", ",", "nSelectedNodes", "=", "[", "]", ",", "poolingSize", "=", "[", "]", ")", ":", "\n", "\n", "# We use this to change the GSO, using the same graph filters.", "\n", "\n", "# Check that the new GSO has the correct", "\n", "        ", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "\n", "# Get dataType and device of the current GSO, so when we replace it, it", "\n", "# is still located in the same type and the same device.", "\n", "", "dataType", "=", "self", ".", "S", ".", "dtype", "\n", "if", "'device'", "in", "dir", "(", "self", ".", "S", ")", ":", "\n", "            ", "device", "=", "self", ".", "S", ".", "device", "\n", "", "else", ":", "\n", "            ", "device", "=", "None", "\n", "\n", "# Reorder the new GSO", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "# Change data type and device as required", "\n", "self", ".", "S", "=", "changeDataType", "(", "self", ".", "S", ",", "dataType", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "\n", "# Before making decisions, check if there is a new poolingSize list", "\n", "", "if", "len", "(", "poolingSize", ")", ">", "0", ":", "\n", "# Check it has the right length", "\n", "            ", "assert", "len", "(", "poolingSize", ")", "==", "self", ".", "L", "\n", "# And update it", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "\n", "# Now, check if we have a new list of nodes", "\n", "", "if", "len", "(", "nSelectedNodes", ")", ">", "0", ":", "\n", "# If we do, then we need to change the pooling functions to select", "\n", "# less nodes. This would allow to use graphs of different size.", "\n", "# Note that the pooling function, there is nothing learnable, so", "\n", "# they can easily be re-made, re-initialized.", "\n", "# The first thing we need to check, is that the length of the", "\n", "# number of nodes is equal to the number of layers (this list ", "\n", "# indicates the number of nodes selected at the output of each", "\n", "# layer)", "\n", "            ", "assert", "len", "(", "nSelectedNodes", ")", "==", "self", ".", "L", "\n", "# Then, update the N that we have stored", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "\n", "# And get the new pooling functions", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "# For each layer, add the pooling function", "\n", "                ", "self", ".", "SGFL", "[", "3", "*", "l", "+", "2", "]", "=", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "alpha", "[", "l", "]", ")", "\n", "self", ".", "SGFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "", "else", ":", "\n", "# Just update the GSO", "\n", "            ", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "SGFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n", "# And update in the Spectral GF that is still missing", "\n", "", "", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "SGFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "# Graph convolutional layer", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SpectralGNN.splitForward": [[1445, 1459], ["architectures.SpectralGNN.SGFL", "architectures.SpectralGNN.reshape", "len", "architectures.SpectralGNN.MLP"], "methods", ["None"], ["", "", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph filtering layer", "\n", "y", "=", "self", ".", "SGFL", "(", "x", ")", "\n", "# Flatten the output", "\n", "yFlat", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "yFlat", ")", ",", "y", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SpectralGNN.forward": [[1461, 1471], ["architectures.SpectralGNN.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.SpectralGNN.to": [[1472, 1484], ["super().to", "architectures.SpectralGNN.S.to", "range", "architectures.SpectralGNN.SGFL[].addGSO", "architectures.SpectralGNN.SGFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "SGFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "SGFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.NodeVariantGNN.__init__": [[1572, 1679], ["torch.Module.__init__", "len", "architectures.NodeVariantGNN.permFunction", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "nvgfl.append", "nvgfl[].addGSO", "nvgfl.append", "nvgfl.append", "nvgfl[].addGSO", "len", "fc.append", "range", "len", "len", "len", "alegnn.NodeVariantGF", "alegnn.NodeVariantGF", "architectures.NodeVariantGNN.sigma", "architectures.NodeVariantGNN.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architectures.NodeVariantGNN.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nShiftTaps", ",", "nNodeTaps", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than the number of", "\n", "# filter taps (because of the input number of features)", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nShiftTaps", ")", "+", "1", "\n", "# The length of the shift taps list should be equal to the length of the", "\n", "# node taps list", "\n", "assert", "len", "(", "nShiftTaps", ")", "==", "len", "(", "nNodeTaps", ")", "\n", "# nSelectedNodes should be a list of size nShiftTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nShiftTaps", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nShiftTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nShiftTaps", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nShiftTaps", "# Filter Shift taps", "\n", "self", ".", "M", "=", "nNodeTaps", "# Filter node taps", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "nvgfl", "=", "[", "]", "# Node Variant GF Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "nvgfl", ".", "append", "(", "gml", ".", "NodeVariantGF", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "K", "[", "l", "]", ",", "self", ".", "M", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ")", ")", "\n", "# There is a 3*l below here, because we have three elements per", "\n", "# layer: graph filter, nonlinearity and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "nvgfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Nonlinearity", "\n", "nvgfl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "nvgfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 3*l+2", "\n", "nvgfl", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "NVGFL", "=", "nn", ".", "Sequential", "(", "*", "nvgfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "            ", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.NodeVariantGNN.splitForward": [[1681, 1695], ["architectures.NodeVariantGNN.NVGFL", "architectures.NodeVariantGNN.reshape", "len", "architectures.NodeVariantGNN.MLP"], "methods", ["None"], ["", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph filtering layer", "\n", "y", "=", "self", ".", "NVGFL", "(", "x", ")", "\n", "# Flatten the output", "\n", "yFlat", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "yFlat", ")", ",", "y", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.NodeVariantGNN.forward": [[1697, 1707], ["architectures.NodeVariantGNN.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.NodeVariantGNN.to": [[1708, 1720], ["super().to", "architectures.NodeVariantGNN.S.to", "range", "architectures.NodeVariantGNN.NVGFL[].addGSO", "architectures.NodeVariantGNN.NVGFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "NVGFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "NVGFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantGNN.__init__": [[1807, 1915], ["torch.Module.__init__", "len", "architectures.EdgeVariantGNN.permFunction", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "evgfl.append", "evgfl[].addGSO", "evgfl.append", "evgfl.append", "evgfl[].addGSO", "len", "fc.append", "range", "len", "len", "len", "alegnn.EdgeVariantGF", "alegnn.EdgeVariantGF", "architectures.EdgeVariantGNN.sigma", "architectures.EdgeVariantGNN.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architectures.EdgeVariantGNN.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nShiftTaps", ",", "nFilterNodes", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than the number of", "\n", "# filter taps (because of the input number of features)", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nShiftTaps", ")", "+", "1", "\n", "# Filter nodes is a list of int with the number of nodes to select for", "\n", "# the EV part at each layer; it should have the same length as the", "\n", "# number of filter taps", "\n", "assert", "len", "(", "nFilterNodes", ")", "==", "len", "(", "nShiftTaps", ")", "\n", "# nSelectedNodes should be a list of size nShiftTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nShiftTaps", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nShiftTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nShiftTaps", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nShiftTaps", "# Filter Shift taps", "\n", "self", ".", "M", "=", "nFilterNodes", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "evgfl", "=", "[", "]", "# Node Variant GF Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "evgfl", ".", "append", "(", "gml", ".", "EdgeVariantGF", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "K", "[", "l", "]", ",", "self", ".", "M", "[", "l", "]", ",", "self", ".", "N", "[", "0", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ")", ")", "\n", "# There is a 3*l below here, because we have three elements per", "\n", "# layer: graph filter, nonlinearity and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "evgfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Nonlinearity", "\n", "evgfl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "evgfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 3*l+2", "\n", "evgfl", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "EVGFL", "=", "nn", ".", "Sequential", "(", "*", "evgfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "            ", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantGNN.splitForward": [[1917, 1931], ["architectures.EdgeVariantGNN.EVGFL", "architectures.EdgeVariantGNN.reshape", "len", "architectures.EdgeVariantGNN.MLP"], "methods", ["None"], ["", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph filtering layer", "\n", "y", "=", "self", ".", "EVGFL", "(", "x", ")", "\n", "# Flatten the output", "\n", "yFlat", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "yFlat", ")", ",", "y", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantGNN.forward": [[1933, 1943], ["architectures.EdgeVariantGNN.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantGNN.to": [[1944, 1956], ["super().to", "architectures.EdgeVariantGNN.S.to", "range", "architectures.EdgeVariantGNN.EVGFL[].addGSO", "architectures.EdgeVariantGNN.EVGFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "EVGFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "EVGFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalEdgeNet.__init__": [[2041, 2146], ["torch.Module.__init__", "len", "architectures.LocalEdgeNet.permFunction", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "evgfl.append", "evgfl[].addGSO", "evgfl.append", "evgfl.append", "evgfl[].addGSO", "len", "fc.append", "range", "len", "len", "len", "alegnn.EdgeVariantGF", "alegnn.EdgeVariantGF", "architectures.LocalEdgeNet.sigma", "architectures.LocalEdgeNet.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architectures.LocalEdgeNet.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nShiftTaps", ",", "nFilterNodes", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimReadout", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than the number of", "\n", "# filter taps (because of the input number of features)", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nShiftTaps", ")", "+", "1", "\n", "# Filter nodes is a list of int with the number of nodes to select for", "\n", "# the EV part at each layer; it should have the same length as the", "\n", "# number of filter taps", "\n", "assert", "len", "(", "nFilterNodes", ")", "==", "len", "(", "nShiftTaps", ")", "\n", "# nSelectedNodes should be a list of size nShiftTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nShiftTaps", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nShiftTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nShiftTaps", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nShiftTaps", "# Filter Shift taps", "\n", "self", ".", "M", "=", "nFilterNodes", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "dimReadout", "=", "dimReadout", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "evgfl", "=", "[", "]", "# Node Variant GF Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "evgfl", ".", "append", "(", "gml", ".", "EdgeVariantGF", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "K", "[", "l", "]", ",", "self", ".", "M", "[", "l", "]", ",", "self", ".", "N", "[", "0", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ")", ")", "\n", "# There is a 3*l below here, because we have three elements per", "\n", "# layer: graph filter, nonlinearity and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "evgfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Nonlinearity", "\n", "evgfl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "evgfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 3*l+2", "\n", "evgfl", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "EVGFL", "=", "nn", ".", "Sequential", "(", "*", "evgfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimReadout", ")", ">", "0", ":", "# Maybe we don't want to readout anything", "\n", "# The first layer has to connect whatever was left of the graph ", "\n", "# filtering stage to create the number of features required by", "\n", "# the readout layer", "\n", "            ", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "F", "[", "-", "1", "]", ",", "dimReadout", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimReadout", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimReadout", "[", "l", "]", ",", "dimReadout", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "Readout", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalEdgeNet.splitForward": [[2148, 2163], ["architectures.LocalEdgeNet.EVGFL", "architectures.LocalEdgeNet.permute", "architectures.LocalEdgeNet.Readout", "len", "architectures.LocalEdgeNet.permute"], "methods", ["None"], ["", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph filtering layer", "\n", "yEVGFL", "=", "self", ".", "EVGFL", "(", "x", ")", "\n", "# Change the order, for the readout", "\n", "y", "=", "yEVGFL", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# B x N[-1] x F[-1]", "\n", "# And, feed it into the Readout layer", "\n", "y", "=", "self", ".", "Readout", "(", "y", ")", "# B x N[-1] x dimReadout[-1]", "\n", "# Reshape and return", "\n", "return", "y", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "yEVGFL", "\n", "# B x dimReadout[-1] x N[-1], B x dimFeatures[-1] x N[-1]", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalEdgeNet.forward": [[2165, 2175], ["architectures.LocalEdgeNet.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalEdgeNet.singleNodeForward": [[2176, 2230], ["numpy.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "architectures.LocalEdgeNet.forward", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.squeeze", "torch.matmul.squeeze", "type", "architectures.LocalEdgeNet.order.index", "numpy.array", "numpy.tile", "type", "numpy.array", "type", "type", "type", "architectures.LocalEdgeNet.order.index", "type", "numpy.array", "numpy.array.astype", "numpy.arange", "numpy.where", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.forward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "singleNodeForward", "(", "self", ",", "x", ",", "nodes", ")", ":", "\n", "\n", "# x is of shape B x F[0] x N[-1]", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "# nodes is either an int, or a list/np.array of ints of size B", "\n", "assert", "type", "(", "nodes", ")", "is", "int", "or", "type", "(", "nodes", ")", "is", "list", "or", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", "\n", "\n", "# Let us start by building the selection matrix", "\n", "# This selection matrix has to be a matrix of shape", "\n", "#   B x N[-1] x 1", "\n", "# so that when multiplying with the output of the forward, we get a", "\n", "#   B x dimRedout[-1] x 1", "\n", "# and we just squeeze the last dimension", "\n", "\n", "# TODO: The big question here is if multiplying by a matrix is faster", "\n", "# than doing torch.index_select", "\n", "\n", "# Let's always work with numpy arrays to make it easier.", "\n", "if", "type", "(", "nodes", ")", "is", "int", ":", "\n", "# Change the node number to accommodate the new order", "\n", "            ", "nodes", "=", "self", ".", "order", ".", "index", "(", "nodes", ")", "\n", "# If it's int, make it a list and an array", "\n", "nodes", "=", "np", ".", "array", "(", "[", "nodes", "]", ",", "dtype", "=", "np", ".", "int", ")", "\n", "# And repeat for the number of batches", "\n", "nodes", "=", "np", ".", "tile", "(", "nodes", ",", "batchSize", ")", "\n", "", "if", "type", "(", "nodes", ")", "is", "list", ":", "\n", "            ", "newNodes", "=", "[", "self", ".", "order", ".", "index", "(", "n", ")", "for", "n", "in", "nodes", "]", "\n", "nodes", "=", "np", ".", "array", "(", "newNodes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "", "elif", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", ":", "\n", "            ", "newNodes", "=", "np", ".", "array", "(", "[", "np", ".", "where", "(", "np", ".", "array", "(", "self", ".", "order", ")", "==", "n", ")", "[", "0", "]", "[", "0", "]", "for", "n", "in", "nodes", "]", ")", "\n", "nodes", "=", "newNodes", ".", "astype", "(", "np", ".", "int", ")", "\n", "# Now, nodes is an np.int np.ndarray with shape batchSize", "\n", "\n", "# Build the selection matrix", "\n", "", "selectionMatrix", "=", "np", ".", "zeros", "(", "[", "batchSize", ",", "self", ".", "N", "[", "-", "1", "]", ",", "1", "]", ")", "\n", "selectionMatrix", "[", "np", ".", "arange", "(", "batchSize", ")", ",", "nodes", ",", "0", "]", "=", "1.", "\n", "# And convert it to a tensor", "\n", "selectionMatrix", "=", "torch", ".", "tensor", "(", "selectionMatrix", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Now compute the output", "\n", "y", "=", "self", ".", "forward", "(", "x", ")", "\n", "# This output is of size B x dimReadout[-1] x N[-1]", "\n", "\n", "# Multiply the output", "\n", "y", "=", "torch", ".", "matmul", "(", "y", ",", "selectionMatrix", ")", "\n", "#   B x dimReadout[-1] x 1", "\n", "\n", "# Squeeze the last dimension and return", "\n", "return", "y", ".", "squeeze", "(", "2", ")", "\n", "", "def", "to", "(", "self", ",", "device", ")", ":", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalEdgeNet.to": [[2230, 2242], ["super().to", "architectures.LocalEdgeNet.S.to", "range", "architectures.LocalEdgeNet.EVGFL[].addGSO", "architectures.LocalEdgeNet.EVGFL[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "EVGFL", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "EVGFL", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.ARMAfilterGNN.__init__": [[2346, 2452], ["torch.Module.__init__", "len", "architectures.ARMAfilterGNN.permFunction", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "gfl.append", "gfl[].addGSO", "gfl.append", "gfl.append", "gfl[].addGSO", "len", "fc.append", "range", "len", "len", "len", "len", "alegnn.GraphFilterARMA", "alegnn.GraphFilterARMA", "architectures.ARMAfilterGNN.sigma", "architectures.ARMAfilterGNN.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architectures.ARMAfilterGNN.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nDenominatorTaps", ",", "nResidueTaps", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ",", "tMax", "=", "5", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nDenominatorTaps", ")", "+", "1", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nResidueTaps", ")", "+", "1", "\n", "# nSelectedNodes should be a list of size nFilterTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nResidueTaps", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nResidueTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nResidueTaps", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "P", "=", "nDenominatorTaps", "# Denominator taps (order - 1)", "\n", "self", ".", "K", "=", "nResidueTaps", "# Residue taps (order - 1)", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "# Store the rest of the variables", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "self", ".", "tMax", "=", "tMax", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "gfl", "=", "[", "]", "# Graph Filtering Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "gfl", ".", "append", "(", "gml", ".", "GraphFilterARMA", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "P", "[", "l", "]", ",", "self", ".", "K", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ",", "self", ".", "tMax", ")", ")", "\n", "# There is a 3*l below here, because we have three elements per", "\n", "# layer: graph filter, nonlinearity and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "gfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Nonlinearity", "\n", "gfl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "gfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 3*l+2", "\n", "gfl", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "jARMA", "=", "nn", ".", "Sequential", "(", "*", "gfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "            ", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.ARMAfilterGNN.changeGSO": [[2454, 2515], ["architectures.ARMAfilterGNN.permFunction", "alegnn.utils.dataTools.changeDataType", "alegnn.utils.dataTools.changeDataType", "range", "len", "GSO.reshape.reshape.reshape", "dir", "architectures.ARMAfilterGNN.S.to", "len", "len", "range", "range", "architectures.ARMAfilterGNN.jARMA[].addGSO", "len", "len", "len", "len", "architectures.ARMAfilterGNN.rho", "architectures.ARMAfilterGNN.jARMA[].addGSO", "architectures.ARMAfilterGNN.jARMA[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "changeGSO", "(", "self", ",", "GSO", ",", "nSelectedNodes", "=", "[", "]", ",", "poolingSize", "=", "[", "]", ")", ":", "\n", "\n", "# We use this to change the GSO, using the same graph filters.", "\n", "\n", "# Check that the new GSO has the correct", "\n", "        ", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "\n", "# Get dataType and device of the current GSO, so when we replace it, it", "\n", "# is still located in the same type and the same device.", "\n", "", "dataType", "=", "self", ".", "S", ".", "dtype", "\n", "if", "'device'", "in", "dir", "(", "self", ".", "S", ")", ":", "\n", "            ", "device", "=", "self", ".", "S", ".", "device", "\n", "", "else", ":", "\n", "            ", "device", "=", "None", "\n", "\n", "# Reorder the new GSO", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "# Change data type and device as required", "\n", "self", ".", "S", "=", "changeDataType", "(", "self", ".", "S", ",", "dataType", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "\n", "# Before making decisions, check if there is a new poolingSize list", "\n", "", "if", "len", "(", "poolingSize", ")", ">", "0", ":", "\n", "# Check it has the right length", "\n", "            ", "assert", "len", "(", "poolingSize", ")", "==", "self", ".", "L", "\n", "# And update it", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "\n", "# Now, check if we have a new list of nodes", "\n", "", "if", "len", "(", "nSelectedNodes", ")", ">", "0", ":", "\n", "# If we do, then we need to change the pooling functions to select", "\n", "# less nodes. This would allow to use graphs of different size.", "\n", "# Note that the pooling function, there is nothing learnable, so", "\n", "# they can easily be re-made, re-initialized.", "\n", "# The first thing we need to check, is that the length of the", "\n", "# number of nodes is equal to the number of layers (this list ", "\n", "# indicates the number of nodes selected at the output of each", "\n", "# layer)", "\n", "            ", "assert", "len", "(", "nSelectedNodes", ")", "==", "self", ".", "L", "\n", "# Then, update the N that we have stored", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "\n", "# And get the new pooling functions", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "# For each layer, add the pooling function", "\n", "                ", "self", ".", "jARMA", "[", "3", "*", "l", "+", "2", "]", "=", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "alpha", "[", "l", "]", ")", "\n", "self", ".", "jARMA", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "", "else", ":", "\n", "# Just update the GSO", "\n", "            ", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "jARMA", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n", "# And update in the LSIGF that is still missing", "\n", "", "", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "jARMA", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "# Graph convolutional layer", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.ARMAfilterGNN.splitForward": [[2516, 2530], ["architectures.ARMAfilterGNN.jARMA", "architectures.ARMAfilterGNN.reshape", "len", "architectures.ARMAfilterGNN.MLP"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.jARMA"], ["", "", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph filtering layer", "\n", "y", "=", "self", ".", "jARMA", "(", "x", ")", "\n", "# Flatten the output", "\n", "yFlat", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "yFlat", ")", ",", "y", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.ARMAfilterGNN.forward": [[2532, 2542], ["architectures.ARMAfilterGNN.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.ARMAfilterGNN.to": [[2543, 2555], ["super().to", "architectures.ARMAfilterGNN.S.to", "range", "architectures.ARMAfilterGNN.jARMA[].addGSO", "architectures.ARMAfilterGNN.jARMA[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "jARMA", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "jARMA", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalARMA.__init__": [[2658, 2761], ["torch.Module.__init__", "len", "architectures.LocalARMA.permFunction", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "gfl.append", "gfl[].addGSO", "gfl.append", "gfl.append", "gfl[].addGSO", "len", "fc.append", "range", "len", "len", "len", "len", "alegnn.GraphFilterARMA", "alegnn.GraphFilterARMA", "architectures.LocalARMA.sigma", "architectures.LocalARMA.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architectures.LocalARMA.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimNodeSignals", ",", "nDenominatorTaps", ",", "nResidueTaps", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimReadout", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ",", "tMax", "=", "5", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nDenominatorTaps", ")", "+", "1", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nResidueTaps", ")", "+", "1", "\n", "# nSelectedNodes should be a list of size nFilterTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nResidueTaps", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nResidueTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nResidueTaps", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "P", "=", "nDenominatorTaps", "# Denominator taps (order - 1)", "\n", "self", ".", "K", "=", "nResidueTaps", "# Residue taps (order - 1)", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "# Store the rest of the variables", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "dimReadout", "=", "dimReadout", "\n", "self", ".", "tMax", "=", "tMax", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "gfl", "=", "[", "]", "# Graph Filtering Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "gfl", ".", "append", "(", "gml", ".", "GraphFilterARMA", "(", "self", ".", "F", "[", "l", "]", ",", "self", ".", "F", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "P", "[", "l", "]", ",", "self", ".", "K", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "bias", ",", "self", ".", "tMax", ")", ")", "\n", "# There is a 3*l below here, because we have three elements per", "\n", "# layer: graph filter, nonlinearity and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "gfl", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Nonlinearity", "\n", "gfl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "gfl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 3*l+2", "\n", "gfl", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "jARMA", "=", "nn", ".", "Sequential", "(", "*", "gfl", ")", "# Graph Filtering Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimReadout", ")", ">", "0", ":", "# Maybe we don't want to readout anything", "\n", "# The first layer has to connect whatever was left of the graph ", "\n", "# filtering stage to create the number of features required by", "\n", "# the readout layer", "\n", "            ", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "F", "[", "-", "1", "]", ",", "dimReadout", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimReadout", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimReadout", "[", "l", "]", ",", "dimReadout", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "Readout", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalARMA.changeGSO": [[2763, 2824], ["architectures.LocalARMA.permFunction", "alegnn.utils.dataTools.changeDataType", "alegnn.utils.dataTools.changeDataType", "range", "len", "GSO.reshape.reshape.reshape", "dir", "architectures.LocalARMA.S.to", "len", "len", "range", "range", "architectures.LocalARMA.jARMA[].addGSO", "len", "len", "len", "len", "architectures.LocalARMA.rho", "architectures.LocalARMA.jARMA[].addGSO", "architectures.LocalARMA.jARMA[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "changeGSO", "(", "self", ",", "GSO", ",", "nSelectedNodes", "=", "[", "]", ",", "poolingSize", "=", "[", "]", ")", ":", "\n", "\n", "# We use this to change the GSO, using the same graph filters.", "\n", "\n", "# Check that the new GSO has the correct", "\n", "        ", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "\n", "# Get dataType and device of the current GSO, so when we replace it, it", "\n", "# is still located in the same type and the same device.", "\n", "", "dataType", "=", "self", ".", "S", ".", "dtype", "\n", "if", "'device'", "in", "dir", "(", "self", ".", "S", ")", ":", "\n", "            ", "device", "=", "self", ".", "S", ".", "device", "\n", "", "else", ":", "\n", "            ", "device", "=", "None", "\n", "\n", "# Reorder the new GSO", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "# Change data type and device as required", "\n", "self", ".", "S", "=", "changeDataType", "(", "self", ".", "S", ",", "dataType", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "\n", "# Before making decisions, check if there is a new poolingSize list", "\n", "", "if", "len", "(", "poolingSize", ")", ">", "0", ":", "\n", "# Check it has the right length", "\n", "            ", "assert", "len", "(", "poolingSize", ")", "==", "self", ".", "L", "\n", "# And update it", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "\n", "# Now, check if we have a new list of nodes", "\n", "", "if", "len", "(", "nSelectedNodes", ")", ">", "0", ":", "\n", "# If we do, then we need to change the pooling functions to select", "\n", "# less nodes. This would allow to use graphs of different size.", "\n", "# Note that the pooling function, there is nothing learnable, so", "\n", "# they can easily be re-made, re-initialized.", "\n", "# The first thing we need to check, is that the length of the", "\n", "# number of nodes is equal to the number of layers (this list ", "\n", "# indicates the number of nodes selected at the output of each", "\n", "# layer)", "\n", "            ", "assert", "len", "(", "nSelectedNodes", ")", "==", "self", ".", "L", "\n", "# Then, update the N that we have stored", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "\n", "# And get the new pooling functions", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "# For each layer, add the pooling function", "\n", "                ", "self", ".", "jARMA", "[", "3", "*", "l", "+", "2", "]", "=", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "alpha", "[", "l", "]", ")", "\n", "self", ".", "jARMA", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "", "else", ":", "\n", "# Just update the GSO", "\n", "            ", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "                ", "self", ".", "jARMA", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n", "# And update in the LSIGF that is still missing", "\n", "", "", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "jARMA", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "# Graph convolutional layer", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalARMA.splitForward": [[2825, 2840], ["architectures.LocalARMA.jARMA", "architectures.LocalARMA.permute", "architectures.LocalARMA.Readout", "len", "architectures.LocalARMA.permute"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.jARMA"], ["", "", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph filtering layer", "\n", "yARMA", "=", "self", ".", "jARMA", "(", "x", ")", "\n", "# Change the order, for the readout", "\n", "y", "=", "yARMA", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# B x N[-1] x F[-1]", "\n", "# And, feed it into the Readout layer", "\n", "y", "=", "self", ".", "Readout", "(", "y", ")", "# B x N[-1] x dimReadout[-1]", "\n", "# Reshape and return", "\n", "return", "y", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "yARMA", "\n", "# B x dimReadout[-1] x N[-1], B x dimFeatures[-1] x N[-1]", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalARMA.forward": [[2842, 2852], ["architectures.LocalARMA.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalARMA.singleNodeForward": [[2853, 2907], ["numpy.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "architectures.LocalARMA.forward", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.squeeze", "torch.matmul.squeeze", "type", "architectures.LocalARMA.order.index", "numpy.array", "numpy.tile", "type", "numpy.array", "type", "type", "type", "architectures.LocalARMA.order.index", "type", "numpy.array", "numpy.array.astype", "numpy.arange", "numpy.where", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.forward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "singleNodeForward", "(", "self", ",", "x", ",", "nodes", ")", ":", "\n", "\n", "# x is of shape B x F[0] x N[-1]", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "# nodes is either an int, or a list/np.array of ints of size B", "\n", "assert", "type", "(", "nodes", ")", "is", "int", "or", "type", "(", "nodes", ")", "is", "list", "or", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", "\n", "\n", "# Let us start by building the selection matrix", "\n", "# This selection matrix has to be a matrix of shape", "\n", "#   B x N[-1] x 1", "\n", "# so that when multiplying with the output of the forward, we get a", "\n", "#   B x dimRedout[-1] x 1", "\n", "# and we just squeeze the last dimension", "\n", "\n", "# TODO: The big question here is if multiplying by a matrix is faster", "\n", "# than doing torch.index_select", "\n", "\n", "# Let's always work with numpy arrays to make it easier.", "\n", "if", "type", "(", "nodes", ")", "is", "int", ":", "\n", "# Change the node number to accommodate the new order", "\n", "            ", "nodes", "=", "self", ".", "order", ".", "index", "(", "nodes", ")", "\n", "# If it's int, make it a list and an array", "\n", "nodes", "=", "np", ".", "array", "(", "[", "nodes", "]", ",", "dtype", "=", "np", ".", "int", ")", "\n", "# And repeat for the number of batches", "\n", "nodes", "=", "np", ".", "tile", "(", "nodes", ",", "batchSize", ")", "\n", "", "if", "type", "(", "nodes", ")", "is", "list", ":", "\n", "            ", "newNodes", "=", "[", "self", ".", "order", ".", "index", "(", "n", ")", "for", "n", "in", "nodes", "]", "\n", "nodes", "=", "np", ".", "array", "(", "newNodes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "", "elif", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", ":", "\n", "            ", "newNodes", "=", "np", ".", "array", "(", "[", "np", ".", "where", "(", "np", ".", "array", "(", "self", ".", "order", ")", "==", "n", ")", "[", "0", "]", "[", "0", "]", "for", "n", "in", "nodes", "]", ")", "\n", "nodes", "=", "newNodes", ".", "astype", "(", "np", ".", "int", ")", "\n", "# Now, nodes is an np.int np.ndarray with shape batchSize", "\n", "\n", "# Build the selection matrix", "\n", "", "selectionMatrix", "=", "np", ".", "zeros", "(", "[", "batchSize", ",", "self", ".", "N", "[", "-", "1", "]", ",", "1", "]", ")", "\n", "selectionMatrix", "[", "np", ".", "arange", "(", "batchSize", ")", ",", "nodes", ",", "0", "]", "=", "1.", "\n", "# And convert it to a tensor", "\n", "selectionMatrix", "=", "torch", ".", "tensor", "(", "selectionMatrix", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Now compute the output", "\n", "y", "=", "self", ".", "forward", "(", "x", ")", "\n", "# This output is of size B x dimReadout[-1] x N[-1]", "\n", "\n", "# Multiply the output", "\n", "y", "=", "torch", ".", "matmul", "(", "y", ",", "selectionMatrix", ")", "\n", "#   B x dimReadout[-1] x 1", "\n", "\n", "# Squeeze the last dimension and return", "\n", "return", "y", ".", "squeeze", "(", "2", ")", "\n", "", "def", "to", "(", "self", ",", "device", ")", ":", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.LocalARMA.to": [[2907, 2919], ["super().to", "architectures.LocalARMA.S.to", "range", "architectures.LocalARMA.jARMA[].addGSO", "architectures.LocalARMA.jARMA[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "jARMA", "[", "3", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "jARMA", "[", "3", "*", "l", "+", "2", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.AggregationGNN.__init__": [[2997, 3171], ["torch.Module.__init__", "len", "architectures.AggregationGNN.permFunction", "GSO.reshape.reshape.copy", "range", "numpy.zeros", "range", "numpy.zeros.copy().reshape", "range", "numpy.concatenate.transpose", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "numpy.concatenate", "convl.append", "convl.append", "convl.append", "len", "fc.append", "range", "len", "aggfc.append", "range", "len", "len", "len", "int", "numpy.zeros.copy", "torch.Conv1d", "torch.Conv1d", "architectures.AggregationGNN.sigma", "architectures.AggregationGNN.rho", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "torch.Linear", "torch.Linear", "aggfc.append", "aggfc.append", "numpy.zeros.reshape", "len", "architectures.AggregationGNN.sigma", "torch.Linear", "torch.Linear", "len", "len", "architectures.AggregationGNN.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimFeatures", ",", "nFilterTaps", ",", "bias", ",", "\n", "# Nonlinearity", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ",", "maxN", "=", "None", ",", "\n", "# Multiple nodes options", "\n", "nNodes", "=", "1", ",", "dimLayersAggMLP", "=", "[", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimFeatures", ")", "==", "len", "(", "nFilterTaps", ")", "+", "1", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nFilterTaps", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nFilterTaps", ")", "# Number of convolutional layers", "\n", "self", ".", "F", "=", "dimFeatures", "# Features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "GSO", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "# We need to keep GSO as the numpy version of the GSO and self.S as the", "\n", "# torch version; this is because many of the upcoming operations on the", "\n", "# GSO to define the structure are still in numpy.", "\n", "self", ".", "S", "=", "GSO", ".", "copy", "(", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "sigma", "=", "nonlinearity", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "# This acts as both the kernel_size and the", "\n", "# stride, so there is no overlap on the elements over which we take", "\n", "# the maximum (this is how it works as default)", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "self", ".", "dimLayersAggMLP", "=", "dimLayersAggMLP", "\n", "self", ".", "nNodes", "=", "nNodes", "# Number of nodes on which to process the GNN", "\n", "# Maybe we don't want to aggregate information all the way to the end,", "\n", "# but up to some pre-specificed value maxN (for numerical reasons,", "\n", "# mostly)", "\n", "if", "maxN", "is", "None", ":", "\n", "            ", "self", ".", "maxN", "=", "GSO", ".", "shape", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "maxN", "=", "maxN", "if", "maxN", "<", "GSO", ".", "shape", "[", "1", "]", "else", "GSO", ".", "shape", "[", "1", "]", "\n", "# Let's also record the number of nodes on each layer (L+1, actually)", "\n", "", "self", ".", "N", "=", "[", "self", ".", "maxN", "]", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "# In pyTorch, the convolution is a valid correlation, instead of a", "\n", "# full one, which means that the output is smaller than the input.", "\n", "# Precisely, this smaller (check documentation for nn.conv1d)", "\n", "            ", "outConvN", "=", "self", ".", "N", "[", "l", "]", "-", "(", "self", ".", "K", "[", "l", "]", "-", "1", ")", "# Size of the conv output", "\n", "# The next equation to compute the number of nodes is obtained from", "\n", "# the maxPool1d help in the pytorch documentation", "\n", "self", ".", "N", "+=", "[", "int", "(", "\n", "(", "outConvN", "-", "(", "self", ".", "alpha", "[", "l", "]", "-", "1", ")", "-", "1", ")", "/", "self", ".", "alpha", "[", "l", "]", "+", "1", "\n", ")", "]", "\n", "# int() on a float always applies floor()", "\n", "# Now, compute the necessary matrix. Recall that we want to build the", "\n", "# vector [[x]_{i}, [Sx]_{i}, [S^2x]_{i}, ..., [S^{N-1}x]_{i}] for the ", "\n", "# first i=0,...,nNodes-1 elements. But instead of computing the powers", "\n", "# of S^k and then keeping the ith row, we will multiply S with a ", "\n", "# [delta_i]_i = 1 and 0s elsewhere and keep each result in the row.", "\n", "", "delta", "=", "np", ".", "zeros", "(", "[", "self", ".", "E", ",", "GSO", ".", "shape", "[", "1", "]", ",", "self", ".", "nNodes", "]", ")", "# E x N x nNodes", "\n", "for", "n", "in", "range", "(", "self", ".", "nNodes", ")", ":", "\n", "            ", "delta", "[", ":", ",", "n", ",", "n", "]", "=", "1.", "# E x N x nNodes", "\n", "# And create the place where to store all of this", "\n", "", "SN", "=", "delta", ".", "copy", "(", ")", ".", "reshape", "(", "[", "self", ".", "E", ",", "1", ",", "GSO", ".", "shape", "[", "1", "]", ",", "self", ".", "nNodes", "]", ")", "\n", "for", "k", "in", "range", "(", "1", ",", "self", ".", "maxN", ")", ":", "\n", "            ", "delta", "=", "GSO", "@", "delta", "# E x N x nNodes", "\n", "SN", "=", "np", ".", "concatenate", "(", "(", "SN", ",", "\n", "delta", ".", "reshape", "(", "[", "self", ".", "E", ",", "1", ",", "GSO", ".", "shape", "[", "1", "]", ",", "self", ".", "nNodes", "]", ")", ")", ",", "\n", "axis", "=", "1", ")", "# E x k x N x nNodes", "\n", "# Now, we have constructed the matrix E x maxN x N x nNodes, but we want", "\n", "# is that signal, when multiplied by this matrix, constructs the vector", "\n", "# z for each of the nNodes. This vector z is a map between the N-vector", "\n", "# signal and a maxN-vector z, so we want to map N to maxN linearly,", "\n", "# multiplying by the left. Therefore, we want a N x maxN matrix. So we", "\n", "# reshape the dimensions", "\n", "", "SN", "=", "SN", ".", "transpose", "(", "3", ",", "0", ",", "2", ",", "1", ")", "# nNodes x E x N x maxN", "\n", "# This matrix SN just needs to multiply the incoming x to obtain the", "\n", "# aggregated vector. And that's it.", "\n", "self", ".", "SN", "=", "torch", ".", "tensor", "(", "SN", ")", "\n", "# The idea to handle different features and different nodes with the ", "\n", "# same 1D convolution is realizing that: for each edge feature E we need", "\n", "# a different filter, and for each node we need _the same_ convolutional", "\n", "# filters. Therefore, the different nNodes will go to increase the", "\n", "# batch size, while the edge features will go to increase the feature", "\n", "# space. And since different edge features increase the feature space", "\n", "# we need to consider them now.", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph filtering layers \\\\\\", "\n", "# OBS.: We could join this for with the one before, but we keep separate", "\n", "# for clarity of code.", "\n", "convl", "=", "[", "]", "# Convolutional Layers", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "#\\\\ Graph filtering stage:", "\n", "            ", "convl", ".", "append", "(", "nn", ".", "Conv1d", "(", "self", ".", "F", "[", "l", "]", "*", "self", ".", "E", ",", "\n", "self", ".", "F", "[", "l", "+", "1", "]", "*", "self", ".", "E", ",", "\n", "self", ".", "K", "[", "l", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "#\\\\ Nonlinearity", "\n", "convl", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "#\\\\ Pooling", "\n", "convl", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "ConvLayers", "=", "nn", ".", "Sequential", "(", "*", "convl", ")", "# Convolutional layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "            ", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "E", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done within each node", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# Now let's aggregate the information from all nodes", "\n", "aggfc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersAggMLP", ")", ">", "0", ":", "\n", "# If there's a final aggregation layer, then it will have to mix", "\n", "# the number of features of each of the nNodes. Note that these", "\n", "# number of features will be the output of the last layer of the", "\n", "# MLP if there was one, or the last number of features if not", "\n", "            ", "dimInputAggMLP", "=", "dimLayersMLP", "[", "-", "1", "]", "if", "len", "(", "dimLayersMLP", ")", ">", "0", "else", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "E", "\n", "# This is the input dimension for each node. So now we need to ", "\n", "# multiply this by the number of nodes", "\n", "aggfc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputAggMLP", "*", "nNodes", ",", "dimLayersAggMLP", "[", "0", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And then, for the rest of the layers", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersAggMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity", "\n", "                ", "aggfc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And the linear layer", "\n", "aggfc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersAggMLP", "[", "l", "]", ",", "dimLayersAggMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# so we finally have the architecture.", "\n", "", "", "self", ".", "AggMLP", "=", "nn", ".", "Sequential", "(", "*", "aggfc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.AggregationGNN.forward": [[3172, 3219], ["x.reshape.reshape.reshape", "architectures.AggregationGNN.SN.reshape", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "z.permute.permute.permute().reshape", "z.permute.permute.permute().reshape", "z.permute.permute.permute", "architectures.AggregationGNN.ConvLayers", "y.reshape.reshape.reshape", "architectures.AggregationGNN.MLP", "y.reshape.reshape.permute().reshape().permute", "architectures.AggregationGNN.AggMLP", "len", "y.reshape.reshape.reshape", "z.permute.permute.permute", "z.permute.permute.permute", "y.reshape.reshape.permute().reshape", "len", "y.reshape.reshape.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "# batch size", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "SN", ".", "shape", "[", "2", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# So, up to here, we have:", "\n", "#   x of shape B x F x N", "\n", "F", "=", "x", ".", "shape", "[", "1", "]", "\n", "N", "=", "x", ".", "shape", "[", "2", "]", "\n", "#   SN of shape nNodes x E x N x maxN", "\n", "nNodes", "=", "self", ".", "SN", ".", "shape", "[", "0", "]", "\n", "E", "=", "self", ".", "SN", ".", "shape", "[", "1", "]", "\n", "maxN", "=", "self", ".", "SN", ".", "shape", "[", "3", "]", "\n", "# We will consider a target shape of B x nNodes x E x F x N, so we adapt", "\n", "x", "=", "x", ".", "reshape", "(", "[", "B", ",", "1", ",", "1", ",", "F", ",", "N", "]", ")", "\n", "SN", "=", "self", ".", "SN", ".", "reshape", "(", "[", "1", ",", "nNodes", ",", "E", ",", "N", ",", "maxN", "]", ")", "\n", "# Let's do the aggregation step", "\n", "z", "=", "torch", ".", "matmul", "(", "x", ",", "SN", ")", "# B x nNodes x E x F x maxN", "\n", "# And now, we need to join dimension 0 and 1 (batch and nNodes), and", "\n", "# dimensions 2 and 3 (edge features and node features) before feeding", "\n", "# this into the convolution as a three-dimensional vector.", "\n", "# And since we always join the last dimensions", "\n", "z", "=", "z", ".", "permute", "(", "2", ",", "3", ",", "4", ",", "0", ",", "1", ")", ".", "reshape", "(", "[", "E", ",", "F", ",", "maxN", ",", "B", "*", "nNodes", "]", ")", "\n", "z", "=", "z", ".", "permute", "(", "3", ",", "2", ",", "0", ",", "1", ")", ".", "reshape", "(", "[", "B", "*", "nNodes", ",", "maxN", ",", "E", "*", "F", "]", ")", "\n", "z", "=", "z", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# (B * nNodes) x (E * F) x maxN ", "\n", "# Let's call the convolutional layers", "\n", "y", "=", "self", ".", "ConvLayers", "(", "z", ")", "\n", "# Flatten the output", "\n", "y", "=", "y", ".", "reshape", "(", "[", "B", "*", "self", ".", "nNodes", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "E", "]", ")", "\n", "# And, feed it into the per node MLP", "\n", "y", "=", "self", ".", "MLP", "(", "y", ")", "# (B * nNodes) x dimLayersMLP[-1]", "\n", "# And now we have to unpack it back for every node", "\n", "y", "=", "y", ".", "permute", "(", "1", ",", "0", ")", ".", "reshape", "(", "[", "y", ".", "shape", "[", "1", "]", ",", "B", ",", "nNodes", "]", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "# So that, so far, y is a graph signal (as expected) and as such, has", "\n", "# shape B x dimLayersMLP[-1] x nNodes", "\n", "# And now, if we have to aggregate one last time, this time we cannot", "\n", "# just feed it in the aggregator MLP, because if we're to do so,", "\n", "# we need to reshape, but if there's no aggregator, then the output", "\n", "# has to be the graph signal, so there's no need for a reshape", "\n", "if", "nNodes", "==", "1", "or", "len", "(", "self", ".", "dimLayersAggMLP", ")", ">", "0", ":", "\n", "            ", "y", "=", "y", ".", "reshape", "(", "[", "B", ",", "y", ".", "shape", "[", "1", "]", "*", "nNodes", "]", ")", "\n", "", "y", "=", "self", ".", "AggMLP", "(", "y", ")", "\n", "# And now we're done", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.AggregationGNN.to": [[3220, 3229], ["super().to", "architectures.AggregationGNN.S.to", "architectures.AggregationGNN.SN.to"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move to device the GSO and its related variables.", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "self", ".", "SN", "=", "self", ".", "SN", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.MultiNodeAggregationGNN.__init__": [[3344, 3508], ["torch.Module.__init__", "len", "range", "architectures.MultiNodeAggregationGNN.F.append", "architectures.MultiNodeAggregationGNN.permFunction", "GSO.reshape.reshape.copy", "max", "range", "torch.ModuleList", "torch.ModuleList", "range", "torch.Sequential", "torch.Sequential", "len", "GSO.reshape.reshape.reshape", "len", "len", "len", "len", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "list", "list", "list.remove", "thisOrder.extend", "architectures.MultiNodeAggregationGNN.innerOrder.append", "architectures.MultiNodeAggregationGNN.aggGNNmodules.append", "range", "len", "fc.append", "range", "len", "len", "range", "range", "torch.ModuleList", "torch.ModuleList", "architectures.MultiNodeAggregationGNN.aggGNNmodules[].append", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "architectures.AggregationGNN", "len", "architectures.MultiNodeAggregationGNN.sigma", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__"], ["def", "__init__", "(", "self", ",", "\n", "# Outer Structure", "\n", "nSelectedNodes", ",", "nShifts", ",", "\n", "# Inner Structure", "\n", "#  Graph filtering", "\n", "dimFeatures", ",", "nFilterTaps", ",", "bias", ",", "\n", "#  Nonlinearity", "\n", "nonlinearity", ",", "\n", "#  Pooling", "\n", "poolingFunction", ",", "poolingSize", ",", "\n", "#  MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Graph Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent class", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Check that we have an adequate GSO", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "# And create a third dimension if necessary", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "", "self", ".", "N", "=", "GSO", ".", "shape", "[", "1", "]", "# Store the number of nodes", "\n", "# Now, the interesting thing is that dimFeatures, nFilterTaps, and", "\n", "# poolingSize are all now lists of lists, and all of them need to have", "\n", "# the same length.", "\n", "self", ".", "R", "=", "len", "(", "nSelectedNodes", ")", "# Number of outer layers", "\n", "self", ".", "P", "=", "nSelectedNodes", "# Number of nodes selected on each outer layer", "\n", "# Check that the number of selected nodes does not exceed the number", "\n", "# of total nodes.", "\n", "# TODO: Should we consider that the number of nodes might not be", "\n", "# nonincreasing?", "\n", "for", "r", "in", "range", "(", "self", ".", "R", ")", ":", "\n", "            ", "if", "self", ".", "P", "[", "r", "]", ">", "self", ".", "N", ":", "\n", "# If so, just force it to be the number of nodes.", "\n", "                ", "self", ".", "P", "[", "r", "]", "=", "self", ".", "N", "\n", "", "", "assert", "len", "(", "nShifts", ")", "==", "self", ".", "R", "\n", "self", ".", "Q", "=", "nShifts", "# Number of shifts of each node on each outer layer", "\n", "assert", "len", "(", "dimFeatures", ")", "==", "len", "(", "nFilterTaps", ")", "==", "self", ".", "R", "\n", "assert", "len", "(", "poolingSize", ")", "==", "self", ".", "R", "\n", "self", ".", "F", "=", "dimFeatures", "# List of lists containing the number of features", "\n", "# at each inner layer of each outer layer", "\n", "# Note that we have to add how many features we want in the ``last''", "\n", "# AggGNN layer before going into the MLP layer. Here, I will just", "\n", "# mix in the number of last specified features, but there are a lot of", "\n", "# other options, like no MLP whatsoever at the end of each convolutional", "\n", "# layer. But, why not?", "\n", "# TODO: (This adds quite the number of parameters, it would be nice to", "\n", "# do some reasonable tests to check whether this MLPs are necessary or", "\n", "# not).", "\n", "self", ".", "F", ".", "append", "(", "[", "dimFeatures", "[", "-", "1", "]", "[", "-", "1", "]", "]", ")", "\n", "self", ".", "K", "=", "nFilterTaps", "# List of lists containing the number of filter ", "\n", "# taps at each inner layer of each outer layer.", "\n", "self", ".", "bias", "=", "bias", "# Boolean to include bias or not", "\n", "self", ".", "sigma", "=", "nonlinearity", "# Pointwise nonlinear function to include on", "\n", "# each aggregation GNN", "\n", "self", ".", "rho", "=", "poolingFunction", "# To use on every aggregation GNN", "\n", "self", ".", "alpha", "=", "poolingSize", "# Pooling size on each aggregation GNN", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "# MLP for each inner aggregation GNN", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "GSO", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "# We need to keep GSO as the numpy version of the GSO and self.S as the", "\n", "# torch version; this is because many of the upcoming operations on the", "\n", "# GSO to define the structure are still in numpy.", "\n", "self", ".", "S", "=", "GSO", ".", "copy", "(", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "# Now that there are several things to do next:", "\n", "# - The AggregationGNN module always selects the first node, so if we", "\n", "#   want to select the first R, then we have to reorder it ourselves", "\n", "#   before adding the GSO to each AggregationGNN structure", "\n", "# - A regular python list does not register the parameters of the ", "\n", "#   corresponding nn.Module leading to bugs and issues on optimization.", "\n", "#   For this the class nn.ModuleList() has been created. Unlike ", "\n", "#   nn.Sequential(), this class does not have a forward method, because", "\n", "#   they are not supposed to act in a cascade way, just to keep track of", "\n", "#   dynamically changing numbers of layers.", "\n", "# - Another interesting observation is that, preliminary experiments, ", "\n", "#   show that nn.ModuleList() is also capable of handling lists of ", "\n", "#   lists. And this is precisely what we need: the first element (the", "\n", "#   outer one) corresponds to each outer layer, and each one of these", "\n", "#   elements contains another list with the Aggregation GNNs", "\n", "#   corresponding to the number of selected nodes on each outer layer.", "\n", "\n", "#\\\\\\ Ordering:", "\n", "# So, let us start with the ordering. P (the number of selected nodes)", "\n", "# determines how many different orders we need (it's just rotating", "\n", "# the indices so that each one of those P is first)", "\n", "# The order will be a list of lists, the outer list having as many ", "\n", "# elements as maximum of P.", "\n", "", "self", ".", "innerOrder", "=", "[", "list", "(", "range", "(", "self", ".", "N", ")", ")", "]", "# This is the order for the", "\n", "#   first selected nodes which is, clearly, the identity order", "\n", "maxP", "=", "max", "(", "self", ".", "P", ")", "# Maximum number of nodes to consider", "\n", "for", "p", "in", "range", "(", "1", ",", "maxP", ")", ":", "\n", "            ", "allNodes", "=", "list", "(", "range", "(", "self", ".", "N", ")", ")", "# Create a list of all the nodes in", "\n", "# order.", "\n", "allNodes", ".", "remove", "(", "p", ")", "# Get rid of the element that we need to put", "\n", "# first", "\n", "thisOrder", "=", "[", "p", "]", "#  Take the pth element, put it in a list", "\n", "thisOrder", ".", "extend", "(", "allNodes", ")", "\n", "# extend that list with all other nodes, except for the pth one.", "\n", "self", ".", "innerOrder", ".", "append", "(", "thisOrder", ")", "# Store this in the order list", "\n", "\n", "#\\\\\\ Aggregation GNN stage:", "\n", "", "self", ".", "aggGNNmodules", "=", "nn", ".", "ModuleList", "(", ")", "# List to hold the AggGNN modules", "\n", "# Create the inner modules", "\n", "for", "r", "in", "range", "(", "self", ".", "R", ")", ":", "\n", "# Add the list of inner modules", "\n", "            ", "self", ".", "aggGNNmodules", ".", "append", "(", "nn", ".", "ModuleList", "(", ")", ")", "\n", "# And start going through the inner modules", "\n", "for", "p", "in", "range", "(", "self", ".", "P", "[", "r", "]", ")", ":", "\n", "                ", "thisGSO", "=", "GSO", "[", ":", ",", "self", ".", "innerOrder", "[", "p", "]", ",", ":", "]", "[", ":", ",", ":", ",", "self", ".", "innerOrder", "[", "p", "]", "]", "\n", "# # Reorder the GSO so that the selected node comes first and ", "\n", "# is thus selected by the AggGNN module.", "\n", "# Create the AggGNN module:", "\n", "self", ".", "aggGNNmodules", "[", "r", "]", ".", "append", "(", "\n", "AggregationGNN", "(", "self", ".", "F", "[", "r", "]", ",", "self", ".", "K", "[", "r", "]", ",", "self", ".", "bias", ",", "\n", "self", ".", "sigma", ",", "\n", "self", ".", "rho", ",", "self", ".", "alpha", "[", "r", "]", ",", "\n", "# Now, the number of features in the", "\n", "# output of this AggregationGNN has to", "\n", "# be equal to the number of input ", "\n", "# features required at the next AggGNN", "\n", "# layer.", "\n", "[", "self", ".", "F", "[", "r", "+", "1", "]", "[", "0", "]", "]", ",", "\n", "thisGSO", ",", "maxN", "=", "self", ".", "Q", "[", "r", "]", ")", ")", "\n", "# And this should be it for the creation of the AggGNN layers of the", "\n", "# MultiNodeAggregationGNN architecture. We move onto one last MLP", "\n", "", "", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "\n", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "            ", "dimInputMLP", "=", "self", ".", "P", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "[", "0", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigma", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.MultiNodeAggregationGNN.forward": [[3510, 3564], ["torch.empty().to", "torch.empty().to", "torch.empty().to", "torch.empty().to", "range", "range", "torch.empty().to.reshape", "torch.empty().to.reshape", "architectures.MultiNodeAggregationGNN.MLP", "len", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "zeroPad.type().to.type().to.type().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.empty().to", "torch.empty().to", "torch.empty().to", "torch.empty().to", "torch.empty().to", "torch.empty().to", "torch.empty().to", "torch.empty().to", "thisOutput.unsqueeze", "thisOutput.unsqueeze", "zeroPad.type().to.type().to.type", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "# Check all relative dimensions", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "\n", "# Create an empty vector to store the output of the AggGNN of each node", "\n", "y", "=", "torch", ".", "empty", "(", "0", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "# For each outer layer (except the last one, since in the last one we", "\n", "# do not have to zero-pad)", "\n", "for", "r", "in", "range", "(", "self", ".", "R", "-", "1", ")", ":", "\n", "# For each node", "\n", "            ", "for", "p", "in", "range", "(", "self", ".", "P", "[", "r", "]", ")", ":", "\n", "# Re-order the nodes so that the selected nodes goes first", "\n", "                ", "xReordered", "=", "x", "[", ":", ",", ":", ",", "self", ".", "innerOrder", "[", "p", "]", "]", "\n", "# Compute the output of each GNN", "\n", "thisOutput", "=", "self", ".", "aggGNNmodules", "[", "r", "]", "[", "p", "]", "(", "xReordered", ")", "\n", "# Add it to the corresponding nodes", "\n", "y", "=", "torch", ".", "cat", "(", "(", "y", ",", "thisOutput", ".", "unsqueeze", "(", "2", ")", ")", ",", "dim", "=", "2", ")", "\n", "# After this, y is of size B x F x P[r], but if we need to keep ", "\n", "# going for other outer layers, we need to zero-pad so that we can", "\n", "# keep shifting around on the original graph", "\n", "", "if", "y", ".", "shape", "[", "2", "]", "<", "self", ".", "N", ":", "\n", "# We zero-pad", "\n", "                ", "zeroPad", "=", "torch", ".", "zeros", "(", "batchSize", ",", "y", ".", "shape", "[", "1", "]", ",", "self", ".", "N", "-", "y", ".", "shape", "[", "2", "]", ")", "\n", "zeroPad", "=", "zeroPad", ".", "type", "(", "y", ".", "dtype", ")", ".", "to", "(", "y", ".", "device", ")", "\n", "# Save as x", "\n", "x", "=", "torch", ".", "cat", "(", "(", "y", ",", "zeroPad", ")", ",", "dim", "=", "2", ")", "\n", "# and reset y", "\n", "y", "=", "torch", ".", "empty", "(", "0", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "# At this point, note that x (and, before, y) where in order: ", "\n", "# the first elements corresponds to the first one in the", "\n", "# original ordering and so on. This means that the self.order", "\n", "# stored for the MultiNode still holds", "\n", "", "else", ":", "\n", "# We selected all nodes, so we do not need to zero-pad", "\n", "                ", "x", "=", "y", "\n", "# Save as x, and reset y", "\n", "y", "=", "torch", ".", "empty", "(", "0", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "# Last layer: we do not need to zero pad afterwards, so we just compute", "\n", "# the output of the GNN for each node and store that", "\n", "", "", "for", "p", "in", "range", "(", "self", ".", "P", "[", "-", "1", "]", ")", ":", "\n", "            ", "xReordered", "=", "x", "[", ":", ",", ":", ",", "self", ".", "innerOrder", "[", "p", "]", "]", "\n", "thisOutput", "=", "self", ".", "aggGNNmodules", "[", "-", "1", "]", "[", "p", "]", "(", "xReordered", ")", "\n", "y", "=", "torch", ".", "cat", "(", "(", "y", ",", "thisOutput", ".", "unsqueeze", "(", "2", ")", ")", ",", "dim", "=", "2", ")", "\n", "\n", "# Flatten the output", "\n", "", "y", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "[", "-", "1", "]", "*", "self", ".", "P", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "y", ")", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.MultiNodeAggregationGNN.to": [[3566, 3574], ["super().to", "range", "range", "[].to"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# First, we initialize as always.", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# And then, in particular, move each architecture (that it will", "\n", "# internally move the GSOs and neighbors and stuff)", "\n", "for", "r", "in", "range", "(", "self", ".", "R", ")", ":", "\n", "            ", "for", "p", "in", "range", "(", "self", ".", "P", "[", "r", "]", ")", ":", "\n", "                ", "self", ".", "aggGNNmodules", "[", "r", "]", "[", "p", "]", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphAttentionNetwork.__init__": [[3645, 3784], ["torch.Module.__init__", "len", "architectures.GraphAttentionNetwork.permFunction", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "range", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "len", "fc.append", "range", "len", "len", "len", "alegnn.GraphAttentional", "alegnn.GraphAttentional", "architectures.GraphAttentionNetwork.rho", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "alegnn.GraphAttentional", "alegnn.GraphAttentional", "architectures.GraphAttentionNetwork.rho", "alegnn.GraphAttentional", "alegnn.GraphAttentional", "architectures.GraphAttentionNetwork.rho", "str().find", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "alegnn.GraphAttentional", "alegnn.GraphAttentional", "architectures.GraphAttentionNetwork.rho", "str().find", "torch.Tanh", "torch.Tanh", "len", "architectures.GraphAttentionNetwork.sigmaMLP", "torch.Linear", "torch.Linear", "str", "str"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph attentional layer", "\n", "dimNodeSignals", ",", "nAttentionHeads", ",", "\n", "# Nonlinearity (nn.functional)", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "bias", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nAttentionHeads", ")", "+", "1", "\n", "# nSelectedNodes should be a list of size nFilterTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nAttentionHeads", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nAttentionHeads", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nAttentionHeads", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nAttentionHeads", "# Attention Heads", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "sigma", "=", "nonlinearity", "# This has to be a nn.functional instead of", "\n", "# just a nn", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "self", ".", "bias", "=", "bias", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph Attentional Layers \\\\\\", "\n", "# OBS.: The last layer has to have concatenate False, whereas the rest", "\n", "# have concatenate True. So we go all the way except for the last layer", "\n", "gat", "=", "[", "]", "# Graph Attentional Layers", "\n", "if", "self", ".", "L", ">", "1", ":", "\n", "# First layer (this goes separate because there are not attention", "\n", "# heads increasing the number of features)", "\n", "#\\\\ Graph attention stage:", "\n", "            ", "gat", ".", "append", "(", "gml", ".", "GraphAttentional", "(", "self", ".", "F", "[", "0", "]", ",", "self", ".", "F", "[", "1", "]", ",", "self", ".", "K", "[", "0", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "sigma", ",", "True", ")", ")", "\n", "gat", "[", "0", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "0", "]", ",", "self", ".", "N", "[", "1", "]", ",", "self", ".", "alpha", "[", "0", "]", ")", ")", "\n", "gat", "[", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# All the next layers (attention heads appear):", "\n", "for", "l", "in", "range", "(", "1", ",", "self", ".", "L", "-", "1", ")", ":", "\n", "#\\\\ Graph attention stage:", "\n", "                ", "gat", ".", "append", "(", "gml", ".", "GraphAttentional", "(", "self", ".", "F", "[", "l", "]", "*", "self", ".", "K", "[", "l", "-", "1", "]", ",", "\n", "self", ".", "F", "[", "l", "+", "1", "]", ",", "self", ".", "K", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "sigma", ",", "True", ")", ")", "\n", "# There is a 2*l below here, because we have two elements per", "\n", "# layer: graph filter and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "gat", "[", "2", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 2*l+1", "\n", "gat", "[", "2", "*", "l", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And the last layer (set concatenate to False):", "\n", "#\\\\ Graph attention stage:", "\n", "", "gat", ".", "append", "(", "gml", ".", "GraphAttentional", "(", "self", ".", "F", "[", "self", ".", "L", "-", "1", "]", "*", "self", ".", "K", "[", "self", ".", "L", "-", "2", "]", ",", "\n", "self", ".", "F", "[", "self", ".", "L", "]", ",", "self", ".", "K", "[", "self", ".", "L", "-", "1", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "sigma", ",", "False", ")", ")", "\n", "gat", "[", "2", "*", "(", "self", ".", "L", "-", "1", ")", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "self", ".", "L", "-", "1", "]", ",", "self", ".", "N", "[", "self", ".", "L", "]", ",", "\n", "self", ".", "alpha", "[", "self", ".", "L", "-", "1", "]", ")", ")", "\n", "gat", "[", "2", "*", "(", "self", ".", "L", "-", "1", ")", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "else", ":", "\n", "# If there's only one layer, it just go straightforward, adding a", "\n", "# False to the concatenation and no increase in the input features", "\n", "# due to attention heads", "\n", "            ", "gat", ".", "append", "(", "gml", ".", "GraphAttentional", "(", "self", ".", "F", "[", "0", "]", ",", "self", ".", "F", "[", "1", "]", ",", "self", ".", "K", "[", "0", "]", ",", "\n", "self", ".", "E", ",", "self", ".", "sigma", ",", "False", ")", ")", "\n", "gat", "[", "0", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "0", "]", ",", "self", ".", "N", "[", "1", "]", ",", "self", ".", "alpha", "[", "0", "]", ")", ")", "\n", "gat", "[", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "GAT", "=", "nn", ".", "Sequential", "(", "*", "gat", ")", "# Graph Attentional Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "# NOTE: Because sigma is a functional, instead of the layer, then", "\n", "# we need to pick up the layer for the MLP part.", "\n", "            ", "if", "str", "(", "self", ".", "sigma", ")", ".", "find", "(", "'relu'", ")", ">=", "0", ":", "\n", "                ", "self", ".", "sigmaMLP", "=", "nn", ".", "ReLU", "(", ")", "\n", "", "elif", "str", "(", "self", ".", "sigma", ")", ".", "find", "(", "'tanh'", ")", ">=", "0", ":", "\n", "                ", "self", ".", "sigmaMLP", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n", "", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigmaMLP", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphAttentionNetwork.forward": [[3786, 3800], ["architectures.GraphAttentionNetwork.GAT", "y.reshape.reshape.reshape", "architectures.GraphAttentionNetwork.MLP", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph attentional layers", "\n", "y", "=", "self", ".", "GAT", "(", "x", ")", "\n", "# Flatten the output", "\n", "y", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "y", ")", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphAttentionNetwork.to": [[3802, 3814], ["super().to", "architectures.GraphAttentionNetwork.S.to", "range", "architectures.GraphAttentionNetwork.GAT[].addGSO", "architectures.GraphAttentionNetwork.GAT[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "GAT", "[", "2", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "GAT", "[", "2", "*", "l", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphConvolutionAttentionNetwork.__init__": [[3892, 4057], ["torch.Module.__init__", "len", "architectures.GraphConvolutionAttentionNetwork.permFunction", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "range", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "len", "fc.append", "range", "len", "len", "len", "len", "alegnn.GraphFilterAttentional", "alegnn.GraphFilterAttentional", "architectures.GraphConvolutionAttentionNetwork.rho", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "alegnn.GraphFilterAttentional", "alegnn.GraphFilterAttentional", "architectures.GraphConvolutionAttentionNetwork.rho", "alegnn.GraphFilterAttentional", "alegnn.GraphFilterAttentional", "architectures.GraphConvolutionAttentionNetwork.rho", "str().find", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "alegnn.GraphFilterAttentional", "alegnn.GraphFilterAttentional", "architectures.GraphConvolutionAttentionNetwork.rho", "str().find", "torch.Tanh", "torch.Tanh", "len", "architectures.GraphConvolutionAttentionNetwork.sigmaMLP", "torch.Linear", "torch.Linear", "str", "str"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph attentional layer", "\n", "dimNodeSignals", ",", "nFilterTaps", ",", "nAttentionHeads", ",", "bias", ",", "\n", "# Nonlinearity (nn.functional)", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilterTaps", "\n", "# and nAttentionHeads", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nFilterTaps", ")", "+", "1", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nAttentionHeads", ")", "+", "1", "\n", "# nSelectedNodes should be a list of size nFilterTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nAttentionHeads", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nAttentionHeads", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nAttentionHeads", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Number of filter taps", "\n", "self", ".", "P", "=", "nAttentionHeads", "# Attention Heads", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "sigma", "=", "nonlinearity", "# This has to be a nn.functional instead of", "\n", "# just a nn", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "self", ".", "bias", "=", "bias", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph Attentional Layers \\\\\\", "\n", "# OBS.: The last layer has to have concatenate False, whereas the rest", "\n", "# have concatenate True. So we go all the way except for the last layer", "\n", "gat", "=", "[", "]", "# Graph Attentional Layers", "\n", "if", "self", ".", "L", ">", "1", ":", "\n", "# First layer (this goes separate because there are not attention", "\n", "# heads increasing the number of features)", "\n", "#\\\\ Graph attention stage:", "\n", "            ", "gat", ".", "append", "(", "gml", ".", "GraphFilterAttentional", "(", "self", ".", "F", "[", "0", "]", ",", "\n", "self", ".", "F", "[", "1", "]", ",", "\n", "self", ".", "K", "[", "0", "]", ",", "\n", "self", ".", "P", "[", "0", "]", ",", "\n", "self", ".", "E", ",", "\n", "self", ".", "bias", ",", "\n", "self", ".", "sigma", ",", "\n", "True", ")", ")", "\n", "gat", "[", "0", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "0", "]", ",", "self", ".", "N", "[", "1", "]", ",", "self", ".", "alpha", "[", "0", "]", ")", ")", "\n", "gat", "[", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# All the next layers (attention heads appear):", "\n", "for", "l", "in", "range", "(", "1", ",", "self", ".", "L", "-", "1", ")", ":", "\n", "#\\\\ Graph attention stage:", "\n", "                ", "gat", ".", "append", "(", "gml", ".", "GraphFilterAttentional", "(", "self", ".", "F", "[", "l", "]", "*", "self", ".", "P", "[", "l", "-", "1", "]", ",", "\n", "self", ".", "F", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "K", "[", "l", "]", ",", "\n", "self", ".", "P", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "\n", "self", ".", "bias", ",", "\n", "self", ".", "sigma", ",", "\n", "True", ")", ")", "\n", "# There is a 2*l below here, because we have two elements per", "\n", "# layer: graph filter and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "gat", "[", "2", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 2*l+1", "\n", "gat", "[", "2", "*", "l", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And the last layer (set concatenate to False):", "\n", "#\\\\ Graph attention stage:", "\n", "", "gat", ".", "append", "(", "gml", ".", "GraphFilterAttentional", "(", "self", ".", "F", "[", "self", ".", "L", "-", "1", "]", "*", "self", ".", "P", "[", "self", ".", "L", "-", "2", "]", ",", "\n", "self", ".", "F", "[", "self", ".", "L", "]", ",", "\n", "self", ".", "K", "[", "self", ".", "L", "-", "1", "]", ",", "\n", "self", ".", "P", "[", "self", ".", "L", "-", "1", "]", ",", "\n", "self", ".", "E", ",", "\n", "self", ".", "bias", ",", "\n", "self", ".", "sigma", ",", "\n", "False", ")", ")", "\n", "gat", "[", "2", "*", "(", "self", ".", "L", "-", "1", ")", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "self", ".", "L", "-", "1", "]", ",", "self", ".", "N", "[", "self", ".", "L", "]", ",", "\n", "self", ".", "alpha", "[", "self", ".", "L", "-", "1", "]", ")", ")", "\n", "gat", "[", "2", "*", "(", "self", ".", "L", "-", "1", ")", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "else", ":", "\n", "# If there's only one layer, it just go straightforward, adding a", "\n", "# False to the concatenation and no increase in the input features", "\n", "# due to attention heads", "\n", "            ", "gat", ".", "append", "(", "gml", ".", "GraphFilterAttentional", "(", "self", ".", "F", "[", "0", "]", ",", "\n", "self", ".", "F", "[", "1", "]", ",", "\n", "self", ".", "K", "[", "0", "]", ",", "\n", "self", ".", "P", "[", "0", "]", ",", "\n", "self", ".", "E", ",", "\n", "self", ".", "bias", ",", "\n", "self", ".", "sigma", ",", "\n", "False", ")", ")", "\n", "gat", "[", "0", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "0", "]", ",", "self", ".", "N", "[", "1", "]", ",", "self", ".", "alpha", "[", "0", "]", ")", ")", "\n", "gat", "[", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "GCAT", "=", "nn", ".", "Sequential", "(", "*", "gat", ")", "# Graph Attentional Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "# NOTE: Because sigma is a functional, instead of the layer, then", "\n", "# we need to pick up the layer for the MLP part.", "\n", "            ", "if", "str", "(", "self", ".", "sigma", ")", ".", "find", "(", "'relu'", ")", ">=", "0", ":", "\n", "                ", "self", ".", "sigmaMLP", "=", "nn", ".", "ReLU", "(", ")", "\n", "", "elif", "str", "(", "self", ".", "sigma", ")", ".", "find", "(", "'tanh'", ")", ">=", "0", ":", "\n", "                ", "self", ".", "sigmaMLP", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n", "", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigmaMLP", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphConvolutionAttentionNetwork.forward": [[4059, 4073], ["architectures.GraphConvolutionAttentionNetwork.GCAT", "y.reshape.reshape.reshape", "architectures.GraphConvolutionAttentionNetwork.MLP", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph attentional layers", "\n", "y", "=", "self", ".", "GCAT", "(", "x", ")", "\n", "# Flatten the output", "\n", "y", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "y", ")", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphConvolutionAttentionNetwork.to": [[4075, 4087], ["super().to", "architectures.GraphConvolutionAttentionNetwork.S.to", "range", "architectures.GraphConvolutionAttentionNetwork.GCAT[].addGSO", "architectures.GraphConvolutionAttentionNetwork.GCAT[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "GCAT", "[", "2", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "GCAT", "[", "2", "*", "l", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.__init__": [[4162, 4326], ["torch.Module.__init__", "len", "architectures.EdgeVariantAttention.permFunction", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "len", "len", "len", "len", "len", "len", "GSO.reshape.reshape.reshape", "eval", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "range", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "len", "fc.append", "range", "len", "len", "len", "len", "alegnn.EdgeVariantAttentional", "alegnn.EdgeVariantAttentional", "architectures.EdgeVariantAttention.rho", "gat.append", "gat[].addGSO", "gat.append", "gat[].addGSO", "alegnn.EdgeVariantAttentional", "alegnn.EdgeVariantAttentional", "architectures.EdgeVariantAttention.rho", "alegnn.EdgeVariantAttentional", "alegnn.EdgeVariantAttentional", "architectures.EdgeVariantAttention.rho", "str().find", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "alegnn.EdgeVariantAttentional", "alegnn.EdgeVariantAttentional", "architectures.EdgeVariantAttention.rho", "str().find", "torch.Tanh", "torch.Tanh", "len", "architectures.EdgeVariantAttention.sigmaMLP", "torch.Linear", "torch.Linear", "str", "str"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph attentional layer", "\n", "dimNodeSignals", ",", "nFilterTaps", ",", "nAttentionHeads", ",", "bias", ",", "\n", "# Nonlinearity (nn.functional)", "\n", "nonlinearity", ",", "\n", "# Pooling", "\n", "nSelectedNodes", ",", "poolingFunction", ",", "poolingSize", ",", "\n", "# MLP in the end", "\n", "dimLayersMLP", ",", "\n", "# Structure", "\n", "GSO", ",", "order", "=", "None", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# dimNodeSignals should be a list and of size 1 more than nFilter taps.", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nFilterTaps", ")", "+", "1", "\n", "assert", "len", "(", "dimNodeSignals", ")", "==", "len", "(", "nAttentionHeads", ")", "+", "1", "\n", "# nSelectedNodes should be a list of size nFilterTaps, since the number", "\n", "# of nodes in the first layer is always the size of the graph", "\n", "assert", "len", "(", "nSelectedNodes", ")", "==", "len", "(", "nAttentionHeads", ")", "\n", "# poolingSize also has to be a list of the same size", "\n", "assert", "len", "(", "poolingSize", ")", "==", "len", "(", "nAttentionHeads", ")", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "# Store the values (using the notation in the paper):", "\n", "", "self", ".", "L", "=", "len", "(", "nAttentionHeads", ")", "# Number of graph filtering layers", "\n", "self", ".", "F", "=", "dimNodeSignals", "# Features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "P", "=", "nAttentionHeads", "# Attention Heads", "\n", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "[", "GSO", ".", "shape", "[", "1", "]", "]", "+", "nSelectedNodes", "# Number of nodes", "\n", "# See that we adding N_{0} = N as the number of nodes input the first", "\n", "# layer: this above is the list containing how many nodes are between", "\n", "# each layer.", "\n", "if", "order", "is", "not", "None", ":", "\n", "# If there's going to be reordering, then the value of the", "\n", "# permutation function will be given by the criteria in ", "\n", "# self.reorder. For instance, if self.reorder = 'Degree', then", "\n", "# we end up calling the function Utils.graphTools.permDegree.", "\n", "# We need to be sure that the function 'perm' + self.reorder", "\n", "# is available in the Utils.graphTools module.", "\n", "            ", "self", ".", "permFunction", "=", "eval", "(", "'Utils.graphTools.perm'", "+", "order", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "permFunction", "=", "alegnn", ".", "utils", ".", "graphTools", ".", "permIdentity", "\n", "# This is overriden if coarsening is selected, since the ordering", "\n", "# function is native to that pooling method.", "\n", "", "self", ".", "S", ",", "self", ".", "order", "=", "self", ".", "permFunction", "(", "GSO", ")", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "", "self", ".", "sigma", "=", "nonlinearity", "# This has to be a nn.functional instead of", "\n", "# just a nn", "\n", "self", ".", "rho", "=", "poolingFunction", "\n", "self", ".", "alpha", "=", "poolingSize", "\n", "self", ".", "dimLayersMLP", "=", "dimLayersMLP", "\n", "self", ".", "bias", "=", "bias", "\n", "# And now, we're finally ready to create the architecture:", "\n", "#\\\\\\ Graph Attentional Layers \\\\\\", "\n", "# OBS.: The last layer has to have concatenate False, whereas the rest", "\n", "# have concatenate True. So we go all the way except for the last layer", "\n", "gat", "=", "[", "]", "# Graph Attentional Layers", "\n", "if", "self", ".", "L", ">", "1", ":", "\n", "# First layer (this goes separate because there are not attention", "\n", "# heads increasing the number of features)", "\n", "#\\\\ Graph attention stage:", "\n", "            ", "gat", ".", "append", "(", "gml", ".", "EdgeVariantAttentional", "(", "self", ".", "F", "[", "0", "]", ",", "\n", "self", ".", "F", "[", "1", "]", ",", "\n", "self", ".", "K", "[", "0", "]", ",", "\n", "self", ".", "P", "[", "0", "]", ",", "\n", "self", ".", "E", ",", "\n", "self", ".", "bias", ",", "\n", "self", ".", "sigma", ",", "\n", "True", ")", ")", "\n", "gat", "[", "0", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "0", "]", ",", "self", ".", "N", "[", "1", "]", ",", "self", ".", "alpha", "[", "0", "]", ")", ")", "\n", "gat", "[", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# All the next layers (attention heads appear):", "\n", "for", "l", "in", "range", "(", "1", ",", "self", ".", "L", "-", "1", ")", ":", "\n", "#\\\\ Graph attention stage:", "\n", "                ", "gat", ".", "append", "(", "gml", ".", "EdgeVariantAttentional", "(", "self", ".", "F", "[", "l", "]", "*", "self", ".", "P", "[", "l", "-", "1", "]", ",", "\n", "self", ".", "F", "[", "l", "+", "1", "]", ",", "\n", "self", ".", "K", "[", "l", "]", ",", "\n", "self", ".", "P", "[", "l", "]", ",", "\n", "self", ".", "E", ",", "\n", "self", ".", "bias", ",", "\n", "self", ".", "sigma", ",", "\n", "True", ")", ")", "\n", "# There is a 2*l below here, because we have two elements per", "\n", "# layer: graph filter and pooling, so after each layer", "\n", "# we're actually adding elements to the (sequential) list.", "\n", "gat", "[", "2", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "l", "]", ",", "self", ".", "N", "[", "l", "+", "1", "]", ",", "self", ".", "alpha", "[", "l", "]", ")", ")", "\n", "# Same as before, this is 2*l+1", "\n", "gat", "[", "2", "*", "l", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And the last layer (set concatenate to False):", "\n", "#\\\\ Graph attention stage:", "\n", "", "gat", ".", "append", "(", "gml", ".", "EdgeVariantAttentional", "(", "self", ".", "F", "[", "self", ".", "L", "-", "1", "]", "*", "self", ".", "K", "[", "self", ".", "L", "-", "2", "]", ",", "\n", "self", ".", "F", "[", "self", ".", "L", "]", ",", "\n", "self", ".", "K", "[", "self", ".", "L", "-", "1", "]", ",", "\n", "self", ".", "P", "[", "self", ".", "L", "-", "1", "]", ",", "\n", "self", ".", "E", ",", "\n", "self", ".", "bias", ",", "\n", "self", ".", "sigma", ",", "\n", "False", ")", ")", "\n", "gat", "[", "2", "*", "(", "self", ".", "L", "-", "1", ")", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "self", ".", "L", "-", "1", "]", ",", "self", ".", "N", "[", "self", ".", "L", "]", ",", "\n", "self", ".", "alpha", "[", "self", ".", "L", "-", "1", "]", ")", ")", "\n", "gat", "[", "2", "*", "(", "self", ".", "L", "-", "1", ")", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "else", ":", "\n", "# If there's only one layer, it just go straightforward, adding a", "\n", "# False to the concatenation and no increase in the input features", "\n", "# due to attention heads", "\n", "            ", "gat", ".", "append", "(", "gml", ".", "EdgeVariantAttentional", "(", "self", ".", "F", "[", "0", "]", ",", "\n", "self", ".", "F", "[", "1", "]", ",", "\n", "self", ".", "K", "[", "0", "]", ",", "\n", "self", ".", "P", "[", "0", "]", ",", "\n", "self", ".", "E", ",", "\n", "self", ".", "bias", ",", "\n", "self", ".", "sigma", ",", "\n", "False", ")", ")", "\n", "gat", "[", "0", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "#\\\\ Pooling", "\n", "gat", ".", "append", "(", "self", ".", "rho", "(", "self", ".", "N", "[", "0", "]", ",", "self", ".", "N", "[", "1", "]", ",", "self", ".", "alpha", "[", "0", "]", ")", ")", "\n", "gat", "[", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "# And now feed them into the sequential", "\n", "", "self", ".", "EVGAT", "=", "nn", ".", "Sequential", "(", "*", "gat", ")", "# Graph Attentional Layers", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimLayersMLP", ")", ">", "0", ":", "# Maybe we don't want to MLP anything", "\n", "# The first layer has to connect whatever was left of the graph", "\n", "# signal, flattened.", "\n", "# NOTE: Because sigma is a functional, instead of the layer, then", "\n", "# we need to pick up the layer for the MLP part.", "\n", "            ", "if", "str", "(", "self", ".", "sigma", ")", ".", "find", "(", "'relu'", ")", ">=", "0", ":", "\n", "                ", "self", ".", "sigmaMLP", "=", "nn", ".", "ReLU", "(", ")", "\n", "", "elif", "str", "(", "self", ".", "sigma", ")", ".", "find", "(", "'tanh'", ")", ">=", "0", ":", "\n", "                ", "self", ".", "sigmaMLP", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n", "", "dimInputMLP", "=", "self", ".", "N", "[", "-", "1", "]", "*", "self", ".", "F", "[", "-", "1", "]", "\n", "# (i.e., we have N[-1] nodes left, each one described by F[-1]", "\n", "# features which means this will be flattened into a vector of size", "\n", "# N[-1]*F[-1])", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimInputMLP", ",", "dimLayersMLP", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimLayersMLP", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "sigmaMLP", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimLayersMLP", "[", "l", "]", ",", "dimLayersMLP", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "MLP", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.forward": [[4328, 4342], ["architectures.EdgeVariantAttention.EVGAT", "y.reshape.reshape.reshape", "architectures.EdgeVariantAttention.MLP", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Now we compute the forward call", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", "\n", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "F", "[", "0", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "N", "[", "0", "]", "\n", "# Reorder", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "self", ".", "order", "]", "# B x F x N", "\n", "# Let's call the graph attentional layers", "\n", "y", "=", "self", ".", "EVGAT", "(", "x", ")", "\n", "# Flatten the output", "\n", "y", "=", "y", ".", "reshape", "(", "batchSize", ",", "self", ".", "F", "[", "-", "1", "]", "*", "self", ".", "N", "[", "-", "1", "]", ")", "\n", "# And, feed it into the MLP", "\n", "return", "self", ".", "MLP", "(", "y", ")", "\n", "# If self.MLP is a sequential on an empty list it just does nothing.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to": [[4344, 4356], ["super().to", "architectures.EdgeVariantAttention.S.to", "range", "architectures.EdgeVariantAttention.EVGAT[].addGSO", "architectures.EdgeVariantAttention.EVGAT[].addGSO"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "# Because only the filter taps and the weights are registered as", "\n", "# parameters, when we do a .to(device) operation it does not move the", "\n", "# GSOs. So we need to move them ourselves.", "\n", "# Call the parent .to() method (to move the registered parameters)", "\n", "        ", "super", "(", ")", ".", "to", "(", "device", ")", "\n", "# Move the GSO", "\n", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "# And all the other variables derived from it.", "\n", "for", "l", "in", "range", "(", "self", ".", "L", ")", ":", "\n", "            ", "self", ".", "EVGAT", "[", "2", "*", "l", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "EVGAT", "[", "2", "*", "l", "+", "1", "]", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphRecurrentNN.__init__": [[4449, 4527], ["torch.Module.__init__", "alegnn.HiddenState", "alegnn.HiddenState", "alegnn.GraphFilter", "alegnn.GraphFilter", "architectures.GraphRecurrentNN.hiddenState.addGSO", "architectures.GraphRecurrentNN.outputState.addGSO", "torch.Sequential", "torch.Sequential", "len", "len", "GSO.reshape.reshape.reshape", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "fc.append", "range", "len", "len", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "len", "architectures.GraphRecurrentNN.nonlinearityReadout", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimInputSignals", ",", "\n", "dimOutputSignals", ",", "\n", "dimHiddenSignals", ",", "\n", "nFilterTaps", ",", "bias", ",", "\n", "# Nonlinearities", "\n", "nonlinearityHidden", ",", "\n", "nonlinearityOutput", ",", "\n", "nonlinearityReadout", ",", "# nn.Module", "\n", "# Local MLP in the end", "\n", "dimReadout", ",", "\n", "# Structure", "\n", "GSO", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# A list of two int, one for the number of filter taps (the computation", "\n", "# of the hidden state has the same number of filter taps)", "\n", "assert", "len", "(", "nFilterTaps", ")", "==", "2", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "F", "=", "dimInputSignals", "# Number of input features", "\n", "self", ".", "G", "=", "dimOutputSignals", "# Number of output features", "\n", "self", ".", "H", "=", "dimHiddenSignals", "# NUmber of hidden features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "# Store the rest of the variables", "\n", "self", ".", "sigma", "=", "nonlinearityHidden", "\n", "self", ".", "rho", "=", "nonlinearityOutput", "\n", "self", ".", "nonlinearityReadout", "=", "nonlinearityReadout", "\n", "self", ".", "dimReadout", "=", "dimReadout", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "GSO", ".", "shape", "[", "1", "]", "\n", "self", ".", "S", "=", "GSO", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "\n", "#\\\\\\ Hidden State RNN \\\\\\", "\n", "# Create the layer that generates the hidden state, and generate z0", "\n", "", "self", ".", "hiddenState", "=", "gml", ".", "HiddenState", "(", "self", ".", "F", ",", "self", ".", "H", ",", "self", ".", "K", "[", "0", "]", ",", "\n", "nonlinearity", "=", "self", ".", "sigma", ",", "E", "=", "self", ".", "E", ",", "\n", "bias", "=", "self", ".", "bias", ")", "\n", "#\\\\\\ Output Graph Filters \\\\\\", "\n", "self", ".", "outputState", "=", "gml", ".", "GraphFilter", "(", "self", ".", "H", ",", "self", ".", "G", ",", "self", ".", "K", "[", "1", "]", ",", "\n", "E", "=", "self", ".", "E", ",", "bias", "=", "self", ".", "bias", ")", "\n", "# Add the GSO for each graph filter", "\n", "self", ".", "hiddenState", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "outputState", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimReadout", ")", ">", "0", ":", "# Maybe we don't want to readout anything", "\n", "# The first layer has to connect whatever was left of the graph ", "\n", "# filtering stage to create the number of features required by", "\n", "# the readout layer", "\n", "            ", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "G", ",", "dimReadout", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimReadout", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "nonlinearityReadout", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimReadout", "[", "l", "]", ",", "dimReadout", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "Readout", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphRecurrentNN.splitForward": [[4529, 4563], ["torch.randn", "torch.randn", "torch.randn", "torch.randn", "architectures.GraphRecurrentNN.hiddenState", "z.reshape.reshape.reshape", "architectures.GraphRecurrentNN.outputState", "architectures.GraphRecurrentNN.rho", "yOut.reshape.reshape.reshape", "yOut.reshape.reshape.permute", "architectures.GraphRecurrentNN.Readout", "len", "len", "architectures.GraphRecurrentNN.permute"], "methods", ["None"], ["", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Check the dimensions of the input", "\n", "#   S: E x N x N", "\n", "#   x: B x T x F[0] x N", "\n", "        ", "assert", "len", "(", "self", ".", "S", ".", "shape", ")", "==", "3", "\n", "assert", "self", ".", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "N", "=", "self", ".", "S", ".", "shape", "[", "1", "]", "\n", "assert", "self", ".", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "\n", "assert", "x", ".", "shape", "[", "3", "]", "==", "N", "\n", "\n", "# This can be generated here or generated outside of here, not clear yet", "\n", "# what's the most coherent option", "\n", "z0", "=", "torch", ".", "randn", "(", "(", "B", ",", "self", ".", "H", ",", "N", ")", ",", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Compute the trajectory of hidden states", "\n", "z", ",", "_", "=", "self", ".", "hiddenState", "(", "x", ",", "z0", ")", "\n", "z", "=", "z", ".", "reshape", "(", "(", "B", "*", "T", ",", "self", ".", "H", ",", "N", ")", ")", "\n", "# Compute the output trajectory from the hidden states", "\n", "yOut", "=", "self", ".", "outputState", "(", "z", ")", "\n", "yOut", "=", "self", ".", "rho", "(", "yOut", ")", "# Don't forget the nonlinearity!", "\n", "yOut", "=", "yOut", ".", "reshape", "(", "(", "B", ",", "T", ",", "self", ".", "G", ",", "N", ")", ")", "\n", "#   B x T x G x N", "\n", "# Change the order, for the readout", "\n", "y", "=", "yOut", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# B x T x N x G", "\n", "# And, feed it into the Readout layer", "\n", "y", "=", "self", ".", "Readout", "(", "y", ")", "# B x T x N x dimReadout[-1]", "\n", "# Reshape and return", "\n", "return", "y", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", ",", "yOut", "\n", "# B x T x dimReadout[-1] x N, B x T x dimFeatures[-1] x N", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphRecurrentNN.forward": [[4565, 4575], ["architectures.GraphRecurrentNN.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphRecurrentNN.singleNodeForward": [[4576, 4632], ["numpy.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "architectures.GraphRecurrentNN.forward", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.squeeze", "torch.matmul.squeeze", "type", "architectures.GraphRecurrentNN.order.index", "numpy.array", "numpy.tile", "type", "numpy.array", "type", "type", "type", "architectures.GraphRecurrentNN.order.index", "type", "numpy.array", "numpy.array.astype", "numpy.arange", "numpy.where", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.forward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "singleNodeForward", "(", "self", ",", "x", ",", "nodes", ")", ":", "\n", "\n", "# x is of shape B x T x F[0] x N", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "N", "=", "x", ".", "shape", "[", "3", "]", "\n", "\n", "# nodes is either an int, or a list/np.array of ints of size B", "\n", "assert", "type", "(", "nodes", ")", "is", "int", "or", "type", "(", "nodes", ")", "is", "list", "or", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", "\n", "\n", "# Let us start by building the selection matrix", "\n", "# This selection matrix has to be a matrix of shape", "\n", "#   B x 1 x N[-1] x 1", "\n", "# so that when multiplying with the output of the forward, we get a", "\n", "#   B x T x dimRedout[-1] x 1", "\n", "# and we just squeeze the last dimension", "\n", "\n", "# TODO: The big question here is if multiplying by a matrix is faster", "\n", "# than doing torch.index_select", "\n", "\n", "# Let's always work with numpy arrays to make it easier.", "\n", "if", "type", "(", "nodes", ")", "is", "int", ":", "\n", "# Change the node number to accommodate the new order", "\n", "            ", "nodes", "=", "self", ".", "order", ".", "index", "(", "nodes", ")", "\n", "# If it's int, make it a list and an array", "\n", "nodes", "=", "np", ".", "array", "(", "[", "nodes", "]", ",", "dtype", "=", "np", ".", "int", ")", "\n", "# And repeat for the number of batches", "\n", "nodes", "=", "np", ".", "tile", "(", "nodes", ",", "batchSize", ")", "\n", "", "if", "type", "(", "nodes", ")", "is", "list", ":", "\n", "            ", "newNodes", "=", "[", "self", ".", "order", ".", "index", "(", "n", ")", "for", "n", "in", "nodes", "]", "\n", "nodes", "=", "np", ".", "array", "(", "newNodes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "", "elif", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", ":", "\n", "            ", "newNodes", "=", "np", ".", "array", "(", "[", "np", ".", "where", "(", "np", ".", "array", "(", "self", ".", "order", ")", "==", "n", ")", "[", "0", "]", "[", "0", "]", "for", "n", "in", "nodes", "]", ")", "\n", "nodes", "=", "newNodes", ".", "astype", "(", "np", ".", "int", ")", "\n", "# Now, nodes is an np.int np.ndarray with shape batchSize", "\n", "\n", "# Build the selection matrix", "\n", "", "selectionMatrix", "=", "np", ".", "zeros", "(", "[", "batchSize", ",", "1", ",", "N", ",", "1", "]", ")", "\n", "selectionMatrix", "[", "np", ".", "arange", "(", "batchSize", ")", ",", "nodes", ",", "0", "]", "=", "1.", "\n", "# And convert it to a tensor", "\n", "selectionMatrix", "=", "torch", ".", "tensor", "(", "selectionMatrix", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Now compute the output", "\n", "y", "=", "self", ".", "forward", "(", "x", ")", "\n", "# This output is of size B x T x dimReadout[-1] x N", "\n", "\n", "# Multiply the output", "\n", "y", "=", "torch", ".", "matmul", "(", "y", ",", "selectionMatrix", ")", "\n", "#   B x T x dimReadout[-1] x 1", "\n", "\n", "# Squeeze the last dimension and return", "\n", "return", "y", ".", "squeeze", "(", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GraphRecurrentNN.changeGSO": [[4633, 4662], ["alegnn.utils.dataTools.changeDataType", "alegnn.utils.dataTools.changeDataType", "architectures.GraphRecurrentNN.hiddenState.addGSO", "architectures.GraphRecurrentNN.outputState.addGSO", "len", "GSO.reshape.reshape.reshape", "dir", "architectures.GraphRecurrentNN.S.to", "len", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "changeGSO", "(", "self", ",", "GSO", ")", ":", "\n", "\n", "# We use this to change the GSO, using the same graph filters.", "\n", "\n", "# Check that the new GSO has the correct", "\n", "        ", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "\n", "# Get dataType and device of the current GSO, so when we replace it, it", "\n", "# is still located in the same type and the same device.", "\n", "", "dataType", "=", "self", ".", "S", ".", "dtype", "\n", "if", "'device'", "in", "dir", "(", "self", ".", "S", ")", ":", "\n", "            ", "device", "=", "self", ".", "S", ".", "device", "\n", "", "else", ":", "\n", "            ", "device", "=", "None", "\n", "\n", "", "self", ".", "S", "=", "GSO", "\n", "# Change data type and device as required", "\n", "self", ".", "S", "=", "changeDataType", "(", "self", ".", "S", ",", "dataType", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "\n", "# Add the GSO for each graph filter", "\n", "", "self", ".", "hiddenState", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "outputState", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__": [[4759, 4850], ["torch.Module.__init__", "alegnn.GraphFilter", "alegnn.GraphFilter", "architectures.GatedGraphRecurrentNN.hiddenState.addGSO", "architectures.GatedGraphRecurrentNN.outputState.addGSO", "torch.Sequential", "torch.Sequential", "len", "len", "GSO.reshape.reshape.reshape", "repr", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "alegnn.TimeGatedHiddenState", "alegnn.TimeGatedHiddenState", "len", "fc.append", "range", "len", "len", "alegnn.NodeGatedHiddenState", "alegnn.NodeGatedHiddenState", "torch.Linear", "torch.Linear", "fc.append", "fc.append", "alegnn.EdgeGatedHiddenState", "alegnn.EdgeGatedHiddenState", "len", "architectures.GatedGraphRecurrentNN.nonlinearityReadout", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.__init__", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO"], ["def", "__init__", "(", "self", ",", "\n", "# Graph filtering", "\n", "dimInputSignals", ",", "\n", "dimOutputSignals", ",", "\n", "dimHiddenSignals", ",", "\n", "nFilterTaps", ",", "bias", ",", "\n", "# Nonlinearities", "\n", "nonlinearityHidden", ",", "\n", "nonlinearityOutput", ",", "\n", "nonlinearityReadout", ",", "# nn.Module", "\n", "# Local MLP in the end", "\n", "dimReadout", ",", "\n", "# Structure", "\n", "GSO", ",", "\n", "# Gating", "\n", "gateType", ")", ":", "\n", "# Initialize parent:", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# A list of two int, one for the number of filter taps (the computation", "\n", "# of the hidden state has the same number of filter taps)", "\n", "assert", "len", "(", "nFilterTaps", ")", "==", "2", "\n", "\n", "# Store the values (using the notation in the paper):", "\n", "self", ".", "F", "=", "dimInputSignals", "# Number of input features", "\n", "self", ".", "G", "=", "dimOutputSignals", "# Number of output features", "\n", "self", ".", "H", "=", "dimHiddenSignals", "# NUmber of hidden features", "\n", "self", ".", "K", "=", "nFilterTaps", "# Filter taps", "\n", "self", ".", "bias", "=", "bias", "# Boolean", "\n", "# Store the rest of the variables", "\n", "self", ".", "sigma", "=", "nonlinearityHidden", "\n", "self", ".", "rho", "=", "nonlinearityOutput", "\n", "self", ".", "nonlinearityReadout", "=", "nonlinearityReadout", "\n", "self", ".", "dimReadout", "=", "dimReadout", "\n", "# Check whether the GSO has features or not. After that, always handle", "\n", "# it as a matrix of dimension E x N x N.", "\n", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "", "self", ".", "E", "=", "GSO", ".", "shape", "[", "0", "]", "# Number of edge features", "\n", "self", ".", "N", "=", "GSO", ".", "shape", "[", "1", "]", "\n", "self", ".", "S", "=", "GSO", "\n", "if", "'torch'", "not", "in", "repr", "(", "self", ".", "S", ".", "dtype", ")", ":", "\n", "            ", "self", ".", "S", "=", "torch", ".", "tensor", "(", "self", ".", "S", ")", "\n", "\n", "#\\\\\\ Hidden State RNN \\\\\\", "\n", "# Create the layer that generates the hidden state, and generate z0", "\n", "# The type of hidden layer depends on the type of gating", "\n", "", "assert", "gateType", "==", "'time'", "or", "gateType", "==", "'node'", "or", "gateType", "==", "'edge'", "\n", "if", "gateType", "==", "'time'", ":", "\n", "            ", "self", ".", "hiddenState", "=", "gml", ".", "TimeGatedHiddenState", "(", "self", ".", "F", ",", "self", ".", "H", ",", "self", ".", "K", "[", "0", "]", ",", "\n", "nonlinearity", "=", "self", ".", "sigma", ",", "E", "=", "self", ".", "E", ",", "\n", "bias", "=", "self", ".", "bias", ")", "\n", "", "elif", "gateType", "==", "'node'", ":", "\n", "            ", "self", ".", "hiddenState", "=", "gml", ".", "NodeGatedHiddenState", "(", "self", ".", "F", ",", "self", ".", "H", ",", "self", ".", "K", "[", "0", "]", ",", "\n", "nonlinearity", "=", "self", ".", "sigma", ",", "E", "=", "self", ".", "E", ",", "\n", "bias", "=", "self", ".", "bias", ")", "\n", "", "elif", "gateType", "==", "'edge'", ":", "\n", "            ", "self", ".", "hiddenState", "=", "gml", ".", "EdgeGatedHiddenState", "(", "self", ".", "F", ",", "self", ".", "H", ",", "self", ".", "K", "[", "0", "]", ",", "\n", "nonlinearity", "=", "self", ".", "sigma", ",", "E", "=", "self", ".", "E", ",", "\n", "bias", "=", "self", ".", "bias", ")", "\n", "#\\\\\\ Output Graph Filters \\\\\\", "\n", "", "self", ".", "outputState", "=", "gml", ".", "GraphFilter", "(", "self", ".", "H", ",", "self", ".", "G", ",", "self", ".", "K", "[", "1", "]", ",", "\n", "E", "=", "self", ".", "E", ",", "bias", "=", "self", ".", "bias", ")", "\n", "# Add the GSO for each graph filter", "\n", "self", ".", "hiddenState", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "outputState", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "\n", "#\\\\\\ MLP (Fully Connected Layers) \\\\\\", "\n", "fc", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "dimReadout", ")", ">", "0", ":", "# Maybe we don't want to readout anything", "\n", "# The first layer has to connect whatever was left of the graph ", "\n", "# filtering stage to create the number of features required by", "\n", "# the readout layer", "\n", "            ", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "G", ",", "dimReadout", "[", "0", "]", ",", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# The last linear layer cannot be followed by nonlinearity, because", "\n", "# usually, this nonlinearity depends on the loss function (for", "\n", "# instance, if we have a classification problem, this nonlinearity", "\n", "# is already handled by the cross entropy loss or we add a softmax.)", "\n", "for", "l", "in", "range", "(", "len", "(", "dimReadout", ")", "-", "1", ")", ":", "\n", "# Add the nonlinearity because there's another linear layer", "\n", "# coming", "\n", "                ", "fc", ".", "append", "(", "self", ".", "nonlinearityReadout", "(", ")", ")", "\n", "# And add the linear layer", "\n", "fc", ".", "append", "(", "nn", ".", "Linear", "(", "dimReadout", "[", "l", "]", ",", "dimReadout", "[", "l", "+", "1", "]", ",", "\n", "bias", "=", "self", ".", "bias", ")", ")", "\n", "# And we're done", "\n", "", "", "self", ".", "Readout", "=", "nn", ".", "Sequential", "(", "*", "fc", ")", "\n", "# so we finally have the architecture.", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward": [[4852, 4886], ["torch.randn", "torch.randn", "torch.randn", "torch.randn", "architectures.GatedGraphRecurrentNN.hiddenState", "z.reshape.reshape.reshape", "architectures.GatedGraphRecurrentNN.outputState", "architectures.GatedGraphRecurrentNN.rho", "yOut.reshape.reshape.reshape", "yOut.reshape.reshape.permute", "architectures.GatedGraphRecurrentNN.Readout", "len", "len", "architectures.GatedGraphRecurrentNN.permute"], "methods", ["None"], ["", "def", "splitForward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Check the dimensions of the input", "\n", "#   S: E x N x N", "\n", "#   x: B x T x F[0] x N", "\n", "        ", "assert", "len", "(", "self", ".", "S", ".", "shape", ")", "==", "3", "\n", "assert", "self", ".", "S", ".", "shape", "[", "0", "]", "==", "self", ".", "E", "\n", "N", "=", "self", ".", "S", ".", "shape", "[", "1", "]", "\n", "assert", "self", ".", "S", ".", "shape", "[", "2", "]", "==", "N", "\n", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "T", "=", "x", ".", "shape", "[", "1", "]", "\n", "assert", "x", ".", "shape", "[", "2", "]", "==", "self", ".", "F", "\n", "assert", "x", ".", "shape", "[", "3", "]", "==", "N", "\n", "\n", "# This can be generated here or generated outside of here, not clear yet", "\n", "# what's the most coherent option", "\n", "z0", "=", "torch", ".", "randn", "(", "(", "B", ",", "self", ".", "H", ",", "N", ")", ",", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Compute the trajectory of hidden states", "\n", "z", ",", "_", "=", "self", ".", "hiddenState", "(", "x", ",", "z0", ")", "\n", "z", "=", "z", ".", "reshape", "(", "(", "B", "*", "T", ",", "self", ".", "H", ",", "N", ")", ")", "\n", "# Compute the output trajectory from the hidden states", "\n", "yOut", "=", "self", ".", "outputState", "(", "z", ")", "\n", "yOut", "=", "self", ".", "rho", "(", "yOut", ")", "# Don't forget the nonlinearity!", "\n", "yOut", "=", "yOut", ".", "reshape", "(", "(", "B", ",", "T", ",", "self", ".", "G", ",", "N", ")", ")", "\n", "#   B x T x G x N", "\n", "# Change the order, for the readout", "\n", "y", "=", "yOut", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "# B x T x N x G", "\n", "# And, feed it into the Readout layer", "\n", "y", "=", "self", ".", "Readout", "(", "y", ")", "# B x T x N x dimReadout[-1]", "\n", "# Reshape and return", "\n", "return", "y", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", ",", "yOut", "\n", "# B x T x dimReadout[-1] x N, B x T x dimFeatures[-1] x N", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.forward": [[4888, 4898], ["architectures.GatedGraphRecurrentNN.splitForward"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.splitForward"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Most of the times, we just need the actual, last output. But, since in", "\n", "# this case, we also want to compare with the output of the GNN itself,", "\n", "# we need to create this other forward funciton that takes both outputs", "\n", "# (the GNN and the MLP) and returns only the MLP output in the proper", "\n", "# forward function.", "\n", "        ", "output", ",", "_", "=", "self", ".", "splitForward", "(", "x", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.singleNodeForward": [[4899, 4955], ["numpy.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "architectures.GatedGraphRecurrentNN.forward", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.squeeze", "torch.matmul.squeeze", "type", "architectures.GatedGraphRecurrentNN.order.index", "numpy.array", "numpy.tile", "type", "numpy.array", "type", "type", "type", "architectures.GatedGraphRecurrentNN.order.index", "type", "numpy.array", "numpy.array.astype", "numpy.arange", "numpy.where", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.forward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.astype"], ["", "def", "singleNodeForward", "(", "self", ",", "x", ",", "nodes", ")", ":", "\n", "\n", "# x is of shape B x T x F[0] x N", "\n", "        ", "batchSize", "=", "x", ".", "shape", "[", "0", "]", "\n", "N", "=", "x", ".", "shape", "[", "3", "]", "\n", "\n", "# nodes is either an int, or a list/np.array of ints of size B", "\n", "assert", "type", "(", "nodes", ")", "is", "int", "or", "type", "(", "nodes", ")", "is", "list", "or", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", "\n", "\n", "# Let us start by building the selection matrix", "\n", "# This selection matrix has to be a matrix of shape", "\n", "#   B x 1 x N[-1] x 1", "\n", "# so that when multiplying with the output of the forward, we get a", "\n", "#   B x T x dimRedout[-1] x 1", "\n", "# and we just squeeze the last dimension", "\n", "\n", "# TODO: The big question here is if multiplying by a matrix is faster", "\n", "# than doing torch.index_select", "\n", "\n", "# Let's always work with numpy arrays to make it easier.", "\n", "if", "type", "(", "nodes", ")", "is", "int", ":", "\n", "# Change the node number to accommodate the new order", "\n", "            ", "nodes", "=", "self", ".", "order", ".", "index", "(", "nodes", ")", "\n", "# If it's int, make it a list and an array", "\n", "nodes", "=", "np", ".", "array", "(", "[", "nodes", "]", ",", "dtype", "=", "np", ".", "int", ")", "\n", "# And repeat for the number of batches", "\n", "nodes", "=", "np", ".", "tile", "(", "nodes", ",", "batchSize", ")", "\n", "", "if", "type", "(", "nodes", ")", "is", "list", ":", "\n", "            ", "newNodes", "=", "[", "self", ".", "order", ".", "index", "(", "n", ")", "for", "n", "in", "nodes", "]", "\n", "nodes", "=", "np", ".", "array", "(", "newNodes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "", "elif", "type", "(", "nodes", ")", "is", "np", ".", "ndarray", ":", "\n", "            ", "newNodes", "=", "np", ".", "array", "(", "[", "np", ".", "where", "(", "np", ".", "array", "(", "self", ".", "order", ")", "==", "n", ")", "[", "0", "]", "[", "0", "]", "for", "n", "in", "nodes", "]", ")", "\n", "nodes", "=", "newNodes", ".", "astype", "(", "np", ".", "int", ")", "\n", "# Now, nodes is an np.int np.ndarray with shape batchSize", "\n", "\n", "# Build the selection matrix", "\n", "", "selectionMatrix", "=", "np", ".", "zeros", "(", "[", "batchSize", ",", "1", ",", "N", ",", "1", "]", ")", "\n", "selectionMatrix", "[", "np", ".", "arange", "(", "batchSize", ")", ",", "nodes", ",", "0", "]", "=", "1.", "\n", "# And convert it to a tensor", "\n", "selectionMatrix", "=", "torch", ".", "tensor", "(", "selectionMatrix", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", "device", "=", "x", ".", "device", ")", "\n", "\n", "# Now compute the output", "\n", "y", "=", "self", ".", "forward", "(", "x", ")", "\n", "# This output is of size B x T x dimReadout[-1] x N", "\n", "\n", "# Multiply the output", "\n", "y", "=", "torch", ".", "matmul", "(", "y", ",", "selectionMatrix", ")", "\n", "#   B x T x dimReadout[-1] x 1", "\n", "\n", "# Squeeze the last dimension and return", "\n", "return", "y", ".", "squeeze", "(", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.changeGSO": [[4956, 4985], ["alegnn.utils.dataTools.changeDataType", "alegnn.utils.dataTools.changeDataType", "architectures.GatedGraphRecurrentNN.hiddenState.addGSO", "architectures.GatedGraphRecurrentNN.outputState.addGSO", "len", "GSO.reshape.reshape.reshape", "dir", "architectures.GatedGraphRecurrentNN.S.to", "len", "len"], "methods", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.changeDataType", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.graphML.EdgeGatedHiddenState.addGSO", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to"], ["", "def", "changeGSO", "(", "self", ",", "GSO", ")", ":", "\n", "\n", "# We use this to change the GSO, using the same graph filters.", "\n", "\n", "# Check that the new GSO has the correct", "\n", "        ", "assert", "len", "(", "GSO", ".", "shape", ")", "==", "2", "or", "len", "(", "GSO", ".", "shape", ")", "==", "3", "\n", "if", "len", "(", "GSO", ".", "shape", ")", "==", "2", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "0", "]", "==", "GSO", ".", "shape", "[", "1", "]", "\n", "GSO", "=", "GSO", ".", "reshape", "(", "[", "1", ",", "GSO", ".", "shape", "[", "0", "]", ",", "GSO", ".", "shape", "[", "1", "]", "]", ")", "# 1 x N x N", "\n", "", "else", ":", "\n", "            ", "assert", "GSO", ".", "shape", "[", "1", "]", "==", "GSO", ".", "shape", "[", "2", "]", "# E x N x N", "\n", "\n", "# Get dataType and device of the current GSO, so when we replace it, it", "\n", "# is still located in the same type and the same device.", "\n", "", "dataType", "=", "self", ".", "S", ".", "dtype", "\n", "if", "'device'", "in", "dir", "(", "self", ".", "S", ")", ":", "\n", "            ", "device", "=", "self", ".", "S", ".", "device", "\n", "", "else", ":", "\n", "            ", "device", "=", "None", "\n", "\n", "", "self", ".", "S", "=", "GSO", "\n", "# Change data type and device as required", "\n", "self", ".", "S", "=", "changeDataType", "(", "self", ".", "S", ",", "dataType", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "S", "=", "self", ".", "S", ".", "to", "(", "device", ")", "\n", "\n", "# Add the GSO for each graph filter", "\n", "", "self", ".", "hiddenState", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "self", ".", "outputState", ".", "addGSO", "(", "self", ".", "S", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate": [[18, 90], ["data.getSamples", "xTest.to.to", "yTest.to.to", "model.load", "model.load", "data.evaluate.item", "data.evaluate.item", "kwargs.keys", "torch.no_grad", "model.archit", "data.evaluate", "torch.no_grad", "model.archit", "data.evaluate", "os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "open", "pickle.dump"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.getSamples", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate"], ["def", "evaluate", "(", "model", ",", "data", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    evaluate: evaluate a model using classification error\n    \n    Input:\n        model (model class): class from Modules.model\n        data (data class): a data class from the Utils.dataTools; it needs to\n            have a getSamples method and an evaluate method.\n        doPrint (optional, bool): if True prints results\n    \n    Output:\n        evalVars (dict): 'errorBest' contains the error rate for the best\n            model, and 'errorLast' contains the error rate for the last model\n    \"\"\"", "\n", "\n", "# Get the device we're working on", "\n", "device", "=", "model", ".", "device", "\n", "\n", "if", "'doSaveVars'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "doSaveVars", "=", "kwargs", "[", "'doSaveVars'", "]", "\n", "", "else", ":", "\n", "        ", "doSaveVars", "=", "True", "\n", "\n", "########", "\n", "# DATA #", "\n", "########", "\n", "\n", "", "xTest", ",", "yTest", "=", "data", ".", "getSamples", "(", "'test'", ")", "\n", "xTest", "=", "xTest", ".", "to", "(", "device", ")", "\n", "yTest", "=", "yTest", ".", "to", "(", "device", ")", "\n", "\n", "##############", "\n", "# BEST MODEL #", "\n", "##############", "\n", "\n", "model", ".", "load", "(", "label", "=", "'Best'", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Process the samples", "\n", "        ", "yHatTest", "=", "model", ".", "archit", "(", "xTest", ")", "\n", "# yHatTest is of shape", "\n", "#   testSize x numberOfClasses", "\n", "# We compute the error", "\n", "costBest", "=", "data", ".", "evaluate", "(", "yHatTest", ",", "yTest", ")", "\n", "\n", "##############", "\n", "# LAST MODEL #", "\n", "##############", "\n", "\n", "", "model", ".", "load", "(", "label", "=", "'Last'", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Process the samples", "\n", "        ", "yHatTest", "=", "model", ".", "archit", "(", "xTest", ")", "\n", "# yHatTest is of shape", "\n", "#   testSize x numberOfClasses", "\n", "# We compute the error", "\n", "costLast", "=", "data", ".", "evaluate", "(", "yHatTest", ",", "yTest", ")", "\n", "\n", "", "evalVars", "=", "{", "}", "\n", "evalVars", "[", "'costBest'", "]", "=", "costBest", ".", "item", "(", ")", "\n", "evalVars", "[", "'costLast'", "]", "=", "costLast", ".", "item", "(", ")", "\n", "\n", "if", "doSaveVars", ":", "\n", "        ", "saveDirVars", "=", "os", ".", "path", ".", "join", "(", "model", ".", "saveDir", ",", "'evalVars'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "saveDirVars", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "saveDirVars", ")", "\n", "", "pathToFile", "=", "os", ".", "path", ".", "join", "(", "saveDirVars", ",", "model", ".", "name", "+", "'evalVars.pkl'", ")", "\n", "with", "open", "(", "pathToFile", ",", "'wb'", ")", "as", "evalVarsFile", ":", "\n", "            ", "pickle", ".", "dump", "(", "evalVars", ",", "evalVarsFile", ")", "\n", "\n", "", "", "return", "evalVars", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluateSingleNode": [[91, 169], ["data.getSamples", "xTest.to.to", "yTest.to.to", "data.getLabelID", "model.load", "model.load", "data.evaluate.item", "data.evaluate.item", "dir", "dir", "kwargs.keys", "torch.no_grad", "model.archit.singleNodeForward", "data.evaluate", "torch.no_grad", "model.archit.singleNodeForward", "data.evaluate", "os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "open", "pickle.dump"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools._data.getSamples", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.EdgeVariantAttention.to", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.MovieLens.getLabelID", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.singleNodeForward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.architectures.GatedGraphRecurrentNN.singleNodeForward", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate"], ["", "def", "evaluateSingleNode", "(", "model", ",", "data", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    evaluateSingleNode: evaluate a model that has a single node forward\n    \n    Input:\n        model (model class): class from Modules.model, needs to have a \n            'singleNodeForward' method\n        data (data class): a data class from the Utils.dataTools; it needs to\n            have a getSamples method and an evaluate method and it also needs to\n            have a 'getLabelID' method\n        doPrint (optional, bool): if True prints results\n    \n    Output:\n        evalVars (dict): 'errorBest' contains the error rate for the best\n            model, and 'errorLast' contains the error rate for the last model\n    \"\"\"", "\n", "\n", "assert", "'singleNodeForward'", "in", "dir", "(", "model", ".", "archit", ")", "\n", "assert", "'getLabelID'", "in", "dir", "(", "data", ")", "\n", "\n", "# Get the device we're working on", "\n", "device", "=", "model", ".", "device", "\n", "\n", "if", "'doSaveVars'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "doSaveVars", "=", "kwargs", "[", "'doSaveVars'", "]", "\n", "", "else", ":", "\n", "        ", "doSaveVars", "=", "True", "\n", "\n", "########", "\n", "# DATA #", "\n", "########", "\n", "\n", "", "xTest", ",", "yTest", "=", "data", ".", "getSamples", "(", "'test'", ")", "\n", "xTest", "=", "xTest", ".", "to", "(", "device", ")", "\n", "yTest", "=", "yTest", ".", "to", "(", "device", ")", "\n", "targetIDs", "=", "data", ".", "getLabelID", "(", "'test'", ")", "\n", "\n", "##############", "\n", "# BEST MODEL #", "\n", "##############", "\n", "\n", "model", ".", "load", "(", "label", "=", "'Best'", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Process the samples", "\n", "        ", "yHatTest", "=", "model", ".", "archit", ".", "singleNodeForward", "(", "xTest", ",", "targetIDs", ")", "\n", "# yHatTest is of shape", "\n", "#   testSize x numberOfClasses", "\n", "# We compute the error", "\n", "costBest", "=", "data", ".", "evaluate", "(", "yHatTest", ",", "yTest", ")", "\n", "\n", "##############", "\n", "# LAST MODEL #", "\n", "##############", "\n", "\n", "", "model", ".", "load", "(", "label", "=", "'Last'", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Process the samples", "\n", "        ", "yHatTest", "=", "model", ".", "archit", ".", "singleNodeForward", "(", "xTest", ",", "targetIDs", ")", "\n", "# yHatTest is of shape", "\n", "#   testSize x numberOfClasses", "\n", "# We compute the error", "\n", "costLast", "=", "data", ".", "evaluate", "(", "yHatTest", ",", "yTest", ")", "\n", "\n", "", "evalVars", "=", "{", "}", "\n", "evalVars", "[", "'costBest'", "]", "=", "costBest", ".", "item", "(", ")", "\n", "evalVars", "[", "'costLast'", "]", "=", "costLast", ".", "item", "(", ")", "\n", "\n", "if", "doSaveVars", ":", "\n", "        ", "saveDirVars", "=", "os", ".", "path", ".", "join", "(", "model", ".", "saveDir", ",", "'evalVars'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "saveDirVars", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "saveDirVars", ")", "\n", "", "pathToFile", "=", "os", ".", "path", ".", "join", "(", "saveDirVars", ",", "model", ".", "name", "+", "'evalVars.pkl'", ")", "\n", "with", "open", "(", "pathToFile", ",", "'wb'", ")", "as", "evalVarsFile", ":", "\n", "            ", "pickle", ".", "dump", "(", "evalVars", ",", "evalVarsFile", ")", "\n", "\n", "", "", "return", "evalVars", "\n", "\n"]], "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluateFlocking": [[170, 329], ["data.getData", "data.getData", "model.load", "data.computeTrajectory", "model.load", "data.computeTrajectory", "os.path.join", "os.path.join", "data.saveVideo", "data.saveVideo", "data.evaluate", "data.evaluate", "data.evaluate", "data.evaluate", "kwargs.keys", "kwargs.keys", "kwargs.keys", "kwargs.keys", "print", "print", "print", "print", "os.path.exists", "os.mkdir", "os.path.join", "os.path.join", "os.path.exists", "os.mkdir", "print", "os.path.join", "os.path.join", "print", "kwargs.keys", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir"], "function", ["home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.TwentyNews.getData", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeTrajectory", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.model.Model.load", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.computeTrajectory", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.saveVideo", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.utils.dataTools.Flocking.saveVideo", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate", "home.repos.pwc.inspect_result.alelab-upenn_graph-neural-networks.modules.evaluation.evaluate"], ["", "def", "evaluateFlocking", "(", "model", ",", "data", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    evaluateClassif: evaluate a model using the flocking cost of velocity \n        variacne of the team\n    \n    Input:\n        model (model class): class from Modules.model\n        data (data class): the data class that generates the flocking data\n        doPrint (optional; bool, default: True): if True prints results\n        nVideos (optional; int, default: 3): number of videos to save\n        graphNo (optional): identify the run with a number\n        realizationNo (optional): identify the run with another number\n    \n    Output:\n        evalVars (dict):\n            'costBestFull': cost of the best model over the full trajectory\n            'costBestEnd': cost of the best model at the end of the trajectory\n            'costLastFull': cost of the last model over the full trajectory\n            'costLastEnd': cost of the last model at the end of the trajectory\n    \"\"\"", "\n", "\n", "if", "'doPrint'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "doPrint", "=", "kwargs", "[", "'doPrint'", "]", "\n", "", "else", ":", "\n", "        ", "doPrint", "=", "True", "\n", "\n", "", "if", "'nVideos'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "nVideos", "=", "kwargs", "[", "'nVideos'", "]", "\n", "", "else", ":", "\n", "        ", "nVideos", "=", "3", "\n", "\n", "", "if", "'graphNo'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "graphNo", "=", "kwargs", "[", "'graphNo'", "]", "\n", "", "else", ":", "\n", "        ", "graphNo", "=", "-", "1", "\n", "\n", "", "if", "'realizationNo'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "        ", "if", "'graphNo'", "in", "kwargs", ".", "keys", "(", ")", ":", "\n", "            ", "realizationNo", "=", "kwargs", "[", "'realizationNo'", "]", "\n", "", "else", ":", "\n", "            ", "graphNo", "=", "kwargs", "[", "'realizationNo'", "]", "\n", "realizationNo", "=", "-", "1", "\n", "", "", "else", ":", "\n", "        ", "realizationNo", "=", "-", "1", "\n", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "#\\\\\\ TRAJECTORIES \\\\\\", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "\n", "########", "\n", "# DATA #", "\n", "########", "\n", "\n", "# Initial data", "\n", "", "initPosTest", "=", "data", ".", "getData", "(", "'initPos'", ",", "'test'", ")", "\n", "initVelTest", "=", "data", ".", "getData", "(", "'initVel'", ",", "'test'", ")", "\n", "\n", "##############", "\n", "# BEST MODEL #", "\n", "##############", "\n", "\n", "model", ".", "load", "(", "label", "=", "'Best'", ")", "\n", "\n", "if", "doPrint", ":", "\n", "        ", "print", "(", "\"\\tComputing learned trajectory for best model...\"", ",", "\n", "end", "=", "' '", ",", "flush", "=", "True", ")", "\n", "\n", "", "posTestBest", ",", "velTestBest", ",", "accelTestBest", ",", "stateTestBest", ",", "commGraphTestBest", "=", "data", ".", "computeTrajectory", "(", "initPosTest", ",", "initVelTest", ",", "data", ".", "duration", ",", "\n", "archit", "=", "model", ".", "archit", ")", "\n", "\n", "if", "doPrint", ":", "\n", "        ", "print", "(", "\"OK\"", ")", "\n", "\n", "##############", "\n", "# LAST MODEL #", "\n", "##############", "\n", "\n", "", "model", ".", "load", "(", "label", "=", "'Last'", ")", "\n", "\n", "if", "doPrint", ":", "\n", "        ", "print", "(", "\"\\tComputing learned trajectory for last model...\"", ",", "\n", "end", "=", "' '", ",", "flush", "=", "True", ")", "\n", "\n", "", "posTestLast", ",", "velTestLast", ",", "accelTestLast", ",", "stateTestLast", ",", "commGraphTestLast", "=", "data", ".", "computeTrajectory", "(", "initPosTest", ",", "initVelTest", ",", "data", ".", "duration", ",", "\n", "archit", "=", "model", ".", "archit", ")", "\n", "\n", "if", "doPrint", ":", "\n", "        ", "print", "(", "\"OK\"", ")", "\n", "\n", "###########", "\n", "# PREVIEW #", "\n", "###########", "\n", "\n", "", "learnedTrajectoriesDir", "=", "os", ".", "path", ".", "join", "(", "model", ".", "saveDir", ",", "\n", "'learnedTrajectories'", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "learnedTrajectoriesDir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "learnedTrajectoriesDir", ")", "\n", "\n", "", "if", "graphNo", ">", "-", "1", ":", "\n", "        ", "learnedTrajectoriesDir", "=", "os", ".", "path", ".", "join", "(", "learnedTrajectoriesDir", ",", "\n", "'%03d'", "%", "graphNo", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "learnedTrajectoriesDir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "learnedTrajectoriesDir", ")", "\n", "", "", "if", "realizationNo", ">", "-", "1", ":", "\n", "        ", "learnedTrajectoriesDir", "=", "os", ".", "path", ".", "join", "(", "learnedTrajectoriesDir", ",", "\n", "'%03d'", "%", "realizationNo", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "learnedTrajectoriesDir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "learnedTrajectoriesDir", ")", "\n", "\n", "", "", "learnedTrajectoriesDir", "=", "os", ".", "path", ".", "join", "(", "learnedTrajectoriesDir", ",", "model", ".", "name", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "learnedTrajectoriesDir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "learnedTrajectoriesDir", ")", "\n", "\n", "", "if", "doPrint", ":", "\n", "        ", "print", "(", "\"\\tPreview data...\"", ",", "\n", "end", "=", "' '", ",", "flush", "=", "True", ")", "\n", "\n", "", "data", ".", "saveVideo", "(", "os", ".", "path", ".", "join", "(", "learnedTrajectoriesDir", ",", "'Best'", ")", ",", "\n", "posTestBest", ",", "\n", "nVideos", ",", "\n", "commGraph", "=", "commGraphTestBest", ",", "\n", "vel", "=", "velTestBest", ",", "\n", "videoSpeed", "=", "0.5", ",", "\n", "doPrint", "=", "False", ")", "\n", "\n", "data", ".", "saveVideo", "(", "os", ".", "path", ".", "join", "(", "learnedTrajectoriesDir", ",", "'Last'", ")", ",", "\n", "posTestLast", ",", "\n", "nVideos", ",", "\n", "commGraph", "=", "commGraphTestLast", ",", "\n", "vel", "=", "velTestLast", ",", "\n", "videoSpeed", "=", "0.5", ",", "\n", "doPrint", "=", "False", ")", "\n", "\n", "if", "doPrint", ":", "\n", "        ", "print", "(", "\"OK\"", ",", "flush", "=", "True", ")", "\n", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "#\\\\\\ EVALUATION \\\\\\", "\n", "#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "\n", "\n", "", "evalVars", "=", "{", "}", "\n", "evalVars", "[", "'costBestFull'", "]", "=", "data", ".", "evaluate", "(", "vel", "=", "velTestBest", ")", "\n", "evalVars", "[", "'costBestEnd'", "]", "=", "data", ".", "evaluate", "(", "vel", "=", "velTestBest", "[", ":", ",", "-", "1", ":", ",", ":", ",", ":", "]", ")", "\n", "evalVars", "[", "'costLastFull'", "]", "=", "data", ".", "evaluate", "(", "vel", "=", "velTestLast", ")", "\n", "evalVars", "[", "'costLastEnd'", "]", "=", "data", ".", "evaluate", "(", "vel", "=", "velTestLast", "[", ":", ",", "-", "1", ":", ",", ":", ",", ":", "]", ")", "\n", "\n", "return", "evalVars", "", "", ""]]}