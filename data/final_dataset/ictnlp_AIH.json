{"home.repos.pwc.inspect_result.ictnlp_AIH.None.play.PlatoAgent.__init__": [[18, 21], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "userid", ")", ":", "\n", "        ", "self", ".", "userid", "=", "userid", "\n", "self", ".", "url", "=", "agent_pool", "[", "\"plato\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.None.play.PlatoAgent.act": [[22, 27], ["json.dumps", "requests.post", "json.loads"], "methods", ["None"], ["", "def", "act", "(", "self", ",", "text", ",", "replace", "=", "False", ")", ":", "\n", "        ", "data", "=", "json", ".", "dumps", "(", "{", "\"userID\"", ":", "self", ".", "userid", ",", "\"text\"", ":", "text", ",", "\"replace\"", ":", "replace", "}", ")", "\n", "r", "=", "requests", ".", "post", "(", "self", ".", "url", ",", "data", "=", "data", ")", "\n", "text", "=", "json", ".", "loads", "(", "r", ".", "text", ")", "[", "'body'", "]", "[", "'utterance'", "]", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.None.play.DialoFlowAgent.__init__": [[29, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "userid", ")", ":", "\n", "        ", "self", ".", "userid", "=", "userid", "\n", "self", ".", "url", "=", "agent_pool", "[", "\"dialoflow\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.None.play.DialoFlowAgent.act": [[33, 38], ["json.dumps", "requests.post", "json.loads"], "methods", ["None"], ["", "def", "act", "(", "self", ",", "text", ",", "replace", "=", "False", ")", ":", "\n", "        ", "data", "=", "json", ".", "dumps", "(", "{", "\"userID\"", ":", "self", ".", "userid", ",", "\"text\"", ":", "text", ",", "\"replace\"", ":", "replace", "}", ")", "\n", "r", "=", "requests", ".", "post", "(", "self", ".", "url", ",", "data", "=", "data", ")", "\n", "text", "=", "json", ".", "loads", "(", "r", ".", "text", ")", "[", "'body'", "]", "[", "'utterance'", "]", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.None.play.DialoGPTAgent.__init__": [[40, 43], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "userid", ")", ":", "\n", "        ", "self", ".", "userid", "=", "userid", "\n", "self", ".", "url", "=", "agent_pool", "[", "\"dialogpt\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.None.play.DialoGPTAgent.act": [[44, 49], ["json.dumps", "requests.post", "json.loads"], "methods", ["None"], ["", "def", "act", "(", "self", ",", "text", ",", "replace", "=", "False", ")", ":", "\n", "        ", "data", "=", "json", ".", "dumps", "(", "{", "\"userID\"", ":", "self", ".", "userid", ",", "\"text\"", ":", "text", ",", "\"replace\"", ":", "replace", "}", ")", "\n", "r", "=", "requests", ".", "post", "(", "self", ".", "url", ",", "data", "=", "data", ")", "\n", "text", "=", "json", ".", "loads", "(", "r", ".", "text", ")", "[", "'body'", "]", "[", "'utterance'", "]", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.None.play.BlenderAgent.__init__": [[53, 56], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "userid", ")", ":", "\n", "        ", "self", ".", "userid", "=", "userid", "\n", "self", ".", "url", "=", "agent_pool", "[", "\"blender\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.None.play.BlenderAgent.act": [[57, 65], ["requests.post", "json.loads", "data.encode"], "methods", ["None"], ["", "def", "act", "(", "self", ",", "text", ",", "replace", "=", "False", ")", ":", "\n", "        ", "if", "replace", ":", "\n", "            ", "data", "=", "text", "+", "self", ".", "userid", "+", "\"*\"", "\n", "", "else", ":", "\n", "            ", "data", "=", "text", "+", "self", ".", "userid", "\n", "", "r", "=", "requests", ".", "post", "(", "self", ".", "url", ",", "data", "=", "data", ".", "encode", "(", "\"utf-8\"", ")", ")", "\n", "text", "=", "json", ".", "loads", "(", "r", ".", "text", ")", "[", "\"text\"", "]", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.None.play.gen_q": [[66, 72], ["json.dumps", "requests.post", "json.loads"], "function", ["None"], ["", "", "def", "gen_q", "(", "text", ")", ":", "\n", "    ", "data", "=", "json", ".", "dumps", "(", "{", "\"text\"", ":", "text", "}", ")", "\n", "url", "=", "\"http://127.0.0.1:8084/gen\"", "\n", "r", "=", "requests", ".", "post", "(", "url", ",", "data", "=", "data", ")", "\n", "text", "=", "json", ".", "loads", "(", "r", ".", "text", ")", "[", "'body'", "]", "[", "'text'", "]", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.None.play.nli": [[73, 79], ["json.dumps", "requests.post", "json.loads"], "function", ["None"], ["", "def", "nli", "(", "res", ",", "res_gold", ")", ":", "\n", "    ", "data", "=", "json", ".", "dumps", "(", "{", "\"res\"", ":", "res", ",", "\"res_gold\"", ":", "res_gold", "}", ")", "\n", "url", "=", "\"http://127.0.0.1:8085/NLI\"", "\n", "r", "=", "requests", ".", "post", "(", "url", ",", "data", "=", "data", ")", "\n", "score", "=", "json", ".", "loads", "(", "r", ".", "text", ")", "[", "'nli_score'", "]", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.question_generator.deploy.extract": [[14, 45], ["features.to_dict", "results_upos.append", "temp_upos_text.append", "results_xpos.append", "temp_xpos_text.append", "i.to_dict", "r_list.append"], "function", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.to_dict", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.to_dict"], ["def", "extract", "(", "features", ",", "select_list", "=", "None", ")", ":", "\n", "    ", "upos", "=", "\"\"", "\n", "xpos", "=", "\"\"", "\n", "results_upos", "=", "[", "]", "\n", "results_xpos", "=", "[", "]", "\n", "temp_upos_text", "=", "[", "]", "\n", "temp_xpos_text", "=", "[", "]", "\n", "for", "feature", "in", "features", ".", "to_dict", "(", ")", ":", "\n", "        ", "for", "i", "in", "feature", ":", "\n", "            ", "if", "upos", "!=", "i", "[", "\"upos\"", "]", ":", "\n", "                ", "results_upos", ".", "append", "(", "(", "\" \"", ".", "join", "(", "temp_upos_text", ")", ",", "upos", ")", ")", "\n", "temp_upos_text", "=", "[", "i", "[", "\"text\"", "]", "]", "\n", "upos", "=", "i", "[", "\"upos\"", "]", "\n", "", "else", ":", "\n", "                ", "temp_upos_text", ".", "append", "(", "i", "[", "\"text\"", "]", ")", "\n", "", "if", "xpos", "!=", "i", "[", "\"xpos\"", "]", ":", "\n", "                ", "results_xpos", ".", "append", "(", "(", "\" \"", ".", "join", "(", "temp_xpos_text", ")", ",", "xpos", ")", ")", "\n", "temp_xpos_text", "=", "[", "i", "[", "\"text\"", "]", "]", "\n", "xpos", "=", "i", "[", "\"xpos\"", "]", "\n", "", "else", ":", "\n", "                ", "temp_xpos_text", ".", "append", "(", "i", "[", "\"text\"", "]", ")", "\n", "\n", "", "", "", "if", "select_list", "is", "not", "None", ":", "\n", "        ", "r_list", "=", "[", "i", ".", "to_dict", "(", ")", "[", "\"text\"", "]", "for", "i", "in", "features", ".", "entities", "]", "\n", "for", "i", "in", "results_xpos", ":", "\n", "            ", "if", "i", "[", "1", "]", "in", "select_list", ":", "\n", "                ", "if", "i", "[", "0", "]", "not", "in", "\" \"", ".", "join", "(", "r_list", ")", ":", "\n", "                    ", "r_list", ".", "append", "(", "i", "[", "0", "]", ")", "\n", "", "", "", "return", "r_list", "\n", "", "else", ":", "\n", "        ", "return", "results_xpos", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.question_generator.deploy.gen": [[46, 63], ["app.route", "flask.request.get_data", "json.loads", "truecase.get_true_case", "nlp", "deploy.extract", "print", "qg.predict", "flask.jsonify", "results.append", "i[].replace().replace().replace", "i[].replace().replace", "i[].replace"], "function", ["home.repos.pwc.inspect_result.ictnlp_AIH.question_generator.deploy.extract", "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator.predict"], ["", "", "@", "app", ".", "route", "(", "\"/gen\"", ",", "methods", "=", "[", "\"POST\"", "]", ")", "\n", "def", "gen", "(", ")", ":", "\n", "    ", "body", "=", "request", ".", "get_data", "(", ")", "\n", "body", "=", "json", ".", "loads", "(", "body", ")", "\n", "text", "=", "truecase", ".", "get_true_case", "(", "body", "[", "\"text\"", "]", ")", "\n", "select_list", "=", "[", "\"CD\"", ",", "\"NNP\"", "]", "\n", "doc", "=", "nlp", "(", "text", ")", "\n", "ents", "=", "extract", "(", "doc", ",", "select_list", ")", "\n", "#ents = [ent.text for sent in doc.sentences for ent in sent.ents]", "\n", "print", "(", "ents", ")", "\n", "candidates", "=", "[", "text", "+", "\" [SEP] \"", "+", "i", "for", "i", "in", "ents", "]", "\n", "q", "=", "qg", ".", "predict", "(", "candidates", ")", "\n", "results", "=", "[", "]", "\n", "for", "i", "in", "q", ":", "\n", "        ", "if", "i", "!=", "\"\\n\"", ":", "\n", "            ", "results", ".", "append", "(", "(", "i", "[", "0", "]", ".", "replace", "(", "\" I \"", ",", "\" you \"", ")", ".", "replace", "(", "\" i \"", ",", "\" you \"", ")", ".", "replace", "(", "\" my \"", ",", "\" your \"", ")", ",", "text", ")", ")", "\n", "", "", "return", "jsonify", "(", "{", "\"body\"", ":", "{", "\"text\"", ":", "results", "}", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator.__init__": [[61, 137], ["text_generator.TextGenerator.__dict__.update", "text_generator.TextGenerator.__dict__.update", "torch.device", "torch.cuda.device_count", "random.seed", "numpy.random.seed", "torch.manual_seed", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "text_generator.TextGenerator.bi_uni_pipeline.append", "text_generator.TextGenerator.tokenizer.convert_tokens_to_ids", "text_generator.TextGenerator._get_token_id_set", "text_generator.TextGenerator._get_token_id_set", "text_generator.TextGenerator.download_pretrained_model", "glob.glob", "ValueError", "ValueError", "torch.cuda.manual_seed_all", "biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder", "text_generator.TextGenerator.model_recover_path.strip", "print", "pytorch_pretrained_bert.modeling.BertForSeq2SeqDecoder.from_pretrained", "text_generator.TextGenerator.model.to", "torch.cuda.empty_cache", "text_generator.TextGenerator.model.eval", "torch.cuda.is_available", "list", "torch.cuda.is_available", "text_generator.TextGenerator.model.half", "torch.nn.DataParallel", "text_generator.TextGenerator.tokenizer.vocab.keys"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator._get_token_id_set", "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator._get_token_id_set", "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator.download_pretrained_model", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained"], ["  ", "def", "__init__", "(", "self", ",", "output_type", "=", "\"question\"", ",", "**", "kwargs", ")", ":", "\n", "    ", "self", ".", "output_type", "=", "output_type", "\n", "self", ".", "bert_model", "=", "\"bert-large-cased\"", "\n", "self", ".", "ffn_type", "=", "0", "\n", "self", ".", "num_qkv", "=", "0", "\n", "self", ".", "seg_emb", "=", "False", "\n", "self", ".", "split", "=", "\"test\"", "\n", "self", ".", "seed", "=", "123", "\n", "self", ".", "do_lower_case", "=", "False", "\n", "self", ".", "new_segment_ids", "=", "True", "\n", "self", ".", "new_pos_ids", "=", "False", "\n", "self", ".", "min_len", "=", "None", "\n", "self", ".", "ngram_size", "=", "3", "\n", "self", ".", "mode", "=", "\"s2s\"", "\n", "self", ".", "s2s_special_token", "=", "False", "\n", "self", ".", "s2s_add_segment", "=", "False", "\n", "self", ".", "s2s_share_segment", "=", "False", "\n", "self", ".", "pos_shift", "=", "False", "\n", "self", ".", "not_predict_token", "=", "None", "\n", "self", ".", "__dict__", ".", "update", "(", "PRETRAINED_PARAMS", "[", "output_type", "]", ")", "\n", "self", ".", "__dict__", ".", "update", "(", "kwargs", ")", "\n", "\n", "if", "output_type", "not", "in", "[", "\"question\"", ",", "\"summary\"", "]", ":", "\n", "      ", "raise", "ValueError", "(", "f'{output_type} unacceptable for output_type. Choose either \"question\" or \"summary\".'", ")", "\n", "\n", "", "if", "self", ".", "max_tgt_length", ">=", "self", ".", "max_seq_length", "-", "2", ":", "\n", "      ", "raise", "ValueError", "(", "\"Maximum tgt length exceeds max seq length - 2.\"", ")", "\n", "\n", "", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "      ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "self", ".", "seed", ")", "\n", "\n", "", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "self", ".", "bert_model", ",", "do_lower_case", "=", "self", ".", "do_lower_case", ")", "\n", "\n", "self", ".", "tokenizer", ".", "max_len", "=", "self", ".", "max_seq_length", "\n", "\n", "pair_num_relation", "=", "0", "\n", "self", ".", "bi_uni_pipeline", "=", "[", "]", "\n", "self", ".", "bi_uni_pipeline", ".", "append", "(", "seq2seq_loader", ".", "Preprocess4Seq2seqDecoder", "(", "list", "(", "self", ".", "tokenizer", ".", "vocab", ".", "keys", "(", ")", ")", ",", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", ",", "self", ".", "max_seq_length", ",", "max_tgt_length", "=", "self", ".", "max_tgt_length", ",", "new_segment_ids", "=", "self", ".", "new_segment_ids", ",", "\n", "mode", "=", "\"s2s\"", ",", "num_qkv", "=", "self", ".", "num_qkv", ",", "s2s_special_token", "=", "self", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "self", ".", "s2s_add_segment", ",", "s2s_share_segment", "=", "self", ".", "s2s_share_segment", ",", "pos_shift", "=", "self", ".", "pos_shift", ")", ")", "\n", "\n", "# Prepare model", "\n", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "self", ".", "s2s_add_segment", "else", "0", ")", "if", "self", ".", "new_segment_ids", "else", "2", "\n", "mask_word_id", ",", "eos_word_ids", ",", "sos_word_id", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "\"[MASK]\"", ",", "\"[SEP]\"", ",", "\"[S2S_SOS]\"", "]", ")", "\n", "\n", "forbid_ignore_set", "=", "self", ".", "_get_token_id_set", "(", "self", ".", "forbid_ignore_word", ")", "\n", "not_predict_set", "=", "self", ".", "_get_token_id_set", "(", "self", ".", "not_predict_token", ")", "\n", "\n", "self", ".", "download_pretrained_model", "(", ")", "\n", "\n", "for", "model_recover_path", "in", "glob", ".", "glob", "(", "self", ".", "model_recover_path", ".", "strip", "(", ")", ")", ":", "\n", "      ", "print", "(", "\"***** Recover model: %s *****\"", ",", "model_recover_path", ")", "\n", "map_device", "=", "None", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "map_device", "=", "'cpu'", "\n", "#model_recover = torch.load(model_recover_path,map_location=map_device)", "\n", "#config = BertConfig.from_json_file(\"/data1/DialoAttack/AgentChat/text2text/config.json\")", "\n", "", "self", ".", "model", "=", "BertForSeq2SeqDecoder", ".", "from_pretrained", "(", "\"/data1/DialoAttack/AgentChat/text2text/model\"", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "pair_num_relation", ",", "type_vocab_size", "=", "type_vocab_size", ",", "task_idx", "=", "3", ",", "mask_word_id", "=", "mask_word_id", ",", "search_beam_size", "=", "self", ".", "beam_size", ",", "length_penalty", "=", "self", ".", "length_penalty", ",", "eos_id", "=", "eos_word_ids", ",", "sos_id", "=", "sos_word_id", ",", "forbid_duplicate_ngrams", "=", "self", ".", "forbid_duplicate_ngrams", ",", "forbid_ignore_set", "=", "forbid_ignore_set", ",", "not_predict_set", "=", "not_predict_set", ",", "ngram_size", "=", "self", ".", "ngram_size", ",", "min_len", "=", "self", ".", "min_len", ",", "mode", "=", "self", ".", "mode", ",", "max_position_embeddings", "=", "self", ".", "max_seq_length", ",", "ffn_type", "=", "self", ".", "ffn_type", ",", "num_qkv", "=", "self", ".", "num_qkv", ",", "seg_emb", "=", "self", ".", "seg_emb", ",", "pos_shift", "=", "self", ".", "pos_shift", ")", "\n", "#self.model.load_state_dict(model_recover)", "\n", "#del model_recover", "\n", "\n", "if", "self", ".", "fp16", ":", "\n", "        ", "self", ".", "model", ".", "half", "(", ")", "\n", "", "self", ".", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "self", ".", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "self", ".", "model", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator.download_pretrained_model": [[138, 149], ["os.path.isfile", "requests.session", "requests.session.get", "requests.session.get", "zipfile.ZipFile", "zipfile.ZipFile.extractall", "print", "[].split", "io.BytesIO", "requests.session.get.text.split"], "methods", ["None"], ["", "", "def", "download_pretrained_model", "(", "self", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "isfile", "(", "self", ".", "model_recover_path", ")", ":", "\n", "      ", "print", "(", "f\"{self.model_recover_path} found in current directory.\"", ")", "\n", "return", "\n", "", "s", "=", "requests", ".", "session", "(", ")", "\n", "file_id", "=", "self", ".", "file_id", "\n", "r", "=", "s", ".", "get", "(", "f'https://docs.google.com/uc?export=download&id={file_id}'", ")", "\n", "confirm_code", "=", "r", ".", "text", ".", "split", "(", "\"/uc?export=download&amp;confirm=\"", ")", "[", "1", "]", ".", "split", "(", "\"&amp;id=\"", ")", "[", "0", "]", "\n", "r", "=", "s", ".", "get", "(", "f'https://docs.google.com/uc?export=download&confirm={confirm_code}&id={file_id}'", ")", "\n", "z", "=", "zipfile", ".", "ZipFile", "(", "io", ".", "BytesIO", "(", "r", ".", "content", ")", ")", "\n", "z", ".", "extractall", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator._get_token_id_set": [[150, 161], ["s.split", "set", "text_generator.TextGenerator.tokenizer.convert_tokens_to_ids", "w.startswith", "w.endswith", "w_list.append", "w_list.append", "w.upper"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "def", "_get_token_id_set", "(", "self", ",", "s", ")", ":", "\n", "    ", "r", "=", "None", "\n", "if", "s", ":", "\n", "      ", "w_list", "=", "[", "]", "\n", "for", "w", "in", "s", ".", "split", "(", "'|'", ")", ":", "\n", "        ", "if", "w", ".", "startswith", "(", "'['", ")", "and", "w", ".", "endswith", "(", "']'", ")", ":", "\n", "          ", "w_list", ".", "append", "(", "w", ".", "upper", "(", ")", ")", "\n", "", "else", ":", "\n", "          ", "w_list", ".", "append", "(", "w", ")", "\n", "", "", "r", "=", "set", "(", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "w_list", ")", ")", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator._get_answer_tokens": [[162, 170], ["text_generator.detokenize", "text_generator.TextGenerator.tokenizer.tokenize", "len", "random.choice", "w.lower", "answers.append"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.detokenize", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "_get_answer_tokens", "(", "self", ",", "tkns", ")", ":", "\n", "    ", "words", "=", "detokenize", "(", "tkns", ")", "\n", "answers", "=", "[", "]", "\n", "for", "w", "in", "words", ":", "\n", "      ", "if", "len", "(", "w", ")", ">", "1", ":", "\n", "        ", "if", "w", ".", "lower", "(", ")", "not", "in", "STOP_WORDS", ":", "\n", "          ", "answers", ".", "append", "(", "w", ")", "\n", "", "", "", "return", "self", ".", "tokenizer", ".", "tokenize", "(", "random", ".", "choice", "(", "answers", ")", "if", "answers", "else", "words", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator.predict": [[171, 224], ["sorted", "math.ceil", "pytorch_pretrained_bert.tokenization.WhitespaceTokenizer", "list", "len", "len", "torch.no_grad", "data_tokenizer.tokenize", "enumerate", "len", "len", "max", "torch.no_grad", "biunilm.seq2seq_loader.batch_list_to_batch_tensors", "text_generator.TextGenerator.model", "range", "text_generator.TextGenerator._get_answer_tokens", "len", "len", "instances.append", "text_generator.TextGenerator.tolist", "len", "text_generator.TextGenerator.tokenizer.convert_ids_to_tokens", "output_sequence.replace().replace.replace().replace.replace().replace", "proc", "t.to", "v.tolist", "output_tokens.append", "text_generator.detokenize", "buf[].index", "text_generator.TextGenerator.items", "output_sequence.replace().replace.replace().replace.replace", "text_generator.detokenize"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.TextGenerator._get_answer_tokens", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.detokenize", "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.detokenize"], ["", "def", "predict", "(", "self", ",", "input_lines", ",", "tokenized_input", "=", "False", ")", ":", "\n", "    ", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "tokenized_input", "else", "self", ".", "tokenizer", "\n", "max_src_length", "=", "self", ".", "max_seq_length", "-", "2", "-", "self", ".", "max_tgt_length", "\n", "input_lines", "=", "[", "data_tokenizer", ".", "tokenize", "(", "x", ")", "[", ":", "max_src_length", "]", "for", "x", "in", "input_lines", "]", "\n", "\n", "if", "self", ".", "output_type", "==", "\"question\"", ":", "\n", "      ", "input_lines", "=", "[", "x", "+", "[", "\"[SEP]\"", "]", "+", "self", ".", "_get_answer_tokens", "(", "x", ")", "if", "\"[SEP]\"", "not", "in", "x", "else", "x", "for", "x", "in", "input_lines", "]", "\n", "\n", "", "input_lines", "=", "sorted", "(", "list", "(", "enumerate", "(", "input_lines", ")", ")", ",", "key", "=", "lambda", "x", ":", "-", "len", "(", "x", "[", "1", "]", ")", ")", "\n", "output_lines", "=", "[", "\"\"", "]", "*", "len", "(", "input_lines", ")", "\n", "score_trace_list", "=", "[", "None", "]", "*", "len", "(", "input_lines", ")", "\n", "total_batch", "=", "math", ".", "ceil", "(", "len", "(", "input_lines", ")", "/", "self", ".", "batch_size", ")", "\n", "next_i", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "      ", "while", "next_i", "<", "len", "(", "input_lines", ")", ":", "\n", "        ", "_chunk", "=", "input_lines", "[", "next_i", ":", "next_i", "+", "self", ".", "batch_size", "]", "\n", "buf_id", "=", "[", "x", "[", "0", "]", "for", "x", "in", "_chunk", "]", "\n", "buf", "=", "[", "x", "[", "1", "]", "for", "x", "in", "_chunk", "]", "\n", "next_i", "+=", "self", ".", "batch_size", "\n", "max_a_len", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "buf", "]", ")", "\n", "instances", "=", "[", "]", "\n", "for", "instance", "in", "[", "(", "x", ",", "max_a_len", ")", "for", "x", "in", "buf", "]", ":", "\n", "          ", "for", "proc", "in", "self", ".", "bi_uni_pipeline", ":", "\n", "            ", "instances", ".", "append", "(", "proc", "(", "instance", ")", ")", "\n", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "          ", "batch", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", "(", "instances", ")", "\n", "batch", "=", "[", "t", ".", "to", "(", "self", ".", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "input_mask", ",", "mask_qkv", ",", "task_idx", "=", "batch", "\n", "traces", "=", "self", ".", "model", "(", "input_ids", ",", "token_type_ids", ",", "\n", "position_ids", ",", "input_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "if", "self", ".", "beam_size", ">", "1", ":", "\n", "            ", "traces", "=", "{", "k", ":", "v", ".", "tolist", "(", ")", "for", "k", ",", "v", "in", "traces", ".", "items", "(", ")", "}", "\n", "output_ids", "=", "traces", "[", "'pred_seq'", "]", "\n", "", "else", ":", "\n", "            ", "output_ids", "=", "traces", ".", "tolist", "(", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "buf", ")", ")", ":", "\n", "            ", "w_ids", "=", "output_ids", "[", "i", "]", "\n", "output_buf", "=", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "w_ids", ")", "\n", "output_tokens", "=", "[", "]", "\n", "for", "t", "in", "output_buf", ":", "\n", "              ", "if", "t", "in", "(", "\"[SEP]\"", ",", "\"[PAD]\"", ")", ":", "\n", "                ", "break", "\n", "", "output_tokens", ".", "append", "(", "t", ")", "\n", "", "output_sequence", "=", "' '", ".", "join", "(", "detokenize", "(", "output_tokens", ")", ")", "\n", "output_sequence", "=", "output_sequence", ".", "replace", "(", "\" ' \"", ",", "\"'\"", ")", ".", "replace", "(", "\" ?\"", ",", "\"?\"", ")", "\n", "if", "self", ".", "output_type", "==", "\"question\"", ":", "\n", "              ", "ans_idx", "=", "buf", "[", "i", "]", ".", "index", "(", "\"[SEP]\"", ")", "\n", "corresponding_answer", "=", "' '", ".", "join", "(", "detokenize", "(", "buf", "[", "i", "]", "[", "ans_idx", "+", "1", ":", "]", ")", ")", "\n", "output_lines", "[", "buf_id", "[", "i", "]", "]", "=", "(", "output_sequence", ",", "corresponding_answer", ")", "\n", "", "else", ":", "\n", "              ", "output_lines", "[", "buf_id", "[", "i", "]", "]", "=", "output_sequence", "\n", "\n", "", "", "", "", "", "return", "output_lines", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.detokenize": [[17, 25], ["tk.startswith", "r_list.append", "len"], "function", ["None"], ["def", "detokenize", "(", "tk_list", ")", ":", "\n", "  ", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "    ", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "      ", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "      ", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "r_list", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.text2text.text_generator.ascii_print": [[26, 29], ["text.encode.encode", "print"], "function", ["None"], ["", "def", "ascii_print", "(", "text", ")", ":", "\n", "  ", "text", "=", "text", ".", "encode", "(", "\"ascii\"", ",", "\"ignore\"", ")", "\n", "print", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.__init__": [[61, 132], ["isinstance", "json.loads.items", "isinstance", "open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "relax_projection", "=", "0", ",", "\n", "new_pos_ids", "=", "False", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "task_idx", "=", "None", ",", "\n", "fp32_embedding", "=", "False", ",", "\n", "ffn_type", "=", "0", ",", "\n", "label_smoothing", "=", "None", ",", "\n", "num_qkv", "=", "0", ",", "\n", "seg_emb", "=", "False", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "relax_projection", "=", "relax_projection", "\n", "self", ".", "new_pos_ids", "=", "new_pos_ids", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "task_idx", "=", "task_idx", "\n", "self", ".", "fp32_embedding", "=", "fp32_embedding", "\n", "self", ".", "ffn_type", "=", "ffn_type", "\n", "self", ".", "label_smoothing", "=", "label_smoothing", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "seg_emb", "=", "seg_emb", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.from_dict": [[134, 141], ["modeling.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.from_json_file": [[142, 148], ["cls.from_dict", "open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.__repr__": [[149, 151], ["str", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.to_dict": [[152, 156], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.to_json_string": [[157, 160], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.PositionalEmbedding.__init__": [[184, 191], ["torch.nn.Module.__init__", "modeling.PositionalEmbedding.register_buffer", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "demb", ")", ":", "\n", "        ", "super", "(", "PositionalEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "demb", "=", "demb", "\n", "\n", "inv_freq", "=", "1", "/", "(", "10000", "**", "(", "torch", ".", "arange", "(", "0.0", ",", "demb", ",", "2.0", ")", "/", "demb", ")", ")", "\n", "self", ".", "register_buffer", "(", "'inv_freq'", ",", "inv_freq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.PositionalEmbedding.forward": [[192, 200], ["torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pos_emb[].expand", "torch.ger.sin", "torch.ger.sin", "torch.ger.cos", "torch.ger.cos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pos_seq", ",", "bsz", "=", "None", ")", ":", "\n", "        ", "sinusoid_inp", "=", "torch", ".", "ger", "(", "pos_seq", ",", "self", ".", "inv_freq", ")", "\n", "pos_emb", "=", "torch", ".", "cat", "(", "[", "sinusoid_inp", ".", "sin", "(", ")", ",", "sinusoid_inp", ".", "cos", "(", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "bsz", "is", "not", "None", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", ".", "expand", "(", "-", "1", ",", "bsz", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertEmbeddings.__init__": [[206, 228], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "hasattr", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "hasattr"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "hasattr", "(", "config", ",", "'fp32_embedding'", ")", ":", "\n", "            ", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "", "else", ":", "\n", "            ", "self", ".", "fp32_embedding", "=", "False", "\n", "\n", "", "if", "hasattr", "(", "config", ",", "'new_pos_ids'", ")", "and", "config", ".", "new_pos_ids", ":", "\n", "            ", "self", ".", "num_pos_emb", "=", "4", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_pos_emb", "=", "1", "\n", "", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", "*", "self", ".", "num_pos_emb", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertEmbeddings.forward": [[229, 254], ["input_ids.size", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.LayerNorm", "modeling.BertEmbeddings.dropout", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "modeling.BertEmbeddings.size", "modeling.BertEmbeddings.size", "embeddings.half.half.half", "modeling.BertEmbeddings.view", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "\n", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "if", "self", ".", "num_pos_emb", ">", "1", ":", "\n", "            ", "num_batch", "=", "position_embeddings", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "position_embeddings", ".", "size", "(", "1", ")", "\n", "position_embeddings", "=", "position_embeddings", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "num_pos_emb", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "\n", "", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "if", "self", ".", "fp32_embedding", ":", "\n", "            ", "embeddings", "=", "embeddings", ".", "half", "(", ")", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertSelfAttention.__init__": [[257, 295], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "ValueError", "hasattr", "os.getenv", "modeling.BertSelfAttention.register_buffer", "hasattr", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Embedding", "torch.nn.Embedding", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "\n", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "if", "hasattr", "(", "config", ",", "'num_qkv'", ")", "and", "(", "config", ".", "num_qkv", ">", "1", ")", ":", "\n", "            ", "self", ".", "num_qkv", "=", "config", ".", "num_qkv", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_qkv", "=", "1", "\n", "\n", "", "self", ".", "query", "=", "nn", ".", "Linear", "(", "\n", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "\n", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "\n", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n", "self", ".", "uni_debug_flag", "=", "True", "if", "os", ".", "getenv", "(", "\n", "'UNI_DEBUG_FLAG'", ",", "''", ")", "else", "False", "\n", "if", "self", ".", "uni_debug_flag", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'debug_attention_probs'", ",", "\n", "torch", ".", "zeros", "(", "(", "512", ",", "512", ")", ")", ")", "\n", "", "if", "hasattr", "(", "config", ",", "'seg_emb'", ")", "and", "config", ".", "seg_emb", ":", "\n", "            ", "self", ".", "b_q_s", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "\n", "1", ",", "self", ".", "num_attention_heads", ",", "1", ",", "self", ".", "attention_head_size", ")", ")", "\n", "self", ".", "seg_emb", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "type_vocab_size", ",", "self", ".", "all_head_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "b_q_s", "=", "None", "\n", "self", ".", "seg_emb", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores": [[296, 320], ["x.gather().squeeze.gather().squeeze.permute", "x.gather().squeeze.gather().squeeze.view", "x.gather().squeeze.gather().squeeze.view", "isinstance", "x.gather().squeeze.gather().squeeze.size", "x.gather().squeeze.gather().squeeze.gather().squeeze", "x.gather().squeeze.gather().squeeze.size", "mask_qkv.size", "x.gather().squeeze.gather().squeeze.gather", "mask_qkv.view().expand", "mask_qkv.view"], "methods", ["None"], ["", "", "def", "transpose_for_scores", "(", "self", ",", "x", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "sz", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_qkv", ",", "\n", "self", ".", "num_attention_heads", ",", "self", ".", "all_head_size", ")", "\n", "# (batch, pos, num_qkv, head, head_hid)", "\n", "x", "=", "x", ".", "view", "(", "*", "sz", ")", "\n", "if", "mask_qkv", "is", "None", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", ":", ",", "0", ",", ":", ",", ":", "]", "\n", "", "elif", "isinstance", "(", "mask_qkv", ",", "int", ")", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", ":", ",", "mask_qkv", ",", ":", ",", ":", "]", "\n", "", "else", ":", "\n", "# mask_qkv: (batch, pos)", "\n", "                ", "if", "mask_qkv", ".", "size", "(", "1", ")", ">", "sz", "[", "1", "]", ":", "\n", "                    ", "mask_qkv", "=", "mask_qkv", "[", ":", ",", ":", "sz", "[", "1", "]", "]", "\n", "# -> x: (batch, pos, head, head_hid)", "\n", "", "x", "=", "x", ".", "gather", "(", "2", ",", "mask_qkv", ".", "view", "(", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1", ",", "1", ",", "1", ")", ".", "expand", "(", "\n", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1", ",", "sz", "[", "3", "]", ",", "sz", "[", "4", "]", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "", "", "else", ":", "\n", "            ", "sz", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", ")", "\n", "# (batch, pos, head, head_hid)", "\n", "x", "=", "x", ".", "view", "(", "*", "sz", ")", "\n", "# (batch, head, pos, head_hid)", "\n", "", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertSelfAttention.forward": [[321, 373], ["modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling.BertSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "modeling.BertSelfAttention.transpose", "modeling.BertSelfAttention.seg_emb", "seg_rep.view.view.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.nn.Softmax", "torch.nn.Softmax", "modeling.BertSelfAttention.size", "modeling.BertSelfAttention.debug_attention_probs[].copy_", "math.sqrt", "seg_rep.view.view.size", "seg_rep.view.view.size", "attention_probs[].mean().view", "context_layer.view.view.permute", "context_layer.view.view.size", "attention_probs[].mean"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "if", "history_states", "is", "None", ":", "\n", "            ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "", "else", ":", "\n", "            ", "x_states", "=", "torch", ".", "cat", "(", "(", "history_states", ",", "hidden_states", ")", ",", "dim", "=", "1", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "x_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "x_states", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ",", "mask_qkv", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ",", "mask_qkv", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ",", "mask_qkv", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "# (batch, head, pos, pos)", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "\n", "query_layer", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "if", "self", ".", "seg_emb", "is", "not", "None", ":", "\n", "            ", "seg_rep", "=", "self", ".", "seg_emb", "(", "seg_ids", ")", "\n", "# (batch, pos, head, head_hid)", "\n", "seg_rep", "=", "seg_rep", ".", "view", "(", "seg_rep", ".", "size", "(", "0", ")", ",", "seg_rep", ".", "size", "(", "\n", "1", ")", ",", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "qs", "=", "torch", ".", "einsum", "(", "'bnih,bjnh->bnij'", ",", "\n", "query_layer", "+", "self", ".", "b_q_s", ",", "seg_rep", ")", "\n", "attention_scores", "=", "attention_scores", "+", "qs", "\n", "\n", "# attention_scores = attention_scores / math.sqrt(self.attention_head_size)", "\n", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "if", "self", ".", "uni_debug_flag", ":", "\n", "            ", "_pos", "=", "attention_probs", ".", "size", "(", "-", "1", ")", "\n", "self", ".", "debug_attention_probs", "[", ":", "_pos", ",", ":", "_pos", "]", ".", "copy_", "(", "\n", "attention_probs", "[", "0", "]", ".", "mean", "(", "0", ")", ".", "view", "(", "_pos", ",", "_pos", ")", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", "\n", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertSelfOutput.__init__": [[376, 381], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertSelfOutput.forward": [[382, 387], ["modeling.BertSelfOutput.dense", "modeling.BertSelfOutput.dropout", "modeling.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertAttention.__init__": [[390, 394], ["torch.nn.Module.__init__", "modeling.BertSelfAttention", "modeling.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertAttention.forward": [[395, 400], ["modeling.BertAttention.self", "modeling.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "\n", "input_tensor", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertIntermediate.__init__": [[403, 408], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertIntermediate.forward": [[409, 413], ["modeling.BertIntermediate.dense", "modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertOutput.__init__": [[416, 421], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertOutput.forward": [[422, 427], ["modeling.BertOutput.dense", "modeling.BertOutput.dropout", "modeling.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.TransformerFFN.__init__": [[430, 442], ["torch.nn.Module.__init__", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "TransformerFFN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ffn_type", "=", "config", ".", "ffn_type", "\n", "assert", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", "\n", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "self", ".", "wx0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "ffn_type", "in", "(", "2", ",", ")", ":", "\n", "            ", "self", ".", "wx1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "self", ".", "output", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.TransformerFFN.forward": [[443, 454], ["modeling.TransformerFFN.dropout", "modeling.TransformerFFN.LayerNorm", "modeling.TransformerFFN.wx0", "modeling.TransformerFFN.output", "modeling.TransformerFFN.wx1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "x0", "=", "self", ".", "wx0", "(", "x", ")", "\n", "if", "self", ".", "ffn_type", "==", "1", ":", "\n", "                ", "x1", "=", "x", "\n", "", "elif", "self", ".", "ffn_type", "==", "2", ":", "\n", "                ", "x1", "=", "self", ".", "wx1", "(", "x", ")", "\n", "", "out", "=", "self", ".", "output", "(", "x0", "*", "x1", ")", "\n", "", "out", "=", "self", ".", "dropout", "(", "out", ")", "\n", "out", "=", "self", ".", "LayerNorm", "(", "out", "+", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertLayer.__init__": [[457, 466], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.TransformerFFN", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "ffn_type", "=", "config", ".", "ffn_type", "\n", "if", "self", ".", "ffn_type", ":", "\n", "            ", "self", ".", "ffn", "=", "TransformerFFN", "(", "config", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertLayer.forward": [[467, 476], ["modeling.BertLayer.attention", "modeling.BertLayer.ffn", "modeling.BertLayer.intermediate", "modeling.BertLayer.output"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "self", ".", "ffn_type", ":", "\n", "            ", "layer_output", "=", "self", ".", "ffn", "(", "attention_output", ")", "\n", "", "else", ":", "\n", "            ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertEncoder.__init__": [[479, 484], ["torch.nn.Module.__init__", "modeling.BertLayer", "torch.nn.ModuleList", "torch.nn.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BertLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "\n", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertEncoder.forward": [[485, 508], ["enumerate", "all_encoder_layers.append", "layer_module", "layer_module", "all_encoder_layers.append", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "None", ",", "prev_encoded_layers", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "# history embedding and encoded layer must be simultanously given", "\n", "        ", "assert", "(", "prev_embedding", "is", "None", ")", "==", "(", "prev_encoded_layers", "is", "None", ")", "\n", "\n", "all_encoder_layers", "=", "[", "]", "\n", "if", "(", "prev_embedding", "is", "not", "None", ")", "and", "(", "prev_encoded_layers", "is", "not", "None", ")", ":", "\n", "            ", "history_states", "=", "prev_embedding", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                    ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "if", "prev_encoded_layers", "is", "not", "None", ":", "\n", "                    ", "history_states", "=", "prev_encoded_layers", "[", "i", "]", "\n", "", "", "", "else", ":", "\n", "            ", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                    ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPooler.__init__": [[511, 515], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPooler.forward": [[516, 523], ["modeling.BertPooler.dense", "modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.__init__": [[526, 535], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "isinstance", "hasattr"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "hid_size", "=", "config", ".", "hidden_size", "\n", "if", "hasattr", "(", "config", ",", "'relax_projection'", ")", "and", "(", "config", ".", "relax_projection", ">", "1", ")", ":", "\n", "            ", "hid_size", "*=", "config", ".", "relax_projection", "\n", "", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "hid_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "hid_size", ",", "eps", "=", "1e-5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.forward": [[536, 541], ["modeling.BertPredictionHeadTransform.dense", "modeling.BertPredictionHeadTransform.transform_act_fn", "modeling.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertLMPredictionHead.__init__": [[544, 569], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "hasattr", "bert_model_embedding_weights.size", "tensor.half"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "if", "hasattr", "(", "config", ",", "'relax_projection'", ")", "and", "(", "config", ".", "relax_projection", ">", "1", ")", ":", "\n", "            ", "self", ".", "relax_projection", "=", "config", ".", "relax_projection", "\n", "", "else", ":", "\n", "            ", "self", ".", "relax_projection", "=", "0", "\n", "", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "\n", "def", "convert_to_type", "(", "tensor", ")", ":", "\n", "            ", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "return", "tensor", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                ", "return", "tensor", "\n", "", "", "self", ".", "type_converter", "=", "convert_to_type", "\n", "self", ".", "converted", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertLMPredictionHead.forward": [[570, 588], ["modeling.BertLMPredictionHead.transform", "modeling.BertLMPredictionHead.type_converter", "torch.linear.size", "torch.linear.size", "torch.linear", "torch.linear", "modeling.BertLMPredictionHead.transform.half", "torch.linear.view", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.decoder", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "if", "not", "self", ".", "converted", ":", "\n", "            ", "self", ".", "converted", "=", "True", "\n", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "self", ".", "transform", ".", "half", "(", ")", "\n", "", "", "hidden_states", "=", "self", ".", "transform", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ")", "\n", "if", "self", ".", "relax_projection", ">", "1", ":", "\n", "            ", "num_batch", "=", "hidden_states", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "hidden_states", ".", "size", "(", "1", ")", "\n", "# (batch, num_pos, relax_projection*hid) -> (batch, num_pos, relax_projection, hid) -> (batch, num_pos, hid)", "\n", "hidden_states", "=", "hidden_states", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "relax_projection", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "", "if", "self", ".", "fp32_embedding", ":", "\n", "            ", "hidden_states", "=", "F", ".", "linear", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ",", "self", ".", "type_converter", "(", "\n", "self", ".", "decoder", ".", "weight", ")", ",", "self", ".", "type_converter", "(", "self", ".", "bias", ")", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertOnlyMLMHead.__init__": [[591, 595], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "\n", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertOnlyMLMHead.forward": [[596, 599], ["modeling.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertOnlyNSPHead.__init__": [[602, 605], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertOnlyNSPHead.forward": [[606, 609], ["modeling.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPreTrainingHeads.__init__": [[612, 617], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "\n", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPreTrainingHeads.forward": [[618, 625], ["modeling.BertPreTrainingHeads.predictions", "modeling.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ",", "task_idx", ")", "\n", "if", "pooled_output", "is", "None", ":", "\n", "            ", "seq_relationship_score", "=", "None", "\n", "", "else", ":", "\n", "            ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.PreTrainedBertModel.__init__": [[632, 642], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "PreTrainedBertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.PreTrainedBertModel.init_bert_weights": [[643, 653], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.PreTrainedBertModel.from_pretrained": [[654, 959], ["os.path.isdir", "modeling.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "torch.load.keys", "zip", "getattr", "torch.load.copy", "torch.load.copy", "modeling.PreTrainedBertModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertConfig.from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-base-multilingual`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "\n", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "if", "(", "'config_path'", "in", "kwargs", ")", "and", "kwargs", "[", "'config_path'", "]", ":", "\n", "            ", "config_file", "=", "kwargs", "[", "'config_path'", "]", "\n", "", "else", ":", "\n", "            ", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "\n", "# define new type_vocab_size (there might be different numbers of segment ids)", "\n", "if", "'type_vocab_size'", "in", "kwargs", ":", "\n", "            ", "config", ".", "type_vocab_size", "=", "kwargs", "[", "'type_vocab_size'", "]", "\n", "# define new relax_projection", "\n", "", "if", "(", "'relax_projection'", "in", "kwargs", ")", "and", "kwargs", "[", "'relax_projection'", "]", ":", "\n", "            ", "config", ".", "relax_projection", "=", "kwargs", "[", "'relax_projection'", "]", "\n", "# new position embedding", "\n", "", "if", "(", "'new_pos_ids'", "in", "kwargs", ")", "and", "kwargs", "[", "'new_pos_ids'", "]", ":", "\n", "            ", "config", ".", "new_pos_ids", "=", "kwargs", "[", "'new_pos_ids'", "]", "\n", "# define new relax_projection", "\n", "", "if", "(", "'task_idx'", "in", "kwargs", ")", "and", "kwargs", "[", "'task_idx'", "]", ":", "\n", "            ", "config", ".", "task_idx", "=", "kwargs", "[", "'task_idx'", "]", "\n", "# define new max position embedding for length expansion", "\n", "", "if", "(", "'max_position_embeddings'", "in", "kwargs", ")", "and", "kwargs", "[", "'max_position_embeddings'", "]", ":", "\n", "            ", "config", ".", "max_position_embeddings", "=", "kwargs", "[", "'max_position_embeddings'", "]", "\n", "# use fp32 for embeddings", "\n", "", "if", "(", "'fp32_embedding'", "in", "kwargs", ")", "and", "kwargs", "[", "'fp32_embedding'", "]", ":", "\n", "            ", "config", ".", "fp32_embedding", "=", "kwargs", "[", "'fp32_embedding'", "]", "\n", "# type of FFN in transformer blocks", "\n", "", "if", "(", "'ffn_type'", "in", "kwargs", ")", "and", "kwargs", "[", "'ffn_type'", "]", ":", "\n", "            ", "config", ".", "ffn_type", "=", "kwargs", "[", "'ffn_type'", "]", "\n", "# label smoothing", "\n", "", "if", "(", "'label_smoothing'", "in", "kwargs", ")", "and", "kwargs", "[", "'label_smoothing'", "]", ":", "\n", "            ", "config", ".", "label_smoothing", "=", "kwargs", "[", "'label_smoothing'", "]", "\n", "# dropout", "\n", "", "if", "(", "'hidden_dropout_prob'", "in", "kwargs", ")", "and", "kwargs", "[", "'hidden_dropout_prob'", "]", ":", "\n", "            ", "config", ".", "hidden_dropout_prob", "=", "kwargs", "[", "'hidden_dropout_prob'", "]", "\n", "", "if", "(", "'attention_probs_dropout_prob'", "in", "kwargs", ")", "and", "kwargs", "[", "'attention_probs_dropout_prob'", "]", ":", "\n", "            ", "config", ".", "attention_probs_dropout_prob", "=", "kwargs", "[", "'attention_probs_dropout_prob'", "]", "\n", "# different QKV", "\n", "", "if", "(", "'num_qkv'", "in", "kwargs", ")", "and", "kwargs", "[", "'num_qkv'", "]", ":", "\n", "            ", "config", ".", "num_qkv", "=", "kwargs", "[", "'num_qkv'", "]", "\n", "# segment embedding for self-attention", "\n", "", "if", "(", "'seg_emb'", "in", "kwargs", ")", "and", "kwargs", "[", "'seg_emb'", "]", ":", "\n", "            ", "config", ".", "seg_emb", "=", "kwargs", "[", "'seg_emb'", "]", "\n", "# initialize word embeddings", "\n", "", "_word_emb_map", "=", "None", "\n", "if", "(", "'word_emb_map'", "in", "kwargs", ")", "and", "kwargs", "[", "'word_emb_map'", "]", ":", "\n", "            ", "_word_emb_map", "=", "kwargs", "[", "'word_emb_map'", "]", "\n", "\n", "", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "\n", "# clean the arguments in kwargs", "\n", "for", "arg_clean", "in", "(", "'config_path'", ",", "'type_vocab_size'", ",", "'relax_projection'", ",", "'new_pos_ids'", ",", "'task_idx'", ",", "'max_position_embeddings'", ",", "'fp32_embedding'", ",", "'ffn_type'", ",", "'label_smoothing'", ",", "'hidden_dropout_prob'", ",", "'attention_probs_dropout_prob'", ",", "'num_qkv'", ",", "'seg_emb'", ",", "'word_emb_map'", ")", ":", "\n", "            ", "if", "arg_clean", "in", "kwargs", ":", "\n", "                ", "del", "kwargs", "[", "arg_clean", "]", "\n", "\n", "# Instantiate model.", "\n", "", "", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ")", "\n", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "# initialize new segment embeddings", "\n", "", "_k", "=", "'bert.embeddings.token_type_embeddings.weight'", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "config", ".", "type_vocab_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"config.type_vocab_size != state_dict[bert.embeddings.token_type_embeddings.weight] ({0} != {1})\"", ".", "format", "(", "\n", "config", ".", "type_vocab_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "config", ".", "type_vocab_size", ">", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "# state_dict[_k].data = state_dict[_k].data.resize_(config.type_vocab_size, state_dict[_k].shape[1])", "\n", "                ", "state_dict", "[", "_k", "]", ".", "resize_", "(", "\n", "config", ".", "type_vocab_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", "\n", "# L2R", "\n", "if", "config", ".", "type_vocab_size", ">=", "3", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "2", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "# R2L", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "4", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "3", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "# S2S", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "6", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "4", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "5", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "1", ",", ":", "]", ")", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "7", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "6", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "1", ",", ":", "]", ")", "\n", "", "", "elif", "config", ".", "type_vocab_size", "<", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "config", ".", "type_vocab_size", ",", ":", "]", "\n", "\n", "", "", "_k", "=", "'bert.embeddings.position_embeddings.weight'", "\n", "n_config_pos_emb", "=", "4", "if", "config", ".", "new_pos_ids", "else", "1", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_pos_emb", "*", "config", ".", "hidden_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"n_config_pos_emb*config.hidden_size != state_dict[bert.embeddings.position_embeddings.weight] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_pos_emb", ",", "config", ".", "hidden_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", "%", "config", ".", "hidden_size", "==", "0", "\n", "n_state_pos_emb", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", "/", "config", ".", "hidden_size", ")", "\n", "assert", "(", "n_state_pos_emb", "==", "1", ")", "!=", "(", "n_config_pos_emb", "==", "\n", "1", ")", ",", "\"!!!!n_state_pos_emb == 1 xor n_config_pos_emb == 1!!!!\"", "\n", "if", "n_state_pos_emb", "==", "1", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "\n", "1", ",", "n_config_pos_emb", ",", "1", ")", ".", "reshape", "(", "(", "config", ".", "max_position_embeddings", ",", "n_config_pos_emb", "*", "config", ".", "hidden_size", ")", ")", "\n", "", "elif", "n_config_pos_emb", "==", "1", ":", "\n", "                ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                    ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                    ", "_task_idx", "=", "0", "\n", "", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "config", ".", "max_position_embeddings", ",", "n_state_pos_emb", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "1", ",", "_task_idx", ")", "\n", "\n", "# initialize new position embeddings", "\n", "", "", "_k", "=", "'bert.embeddings.position_embeddings.weight'", "\n", "if", "_k", "in", "state_dict", "and", "config", ".", "max_position_embeddings", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "            ", "logger", ".", "info", "(", "\"config.max_position_embeddings != state_dict[bert.embeddings.position_embeddings.weight] ({0} - {1})\"", ".", "format", "(", "\n", "config", ".", "max_position_embeddings", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "config", ".", "max_position_embeddings", ">", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "old_size", "=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "\n", "# state_dict[_k].data = state_dict[_k].data.resize_(config.max_position_embeddings, state_dict[_k].shape[1])", "\n", "state_dict", "[", "_k", "]", ".", "resize_", "(", "\n", "config", ".", "max_position_embeddings", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", "\n", "start", "=", "old_size", "\n", "while", "start", "<", "config", ".", "max_position_embeddings", ":", "\n", "                    ", "chunk_size", "=", "min", "(", "\n", "old_size", ",", "config", ".", "max_position_embeddings", "-", "start", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "start", ":", "start", "+", "chunk_size", ",", "\n", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "chunk_size", ",", ":", "]", ")", "\n", "start", "+=", "chunk_size", "\n", "", "", "elif", "config", ".", "max_position_embeddings", "<", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "config", ".", "max_position_embeddings", ",", ":", "]", "\n", "\n", "# initialize relax projection", "\n", "", "", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "n_config_relax", "=", "1", "if", "(", "config", ".", "relax_projection", "<", "\n", "1", ")", "else", "config", ".", "relax_projection", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_relax", "*", "config", ".", "hidden_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"n_config_relax*config.hidden_size != state_dict[cls.predictions.transform.dense.weight] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_relax", ",", "config", ".", "hidden_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "%", "config", ".", "hidden_size", "==", "0", "\n", "n_state_relax", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "/", "config", ".", "hidden_size", ")", "\n", "assert", "(", "n_state_relax", "==", "1", ")", "!=", "(", "n_config_relax", "==", "\n", "1", ")", ",", "\"!!!!n_state_relax == 1 xor n_config_relax == 1!!!!\"", "\n", "if", "n_state_relax", "==", "1", ":", "\n", "                ", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "\n", "n_config_relax", ",", "1", ",", "1", ")", ".", "reshape", "(", "(", "n_config_relax", "*", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", ")", "\n", "for", "_k", "in", "(", "'cls.predictions.transform.dense.bias'", ",", "'cls.predictions.transform.LayerNorm.weight'", ",", "'cls.predictions.transform.LayerNorm.bias'", ")", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "\n", "0", ")", ".", "repeat", "(", "n_config_relax", ",", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "", "", "elif", "n_config_relax", "==", "1", ":", "\n", "                ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                    ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                    ", "_task_idx", "=", "0", "\n", "", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_relax", ",", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "0", ",", "_task_idx", ")", "\n", "for", "_k", "in", "(", "'cls.predictions.transform.dense.bias'", ",", "'cls.predictions.transform.LayerNorm.weight'", ",", "'cls.predictions.transform.LayerNorm.bias'", ")", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_relax", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "0", ",", "_task_idx", ")", "\n", "\n", "# initialize QKV", "\n", "", "", "", "_all_head_size", "=", "config", ".", "num_attention_heads", "*", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "n_config_num_qkv", "=", "1", "if", "(", "config", ".", "num_qkv", "<", "1", ")", "else", "config", ".", "num_qkv", "\n", "for", "qkv_name", "in", "(", "'query'", ",", "'key'", ",", "'value'", ")", ":", "\n", "            ", "_k", "=", "'bert.encoder.layer.0.attention.self.{0}.weight'", ".", "format", "(", "\n", "qkv_name", ")", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_num_qkv", "*", "_all_head_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"n_config_num_qkv*_all_head_size != state_dict[_k] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_num_qkv", ",", "_all_head_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "for", "layer_idx", "in", "range", "(", "config", ".", "num_hidden_layers", ")", ":", "\n", "                    ", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "%", "_all_head_size", "==", "0", "\n", "n_state_qkv", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "/", "_all_head_size", ")", "\n", "assert", "(", "n_state_qkv", "==", "1", ")", "!=", "(", "n_config_num_qkv", "==", "\n", "1", ")", ",", "\"!!!!n_state_qkv == 1 xor n_config_num_qkv == 1!!!!\"", "\n", "if", "n_state_qkv", "==", "1", ":", "\n", "                        ", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "\n", "n_config_num_qkv", ",", "1", ",", "1", ")", ".", "reshape", "(", "(", "n_config_num_qkv", "*", "_all_head_size", ",", "_all_head_size", ")", ")", "\n", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.bias'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "\n", "0", ")", ".", "repeat", "(", "n_config_num_qkv", ",", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "", "elif", "n_config_num_qkv", "==", "1", ":", "\n", "                        ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                            ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                            ", "_task_idx", "=", "0", "\n", "", "assert", "_task_idx", "!=", "3", ",", "\"[INVALID] _task_idx=3: n_config_num_qkv=1 (should be 2)\"", "\n", "if", "_task_idx", "==", "0", ":", "\n", "                            ", "_qkv_idx", "=", "0", "\n", "", "else", ":", "\n", "                            ", "_qkv_idx", "=", "1", "\n", "", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_qkv", ",", "_all_head_size", ",", "_all_head_size", ")", ".", "select", "(", "0", ",", "_qkv_idx", ")", "\n", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.bias'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_qkv", ",", "_all_head_size", ")", ".", "select", "(", "0", ",", "_qkv_idx", ")", "\n", "\n", "", "", "", "", "if", "_word_emb_map", ":", "\n", "            ", "_k", "=", "'bert.embeddings.word_embeddings.weight'", "\n", "for", "_tgt", ",", "_src", "in", "_word_emb_map", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "[", "_tgt", ",", ":", "]", ".", "copy_", "(", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "_src", ",", ":", "]", ")", "\n", "\n", "", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "\n", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "load", "(", "model", ",", "prefix", "=", "''", "if", "hasattr", "(", "model", ",", "'bert'", ")", "else", "'bert.'", ")", "\n", "model", ".", "missing_keys", "=", "missing_keys", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'\\n'", ".", "join", "(", "error_msgs", ")", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertModel.__init__": [[993, 999], ["modeling.PreTrainedBertModel.__init__", "modeling.BertEmbeddings", "modeling.BertEncoder", "modeling.BertPooler", "modeling.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertModel.rescale_some_parameters": [[1000, 1005], ["enumerate", "layer.attention.output.dense.weight.data.div_", "layer.output.dense.weight.data.div_", "math.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "rescale_some_parameters", "(", "self", ")", ":", "\n", "        ", "for", "layer_id", ",", "layer", "in", "enumerate", "(", "self", ".", "encoder", ".", "layer", ")", ":", "\n", "            ", "layer", ".", "attention", ".", "output", ".", "dense", ".", "weight", ".", "data", ".", "div_", "(", "\n", "math", ".", "sqrt", "(", "2.0", "*", "(", "layer_id", "+", "1", ")", ")", ")", "\n", "layer", ".", "output", ".", "dense", ".", "weight", ".", "data", ".", "div_", "(", "math", ".", "sqrt", "(", "2.0", "*", "(", "layer_id", "+", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertModel.get_extended_attention_mask": [[1006, 1033], ["torch.ones_like.unsqueeze.to", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.dim", "torch.ones_like.dim", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.dim", "torch.ones_like.dim", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "next", "modeling.BertModel.parameters"], "methods", ["None"], ["", "", "def", "get_extended_attention_mask", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "if", "attention_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "", "elif", "attention_mask", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "return", "extended_attention_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertModel.forward": [[1034, 1047], ["modeling.BertModel.get_extended_attention_mask", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "modeling.BertModel.pooler"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertModel.get_extended_attention_mask"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertModelIncr.__init__": [[1050, 1052], ["modeling.BertModel.__init__"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModelIncr", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertModelIncr.forward": [[1053, 1070], ["modeling.BertModelIncr.get_extended_attention_mask", "modeling.BertModelIncr.embeddings", "modeling.BertModelIncr.encoder", "modeling.BertModelIncr.pooler"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertModel.get_extended_attention_mask"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "None", ",", "\n", "prev_encoded_layers", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "task_idx", "=", "task_idx", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "\n", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "embedding_output", ",", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForPreTraining.__init__": [[1117, 1123], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertPreTrainingHeads", "modeling.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForPreTraining", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForPreTraining.forward": [[1124, 1140], ["modeling.BertForPreTraining.bert", "modeling.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "next_sentence_label", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output", ",", "pooled_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "\n", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "\n", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPreTrainingPairTransform.__init__": [[1143, 1148], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingPairTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "# self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-5)", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPreTrainingPairTransform.forward": [[1150, 1156], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertPreTrainingPairTransform.dense", "modeling.BertPreTrainingPairTransform.transform_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pair_x", ",", "pair_y", ")", ":", "\n", "        ", "hidden_states", "=", "torch", ".", "cat", "(", "[", "pair_x", ",", "pair_y", "]", ",", "dim", "=", "-", "1", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "# hidden_states = self.LayerNorm(hidden_states)", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPreTrainingPairRel.__init__": [[1159, 1163], ["torch.nn.Module.__init__", "modeling.BertPreTrainingPairTransform", "torch.nn.Embedding", "torch.nn.Embedding"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_rel", "=", "0", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingPairRel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "R_xy", "=", "BertPreTrainingPairTransform", "(", "config", ")", "\n", "self", ".", "rel_emb", "=", "nn", ".", "Embedding", "(", "num_rel", ",", "config", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertPreTrainingPairRel.forward": [[1164, 1173], ["modeling.BertPreTrainingPairRel.R_xy", "modeling.BertPreTrainingPairRel.rel_emb", "modeling.BertPreTrainingPairRel.size", "torch.logsigmoid().mul_", "torch.logsigmoid().mul_", "torch.logsigmoid", "torch.logsigmoid", "pair_pos_neg_mask.type_as"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pair_x", ",", "pair_y", ",", "pair_r", ",", "pair_pos_neg_mask", ")", ":", "\n", "# (batch, num_pair, hidden)", "\n", "        ", "xy", "=", "self", ".", "R_xy", "(", "pair_x", ",", "pair_y", ")", "\n", "r", "=", "self", ".", "rel_emb", "(", "pair_r", ")", "\n", "_batch", ",", "_num_pair", ",", "_hidden", "=", "xy", ".", "size", "(", ")", "\n", "pair_score", "=", "(", "xy", "*", "r", ")", ".", "sum", "(", "-", "1", ")", "\n", "# torch.bmm(xy.view(-1, 1, _hidden),r.view(-1, _hidden, 1)).view(_batch, _num_pair)", "\n", "# .mul_(-1.0): objective to loss", "\n", "return", "F", ".", "logsigmoid", "(", "pair_score", "*", "pair_pos_neg_mask", ".", "type_as", "(", "pair_score", ")", ")", ".", "mul_", "(", "-", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.__init__": [[1178, 1207], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertPreTrainingHeads", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "modeling.BertForPreTrainingLossMask.apply", "modeling.BertForPreTrainingLossMask.bert.rescale_some_parameters", "torch.nn.Embedding", "torch.nn.Embedding", "modeling.BertPreTrainingHeads", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "modeling.BertPreTrainingPairRel", "hasattr", "loss.LabelSmoothingLoss"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertModel.rescale_some_parameters"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ",", "num_rel", "=", "0", ",", "num_sentlvl_labels", "=", "0", ",", "no_nsp", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForPreTrainingLossMask", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ",", "num_labels", "=", "num_labels", ")", "\n", "self", ".", "num_sentlvl_labels", "=", "num_sentlvl_labels", "\n", "self", ".", "cls2", "=", "None", "\n", "if", "self", ".", "num_sentlvl_labels", ">", "0", ":", "\n", "            ", "self", ".", "secondary_pred_proj", "=", "nn", ".", "Embedding", "(", "\n", "num_sentlvl_labels", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "cls2", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "secondary_pred_proj", ".", "weight", ",", "num_labels", "=", "num_sentlvl_labels", ")", "\n", "", "self", ".", "crit_mask_lm", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "if", "no_nsp", ":", "\n", "            ", "self", ".", "crit_next_sent", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "crit_next_sent", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "num_rel", "=", "num_rel", "\n", "if", "self", ".", "num_rel", ">", "0", ":", "\n", "            ", "self", ".", "crit_pair_rel", "=", "BertPreTrainingPairRel", "(", "\n", "config", ",", "num_rel", "=", "num_rel", ")", "\n", "", "if", "hasattr", "(", "config", ",", "'label_smoothing'", ")", "and", "config", ".", "label_smoothing", ":", "\n", "            ", "self", ".", "crit_mask_lm_smoothed", "=", "LabelSmoothingLoss", "(", "\n", "config", ".", "label_smoothing", ",", "config", ".", "vocab_size", ",", "ignore_index", "=", "0", ",", "reduction", "=", "'none'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "crit_mask_lm_smoothed", "=", "None", "\n", "", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "self", ".", "bert", ".", "rescale_some_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.forward": [[1208, 1339], ["modeling.BertForPreTrainingLossMask.bert", "modeling.BertForPreTrainingLossMask.forward.gather_seq_out_by_pos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "\n", "next_sentence_label", "=", "None", ",", "masked_pos", "=", "None", ",", "masked_weights", "=", "None", ",", "task_idx", "=", "None", ",", "pair_x", "=", "None", ",", "\n", "pair_x_mask", "=", "None", ",", "pair_y", "=", "None", ",", "pair_y_mask", "=", "None", ",", "pair_r", "=", "None", ",", "pair_pos_neg_mask", "=", "None", ",", "\n", "pair_loss_mask", "=", "None", ",", "masked_pos_2", "=", "None", ",", "masked_weights_2", "=", "None", ",", "masked_labels_2", "=", "None", ",", "\n", "num_tokens_a", "=", "None", ",", "num_tokens_b", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "if", "token_type_ids", "is", "None", "and", "attention_mask", "is", "None", ":", "\n", "            ", "task_0", "=", "(", "task_idx", "==", "0", ")", "\n", "task_1", "=", "(", "task_idx", "==", "1", ")", "\n", "task_2", "=", "(", "task_idx", "==", "2", ")", "\n", "task_3", "=", "(", "task_idx", "==", "3", ")", "\n", "\n", "sequence_length", "=", "input_ids", ".", "shape", "[", "-", "1", "]", "\n", "index_matrix", "=", "torch", ".", "arange", "(", "sequence_length", ")", ".", "view", "(", "\n", "1", ",", "sequence_length", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", "\n", "num_tokens", "=", "num_tokens_a", "+", "num_tokens_b", "\n", "\n", "base_mask", "=", "(", "index_matrix", "<", "num_tokens", ".", "view", "(", "-", "1", ",", "1", ")", "\n", ")", ".", "type_as", "(", "input_ids", ")", "\n", "segment_a_mask", "=", "(", "\n", "index_matrix", "<", "num_tokens_a", ".", "view", "(", "-", "1", ",", "1", ")", ")", ".", "type_as", "(", "input_ids", ")", "\n", "\n", "token_type_ids", "=", "(", "\n", "task_idx", "+", "1", "+", "task_3", ".", "type_as", "(", "task_idx", ")", ")", ".", "view", "(", "-", "1", ",", "1", ")", "*", "base_mask", "\n", "token_type_ids", "=", "token_type_ids", "-", "segment_a_mask", "*", "(", "task_0", "|", "task_3", ")", ".", "type_as", "(", "segment_a_mask", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "index_matrix", "=", "index_matrix", ".", "view", "(", "1", ",", "1", ",", "sequence_length", ")", "\n", "index_matrix_t", "=", "index_matrix", ".", "view", "(", "1", ",", "sequence_length", ",", "1", ")", "\n", "\n", "tril", "=", "index_matrix", "<=", "index_matrix_t", "\n", "\n", "attention_mask_task_0", "=", "(", "\n", "index_matrix", "<", "num_tokens", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "&", "(", "index_matrix_t", "<", "num_tokens", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "\n", "attention_mask_task_1", "=", "tril", "&", "attention_mask_task_0", "\n", "attention_mask_task_2", "=", "torch", ".", "transpose", "(", "\n", "tril", ",", "dim0", "=", "-", "2", ",", "dim1", "=", "-", "1", ")", "&", "attention_mask_task_0", "\n", "attention_mask_task_3", "=", "(", "\n", "(", "index_matrix", "<", "num_tokens_a", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "|", "tril", ")", "&", "attention_mask_task_0", "\n", "\n", "attention_mask", "=", "(", "attention_mask_task_0", "&", "task_0", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "|", "(", "attention_mask_task_1", "&", "task_1", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "|", "(", "attention_mask_task_2", "&", "task_2", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "|", "(", "attention_mask_task_3", "&", "task_3", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "\n", "attention_mask", "=", "attention_mask", ".", "type_as", "(", "input_ids", ")", "\n", "", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "\n", "def", "gather_seq_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "            ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "\n", "", "def", "gather_seq_out_by_pos_average", "(", "seq", ",", "pos", ",", "mask", ")", ":", "\n", "# pos/mask: (batch, num_pair, max_token_num)", "\n", "            ", "batch_size", ",", "max_token_num", "=", "pos", ".", "size", "(", "0", ")", ",", "pos", ".", "size", "(", "-", "1", ")", "\n", "# (batch, num_pair, max_token_num, seq.size(-1))", "\n", "pos_vec", "=", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "unsqueeze", "(", "\n", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", ".", "view", "(", "batch_size", ",", "-", "1", ",", "max_token_num", ",", "seq", ".", "size", "(", "-", "1", ")", ")", "\n", "# (batch, num_pair, seq.size(-1))", "\n", "mask", "=", "mask", ".", "type_as", "(", "pos_vec", ")", "\n", "pos_vec_masked_sum", "=", "(", "\n", "pos_vec", "*", "mask", ".", "unsqueeze", "(", "3", ")", ".", "expand_as", "(", "pos_vec", ")", ")", ".", "sum", "(", "2", ")", "\n", "return", "pos_vec_masked_sum", "/", "mask", ".", "sum", "(", "2", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "pos_vec_masked_sum", ")", "\n", "\n", "", "def", "loss_mask_and_normalize", "(", "loss", ",", "mask", ")", ":", "\n", "            ", "mask", "=", "mask", ".", "type_as", "(", "loss", ")", "\n", "loss", "=", "loss", "*", "mask", "\n", "denominator", "=", "torch", ".", "sum", "(", "mask", ")", "+", "1e-5", "\n", "return", "(", "loss", "/", "denominator", ")", ".", "sum", "(", ")", "\n", "\n", "", "if", "masked_lm_labels", "is", "None", ":", "\n", "            ", "if", "masked_pos", "is", "None", ":", "\n", "                ", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "", "else", ":", "\n", "                ", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n", "# masked lm", "\n", "", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores_masked", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "crit_mask_lm_smoothed", ":", "\n", "            ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm_smoothed", "(", "\n", "F", ".", "log_softmax", "(", "prediction_scores_masked", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ",", "masked_lm_labels", ")", "\n", "", "else", ":", "\n", "            ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm", "(", "\n", "prediction_scores_masked", ".", "transpose", "(", "1", ",", "2", ")", ".", "float", "(", ")", ",", "masked_lm_labels", ")", "\n", "", "masked_lm_loss", "=", "loss_mask_and_normalize", "(", "\n", "masked_lm_loss", ".", "float", "(", ")", ",", "masked_weights", ")", "\n", "\n", "# next sentence", "\n", "if", "self", ".", "crit_next_sent", "is", "None", "or", "next_sentence_label", "is", "None", ":", "\n", "            ", "next_sentence_loss", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "next_sentence_loss", "=", "self", ".", "crit_next_sent", "(", "\n", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ".", "float", "(", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "self", ".", "cls2", "is", "not", "None", "and", "masked_pos_2", "is", "not", "None", ":", "\n", "            ", "sequence_output_masked_2", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos_2", ")", "\n", "prediction_scores_masked_2", ",", "_", "=", "self", ".", "cls2", "(", "\n", "sequence_output_masked_2", ",", "None", ")", "\n", "masked_lm_loss_2", "=", "self", ".", "crit_mask_lm", "(", "\n", "prediction_scores_masked_2", ".", "transpose", "(", "1", ",", "2", ")", ".", "float", "(", ")", ",", "masked_labels_2", ")", "\n", "masked_lm_loss_2", "=", "loss_mask_and_normalize", "(", "\n", "masked_lm_loss_2", ".", "float", "(", ")", ",", "masked_weights_2", ")", "\n", "masked_lm_loss", "=", "masked_lm_loss", "+", "masked_lm_loss_2", "\n", "\n", "", "if", "pair_x", "is", "None", "or", "pair_y", "is", "None", "or", "pair_r", "is", "None", "or", "pair_pos_neg_mask", "is", "None", "or", "pair_loss_mask", "is", "None", ":", "\n", "            ", "return", "masked_lm_loss", ",", "next_sentence_loss", "\n", "\n", "# pair and relation", "\n", "", "if", "pair_x_mask", "is", "None", "or", "pair_y_mask", "is", "None", ":", "\n", "            ", "pair_x_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "pair_x", ")", "\n", "pair_y_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "pair_y", ")", "\n", "", "else", ":", "\n", "            ", "pair_x_output_masked", "=", "gather_seq_out_by_pos_average", "(", "\n", "sequence_output", ",", "pair_x", ",", "pair_x_mask", ")", "\n", "pair_y_output_masked", "=", "gather_seq_out_by_pos_average", "(", "\n", "sequence_output", ",", "pair_y", ",", "pair_y_mask", ")", "\n", "", "pair_loss", "=", "self", ".", "crit_pair_rel", "(", "\n", "pair_x_output_masked", ",", "pair_y_output_masked", ",", "pair_r", ",", "pair_pos_neg_mask", ")", "\n", "pair_loss", "=", "loss_mask_and_normalize", "(", "\n", "pair_loss", ".", "float", "(", ")", ",", "pair_loss_mask", ")", "\n", "return", "masked_lm_loss", ",", "next_sentence_loss", ",", "pair_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForExtractiveSummarization.__init__": [[1344, 1351], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Embedding", "torch.nn.Embedding", "modeling.BertPreTrainingHeads", "modeling.BertForExtractiveSummarization.apply"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForExtractiveSummarization", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "secondary_pred_proj", "=", "nn", ".", "Embedding", "(", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "cls2", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "secondary_pred_proj", ".", "weight", ",", "num_labels", "=", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForExtractiveSummarization.forward": [[1352, 1368], ["modeling.BertForExtractiveSummarization.bert", "modeling.BertForExtractiveSummarization.forward.gather_seq_out_by_pos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_pos_2", "=", "None", ",", "masked_weights_2", "=", "None", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "\n", "def", "gather_seq_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "            ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "\n", "", "sequence_output_masked_2", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos_2", ")", "\n", "prediction_scores_masked_2", ",", "_", "=", "self", ".", "cls2", "(", "\n", "sequence_output_masked_2", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "\n", "predicted_probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "\n", "prediction_scores_masked_2", ",", "dim", "=", "-", "1", ")", "\n", "\n", "return", "predicted_probs", ",", "masked_pos_2", ",", "masked_weights_2", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForSeq2SeqDecoder.__init__": [[1373, 1401], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModelIncr", "modeling.BertPreTrainingHeads", "modeling.BertForSeq2SeqDecoder.apply", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "modeling.BertPreTrainingPairRel"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "mask_word_id", "=", "0", ",", "num_labels", "=", "2", ",", "num_rel", "=", "0", ",", "\n", "search_beam_size", "=", "1", ",", "length_penalty", "=", "1.0", ",", "eos_id", "=", "0", ",", "sos_id", "=", "0", ",", "\n", "forbid_duplicate_ngrams", "=", "False", ",", "forbid_ignore_set", "=", "None", ",", "not_predict_set", "=", "None", ",", "ngram_size", "=", "3", ",", "min_len", "=", "0", ",", "mode", "=", "\"s2s\"", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForSeq2SeqDecoder", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModelIncr", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ",", "num_labels", "=", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "self", ".", "crit_mask_lm", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "self", ".", "crit_next_sent", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "self", ".", "mask_word_id", "=", "mask_word_id", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "num_rel", "=", "num_rel", "\n", "if", "self", ".", "num_rel", ">", "0", ":", "\n", "            ", "self", ".", "crit_pair_rel", "=", "BertPreTrainingPairRel", "(", "\n", "config", ",", "num_rel", "=", "num_rel", ")", "\n", "", "self", ".", "search_beam_size", "=", "search_beam_size", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "eos_id", "=", "eos_id", "\n", "self", ".", "sos_id", "=", "sos_id", "\n", "self", ".", "forbid_duplicate_ngrams", "=", "forbid_duplicate_ngrams", "\n", "self", ".", "forbid_ignore_set", "=", "forbid_ignore_set", "\n", "self", ".", "not_predict_set", "=", "not_predict_set", "\n", "self", ".", "ngram_size", "=", "ngram_size", "\n", "self", ".", "min_len", "=", "min_len", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForSeq2SeqDecoder.forward": [[1402, 1479], ["list", "list", "input_ids.new().fill_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForSeq2SeqDecoder.beam_search", "input_ids.size", "token_type_ids.size", "input_ids.new().fill_", "modeling.BertForSeq2SeqDecoder.bert", "modeling.BertForSeq2SeqDecoder.cls", "torch.max", "torch.max", "torch.max", "torch.max", "output_ids.append", "input_ids.new", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "input_ids.new", "curr_ids.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prediction_scores[].fill_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForSeq2SeqDecoder.beam_search"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "search_beam_size", ">", "1", ":", "\n", "            ", "return", "self", ".", "beam_search", "(", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "\n", "", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "\n", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "prediction_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "_", ",", "max_ids", "=", "torch", ".", "max", "(", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "output_ids", ".", "append", "(", "max_ids", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "new_embedding", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "x", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "", "", "curr_ids", "=", "max_ids", "\n", "next_pos", "+=", "1", "\n", "\n", "", "return", "torch", ".", "cat", "(", "output_ids", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForSeq2SeqDecoder.beam_search": [[1480, 1754], ["list", "list", "input_ids.new().fill_", "range", "input_ids.size", "first_expand.size", "input_ids.new().fill_", "modeling.BertForSeq2SeqDecoder.bert", "modeling.BertForSeq2SeqDecoder.cls", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "step_back_ptrs.append", "step_ids.append", "beam_masks.append", "total_scores.append", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "traces[].append", "traces[].append", "traces[].append", "enumerate", "range", "sequences[].data.new().fill_", "enumerate", "_pad_sequence().to", "input_ids.new", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "log_scores[].fill_", "len", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.div", "torch.div", "torch.div", "torch.div", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.eq().float", "torch.eq().float", "torch.eq().float", "torch.eq().float", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.repeat", "torch.reshape.repeat", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "list", "len", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "modeling.BertForSeq2SeqDecoder.beam_search.first_expand"], "methods", ["None"], ["", "def", "beam_search", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "K", "=", "self", ".", "search_beam_size", "\n", "\n", "total_scores", "=", "[", "]", "\n", "beam_masks", "=", "[", "]", "\n", "step_ids", "=", "[", "]", "\n", "step_back_ptrs", "=", "[", "]", "\n", "partial_seqs", "=", "[", "]", "\n", "forbid_word_mask", "=", "None", "\n", "buf_matrix", "=", "None", "\n", "\n", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "\n", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "log_scores", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "\n", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "if", "forbid_word_mask", "is", "not", "None", ":", "\n", "                ", "log_scores", "+=", "(", "forbid_word_mask", "*", "-", "10000.0", ")", "\n", "", "if", "self", ".", "min_len", "and", "(", "next_pos", "-", "input_length", "+", "1", "<=", "self", ".", "min_len", ")", ":", "\n", "                ", "log_scores", "[", ":", ",", ":", ",", "self", ".", "eos_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "log_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "kk_scores", ",", "kk_ids", "=", "torch", ".", "topk", "(", "log_scores", ",", "k", "=", "K", ")", "\n", "if", "len", "(", "total_scores", ")", "==", "0", ":", "\n", "                ", "k_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "back_ptrs", "=", "torch", ".", "zeros", "(", "batch_size", ",", "K", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "k_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "", "else", ":", "\n", "                ", "last_eos", "=", "torch", ".", "reshape", "(", "\n", "beam_masks", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "last_seq_scores", "=", "torch", ".", "reshape", "(", "\n", "total_scores", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "kk_scores", "+=", "last_eos", "*", "(", "-", "10000.0", ")", "+", "last_seq_scores", "\n", "kk_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_scores", ",", "k_ids", "=", "torch", ".", "topk", "(", "kk_scores", ",", "k", "=", "K", ")", "\n", "back_ptrs", "=", "torch", ".", "div", "(", "k_ids", ",", "K", ")", "\n", "kk_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_ids", "=", "torch", ".", "gather", "(", "kk_ids", ",", "1", ",", "k_ids", ")", "\n", "", "step_back_ptrs", ".", "append", "(", "back_ptrs", ")", "\n", "step_ids", ".", "append", "(", "k_ids", ")", "\n", "beam_masks", ".", "append", "(", "torch", ".", "eq", "(", "k_ids", ",", "self", ".", "eos_id", ")", ".", "float", "(", ")", ")", "\n", "total_scores", ".", "append", "(", "k_scores", ")", "\n", "\n", "def", "first_expand", "(", "x", ")", ":", "\n", "                ", "input_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "expanded_shape", "=", "input_shape", "[", ":", "1", "]", "+", "[", "1", "]", "+", "input_shape", "[", "1", ":", "]", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "expanded_shape", ")", "\n", "repeat_count", "=", "[", "1", ",", "K", "]", "+", "[", "1", "]", "*", "(", "len", "(", "input_shape", ")", "-", "1", ")", "\n", "x", "=", "x", ".", "repeat", "(", "*", "repeat_count", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "input_shape", "[", "0", "]", "*", "K", "]", "+", "input_shape", "[", "1", ":", "]", ")", "\n", "return", "x", "\n", "\n", "", "def", "select_beam_items", "(", "x", ",", "ids", ")", ":", "\n", "                ", "id_shape", "=", "list", "(", "ids", ".", "size", "(", ")", ")", "\n", "id_rank", "=", "len", "(", "id_shape", ")", "\n", "assert", "len", "(", "id_shape", ")", "==", "2", "\n", "x_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "batch_size", ",", "K", "]", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "x_rank", "=", "len", "(", "x_shape", ")", "+", "1", "\n", "assert", "x_rank", ">=", "2", "\n", "if", "id_rank", "<", "x_rank", ":", "\n", "                    ", "ids", "=", "torch", ".", "reshape", "(", "\n", "ids", ",", "id_shape", "+", "[", "1", "]", "*", "(", "x_rank", "-", "id_rank", ")", ")", "\n", "ids", "=", "ids", ".", "expand", "(", "id_shape", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "", "y", "=", "torch", ".", "gather", "(", "x", ",", "1", ",", "ids", ")", "\n", "y", "=", "torch", ".", "reshape", "(", "y", ",", "x_shape", ")", "\n", "return", "y", "\n", "\n", "", "is_first", "=", "(", "prev_embedding", "is", "None", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "\n", "", "", "curr_ids", "=", "torch", ".", "reshape", "(", "k_ids", ",", "[", "batch_size", "*", "K", ",", "1", "]", ")", "\n", "\n", "if", "is_first", ":", "\n", "                ", "token_type_ids", "=", "first_expand", "(", "token_type_ids", ")", "\n", "position_ids", "=", "first_expand", "(", "position_ids", ")", "\n", "attention_mask", "=", "first_expand", "(", "attention_mask", ")", "\n", "mask_ids", "=", "first_expand", "(", "mask_ids", ")", "\n", "if", "mask_qkv", "is", "not", "None", ":", "\n", "                    ", "mask_qkv", "=", "first_expand", "(", "mask_qkv", ")", "\n", "\n", "", "", "if", "self", ".", "forbid_duplicate_ngrams", ":", "\n", "                ", "wids", "=", "step_ids", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "ptrs", "=", "step_back_ptrs", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "if", "is_first", ":", "\n", "                    ", "partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "partial_seqs", ".", "append", "(", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "new_partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "new_partial_seqs", ".", "append", "(", "\n", "partial_seqs", "[", "ptrs", "[", "b", "]", "[", "k", "]", "+", "b", "*", "K", "]", "+", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "partial_seqs", "=", "new_partial_seqs", "\n", "\n", "", "def", "get_dup_ngram_candidates", "(", "seq", ",", "n", ")", ":", "\n", "                    ", "cands", "=", "set", "(", ")", "\n", "if", "len", "(", "seq", ")", "<", "n", ":", "\n", "                        ", "return", "[", "]", "\n", "", "tail", "=", "seq", "[", "-", "(", "n", "-", "1", ")", ":", "]", "\n", "if", "self", ".", "forbid_ignore_set", "and", "any", "(", "tk", "in", "self", ".", "forbid_ignore_set", "for", "tk", "in", "tail", ")", ":", "\n", "                        ", "return", "[", "]", "\n", "", "for", "i", "in", "range", "(", "len", "(", "seq", ")", "-", "(", "n", "-", "1", ")", ")", ":", "\n", "                        ", "mismatch", "=", "False", "\n", "for", "j", "in", "range", "(", "n", "-", "1", ")", ":", "\n", "                            ", "if", "tail", "[", "j", "]", "!=", "seq", "[", "i", "+", "j", "]", ":", "\n", "                                ", "mismatch", "=", "True", "\n", "break", "\n", "", "", "if", "(", "not", "mismatch", ")", "and", "not", "(", "self", ".", "forbid_ignore_set", "and", "(", "seq", "[", "i", "+", "n", "-", "1", "]", "in", "self", ".", "forbid_ignore_set", ")", ")", ":", "\n", "                            ", "cands", ".", "add", "(", "seq", "[", "i", "+", "n", "-", "1", "]", ")", "\n", "", "", "return", "list", "(", "sorted", "(", "cands", ")", ")", "\n", "\n", "", "if", "len", "(", "partial_seqs", "[", "0", "]", ")", ">=", "self", ".", "ngram_size", ":", "\n", "                    ", "dup_cands", "=", "[", "]", "\n", "for", "seq", "in", "partial_seqs", ":", "\n", "                        ", "dup_cands", ".", "append", "(", "\n", "get_dup_ngram_candidates", "(", "seq", ",", "self", ".", "ngram_size", ")", ")", "\n", "", "if", "max", "(", "len", "(", "x", ")", "for", "x", "in", "dup_cands", ")", ">", "0", ":", "\n", "                        ", "if", "buf_matrix", "is", "None", ":", "\n", "                            ", "vocab_size", "=", "list", "(", "log_scores", ".", "size", "(", ")", ")", "[", "-", "1", "]", "\n", "buf_matrix", "=", "np", ".", "zeros", "(", "\n", "(", "batch_size", "*", "K", ",", "vocab_size", ")", ",", "dtype", "=", "float", ")", "\n", "", "else", ":", "\n", "                            ", "buf_matrix", ".", "fill", "(", "0", ")", "\n", "", "for", "bk", ",", "cands", "in", "enumerate", "(", "dup_cands", ")", ":", "\n", "                            ", "for", "i", ",", "wid", "in", "enumerate", "(", "cands", ")", ":", "\n", "                                ", "buf_matrix", "[", "bk", ",", "wid", "]", "=", "1.0", "\n", "", "", "forbid_word_mask", "=", "torch", ".", "tensor", "(", "\n", "buf_matrix", ",", "dtype", "=", "log_scores", ".", "dtype", ")", "\n", "forbid_word_mask", "=", "torch", ".", "reshape", "(", "\n", "forbid_word_mask", ",", "[", "batch_size", "*", "K", ",", "1", ",", "vocab_size", "]", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "                        ", "forbid_word_mask", "=", "None", "\n", "", "", "", "next_pos", "+=", "1", "\n", "\n", "# [(batch, beam)]", "\n", "", "total_scores", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "total_scores", "]", "\n", "step_ids", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_ids", "]", "\n", "step_back_ptrs", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_back_ptrs", "]", "\n", "# back tracking", "\n", "traces", "=", "{", "'pred_seq'", ":", "[", "]", ",", "'scores'", ":", "[", "]", ",", "'wids'", ":", "[", "]", ",", "'ptrs'", ":", "[", "]", "}", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "# [(beam,)]", "\n", "            ", "scores", "=", "[", "x", "[", "b", "]", "for", "x", "in", "total_scores", "]", "\n", "wids_list", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_ids", "]", "\n", "ptrs", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_back_ptrs", "]", "\n", "traces", "[", "'scores'", "]", ".", "append", "(", "scores", ")", "\n", "traces", "[", "'wids'", "]", ".", "append", "(", "wids_list", ")", "\n", "traces", "[", "'ptrs'", "]", ".", "append", "(", "ptrs", ")", "\n", "# first we need to find the eos frame where all symbols are eos", "\n", "# any frames after the eos frame are invalid", "\n", "last_frame_id", "=", "len", "(", "scores", ")", "-", "1", "\n", "for", "i", ",", "wids", "in", "enumerate", "(", "wids_list", ")", ":", "\n", "                ", "if", "all", "(", "wid", "==", "self", ".", "eos_id", "for", "wid", "in", "wids", ")", ":", "\n", "                    ", "last_frame_id", "=", "i", "\n", "break", "\n", "", "", "max_score", "=", "-", "math", ".", "inf", "\n", "frame_id", "=", "-", "1", "\n", "pos_in_frame", "=", "-", "1", "\n", "\n", "for", "fid", "in", "range", "(", "last_frame_id", "+", "1", ")", ":", "\n", "                ", "for", "i", ",", "wid", "in", "enumerate", "(", "wids_list", "[", "fid", "]", ")", ":", "\n", "                    ", "if", "wid", "==", "self", ".", "eos_id", "or", "fid", "==", "last_frame_id", ":", "\n", "                        ", "s", "=", "scores", "[", "fid", "]", "[", "i", "]", "\n", "if", "self", ".", "length_penalty", ">", "0", ":", "\n", "                            ", "s", "/=", "math", ".", "pow", "(", "(", "5", "+", "fid", "+", "1", ")", "/", "6.0", ",", "\n", "self", ".", "length_penalty", ")", "\n", "", "if", "s", ">", "max_score", ":", "\n", "                            ", "max_score", "=", "s", "\n", "frame_id", "=", "fid", "\n", "pos_in_frame", "=", "i", "\n", "", "", "", "", "if", "frame_id", "==", "-", "1", ":", "\n", "                ", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "seq", "=", "[", "wids_list", "[", "frame_id", "]", "[", "pos_in_frame", "]", "]", "\n", "for", "fid", "in", "range", "(", "frame_id", ",", "0", ",", "-", "1", ")", ":", "\n", "                    ", "pos_in_frame", "=", "ptrs", "[", "fid", "]", "[", "pos_in_frame", "]", "\n", "seq", ".", "append", "(", "wids_list", "[", "fid", "-", "1", "]", "[", "pos_in_frame", "]", ")", "\n", "", "seq", ".", "reverse", "(", ")", "\n", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "seq", ")", "\n", "\n", "", "", "def", "_pad_sequence", "(", "sequences", ",", "max_len", ",", "padding_value", "=", "0", ")", ":", "\n", "            ", "trailing_dims", "=", "sequences", "[", "0", "]", ".", "size", "(", ")", "[", "1", ":", "]", "\n", "out_dims", "=", "(", "len", "(", "sequences", ")", ",", "max_len", ")", "+", "trailing_dims", "\n", "\n", "out_tensor", "=", "sequences", "[", "0", "]", ".", "data", ".", "new", "(", "*", "out_dims", ")", ".", "fill_", "(", "padding_value", ")", "\n", "for", "i", ",", "tensor", "in", "enumerate", "(", "sequences", ")", ":", "\n", "                ", "length", "=", "tensor", ".", "size", "(", "0", ")", "\n", "# use index notation to prevent duplicate references to the tensor", "\n", "out_tensor", "[", "i", ",", ":", "length", ",", "...", "]", "=", "tensor", "\n", "", "return", "out_tensor", "\n", "\n", "# convert to tensors for DataParallel", "\n", "", "for", "k", "in", "(", "'pred_seq'", ",", "'scores'", ",", "'wids'", ",", "'ptrs'", ")", ":", "\n", "            ", "ts_list", "=", "traces", "[", "k", "]", "\n", "if", "not", "isinstance", "(", "ts_list", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "dt", "=", "torch", ".", "float", "if", "k", "==", "'scores'", "else", "torch", ".", "long", "\n", "ts_list", "=", "[", "torch", ".", "tensor", "(", "it", ",", "dtype", "=", "dt", ")", "for", "it", "in", "ts_list", "]", "\n", "", "traces", "[", "k", "]", "=", "_pad_sequence", "(", "\n", "ts_list", ",", "output_length", ",", "padding_value", "=", "0", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", "\n", "", "return", "traces", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForMaskedLM.__init__": [[1799, 1805], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertOnlyMLMHead", "modeling.BertForMaskedLM.apply"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForMaskedLM", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForMaskedLM.forward": [[1806, 1818], ["modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling.BertForMaskedLM.view", "masked_lm_labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "\n", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "masked_lm_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForNextSentencePrediction.__init__": [[1864, 1869], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertOnlyNSPHead", "modeling.BertForNextSentencePrediction.apply"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForNextSentencePrediction", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyNSPHead", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForNextSentencePrediction.forward": [[1870, 1882], ["modeling.BertForNextSentencePrediction.bert", "modeling.BertForNextSentencePrediction.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling.BertForNextSentencePrediction.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "next_sentence_label", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "seq_relationship_score", "=", "self", ".", "cls", "(", "pooled_output", ")", "\n", "\n", "if", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "\n", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "next_sentence_loss", "\n", "", "else", ":", "\n", "            ", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForSequenceClassification.__init__": [[1930, 1937], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForSequenceClassification.apply"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForSequenceClassification.forward": [[1938, 1958], ["modeling.BertForSequenceClassification.bert", "modeling.BertForSequenceClassification.dropout", "modeling.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "modeling.BertForSequenceClassification.view", "labels.view", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "print", "modeling.BertForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "labels", ".", "dtype", "==", "torch", ".", "long", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "\n", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "labels", ".", "dtype", "==", "torch", ".", "half", "or", "labels", ".", "dtype", "==", "torch", ".", "float", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'unkown labels.dtype'", ")", "\n", "loss", "=", "None", "\n", "", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForMultipleChoice.__init__": [[2005, 2012], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForMultipleChoice.apply"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_choices", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForMultipleChoice", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_choices", "=", "num_choices", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForMultipleChoice.forward": [[2013, 2029], ["input_ids.view", "token_type_ids.view", "attention_mask.view", "modeling.BertForMultipleChoice.bert", "modeling.BertForMultipleChoice.dropout", "modeling.BertForMultipleChoice.classifier", "modeling.BertForMultipleChoice.view", "input_ids.size", "token_type_ids.size", "attention_mask.size", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "flat_input_ids", ",", "flat_token_type_ids", ",", "flat_attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_choices", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "reshaped_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForTokenClassification.__init__": [[2077, 2084], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForTokenClassification.apply"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForTokenClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForTokenClassification.forward": [[2085, 2105], ["modeling.BertForTokenClassification.bert", "modeling.BertForTokenClassification.dropout", "modeling.BertForTokenClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "modeling.BertForTokenClassification.view", "labels.view", "modeling.BertForTokenClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "\n", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForQuestionAnswering.__init__": [[2163, 2169], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForQuestionAnswering.apply"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.BertForQuestionAnswering.forward": [[2170, 2196], ["modeling.BertForQuestionAnswering.bert", "modeling.BertForQuestionAnswering.qa_outputs", "modeling.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "task_idx", "=", "task_idx", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.gelu": [[42, 48], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.modeling.swish": [[50, 52], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.url_to_filename": [[30, 46], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ":", "str", ",", "etag", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.filename_to_url": [[48, 72], ["isinstance", "os.path.join", "str", "os.path.exists", "FileNotFoundError", "os.path.exists", "FileNotFoundError", "open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.cached_path": [[74, 102], ["isinstance", "isinstance", "urllib.parse.urlparse", "str", "str", "file_utils.get_from_cache", "os.path.exists", "FileNotFoundError", "ValueError"], "function", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ":", "Union", "[", "str", ",", "Path", "]", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.split_s3_path": [[104, 115], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ":", "str", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.s3_request": [[117, 134], ["functools.wraps", "func", "int", "FileNotFoundError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ":", "Callable", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ":", "str", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.s3_etag": [[136, 143], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ":", "str", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.s3_get": [[145, 151], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.http_get": [[153, 163], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["None"], ["", "def", "http_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.get_from_cache": [[165, 222], ["isinstance", "os.makedirs", "url.startswith", "file_utils.url_to_filename", "os.path.join", "str", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "open", "shutil.copyfileobj", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.url_to_filename", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.s3_etag", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.s3_get", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.http_get"], ["", "def", "get_from_cache", "(", "url", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.read_set_from_file": [[224, 234], ["set", "open", "set.add", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.add"], ["", "def", "read_set_from_file", "(", "filename", ":", "str", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.get_file_extension": [[236, 240], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ":", "str", ",", "dot", "=", "True", ",", "lower", ":", "bool", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.__init__": [[93, 105], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ",", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[X_SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "\n", "do_lower_case", "=", "do_lower_case", ",", "never_split", "=", "never_split", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.tokenize": [[106, 112], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids": [[113, 126], ["ids.append", "len", "ValueError", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "\n", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_ids_to_tokens": [[127, 133], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained": [[134, 171], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.file_utils.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WhitespaceTokenizer.tokenize": [[174, 176], ["tokenization.whitespace_tokenize"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.whitespace_tokenize"], ["    ", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "return", "whitespace_tokenize", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer.__init__": [[181, 189], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ",", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer.tokenize": [[190, 210], ["tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_strip_accents": [[211, 221], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_split_on_punc": [[222, 243], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._tokenize_chinese_chars": [[244, 256], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._is_chinese_char": [[257, 278], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.BasicTokenizer._clean_text": [[279, 291], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__": [[296, 300], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize": [[301, 351], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.load_vocab": [[51, 79], ["range", "collections.OrderedDict", "open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "# mapping unused tokens to special tokens", "\n", "extra_map", "=", "{", "}", "\n", "extra_map", "[", "'[unused1]'", "]", "=", "'[X_SEP]'", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "        ", "extra_map", "[", "'[unused{}]'", ".", "format", "(", "i", "+", "2", ")", "]", "=", "'[SEP_{}]'", ".", "format", "(", "i", ")", "\n", "", "extra_map", "[", "'[unused12]'", "]", "=", "'[S2S_SEP]'", "\n", "extra_map", "[", "'[unused13]'", "]", "=", "'[S2S_CLS]'", "\n", "extra_map", "[", "'[unused14]'", "]", "=", "'[L2R_SEP]'", "\n", "extra_map", "[", "'[unused15]'", "]", "=", "'[L2R_CLS]'", "\n", "extra_map", "[", "'[unused16]'", "]", "=", "'[R2L_SEP]'", "\n", "extra_map", "[", "'[unused17]'", "]", "=", "'[R2L_CLS]'", "\n", "extra_map", "[", "'[unused18]'", "]", "=", "'[S2S_SOS]'", "\n", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "if", "token", "in", "extra_map", ":", "\n", "                ", "token", "=", "extra_map", "[", "token", "]", "\n", "", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.whitespace_tokenize": [[81, 88], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization._is_whitespace": [[353, 363], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization._is_control": [[365, 375], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization._is_punctuation": [[377, 391], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.loss.LabelSmoothingLoss.__init__": [[19, 34], ["torch.nn.modules.loss._Loss.__init__", "torch.full", "torch.full", "torch.full", "torch.full", "loss.LabelSmoothingLoss.register_buffer", "torch.full.unsqueeze", "torch.full.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "label_smoothing", "=", "0", ",", "tgt_vocab_size", "=", "0", ",", "ignore_index", "=", "0", ",", "size_average", "=", "None", ",", "reduce", "=", "None", ",", "reduction", "=", "'mean'", ")", ":", "\n", "        ", "assert", "0.0", "<", "label_smoothing", "<=", "1.0", "\n", "self", ".", "ignore_index", "=", "ignore_index", "\n", "super", "(", "LabelSmoothingLoss", ",", "self", ")", ".", "__init__", "(", "\n", "size_average", "=", "size_average", ",", "reduce", "=", "reduce", ",", "reduction", "=", "reduction", ")", "\n", "\n", "assert", "label_smoothing", ">", "0", "\n", "assert", "tgt_vocab_size", ">", "0", "\n", "\n", "smoothing_value", "=", "label_smoothing", "/", "(", "tgt_vocab_size", "-", "2", ")", "\n", "one_hot", "=", "torch", ".", "full", "(", "(", "tgt_vocab_size", ",", ")", ",", "smoothing_value", ")", "\n", "one_hot", "[", "self", ".", "ignore_index", "]", "=", "0", "\n", "self", ".", "register_buffer", "(", "'one_hot'", ",", "one_hot", ".", "unsqueeze", "(", "0", ")", ")", "\n", "self", ".", "confidence", "=", "1.0", "-", "label_smoothing", "\n", "self", ".", "tgt_vocab_size", "=", "tgt_vocab_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.loss.LabelSmoothingLoss.forward": [[35, 49], ["output.view.view.view", "target.view.view.view", "loss.LabelSmoothingLoss.one_hot.repeat", "loss.LabelSmoothingLoss.scatter_", "loss.LabelSmoothingLoss.masked_fill_", "torch.kl_div().view().sum", "torch.kl_div().view().sum", "output.view.view.size", "target.view.view.size", "target.view.view.size", "target.view.view.size", "target.view.view.unsqueeze", "torch.kl_div().view", "torch.kl_div().view", "torch.kl_div", "torch.kl_div"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        output (FloatTensor): batch_size * num_pos * n_classes\n        target (LongTensor): batch_size * num_pos\n        \"\"\"", "\n", "assert", "self", ".", "tgt_vocab_size", "==", "output", ".", "size", "(", "2", ")", "\n", "batch_size", ",", "num_pos", "=", "target", ".", "size", "(", "0", ")", ",", "target", ".", "size", "(", "1", ")", "\n", "output", "=", "output", ".", "view", "(", "-", "1", ",", "self", ".", "tgt_vocab_size", ")", "\n", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "model_prob", "=", "self", ".", "one_hot", ".", "repeat", "(", "target", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "model_prob", ".", "scatter_", "(", "1", ",", "target", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "confidence", ")", "\n", "model_prob", ".", "masked_fill_", "(", "(", "target", "==", "self", ".", "ignore_index", ")", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "return", "F", ".", "kl_div", "(", "output", ",", "model_prob", ",", "reduction", "=", "'none'", ")", ".", "view", "(", "batch_size", ",", "num_pos", ",", "-", "1", ")", ".", "sum", "(", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.__main__.main": [[2, 20], ["len", "print", "sys.argv.pop", "sys.argv.pop", "sys.argv.pop", "convert_tf_checkpoint_to_pytorch", "print"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "import", "sys", "\n", "try", ":", "\n", "        ", "from", ".", "convert_tf_checkpoint_to_pytorch", "import", "convert_tf_checkpoint_to_pytorch", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "        ", "print", "(", "\"pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, \"", "\n", "\"In that case, it requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "\n", "", "if", "len", "(", "sys", ".", "argv", ")", "!=", "5", ":", "\n", "# pylint: disable=line-too-long", "\n", "        ", "print", "(", "\"Should be used as `pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`\"", ")", "\n", "", "else", ":", "\n", "        ", "PYTORCH_DUMP_OUTPUT", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "TF_CONFIG", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "TF_CHECKPOINT", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "convert_tf_checkpoint_to_pytorch", "(", "TF_CHECKPOINT", ",", "TF_CONFIG", ",", "PYTORCH_DUMP_OUTPUT", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieNode.__init__": [[27, 30], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "children", "=", "{", "}", "\n", "self", ".", "is_leaf", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieNode.try_get_children": [[31, 35], ["loader_utils.TrieNode"], "methods", ["None"], ["", "def", "try_get_children", "(", "self", ",", "key", ")", ":", "\n", "        ", "if", "key", "not", "in", "self", ".", "children", ":", "\n", "            ", "self", ".", "children", "[", "key", "]", "=", "TrieNode", "(", ")", "\n", "", "return", "self", ".", "children", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.__init__": [[38, 40], ["loader_utils.TrieNode"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "root", "=", "TrieNode", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.add": [[41, 46], ["r.try_get_children.try_get_children.try_get_children"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieNode.try_get_children"], ["", "def", "add", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "r", "=", "self", ".", "root", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "r", "=", "r", ".", "try_get_children", "(", "token", ")", "\n", "", "r", ".", "is_leaf", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.get_pieces": [[47, 69], ["len", "pieces.append", "len", "list", "range"], "methods", ["None"], ["", "def", "get_pieces", "(", "self", ",", "tokens", ",", "offset", ")", ":", "\n", "        ", "pieces", "=", "[", "]", "\n", "r", "=", "self", ".", "root", "\n", "token_id", "=", "0", "\n", "last_valid", "=", "0", "\n", "match_count", "=", "0", "\n", "while", "last_valid", "<", "len", "(", "tokens", ")", ":", "\n", "            ", "if", "token_id", "<", "len", "(", "tokens", ")", "and", "tokens", "[", "token_id", "]", "in", "r", ".", "children", ":", "\n", "                ", "r", "=", "r", ".", "children", "[", "tokens", "[", "token_id", "]", "]", "\n", "match_count", "+=", "1", "\n", "if", "r", ".", "is_leaf", ":", "\n", "                    ", "last_valid", "=", "token_id", "\n", "", "token_id", "+=", "1", "\n", "", "else", ":", "\n", "                ", "pieces", ".", "append", "(", "\n", "list", "(", "range", "(", "token_id", "-", "match_count", "+", "offset", ",", "last_valid", "+", "1", "+", "offset", ")", ")", ")", "\n", "last_valid", "+=", "1", "\n", "token_id", "=", "last_valid", "\n", "r", "=", "self", ".", "root", "\n", "match_count", "=", "0", "\n", "\n", "", "", "return", "pieces", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.Pipeline.__init__": [[94, 110], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "skipgram_prb", "=", "None", "\n", "self", ".", "skipgram_size", "=", "None", "\n", "self", ".", "pre_whole_word", "=", "None", "\n", "self", ".", "mask_whole_word", "=", "None", "\n", "self", ".", "word_subsample_prb", "=", "None", "\n", "self", ".", "sp_prob", "=", "None", "\n", "self", ".", "pieces_dir", "=", "None", "\n", "self", ".", "vocab_words", "=", "None", "\n", "self", ".", "pieces_threshold", "=", "10", "\n", "self", ".", "trie", "=", "None", "\n", "self", ".", "call_count", "=", "0", "\n", "self", ".", "offline_mode", "=", "False", "\n", "self", ".", "skipgram_size_geo_list", "=", "None", "\n", "self", ".", "span_same_mask", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.Pipeline.init_skipgram_size_geo_list": [[111, 120], ["range", "sum", "g_list.append"], "methods", ["None"], ["", "def", "init_skipgram_size_geo_list", "(", "self", ",", "p", ")", ":", "\n", "        ", "if", "p", ">", "0", ":", "\n", "            ", "g_list", "=", "[", "]", "\n", "t", "=", "p", "\n", "for", "_", "in", "range", "(", "self", ".", "skipgram_size", ")", ":", "\n", "                ", "g_list", ".", "append", "(", "t", ")", "\n", "t", "*=", "(", "1", "-", "p", ")", "\n", "", "s", "=", "sum", "(", "g_list", ")", "\n", "self", ".", "skipgram_size_geo_list", "=", "[", "x", "/", "s", "for", "x", "in", "g_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.Pipeline.create_trie_tree": [[121, 140], ["print", "print", "loader_utils.TrieTree", "loader_utils.Pipeline.trie.add", "print", "open", "line.split", "loader_utils.Pipeline.trie.add", "int", "tokens.extend", "part.split"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.add"], ["", "", "def", "create_trie_tree", "(", "self", ",", "pieces_dir", ")", ":", "\n", "        ", "print", "(", "\"sp_prob = {}\"", ".", "format", "(", "self", ".", "sp_prob", ")", ")", "\n", "print", "(", "\"pieces_threshold = {}\"", ".", "format", "(", "self", ".", "pieces_threshold", ")", ")", "\n", "if", "pieces_dir", "is", "not", "None", ":", "\n", "            ", "self", ".", "trie", "=", "TrieTree", "(", ")", "\n", "pieces_files", "=", "[", "pieces_dir", "]", "\n", "for", "token", "in", "self", ".", "vocab_words", ":", "\n", "                ", "self", ".", "trie", ".", "add", "(", "[", "token", "]", ")", "\n", "", "for", "piece_file", "in", "pieces_files", ":", "\n", "                ", "print", "(", "\"Load piece file: {}\"", ".", "format", "(", "piece_file", ")", ")", "\n", "with", "open", "(", "piece_file", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                    ", "for", "line", "in", "reader", ":", "\n", "                        ", "parts", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "int", "(", "parts", "[", "-", "1", "]", ")", "<", "self", ".", "pieces_threshold", ":", "\n", "                            ", "pass", "\n", "", "tokens", "=", "[", "]", "\n", "for", "part", "in", "parts", "[", ":", "-", "1", "]", ":", "\n", "                            ", "tokens", ".", "extend", "(", "part", ".", "split", "(", "' '", ")", ")", "\n", "", "self", ".", "trie", ".", "add", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.Pipeline.__call__": [[141, 143], ["None"], "methods", ["None"], ["", "", "", "", "", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.Pipeline.get_masked_pos": [[146, 285], ["list", "set", "enumerate", "random.random.shuffle", "set", "list", "loader_utils.Pipeline.create_trie_tree", "list", "zip", "enumerate", "any", "range", "len", "random.random.shuffle", "len", "loader_utils.Pipeline.trie.get_pieces", "list", "_get_word_split_index.append", "loader_utils._get_word_split_index", "range", "loader_utils.Pipeline.trie.get_pieces", "set.add", "len", "range", "loader_utils._expand_whole_word", "set", "enumerate", "range", "len", "len", "tokens[].endswith", "tokens[].endswith", "tokens[].endswith", "cand_pos.append", "loader_utils._get_word_split_index", "list", "len", "list.add", "tokens[].startswith", "new_pieces[].extend", "new_pieces.append", "len", "cand_pos.append", "len", "range", "random.random.random", "min", "zip", "range", "list.add", "len", "numpy.random.choice", "random.random.random", "random.random.randint", "random.random.random", "set.add", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.Pipeline.create_trie_tree", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.get_pieces", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils._get_word_split_index", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.get_pieces", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils._expand_whole_word", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils._get_word_split_index", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.add"], ["", "def", "get_masked_pos", "(", "self", ",", "tokens", ",", "n_pred", ",", "add_skipgram", "=", "False", ",", "mask_segment", "=", "None", ",", "protect_range", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "pieces_dir", "is", "not", "None", "and", "self", ".", "trie", "is", "None", ":", "\n", "            ", "self", ".", "create_trie_tree", "(", "self", ".", "pieces_dir", ")", "\n", "", "if", "self", ".", "pre_whole_word", ":", "\n", "            ", "if", "self", ".", "trie", "is", "not", "None", ":", "\n", "                ", "pieces", "=", "self", ".", "trie", ".", "get_pieces", "(", "tokens", ",", "0", ")", "\n", "\n", "new_pieces", "=", "[", "]", "\n", "for", "piece", "in", "pieces", ":", "\n", "                    ", "if", "len", "(", "new_pieces", ")", ">", "0", "and", "tokens", "[", "piece", "[", "0", "]", "]", ".", "startswith", "(", "\"##\"", ")", ":", "\n", "                        ", "new_pieces", "[", "-", "1", "]", ".", "extend", "(", "piece", ")", "\n", "", "else", ":", "\n", "                        ", "new_pieces", ".", "append", "(", "piece", ")", "\n", "", "", "del", "pieces", "\n", "pieces", "=", "new_pieces", "\n", "\n", "pre_word_split", "=", "list", "(", "_", "[", "-", "1", "]", "for", "_", "in", "pieces", ")", "\n", "pre_word_split", ".", "append", "(", "len", "(", "tokens", ")", ")", "\n", "", "else", ":", "\n", "                ", "pre_word_split", "=", "_get_word_split_index", "(", "tokens", ",", "0", ",", "len", "(", "tokens", ")", ")", "\n", "", "index2piece", "=", "None", "\n", "", "else", ":", "\n", "            ", "pre_word_split", "=", "list", "(", "range", "(", "0", ",", "len", "(", "tokens", ")", "+", "1", ")", ")", "\n", "\n", "if", "self", ".", "trie", "is", "not", "None", ":", "\n", "                ", "pieces", "=", "self", ".", "trie", ".", "get_pieces", "(", "tokens", ",", "0", ")", "\n", "\n", "index2piece", "=", "{", "}", "\n", "for", "piece", "in", "pieces", ":", "\n", "                    ", "for", "index", "in", "piece", ":", "\n", "                        ", "index2piece", "[", "index", "]", "=", "(", "piece", "[", "0", "]", ",", "piece", "[", "-", "1", "]", ")", "\n", "", "", "", "else", ":", "\n", "                ", "index2piece", "=", "None", "\n", "\n", "", "", "span_list", "=", "list", "(", "zip", "(", "pre_word_split", "[", ":", "-", "1", "]", ",", "pre_word_split", "[", "1", ":", "]", ")", ")", "\n", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "if", "mask_segment", ":", "\n", "            ", "for", "i", ",", "sp", "in", "enumerate", "(", "span_list", ")", ":", "\n", "                ", "sp_st", ",", "sp_end", "=", "sp", "\n", "if", "(", "sp_end", "-", "sp_st", "==", "1", ")", "and", "tokens", "[", "sp_st", "]", ".", "endswith", "(", "'SEP]'", ")", ":", "\n", "                    ", "segment_index", "=", "i", "\n", "break", "\n", "", "", "", "for", "i", ",", "sp", "in", "enumerate", "(", "span_list", ")", ":", "\n", "            ", "sp_st", ",", "sp_end", "=", "sp", "\n", "if", "(", "sp_end", "-", "sp_st", "==", "1", ")", "and", "(", "tokens", "[", "sp_st", "]", ".", "endswith", "(", "'CLS]'", ")", "or", "tokens", "[", "sp_st", "]", ".", "endswith", "(", "'SEP]'", ")", ")", ":", "\n", "                ", "special_pos", ".", "add", "(", "i", ")", "\n", "", "else", ":", "\n", "                ", "if", "mask_segment", ":", "\n", "                    ", "if", "(", "(", "i", "<", "segment_index", ")", "and", "(", "'a'", "in", "mask_segment", ")", ")", "or", "(", "(", "i", ">", "segment_index", ")", "and", "(", "'b'", "in", "mask_segment", ")", ")", ":", "\n", "                        ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "", "else", ":", "\n", "                    ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "for", "i_span", "in", "cand_pos", ":", "\n", "            ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                ", "break", "\n", "", "cand_st", ",", "cand_end", "=", "span_list", "[", "i_span", "]", "\n", "if", "len", "(", "masked_pos", ")", "+", "cand_end", "-", "cand_st", ">", "n_pred", ":", "\n", "                ", "continue", "\n", "", "if", "any", "(", "p", "in", "masked_pos", "for", "p", "in", "range", "(", "cand_st", ",", "cand_end", ")", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "n_span", "=", "1", "\n", "if", "index2piece", "is", "not", "None", ":", "\n", "                ", "p_start", ",", "p_end", "=", "index2piece", "[", "i_span", "]", "\n", "if", "p_start", "<", "p_end", "and", "(", "rand", "(", ")", "<", "self", ".", "sp_prob", ")", ":", "\n", "# n_span = p_end - p_start + 1", "\n", "                    ", "st_span", ",", "end_span", "=", "p_start", ",", "p_end", "+", "1", "\n", "", "else", ":", "\n", "                    ", "st_span", ",", "end_span", "=", "i_span", ",", "i_span", "+", "1", "\n", "", "", "else", ":", "\n", "                ", "rand_skipgram_size", "=", "0", "\n", "# ngram", "\n", "if", "self", ".", "skipgram_size_geo_list", ":", "\n", "# sampling ngram size from geometric distribution", "\n", "                    ", "rand_skipgram_size", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "self", ".", "skipgram_size_geo_list", ")", ",", "1", ",", "p", "=", "self", ".", "skipgram_size_geo_list", ")", "[", "0", "]", "+", "1", "\n", "", "else", ":", "\n", "                    ", "if", "add_skipgram", "and", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "                        ", "rand_skipgram_size", "=", "min", "(", "\n", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", ",", "len", "(", "span_list", ")", "-", "i_span", ")", "\n", "", "", "for", "n", "in", "range", "(", "2", ",", "rand_skipgram_size", "+", "1", ")", ":", "\n", "                    ", "tail_st", ",", "tail_end", "=", "span_list", "[", "i_span", "+", "n", "-", "1", "]", "\n", "if", "(", "tail_end", "-", "tail_st", "==", "1", ")", "and", "(", "tail_st", "in", "special_pos", ")", ":", "\n", "                        ", "break", "\n", "", "if", "len", "(", "masked_pos", ")", "+", "tail_end", "-", "cand_st", ">", "n_pred", ":", "\n", "                        ", "break", "\n", "", "n_span", "=", "n", "\n", "", "st_span", ",", "end_span", "=", "i_span", ",", "i_span", "+", "n_span", "\n", "\n", "", "if", "self", ".", "mask_whole_word", ":", "\n", "# pre_whole_word==False: position index of span_list is the same as tokens", "\n", "                ", "st_span", ",", "end_span", "=", "_expand_whole_word", "(", "\n", "tokens", ",", "st_span", ",", "end_span", ")", "\n", "\n", "# subsampling according to frequency", "\n", "", "if", "self", ".", "word_subsample_prb", ":", "\n", "                ", "skip_pos", "=", "set", "(", ")", "\n", "if", "self", ".", "pre_whole_word", ":", "\n", "                    ", "w_span_list", "=", "span_list", "[", "st_span", ":", "end_span", "]", "\n", "", "else", ":", "\n", "                    ", "split_idx", "=", "_get_word_split_index", "(", "\n", "tokens", ",", "st_span", ",", "end_span", ")", "\n", "w_span_list", "=", "list", "(", "\n", "zip", "(", "split_idx", "[", ":", "-", "1", "]", ",", "split_idx", "[", "1", ":", "]", ")", ")", "\n", "", "for", "i", ",", "sp", "in", "enumerate", "(", "w_span_list", ")", ":", "\n", "                    ", "sp_st", ",", "sp_end", "=", "sp", "\n", "if", "sp_end", "-", "sp_st", "==", "1", ":", "\n", "                        ", "w_cat", "=", "tokens", "[", "sp_st", "]", "\n", "", "else", ":", "\n", "                        ", "w_cat", "=", "''", ".", "join", "(", "tokens", "[", "sp_st", ":", "sp_end", "]", ")", "\n", "", "if", "(", "w_cat", "in", "self", ".", "word_subsample_prb", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "word_subsample_prb", "[", "w_cat", "]", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "sp_st", ",", "sp_end", ")", ":", "\n", "                            ", "skip_pos", ".", "add", "(", "k", ")", "\n", "", "", "", "", "else", ":", "\n", "                ", "skip_pos", "=", "None", "\n", "\n", "", "for", "sp", "in", "range", "(", "st_span", ",", "end_span", ")", ":", "\n", "                ", "for", "mp", "in", "range", "(", "span_list", "[", "sp", "]", "[", "0", "]", ",", "span_list", "[", "sp", "]", "[", "1", "]", ")", ":", "\n", "                    ", "if", "not", "(", "skip_pos", "and", "(", "mp", "in", "skip_pos", ")", ")", "and", "(", "mp", "not", "in", "special_pos", ")", "and", "not", "(", "protect_range", "and", "(", "protect_range", "[", "0", "]", "<=", "mp", "<", "protect_range", "[", "1", "]", ")", ")", ":", "\n", "                        ", "masked_pos", ".", "add", "(", "mp", ")", "\n", "\n", "", "", "", "", "if", "len", "(", "masked_pos", ")", "<", "n_pred", ":", "\n", "            ", "shuffle", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "                ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                    ", "break", "\n", "", "if", "pos", "not", "in", "masked_pos", ":", "\n", "                    ", "masked_pos", ".", "add", "(", "pos", ")", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "# shuffle(masked_pos)", "\n", "            ", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "", "return", "masked_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.Pipeline.replace_masked_tokens": [[286, 300], ["sorted", "list", "random.random.random", "loader_utils.get_random_word"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.get_random_word"], ["", "def", "replace_masked_tokens", "(", "self", ",", "tokens", ",", "masked_pos", ")", ":", "\n", "        ", "if", "self", ".", "span_same_mask", ":", "\n", "            ", "masked_pos", "=", "sorted", "(", "list", "(", "masked_pos", ")", ")", "\n", "", "prev_pos", ",", "prev_rand", "=", "None", ",", "None", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "            ", "if", "self", ".", "span_same_mask", "and", "(", "pos", "-", "1", "==", "prev_pos", ")", ":", "\n", "                ", "t_rand", "=", "prev_rand", "\n", "", "else", ":", "\n", "                ", "t_rand", "=", "rand", "(", ")", "\n", "", "if", "t_rand", "<", "0.8", ":", "# 80%", "\n", "                ", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "t_rand", "<", "0.9", ":", "# 10%", "\n", "                ", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "", "prev_pos", ",", "prev_rand", "=", "pos", ",", "t_rand", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.get_random_word": [[9, 12], ["random.randint", "len"], "function", ["None"], ["def", "get_random_word", "(", "vocab_words", ")", ":", "\n", "    ", "i", "=", "randint", "(", "0", ",", "len", "(", "vocab_words", ")", "-", "1", ")", "\n", "return", "vocab_words", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.batch_list_to_batch_tensors": [[14, 24], ["zip", "batch_tensors.append", "isinstance", "batch_tensors.append", "batch_tensors.append", "torch.stack", "torch.stack", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "def", "batch_list_to_batch_tensors", "(", "batch", ")", ":", "\n", "    ", "batch_tensors", "=", "[", "]", "\n", "for", "x", "in", "zip", "(", "*", "batch", ")", ":", "\n", "        ", "if", "x", "[", "0", "]", "is", "None", ":", "\n", "            ", "batch_tensors", ".", "append", "(", "None", ")", "\n", "", "elif", "isinstance", "(", "x", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "batch_tensors", ".", "append", "(", "torch", ".", "stack", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "            ", "batch_tensors", ".", "append", "(", "torch", ".", "tensor", "(", "x", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "", "return", "batch_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils._get_word_split_index": [[71, 80], ["split_idx.append", "split_idx.append", "tokens[].startswith"], "function", ["None"], ["", "", "def", "_get_word_split_index", "(", "tokens", ",", "st", ",", "end", ")", ":", "\n", "    ", "split_idx", "=", "[", "]", "\n", "i", "=", "st", "\n", "while", "i", "<", "end", ":", "\n", "        ", "if", "(", "not", "tokens", "[", "i", "]", ".", "startswith", "(", "'##'", ")", ")", "or", "(", "i", "==", "st", ")", ":", "\n", "            ", "split_idx", ".", "append", "(", "i", ")", "\n", "", "i", "+=", "1", "\n", "", "split_idx", ".", "append", "(", "end", ")", "\n", "return", "split_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils._expand_whole_word": [[82, 89], ["tokens[].startswith", "tokens[].startswith", "len"], "function", ["None"], ["", "def", "_expand_whole_word", "(", "tokens", ",", "st", ",", "end", ")", ":", "\n", "    ", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "        ", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "        ", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Seq2SeqDataset.__init__": [[57, 88], ["super().__init__", "print", "open", "open", "zip", "open", "open", "open", "zip", "len", "tokenizer.tokenize", "tokenizer.tokenize", "seq2seq_loader.Seq2SeqDataset.ex_list.append", "tokenizer.tokenize", "tokenizer.tokenize", "orc.split", "seq2seq_loader.Seq2SeqDataset.ex_list.append", "src.strip", "tgt.strip", "len", "len", "src.strip", "tgt.strip", "int", "int", "s_st.split", "labl.split"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.ictnlp_AIH.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize"], ["def", "__init__", "(", "self", ",", "file_src", ",", "file_tgt", ",", "batch_size", ",", "tokenizer", ",", "max_len", ",", "file_oracle", "=", "None", ",", "short_sampling_prob", "=", "0.1", ",", "sent_reverse_order", "=", "False", ",", "bi_uni_pipeline", "=", "[", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "# tokenize function", "\n", "self", ".", "max_len", "=", "max_len", "# maximum length of tokens", "\n", "self", ".", "short_sampling_prob", "=", "short_sampling_prob", "\n", "self", ".", "bi_uni_pipeline", "=", "bi_uni_pipeline", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "sent_reverse_order", "=", "sent_reverse_order", "\n", "\n", "# read the file into memory", "\n", "self", ".", "ex_list", "=", "[", "]", "\n", "if", "file_oracle", "is", "None", ":", "\n", "            ", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ":", "\n", "                ", "for", "src", ",", "tgt", "in", "zip", "(", "f_src", ",", "f_tgt", ")", ":", "\n", "                    ", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ")", ")", "\n", "", "", "", "else", ":", "\n", "            ", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ",", "open", "(", "file_oracle", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_orc", ":", "\n", "                ", "for", "src", ",", "tgt", ",", "orc", "in", "zip", "(", "f_src", ",", "f_tgt", ",", "f_orc", ")", ":", "\n", "                    ", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "s_st", ",", "labl", "=", "orc", ".", "split", "(", "'\\t'", ")", "\n", "s_st", "=", "[", "int", "(", "x", ")", "for", "x", "in", "s_st", ".", "split", "(", ")", "]", "\n", "labl", "=", "[", "int", "(", "x", ")", "for", "x", "in", "labl", ".", "split", "(", ")", "]", "\n", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ",", "s_st", ",", "labl", ")", ")", "\n", "", "", "", "print", "(", "'Load {0} documents'", ".", "format", "(", "len", "(", "self", ".", "ex_list", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Seq2SeqDataset.__len__": [[89, 91], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ex_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Seq2SeqDataset.__getitem__": [[92, 97], ["random.random.choice", "random.random.choice."], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "instance", "=", "self", ".", "ex_list", "[", "idx", "]", "\n", "proc", "=", "choice", "(", "self", ".", "bi_uni_pipeline", ")", "\n", "instance", "=", "proc", "(", "instance", ")", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Seq2SeqDataset.__iter__": [[98, 106], ["range", "math.ceil", "range", "random.random.randint", "batch.append", "loader_utils.batch_list_to_batch_tensors", "len", "float", "seq2seq_loader.Seq2SeqDataset.__getitem__", "len"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Seq2SeqDataset.__getitem__"], ["", "def", "__iter__", "(", "self", ")", ":", "# iterator to load data", "\n", "        ", "for", "__", "in", "range", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "ex_list", ")", "/", "float", "(", "self", ".", "batch_size", ")", ")", ")", ":", "\n", "            ", "batch", "=", "[", "]", "\n", "for", "__", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "                ", "idx", "=", "randint", "(", "0", ",", "len", "(", "self", ".", "ex_list", ")", "-", "1", ")", "\n", "batch", ".", "append", "(", "self", ".", "__getitem__", "(", "idx", ")", ")", "\n", "# To Tensor", "\n", "", "yield", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seq.__init__": [[111, 140], ["loader_utils.Pipeline.__init__", "torch.tril", "truncate_config.get", "truncate_config.get", "truncate_config.get", "truncate_config.get", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "max_pred", ",", "mask_prob", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "skipgram_prb", "=", "0", ",", "skipgram_size", "=", "0", ",", "block_mask", "=", "False", ",", "mask_whole_word", "=", "False", ",", "new_segment_ids", "=", "False", ",", "truncate_config", "=", "{", "}", ",", "mask_source_words", "=", "False", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "False", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "max_pred", "=", "max_pred", "# max tokens of prediction", "\n", "self", ".", "mask_prob", "=", "mask_prob", "# masking probability", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "skipgram_prb", "=", "skipgram_prb", "\n", "self", ".", "skipgram_size", "=", "skipgram_size", "\n", "self", ".", "mask_whole_word", "=", "mask_whole_word", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "always_truncate_tail", "=", "truncate_config", ".", "get", "(", "\n", "'always_truncate_tail'", ",", "False", ")", "\n", "self", ".", "max_len_a", "=", "truncate_config", ".", "get", "(", "'max_len_a'", ",", "None", ")", "\n", "self", ".", "max_len_b", "=", "truncate_config", ".", "get", "(", "'max_len_b'", ",", "None", ")", "\n", "self", ".", "trunc_seg", "=", "truncate_config", ".", "get", "(", "'trunc_seg'", ",", "None", ")", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "self", ".", "mask_source_words", "=", "mask_source_words", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "has_oracle", "=", "has_oracle", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seq.__call__": [[141, 316], ["seq2seq_loader.truncate_tokens_pair", "seq2seq_loader.Preprocess4Seq2seq.indexer", "seq2seq_loader.Preprocess4Seq2seq.extend", "segment_ids.extend", "torch.zeros", "min", "seq2seq_loader.Preprocess4Seq2seq.indexer", "len", "min", "set", "enumerate", "random.random.shuffle", "set", "max", "list", "seq2seq_loader.Preprocess4Seq2seq.indexer", "len", "mask_qkv.extend", "input_mask[].fill_", "input_mask[].copy_", "input_mask[].copy_", "zip", "len", "len", "max", "range", "len", "random.random.shuffle", "len", "seq2seq_loader.Preprocess4Seq2seq.extend", "list.extend", "masked_weights.extend", "len", "len", "oracle_pos.extend", "oracle_labels.extend", "oracle_weights.extend", "len", "range", "int", "cand_pos.append", "len", "random.random.randint", "random.random.random", "len", "oracle_pos.append", "oracle_labels.append", "len", "len", "len", "len", "len", "round", "cand_pos.append", "set.add", "tokens[].startswith", "tokens[].startswith", "random.random.random", "seq2seq_loader.Preprocess4Seq2seq.__call__._expand_whole_word"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.truncate_tokens_pair", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.loader_utils._expand_whole_word"], ["", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "        ", "tokens_a", ",", "tokens_b", "=", "instance", "[", ":", "2", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "tokens_b", "=", "[", "'[S2S_SOS]'", "]", "+", "tokens_b", "\n", "\n", "# -3  for special tokens [CLS], [SEP], [SEP]", "\n", "", "num_truncated_a", ",", "_", "=", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "self", ".", "max_len", "-", "3", ",", "max_len_a", "=", "self", ".", "max_len_a", ",", "\n", "max_len_b", "=", "self", ".", "max_len_b", ",", "trunc_seg", "=", "self", ".", "trunc_seg", ",", "always_truncate_tail", "=", "self", ".", "always_truncate_tail", ")", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "            ", "tokens", "=", "[", "'[S2S_CLS]'", "]", "+", "tokens_a", "+", "[", "'[S2S_SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "\n", "", "if", "self", ".", "new_segment_ids", ":", "\n", "            ", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "                ", "if", "self", ".", "s2s_add_segment", ":", "\n", "                    ", "if", "self", ".", "s2s_share_segment", ":", "\n", "                        ", "segment_ids", "=", "[", "0", "]", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "                        ", "segment_ids", "=", "[", "4", "]", "+", "[", "6", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "                    ", "segment_ids", "=", "[", "4", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "segment_ids", "=", "[", "2", "]", "*", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "if", "self", ".", "pos_shift", ":", "\n", "            ", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "len", "(", "tokens_b", ")", ")", "\n", "masked_pos", "=", "[", "len", "(", "tokens_a", ")", "+", "2", "+", "i", "for", "i", "in", "range", "(", "len", "(", "tokens_b", ")", ")", "]", "\n", "masked_weights", "=", "[", "1", "]", "*", "n_pred", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "tokens_b", "[", "1", ":", "]", "+", "[", "'[SEP]'", "]", ")", "\n", "", "else", ":", "\n", "# For masked Language Models", "\n", "# the number of prediction is sometimes less than max_pred when sequence is short", "\n", "            ", "effective_length", "=", "len", "(", "tokens_b", ")", "\n", "if", "self", ".", "mask_source_words", ":", "\n", "                ", "effective_length", "+=", "len", "(", "tokens_a", ")", "\n", "", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "max", "(", "\n", "1", ",", "int", "(", "round", "(", "effective_length", "*", "self", ".", "mask_prob", ")", ")", ")", ")", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# only mask tokens_b (target sequence)", "\n", "# we will mask [SEP] as an ending symbol", "\n", "                ", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", ":", "\n", "                    ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "elif", "self", ".", "mask_source_words", "and", "(", "i", "<", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", "and", "(", "not", "tk", ".", "startswith", "(", "'[SEP'", ")", ")", ":", "\n", "                    ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                    ", "special_pos", ".", "add", "(", "i", ")", "\n", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "max_cand_pos", "=", "max", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "                ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                    ", "break", "\n", "", "if", "pos", "in", "masked_pos", ":", "\n", "                    ", "continue", "\n", "\n", "", "def", "_expand_whole_word", "(", "st", ",", "end", ")", ":", "\n", "                    ", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "                        ", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "                        ", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n", "", "if", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "# ngram", "\n", "                    ", "cur_skipgram_size", "=", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", "\n", "if", "self", ".", "mask_whole_word", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "\n", "pos", ",", "pos", "+", "cur_skipgram_size", ")", "\n", "", "else", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "cur_skipgram_size", "\n", "", "", "else", ":", "\n", "# directly mask", "\n", "                    ", "if", "self", ".", "mask_whole_word", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "pos", ",", "pos", "+", "1", ")", "\n", "", "else", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "1", "\n", "\n", "", "", "for", "mp", "in", "range", "(", "st_pos", ",", "end_pos", ")", ":", "\n", "                    ", "if", "(", "0", "<", "mp", "<=", "max_cand_pos", ")", "and", "(", "mp", "not", "in", "special_pos", ")", ":", "\n", "                        ", "masked_pos", ".", "add", "(", "mp", ")", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "                ", "shuffle", "(", "masked_pos", ")", "\n", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "\n", "", "masked_tokens", "=", "[", "tokens", "[", "pos", "]", "for", "pos", "in", "masked_pos", "]", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "                ", "if", "rand", "(", ")", "<", "0.8", ":", "# 80%", "\n", "                    ", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "rand", "(", ")", "<", "0.5", ":", "# 10%", "\n", "                    ", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "# when n_pred < max_pred, we only calculate loss within n_pred", "\n", "", "", "masked_weights", "=", "[", "1", "]", "*", "len", "(", "masked_tokens", ")", "\n", "\n", "# Token Indexing", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "masked_tokens", ")", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "n_pad", "=", "self", ".", "max_len", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "mask_qkv", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "else", ":", "\n", "            ", "mask_qkv", "=", "None", "\n", "\n", "", "input_mask", "=", "torch", ".", "zeros", "(", "self", ".", "max_len", ",", "self", ".", "max_len", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "            ", "input_mask", "[", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "second_st", ",", "second_end", "=", "len", "(", "\n", "tokens_a", ")", "+", "2", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "second_st", ":", "second_end", ",", "second_st", ":", "second_end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "second_end", "-", "second_st", ",", ":", "second_end", "-", "second_st", "]", ")", "\n", "", "else", ":", "\n", "            ", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "\n", "# Zero Padding for masked target", "\n", "", "if", "self", ".", "max_pred", ">", "n_pred", ":", "\n", "            ", "n_pad", "=", "self", ".", "max_pred", "-", "n_pred", "\n", "if", "masked_ids", "is", "not", "None", ":", "\n", "                ", "masked_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_pos", "is", "not", "None", ":", "\n", "                ", "masked_pos", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_weights", "is", "not", "None", ":", "\n", "                ", "masked_weights", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "", "", "oracle_pos", "=", "None", "\n", "oracle_weights", "=", "None", "\n", "oracle_labels", "=", "None", "\n", "if", "self", ".", "has_oracle", ":", "\n", "            ", "s_st", ",", "labls", "=", "instance", "[", "2", ":", "]", "\n", "oracle_pos", "=", "[", "]", "\n", "oracle_labels", "=", "[", "]", "\n", "for", "st", ",", "lb", "in", "zip", "(", "s_st", ",", "labls", ")", ":", "\n", "                ", "st", "=", "st", "-", "num_truncated_a", "[", "0", "]", "\n", "if", "st", ">", "0", "and", "st", "<", "len", "(", "tokens_a", ")", ":", "\n", "                    ", "oracle_pos", ".", "append", "(", "st", ")", "\n", "oracle_labels", ".", "append", "(", "lb", ")", "\n", "", "", "oracle_pos", "=", "oracle_pos", "[", ":", "20", "]", "\n", "oracle_labels", "=", "oracle_labels", "[", ":", "20", "]", "\n", "oracle_weights", "=", "[", "1", "]", "*", "len", "(", "oracle_pos", ")", "\n", "if", "len", "(", "oracle_pos", ")", "<", "20", ":", "\n", "                ", "x_pad", "=", "20", "-", "len", "(", "oracle_pos", ")", "\n", "oracle_pos", ".", "extend", "(", "[", "0", "]", "*", "x_pad", ")", "\n", "oracle_labels", ".", "extend", "(", "[", "0", "]", "*", "x_pad", ")", "\n", "oracle_weights", ".", "extend", "(", "[", "0", "]", "*", "x_pad", ")", "\n", "\n", "", "return", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "masked_ids", ",", "\n", "masked_pos", ",", "masked_weights", ",", "-", "1", ",", "self", ".", "task_idx", ",", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", ")", "\n", "\n", "", "return", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "masked_ids", ",", "masked_pos", ",", "masked_weights", ",", "-", "1", ",", "self", ".", "task_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__": [[321, 339], ["loader_utils.Pipeline.__init__", "torch.tril", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__"], ["def", "__init__", "(", "self", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "max_tgt_length", "=", "128", ",", "new_segment_ids", "=", "False", ",", "mode", "=", "\"s2s\"", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "max_tgt_length", "=", "max_tgt_length", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__call__": [[340, 408], ["min", "range", "range", "range", "seq2seq_loader.Preprocess4Seq2seqDecoder.indexer", "torch.zeros", "input_mask[].copy_", "len", "len", "len", "position_ids.append", "position_ids.append", "position_ids.append", "input_mask[].fill_", "input_mask[].copy_", "input_mask[].fill_", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "        ", "tokens_a", ",", "max_a_len", "=", "instance", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "            ", "padded_tokens_a", "=", "[", "'[S2S_CLS]'", "]", "+", "tokens_a", "+", "[", "'[S2S_SEP]'", "]", "\n", "", "else", ":", "\n", "            ", "padded_tokens_a", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "\n", "", "assert", "len", "(", "padded_tokens_a", ")", "<=", "max_a_len", "+", "2", "\n", "if", "max_a_len", "+", "2", ">", "len", "(", "padded_tokens_a", ")", ":", "\n", "            ", "padded_tokens_a", "+=", "[", "'[PAD]'", "]", "*", "(", "max_a_len", "+", "2", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "assert", "len", "(", "padded_tokens_a", ")", "==", "max_a_len", "+", "2", "\n", "max_len_in_batch", "=", "min", "(", "self", ".", "max_tgt_length", "+", "\n", "max_a_len", "+", "2", ",", "self", ".", "max_len", ")", "\n", "tokens", "=", "padded_tokens_a", "\n", "if", "self", ".", "new_segment_ids", ":", "\n", "            ", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "                ", "_enc_seg1", "=", "0", "if", "self", ".", "s2s_share_segment", "else", "4", "\n", "if", "self", ".", "s2s_add_segment", ":", "\n", "                    ", "if", "self", ".", "s2s_share_segment", ":", "\n", "                        ", "segment_ids", "=", "[", "\n", "0", "]", "+", "[", "1", "]", "*", "(", "len", "(", "padded_tokens_a", ")", "-", "1", ")", "+", "[", "5", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "else", ":", "\n", "                        ", "segment_ids", "=", "[", "\n", "4", "]", "+", "[", "6", "]", "*", "(", "len", "(", "padded_tokens_a", ")", "-", "1", ")", "+", "[", "5", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "segment_ids", "=", "[", "4", "]", "*", "(", "len", "(", "padded_tokens_a", ")", ")", "+", "[", "5", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "segment_ids", "=", "[", "2", "]", "*", "max_len_in_batch", "\n", "", "", "else", ":", "\n", "            ", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "padded_tokens_a", ")", ")", "+", "[", "1", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "\n", "", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "padded_tokens_a", ")", ")", "+", "[", "1", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "else", ":", "\n", "            ", "mask_qkv", "=", "None", "\n", "\n", "", "position_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "tokens_a", ")", "+", "2", ")", ":", "\n", "            ", "position_ids", ".", "append", "(", "i", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "tokens_a", ")", "+", "2", ",", "max_a_len", "+", "2", ")", ":", "\n", "            ", "position_ids", ".", "append", "(", "0", ")", "\n", "", "for", "i", "in", "range", "(", "max_a_len", "+", "2", ",", "max_len_in_batch", ")", ":", "\n", "            ", "position_ids", ".", "append", "(", "i", "-", "(", "max_a_len", "+", "2", ")", "+", "len", "(", "tokens_a", ")", "+", "2", ")", "\n", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "input_mask", "=", "torch", ".", "zeros", "(", "\n", "max_len_in_batch", ",", "max_len_in_batch", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "            ", "input_mask", "[", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "2", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "input_mask", "[", "end", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "", "second_st", ",", "second_end", "=", "len", "(", "padded_tokens_a", ")", ",", "max_len_in_batch", "\n", "\n", "input_mask", "[", "second_st", ":", "second_end", ",", "second_st", ":", "second_end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "second_end", "-", "second_st", ",", ":", "second_end", "-", "second_st", "]", ")", "\n", "\n", "return", "(", "input_ids", ",", "segment_ids", ",", "position_ids", ",", "input_mask", ",", "mask_qkv", ",", "self", ".", "task_idx", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ictnlp_AIH.biunilm.seq2seq_loader.truncate_tokens_pair": [[16, 52], ["trunc_tokens.pop", "len", "len", "len", "random.random", "len", "len", "len"], "function", ["None"], ["def", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_len", ",", "max_len_a", "=", "0", ",", "max_len_b", "=", "0", ",", "trunc_seg", "=", "None", ",", "always_truncate_tail", "=", "False", ")", ":", "\n", "    ", "num_truncated_a", "=", "[", "0", ",", "0", "]", "\n", "num_truncated_b", "=", "[", "0", ",", "0", "]", "\n", "while", "True", ":", "\n", "        ", "if", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "<=", "max_len", ":", "\n", "            ", "break", "\n", "", "if", "(", "max_len_a", ">", "0", ")", "and", "len", "(", "tokens_a", ")", ">", "max_len_a", ":", "\n", "            ", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "elif", "(", "max_len_b", ">", "0", ")", "and", "len", "(", "tokens_b", ")", ">", "max_len_b", ":", "\n", "            ", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "elif", "trunc_seg", ":", "\n", "# truncate the specified segment", "\n", "            ", "if", "trunc_seg", "==", "'a'", ":", "\n", "                ", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "                ", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "", "else", ":", "\n", "# truncate the longer segment", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "                ", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "                ", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "# whether always truncate source sequences", "\n", "", "", "if", "(", "not", "always_truncate_tail", ")", "and", "(", "rand", "(", ")", "<", "0.5", ")", ":", "\n", "            ", "del", "trunc_tokens", "[", "0", "]", "\n", "num_truncated", "[", "0", "]", "+=", "1", "\n", "", "else", ":", "\n", "            ", "trunc_tokens", ".", "pop", "(", ")", "\n", "num_truncated", "[", "1", "]", "+=", "1", "\n", "", "", "return", "num_truncated_a", ",", "num_truncated_b", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.NLI.deploy_NLI.score": [[15, 20], ["torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.cat().unsqueeze", "sm", "torch.LongTensor", "torch.LongTensor", "torch.cat", "model", "tokenizer", "tokenizer"], "function", ["None"], ["def", "score", "(", "c1", ",", "c2", ")", ":", "\n", "    ", "c1_t", "=", "torch", ".", "LongTensor", "(", "tokenizer", "(", "c1", ",", "padding", "=", "\"longest\"", ")", "[", "\"input_ids\"", "]", ")", ".", "cuda", "(", ")", "\n", "c2_t", "=", "torch", ".", "LongTensor", "(", "tokenizer", "(", "c2", ",", "padding", "=", "\"longest\"", ")", "[", "\"input_ids\"", "]", ")", ".", "cuda", "(", ")", "\n", "text", "=", "torch", ".", "cat", "(", "[", "c2_t", ",", "c1_t", "]", ",", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "0", ")", "\n", "return", "sm", "(", "model", "(", "text", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ictnlp_AIH.NLI.deploy_NLI.nli": [[21, 30], ["app.route", "flask.request.get_data", "json.loads", "flask.jsonify", "score().cpu().tolist", "score().cpu", "deploy_NLI.score"], "function", ["home.repos.pwc.inspect_result.ictnlp_AIH.NLI.deploy_NLI.score"], ["", "@", "app", ".", "route", "(", "\"/NLI\"", ",", "methods", "=", "[", "\"POST\"", "]", ")", "\n", "def", "nli", "(", ")", ":", "\n", "    ", "body", "=", "request", ".", "get_data", "(", ")", "\n", "body", "=", "json", ".", "loads", "(", "body", ")", "\n", "res", "=", "body", "[", "\"res\"", "]", "\n", "res_gold", "=", "body", "[", "\"res_gold\"", "]", "\n", "results", "=", "score", "(", "res", ",", "res_gold", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "\n", "return", "jsonify", "(", "{", "\"nli_score\"", ":", "results", "}", ")", "\n", "\n"]]}