{"home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.run_experiments.work": [[24, 47], ["range", "print", "os.system", "print", "aug.split", "os.system", "os.system", "set().issubset", "print", "map", "set", "set", "len"], "function", ["None"], ["", "def", "work", "(", ")", ":", "\n", "    ", "for", "rand", "in", "range", "(", "20200303", ",", "20200304", ")", ":", "\n", "# for rand in range(20200303,20200333):", "\n", "        ", "for", "clf", "in", "classifiers", ":", "\n", "            ", "for", "aug", "in", "augmentations", ":", "\n", "                ", "print", "(", "rand", ")", "\n", "syscall", "=", "'python run_experiment.py -d threat -a %s -c %s -r %d --remove_cache'", "%", "(", "aug", ",", "clf", ",", "rand", ")", "\n", "os", ".", "system", "(", "syscall", ")", "\n", "\n", "", "for", "aug", "in", "[", "'add_bpemb'", ",", "'add_bpemb_gpt2'", "]", ":", "\n", "                ", "print", "(", "rand", ")", "\n", "techniques", "=", "aug", ".", "split", "(", "'_'", ")", "\n", "techniques_str", "=", "' '", ".", "join", "(", "techniques", ")", "\n", "if", "not", "set", "(", "techniques", ")", ".", "issubset", "(", "set", "(", "augmentations", ")", ")", ":", "\n", "                    ", "print", "(", "'Please run all of %s before combining augmentations.'", "%", "(", "techniques_str", ")", ")", "\n", "continue", "\n", "\n", "", "ratios", "=", "' '", ".", "join", "(", "map", "(", "str", ",", "[", "1.", "/", "len", "(", "techniques", ")", "for", "i", "in", "techniques", "]", ")", ")", "\n", "syscall", "=", "'python combine_augmentations.py -d threat -p 5 -t %s -r %d -n %s'", "%", "(", "techniques_str", ",", "rand", ",", "ratios", ")", "\n", "os", ".", "system", "(", "syscall", ")", "\n", "\n", "syscall", "=", "'python run_experiment.py -d threat -a %s -c %s -r %d --remove_cache'", "%", "(", "aug", ",", "clf", ",", "rand", ")", "\n", "os", ".", "system", "(", "syscall", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.combine_augmentations.combine_augmentations": [[10, 52], ["print", "pandas.DataFrame", "os.makedirs", "numpy.random.seed", "open", "open", "f.readlines", "pandas.Series", "pandas.DataFrame", "len", "numpy.random.choice", "f.write", "df.join.join", "numpy.arange", "numpy.arange", "len", "len", "len", "len"], "function", ["None"], ["def", "combine_augmentations", "(", "techniques", "=", "[", "'add'", ",", "'bpemb'", ",", "'gpt2'", "]", ",", "\n", "probability", "=", "[", "0.33", ",", "0.33", ",", "0.34", "]", ",", "\n", "dataset_name", "=", "'threat'", ",", "\n", "random_seed", "=", "20200303", ",", "\n", "percentage", "=", "5", ")", ":", "\n", "\n", "\n", "    ", "new_augmentation_name", "=", "'_'", ".", "join", "(", "techniques", ")", "\n", "print", "(", "new_augmentation_name", ",", "random_seed", ")", "\n", "\n", "df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "prev_aug_files", "=", "[", "]", "\n", "\n", "for", "aug_name", "in", "techniques", ":", "\n", "\n", "        ", "aug_dir", "=", "'../data/augmentations/%s-%03d/%s-1'", "%", "(", "dataset_name", ",", "\n", "percentage", ",", "\n", "aug_name", ")", "\n", "\n", "prev_aug_files", "+=", "[", "]", "\n", "\n", "with", "open", "(", "'%s/%s.txt'", "%", "(", "aug_dir", ",", "random_seed", ")", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "lines", "=", "pd", ".", "Series", "(", "lines", ")", "\n", "df2", "=", "pd", ".", "DataFrame", "(", "data", "=", "lines", ",", "index", "=", "np", ".", "arange", "(", "len", "(", "lines", ")", ")", ",", "columns", "=", "[", "aug_name", "]", ")", "\n", "if", "len", "(", "df", ")", ":", "\n", "                ", "assert", "len", "(", "df", ")", "==", "len", "(", "df2", ")", "\n", "df", "=", "df", ".", "join", "(", "df2", ")", "\n", "", "else", ":", "\n", "                ", "df", "=", "df2", "\n", "\n", "", "", "", "new_aug_dir", "=", "'../data/augmentations/%s-%03d/%s-1'", "%", "(", "dataset_name", ",", "\n", "percentage", ",", "\n", "new_augmentation_name", ")", "\n", "\n", "os", ".", "makedirs", "(", "new_aug_dir", ",", "exist_ok", "=", "True", ")", "\n", "np", ".", "random", ".", "seed", "(", "random_seed", ")", "\n", "\n", "with", "open", "(", "'%s/%s.txt'", "%", "(", "new_aug_dir", ",", "random_seed", ")", ",", "'w'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "        ", "for", "row", "in", "df", ".", "values", ":", "\n", "            ", "c", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len", "(", "techniques", ")", ")", ")", "\n", "f", ".", "write", "(", "'%s'", "%", "(", "row", "[", "c", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.augmentation_helpers.augmentation_helper": [[12, 31], ["collections.deque", "open", "enumerate", "open", "tqdm.tqdm", "open", "fo.write", "line.strip().split", "int", "aug_function", "collections.deque.append", "list", "line.strip"], "function", ["None"], ["def", "augmentation_helper", "(", "input", ",", "output", ",", "aug_function", ",", "num_aug", "=", "20", ",", "target_classes", "=", "[", "0", ",", "1", "]", ")", ":", "\n", "    ", "with", "open", "(", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "total", ",", "_", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "pass", "\n", "\n", "# write files temporarily into a deque", "\n", "", "", "dq", "=", "deque", "(", ")", "\n", "with", "open", "(", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "tqdm", ".", "tqdm", "(", "f", ",", "total", "=", "total", ")", ":", "\n", "            ", "id", ",", "sent", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "int", "(", "id", ")", "in", "target_classes", ":", "\n", "                ", "aug_sents", "=", "aug_function", "(", "sent", ",", "num_aug", "=", "num_aug", ")", "\n", "", "else", ":", "\n", "                ", "aug_sents", "=", "[", "sent", "]", "\n", "", "for", "sent", "in", "aug_sents", ":", "\n", "                ", "dq", ".", "append", "(", "'%s\\t%s\\n'", "%", "(", "id", ",", "sent", ")", ")", "\n", "\n", "", "", "", "with", "open", "(", "output", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fo", ":", "\n", "        ", "fo", ".", "write", "(", "''", ".", "join", "(", "list", "(", "dq", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.augmentation_helpers.add_sentences": [[34, 68], ["nltk.sent_tokenize", "np.random.RandomState.randint", "len", "range", "numpy.random.RandomState", "list", "range", "doc_aug.append", "np.random.RandomState.randint", "np.random.RandomState.randint", "list.insert", "len"], "function", ["None"], ["", "", "def", "add_sentences", "(", "document", ",", "\n", "add_sentence_corpus", ",", "\n", "num_aug", "=", "20", ",", "\n", "max_replacements", "=", "1", ",", "\n", "incl_orig", "=", "True", ",", "\n", "rng", "=", "None", ")", ":", "\n", "\n", "    ", "if", "not", "rng", ":", "\n", "        ", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "20200303", ")", "\n", "\n", "", "sents", "=", "sent_tokenize", "(", "document", ")", "\n", "\n", "num_add", "=", "rng", ".", "randint", "(", "1", ",", "max_replacements", "+", "1", ")", "\n", "add_range", "=", "len", "(", "add_sentence_corpus", ")", "\n", "\n", "doc_aug", "=", "[", "]", "\n", "if", "incl_orig", ":", "\n", "        ", "doc_aug", "=", "[", "document", "]", "\n", "num_aug", "-=", "1", "\n", "\n", "", "for", "aug", "in", "range", "(", "num_aug", ")", ":", "\n", "\n", "        ", "sents_add", "=", "list", "(", "sents", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_add", ")", ":", "\n", "            ", "add_position", "=", "rng", ".", "randint", "(", "len", "(", "sents_add", ")", "+", "1", ")", "\n", "add_sent_ix", "=", "rng", ".", "randint", "(", "add_range", ")", "\n", "add_sent", "=", "add_sentence_corpus", "[", "add_sent_ix", "]", "\n", "#            add_sent = rng.choice(add_sentence_corpus)", "\n", "sents_add", ".", "insert", "(", "add_position", ",", "add_sent", ")", "\n", "\n", "", "doc_aug", ".", "append", "(", "' '", ".", "join", "(", "sents_add", ")", ")", "\n", "\n", "", "return", "doc_aug", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.augmentation_helpers.augmentation_helper_add": [[70, 109], ["numpy.random.RandomState", "collections.deque", "open", "enumerate", "open", "open", "tqdm.tqdm", "open", "fo.write", "line.strip().split", "line.strip().split", "doc.strip", "nltk.sent_tokenize", "int", "augmentation_helpers.add_sentences", "collections.deque.append", "list", "line.strip", "int", "line.strip"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.augmentation_helpers.add_sentences"], ["", "def", "augmentation_helper_add", "(", "input", ",", "\n", "output", ",", "\n", "num_aug", "=", "20", ",", "\n", "add_from_classes", "=", "[", "0", "]", ",", "\n", "target_classes", "=", "[", "0", ",", "1", "]", ",", "\n", "random_state", "=", "20200303", ")", ":", "\n", "\n", "    ", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "random_state", ")", "\n", "\n", "with", "open", "(", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "total", ",", "_", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "pass", "\n", "\n", "", "", "dq", "=", "deque", "(", ")", "\n", "\n", "with", "open", "(", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "\n", "# Create corpus to add sentences from", "\n", "        ", "add_sentence_corpus", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "            ", "label", ",", "doc", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "int", "(", "label", ")", "in", "add_from_classes", "and", "doc", ".", "strip", "(", ")", ":", "\n", "                ", "add_sentence_corpus", "+=", "sent_tokenize", "(", "doc", ")", "\n", "\n", "", "", "", "with", "open", "(", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "\n", "        ", "for", "line", "in", "tqdm", ".", "tqdm", "(", "f", ",", "total", "=", "total", ")", ":", "\n", "            ", "label", ",", "doc", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "int", "(", "label", ")", "in", "target_classes", ":", "\n", "                ", "aug_sents", "=", "add_sentences", "(", "document", "=", "doc", ",", "add_sentence_corpus", "=", "add_sentence_corpus", ",", "num_aug", "=", "num_aug", ",", "rng", "=", "rng", ")", "\n", "", "else", ":", "\n", "                ", "aug_sents", "=", "[", "doc", "]", "\n", "\n", "", "for", "sent", "in", "aug_sents", ":", "\n", "                ", "dq", ".", "append", "(", "'%s\\t%s\\n'", "%", "(", "label", ",", "sent", ")", ")", "\n", "#fo.write('%s\\t%s\\n'%(id,sent))", "\n", "\n", "", "", "", "with", "open", "(", "output", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fo", ":", "\n", "        ", "fo", ".", "write", "(", "''", ".", "join", "(", "list", "(", "dq", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.logistic_regression.save_pipeline": [[13, 15], ["joblib.dump", "os.path.join"], "function", ["None"], ["def", "save_pipeline", "(", "pipeline", ",", "directory", ",", "name", ")", ":", "\n", "    ", "joblib", ".", "dump", "(", "pipeline", ",", "os", ".", "path", ".", "join", "(", "directory", ",", "'%s_pipeline.pkl'", "%", "name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.logistic_regression.load_pipeline": [[16, 18], ["joblib.load", "os.path.join"], "function", ["None"], ["", "def", "load_pipeline", "(", "directory", ",", "name", ")", ":", "\n", "    ", "return", "joblib", ".", "load", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "'%s_pipeline.pkl'", "%", "name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.logistic_regression.exists_pipeline": [[19, 21], ["os.path.exists", "os.path.join"], "function", ["None"], ["", "def", "exists_pipeline", "(", "directory", ",", "name", ")", ":", "\n", "    ", "return", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "'%s_pipeline.pkl'", "%", "name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.logistic_regression.train_model": [[22, 52], ["sklearn.pipeline.Pipeline", "sklearn.pipeline.Pipeline.set_params().fit", "sklearn.pipeline.Pipeline.set_params", "sklearn.feature_extraction.text.CountVectorizer", "sklearn.feature_extraction.text.TfidfTransformer", "sklearn.linear_model.LogisticRegression"], "function", ["None"], ["", "def", "train_model", "(", "X", ",", "y", ",", "ngram_type", "=", "'char'", ",", "ngram_range", "=", "(", "1", ",", "4", ")", ",", "vocab_size", "=", "10000", ",", "clf_C", "=", "10", ",", "lowercase", "=", "True", ",", "random_state", "=", "42", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Trains a model with sklearn logistic regression.\n    Adapted from https://github.com/ewulczyn/wiki-detox/blob/master/src/modeling/get_prod_models.py\n    License displayed in preamble.\n    \"\"\"", "\n", "\n", "assert", "(", "ngram_type", "in", "[", "'word'", ",", "'char'", "]", ")", "\n", "\n", "clf", "=", "Pipeline", "(", "[", "\n", "(", "'vect'", ",", "CountVectorizer", "(", ")", ")", ",", "\n", "(", "'tfidf'", ",", "TfidfTransformer", "(", ")", ")", ",", "\n", "(", "'clf'", ",", "LogisticRegression", "(", ")", ")", ",", "\n", "]", ")", "\n", "\n", "params", "=", "{", "\n", "'vect__max_features'", ":", "vocab_size", ",", "\n", "'vect__ngram_range'", ":", "ngram_range", ",", "\n", "'vect__analyzer'", ":", "ngram_type", ",", "\n", "'vect__lowercase'", ":", "lowercase", ",", "\n", "'vect__strip_accents'", ":", "'ascii'", ",", "\n", "'tfidf__sublinear_tf'", ":", "True", ",", "\n", "'tfidf__norm'", ":", "'l2'", ",", "\n", "'clf__C'", ":", "10", ",", "\n", "'clf__solver'", ":", "'lbfgs'", ",", "\n", "'clf__verbose'", ":", "1", "if", "verbose", "else", "0", ",", "\n", "'clf__random_state'", ":", "random_state", "\n", "}", "\n", "\n", "return", "clf", ".", "set_params", "(", "**", "params", ")", ".", "fit", "(", "X", ",", "y", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.data_helpers.get_stratified_subset": [[12, 39], ["sklearn.model_selection.StratifiedKFold", "collections.deque", "collections.deque", "enumerate", "pandas.concat", "pandas.concat", "print", "sklearn.model_selection.StratifiedKFold.split", "pd.concat.append", "pd.concat.append", "list", "list"], "function", ["None"], ["def", "get_stratified_subset", "(", "X_train", ",", "y_train", ",", "percentage", "=", "5", ",", "random_state", "=", "20200303", ")", ":", "\n", "    ", "'''\n    Returns a percentage of the original data.\n    Uses a deque data structure for O(1) appends.\n    '''", "\n", "assert", "percentage", ">", "0", "and", "percentage", "<=", "100", "\n", "skf_tiny", "=", "StratifiedKFold", "(", "n_splits", "=", "100", ",", "shuffle", "=", "True", ",", "random_state", "=", "random_state", ")", "\n", "X_return", "=", "deque", "(", ")", "\n", "y_return", "=", "deque", "(", ")", "\n", "\n", "# fetch non-overlapping 1 percent splits", "\n", "# take union of sets until dataset size reached", "\n", "for", "i", ",", "(", "_", ",", "tiny_train_index", ")", "in", "enumerate", "(", "skf_tiny", ".", "split", "(", "X_train", ",", "y_train", ")", ")", ":", "\n", "\n", "# increase the number of samples", "\n", "        ", "X_tiny_train", ",", "y_tiny_train", "=", "X_train", "[", "tiny_train_index", "]", ",", "y_train", "[", "tiny_train_index", "]", "\n", "X_return", ".", "append", "(", "X_tiny_train", ")", "\n", "y_return", ".", "append", "(", "y_tiny_train", ")", "\n", "\n", "# stop when reaching predetermined percentage", "\n", "if", "i", "+", "1", "==", "percentage", ":", "\n", "            ", "break", "\n", "\n", "", "", "X_return", "=", "pd", ".", "concat", "(", "list", "(", "X_return", ")", ")", "\n", "y_return", "=", "pd", ".", "concat", "(", "list", "(", "y_return", ")", ")", "\n", "print", "(", "'Input shape:'", ",", "X_train", ".", "shape", ",", "'\\tOutput shape:'", ",", "X_return", ".", "shape", ")", "\n", "return", "X_return", ",", "y_return", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.data_helpers.save_categorical_dataset": [[41, 59], ["os.path.join", "print", "open", "zip", "f.write", "int"], "function", ["None"], ["", "def", "save_categorical_dataset", "(", "X", ",", "y", ",", "directory", ",", "name", ")", ":", "\n", "    ", "'''\n    Saves a text dataset to separate files, in format \"name.txt\".\n    Each row in format %d\\t%s\\n, where %d is the class and %s is the text.\n    Typically, 0 (negative class) and 1 (positive class).\n    Dataset order is retained when saved using this function.\n    '''", "\n", "\n", "# ensure no newlines are in the text: each row will be a separate entry", "\n", "for", "x", "in", "X", ":", "\n", "        ", "assert", "'\\n'", "not", "in", "x", "\n", "assert", "'\\t'", "not", "in", "x", "\n", "\n", "", "fname", "=", "os", ".", "path", ".", "join", "(", "directory", ",", "'%s.txt'", "%", "(", "name", ")", ")", "\n", "print", "(", "'Writing file %s'", "%", "fname", ")", "\n", "with", "open", "(", "fname", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "x", ",", "u", "in", "zip", "(", "X", ",", "y", ")", ":", "\n", "            ", "f", ".", "write", "(", "'%d\\t%s\\n'", "%", "(", "int", "(", "u", ")", ",", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.data_helpers.load_categorical_dataset": [[63, 85], ["collections.deque", "collections.deque", "pandas.Series", "pandas.Series", "open", "list", "list", "numpy.int64", "pd.Series.append", "pd.Series.append", "len", "line.strip().split", "line.strip().split", "line.strip", "line.strip", "line.strip"], "function", ["None"], ["", "", "", "def", "load_categorical_dataset", "(", "fname", ")", ":", "\n", "    ", "'''\n    Loads a dataset (pandas.Series text entries, and pandas.Series integer entries)\n    that have been saved with 'save_categorical_dataset'.\n    '''", "\n", "# saves entries initially in deque for O(1) operations", "\n", "X", "=", "deque", "(", ")", "\n", "y", "=", "deque", "(", ")", "\n", "\n", "with", "open", "(", "fname", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "if", "len", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", ")", "==", "2", ":", "\n", "                ", "num", ",", "text", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "", "else", ":", "\n", "                ", "num", ",", "text", "=", "line", ".", "strip", "(", ")", ",", "''", "\n", "", "num", "=", "np", ".", "int64", "(", "num", ")", "\n", "X", ".", "append", "(", "text", ")", "\n", "y", ".", "append", "(", "num", ")", "\n", "\n", "", "", "X", "=", "pd", ".", "Series", "(", "list", "(", "X", ")", ")", "\n", "y", "=", "pd", ".", "Series", "(", "list", "(", "y", ")", ")", "\n", "return", "X", ",", "y", "\n", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.run_experiment.import_optional_libraries": [[33, 63], ["None"], "function", ["None"], ["", "def", "import_optional_libraries", "(", ")", ":", "\n", "    ", "if", "ExperimentParameters", ".", "aug_name", "==", "'eda'", ":", "\n", "        ", "global", "eda", "\n", "from", "eda_scripts", ".", "eda", "import", "eda", "\n", "", "elif", "ExperimentParameters", ".", "aug_name", "==", "'add'", ":", "\n", "        ", "global", "add_sentences", ",", "augmentation_helper_add", "\n", "from", "augmentation_helpers", "import", "add_sentences", ",", "augmentation_helper_add", "\n", "", "if", "ExperimentParameters", ".", "aug_name", "==", "'ppdb'", ":", "\n", "        ", "global", "augment_ppdb", "\n", "from", "ppdb_scripts", ".", "augmentation_ppdb", "import", "augment_ppdb", "\n", "", "elif", "ExperimentParameters", ".", "aug_name", "==", "'wn'", ":", "\n", "        ", "global", "augment_wn", "\n", "from", "wn_scripts", ".", "augmentation_wn", "import", "augment_wn", "\n", "", "elif", "ExperimentParameters", ".", "aug_name", "==", "'gensim'", ":", "\n", "        ", "global", "gensim_aug", "\n", "from", "embedding_scripts", ".", "augmentation_gensim", "import", "gensim_aug", "\n", "", "elif", "ExperimentParameters", ".", "aug_name", "==", "'bpemb'", ":", "\n", "        ", "global", "bpemb_aug", "\n", "from", "embedding_scripts", ".", "augmentation_bpemb", "import", "bpemb_aug", "\n", "", "elif", "ExperimentParameters", ".", "aug_name", "==", "'gpt2'", ":", "\n", "        ", "global", "lm_aug", ",", "LanguageModelWrapper", ",", "load_lm_corpus", "\n", "from", "gpt2_scripts", ".", "augmentation_lm", "import", "lm_aug", ",", "LanguageModelWrapper", ",", "load_lm_corpus", "\n", "\n", "# import classifier-specific libraries", "\n", "", "if", "ExperimentParameters", ".", "classifier_name", "==", "'bert'", ":", "\n", "        ", "global", "BertWrapper", "\n", "from", "fast_bert_scripts", ".", "bert_helpers", "import", "BertWrapper", "\n", "", "elif", "ExperimentParameters", ".", "classifier_name", "==", "'cnn'", ":", "\n", "        ", "global", "CNN_wrapper", "\n", "from", "cnn_scripts", ".", "cnn_helpers", "import", "CNN_wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.run_experiment.run_experiment": [[64, 340], ["run_experiment.import_optional_libraries", "os.path.join", "os.path.join", "os.path.exists", "pandas.read_csv", "os.path.join", "pandas.read_csv", "os.path.join", "pandas.read_csv", "pandas.concat", "pd.concat.head", "run_experiment.run_experiment.parse_dataset"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.run_experiment.import_optional_libraries"], ["", "", "def", "run_experiment", "(", ")", ":", "\n", "\n", "    ", "import_optional_libraries", "(", ")", "\n", "\n", "# Phase 1: determine experiment parameters", "\n", "assert", "ExperimentParameters", ".", "dataset_name", "in", "[", "'threat'", ",", "'identity_hate'", "]", "\n", "\n", "ExperimentParameters", ".", "aug_dir", "=", "'../data/augmentations/%s-%03d/%s-%s'", "%", "(", "ExperimentParameters", ".", "dataset_name", ",", "\n", "ExperimentParameters", ".", "percentage", ",", "\n", "ExperimentParameters", ".", "aug_name", ",", "\n", "''", ".", "join", "(", "list", "(", "map", "(", "str", ",", "ExperimentParameters", ".", "augment_classes", ")", ")", ")", ")", "\n", "\n", "ExperimentParameters", ".", "save_dir", "=", "'../results/%s-%03d/%s/%s-%s'", "%", "(", "ExperimentParameters", ".", "dataset_name", ",", "\n", "ExperimentParameters", ".", "percentage", ",", "\n", "ExperimentParameters", ".", "classifier_name", ",", "\n", "ExperimentParameters", ".", "aug_name", ",", "\n", "''", ".", "join", "(", "list", "(", "map", "(", "str", ",", "ExperimentParameters", ".", "augment_classes", ")", ")", ")", ")", "\n", "\n", "ExperimentParameters", ".", "results_file", "=", "os", ".", "path", ".", "join", "(", "ExperimentParameters", ".", "save_dir", ",", "'%s.txt'", "%", "ExperimentParameters", ".", "random_seed", ")", "\n", "\n", "if", "ExperimentParameters", ".", "classifier_name", "==", "'bert'", ":", "\n", "        ", "bert_wrapper", "=", "BertWrapper", "(", "model_dir", "=", "ExperimentParameters", ".", "save_dir", ",", "\n", "data_dir", "=", "ExperimentParameters", ".", "aug_dir", ",", "\n", "results_filename", "=", "'%s'", "%", "ExperimentParameters", ".", "random_seed", ",", "\n", "random_state", "=", "ExperimentParameters", ".", "random_seed", ")", "\n", "", "else", ":", "\n", "        ", "bert_wrapper", "=", "None", "\n", "\n", "", "if", "ExperimentParameters", ".", "classifier_name", "==", "'cnn'", ":", "\n", "        ", "cnn_wrapper", "=", "CNN_wrapper", "(", "model_dir", "=", "ExperimentParameters", ".", "save_dir", ",", "\n", "data_dir", "=", "ExperimentParameters", ".", "aug_dir", ",", "\n", "results_filename", "=", "'%s'", "%", "ExperimentParameters", ".", "random_seed", ",", "\n", "random_state", "=", "ExperimentParameters", ".", "random_seed", ")", "\n", "", "else", ":", "\n", "        ", "cnn_wrapper", "=", "None", "\n", "\n", "# Phase 2: load datasets", "\n", "", "DATA_DIR", "=", "'../data/'", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "DATA_DIR", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "DATA_DIR", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "DATA_DIR", ",", "'jigsaw-toxic-comment-classification-challenge'", ")", ")", ":", "\n", "        ", "print", "(", "'Jigsaw dataset not found. '", ")", "\n", "print", "(", "'Please download from https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge'", ")", "\n", "print", "(", "'.. and extract to the directory data-augmentation/data/jigsaw-toxic-comment-classification-challenge'", ")", "\n", "return", "\n", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Found Jigsaw dataset.'", ")", "\n", "\n", "\n", "# load train data", "\n", "", "data_dir", "=", "'../data/jigsaw-toxic-comment-classification-challenge'", "\n", "file", "=", "'train.csv'", "\n", "train_csv", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "file", ")", "\n", "\n", "assert", "os", ".", "path", ".", "exists", "(", "train_csv", ")", "\n", "\n", "df_train", "=", "pd", ".", "read_csv", "(", "train_csv", ")", "\n", "\n", "\n", "# load test data", "\n", "file", "=", "'test.csv'", "\n", "test_csv_comments", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "file", ")", "\n", "df_test_comments", "=", "pd", ".", "read_csv", "(", "test_csv_comments", ")", "\n", "\n", "file", "=", "'test_labels.csv'", "\n", "test_csv_labels", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "file", ")", "\n", "df_test_labels", "=", "pd", ".", "read_csv", "(", "test_csv_labels", ")", "\n", "\n", "# verify 'id' is unique for each row", "\n", "assert", "len", "(", "np", ".", "unique", "(", "df_test_labels", "[", "'id'", "]", ".", "values", ")", ")", "==", "len", "(", "df_test_labels", ")", "\n", "\n", "# reindex with 'id'", "\n", "df_test_labels", ".", "index", "=", "df_test_labels", ".", "id", "\n", "del", "df_test_labels", "[", "'id'", "]", "\n", "\n", "# verify 'id' is unique for each row", "\n", "assert", "len", "(", "np", ".", "unique", "(", "df_test_comments", "[", "'id'", "]", ".", "values", ")", ")", "==", "len", "(", "df_test_comments", ")", "\n", "\n", "# reindex with 'id'", "\n", "df_test_comments", ".", "index", "=", "df_test_comments", ".", "id", "\n", "del", "df_test_comments", "[", "'id'", "]", "\n", "\n", "for", "ind", "in", "df_test_comments", ".", "index", ":", "\n", "        ", "assert", "ind", "in", "df_test_labels", ".", "index", "\n", "\n", "", "for", "ind", "in", "df_test_labels", ".", "index", ":", "\n", "        ", "assert", "ind", "in", "df_test_comments", ".", "index", "\n", "\n", "# merge dataframes (add columns)", "\n", "", "df_test", "=", "pd", ".", "concat", "(", "[", "df_test_comments", ",", "df_test_labels", "]", ",", "axis", "=", "1", ")", "\n", "df_test", ".", "head", "(", ")", "\n", "\n", "\n", "# drop the comments that do not have annotation", "\n", "def", "parse_dataset", "(", "df", ")", ":", "\n", "        ", "q", "=", "'%s != -1'", "%", "ExperimentParameters", ".", "dataset_name", "\n", "df", "=", "df", ".", "query", "(", "q", ")", "\n", "X", "=", "df", "[", "'comment_text'", "]", ".", "str", ".", "replace", "(", "'\\n'", ",", "' '", ")", ".", "str", ".", "replace", "(", "'\\t'", ",", "' '", ")", ".", "str", ".", "strip", "(", ")", ".", "str", ".", "split", "(", ")", ".", "str", ".", "join", "(", "' '", ")", "\n", "y", "=", "df", "[", "ExperimentParameters", ".", "dataset_name", "]", "\n", "return", "X", ",", "y", "\n", "\n", "\n", "", "X_train", ",", "y_train", "=", "parse_dataset", "(", "df_train", ")", "\n", "X_test", ",", "y_test", "=", "parse_dataset", "(", "df_test", ")", "\n", "\n", "\n", "\n", "# obtain bootstrapped variation of training data", "\n", "dataset_name", "=", "\"%s_%s_%d\"", "%", "(", "ExperimentParameters", ".", "dataset_name", ",", "\n", "'Percentage-%03d'", "%", "ExperimentParameters", ".", "percentage", ",", "\n", "ExperimentParameters", ".", "random_seed", ")", "\n", "\n", "\n", "X_train_tiny", ",", "y_train_tiny", "=", "data_helpers", ".", "get_stratified_subset", "(", "X_train", ",", "y_train", ",", "percentage", "=", "ExperimentParameters", ".", "percentage", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ")", "\n", "data_helpers", ".", "save_categorical_dataset", "(", "X_train_tiny", ",", "y_train_tiny", ",", "DATA_DIR", ",", "dataset_name", ")", "\n", "\n", "\n", "if", "ExperimentParameters", ".", "aug_name", "==", "'gpt2'", ":", "\n", "# keep all epochs in the same folder", "\n", "        ", "load_corpus_fun", "=", "lambda", ":", "load_lm_corpus", "(", "'../data/'", "+", "dataset_name", "+", "'.txt'", ")", "\n", "print", "(", "'Creating LMWrapper for dataset'", ")", "\n", "model_save_dir", "=", "'../models/%s/%s/'", "%", "(", "'gpt2'", ",", "dataset_name", ")", "\n", "lm", "=", "LanguageModelWrapper", "(", "load_corpus_fun", ",", "model_save_dir", "=", "model_save_dir", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "model_save_dir", ")", ":", "\n", "            ", "print", "(", "'Fine-tuned GPT2 path not found. Training...'", ")", "\n", "lm", ".", "train", "(", "num_epochs", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Found fine-tuned GPT2 directory'", ")", "\n", "\n", "# maps augmentation names to functions", "\n", "", "", "class", "Mapper", ":", "\n", "        ", "dict", "=", "{", "\n", "'baseline'", ":", "lambda", "document", ",", "num_aug", ":", "[", "document", "]", ",", "\n", "'copy'", ":", "lambda", "document", ",", "num_aug", ":", "[", "document", "]", "*", "num_aug", ",", "\n", "\n", "# set recommended settings for small-dataset EDA", "\n", "'eda'", ":", "lambda", "document", ",", "num_aug", ":", "eda", "(", "document", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ",", "alpha_sr", "=", "0.05", ",", "alpha_ri", "=", "0.05", ",", "alpha_rs", "=", "0.05", ",", "p_rd", "=", "0.05", ",", "num_aug", "=", "num_aug", ")", ",", "\n", "\n", "'add'", ":", "None", ",", "\n", "'ppdb'", ":", "lambda", "document", ",", "num_aug", ":", "augment_ppdb", "(", "document", ",", "num_aug", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ")", ",", "\n", "'wn'", ":", "lambda", "document", ",", "num_aug", ":", "augment_wn", "(", "document", ",", "num_aug", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ")", ",", "\n", "'bpemb'", ":", "lambda", "document", ",", "num_aug", ":", "bpemb_aug", "(", "document", ",", "num_aug", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ",", "max_candidates", "=", "10", ",", "incl_orig", "=", "True", ",", "min_similarity", "=", "0.", ",", "change_rate", "=", ".25", ")", ",", "\n", "'gensim'", ":", "lambda", "document", ",", "num_aug", ":", "gensim_aug", "(", "document", ",", "num_aug", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ",", "max_candidates", "=", "10", ",", "incl_orig", "=", "True", ",", "min_similarity", "=", "0.", ",", "change_rate", "=", ".25", ")", ",", "\n", "\n", "'gpt2'", ":", "lambda", "document", ",", "num_aug", ":", "lm_aug", "(", "document", ",", "num_aug", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ",", "incl_orig", "=", "True", ",", "epoch", "=", "2", ",", "lm_wrapper", "=", "lm", ")", ",", "\n", "\n", "'add_bpemb'", ":", "None", ",", "# run combine_augmentations.py", "\n", "'add_bpemb_gpt2'", ":", "None", ",", "# run combine_augmentations.py", "\n", "\n", "# functions for training networks", "\n", "# (numpy array, numpy array) -> object", "\n", "'char-lr'", ":", "lambda", "x", ",", "y", ":", "logistic_regression", ".", "train_model", "(", "x", ",", "y", ",", "ngram_type", "=", "'char'", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ")", ",", "\n", "'word-lr'", ":", "lambda", "x", ",", "y", ":", "logistic_regression", ".", "train_model", "(", "x", ",", "y", ",", "ngram_type", "=", "'word'", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ")", ",", "\n", "'bert'", ":", "lambda", "x", ",", "y", ":", "bert_wrapper", ".", "train_model", "(", "x", ",", "y", ",", "num_epochs", "=", "6", ")", ",", "\n", "'cnn'", ":", "lambda", "x", ",", "y", ":", "cnn_wrapper", ".", "train_model", "(", "x", ",", "y", ",", "num_epochs", "=", "4", ")", "\n", "}", "\n", "\n", "", "assert", "ExperimentParameters", ".", "aug_name", "in", "Mapper", ".", "dict", "\n", "ExperimentParameters", ".", "aug_fun", "=", "Mapper", ".", "dict", "[", "ExperimentParameters", ".", "aug_name", "]", "\n", "\n", "os", ".", "makedirs", "(", "ExperimentParameters", ".", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "ExperimentParameters", ".", "results_file", ")", ":", "\n", "        ", "print", "(", "'Experiment results found.'", ")", "\n", "with", "open", "(", "ExperimentParameters", ".", "results_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "print", "(", "f", ".", "read", "(", ")", ")", "\n", "", "print", "(", "'Exiting...'", ")", "\n", "return", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Experiment results will be saved in\\n\\t'", ",", "ExperimentParameters", ".", "results_file", ")", "\n", "\n", "\n", "", "os", ".", "makedirs", "(", "ExperimentParameters", ".", "aug_dir", ",", "exist_ok", "=", "True", ")", "\n", "ExperimentParameters", ".", "aug_file", "=", "os", ".", "path", ".", "join", "(", "ExperimentParameters", ".", "aug_dir", ",", "'%s.txt'", "%", "ExperimentParameters", ".", "random_seed", ")", "\n", "\n", "print", "(", "'Augmentations will be saved in\\n\\t'", ",", "ExperimentParameters", ".", "aug_file", ")", "\n", "\n", "assert", "ExperimentParameters", ".", "classifier_name", "in", "Mapper", ".", "dict", "\n", "\n", "ExperimentParameters", ".", "classifier_train_fun", "=", "Mapper", ".", "dict", "[", "ExperimentParameters", ".", "classifier_name", "]", "\n", "ExperimentParameters", ".", "classifier_file", "=", "os", ".", "path", ".", "join", "(", "ExperimentParameters", ".", "save_dir", ",", "'clf_%s.pkl'", "%", "ExperimentParameters", ".", "random_seed", ")", "\n", "\n", "print", "(", "'Classifier will be saved in\\n\\t'", ",", "ExperimentParameters", ".", "classifier_file", ")", "\n", "\n", "\n", "# run data augmentation", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "ExperimentParameters", ".", "aug_file", ")", ":", "\n", "\n", "        ", "if", "ExperimentParameters", ".", "aug_name", "==", "'add'", ":", "\n", "            ", "augmentation_helper_add", "(", "input", "=", "'../data/%s.txt'", "%", "dataset_name", ",", "output", "=", "ExperimentParameters", ".", "aug_file", ",", "num_aug", "=", "20", ",", "target_classes", "=", "ExperimentParameters", ".", "augment_classes", ",", "random_state", "=", "ExperimentParameters", ".", "random_seed", ")", "\n", "", "else", ":", "\n", "            ", "augmentation_helper", "(", "input", "=", "'../data/%s.txt'", "%", "dataset_name", ",", "output", "=", "ExperimentParameters", ".", "aug_file", ",", "aug_function", "=", "ExperimentParameters", ".", "aug_fun", ",", "num_aug", "=", "20", ",", "target_classes", "=", "ExperimentParameters", ".", "augment_classes", ")", "\n", "\n", "\n", "# load augmented dataset", "\n", "", "", "X_train_aug", ",", "y_train_aug", "=", "data_helpers", ".", "load_categorical_dataset", "(", "ExperimentParameters", ".", "aug_file", ")", "\n", "X_train_aug", "=", "pd", ".", "Series", "(", "[", "x", ".", "lower", "(", ")", "for", "x", "in", "X_train_aug", "]", ")", "\n", "print", "(", "'\\tDataset shapes'", ",", "X_train_aug", ".", "shape", ",", "y_train_aug", ".", "shape", ")", "\n", "\n", "# create classifier", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "ExperimentParameters", ".", "classifier_file", ")", ":", "\n", "        ", "print", "(", "\"Training model...\"", ")", "\n", "\n", "clf", "=", "ExperimentParameters", ".", "classifier_train_fun", "(", "X_train_aug", ",", "y_train_aug", ")", "\n", "\n", "print", "(", "\"\\tSaving model to... %s\"", "%", "ExperimentParameters", ".", "classifier_file", ")", "\n", "joblib", ".", "dump", "(", "clf", ",", "ExperimentParameters", ".", "classifier_file", ")", "\n", "\n", "\n", "", "print", "(", "\"Reloading Model...\"", ")", "\n", "clf", "=", "joblib", ".", "load", "(", "ExperimentParameters", ".", "classifier_file", ")", "\n", "\n", "print", "(", "clf", ".", "predict_proba", "(", "[", "'if you do not stop, the wikapidea nijas will come to your house and kill you'", "]", ")", ")", "\n", "\n", "print", "(", "\"Predicting test data...\"", ")", "\n", "\n", "if", "ExperimentParameters", ".", "classifier_name", "==", "'bert'", ":", "\n", "        ", "y_pred", "=", "clf", ".", "predict", "(", "X_test", ")", "\n", "", "else", ":", "\n", "        ", "prediction_dir", "=", "'%s/%s'", "%", "(", "ExperimentParameters", ".", "save_dir", ",", "'predictions'", ")", "\n", "os", ".", "makedirs", "(", "prediction_dir", ",", "exist_ok", "=", "True", ")", "\n", "prediction_file", "=", "'%s/%d-output.csv'", "%", "(", "prediction_dir", ",", "ExperimentParameters", ".", "random_seed", ")", "\n", "\n", "print", "(", "'... saving prediction results to %s.'", "%", "prediction_dir", ")", "\n", "y_pred_prob", "=", "clf", ".", "predict_proba", "(", "X_test", ")", "\n", "# output from CNN is a list of lists", "\n", "y_pred_prob", "=", "np", ".", "array", "(", "y_pred_prob", ")", "\n", "y_pred", "=", "np", ".", "array", "(", "y_pred_prob", "[", ":", ",", "1", "]", ">", "0.5", ",", "dtype", "=", "int", ")", "\n", "\n", "with", "open", "(", "prediction_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "y_pred_prob", ":", "\n", "                ", "line", "=", "', '", ".", "join", "(", "map", "(", "str", ",", "line", ")", ")", "\n", "f", ".", "write", "(", "'%s\\n'", "%", "line", ")", "\n", "\n", "\n", "", "", "", "print", "(", "classification_report", "(", "y_test", ",", "y_pred", ")", ")", "\n", "\n", "res_0", "=", "precision_recall_fscore_support", "(", "y_test", ",", "y_pred", ",", "pos_label", "=", "0", ",", "average", "=", "'binary'", ")", "\n", "res_1", "=", "precision_recall_fscore_support", "(", "y_test", ",", "y_pred", ",", "pos_label", "=", "1", ",", "average", "=", "'binary'", ")", "\n", "res_m", "=", "precision_recall_fscore_support", "(", "y_test", ",", "y_pred", ",", "average", "=", "'macro'", ")", "\n", "\n", "\n", "with", "open", "(", "ExperimentParameters", ".", "results_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'\\tPrecision\\tRecall\\tF-score\\n'", ")", "\n", "\n", "f", ".", "write", "(", "'0:\\t'", ")", "\n", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "map", "(", "str", ",", "res_0", "[", ":", "-", "1", "]", ")", ")", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n", "f", ".", "write", "(", "'1:\\t'", ")", "\n", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "map", "(", "str", ",", "res_1", "[", ":", "-", "1", "]", ")", ")", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n", "f", ".", "write", "(", "'macro:\\t'", ")", "\n", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "map", "(", "str", ",", "res_m", "[", ":", "-", "1", "]", ")", ")", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n", "\n", "", "if", "args", ".", "remove_cache", ":", "\n", "# BERT classifier weigths take 421 MB", "\n", "        ", "if", "ExperimentParameters", ".", "classifier_name", "==", "'bert'", ":", "\n", "            ", "bert_wrapper", ".", "delete_cache", "(", ")", "\n", "print", "(", "'Done.'", ")", "\n", "# word-lr takes 30 MB per file", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Deleting classifier file...'", ",", "end", "=", "''", ")", "\n", "os", ".", "remove", "(", "ExperimentParameters", ".", "classifier_file", ")", "\n", "print", "(", "'Done.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.read_experiments.parse_file": [[17, 25], ["open", "file.readline", "file.readline().rstrip().split", "file.readline().rstrip().split", "file.readline().rstrip().split", "list", "map", "file.readline().rstrip", "file.readline().rstrip", "file.readline().rstrip", "file.readline", "file.readline", "file.readline"], "function", ["None"], ["", "def", "parse_file", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file", ":", "\n", "        ", "file", ".", "readline", "(", ")", "\n", "_", ",", "a", ",", "b", ",", "_", "=", "file", ".", "readline", "(", ")", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "_", ",", "c", ",", "d", ",", "_", "=", "file", ".", "readline", "(", ")", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "_", ",", "_", ",", "_", ",", "e", "=", "file", ".", "readline", "(", ")", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "res", "=", "[", "c", ",", "d", ",", "a", ",", "b", ",", "e", "]", "\n", "return", "list", "(", "map", "(", "float", ",", "res", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.read_experiments.read_results": [[26, 36], ["pandas.DataFrame", "os.walk", "sorted", "read_experiments.parse_file", "pandas.DataFrame", "df.append.append", "os.path.join", "int"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.read_experiments.parse_file"], ["", "", "def", "read_results", "(", "results_dir", ")", ":", "\n", "    ", "columns", "=", "[", "'off precision'", ",", "'off recall'", ",", "'non-off precision'", ",", "'non-off recall'", ",", "'macro-averaged F1'", "]", "\n", "df", "=", "pd", ".", "DataFrame", "(", "columns", "=", "columns", ")", "\n", "for", "r", ",", "d", ",", "fs", "in", "os", ".", "walk", "(", "results_dir", ")", ":", "\n", "        ", "for", "filename", "in", "sorted", "(", "fs", ")", ":", "\n", "            ", "if", "filename", "[", "-", "3", ":", "]", "==", "'txt'", ":", "\n", "                ", "row", "=", "parse_file", "(", "os", ".", "path", ".", "join", "(", "r", ",", "filename", ")", ")", "\n", "df2", "=", "pd", ".", "DataFrame", "(", "data", "=", "[", "row", "]", ",", "columns", "=", "columns", ",", "index", "=", "[", "int", "(", "filename", "[", ":", "-", "4", "]", ")", "]", ")", "\n", "df", "=", "df", ".", "append", "(", "df2", ")", "\n", "", "", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.ExperimentParameters.__init__": [[66, 113], ["pathlib.Path", "print", "pathlib.Path", "pathlib.Path", "pathlib.Path", "bert_helpers.ExperimentParameters.DATA_PATH.mkdir", "bert_helpers.ExperimentParameters.MODEL_PATH.mkdir", "bert_helpers.ExperimentParameters.LOG_PATH.mkdir", "bert_helpers.ExperimentParameters.OUTPUT_PATH.mkdir", "bert_helpers.ExperimentParameters.LABEL_PATH.mkdir", "bert_helpers.ExperimentParameters.PRED_PATH.mkdir", "os.path.exists", "os.path.exists", "pathlib.Path", "pathlib.Path", "bert_helpers.ExperimentParameters.BERT_PRETRAINED_PATH.exists", "print", "transformers.BertTokenizer.from_pretrained", "open", "f.write", "bert_helpers.ExperimentParameters.BERT_PRETRAINED_PATH.absolute().as_posix", "bert_helpers.download_bert_base", "print", "bert_helpers.ExperimentParameters.BERT_PRETRAINED_PATH.absolute().as_posix", "bert_helpers.ExperimentParameters.BERT_PRETRAINED_PATH.absolute().as_posix", "bert_helpers.ExperimentParameters.BERT_PRETRAINED_PATH.absolute", "bert_helpers.ExperimentParameters.BERT_PRETRAINED_PATH.absolute", "bert_helpers.ExperimentParameters.BERT_PRETRAINED_PATH.absolute"], "methods", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.download_bert_base"], ["def", "__init__", "(", "self", ",", "model_dir", ",", "data_dir", ",", "filename", ",", "class_name", "=", "'threat'", ",", "top_level", "=", "'../../'", ",", "random_state", "=", "42", ")", ":", "\n", "        ", "'''\n        (str, str, str, str, str) -> None\n        '''", "\n", "self", ".", "MODEL_PATH", "=", "Path", "(", "model_dir", ")", "\n", "self", ".", "DATA_PATH", "=", "Path", "(", "data_dir", ")", "/", "filename", "\n", "print", "(", "'\\t\\tdata_dir='", ",", "data_dir", ")", "\n", "self", ".", "RESULTS_FILENAME", "=", "filename", "+", "'_bert-output.csv'", "\n", "\n", "self", ".", "LABEL_COLS", "=", "[", "class_name", "]", "\n", "self", ".", "BERT_PRETRAINED_PATH", "=", "Path", "(", "top_level", "+", "'tf_bert_models/pretrained-weights/uncased_L-12_H-768_A-12/'", ")", "\n", "\n", "self", ".", "LABEL_PATH", "=", "Path", "(", "top_level", "+", "'data/%s-labels/'", "%", "class_name", ")", "\n", "self", ".", "LOG_PATH", "=", "Path", "(", "top_level", "+", "'logs/'", ")", "\n", "#self.PRED_PATH = Path(top_level+'data/predictions/')", "\n", "self", ".", "PRED_PATH", "=", "Path", "(", "model_dir", ")", "/", "'predictions/'", "\n", "\n", "self", ".", "random_state", "=", "random_state", "\n", "output_dir", "=", "'output-%d'", "%", "random_state", "\n", "self", ".", "OUTPUT_PATH", "=", "self", ".", "MODEL_PATH", "/", "output_dir", "\n", "self", ".", "DATA_PATH", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "self", ".", "MODEL_PATH", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "self", ".", "LOG_PATH", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "self", ".", "OUTPUT_PATH", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "self", ".", "LABEL_PATH", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "self", ".", "PRED_PATH", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "\n", "# download BERT weights", "\n", "if", "not", "self", ".", "BERT_PRETRAINED_PATH", ".", "exists", "(", ")", ":", "\n", "            ", "print", "(", "'Directory'", ",", "self", ".", "BERT_PRETRAINED_PATH", ",", "'not found'", ")", "\n", "\n", "if", "'uncased_L-12_H-768_A-12'", "in", "self", ".", "BERT_PRETRAINED_PATH", ".", "absolute", "(", ")", ".", "as_posix", "(", ")", ":", "\n", "                ", "download_bert_base", "(", "self", ".", "BERT_PRETRAINED_PATH", ".", "absolute", "(", ")", ".", "as_posix", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'Download pre-trained models from site:\\nhttps://github.com/google-research/bert#pre-trained-models'", ")", "\n", "\n", "", "", "assert", "os", ".", "path", ".", "exists", "(", "self", ".", "BERT_PRETRAINED_PATH", ")", "\n", "\n", "# setup Tokenizer", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "BERT_PRETRAINED_PATH", ")", ":", "\n", "            ", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "self", ".", "BERT_PRETRAINED_PATH", ".", "absolute", "(", ")", ".", "as_posix", "(", ")", ",", "do_lower_case", "=", "self", ".", "DO_LOWERCASE", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "tokenizer", "=", "self", ".", "MODEL_NAME", "\n", "\n", "# write label name into directory (for fast-bert)", "\n", "", "with", "open", "(", "self", ".", "LABEL_PATH", "/", "'labels.csv'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "class_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.BertWrapper.__init__": [[290, 307], ["print", "bert_helpers.ExperimentParameters"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "model_dir", ",", "\n", "data_dir", ",", "\n", "results_filename", ",", "\n", "random_state", ")", ":", "\n", "\n", "        ", "self", ".", "random_state", "=", "random_state", "\n", "# create BertExperimentParameters object", "\n", "self", ".", "experiment_parameters", "=", "lambda", ":", "ExperimentParameters", "(", "model_dir", "=", "model_dir", ",", "\n", "data_dir", "=", "data_dir", ",", "\n", "filename", "=", "results_filename", ",", "\n", "class_name", "=", "'offensive'", ",", "\n", "top_level", "=", "'../'", ",", "\n", "random_state", "=", "random_state", ")", "\n", "print", "(", "'\\t\\tdata_dir='", ",", "data_dir", ")", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.BertWrapper.train_model": [[308, 326], ["bert_helpers.BertWrapper.experiment_parameters", "bert_helpers.get_bert_args", "pandas.DataFrame", "pandas.DataFrame.to_csv", "bert_helpers.train_bert"], "methods", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.get_bert_args", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.train_bert"], ["", "def", "train_model", "(", "self", ",", "x", ",", "y", ",", "num_epochs", "=", "6", ",", "mixed_precision", "=", "True", ")", ":", "\n", "\n", "# create experiment parameters", "\n", "# lazy initialization", "\n", "        ", "self", ".", "experiment_parameters", "=", "self", ".", "experiment_parameters", "(", ")", "\n", "self", ".", "output_path", "=", "self", ".", "experiment_parameters", ".", "OUTPUT_PATH", "\n", "self", ".", "args", "=", "get_bert_args", "(", "self", ".", "experiment_parameters", ",", "\n", "num_epochs", ",", "mixed_precision", ",", "self", ".", "experiment_parameters", ".", "random_state", ")", "\n", "\n", "# create pandas dataframe", "\n", "df", "=", "pd", ".", "DataFrame", "(", "{", "'comment_text'", ":", "x", ",", "'offensive'", ":", "y", "}", ")", "\n", "df", ".", "to_csv", "(", "self", ".", "experiment_parameters", ".", "DATA_PATH", "/", "'train.csv'", ")", "\n", "\n", "# train", "\n", "train_bert", "(", "self", ".", "experiment_parameters", ",", "self", ".", "args", ")", "\n", "\n", "# return self (wrapper that has a predict method)", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.BertWrapper.predict_proba": [[327, 354], ["print", "pandas.DataFrame", "pandas.DataFrame.to_csv", "fast_bert.prediction.BertClassificationPredictor", "fast_bert.prediction.BertClassificationPredictor.predict_batch", "pandas.DataFrame().to_csv", "pandas.DataFrame", "print", "pandas.DataFrame.values.reshape", "pandas.Series", "list", "pandas.DataFrame.head", "pandas.DataFrame", "pandas.read_csv"], "methods", ["None"], ["", "def", "predict_proba", "(", "self", ",", "x", ")", ":", "\n", "        ", "print", "(", "'\\tpredicting probabilities...'", ")", "\n", "# create pandas dataframe", "\n", "df", "=", "pd", ".", "DataFrame", "(", "{", "'comment_text'", ":", "x", "}", ")", "\n", "df", ".", "to_csv", "(", "self", ".", "experiment_parameters", ".", "DATA_PATH", "/", "'test.csv'", ")", "\n", "\n", "# create predictor object", "\n", "output_dir", "=", "'output-%d/model_out'", "%", "self", ".", "experiment_parameters", ".", "random_state", "\n", "predictor", "=", "BertClassificationPredictor", "(", "model_path", "=", "(", "self", ".", "experiment_parameters", ".", "MODEL_PATH", "/", "output_dir", ")", ".", "absolute", "(", ")", ".", "as_posix", "(", ")", ",", "\n", "label_path", "=", "self", ".", "experiment_parameters", ".", "LABEL_PATH", ",", "\n", "multi_label", "=", "True", ",", "\n", "model_type", "=", "self", ".", "experiment_parameters", ".", "MODEL_TYPE", ",", "\n", "do_lower_case", "=", "True", ")", "\n", "\n", "# predict test labels", "\n", "output", "=", "predictor", ".", "predict_batch", "(", "list", "(", "pd", ".", "read_csv", "(", "self", ".", "experiment_parameters", ".", "DATA_PATH", "/", "'test.csv'", ")", "[", "'comment_text'", "]", ".", "values", ")", ")", "\n", "\n", "# dump results", "\n", "pd", ".", "DataFrame", "(", "output", ")", ".", "to_csv", "(", "self", ".", "experiment_parameters", ".", "PRED_PATH", "/", "self", ".", "experiment_parameters", ".", "RESULTS_FILENAME", ")", "\n", "\n", "# clean output", "\n", "preds", "=", "pd", ".", "DataFrame", "(", "[", "{", "item", "[", "0", "]", ":", "item", "[", "1", "]", "for", "item", "in", "pred", "}", "for", "pred", "in", "output", "]", ")", "\n", "print", "(", "preds", ".", "head", "(", "5", ")", ")", "\n", "\n", "y_pred_prob", "=", "preds", ".", "values", ".", "reshape", "(", "-", "1", ")", "\n", "y_pred_prob", "=", "pd", ".", "Series", "(", "y_pred_prob", ")", "\n", "return", "y_pred_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.BertWrapper.predict": [[355, 360], ["bert_helpers.BertWrapper.predict_proba", "pandas.Series", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.predict_proba"], ["", "def", "predict", "(", "self", ",", "x", ")", ":", "\n", "        ", "y_pred_prob", "=", "self", ".", "predict_proba", "(", "x", ")", "\n", "y_pred", "=", "pd", ".", "Series", "(", "np", ".", "array", "(", "y_pred_prob", ">", "0.5", ",", "dtype", "=", "int", ")", ")", "\n", "\n", "return", "y_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.BertWrapper.delete_cache": [[363, 370], ["print", "rmtree", "print"], "methods", ["None"], ["", "def", "delete_cache", "(", "self", ")", ":", "\n", "        ", "from", "shutil", "import", "rmtree", "\n", "output_dir", "=", "'output-%d'", "%", "self", ".", "random_state", "\n", "temp_dir", "=", "self", ".", "experiment_parameters", ".", "MODEL_PATH", "/", "output_dir", "\n", "print", "(", "'\\tRemoving cache directory...'", ",", "end", "=", "''", ")", "\n", "rmtree", "(", "temp_dir", ")", "\n", "print", "(", "' Done.'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.download_bert_base": [[25, 53], ["os.getcwd", "os.makedirs", "os.chdir", "os.system", "os.system", "os.system", "os.system", "os.system", "os.system", "os.system", "os.system", "os.chdir"], "function", ["None"], ["def", "download_bert_base", "(", "pretrained_path", ")", ":", "\n", "    ", "'''\n    (str) -> None\n    '''", "\n", "\n", "# get current directory", "\n", "current", "=", "os", ".", "getcwd", "(", ")", "\n", "\n", "# make new directories", "\n", "os", ".", "makedirs", "(", "pretrained_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "os", ".", "chdir", "(", "pretrained_path", "+", "'/..'", ")", "\n", "os", ".", "system", "(", "\"echo 'Pre-trained weights are downloaded to directory:'\"", ")", "\n", "\n", "# download the pre-trained weights", "\n", "os", ".", "system", "(", "\"echo 'Downloading pre-trained weights'\"", ")", "\n", "os", ".", "system", "(", "\"wget --no-check-certificate https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\"", ")", "\n", "\n", "os", ".", "system", "(", "\"echo 'Unzipping file archive'\"", ")", "\n", "os", ".", "system", "(", "\"unzip uncased_L-12_H-768_A-12.zip\"", ")", "\n", "\n", "os", ".", "system", "(", "\"echo 'Removing orphan file'\"", ")", "\n", "os", ".", "system", "(", "\"rm uncased_L-12_H-768_A-12.zip\"", ")", "\n", "\n", "os", ".", "system", "(", "\"echo 'Returning to jupyter notebook directory'\"", ")", "\n", "os", ".", "chdir", "(", "current", ")", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.get_bert_args": [[118, 162], ["box.Box"], "function", ["None"], ["", "", "", "def", "get_bert_args", "(", "experiment_parameters", ",", "num_epochs", ",", "mixed_precision", "=", "True", ",", "random_state", "=", "42", ")", ":", "\n", "    ", "args", "=", "Box", "(", "{", "\n", "\"run_text\"", ":", "\"multilabel toxic comments with freezable layers\"", ",", "\n", "\"train_size\"", ":", "-", "1", ",", "\n", "\"val_size\"", ":", "-", "1", ",", "\n", "\"log_path\"", ":", "experiment_parameters", ".", "LOG_PATH", ",", "\n", "\"full_data_dir\"", ":", "experiment_parameters", ".", "DATA_PATH", ",", "\n", "\"data_dir\"", ":", "experiment_parameters", ".", "DATA_PATH", ",", "\n", "\"task_name\"", ":", "\"toxic_classification_lib\"", ",", "\n", "\"no_cuda\"", ":", "False", ",", "\n", "\"bert_model\"", ":", "experiment_parameters", ".", "BERT_PRETRAINED_PATH", ",", "\n", "\"output_dir\"", ":", "experiment_parameters", ".", "OUTPUT_PATH", ",", "\n", "\"max_seq_length\"", ":", "128", ",", "# down from 512", "\n", "\"do_train\"", ":", "experiment_parameters", ".", "DO_TRAIN", ",", "\n", "\"do_eval\"", ":", "experiment_parameters", ".", "DO_EVAL", ",", "\n", "\"do_lower_case\"", ":", "experiment_parameters", ".", "DO_LOWERCASE", ",", "\n", "\"train_batch_size\"", ":", "8", ",", "\n", "\"eval_batch_size\"", ":", "16", ",", "\n", "\"learning_rate\"", ":", "5e-5", ",", "\n", "\"num_train_epochs\"", ":", "num_epochs", ",", "\n", "\"warmup_proportion\"", ":", "0.0", ",", "\n", "\"no_cuda\"", ":", "False", ",", "\n", "\"local_rank\"", ":", "-", "1", ",", "\n", "\"seed\"", ":", "42", ",", "\n", "\"gradient_accumulation_steps\"", ":", "1", ",", "\n", "\"optimize_on_cpu\"", ":", "False", ",", "\n", "\"fp16\"", ":", "mixed_precision", ",", "# on RTX 20-series", "\n", "\"fp16_opt_level\"", ":", "\"O1\"", ",", "# Mixed Precision (recommended for typical use), also try", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "\"adam_epsilon\"", ":", "1e-8", ",", "\n", "\"max_grad_norm\"", ":", "1.0", ",", "\n", "\"max_steps\"", ":", "-", "1", ",", "\n", "\"warmup_steps\"", ":", "500", ",", "\n", "\"logging_steps\"", ":", "50", ",", "\n", "\"eval_all_checkpoints\"", ":", "True", ",", "\n", "\"overwrite_output_dir\"", ":", "True", ",", "\n", "\"overwrite_cache\"", ":", "False", ",", "\n", "\"seed\"", ":", "random_state", ",", "\n", "\"loss_scale\"", ":", "128", ",", "\n", "\"task_name\"", ":", "'intent'", ",", "\n", "\"model_name\"", ":", "experiment_parameters", ".", "MODEL_NAME", ",", "\n", "\"model_type\"", ":", "experiment_parameters", ".", "MODEL_TYPE", ",", "\n", "}", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.train_bert": [[163, 224], ["datetime.datetime.today().strftime", "str", "logging.basicConfig", "logging.getLogger", "torch.device", "print", "print", "torch.manual_seed", "fast_bert.data_cls.BertDataBunch", "metrics.append", "metrics.append", "metrics.append", "fast_bert.learner_cls.BertLearner.from_pretrained_model", "torch.manual_seed", "BertLearner.from_pretrained_model.fit", "BertLearner.from_pretrained_model.save_model", "torch.cuda.device_count", "datetime.datetime.today", "logging.FileHandler", "logging.StreamHandler"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.save_model"], ["", "def", "train_bert", "(", "experiment_parameters", ",", "args", ")", ":", "\n", "\n", "# logging", "\n", "    ", "run_start_time", "=", "datetime", ".", "datetime", ".", "today", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S'", ")", "\n", "logfile", "=", "str", "(", "experiment_parameters", ".", "LOG_PATH", "/", "'log-{}-{}.txt'", ".", "format", "(", "run_start_time", ",", "args", "[", "\"run_text\"", "]", ")", ")", "\n", "\n", "logging", ".", "basicConfig", "(", "\n", "level", "=", "logging", ".", "INFO", ",", "\n", "format", "=", "'%(asctime)s - %(levelname)s - %(name)s -   %(message)s'", ",", "\n", "datefmt", "=", "'%m/%d/%Y %H:%M:%S'", ",", "\n", "handlers", "=", "[", "\n", "logging", ".", "FileHandler", "(", "logfile", ")", ",", "\n", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "\n", "]", ")", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "\n", "# cuda", "\n", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", ":", "\n", "        ", "args", ".", "multi_gpu", "=", "True", "\n", "", "else", ":", "\n", "        ", "args", ".", "multi_gpu", "=", "False", "\n", "\n", "", "print", "(", ")", "\n", "print", "(", "'BERT training file: '", ",", "args", "[", "'data_dir'", "]", ",", "'train.csv'", ")", "\n", "\n", "# create a fast-bert-specific data format", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "databunch", "=", "BertDataBunch", "(", "args", "[", "'data_dir'", "]", ",", "experiment_parameters", ".", "LABEL_PATH", ",", "\n", "experiment_parameters", ".", "tokenizer", ",", "\n", "train_file", "=", "'train.csv'", ",", "\n", "val_file", "=", "None", ",", "#'test.csv',", "\n", "test_data", "=", "'test.csv'", ",", "\n", "text_col", "=", "\"comment_text\"", ",", "label_col", "=", "experiment_parameters", ".", "LABEL_COLS", ",", "\n", "batch_size_per_gpu", "=", "args", "[", "'train_batch_size'", "]", ",", "max_seq_length", "=", "args", "[", "'max_seq_length'", "]", ",", "\n", "multi_gpu", "=", "args", ".", "multi_gpu", ",", "multi_label", "=", "True", ",", "model_type", "=", "args", ".", "model_type", ",", "clear_cache", "=", "False", ")", "\n", "\n", "metrics", "=", "[", "]", "\n", "metrics", ".", "append", "(", "{", "'name'", ":", "'accuracy_thresh'", ",", "'function'", ":", "accuracy_thresh", "}", ")", "\n", "metrics", ".", "append", "(", "{", "'name'", ":", "'roc_auc'", ",", "'function'", ":", "roc_auc", "}", ")", "\n", "metrics", ".", "append", "(", "{", "'name'", ":", "'fbeta'", ",", "'function'", ":", "fbeta", "}", ")", "\n", "\n", "# create learner object", "\n", "learner", "=", "BertLearner", ".", "from_pretrained_model", "(", "databunch", ",", "args", ".", "model_name", ",", "metrics", "=", "metrics", ",", "\n", "device", "=", "device", ",", "logger", "=", "logger", ",", "output_dir", "=", "args", ".", "output_dir", ",", "\n", "finetuned_wgts_path", "=", "experiment_parameters", ".", "FINETUNED_PATH", ",", "\n", "warmup_steps", "=", "args", ".", "warmup_steps", ",", "\n", "multi_gpu", "=", "args", ".", "multi_gpu", ",", "is_fp16", "=", "args", ".", "fp16", ",", "\n", "multi_label", "=", "True", ",", "logging_steps", "=", "0", ")", "\n", "\n", "# train", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "learner", ".", "fit", "(", "args", ".", "num_train_epochs", ",", "args", ".", "learning_rate", ",", "validate", "=", "False", ")", "\n", "\n", "# save", "\n", "learner", ".", "save_model", "(", ")", "\n", "\n", "# free memory and exit", "\n", "del", "learner", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.predict_bert": [[227, 263], ["fast_bert.prediction.BertClassificationPredictor", "fast_bert.prediction.BertClassificationPredictor.predict_batch", "pandas.DataFrame().to_csv", "pandas.DataFrame", "print", "pandas.read_csv", "print", "pandas.merge", "print", "pd.merge.to_csv", "list", "pd.DataFrame.head", "pd.read_csv.head", "pd.merge.head", "pandas.DataFrame", "pandas.read_csv"], "function", ["None"], ["", "def", "predict_bert", "(", "experiment_parameters", ")", ":", "\n", "# create predictor object", "\n", "    ", "predictor", "=", "BertClassificationPredictor", "(", "model_path", "=", "(", "experiment_parameters", ".", "MODEL_PATH", "/", "'output/model_out'", ")", ".", "absolute", "(", ")", ".", "as_posix", "(", ")", ",", "\n", "label_path", "=", "experiment_parameters", ".", "LABEL_PATH", ",", "\n", "multi_label", "=", "True", ",", "\n", "model_type", "=", "experiment_parameters", ".", "MODEL_TYPE", ",", "\n", "do_lower_case", "=", "True", ")", "\n", "\n", "# predict test labels", "\n", "output", "=", "predictor", ".", "predict_batch", "(", "list", "(", "pd", ".", "read_csv", "(", "experiment_parameters", ".", "DATA_PATH", "/", "'test.csv'", ")", "[", "'comment_text'", "]", ".", "values", ")", ")", "\n", "\n", "# dump results", "\n", "pd", ".", "DataFrame", "(", "output", ")", ".", "to_csv", "(", "experiment_parameters", ".", "PRED_PATH", "/", "experiment_parameters", ".", "RESULTS_FILENAME", ")", "\n", "\n", "# clean output", "\n", "preds", "=", "pd", ".", "DataFrame", "(", "[", "{", "item", "[", "0", "]", ":", "item", "[", "1", "]", "for", "item", "in", "pred", "}", "for", "pred", "in", "output", "]", ")", "\n", "print", "(", "preds", ".", "head", "(", ")", ")", "\n", "\n", "# load test data", "\n", "df_test", "=", "pd", ".", "read_csv", "(", "experiment_parameters", ".", "DATA_PATH", "/", "'test.csv'", ")", "\n", "print", "(", "df_test", ".", "head", "(", ")", ")", "\n", "\n", "# merge dataframes", "\n", "df_pred", "=", "pd", ".", "merge", "(", "df_test", ",", "preds", ",", "how", "=", "'left'", ",", "left_index", "=", "True", ",", "right_index", "=", "True", ")", "\n", "del", "df_pred", "[", "'comment_text'", "]", "\n", "\n", "#df_pred = df_pred['id', 'obscene']", "\n", "df_pred", "[", "'ground_truth'", "]", "=", "df_pred", "[", "'%s_x'", "%", "LABEL_COLS", "[", "0", "]", "]", "\n", "df_pred", "[", "'pred_prob'", "]", "=", "df_pred", "[", "'%s_y'", "%", "LABEL_COLS", "[", "0", "]", "]", "\n", "del", "df_pred", "[", "'%s_x'", "%", "LABEL_COLS", "[", "0", "]", "]", "\n", "del", "df_pred", "[", "'%s_y'", "%", "LABEL_COLS", "[", "0", "]", "]", "\n", "print", "(", "df_pred", ".", "head", "(", ")", ")", "\n", "\n", "# write results to file", "\n", "df_pred", ".", "to_csv", "(", "experiment_parameters", ".", "PRED_PATH", "/", "experiment_parameters", ".", "RESULTS_FILENAME", ",", "index", "=", "None", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.fast_bert_scripts.bert_helpers.test_bert": [[265, 285], ["pandas.read_csv", "print", "numpy.array", "print", "pandas.DataFrame", "pd.read_csv.head", "sklearn.metrics.classification_report", "list", "list", "list", "sklearn.metrics.precision_recall_fscore_support", "sklearn.metrics.precision_recall_fscore_support", "sklearn.metrics.precision_recall_fscore_support"], "function", ["None"], ["", "def", "test_bert", "(", "experiment_parameters", ")", ":", "\n", "\n", "    ", "df_pred", "=", "pd", ".", "read_csv", "(", "experiment_parameters", ".", "PRED_PATH", "/", "experiment_parameters", ".", "RESULTS_FILENAME", ")", "\n", "print", "(", "df_pred", ".", "head", "(", ")", ")", "\n", "\n", "# pick top probability", "\n", "y_pred_prob", "=", "df_pred", "[", "'pred_prob'", "]", ".", "values", "\n", "y", "=", "df_pred", "[", "'ground_truth'", "]", ".", "values", "\n", "y_pred", "=", "np", ".", "array", "(", "y_pred_prob", ">", "0.5", ",", "dtype", "=", "int", ")", "\n", "\n", "# print classification report and return", "\n", "print", "(", "classification_report", "(", "y", ",", "y_pred", ")", ")", "\n", "\n", "res", "=", "[", "list", "(", "precision_recall_fscore_support", "(", "y", ",", "y_pred", ",", "average", "=", "'binary'", ",", "pos_label", "=", "0", ")", ")", "]", "\n", "res", "+=", "[", "list", "(", "precision_recall_fscore_support", "(", "y", ",", "y_pred", ",", "average", "=", "'binary'", ",", "pos_label", "=", "1", ")", ")", "]", "\n", "res", "+=", "[", "list", "(", "precision_recall_fscore_support", "(", "y", ",", "y_pred", ",", "average", "=", "'macro'", ")", ")", "]", "\n", "\n", "res", "=", "pd", ".", "DataFrame", "(", "data", "=", "res", ",", "columns", "=", "[", "'precision'", ",", "'recall'", ",", "'fscore'", ",", "'support'", "]", ")", "\n", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.augmentation_wn.wn_pos": [[28, 39], ["None"], "function", ["None"], ["def", "wn_pos", "(", "postag", ")", ":", "\n", "    ", "pos_wn", "=", "[", "]", "\n", "if", "postag", "[", "0", "]", "==", "'V'", ":", "\n", "        ", "pos_wn", "=", "[", "'v'", "]", "\n", "", "elif", "postag", "[", "0", "]", "==", "'N'", ":", "\n", "        ", "pos_wn", "=", "[", "'n'", "]", "\n", "", "elif", "postag", "[", ":", "2", "]", "==", "'JJ'", ":", "\n", "        ", "pos_wn", "=", "[", "'a'", ",", "'s'", "]", "\n", "", "elif", "postag", "[", ":", "2", "]", "==", "'RB'", ":", "\n", "        ", "pos_wn", "=", "[", "'r'", "]", "\n", "", "return", "pos_wn", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.augmentation_wn.augment_wn": [[48, 132], ["numpy.random.RandomState", "nltk.sent_tokenize", "enumerate", "len", "numpy.array", "numpy.clip", "range", "nltk.word_tokenize", "nltk.pos_tag", "numpy.round", "doc_aug.append", "enumerate", "doc_aug.append", "sent.lower", "enumerate", "enumerate", "np.random.RandomState.permutation", "words_aug.append", "word.lower.lower", "set", "sent_synonyms[].append", "len", "augmentation_wn.wn_pos", "np.random.RandomState.choice", "np.random.RandomState.choice", "len", "sorted", "wsd", "synsets.append", "s.lemma_names", "list", "wsd.hypernyms", "wsd.hyponyms", "set.add", "set.add"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.make_inflections.wn_pos"], ["def", "augment_wn", "(", "document", ",", "\n", "num_aug", "=", "20", ",", "\n", "incl_orig", "=", "True", ",", "\n", "incl_orig_as_paraphrase", "=", "False", ",", "\n", "hypernyms", "=", "False", ",", "\n", "hyponyms", "=", "False", ",", "\n", "allow_lemma", "=", "False", ",", "\n", "change_rate", "=", "0.25", ",", "\n", "max_len", "=", "100", ",", "\n", "infl_dict", "=", "inflections", ",", "\n", "wsd", "=", "simple_lesk", ",", "\n", "sent_synonym_dict", "=", "None", ",", "\n", "return_synonym_dict", "=", "False", ",", "\n", "random_state", "=", "20200303", ")", ":", "\n", "\n", "    ", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "random_state", ")", "\n", "\n", "sents", "=", "sent_tokenize", "(", "document", ")", "\n", "sent_synonyms", "=", "{", "}", "if", "not", "sent_synonym_dict", "else", "sent_synonym_dict", "\n", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "sents", ")", ":", "\n", "        ", "words", "=", "word_tokenize", "(", "sent", ".", "lower", "(", ")", ")", "\n", "words_tags", "=", "pos_tag", "(", "words", ")", "\n", "sent", "=", "' '", ".", "join", "(", "words", ")", "\n", "sents", "[", "i", "]", "=", "sent", "\n", "\n", "if", "sent", "not", "in", "sent_synonyms", ":", "\n", "            ", "sent_synonyms", "[", "sent", "]", "=", "[", "]", "\n", "for", "i", ",", "(", "word", ",", "tag", ")", "in", "enumerate", "(", "words_tags", ")", ":", "\n", "                ", "word", "=", "word", ".", "lower", "(", ")", "\n", "word_synonyms", "=", "set", "(", ")", "\n", "if", "0", "<", "max_len", ">", "i", "and", "len", "(", "word", ")", ">", "2", "and", "word", "not", "in", "no_synonyms", ":", "\n", "                    ", "for", "pos", "in", "wn_pos", "(", "tag", ")", ":", "\n", "                        ", "try", ":", "\n", "                            ", "synset", "=", "wsd", "(", "sent", ",", "word", ",", "pos", ")", "\n", "", "except", ":", "\n", "                            ", "synset", "=", "None", "\n", "", "synsets", "=", "[", "]", "\n", "if", "synset", ":", "\n", "                            ", "synsets", ".", "append", "(", "synset", ")", "\n", "if", "hypernyms", ":", "\n", "                                ", "synsets", "+=", "synset", ".", "hypernyms", "(", ")", "\n", "", "if", "hyponyms", ":", "\n", "                                ", "synsets", "+=", "synset", ".", "hyponyms", "(", ")", "\n", "", "", "for", "s", "in", "synsets", ":", "\n", "                            ", "for", "lemma", "in", "s", ".", "lemma_names", "(", ")", ":", "\n", "                                ", "if", "lemma", "in", "infl_dict", "and", "tag", "in", "infl_dict", "[", "lemma", "]", ":", "\n", "                                    ", "word_synonyms", ".", "add", "(", "infl_dict", "[", "lemma", "]", "[", "tag", "]", ")", "\n", "", "elif", "allow_lemma", ":", "\n", "                                    ", "word_synonyms", ".", "add", "(", "lemma", ")", "\n", "\n", "", "", "", "", "", "sent_synonyms", "[", "sent", "]", ".", "append", "(", "[", "word", "]", "+", "sorted", "(", "list", "(", "word_synonyms", "-", "{", "word", "}", ")", ")", ")", "\n", "\n", "", "", "", "document_synonyms", "=", "[", "]", "\n", "for", "s", "in", "sents", ":", "\n", "        ", "document_synonyms", "+=", "sent_synonyms", "[", "s", "]", "\n", "\n", "", "synonym_ix", "=", "[", "i", "for", "(", "i", ",", "l", ")", "in", "enumerate", "(", "document_synonyms", ")", "if", "len", "(", "l", ")", ">", "1", "]", "\n", "\n", "num_possible_substitution_positions", "=", "len", "(", "synonym_ix", ")", "\n", "num_substitutions", "=", "num_possible_substitution_positions", "*", "change_rate", "\n", "num_substitutions", "=", "np", ".", "array", "(", "np", ".", "round", "(", "num_substitutions", ")", ",", "dtype", "=", "int", ")", "\n", "num_substitutions", "=", "np", ".", "clip", "(", "num_substitutions", ",", "1", ",", "num_possible_substitution_positions", ")", "\n", "\n", "doc_aug", "=", "[", "]", "\n", "if", "incl_orig", ":", "\n", "        ", "num_aug", "-=", "1", "\n", "doc_aug", ".", "append", "(", "document", ")", "\n", "\n", "", "for", "augm", "in", "range", "(", "num_aug", ")", ":", "\n", "        ", "words_aug", "=", "[", "]", "\n", "phrases_ix", "=", "rng", ".", "permutation", "(", "num_possible_substitution_positions", ")", "[", ":", "num_substitutions", "]", "\n", "phrases_ix", "=", "[", "synonym_ix", "[", "i", "]", "for", "i", "in", "phrases_ix", "]", "\n", "for", "i", ",", "word_list", "in", "enumerate", "(", "document_synonyms", ")", ":", "\n", "            ", "if", "i", "in", "phrases_ix", ":", "\n", "                ", "word", "=", "rng", ".", "choice", "(", "word_list", ")", "if", "incl_orig_as_paraphrase", "else", "rng", ".", "choice", "(", "word_list", "[", "1", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "word", "=", "word_list", "[", "0", "]", "\n", "", "words_aug", ".", "append", "(", "word", ")", "\n", "", "doc_aug", ".", "append", "(", "' '", ".", "join", "(", "words_aug", ")", ")", "\n", "\n", "", "if", "return_synonym_dict", ":", "\n", "        ", "return", "doc_aug", ",", "sent_synonyms", "\n", "", "return", "doc_aug", "\n", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.make_inflections.load_inflections": [[14, 21], ["os.path.isfile", "print", "open", "pickle.load"], "function", ["None"], ["def", "load_inflections", "(", "infl_path", "=", "'wn/inflections.pkl'", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "isfile", "(", "infl_path", ")", ":", "\n", "        ", "with", "open", "(", "infl_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "infl", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "infl", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"No inflection file found\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.make_inflections.wn_pos": [[23, 34], ["None"], "function", ["None"], ["", "", "def", "wn_pos", "(", "postag", ")", ":", "\n", "    ", "pos_wn", "=", "[", "]", "\n", "if", "postag", "[", "0", "]", "==", "'V'", ":", "\n", "        ", "pos_wn", "=", "[", "'v'", "]", "\n", "", "elif", "postag", "[", "0", "]", "==", "'N'", ":", "\n", "        ", "pos_wn", "=", "[", "'n'", "]", "\n", "", "elif", "postag", "[", ":", "2", "]", "==", "'JJ'", ":", "\n", "        ", "pos_wn", "=", "[", "'a'", ",", "'s'", "]", "\n", "", "elif", "postag", "[", ":", "2", "]", "==", "'RB'", ":", "\n", "        ", "pos_wn", "=", "[", "'r'", "]", "\n", "", "return", "pos_wn", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.make_inflections.inflection_count": [[36, 50], ["collections.defaultdict", "len", "enumerate", "type", "nltk.sent_tokenize", "nltk.pos_tag", "collections.defaultdict", "nltk.word_tokenize", "make_inflections.wn_pos", "print", "lemmatizer.lemmatize", "collections.Counter", "word.lower"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.make_inflections.wn_pos"], ["", "def", "inflection_count", "(", "text", ",", "print_every", "=", "1000", ")", ":", "\n", "    ", "infl_count", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "lambda", ":", "Counter", "(", ")", ")", ")", "\n", "if", "type", "(", "text", ")", "==", "str", ":", "\n", "        ", "text", "=", "sent_tokenize", "(", "text", ")", "\n", "", "sent_amt", "=", "len", "(", "text", ")", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "text", ")", ":", "\n", "        ", "for", "word", ",", "tag", "in", "pos_tag", "(", "word_tokenize", "(", "sent", ")", ")", ":", "\n", "            ", "wn_tags", "=", "wn_pos", "(", "tag", ")", "\n", "for", "t", "in", "wn_tags", ":", "\n", "                ", "lemma", "=", "lemmatizer", ".", "lemmatize", "(", "word", ",", "t", ")", "\n", "infl_count", "[", "lemma", "]", "[", "tag", "]", "[", "word", ".", "lower", "(", ")", "]", "+=", "1", "\n", "", "", "if", "i", "%", "print_every", "==", "0", ":", "\n", "            ", "print", "(", "'Finding inflections:'", ",", "i", ",", "'/'", ",", "sent_amt", ")", "\n", "", "", "return", "infl_count", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.make_inflections.max_count": [[52, 60], ["[].most_common"], "function", ["None"], ["", "def", "max_count", "(", "count_dict", ")", ":", "\n", "    ", "max_dict", "=", "{", "}", "\n", "for", "word", "in", "count_dict", ":", "\n", "        ", "if", "word", "not", "in", "max_dict", ":", "\n", "            ", "max_dict", "[", "word", "]", "=", "{", "}", "\n", "", "for", "tag", "in", "count_dict", "[", "word", "]", ":", "\n", "            ", "max_dict", "[", "word", "]", "[", "tag", "]", "=", "count_dict", "[", "word", "]", "[", "tag", "]", ".", "most_common", "(", ")", "[", "0", "]", "[", "0", "]", "\n", "", "", "return", "max_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.make_inflections.save_inflection_dict": [[62, 67], ["make_inflections.inflection_count", "make_inflections.max_count", "open", "pickle.dump"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.make_inflections.inflection_count", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.wn_scripts.make_inflections.max_count"], ["", "def", "save_inflection_dict", "(", "corpus", ",", "fpath", ")", ":", "\n", "    ", "count_dict", "=", "inflection_count", "(", "corpus", ")", "\n", "count_dict_max", "=", "max_count", "(", "count_dict", ")", "\n", "with", "open", "(", "fpath", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "count_dict_max", ",", "f", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN.__init__": [[19, 32], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "len"], "methods", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "input_size", ",", "\n", "embedding_dim", "=", "300", ",", "\n", "kernel_sizes", "=", "(", "3", ",", "4", ",", "5", ")", ",", "\n", "kernel_num", "=", "10", ",", "\n", "dropout", "=", "0.1", ",", "\n", "class_num", "=", "2", ")", ":", "\n", "\n", "        ", "super", "(", "CNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "input_size", ",", "embedding_dim", ")", "#embedding layer", "\n", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv2d", "(", "1", ",", "kernel_num", ",", "(", "size", ",", "embedding_dim", ")", ")", "for", "size", "in", "kernel_sizes", "]", ")", "#list of convolution layers with different kernel sizes", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "#dropout layer", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "kernel_num", "*", "len", "(", "kernel_sizes", ")", ",", "class_num", ")", "#fully connected layer", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN.forward": [[33, 44], ["cnn_helpers.CNN.embedding", "emb.unsqueeze.unsqueeze.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "cnn_helpers.CNN.dropout", "cnn_helpers.CNN.fc", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "conv", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "i.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "#Shape of x: (batch_size, sequence_length)", "\n", "        ", "emb", "=", "self", ".", "embedding", "(", "x", ")", "#Embed word sequence: (batch_size, sequence_length, embedding_dim)", "\n", "emb", "=", "emb", ".", "unsqueeze", "(", "1", ")", "#Add dimension: (batch_size, 1, sequence_length, embedding_dim)", "\n", "y_conv", "=", "[", "conv", "(", "emb", ")", "for", "conv", "in", "self", ".", "convs", "]", "#convolution for all kernel sizes: (batch_size, kernel_num, kernel_appl_num, 1) \"kernel_appl_num\" = how many times has the kernel fit the data based on size and stride", "\n", "y_relu", "=", "[", "F", ".", "relu", "(", "c", ")", ".", "squeeze", "(", "3", ")", "for", "c", "in", "y_conv", "]", "#ReLu over conv-results and remove last dim: (batch_size, kernel_num, kernel_appl_num)", "\n", "y_mp", "=", "[", "F", ".", "max_pool1d", "(", "i", ",", "i", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "for", "i", "in", "y_relu", "]", "#max pool relu results: (batch_size, kernel_num)", "\n", "y_cat", "=", "torch", ".", "cat", "(", "y_mp", ",", "1", ")", "#concatenate max-pooled results for all kernel sizes: (batch_size, kernel_num*len(kernel_sizes))", "\n", "y_do", "=", "self", ".", "dropout", "(", "y_cat", ")", "#dropout: (batch_size, kernel_num*len(kernel_sizes))", "\n", "o", "=", "self", ".", "fc", "(", "y_do", ")", "#apply linear layer: (batch_size, class_num)", "\n", "o", "=", "F", ".", "log_softmax", "(", "o", ",", "dim", "=", "1", ")", "\n", "return", "o", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.Dataset.__init__": [[48, 90], ["list", "tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm.set_description", "text.lower.lower.strip", "data_preprocessed.append", "enumerate", "text.lower.lower.lower", "t.split", "nltk.word_tokenize", "collections.Counter().most_common", "collections.Counter"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "data", ",", "\n", "labels", ",", "\n", "discard_punct", "=", "False", ",", "\n", "separate_punct", "=", "True", ",", "\n", "lowercase", "=", "True", ",", "\n", "word_to_ix", "=", "None", ",", "\n", "min_occur", "=", "1", ",", "\n", "vocab_size", "=", "10000", ",", "\n", "print_progress", "=", "True", ",", "\n", "print_description", "=", "'Building dataset'", ")", ":", "\n", "\n", "        ", "data_pbar", "=", "data", "\n", "if", "print_progress", ":", "\n", "            ", "data_pbar", "=", "tqdm", "(", "data", ")", "\n", "data_pbar", ".", "set_description", "(", "print_description", ")", "\n", "\n", "", "data_preprocessed", "=", "[", "]", "\n", "\n", "for", "text", "in", "data_pbar", ":", "\n", "            ", "if", "discard_punct", ":", "\n", "                ", "text", "=", "''", ".", "join", "(", "[", "char", "for", "char", "in", "text", "if", "char", "not", "in", "string", ".", "punctuation", "]", ")", "\n", "", "elif", "separate_punct", ":", "\n", "                ", "text", "=", "' '", ".", "join", "(", "word_tokenize", "(", "text", ")", ")", "\n", "", "if", "lowercase", ":", "\n", "                ", "text", "=", "text", ".", "lower", "(", ")", "\n", "", "text", "=", "text", ".", "strip", "(", ")", "\n", "data_preprocessed", ".", "append", "(", "text", ")", "\n", "\n", "", "self", ".", "data", "=", "data_preprocessed", "\n", "self", ".", "labels", "=", "list", "(", "labels", ")", "\n", "\n", "if", "not", "word_to_ix", ":", "\n", "            ", "vocab", "=", "[", "t", ".", "split", "(", ")", "for", "t", "in", "self", ".", "data", "]", "\n", "vocab", "=", "[", "w", "for", "l", "in", "vocab", "for", "w", "in", "l", "]", "\n", "vocab", "=", "[", "w", "for", "(", "w", ",", "c", ")", "in", "Counter", "(", "vocab", ")", ".", "most_common", "(", ")", "[", ":", "vocab_size", "]", "if", "c", ">=", "min_occur", "]", "\n", "word_to_ix", "=", "{", "'<PAD>'", ":", "0", ",", "'<UNK>'", ":", "1", "}", "\n", "for", "i", ",", "w", "in", "enumerate", "(", "vocab", ")", ":", "\n", "                ", "word_to_ix", "[", "w", "]", "=", "i", "+", "2", "\n", "\n", "", "", "self", ".", "word_to_ix", "=", "word_to_ix", "\n", "self", ".", "ix_to_word", "=", "{", "word_to_ix", "[", "w", "]", ":", "w", "for", "w", "in", "word_to_ix", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.Dataset.__len__": [[91, 93], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.Dataset.__getitem__": [[94, 96], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "data", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.__init__": [[251, 264], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "model_dir", ",", "\n", "data_dir", ",", "\n", "results_filename", ",", "\n", "random_state", ")", ":", "\n", "\n", "        ", "self", ".", "model_dir", "=", "model_dir", "\n", "self", ".", "data_dir", "=", "data_dir", "\n", "self", ".", "results_filename", "=", "results_filename", "\n", "self", ".", "random_state", "=", "random_state", "\n", "\n", "self", ".", "word_to_ix", "=", "None", "\n", "self", ".", "model", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.train_model": [[265, 270], ["cnn_helpers.Dataset", "cnn_helpers.train_cnn"], "methods", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.train_cnn"], ["", "def", "train_model", "(", "self", ",", "x", ",", "y", ",", "num_epochs", "=", "2", ")", ":", "\n", "        ", "train_dataset", "=", "Dataset", "(", "x", ",", "y", ")", "\n", "self", ".", "word_to_ix", "=", "train_dataset", ".", "word_to_ix", "\n", "self", ".", "model", "=", "train_cnn", "(", "train_dataset", "=", "train_dataset", ",", "epochs", "=", "num_epochs", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.save_model": [[271, 276], ["os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "cnn_helpers.CNN_wrapper.model.state_dict", "filename.split"], "methods", ["None"], ["", "def", "save_model", "(", "self", ",", "filename", "=", "'model'", ")", ":", "\n", "        ", "if", "filename", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", "!=", "'pt'", ":", "\n", "            ", "filename", "+=", "'.pt'", "\n", "", "fpath", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "filename", ")", "\n", "torch", ".", "save", "(", "self", ".", "model", ".", "state_dict", "(", ")", ",", "fpath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.load_model": [[277, 286], ["os.path.join", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "saved_state_dict[].size", "cnn_helpers.CNN", "CNN.load_state_dict", "filename.split"], "methods", ["None"], ["", "def", "load_model", "(", "self", ",", "filename", "=", "'model'", ")", ":", "\n", "        ", "if", "filename", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", "!=", "'pt'", ":", "\n", "            ", "filename", "+=", "'.pt'", "\n", "", "fpath", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "filename", ")", "\n", "saved_state_dict", "=", "torch", ".", "load", "(", "fpath", ")", "\n", "input_size", ",", "embedding_dim", "=", "saved_state_dict", "[", "'embedding.weight'", "]", ".", "size", "(", ")", "\n", "model", "=", "CNN", "(", "input_size", "=", "input_size", ",", "embedding_dim", "=", "embedding_dim", ")", "\n", "model", ".", "load_state_dict", "(", "saved_state_dict", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.predict_proba": [[287, 292], ["cnn_helpers.Dataset", "cnn_helpers.predict_cnn"], "methods", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.predict_cnn"], ["", "def", "predict_proba", "(", "self", ",", "x", ")", ":", "\n", "        ", "default_labels", "=", "[", "0", "for", "d", "in", "x", "]", "\n", "test_dataset", "=", "Dataset", "(", "x", ",", "default_labels", ",", "word_to_ix", "=", "self", ".", "word_to_ix", ")", "\n", "y_pred_prob", "=", "predict_cnn", "(", "model", "=", "self", ".", "model", ",", "test_dataset", "=", "test_dataset", ",", "word_to_ix", "=", "self", ".", "word_to_ix", ")", "\n", "return", "y_pred_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.predict": [[293, 297], ["cnn_helpers.CNN_wrapper.predict_proba", "l.index", "max"], "methods", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.predict_proba"], ["", "def", "predict", "(", "self", ",", "x", ")", ":", "\n", "        ", "y_pred_prob", "=", "self", ".", "predict_proba", "(", "x", ")", "\n", "y_pred", "=", "[", "l", ".", "index", "(", "max", "(", "l", ")", ")", "for", "l", "in", "y_pred_prob", "]", "\n", "return", "y_pred", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.pad_batch": [[98, 100], ["range", "len"], "function", ["None"], ["", "", "def", "pad_batch", "(", "batch", ",", "size", ",", "pad_ix", "=", "0", ")", ":", "\n", "    ", "return", "[", "l", "+", "[", "pad_ix", "for", "i", "in", "range", "(", "size", "-", "len", "(", "l", ")", ")", "]", "for", "l", "in", "batch", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.preprocess_batch": [[102, 108], ["max", "max", "cnn_helpers.pad_batch", "len", "t.split"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.pad_batch"], ["", "def", "preprocess_batch", "(", "batch", ",", "word_to_ix", ",", "pad_ix", "=", "0", ",", "min_pad", "=", "5", ",", "max_pad", "=", "None", ")", ":", "\n", "    ", "batch", "=", "[", "[", "word_to_ix", "[", "w", "]", "if", "w", "in", "word_to_ix", "else", "word_to_ix", "[", "'<UNK>'", "]", "for", "w", "in", "t", ".", "split", "(", ")", "]", "[", ":", "max_pad", "]", "for", "t", "in", "batch", "]", "\n", "max_len", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "batch", "]", ")", "\n", "max_len", "=", "max", "(", "min_pad", ",", "max_len", ")", "\n", "batch", "=", "pad_batch", "(", "batch", ",", "max_len", ",", "pad_ix", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.train_cnn": [[110, 216], ["torch.NLLLoss", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "cnn_helpers.CNN", "CNN.to", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "range", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "CNN.parameters", "CNN.train", "train_losses.append", "train_accuracies.append", "print", "len", "tqdm.tqdm", "tqdm.tqdm.set_description", "cnn_helpers.preprocess_batch", "torch.tensor().to.to", "torch.optim.Adam.zero_grad", "CNN.", "nn.NLLLoss.", "loss_ftion.backward", "torch.optim.Adam.step", "loss_ftion.data.item", "sklearn.metrics.accuracy_score", "CNN.eval", "tqdm.tqdm", "tqdm.tqdm.set_description", "len", "len", "dev_losses.append", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "l.tolist().index", "list", "cnn_helpers.preprocess_batch", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "CNN.", "nn.NLLLoss.", "loss_ftion.data.item", "len", "str", "max", "torch.tensor().to.cpu", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "l.tolist().index", "torch.tensor", "torch.tensor", "torch.tensor", "l.tolist", "l.tolist", "str", "torch.tensor", "torch.tensor", "torch.tensor", "max", "torch.tensor", "torch.tensor", "torch.tensor", "l.tolist", "l.tolist"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.train", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.preprocess_batch", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.preprocess_batch"], ["", "def", "train_cnn", "(", "train_dataset", ",", "\n", "dev_dataset", "=", "None", ",", "\n", "pad_ix", "=", "0", ",", "\n", "max_pad", "=", "None", ",", "\n", "embedding_dim", "=", "300", ",", "\n", "kernel_sizes", "=", "(", "3", ",", "4", ",", "5", ")", ",", "\n", "kernel_num", "=", "10", ",", "\n", "dropout", "=", "0.1", ",", "\n", "learning_rate", "=", "0.001", ",", "\n", "learning_rate_decay", "=", "0", ",", "\n", "epochs", "=", "3", ",", "\n", "batch_size", "=", "32", ",", "\n", "device", "=", "'cuda'", ",", "\n", "progress_bar", "=", "True", ")", ":", "\n", "\n", "    ", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "device", "=", "'cpu'", "\n", "\n", "", "loss_ftion", "=", "nn", ".", "NLLLoss", "(", ")", "\n", "\n", "train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "dev_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dev_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "if", "dev_dataset", "else", "None", "\n", "\n", "model", "=", "CNN", "(", "input_size", "=", "len", "(", "train_dataset", ".", "word_to_ix", ")", ",", "\n", "embedding_dim", "=", "embedding_dim", ",", "\n", "kernel_sizes", "=", "kernel_sizes", ",", "\n", "kernel_num", "=", "kernel_num", ",", "\n", "dropout", "=", "dropout", ",", "\n", "class_num", "=", "2", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "learning_rate", ",", "weight_decay", "=", "learning_rate_decay", ")", "\n", "\n", "train_losses", "=", "[", "]", "\n", "train_accuracies", "=", "[", "]", "\n", "dev_losses", "=", "[", "]", "\n", "\n", "for", "epoch", "in", "range", "(", "epochs", ")", ":", "\n", "        ", "epoch", "+=", "1", "\n", "\n", "model", ".", "train", "(", ")", "# Activate dropout", "\n", "avg_train_loss", "=", "0", "\n", "avg_train_acc", "=", "0", "\n", "\n", "train_pbar", "=", "train_dataloader", "\n", "if", "progress_bar", ":", "\n", "            ", "train_pbar", "=", "tqdm", "(", "train_dataloader", ")", "\n", "train_pbar", ".", "set_description", "(", "\"Training CNN: epoch \"", "+", "str", "(", "epoch", ")", ")", "\n", "\n", "#       Training", "\n", "", "for", "batch", ",", "tgt", "in", "train_pbar", ":", "\n", "\n", "            ", "batch", "=", "preprocess_batch", "(", "batch", ",", "word_to_ix", "=", "train_dataset", ".", "word_to_ix", ",", "pad_ix", "=", "pad_ix", ",", "max_pad", "=", "max_pad", ")", "\n", "\n", "tgt", "=", "tgt", ".", "to", "(", "device", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "clf", "=", "model", "(", "torch", ".", "tensor", "(", "batch", ")", ".", "to", "(", "device", ")", ")", "\n", "loss", "=", "loss_ftion", "(", "clf", ",", "tgt", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "avg_train_loss", "+=", "loss", ".", "data", ".", "item", "(", ")", "\n", "preds", "=", "[", "l", ".", "tolist", "(", ")", ".", "index", "(", "max", "(", "l", ".", "tolist", "(", ")", ")", ")", "for", "l", "in", "clf", "]", "\n", "acc", "=", "accuracy_score", "(", "list", "(", "tgt", ".", "cpu", "(", ")", ")", ",", "preds", ")", "\n", "avg_train_acc", "+=", "acc", "\n", "\n", "#       Validation (if validation dataset is given)", "\n", "", "avg_dev_loss", "=", "0", "\n", "\n", "if", "dev_dataloader", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "\n", "dev_pbar", "=", "tqdm", "(", "dev_dataloader", ")", "\n", "dev_pbar", ".", "set_description", "(", "'Epoch '", "+", "str", "(", "epoch", ")", "+", "' validation'", ")", "\n", "\n", "for", "batch", ",", "tgt", "in", "dev_pbar", ":", "\n", "\n", "                ", "batch", "=", "preprocess_batch", "(", "batch", ",", "word_to_ix", "=", "train_dataset", ".", "word_to_ix", ",", "pad_ix", "=", "pad_ix", ",", "max_pad", "=", "max_pad", ")", "\n", "\n", "tgt", "=", "torch", ".", "tensor", "(", "tgt", ")", ".", "to", "(", "device", ")", "\n", "clf", "=", "model", "(", "torch", ".", "tensor", "(", "batch", ")", ".", "to", "(", "device", ")", ")", "\n", "loss", "=", "loss_ftion", "(", "clf", ",", "tgt", ")", "\n", "avg_dev_loss", "+=", "loss", ".", "data", ".", "item", "(", ")", "\n", "preds", "=", "[", "l", ".", "tolist", "(", ")", ".", "index", "(", "max", "(", "l", ".", "tolist", "(", ")", ")", ")", "for", "l", "in", "clf", "]", "\n", "\n", "", "", "avg_train_loss", "=", "avg_train_loss", "/", "len", "(", "train_dataloader", ")", "\n", "avg_train_acc", "=", "avg_train_acc", "/", "len", "(", "train_dataloader", ")", "\n", "\n", "if", "dev_dataloader", ":", "\n", "            ", "avg_dev_loss", "=", "avg_dev_loss", "/", "len", "(", "dev_dataloader", ")", "\n", "\n", "", "if", "dev_losses", ":", "\n", "            ", "if", "avg_dev_loss", ">", "dev_losses", "[", "-", "1", "]", ":", "\n", "                ", "break", "\n", "", "", "elif", "train_losses", "and", "avg_train_loss", ">", "train_losses", "[", "-", "1", "]", ":", "\n", "            ", "break", "\n", "", "elif", "avg_train_acc", "==", "1", ":", "\n", "            ", "break", "\n", "\n", "", "train_losses", ".", "append", "(", "avg_train_loss", ")", "\n", "train_accuracies", ".", "append", "(", "avg_train_acc", ")", "\n", "if", "dev_dataloader", ":", "\n", "            ", "dev_losses", ".", "append", "(", "avg_dev_loss", ")", "\n", "\n", "", "print", "(", "'Train loss:'", ",", "avg_train_loss", ",", "\"Dev loss:\"", ",", "avg_dev_loss", ")", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.predict_cnn": [[218, 247], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.to", "model.eval", "torch.exp().tolist", "torch.exp().tolist", "torch.exp().tolist", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "tqdm.tqdm", "tqdm.tqdm.set_description", "cnn_helpers.preprocess_batch", "model", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "l.tolist", "torch.exp", "torch.exp", "torch.exp", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.preprocess_batch"], ["", "def", "predict_cnn", "(", "model", ",", "\n", "test_dataset", ",", "\n", "word_to_ix", ",", "\n", "pad_ix", "=", "0", ",", "\n", "max_pad", "=", "None", ",", "\n", "batch_size", "=", "32", ",", "\n", "device", "=", "'cpu'", ",", "\n", "progress_bar", "=", "False", ")", ":", "\n", "\n", "    ", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "device", "=", "'cpu'", "\n", "\n", "", "test_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "test_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "test_pbar", "=", "test_dataloader", "\n", "if", "progress_bar", ":", "\n", "        ", "test_pbar", "=", "tqdm", "(", "test_dataloader", ")", "\n", "test_pbar", ".", "set_description", "(", "'Running test'", ")", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "preds", "=", "[", "]", "\n", "for", "batch", ",", "tgt", "in", "test_pbar", ":", "\n", "        ", "batch", "=", "preprocess_batch", "(", "batch", ",", "word_to_ix", "=", "word_to_ix", ",", "pad_ix", "=", "pad_ix", ",", "max_pad", "=", "max_pad", ")", "\n", "clf", "=", "model", "(", "torch", ".", "tensor", "(", "batch", ")", ".", "to", "(", "device", ")", ")", "\n", "preds", "+=", "[", "l", ".", "tolist", "(", ")", "for", "l", "in", "clf", "]", "\n", "\n", "", "return", "torch", ".", "exp", "(", "torch", ".", "tensor", "(", "preds", ")", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.__init__": [[46, 68], ["augmentation_lm.LanguageModelWrapper.load_corpus_fun", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "load_corpus_fun", ",", "\n", "model_save_dir", ":", "str", ",", "\n", "random_state", "=", "20200303", ")", ":", "\n", "\n", "        ", "self", ".", "load_corpus_fun", "=", "load_corpus_fun", "\n", "self", ".", "model_save_dir", "=", "model_save_dir", "\n", "self", ".", "random_state", "=", "random_state", "\n", "\n", "# cache_dir", "\n", "self", ".", "cache_dir", "=", "'./cache'", "\n", "\n", "# some default parameter values", "\n", "self", ".", "max_length", "=", "100", "\n", "self", ".", "temperature", "=", "1.", "\n", "self", ".", "top_k", "=", "None", "\n", "self", ".", "top_p", "=", "0.9", "\n", "self", ".", "stop_token", "=", "'<|endoftext|>'", "\n", "\n", "corpus", "=", "self", ".", "load_corpus_fun", "(", ")", "\n", "self", ".", "corpus_size", "=", "len", "(", "corpus", ")", "\n", "del", "corpus", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.train": [[70, 101], ["augmentation_lm.LanguageModelWrapper.set_seed", "augmentation_lm.LanguageModelWrapper.load_corpus_fun", "os.system", "os.remove", "open", "f.write", "len"], "methods", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.set_seed"], ["", "def", "train", "(", "self", ",", "num_epochs", "=", "2", ")", ":", "\n", "\n", "        ", "self", ".", "set_seed", "(", "self", ".", "random_state", ")", "\n", "\n", "# reload corpus, prepare all files", "\n", "# assumption: loading function does all necessary preprocessing to lines", "\n", "# corpus is a list of strings, or pandas.Series", "\n", "corpus", "=", "self", ".", "load_corpus_fun", "(", ")", "\n", "self", ".", "text_dir", "=", "'temp.txt'", "\n", "with", "open", "(", "self", ".", "text_dir", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "corpus", ":", "\n", "                ", "f", ".", "write", "(", "'%s\\n'", "%", "line", ")", "\n", "\n", "# run huggingface code", "\n", "#        \"--fp16 O1\"", "\n", "# code directly adapted from https://github.com/huggingface/transformers/blob/master/examples/run_generation.py", "\n", "", "", "syscall", "=", "(", "\"python ./gpt2_scripts/run_language_modeling.py \"", "\n", "\"--model_type=gpt2 --model_name_or_path=gpt2 \"", "\n", "\"--do_train --overwrite_output_dir --line_by_line \"", "\n", "\"--per_device_train_batch_size 1 --learning_rate 2e-5 \"", "\n", "\" --output_dir=%s \"", "\n", "\" --train_data_file=%s \"", "\n", "\"--num_train_epochs %d \"", "\n", "\"--save_steps %d \"", "\n", "\"--seed %d\"", ")", "%", "(", "self", ".", "model_save_dir", ",", "self", ".", "text_dir", ",", "num_epochs", ",", "\n", "len", "(", "corpus", ")", "*", "10", ",", "self", ".", "random_state", ")", "\n", "os", ".", "system", "(", "syscall", ")", "\n", "\n", "# clean up", "\n", "os", ".", "remove", "(", "self", ".", "text_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.set_seed": [[102, 107], ["numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "methods", ["None"], ["", "def", "set_seed", "(", "self", ",", "seed", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", ":", "\n", "            ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.generate": [[109, 185], ["torch.no_grad", "augmentation_lm.LanguageModelWrapper.set_seed", "transformers.GPT2Tokenizer.from_pretrained", "transformers.GPT2LMHeadModel.from_pretrained", "transformers.GPT2LMHeadModel.from_pretrained.to", "transformers.GPT2Tokenizer.from_pretrained.encode", "encoded_prompt.to.to.to", "transformers.GPT2LMHeadModel.from_pretrained.generate", "enumerate", "augmentation_lm.filter_dataset", "len", "len", "[].find", "len", "model_class.from_pretrained.generate.squeeze_", "generated_sequence.tolist.tolist.tolist", "transformers.GPT2Tokenizer.from_pretrained.decode", "filter_dataset.append", "encoded_prompt.to.to.size", "len", "len", "len", "tokenizer_class.from_pretrained.decode.find", "transformers.GPT2Tokenizer.from_pretrained.decode"], "methods", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.set_seed", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.generate", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.filter_dataset"], ["", "", "def", "generate", "(", "self", ",", "input", ":", "str", ",", "num_outputs", ":", "int", ",", "\n", "epoch_model", "=", "2", ",", "\n", "device", "=", "'cuda'", ",", "\n", "length", "=", "None", ",", "\n", "seed", "=", "20200505", ")", ":", "\n", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "            ", "self", ".", "set_seed", "(", "seed", ")", "\n", "\n", "# ft_model_dir = os.path.join(self.model_save_dir,'checkpoint-%d'%(25*epoch_model))", "\n", "# ft_model_dir = os.path.join(self.model_save_dir,'checkpoint-%d'%(self.corpus_size*epoch_model))", "\n", "ft_model_dir", "=", "self", ".", "model_save_dir", "\n", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "self", ".", "model_save_dir", ")", "\n", "\n", "# possibly this one needs to be updated", "\n", "model", "=", "model_class", ".", "from_pretrained", "(", "ft_model_dir", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "length", "is", "None", ":", "\n", "                ", "length", "=", "len", "(", "input", ")", "\n", "", "if", "length", ">", "self", ".", "max_length", ":", "\n", "                ", "length", "=", "self", ".", "max_length", "\n", "\n", "", "if", "len", "(", "input", ")", ">", "self", ".", "max_length", ":", "\n", "# find the last whole word before max_len encountered", "\n", "# to avoid splitting subwords at the start of generation", "\n", "                ", "last_space", "=", "input", "[", ":", "self", ".", "max_length", "]", "[", ":", ":", "-", "1", "]", ".", "find", "(", "' '", ")", "\n", "input", "=", "input", "[", ":", "self", ".", "max_length", "-", "last_space", "]", "\n", "\n", "", "encoded_prompt", "=", "tokenizer", ".", "encode", "(", "input", ",", "add_special_tokens", "=", "False", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "encoded_prompt", "=", "encoded_prompt", ".", "to", "(", "device", ")", "\n", "\n", "if", "encoded_prompt", ".", "size", "(", ")", "[", "-", "1", "]", "==", "0", ":", "\n", "                ", "input_ids", "=", "None", "\n", "", "else", ":", "\n", "                ", "input_ids", "=", "encoded_prompt", "\n", "\n", "", "output_sequences", "=", "model", ".", "generate", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "max_length", "=", "length", "+", "len", "(", "input_ids", "[", "0", "]", ")", ",", "\n", "temperature", "=", "self", ".", "temperature", ",", "\n", "top_k", "=", "self", ".", "top_k", ",", "\n", "top_p", "=", "self", ".", "top_p", ",", "\n", "repetition_penalty", "=", "1.0", ",", "\n", "do_sample", "=", "True", ",", "\n", "num_return_sequences", "=", "num_outputs", ",", "\n", ")", "\n", "\n", "# remove the batch dimension when returning multiple sequences", "\n", "if", "len", "(", "output_sequences", ".", "shape", ")", ">", "2", ":", "\n", "                ", "output_sequences", ".", "squeeze_", "(", ")", "\n", "\n", "", "generated_sequences", "=", "[", "]", "\n", "\n", "for", "generated_sequence_idx", ",", "generated_sequence", "in", "enumerate", "(", "output_sequences", ")", ":", "\n", "                ", "generated_sequence", "=", "generated_sequence", ".", "tolist", "(", ")", "\n", "\n", "# Decode text", "\n", "text", "=", "tokenizer", ".", "decode", "(", "generated_sequence", ",", "clean_up_tokenization_spaces", "=", "True", ")", "\n", "\n", "# Remove all text after the stop token", "\n", "text", "=", "text", "[", ":", "text", ".", "find", "(", "self", ".", "stop_token", ")", "if", "self", ".", "stop_token", "else", "None", "]", "\n", "\n", "# Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing", "\n", "total_sequence", "=", "(", "\n", "# input_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]", "\n", "text", "[", "len", "(", "tokenizer", ".", "decode", "(", "encoded_prompt", "[", "0", "]", ",", "clean_up_tokenization_spaces", "=", "True", ")", ")", ":", "]", "\n", ")", "\n", "\n", "generated_sequences", ".", "append", "(", "total_sequence", ")", "\n", "\n", "", "generated_sequences", "=", "filter_dataset", "(", "generated_sequences", ",", "[", "1", "]", "*", "len", "(", "generated_sequences", ")", ")", "\n", "\n", "return", "generated_sequences", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.apply": [[18, 23], ["collections.deque", "list", "collections.deque.append", "fun"], "function", ["None"], ["def", "apply", "(", "documents", ",", "fun", ")", ":", "\n", "    ", "result", "=", "deque", "(", ")", "\n", "for", "doc", "in", "documents", ":", "\n", "        ", "result", ".", "append", "(", "fun", "(", "doc", ")", ")", "\n", "", "return", "list", "(", "result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.filter_dataset": [[25, 37], ["collections.deque", "zip", "list", "line.replace().replace().strip().split.replace().replace().strip().split", "collections.deque.append", "line.replace().replace().strip().split.replace().replace().strip", "line.replace().replace().strip().split.replace().replace", "line.replace().replace().strip().split.replace"], "function", ["None"], ["", "def", "filter_dataset", "(", "X", ",", "y", ",", "target_classes", "=", "[", "1", "]", ")", ":", "\n", "    ", "dq", "=", "deque", "(", ")", "\n", "for", "line", ",", "cls", "in", "zip", "(", "X", ",", "y", ")", ":", "\n", "        ", "if", "cls", "in", "target_classes", ":", "\n", "            ", "line", "=", "line", ".", "replace", "(", "'\\n'", ",", "' '", ")", ".", "replace", "(", "'\\t'", ",", "' '", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "line", "=", "' '", ".", "join", "(", "line", ")", "\n", "dq", ".", "append", "(", "line", ")", "\n", "", "", "return", "list", "(", "dq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.load_lm_corpus": [[39, 42], ["data_helpers.load_categorical_dataset", "augmentation_lm.filter_dataset"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.src.data_helpers.load_categorical_dataset", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.filter_dataset"], ["", "def", "load_lm_corpus", "(", "dataset_name", ",", "target_classes", "=", "[", "1", "]", ")", ":", "\n", "    ", "X", ",", "y", "=", "load_categorical_dataset", "(", "dataset_name", ")", "\n", "return", "filter_dataset", "(", "X", ",", "y", ",", "target_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.lm_aug": [[187, 202], ["collections.deque", "lm_wrapper.generate", "list", "collections.deque.append", "collections.deque.append"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.generate"], ["", "", "", "def", "lm_aug", "(", "document", ",", "num_aug", "=", "20", ",", "random_state", "=", "20200303", ",", "\n", "incl_orig", "=", "True", ",", "epoch", "=", "2", ",", "lm_wrapper", "=", "None", ")", ":", "\n", "\n", "    ", "assert", "lm_wrapper", "is", "not", "None", "\n", "\n", "return_vals", "=", "deque", "(", ")", "\n", "if", "incl_orig", ":", "\n", "        ", "num_aug", "-=", "1", "\n", "return_vals", ".", "append", "(", "document", ")", "\n", "\n", "", "generated", "=", "lm_wrapper", ".", "generate", "(", "document", ",", "num_aug", ",", "seed", "=", "random_state", ",", "epoch_model", "=", "epoch", ")", "\n", "for", "sent", "in", "generated", ":", "\n", "        ", "return_vals", ".", "append", "(", "sent", ")", "\n", "\n", "", "return", "list", "(", "return_vals", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.run_language_modeling.get_dataset": [[128, 135], ["transformers.LineByLineTextDataset", "transformers.TextDataset"], "function", ["None"], ["", "def", "get_dataset", "(", "args", ":", "DataTrainingArguments", ",", "tokenizer", ":", "PreTrainedTokenizer", ",", "evaluate", "=", "False", ")", ":", "\n", "    ", "file_path", "=", "args", ".", "eval_data_file", "if", "evaluate", "else", "args", ".", "train_data_file", "\n", "if", "args", ".", "line_by_line", ":", "\n", "        ", "return", "LineByLineTextDataset", "(", "tokenizer", "=", "tokenizer", ",", "file_path", "=", "file_path", ",", "block_size", "=", "args", ".", "block_size", ")", "\n", "", "else", ":", "\n", "        ", "return", "TextDataset", "(", "\n", "tokenizer", "=", "tokenizer", ",", "file_path", "=", "file_path", ",", "block_size", "=", "args", ".", "block_size", ",", "overwrite_cache", "=", "args", ".", "overwrite_cache", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.run_language_modeling.main": [[138, 288], ["transformers.HfArgumentParser", "transformers.HfArgumentParser.parse_args_into_dataclasses", "logging.basicConfig", "logger.warning", "logger.info", "transformers.set_seed", "AutoModelWithLMHead.from_config.resize_token_embeddings", "transformers.Trainer", "ValueError", "os.path.exists", "os.listdir", "ValueError", "bool", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModelWithLMHead.from_pretrained", "logger.info", "transformers.AutoModelWithLMHead.from_config", "len", "ValueError", "min", "run_language_modeling.get_dataset", "run_language_modeling.get_dataset", "DataCollatorForPermutationLanguageModeling", "transformers.DataCollatorForLanguageModeling", "transformers.Trainer.train", "transformers.Trainer.save_model", "transformers.Trainer.is_world_master", "logger.info", "transformers.Trainer.evaluate", "math.exp", "os.path.join", "transformers.Trainer.is_world_master", "results.update", "transformers.AutoConfig.from_pretrained", "logger.warning", "transformers.AutoTokenizer.from_pretrained", "ValueError", "AutoTokenizer.from_pretrained.save_pretrained", "bool", "os.path.isdir", "open", "logger.info", "sorted", "result.keys", "logger.info", "writer.write", "str", "str"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.set_seed", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.run_language_modeling.get_dataset", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.run_language_modeling.get_dataset", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.augmentation_lm.LanguageModelWrapper.train", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.cnn_scripts.cnn_helpers.CNN_wrapper.save_model"], ["", "", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", ")", "\n", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "if", "data_args", ".", "eval_data_file", "is", "None", "and", "training_args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"", "\n", "\"or remove the --do_eval argument.\"", "\n", ")", "\n", "\n", "", "if", "(", "\n", "os", ".", "path", ".", "exists", "(", "training_args", ".", "output_dir", ")", "\n", "and", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", "\n", "and", "training_args", ".", "do_train", "\n", "and", "not", "training_args", ".", "overwrite_output_dir", "\n", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "training_args", ".", "local_rank", "in", "[", "-", "1", ",", "0", "]", "else", "logging", ".", "WARN", ",", "\n", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "training_args", ".", "local_rank", ",", "\n", "training_args", ".", "device", ",", "\n", "training_args", ".", "n_gpu", ",", "\n", "bool", "(", "training_args", ".", "local_rank", "!=", "-", "1", ")", ",", "\n", "training_args", ".", "fp16", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "\n", "if", "model_args", ".", "config_name", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "config_name", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "CONFIG_MAPPING", "[", "model_args", ".", "model_type", "]", "(", ")", "\n", "logger", ".", "warning", "(", "\"You are instantiating a new config instance from scratch.\"", ")", "\n", "\n", "", "if", "model_args", ".", "tokenizer_name", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "tokenizer_name", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ",", "cache_dir", "=", "model_args", ".", "cache_dir", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"", "\n", "\"and load it from here, using --tokenizer_name\"", "\n", ")", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "model", "=", "AutoModelWithLMHead", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training new model from scratch\"", ")", "\n", "model", "=", "AutoModelWithLMHead", ".", "from_config", "(", "config", ")", "\n", "\n", "", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "\n", "if", "config", ".", "model_type", "in", "[", "\"bert\"", ",", "\"roberta\"", ",", "\"distilbert\"", ",", "\"camembert\"", "]", "and", "not", "data_args", ".", "mlm", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the\"", "\n", "\"--mlm flag (masked language modeling).\"", "\n", ")", "\n", "\n", "", "if", "data_args", ".", "block_size", "<=", "0", ":", "\n", "        ", "data_args", ".", "block_size", "=", "tokenizer", ".", "max_len", "\n", "# Our input block size will be the max possible for the model", "\n", "", "else", ":", "\n", "        ", "data_args", ".", "block_size", "=", "min", "(", "data_args", ".", "block_size", ",", "tokenizer", ".", "max_len", ")", "\n", "\n", "# Get datasets", "\n", "\n", "", "train_dataset", "=", "get_dataset", "(", "data_args", ",", "tokenizer", "=", "tokenizer", ")", "if", "training_args", ".", "do_train", "else", "None", "\n", "eval_dataset", "=", "get_dataset", "(", "data_args", ",", "tokenizer", "=", "tokenizer", ",", "evaluate", "=", "True", ")", "if", "training_args", ".", "do_eval", "else", "None", "\n", "if", "config", ".", "model_type", "==", "\"xlnet\"", ":", "\n", "        ", "data_collator", "=", "DataCollatorForPermutationLanguageModeling", "(", "\n", "tokenizer", "=", "tokenizer", ",", "plm_probability", "=", "data_args", ".", "plm_probability", ",", "max_span_length", "=", "data_args", ".", "max_span_length", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "data_collator", "=", "DataCollatorForLanguageModeling", "(", "\n", "tokenizer", "=", "tokenizer", ",", "mlm", "=", "data_args", ".", "mlm", ",", "mlm_probability", "=", "data_args", ".", "mlm_probability", "\n", ")", "\n", "\n", "# Initialize our Trainer", "\n", "", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "train_dataset", "=", "train_dataset", ",", "\n", "eval_dataset", "=", "eval_dataset", ",", "\n", "prediction_loss_only", "=", "True", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "model_path", "=", "(", "\n", "model_args", ".", "model_name_or_path", "\n", "if", "model_args", ".", "model_name_or_path", "is", "not", "None", "and", "os", ".", "path", ".", "isdir", "(", "model_args", ".", "model_name_or_path", ")", "\n", "else", "None", "\n", ")", "\n", "trainer", ".", "train", "(", "model_path", "=", "model_path", ")", "\n", "trainer", ".", "save_model", "(", ")", "\n", "# For convenience, we also re-save the tokenizer to the same directory,", "\n", "# so that you can share your model easily on huggingface.co/models =)", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "tokenizer", ".", "save_pretrained", "(", "training_args", ".", "output_dir", ")", "\n", "\n", "# Evaluation", "\n", "", "", "results", "=", "{", "}", "\n", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "eval_output", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "perplexity", "=", "math", ".", "exp", "(", "eval_output", "[", "\"eval_loss\"", "]", ")", "\n", "result", "=", "{", "\"perplexity\"", ":", "perplexity", "}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"eval_results_lm.txt\"", ")", "\n", "if", "trainer", ".", "is_world_master", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "results", ".", "update", "(", "result", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.run_language_modeling._mp_fn": [[290, 293], ["run_language_modeling.main"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.gpt2_scripts.run_language_modeling.main"], ["", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.ppdb_to_dict": [[20, 30], ["collections.defaultdict", "collections.defaultdict", "[].add", "[].add", "collections.defaultdict", "collections.defaultdict", "context.split", "collections.defaultdict", "collections.defaultdict"], "function", ["None"], ["def", "ppdb_to_dict", "(", "ppdb_list", ",", "num_chars", "=", "3", ")", ":", "\n", "    ", "ppdb_dict", "=", "{", "'comp'", ":", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "set", ")", ")", ")", ",", "'pos'", ":", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "set", ")", ")", ")", "}", "\n", "for", "context", ",", "phr", ",", "par", "in", "ppdb_list", ":", "\n", "        ", "context", "=", "context", "[", "1", ":", "-", "1", "]", "\n", "if", "'/'", "in", "context", ":", "\n", "            ", "comp", "=", "context", ".", "split", "(", "'/'", ")", "[", "1", "]", "\n", "ppdb_dict", "[", "'comp'", "]", "[", "phr", "[", ":", "num_chars", "]", "]", "[", "phr", "]", "[", "comp", "]", ".", "add", "(", "par", ")", "\n", "", "else", ":", "\n", "            ", "ppdb_dict", "[", "'pos'", "]", "[", "phr", "[", ":", "num_chars", "]", "]", "[", "phr", "]", "[", "context", "]", ".", "add", "(", "par", ")", "\n", "", "", "return", "ppdb_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.load_ppdb": [[31, 45], ["augmentation_ppdb.ppdb_to_dict", "os.path.exists", "print", "print", "print", "print", "print", "Exception", "open", "f.readlines", "p.split", "x.strip"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.ppdb_to_dict"], ["", "def", "load_ppdb", "(", "path", "=", "'ppdb_scripts/ppdb_equivalent.txt'", ",", "num_chars", "=", "3", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "print", "(", "'-'", "*", "80", ")", "\n", "print", "(", "path", ",", "'not found.'", ")", "\n", "print", "(", "'Please extract ppdb_equivalent.txt from supplementary data '", ",", "end", "=", "''", ")", "\n", "print", "(", "'and add it to ./src/ppdb_scripts/'", ")", "\n", "print", "(", "'-'", "*", "80", ")", "\n", "raise", "Exception", "(", "'ppdb_equivalent.txt not found'", ")", "\n", "", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "ppdb", "=", "f", ".", "readlines", "(", ")", "\n", "", "ppdb", "=", "[", "p", ".", "split", "(", "'|'", ")", "for", "p", "in", "ppdb", "]", "\n", "ppdb", "=", "[", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "l", "]", "for", "l", "in", "ppdb", "]", "\n", "ppdb", "=", "ppdb_to_dict", "(", "ppdb", ",", "num_chars", "=", "num_chars", ")", "\n", "return", "ppdb", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.subsets": [[46, 48], ["itertools.chain.from_iterable", "itertools.combinations", "range", "len"], "function", ["None"], ["", "def", "subsets", "(", "ls", ")", ":", "\n", "    ", "return", "chain", ".", "from_iterable", "(", "combinations", "(", "ls", ",", "n", ")", "for", "n", "in", "range", "(", "len", "(", "ls", ")", "+", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.flatten": [[49, 51], ["None"], "function", ["None"], ["", "def", "flatten", "(", "ls", ")", ":", "\n", "    ", "return", "[", "item", "for", "l", "in", "ls", "for", "item", "in", "l", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.fix_articles": [[54, 68], ["text.split", "range", "len"], "function", ["None"], ["def", "fix_articles", "(", "text", ")", ":", "\n", "    ", "new_text", "=", "''", "\n", "words", "=", "text", ".", "split", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "words", ")", "-", "1", ")", ":", "\n", "        ", "word", "=", "words", "[", "i", "]", "\n", "next_word", "=", "words", "[", "i", "+", "1", "]", "\n", "if", "word", "==", "'a'", "and", "next_word", "[", "0", "]", "in", "vowels", ":", "\n", "            ", "new_text", "+=", "'an '", "\n", "", "elif", "word", "==", "'an'", "and", "next_word", "[", "0", "]", "not", "in", "vowels", ":", "\n", "            ", "new_text", "+=", "'a '", "\n", "", "else", ":", "\n", "            ", "new_text", "+=", "word", "+", "' '", "\n", "", "", "new_text", "+=", "words", "[", "-", "1", "]", "\n", "return", "new_text", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.all_ngrams": [[69, 74], ["range", "len", "list", "nltk.ngrams"], "function", ["None"], ["", "def", "all_ngrams", "(", "word_list", ")", ":", "\n", "    ", "all_ngr", "=", "[", "word_list", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "word_list", ")", ")", ":", "\n", "        ", "all_ngr", "+=", "list", "(", "ngrams", "(", "word_list", ",", "i", ")", ")", "\n", "", "return", "[", "' '", ".", "join", "(", "ngr", ")", "for", "ngr", "in", "all_ngr", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.strip_punctuation": [[75, 81], ["None"], "function", ["None"], ["", "def", "strip_punctuation", "(", "text", ")", ":", "\n", "    ", "while", "text", "and", "text", "[", "0", "]", "in", "punctuation", ":", "\n", "        ", "text", "=", "text", "[", "1", ":", "]", "\n", "", "while", "text", "and", "text", "[", "-", "1", "]", "in", "punctuation", ":", "\n", "        ", "text", "=", "text", "[", ":", "-", "1", "]", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.all_subtrees": [[89, 104], ["list", "list.index", "range", "len", "len", "subtree_list.append", "subtree_list.append", "t.orth_.lower", "t.orth_.lower"], "function", ["None"], ["def", "all_subtrees", "(", "token", ",", "label", ",", "parser", ")", ":", "\n", "    ", "subtree_list", "=", "[", "]", "\n", "token_subtree", "=", "list", "(", "token", ".", "subtree", ")", "\n", "token_index", "=", "token_subtree", ".", "index", "(", "token", ")", "\n", "beg", ",", "end", "=", "token_subtree", "[", ":", "token_index", "+", "1", "]", ",", "token_subtree", "[", "token_index", "+", "1", ":", "]", "\n", "end_str", "=", "' '", ".", "join", "(", "[", "t", ".", "orth_", ".", "lower", "(", ")", "for", "t", "in", "end", "]", ")", ".", "strip", "(", ")", "\n", "# Modifiers from left to N-head; keep possible right-modifiers intact", "\n", "for", "i", "in", "range", "(", "len", "(", "beg", ")", ")", ":", "\n", "        ", "beg_str", "=", "' '", ".", "join", "(", "[", "t", ".", "orth_", ".", "lower", "(", ")", "for", "t", "in", "beg", "]", ")", ".", "strip", "(", ")", "\n", "if", "len", "(", "beg", ")", ">", "1", ":", "\n", "            ", "subtree_list", ".", "append", "(", "(", "label", ",", "beg_str", ")", ")", "\n", "", "if", "end", ":", "\n", "            ", "subtree_list", ".", "append", "(", "(", "label", ",", "beg_str", "+", "' '", "+", "end_str", ")", ")", "\n", "", "beg", "=", "beg", "[", "1", ":", "]", "\n", "", "return", "subtree_list", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.trees": [[106, 185], ["type", "parser", "t.orth_.lower", "tree_list.append", "list", "set", "tree_list.append", "set", "tree_list.append", "tree_list.append", "list.index", "range", "range", "augmentation_ppdb.strip_punctuation", "tree_list.append", "len", "tree_list.append", "len", "tree_list.append", "tree_list.append", "augmentation_ppdb.all_subtrees", "tree_list.append", "set", "set", "tree_list.append", "augmentation_ppdb.all_subtrees", "tree_list.append", "augmentation_ppdb.all_subtrees", "tree_list.append", "augmentation_ppdb.all_subtrees", "set", "tree_list.append", "t.orth_.lower", "t.orth_.lower", "t.orth_.lower", "t.orth_.lower", "t.orth_.lower"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.strip_punctuation", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.all_subtrees", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.all_subtrees", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.all_subtrees", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.all_subtrees"], ["", "def", "trees", "(", "sent", ",", "parser", ")", ":", "\n", "    ", "if", "type", "(", "sent", ")", "==", "str", ":", "\n", "        ", "sent", "=", "parser", "(", "sent", ")", "\n", "", "sent_words", "=", "[", "t", ".", "orth_", ".", "lower", "(", ")", "for", "t", "in", "sent", "]", "\n", "sent_orth", "=", "' '", ".", "join", "(", "sent_words", ")", ".", "strip", "(", ")", "\n", "tree_list", "=", "[", "]", "\n", "\n", "for", "token", "in", "sent", ":", "\n", "\n", "# Verb head", "\n", "        ", "if", "token", ".", "pos_", "==", "'VERB'", "and", "token", ".", "dep_", "!=", "'aux'", ":", "\n", "            ", "subtree_str", "=", "' '", ".", "join", "(", "[", "t", ".", "orth_", "for", "t", "in", "token", ".", "subtree", "]", ")", ".", "lower", "(", ")", "\n", "tree_list", ".", "append", "(", "(", "'VB'", ",", "token", ".", "orth_", ")", ")", "\n", "\n", "# S/SBAR: Full sentences", "\n", "if", "token", ".", "dep_", "==", "'ROOT'", ":", "\n", "                ", "tree_list", ".", "append", "(", "(", "'S'", ",", "subtree_str", ")", ")", "\n", "", "elif", "token", ".", "dep_", "==", "'ccomp'", ":", "\n", "                ", "tree_list", ".", "append", "(", "(", "'SBAR'", ",", "subtree_str", ")", ")", "\n", "\n", "# VP: Sentence without subject", "\n", "", "subjects", "=", "[", "t", "for", "t", "in", "sent", "if", "t", ".", "head", "==", "token", "and", "t", ".", "dep_", "==", "'nsubj'", "]", "\n", "vp_subtree", "=", "list", "(", "token", ".", "subtree", ")", "\n", "subject_parts", "=", "set", "(", ")", "\n", "for", "subj", "in", "subjects", ":", "\n", "                ", "subject_parts", "=", "subject_parts", "|", "set", "(", "subj", ".", "subtree", ")", "\n", "", "vp_subtree", "=", "[", "t", "for", "t", "in", "vp_subtree", "if", "t", "not", "in", "subject_parts", "]", "\n", "tree_list", ".", "append", "(", "(", "'VP'", ",", "' '", ".", "join", "(", "[", "t", ".", "orth_", ".", "lower", "(", ")", "for", "t", "in", "vp_subtree", "]", ")", ".", "strip", "(", ")", ")", ")", "\n", "\n", "# VP: Verb + object, no modifiers except aux + negation", "\n", "aux_neg", "=", "[", "t", "for", "t", "in", "sent", "if", "t", ".", "head", "==", "token", "and", "t", ".", "dep_", "in", "[", "'aux'", ",", "'neg'", "]", "]", "\n", "objects", "=", "[", "t", "for", "t", "in", "sent", "if", "t", ".", "head", "==", "token", "and", "t", ".", "dep_", "in", "[", "'dobj'", ",", "'dative'", "]", "]", "\n", "object_parts", "=", "set", "(", ")", "\n", "for", "obj", "in", "objects", ":", "\n", "                ", "object_parts", "=", "object_parts", "|", "set", "(", "obj", ".", "subtree", ")", "\n", "", "vp_subtree", "=", "[", "t", "for", "t", "in", "vp_subtree", "if", "t", "==", "token", "or", "t", "in", "object_parts", "|", "set", "(", "aux_neg", ")", "]", "\n", "tree_list", ".", "append", "(", "(", "'VP'", ",", "' '", ".", "join", "(", "[", "t", ".", "orth_", ".", "lower", "(", ")", "for", "t", "in", "vp_subtree", "]", ")", ".", "strip", "(", ")", ")", ")", "\n", "\n", "# VP: Verb + object, no modifiers", "\n", "vp_subtree", "=", "[", "t", "for", "t", "in", "vp_subtree", "if", "t", "==", "token", "or", "t", "in", "object_parts", "]", "\n", "tree_list", ".", "append", "(", "(", "'VP'", ",", "' '", ".", "join", "(", "[", "t", ".", "orth_", ".", "lower", "(", ")", "for", "t", "in", "vp_subtree", "]", ")", ".", "strip", "(", ")", ")", ")", "\n", "\n", "# VP: modifiers + verb, no subject or object", "\n", "vp_subtree", "=", "[", "t", "for", "t", "in", "token", ".", "subtree", "if", "t", "not", "in", "subject_parts", "|", "object_parts", "]", "\n", "token_index", "=", "vp_subtree", ".", "index", "(", "token", ")", "\n", "beg", ",", "end", "=", "vp_subtree", "[", ":", "token_index", "+", "1", "]", ",", "vp_subtree", "[", "token_index", ":", "]", "[", ":", ":", "-", "1", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "beg", ")", ")", ":", "\n", "                ", "tree_list", ".", "append", "(", "(", "'VP'", ",", "' '", ".", "join", "(", "[", "t", ".", "orth_", ".", "lower", "(", ")", "for", "t", "in", "beg", "]", ")", ".", "strip", "(", ")", ")", ")", "\n", "beg", "=", "beg", "[", "1", ":", "]", "\n", "", "for", "i", "in", "range", "(", "len", "(", "end", ")", ")", ":", "\n", "                ", "tree_list", ".", "append", "(", "(", "'VP'", ",", "' '", ".", "join", "(", "[", "t", ".", "orth_", ".", "lower", "(", ")", "for", "t", "in", "end", "]", "[", ":", ":", "-", "1", "]", ")", ".", "strip", "(", ")", ")", ")", "\n", "end", "=", "end", "[", "1", ":", "]", "\n", "\n", "", "", "else", ":", "\n", "            ", "if", "token", ".", "pos_", "==", "'NOUN'", ":", "\n", "                ", "tree_list", ".", "append", "(", "(", "'NN'", ",", "token", ".", "orth_", ")", ")", "\n", "tree_list", "+=", "all_subtrees", "(", "token", ",", "'NP'", ",", "parser", "=", "parser", ")", "\n", "\n", "", "elif", "token", ".", "pos_", "==", "'ADJ'", ":", "\n", "                ", "tree_list", ".", "append", "(", "(", "'JJ'", ",", "token", ".", "orth_", ")", ")", "\n", "tree_list", "+=", "all_subtrees", "(", "token", ",", "'ADJP'", ",", "parser", "=", "parser", ")", "\n", "\n", "", "elif", "token", ".", "pos_", "==", "'ADJ'", ":", "\n", "                ", "tree_list", ".", "append", "(", "(", "'JJ'", ",", "token", ".", "orth_", ")", ")", "\n", "tree_list", "+=", "all_subtrees", "(", "token", ",", "'ADJP'", ",", "parser", "=", "parser", ")", "\n", "\n", "", "elif", "token", ".", "pos_", "==", "'ADV'", ":", "\n", "                ", "tree_list", ".", "append", "(", "(", "'RB'", ",", "token", ".", "orth_", ")", ")", "\n", "tree_list", "+=", "all_subtrees", "(", "token", ",", "'ADVP'", ",", "parser", "=", "parser", ")", "\n", "\n", "", "elif", "token", ".", "pos_", "==", "'ADP'", ":", "\n", "                ", "subtree_str", "=", "' '", ".", "join", "(", "[", "t", ".", "orth_", "for", "t", "in", "token", ".", "subtree", "]", ")", ".", "lower", "(", ")", "\n", "tree_list", ".", "append", "(", "(", "'PP'", ",", "subtree_str", ")", ")", "\n", "\n", "", "", "", "tree_list", "=", "[", "pair", "for", "pair", "in", "tree_list", "if", "pair", "[", "1", "]", "in", "sent_orth", "]", "\n", "tree_list", "+=", "[", "(", "label", ",", "strip_punctuation", "(", "s", ")", ")", "for", "(", "label", ",", "s", ")", "in", "tree_list", "]", "\n", "\n", "\n", "return", "tree_list", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.ppdb_matches": [[187, 276], ["collections.defaultdict", "augmentation_ppdb.trees", "augmentation_ppdb.all_ngrams", "parser", "sent.index", "sent[].strip", "sorted", "sorted", "sorted", "sent.lower", "len", "list", "list", "sorted", "parser", "set", "set", "sorted", "collections.defaultdict.keys", "list", "phr_words.append", "set.add", "list", "set.add", "parser", "set", "set", "enumerate", "par_words.append", "set.add", "sorted.remove", "set.add", "sorted.remove", "sent[].split", "sorted.remove", "sorted.remove", "sent.index"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.trees", "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.all_ngrams"], ["", "def", "ppdb_matches", "(", "ppdb_dict", ",", "\n", "sent", ",", "\n", "parser", ",", "\n", "sent_parsed", "=", "None", ",", "\n", "sent_words", "=", "None", ",", "\n", "max_len", "=", "100", ",", "\n", "single_words", "=", "True", ",", "\n", "filter_ungrammatical", "=", "False", ",", "\n", "ppdb_dict_key_len", "=", "3", ")", ":", "\n", "\n", "#    pars = defaultdict(list)", "\n", "    ", "pars", "=", "defaultdict", "(", "set", ")", "\n", "if", "not", "sent_parsed", ":", "\n", "        ", "sent_parsed", "=", "parser", "(", "sent", ".", "lower", "(", ")", ")", "\n", "", "if", "not", "sent_words", ":", "\n", "        ", "sent_words", "=", "[", "t", ".", "orth_", "for", "t", "in", "sent_parsed", "]", "\n", "", "sent", "=", "' '", ".", "join", "(", "sent_words", ")", "\n", "sent_trees", "=", "trees", "(", "sent_parsed", "[", ":", "max_len", "]", ",", "parser", "=", "parser", ")", "\n", "\n", "for", "ngr", "in", "all_ngrams", "(", "sent_words", "[", ":", "max_len", "]", ")", ":", "\n", "        ", "start", "=", "sent", ".", "index", "(", "ngr", ")", "\n", "end", "=", "start", "+", "len", "(", "ngr", ")", "\n", "complement", "=", "sent", "[", "end", ":", "]", ".", "strip", "(", ")", "\n", "\n", "if", "ngr", "in", "ppdb_dict", "[", "'comp'", "]", "[", "ngr", "[", ":", "ppdb_dict_key_len", "]", "]", ":", "\n", "            ", "for", "required_label", "in", "ppdb_dict", "[", "'comp'", "]", "[", "ngr", "[", ":", "ppdb_dict_key_len", "]", "]", "[", "ngr", "]", ":", "\n", "                ", "for", "label", ",", "phrase", "in", "sent_trees", ":", "\n", "                    ", "if", "phrase", "in", "complement", "and", "phrase", "[", ":", "10", "]", "==", "complement", "[", ":", "10", "]", "and", "(", "label", "==", "required_label", "or", "(", "label", "[", "0", "]", "in", "[", "'N'", ",", "'V'", "]", "and", "label", "[", "0", "]", "==", "required_label", "[", "0", "]", ")", ")", ":", "\n", "                        ", "pars", "[", "ngr", "]", "=", "pars", "[", "ngr", "]", "|", "ppdb_dict", "[", "'comp'", "]", "[", "ngr", "[", ":", "ppdb_dict_key_len", "]", "]", "[", "ngr", "]", "[", "required_label", "]", "\n", "\n", "", "", "", "", "", "if", "single_words", ":", "\n", "        ", "for", "label", ",", "phrase", "in", "sent_trees", ":", "\n", "            ", "char_key", "=", "phrase", "[", ":", "ppdb_dict_key_len", "]", "\n", "if", "phrase", "in", "ppdb_dict", "[", "'pos'", "]", "[", "char_key", "]", ":", "\n", "                ", "if", "label", "in", "ppdb_dict", "[", "'pos'", "]", "[", "char_key", "]", "[", "phrase", "]", ":", "\n", "#                    pars[phrase] += ppdb_dict['pos'][char_key][phrase][label]", "\n", "                    ", "pars", "[", "phrase", "]", "=", "pars", "[", "phrase", "]", "|", "ppdb_dict", "[", "'pos'", "]", "[", "char_key", "]", "[", "phrase", "]", "[", "label", "]", "\n", "\n", "", "", "", "", "pars", "=", "{", "phr", ":", "sorted", "(", "list", "(", "pars", "[", "phr", "]", ")", ")", "for", "phr", "in", "pars", "}", "\n", "\n", "# Filter out ungrammatical paraphrases", "\n", "# sort to ensure consistency when iterating dict keys", "\n", "if", "filter_ungrammatical", ":", "\n", "        ", "for", "phr", "in", "sorted", "(", "list", "(", "pars", ".", "keys", "(", ")", ")", ")", ":", "\n", "            ", "pars_filter", "=", "sorted", "(", "list", "(", "pars", "[", "phr", "]", ")", ")", "\n", "phr_parsed", "=", "parser", "(", "phr", ")", "\n", "phr_words", "=", "[", "]", "\n", "phr_tags", "=", "set", "(", ")", "\n", "phr_prons", "=", "set", "(", ")", "\n", "for", "t", "in", "phr_parsed", ":", "\n", "                ", "phr_words", ".", "append", "(", "t", ".", "orth_", ")", "\n", "phr_tags", ".", "add", "(", "t", ".", "tag_", ")", "\n", "if", "t", ".", "pos_", "==", "'PRON'", "or", "t", ".", "tag_", "==", "'PRP$'", ":", "\n", "                    ", "phr_prons", ".", "add", "(", "t", ".", "orth_", ")", "\n", "\n", "", "", "for", "par", "in", "sorted", "(", "list", "(", "pars", "[", "phr", "]", ")", ")", ":", "\n", "                ", "if", "par", "in", "pars_filter", ":", "\n", "                    ", "par_parsed", "=", "parser", "(", "par", ")", "\n", "par_words", "=", "[", "]", "\n", "par_tags", "=", "set", "(", ")", "\n", "par_prons", "=", "set", "(", ")", "\n", "for", "t", "in", "par_parsed", ":", "\n", "                        ", "par_words", ".", "append", "(", "t", ".", "orth_", ")", "\n", "par_tags", ".", "add", "(", "t", ".", "tag_", ")", "\n", "if", "t", ".", "pos_", "==", "'PRON'", "or", "t", ".", "tag_", "==", "'PRP$'", ":", "\n", "                            ", "par_prons", ".", "add", "(", "t", ".", "orth_", ")", "\n", "\n", "# No pronoun changes", "\n", "", "", "if", "phr_prons", "!=", "par_prons", ":", "\n", "                        ", "pars_filter", ".", "remove", "(", "par", ")", "\n", "# First and last tags must be the same", "\n", "", "for", "i", "in", "[", "0", ",", "-", "1", "]", ":", "\n", "                        ", "if", "par", "in", "pars_filter", "and", "par_parsed", "[", "i", "]", ".", "tag_", "!=", "phr_parsed", "[", "i", "]", ".", "tag_", ":", "\n", "                            ", "pars_filter", ".", "remove", "(", "par", ")", "\n", "# Tense/number inflection", "\n", "", "", "for", "tag", "in", "[", "'VB'", ",", "'VBP'", ",", "'VBD'", ",", "'VBG'", ",", "'VBN'", ",", "'VBZ'", ",", "'NN'", ",", "'NNP'", ",", "'NNPS'", ",", "'NNS'", "]", ":", "\n", "                        ", "if", "par", "in", "pars_filter", "and", "tag", "in", "(", "phr_tags", "-", "par_tags", ")", "|", "(", "par_tags", "-", "phr_tags", ")", ":", "\n", "                            ", "if", "not", "(", "tag", "in", "[", "'VB'", ",", "'VBP'", "]", "and", "{", "'VB'", ",", "'VBP'", "}", "&", "par_tags", ")", ":", "\n", "                                ", "pars_filter", ".", "remove", "(", "par", ")", "\n", "# Copula inflection with \"I\"", "\n", "", "", "", "for", "i", ",", "word", "in", "enumerate", "(", "par_words", ")", ":", "\n", "                        ", "if", "word", "in", "[", "'am'", ",", "'are'", "]", "and", "word", "not", "in", "phr_words", "and", "(", "i", "==", "0", "or", "par_words", "[", "i", "-", "1", "]", "!=", "'i'", ")", ":", "\n", "                            ", "prev", "=", "sent", "[", ":", "sent", ".", "index", "(", "phr", ")", "]", ".", "split", "(", ")", "\n", "if", "par", "in", "pars_filter", "and", "(", "(", "word", "==", "'am'", "and", "'i'", "not", "in", "prev", ")", "or", "(", "word", "==", "'are'", "and", "'i'", "in", "prev", ")", ")", ":", "\n", "                                ", "pars_filter", ".", "remove", "(", "par", ")", "\n", "", "", "", "", "", "pars", "[", "phr", "]", "=", "pars_filter", "\n", "", "", "pars", "=", "{", "p", ":", "sorted", "(", "pars", "[", "p", "]", ")", "for", "p", "in", "pars", "if", "pars", "[", "p", "]", "}", "\n", "return", "pars", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.augment_ppdb": [[278, 335], ["numpy.random.RandomState", "nltk.sent_tokenize", "enumerate", "enumerate", "len", "numpy.array", "numpy.clip", "range", "parser", "numpy.round", "doc_aug.append", "list", "doc_aug.append", "t.orth_.lower", "augmentation_ppdb.ppdb_matches", "sent_pars_list.append", "np.random.RandomState.permutation", "sents_aug[].replace", "par_list.append", "np.random.RandomState.choice"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.ppdb_scripts.augmentation_ppdb.ppdb_matches"], ["", "def", "augment_ppdb", "(", "document", ",", "\n", "num_aug", "=", "20", ",", "\n", "incl_orig", "=", "True", ",", "\n", "incl_orig_as_paraphrase", "=", "False", ",", "\n", "change_rate", "=", "0.25", ",", "\n", "max_len", "=", "100", ",", "\n", "ppdb_dict", "=", "ppdb", ",", "\n", "parser", "=", "parser", ",", "\n", "pars_dict", "=", "None", ",", "\n", "return_pars_dict", "=", "False", ",", "\n", "random_state", "=", "20200303", ")", ":", "\n", "\n", "    ", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "random_state", ")", "# numpy-random", "\n", "\n", "sents", "=", "sent_tokenize", "(", "document", ")", "\n", "sent_pars", "=", "{", "}", "if", "not", "pars_dict", "else", "pars_dict", "\n", "\n", "for", "i", ",", "sent", "in", "enumerate", "(", "sents", ")", ":", "\n", "        ", "sent_parsed", "=", "parser", "(", "sent", ")", "\n", "sent_words", "=", "[", "t", ".", "orth_", ".", "lower", "(", ")", "for", "t", "in", "sent_parsed", "]", "\n", "sent", "=", "' '", ".", "join", "(", "sent_words", ")", ".", "strip", "(", ")", "\n", "sents", "[", "i", "]", "=", "sent", "\n", "\n", "if", "sent", "not", "in", "sent_pars", ":", "\n", "            ", "pars", "=", "ppdb_matches", "(", "ppdb_dict", "=", "ppdb_dict", ",", "sent", "=", "sent", ",", "parser", "=", "parser", ",", "sent_parsed", "=", "sent_parsed", ",", "sent_words", "=", "sent_words", ",", "max_len", "=", "max_len", ",", "single_words", "=", "True", ")", "\n", "sent_pars", "[", "sent", "]", "=", "pars", "\n", "\n", "", "", "sent_pars_list", "=", "[", "]", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "sents", ")", ":", "\n", "        ", "for", "p", "in", "sent_pars", "[", "s", "]", ":", "\n", "            ", "par_list", "=", "sent_pars", "[", "s", "]", "[", "p", "]", "\n", "if", "incl_orig_as_paraphrase", ":", "\n", "                ", "par_list", ".", "append", "(", "p", ")", "\n", "", "sent_pars_list", ".", "append", "(", "(", "i", ",", "p", ",", "par_list", ")", ")", "\n", "\n", "", "", "num_possible_substitution_positions", "=", "len", "(", "sent_pars_list", ")", "\n", "num_substitutions", "=", "num_possible_substitution_positions", "*", "change_rate", "\n", "num_substitutions", "=", "np", ".", "array", "(", "np", ".", "round", "(", "num_substitutions", ")", ",", "dtype", "=", "int", ")", "\n", "num_substitutions", "=", "np", ".", "clip", "(", "num_substitutions", ",", "1", ",", "num_possible_substitution_positions", ")", "\n", "\n", "doc_aug", "=", "[", "]", "\n", "if", "incl_orig", ":", "\n", "        ", "num_aug", "-=", "1", "\n", "doc_aug", ".", "append", "(", "document", ")", "\n", "\n", "", "for", "augm", "in", "range", "(", "num_aug", ")", ":", "\n", "        ", "phrases_ix", "=", "rng", ".", "permutation", "(", "num_possible_substitution_positions", ")", "[", ":", "num_substitutions", "]", "\n", "paraphrases", "=", "[", "sent_pars_list", "[", "i", "]", "for", "i", "in", "phrases_ix", "]", "\n", "sents_aug", "=", "list", "(", "sents", ")", "\n", "for", "sent_ix", ",", "phrase", ",", "paraphrase_list", "in", "paraphrases", ":", "\n", "            ", "sent_aug", "=", "sents_aug", "[", "sent_ix", "]", ".", "replace", "(", "phrase", ",", "rng", ".", "choice", "(", "paraphrase_list", ")", ")", "\n", "sents_aug", "[", "sent_ix", "]", "=", "sent_aug", "\n", "", "doc_aug", ".", "append", "(", "' '", ".", "join", "(", "sents_aug", ")", ")", "\n", "\n", "", "if", "return_pars_dict", ":", "\n", "        ", "return", "doc_aug", ",", "sent_pars", "\n", "", "return", "doc_aug", "\n", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.embedding_scripts.augmentation_gensim.gensim_aug": [[19, 84], ["numpy.clip", "nltk.word_tokenize", "len", "numpy.array", "numpy.clip", "int", "collections.deque", "collections.deque", "range", "sorted", "collections.deque", "range", "zip", "numpy.random.RandomState", "document.lower", "numpy.round", "min", "collections.deque.append", "sorted.append", "range", "collections.deque.append", "nltk.word_tokenize.copy", "zip", "collections.deque.append", "enumerate", "numpy.round", "gensim_model.most_similar", "sorted", "doc_words.copy.append", "np.random.RandomState.randint", "most_similar.append", "np.random.RandomState.permutation", "len"], "function", ["None"], ["", "def", "gensim_aug", "(", "document", ",", "num_aug", "=", "20", ",", "gensim_model", "=", "Storage", ".", "twitter25", ",", "\n", "rng", "=", "None", ",", "random_state", "=", "20200303", ",", "\n", "max_candidates", "=", "10", ",", "incl_orig", "=", "True", ",", "incl_orig_as_paraphrase", "=", "False", ",", "\n", "change_rate", "=", "0.25", ",", "min_similarity", "=", "0.", ")", ":", "\n", "\n", "    ", "change_rate", "=", "np", ".", "clip", "(", "change_rate", ",", "0.", ",", "1.", ")", "\n", "\n", "if", "not", "rng", ":", "\n", "        ", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "random_state", ")", "\n", "\n", "", "doc_words", "=", "word_tokenize", "(", "document", ".", "lower", "(", ")", ")", "\n", "possible_substitution_positions", "=", "[", "i", "for", "(", "i", ",", "w", ")", "in", "enumerate", "(", "doc_words", ")", "if", "w", "in", "gensim_model", "]", "\n", "\n", "num_possible_substitution_positions", "=", "len", "(", "possible_substitution_positions", ")", "\n", "num_substitutions", "=", "num_possible_substitution_positions", "*", "change_rate", "\n", "num_substitutions", "=", "np", ".", "array", "(", "np", ".", "round", "(", "num_substitutions", ")", ",", "dtype", "=", "int", ")", "\n", "\n", "# at least one substitution", "\n", "num_substitutions", "=", "np", ".", "clip", "(", "num_substitutions", ",", "1", ",", "num_possible_substitution_positions", ")", "\n", "\n", "if", "num_substitutions", "*", "max_candidates", "<", "num_aug", ":", "\n", "        ", "max_candidates", "=", "min", "(", "10", ",", "np", ".", "round", "(", "num_aug", "/", "num_substitutions", ")", ")", "\n", "\n", "", "max_candidates", "=", "int", "(", "max_candidates", ")", "\n", "\n", "return_vals", "=", "deque", "(", ")", "\n", "if", "incl_orig", ":", "\n", "        ", "num_aug", "-=", "1", "\n", "return_vals", ".", "append", "(", "document", ")", "\n", "\n", "# pre-compute most similar words", "\n", "", "most_similar_dict", "=", "{", "}", "\n", "for", "w", "in", "doc_words", ":", "\n", "        ", "most_similar", "=", "[", "w", "]", "if", "incl_orig_as_paraphrase", "else", "[", "]", "\n", "if", "w", "in", "gensim_model", ":", "\n", "            ", "for", "sim_w", ",", "score", "in", "gensim_model", ".", "most_similar", "(", "w", ",", "topn", "=", "max_candidates", ")", ":", "\n", "                ", "if", "score", ">", "min_similarity", ":", "\n", "                    ", "most_similar", ".", "append", "(", "sim_w", ")", "\n", "", "", "", "if", "most_similar", "==", "[", "]", ":", "\n", "            ", "most_similar", "=", "[", "w", "]", "\n", "", "most_similar_dict", "[", "w", "]", "=", "most_similar", "\n", "\n", "#    old words to be replaced", "\n", "", "substitute_positions", "=", "deque", "(", ")", "\n", "for", "_", "in", "range", "(", "num_aug", ")", ":", "\n", "        ", "substitute_positions", ".", "append", "(", "sorted", "(", "rng", ".", "permutation", "(", "num_possible_substitution_positions", ")", "[", ":", "num_substitutions", "]", ")", ")", "\n", "", "substitute_positions", "=", "sorted", "(", "substitute_positions", ")", "\n", "#", "\n", "#    # new words", "\n", "substituted_similar_subwords", "=", "deque", "(", ")", "\n", "for", "_", "in", "range", "(", "num_aug", ")", ":", "\n", "        ", "temp_list", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_substitutions", ")", ":", "\n", "            ", "temp_list", ".", "append", "(", "rng", ".", "randint", "(", "max_candidates", ")", ")", "\n", "", "substituted_similar_subwords", ".", "append", "(", "temp_list", ")", "\n", "\n", "", "for", "batch_positions", ",", "batch_substitutions", "in", "zip", "(", "substitute_positions", ",", "substituted_similar_subwords", ")", ":", "\n", "        ", "temp_list", "=", "doc_words", ".", "copy", "(", ")", "\n", "for", "position_idx", ",", "substitution_idx", "in", "zip", "(", "batch_positions", ",", "batch_substitutions", ")", ":", "\n", "            ", "sw", "=", "doc_words", "[", "position_idx", "]", "\n", "sw_substitution", "=", "most_similar_dict", "[", "sw", "]", "[", "substitution_idx", "%", "len", "(", "most_similar_dict", "[", "sw", "]", ")", "]", "\n", "temp_list", "[", "position_idx", "]", "=", "sw_substitution", "\n", "", "return_vals", ".", "append", "(", "' '", ".", "join", "(", "temp_list", ")", ")", "\n", "\n", "", "return", "return_vals", "\n", "", ""]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.embedding_scripts.augmentation_bpemb.apply": [[15, 20], ["collections.deque", "list", "collections.deque.append", "fun"], "function", ["None"], ["", "def", "apply", "(", "documents", ",", "fun", ")", ":", "\n", "    ", "result", "=", "deque", "(", ")", "\n", "for", "doc", "in", "documents", ":", "\n", "        ", "result", ".", "append", "(", "fun", "(", "doc", ")", ")", "\n", "", "return", "list", "(", "result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.embedding_scripts.augmentation_bpemb.bpemb_aug": [[21, 91], ["numpy.clip", "Storage.bpemb.encode", "len", "numpy.array", "int", "numpy.clip", "collections.deque", "collections.deque", "range", "sorted", "collections.deque", "range", "zip", "list", "numpy.random.RandomState", "numpy.round", "min", "collections.deque.append", "sorted.append", "range", "collections.deque.append", "Storage.bpemb.encode.copy", "zip", "collections.deque.append", "numpy.round", "zip", "zip", "sorted", "subwords.copy.append", "Storage.bpemb.decode", "np.random.RandomState.randint", "Storage.bpemb.most_similar", "approved_similar.append", "np.random.RandomState.permutation", "len"], "function", ["None"], ["", "def", "bpemb_aug", "(", "document", ",", "num_aug", "=", "20", ",", "rng", "=", "None", ",", "random_state", "=", "20200303", ",", "\n", "max_candidates", "=", "10", ",", "incl_orig", "=", "True", ",", "change_rate", "=", "0.25", ",", "\n", "min_similarity", "=", "0.", ")", ":", "\n", "\n", "\n", "    ", "change_rate", "=", "np", ".", "clip", "(", "change_rate", ",", "0.", ",", "1.", ")", "\n", "\n", "if", "not", "rng", ":", "\n", "        ", "rng", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "random_state", ")", "\n", "\n", "", "subwords", "=", "Storage", ".", "bpemb", ".", "encode", "(", "document", ")", "\n", "\n", "num_possible_substitution_positions", "=", "len", "(", "subwords", ")", "\n", "num_substitutions", "=", "num_possible_substitution_positions", "*", "change_rate", "\n", "num_substitutions", "=", "np", ".", "array", "(", "np", ".", "round", "(", "num_substitutions", ")", ",", "dtype", "=", "int", ")", "\n", "\n", "if", "num_substitutions", "*", "max_candidates", "<", "num_aug", ":", "\n", "        ", "max_candidates", "=", "min", "(", "10", ",", "np", ".", "round", "(", "num_aug", "/", "num_substitutions", ")", ")", "\n", "\n", "", "max_candidates", "=", "int", "(", "max_candidates", ")", "\n", "\n", "# at least one substitution", "\n", "num_substitutions", "=", "np", ".", "clip", "(", "num_substitutions", ",", "1", ",", "num_possible_substitution_positions", ")", "\n", "\n", "return_vals", "=", "deque", "(", ")", "\n", "if", "incl_orig", ":", "\n", "        ", "num_aug", "-=", "1", "\n", "return_vals", ".", "append", "(", "document", ")", "\n", "\n", "# pre-compute most similar subwords", "\n", "", "most_similar_dict", "=", "{", "}", "\n", "for", "sw", "in", "subwords", ":", "\n", "        ", "approved_similar", "=", "[", "sw", "]", "\n", "try", ":", "\n", "            ", "most_similar", ",", "scores", "=", "zip", "(", "*", "Storage", ".", "bpemb", ".", "most_similar", "(", "sw", ")", ")", "\n", "approved_similar", "=", "[", "]", "\n", "# print(sw)", "\n", "for", "a", ",", "b", "in", "zip", "(", "most_similar", ",", "scores", ")", ":", "\n", "                ", "if", "b", ">", "min_similarity", ":", "\n", "                    ", "approved_similar", ".", "append", "(", "a", ")", "\n", "", "", "most_similar", "=", "approved_similar", "[", ":", "max_candidates", "]", "\n", "", "except", ":", "\n", "            ", "most_similar", "=", "[", "sw", "]", "\n", "", "if", "most_similar", "==", "[", "]", ":", "\n", "            ", "most_similar", "=", "[", "sw", "]", "\n", "", "most_similar_dict", "[", "sw", "]", "=", "most_similar", "\n", "\n", "# old subwords that are replaced", "\n", "", "substitute_positions", "=", "deque", "(", ")", "\n", "for", "_", "in", "range", "(", "num_aug", ")", ":", "\n", "        ", "substitute_positions", ".", "append", "(", "sorted", "(", "rng", ".", "permutation", "(", "num_possible_substitution_positions", ")", "[", ":", "num_substitutions", "]", ")", ")", "\n", "", "substitute_positions", "=", "sorted", "(", "substitute_positions", ")", "\n", "\n", "# new subwords", "\n", "substituted_similar_subwords", "=", "deque", "(", ")", "\n", "for", "_", "in", "range", "(", "num_aug", ")", ":", "\n", "        ", "temp_list", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_substitutions", ")", ":", "\n", "            ", "temp_list", ".", "append", "(", "rng", ".", "randint", "(", "max_candidates", ")", ")", "\n", "", "substituted_similar_subwords", ".", "append", "(", "temp_list", ")", "\n", "\n", "", "for", "batch_positions", ",", "batch_substitutions", "in", "zip", "(", "substitute_positions", ",", "substituted_similar_subwords", ")", ":", "\n", "        ", "temp_list", "=", "subwords", ".", "copy", "(", ")", "\n", "for", "position_idx", ",", "substitution_idx", "in", "zip", "(", "batch_positions", ",", "batch_substitutions", ")", ":", "\n", "            ", "sw", "=", "subwords", "[", "position_idx", "]", "\n", "sw_substitution", "=", "most_similar_dict", "[", "sw", "]", "[", "substitution_idx", "%", "len", "(", "most_similar_dict", "[", "sw", "]", ")", "]", "\n", "temp_list", "[", "position_idx", "]", "=", "sw_substitution", "\n", "", "return_vals", ".", "append", "(", "Storage", ".", "bpemb", ".", "decode", "(", "temp_list", ")", ")", "\n", "\n", "", "return", "list", "(", "return_vals", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.embedding_scripts.augmentation_bpemb.augmentation_helper": [[94, 113], ["collections.deque", "open", "enumerate", "open", "tqdm.tqdm", "open", "fo.write", "line.strip().split", "int", "aug_function", "collections.deque.append", "list", "line.strip"], "function", ["None"], ["", "def", "augmentation_helper", "(", "input", ",", "output", ",", "aug_function", ",", "num_aug", "=", "20", ",", "target_classes", "=", "[", "0", ",", "1", "]", ")", ":", "\n", "    ", "with", "open", "(", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "total", ",", "_", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "pass", "\n", "\n", "# write files temporarily into a deque", "\n", "", "", "dq", "=", "deque", "(", ")", "\n", "with", "open", "(", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "tqdm", ".", "tqdm", "(", "f", ",", "total", "=", "total", ")", ":", "\n", "            ", "id", ",", "sent", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "int", "(", "id", ")", "in", "target_classes", ":", "\n", "                ", "aug_sents", "=", "aug_function", "(", "sent", ",", "num_aug", "=", "num_aug", ")", "\n", "", "else", ":", "\n", "                ", "aug_sents", "=", "[", "sent", "]", "\n", "", "for", "sent", "in", "aug_sents", ":", "\n", "                ", "dq", ".", "append", "(", "'%s\\t%s\\n'", "%", "(", "id", ",", "sent", ")", ")", "\n", "\n", "", "", "", "with", "open", "(", "output", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fo", ":", "\n", "        ", "fo", ".", "write", "(", "''", ".", "join", "(", "list", "(", "dq", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.embedding_scripts.augmentation_bpemb.bpemb_augment": [[116, 134], ["fo.write", "open", "enumerate", "open", "open", "tqdm.tqdm", "list", "line.strip().split", "int", "augmentation_bpemb.bpemb_aug", "dq.append", "fo.write", "line.strip"], "function", ["home.repos.pwc.inspect_result.ssg-research_language-data-augmentation.embedding_scripts.augmentation_bpemb.bpemb_aug"], ["", "", "def", "bpemb_augment", "(", "input", ",", "output", ",", "num_aug", "=", "20", ",", "target_classes", "=", "[", "0", ",", "1", "]", ")", ":", "\n", "    ", "with", "open", "(", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "total", ",", "_", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "pass", "\n", "\n", "\n", "", "", "with", "open", "(", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ",", "open", "(", "output", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fo", ":", "\n", "        ", "for", "line", "in", "tqdm", ".", "tqdm", "(", "f", ",", "total", "=", "total", ")", ":", "\n", "            ", "id", ",", "sent", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "int", "(", "id", ")", "in", "target_classes", ":", "\n", "                ", "aug_sents", "=", "bpemb_aug", "(", "sent", ",", "num_aug", "=", "num_aug", ")", "\n", "", "else", ":", "\n", "                ", "aug_sents", "=", "[", "sent", "]", "\n", "", "for", "sent", "in", "aug_sents", ":", "\n", "                ", "dq", ".", "append", "(", "'%s\\t%s\\n'", "%", "(", "id", ",", "sent", ")", ")", "\n", "fo", ".", "write", "(", "'%s\\t%s\\n'", "%", "(", "id", ",", "sent", ")", ")", "\n", "\n", "", "", "", "fo", ".", "write", "(", "list", "(", "dq", ")", ")", "\n", "", ""]]}