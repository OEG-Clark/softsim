{"home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.load_pytorch_to_tf.to_tf_var_name": [[39, 43], ["iter", "name.replace.replace"], "function", ["None"], ["def", "to_tf_var_name", "(", "name", ":", "str", ")", ":", "\n", "    ", "for", "patt", ",", "repl", "in", "iter", "(", "var_map", ")", ":", "\n", "        ", "name", "=", "name", ".", "replace", "(", "patt", ",", "repl", ")", "\n", "", "return", "'{}'", ".", "format", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.load_pytorch_to_tf.my_convert_keys": [[45, 51], ["model.items", "load_pytorch_to_tf.to_tf_var_name"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.load_pytorch_to_tf.to_tf_var_name"], ["", "def", "my_convert_keys", "(", "model", ")", ":", "\n", "    ", "converted", "=", "{", "}", "\n", "for", "k_pt", ",", "v", "in", "model", ".", "items", "(", ")", ":", "\n", "        ", "k_tf", "=", "to_tf_var_name", "(", "k_pt", ")", "\n", "converted", "[", "k_tf", "]", "=", "v", "\n", "", "return", "converted", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.load_pytorch_to_tf.load_from_pytorch_checkpoint": [[53, 70], ["torch.load", "load_pytorch_to_tf.my_convert_keys", "assignment_map.items", "store_vars.get", "pt_model_with_tf_keys[].cpu().numpy", "any", "tensorflow.python.framework.ops.convert_to_tensor", "store_vars.get.assign", "tensorflow.python.ops.variable_scope._get_default_variable_store", "print", "array.transpose.transpose", "tuple", "tuple", "pt_model_with_tf_keys[].cpu", "store_vars.get.get_shape().as_list", "store_vars.get.get_shape"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.load_pytorch_to_tf.my_convert_keys", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape"], ["", "def", "load_from_pytorch_checkpoint", "(", "checkpoint", ",", "assignment_map", ")", ":", "\n", "    ", "pytorch_model", "=", "torch", ".", "load", "(", "checkpoint", ",", "map_location", "=", "'cpu'", ")", "\n", "pt_model_with_tf_keys", "=", "my_convert_keys", "(", "pytorch_model", ")", "\n", "for", "_", ",", "name", "in", "assignment_map", ".", "items", "(", ")", ":", "\n", "        ", "store_vars", "=", "vs", ".", "_get_default_variable_store", "(", ")", ".", "_vars", "\n", "var", "=", "store_vars", ".", "get", "(", "name", ",", "None", ")", "\n", "assert", "var", "is", "not", "None", "\n", "if", "name", "not", "in", "pt_model_with_tf_keys", ":", "\n", "            ", "print", "(", "'WARNING:'", ",", "name", ",", "'not found in original model.'", ")", "\n", "continue", "\n", "", "array", "=", "pt_model_with_tf_keys", "[", "name", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "any", "(", "[", "x", "in", "name", "for", "x", "in", "tensors_to_transpose", "]", ")", ":", "\n", "            ", "array", "=", "array", ".", "transpose", "(", ")", "\n", "", "assert", "tuple", "(", "var", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", ")", "==", "tuple", "(", "array", ".", "shape", ")", "\n", "init_value", "=", "ops", ".", "convert_to_tensor", "(", "array", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "var", ".", "_initial_value", "=", "init_value", "\n", "var", ".", "_initializer_op", "=", "var", ".", "assign", "(", "init_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.load_pytorch_to_tf.print_vars": [[72, 103], ["tensorflow.train.list_variables", "torch.load", "load_pytorch_to_tf.my_convert_keys", "set", "my_convert_keys.items", "print", "print", "print", "print", "print", "print", "tf.train.list_variables.keys", "len", "pt_model_with_tf_keys[].cpu().numpy", "any", "tuple", "tuple", "len", "print", "len", "print", "common.append", "set.remove", "only_pytorch.append", "array.transpose.transpose", "print", "pt_model_with_tf_keys[].size", "pt_model_with_tf_keys[].cpu"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.load_pytorch_to_tf.my_convert_keys"], ["", "", "def", "print_vars", "(", "pytorch_ckpt", ",", "tf_ckpt", ")", ":", "\n", "    ", "tf_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_ckpt", ")", "\n", "tf_vars", "=", "{", "k", ":", "v", "for", "(", "k", ",", "v", ")", "in", "tf_vars", "}", "\n", "pytorch_model", "=", "torch", ".", "load", "(", "pytorch_ckpt", ")", "\n", "pt_model_with_tf_keys", "=", "my_convert_keys", "(", "pytorch_model", ")", "\n", "only_pytorch", ",", "only_tf", ",", "common", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "tf_only", "=", "set", "(", "tf_vars", ".", "keys", "(", ")", ")", "\n", "for", "k", ",", "v", "in", "pt_model_with_tf_keys", ".", "items", "(", ")", ":", "\n", "        ", "if", "k", "in", "tf_vars", ":", "\n", "            ", "common", ".", "append", "(", "k", ")", "\n", "tf_only", ".", "remove", "(", "k", ")", "\n", "", "else", ":", "\n", "            ", "only_pytorch", ".", "append", "(", "k", ")", "\n", "", "", "print", "(", "'-------------------'", ")", "\n", "print", "(", "'Common'", ",", "len", "(", "common", ")", ")", "\n", "for", "k", "in", "common", ":", "\n", "        ", "array", "=", "pt_model_with_tf_keys", "[", "k", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "any", "(", "[", "x", "in", "k", "for", "x", "in", "tensors_to_transpose", "]", ")", ":", "\n", "            ", "array", "=", "array", ".", "transpose", "(", ")", "\n", "", "tf_shape", "=", "tuple", "(", "tf_vars", "[", "k", "]", ")", "\n", "pt_shape", "=", "tuple", "(", "array", ".", "shape", ")", "\n", "if", "tf_shape", "!=", "pt_shape", ":", "\n", "            ", "print", "(", "k", ",", "tf_shape", ",", "pt_shape", ")", "\n", "", "", "print", "(", "'-------------------'", ")", "\n", "print", "(", "'Pytorch only'", ",", "len", "(", "only_pytorch", ")", ")", "\n", "for", "k", "in", "only_pytorch", ":", "\n", "        ", "print", "(", "k", ",", "pt_model_with_tf_keys", "[", "k", "]", ".", "size", "(", ")", ")", "\n", "", "print", "(", "'-------------------'", ")", "\n", "print", "(", "'TF only'", ",", "len", "(", "tf_only", ")", ")", "\n", "for", "k", "in", "tf_only", ":", "\n", "        ", "print", "(", "k", ",", "tf_vars", "[", "k", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.CorefEvaluator.__init__": [[55, 57], ["metrics.Evaluator"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "evaluators", "=", "[", "Evaluator", "(", "m", ")", "for", "m", "in", "(", "muc", ",", "b_cubed", ",", "ceafe", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.CorefEvaluator.update": [[58, 61], ["e.update"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.update"], ["", "def", "update", "(", "self", ",", "predicted", ",", "gold", ",", "mention_to_predicted", ",", "mention_to_gold", ")", ":", "\n", "        ", "for", "e", "in", "self", ".", "evaluators", ":", "\n", "            ", "e", ".", "update", "(", "predicted", ",", "gold", ",", "mention_to_predicted", ",", "mention_to_gold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.CorefEvaluator.get_f1": [[62, 64], ["sum", "len", "e.get_f1"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_f1"], ["", "", "def", "get_f1", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "e", ".", "get_f1", "(", ")", "for", "e", "in", "self", ".", "evaluators", ")", "/", "len", "(", "self", ".", "evaluators", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.CorefEvaluator.get_recall": [[65, 67], ["sum", "len", "e.get_recall"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_recall"], ["", "def", "get_recall", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "e", ".", "get_recall", "(", ")", "for", "e", "in", "self", ".", "evaluators", ")", "/", "len", "(", "self", ".", "evaluators", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.CorefEvaluator.get_precision": [[68, 70], ["sum", "len", "e.get_precision"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_precision"], ["", "def", "get_precision", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "e", ".", "get_precision", "(", ")", "for", "e", "in", "self", ".", "evaluators", ")", "/", "len", "(", "self", ".", "evaluators", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.CorefEvaluator.get_prf": [[71, 73], ["metrics.CorefEvaluator.get_precision", "metrics.CorefEvaluator.get_recall", "metrics.CorefEvaluator.get_f1"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_precision", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_recall", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_f1"], ["", "def", "get_prf", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_precision", "(", ")", ",", "self", ".", "get_recall", "(", ")", ",", "self", ".", "get_f1", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.__init__": [[76, 83], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "metric", ",", "beta", "=", "1", ")", ":", "\n", "        ", "self", ".", "p_num", "=", "0", "\n", "self", ".", "p_den", "=", "0", "\n", "self", ".", "r_num", "=", "0", "\n", "self", ".", "r_den", "=", "0", "\n", "self", ".", "metric", "=", "metric", "\n", "self", ".", "beta", "=", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.update": [[84, 94], ["metrics.Evaluator.metric", "metrics.Evaluator.metric", "metrics.Evaluator.metric"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "predicted", ",", "gold", ",", "mention_to_predicted", ",", "mention_to_gold", ")", ":", "\n", "        ", "if", "self", ".", "metric", "==", "ceafe", ":", "\n", "            ", "pn", ",", "pd", ",", "rn", ",", "rd", "=", "self", ".", "metric", "(", "predicted", ",", "gold", ")", "\n", "", "else", ":", "\n", "            ", "pn", ",", "pd", "=", "self", ".", "metric", "(", "predicted", ",", "mention_to_gold", ")", "\n", "rn", ",", "rd", "=", "self", ".", "metric", "(", "gold", ",", "mention_to_predicted", ")", "\n", "", "self", ".", "p_num", "+=", "pn", "\n", "self", ".", "p_den", "+=", "pd", "\n", "self", ".", "r_num", "+=", "rn", "\n", "self", ".", "r_den", "+=", "rd", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_f1": [[95, 97], ["metrics.f1"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.f1"], ["", "def", "get_f1", "(", "self", ")", ":", "\n", "        ", "return", "f1", "(", "self", ".", "p_num", ",", "self", ".", "p_den", ",", "self", ".", "r_num", ",", "self", ".", "r_den", ",", "beta", "=", "self", ".", "beta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_recall": [[98, 100], ["float"], "methods", ["None"], ["", "def", "get_recall", "(", "self", ")", ":", "\n", "        ", "return", "0", "if", "self", ".", "r_num", "==", "0", "else", "self", ".", "r_num", "/", "float", "(", "self", ".", "r_den", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_precision": [[101, 103], ["float"], "methods", ["None"], ["", "def", "get_precision", "(", "self", ")", ":", "\n", "        ", "return", "0", "if", "self", ".", "p_num", "==", "0", "else", "self", ".", "p_num", "/", "float", "(", "self", ".", "p_den", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_prf": [[104, 106], ["metrics.Evaluator.get_precision", "metrics.Evaluator.get_recall", "metrics.Evaluator.get_f1"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_precision", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_recall", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_f1"], ["", "def", "get_prf", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_precision", "(", ")", ",", "self", ".", "get_recall", "(", ")", ",", "self", ".", "get_f1", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_counts": [[107, 109], ["None"], "methods", ["None"], ["", "def", "get_counts", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "p_num", ",", "self", ".", "p_den", ",", "self", ".", "r_num", ",", "self", ".", "r_den", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.mention_proposal_prediction": [[11, 46], ["numpy.reshape", "numpy.reshape", "numpy.tile", "numpy.tile", "numpy.reshape", "numpy.reshape", "numpy.expand_dims", "numpy.expand_dims"], "function", ["None"], ["def", "mention_proposal_prediction", "(", "config", ",", "current_doc_result", ",", "concat_only", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    current_doc_result: \n        \"total_loss\": total_loss,\n        \"start_scores\": start_scores,\n        \"start_gold\": gold_starts,\n        \"end_gold\": gold_ends,\n        \"end_scores\": end_scores, \n        \"span_scores\": span_scores, \n        \"span_gold\": span_mention\n\n    \"\"\"", "\n", "\n", "span_scores", "=", "current_doc_result", "[", "\"span_scores\"", "]", "\n", "span_gold", "=", "current_doc_result", "[", "\"span_gold\"", "]", "\n", "\n", "if", "concat_only", ":", "\n", "        ", "scores", "=", "span_scores", "\n", "", "else", ":", "\n", "        ", "start_scores", "=", "current_doc_result", "[", "\"start_scores\"", "]", ",", "\n", "end_scores", "=", "current_doc_result", "[", "\"end_scores\"", "]", "\n", "# start_scores = tf.tile(tf.expand_dims(start_scores, 2), [1, 1, config[\"max_segment_len\"]])", "\n", "start_scores", "=", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "start_scores", ",", "axis", "=", "2", ")", ",", "(", "1", ",", "1", ",", "config", "[", "\"max_segment_len\"", "]", ")", ")", "\n", "end_scores", "=", "np", ".", "tile", "(", "np", ".", "expand_dims", "(", "end_scores", ",", "axis", "=", "2", ")", ",", "(", "1", ",", "1", ",", "config", "[", "\"max_segment_len\"", "]", ")", ")", "\n", "start_scores", "=", "np", ".", "reshape", "(", "start_scores", ",", "[", "-", "1", ",", "config", "[", "\"max_segment_len\"", "]", ",", "config", "[", "\"max_segment_len\"", "]", "]", ")", "\n", "end_scores", "=", "np", ".", "reshape", "(", "end_scores", ",", "[", "-", "1", ",", "config", "[", "\"max_segment_len\"", "]", ",", "config", "[", "\"max_segment_len\"", "]", "]", ")", "\n", "\n", "# end_scores -> max_training_sent, max_segment_len ", "\n", "scores", "=", "(", "start_scores", "+", "end_scores", "+", "span_scores", ")", "/", "3", "\n", "\n", "", "pred_span_label", "=", "scores", ">=", "0.5", "\n", "pred_span_label", "=", "np", ".", "reshape", "(", "pred_span_label", ",", "[", "-", "1", "]", ")", "\n", "gold_span_label", "=", "np", ".", "reshape", "(", "span_gold", ",", "[", "-", "1", "]", ")", "\n", "\n", "return", "pred_span_label", ",", "gold_span_label", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.f1": [[48, 52], ["float", "float"], "function", ["None"], ["", "def", "f1", "(", "p_num", ",", "p_den", ",", "r_num", ",", "r_den", ",", "beta", "=", "1", ")", ":", "\n", "    ", "p", "=", "0", "if", "p_den", "==", "0", "else", "p_num", "/", "float", "(", "p_den", ")", "\n", "r", "=", "0", "if", "r_den", "==", "0", "else", "r_num", "/", "float", "(", "r_den", ")", "\n", "return", "0", "if", "p", "+", "r", "==", "0", "else", "(", "1", "+", "beta", "*", "beta", ")", "*", "p", "*", "r", "/", "(", "beta", "*", "beta", "*", "p", "+", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.evaluate_documents": [[111, 116], ["metrics.Evaluator", "metrics.Evaluator.update", "metrics.Evaluator.get_precision", "metrics.Evaluator.get_recall", "metrics.Evaluator.get_f1"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.update", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_precision", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_recall", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_f1"], ["", "", "def", "evaluate_documents", "(", "documents", ",", "metric", ",", "beta", "=", "1", ")", ":", "\n", "    ", "evaluator", "=", "Evaluator", "(", "metric", ",", "beta", "=", "beta", ")", "\n", "for", "document", "in", "documents", ":", "\n", "        ", "evaluator", ".", "update", "(", "document", ")", "\n", "", "return", "evaluator", ".", "get_precision", "(", ")", ",", "evaluator", ".", "get_recall", "(", ")", ",", "evaluator", ".", "get_f1", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.b_cubed": [[118, 138], ["collections.Counter", "collections.Counter.items", "len", "len", "float", "len", "len", "tuple"], "function", ["None"], ["", "def", "b_cubed", "(", "clusters", ",", "mention_to_gold", ")", ":", "\n", "    ", "num", ",", "dem", "=", "0", ",", "0", "\n", "\n", "for", "c", "in", "clusters", ":", "\n", "        ", "if", "len", "(", "c", ")", "==", "1", ":", "\n", "            ", "continue", "\n", "\n", "", "gold_counts", "=", "Counter", "(", ")", "\n", "correct", "=", "0", "\n", "for", "m", "in", "c", ":", "\n", "            ", "if", "m", "in", "mention_to_gold", ":", "\n", "                ", "gold_counts", "[", "tuple", "(", "mention_to_gold", "[", "m", "]", ")", "]", "+=", "1", "\n", "", "", "for", "c2", ",", "count", "in", "gold_counts", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "c2", ")", "!=", "1", ":", "\n", "                ", "correct", "+=", "count", "*", "count", "\n", "\n", "", "", "num", "+=", "correct", "/", "float", "(", "len", "(", "c", ")", ")", "\n", "dem", "+=", "len", "(", "c", ")", "\n", "\n", "", "return", "num", ",", "dem", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.muc": [[140, 153], ["len", "set", "len", "len", "set.add"], "function", ["None"], ["", "def", "muc", "(", "clusters", ",", "mention_to_gold", ")", ":", "\n", "    ", "tp", ",", "p", "=", "0", ",", "0", "\n", "for", "c", "in", "clusters", ":", "\n", "        ", "p", "+=", "len", "(", "c", ")", "-", "1", "\n", "tp", "+=", "len", "(", "c", ")", "\n", "linked", "=", "set", "(", ")", "\n", "for", "m", "in", "c", ":", "\n", "            ", "if", "m", "in", "mention_to_gold", ":", "\n", "                ", "linked", ".", "add", "(", "mention_to_gold", "[", "m", "]", ")", "\n", "", "else", ":", "\n", "                ", "tp", "-=", "1", "\n", "", "", "tp", "-=", "len", "(", "linked", ")", "\n", "", "return", "tp", ",", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.phi4": [[155, 157], ["float", "len", "len", "len"], "function", ["None"], ["", "def", "phi4", "(", "c1", ",", "c2", ")", ":", "\n", "    ", "return", "2", "*", "len", "(", "[", "m", "for", "m", "in", "c1", "if", "m", "in", "c2", "]", ")", "/", "float", "(", "len", "(", "c1", ")", "+", "len", "(", "c2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.ceafe": [[159, 168], ["numpy.zeros", "range", "scipy.optimize.linear_sum_assignment", "sum", "len", "range", "len", "len", "len", "len", "len", "metrics.phi4", "len"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.phi4"], ["", "def", "ceafe", "(", "clusters", ",", "gold_clusters", ")", ":", "\n", "    ", "clusters", "=", "[", "c", "for", "c", "in", "clusters", "if", "len", "(", "c", ")", "!=", "1", "]", "\n", "scores", "=", "np", ".", "zeros", "(", "(", "len", "(", "gold_clusters", ")", ",", "len", "(", "clusters", ")", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "gold_clusters", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "clusters", ")", ")", ":", "\n", "            ", "scores", "[", "i", ",", "j", "]", "=", "phi4", "(", "gold_clusters", "[", "i", "]", ",", "clusters", "[", "j", "]", ")", "\n", "", "", "row_ind", ",", "col_ind", "=", "linear_sum_assignment", "(", "-", "scores", ")", "\n", "similarity", "=", "sum", "(", "scores", "[", "row_ind", ",", "col_ind", "]", ")", "\n", "return", "similarity", ",", "len", "(", "clusters", ")", ",", "similarity", ",", "len", "(", "gold_clusters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.lea": [[170, 189], ["enumerate", "len", "len", "float", "len", "len", "len"], "function", ["None"], ["", "def", "lea", "(", "clusters", ",", "mention_to_gold", ")", ":", "\n", "    ", "num", ",", "dem", "=", "0", ",", "0", "\n", "\n", "for", "c", "in", "clusters", ":", "\n", "        ", "if", "len", "(", "c", ")", "==", "1", ":", "\n", "            ", "continue", "\n", "\n", "", "common_links", "=", "0", "\n", "all_links", "=", "len", "(", "c", ")", "*", "(", "len", "(", "c", ")", "-", "1", ")", "/", "2.0", "\n", "for", "i", ",", "m", "in", "enumerate", "(", "c", ")", ":", "\n", "            ", "if", "m", "in", "mention_to_gold", ":", "\n", "                ", "for", "m2", "in", "c", "[", "i", "+", "1", ":", "]", ":", "\n", "                    ", "if", "m2", "in", "mention_to_gold", "and", "mention_to_gold", "[", "m", "]", "==", "mention_to_gold", "[", "m2", "]", ":", "\n", "                        ", "common_links", "+=", "1", "\n", "\n", "", "", "", "", "num", "+=", "len", "(", "c", ")", "*", "common_links", "/", "float", "(", "all_links", ")", "\n", "dem", "+=", "len", "(", "c", ")", "\n", "\n", "", "return", "num", ",", "dem", "\n", "", ""]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam.__init__": [[20, 55], ["tensorflow.python.training.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "\n", "learning_rate", "=", "0.001", ",", "\n", "beta1", "=", "0.9", ",", "\n", "beta2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-8", ",", "\n", "amsgrad", "=", "False", ",", "\n", "use_locking", "=", "False", ",", "\n", "name", "=", "'RAdam'", ")", ":", "\n", "        ", "r\"\"\"Construct a new Rectified Adam optimizer.\n        Args:\n            learning_rate: A Tensor or a floating point value.    The learning rate.\n            beta1: A float value or a constant float tensor. The exponential decay\n                rate for the 1st moment estimates.\n            beta2: A float value or a constant float tensor. The exponential decay\n                rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n                the paper \"On the Convergence of Adam and beyond\".\n            use_locking: If `True` use locks for update operations.\n            name: Optional name for the operations created when applying gradients.\n                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n                enabled, `learning_rate`, `beta1`, `beta2`, and `epsilon` can each be\n                a callable that takes no arguments and returns the actual value to use.\n                This can be useful for changing these values across different\n                invocations of optimizer functions. @end_compatibility\n        \"\"\"", "\n", "\n", "super", "(", "RAdam", ",", "self", ")", ".", "__init__", "(", "use_locking", ",", "name", ")", "\n", "self", ".", "_lr", "=", "learning_rate", "\n", "self", ".", "_beta1", "=", "beta1", "\n", "self", ".", "_beta2", "=", "beta2", "\n", "self", ".", "_epsilon", "=", "epsilon", "\n", "self", ".", "_amsgrad", "=", "amsgrad", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._get_beta_accumulators": [[56, 61], ["tensorflow.python.ops.init_scope", "tensorflow.python.ops.get_default_graph", "radam.RAdam._get_non_slot_variable", "radam.RAdam._get_non_slot_variable"], "methods", ["None"], ["", "def", "_get_beta_accumulators", "(", "self", ")", ":", "\n", "        ", "with", "ops", ".", "init_scope", "(", ")", ":", "\n", "            ", "graph", "=", "ops", ".", "get_default_graph", "(", ")", "\n", "return", "(", "self", ".", "_get_non_slot_variable", "(", "\"beta1_power\"", ",", "graph", "=", "graph", ")", ",", "\n", "self", ".", "_get_non_slot_variable", "(", "\"beta2_power\"", ",", "graph", "=", "graph", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._get_niter": [[63, 67], ["tensorflow.python.ops.init_scope", "tensorflow.python.ops.get_default_graph", "radam.RAdam._get_non_slot_variable"], "methods", ["None"], ["", "", "def", "_get_niter", "(", "self", ")", ":", "\n", "        ", "with", "ops", ".", "init_scope", "(", ")", ":", "\n", "            ", "graph", "=", "ops", ".", "get_default_graph", "(", ")", "\n", "return", "self", ".", "_get_non_slot_variable", "(", "\"niter\"", ",", "graph", "=", "graph", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._create_slots": [[68, 82], ["min", "radam.RAdam._create_non_slot_variable", "radam.RAdam._create_non_slot_variable", "radam.RAdam._create_non_slot_variable", "radam.RAdam._zeros_slot", "radam.RAdam._zeros_slot", "radam.RAdam._zeros_slot"], "methods", ["None"], ["", "", "def", "_create_slots", "(", "self", ",", "var_list", ")", ":", "\n", "        ", "first_var", "=", "min", "(", "var_list", ",", "key", "=", "lambda", "x", ":", "x", ".", "name", ")", "\n", "self", ".", "_create_non_slot_variable", "(", "\n", "initial_value", "=", "self", ".", "_beta1", ",", "name", "=", "\"beta1_power\"", ",", "colocate_with", "=", "first_var", ")", "\n", "self", ".", "_create_non_slot_variable", "(", "\n", "initial_value", "=", "self", ".", "_beta2", ",", "name", "=", "\"beta2_power\"", ",", "colocate_with", "=", "first_var", ")", "\n", "self", ".", "_create_non_slot_variable", "(", "\n", "initial_value", "=", "1", ",", "name", "=", "\"niter\"", ",", "colocate_with", "=", "first_var", ")", "\n", "for", "var", "in", "var_list", ":", "\n", "            ", "self", ".", "_zeros_slot", "(", "var", ",", "'m'", ",", "self", ".", "_name", ")", "\n", "self", ".", "_zeros_slot", "(", "var", ",", "'v'", ",", "self", ".", "_name", ")", "\n", "", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "for", "var", "in", "var_list", ":", "\n", "                ", "self", ".", "_zeros_slot", "(", "var", ",", "'vhat'", ",", "self", ".", "_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._prepare": [[83, 93], ["radam.RAdam._call_if_callable", "radam.RAdam._call_if_callable", "radam.RAdam._call_if_callable", "radam.RAdam._call_if_callable", "tensorflow.python.ops.convert_to_tensor", "tensorflow.python.ops.convert_to_tensor", "tensorflow.python.ops.convert_to_tensor", "tensorflow.python.ops.convert_to_tensor"], "methods", ["None"], ["", "", "", "def", "_prepare", "(", "self", ")", ":", "\n", "        ", "learning_rate", "=", "self", ".", "_call_if_callable", "(", "self", ".", "_lr", ")", "\n", "beta1", "=", "self", ".", "_call_if_callable", "(", "self", ".", "_beta1", ")", "\n", "beta2", "=", "self", ".", "_call_if_callable", "(", "self", ".", "_beta2", ")", "\n", "epsilon", "=", "self", ".", "_call_if_callable", "(", "self", ".", "_epsilon", ")", "\n", "\n", "self", ".", "_lr_t", "=", "ops", ".", "convert_to_tensor", "(", "learning_rate", ",", "name", "=", "\"learning_rate\"", ")", "\n", "self", ".", "_beta1_t", "=", "ops", ".", "convert_to_tensor", "(", "beta1", ",", "name", "=", "\"beta1\"", ")", "\n", "self", ".", "_beta2_t", "=", "ops", ".", "convert_to_tensor", "(", "beta2", ",", "name", "=", "\"beta2\"", ")", "\n", "self", ".", "_epsilon_t", "=", "ops", ".", "convert_to_tensor", "(", "epsilon", ",", "name", "=", "\"epsilon\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._apply_dense_shared": [[94, 143], ["radam.RAdam._get_beta_accumulators", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "radam.RAdam._get_niter", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "radam.RAdam.get_slot", "tensorflow.python.state_ops.assign", "radam.RAdam.get_slot", "tensorflow.python.state_ops.assign", "tensorflow.python.math_ops.sqrt", "tensorflow.where", "tensorflow.python.state_ops.assign_sub", "tensorflow.python.control_flow_ops.group", "radam.RAdam.get_slot", "tensorflow.python.state_ops.assign", "tensorflow.python.math_ops.sqrt", "tensorflow.python.math_ops.sqrt", "updates.append", "tensorflow.python.math_ops.maximum", "tensorflow.python.math_ops.square"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._get_beta_accumulators", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._get_niter"], ["", "def", "_apply_dense_shared", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "var_dtype", "=", "var", ".", "dtype", ".", "base_dtype", "\n", "beta1_power", ",", "beta2_power", "=", "self", ".", "_get_beta_accumulators", "(", ")", "\n", "beta1_power", "=", "math_ops", ".", "cast", "(", "beta1_power", ",", "var_dtype", ")", "\n", "beta2_power", "=", "math_ops", ".", "cast", "(", "beta2_power", ",", "var_dtype", ")", "\n", "niter", "=", "self", ".", "_get_niter", "(", ")", "\n", "niter", "=", "math_ops", ".", "cast", "(", "niter", ",", "var_dtype", ")", "\n", "lr_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_lr_t", ",", "var_dtype", ")", "\n", "beta1_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta1_t", ",", "var_dtype", ")", "\n", "beta2_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta2_t", ",", "var_dtype", ")", "\n", "epsilon_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_epsilon_t", ",", "var_dtype", ")", "\n", "\n", "sma_inf", "=", "2.0", "/", "(", "1.0", "-", "beta2_t", ")", "-", "1.0", "\n", "sma_t", "=", "sma_inf", "-", "2.0", "*", "niter", "*", "beta2_power", "/", "(", "1.0", "-", "beta2_power", ")", "\n", "\n", "m", "=", "self", ".", "get_slot", "(", "var", ",", "'m'", ")", "\n", "m_t", "=", "state_ops", ".", "assign", "(", "m", ",", "\n", "beta1_t", "*", "m", "+", "(", "1.0", "-", "beta1_t", ")", "*", "grad", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "m_corr_t", "=", "m_t", "/", "(", "1.0", "-", "beta1_power", ")", "\n", "\n", "v", "=", "self", ".", "get_slot", "(", "var", ",", "'v'", ")", "\n", "v_t", "=", "state_ops", ".", "assign", "(", "v", ",", "\n", "beta2_t", "*", "v", "+", "(", "1.0", "-", "beta2_t", ")", "*", "math_ops", ".", "square", "(", "grad", ")", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "vhat", "=", "self", ".", "get_slot", "(", "var", ",", "'vhat'", ")", "\n", "vhat_t", "=", "state_ops", ".", "assign", "(", "vhat", ",", "\n", "math_ops", ".", "maximum", "(", "vhat", ",", "v_t", ")", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "vhat_t", "/", "(", "1.0", "-", "beta2_power", ")", "+", "epsilon_t", ")", "\n", "", "else", ":", "\n", "            ", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "v_t", "/", "(", "1.0", "-", "beta2_power", ")", "+", "epsilon_t", ")", "\n", "\n", "", "r_t", "=", "math_ops", ".", "sqrt", "(", "(", "sma_t", "-", "4.0", ")", "/", "(", "sma_inf", "-", "4.0", ")", "*", "\n", "(", "sma_t", "-", "2.0", ")", "/", "(", "sma_inf", "-", "2.0", ")", "*", "\n", "sma_inf", "/", "sma_t", ")", "\n", "\n", "var_t", "=", "tf", ".", "where", "(", "sma_t", ">", "5.0", ",", "r_t", "*", "m_corr_t", "/", "v_corr_t", ",", "m_corr_t", ")", "\n", "\n", "var_update", "=", "state_ops", ".", "assign_sub", "(", "var", ",", "\n", "lr_t", "*", "var_t", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "updates", "=", "[", "var_update", ",", "m_t", ",", "v_t", "]", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "updates", ".", "append", "(", "vhat_t", ")", "\n", "", "return", "control_flow_ops", ".", "group", "(", "*", "updates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._apply_dense": [[144, 146], ["radam.RAdam._apply_dense_shared"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._apply_dense_shared"], ["", "def", "_apply_dense", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "return", "self", ".", "_apply_dense_shared", "(", "grad", ",", "var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._resource_apply_dense": [[147, 149], ["radam.RAdam._apply_dense_shared"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._apply_dense_shared"], ["", "def", "_resource_apply_dense", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "return", "self", ".", "_apply_dense_shared", "(", "grad", ",", "var", ".", "handle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._apply_sparse_shared": [[150, 199], ["radam.RAdam._get_beta_accumulators", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "radam.RAdam._get_niter", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "tensorflow.python.math_ops.cast", "radam.RAdam.get_slot", "tensorflow.python.state_ops.assign", "radam.RAdam.get_slot", "tensorflow.python.state_ops.assign", "tensorflow.python.math_ops.sqrt", "tensorflow.where", "tensorflow.python.state_ops.assign_sub", "tensorflow.python.control_flow_ops.group", "tensorflow.python.ops.control_dependencies", "scatter_add", "tensorflow.python.ops.control_dependencies", "scatter_add", "radam.RAdam.get_slot", "tensorflow.python.state_ops.assign", "tensorflow.python.math_ops.sqrt", "tensorflow.python.math_ops.sqrt", "updates.append", "tensorflow.python.math_ops.maximum", "tensorflow.python.math_ops.square"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._get_beta_accumulators", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._get_niter"], ["", "def", "_apply_sparse_shared", "(", "self", ",", "grad", ",", "var", ",", "indices", ",", "scatter_add", ")", ":", "\n", "        ", "var_dtype", "=", "var", ".", "dtype", ".", "base_dtype", "\n", "beta1_power", ",", "beta2_power", "=", "self", ".", "_get_beta_accumulators", "(", ")", "\n", "beta1_power", "=", "math_ops", ".", "cast", "(", "beta1_power", ",", "var_dtype", ")", "\n", "beta2_power", "=", "math_ops", ".", "cast", "(", "beta2_power", ",", "var_dtype", ")", "\n", "niter", "=", "self", ".", "_get_niter", "(", ")", "\n", "niter", "=", "math_ops", ".", "cast", "(", "niter", ",", "var_dtype", ")", "\n", "lr_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_lr_t", ",", "var_dtype", ")", "\n", "beta1_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta1_t", ",", "var_dtype", ")", "\n", "beta2_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_beta2_t", ",", "var_dtype", ")", "\n", "epsilon_t", "=", "math_ops", ".", "cast", "(", "self", ".", "_epsilon_t", ",", "var_dtype", ")", "\n", "\n", "sma_inf", "=", "2.0", "/", "(", "1.0", "-", "beta2_t", ")", "-", "1.0", "\n", "sma_t", "=", "sma_inf", "-", "2.0", "*", "niter", "*", "beta2_power", "/", "(", "1.0", "-", "beta2_power", ")", "\n", "\n", "m", "=", "self", ".", "get_slot", "(", "var", ",", "'m'", ")", "\n", "m_t", "=", "state_ops", ".", "assign", "(", "m", ",", "beta1_t", "*", "m", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "with", "ops", ".", "control_dependencies", "(", "[", "m_t", "]", ")", ":", "\n", "            ", "m_t", "=", "scatter_add", "(", "m", ",", "indices", ",", "grad", "*", "(", "1", "-", "beta1_t", ")", ")", "\n", "", "m_corr_t", "=", "m_t", "/", "(", "1.0", "-", "beta1_power", ")", "\n", "\n", "v", "=", "self", ".", "get_slot", "(", "var", ",", "'v'", ")", "\n", "v_t", "=", "state_ops", ".", "assign", "(", "v", ",", "beta2_t", "*", "v", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "with", "ops", ".", "control_dependencies", "(", "[", "v_t", "]", ")", ":", "\n", "            ", "v_t", "=", "scatter_add", "(", "v", ",", "indices", ",", "(", "1.0", "-", "beta2_t", ")", "*", "math_ops", ".", "square", "(", "grad", ")", ")", "\n", "\n", "", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "vhat", "=", "self", ".", "get_slot", "(", "var", ",", "'vhat'", ")", "\n", "vhat_t", "=", "state_ops", ".", "assign", "(", "vhat", ",", "\n", "math_ops", ".", "maximum", "(", "vhat", ",", "v_t", ")", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "vhat_t", "/", "(", "1.0", "-", "beta2_power", ")", "+", "epsilon_t", ")", "\n", "", "else", ":", "\n", "            ", "v_corr_t", "=", "math_ops", ".", "sqrt", "(", "v_t", "/", "(", "1.0", "-", "beta2_power", ")", "+", "epsilon_t", ")", "\n", "\n", "", "r_t", "=", "math_ops", ".", "sqrt", "(", "(", "sma_t", "-", "4.0", ")", "/", "(", "sma_inf", "-", "4.0", ")", "*", "\n", "(", "sma_t", "-", "2.0", ")", "/", "(", "sma_inf", "-", "2.0", ")", "*", "\n", "sma_inf", "/", "sma_t", ")", "\n", "\n", "var_t", "=", "tf", ".", "where", "(", "sma_t", ">", "5.0", ",", "r_t", "*", "m_corr_t", "/", "v_corr_t", ",", "m_corr_t", ")", "\n", "\n", "var_update", "=", "state_ops", ".", "assign_sub", "(", "var", ",", "\n", "lr_t", "*", "var_t", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "\n", "updates", "=", "[", "var_update", ",", "m_t", ",", "v_t", "]", "\n", "if", "self", ".", "_amsgrad", ":", "\n", "            ", "updates", ".", "append", "(", "vhat_t", ")", "\n", "", "return", "control_flow_ops", ".", "group", "(", "*", "updates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._apply_sparse": [[200, 210], ["radam.RAdam._apply_sparse_shared", "tensorflow.python.state_ops.scatter_add"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._apply_sparse_shared"], ["", "def", "_apply_sparse", "(", "self", ",", "grad", ",", "var", ")", ":", "\n", "        ", "return", "self", ".", "_apply_sparse_shared", "(", "\n", "grad", ".", "values", ",", "\n", "var", ",", "\n", "grad", ".", "indices", ",", "\n", "lambda", "x", ",", "i", ",", "v", ":", "state_ops", ".", "scatter_add", "(", "# pylint: disable=g-long-lambda", "\n", "x", ",", "\n", "i", ",", "\n", "v", ",", "\n", "use_locking", "=", "self", ".", "_use_locking", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._resource_apply_sparse": [[211, 214], ["radam.RAdam._apply_sparse_shared"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._apply_sparse_shared"], ["", "def", "_resource_apply_sparse", "(", "self", ",", "grad", ",", "var", ",", "indices", ")", ":", "\n", "        ", "return", "self", ".", "_apply_sparse_shared", "(", "grad", ",", "var", ",", "indices", ",", "\n", "self", ".", "_resource_scatter_add", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._resource_scatter_add": [[215, 219], ["tensorflow.python.ops.control_dependencies", "x.value", "tensorflow.python.resource_variable_ops.resource_scatter_add"], "methods", ["None"], ["", "def", "_resource_scatter_add", "(", "self", ",", "x", ",", "i", ",", "v", ")", ":", "\n", "        ", "with", "ops", ".", "control_dependencies", "(", "\n", "[", "resource_variable_ops", ".", "resource_scatter_add", "(", "x", ".", "handle", ",", "i", ",", "v", ")", "]", ")", ":", "\n", "            ", "return", "x", ".", "value", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._finish": [[220, 234], ["tensorflow.python.control_flow_ops.group", "tensorflow.python.ops.control_dependencies", "radam.RAdam._get_beta_accumulators", "radam.RAdam._get_niter", "tensorflow.python.ops.colocate_with", "beta1_power.assign", "beta2_power.assign", "radam.RAdam.assign"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._get_beta_accumulators", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.radam.RAdam._get_niter"], ["", "", "def", "_finish", "(", "self", ",", "update_ops", ",", "name_scope", ")", ":", "\n", "# Update the power accumulators.", "\n", "        ", "with", "ops", ".", "control_dependencies", "(", "update_ops", ")", ":", "\n", "            ", "beta1_power", ",", "beta2_power", "=", "self", ".", "_get_beta_accumulators", "(", ")", "\n", "niter", "=", "self", ".", "_get_niter", "(", ")", "\n", "with", "ops", ".", "colocate_with", "(", "beta1_power", ")", ":", "\n", "                ", "update_beta1", "=", "beta1_power", ".", "assign", "(", "\n", "beta1_power", "*", "self", ".", "_beta1_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "update_beta2", "=", "beta2_power", ".", "assign", "(", "\n", "beta2_power", "*", "self", ".", "_beta2_t", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "update_niter", "=", "niter", ".", "assign", "(", "\n", "niter", "+", "1", ",", "use_locking", "=", "self", ".", "_use_locking", ")", "\n", "", "", "return", "control_flow_ops", ".", "group", "(", "\n", "*", "update_ops", "+", "[", "update_beta1", ",", "update_beta2", ",", "update_niter", "]", ",", "name", "=", "name_scope", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.get_model": [[20, 25], ["models.corefqa.CorefQAModel", "models.mention_proposal.MentionProposalModel"], "function", ["None"], ["def", "get_model", "(", "config", ",", "model_sign", "=", "\"corefqa\"", ")", ":", "\n", "    ", "if", "model_sign", "==", "\"corefqa\"", ":", "\n", "        ", "return", "corefqa", ".", "CorefQAModel", "(", "config", ")", "\n", "", "else", ":", "\n", "        ", "return", "mention_proposal", ".", "MentionProposalModel", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.initialize_from_env": [[27, 52], ["util.mkdirs", "print", "pyhocon.ConfigFactory.parse_file", "print", "pyhocon.ConfigFactory.parse_file", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "pyhocon.ConfigFactory.parse_file.items", "tensorflow.logging.info", "tensorflow.logging.info", "os.path.join", "tensorflow.logging.info", "os.path.join", "os.path.join", "tensorflow.logging.info", "pyhocon.HOCONConverter.convert", "str", "str"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.mkdirs"], ["", "", "def", "initialize_from_env", "(", "eval_test", "=", "False", ",", "config_params", "=", "\"train_spanbert_base\"", ",", "config_file", "=", "\"experiments_tinybert.conf\"", ",", "use_tpu", "=", "False", ",", "print_info", "=", "False", ")", ":", "\n", "    ", "if", "not", "use_tpu", ":", "\n", "        ", "print", "(", "\"loading experiments.conf ... \"", ")", "\n", "config", "=", "pyhocon", ".", "ConfigFactory", ".", "parse_file", "(", "os", ".", "path", ".", "join", "(", "repo_path", ",", "config_file", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"loading experiments_tpu.conf ... \"", ")", "\n", "config", "=", "pyhocon", ".", "ConfigFactory", ".", "parse_file", "(", "os", ".", "path", ".", "join", "(", "repo_path", ",", "config_file", ")", ")", "\n", "\n", "", "config", "=", "config", "[", "config_params", "]", "\n", "\n", "if", "print_info", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"%*%\"", "*", "20", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"%*%\"", "*", "20", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"%%%%%%%% Configs are showed as follows : %%%%%%%%\"", ")", "\n", "for", "tmp_key", ",", "tmp_value", "in", "config", ".", "items", "(", ")", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "str", "(", "tmp_key", ")", "+", "\" : \"", "+", "str", "(", "tmp_value", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"%*%\"", "*", "20", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"%*%\"", "*", "20", ")", "\n", "\n", "", "config", "[", "\"log_dir\"", "]", "=", "mkdirs", "(", "os", ".", "path", ".", "join", "(", "config", "[", "\"log_root\"", "]", ",", "config_params", ")", ")", "\n", "\n", "if", "print_info", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "pyhocon", ".", "HOCONConverter", ".", "convert", "(", "config", ",", "\"hocon\"", ")", ")", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.copy_checkpoint": [[54, 57], ["shutil.copyfile"], "function", ["None"], ["", "def", "copy_checkpoint", "(", "source", ",", "target", ")", ":", "\n", "    ", "for", "ext", "in", "(", "\".index\"", ",", "\".data-00000-of-00001\"", ")", ":", "\n", "        ", "shutil", ".", "copyfile", "(", "source", "+", "ext", ",", "target", "+", "ext", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.make_summary": [[59, 61], ["tensorflow.Summary", "tensorflow.Summary.Value", "value_dict.items"], "function", ["None"], ["", "", "def", "make_summary", "(", "value_dict", ")", ":", "\n", "    ", "return", "tf", ".", "Summary", "(", "value", "=", "[", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "k", ",", "simple_value", "=", "v", ")", "for", "k", ",", "v", "in", "value_dict", ".", "items", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.flatten": [[63, 65], ["None"], "function", ["None"], ["", "def", "flatten", "(", "l", ")", ":", "\n", "    ", "return", "[", "item", "for", "sublist", "in", "l", "for", "item", "in", "sublist", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.set_gpus": [[67, 71], ["print", "str"], "function", ["None"], ["", "def", "set_gpus", "(", "*", "gpus", ")", ":", "\n", "# pass", "\n", "    ", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "\",\"", ".", "join", "(", "str", "(", "g", ")", "for", "g", "in", "gpus", ")", "\n", "print", "(", "\"Setting CUDA_VISIBLE_DEVICES to: {}\"", ".", "format", "(", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.mkdirs": [[73, 80], ["os.makedirs"], "function", ["None"], ["", "def", "mkdirs", "(", "path", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "os", ".", "makedirs", "(", "path", ")", "\n", "", "except", "OSError", "as", "exception", ":", "\n", "        ", "if", "exception", ".", "errno", "!=", "errno", ".", "EEXIST", ":", "\n", "            ", "raise", "\n", "", "", "return", "path", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.load_char_dict": [[82, 89], ["collections.defaultdict", "collections.defaultdict.update", "codecs.open", "vocab.extend", "l.strip", "enumerate", "f.readlines"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.update"], ["", "def", "load_char_dict", "(", "char_vocab_path", ")", ":", "\n", "    ", "vocab", "=", "[", "u\"<unk>\"", "]", "\n", "with", "codecs", ".", "open", "(", "char_vocab_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "vocab", ".", "extend", "(", "l", ".", "strip", "(", ")", "for", "l", "in", "f", ".", "readlines", "(", ")", ")", "\n", "", "char_dict", "=", "collections", ".", "defaultdict", "(", "int", ")", "\n", "char_dict", ".", "update", "(", "{", "c", ":", "i", "for", "i", ",", "c", "in", "enumerate", "(", "vocab", ")", "}", ")", "\n", "return", "char_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.maybe_divide": [[91, 93], ["float"], "function", ["None"], ["", "def", "maybe_divide", "(", "x", ",", "y", ")", ":", "\n", "    ", "return", "0", "if", "y", "==", "0", "else", "x", "/", "float", "(", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.shape": [[96, 98], ["tensorflow.shape", "x.get_shape"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape"], ["", "def", "shape", "(", "x", ",", "dim", ")", ":", "\n", "    ", "return", "x", ".", "get_shape", "(", ")", "[", "dim", "]", ".", "value", "or", "tf", ".", "shape", "(", "x", ")", "[", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.ffnn": [[100, 112], ["tensorflow.truncated_normal_initializer", "tensorflow.truncated_normal_initializer", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.relu", "len", "ValueError", "tensorflow.nn.xw_plus_b", "inputs.get_shape", "tensorflow.zeros_initializer", "len", "inputs.get_shape"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape"], ["", "def", "ffnn", "(", "inputs", ",", "num_hidden_layers", ",", "hidden_size", ",", "output_size", ",", "dropout", ",", "\n", "output_weights_initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ",", "\n", "hidden_initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", ":", "\n", "    ", "if", "len", "(", "inputs", ".", "get_shape", "(", ")", ")", ">", "3", ":", "\n", "        ", "raise", "ValueError", "(", "\"FFNN with rank {} not supported\"", ".", "format", "(", "len", "(", "inputs", ".", "get_shape", "(", ")", ")", ")", ")", "\n", "", "current_inputs", "=", "inputs", "\n", "hidden_weights", "=", "tf", ".", "get_variable", "(", "\"hidden_weights\"", ",", "[", "hidden_size", ",", "output_size", "]", ",", "\n", "initializer", "=", "hidden_initializer", ")", "\n", "hidden_bias", "=", "tf", ".", "get_variable", "(", "\"hidden_bias\"", ",", "[", "output_size", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "current_outputs", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "current_inputs", ",", "hidden_weights", ",", "hidden_bias", ")", ")", "\n", "\n", "return", "current_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.batch_gather": [[114, 127], ["util.shape", "util.shape", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.gather", "len", "util.shape", "len", "tensorflow.squeeze", "emb.get_shape", "tensorflow.range", "emb.get_shape"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape"], ["", "def", "batch_gather", "(", "emb", ",", "indices", ")", ":", "\n", "    ", "batch_size", "=", "shape", "(", "emb", ",", "0", ")", "\n", "seqlen", "=", "shape", "(", "emb", ",", "1", ")", "\n", "if", "len", "(", "emb", ".", "get_shape", "(", ")", ")", ">", "2", ":", "\n", "        ", "emb_size", "=", "shape", "(", "emb", ",", "2", ")", "\n", "", "else", ":", "\n", "        ", "emb_size", "=", "1", "\n", "", "flattened_emb", "=", "tf", ".", "reshape", "(", "emb", ",", "[", "batch_size", "*", "seqlen", ",", "emb_size", "]", ")", "# [batch_size * seqlen, emb]", "\n", "offset", "=", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "batch_size", ")", "*", "seqlen", ",", "1", ")", "# [batch_size, 1]", "\n", "gathered", "=", "tf", ".", "gather", "(", "flattened_emb", ",", "indices", "+", "offset", ")", "# [batch_size, num_indices, emb]", "\n", "if", "len", "(", "emb", ".", "get_shape", "(", ")", ")", "==", "2", ":", "\n", "        ", "gathered", "=", "tf", ".", "squeeze", "(", "gathered", ",", "2", ")", "# [batch_size, num_indices]", "\n", "", "return", "gathered", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.conll.get_doc_key": [[28, 30], ["int"], "function", ["None"], ["def", "get_doc_key", "(", "doc_id", ",", "part", ")", ":", "\n", "    ", "return", "\"{}_{}\"", ".", "format", "(", "doc_id", ",", "int", "(", "part", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.conll.output_conll": [[32, 86], ["predictions.items", "input_file.readlines", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "enumerate", "collections.defaultdict.items", "collections.defaultdict.items", "line.split", "len", "output_file.write", "row[].startswith", "re.match", "output_file.write", "output_file.write", "output_file.write", "output_file.write", "word_map[].append", "start_map[].append", "end_map[].append", "sorted", "sorted", "conll.get_doc_key", "conll.get_doc_key", "len", "re.match.group", "re.match.group", "coref_list.append", "coref_list.append", "coref_list.append", "operator.itemgetter", "operator.itemgetter"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.conll.get_doc_key", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.conll.get_doc_key"], ["", "def", "output_conll", "(", "input_file", ",", "output_file", ",", "predictions", ",", "subtoken_map", ")", ":", "\n", "    ", "prediction_map", "=", "{", "}", "\n", "for", "doc_key", ",", "clusters", "in", "predictions", ".", "items", "(", ")", ":", "\n", "        ", "start_map", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "end_map", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "word_map", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "cluster_id", ",", "mentions", "in", "enumerate", "(", "clusters", ")", ":", "\n", "            ", "for", "start", ",", "end", "in", "mentions", ":", "\n", "                ", "start", ",", "end", "=", "subtoken_map", "[", "doc_key", "]", "[", "start", "]", ",", "subtoken_map", "[", "doc_key", "]", "[", "end", "]", "\n", "if", "start", "==", "end", ":", "\n", "                    ", "word_map", "[", "start", "]", ".", "append", "(", "cluster_id", ")", "\n", "", "else", ":", "\n", "                    ", "start_map", "[", "start", "]", ".", "append", "(", "(", "cluster_id", ",", "end", ")", ")", "\n", "end_map", "[", "end", "]", ".", "append", "(", "(", "cluster_id", ",", "start", ")", ")", "\n", "", "", "", "for", "k", ",", "v", "in", "start_map", ".", "items", "(", ")", ":", "\n", "            ", "start_map", "[", "k", "]", "=", "[", "cluster_id", "for", "cluster_id", ",", "end", "in", "sorted", "(", "v", ",", "key", "=", "operator", ".", "itemgetter", "(", "1", ")", ",", "reverse", "=", "True", ")", "]", "\n", "", "for", "k", ",", "v", "in", "end_map", ".", "items", "(", ")", ":", "\n", "            ", "end_map", "[", "k", "]", "=", "[", "cluster_id", "for", "cluster_id", ",", "start", "in", "sorted", "(", "v", ",", "key", "=", "operator", ".", "itemgetter", "(", "1", ")", ",", "reverse", "=", "True", ")", "]", "\n", "", "prediction_map", "[", "doc_key", "]", "=", "(", "start_map", ",", "end_map", ",", "word_map", ")", "\n", "\n", "", "word_index", "=", "0", "\n", "for", "line", "in", "input_file", ".", "readlines", "(", ")", ":", "\n", "        ", "row", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "row", ")", "==", "0", ":", "\n", "            ", "output_file", ".", "write", "(", "\"\\n\"", ")", "\n", "", "elif", "row", "[", "0", "]", ".", "startswith", "(", "\"#\"", ")", ":", "\n", "            ", "begin_match", "=", "re", ".", "match", "(", "BEGIN_DOCUMENT_REGEX", ",", "line", ")", "\n", "if", "begin_match", ":", "\n", "                ", "doc_key", "=", "get_doc_key", "(", "begin_match", ".", "group", "(", "1", ")", ",", "begin_match", ".", "group", "(", "2", ")", ")", "\n", "start_map", ",", "end_map", ",", "word_map", "=", "prediction_map", "[", "doc_key", "]", "\n", "word_index", "=", "0", "\n", "", "output_file", ".", "write", "(", "line", ")", "\n", "output_file", ".", "write", "(", "\"\\n\"", ")", "\n", "", "else", ":", "\n", "            ", "assert", "get_doc_key", "(", "row", "[", "0", "]", ",", "row", "[", "1", "]", ")", "==", "doc_key", "\n", "coref_list", "=", "[", "]", "\n", "if", "word_index", "in", "end_map", ":", "\n", "                ", "for", "cluster_id", "in", "end_map", "[", "word_index", "]", ":", "\n", "                    ", "coref_list", ".", "append", "(", "\"{})\"", ".", "format", "(", "cluster_id", ")", ")", "\n", "", "", "if", "word_index", "in", "word_map", ":", "\n", "                ", "for", "cluster_id", "in", "word_map", "[", "word_index", "]", ":", "\n", "                    ", "coref_list", ".", "append", "(", "\"({})\"", ".", "format", "(", "cluster_id", ")", ")", "\n", "", "", "if", "word_index", "in", "start_map", ":", "\n", "                ", "for", "cluster_id", "in", "start_map", "[", "word_index", "]", ":", "\n", "                    ", "coref_list", ".", "append", "(", "\"({}\"", ".", "format", "(", "cluster_id", ")", ")", "\n", "\n", "", "", "if", "len", "(", "coref_list", ")", "==", "0", ":", "\n", "                ", "row", "[", "-", "1", "]", "=", "\"-\"", "\n", "", "else", ":", "\n", "                ", "row", "[", "-", "1", "]", "=", "\"|\"", ".", "join", "(", "coref_list", ")", "\n", "\n", "", "output_file", ".", "write", "(", "\"   \"", ".", "join", "(", "row", ")", ")", "\n", "output_file", ".", "write", "(", "\"\\n\"", ")", "\n", "word_index", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.conll.official_conll_eval": [[88, 107], ["subprocess.Popen", "subprocess.Popen.communicate", "subprocess.Popen.wait", "stdout.decode.decode", "re.match", "float", "float", "float", "os.path.join", "print", "print", "print", "re.match.group", "re.match.group", "re.match.group"], "function", ["None"], ["", "", "", "def", "official_conll_eval", "(", "gold_path", ",", "predicted_path", ",", "metric", ",", "official_stdout", "=", "False", ")", ":", "\n", "    ", "cmd", "=", "[", "\"perl\"", ",", "os", ".", "path", ".", "join", "(", "REPO_PATH", ",", "\"conll-2012/scorer/v8.01/scorer.pl\"", ")", ",", "metric", ",", "gold_path", ",", "predicted_path", ",", "\"none\"", "]", "\n", "process", "=", "subprocess", ".", "Popen", "(", "cmd", ",", "stdout", "=", "subprocess", ".", "PIPE", ")", "\n", "stdout", ",", "stderr", "=", "process", ".", "communicate", "(", ")", "\n", "process", ".", "wait", "(", ")", "\n", "\n", "stdout", "=", "stdout", ".", "decode", "(", "\"utf-8\"", ")", "\n", "if", "stderr", "is", "not", "None", ":", "\n", "        ", "print", "(", "stderr", ")", "\n", "\n", "", "if", "official_stdout", ":", "\n", "        ", "print", "(", "\"Official result for {}\"", ".", "format", "(", "metric", ")", ")", "\n", "print", "(", "stdout", ")", "\n", "\n", "", "coref_results_match", "=", "re", ".", "match", "(", "COREF_RESULTS_REGEX", ",", "stdout", ")", "\n", "recall", "=", "float", "(", "coref_results_match", ".", "group", "(", "1", ")", ")", "\n", "precision", "=", "float", "(", "coref_results_match", ".", "group", "(", "2", ")", ")", "\n", "f1", "=", "float", "(", "coref_results_match", ".", "group", "(", "3", ")", ")", "\n", "return", "{", "\"r\"", ":", "recall", ",", "\"p\"", ":", "precision", ",", "\"f\"", ":", "f1", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.conll.evaluate_conll": [[109, 116], ["tempfile.NamedTemporaryFile", "print", "conll.official_conll_eval", "open", "conll.output_conll"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.conll.official_conll_eval", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.conll.output_conll"], ["", "def", "evaluate_conll", "(", "gold_path", ",", "predictions", ",", "subtoken_maps", ",", "official_stdout", "=", "False", ")", ":", "\n", "    ", "with", "tempfile", ".", "NamedTemporaryFile", "(", "delete", "=", "False", ",", "mode", "=", "\"w\"", ")", "as", "prediction_file", ":", "\n", "        ", "with", "open", "(", "gold_path", ",", "\"r\"", ")", "as", "gold_file", ":", "\n", "            ", "output_conll", "(", "gold_file", ",", "prediction_file", ",", "predictions", ",", "subtoken_maps", ")", "\n", "", "print", "(", "\"Predicted conll file: {}\"", ".", "format", "(", "prediction_file", ".", "name", ")", ")", "\n", "", "return", "{", "m", ":", "official_conll_eval", "(", "gold_file", ".", "name", ",", "prediction_file", ".", "name", ",", "m", ",", "official_stdout", ")", "for", "m", "in", "\n", "(", "\"muc\"", ",", "\"bcub\"", ",", "\"ceafe\"", ")", "}", "\n", "", ""]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.config_utils.ModelConfig.__init__": [[18, 26], ["tf_flags.flag_values_dict", "tf_flags.flag_values_dict.items", "os.path.join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tf_flags", ",", "output_dir", ",", "model_sign", "=", "\"model\"", ")", ":", "\n", "        ", "key_value_pairs", "=", "tf_flags", ".", "flag_values_dict", "(", ")", "\n", "\n", "for", "item_key", ",", "item_value", "in", "key_value_pairs", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "__dict__", "[", "item_key", "]", "=", "item_value", "\n", "\n", "", "self", ".", "output_dir", "=", "output_dir", "\n", "config_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "output_dir", ",", "\"{}_config.json\"", ".", "format", "(", "model_sign", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.config_utils.ModelConfig.logging_configs": [[27, 34], ["tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "config_utils.ModelConfig.__dict__.items", "tensorflow.logging.info", "str", "str"], "methods", ["None"], ["", "def", "logging_configs", "(", "self", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"$*$\"", "*", "30", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"****** print model configs : ******\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"$*$\"", "*", "30", ")", "\n", "\n", "for", "item_key", ",", "item_value", "in", "self", ".", "__dict__", ".", "items", "(", ")", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"{} : {}\"", ".", "format", "(", "str", "(", "item_key", ")", ",", "str", "(", "item_value", ")", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.input_fn_builder.file_based_input_fn_builder": [[15, 63], ["tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.io.parse_single_example", "list", "tensorflow.data.TFRecordDataset", "d.shuffle.apply", "tf.io.parse_single_example.keys", "d.shuffle.repeat", "d.shuffle.shuffle", "tensorflow.contrib.data.map_and_batch", "tensorflow.to_int32", "input_fn_builder.file_based_input_fn_builder._decode_record"], "function", ["None"], ["def", "file_based_input_fn_builder", "(", "input_file", ",", "num_window", "=", "None", ",", "window_size", "=", "None", ",", "max_num_mention", "=", "None", ",", "is_training", "=", "False", ",", "drop_remainder", "=", "True", ")", ":", "\n", "    ", "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"", "\n", "name_to_features", "=", "{", "\n", "'sentence_map'", ":", "tf", ".", "FixedLenFeature", "(", "[", "num_window", "*", "window_size", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'text_len'", ":", "tf", ".", "FixedLenFeature", "(", "[", "num_window", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'subtoken_map'", ":", "tf", ".", "FixedLenFeature", "(", "[", "num_window", "*", "window_size", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'speaker_ids'", ":", "tf", ".", "FixedLenFeature", "(", "[", "num_window", "*", "window_size", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'flattened_input_ids'", ":", "tf", ".", "FixedLenFeature", "(", "[", "num_window", "*", "window_size", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'flattened_input_mask'", ":", "tf", ".", "FixedLenFeature", "(", "[", "num_window", "*", "window_size", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'span_starts'", ":", "tf", ".", "FixedLenFeature", "(", "[", "max_num_mention", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'span_ends'", ":", "tf", ".", "FixedLenFeature", "(", "[", "max_num_mention", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'cluster_ids'", ":", "tf", ".", "FixedLenFeature", "(", "[", "max_num_mention", "]", ",", "tf", ".", "int64", ")", ",", "\n", "}", "\n", "\n", "\n", "def", "_decode_record", "(", "record", ",", "name_to_features", ")", ":", "\n", "        ", "\"\"\"Decodes a record to a TensorFlow example.\"\"\"", "\n", "example", "=", "tf", ".", "io", ".", "parse_single_example", "(", "record", ",", "name_to_features", ")", "\n", "# tf.Example only supports tf.int64, but the TPU only supports tf.int32.", "\n", "# So cast all int64 to int32.", "\n", "for", "name", "in", "list", "(", "example", ".", "keys", "(", ")", ")", ":", "\n", "            ", "t", "=", "example", "[", "name", "]", "\n", "if", "t", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "                ", "t", "=", "tf", ".", "to_int32", "(", "t", ")", "\n", "", "example", "[", "name", "]", "=", "t", "\n", "", "return", "example", "\n", "\n", "\n", "", "def", "input_fn_from_tfrecord", "(", "params", ")", ":", "\n", "        ", "\"\"\"The actual input function.\"\"\"", "\n", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "# For training, we want a lot of parallel reading and shuffling.", "\n", "# For eval, we want no shuffling and parallel reading doesn't matter.", "\n", "d", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "input_file", ")", "\n", "if", "is_training", ":", "\n", "            ", "d", "=", "d", ".", "repeat", "(", ")", "\n", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "100", ")", "\n", "\n", "", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "\n", "lambda", "record", ":", "_decode_record", "(", "record", ",", "name_to_features", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "drop_remainder", "=", "drop_remainder", ")", ")", "\n", "\n", "return", "d", "\n", "\n", "", "return", "input_fn_from_tfrecord", "", "", ""]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.model_fn_builder.model_fn_builder": [[18, 218], ["utils.util.get_model", "utils.util.get_model", "tensorflow.logging.info", "tensorflow.logging.info", "sorted", "util.get_model.get_mention_proposal_and_loss", "util.get_model.get_gold_mention_sequence_labels_from_pad_index", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "sorted", "util.get_model.get_coreference_resolution_and_loss", "ValueError", "tensorflow.train.Scaffold", "features.keys", "tensorflow.logging.info", "tensorflow.train.AdamOptimizer", "tensorflow.contrib.tpu.CrossShardOptimizer", "utils.radam.RAdam.minimize", "tensorflow.contrib.tpu.TPUEstimatorSpec", "utils.radam.RAdam", "utils.radam.RAdam.minimize", "tensorflow.train.LoggingTensorHook", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.logging.info", "util.get_model.get_mention_proposal_and_loss", "util.get_model.get_mention_proposal_and_loss", "util.get_model.get_gold_mention_sequence_labels_from_pad_index", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.train.Scaffold", "features.keys", "tensorflow.logging.info", "tensorflow.train.AdamOptimizer", "tensorflow.contrib.tpu.CrossShardOptimizer", "utils.radam.RAdam.minimize", "tensorflow.contrib.tpu.TPUEstimatorSpec", "utils.radam.RAdam", "utils.radam.RAdam.minimize", "tensorflow.train.LoggingTensorHook", "tensorflow.contrib.tpu.TPUEstimatorSpec", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.train.get_global_step", "tensorflow.train.get_global_step", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.tile", "tensorflow.tile", "tensorflow.cast", "tensorflow.cast", "tensorflow.logging.info", "util.get_model.get_mention_proposal_and_loss", "util.get_model.get_gold_mention_sequence_labels_from_pad_index", "tensorflow.contrib.tpu.TPUEstimatorSpec", "ValueError", "tensorflow.train.get_global_step", "tensorflow.train.get_global_step", "tensorflow.logging.info", "util.get_model.get_coreference_resolution_and_loss", "tensorflow.math.argmax", "tensorflow.contrib.tpu.TPUEstimatorSpec", "ValueError", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.compat.v1.metrics.precision", "tensorflow.compat.v1.metrics.recall", "tensorflow.math.greater_equal"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.get_model", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.get_model", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_mention_proposal_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_gold_mention_sequence_labels_from_pad_index", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_coreference_resolution_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_mention_proposal_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_mention_proposal_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_gold_mention_sequence_labels_from_pad_index", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_mention_proposal_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_gold_mention_sequence_labels_from_pad_index", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_coreference_resolution_and_loss"], ["def", "model_fn_builder", "(", "config", ",", "model_sign", "=", "\"mention_proposal\"", ")", ":", "\n", "\n", "    ", "def", "mention_proposal_model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "\n", "        ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "input_ids", "=", "features", "[", "\"flattened_input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"flattened_input_mask\"", "]", "\n", "text_len", "=", "features", "[", "\"text_len\"", "]", "\n", "speaker_ids", "=", "features", "[", "\"speaker_ids\"", "]", "\n", "gold_starts", "=", "features", "[", "\"span_starts\"", "]", "\n", "gold_ends", "=", "features", "[", "\"span_ends\"", "]", "\n", "cluster_ids", "=", "features", "[", "\"cluster_ids\"", "]", "\n", "sentence_map", "=", "features", "[", "\"sentence_map\"", "]", "\n", "\n", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "model", "=", "util", ".", "get_model", "(", "config", ",", "model_sign", "=", "\"mention_proposal\"", ")", "\n", "\n", "if", "config", ".", "use_tpu", ":", "\n", "            ", "def", "tpu_scaffold", "(", ")", ":", "\n", "                ", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "            ", "scaffold_fn", "=", "None", "\n", "\n", "", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"****************************** tf.estimator.ModeKeys.TRAIN ******************************\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"********* Features *********\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "                ", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "instance", "=", "(", "input_ids", ",", "input_mask", ",", "sentence_map", ",", "text_len", ",", "speaker_ids", ",", "gold_starts", ",", "gold_ends", ",", "cluster_ids", ")", "\n", "total_loss", ",", "start_scores", ",", "end_scores", ",", "span_scores", "=", "model", ".", "get_mention_proposal_and_loss", "(", "instance", ",", "is_training", ")", "\n", "gold_start_sequence_labels", ",", "gold_end_sequence_labels", ",", "gold_span_sequence_labels", "=", "model", ".", "get_gold_mention_sequence_labels_from_pad_index", "(", "gold_starts", ",", "gold_ends", ",", "text_len", ")", "\n", "\n", "if", "config", ".", "use_tpu", ":", "\n", "                ", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "config", ".", "learning_rate", ",", "beta1", "=", "0.9", ",", "beta2", "=", "0.999", ",", "epsilon", "=", "1e-08", ")", "\n", "optimizer", "=", "tf", ".", "contrib", ".", "tpu", ".", "CrossShardOptimizer", "(", "optimizer", ")", "\n", "train_op", "=", "optimizer", ".", "minimize", "(", "total_loss", ",", "tf", ".", "train", ".", "get_global_step", "(", ")", ")", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "                ", "optimizer", "=", "RAdam", "(", "learning_rate", "=", "config", ".", "learning_rate", ",", "epsilon", "=", "1e-8", ",", "beta1", "=", "0.9", ",", "beta2", "=", "0.999", ")", "\n", "train_op", "=", "optimizer", ".", "minimize", "(", "total_loss", ",", "tf", ".", "train", ".", "get_global_step", "(", ")", ")", "\n", "\n", "train_logging_hook", "=", "tf", ".", "train", ".", "LoggingTensorHook", "(", "{", "\"loss\"", ":", "total_loss", "}", ",", "every_n_iter", "=", "1", ")", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ",", "\n", "training_hooks", "=", "[", "train_logging_hook", "]", ")", "\n", "\n", "", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"****************************** tf.estimator.ModeKeys.EVAL ******************************\"", ")", "\n", "\n", "instance", "=", "(", "input_ids", ",", "input_mask", ",", "sentence_map", ",", "text_len", ",", "speaker_ids", ",", "gold_starts", ",", "gold_ends", ",", "cluster_ids", ")", "\n", "total_loss", ",", "start_scores", ",", "end_scores", ",", "span_scores", "=", "model", ".", "get_mention_proposal_and_loss", "(", "instance", ",", "is_training", ")", "\n", "total_loss", ",", "start_scores", ",", "end_scores", ",", "span_scores", "=", "model", ".", "get_mention_proposal_and_loss", "(", "instance", ",", "is_training", ")", "\n", "gold_start_sequence_labels", ",", "gold_end_sequence_labels", ",", "gold_span_sequence_labels", "=", "model", ".", "get_gold_mention_sequence_labels_from_pad_index", "(", "gold_starts", ",", "gold_ends", ",", "text_len", ")", "\n", "\n", "def", "metric_fn", "(", "start_scores", ",", "end_scores", ",", "span_scores", ",", "gold_span_label", ")", ":", "\n", "                ", "start_scores", "=", "tf", ".", "reshape", "(", "start_scores", ",", "[", "-", "1", ",", "config", ".", "window_size", "]", ")", "\n", "end_scores", "=", "tf", ".", "reshape", "(", "end_scores", ",", "[", "-", "1", ",", "config", ".", "window_size", "]", ")", "\n", "start_scores", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "start_scores", ",", "2", ")", ",", "[", "1", ",", "1", ",", "config", ".", "window_size", "]", ")", "\n", "end_scores", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "end_scores", ",", "2", ")", ",", "[", "1", ",", "1", ",", "config", ".", "window_size", "]", ")", "\n", "sce_span_scores", "=", "(", "start_scores", "+", "end_scores", "+", "span_scores", ")", "/", "3", "\n", "pred_span_label", "=", "tf", ".", "cast", "(", "tf", ".", "reshape", "(", "tf", ".", "math", ".", "greater_equal", "(", "sce_span_scores", ",", "config", ".", "mention_threshold", ")", ",", "[", "-", "1", "]", ")", ",", "tf", ".", "bool", ")", "\n", "\n", "gold_span_label", "=", "tf", ".", "cast", "(", "tf", ".", "reshape", "(", "gold_span_sequence_labels", ",", "[", "-", "1", "]", ")", ",", "tf", ".", "bool", ")", "\n", "\n", "return", "{", "\"precision\"", ":", "tf", ".", "compat", ".", "v1", ".", "metrics", ".", "precision", "(", "gold_span_label", ",", "pred_span_label", ")", ",", "\n", "\"recall\"", ":", "tf", ".", "compat", ".", "v1", ".", "metrics", ".", "recall", "(", "gold_span_label", ",", "pred_span_label", ")", "}", "\n", "\n", "", "eval_metrics", "=", "(", "metric_fn", ",", "[", "start_scores", ",", "end_scores", ",", "span_scores", "]", ")", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ",", "\n", "loss", "=", "total_loss", ",", "\n", "eval_metrics", "=", "eval_metrics", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"****************************** tf.estimator.ModeKeys.PREDICT ******************************\"", ")", "\n", "\n", "instance", "=", "(", "input_ids", ",", "input_mask", ",", "sentence_map", ",", "text_len", ",", "speaker_ids", ",", "gold_starts", ",", "gold_ends", ",", "cluster_ids", ")", "\n", "total_loss", ",", "start_scores", ",", "end_scores", ",", "span_scores", "=", "model", ".", "get_mention_proposal_and_loss", "(", "instance", ",", "is_training", ")", "\n", "gold_start_sequence_labels", ",", "gold_end_sequence_labels", ",", "gold_span_sequence_labels", "=", "model", ".", "get_gold_mention_sequence_labels_from_pad_index", "(", "gold_starts", ",", "gold_ends", ",", "text_len", ")", "\n", "predictions", "=", "{", "\n", "\"total_loss\"", ":", "total_loss", ",", "\n", "\"start_scores\"", ":", "start_scores", ",", "\n", "\"start_gold\"", ":", "gold_starts", ",", "\n", "\"end_gold\"", ":", "gold_ends", ",", "\n", "\"end_scores\"", ":", "end_scores", ",", "\n", "\"span_scores\"", ":", "span_scores", "\n", "}", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ",", "\n", "predictions", "=", "predictions", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Please check the the mode ! \"", ")", "\n", "\n", "", "return", "output_spec", "\n", "\n", "\n", "", "def", "corefqa_model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "\n", "\n", "        ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "input_ids", "=", "features", "[", "\"flattened_input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"flattened_input_mask\"", "]", "\n", "text_len", "=", "features", "[", "\"text_len\"", "]", "\n", "speaker_ids", "=", "features", "[", "\"speaker_ids\"", "]", "\n", "gold_starts", "=", "features", "[", "\"span_starts\"", "]", "\n", "gold_ends", "=", "features", "[", "\"span_ends\"", "]", "\n", "cluster_ids", "=", "features", "[", "\"cluster_ids\"", "]", "\n", "sentence_map", "=", "features", "[", "\"sentence_map\"", "]", "\n", "\n", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "model", "=", "util", ".", "get_model", "(", "config", ",", "model_sign", "=", "\"corefqa\"", ")", "\n", "\n", "if", "config", ".", "use_tpu", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"****************************** Training on TPU ******************************\"", ")", "\n", "def", "tpu_scaffold", "(", ")", ":", "\n", "                ", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "            ", "scaffold_fn", "=", "None", "\n", "\n", "\n", "", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"****************************** tf.estimator.ModeKeys.TRAIN ******************************\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"********* Features *********\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "                ", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "instance", "=", "(", "input_ids", ",", "input_mask", ",", "sentence_map", ",", "text_len", ",", "speaker_ids", ",", "gold_starts", ",", "gold_ends", ",", "cluster_ids", ")", "\n", "total_loss", ",", "(", "topk_mention_start_indices", ",", "topk_mention_end_indices", ")", ",", "(", "forward_topc_mention_start_indices", ",", "forward_topc_mention_end_indices", ")", ",", "top_mention_span_linking_scores", "=", "model", ".", "get_coreference_resolution_and_loss", "(", "instance", ",", "is_training", ",", "use_tpu", "=", "config", ".", "use_tpu", ")", "\n", "\n", "if", "config", ".", "use_tpu", ":", "\n", "                ", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "config", ".", "learning_rate", ",", "beta1", "=", "0.9", ",", "beta2", "=", "0.999", ",", "epsilon", "=", "1e-08", ")", "\n", "optimizer", "=", "tf", ".", "contrib", ".", "tpu", ".", "CrossShardOptimizer", "(", "optimizer", ")", "\n", "train_op", "=", "optimizer", ".", "minimize", "(", "total_loss", ",", "tf", ".", "train", ".", "get_global_step", "(", ")", ")", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "                ", "optimizer", "=", "RAdam", "(", "learning_rate", "=", "config", ".", "learning_rate", ",", "epsilon", "=", "1e-8", ",", "beta1", "=", "0.9", ",", "beta2", "=", "0.999", ")", "\n", "train_op", "=", "optimizer", ".", "minimize", "(", "total_loss", ",", "tf", ".", "train", ".", "get_global_step", "(", ")", ")", "\n", "\n", "training_logging_hook", "=", "tf", ".", "train", ".", "LoggingTensorHook", "(", "{", "\"loss\"", ":", "total_loss", "}", ",", "every_n_iter", "=", "1", ")", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ",", "\n", "training_hooks", "=", "[", "training_logging_hook", "]", ")", "\n", "\n", "\n", "", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"****************************** tf.estimator.ModeKeys.EVAL ******************************\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"@@@@@ MERELY support tf.estimator.ModeKeys.PREDICT ! @@@@@\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"@@@@@ YOU can EVAL your checkpoints after the training process. @@@@@\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"****************************** tf.estimator.ModeKeys.EVAL ******************************\"", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"****************************** tf.estimator.ModeKeys.PREDICT ******************************\"", ")", "\n", "\n", "instance", "=", "(", "input_ids", ",", "input_mask", ",", "sentence_map", ",", "text_len", ",", "speaker_ids", ",", "gold_starts", ",", "gold_ends", ",", "cluster_ids", ")", "\n", "total_loss", ",", "(", "topk_mention_start_indices", ",", "topk_mention_end_indices", ")", ",", "(", "forward_topc_mention_start_indices", ",", "forward_topc_mention_end_indices", ")", ",", "top_mention_span_linking_scores", "=", "model", ".", "get_coreference_resolution_and_loss", "(", "instance", ",", "True", ",", "use_tpu", "=", "config", ".", "use_tpu", ")", "\n", "\n", "top_antecedent", "=", "tf", ".", "math", ".", "argmax", "(", "top_mention_span_linking_scores", ",", "axis", "=", "-", "1", ")", "\n", "predictions", "=", "{", "\n", "\"total_loss\"", ":", "total_loss", ",", "\n", "\"topk_span_starts\"", ":", "topk_mention_start_indices", ",", "\n", "\"topk_span_ends\"", ":", "topk_mention_end_indices", ",", "\n", "\"top_antecedent_scores\"", ":", "top_mention_span_linking_scores", ",", "\n", "\"top_antecedent\"", ":", "top_antecedent", ",", "\n", "\"cluster_ids\"", ":", "cluster_ids", ",", "\n", "\"gold_starts\"", ":", "gold_starts", ",", "\n", "\"gold_ends\"", ":", "gold_ends", "}", "\n", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "mode", "=", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ",", "\n", "predictions", "=", "predictions", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Please check the the mode ! \"", ")", "\n", "", "return", "output_spec", "\n", "\n", "\n", "", "if", "model_sign", "==", "\"mention_proposal\"", ":", "\n", "        ", "return", "mention_proposal_model_fn", "\n", "", "elif", "model_sign", "==", "\"corefqa\"", ":", "\n", "        ", "return", "corefqa_model_fn", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Please check the model sign! Only support [mention_proposal] and [corefqa] .\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.prepare_train_dataset": [[87, 121], ["tensorflow.python_io.TFRecordWriter", "build_dataset_to_tfrecord.read_conll_file", "enumerate", "bert.tokenization.FullTokenizer", "os.path.join", "build_dataset_to_tfrecord.parse_document", "build_dataset_to_tfrecord.tokenize_document", "build_dataset_to_tfrecord.convert_to_sliding_window", "build_dataset_to_tfrecord.flatten_clusters", "build_dataset_to_tfrecord.write_instance_to_example_file", "open", "json.dump", "os.path.join", "os.path.join", "bert.tokenization.FullTokenizer.convert_tokens_to_ids", "os.path.join"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.read_conll_file", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.parse_document", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.tokenize_document", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.convert_to_sliding_window", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.flatten_clusters", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.write_instance_to_example_file", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_tokens_to_ids"], ["def", "prepare_train_dataset", "(", "input_file", ",", "output_data_dir", ",", "output_filename", ",", "window_size", ",", "num_window", ",", "\n", "tokenizer", "=", "None", ",", "vocab_file", "=", "None", ",", "language", "=", "\"english\"", ",", "max_doc_length", "=", "None", ",", "genres", "=", "None", ",", "\n", "max_num_mention", "=", "10", ",", "max_num_cluster", "=", "30", ",", "demo", "=", "False", ",", "lowercase", "=", "False", ")", ":", "\n", "\n", "    ", "if", "vocab_file", "is", "None", ":", "\n", "        ", "if", "not", "lowercase", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "REPO_PATH", ",", "\"data_utils\"", ",", "\"uppercase_vocab.txt\"", ")", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "REPO_PATH", ",", "\"data_utils\"", ",", "\"lowercase_vocab.txt\"", ")", "\n", "\n", "", "", "if", "tokenizer", "is", "None", ":", "\n", "        ", "tokenizer", "=", "FullTokenizer", "(", "vocab_file", "=", "vocab_file", ",", "do_lower_case", "=", "lowercase", ")", "\n", "\n", "", "writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "os", ".", "path", ".", "join", "(", "output_data_dir", ",", "\"{}.{}.tfrecord\"", ".", "format", "(", "output_filename", ",", "language", ")", ")", ")", "\n", "doc_map", "=", "{", "}", "\n", "documents", "=", "read_conll_file", "(", "input_file", ")", "\n", "for", "doc_idx", ",", "document", "in", "enumerate", "(", "documents", ")", ":", "\n", "        ", "doc_info", "=", "parse_document", "(", "document", ",", "language", ")", "\n", "tokenized_document", "=", "tokenize_document", "(", "genres", ",", "doc_info", ",", "tokenizer", ")", "\n", "doc_key", "=", "tokenized_document", "[", "'doc_key'", "]", "\n", "token_windows", ",", "mask_windows", ",", "text_len", "=", "convert_to_sliding_window", "(", "tokenized_document", ",", "window_size", ")", "\n", "input_id_windows", "=", "[", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "for", "tokens", "in", "token_windows", "]", "\n", "span_start", ",", "span_end", ",", "mention_span", ",", "cluster_ids", "=", "flatten_clusters", "(", "tokenized_document", "[", "'clusters'", "]", ")", "\n", "\n", "tmp_speaker_ids", "=", "tokenized_document", "[", "\"speakers\"", "]", "\n", "tmp_speaker_ids", "=", "[", "[", "0", "]", "*", "130", "]", "*", "num_window", "\n", "instance", "=", "(", "input_id_windows", ",", "mask_windows", ",", "text_len", ",", "tmp_speaker_ids", ",", "tokenized_document", "[", "\"genre\"", "]", ",", "span_start", ",", "span_end", ",", "cluster_ids", ",", "tokenized_document", "[", "'sentence_map'", "]", ")", "\n", "write_instance_to_example_file", "(", "writer", ",", "instance", ",", "doc_key", ",", "window_size", "=", "window_size", ",", "num_window", "=", "num_window", ",", "\n", "max_num_mention", "=", "max_num_mention", ",", "max_num_cluster", "=", "max_num_cluster", ")", "\n", "doc_map", "[", "doc_idx", "]", "=", "doc_key", "\n", "if", "demo", "and", "doc_idx", ">", "3", ":", "\n", "            ", "break", "\n", "", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_data_dir", ",", "\"{}.{}.map\"", ".", "format", "(", "output_filename", ",", "language", ")", ")", ",", "'w'", ")", "as", "fo", ":", "\n", "        ", "json", ".", "dump", "(", "doc_map", ",", "fo", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.write_instance_to_example_file": [[124, 162], ["int", "int", "build_dataset_to_tfrecord.clip_or_pad", "build_dataset_to_tfrecord.clip_or_pad", "build_dataset_to_tfrecord.clip_or_pad", "build_dataset_to_tfrecord.clip_or_pad", "build_dataset_to_tfrecord.clip_or_pad", "build_dataset_to_tfrecord.clip_or_pad", "build_dataset_to_tfrecord.clip_or_pad", "build_dataset_to_tfrecord.clip_or_pad", "build_dataset_to_tfrecord.clip_or_pad", "collections.OrderedDict", "build_dataset_to_tfrecord.create_int_feature", "build_dataset_to_tfrecord.create_int_feature", "build_dataset_to_tfrecord.create_int_feature", "build_dataset_to_tfrecord.create_int_feature", "build_dataset_to_tfrecord.create_int_feature", "build_dataset_to_tfrecord.create_int_feature", "build_dataset_to_tfrecord.create_int_feature", "build_dataset_to_tfrecord.create_int_feature", "build_dataset_to_tfrecord.create_int_feature", "tensorflow.train.Example", "writer.write", "int", "tf.train.Example.SerializeToString", "tensorflow.train.Features"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature"], ["", "", "def", "write_instance_to_example_file", "(", "writer", ",", "instance", ",", "doc_key", ",", "window_size", "=", "64", ",", "num_window", "=", "5", ",", "max_num_mention", "=", "20", ",", "\n", "max_num_cluster", "=", "30", ",", "pad_idx", "=", "-", "1", ")", ":", "\n", "\n", "    ", "input_ids", ",", "input_mask", ",", "text_len", ",", "speaker_ids", ",", "genre", ",", "gold_starts", ",", "gold_ends", ",", "cluster_ids", ",", "sentence_map", "=", "instance", "\n", "input_id_windows", "=", "input_ids", "\n", "mask_windows", "=", "input_mask", "\n", "flattened_input_ids", "=", "[", "i", "for", "j", "in", "input_id_windows", "for", "i", "in", "j", "]", "\n", "flattened_input_mask", "=", "[", "i", "for", "j", "in", "mask_windows", "for", "i", "in", "j", "]", "\n", "cluster_ids", "=", "[", "int", "(", "tmp", ")", "for", "tmp", "in", "cluster_ids", "]", "\n", "\n", "max_sequence_len", "=", "int", "(", "num_window", ")", "\n", "max_seg_len", "=", "int", "(", "window_size", ")", "\n", "\n", "sentence_map", "=", "clip_or_pad", "(", "sentence_map", ",", "max_sequence_len", "*", "max_seg_len", ",", "pad_idx", "=", "pad_idx", ")", "\n", "text_len", "=", "clip_or_pad", "(", "text_len", ",", "max_sequence_len", ",", "pad_idx", "=", "pad_idx", ")", "\n", "tmp_subtoken_maps", "=", "clip_or_pad", "(", "subtoken_maps", "[", "doc_key", "]", ",", "max_sequence_len", "*", "max_seg_len", ",", "pad_idx", "=", "pad_idx", ")", "\n", "\n", "tmp_speaker_ids", "=", "clip_or_pad", "(", "speaker_ids", "[", "0", "]", ",", "max_sequence_len", "*", "max_seg_len", ",", "pad_idx", "=", "pad_idx", ")", "\n", "\n", "flattened_input_ids", "=", "clip_or_pad", "(", "flattened_input_ids", ",", "max_sequence_len", "*", "max_seg_len", ",", "pad_idx", "=", "pad_idx", ")", "\n", "flattened_input_mask", "=", "clip_or_pad", "(", "flattened_input_mask", ",", "max_sequence_len", "*", "max_seg_len", ",", "pad_idx", "=", "pad_idx", ")", "\n", "gold_starts", "=", "clip_or_pad", "(", "gold_starts", ",", "max_num_mention", ",", "pad_idx", "=", "pad_idx", ")", "\n", "gold_ends", "=", "clip_or_pad", "(", "gold_ends", ",", "max_num_mention", ",", "pad_idx", "=", "pad_idx", ")", "\n", "cluster_ids", "=", "clip_or_pad", "(", "cluster_ids", ",", "max_num_cluster", ",", "pad_idx", "=", "pad_idx", ")", "\n", "\n", "features", "=", "OrderedDict", "(", ")", "\n", "features", "[", "'sentence_map'", "]", "=", "create_int_feature", "(", "sentence_map", ")", "\n", "features", "[", "'text_len'", "]", "=", "create_int_feature", "(", "text_len", ")", "\n", "features", "[", "'subtoken_map'", "]", "=", "create_int_feature", "(", "tmp_subtoken_maps", ")", "\n", "features", "[", "'speaker_ids'", "]", "=", "create_int_feature", "(", "tmp_speaker_ids", ")", "\n", "features", "[", "'flattened_input_ids'", "]", "=", "create_int_feature", "(", "flattened_input_ids", ")", "\n", "features", "[", "'flattened_input_mask'", "]", "=", "create_int_feature", "(", "flattened_input_mask", ")", "\n", "features", "[", "'span_starts'", "]", "=", "create_int_feature", "(", "gold_starts", ")", "\n", "features", "[", "'span_ends'", "]", "=", "create_int_feature", "(", "gold_ends", ")", "\n", "features", "[", "'cluster_ids'", "]", "=", "create_int_feature", "(", "cluster_ids", ")", "\n", "\n", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "features", ")", ")", "\n", "writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature": [[164, 167], ["tensorflow.train.Feature", "tensorflow.train.Int64List", "list"], "function", ["None"], ["", "def", "create_int_feature", "(", "values", ")", ":", "\n", "    ", "feature", "=", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.clip_or_pad": [[169, 177], ["len", "list", "list", "len"], "function", ["None"], ["", "def", "clip_or_pad", "(", "var", ",", "max_var_len", ",", "pad_idx", "=", "-", "1", ")", ":", "\n", "\n", "    ", "if", "len", "(", "var", ")", ">=", "max_var_len", ":", "\n", "        ", "return", "var", "[", ":", "max_var_len", "]", "\n", "", "else", ":", "\n", "        ", "pad_var", "=", "(", "max_var_len", "-", "len", "(", "var", ")", ")", "*", "[", "pad_idx", "]", "\n", "var", "=", "list", "(", "var", ")", "+", "list", "(", "pad_var", ")", "\n", "return", "var", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.flatten_clusters": [[179, 192], ["enumerate", "span_starts.append", "span_ends.append", "mention_span.append", "cluster_ids.append"], "function", ["None"], ["", "", "def", "flatten_clusters", "(", "clusters", ")", ":", "\n", "\n", "    ", "span_starts", "=", "[", "]", "\n", "span_ends", "=", "[", "]", "\n", "cluster_ids", "=", "[", "]", "\n", "mention_span", "=", "[", "]", "\n", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "clusters", ")", ":", "\n", "        ", "for", "start", ",", "end", "in", "cluster", ":", "\n", "            ", "span_starts", ".", "append", "(", "start", ")", "\n", "span_ends", ".", "append", "(", "end", ")", "\n", "mention_span", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "cluster_ids", ".", "append", "(", "cluster_id", "+", "1", ")", "\n", "", "", "return", "span_starts", ",", "span_ends", ",", "mention_span", ",", "cluster_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.read_conll_file": [[194, 207], ["open", "re.match", "data_utils.conll.get_doc_key", "documents.append", "line.startswith", "re.match.group", "re.match.group", "[].append", "line.strip"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.conll.get_doc_key"], ["", "def", "read_conll_file", "(", "conll_file_path", ")", ":", "\n", "    ", "documents", "=", "[", "]", "\n", "with", "open", "(", "conll_file_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fi", ":", "\n", "        ", "for", "line", "in", "fi", ":", "\n", "            ", "begin_document_match", "=", "re", ".", "match", "(", "conll", ".", "BEGIN_DOCUMENT_REGEX", ",", "line", ")", "\n", "if", "begin_document_match", ":", "\n", "                ", "doc_key", "=", "conll", ".", "get_doc_key", "(", "begin_document_match", ".", "group", "(", "1", ")", ",", "begin_document_match", ".", "group", "(", "2", ")", ")", "\n", "documents", ".", "append", "(", "(", "doc_key", ",", "[", "]", ")", ")", "\n", "", "elif", "line", ".", "startswith", "(", "\"#end document\"", ")", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "                ", "documents", "[", "-", "1", "]", "[", "1", "]", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "", "return", "documents", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.parse_document": [[209, 240], ["enumerate", "build_dataset_to_tfrecord.coreference_annotations_to_clusters", "line.split", "len", "build_dataset_to_tfrecord.normalize_word", "sentences[].append", "coreferences.append", "sentences.append", "len", "speakers.append"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.coreference_annotations_to_clusters", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.normalize_word"], ["", "def", "parse_document", "(", "document", ",", "language", ")", ":", "\n", "    ", "\"\"\"\n    get basic information from one document annotation.\n    :param document:\n    :param language: english, chinese or arabic\n    :return:\n    \"\"\"", "\n", "doc_key", "=", "document", "[", "0", "]", "\n", "sentences", "=", "[", "[", "]", "]", "\n", "speakers", "=", "[", "]", "\n", "coreferences", "=", "[", "]", "\n", "word_idx", "=", "-", "1", "\n", "last_speaker", "=", "''", "\n", "for", "line_id", ",", "line", "in", "enumerate", "(", "document", "[", "1", "]", ")", ":", "\n", "        ", "row", "=", "line", ".", "split", "(", ")", "\n", "sentence_end", "=", "len", "(", "row", ")", "==", "0", "\n", "if", "not", "sentence_end", ":", "\n", "            ", "assert", "len", "(", "row", ")", ">=", "12", "\n", "word_idx", "+=", "1", "\n", "word", "=", "normalize_word", "(", "row", "[", "3", "]", ",", "language", ")", "\n", "sentences", "[", "-", "1", "]", ".", "append", "(", "word", ")", "\n", "speaker", "=", "row", "[", "9", "]", "\n", "if", "speaker", "!=", "last_speaker", ":", "\n", "                ", "speakers", ".", "append", "(", "(", "word_idx", ",", "speaker", ")", ")", "\n", "last_speaker", "=", "speaker", "\n", "", "coreferences", ".", "append", "(", "row", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "sentences", ".", "append", "(", "[", "]", ")", "\n", "", "", "clusters", "=", "coreference_annotations_to_clusters", "(", "coreferences", ")", "\n", "doc_info", "=", "{", "'doc_key'", ":", "doc_key", ",", "'sentences'", ":", "sentences", "[", ":", "-", "1", "]", ",", "'speakers'", ":", "speakers", ",", "'clusters'", ":", "clusters", "}", "\n", "return", "doc_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.normalize_word": [[242, 249], ["word.find"], "function", ["None"], ["", "def", "normalize_word", "(", "word", ",", "language", ")", ":", "\n", "    ", "if", "language", "==", "\"arabic\"", ":", "\n", "        ", "word", "=", "word", "[", ":", "word", ".", "find", "(", "\"#\"", ")", "]", "\n", "", "if", "word", "==", "\"/.\"", "or", "word", "==", "\"/?\"", ":", "\n", "        ", "return", "word", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.coreference_annotations_to_clusters": [[251, 284], ["collections.OrderedDict", "collections.OrderedDict", "enumerate", "all", "list", "annotation.split", "collections.OrderedDict.values", "int", "ann.replace().replace", "len", "collections.OrderedDict.values", "collections.OrderedDict.keys", "clusters[].append", "ann.replace", "collections.OrderedDict.keys", "coref_stack[].append", "coref_stack[].pop", "collections.OrderedDict.keys", "clusters[].append"], "function", ["None"], ["", "", "def", "coreference_annotations_to_clusters", "(", "annotations", ")", ":", "\n", "    ", "\"\"\"\n    convert coreference information to clusters\n    :param annotations:\n    :return:\n    \"\"\"", "\n", "clusters", "=", "OrderedDict", "(", ")", "\n", "coref_stack", "=", "OrderedDict", "(", ")", "\n", "for", "word_idx", ",", "annotation", "in", "enumerate", "(", "annotations", ")", ":", "\n", "        ", "if", "annotation", "==", "'-'", ":", "\n", "            ", "continue", "\n", "", "for", "ann", "in", "annotation", ".", "split", "(", "'|'", ")", ":", "\n", "            ", "cluster_id", "=", "int", "(", "ann", ".", "replace", "(", "'('", ",", "''", ")", ".", "replace", "(", "')'", ",", "''", ")", ")", "\n", "if", "ann", "[", "0", "]", "==", "'('", "and", "ann", "[", "-", "1", "]", "==", "')'", ":", "\n", "                ", "if", "cluster_id", "not", "in", "clusters", ".", "keys", "(", ")", ":", "\n", "                    ", "clusters", "[", "cluster_id", "]", "=", "[", "(", "word_idx", ",", "word_idx", ")", "]", "\n", "", "else", ":", "\n", "                    ", "clusters", "[", "cluster_id", "]", ".", "append", "(", "(", "word_idx", ",", "word_idx", ")", ")", "\n", "", "", "elif", "ann", "[", "0", "]", "==", "'('", ":", "\n", "                ", "if", "cluster_id", "not", "in", "coref_stack", ".", "keys", "(", ")", ":", "\n", "                    ", "coref_stack", "[", "cluster_id", "]", "=", "[", "word_idx", "]", "\n", "", "else", ":", "\n", "                    ", "coref_stack", "[", "cluster_id", "]", ".", "append", "(", "word_idx", ")", "\n", "", "", "elif", "ann", "[", "-", "1", "]", "==", "')'", ":", "\n", "                ", "span_start", "=", "coref_stack", "[", "cluster_id", "]", ".", "pop", "(", ")", "\n", "if", "cluster_id", "not", "in", "clusters", ".", "keys", "(", ")", ":", "\n", "                    ", "clusters", "[", "cluster_id", "]", "=", "[", "(", "span_start", ",", "word_idx", ")", "]", "\n", "", "else", ":", "\n", "                    ", "clusters", "[", "cluster_id", "]", ".", "append", "(", "(", "span_start", ",", "word_idx", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "", "", "", "assert", "all", "(", "[", "len", "(", "starts", ")", "==", "0", "for", "starts", "in", "coref_stack", ".", "values", "(", ")", "]", ")", "\n", "return", "list", "(", "clusters", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.checkout_clusters": [[286, 290], ["print"], "function", ["None"], ["", "def", "checkout_clusters", "(", "doc_info", ")", ":", "\n", "    ", "words", "=", "[", "i", "for", "j", "in", "doc_info", "[", "'sentences'", "]", "for", "i", "in", "j", "]", "\n", "clusters", "=", "[", "[", "' '", ".", "join", "(", "words", "[", "start", ":", "end", "+", "1", "]", ")", "for", "start", ",", "end", "in", "cluster", "]", "for", "cluster", "in", "doc_info", "[", "'clusters'", "]", "]", "\n", "print", "(", "clusters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.tokenize_document": [[292, 326], ["enumerate", "genres.get", "subtoken_map.index", "tokenizer.tokenize", "enumerate", "tokenizer.tokenize", "sub_tokens.extend", "sentence_map.extend", "subtoken_map.extend", "subtoken_map.index", "len", "len", "subtoken_map[].index", "len"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize_document", "(", "genres", ",", "doc_info", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"\n    tokenize into sub tokens\n    :param doc_info:\n    :param tokenizer:\n    max_doc_length: pad to max_doc_length\n    :return:\n    \"\"\"", "\n", "genres", "=", "{", "g", ":", "i", "for", "i", ",", "g", "in", "enumerate", "(", "genres", ")", "}", "\n", "sub_tokens", "=", "[", "]", "# all sub tokens of a document", "\n", "sentence_map", "=", "[", "]", "# collected tokenized tokens -> sentence id", "\n", "subtoken_map", "=", "[", "]", "# collected tokenized tokens -> original token id", "\n", "\n", "word_idx", "=", "-", "1", "\n", "\n", "for", "sentence_id", ",", "sentence", "in", "enumerate", "(", "doc_info", "[", "'sentences'", "]", ")", ":", "\n", "        ", "for", "token", "in", "sentence", ":", "\n", "            ", "word_idx", "+=", "1", "\n", "word_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "sub_tokens", ".", "extend", "(", "word_tokens", ")", "\n", "sentence_map", ".", "extend", "(", "[", "sentence_id", "]", "*", "len", "(", "word_tokens", ")", ")", "\n", "subtoken_map", ".", "extend", "(", "[", "word_idx", "]", "*", "len", "(", "word_tokens", ")", ")", "\n", "\n", "\n", "", "", "subtoken_maps", "[", "doc_info", "[", "'doc_key'", "]", "]", "=", "subtoken_map", "\n", "genre", "=", "genres", ".", "get", "(", "doc_info", "[", "'doc_key'", "]", "[", ":", "2", "]", ",", "0", ")", "\n", "speakers", "=", "{", "subtoken_map", ".", "index", "(", "word_index", ")", ":", "tokenizer", ".", "tokenize", "(", "speaker", ")", "\n", "for", "word_index", ",", "speaker", "in", "doc_info", "[", "'speakers'", "]", "}", "\n", "clusters", "=", "[", "[", "(", "subtoken_map", ".", "index", "(", "start", ")", ",", "len", "(", "subtoken_map", ")", "-", "1", "-", "subtoken_map", "[", ":", ":", "-", "1", "]", ".", "index", "(", "end", ")", ")", "\n", "for", "start", ",", "end", "in", "cluster", "]", "for", "cluster", "in", "doc_info", "[", "'clusters'", "]", "]", "\n", "tokenized_document", "=", "{", "'sub_tokens'", ":", "sub_tokens", ",", "'sentence_map'", ":", "sentence_map", ",", "'subtoken_map'", ":", "subtoken_map", ",", "\n", "'speakers'", ":", "speakers", ",", "'clusters'", ":", "clusters", ",", "'doc_key'", ":", "doc_info", "[", "'doc_key'", "]", ",", "\n", "\"genre\"", ":", "genre", "}", "\n", "return", "tokenized_document", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.convert_to_sliding_window": [[328, 358], ["build_dataset_to_tfrecord.expand_with_speakers", "build_dataset_to_tfrecord.construct_sliding_windows", "numpy.array", "len", "np.array.append", "token_windows.append", "mask_windows.append", "len", "sum", "len", "len", "len", "zip", "len", "len"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.expand_with_speakers", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.construct_sliding_windows"], ["", "def", "convert_to_sliding_window", "(", "tokenized_document", ",", "sliding_window_size", ")", ":", "\n", "    ", "\"\"\"\n    construct sliding windows, allocate tokens and masks into each window\n    :param tokenized_document:\n    :param sliding_window_size:\n    :return:\n    \"\"\"", "\n", "expanded_tokens", ",", "expanded_masks", "=", "expand_with_speakers", "(", "tokenized_document", ")", "\n", "sliding_windows", "=", "construct_sliding_windows", "(", "len", "(", "expanded_tokens", ")", ",", "sliding_window_size", "-", "2", ")", "\n", "token_windows", "=", "[", "]", "# expanded tokens to sliding window", "\n", "mask_windows", "=", "[", "]", "# expanded masks to sliding window", "\n", "text_len", "=", "[", "]", "\n", "\n", "for", "window_start", ",", "window_end", ",", "window_mask", "in", "sliding_windows", ":", "\n", "        ", "original_tokens", "=", "expanded_tokens", "[", "window_start", ":", "window_end", "]", "\n", "original_masks", "=", "expanded_masks", "[", "window_start", ":", "window_end", "]", "\n", "window_masks", "=", "[", "-", "2", "if", "w", "==", "0", "else", "o", "for", "w", ",", "o", "in", "zip", "(", "window_mask", ",", "original_masks", ")", "]", "\n", "one_window_token", "=", "[", "'[CLS]'", "]", "+", "original_tokens", "+", "[", "'[SEP]'", "]", "+", "[", "'[PAD]'", "]", "*", "(", "\n", "sliding_window_size", "-", "2", "-", "len", "(", "original_tokens", ")", ")", "\n", "one_window_mask", "=", "[", "-", "3", "]", "+", "window_masks", "+", "[", "-", "3", "]", "+", "[", "-", "4", "]", "*", "(", "sliding_window_size", "-", "2", "-", "len", "(", "original_tokens", ")", ")", "\n", "token_calculate", "=", "[", "tmp", "for", "tmp", "in", "one_window_mask", "if", "tmp", ">=", "0", "]", "\n", "text_len", ".", "append", "(", "len", "(", "token_calculate", ")", ")", "\n", "assert", "len", "(", "one_window_token", ")", "==", "sliding_window_size", "\n", "assert", "len", "(", "one_window_mask", ")", "==", "sliding_window_size", "\n", "token_windows", ".", "append", "(", "one_window_token", ")", "\n", "mask_windows", ".", "append", "(", "one_window_mask", ")", "\n", "", "assert", "len", "(", "tokenized_document", "[", "'sentence_map'", "]", ")", "==", "sum", "(", "[", "i", ">=", "0", "for", "j", "in", "mask_windows", "for", "i", "in", "j", "]", ")", "\n", "\n", "text_len", "=", "np", ".", "array", "(", "text_len", ")", "\n", "return", "token_windows", ",", "mask_windows", ",", "text_len", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.expand_with_speakers": [[360, 376], ["enumerate", "expanded_tokens.append", "expanded_masks.append", "expanded_tokens.extend", "expanded_masks.extend", "len"], "function", ["None"], ["", "def", "expand_with_speakers", "(", "tokenized_document", ")", ":", "\n", "    ", "\"\"\"\n    add speaker name information\n    :param tokenized_document: tokenized document information\n    :return:\n    \"\"\"", "\n", "expanded_tokens", "=", "[", "]", "\n", "expanded_masks", "=", "[", "]", "\n", "for", "token_idx", ",", "token", "in", "enumerate", "(", "tokenized_document", "[", "'sub_tokens'", "]", ")", ":", "\n", "        ", "if", "token_idx", "in", "tokenized_document", "[", "'speakers'", "]", ":", "\n", "            ", "speaker", "=", "[", "SPEAKER_START", "]", "+", "tokenized_document", "[", "'speakers'", "]", "[", "token_idx", "]", "+", "[", "SPEAKER_END", "]", "\n", "expanded_tokens", ".", "extend", "(", "speaker", ")", "\n", "expanded_masks", ".", "extend", "(", "[", "-", "1", "]", "*", "len", "(", "speaker", ")", ")", "\n", "", "expanded_tokens", ".", "append", "(", "token", ")", "\n", "expanded_masks", ".", "append", "(", "token_idx", ")", "\n", "", "return", "expanded_tokens", ",", "expanded_masks", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.construct_sliding_windows": [[378, 400], ["int", "min", "sliding_windows.append", "sum", "sum", "int", "int", "int", "int"], "function", ["None"], ["", "def", "construct_sliding_windows", "(", "sequence_length", ",", "sliding_window_size", ")", ":", "\n", "    ", "\"\"\"\n    construct sliding windows for BERT processing\n    :param sequence_length: e.g. 9\n    :param sliding_window_size: e.g. 4\n    :return: [(0, 4, [1, 1, 1, 0]), (2, 6, [0, 1, 1, 0]), (4, 8, [0, 1, 1, 0]), (6, 9, [0, 1, 1])]\n    \"\"\"", "\n", "sliding_windows", "=", "[", "]", "\n", "stride", "=", "int", "(", "sliding_window_size", "/", "2", ")", "\n", "start_index", "=", "0", "\n", "end_index", "=", "0", "\n", "while", "end_index", "<", "sequence_length", ":", "\n", "        ", "end_index", "=", "min", "(", "start_index", "+", "sliding_window_size", ",", "sequence_length", ")", "\n", "left_value", "=", "1", "if", "start_index", "==", "0", "else", "0", "\n", "right_value", "=", "1", "if", "end_index", "==", "sequence_length", "else", "0", "\n", "mask", "=", "[", "left_value", "]", "*", "int", "(", "sliding_window_size", "/", "4", ")", "+", "[", "1", "]", "*", "int", "(", "sliding_window_size", "/", "2", ")", "+", "[", "right_value", "]", "*", "(", "sliding_window_size", "-", "int", "(", "sliding_window_size", "/", "2", ")", "-", "int", "(", "sliding_window_size", "/", "4", ")", ")", "\n", "mask", "=", "mask", "[", ":", "end_index", "-", "start_index", "]", "\n", "sliding_windows", ".", "append", "(", "(", "start_index", ",", "end_index", ",", "mask", ")", ")", "\n", "start_index", "+=", "stride", "\n", "", "assert", "sum", "(", "[", "sum", "(", "window", "[", "2", "]", ")", "for", "window", "in", "sliding_windows", "]", ")", "==", "sequence_length", "\n", "return", "sliding_windows", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.parse_args": [[403, 426], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.makedirs", "numpy.random.seed", "tensorflow.set_random_seed"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.transform_spanbert_pytorch_to_tf.parse_args"], ["", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--source_files_dir\"", ",", "default", "=", "\"/home/lixiaoya/data\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--target_output_dir\"", ",", "default", "=", "\"/home/lixiaoya/tfrecord_data\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_window\"", ",", "default", "=", "5", ",", "type", "=", "int", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--window_size\"", ",", "default", "=", "64", ",", "type", "=", "int", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_num_mention\"", ",", "default", "=", "30", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_num_cluster\"", ",", "default", "=", "20", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_file\"", ",", "default", "=", "\"/home/lixiaoya/spanbert_large_cased/vocab.txt\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--language\"", ",", "default", "=", "\"english\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_doc_length\"", ",", "default", "=", "600", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--lowercase\"", ",", "help", "=", "\"DO or NOT lowercase the datasets.\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--demo\"", ",", "help", "=", "\"Wether to generate a small dataset for testing the code.\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "'--genres'", ",", "default", "=", "[", "\"bc\"", ",", "\"bn\"", ",", "\"mz\"", ",", "\"nw\"", ",", "\"pt\"", ",", "\"tc\"", ",", "\"wb\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "default", "=", "2333", ",", "type", "=", "int", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "target_output_dir", ",", "exist_ok", "=", "True", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "tf", ".", "set_random_seed", "(", "args", ".", "seed", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.main": [[428, 453], ["build_dataset_to_tfrecord.parse_args", "print", "print", "print", "print", "print", "os.path.join", "print", "print", "build_dataset_to_tfrecord.prepare_train_dataset", "str", "str"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.transform_spanbert_pytorch_to_tf.parse_args", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.prepare_train_dataset"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args_config", "=", "parse_args", "(", ")", "\n", "\n", "print", "(", "\"*\"", "*", "60", ")", "\n", "print", "(", "\"***** ***** show configs ***** ***** \"", ")", "\n", "print", "(", "\"window_size : {}\"", ".", "format", "(", "str", "(", "args_config", ".", "window_size", ")", ")", ")", "\n", "print", "(", "\"num_window : {}\"", ".", "format", "(", "str", "(", "args_config", ".", "num_window", ")", ")", ")", "\n", "print", "(", "\"*\"", "*", "60", ")", "\n", "\n", "for", "data_sign", "in", "[", "\"train\"", ",", "\"dev\"", ",", "\"test\"", "]", ":", "\n", "        ", "source_data_file", "=", "os", ".", "path", ".", "join", "(", "args_config", ".", "source_files_dir", ",", "\"{}.{}.v4_gold_conll\"", ".", "format", "(", "data_sign", ",", "args_config", ".", "language", ")", ")", "\n", "output_filename", "=", "\"{}.overlap.corefqa\"", ".", "format", "(", "data_sign", ")", "\n", "\n", "if", "args_config", ".", "demo", ":", "\n", "            ", "if", "args_config", ".", "lowercase", ":", "\n", "                ", "output_filename", "=", "\"demo.lowercase.{}.overlap.corefqa\"", ".", "format", "(", "data_sign", ")", "\n", "", "else", ":", "\n", "                ", "output_filename", "=", "\"demo.{}.overlap.corefqa\"", ".", "format", "(", "data_sign", ")", "\n", "\n", "", "", "print", "(", "\"$\"", "*", "60", ")", "\n", "print", "(", "\"generate {}/{}\"", ".", "format", "(", "args_config", ".", "target_output_dir", ",", "output_filename", ")", ")", "\n", "prepare_train_dataset", "(", "source_data_file", ",", "args_config", ".", "target_output_dir", ",", "output_filename", ",", "args_config", ".", "window_size", ",", "\n", "args_config", ".", "num_window", ",", "vocab_file", "=", "args_config", ".", "vocab_file", ",", "language", "=", "args_config", ".", "language", ",", "\n", "max_doc_length", "=", "args_config", ".", "max_doc_length", ",", "genres", "=", "args_config", ".", "genres", ",", "max_num_mention", "=", "args_config", ".", "max_num_mention", ",", "\n", "max_num_cluster", "=", "args_config", ".", "max_num_cluster", ",", "demo", "=", "args_config", ".", "demo", ",", "lowercase", "=", "args_config", ".", "lowercase", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_corefqa.main": [[85, 198], ["tensorflow.logging.set_verbosity", "max", "tensorflow.gfile.MakeDirs", "tensorflow.contrib.tpu.RunConfig", "data_utils.config_utils.ModelConfig", "data_utils.config_utils.ModelConfig.logging_configs", "func_builders.model_fn_builder.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "math.ceil", "ValueError", "tensorflow.distribute.cluster_resolver.TPUClusterResolver", "tensorflow.config.experimental_connect_to_cluster", "tensorflow.tpu.experimental.initialize_tpu_system", "tf.contrib.tpu.TPUEstimator.train", "utils.util.get_model", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "utils.metrics.CorefEvaluator", "utils.util.get_model", "tf.contrib.tpu.TPUEstimator.predict", "metrics.CorefEvaluator.get_prf", "tensorflow.logging.info", "tensorflow.ConfigProto", "tensorflow.contrib.tpu.TPUConfig", "tensorflow.estimator.WarmStartSettings", "os.path.join", "utils.metrics.CorefEvaluator", "tf.contrib.tpu.TPUEstimator.predict", "metrics.CorefEvaluator.get_prf", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "func_builders.input_fn_builder.file_based_input_fn_builder", "util.get_model.evaluate", "metrics.CorefEvaluator.update", "func_builders.input_fn_builder.file_based_input_fn_builder", "range", "func_builders.input_fn_builder.file_based_input_fn_builder", "util.get_model.evaluate", "metrics.CorefEvaluator.update", "utils.metrics.CorefEvaluator", "tf.contrib.tpu.TPUEstimator.predict", "metrics.CorefEvaluator.get_prf", "tensorflow.logging.info", "tensorflow.logging.info", "str", "func_builders.input_fn_builder.file_based_input_fn_builder", "util.get_model.evaluate", "metrics.CorefEvaluator.update", "int"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.config_utils.ModelConfig.logging_configs", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.model_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.get_model", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.util.get_model", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_prf", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_prf", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.input_fn_builder.file_based_input_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.evaluate", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.update", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.input_fn_builder.file_based_input_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.input_fn_builder.file_based_input_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.evaluate", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.update", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.get_prf", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.input_fn_builder.file_based_input_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.evaluate", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.Evaluator.update"], ["def", "main", "(", "_", ")", ":", "\n", "\n", "    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "num_train_steps", "=", "FLAGS", ".", "num_docs", "*", "FLAGS", ".", "num_epochs", "\n", "\n", "\n", "keep_chceckpoint_max", "=", "max", "(", "math", ".", "ceil", "(", "num_train_steps", "/", "FLAGS", ".", "save_checkpoints_steps", ")", ",", "FLAGS", ".", "keep_checkpoint_max", ")", "\n", "\n", "if", "not", "FLAGS", ".", "do_train", "and", "not", "FLAGS", ".", "do_eval", "and", "not", "FLAGS", ".", "do_predict", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train`, `do_eval` or `do_predict' must be True.\"", ")", "\n", "\n", "", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "output_dir", ")", "\n", "tpu_cluster_resolver", "=", "None", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "        ", "tpu_cluster_resolver", "=", "tf", ".", "distribute", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "tf", ".", "config", ".", "experimental_connect_to_cluster", "(", "tpu_cluster_resolver", ")", "\n", "tf", ".", "tpu", ".", "experimental", ".", "initialize_tpu_system", "(", "tpu_cluster_resolver", ")", "\n", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "FLAGS", ".", "output_dir", ",", "\n", "evaluation_master", "=", "FLAGS", ".", "master", ",", "\n", "keep_checkpoint_max", "=", "keep_chceckpoint_max", ",", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "save_checkpoints_steps", ",", "\n", "session_config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ",", "log_device_placement", "=", "True", ")", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "\n", "model_config", "=", "ModelConfig", "(", "FLAGS", ",", "FLAGS", ".", "output_dir", ")", "\n", "model_config", ".", "logging_configs", "(", ")", "\n", "\n", "\n", "model_fn", "=", "model_fn_builder", "(", "model_config", ",", "model_sign", "=", "\"corefqa\"", ")", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "eval_on_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "warm_start_from", "=", "tf", ".", "estimator", ".", "WarmStartSettings", "(", "FLAGS", ".", "init_checkpoint", ",", "\n", "vars_to_warm_start", "=", "\"bert*\"", ")", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "train_batch_size", "=", "1", ",", "\n", "eval_batch_size", "=", "1", ",", "\n", "predict_batch_size", "=", "1", ")", "\n", "\n", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "        ", "estimator", ".", "train", "(", "input_fn", "=", "file_based_input_fn_builder", "(", "FLAGS", ".", "train_file", ",", "num_window", "=", "FLAGS", ".", "num_window", ",", "\n", "window_size", "=", "FLAGS", ".", "window_size", ",", "max_num_mention", "=", "FLAGS", ".", "max_num_mention", ",", "is_training", "=", "True", ",", "drop_remainder", "=", "True", ")", ",", "\n", "max_steps", "=", "num_train_steps", ")", "\n", "\n", "\n", "", "if", "FLAGS", ".", "do_eval", ":", "\n", "        ", "best_dev_f1", ",", "best_dev_prec", ",", "best_dev_rec", ",", "test_f1_when_dev_best", ",", "test_prec_when_dev_best", ",", "test_rec_when_dev_best", "=", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "\n", "best_ckpt_path", "=", "\"\"", "\n", "checkpoints_iterator", "=", "[", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "eval_dir", ",", "\"model.ckpt-{}\"", ".", "format", "(", "str", "(", "int", "(", "ckpt_idx", ")", ")", ")", ")", "for", "ckpt_idx", "in", "range", "(", "0", ",", "num_train_steps", "+", "1", ",", "FLAGS", ".", "save_checkpoints_steps", ")", "]", "\n", "model", "=", "util", ".", "get_model", "(", "model_config", ",", "model_sign", "=", "\"corefqa\"", ")", "\n", "for", "checkpoint_path", "in", "checkpoints_iterator", "[", "1", ":", "]", ":", "\n", "            ", "dev_coref_evaluator", "=", "metrics", ".", "CorefEvaluator", "(", ")", "\n", "for", "result", "in", "estimator", ".", "predict", "(", "file_based_input_fn_builder", "(", "FLAGS", ".", "dev_file", ",", "num_window", "=", "FLAGS", ".", "num_window", ",", "\n", "window_size", "=", "FLAGS", ".", "window_size", ",", "max_num_mention", "=", "FLAGS", ".", "max_num_mention", ",", "is_training", "=", "False", ",", "drop_remainder", "=", "False", ")", ",", "\n", "steps", "=", "698", ",", "checkpoint_path", "=", "checkpoint_path", ",", "yield_single_examples", "=", "False", ")", ":", "\n", "\n", "                ", "predicted_clusters", ",", "gold_clusters", ",", "mention_to_predicted", ",", "mention_to_gold", "=", "model", ".", "evaluate", "(", "result", "[", "\"topk_span_starts\"", "]", ",", "result", "[", "\"topk_span_ends\"", "]", ",", "result", "[", "\"top_antecedent\"", "]", ",", "\n", "result", "[", "\"cluster_ids\"", "]", ",", "result", "[", "\"gold_starts\"", "]", ",", "result", "[", "\"gold_ends\"", "]", ")", "\n", "dev_coref_evaluator", ".", "update", "(", "predicted_clusters", ",", "gold_clusters", ",", "mention_to_predicted", ",", "mention_to_gold", ")", "\n", "", "dev_prec", ",", "dev_rec", ",", "dev_f1", "=", "dev_coref_evaluator", ".", "get_prf", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Current ckpt path is ***** : {}\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** EVAL ON DEV SET *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** [DEV EVAL] ***** : precision: {:.4f}, recall: {:.4f}, f1: {:.4f}\"", ".", "format", "(", "dev_prec", ",", "dev_rec", ",", "dev_f1", ")", ")", "\n", "if", "dev_f1", ">", "best_dev_f1", ":", "\n", "                ", "best_ckpt_path", "=", "checkpoint_path", "\n", "best_dev_f1", "=", "dev_f1", "\n", "best_dev_prec", "=", "dev_prec", "\n", "best_dev_rec", "=", "dev_rec", "\n", "test_coref_evaluator", "=", "metrics", ".", "CorefEvaluator", "(", ")", "\n", "for", "result", "in", "estimator", ".", "predict", "(", "file_based_input_fn_builder", "(", "FLAGS", ".", "test_file", ",", "\n", "num_window", "=", "FLAGS", ".", "num_window", ",", "window_size", "=", "FLAGS", ".", "window_size", ",", "max_num_mention", "=", "FLAGS", ".", "max_num_mention", ",", "\n", "is_training", "=", "False", ",", "drop_remainder", "=", "False", ")", ",", "steps", "=", "698", ",", "checkpoint_path", "=", "checkpoint_path", ",", "yield_single_examples", "=", "False", ")", ":", "\n", "                    ", "predicted_clusters", ",", "gold_clusters", ",", "mention_to_predicted", ",", "mention_to_gold", "=", "model", ".", "evaluate", "(", "result", "[", "\"topk_span_starts\"", "]", ",", "result", "[", "\"topk_span_ends\"", "]", ",", "result", "[", "\"top_antecedent\"", "]", ",", "\n", "result", "[", "\"cluster_ids\"", "]", ",", "result", "[", "\"gold_starts\"", "]", ",", "result", "[", "\"gold_ends\"", "]", ")", "\n", "test_coref_evaluator", ".", "update", "(", "predicted_clusters", ",", "gold_clusters", ",", "mention_to_predicted", ",", "mention_to_gold", ")", "\n", "\n", "", "test_pre", ",", "test_rec", ",", "test_f1", "=", "test_coref_evaluator", ".", "get_prf", "(", ")", "\n", "test_f1_when_dev_best", ",", "test_prec_when_dev_best", ",", "test_rec_when_dev_best", "=", "test_f1", ",", "test_pre", ",", "test_rec", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** EVAL ON TEST SET *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** [TEST EVAL] ***** : precision: {:.4f}, recall: {:.4f}, f1: {:.4f}\"", ".", "format", "(", "test_pre", ",", "test_rec", ",", "test_f1", ")", ")", "\n", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"*\"", "*", "20", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"- @@@@@ the path to the BEST DEV result is : {}\"", ".", "format", "(", "best_ckpt_path", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"- @@@@@ BEST DEV F1 : {:.4f}, Precision : {:.4f}, Recall : {:.4f},\"", ".", "format", "(", "best_dev_f1", ",", "best_dev_prec", ",", "best_dev_rec", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"- @@@@@ TEST when DEV best F1 : {:.4f}, Precision : {:.4f}, Recall : {:.4f},\"", ".", "format", "(", "test_f1_when_dev_best", ",", "test_prec_when_dev_best", ",", "test_rec_when_dev_best", ")", ")", "\n", "\n", "\n", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "        ", "coref_evaluator", "=", "metrics", ".", "CorefEvaluator", "(", ")", "\n", "model", "=", "util", ".", "get_model", "(", "model_config", ",", "model_sign", "=", "\"corefqa\"", ")", "\n", "for", "result", "in", "estimator", ".", "predict", "(", "file_based_input_fn_builder", "(", "FLAGS", ".", "test_file", ",", "\n", "num_window", "=", "FLAGS", ".", "num_window", ",", "window_size", "=", "FLAGS", ".", "window_size", ",", "max_num_mention", "=", "FLAGS", ".", "max_num_mention", ",", "\n", "is_training", "=", "False", ",", "drop_remainder", "=", "False", ")", ",", "steps", "=", "698", ",", "checkpoint_path", "=", "checkpoint_path", ",", "yield_single_examples", "=", "False", ")", ":", "\n", "\n", "            ", "predicted_clusters", ",", "gold_clusters", ",", "mention_to_predicted", ",", "mention_to_gold", "=", "model", ".", "evaluate", "(", "result", "[", "\"topk_span_starts\"", "]", ",", "result", "[", "\"topk_span_ends\"", "]", ",", "\n", "result", "[", "\"top_antecedent\"", "]", ",", "result", "[", "\"cluster_ids\"", "]", ",", "result", "[", "\"gold_starts\"", "]", ",", "result", "[", "\"gold_ends\"", "]", ")", "\n", "coref_evaluator", ".", "update", "(", "predicted_clusters", ",", "gold_clusters", ",", "mention_to_predicted", ",", "mention_to_gold", ")", "\n", "\n", "", "p", ",", "r", ",", "f", "=", "coref_evaluator", ".", "get_prf", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Average precision: {:.4f}, Average recall: {:.4f}, Average F1 {:.4f}\"", ".", "format", "(", "p", ",", "r", ",", "f", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_mention_proposal.main": [[79, 183], ["tensorflow.logging.set_verbosity", "max", "tensorflow.gfile.MakeDirs", "tensorflow.contrib.tpu.RunConfig", "data_utils.config_utils.ModelConfig", "data_utils.config_utils.ModelConfig.logging_configs", "func_builders.model_fn_builder.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "math.ceil", "ValueError", "tensorflow.distribute.cluster_resolver.TPUClusterResolver", "tensorflow.config.experimental_connect_to_cluster", "tensorflow.tpu.experimental.initialize_tpu_system", "tf.contrib.tpu.TPUEstimator.train", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tf.contrib.tpu.TPUEstimator.predict", "tensorflow.logging.info", "tensorflow.contrib.tpu.TPUConfig", "tensorflow.estimator.WarmStartSettings", "os.path.join", "tf.contrib.tpu.TPUEstimator.evaluate", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "func_builders.input_fn_builder.file_based_input_fn_builder", "utils.metrics.mention_proposal_prediction", "numpy.logical_and().sum", "numpy.logical_and().sum", "numpy.logical_and().sum", "func_builders.input_fn_builder.file_based_input_fn_builder", "range", "tf.contrib.tpu.TPUEstimator.evaluate", "tensorflow.logging.info", "tensorflow.logging.info", "str", "func_builders.input_fn_builder.file_based_input_fn_builder", "numpy.logical_and", "numpy.logical_and", "numpy.logical_and", "int", "func_builders.input_fn_builder.file_based_input_fn_builder", "numpy.logical_not", "numpy.logical_not"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.data_utils.config_utils.ModelConfig.logging_configs", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.model_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.evaluate", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.input_fn_builder.file_based_input_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.utils.metrics.mention_proposal_prediction", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.input_fn_builder.file_based_input_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.evaluate", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.input_fn_builder.file_based_input_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.func_builders.input_fn_builder.file_based_input_fn_builder"], ["def", "main", "(", "_", ")", ":", "\n", "\n", "    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "num_train_steps", "=", "FLAGS", ".", "num_docs", "*", "FLAGS", ".", "num_epochs", "\n", "# num_train_steps = 100 ", "\n", "keep_chceckpoint_max", "=", "max", "(", "math", ".", "ceil", "(", "num_train_steps", "/", "FLAGS", ".", "save_checkpoints_steps", ")", ",", "FLAGS", ".", "keep_checkpoint_max", ")", "\n", "\n", "if", "not", "FLAGS", ".", "do_train", "and", "not", "FLAGS", ".", "do_eval", "and", "not", "FLAGS", ".", "do_predict", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train`, `do_eval` or `do_predict' must be True.\"", ")", "\n", "\n", "", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "output_dir", ")", "\n", "tpu_cluster_resolver", "=", "None", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "        ", "tpu_cluster_resolver", "=", "tf", ".", "distribute", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "tf", ".", "config", ".", "experimental_connect_to_cluster", "(", "tpu_cluster_resolver", ")", "\n", "tf", ".", "tpu", ".", "experimental", ".", "initialize_tpu_system", "(", "tpu_cluster_resolver", ")", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "# evaluation_master=FLAGS.master,", "\n", "model_dir", "=", "FLAGS", ".", "output_dir", ",", "\n", "keep_checkpoint_max", "=", "keep_chceckpoint_max", ",", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "save_checkpoints_steps", ",", "\n", "# session_config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True),", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "\n", "model_config", "=", "ModelConfig", "(", "FLAGS", ",", "FLAGS", ".", "output_dir", ")", "\n", "model_config", ".", "logging_configs", "(", ")", "\n", "\n", "model_fn", "=", "model_fn_builder", "(", "model_config", ",", "model_sign", "=", "\"mention_proposal\"", ")", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "# eval_on_tpu=FLAGS.use_tpu,", "\n", "warm_start_from", "=", "tf", ".", "estimator", ".", "WarmStartSettings", "(", "FLAGS", ".", "init_checkpoint", ",", "\n", "vars_to_warm_start", "=", "\"bert*\"", ")", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "train_batch_size", "=", "1", ",", "\n", "predict_batch_size", "=", "1", ")", "\n", "\n", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "        ", "estimator", ".", "train", "(", "input_fn", "=", "file_based_input_fn_builder", "(", "model_config", ".", "train_file", ",", "num_window", "=", "model_config", ".", "num_window", ",", "\n", "window_size", "=", "model_config", ".", "window_size", ",", "max_num_mention", "=", "model_config", ".", "max_num_mention", ",", "is_training", "=", "True", ",", "drop_remainder", "=", "True", ")", ",", "max_steps", "=", "num_train_steps", ")", "\n", "\n", "\n", "", "if", "FLAGS", ".", "do_eval", ":", "\n", "# doing evaluation  on a set of trained checkpoints, the checkpoint with the best score on the dev set will be selected.", "\n", "        ", "best_dev_f1", ",", "best_dev_prec", ",", "best_dev_rec", ",", "test_f1_when_dev_best", ",", "test_prec_when_dev_best", ",", "test_rec_when_dev_best", "=", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "\n", "best_ckpt_path", "=", "\"\"", "\n", "checkpoints_iterator", "=", "[", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "eval_dir", ",", "\"model.ckpt-{}\"", ".", "format", "(", "str", "(", "int", "(", "ckpt_idx", ")", ")", ")", ")", "for", "ckpt_idx", "in", "range", "(", "0", ",", "num_train_steps", ",", "FLAGS", ".", "save_checkpoints_steps", ")", "]", "\n", "for", "checkpoint_path", "in", "checkpoints_iterator", "[", "1", ":", "]", ":", "\n", "            ", "eval_dev_result", "=", "estimator", ".", "evaluate", "(", "input_fn", "=", "file_based_input_fn_builder", "(", "FLAGS", ".", "dev_file", ",", "num_window", "=", "FLAGS", ".", "num_window", ",", "\n", "window_size", "=", "FLAGS", ".", "window_size", ",", "max_num_mention", "=", "FLAGS", ".", "max_num_mention", ",", "is_training", "=", "False", ",", "drop_remainder", "=", "False", ")", ",", "\n", "steps", "=", "698", ",", "checkpoint_path", "=", "checkpoint_path", ")", "\n", "dev_f1", "=", "2", "*", "eval_dev_result", "[", "\"precision\"", "]", "*", "eval_dev_result", "[", "\"recall\"", "]", "/", "(", "eval_dev_result", "[", "\"precision\"", "]", "+", "eval_dev_result", "[", "\"recall\"", "]", "+", "1e-10", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Current ckpt path is ***** : {}\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** EVAL ON DEV SET *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** [DEV EVAL] ***** : precision: {:.4f}, recall: {:.4f}, f1: {:.4f}\"", ".", "format", "(", "eval_dev_result", "[", "\"precision\"", "]", ",", "eval_dev_result", "[", "\"recall\"", "]", ",", "dev_f1", ")", ")", "\n", "if", "dev_f1", ">", "best_dev_f1", ":", "\n", "                ", "best_dev_f1", ",", "best_dev_prec", ",", "best_dev_rec", "=", "dev_f1", ",", "eval_dev_result", "[", "\"precision\"", "]", ",", "eval_dev_result", "[", "\"recall\"", "]", "\n", "best_ckpt_path", "=", "checkpoint_path", "\n", "eval_test_result", "=", "estimator", ".", "evaluate", "(", "input_fn", "=", "file_based_input_fn_builder", "(", "FLAGS", ".", "test_file", ",", "\n", "num_window", "=", "FLAGS", ".", "num_window", ",", "window_size", "=", "FLAGS", ".", "window_size", ",", "max_num_mention", "=", "FLAGS", ".", "max_num_mention", ",", "\n", "is_training", "=", "False", ",", "drop_remainder", "=", "False", ")", ",", "steps", "=", "698", ",", "checkpoint_path", "=", "checkpoint_path", ")", "\n", "test_f1", "=", "2", "*", "eval_test_result", "[", "\"precision\"", "]", "*", "eval_test_result", "[", "\"recall\"", "]", "/", "(", "eval_test_result", "[", "\"precision\"", "]", "+", "eval_test_result", "[", "\"recall\"", "]", "+", "1e-10", ")", "\n", "test_f1_when_dev_best", ",", "test_prec_when_dev_best", ",", "test_rec_when_dev_best", "=", "test_f1", ",", "eval_test_result", "[", "\"precision\"", "]", ",", "eval_test_result", "[", "\"recall\"", "]", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** EVAL ON TEST SET *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** [TEST EVAL] ***** : precision: {:.4f}, recall: {:.4f}, f1: {:.4f}\"", ".", "format", "(", "eval_test_result", "[", "\"precision\"", "]", ",", "eval_test_result", "[", "\"recall\"", "]", ",", "test_f1", ")", ")", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"*\"", "*", "20", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"- @@@@@ the path to the BEST DEV result is : {}\"", ".", "format", "(", "best_ckpt_path", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"- @@@@@ BEST DEV F1 : {:.4f}, Precision : {:.4f}, Recall : {:.4f},\"", ".", "format", "(", "best_dev_f1", ",", "best_dev_prec", ",", "best_dev_rec", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"- @@@@@ TEST when DEV best F1 : {:.4f}, Precision : {:.4f}, Recall : {:.4f},\"", ".", "format", "(", "test_f1_when_dev_best", ",", "test_prec_when_dev_best", ",", "test_rec_when_dev_best", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"- @@@@@ mention_proposal_only_concate {}\"", ".", "format", "(", "FLAGS", ".", "mention_proposal_only_concate", ")", ")", "\n", "\n", "\n", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "        ", "tp", ",", "fp", ",", "fn", "=", "0", ",", "0", ",", "0", "\n", "epsilon", "=", "1e-10", "\n", "for", "doc_output", "in", "estimator", ".", "predict", "(", "file_based_input_fn_builder", "(", "FLAGS", ".", "test_file", ",", "\n", "num_window", "=", "FLAGS", ".", "num_window", ",", "window_size", "=", "FLAGS", ".", "window_size", ",", "max_num_mention", "=", "FLAGS", ".", "max_num_mention", ",", "\n", "is_training", "=", "False", ",", "drop_remainder", "=", "False", ")", ",", "checkpoint_path", "=", "FLAGS", ".", "eval_checkpoint", ",", "yield_single_examples", "=", "False", ")", ":", "\n", "# iterate over each doc for evaluation", "\n", "            ", "pred_span_label", ",", "gold_span_label", "=", "mention_proposal_prediction", "(", "FLAGS", ",", "doc_output", ")", "\n", "\n", "tem_tp", "=", "np", ".", "logical_and", "(", "pred_span_label", ",", "gold_span_label", ")", ".", "sum", "(", ")", "\n", "tem_fp", "=", "np", ".", "logical_and", "(", "pred_span_label", ",", "np", ".", "logical_not", "(", "gold_span_label", ")", ")", ".", "sum", "(", ")", "\n", "tem_fn", "=", "np", ".", "logical_and", "(", "np", ".", "logical_not", "(", "pred_span_label", ")", ",", "gold_span_label", ")", ".", "sum", "(", ")", "\n", "\n", "tp", "+=", "tem_tp", "\n", "fp", "+=", "tem_fp", "\n", "fn", "+=", "tem_fn", "\n", "\n", "", "p", "=", "tp", "/", "(", "tp", "+", "fp", "+", "epsilon", ")", "\n", "r", "=", "tp", "/", "(", "tp", "+", "fn", "+", "epsilon", ")", "\n", "f", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", "+", "epsilon", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Average precision: {:.4f}, Average recall: {:.4f}, Average F1 {:.4f}\"", ".", "format", "(", "p", ",", "r", ",", "f", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.transform_spanbert_pytorch_to_tf.load_models": [[30, 42], ["bert.modeling.BertConfig.from_json_file", "tensorflow.ones", "bert.modeling.BertModel"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.from_json_file"], ["def", "load_models", "(", "bert_config_path", ",", ")", ":", "\n", "    ", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "bert_config_path", ")", "\n", "input_ids", "=", "tf", ".", "ones", "(", "(", "8", ",", "128", ")", ",", "tf", ".", "int32", ")", "\n", "\n", "model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "False", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "use_one_hot_embeddings", "=", "False", ",", "\n", "scope", "=", "\"bert\"", ")", "\n", "\n", "return", "model", ",", "bert_config", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.transform_spanbert_pytorch_to_tf.copy_checkpoint": [[44, 47], ["shutil.copyfile"], "function", ["None"], ["", "def", "copy_checkpoint", "(", "source", ",", "target", ")", ":", "\n", "  ", "for", "ext", "in", "(", "\".index\"", ",", "\".data-00000-of-00001\"", ")", ":", "\n", "    ", "shutil", ".", "copyfile", "(", "source", "+", "ext", ",", "target", "+", "ext", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.transform_spanbert_pytorch_to_tf.main": [[49, 71], ["tensorflow.Session", "transform_spanbert_pytorch_to_tf.load_models", "tensorflow.trainable_variables", "bert.modeling.get_assignment_map_from_checkpoint", "session.run", "init_from_checkpoint", "tensorflow.train.Saver", "tf.train.Saver.save", "transform_spanbert_pytorch_to_tf.copy_checkpoint", "print", "print", "print", "tensorflow.global_variables_initializer", "os.path.join", "os.path.join", "os.path.join", "print", "str"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.transform_spanbert_pytorch_to_tf.load_models", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_assignment_map_from_checkpoint", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.transform_spanbert_pytorch_to_tf.copy_checkpoint"], ["", "", "def", "main", "(", "bert_config_path", ",", "bert_ckpt_path", ",", "pytorch_init_checkpoint", ",", "output_tf_dir", ")", ":", "\n", "\n", "    ", "with", "tf", ".", "Session", "(", ")", "as", "session", ":", "\n", "        ", "model", ",", "bert_config", "=", "load_models", "(", "bert_config_path", ")", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "assignment_map", ",", "initialized_variable_names", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "bert_ckpt_path", ")", "\n", "session", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "init_from_checkpoint", "=", "load_from_pytorch_checkpoint", "\n", "init_from_checkpoint", "(", "pytorch_init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "for", "var", "in", "tvars", ":", "\n", "            ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "                ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "print", "(", "\"name = %s, shape = %s%s\"", "%", "(", "var", ".", "name", ",", "var", ".", "shape", ",", "init_string", ")", ")", "\n", "\n", "", "", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "saver", ".", "save", "(", "session", ",", "os", ".", "path", ".", "join", "(", "output_tf_dir", ",", "\"model\"", ")", ",", "global_step", "=", "100", ")", "\n", "copy_checkpoint", "(", "os", ".", "path", ".", "join", "(", "output_tf_dir", ",", "\"model-{}\"", ".", "format", "(", "str", "(", "100", ")", ")", ")", ",", "os", ".", "path", ".", "join", "(", "output_tf_dir", ",", "\"bert_model.ckpt\"", ")", ")", "\n", "print", "(", "\"=*=\"", "*", "30", ")", "\n", "print", "(", "\"save models : {}\"", ".", "format", "(", "output_tf_dir", ")", ")", "\n", "print", "(", "\"=*=\"", "*", "30", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.transform_spanbert_pytorch_to_tf.parse_args": [[73, 99], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "random.seed", "numpy.random.seed", "torch.manual_seed", "tensorflow.set_random_seed", "torch.cuda.manual_seed_all", "os.makedirs", "shutil", "print", "print"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.transform_spanbert_pytorch_to_tf.parse_args"], ["", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--spanbert_config_path\"", ",", "default", "=", "\"/home/lixiaoya/spanbert_base_cased/config.json\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_tf_ckpt_path\"", ",", "default", "=", "\"/home/lixiaoya/cased_L-12_H-768_A-12/bert_model.ckpt\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--spanbert_pytorch_bin_path\"", ",", "default", "=", "\"/home/lixiaoya/spanbert_base_cased/pytorch_model.bin\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_spanbert_tf_dir\"", ",", "default", "=", "\"/home/lixiaoya/tf_spanbert_base_case\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "default", "=", "2333", ",", "type", "=", "int", ")", "\n", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "tf", ".", "set_random_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "output_spanbert_tf_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "try", ":", "\n", "        ", "shutil", "(", "args", ".", "spanbert_config_path", ",", "args", ".", "output_spanbert_tf_dir", ")", "\n", "", "except", ":", "\n", "        ", "print", "(", "\"#=#\"", "*", "30", ")", "\n", "print", "(", "\"copy spanbert_config from {} to {}\"", ".", "format", "(", "args", ".", "spanbert_config_path", ",", "args", ".", "output_spanbert_tf_dir", ")", ")", "\n", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.SquadExample.__init__": [[164, 179], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "qas_id", ",", "\n", "question_text", ",", "\n", "doc_tokens", ",", "\n", "orig_answer_text", "=", "None", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "False", ")", ":", "\n", "    ", "self", ".", "qas_id", "=", "qas_id", "\n", "self", ".", "question_text", "=", "question_text", "\n", "self", ".", "doc_tokens", "=", "doc_tokens", "\n", "self", ".", "orig_answer_text", "=", "orig_answer_text", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.SquadExample.__str__": [[180, 182], ["run_squad.SquadExample.__repr__"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.SquadExample.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.SquadExample.__repr__": [[183, 196], ["bert.tokenization.printable_text", "bert.tokenization.printable_text"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.printable_text", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.printable_text"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "    ", "s", "=", "\"\"", "\n", "s", "+=", "\"qas_id: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "self", ".", "qas_id", ")", ")", "\n", "s", "+=", "\", question_text: %s\"", "%", "(", "\n", "tokenization", ".", "printable_text", "(", "self", ".", "question_text", ")", ")", "\n", "s", "+=", "\", doc_tokens: [%s]\"", "%", "(", "\" \"", ".", "join", "(", "self", ".", "doc_tokens", ")", ")", "\n", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", start_position: %d\"", "%", "(", "self", ".", "start_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", end_position: %d\"", "%", "(", "self", ".", "end_position", ")", "\n", "", "if", "self", ".", "start_position", ":", "\n", "      ", "s", "+=", "\", is_impossible: %r\"", "%", "(", "self", ".", "is_impossible", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.InputFeatures.__init__": [[201, 226], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "unique_id", ",", "\n", "example_index", ",", "\n", "doc_span_index", ",", "\n", "tokens", ",", "\n", "token_to_orig_map", ",", "\n", "token_is_max_context", ",", "\n", "input_ids", ",", "\n", "input_mask", ",", "\n", "segment_ids", ",", "\n", "start_position", "=", "None", ",", "\n", "end_position", "=", "None", ",", "\n", "is_impossible", "=", "None", ")", ":", "\n", "    ", "self", ".", "unique_id", "=", "unique_id", "\n", "self", ".", "example_index", "=", "example_index", "\n", "self", ".", "doc_span_index", "=", "doc_span_index", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "token_to_orig_map", "=", "token_to_orig_map", "\n", "self", ".", "token_is_max_context", "=", "token_is_max_context", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "start_position", "=", "start_position", "\n", "self", ".", "end_position", "=", "end_position", "\n", "self", ".", "is_impossible", "=", "is_impossible", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.FeatureWriter.__init__": [[1062, 1067], ["tensorflow.python_io.TFRecordWriter"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "filename", ",", "is_training", ")", ":", "\n", "    ", "self", ".", "filename", "=", "filename", "\n", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "num_features", "=", "0", "\n", "self", ".", "_writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.FeatureWriter.process_feature": [[1068, 1093], ["collections.OrderedDict", "run_squad.FeatureWriter.process_feature.create_int_feature"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.build_dataset_to_tfrecord.create_int_feature"], ["", "def", "process_feature", "(", "self", ",", "feature", ")", ":", "\n", "    ", "\"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"", "\n", "self", ".", "num_features", "+=", "1", "\n", "\n", "def", "create_int_feature", "(", "values", ")", ":", "\n", "      ", "feature", "=", "tf", ".", "train", ".", "Feature", "(", "\n", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "feature", "\n", "\n", "", "features", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "features", "[", "\"unique_ids\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "unique_id", "]", ")", "\n", "features", "[", "\"input_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_ids", ")", "\n", "features", "[", "\"input_mask\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_mask", ")", "\n", "features", "[", "\"segment_ids\"", "]", "=", "create_int_feature", "(", "feature", ".", "segment_ids", ")", "\n", "\n", "if", "self", ".", "is_training", ":", "\n", "      ", "features", "[", "\"start_positions\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "start_position", "]", ")", "\n", "features", "[", "\"end_positions\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "end_position", "]", ")", "\n", "impossible", "=", "0", "\n", "if", "feature", ".", "is_impossible", ":", "\n", "        ", "impossible", "=", "1", "\n", "", "features", "[", "\"is_impossible\"", "]", "=", "create_int_feature", "(", "[", "impossible", "]", ")", "\n", "\n", "", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "features", ")", ")", "\n", "self", ".", "_writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.FeatureWriter.close": [[1094, 1096], ["run_squad.FeatureWriter._writer.close"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.FeatureWriter.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "    ", "self", ".", "_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.read_squad_examples": [[228, 308], ["tensorflow.gfile.Open", "json.load", "ord", "run_squad.read_squad_examples.is_whitespace"], "function", ["None"], ["", "", "def", "read_squad_examples", "(", "input_file", ",", "is_training", ")", ":", "\n", "  ", "\"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "input_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "input_data", "=", "json", ".", "load", "(", "reader", ")", "[", "\"data\"", "]", "\n", "\n", "", "def", "is_whitespace", "(", "c", ")", ":", "\n", "    ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", ":", "\n", "      ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "examples", "=", "[", "]", "\n", "for", "entry", "in", "input_data", ":", "\n", "    ", "for", "paragraph", "in", "entry", "[", "\"paragraphs\"", "]", ":", "\n", "      ", "paragraph_text", "=", "paragraph", "[", "\"context\"", "]", "\n", "doc_tokens", "=", "[", "]", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "        ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "          ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "          ", "if", "prev_is_whitespace", ":", "\n", "            ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "            ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "for", "qa", "in", "paragraph", "[", "\"qas\"", "]", ":", "\n", "        ", "qas_id", "=", "qa", "[", "\"id\"", "]", "\n", "question_text", "=", "qa", "[", "\"question\"", "]", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "None", "\n", "is_impossible", "=", "False", "\n", "if", "is_training", ":", "\n", "\n", "          ", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "            ", "is_impossible", "=", "qa", "[", "\"is_impossible\"", "]", "\n", "", "if", "(", "len", "(", "qa", "[", "\"answers\"", "]", ")", "!=", "1", ")", "and", "(", "not", "is_impossible", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"For training, each question should have exactly 1 answer.\"", ")", "\n", "", "if", "not", "is_impossible", ":", "\n", "            ", "answer", "=", "qa", "[", "\"answers\"", "]", "[", "0", "]", "\n", "orig_answer_text", "=", "answer", "[", "\"text\"", "]", "\n", "answer_offset", "=", "answer", "[", "\"answer_start\"", "]", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "answer_length", "-", "\n", "1", "]", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "\n", "doc_tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "tokenization", ".", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", ":", "\n", "              ", "tf", ".", "logging", ".", "warning", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "continue", "\n", "", "", "else", ":", "\n", "            ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "\n", "", "", "example", "=", "SquadExample", "(", "\n", "qas_id", "=", "qas_id", ",", "\n", "question_text", "=", "question_text", ",", "\n", "doc_tokens", "=", "doc_tokens", ",", "\n", "orig_answer_text", "=", "orig_answer_text", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "is_impossible", ")", "\n", "examples", ".", "append", "(", "example", ")", "\n", "\n", "", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.convert_examples_to_features": [[310, 475], ["enumerate", "tokenizer.tokenize", "enumerate", "collections.namedtuple", "enumerate", "len", "orig_to_tok_index.append", "tokenizer.tokenize", "run_squad._improve_answer_span", "len", "doc_spans.append", "min", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "range", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "run_squad.InputFeatures", "output_fn", "len", "tok_to_orig_index.append", "all_doc_tokens.append", "len", "len", "collections.namedtuple.", "len", "tokens.append", "segment_ids.append", "run_squad._check_is_max_context", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "len", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "len", "len", "len", "bert.tokenization.printable_text", "bert.tokenization.printable_text", "str", "str", "str", "six.iteritems", "six.iteritems"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad._improve_answer_span", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_tokens_to_ids", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad._check_is_max_context", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.printable_text", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.printable_text"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "tokenizer", ",", "max_seq_length", ",", "\n", "doc_stride", ",", "max_query_length", ",", "is_training", ",", "\n", "output_fn", ")", ":", "\n", "  ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "unique_id", "=", "1000000000", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "    ", "query_tokens", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "question_text", ")", "\n", "\n", "if", "len", "(", "query_tokens", ")", ">", "max_query_length", ":", "\n", "      ", "query_tokens", "=", "query_tokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "tok_to_orig_index", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "\n", "all_doc_tokens", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "example", ".", "doc_tokens", ")", ":", "\n", "      ", "orig_to_tok_index", ".", "append", "(", "len", "(", "all_doc_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "for", "sub_token", "in", "sub_tokens", ":", "\n", "        ", "tok_to_orig_index", ".", "append", "(", "i", ")", "\n", "all_doc_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "tok_start_position", "=", "None", "\n", "tok_end_position", "=", "None", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "      ", "tok_start_position", "=", "-", "1", "\n", "tok_end_position", "=", "-", "1", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "      ", "tok_start_position", "=", "orig_to_tok_index", "[", "example", ".", "start_position", "]", "\n", "if", "example", ".", "end_position", "<", "len", "(", "example", ".", "doc_tokens", ")", "-", "1", ":", "\n", "        ", "tok_end_position", "=", "orig_to_tok_index", "[", "example", ".", "end_position", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "        ", "tok_end_position", "=", "len", "(", "all_doc_tokens", ")", "-", "1", "\n", "", "(", "tok_start_position", ",", "tok_end_position", ")", "=", "_improve_answer_span", "(", "\n", "all_doc_tokens", ",", "tok_start_position", ",", "tok_end_position", ",", "tokenizer", ",", "\n", "example", ".", "orig_answer_text", ")", "\n", "\n", "# The -3 accounts for [CLS], [SEP] and [SEP]", "\n", "", "max_tokens_for_doc", "=", "max_seq_length", "-", "len", "(", "query_tokens", ")", "-", "3", "\n", "\n", "# We can have documents that are longer than the maximum sequence length.", "\n", "# To deal with this we do a sliding window approach, where we take chunks", "\n", "# of the up to our max length with a stride of `doc_stride`.", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "all_doc_tokens", ")", ":", "\n", "      ", "length", "=", "len", "(", "all_doc_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "        ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "all_doc_tokens", ")", ":", "\n", "        ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "      ", "tokens", "=", "[", "]", "\n", "token_to_orig_map", "=", "{", "}", "\n", "token_is_max_context", "=", "{", "}", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "query_tokens", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "        ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "token_to_orig_map", "[", "len", "(", "tokens", ")", "]", "=", "tok_to_orig_index", "[", "split_token_index", "]", "\n", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "tokens", ")", "]", "=", "is_max_context", "\n", "tokens", ".", "append", "(", "all_doc_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "        ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "out_of_span", "=", "False", "\n", "if", "not", "(", "tok_start_position", ">=", "doc_start", "and", "\n", "tok_end_position", "<=", "doc_end", ")", ":", "\n", "          ", "out_of_span", "=", "True", "\n", "", "if", "out_of_span", ":", "\n", "          ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "", "else", ":", "\n", "          ", "doc_offset", "=", "len", "(", "query_tokens", ")", "+", "2", "\n", "start_position", "=", "tok_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "tok_end_position", "-", "doc_start", "+", "doc_offset", "\n", "\n", "", "", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "        ", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "\n", "", "if", "example_index", "<", "20", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"unique_id: %s\"", "%", "(", "unique_id", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"example_index: %s\"", "%", "(", "example_index", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"doc_span_index: %s\"", "%", "(", "doc_span_index", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"token_to_orig_map: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "\"%d:%d\"", "%", "(", "x", ",", "y", ")", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_to_orig_map", ")", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"token_is_max_context: %s\"", "%", "\" \"", ".", "join", "(", "[", "\n", "\"%d:%s\"", "%", "(", "x", ",", "y", ")", "for", "(", "x", ",", "y", ")", "in", "six", ".", "iteritems", "(", "token_is_max_context", ")", "\n", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "if", "is_training", "and", "example", ".", "is_impossible", ":", "\n", "          ", "tf", ".", "logging", ".", "info", "(", "\"impossible example\"", ")", "\n", "", "if", "is_training", "and", "not", "example", ".", "is_impossible", ":", "\n", "          ", "answer_text", "=", "\" \"", ".", "join", "(", "tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"start_position: %d\"", "%", "(", "start_position", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"end_position: %d\"", "%", "(", "end_position", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"answer: %s\"", "%", "(", "tokenization", ".", "printable_text", "(", "answer_text", ")", ")", ")", "\n", "\n", "", "", "feature", "=", "InputFeatures", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "example_index", "=", "example_index", ",", "\n", "doc_span_index", "=", "doc_span_index", ",", "\n", "tokens", "=", "tokens", ",", "\n", "token_to_orig_map", "=", "token_to_orig_map", ",", "\n", "token_is_max_context", "=", "token_is_max_context", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "start_position", "=", "start_position", ",", "\n", "end_position", "=", "end_position", ",", "\n", "is_impossible", "=", "example", ".", "is_impossible", ")", "\n", "\n", "# Run callback", "\n", "output_fn", "(", "feature", ")", "\n", "\n", "unique_id", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad._improve_answer_span": [[477, 512], ["range", "tokenizer.tokenize", "range"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "", "", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "  ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "\n", "# The SQuAD annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in SQuAD, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "    ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "      ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "        ", "return", "(", "new_start", ",", "new_end", ")", "\n", "\n", "", "", "", "return", "(", "input_start", ",", "input_end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad._check_is_max_context": [[514, 549], ["enumerate", "min"], "function", ["None"], ["", "def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "  ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "    ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "      ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "      ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "      ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.create_model": [[551, 589], ["bert.modeling.BertModel", "modeling.BertModel.get_sequence_output", "bert.modeling.get_shape_list", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.nn.bias_add", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.unstack", "tensorflow.truncated_normal_initializer", "tensorflow.zeros_initializer"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list"], ["", "def", "create_model", "(", "bert_config", ",", "is_training", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "\n", "use_one_hot_embeddings", ")", ":", "\n", "  ", "\"\"\"Creates a classification model.\"\"\"", "\n", "model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "token_type_ids", "=", "segment_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "final_hidden", "=", "model", ".", "get_sequence_output", "(", ")", "\n", "\n", "final_hidden_shape", "=", "modeling", ".", "get_shape_list", "(", "final_hidden", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "final_hidden_shape", "[", "0", "]", "\n", "seq_length", "=", "final_hidden_shape", "[", "1", "]", "\n", "hidden_size", "=", "final_hidden_shape", "[", "2", "]", "\n", "\n", "output_weights", "=", "tf", ".", "get_variable", "(", "\n", "\"cls/squad/output_weights\"", ",", "[", "2", ",", "hidden_size", "]", ",", "\n", "initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", "\n", "\n", "output_bias", "=", "tf", ".", "get_variable", "(", "\n", "\"cls/squad/output_bias\"", ",", "[", "2", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "final_hidden_matrix", "=", "tf", ".", "reshape", "(", "final_hidden", ",", "\n", "[", "batch_size", "*", "seq_length", ",", "hidden_size", "]", ")", "\n", "logits", "=", "tf", ".", "matmul", "(", "final_hidden_matrix", ",", "output_weights", ",", "transpose_b", "=", "True", ")", "\n", "logits", "=", "tf", ".", "nn", ".", "bias_add", "(", "logits", ",", "output_bias", ")", "\n", "\n", "logits", "=", "tf", ".", "reshape", "(", "logits", ",", "[", "batch_size", ",", "seq_length", ",", "2", "]", ")", "\n", "logits", "=", "tf", ".", "transpose", "(", "logits", ",", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "\n", "unstacked_logits", "=", "tf", ".", "unstack", "(", "logits", ",", "axis", "=", "0", ")", "\n", "\n", "(", "start_logits", ",", "end_logits", ")", "=", "(", "unstacked_logits", "[", "0", "]", ",", "unstacked_logits", "[", "1", "]", ")", "\n", "\n", "return", "(", "start_logits", ",", "end_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.model_fn_builder": [[591, 686], ["tensorflow.logging.info", "sorted", "run_squad.create_model", "tensorflow.trainable_variables", "tensorflow.logging.info", "features.keys", "tensorflow.logging.info", "bert.modeling.get_assignment_map_from_checkpoint", "tensorflow.logging.info", "compute_loss"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.create_model", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_assignment_map_from_checkpoint"], ["", "def", "model_fn_builder", "(", "bert_config", ",", "init_checkpoint", ",", "learning_rate", ",", "\n", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ",", "\n", "use_one_hot_embeddings", ")", ":", "\n", "  ", "\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Features ***\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "unique_ids", "=", "features", "[", "\"unique_ids\"", "]", "\n", "input_ids", "=", "features", "[", "\"input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"input_mask\"", "]", "\n", "segment_ids", "=", "features", "[", "\"segment_ids\"", "]", "\n", "\n", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "(", "start_logits", ",", "end_logits", ")", "=", "create_model", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "\n", "initialized_variable_names", "=", "{", "}", "\n", "scaffold_fn", "=", "None", "\n", "if", "init_checkpoint", ":", "\n", "      ", "(", "assignment_map", ",", "initialized_variable_names", "\n", ")", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", "\n", "if", "use_tpu", ":", "\n", "\n", "        ", "def", "tpu_scaffold", "(", ")", ":", "\n", "          ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "        ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "      ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "        ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "\n", "init_string", ")", "\n", "\n", "", "output_spec", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "seq_length", "=", "modeling", ".", "get_shape_list", "(", "input_ids", ")", "[", "1", "]", "\n", "\n", "def", "compute_loss", "(", "logits", ",", "positions", ")", ":", "\n", "        ", "one_hot_positions", "=", "tf", ".", "one_hot", "(", "\n", "positions", ",", "depth", "=", "seq_length", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "loss", "=", "-", "tf", ".", "reduce_mean", "(", "\n", "tf", ".", "reduce_sum", "(", "one_hot_positions", "*", "log_probs", ",", "axis", "=", "-", "1", ")", ")", "\n", "return", "loss", "\n", "\n", "", "start_positions", "=", "features", "[", "\"start_positions\"", "]", "\n", "end_positions", "=", "features", "[", "\"end_positions\"", "]", "\n", "\n", "start_loss", "=", "compute_loss", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "compute_loss", "(", "end_logits", ",", "end_positions", ")", "\n", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2.0", "\n", "\n", "train_op", "=", "optimization", ".", "create_optimizer", "(", "\n", "total_loss", ",", "learning_rate", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ")", "\n", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "total_loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "      ", "predictions", "=", "{", "\n", "\"unique_ids\"", ":", "unique_ids", ",", "\n", "\"start_logits\"", ":", "start_logits", ",", "\n", "\"end_logits\"", ":", "end_logits", ",", "\n", "}", "\n", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "predictions", "=", "predictions", ",", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"Only TRAIN and PREDICT modes are supported: %s\"", "%", "(", "mode", ")", ")", "\n", "\n", "", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.input_fn_builder": [[688, 736], ["tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.parse_single_example", "list", "tensorflow.data.TFRecordDataset", "d.shuffle.apply", "tf.parse_single_example.keys", "d.shuffle.repeat", "d.shuffle.shuffle", "tensorflow.contrib.data.map_and_batch", "tensorflow.to_int32", "run_squad.input_fn_builder._decode_record"], "function", ["None"], ["", "def", "input_fn_builder", "(", "input_file", ",", "seq_length", ",", "is_training", ",", "drop_remainder", ")", ":", "\n", "  ", "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"", "\n", "\n", "name_to_features", "=", "{", "\n", "\"unique_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_mask\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"segment_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "}", "\n", "\n", "if", "is_training", ":", "\n", "    ", "name_to_features", "[", "\"start_positions\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", "\n", "name_to_features", "[", "\"end_positions\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", "\n", "\n", "", "def", "_decode_record", "(", "record", ",", "name_to_features", ")", ":", "\n", "    ", "\"\"\"Decodes a record to a TensorFlow example.\"\"\"", "\n", "example", "=", "tf", ".", "parse_single_example", "(", "record", ",", "name_to_features", ")", "\n", "\n", "# tf.Example only supports tf.int64, but the TPU only supports tf.int32.", "\n", "# So cast all int64 to int32.", "\n", "for", "name", "in", "list", "(", "example", ".", "keys", "(", ")", ")", ":", "\n", "      ", "t", "=", "example", "[", "name", "]", "\n", "if", "t", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "        ", "t", "=", "tf", ".", "to_int32", "(", "t", ")", "\n", "", "example", "[", "name", "]", "=", "t", "\n", "\n", "", "return", "example", "\n", "\n", "", "def", "input_fn", "(", "params", ")", ":", "\n", "    ", "\"\"\"The actual input function.\"\"\"", "\n", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "# For training, we want a lot of parallel reading and shuffling.", "\n", "# For eval, we want no shuffling and parallel reading doesn't matter.", "\n", "d", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "input_file", ")", "\n", "if", "is_training", ":", "\n", "      ", "d", "=", "d", ".", "repeat", "(", ")", "\n", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "100", ")", "\n", "\n", "", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "\n", "lambda", "record", ":", "_decode_record", "(", "record", ",", "name_to_features", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "drop_remainder", "=", "drop_remainder", ")", ")", "\n", "\n", "return", "d", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.write_predictions": [[742, 926], ["tensorflow.logging.info", "tensorflow.logging.info", "collections.defaultdict", "collections.namedtuple", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "example_index_to_features[].append", "enumerate", "sorted", "collections.namedtuple", "run_squad._compute_softmax", "enumerate", "tensorflow.gfile.GFile", "writer.write", "tensorflow.gfile.GFile", "writer.write", "run_squad._get_best_indexes", "run_squad._get_best_indexes", "sorted.append", "nbest.append", "nbest.append", "len", "total_scores.append", "collections.OrderedDict", "nbest_json.append", "len", "tensorflow.gfile.GFile", "writer.write", "collections.namedtuple.", "len", "tok_text.strip.replace", "tok_text.strip.replace", "tok_text.strip.strip", "run_squad.get_final_text", "collections.namedtuple.", "nbest.append", "collections.namedtuple.", "json.dumps", "json.dumps", "sorted.append", "tok_text.strip.split", "collections.namedtuple.", "json.dumps", "len", "len", "feature.token_is_max_context.get", "collections.namedtuple."], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad._compute_softmax", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad._get_best_indexes", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad._get_best_indexes", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.get_final_text"], ["def", "write_predictions", "(", "all_examples", ",", "all_features", ",", "all_results", ",", "n_best_size", ",", "\n", "max_answer_length", ",", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ")", ":", "\n", "  ", "\"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"", "\n", "tf", ".", "logging", ".", "info", "(", "\"Writing predictions to: %s\"", "%", "(", "output_prediction_file", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Writing nbest to: %s\"", "%", "(", "output_nbest_file", ")", ")", "\n", "\n", "example_index_to_features", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "feature", "in", "all_features", ":", "\n", "    ", "example_index_to_features", "[", "feature", ".", "example_index", "]", ".", "append", "(", "feature", ")", "\n", "\n", "", "unique_id_to_result", "=", "{", "}", "\n", "for", "result", "in", "all_results", ":", "\n", "    ", "unique_id_to_result", "[", "result", ".", "unique_id", "]", "=", "result", "\n", "\n", "", "_PrelimPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"PrelimPrediction\"", ",", "\n", "[", "\"feature_index\"", ",", "\"start_index\"", ",", "\"end_index\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "all_predictions", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "all_nbest_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "scores_diff_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "\n", "for", "(", "example_index", ",", "example", ")", "in", "enumerate", "(", "all_examples", ")", ":", "\n", "    ", "features", "=", "example_index_to_features", "[", "example_index", "]", "\n", "\n", "prelim_predictions", "=", "[", "]", "\n", "# keep track of the minimum score of null start+end of position 0", "\n", "score_null", "=", "1000000", "# large and positive", "\n", "min_null_feature_index", "=", "0", "# the paragraph slice with min mull score", "\n", "null_start_logit", "=", "0", "# the start logit at the slice with min null score", "\n", "null_end_logit", "=", "0", "# the end logit at the slice with min null score", "\n", "for", "(", "feature_index", ",", "feature", ")", "in", "enumerate", "(", "features", ")", ":", "\n", "      ", "result", "=", "unique_id_to_result", "[", "feature", ".", "unique_id", "]", "\n", "start_indexes", "=", "_get_best_indexes", "(", "result", ".", "start_logits", ",", "n_best_size", ")", "\n", "end_indexes", "=", "_get_best_indexes", "(", "result", ".", "end_logits", ",", "n_best_size", ")", "\n", "# if we could have irrelevant answers, get the min score of irrelevant", "\n", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "        ", "feature_null_score", "=", "result", ".", "start_logits", "[", "0", "]", "+", "result", ".", "end_logits", "[", "0", "]", "\n", "if", "feature_null_score", "<", "score_null", ":", "\n", "          ", "score_null", "=", "feature_null_score", "\n", "min_null_feature_index", "=", "feature_index", "\n", "null_start_logit", "=", "result", ".", "start_logits", "[", "0", "]", "\n", "null_end_logit", "=", "result", ".", "end_logits", "[", "0", "]", "\n", "", "", "for", "start_index", "in", "start_indexes", ":", "\n", "        ", "for", "end_index", "in", "end_indexes", ":", "\n", "# We could hypothetically create invalid predictions, e.g., predict", "\n", "# that the start of the span is in the question. We throw out all", "\n", "# invalid predictions.", "\n", "          ", "if", "start_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", ">=", "len", "(", "feature", ".", "tokens", ")", ":", "\n", "            ", "continue", "\n", "", "if", "start_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", "not", "in", "feature", ".", "token_to_orig_map", ":", "\n", "            ", "continue", "\n", "", "if", "not", "feature", ".", "token_is_max_context", ".", "get", "(", "start_index", ",", "False", ")", ":", "\n", "            ", "continue", "\n", "", "if", "end_index", "<", "start_index", ":", "\n", "            ", "continue", "\n", "", "length", "=", "end_index", "-", "start_index", "+", "1", "\n", "if", "length", ">", "max_answer_length", ":", "\n", "            ", "continue", "\n", "", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "feature_index", ",", "\n", "start_index", "=", "start_index", ",", "\n", "end_index", "=", "end_index", ",", "\n", "start_logit", "=", "result", ".", "start_logits", "[", "start_index", "]", ",", "\n", "end_logit", "=", "result", ".", "end_logits", "[", "end_index", "]", ")", ")", "\n", "\n", "", "", "", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "      ", "prelim_predictions", ".", "append", "(", "\n", "_PrelimPrediction", "(", "\n", "feature_index", "=", "min_null_feature_index", ",", "\n", "start_index", "=", "0", ",", "\n", "end_index", "=", "0", ",", "\n", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "", "prelim_predictions", "=", "sorted", "(", "\n", "prelim_predictions", ",", "\n", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "_NbestPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"NbestPrediction\"", ",", "[", "\"text\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "seen_predictions", "=", "{", "}", "\n", "nbest", "=", "[", "]", "\n", "for", "pred", "in", "prelim_predictions", ":", "\n", "      ", "if", "len", "(", "nbest", ")", ">=", "n_best_size", ":", "\n", "        ", "break", "\n", "", "feature", "=", "features", "[", "pred", ".", "feature_index", "]", "\n", "if", "pred", ".", "start_index", ">", "0", ":", "# this is a non-null prediction", "\n", "        ", "tok_tokens", "=", "feature", ".", "tokens", "[", "pred", ".", "start_index", ":", "(", "pred", ".", "end_index", "+", "1", ")", "]", "\n", "orig_doc_start", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "start_index", "]", "\n", "orig_doc_end", "=", "feature", ".", "token_to_orig_map", "[", "pred", ".", "end_index", "]", "\n", "orig_tokens", "=", "example", ".", "doc_tokens", "[", "orig_doc_start", ":", "(", "orig_doc_end", "+", "1", ")", "]", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_tokens", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "tok_text", "=", "tok_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "tok_text", "=", "tok_text", ".", "strip", "(", ")", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tok_text", ".", "split", "(", ")", ")", "\n", "orig_text", "=", "\" \"", ".", "join", "(", "orig_tokens", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "tok_text", ",", "orig_text", ",", "do_lower_case", ")", "\n", "if", "final_text", "in", "seen_predictions", ":", "\n", "          ", "continue", "\n", "\n", "", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "", "else", ":", "\n", "        ", "final_text", "=", "\"\"", "\n", "seen_predictions", "[", "final_text", "]", "=", "True", "\n", "\n", "", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "final_text", ",", "\n", "start_logit", "=", "pred", ".", "start_logit", ",", "\n", "end_logit", "=", "pred", ".", "end_logit", ")", ")", "\n", "\n", "# if we didn't inlude the empty option in the n-best, inlcude it", "\n", "", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "      ", "if", "\"\"", "not", "in", "seen_predictions", ":", "\n", "        ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "\n", "text", "=", "\"\"", ",", "start_logit", "=", "null_start_logit", ",", "\n", "end_logit", "=", "null_end_logit", ")", ")", "\n", "# In very rare edge cases we could have no valid predictions. So we", "\n", "# just create a nonce prediction in this case to avoid failure.", "\n", "", "", "if", "not", "nbest", ":", "\n", "      ", "nbest", ".", "append", "(", "\n", "_NbestPrediction", "(", "text", "=", "\"empty\"", ",", "start_logit", "=", "0.0", ",", "end_logit", "=", "0.0", ")", ")", "\n", "\n", "", "assert", "len", "(", "nbest", ")", ">=", "1", "\n", "\n", "total_scores", "=", "[", "]", "\n", "best_non_null_entry", "=", "None", "\n", "for", "entry", "in", "nbest", ":", "\n", "      ", "total_scores", ".", "append", "(", "entry", ".", "start_logit", "+", "entry", ".", "end_logit", ")", "\n", "if", "not", "best_non_null_entry", ":", "\n", "        ", "if", "entry", ".", "text", ":", "\n", "          ", "best_non_null_entry", "=", "entry", "\n", "\n", "", "", "", "probs", "=", "_compute_softmax", "(", "total_scores", ")", "\n", "\n", "nbest_json", "=", "[", "]", "\n", "for", "(", "i", ",", "entry", ")", "in", "enumerate", "(", "nbest", ")", ":", "\n", "      ", "output", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output", "[", "\"text\"", "]", "=", "entry", ".", "text", "\n", "output", "[", "\"probability\"", "]", "=", "probs", "[", "i", "]", "\n", "output", "[", "\"start_logit\"", "]", "=", "entry", ".", "start_logit", "\n", "output", "[", "\"end_logit\"", "]", "=", "entry", ".", "end_logit", "\n", "nbest_json", ".", "append", "(", "output", ")", "\n", "\n", "", "assert", "len", "(", "nbest_json", ")", ">=", "1", "\n", "\n", "if", "not", "FLAGS", ".", "version_2_with_negative", ":", "\n", "      ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "[", "0", "]", "[", "\"text\"", "]", "\n", "", "else", ":", "\n", "# predict \"\" iff the null score - the score of best non-null > threshold", "\n", "      ", "score_diff", "=", "score_null", "-", "best_non_null_entry", ".", "start_logit", "-", "(", "\n", "best_non_null_entry", ".", "end_logit", ")", "\n", "scores_diff_json", "[", "example", ".", "qas_id", "]", "=", "score_diff", "\n", "if", "score_diff", ">", "FLAGS", ".", "null_score_diff_threshold", ":", "\n", "        ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "\"\"", "\n", "", "else", ":", "\n", "        ", "all_predictions", "[", "example", ".", "qas_id", "]", "=", "best_non_null_entry", ".", "text", "\n", "\n", "", "", "all_nbest_json", "[", "example", ".", "qas_id", "]", "=", "nbest_json", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_prediction_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "    ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_predictions", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_nbest_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "    ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "all_nbest_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "if", "FLAGS", ".", "version_2_with_negative", ":", "\n", "    ", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_null_log_odds_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "      ", "writer", ".", "write", "(", "json", ".", "dumps", "(", "scores_diff_json", ",", "indent", "=", "4", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.get_final_text": [[928, 1022], ["bert.tokenization.BasicTokenizer", "tok_text.find", "run_squad.get_final_text._strip_spaces"], "function", ["None"], ["", "", "", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ")", ":", "\n", "  ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "\n", "# When we created the data, we kept track of the alignment between original", "\n", "# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So", "\n", "# now `orig_text` contains the span of our original text corresponding to the", "\n", "# span that we predicted.", "\n", "#", "\n", "# However, `orig_text` may contain extra characters that we don't want in", "\n", "# our prediction.", "\n", "#", "\n", "# For example, let's say:", "\n", "#   pred_text = steve smith", "\n", "#   orig_text = Steve Smith's", "\n", "#", "\n", "# We don't want to return `orig_text` because it contains the extra \"'s\".", "\n", "#", "\n", "# We don't want to return `pred_text` because it's already been normalized", "\n", "# (the SQuAD eval script also does punctuation stripping/lower casing but", "\n", "# our tokenizer does additional normalization like stripping accent", "\n", "# characters).", "\n", "#", "\n", "# What we really want to return is \"Steve Smith\".", "\n", "#", "\n", "# Therefore, we have to apply a semi-complicated alignment heruistic between", "\n", "# `pred_text` and `orig_text` to get a character-to-charcter alignment. This", "\n", "# can fail in certain cases in which case we just return `orig_text`.", "\n", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "    ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "      ", "if", "c", "==", "\" \"", ":", "\n", "        ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\n", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "six", ".", "iteritems", "(", "tok_ns_to_s_map", ")", ":", "\n", "    ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "    ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "      ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "    ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "      ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "    ", "if", "FLAGS", ".", "verbose_logging", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad._get_best_indexes": [[1024, 1034], ["sorted", "range", "enumerate", "len", "best_indexes.append"], "function", ["None"], ["", "def", "_get_best_indexes", "(", "logits", ",", "n_best_size", ")", ":", "\n", "  ", "\"\"\"Get the n-best logits from a list.\"\"\"", "\n", "index_and_score", "=", "sorted", "(", "enumerate", "(", "logits", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "best_indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "index_and_score", ")", ")", ":", "\n", "    ", "if", "i", ">=", "n_best_size", ":", "\n", "      ", "break", "\n", "", "best_indexes", ".", "append", "(", "index_and_score", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "best_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad._compute_softmax": [[1036, 1057], ["math.exp", "exp_scores.append", "probs.append"], "function", ["None"], ["", "def", "_compute_softmax", "(", "scores", ")", ":", "\n", "  ", "\"\"\"Compute softmax probability over raw logits.\"\"\"", "\n", "if", "not", "scores", ":", "\n", "    ", "return", "[", "]", "\n", "\n", "", "max_score", "=", "None", "\n", "for", "score", "in", "scores", ":", "\n", "    ", "if", "max_score", "is", "None", "or", "score", ">", "max_score", ":", "\n", "      ", "max_score", "=", "score", "\n", "\n", "", "", "exp_scores", "=", "[", "]", "\n", "total_sum", "=", "0.0", "\n", "for", "score", "in", "scores", ":", "\n", "    ", "x", "=", "math", ".", "exp", "(", "score", "-", "max_score", ")", "\n", "exp_scores", ".", "append", "(", "x", ")", "\n", "total_sum", "+=", "x", "\n", "\n", "", "probs", "=", "[", "]", "\n", "for", "score", "in", "exp_scores", ":", "\n", "    ", "probs", ".", "append", "(", "score", "/", "total_sum", ")", "\n", "", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.validate_flags_or_throw": [[1098, 1125], ["bert.tokenization.validate_case_matches_checkpoint", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.validate_case_matches_checkpoint"], ["", "", "def", "validate_flags_or_throw", "(", "bert_config", ")", ":", "\n", "  ", "\"\"\"Validate the input FLAGS or throw an exception.\"\"\"", "\n", "tokenization", ".", "validate_case_matches_checkpoint", "(", "FLAGS", ".", "do_lower_case", ",", "\n", "FLAGS", ".", "init_checkpoint", ")", "\n", "\n", "if", "not", "FLAGS", ".", "do_train", "and", "not", "FLAGS", ".", "do_predict", ":", "\n", "    ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_predict` must be True.\"", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "if", "not", "FLAGS", ".", "train_file", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"If `do_train` is True, then `train_file` must be specified.\"", ")", "\n", "", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "    ", "if", "not", "FLAGS", ".", "predict_file", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"If `do_predict` is True, then `predict_file` must be specified.\"", ")", "\n", "\n", "", "", "if", "FLAGS", ".", "max_seq_length", ">", "bert_config", ".", "max_position_embeddings", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"Cannot use sequence length %d because the BERT model \"", "\n", "\"was only trained up to sequence length %d\"", "%", "\n", "(", "FLAGS", ".", "max_seq_length", ",", "bert_config", ".", "max_position_embeddings", ")", ")", "\n", "\n", "", "if", "FLAGS", ".", "max_seq_length", "<=", "FLAGS", ".", "max_query_length", "+", "3", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The max_seq_length (%d) must be greater than max_query_length \"", "\n", "\"(%d) + 3\"", "%", "(", "FLAGS", ".", "max_seq_length", ",", "FLAGS", ".", "max_query_length", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.main": [[1127, 1278], ["tensorflow.logging.set_verbosity", "bert.modeling.BertConfig.from_json_file", "run_squad.validate_flags_or_throw", "tensorflow.gfile.MakeDirs", "bert.tokenization.FullTokenizer", "tensorflow.contrib.tpu.RunConfig", "run_squad.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "tensorflow.contrib.cluster_resolver.TPUClusterResolver", "run_squad.read_squad_examples", "int", "int", "random.Random", "random.Random.shuffle", "run_squad.FeatureWriter", "run_squad.convert_examples_to_features", "run_squad.FeatureWriter.close", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_squad.input_fn_builder", "tf.contrib.tpu.TPUEstimator.train", "run_squad.read_squad_examples", "run_squad.FeatureWriter", "run_squad.convert_examples_to_features", "run_squad.FeatureWriter.close", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_squad.input_fn_builder", "tf.contrib.tpu.TPUEstimator.predict", "os.path.join", "os.path.join", "os.path.join", "run_squad.write_predictions", "tensorflow.contrib.tpu.TPUConfig", "len", "eval_features.append", "run_squad.FeatureWriter.process_feature", "len", "len", "int", "all_results.append", "os.path.join", "os.path.join", "tensorflow.logging.info", "float", "float", "RawResult", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.validate_flags_or_throw", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.model_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.read_squad_examples", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.convert_examples_to_features", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.input_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.read_squad_examples", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.convert_examples_to_features", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.FeatureWriter.close", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.input_fn_builder", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.write_predictions", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.run.run_squad.FeatureWriter.process_feature"], ["", "", "def", "main", "(", "_", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "FLAGS", ".", "bert_config_file", ")", "\n", "\n", "validate_flags_or_throw", "(", "bert_config", ")", "\n", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "FLAGS", ".", "output_dir", ")", "\n", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "\n", "tpu_cluster_resolver", "=", "None", "\n", "if", "FLAGS", ".", "use_tpu", "and", "FLAGS", ".", "tpu_name", ":", "\n", "    ", "tpu_cluster_resolver", "=", "tf", ".", "contrib", ".", "cluster_resolver", ".", "TPUClusterResolver", "(", "\n", "FLAGS", ".", "tpu_name", ",", "zone", "=", "FLAGS", ".", "tpu_zone", ",", "project", "=", "FLAGS", ".", "gcp_project", ")", "\n", "\n", "", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "cluster", "=", "tpu_cluster_resolver", ",", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "model_dir", "=", "FLAGS", ".", "output_dir", ",", "\n", "save_checkpoints_steps", "=", "FLAGS", ".", "save_checkpoints_steps", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "iterations_per_loop", "=", "FLAGS", ".", "iterations_per_loop", ",", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "num_warmup_steps", "=", "None", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "    ", "train_examples", "=", "read_squad_examples", "(", "\n", "input_file", "=", "FLAGS", ".", "train_file", ",", "is_training", "=", "True", ")", "\n", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "num_train_epochs", ")", "\n", "num_warmup_steps", "=", "int", "(", "num_train_steps", "*", "FLAGS", ".", "warmup_proportion", ")", "\n", "\n", "# Pre-shuffle the input to avoid having to make a very large shuffle", "\n", "# buffer in in the `input_fn`.", "\n", "rng", "=", "random", ".", "Random", "(", "12345", ")", "\n", "rng", ".", "shuffle", "(", "train_examples", ")", "\n", "\n", "", "model_fn", "=", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "learning_rate", "=", "FLAGS", ".", "learning_rate", ",", "\n", "num_train_steps", "=", "num_train_steps", ",", "\n", "num_warmup_steps", "=", "num_warmup_steps", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "use_one_hot_embeddings", "=", "FLAGS", ".", "use_tpu", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "train_batch_size", "=", "FLAGS", ".", "train_batch_size", ",", "\n", "predict_batch_size", "=", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "if", "FLAGS", ".", "do_train", ":", "\n", "# We write to a temporary file to avoid storing very large constant tensors", "\n", "# in memory.", "\n", "    ", "train_writer", "=", "FeatureWriter", "(", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"train.tf_record\"", ")", ",", "\n", "is_training", "=", "True", ")", "\n", "convert_examples_to_features", "(", "\n", "examples", "=", "train_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "doc_stride", "=", "FLAGS", ".", "doc_stride", ",", "\n", "max_query_length", "=", "FLAGS", ".", "max_query_length", ",", "\n", "is_training", "=", "True", ",", "\n", "output_fn", "=", "train_writer", ".", "process_feature", ")", "\n", "train_writer", ".", "close", "(", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num split examples = %d\"", ",", "train_writer", ".", "num_features", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "del", "train_examples", "\n", "\n", "train_input_fn", "=", "input_fn_builder", "(", "\n", "input_file", "=", "train_writer", ".", "filename", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "True", ",", "\n", "drop_remainder", "=", "True", ")", "\n", "estimator", ".", "train", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "num_train_steps", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "    ", "eval_examples", "=", "read_squad_examples", "(", "\n", "input_file", "=", "FLAGS", ".", "predict_file", ",", "is_training", "=", "False", ")", "\n", "\n", "eval_writer", "=", "FeatureWriter", "(", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"eval.tf_record\"", ")", ",", "\n", "is_training", "=", "False", ")", "\n", "eval_features", "=", "[", "]", "\n", "\n", "def", "append_feature", "(", "feature", ")", ":", "\n", "      ", "eval_features", ".", "append", "(", "feature", ")", "\n", "eval_writer", ".", "process_feature", "(", "feature", ")", "\n", "\n", "", "convert_examples_to_features", "(", "\n", "examples", "=", "eval_examples", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "doc_stride", "=", "FLAGS", ".", "doc_stride", ",", "\n", "max_query_length", "=", "FLAGS", ".", "max_query_length", ",", "\n", "is_training", "=", "False", ",", "\n", "output_fn", "=", "append_feature", ")", "\n", "eval_writer", ".", "close", "(", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running predictions *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num orig examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num split examples = %d\"", ",", "len", "(", "eval_features", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "all_results", "=", "[", "]", "\n", "\n", "predict_input_fn", "=", "input_fn_builder", "(", "\n", "input_file", "=", "eval_writer", ".", "filename", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "False", ",", "\n", "drop_remainder", "=", "False", ")", "\n", "\n", "# If running eval on the TPU, you will need to specify the number of", "\n", "# steps.", "\n", "all_results", "=", "[", "]", "\n", "for", "result", "in", "estimator", ".", "predict", "(", "\n", "predict_input_fn", ",", "yield_single_examples", "=", "True", ")", ":", "\n", "      ", "if", "len", "(", "all_results", ")", "%", "1000", "==", "0", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"Processing example: %d\"", "%", "(", "len", "(", "all_results", ")", ")", ")", "\n", "", "unique_id", "=", "int", "(", "result", "[", "\"unique_ids\"", "]", ")", "\n", "start_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "result", "[", "\"start_logits\"", "]", ".", "flat", "]", "\n", "end_logits", "=", "[", "float", "(", "x", ")", "for", "x", "in", "result", "[", "\"end_logits\"", "]", ".", "flat", "]", "\n", "all_results", ".", "append", "(", "\n", "RawResult", "(", "\n", "unique_id", "=", "unique_id", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ")", ")", "\n", "\n", "", "output_prediction_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"predictions.json\"", ")", "\n", "output_nbest_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"nbest_predictions.json\"", ")", "\n", "output_null_log_odds_file", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_dir", ",", "\"null_odds.json\"", ")", "\n", "\n", "write_predictions", "(", "eval_examples", ",", "eval_features", ",", "all_results", ",", "\n", "FLAGS", ".", "n_best_size", ",", "FLAGS", ".", "max_answer_length", ",", "\n", "FLAGS", ".", "do_lower_case", ",", "output_prediction_file", ",", "\n", "output_nbest_file", ",", "output_null_log_odds_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.__init__": [[18, 28], ["bert.modeling.BertConfig.from_json_file"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.from_json_file"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "dropout", "=", "None", "\n", "self", ".", "pad_idx", "=", "0", "\n", "self", ".", "mention_start_idx", "=", "37", "\n", "self", ".", "mention_end_idx", "=", "42", "\n", "self", ".", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "config", ".", "bert_config_file", ")", "\n", "self", ".", "bert_config", ".", "hidden_dropout_prob", "=", "config", ".", "dropout_rate", "\n", "self", ".", "cls_in_vocab", "=", "101", "\n", "self", ".", "sep_in_vocab", "=", "102", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_coreference_resolution_and_loss": [[30, 372], ["corefqa.CorefQAModel.get_dropout", "tensorflow.math.maximum", "tensorflow.where", "tensorflow.math.maximum", "tensorflow.cast", "corefqa.CorefQAModel.boolean_mask_1d", "corefqa.CorefQAModel.boolean_mask_1d", "tensorflow.cast", "corefqa.CorefQAModel.boolean_mask_1d", "tensorflow.math.maximum", "tensorflow.math.reduce_sum", "tensorflow.reshape", "tensorflow.ones_like", "bert.modeling.BertModel", "bert.modeling.BertModel.get_sequence_output", "tensorflow.reshape", "corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "tensorflow.reshape", "tensorflow.tile", "tensorflow.math.add", "tensorflow.gather", "tensorflow.gather", "tensorflow.logical_and", "tensorflow.reshape", "corefqa.CorefQAModel.boolean_mask_1d", "corefqa.CorefQAModel.boolean_mask_1d", "corefqa.CorefQAModel.get_candidate_cluster_labels", "corefqa.CorefQAModel.get_candidate_span_embedding", "corefqa.CorefQAModel.get_candidate_mention_gold_sequence_label", "corefqa.CorefQAModel.get_mention_score_and_loss", "tensorflow.minimum", "tensorflow.reshape", "tensorflow.nn.top_k", "tensorflow.reshape", "tensorflow.gather", "tensorflow.gather", "tensorflow.gather", "tensorflow.gather", "tensorflow.constant", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.reshape", "corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "tensorflow.reshape", "tensorflow.while_loop", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "bert.modeling.BertModel", "bert.modeling.BertModel.get_sequence_output", "corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "tensorflow.reshape", "tensorflow.tile", "tensorflow.reshape", "corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "tensorflow.reshape", "corefqa.CorefQAModel.get_shape", "tensorflow.cast", "corefqa.CorefQAModel.get_candidate_span_embedding", "tensorflow.to_int32", "corefqa.CorefQAModel.get_mention_score_and_loss", "tensorflow.reshape", "tensorflow.nn.top_k", "tensorflow.reshape", "tensorflow.gather", "tensorflow.gather", "tensorflow.gather", "tensorflow.gather", "tensorflow.constant", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "tensorflow.while_loop", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "bert.modeling.BertModel", "bert.modeling.BertModel.get_sequence_output", "tensorflow.reshape", "tensorflow.reshape", "corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "corefqa.CorefQAModel.get_candidate_span_embedding", "corefqa.CorefQAModel.get_mention_score_and_loss", "tensorflow.reshape", "tensorflow.tile", "tensorflow.tile", "tensorflow.tile", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.zeros", "tensorflow.concat", "tensorflow.reshape", "tensorflow.equal", "tensorflow.expand_dims", "tensorflow.logical_and", "tensorflow.logical_not", "tensorflow.concat", "corefqa.CorefQAModel.marginal_likelihood_loss", "tensorflow.zeros_like", "tensorflow.math.greater_equal", "tensorflow.zeros_like", "tensorflow.math.greater_equal", "tensorflow.math.greater_equal", "tensorflow.zeros_like", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.equal", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.to_int32", "tensorflow.gather", "tensorflow.gather", "corefqa.CorefQAModel.get_query_token_ids", "tensorflow.zeros", "tensorflow.concat", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.tile", "tensorflow.tile", "tensorflow.tile", "tensorflow.cast", "tensorflow.cast", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.tile", "tensorflow.tile", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.minimum", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.constant", "tensorflow.constant", "tensorflow.gather", "tensorflow.gather", "tensorflow.floor_div", "tensorflow.gather", "tensorflow.gather", "corefqa.CorefQAModel.get_query_token_ids", "corefqa.CorefQAModel.get_query_token_ids", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.concat", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.concat", "tensorflow.ones_like", "tensorflow.ones_like", "tensorflow.cast", "tensorflow.cast", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.reduce_any", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.range", "tensorflow.range", "tensorflow.floor", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.fill", "tensorflow.fill", "tensorflow.math.add", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.fill", "tensorflow.fill", "tensorflow.math.add", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.range", "tensorflow.range", "tensorflow.ones_like", "tensorflow.ones_like", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.ones_like", "tensorflow.constant.get_shape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.constant.get_shape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.to_float", "corefqa.CorefQAModel.get_shape", "tensorflow.range", "corefqa.CorefQAModel.get_shape", "corefqa.CorefQAModel.get_shape"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_dropout", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_candidate_cluster_labels", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_candidate_span_embedding", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_candidate_mention_gold_sequence_label", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_mention_score_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_candidate_span_embedding", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_mention_score_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_candidate_span_embedding", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_mention_score_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.marginal_likelihood_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_query_token_ids", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_query_token_ids", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_query_token_ids", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape"], ["", "def", "get_coreference_resolution_and_loss", "(", "self", ",", "instance", ",", "is_training", ",", "use_tpu", "=", "False", ")", ":", "\n", "\n", "\n", "        ", "self", ".", "use_tpu", "=", "use_tpu", "\n", "self", ".", "dropout", "=", "self", ".", "get_dropout", "(", "self", ".", "config", ".", "dropout_rate", ",", "is_training", ")", "\n", "\n", "flat_window_input_ids", ",", "flat_window_input_mask", ",", "flat_doc_sentence_map", ",", "window_text_len", ",", "speaker_ids", ",", "gold_starts", ",", "gold_ends", ",", "gold_cluster_ids", "=", "instance", "\n", "# flat_input_ids: (num_window, window_size)", "\n", "# flat_doc_overlap_input_mask: (num_window, window_size)", "\n", "# flat_sentence_map: (num_window, window_size)", "\n", "# text_len: dynamic length and is padded to fix length", "\n", "# gold_start: (max_num_mention), mention start index in the original (NON-OVERLAP) document. Pad with -1 to the fix length max_num_mention.", "\n", "# gold_end: (max_num_mention), mention end index in the original (NON-OVERLAP) document. Pad with -1 to the fix length max_num_mention.", "\n", "# cluster_ids/speaker_ids is not used in the mention proposal model.", "\n", "\n", "flat_window_input_ids", "=", "tf", ".", "math", ".", "maximum", "(", "flat_window_input_ids", ",", "tf", ".", "zeros_like", "(", "flat_window_input_ids", ",", "tf", ".", "int32", ")", ")", "# (num_window * window_size)", "\n", "\n", "flat_doc_overlap_input_mask", "=", "tf", ".", "where", "(", "tf", ".", "math", ".", "greater_equal", "(", "flat_window_input_mask", ",", "0", ")", ",", "\n", "x", "=", "tf", ".", "ones_like", "(", "flat_window_input_mask", ",", "tf", ".", "int32", ")", ",", "y", "=", "tf", ".", "zeros_like", "(", "flat_window_input_mask", ",", "tf", ".", "int32", ")", ")", "# (num_window * window_size)", "\n", "# flat_doc_overlap_input_mask = tf.math.maximum(flat_doc_overlap_input_mask, tf.zeros_like(flat_doc_overlap_input_mask, tf.int32))", "\n", "flat_doc_sentence_map", "=", "tf", ".", "math", ".", "maximum", "(", "flat_doc_sentence_map", ",", "tf", ".", "zeros_like", "(", "flat_doc_sentence_map", ",", "tf", ".", "int32", ")", ")", "# (num_window * window_size)", "\n", "\n", "gold_start_end_mask", "=", "tf", ".", "cast", "(", "tf", ".", "math", ".", "greater_equal", "(", "gold_starts", ",", "tf", ".", "zeros_like", "(", "gold_starts", ",", "tf", ".", "int32", ")", ")", ",", "tf", ".", "bool", ")", "# (max_num_mention)", "\n", "gold_start_index_labels", "=", "self", ".", "boolean_mask_1d", "(", "gold_starts", ",", "gold_start_end_mask", ",", "name_scope", "=", "\"gold_starts\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "# (num_of_mention)", "\n", "gold_end_index_labels", "=", "self", ".", "boolean_mask_1d", "(", "gold_ends", ",", "gold_start_end_mask", ",", "name_scope", "=", "\"gold_ends\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "# (num_of_mention)", "\n", "\n", "gold_cluster_mask", "=", "tf", ".", "cast", "(", "tf", ".", "math", ".", "greater_equal", "(", "gold_cluster_ids", ",", "tf", ".", "zeros_like", "(", "gold_cluster_ids", ",", "tf", ".", "int32", ")", ")", ",", "tf", ".", "bool", ")", "# (max_num_cluster)", "\n", "gold_cluster_ids", "=", "self", ".", "boolean_mask_1d", "(", "gold_cluster_ids", ",", "gold_cluster_mask", ",", "name_scope", "=", "\"gold_cluster\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "\n", "\n", "window_text_len", "=", "tf", ".", "math", ".", "maximum", "(", "window_text_len", ",", "tf", ".", "zeros_like", "(", "window_text_len", ",", "tf", ".", "int32", ")", ")", "# (num_of_non_empty_window)", "\n", "num_subtoken_in_doc", "=", "tf", ".", "math", ".", "reduce_sum", "(", "window_text_len", ")", "# the value should be num_subtoken_in_doc ", "\n", "####################", "\n", "####################", "\n", "## mention proposal stage starts ", "\n", "mention_input_ids", "=", "tf", ".", "reshape", "(", "flat_window_input_ids", ",", "[", "-", "1", ",", "self", ".", "config", ".", "window_size", "]", ")", "# (num_window, window_size)", "\n", "# each row of mention_input_ids is a subdocument ", "\n", "mention_input_mask", "=", "tf", ".", "ones_like", "(", "mention_input_ids", ",", "tf", ".", "int32", ")", "# (num_window, window_size)", "\n", "mention_model", "=", "modeling", ".", "BertModel", "(", "config", "=", "self", ".", "bert_config", ",", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "mention_input_ids", ",", "input_mask", "=", "mention_input_mask", ",", "use_one_hot_embeddings", "=", "False", ",", "scope", "=", "'bert'", ")", "\n", "\n", "mention_doc_overlap_window_embs", "=", "mention_model", ".", "get_sequence_output", "(", ")", "# (num_window, window_size, hidden_size)", "\n", "# get BERT embeddings for mention_input_ids ", "\n", "doc_overlap_input_mask", "=", "tf", ".", "reshape", "(", "flat_doc_overlap_input_mask", ",", "[", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "window_size", "]", ")", "# (num_window, window_size)", "\n", "\n", "mention_doc_flat_embs", "=", "self", ".", "transform_overlap_sliding_windows_to_original_document", "(", "mention_doc_overlap_window_embs", ",", "doc_overlap_input_mask", ")", "\n", "mention_doc_flat_embs", "=", "tf", ".", "reshape", "(", "mention_doc_flat_embs", ",", "[", "-", "1", ",", "self", ".", "config", ".", "hidden_size", "]", ")", "# (num_subtoken_in_doc, hidden_size) ", "\n", "\n", "candidate_mention_starts", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "num_subtoken_in_doc", ")", ",", "1", ")", ",", "[", "1", ",", "self", ".", "config", ".", "max_span_width", "]", ")", "# (num_subtoken_in_doc, max_span_width)", "\n", "# getting all eligible mentions in each subdocument", "\n", "# the number if eligible mentions of each subdocument is  config.max_span_width * num_subtoken_in_doc", "\n", "candidate_mention_ends", "=", "tf", ".", "math", ".", "add", "(", "candidate_mention_starts", ",", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "self", ".", "config", ".", "max_span_width", ")", ",", "0", ")", ")", "# (num_subtoken_in_doc, max_span_width)", "\n", "\n", "candidate_mention_sentence_start_idx", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "flat_doc_sentence_map", ",", "[", "-", "1", "]", ")", ",", "candidate_mention_starts", ")", "# (num_subtoken_in_doc, max_span_width)", "\n", "candidate_mention_sentence_end_idx", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "flat_doc_sentence_map", ",", "[", "-", "1", "]", ")", ",", "candidate_mention_ends", ")", "# (num_subtoken_in_doc, max_span_width)", "\n", "\n", "candidate_mention_mask", "=", "tf", ".", "logical_and", "(", "candidate_mention_ends", "<", "num_subtoken_in_doc", ",", "tf", ".", "equal", "(", "candidate_mention_sentence_start_idx", ",", "candidate_mention_sentence_end_idx", ")", ")", "\n", "candidate_mention_mask", "=", "tf", ".", "reshape", "(", "candidate_mention_mask", ",", "[", "-", "1", "]", ")", "\n", "\n", "candidate_mention_starts", "=", "self", ".", "boolean_mask_1d", "(", "tf", ".", "reshape", "(", "candidate_mention_starts", ",", "[", "-", "1", "]", ")", ",", "candidate_mention_mask", ",", "name_scope", "=", "\"candidate_mention_starts\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "\n", "candidate_mention_ends", "=", "self", ".", "boolean_mask_1d", "(", "tf", ".", "reshape", "(", "candidate_mention_ends", ",", "[", "-", "1", "]", ")", ",", "candidate_mention_mask", ",", "name_scope", "=", "\"candidate_mention_ends\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "\n", "# num_candidate_mention_in_doc is smaller than num_subtoken_in_doc", "\n", "\n", "candidate_cluster_idx_labels", "=", "self", ".", "get_candidate_cluster_labels", "(", "candidate_mention_starts", ",", "candidate_mention_ends", ",", "gold_start_index_labels", ",", "gold_end_index_labels", ",", "gold_cluster_ids", ")", "\n", "\n", "candidate_mention_span_embs", ",", "candidate_mention_start_embs", ",", "candidate_mention_end_embs", "=", "self", ".", "get_candidate_span_embedding", "(", "\n", "mention_doc_flat_embs", ",", "candidate_mention_starts", ",", "candidate_mention_ends", ")", "\n", "# candidate_mention_span_embs -> (num_candidate_mention_in_doc, 2 * hidden_size)", "\n", "# candidate_mention_start_embs -> (num_candidate_mention_in_doc, hidden_size)", "\n", "# candidate_mention_end_embs -> (num_candidate_mention_in_doc, hidden_size)", "\n", "\n", "gold_label_candidate_mention_spans", ",", "gold_label_candidate_mention_starts", ",", "gold_label_candidate_mention_ends", "=", "self", ".", "get_candidate_mention_gold_sequence_label", "(", "\n", "candidate_mention_starts", ",", "candidate_mention_ends", ",", "gold_start_index_labels", ",", "gold_end_index_labels", ",", "num_subtoken_in_doc", ")", "\n", "# gold_label_candidate_mention_spans -> (num_candidate_mention_in_doc)", "\n", "# gold_label_candidate_mention_starts -> (num_candidate_mention_in_doc)", "\n", "# gold_label_candidate_mention_ends -> (num_candidate_mention_in_doc)", "\n", "\n", "mention_proposal_loss", ",", "candidate_mention_start_prob", ",", "candidate_mention_end_prob", ",", "candidate_mention_span_prob", ",", "candidate_mention_span_scores", "=", "self", ".", "get_mention_score_and_loss", "(", "\n", "candidate_mention_span_embs", ",", "candidate_mention_start_embs", ",", "candidate_mention_end_embs", ",", "gold_label_candidate_mention_spans", "=", "gold_label_candidate_mention_spans", ",", "\n", "gold_label_candidate_mention_starts", "=", "gold_label_candidate_mention_starts", ",", "gold_label_candidate_mention_ends", "=", "gold_label_candidate_mention_ends", ",", "expect_length_of_labels", "=", "num_subtoken_in_doc", ")", "\n", "# mention_proposal_loss -> a scalar ", "\n", "# candidate_mention_start_prob, candidate_mention_end_prob, candidate_mention_span_prob, -> (num_candidate_mention_in_doc)", "\n", "\n", "self", ".", "k", "=", "tf", ".", "minimum", "(", "self", ".", "config", ".", "max_candidate_mentions", ",", "tf", ".", "to_int32", "(", "tf", ".", "floor", "(", "tf", ".", "to_float", "(", "num_subtoken_in_doc", ")", "*", "self", ".", "config", ".", "top_span_ratio", ")", ")", ")", "\n", "# self.k is a hyper-parameter. We want to select the top self.k mentions from the config.max_span_width * num_subtoken_in_doc mentions.", "\n", "\n", "candidate_mention_span_scores", "=", "tf", ".", "reshape", "(", "candidate_mention_span_scores", ",", "[", "-", "1", "]", ")", "\n", "topk_mention_span_scores", ",", "topk_mention_span_indices", "=", "tf", ".", "nn", ".", "top_k", "(", "candidate_mention_span_scores", ",", "self", ".", "k", ",", "sorted", "=", "False", ")", "\n", "topk_mention_span_indices", "=", "tf", ".", "reshape", "(", "topk_mention_span_indices", ",", "[", "-", "1", "]", ")", "\n", "# topk_mention_span_scores -> (k,)", "\n", "# topk_mention_span_indices -> (k,)", "\n", "\n", "topk_mention_start_indices", "=", "tf", ".", "gather", "(", "candidate_mention_starts", ",", "topk_mention_span_indices", ")", "# (k,)", "\n", "topk_mention_end_indices", "=", "tf", ".", "gather", "(", "candidate_mention_ends", ",", "topk_mention_span_indices", ")", "# (k,)", "\n", "topk_mention_span_cluster_ids", "=", "tf", ".", "gather", "(", "candidate_cluster_idx_labels", ",", "topk_mention_span_indices", ")", "# (k,)", "\n", "topk_mention_span_scores", "=", "tf", ".", "gather", "(", "candidate_mention_span_scores", ",", "topk_mention_span_indices", ")", "# (k,)", "\n", "## mention proposal stage ends", "\n", "###########", "\n", "###########", "\n", "\n", "\n", "###### mention linking stage starts", "\n", "## foward QA score computation starts", "\n", "## for a given proposed mention i, we first compute the score of a span j being the correferent answer to i, denoted by s(j|i) ", "\n", "i0", "=", "tf", ".", "constant", "(", "0", ")", "\n", "forward_qa_input_ids", "=", "tf", ".", "zeros", "(", "(", "1", ",", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "window_size", "+", "self", ".", "config", ".", "max_query_len", "+", "2", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "# (1, num_window, max_query_len + window_size + 2)", "\n", "forward_qa_input_mask", "=", "tf", ".", "zeros", "(", "(", "1", ",", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "window_size", "+", "self", ".", "config", ".", "max_query_len", "+", "2", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "# (1, num_window, max_query_len + window_size + 2)", "\n", "forward_qa_input_token_type_mask", "=", "tf", ".", "zeros", "(", "(", "1", ",", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "window_size", "+", "self", ".", "config", ".", "max_query_len", "+", "2", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "# (1, num_window, max_query_len + window_size + 2)", "\n", "\n", "# prepare for non-overlap input token ids ", "\n", "doc_overlap_input_mask", "=", "tf", ".", "reshape", "(", "flat_doc_overlap_input_mask", ",", "[", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "window_size", "]", ")", "\n", "nonoverlap_doc_input_ids", "=", "self", ".", "transform_overlap_sliding_windows_to_original_document", "(", "flat_window_input_ids", ",", "doc_overlap_input_mask", ")", "# (num_subtoken_in_doc)", "\n", "overlap_window_input_ids", "=", "tf", ".", "reshape", "(", "flat_window_input_ids", ",", "[", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "window_size", "]", ")", "# (num_window, window_size)", "\n", "\n", "@", "tf", ".", "function", "\n", "def", "forward_qa_mention_linking", "(", "i", ",", "batch_qa_input_ids", ",", "batch_qa_input_mask", ",", "batch_qa_input_token_type_mask", ")", ":", "\n", "            ", "tmp_mention_start_idx", "=", "tf", ".", "gather", "(", "topk_mention_start_indices", ",", "i", ")", "\n", "tmp_mention_end_idx", "=", "tf", ".", "gather", "(", "topk_mention_end_indices", ",", "i", ")", "\n", "\n", "query_input_token_ids", ",", "mention_start_idx_in_sent", ",", "mention_end_idx_in_sent", "=", "self", ".", "get_query_token_ids", "(", "\n", "nonoverlap_doc_input_ids", ",", "flat_doc_sentence_map", ",", "tmp_mention_start_idx", ",", "tmp_mention_end_idx", ")", "\n", "\n", "query_pad_token_ids", "=", "tf", ".", "zeros", "(", "[", "self", ".", "config", ".", "max_query_len", "-", "self", ".", "get_shape", "(", "query_input_token_ids", ",", "0", ")", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "pad_query_input_token_ids", "=", "tf", ".", "concat", "(", "[", "query_input_token_ids", ",", "query_pad_token_ids", "]", ",", "axis", "=", "0", ")", "# (max_query_len,)", "\n", "pad_query_input_token_mask", "=", "tf", ".", "ones_like", "(", "pad_query_input_token_ids", ",", "tf", ".", "int32", ")", "# (max_query_len)", "\n", "pad_query_input_token_type_mask", "=", "tf", ".", "zeros_like", "(", "pad_query_input_token_ids", ",", "tf", ".", "int32", ")", "# (max_query_len)", "\n", "\n", "\n", "expand_pad_query_input_token_ids", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "pad_query_input_token_ids", ",", "0", ")", ",", "[", "self", ".", "config", ".", "num_window", ",", "1", "]", ")", "# (num_window, max_query_len)", "\n", "expand_pad_query_input_token_mask", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "pad_query_input_token_mask", ",", "0", ")", ",", "[", "self", ".", "config", ".", "num_window", ",", "1", "]", ")", "# (num_window, max_query_len)", "\n", "expand_pad_query_input_token_type_mask", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "pad_query_input_token_type_mask", ",", "0", ")", ",", "[", "self", ".", "config", ".", "num_window", ",", "1", "]", ")", "# (num_window, max_query_len)", "\n", "\n", "sep_tokens", "=", "tf", ".", "cast", "(", "tf", ".", "fill", "(", "[", "self", ".", "config", ".", "num_window", ",", "1", "]", ",", "self", ".", "sep_in_vocab", ")", ",", "tf", ".", "int32", ")", "# (num_window, 1)", "\n", "cls_tokens", "=", "tf", ".", "cast", "(", "tf", ".", "fill", "(", "[", "self", ".", "config", ".", "num_window", ",", "1", "]", ",", "self", ".", "cls_in_vocab", ")", ",", "tf", ".", "int32", ")", "# (num_window, 1)", "\n", "\n", "query_context_input_token_ids", "=", "tf", ".", "concat", "(", "[", "cls_tokens", ",", "expand_pad_query_input_token_ids", ",", "sep_tokens", ",", "overlap_window_input_ids", "]", ",", "axis", "=", "1", ")", "# (1, num_window, max_query_len + window_size + 2)", "\n", "query_context_input_token_mask", "=", "tf", ".", "concat", "(", "[", "tf", ".", "ones_like", "(", "cls_tokens", ",", "tf", ".", "int32", ")", ",", "expand_pad_query_input_token_mask", ",", "tf", ".", "ones_like", "(", "sep_tokens", ",", "tf", ".", "int32", ")", ",", "tf", ".", "ones_like", "(", "overlap_window_input_ids", ",", "tf", ".", "int32", ")", "]", ",", "axis", "=", "1", ")", "# (1, num_window, max_query_len + window_size + 2)", "\n", "query_context_input_token_type_mask", "=", "tf", ".", "concat", "(", "[", "tf", ".", "zeros_like", "(", "cls_tokens", ",", "tf", ".", "int32", ")", ",", "expand_pad_query_input_token_type_mask", ",", "tf", ".", "zeros_like", "(", "sep_tokens", ",", "tf", ".", "int32", ")", ",", "tf", ".", "ones_like", "(", "overlap_window_input_ids", ",", "tf", ".", "int32", ")", "]", ",", "axis", "=", "1", ")", "# (1, num_window, max_query_len + window_size + 2)", "\n", "\n", "query_context_input_token_ids", "=", "tf", ".", "reshape", "(", "query_context_input_token_ids", ",", "[", "1", ",", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "window_size", "+", "2", "]", ")", "\n", "query_context_input_token_mask", "=", "tf", ".", "reshape", "(", "query_context_input_token_mask", ",", "[", "1", ",", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "window_size", "+", "2", "]", ")", "\n", "query_context_input_token_type_mask", "=", "tf", ".", "reshape", "(", "query_context_input_token_type_mask", ",", "[", "1", ",", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "window_size", "+", "2", "]", ")", "\n", "\n", "\n", "return", "[", "tf", ".", "math", ".", "add", "(", "i", ",", "1", ")", ",", "tf", ".", "concat", "(", "[", "batch_qa_input_ids", ",", "query_context_input_token_ids", "]", ",", "0", ")", ",", "\n", "tf", ".", "concat", "(", "[", "batch_qa_input_mask", ",", "query_context_input_token_mask", "]", ",", "0", ")", ",", "\n", "tf", ".", "concat", "(", "[", "batch_qa_input_token_type_mask", ",", "query_context_input_token_type_mask", "]", ",", "0", ")", "]", "\n", "\n", "\n", "\n", "", "_", ",", "stack_forward_qa_input_ids", ",", "stack_forward_qa_input_mask", ",", "stack_forward_qa_input_type_mask", "=", "tf", ".", "while_loop", "(", "\n", "cond", "=", "lambda", "i", ",", "o1", ",", "o2", ",", "o3", ":", "i", "<", "self", ".", "k", ",", "\n", "body", "=", "forward_qa_mention_linking", ",", "\n", "loop_vars", "=", "[", "i0", ",", "forward_qa_input_ids", ",", "forward_qa_input_mask", ",", "forward_qa_input_token_type_mask", "]", ",", "\n", "shape_invariants", "=", "[", "i0", ".", "get_shape", "(", ")", ",", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", ",", "None", "]", ")", ",", "\n", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", ",", "None", "]", ")", ",", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", ",", "None", "]", ")", "]", ")", "\n", "\n", "# stack_forward_qa_input_ids, stack_forward_qa_input_mask, stack_forward_qa_input_type_mask -> (k, num_window, max_query_len + window_size + 2)", "\n", "\n", "batch_forward_qa_input_ids", "=", "tf", ".", "reshape", "(", "stack_forward_qa_input_ids", ",", "[", "-", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "window_size", "+", "2", "]", ")", "# (k * num_window, max_query_len + window_size + 2)", "\n", "batch_forward_qa_input_mask", "=", "tf", ".", "reshape", "(", "stack_forward_qa_input_mask", ",", "[", "-", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "window_size", "+", "2", "]", ")", "# (k * num_window, max_query_len + window_size + 2)", "\n", "batch_forward_qa_input_type_mask", "=", "tf", ".", "reshape", "(", "stack_forward_qa_input_type_mask", ",", "[", "-", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "window_size", "+", "2", "]", ")", "# (k * num_window, max_query_len + window_size + 2)", "\n", "\n", "forward_qa_linking_model", "=", "modeling", ".", "BertModel", "(", "config", "=", "self", ".", "bert_config", ",", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "batch_forward_qa_input_ids", ",", "input_mask", "=", "batch_forward_qa_input_mask", ",", "\n", "token_type_ids", "=", "batch_forward_qa_input_type_mask", ",", "use_one_hot_embeddings", "=", "False", ",", "\n", "scope", "=", "\"bert\"", ")", "\n", "\n", "forward_qa_overlap_window_embs", "=", "forward_qa_linking_model", ".", "get_sequence_output", "(", ")", "# (k * num_window, max_query_len + window_size + 2, hidden_size)", "\n", "forward_context_overlap_window_embs", "=", "self", ".", "transform_overlap_sliding_windows_to_original_document", "(", "forward_qa_overlap_window_embs", ",", "batch_forward_qa_input_type_mask", ")", "\n", "forward_context_overlap_window_embs", "=", "tf", ".", "reshape", "(", "forward_context_overlap_window_embs", ",", "[", "self", ".", "k", "*", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "window_size", "]", ")", "\n", "# forward_context_overlap_window_embs -> (k*num_window, window_size, hidden_size)", "\n", "\n", "expand_doc_overlap_input_mask", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "doc_overlap_input_mask", ",", "0", ")", ",", "[", "self", ".", "k", ",", "1", ",", "1", "]", ")", "# (k, num_window, window_size)", "\n", "expand_doc_overlap_input_mask", "=", "tf", ".", "reshape", "(", "expand_doc_overlap_input_mask", ",", "[", "-", "1", ",", "self", ".", "config", ".", "window_size", "]", ")", "# (k * num_window, window_size)", "\n", "\n", "forward_context_flat_doc_embs", "=", "self", ".", "transform_overlap_sliding_windows_to_original_document", "(", "forward_context_overlap_window_embs", ",", "expand_doc_overlap_input_mask", ")", "# (k * num_subtoken_in_doc, hidden_size)", "\n", "forward_context_flat_doc_embs", "=", "tf", ".", "reshape", "(", "forward_context_flat_doc_embs", ",", "[", "self", ".", "k", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", "]", ")", "# (k, num_subtoken_in_doc, hidden_size)", "\n", "num_candidate_mention", "=", "self", ".", "get_shape", "(", "candidate_mention_span_embs", ",", "0", ")", "# (num_candidate_mention_in_doc)", "\n", "forward_qa_mention_pos_offset", "=", "tf", ".", "cast", "(", "tf", ".", "tile", "(", "tf", ".", "reshape", "(", "tf", ".", "range", "(", "0", ",", "num_candidate_mention", ")", "*", "num_subtoken_in_doc", ",", "[", "1", ",", "-", "1", "]", ")", ",", "[", "self", ".", "k", ",", "1", "]", ")", ",", "tf", ".", "int32", ")", "# (k, num_candidate_mention_in_doc)", "\n", "\n", "forward_qa_mention_starts", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "candidate_mention_starts", ",", "0", ")", ",", "[", "self", ".", "k", ",", "1", "]", ")", "+", "forward_qa_mention_pos_offset", "# (k, num_candidate_mention_in_doc)", "\n", "forward_qa_mention_ends", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "candidate_mention_ends", ",", "0", ")", ",", "[", "self", ".", "k", ",", "1", "]", ")", "+", "forward_qa_mention_pos_offset", "# (k, num_candidate_mention_in_doc)", "\n", "\n", "forward_qa_mention_span_embs", ",", "forward_qa_mention_start_embs", ",", "forward_qa_mention_end_embs", "=", "self", ".", "get_candidate_span_embedding", "(", "tf", ".", "reshape", "(", "forward_context_flat_doc_embs", ",", "\n", "[", "-", "1", ",", "self", ".", "config", ".", "hidden_size", "]", ")", ",", "tf", ".", "reshape", "(", "forward_qa_mention_starts", ",", "[", "-", "1", "]", ")", ",", "tf", ".", "reshape", "(", "forward_qa_mention_ends", ",", "[", "-", "1", "]", ")", ")", "\n", "# forward_qa_mention_span_embs -> (k * num_candidate_mention_in_doc, hidden_size*2)", "\n", "# forward_qa_mention_start_embs -> (k * num_candidate_mention_in_doc, hidden_size)", "\n", "\n", "self", ".", "c", "=", "tf", ".", "to_int32", "(", "tf", ".", "minimum", "(", "self", ".", "config", ".", "max_top_antecedents", ",", "self", ".", "k", ")", ")", "\n", "\n", "forward_qa_mention_span_scores", ",", "forward_qa_mention_start_scores", ",", "forward_qa_mention_end_scores", "=", "self", ".", "get_mention_score_and_loss", "(", "forward_qa_mention_span_embs", ",", "\n", "forward_qa_mention_start_embs", ",", "forward_qa_mention_end_embs", ",", "name_scope", "=", "\"forward_qa\"", ")", "\n", "# forward_qa_mention_span_prob, forward_qa_mention_start_prob, forward_qa_mention_end_prob -> (k * num_candidate_mention_in_doc)", "\n", "\n", "# computes the s(j|i) for all eligible span j in the document ", "\n", "if", "self", ".", "config", ".", "sec_qa_mention_score", ":", "\n", "            ", "forward_qa_mention_span_scores", "=", "(", "forward_qa_mention_span_scores", "+", "forward_qa_mention_start_scores", "+", "forward_qa_mention_end_scores", ")", "/", "3.0", "\n", "", "else", ":", "\n", "            ", "forward_qa_mention_span_scores", "=", "forward_qa_mention_span_scores", "\n", "\n", "", "forward_candidate_mention_span_scores", "=", "tf", ".", "reshape", "(", "forward_qa_mention_span_scores", ",", "[", "self", ".", "k", ",", "-", "1", "]", ")", "# (k, num_candidate_mention_in_doc)", "\n", "forward_topc_mention_span_scores", ",", "local_forward_topc_mention_span_indices", "=", "tf", ".", "nn", ".", "top_k", "(", "forward_candidate_mention_span_scores", ",", "self", ".", "c", ",", "sorted", "=", "False", ")", "# (k, c)", "\n", "# for each i, we only maintain the top self.c spans based on s(j|i)", "\n", "local_flat_forward_topc_mention_span_indices", "=", "tf", ".", "reshape", "(", "local_forward_topc_mention_span_indices", ",", "[", "-", "1", "]", ")", "# (k * c)", "\n", "\n", "# topk_mention_start_indices", "\n", "forward_topc_mention_start_indices", "=", "tf", ".", "gather", "(", "candidate_mention_starts", ",", "local_flat_forward_topc_mention_span_indices", ")", "# (k, c)", "\n", "forward_topc_mention_end_indices", "=", "tf", ".", "gather", "(", "candidate_mention_ends", ",", "local_flat_forward_topc_mention_span_indices", ")", "# (k, c)", "\n", "forward_topc_mention_span_scores_in_mention_proposal", "=", "tf", ".", "gather", "(", "candidate_mention_span_scores", ",", "local_flat_forward_topc_mention_span_indices", ")", "# (k, c)", "\n", "forward_topc_span_cluster_ids", "=", "tf", ".", "gather", "(", "candidate_cluster_idx_labels", ",", "local_flat_forward_topc_mention_span_indices", ")", "\n", "## foward QA score computation ends", "\n", "\n", "\n", "## backward QA score computation begins", "\n", "## we need to compute the score of backward score, i.e., the span i is the correferent answer for j, denoted by s(i|j)", "\n", "i0", "=", "tf", ".", "constant", "(", "0", ")", "\n", "backward_qa_input_ids", "=", "tf", ".", "reshape", "(", "tf", ".", "zeros", "(", "(", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", "+", "2", ")", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "[", "1", ",", "-", "1", "]", ")", "# (1, max_query_len + max_context_len + 2)", "\n", "backward_qa_input_mask", "=", "tf", ".", "reshape", "(", "tf", ".", "zeros", "(", "(", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", "+", "2", ")", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "[", "1", ",", "-", "1", "]", ")", "# (1, max_query_len + max_context_len + 2)", "\n", "backward_qa_input_token_type_mask", "=", "tf", ".", "reshape", "(", "tf", ".", "zeros", "(", "(", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", "+", "2", ")", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "[", "1", ",", "-", "1", "]", ")", "# (1, max_query_len + max_context_len + 2)", "\n", "backward_qa_mention_start_in_context", "=", "tf", ".", "convert_to_tensor", "(", "tf", ".", "constant", "(", "[", "0", "]", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "backward_qa_mention_end_in_context", "=", "tf", ".", "convert_to_tensor", "(", "tf", ".", "constant", "(", "[", "0", "]", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "\n", "@", "tf", ".", "function", "\n", "def", "backward_qa_mention_linking", "(", "i", ",", "batch_qa_input_ids", ",", "batch_qa_input_mask", ",", "batch_qa_input_token_type_mask", ",", "\n", "batch_qa_mention_start_in_context", ",", "batch_qa_mention_end_in_context", ")", ":", "\n", "\n", "            ", "tmp_query_mention_start_idx", "=", "tf", ".", "gather", "(", "forward_topc_mention_start_indices", ",", "i", ")", "\n", "tmp_query_mention_end_idx", "=", "tf", ".", "gather", "(", "forward_topc_mention_end_indices", ",", "i", ")", "\n", "\n", "tmp_index_for_topk_mention", "=", "tf", ".", "floor_div", "(", "i", ",", "self", ".", "k", ")", "\n", "tmp_context_mention_start_idx", "=", "tf", ".", "gather", "(", "topk_mention_start_indices", ",", "tmp_index_for_topk_mention", ")", "\n", "tmp_context_mention_end_idx", "=", "tf", ".", "gather", "(", "topk_mention_end_indices", ",", "tmp_index_for_topk_mention", ")", "\n", "\n", "query_input_token_ids", ",", "mention_start_idx_in_query", ",", "mention_end_idx_in_query", "=", "self", ".", "get_query_token_ids", "(", "\n", "nonoverlap_doc_input_ids", ",", "flat_doc_sentence_map", ",", "tmp_query_mention_start_idx", ",", "tmp_query_mention_end_idx", ")", "\n", "\n", "context_input_token_ids", ",", "mention_start_idx_in_context", ",", "mention_end_idx_in_context", "=", "self", ".", "get_query_token_ids", "(", "\n", "nonoverlap_doc_input_ids", ",", "flat_doc_sentence_map", ",", "tmp_context_mention_start_idx", ",", "tmp_context_mention_end_idx", ")", "\n", "\n", "query_pad_token_ids", "=", "tf", ".", "zeros", "(", "[", "self", ".", "config", ".", "max_query_len", "-", "self", ".", "get_shape", "(", "query_input_token_ids", ",", "0", ")", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "context_pad_token_ids", "=", "tf", ".", "zeros", "(", "[", "self", ".", "config", ".", "max_context_len", "-", "self", ".", "get_shape", "(", "context_input_token_ids", ",", "0", ")", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "pad_query_input_token_ids", "=", "tf", ".", "concat", "(", "[", "query_input_token_ids", ",", "query_pad_token_ids", "]", ",", "axis", "=", "0", ")", "# (max_query_len)", "\n", "pad_query_input_token_mask", "=", "tf", ".", "ones_like", "(", "pad_query_input_token_ids", ",", "tf", ".", "int32", ")", "\n", "pad_query_input_token_type_mask", "=", "tf", ".", "zeros_like", "(", "pad_query_input_token_ids", ",", "tf", ".", "int32", ")", "\n", "\n", "pad_context_input_token_ids", "=", "tf", ".", "concat", "(", "[", "context_input_token_ids", ",", "context_pad_token_ids", "]", ",", "axis", "=", "0", ")", "# (max_context_len)", "\n", "pad_context_input_token_mask", "=", "tf", ".", "ones_like", "(", "pad_context_input_token_ids", ",", "tf", ".", "int32", ")", "\n", "pad_context_input_token_type_mask", "=", "tf", ".", "ones_like", "(", "pad_context_input_token_ids", ",", "tf", ".", "int32", ")", "\n", "\n", "sep_tokens", "=", "tf", ".", "cast", "(", "tf", ".", "fill", "(", "[", "1", "]", ",", "self", ".", "sep_in_vocab", ")", ",", "tf", ".", "int32", ")", "# (num_window, 1)", "\n", "cls_tokens", "=", "tf", ".", "cast", "(", "tf", ".", "fill", "(", "[", "1", "]", ",", "self", ".", "cls_in_vocab", ")", ",", "tf", ".", "int32", ")", "# (num_window, 1)", "\n", "\n", "query_context_input_token_ids", "=", "tf", ".", "concat", "(", "[", "cls_tokens", ",", "pad_query_input_token_ids", ",", "sep_tokens", ",", "pad_context_input_token_ids", "]", ",", "axis", "=", "0", ")", "\n", "query_context_input_token_mask", "=", "tf", ".", "concat", "(", "[", "tf", ".", "ones_like", "(", "cls_tokens", ",", "tf", ".", "int32", ")", ",", "pad_query_input_token_mask", ",", "tf", ".", "zeros_like", "(", "sep_tokens", ",", "tf", ".", "int32", ")", ",", "pad_context_input_token_mask", "]", ",", "axis", "=", "0", ")", "\n", "query_context_input_token_type_mask", "=", "tf", ".", "concat", "(", "[", "tf", ".", "zeros_like", "(", "cls_tokens", ",", "tf", ".", "int32", ")", ",", "pad_query_input_token_type_mask", ",", "tf", ".", "zeros_like", "(", "sep_tokens", ",", "tf", ".", "int32", ")", ",", "pad_context_input_token_type_mask", "]", ",", "axis", "=", "0", ")", "\n", "\n", "query_context_input_token_ids", "=", "tf", ".", "reshape", "(", "query_context_input_token_ids", ",", "[", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", "+", "2", "]", ")", "\n", "query_context_input_token_mask", "=", "tf", ".", "reshape", "(", "query_context_input_token_mask", ",", "[", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", "+", "2", "]", ")", "\n", "query_context_input_token_type_mask", "=", "tf", ".", "reshape", "(", "query_context_input_token_type_mask", ",", "[", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", "+", "2", "]", ")", "\n", "mention_start_idx_in_context", "=", "tf", ".", "reshape", "(", "mention_start_idx_in_context", ",", "[", "-", "1", "]", ")", "\n", "mention_end_idx_in_context", "=", "tf", ".", "reshape", "(", "mention_end_idx_in_context", ",", "[", "-", "1", "]", ")", "\n", "\n", "return", "[", "tf", ".", "math", ".", "add", "(", "i", ",", "1", ")", ",", "tf", ".", "concat", "(", "[", "batch_qa_input_ids", ",", "query_context_input_token_ids", "]", ",", "0", ")", ",", "\n", "tf", ".", "concat", "(", "[", "batch_qa_input_mask", ",", "query_context_input_token_mask", "]", ",", "0", ")", ",", "\n", "tf", ".", "concat", "(", "[", "batch_qa_input_token_type_mask", ",", "query_context_input_token_type_mask", "]", ",", "0", ")", ",", "\n", "tf", ".", "concat", "(", "[", "backward_qa_mention_start_in_context", ",", "mention_start_idx_in_context", "]", ",", "0", ")", ",", "\n", "tf", ".", "concat", "(", "[", "backward_qa_mention_end_in_context", ",", "mention_end_idx_in_context", "]", ",", "0", ")", "]", "\n", "\n", "\n", "", "_", ",", "stack_backward_qa_input_ids", ",", "stack_backward_qa_input_mask", ",", "stack_backward_qa_input_type_mask", ",", "stack_backward_mention_start_in_context", ",", "stack_backward_mention_end_in_context", "=", "tf", ".", "while_loop", "(", "\n", "cond", "=", "lambda", "i", ",", "o1", ",", "o2", ",", "o3", ",", "o4", ",", "o5", ":", "i", "<", "self", ".", "k", "*", "self", ".", "c", ",", "\n", "body", "=", "backward_qa_mention_linking", ",", "\n", "loop_vars", "=", "[", "i0", ",", "backward_qa_input_ids", ",", "backward_qa_input_mask", ",", "backward_qa_input_token_type_mask", ",", "backward_qa_mention_start_in_context", ",", "backward_qa_mention_end_in_context", "]", ",", "\n", "shape_invariants", "=", "[", "i0", ".", "get_shape", "(", ")", ",", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", ",", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", ",", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", ",", "\n", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", ",", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", "]", ")", "\n", "\n", "# stack_backward_qa_input_ids, stack_backward_qa_input_mask, stack_backward_qa_input_type_mask -> (k*c, max_query_len + max_context_len + 2)", "\n", "# stack_backward_mention_start_in_context, stack_backward_mention_end_in_context -> (k*c,)", "\n", "\n", "batch_backward_qa_input_ids", "=", "tf", ".", "reshape", "(", "stack_backward_qa_input_ids", ",", "[", "-", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", "+", "2", "]", ")", "\n", "batch_backward_qa_input_mask", "=", "tf", ".", "reshape", "(", "stack_backward_qa_input_mask", ",", "[", "-", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", "+", "2", "]", ")", "\n", "batch_backward_qa_input_type_mask", "=", "tf", ".", "reshape", "(", "stack_backward_qa_input_type_mask", ",", "[", "-", "1", ",", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", "+", "2", "]", ")", "\n", "\n", "backward_qa_linking_model", "=", "modeling", ".", "BertModel", "(", "config", "=", "self", ".", "bert_config", ",", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "batch_backward_qa_input_ids", ",", "input_mask", "=", "batch_backward_qa_input_mask", ",", "\n", "token_type_ids", "=", "batch_backward_qa_input_type_mask", ",", "use_one_hot_embeddings", "=", "False", ",", "\n", "scope", "=", "\"bert\"", ")", "\n", "\n", "backward_query_context_embs", "=", "backward_qa_linking_model", ".", "get_sequence_output", "(", ")", "# (k*c, max_query_len + max_context_len + 2, hidden_size)", "\n", "backward_query_context_embs", "=", "tf", ".", "reshape", "(", "backward_query_context_embs", ",", "[", "self", ".", "k", "*", "self", ".", "c", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", "]", ")", "\n", "flat_batch_backward_qa_input_type_mask", "=", "tf", ".", "reshape", "(", "batch_backward_qa_input_type_mask", ",", "[", "self", ".", "k", "*", "self", ".", "c", ",", "-", "1", "]", ")", "\n", "\n", "backward_context_flat_embs", "=", "self", ".", "transform_overlap_sliding_windows_to_original_document", "(", "backward_query_context_embs", ",", "flat_batch_backward_qa_input_type_mask", ")", "# (k*c, max_context_len, hidden_size)", "\n", "batch_backward_mention_start_in_context", "=", "tf", ".", "reshape", "(", "stack_backward_mention_start_in_context", ",", "[", "-", "1", "]", ")", "+", "tf", ".", "range", "(", "0", ",", "self", ".", "c", "*", "self", ".", "k", ")", "*", "(", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", ")", "\n", "batch_backward_mention_end_in_context", "=", "tf", ".", "reshape", "(", "stack_backward_mention_end_in_context", ",", "[", "-", "1", "]", ")", "+", "tf", ".", "range", "(", "0", ",", "self", ".", "c", "*", "self", ".", "k", ")", "*", "(", "self", ".", "config", ".", "max_query_len", "+", "self", ".", "config", ".", "max_context_len", ")", "\n", "\n", "backward_qa_mention_span_embs", ",", "backward_qa_mention_start_embs", ",", "backward_qa_mention_end_embs", "=", "self", ".", "get_candidate_span_embedding", "(", "tf", ".", "reshape", "(", "backward_context_flat_embs", ",", "\n", "[", "-", "1", ",", "self", ".", "config", ".", "hidden_size", "]", ")", ",", "tf", ".", "reshape", "(", "batch_backward_mention_start_in_context", ",", "[", "-", "1", "]", ")", ",", "tf", ".", "reshape", "(", "batch_backward_mention_end_in_context", ",", "[", "-", "1", "]", ")", ")", "\n", "# backward_qa_mention_span_embs -> (k*c, 2*hidden_size)", "\n", "# backward_qa_mention_start_embs, backward_qa_mention_end_embs -> (k*c, hidden_size)", "\n", "\n", "backward_qa_mention_span_scores", ",", "backward_qa_mention_start_scores", ",", "backward_qa_mention_end_scores", "=", "self", ".", "get_mention_score_and_loss", "(", "backward_qa_mention_span_embs", ",", "\n", "backward_qa_mention_start_embs", ",", "backward_qa_mention_end_embs", ",", "name_scope", "=", "\"backward_qa\"", ")", "\n", "# backward_qa_mention_span_prob -> (k*c)", "\n", "# backward_qa_mention_start_prob, backward_qa_mention_end_prob -> (k*c)", "\n", "\n", "if", "self", ".", "config", ".", "sec_qa_mention_score", ":", "\n", "            ", "backward_qa_mention_span_scores", "=", "(", "backward_qa_mention_span_scores", "+", "backward_qa_mention_start_scores", "+", "backward_qa_mention_end_scores", ")", "/", "3.0", "\n", "", "else", ":", "\n", "            ", "backward_qa_mention_span_scores", "=", "backward_qa_mention_span_scores", "\n", "#############", "\n", "############# backward QA computation ends", "\n", "\n", "", "forward_topc_mention_span_scores", "=", "tf", ".", "reshape", "(", "forward_topc_mention_span_scores", ",", "[", "-", "1", "]", ")", "\n", "expand_forward_topc_mention_span_scores", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "forward_topc_mention_span_scores", ",", "0", ")", ",", "[", "self", ".", "k", ",", "1", "]", ")", "# forward_topc_mention_span_scores -> (c); expand_forward_topc_mention_span_scores -> (c, k)", "\n", "expand_forward_topc_mention_span_scores_in_mention_proposal", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "forward_topc_mention_span_scores_in_mention_proposal", ",", "0", ")", ",", "[", "self", ".", "k", ",", "1", "]", ")", "\n", "expand_topk_mention_span_scores", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "topk_mention_span_scores", ",", "1", ")", ",", "[", "1", ",", "self", ".", "c", "]", ")", "# (k, c)", "\n", "\n", "backward_qa_mention_span_scores", "=", "tf", ".", "reshape", "(", "backward_qa_mention_span_scores", ",", "[", "self", ".", "k", ",", "self", ".", "c", "]", ")", "# (k, c)", "\n", "\n", "mention_span_linking_scores", "=", "(", "expand_forward_topc_mention_span_scores", "+", "backward_qa_mention_span_scores", ")", "/", "2.0", "\n", "mention_span_linking_scores", "=", "mention_span_linking_scores", "+", "expand_forward_topc_mention_span_scores_in_mention_proposal", "+", "expand_topk_mention_span_scores", "\n", "mention_span_linking_scores", "=", "tf", ".", "reshape", "(", "mention_span_linking_scores", ",", "[", "self", ".", "k", ",", "self", ".", "c", "]", ")", "# (k, c)", "\n", "dummy_scores", "=", "tf", ".", "zeros", "(", "[", "self", ".", "k", ",", "1", "]", ")", "# (k, 1)", "\n", "\n", "top_mention_span_linking_scores", "=", "tf", ".", "concat", "(", "[", "dummy_scores", ",", "mention_span_linking_scores", "]", ",", "axis", "=", "1", ")", "# (k, c)", "\n", "\n", "forward_topc_span_cluster_ids", "=", "tf", ".", "reshape", "(", "forward_topc_span_cluster_ids", ",", "[", "self", ".", "k", ",", "self", ".", "c", "]", ")", "# (k, c)", "\n", "same_cluster_indicator", "=", "tf", ".", "equal", "(", "forward_topc_span_cluster_ids", ",", "tf", ".", "expand_dims", "(", "topk_mention_span_cluster_ids", ",", "1", ")", ")", "\n", "non_dummy_indicator", "=", "tf", ".", "expand_dims", "(", "topk_mention_span_cluster_ids", ">", "0", ",", "1", ")", "\n", "pairwise_labels", "=", "tf", ".", "logical_and", "(", "same_cluster_indicator", ",", "non_dummy_indicator", ")", "\n", "dummy_labels", "=", "tf", ".", "logical_not", "(", "tf", ".", "reduce_any", "(", "pairwise_labels", ",", "1", ",", "keepdims", "=", "True", ")", ")", "\n", "top_mention_span_linking_labels", "=", "tf", ".", "concat", "(", "[", "dummy_labels", ",", "pairwise_labels", "]", ",", "1", ")", "\n", "\n", "linking_loss", "=", "self", ".", "marginal_likelihood_loss", "(", "top_mention_span_linking_scores", ",", "top_mention_span_linking_labels", ")", "\n", "\n", "total_loss", "=", "mention_proposal_loss", "+", "linking_loss", "\n", "\n", "return", "total_loss", ",", "(", "topk_mention_start_indices", ",", "topk_mention_end_indices", ")", ",", "(", "forward_topc_mention_start_indices", ",", "forward_topc_mention_end_indices", ")", ",", "top_mention_span_linking_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.marginal_likelihood_loss": [[374, 389], ["tensorflow.math.add", "tensorflow.math.reduce_logsumexp", "tensorflow.math.reduce_logsumexp", "tensorflow.math.reduce_sum", "tensorflow.log", "tensorflow.to_float"], "methods", ["None"], ["", "def", "marginal_likelihood_loss", "(", "self", ",", "antecedent_scores", ",", "antecedent_labels", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            marginal likelihood of gold antecedent spans form coreference cluster \n        Args:\n            antecedent_scores: [k, c+1] the predicted scores by the model\n            antecedent_labels: [k, c+1] the gold-truth cluster labels\n        Returns:\n            a scalar of loss \n        \"\"\"", "\n", "gold_scores", "=", "tf", ".", "math", ".", "add", "(", "antecedent_scores", ",", "tf", ".", "log", "(", "tf", ".", "to_float", "(", "antecedent_labels", ")", ")", ")", "\n", "marginalized_gold_scores", "=", "tf", ".", "math", ".", "reduce_logsumexp", "(", "gold_scores", ",", "[", "1", "]", ")", "# [k]", "\n", "log_norm", "=", "tf", ".", "math", ".", "reduce_logsumexp", "(", "antecedent_scores", ",", "[", "1", "]", ")", "# [k]", "\n", "loss", "=", "log_norm", "-", "marginalized_gold_scores", "# [k]", "\n", "return", "tf", ".", "math", ".", "reduce_sum", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_query_token_ids": [[391, 408], ["tensorflow.reshape", "tensorflow.gather", "tensorflow.math.equal", "corefqa.CorefQAModel.boolean_mask_1d", "tensorflow.where", "tensorflow.equal", "tensorflow.cast", "tensorflow.cast", "tensorflow.gather", "tensorflow.constant"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d"], ["", "def", "get_query_token_ids", "(", "self", ",", "nonoverlap_doc_input_ids", ",", "sentence_map", ",", "mention_start_idx", ",", "mention_end_idx", ",", "paddding", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            construct question based on the selected mention. \n        \"\"\"", "\n", "nonoverlap_doc_input_ids", "=", "tf", ".", "reshape", "(", "nonoverlap_doc_input_ids", ",", "[", "-", "1", "]", ")", "\n", "\n", "sentence_idx_for_mention", "=", "tf", ".", "gather", "(", "sentence_map", ",", "mention_start_idx", ")", "\n", "sentence_mask_for_mention", "=", "tf", ".", "math", ".", "equal", "(", "sentence_map", ",", "sentence_idx_for_mention", ")", "\n", "query_token_input_ids", "=", "self", ".", "boolean_mask_1d", "(", "nonoverlap_doc_input_ids", ",", "sentence_mask_for_mention", ",", "name_scope", "=", "\"query_mention\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "\n", "\n", "sentence_start", "=", "tf", ".", "where", "(", "tf", ".", "equal", "(", "nonoverlap_doc_input_ids", ",", "tf", ".", "gather", "(", "query_token_input_ids", ",", "tf", ".", "constant", "(", "0", ")", ")", ")", ")", "\n", "\n", "mention_start_in_sent", "=", "mention_start_idx", "-", "tf", ".", "cast", "(", "sentence_start", ",", "tf", ".", "int32", ")", "\n", "mention_end_in_sent", "=", "mention_end_idx", "-", "tf", ".", "cast", "(", "sentence_start", ",", "tf", ".", "int32", ")", "\n", "\n", "return", "query_token_input_ids", ",", "mention_start_in_sent", ",", "mention_end_in_sent", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_mention_score_and_loss": [[411, 436], ["corefqa.CorefQAModel.ffnn", "corefqa.CorefQAModel.ffnn", "corefqa.CorefQAModel.ffnn", "corefqa.CorefQAModel.compute_mention_score_and_loss", "corefqa.CorefQAModel.compute_mention_score_and_loss", "corefqa.CorefQAModel.compute_mention_score_and_loss", "tensorflow.math.log", "tensorflow.math.log", "tensorflow.math.log", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.math.log", "tensorflow.math.log", "tensorflow.math.log"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.ffnn", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.ffnn", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.ffnn", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.compute_mention_score_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.compute_mention_score_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.compute_mention_score_and_loss"], ["", "def", "get_mention_score_and_loss", "(", "self", ",", "candidate_mention_span_embs", ",", "candidate_mention_start_embs", ",", "candidate_mention_end_embs", ",", "\n", "gold_label_candidate_mention_spans", "=", "None", ",", "gold_label_candidate_mention_starts", "=", "None", ",", "gold_label_candidate_mention_ends", "=", "None", ",", "expect_length_of_labels", "=", "None", ",", "\n", "name_scope", "=", "\"mention\"", ")", ":", "\n", "\n", "        ", "candidate_mention_span_logits", "=", "self", ".", "ffnn", "(", "candidate_mention_span_embs", ",", "self", ".", "config", ".", "hidden_size", "*", "2", ",", "1", ",", "dropout", "=", "self", ".", "dropout", ",", "name_scope", "=", "\"{}_span\"", ".", "format", "(", "name_scope", ")", ")", "\n", "candidate_mention_start_logits", "=", "self", ".", "ffnn", "(", "candidate_mention_start_embs", ",", "self", ".", "config", ".", "hidden_size", ",", "1", ",", "dropout", "=", "self", ".", "dropout", ",", "name_scope", "=", "\"{}_start\"", ".", "format", "(", "name_scope", ")", ")", "\n", "candidate_mention_end_logits", "=", "self", ".", "ffnn", "(", "candidate_mention_end_embs", ",", "self", ".", "config", ".", "hidden_size", ",", "1", ",", "dropout", "=", "self", ".", "dropout", ",", "name_scope", "=", "\"{}_end\"", ".", "format", "(", "name_scope", ")", ")", "\n", "\n", "if", "gold_label_candidate_mention_spans", "is", "None", "or", "gold_label_candidate_mention_starts", "is", "None", "or", "gold_label_candidate_mention_ends", "is", "None", ":", "\n", "            ", "candidate_mention_span_scores", "=", "tf", ".", "math", ".", "log", "(", "tf", ".", "sigmoid", "(", "candidate_mention_span_logits", ")", ")", "\n", "candidate_mention_start_scores", "=", "tf", ".", "math", ".", "log", "(", "tf", ".", "sigmoid", "(", "candidate_mention_start_logits", ")", ")", "\n", "candidate_mention_end_scores", "=", "tf", ".", "math", ".", "log", "(", "tf", ".", "sigmoid", "(", "candidate_mention_end_logits", ")", ")", "\n", "\n", "return", "candidate_mention_span_scores", ",", "candidate_mention_start_scores", ",", "candidate_mention_end_scores", "\n", "\n", "\n", "", "start_loss", ",", "candidate_mention_start_probability", "=", "self", ".", "compute_mention_score_and_loss", "(", "candidate_mention_start_logits", ",", "gold_label_candidate_mention_starts", ")", "\n", "end_loss", ",", "candidate_mention_end_probability", "=", "self", ".", "compute_mention_score_and_loss", "(", "candidate_mention_end_logits", ",", "gold_label_candidate_mention_ends", ")", "\n", "span_loss", ",", "candidate_mention_span_probability", "=", "self", ".", "compute_mention_score_and_loss", "(", "candidate_mention_span_logits", ",", "gold_label_candidate_mention_spans", ")", "\n", "\n", "\n", "total_loss", "=", "start_loss", "+", "end_loss", "+", "span_loss", "\n", "candidate_mention_span_scores", "=", "(", "tf", ".", "math", ".", "log", "(", "candidate_mention_start_probability", ")", "+", "tf", ".", "math", ".", "log", "(", "candidate_mention_end_probability", ")", "+", "tf", ".", "math", ".", "log", "(", "candidate_mention_span_probability", ")", ")", "/", "3.0", "\n", "\n", "return", "total_loss", ",", "candidate_mention_start_probability", ",", "candidate_mention_end_probability", ",", "candidate_mention_span_probability", ",", "candidate_mention_span_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.compute_mention_score_and_loss": [[438, 463], ["tensorflow.cast", "tensorflow.stack", "tensorflow.cast", "tensorflow.keras.losses.binary_crossentropy", "tensorflow.reduce_mean", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.multiply", "tensorflow.sigmoid", "tensorflow.reshape", "tensorflow.cast"], "methods", ["None"], ["", "def", "compute_mention_score_and_loss", "(", "self", ",", "pred_sequence_logits", ",", "gold_sequence_labels", ",", "loss_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            compute the unifrom start/end loss and probabilities. \n        Args:\n            pred_sequence_logits: (input_shape, 1) \n            gold_sequence_labels: (input_shape, 1)\n            loss_mask: [optional] if is not None, it should be (input_shape). should be tf.int32 0/1 tensor. \n            FOR start/end score and loss, input_shape should be num_subtoken_in_doc.\n            FOR span score and loss, input_shape should be num_subtoken_in_doc * num_subtoken_in_doc. \n        \"\"\"", "\n", "pred_sequence_probabilities", "=", "tf", ".", "cast", "(", "tf", ".", "reshape", "(", "tf", ".", "sigmoid", "(", "pred_sequence_logits", ")", ",", "[", "-", "1", "]", ")", ",", "tf", ".", "float32", ")", "# (input_shape)", "\n", "expand_pred_sequence_scores", "=", "tf", ".", "stack", "(", "[", "(", "1", "-", "pred_sequence_probabilities", ")", ",", "pred_sequence_probabilities", "]", ",", "axis", "=", "-", "1", ")", "# (input_shape, 2)", "\n", "expand_gold_sequence_labels", "=", "tf", ".", "cast", "(", "tf", ".", "one_hot", "(", "tf", ".", "reshape", "(", "gold_sequence_labels", ",", "[", "-", "1", "]", ")", ",", "2", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "float32", ")", "# (input_shape, 2)", "\n", "\n", "loss", "=", "tf", ".", "keras", ".", "losses", ".", "binary_crossentropy", "(", "expand_gold_sequence_labels", ",", "expand_pred_sequence_scores", ")", "\n", "# loss -> shape is (input_shape)", "\n", "\n", "if", "loss_mask", "is", "not", "None", ":", "\n", "            ", "loss", "=", "tf", ".", "multiply", "(", "loss", ",", "tf", ".", "cast", "(", "loss_mask", ",", "tf", ".", "float32", ")", ")", "\n", "\n", "", "total_loss", "=", "tf", ".", "reduce_mean", "(", "loss", ")", "\n", "# total_loss -> a scalar ", "\n", "\n", "return", "total_loss", ",", "pred_sequence_probabilities", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_candidate_span_embedding": [[465, 473], ["tensorflow.reshape", "tensorflow.gather", "tensorflow.gather", "tensorflow.concat"], "methods", ["None"], ["", "def", "get_candidate_span_embedding", "(", "self", ",", "doc_sequence_embeddings", ",", "candidate_span_starts", ",", "candidate_span_ends", ")", ":", "\n", "        ", "doc_sequence_embeddings", "=", "tf", ".", "reshape", "(", "doc_sequence_embeddings", ",", "[", "-", "1", ",", "self", ".", "config", ".", "hidden_size", "]", ")", "\n", "\n", "span_start_embedding", "=", "tf", ".", "gather", "(", "doc_sequence_embeddings", ",", "candidate_span_starts", ")", "\n", "span_end_embedding", "=", "tf", ".", "gather", "(", "doc_sequence_embeddings", ",", "candidate_span_ends", ")", "\n", "span_embedding", "=", "tf", ".", "concat", "(", "[", "span_start_embedding", ",", "span_end_embedding", "]", ",", "1", ")", "\n", "\n", "return", "span_embedding", ",", "span_start_embedding", ",", "span_end_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_candidate_mention_gold_sequence_label": [[474, 492], ["corefqa.CorefQAModel.scatter_gold_index_to_label_sequence", "corefqa.CorefQAModel.scatter_gold_index_to_label_sequence", "tensorflow.gather", "tensorflow.gather", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.Variable", "tensorflow.cast", "tensorflow.stack", "tensorflow.gather_nd", "tensorflow.stack", "tensorflow.ones_like", "tensorflow.scatter_nd", "tensorflow.expand_dims"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence"], ["", "def", "get_candidate_mention_gold_sequence_label", "(", "self", ",", "candidate_mention_starts", ",", "candidate_mention_ends", ",", "\n", "gold_start_index_labels", ",", "gold_end_index_labels", ",", "expect_length_of_labels", ")", ":", "\n", "\n", "        ", "gold_start_sequence_label", "=", "self", ".", "scatter_gold_index_to_label_sequence", "(", "gold_start_index_labels", ",", "expect_length_of_labels", ")", "\n", "gold_end_sequence_label", "=", "self", ".", "scatter_gold_index_to_label_sequence", "(", "gold_end_index_labels", ",", "expect_length_of_labels", ")", "\n", "\n", "gold_label_candidate_mention_starts", "=", "tf", ".", "gather", "(", "gold_start_sequence_label", ",", "candidate_mention_starts", ")", "\n", "gold_label_candidate_mention_ends", "=", "tf", ".", "gather", "(", "gold_end_sequence_label", ",", "candidate_mention_ends", ")", "\n", "\n", "gold_mention_sparse_label", "=", "tf", ".", "reshape", "(", "tf", ".", "stack", "(", "[", "gold_start_index_labels", ",", "gold_end_index_labels", "]", ",", "axis", "=", "1", ")", ",", "[", "2", ",", "-", "1", "]", ")", "\n", "gold_span_value", "=", "tf", ".", "reshape", "(", "tf", ".", "ones_like", "(", "gold_start_index_labels", ",", "tf", ".", "int32", ")", ",", "[", "-", "1", "]", ")", "\n", "gold_span_shape", "=", "tf", ".", "Variable", "(", "[", "expect_length_of_labels", ",", "expect_length_of_labels", "]", ",", "shape", "=", "(", "2", ",", ")", ")", "\n", "gold_span_label", "=", "tf", ".", "cast", "(", "tf", ".", "scatter_nd", "(", "gold_mention_sparse_label", ",", "gold_span_value", ",", "gold_span_shape", ")", ",", "tf", ".", "int32", ")", "\n", "\n", "candidate_mention_spans", "=", "tf", ".", "stack", "(", "[", "candidate_mention_starts", ",", "candidate_mention_ends", "]", ",", "axis", "=", "1", ")", "\n", "gold_label_candidate_mention_spans", "=", "tf", ".", "gather_nd", "(", "gold_span_label", ",", "tf", ".", "expand_dims", "(", "candidate_mention_spans", ",", "1", ")", ")", "\n", "\n", "return", "gold_label_candidate_mention_spans", ",", "gold_label_candidate_mention_starts", ",", "gold_label_candidate_mention_ends", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.scatter_gold_index_to_label_sequence": [[494, 509], ["tensorflow.reshape", "tensorflow.reshape", "tensorflow.Variable", "tensorflow.reshape", "tensorflow.cast", "tensorflow.ones_like", "tensorflow.scatter_nd"], "methods", ["None"], ["", "def", "scatter_gold_index_to_label_sequence", "(", "self", ",", "gold_index_labels", ",", "expect_length_of_labels", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            transform the mention start/end position index tf.int32 Tensor to a tf.int32 Tensor with 1/0 labels for the input subtoken sequences.\n            1 denotes this subtoken is the start/end for a mention. \n        Args:\n            gold_index_labels: a tf.int32 Tensor with mention start/end position index in the original document. \n            expect_length_of_labels: the number of subtokens in the original document. \n        \"\"\"", "\n", "gold_labels_pos", "=", "tf", ".", "reshape", "(", "gold_index_labels", ",", "[", "-", "1", ",", "1", "]", ")", "# (num_of_mention, 1)", "\n", "gold_value", "=", "tf", ".", "reshape", "(", "tf", ".", "ones_like", "(", "gold_index_labels", ")", ",", "[", "-", "1", "]", ")", "# (num_of_mention)", "\n", "label_shape", "=", "tf", ".", "Variable", "(", "expect_length_of_labels", ")", "\n", "label_shape", "=", "tf", ".", "reshape", "(", "label_shape", ",", "[", "1", "]", ")", "# [1]", "\n", "gold_label_sequence", "=", "tf", ".", "cast", "(", "tf", ".", "scatter_nd", "(", "gold_labels_pos", ",", "gold_value", ",", "label_shape", ")", ",", "tf", ".", "int32", ")", "# (num_subtoken_in_doc)", "\n", "return", "gold_label_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.scatter_span_sequence_labels": [[511, 530], ["tensorflow.stack", "tensorflow.reshape", "tensorflow.Variable", "tensorflow.reshape", "tensorflow.cast", "tensorflow.ones_like", "tensorflow.scatter_nd"], "methods", ["None"], ["", "def", "scatter_span_sequence_labels", "(", "self", ",", "gold_start_index_labels", ",", "gold_end_index_labels", ",", "expect_length_of_labels", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            transform the mention (start, end) position pairs to a span matrix gold_span_sequence_labels. \n                matrix[i][j]: whether the subtokens between the position $i$ to $j$ can be a mention.  \n                if matrix[i][j] == 0: from $i$ to $j$ is not a mention. \n                if matrix[i][j] == 1: from $i$ to $j$ is a mention.\n        Args:\n            gold_start_index_labels: a tf.int32 Tensor with mention start position index in the original document. \n            gold_end_index_labels: a tf.int32 Tensor with mention end position index in the original document. \n            expect_length_of_labels: a scalar, should be the same with num_subtoken_in_doc\n        \"\"\"", "\n", "gold_span_index_labels", "=", "tf", ".", "stack", "(", "[", "gold_start_index_labels", ",", "gold_end_index_labels", "]", ",", "axis", "=", "1", ")", "# (num_of_mention, 2)", "\n", "gold_span_value", "=", "tf", ".", "reshape", "(", "tf", ".", "ones_like", "(", "gold_start_index_labels", ",", "tf", ".", "int32", ")", ",", "[", "-", "1", "]", ")", "# (num_of_mention)", "\n", "gold_span_label_shape", "=", "tf", ".", "Variable", "(", "[", "expect_length_of_labels", ",", "expect_length_of_labels", "]", ")", "\n", "gold_span_label_shape", "=", "tf", ".", "reshape", "(", "gold_span_label_shape", ",", "[", "-", "1", "]", ")", "\n", "\n", "gold_span_sequence_labels", "=", "tf", ".", "cast", "(", "tf", ".", "scatter_nd", "(", "gold_span_index_labels", ",", "gold_span_value", ",", "gold_span_label_shape", ")", ",", "tf", ".", "int32", ")", "# (num_subtoken_in_doc, num_subtoken_in_doc)", "\n", "return", "gold_span_sequence_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_candidate_cluster_labels": [[532, 545], ["tensorflow.equal", "tensorflow.equal", "tensorflow.logical_and", "tensorflow.matmul", "tensorflow.squeeze", "tensorflow.expand_dims", "tensorflow.to_int32"], "methods", ["None"], ["", "def", "get_candidate_cluster_labels", "(", "self", ",", "candidate_mention_starts", ",", "candidate_mention_ends", ",", "\n", "gold_mention_starts", ",", "gold_mention_ends", ",", "gold_cluster_ids", ")", ":", "\n", "\n", "        ", "same_mention_start", "=", "tf", ".", "equal", "(", "gold_mention_starts", ",", "candidate_mention_starts", ")", "\n", "same_mention_end", "=", "tf", ".", "equal", "(", "gold_mention_ends", ",", "candidate_mention_ends", ")", "\n", "# same_mention_start = tf.equal(tf.expand_dims(gold_mention_starts, 1), tf.expand_dims(candidate_mention_starts, 0))", "\n", "# same_mention_end = tf.equal(tf.expand_dims(gold_mention_ends, 1), tf.expand_dims(candidate_mention_ends, 0)) ", "\n", "same_mention_span", "=", "tf", ".", "logical_and", "(", "same_mention_start", ",", "same_mention_end", ")", "\n", "\n", "candidate_cluster_idx_labels", "=", "tf", ".", "matmul", "(", "tf", ".", "expand_dims", "(", "gold_cluster_ids", ",", "0", ")", ",", "tf", ".", "to_int32", "(", "same_mention_span", ")", ")", "# [1, num_candidates]", "\n", "candidate_cluster_idx_labels", "=", "tf", ".", "squeeze", "(", "candidate_cluster_idx_labels", ",", "0", ")", "# [num_candidates]", "\n", "\n", "return", "candidate_cluster_idx_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.transform_overlap_sliding_windows_to_original_document": [[547, 574], ["tensorflow.ones_like", "tensorflow.math.cumsum", "corefqa.CorefQAModel.get_shape", "corefqa.CorefQAModel.get_shape", "tensorflow.tile", "tensorflow.math.multiply", "tensorflow.reshape", "corefqa.CorefQAModel.boolean_mask_1d", "tensorflow.reshape", "tensorflow.gather", "tensorflow.expand_dims", "tensorflow.math.greater", "tensorflow.zeros_like", "tensorflow.range"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d"], ["", "def", "transform_overlap_sliding_windows_to_original_document", "(", "self", ",", "overlap_window_inputs", ",", "overlap_window_mask", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            hidden_size should be equal to embeddding_size. \n        Args:\n            doc_overlap_window_embs: (num_window, window_size, hidden_size). \n                the output of (num_window, window_size) input_ids forward into BERT model. \n            doc_overlap_input_mask: (num_window, window_size). A tf.int32 Tensor contains 0/1. \n                0 represents token in this position should be neglected. 1 represents token in this position should be reserved. \n        \"\"\"", "\n", "ones_input_mask", "=", "tf", ".", "ones_like", "(", "overlap_window_mask", ",", "tf", ".", "int32", ")", "# (num_window, window_size)", "\n", "cumsum_input_mask", "=", "tf", ".", "math", ".", "cumsum", "(", "ones_input_mask", ",", "axis", "=", "1", ")", "# (num_window, window_size) # do not equal to cumsum_input_mask -> (num_window, window_size)", "\n", "# offset_input_mask = tf.tile(tf.expand_dims(tf.range(self.config.num_window) * self.config.window_size, 1), [1, self.config.window_size]) # (num_window, window_size)", "\n", "row_cumsum_input_mask", "=", "self", ".", "get_shape", "(", "cumsum_input_mask", ",", "0", ")", "\n", "col_cumsum_input_mask", "=", "self", ".", "get_shape", "(", "cumsum_input_mask", ",", "1", ")", "\n", "offset_input_mask", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "row_cumsum_input_mask", ")", "*", "col_cumsum_input_mask", ",", "1", ")", ",", "[", "1", ",", "col_cumsum_input_mask", "]", ")", "\n", "\n", "\n", "offset_cumsum_input_mask", "=", "offset_input_mask", "+", "cumsum_input_mask", "# (num_window, window_size)", "\n", "global_input_mask", "=", "tf", ".", "math", ".", "multiply", "(", "ones_input_mask", ",", "offset_cumsum_input_mask", ")", "# (num_window, window_size)", "\n", "global_input_mask", "=", "tf", ".", "reshape", "(", "global_input_mask", ",", "[", "-", "1", "]", ")", "# (num_window * window_size)", "\n", "global_input_mask_index", "=", "self", ".", "boolean_mask_1d", "(", "global_input_mask", ",", "tf", ".", "math", ".", "greater", "(", "global_input_mask", ",", "tf", ".", "zeros_like", "(", "global_input_mask", ",", "tf", ".", "int32", ")", ")", ")", "# (num_subtoken_in_doc)", "\n", "\n", "overlap_window_inputs", "=", "tf", ".", "reshape", "(", "overlap_window_inputs", ",", "[", "self", ".", "config", ".", "num_window", "*", "self", ".", "config", ".", "window_size", ",", "-", "1", "]", ")", "# (num_window * window_size, hidden_size)", "\n", "original_doc_inputs", "=", "tf", ".", "gather", "(", "overlap_window_inputs", ",", "global_input_mask_index", ")", "# (num_subtoken_in_doc, hidden_size)", "\n", "\n", "return", "original_doc_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.ffnn": [[576, 594], ["tensorflow.truncated_normal_initializer", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.relu", "tensorflow.nn.xw_plus_b", "tensorflow.nn.dropout", "tensorflow.zeros_initializer"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.dropout"], ["", "def", "ffnn", "(", "self", ",", "inputs", ",", "hidden_size", ",", "output_size", ",", "dropout", "=", "None", ",", "name_scope", "=", "\"fully-conntected-neural-network\"", ",", "\n", "hidden_initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            fully-connected neural network. \n            transform non-linearly the [input] tensor with [hidden_size] to a fix [output_size] size.  \n        Args: \n            hidden_size: should be the size of last dimension of [inputs]. \n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "name_scope", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "            ", "hidden_weights", "=", "tf", ".", "get_variable", "(", "\"hidden_weights\"", ",", "[", "hidden_size", ",", "output_size", "]", ",", "\n", "initializer", "=", "hidden_initializer", ")", "\n", "hidden_bias", "=", "tf", ".", "get_variable", "(", "\"hidden_bias\"", ",", "[", "output_size", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "outputs", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "inputs", ",", "hidden_weights", ",", "hidden_bias", ")", ")", "\n", "\n", "if", "dropout", "is", "not", "None", ":", "\n", "                ", "outputs", "=", "tf", ".", "nn", ".", "dropout", "(", "outputs", ",", "dropout", ")", "\n", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.boolean_mask_1d": [[596, 619], ["tensorflow.name_scope", "tensorflow.reduce_sum", "tensorflow.cast", "tensorflow.cast", "tensorflow.one_hot", "tensorflow.cast", "tensorflow.reshape", "tensorflow.gather", "tensorflow.cast", "tensorflow.multiply", "tensorflow.tensordot", "tensorflow.cumsum", "tensorflow.cast", "tensorflow.range", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape"], ["", "def", "boolean_mask_1d", "(", "self", ",", "itemtensor", ",", "boolmask_indicator", ",", "name_scope", "=", "\"boolean_mask1d\"", ",", "use_tpu", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            the same functionality of tf.boolean_mask. \n            The tf.boolean_mask operation is not available on the cloud TPU. \n        Args:\n            itemtensor : a Tensor contains [tf.int32, tf.float32] numbers. Should be 1-Rank.\n            boolmask_indicator : a tf.bool Tensor. Should be 1-Rank. \n            scope : name scope for the operation. \n            use_tpu : if False, return tf.boolean_mask.  \n        \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "name_scope", ")", ":", "\n", "\n", "            ", "boolmask_sum", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "cast", "(", "boolmask_indicator", ",", "tf", ".", "int32", ")", ")", "\n", "selected_positions", "=", "tf", ".", "cast", "(", "boolmask_indicator", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "indexed_positions", "=", "tf", ".", "cast", "(", "tf", ".", "multiply", "(", "tf", ".", "cumsum", "(", "selected_positions", ")", ",", "selected_positions", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "one_hot_selector", "=", "tf", ".", "one_hot", "(", "indexed_positions", "-", "1", ",", "boolmask_sum", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "sampled_indices", "=", "tf", ".", "cast", "(", "tf", ".", "tensordot", "(", "tf", ".", "cast", "(", "tf", ".", "range", "(", "tf", ".", "shape", "(", "boolmask_indicator", ")", "[", "0", "]", ")", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "\n", "one_hot_selector", ",", "axes", "=", "[", "0", ",", "0", "]", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "sampled_indices", "=", "tf", ".", "reshape", "(", "sampled_indices", ",", "[", "-", "1", "]", ")", "\n", "mask_itemtensor", "=", "tf", ".", "gather", "(", "itemtensor", ",", "sampled_indices", ")", "\n", "\n", "return", "mask_itemtensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_dropout": [[621, 623], ["tensorflow.to_float"], "methods", ["None"], ["", "", "def", "get_dropout", "(", "self", ",", "dropout_rate", ",", "is_training", ")", ":", "\n", "        ", "return", "1", "-", "(", "tf", ".", "to_float", "(", "is_training", ")", "*", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.get_shape": [[625, 631], ["tensorflow.shape", "x.get_shape"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape"], ["", "def", "get_shape", "(", "self", ",", "x", ",", "dim", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            return the size of input x in DIM. \n        \"\"\"", "\n", "return", "x", ".", "get_shape", "(", ")", "[", "dim", "]", ".", "value", "or", "tf", ".", "shape", "(", "x", ")", "[", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.corefqa.CorefQAModel.evaluate": [[633, 683], ["corefqa.CorefQAModel.evaluate.transform_gold_labels"], "methods", ["None"], ["", "def", "evaluate", "(", "self", ",", "top_span_starts", ",", "top_span_ends", ",", "predicted_antecedents", ",", "\n", "gold_clusters", ",", "gold_starts", ",", "gold_ends", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            expected cluster ids is : [[[21, 25], [18, 18]], [[63, 65], [46, 48], [27, 29]], [[88, 88], [89, 89]]]\n        Args:\n            top_span_starts: \n            top_span_ends:\n            predicted_antecedents: \n        Returns:\n            predicted_clusters: \n            gold_clusters:\n            mention_to_predicted:\n            mention_to_gold: \n        \"\"\"", "\n", "# predicted_antecedents = np.argmax(predicted_antecedents, axis=-1)", "\n", "top_span_starts", ",", "top_span_ends", ",", "predicted_antecedents", "=", "top_span_starts", ".", "tolist", "(", ")", ",", "top_span_ends", ".", "tolist", "(", ")", ",", "predicted_antecedents", ".", "tolist", "(", ")", "\n", "gold_clusters", ",", "gold_starts", ",", "gold_ends", "=", "gold_clusters", ".", "tolist", "(", ")", "[", "0", "]", ",", "gold_starts", ".", "tolist", "(", ")", "[", "0", "]", ",", "gold_ends", ".", "tolist", "(", ")", "[", "0", "]", "\n", "\n", "def", "transform_gold_labels", "(", "gold_clusters", ",", "gold_starts", ",", "gold_ends", ")", ":", "\n", "            ", "gold_clusters_idx", "=", "[", "tmp", "for", "tmp", "in", "gold_clusters", "if", "tmp", ">=", "0", "]", "\n", "gold_starts", "=", "[", "tmp", "for", "tmp", "in", "gold_starts", "if", "tmp", ">=", "0", "]", "\n", "gold_ends", "=", "[", "tmp", "for", "tmp", "in", "gold_ends", "if", "tmp", ">=", "0", "]", "\n", "\n", "gold_clusters_dict", "=", "{", "}", "\n", "gold_cluster_lst", "=", "[", "]", "\n", "\n", "for", "idx", ",", "(", "tmp_start", ",", "tmp_end", ")", "in", "enumerate", "(", "zip", "(", "gold_starts", ",", "gold_ends", ")", ")", ":", "\n", "                ", "tmp_cluster_idx", "=", "gold_clusters_idx", "[", "idx", "]", "\n", "if", "tmp_cluster_idx", "not", "in", "gold_clusters_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "gold_cluster_lst", ".", "append", "(", "tmp_cluster_idx", ")", "\n", "gold_clusters_dict", "[", "tmp_cluster_idx", "]", "=", "[", "[", "tmp_start", ",", "tmp_end", "]", "]", "\n", "", "else", ":", "\n", "                    ", "gold_clusters_dict", "[", "tmp_cluster_idx", "]", ".", "append", "(", "[", "tmp_start", ",", "tmp_end", "]", ")", "\n", "\n", "", "", "gold_cluster", "=", "[", "gold_clusters_dict", "[", "tmp_idx", "]", "for", "tmp_idx", "in", "gold_cluster_lst", "]", "\n", "\n", "return", "gold_cluster", ",", "gold_starts", ",", "gold_ends", "\n", "\n", "", "gold_clusters", ",", "gold_starts", ",", "gold_ends", "=", "transform_gold_labels", "(", "gold_clusters", ",", "gold_starts", ",", "gold_ends", ")", "\n", "\n", "gold_clusters", "=", "[", "tuple", "(", "tuple", "(", "m", ")", "for", "m", "in", "gc", ")", "for", "gc", "in", "gold_clusters", "]", "\n", "mention_to_gold", "=", "{", "}", "\n", "for", "gc", "in", "gold_clusters", ":", "\n", "            ", "for", "mention", "in", "gc", ":", "\n", "                ", "mention_to_gold", "[", "mention", "]", "=", "gc", "\n", "\n", "", "", "predicted_clusters", ",", "mention_to_predicted", "=", "self", ".", "get_predicted_clusters", "(", "top_span_starts", ",", "top_span_ends", ",", "predicted_antecedents", ")", "\n", "\n", "return", "predicted_clusters", ",", "gold_clusters", ",", "mention_to_predicted", ",", "mention_to_gold", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.__init__": [[24, 28], ["bert.modeling.BertConfig.from_json_file"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.from_json_file"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "config", ".", "bert_config_file", ")", "\n", "self", ".", "bert_config", ".", "hidden_dropout_prob", "=", "config", ".", "dropout_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_mention_proposal_and_loss": [[29, 107], ["mention_proposal.MentionProposalModel.get_dropout", "tensorflow.math.maximum", "tensorflow.where", "tensorflow.math.maximum", "tensorflow.cast", "mention_proposal.MentionProposalModel.boolean_mask_1d", "mention_proposal.MentionProposalModel.boolean_mask_1d", "tensorflow.math.maximum", "tensorflow.math.reduce_sum", "tensorflow.reshape", "tensorflow.ones_like", "bert.modeling.BertModel", "bert.modeling.BertModel.get_sequence_output", "tensorflow.reshape", "mention_proposal.MentionProposalModel.transform_overlap_windows_to_original_doc", "tensorflow.reshape", "tensorflow.tile", "tensorflow.tile", "tensorflow.concat", "tensorflow.reshape", "mention_proposal.MentionProposalModel.ffnn", "mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence", "mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence", "mention_proposal.MentionProposalModel.compute_score_and_loss", "mention_proposal.MentionProposalModel.compute_score_and_loss", "mention_proposal.MentionProposalModel.scatter_span_sequence_labels", "mention_proposal.MentionProposalModel.compute_score_and_loss", "tensorflow.zeros_like", "tensorflow.math.greater_equal", "tensorflow.zeros_like", "tensorflow.math.greater_equal", "tensorflow.zeros_like", "tensorflow.expand_dims", "tensorflow.expand_dims", "mention_proposal.MentionProposalModel.ffnn", "tensorflow.split", "mention_proposal.MentionProposalModel.ffnn", "mention_proposal.MentionProposalModel.ffnn", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.zeros_like"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_dropout", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.transform_overlap_windows_to_original_doc", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.ffnn", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.compute_score_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.compute_score_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_span_sequence_labels", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.compute_score_and_loss", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.ffnn", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.ffnn", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.ffnn"], ["", "def", "get_mention_proposal_and_loss", "(", "self", ",", "instance", ",", "is_training", ",", "use_tpu", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            forward function for training mention proposal module. \n        Args:\n            instance: a tuple of train/dev/test data instance. \n                e.g., (flat_input_ids, flat_doc_overlap_input_mask, flat_sentence_map, text_len, speaker_ids, gold_starts, gold_ends, cluster_ids)\n            is_training: True/False is in the training process. \n        \"\"\"", "\n", "self", ".", "use_tpu", "=", "use_tpu", "\n", "self", ".", "dropout", "=", "self", ".", "get_dropout", "(", "self", ".", "config", ".", "dropout_rate", ",", "is_training", ")", "\n", "\n", "flat_input_ids", ",", "flat_doc_overlap_input_mask", ",", "flat_sentence_map", ",", "text_len", ",", "speaker_ids", ",", "gold_starts", ",", "gold_ends", ",", "cluster_ids", "=", "instance", "\n", "# flat_input_ids: (num_window, window_size)", "\n", "# flat_doc_overlap_input_mask: (num_window, window_size)", "\n", "# flat_sentence_map: (num_window, window_size)", "\n", "# text_len: dynamic length and is padded to fix length", "\n", "# gold_start: (max_num_mention), mention start index in the original (NON-OVERLAP) document. Pad with -1 to the fix length max_num_mention.", "\n", "# gold_end: (max_num_mention), mention end index in the original (NON-OVERLAP) document. Pad with -1 to the fix length max_num_mention.", "\n", "# cluster_ids/speaker_ids is not used in the mention proposal model.", "\n", "\n", "flat_input_ids", "=", "tf", ".", "math", ".", "maximum", "(", "flat_input_ids", ",", "tf", ".", "zeros_like", "(", "flat_input_ids", ",", "tf", ".", "int32", ")", ")", "# (num_window * window_size)", "\n", "\n", "flat_doc_overlap_input_mask", "=", "tf", ".", "where", "(", "tf", ".", "math", ".", "greater_equal", "(", "flat_doc_overlap_input_mask", ",", "0", ")", ",", "\n", "x", "=", "tf", ".", "ones_like", "(", "flat_doc_overlap_input_mask", ",", "tf", ".", "int32", ")", ",", "y", "=", "tf", ".", "zeros_like", "(", "flat_doc_overlap_input_mask", ",", "tf", ".", "int32", ")", ")", "# (num_window * window_size)", "\n", "# flat_doc_overlap_input_mask = tf.math.maximum(flat_doc_overlap_input_mask, tf.zeros_like(flat_doc_overlap_input_mask, tf.int32))", "\n", "flat_sentence_map", "=", "tf", ".", "math", ".", "maximum", "(", "flat_sentence_map", ",", "tf", ".", "zeros_like", "(", "flat_sentence_map", ",", "tf", ".", "int32", ")", ")", "# (num_window * window_size)", "\n", "\n", "gold_start_end_mask", "=", "tf", ".", "cast", "(", "tf", ".", "math", ".", "greater_equal", "(", "gold_starts", ",", "tf", ".", "zeros_like", "(", "gold_starts", ",", "tf", ".", "int32", ")", ")", ",", "tf", ".", "bool", ")", "# (max_num_mention)", "\n", "gold_start_index_labels", "=", "self", ".", "boolean_mask_1d", "(", "gold_starts", ",", "gold_start_end_mask", ",", "name_scope", "=", "\"gold_starts\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "# (num_of_mention)", "\n", "gold_end_index_labels", "=", "self", ".", "boolean_mask_1d", "(", "gold_ends", ",", "gold_start_end_mask", ",", "name_scope", "=", "\"gold_ends\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "# (num_of_mention)", "\n", "\n", "text_len", "=", "tf", ".", "math", ".", "maximum", "(", "text_len", ",", "tf", ".", "zeros_like", "(", "text_len", ",", "tf", ".", "int32", ")", ")", "# (num_of_non_empty_window)", "\n", "num_subtoken_in_doc", "=", "tf", ".", "math", ".", "reduce_sum", "(", "text_len", ")", "# the value should be num_subtoken_in_doc ", "\n", "\n", "input_ids", "=", "tf", ".", "reshape", "(", "flat_input_ids", ",", "[", "-", "1", ",", "self", ".", "config", ".", "window_size", "]", ")", "# (num_window, window_size)", "\n", "input_mask", "=", "tf", ".", "ones_like", "(", "input_ids", ",", "tf", ".", "int32", ")", "# (num_window, window_size)", "\n", "\n", "model", "=", "modeling", ".", "BertModel", "(", "config", "=", "self", ".", "bert_config", ",", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "input_mask", "=", "input_mask", ",", "\n", "use_one_hot_embeddings", "=", "False", ",", "scope", "=", "'bert'", ")", "\n", "\n", "doc_overlap_window_embs", "=", "model", ".", "get_sequence_output", "(", ")", "# (num_window, window_size, hidden_size)", "\n", "doc_overlap_input_mask", "=", "tf", ".", "reshape", "(", "flat_doc_overlap_input_mask", ",", "[", "self", ".", "config", ".", "num_window", ",", "self", ".", "config", ".", "window_size", "]", ")", "# (num_window, window_size)", "\n", "\n", "doc_flat_embs", "=", "self", ".", "transform_overlap_windows_to_original_doc", "(", "doc_overlap_window_embs", ",", "doc_overlap_input_mask", ")", "\n", "doc_flat_embs", "=", "tf", ".", "reshape", "(", "doc_flat_embs", ",", "[", "-", "1", ",", "self", ".", "config", ".", "hidden_size", "]", ")", "# (num_subtoken_in_doc, hidden_size)", "\n", "\n", "expand_start_embs", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "doc_flat_embs", ",", "1", ")", ",", "[", "1", ",", "num_subtoken_in_doc", ",", "1", "]", ")", "# (num_subtoken_in_doc, num_subtoken_in_doc, hidden_size)", "\n", "expand_end_embs", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "doc_flat_embs", ",", "0", ")", ",", "[", "num_subtoken_in_doc", ",", "1", ",", "1", "]", ")", "# (num_subtoken_in_doc, num_subtoken_in_doc, hidden_size)", "\n", "expand_mention_span_embs", "=", "tf", ".", "concat", "(", "[", "expand_start_embs", ",", "expand_end_embs", "]", ",", "axis", "=", "-", "1", ")", "# (num_subtoken_in_doc, num_subtoken_in_doc, 2*hidden_size)", "\n", "expand_mention_span_embs", "=", "tf", ".", "reshape", "(", "expand_mention_span_embs", ",", "[", "-", "1", ",", "self", ".", "config", ".", "hidden_size", "*", "2", "]", ")", "\n", "span_sequence_logits", "=", "self", ".", "ffnn", "(", "expand_mention_span_embs", ",", "self", ".", "config", ".", "hidden_size", "*", "2", ",", "1", ",", "dropout", "=", "self", ".", "dropout", ",", "name_scope", "=", "\"mention_span\"", ")", "# (num_subtoken_in_doc * num_subtoken_in_doc)", "\n", "\n", "if", "self", ".", "config", ".", "start_end_share", ":", "\n", "            ", "start_end_sequence_logits", "=", "self", ".", "ffnn", "(", "doc_flat_embs", ",", "self", ".", "config", ".", "hidden_size", ",", "2", ",", "dropout", "=", "self", ".", "dropout", ",", "name_scope", "=", "\"mention_start_end\"", ")", "# (num_subtoken_in_doc, 2)", "\n", "start_sequence_logits", ",", "end_sequence_logits", "=", "tf", ".", "split", "(", "start_end_sequence_logits", ",", "axis", "=", "1", ")", "\n", "# start_sequence_logits -> (num_subtoken_in_doc, 1)", "\n", "# end_sequence_logits -> (num_subtoken_in_doc, 1)", "\n", "", "else", ":", "\n", "            ", "start_sequence_logits", "=", "self", ".", "ffnn", "(", "doc_flat_embs", ",", "self", ".", "config", ".", "hidden_size", ",", "1", ",", "dropout", "=", "self", ".", "dropout", ",", "name_scope", "=", "\"mention_start\"", ")", "# (num_subtoken_in_doc)", "\n", "end_sequence_logits", "=", "self", ".", "ffnn", "(", "doc_flat_embs", ",", "self", ".", "config", ".", "hidden_size", ",", "1", ",", "dropout", "=", "self", ".", "dropout", ",", "name_scope", "=", "\"mention_end\"", ")", "# (num_subtoken_in_doc)", "\n", "\n", "", "gold_start_sequence_labels", "=", "self", ".", "scatter_gold_index_to_label_sequence", "(", "gold_start_index_labels", ",", "num_subtoken_in_doc", ")", "# (num_subtoken_in_doc)", "\n", "gold_end_sequence_labels", "=", "self", ".", "scatter_gold_index_to_label_sequence", "(", "gold_end_index_labels", ",", "num_subtoken_in_doc", ")", "# (num_subtoken_in_doc)", "\n", "\n", "start_loss", ",", "start_sequence_probabilities", "=", "self", ".", "compute_score_and_loss", "(", "start_sequence_logits", ",", "gold_start_sequence_labels", ")", "\n", "end_loss", ",", "end_sequence_probabilities", "=", "self", ".", "compute_score_and_loss", "(", "end_sequence_logits", ",", "gold_end_sequence_labels", ")", "\n", "# *_loss -> a scalar ", "\n", "# *_sequence_scores -> (num_subtoken_in_doc)", "\n", "\n", "gold_span_sequence_labels", "=", "self", ".", "scatter_span_sequence_labels", "(", "gold_start_index_labels", ",", "gold_end_index_labels", ",", "num_subtoken_in_doc", ")", "# (num_subtoken_in_doc * num_subtoken_in_doc)", "\n", "span_loss", ",", "span_sequence_probabilities", "=", "self", ".", "compute_score_and_loss", "(", "span_sequence_logits", ",", "gold_span_sequence_labels", ")", "\n", "# span_loss -> a scalar ", "\n", "# span_sequence_probabilities -> (num_subtoken_in_doc * num_subtoken_in_doc)", "\n", "\n", "total_loss", "=", "self", ".", "config", ".", "loss_start_ratio", "*", "start_loss", "+", "self", ".", "config", ".", "loss_end_ratio", "*", "end_loss", "+", "self", ".", "config", ".", "loss_span_ratio", "*", "span_loss", "\n", "return", "total_loss", ",", "start_sequence_probabilities", ",", "end_sequence_probabilities", ",", "span_sequence_probabilities", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_gold_mention_sequence_labels_from_pad_index": [[109, 144], ["tensorflow.math.maximum", "tensorflow.math.reduce_sum", "tensorflow.cast", "mention_proposal.MentionProposalModel.boolean_mask_1d", "mention_proposal.MentionProposalModel.boolean_mask_1d", "mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence", "mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence", "mention_proposal.MentionProposalModel.scatter_span_sequence_labels", "tensorflow.zeros_like", "tensorflow.math.greater_equal", "tensorflow.zeros_like"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_span_sequence_labels"], ["", "def", "get_gold_mention_sequence_labels_from_pad_index", "(", "self", ",", "pad_gold_start_index_labels", ",", "pad_gold_end_index_labels", ",", "pad_text_len", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            the original gold labels is padded to the fixed length and only contains the position index of gold mentions. \n            return the gold sequence of labels for evaluation. \n        Args:\n            pad_gold_start_index_labels: a tf.int32 tensor with a fixed length (self.config.max_num_mention). \n                every element in the tensor is the start position index for the mentions. \n            pad_gold_end_index_labels: a tf.int32 tensor with a fixed length (self.config.max_num_mention). \n                every element in the tensor is the end position index of the mentions. \n            pad_text_len: a tf.int32 tensor with a fixed length (self.config.num_window). \n                every positive element in the tensor indicates that the number of subtokens in the window. \n        Returns:\n            gold_start_sequence_labels: a tf.int32 tensor with the shape of (num_subtoken_in_doc). \n                if the element in the tensor equals to 0, this subtoken is not a start for a mention. \n                if the elemtn in the tensor equals to 1, this subtoken is a start for a mention.  \n            gold_end_sequence_labels: a tf.int32 tensor with the shape of (num_subtoken_in_doc). \n                if the element in the tensor equals to 0, this subtoken is not a end for a mention. \n                if the elemtn in the tensor equals to 1, this subtoken is a end for a mention.  \n            gold_span_sequence_labels: a tf.int32 tensor with the shape of (num_subtoken_in_doc, num_subtoken_in_doc)/ \n                if the element[i][j] equals to 0, this subtokens from $i$ to $j$ is not a mention. \n                if the element[i][j] equals to 1, this subtokens from $i$ to $j$ is a mention. \n        \"\"\"", "\n", "text_len", "=", "tf", ".", "math", ".", "maximum", "(", "pad_text_len", ",", "tf", ".", "zeros_like", "(", "pad_text_len", ",", "tf", ".", "int32", ")", ")", "# (num_of_non_empty_window)", "\n", "num_subtoken_in_doc", "=", "tf", ".", "math", ".", "reduce_sum", "(", "text_len", ")", "# the value should be num_subtoken_in_doc ", "\n", "\n", "gold_start_end_mask", "=", "tf", ".", "cast", "(", "tf", ".", "math", ".", "greater_equal", "(", "pad_gold_start_index_labels", ",", "tf", ".", "zeros_like", "(", "pad_gold_start_index_labels", ",", "tf", ".", "int32", ")", ")", ",", "tf", ".", "bool", ")", "# (max_num_mention)", "\n", "gold_start_index_labels", "=", "self", ".", "boolean_mask_1d", "(", "pad_gold_start_index_labels", ",", "gold_start_end_mask", ",", "name_scope", "=", "\"gold_starts\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "# (num_of_mention)", "\n", "gold_end_index_labels", "=", "self", ".", "boolean_mask_1d", "(", "pad_gold_end_index_labels", ",", "gold_start_end_mask", ",", "name_scope", "=", "\"gold_ends\"", ",", "use_tpu", "=", "self", ".", "use_tpu", ")", "# (num_of_mention)", "\n", "\n", "gold_start_sequence_labels", "=", "self", ".", "scatter_gold_index_to_label_sequence", "(", "gold_start_index_labels", ",", "num_subtoken_in_doc", ")", "# (num_subtoken_in_doc)", "\n", "gold_end_sequence_labels", "=", "self", ".", "scatter_gold_index_to_label_sequence", "(", "gold_end_index_labels", ",", "num_subtoken_in_doc", ")", "# (num_subtoken_in_doc)", "\n", "gold_span_sequence_labels", "=", "self", ".", "scatter_span_sequence_labels", "(", "gold_start_index_labels", ",", "gold_end_index_labels", ",", "num_subtoken_in_doc", ")", "# (num_subtoken_in_doc, num_subtoken_in_doc)", "\n", "\n", "return", "gold_start_sequence_labels", ",", "gold_end_sequence_labels", ",", "gold_span_sequence_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_gold_index_to_label_sequence": [[146, 161], ["tensorflow.reshape", "tensorflow.reshape", "tensorflow.Variable", "tensorflow.reshape", "tensorflow.cast", "tensorflow.ones_like", "tensorflow.scatter_nd"], "methods", ["None"], ["", "def", "scatter_gold_index_to_label_sequence", "(", "self", ",", "gold_index_labels", ",", "expect_length_of_labels", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            transform the mention start/end position index tf.int32 Tensor to a tf.int32 Tensor with 1/0 labels for the input subtoken sequences.\n            1 denotes this subtoken is the start/end for a mention. \n        Args:\n            gold_index_labels: a tf.int32 Tensor with mention start/end position index in the original document. \n            expect_length_of_labels: the number of subtokens in the original document. \n        \"\"\"", "\n", "gold_labels_pos", "=", "tf", ".", "reshape", "(", "gold_index_labels", ",", "[", "-", "1", ",", "1", "]", ")", "# (num_of_mention, 1)", "\n", "gold_value", "=", "tf", ".", "reshape", "(", "tf", ".", "ones_like", "(", "gold_index_labels", ")", ",", "[", "-", "1", "]", ")", "# (num_of_mention)", "\n", "label_shape", "=", "tf", ".", "Variable", "(", "expect_length_of_labels", ")", "\n", "label_shape", "=", "tf", ".", "reshape", "(", "label_shape", ",", "[", "1", "]", ")", "# [1]", "\n", "gold_sequence_labels", "=", "tf", ".", "cast", "(", "tf", ".", "scatter_nd", "(", "gold_labels_pos", ",", "gold_value", ",", "label_shape", ")", ",", "tf", ".", "int32", ")", "# (num_subtoken_in_doc)", "\n", "return", "gold_sequence_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.scatter_span_sequence_labels": [[163, 182], ["tensorflow.stack", "tensorflow.reshape", "tensorflow.Variable", "tensorflow.reshape", "tensorflow.cast", "tensorflow.ones_like", "tensorflow.scatter_nd"], "methods", ["None"], ["", "def", "scatter_span_sequence_labels", "(", "self", ",", "gold_start_index_labels", ",", "gold_end_index_labels", ",", "expect_length_of_labels", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            transform the mention (start, end) position pairs to a span matrix gold_span_sequence_labels. \n                matrix[i][j]: whether the subtokens between the position $i$ to $j$ can be a mention.  \n                if matrix[i][j] == 0: from $i$ to $j$ is not a mention. \n                if matrix[i][j] == 1: from $i$ to $j$ is a mention.\n        Args:\n            gold_start_index_labels: a tf.int32 Tensor with mention start position index in the original document. \n            gold_end_index_labels: a tf.int32 Tensor with mention end position index in the original document. \n            expect_length_of_labels: a scalar, should be the same with num_subtoken_in_doc\n        \"\"\"", "\n", "gold_span_index_labels", "=", "tf", ".", "stack", "(", "[", "gold_start_index_labels", ",", "gold_end_index_labels", "]", ",", "axis", "=", "1", ")", "# (num_of_mention, 2)", "\n", "gold_span_value", "=", "tf", ".", "reshape", "(", "tf", ".", "ones_like", "(", "gold_start_index_labels", ",", "tf", ".", "int32", ")", ",", "[", "-", "1", "]", ")", "# (num_of_mention)", "\n", "gold_span_label_shape", "=", "tf", ".", "Variable", "(", "[", "expect_length_of_labels", ",", "expect_length_of_labels", "]", ")", "\n", "gold_span_label_shape", "=", "tf", ".", "reshape", "(", "gold_span_label_shape", ",", "[", "-", "1", "]", ")", "\n", "\n", "gold_span_sequence_labels", "=", "tf", ".", "cast", "(", "tf", ".", "scatter_nd", "(", "gold_span_index_labels", ",", "gold_span_value", ",", "gold_span_label_shape", ")", ",", "tf", ".", "int32", ")", "# (num_subtoken_in_doc, num_subtoken_in_doc)", "\n", "return", "gold_span_sequence_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.compute_score_and_loss": [[184, 209], ["tensorflow.cast", "tensorflow.stack", "tensorflow.cast", "tensorflow.keras.losses.binary_crossentropy", "tensorflow.reduce_mean", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.multiply", "tensorflow.sigmoid", "tensorflow.reshape", "tensorflow.cast"], "methods", ["None"], ["", "def", "compute_score_and_loss", "(", "self", ",", "pred_sequence_logits", ",", "gold_sequence_labels", ",", "loss_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            compute the unifrom start/end loss and probabilities. \n        Args:\n            pred_sequence_logits: (input_shape, 1) \n            gold_sequence_labels: (input_shape, 1)\n            loss_mask: [optional] if is not None, it should be (input_shape). should be tf.int32 0/1 tensor. \n            FOR start/end score and loss, input_shape should be num_subtoken_in_doc.\n            FOR span score and loss, input_shape should be num_subtoken_in_doc * num_subtoken_in_doc. \n        \"\"\"", "\n", "pred_sequence_probabilities", "=", "tf", ".", "cast", "(", "tf", ".", "reshape", "(", "tf", ".", "sigmoid", "(", "pred_sequence_logits", ")", ",", "[", "-", "1", "]", ")", ",", "tf", ".", "float32", ")", "# (input_shape)", "\n", "expand_pred_sequence_scores", "=", "tf", ".", "stack", "(", "[", "(", "1", "-", "pred_sequence_probabilities", ")", ",", "pred_sequence_probabilities", "]", ",", "axis", "=", "-", "1", ")", "# (input_shape, 2)", "\n", "expand_gold_sequence_labels", "=", "tf", ".", "cast", "(", "tf", ".", "one_hot", "(", "tf", ".", "reshape", "(", "gold_sequence_labels", ",", "[", "-", "1", "]", ")", ",", "2", ",", "axis", "=", "-", "1", ")", ",", "tf", ".", "float32", ")", "# (input_shape, 2)", "\n", "\n", "loss", "=", "tf", ".", "keras", ".", "losses", ".", "binary_crossentropy", "(", "expand_gold_sequence_labels", ",", "expand_pred_sequence_scores", ")", "\n", "# loss -> shape is (input_shape)", "\n", "\n", "if", "loss_mask", "is", "not", "None", ":", "\n", "            ", "loss", "=", "tf", ".", "multiply", "(", "loss", ",", "tf", ".", "cast", "(", "loss_mask", ",", "tf", ".", "float32", ")", ")", "\n", "\n", "", "total_loss", "=", "tf", ".", "reduce_mean", "(", "loss", ")", "\n", "# total_loss -> a scalar ", "\n", "\n", "return", "total_loss", ",", "pred_sequence_probabilities", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.transform_overlap_windows_to_original_doc": [[211, 233], ["tensorflow.ones_like", "tensorflow.math.cumsum", "tensorflow.tile", "tensorflow.math.multiply", "tensorflow.reshape", "mention_proposal.MentionProposalModel.boolean_mask_1d", "tensorflow.reshape", "tensorflow.gather", "tensorflow.expand_dims", "tensorflow.math.greater", "tensorflow.zeros_like", "tensorflow.range"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d"], ["", "def", "transform_overlap_windows_to_original_doc", "(", "self", ",", "doc_overlap_window_embs", ",", "doc_overlap_input_mask", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            hidden_size should be equal to embeddding_size. \n        Args:\n            doc_overlap_window_embs: (num_window, window_size, hidden_size). \n                the output of (num_window, window_size) input_ids forward into BERT model. \n            doc_overlap_input_mask: (num_window, window_size). A tf.int32 Tensor contains 0/1. \n                0 represents token in this position should be neglected. 1 represents token in this position should be reserved. \n        \"\"\"", "\n", "ones_input_mask", "=", "tf", ".", "ones_like", "(", "doc_overlap_input_mask", ",", "tf", ".", "int32", ")", "# (num_window, window_size)", "\n", "cumsum_input_mask", "=", "tf", ".", "math", ".", "cumsum", "(", "ones_input_mask", ",", "axis", "=", "1", ")", "# (num_window, window_size)", "\n", "offset_input_mask", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "self", ".", "config", ".", "num_window", ")", "*", "self", ".", "config", ".", "window_size", ",", "1", ")", ",", "[", "1", ",", "self", ".", "config", ".", "window_size", "]", ")", "# (num_window, window_size)", "\n", "offset_cumsum_input_mask", "=", "offset_input_mask", "+", "cumsum_input_mask", "# (num_window, window_size)", "\n", "global_input_mask", "=", "tf", ".", "math", ".", "multiply", "(", "ones_input_mask", ",", "offset_cumsum_input_mask", ")", "# (num_window, window_size)", "\n", "global_input_mask", "=", "tf", ".", "reshape", "(", "global_input_mask", ",", "[", "-", "1", "]", ")", "# (num_window * window_size)", "\n", "global_input_mask_index", "=", "self", ".", "boolean_mask_1d", "(", "global_input_mask", ",", "tf", ".", "math", ".", "greater", "(", "global_input_mask", ",", "tf", ".", "zeros_like", "(", "global_input_mask", ",", "tf", ".", "int32", ")", ")", ")", "# (num_subtoken_in_doc)", "\n", "\n", "doc_overlap_window_embs", "=", "tf", ".", "reshape", "(", "doc_overlap_window_embs", ",", "[", "-", "1", ",", "self", ".", "config", ".", "hidden_size", "]", ")", "# (num_window * window_size, hidden_size)", "\n", "original_doc_embs", "=", "tf", ".", "gather", "(", "doc_overlap_window_embs", ",", "global_input_mask_index", ")", "# (num_subtoken_in_doc, hidden_size)", "\n", "\n", "return", "original_doc_embs", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.ffnn": [[235, 254], ["tensorflow.truncated_normal_initializer", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.relu", "tensorflow.nn.xw_plus_b", "tensorflow.nn.dropout", "tensorflow.zeros_initializer"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.dropout"], ["", "def", "ffnn", "(", "self", ",", "inputs", ",", "hidden_size", ",", "output_size", ",", "dropout", "=", "None", ",", "name_scope", "=", "\"fully-conntected-neural-network\"", ",", "\n", "hidden_initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            fully-connected neural network. \n            transform non-linearly the [input] tensor with [hidden_size] to a fix [output_size] size.  \n        Args: \n            hidden_size: should be the size of last dimension of [inputs]. \n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "name_scope", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "            ", "hidden_weights", "=", "tf", ".", "get_variable", "(", "\"hidden_weights\"", ",", "[", "hidden_size", ",", "output_size", "]", ",", "\n", "initializer", "=", "hidden_initializer", ")", "\n", "hidden_bias", "=", "tf", ".", "get_variable", "(", "\"hidden_bias\"", ",", "[", "output_size", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "outputs", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "inputs", ",", "hidden_weights", ",", "hidden_bias", ")", ")", "\n", "\n", "if", "dropout", "is", "not", "None", ":", "\n", "                ", "outputs", "=", "tf", ".", "nn", ".", "dropout", "(", "outputs", ",", "dropout", ")", "\n", "\n", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_dropout": [[256, 258], ["tensorflow.to_float"], "methods", ["None"], ["", "def", "get_dropout", "(", "self", ",", "dropout_rate", ",", "is_training", ")", ":", "\n", "        ", "return", "1", "-", "(", "tf", ".", "to_float", "(", "is_training", ")", "*", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape": [[260, 266], ["tensorflow.shape", "x.get_shape"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape"], ["", "def", "get_shape", "(", "self", ",", "x", ",", "dim", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            return the size of input x in DIM. \n        \"\"\"", "\n", "return", "x", ".", "get_shape", "(", ")", "[", "dim", "]", ".", "value", "or", "tf", ".", "shape", "(", "x", ")", "[", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.boolean_mask_1d": [[268, 293], ["tensorflow.name_scope", "tensorflow.reduce_sum", "tensorflow.cast", "tensorflow.cast", "tensorflow.one_hot", "tensorflow.cast", "tensorflow.reshape", "tensorflow.gather", "tensorflow.boolean_mask", "tensorflow.cast", "tensorflow.multiply", "tensorflow.tensordot", "tensorflow.cumsum", "tensorflow.cast", "tensorflow.range", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape"], ["", "def", "boolean_mask_1d", "(", "self", ",", "itemtensor", ",", "boolmask_indicator", ",", "name_scope", "=", "\"boolean_mask1d\"", ",", "use_tpu", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Desc:\n            the same functionality of tf.boolean_mask. \n            The tf.boolean_mask operation is not available on the cloud TPU. \n        Args:\n            itemtensor : a Tensor contains [tf.int32, tf.float32] numbers. Should be 1-Rank.\n            boolmask_indicator : a tf.bool Tensor. Should be 1-Rank. \n            scope : name scope for the operation. \n            use_tpu : if False, return tf.boolean_mask.  \n        \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "name_scope", ")", ":", "\n", "            ", "if", "not", "use_tpu", ":", "\n", "                ", "return", "tf", ".", "boolean_mask", "(", "itemtensor", ",", "boolmask_indicator", ")", "\n", "\n", "", "boolmask_sum", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "cast", "(", "boolmask_indicator", ",", "tf", ".", "int32", ")", ")", "\n", "selected_positions", "=", "tf", ".", "cast", "(", "boolmask_indicator", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "indexed_positions", "=", "tf", ".", "cast", "(", "tf", ".", "multiply", "(", "tf", ".", "cumsum", "(", "selected_positions", ")", ",", "selected_positions", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "one_hot_selector", "=", "tf", ".", "one_hot", "(", "indexed_positions", "-", "1", ",", "boolmask_sum", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "sampled_indices", "=", "tf", ".", "cast", "(", "tf", ".", "tensordot", "(", "tf", ".", "cast", "(", "tf", ".", "range", "(", "tf", ".", "shape", "(", "boolmask_indicator", ")", "[", "0", "]", ")", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "\n", "one_hot_selector", ",", "axes", "=", "[", "0", ",", "0", "]", ")", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "sampled_indices", "=", "tf", ".", "reshape", "(", "sampled_indices", ",", "[", "-", "1", "]", ")", "\n", "mask_itemtensor", "=", "tf", ".", "gather", "(", "itemtensor", ",", "sampled_indices", ")", "\n", "\n", "return", "mask_itemtensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape": [[12, 14], ["tensorflow.shape", "x.get_shape"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.models.mention_proposal.MentionProposalModel.get_shape"], ["def", "shape", "(", "x", ",", "dim", ")", ":", "\n", "    ", "return", "x", ".", "get_shape", "(", ")", "[", "dim", "]", ".", "value", "or", "tf", ".", "shape", "(", "x", ")", "[", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.model_fn.model_fn": [[13, 21], ["print", "print", "print"], "function", ["None"], ["def", "model_fn", "(", "config", ")", ":", "\n", "\n", "    ", "def", "mention_proposal_fn", "(", ")", ":", "\n", "        ", "print", "(", "\"the number of document is : \"", ")", "\n", "print", "(", "config", ".", "document_number", ")", "\n", "print", "(", "config", ".", "number_window_size", ")", "\n", "\n", "", "return", "mention_proposal_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.__init__": [[34, 81], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "16", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n          vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n          hidden_size: Size of the encoder layers and the pooler layer.\n          num_hidden_layers: Number of hidden layers in the Transformer encoder.\n          num_attention_heads: Number of attention heads for each attention layer in\n            the Transformer encoder.\n          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n            layer in the Transformer encoder.\n          hidden_act: The non-linear activation function (function or string) in the\n            encoder and pooler.\n          hidden_dropout_prob: The dropout probability for all fully connected\n            layers in the embeddings, encoder, and pooler.\n          attention_probs_dropout_prob: The dropout ratio for the attention\n            probabilities.\n          max_position_embeddings: The maximum sequence length that this model might\n            ever be used with. Typically set this to something large just in case\n            (e.g., 512 or 1024 or 2048).\n          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n            `BertModel`.\n          initializer_range: The stdev of the truncated_normal_initializer for\n            initializing all weight matrices.\n        \"\"\"", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.from_dict": [[82, 89], ["modeling.BertConfig", "six.iteritems"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size", "=", "None", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "six", ".", "iteritems", "(", "json_object", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.from_json_file": [[90, 96], ["cls.from_dict", "tensorflow.gfile.GFile", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "json_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.to_dict": [[97, 101], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.to_json_string": [[102, 105], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.__init__": [[131, 238], ["copy.deepcopy", "modeling.get_shape_list", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.ones", "tensorflow.zeros", "tensorflow.variable_scope", "tensorflow.variable_scope", "modeling.embedding_lookup", "modeling.embedding_postprocessor", "tensorflow.variable_scope", "modeling.create_attention_mask_from_input_mask", "modeling.transformer_model", "tensorflow.variable_scope", "tensorflow.squeeze", "tensorflow.layers.dense", "modeling.get_activation", "modeling.create_initializer"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.embedding_lookup", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.embedding_postprocessor", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_attention_mask_from_input_mask", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.transformer_model", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_activation", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_initializer"], ["def", "__init__", "(", "self", ",", "\n", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "use_one_hot_embeddings", "=", "True", ",", "\n", "scope", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructor for BertModel.\n\n        Args:\n          config: `BertConfig` instance.\n          is_training: bool. rue for training model, false for eval model. Controls\n            whether dropout will be applied.\n          input_ids: int32 Tensor of shape [batch_size, seq_length].\n          input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n          token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n          use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n            embeddings or tf.embedding_lookup() for the word embeddings. On the TPU,\n            it is must faster if this is True, on the CPU or GPU, it is faster if\n            this is False.\n          scope: (optional) variable scope. Defaults to \"bert\".\n\n        Raises:\n          ValueError: The config is invalid or one of the input tensor shapes\n            is invalid.\n        \"\"\"", "\n", "config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "config", ".", "hidden_dropout_prob", "=", "tf", ".", "to_float", "(", "is_training", ")", "*", "config", ".", "hidden_dropout_prob", "\n", "config", ".", "attention_probs_dropout_prob", "=", "tf", ".", "to_float", "(", "is_training", ")", "*", "config", ".", "attention_probs_dropout_prob", "\n", "# config.hidden_dropout_prob = tf.Print(config.hidden_dropout_prob, [config.hidden_dropout_prob], 'hdden')", "\n", "# if not is_training:", "\n", "# config.hidden_dropout_prob = 0.0", "\n", "# config.attention_probs_dropout_prob = 0.0", "\n", "\n", "input_shape", "=", "get_shape_list", "(", "input_ids", ",", "expected_rank", "=", "2", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "input_mask", "is", "None", ":", "\n", "            ", "input_mask", "=", "tf", ".", "ones", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "tf", ".", "zeros", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"bert\"", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"embeddings\"", ")", ":", "\n", "# Perform embedding lookup on the word ids.", "\n", "                ", "(", "self", ".", "embedding_output", ",", "self", ".", "embedding_table", ")", "=", "embedding_lookup", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "vocab_size", "=", "config", ".", "vocab_size", ",", "\n", "embedding_size", "=", "config", ".", "hidden_size", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "# Add positional embeddings and token type embeddings, then layer", "\n", "# normalize and perform dropout.", "\n", "self", ".", "embedding_output", "=", "embedding_postprocessor", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "use_token_type", "=", "True", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "token_type_vocab_size", "=", "config", ".", "type_vocab_size", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", ",", "\n", "dropout_prob", "=", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ")", ":", "\n", "# This converts a 2D mask of shape [batch_size, seq_length] to a 3D", "\n", "# mask of shape [batch_size, seq_length, seq_length] which is used", "\n", "# for the attention scores.", "\n", "                ", "attention_mask", "=", "create_attention_mask_from_input_mask", "(", "\n", "input_ids", ",", "input_mask", ")", "\n", "\n", "# Run the stacked transformer.", "\n", "# `sequence_output` shape = [batch_size, seq_length, hidden_size].", "\n", "self", ".", "all_encoder_layers", "=", "transformer_model", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "hidden_size", "=", "config", ".", "hidden_size", ",", "\n", "num_hidden_layers", "=", "config", ".", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads", ",", "\n", "intermediate_size", "=", "config", ".", "intermediate_size", ",", "\n", "intermediate_act_fn", "=", "get_activation", "(", "config", ".", "hidden_act", ")", ",", "\n", "hidden_dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "do_return_all_layers", "=", "True", ")", "\n", "\n", "", "self", ".", "sequence_output", "=", "self", ".", "all_encoder_layers", "[", "-", "1", "]", "\n", "# The \"pooler\" converts the encoded sequence tensor of shape", "\n", "# [batch_size, seq_length, hidden_size] to a tensor of shape", "\n", "# [batch_size, hidden_size]. This is necessary for segment-level", "\n", "# (or segment-pair-level) classification tasks where we need a fixed", "\n", "# dimensional representation of the segment.", "\n", "with", "tf", ".", "variable_scope", "(", "\"pooler\"", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token. We assume that this has been pre-trained", "\n", "                ", "first_token_tensor", "=", "tf", ".", "squeeze", "(", "self", ".", "sequence_output", "[", ":", ",", "0", ":", "1", ",", ":", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "pooled_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "first_token_tensor", ",", "\n", "config", ".", "hidden_size", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "config", ".", "initializer_range", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_pooled_output": [[239, 241], ["None"], "methods", ["None"], ["", "", "", "def", "get_pooled_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_sequence_output": [[242, 250], ["None"], "methods", ["None"], ["", "def", "get_sequence_output", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets final hidden layer of encoder.\n\n        Returns:\n          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n          to the final hidden of the transformer encoder.\n        \"\"\"", "\n", "return", "self", ".", "sequence_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_all_encoder_layers": [[251, 253], ["None"], "methods", ["None"], ["", "def", "get_all_encoder_layers", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_embedding_output": [[254, 264], ["None"], "methods", ["None"], ["", "def", "get_embedding_output", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n        Returns:\n          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n          to the output of the embedding layer, after summing the word\n          embeddings with the positional embeddings and the token type embeddings,\n          then performing layer normalization. This is the input to the transformer.\n        \"\"\"", "\n", "return", "self", ".", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.BertModel.get_embedding_table": [[265, 267], ["None"], "methods", ["None"], ["", "def", "get_embedding_table", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embedding_table", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.gelu": [[269, 283], ["tensorflow.erf", "tensorflow.sqrt"], "function", ["None"], ["", "", "def", "gelu", "(", "input_tensor", ")", ":", "\n", "    ", "\"\"\"Gaussian Error Linear Unit.\n\n    This is a smoother version of the RELU.\n    Original paper: https://arxiv.org/abs/1606.08415\n\n    Args:\n      input_tensor: float Tensor to perform activation.\n\n    Returns:\n      `input_tensor` with the GELU activation applied.\n    \"\"\"", "\n", "cdf", "=", "0.5", "*", "(", "1.0", "+", "tf", ".", "erf", "(", "input_tensor", "/", "tf", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "return", "input_tensor", "*", "cdf", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_activation": [[285, 320], ["activation_string.lower", "isinstance", "ValueError"], "function", ["None"], ["", "def", "get_activation", "(", "activation_string", ")", ":", "\n", "    ", "\"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n\n    Args:\n      activation_string: String name of the activation function.\n\n    Returns:\n      A Python function corresponding to the activation function. If\n      `activation_string` is None, empty, or \"linear\", this will return None.\n      If `activation_string` is not a string, it will return `activation_string`.\n\n    Raises:\n      ValueError: The `activation_string` does not correspond to a known\n        activation.\n    \"\"\"", "\n", "\n", "# We assume that anything that\"s not a string is already an activation", "\n", "# function, so we just return it.", "\n", "if", "not", "isinstance", "(", "activation_string", ",", "six", ".", "string_types", ")", ":", "\n", "        ", "return", "activation_string", "\n", "\n", "", "if", "not", "activation_string", ":", "\n", "        ", "return", "None", "\n", "\n", "", "act", "=", "activation_string", ".", "lower", "(", ")", "\n", "if", "act", "==", "\"linear\"", ":", "\n", "        ", "return", "None", "\n", "", "elif", "act", "==", "\"relu\"", ":", "\n", "        ", "return", "tf", ".", "nn", ".", "relu", "\n", "", "elif", "act", "==", "\"gelu\"", ":", "\n", "        ", "return", "gelu", "\n", "", "elif", "act", "==", "\"tanh\"", ":", "\n", "        ", "return", "tf", ".", "tanh", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unsupported activation: %s\"", "%", "act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_assignment_map_from_checkpoint": [[322, 346], ["collections.OrderedDict", "tensorflow.train.list_variables", "collections.OrderedDict", "re.match", "re.match.group"], "function", ["None"], ["", "", "def", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", ":", "\n", "    ", "\"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"", "\n", "initialized_variable_names", "=", "{", "}", "\n", "\n", "name_to_variable", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "        ", "name", "=", "var", ".", "name", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "            ", "name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "name_to_variable", "[", "name", "]", "=", "var", "\n", "\n", "", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "init_checkpoint", ")", "# checkpoint variables,", "\n", "\n", "assignment_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "x", "in", "init_vars", ":", "\n", "        ", "(", "name", ",", "var", ")", "=", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "\n", "if", "name", "not", "in", "name_to_variable", ":", "\n", "            ", "continue", "\n", "", "assignment_map", "[", "name", "]", "=", "name", "\n", "initialized_variable_names", "[", "name", "]", "=", "1", "\n", "initialized_variable_names", "[", "name", "+", "\":0\"", "]", "=", "1", "\n", "\n", "", "return", "assignment_map", ",", "initialized_variable_names", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.dropout": [[348, 364], ["tensorflow.nn.dropout"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.dropout"], ["", "def", "dropout", "(", "input_tensor", ",", "dropout_prob", ")", ":", "\n", "    ", "\"\"\"Perform dropout.\n\n    Args:\n      input_tensor: float Tensor.\n      dropout_prob: Python float. The probability of dropping out a value (NOT of\n        *keeping* a dimension as in `tf.nn.dropout`).\n\n    Returns:\n      A version of `input_tensor` with dropout applied.\n    \"\"\"", "\n", "if", "dropout_prob", "is", "None", "or", "dropout_prob", "==", "0.0", ":", "\n", "        ", "return", "input_tensor", "\n", "\n", "", "output", "=", "tf", ".", "nn", ".", "dropout", "(", "input_tensor", ",", "1.0", "-", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.layer_norm": [[366, 370], ["tensorflow.contrib.layers.layer_norm"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.layer_norm"], ["", "def", "layer_norm", "(", "input_tensor", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"", "\n", "return", "tf", ".", "contrib", ".", "layers", ".", "layer_norm", "(", "\n", "inputs", "=", "input_tensor", ",", "begin_norm_axis", "=", "-", "1", ",", "begin_params_axis", "=", "-", "1", ",", "scope", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.layer_norm_and_dropout": [[372, 377], ["modeling.layer_norm", "modeling.dropout"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.dropout"], ["", "def", "layer_norm_and_dropout", "(", "input_tensor", ",", "dropout_prob", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Runs layer normalization followed by dropout.\"\"\"", "\n", "output_tensor", "=", "layer_norm", "(", "input_tensor", ",", "name", ")", "\n", "output_tensor", "=", "dropout", "(", "output_tensor", ",", "dropout_prob", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_initializer": [[379, 382], ["tensorflow.truncated_normal_initializer"], "function", ["None"], ["", "def", "create_initializer", "(", "initializer_range", "=", "0.02", ")", ":", "\n", "    ", "\"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"", "\n", "return", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "initializer_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.embedding_lookup": [[384, 431], ["tensorflow.get_variable", "modeling.get_shape_list", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.nn.embedding_lookup", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.embedding_lookup", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_initializer"], ["", "def", "embedding_lookup", "(", "input_ids", ",", "\n", "vocab_size", ",", "\n", "embedding_size", "=", "128", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "False", ")", ":", "\n", "    ", "\"\"\"Looks up words embeddings for id tensor.\n\n    Args:\n      input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n        ids.\n      vocab_size: int. Size of the embedding vocabulary.\n      embedding_size: int. Width of the word embeddings.\n      initializer_range: float. Embedding initialization range.\n      word_embedding_name: string. Name of the embedding table.\n      use_one_hot_embeddings: bool. If True, use one-hot method for word\n        embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n        for TPUs.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, embedding_size].\n    \"\"\"", "\n", "# This function assumes that the input is of shape [batch_size, seq_length,", "\n", "# num_inputs].", "\n", "#", "\n", "# If the input is a 2D tensor of shape [batch_size, seq_length], we", "\n", "# reshape to [batch_size, seq_length, 1].", "\n", "if", "input_ids", ".", "shape", ".", "ndims", "==", "2", ":", "\n", "        ", "input_ids", "=", "tf", ".", "expand_dims", "(", "input_ids", ",", "axis", "=", "[", "-", "1", "]", ")", "\n", "\n", "", "embedding_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "word_embedding_name", ",", "\n", "shape", "=", "[", "vocab_size", ",", "embedding_size", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "if", "use_one_hot_embeddings", ":", "\n", "        ", "flat_input_ids", "=", "tf", ".", "reshape", "(", "input_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_input_ids", "=", "tf", ".", "one_hot", "(", "flat_input_ids", ",", "depth", "=", "vocab_size", ")", "\n", "output", "=", "tf", ".", "matmul", "(", "one_hot_input_ids", ",", "embedding_table", ")", "\n", "", "else", ":", "\n", "        ", "output", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_table", ",", "input_ids", ")", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ")", "\n", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "\n", "input_shape", "[", "0", ":", "-", "1", "]", "+", "[", "input_shape", "[", "-", "1", "]", "*", "embedding_size", "]", ")", "\n", "return", "(", "output", ",", "embedding_table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.embedding_postprocessor": [[433, 527], ["modeling.get_shape_list", "modeling.layer_norm_and_dropout", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.assert_less_equal", "ValueError", "tensorflow.control_dependencies", "tensorflow.get_variable", "tensorflow.slice", "len", "range", "position_broadcast_shape.extend", "tensorflow.reshape", "modeling.create_initializer", "layer_norm_and_dropout.shape.as_list", "position_broadcast_shape.append", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.layer_norm_and_dropout", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_initializer", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_initializer"], ["", "def", "embedding_postprocessor", "(", "input_tensor", ",", "\n", "use_token_type", "=", "False", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "token_type_vocab_size", "=", "16", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "dropout_prob", "=", "0.1", ")", ":", "\n", "    ", "\"\"\"Performs various post-processing on a word embedding tensor.\n\n    Args:\n      input_tensor: float Tensor of shape [batch_size, seq_length,\n        embedding_size].\n      use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n        Must be specified if `use_token_type` is True.\n      token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n      token_type_embedding_name: string. The name of the embedding table variable\n        for token type ids.\n      use_position_embeddings: bool. Whether to add position embeddings for the\n        position of each token in the sequence.\n      position_embedding_name: string. The name of the embedding table variable\n        for positional embeddings.\n      initializer_range: float. Range of the weight initialization.\n      max_position_embeddings: int. Maximum sequence length that might ever be\n        used with this model. This can be longer than the sequence length of\n        input_tensor, but cannot be shorter.\n      dropout_prob: float. Dropout probability applied to the final output tensor.\n\n    Returns:\n      float tensor with same shape as `input_tensor`.\n\n    Raises:\n      ValueError: One of the tensor shapes or input values is invalid.\n    \"\"\"", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "width", "=", "input_shape", "[", "2", "]", "\n", "\n", "output", "=", "input_tensor", "\n", "\n", "if", "use_token_type", ":", "\n", "        ", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"`token_type_ids` must be specified if\"", "\n", "\"`use_token_type` is True.\"", ")", "\n", "", "token_type_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "token_type_embedding_name", ",", "\n", "shape", "=", "[", "token_type_vocab_size", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# This vocab will be small so we always do one-hot here, since it is always", "\n", "# faster for a small vocabulary.", "\n", "flat_token_type_ids", "=", "tf", ".", "reshape", "(", "token_type_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_ids", "=", "tf", ".", "one_hot", "(", "flat_token_type_ids", ",", "depth", "=", "token_type_vocab_size", ")", "\n", "token_type_embeddings", "=", "tf", ".", "matmul", "(", "one_hot_ids", ",", "token_type_table", ")", "\n", "token_type_embeddings", "=", "tf", ".", "reshape", "(", "token_type_embeddings", ",", "\n", "[", "batch_size", ",", "seq_length", ",", "width", "]", ")", "\n", "output", "+=", "token_type_embeddings", "\n", "\n", "", "if", "use_position_embeddings", ":", "\n", "        ", "assert_op", "=", "tf", ".", "assert_less_equal", "(", "seq_length", ",", "max_position_embeddings", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "assert_op", "]", ")", ":", "\n", "            ", "full_position_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "position_embedding_name", ",", "\n", "shape", "=", "[", "max_position_embeddings", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# Since the position embedding table is a learned variable, we create it", "\n", "# using a (long) sequence length `max_position_embeddings`. The actual", "\n", "# sequence length might be shorter than this, for faster training of", "\n", "# tasks that do not have long sequences.", "\n", "#", "\n", "# So `full_position_embeddings` is effectively an embedding table", "\n", "# for position [0, 1, 2, ..., max_position_embeddings-1], and the current", "\n", "# sequence has positions [0, 1, 2, ... seq_length-1], so we can just", "\n", "# perform a slice.", "\n", "position_embeddings", "=", "tf", ".", "slice", "(", "full_position_embeddings", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "seq_length", ",", "-", "1", "]", ")", "\n", "num_dims", "=", "len", "(", "output", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "\n", "# Only the last two dimensions are relevant (`seq_length` and `width`), so", "\n", "# we broadcast among the first dimensions, which is typically just", "\n", "# the batch size.", "\n", "position_broadcast_shape", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_dims", "-", "2", ")", ":", "\n", "                ", "position_broadcast_shape", ".", "append", "(", "1", ")", "\n", "", "position_broadcast_shape", ".", "extend", "(", "[", "seq_length", ",", "width", "]", ")", "\n", "position_embeddings", "=", "tf", ".", "reshape", "(", "position_embeddings", ",", "\n", "position_broadcast_shape", ")", "\n", "output", "+=", "position_embeddings", "\n", "\n", "", "", "output", "=", "layer_norm_and_dropout", "(", "output", ",", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_attention_mask_from_input_mask": [[529, 561], ["modeling.get_shape_list", "modeling.get_shape_list", "tensorflow.cast", "tensorflow.ones", "tensorflow.reshape"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list"], ["", "def", "create_attention_mask_from_input_mask", "(", "from_tensor", ",", "to_mask", ")", ":", "\n", "    ", "\"\"\"Create 3D attention mask from a 2D tensor mask.\n\n    Args:\n      from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n      to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n    Returns:\n      float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n    \"\"\"", "\n", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "\n", "to_shape", "=", "get_shape_list", "(", "to_mask", ",", "expected_rank", "=", "2", ")", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "\n", "to_mask", "=", "tf", ".", "cast", "(", "\n", "tf", ".", "reshape", "(", "to_mask", ",", "[", "batch_size", ",", "1", ",", "to_seq_length", "]", ")", ",", "tf", ".", "float32", ")", "\n", "\n", "# We don't assume that `from_tensor` is a mask (although it could be). We", "\n", "# don't actually care if we attend *from* padding tokens (only *to* padding)", "\n", "# tokens so we create a tensor of all ones.", "\n", "#", "\n", "# `broadcast_ones` = [batch_size, from_seq_length, 1]", "\n", "broadcast_ones", "=", "tf", ".", "ones", "(", "\n", "shape", "=", "[", "batch_size", ",", "from_seq_length", ",", "1", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Here we broadcast along two dimensions to create the mask.", "\n", "mask", "=", "broadcast_ones", "*", "to_mask", "\n", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.attention_layer": [[563, 757], ["modeling.get_shape_list", "modeling.get_shape_list", "modeling.reshape_to_matrix", "modeling.reshape_to_matrix", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "modeling.attention_layer.transpose_for_scores"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.reshape_to_matrix"], ["", "def", "attention_layer", "(", "from_tensor", ",", "\n", "to_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "num_attention_heads", "=", "1", ",", "\n", "size_per_head", "=", "512", ",", "\n", "query_act", "=", "None", ",", "\n", "key_act", "=", "None", ",", "\n", "value_act", "=", "None", ",", "\n", "attention_probs_dropout_prob", "=", "0.0", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_2d_tensor", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "from_seq_length", "=", "None", ",", "\n", "to_seq_length", "=", "None", ")", ":", "\n", "    ", "\"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n    This is an implementation of multi-headed attention based on \"Attention\n    is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n    this is self-attention. Each timestep in `from_tensor` attends to the\n    corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n    This function first projects `from_tensor` into a \"query\" tensor and\n    `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n    of tensors of length `num_attention_heads`, where each tensor is of shape\n    [batch_size, seq_length, size_per_head].\n\n    Then, the query and key tensors are dot-producted and scaled. These are\n    softmaxed to obtain attention probabilities. The value tensors are then\n    interpolated by these probabilities, then concatenated back to a single\n    tensor and returned.\n\n    In practice, the multi-headed attention are done with transposes and\n    reshapes rather than actual separate tensors.\n\n    Args:\n      from_tensor: float Tensor of shape [batch_size, from_seq_length,\n        from_width].\n      to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n      attention_mask: (optional) int32 Tensor of shape [batch_size,\n        from_seq_length, to_seq_length]. The values should be 1 or 0. The\n        attention scores will effectively be set to -infinity for any positions in\n        the mask that are 0, and will be unchanged for positions that are 1.\n      num_attention_heads: int. Number of attention heads.\n      size_per_head: int. Size of each attention head.\n      query_act: (optional) Activation function for the query transform.\n      key_act: (optional) Activation function for the key transform.\n      value_act: (optional) Activation function for the value transform.\n      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n        attention probabilities.\n      initializer_range: float. Range of the weight initializer.\n      do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n        * from_seq_length, num_attention_heads * size_per_head]. If False, the\n        output will be of shape [batch_size, from_seq_length, num_attention_heads\n        * size_per_head].\n      batch_size: (Optional) int. If the input is 2D, this might be the batch size\n        of the 3D version of the `from_tensor` and `to_tensor`.\n      from_seq_length: (Optional) If the input is 2D, this might be the seq length\n        of the 3D version of the `from_tensor`.\n      to_seq_length: (Optional) If the input is 2D, this might be the seq length\n        of the 3D version of the `to_tensor`.\n\n    Returns:\n      float Tensor of shape [batch_size, from_seq_length,\n        num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n        true, this will be of shape [batch_size * from_seq_length,\n        num_attention_heads * size_per_head]).\n\n    Raises:\n      ValueError: Any of the arguments or tensor shapes are invalid.\n    \"\"\"", "\n", "\n", "def", "transpose_for_scores", "(", "input_tensor", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "seq_length", ",", "width", ")", ":", "\n", "        ", "output_tensor", "=", "tf", ".", "reshape", "(", "\n", "input_tensor", ",", "[", "batch_size", ",", "seq_length", ",", "num_attention_heads", ",", "width", "]", ")", "\n", "\n", "output_tensor", "=", "tf", ".", "transpose", "(", "output_tensor", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "return", "output_tensor", "\n", "\n", "", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "to_shape", "=", "get_shape_list", "(", "to_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "\n", "if", "len", "(", "from_shape", ")", "!=", "len", "(", "to_shape", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"The rank of `from_tensor` must match the rank of `to_tensor`.\"", ")", "\n", "\n", "", "if", "len", "(", "from_shape", ")", "==", "3", ":", "\n", "        ", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "", "elif", "len", "(", "from_shape", ")", "==", "2", ":", "\n", "        ", "if", "(", "batch_size", "is", "None", "or", "from_seq_length", "is", "None", "or", "to_seq_length", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"When passing in rank 2 tensors to attention_layer, the values \"", "\n", "\"for `batch_size`, `from_seq_length`, and `to_seq_length` \"", "\n", "\"must all be specified.\"", ")", "\n", "\n", "# Scalar dimensions referenced here:", "\n", "#   B = batch size (number of sequences)", "\n", "#   F = `from_tensor` sequence length", "\n", "#   T = `to_tensor` sequence length", "\n", "#   N = `num_attention_heads`", "\n", "#   H = `size_per_head`", "\n", "\n", "", "", "from_tensor_2d", "=", "reshape_to_matrix", "(", "from_tensor", ")", "\n", "to_tensor_2d", "=", "reshape_to_matrix", "(", "to_tensor", ")", "\n", "\n", "# `query_layer` = [B*F, N*H]", "\n", "query_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "from_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "query_act", ",", "\n", "name", "=", "\"query\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `key_layer` = [B*T, N*H]", "\n", "key_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "key_act", ",", "\n", "name", "=", "\"key\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `value_layer` = [B*T, N*H]", "\n", "value_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "value_act", ",", "\n", "name", "=", "\"value\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `query_layer` = [B, N, F, H]", "\n", "query_layer", "=", "transpose_for_scores", "(", "query_layer", ",", "batch_size", ",", "\n", "num_attention_heads", ",", "from_seq_length", ",", "\n", "size_per_head", ")", "\n", "\n", "# `key_layer` = [B, N, T, H]", "\n", "key_layer", "=", "transpose_for_scores", "(", "key_layer", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "to_seq_length", ",", "size_per_head", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw", "\n", "# attention scores.", "\n", "# `attention_scores` = [B, N, F, T]", "\n", "attention_scores", "=", "tf", ".", "matmul", "(", "query_layer", ",", "key_layer", ",", "transpose_b", "=", "True", ")", "\n", "attention_scores", "=", "tf", ".", "multiply", "(", "attention_scores", ",", "\n", "1.0", "/", "math", ".", "sqrt", "(", "float", "(", "size_per_head", ")", ")", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# `attention_mask` = [B, 1, F, T]", "\n", "        ", "attention_mask", "=", "tf", ".", "expand_dims", "(", "attention_mask", ",", "axis", "=", "[", "1", "]", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "adder", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "attention_mask", ",", "tf", ".", "float32", ")", ")", "*", "-", "10000.0", "\n", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_scores", "+=", "adder", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# `attention_probs` = [B, N, F, T]", "\n", "", "attention_probs", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "dropout", "(", "attention_probs", ",", "attention_probs_dropout_prob", ")", "\n", "\n", "# `value_layer` = [B, T, N, H]", "\n", "value_layer", "=", "tf", ".", "reshape", "(", "\n", "value_layer", ",", "\n", "[", "batch_size", ",", "to_seq_length", ",", "num_attention_heads", ",", "size_per_head", "]", ")", "\n", "\n", "# `value_layer` = [B, N, T, H]", "\n", "value_layer", "=", "tf", ".", "transpose", "(", "value_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "# `context_layer` = [B, N, F, H]", "\n", "context_layer", "=", "tf", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "# `context_layer` = [B, F, N, H]", "\n", "context_layer", "=", "tf", ".", "transpose", "(", "context_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "if", "do_return_2d_tensor", ":", "\n", "# `context_layer` = [B*F, N*V]", "\n", "        ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", "*", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "", "else", ":", "\n", "# `context_layer` = [B, F, N*V]", "\n", "        ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", ",", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "\n", "", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.transformer_model": [[759, 898], ["int", "modeling.get_shape_list", "modeling.reshape_to_matrix", "range", "ValueError", "ValueError", "modeling.reshape_from_matrix", "tensorflow.variable_scope", "modeling.reshape_from_matrix", "final_outputs.append", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.dropout", "modeling.layer_norm", "all_layer_outputs.append", "tensorflow.variable_scope", "modeling.attention_layer", "attention_heads.append", "len", "tensorflow.concat", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.dropout", "modeling.layer_norm", "modeling.create_initializer", "modeling.create_initializer", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.dropout", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.attention_layer", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.dropout", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.layer_norm", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_initializer", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_initializer", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.create_initializer"], ["", "def", "transformer_model", "(", "input_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "intermediate_act_fn", "=", "gelu", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_all_layers", "=", "False", ")", ":", "\n", "    ", "\"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n\n    This is almost an exact implementation of the original Transformer encoder.\n\n    See the original paper:\n    https://arxiv.org/abs/1706.03762\n\n    Also see:\n    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n    Args:\n      input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n      attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n        seq_length], with 1 for positions that can be attended to and 0 in\n        positions that should not be.\n      hidden_size: int. Hidden size of the Transformer.\n      num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n      num_attention_heads: int. Number of attention heads in the Transformer.\n      intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n        forward) layer.\n      intermediate_act_fn: function. The non-linear activation function to apply\n        to the output of the intermediate/feed-forward layer.\n      hidden_dropout_prob: float. Dropout probability for the hidden layers.\n      attention_probs_dropout_prob: float. Dropout probability of the attention\n        probabilities.\n      initializer_range: float. Range of the initializer (stddev of truncated\n        normal).\n      do_return_all_layers: Whether to also return all layers or just the final\n        layer.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size], the final\n      hidden layer of the Transformer.\n\n    Raises:\n      ValueError: A Tensor shape or parameter is invalid.\n    \"\"\"", "\n", "if", "hidden_size", "%", "num_attention_heads", "!=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "hidden_size", ",", "num_attention_heads", ")", ")", "\n", "\n", "", "attention_head_size", "=", "int", "(", "hidden_size", "/", "num_attention_heads", ")", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "input_width", "=", "input_shape", "[", "2", "]", "\n", "\n", "# The Transformer performs sum residuals on all layers so the input needs", "\n", "# to be the same as the hidden size.", "\n", "if", "input_width", "!=", "hidden_size", ":", "\n", "        ", "raise", "ValueError", "(", "\"The width of the input tensor (%d) != hidden size (%d)\"", "%", "\n", "(", "input_width", ",", "hidden_size", ")", ")", "\n", "\n", "# We keep the representation as a 2D tensor to avoid re-shaping it back and", "\n", "# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on", "\n", "# the GPU/CPU but may not be free on the TPU, so we want to minimize them to", "\n", "# help the optimizer.", "\n", "", "prev_output", "=", "reshape_to_matrix", "(", "input_tensor", ")", "\n", "\n", "all_layer_outputs", "=", "[", "]", "\n", "for", "layer_idx", "in", "range", "(", "num_hidden_layers", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"layer_%d\"", "%", "layer_idx", ")", ":", "\n", "            ", "layer_input", "=", "prev_output", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"attention\"", ")", ":", "\n", "                ", "attention_heads", "=", "[", "]", "\n", "with", "tf", ".", "variable_scope", "(", "\"self\"", ")", ":", "\n", "                    ", "attention_head", "=", "attention_layer", "(", "\n", "from_tensor", "=", "layer_input", ",", "\n", "to_tensor", "=", "layer_input", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "size_per_head", "=", "attention_head_size", ",", "\n", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "initializer_range", ",", "\n", "do_return_2d_tensor", "=", "True", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "from_seq_length", "=", "seq_length", ",", "\n", "to_seq_length", "=", "seq_length", ")", "\n", "attention_heads", ".", "append", "(", "attention_head", ")", "\n", "\n", "", "attention_output", "=", "None", "\n", "if", "len", "(", "attention_heads", ")", "==", "1", ":", "\n", "                    ", "attention_output", "=", "attention_heads", "[", "0", "]", "\n", "", "else", ":", "\n", "# In the case where we have other sequences, we just concatenate", "\n", "# them to the self-attention head before the projection.", "\n", "                    ", "attention_output", "=", "tf", ".", "concat", "(", "attention_heads", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Run a linear projection of `hidden_size` then add a residual", "\n", "# with `layer_input`.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "                    ", "attention_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "attention_output", "=", "dropout", "(", "attention_output", ",", "hidden_dropout_prob", ")", "\n", "attention_output", "=", "layer_norm", "(", "attention_output", "+", "layer_input", ")", "\n", "\n", "# The activation is only applied to the \"intermediate\" hidden layer.", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"intermediate\"", ")", ":", "\n", "                ", "intermediate_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "intermediate_size", ",", "\n", "activation", "=", "intermediate_act_fn", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# Down-project back to `hidden_size` then add the residual.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "                ", "layer_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "intermediate_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "layer_output", "=", "dropout", "(", "layer_output", ",", "hidden_dropout_prob", ")", "\n", "layer_output", "=", "layer_norm", "(", "layer_output", "+", "attention_output", ")", "\n", "prev_output", "=", "layer_output", "\n", "all_layer_outputs", ".", "append", "(", "layer_output", ")", "\n", "\n", "", "", "", "if", "do_return_all_layers", ":", "\n", "        ", "final_outputs", "=", "[", "]", "\n", "for", "layer_output", "in", "all_layer_outputs", ":", "\n", "            ", "final_output", "=", "reshape_from_matrix", "(", "layer_output", ",", "input_shape", ")", "\n", "final_outputs", ".", "append", "(", "final_output", ")", "\n", "", "return", "final_outputs", "\n", "", "else", ":", "\n", "        ", "final_output", "=", "reshape_from_matrix", "(", "prev_output", ",", "input_shape", ")", "\n", "return", "final_output", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list": [[900, 935], ["tensor.shape.as_list", "enumerate", "tensorflow.shape", "modeling.assert_rank", "non_static_indexes.append"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.tests.tile_repeat.shape", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.assert_rank"], ["", "", "def", "get_shape_list", "(", "tensor", ",", "expected_rank", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\n    Args:\n      tensor: A tf.Tensor object to find the shape of.\n      expected_rank: (optional) int. The expected rank of `tensor`. If this is\n        specified and the `tensor` has a different rank, and exception will be\n        thrown.\n      name: Optional name of the tensor for the error message.\n\n    Returns:\n      A list of dimensions of the shape of tensor. All static dimensions will\n      be returned as python integers, and dynamic dimensions will be returned\n      as tf.Tensor scalars.\n    \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "        ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "if", "expected_rank", "is", "not", "None", ":", "\n", "        ", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", ")", "\n", "\n", "", "shape", "=", "tensor", ".", "shape", ".", "as_list", "(", ")", "\n", "\n", "non_static_indexes", "=", "[", "]", "\n", "for", "(", "index", ",", "dim", ")", "in", "enumerate", "(", "shape", ")", ":", "\n", "        ", "if", "dim", "is", "None", ":", "\n", "            ", "non_static_indexes", ".", "append", "(", "index", ")", "\n", "\n", "", "", "if", "not", "non_static_indexes", ":", "\n", "        ", "return", "shape", "\n", "\n", "", "dyn_shape", "=", "tf", ".", "shape", "(", "tensor", ")", "\n", "for", "index", "in", "non_static_indexes", ":", "\n", "        ", "shape", "[", "index", "]", "=", "dyn_shape", "[", "index", "]", "\n", "", "return", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.reshape_to_matrix": [[937, 949], ["tensorflow.reshape", "ValueError"], "function", ["None"], ["", "def", "reshape_to_matrix", "(", "input_tensor", ")", ":", "\n", "    ", "\"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"", "\n", "ndims", "=", "input_tensor", ".", "shape", ".", "ndims", "\n", "if", "ndims", "<", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Input tensor must have at least rank 2. Shape = %s\"", "%", "\n", "(", "input_tensor", ".", "shape", ")", ")", "\n", "", "if", "ndims", "==", "2", ":", "\n", "        ", "return", "input_tensor", "\n", "\n", "", "width", "=", "input_tensor", ".", "shape", "[", "-", "1", "]", "\n", "output_tensor", "=", "tf", ".", "reshape", "(", "input_tensor", ",", "[", "-", "1", ",", "width", "]", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.reshape_from_matrix": [[951, 962], ["modeling.get_shape_list", "tensorflow.reshape", "len"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.get_shape_list"], ["", "def", "reshape_from_matrix", "(", "output_tensor", ",", "orig_shape_list", ")", ":", "\n", "    ", "\"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"", "\n", "if", "len", "(", "orig_shape_list", ")", "==", "2", ":", "\n", "        ", "return", "output_tensor", "\n", "\n", "", "output_shape", "=", "get_shape_list", "(", "output_tensor", ")", "\n", "\n", "orig_dims", "=", "orig_shape_list", "[", "0", ":", "-", "1", "]", "\n", "width", "=", "output_shape", "[", "-", "1", "]", "\n", "\n", "return", "tf", ".", "reshape", "(", "output_tensor", ",", "orig_dims", "+", "[", "width", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.modeling.assert_rank": [[964, 992], ["isinstance", "ValueError", "tensorflow.get_variable_scope", "str", "str"], "function", ["None"], ["", "def", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Raises an exception if the tensor rank is not of the expected rank.\n\n    Args:\n      tensor: A tf.Tensor to check the rank of.\n      expected_rank: Python integer or list of integers, expected rank.\n      name: Optional name of the tensor for the error message.\n\n    Raises:\n      ValueError: If the expected shape doesn't match the actual shape.\n    \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "        ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "expected_rank_dict", "=", "{", "}", "\n", "if", "isinstance", "(", "expected_rank", ",", "six", ".", "integer_types", ")", ":", "\n", "        ", "expected_rank_dict", "[", "expected_rank", "]", "=", "True", "\n", "", "else", ":", "\n", "        ", "for", "x", "in", "expected_rank", ":", "\n", "            ", "expected_rank_dict", "[", "x", "]", "=", "True", "\n", "\n", "", "", "actual_rank", "=", "tensor", ".", "shape", ".", "ndims", "\n", "if", "actual_rank", "not", "in", "expected_rank_dict", ":", "\n", "        ", "scope_name", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "name", "\n", "raise", "ValueError", "(", "\n", "\"For the tensor `%s` in scope `%s`, the actual rank \"", "\n", "\"`%d` (shape = %s) is not equal to the expected rank `%s`\"", "%", "\n", "(", "name", ",", "scope_name", ",", "actual_rank", ",", "str", "(", "tensor", ".", "shape", ")", ",", "str", "(", "expected_rank", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.optimization.AdamWeightDecayOptimizer.__init__": [[90, 107], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "\n", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.0", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "None", ",", "\n", "name", "=", "\"AdamWeightDecayOptimizer\"", ")", ":", "\n", "    ", "\"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"", "\n", "super", "(", "AdamWeightDecayOptimizer", ",", "self", ")", ".", "__init__", "(", "False", ",", "name", ")", "\n", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "weight_decay_rate", "=", "weight_decay_rate", "\n", "self", ".", "beta_1", "=", "beta_1", "\n", "self", ".", "beta_2", "=", "beta_2", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "exclude_from_weight_decay", "=", "exclude_from_weight_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.optimization.AdamWeightDecayOptimizer.apply_gradients": [[108, 158], ["tensorflow.group", "optimization.AdamWeightDecayOptimizer._get_variable_name", "tensorflow.get_variable", "tensorflow.get_variable", "optimization.AdamWeightDecayOptimizer._do_use_weight_decay", "assignments.extend", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "param.shape.as_list", "tensorflow.zeros_initializer", "param.shape.as_list", "tensorflow.zeros_initializer", "tensorflow.square", "tensorflow.sqrt", "param.assign", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.optimization.AdamWeightDecayOptimizer._get_variable_name", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.optimization.AdamWeightDecayOptimizer._do_use_weight_decay"], ["", "def", "apply_gradients", "(", "self", ",", "grads_and_vars", ",", "global_step", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "assignments", "=", "[", "]", "\n", "for", "(", "grad", ",", "param", ")", "in", "grads_and_vars", ":", "\n", "      ", "if", "grad", "is", "None", "or", "param", "is", "None", ":", "\n", "        ", "continue", "\n", "\n", "", "param_name", "=", "self", ".", "_get_variable_name", "(", "param", ".", "name", ")", "\n", "\n", "m", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_m\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "v", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_v\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "# Standard Adam update.", "\n", "next_m", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_1", ",", "m", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_1", ",", "grad", ")", ")", "\n", "next_v", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_2", ",", "v", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_2", ",", "\n", "tf", ".", "square", "(", "grad", ")", ")", ")", "\n", "\n", "update", "=", "next_m", "/", "(", "tf", ".", "sqrt", "(", "next_v", ")", "+", "self", ".", "epsilon", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want ot decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "self", ".", "_do_use_weight_decay", "(", "param_name", ")", ":", "\n", "        ", "update", "+=", "self", ".", "weight_decay_rate", "*", "param", "\n", "\n", "", "update_with_lr", "=", "self", ".", "learning_rate", "*", "update", "\n", "\n", "next_param", "=", "param", "-", "update_with_lr", "\n", "\n", "assignments", ".", "extend", "(", "\n", "[", "param", ".", "assign", "(", "next_param", ")", ",", "\n", "m", ".", "assign", "(", "next_m", ")", ",", "\n", "v", ".", "assign", "(", "next_v", ")", "]", ")", "\n", "", "return", "tf", ".", "group", "(", "*", "assignments", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.optimization.AdamWeightDecayOptimizer._do_use_weight_decay": [[159, 168], ["re.search"], "methods", ["None"], ["", "def", "_do_use_weight_decay", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"", "\n", "if", "not", "self", ".", "weight_decay_rate", ":", "\n", "      ", "return", "False", "\n", "", "if", "self", ".", "exclude_from_weight_decay", ":", "\n", "      ", "for", "r", "in", "self", ".", "exclude_from_weight_decay", ":", "\n", "        ", "if", "re", ".", "search", "(", "r", ",", "param_name", ")", "is", "not", "None", ":", "\n", "          ", "return", "False", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.optimization.AdamWeightDecayOptimizer._get_variable_name": [[169, 175], ["re.match", "re.match.group"], "methods", ["None"], ["", "def", "_get_variable_name", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Get the variable name from the tensor name.\"\"\"", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "param_name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "param_name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "return", "param_name", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.optimization.create_optimizer": [[25, 85], ["tensorflow.train.get_or_create_global_step", "tensorflow.constant", "tensorflow.train.polynomial_decay", "optimization.AdamWeightDecayOptimizer", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.clip_by_global_norm", "tf.contrib.tpu.CrossShardOptimizer.apply_gradients", "tensorflow.group", "tensorflow.cast", "tensorflow.constant", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.contrib.tpu.CrossShardOptimizer", "zip", "tf.train.get_or_create_global_step.assign"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.optimization.AdamWeightDecayOptimizer.apply_gradients"], ["def", "create_optimizer", "(", "loss", ",", "init_lr", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ")", ":", "\n", "  ", "\"\"\"Creates an optimizer training op.\"\"\"", "\n", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", "\n", "\n", "learning_rate", "=", "tf", ".", "constant", "(", "value", "=", "init_lr", ",", "shape", "=", "[", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Implements linear decay of the learning rate.", "\n", "learning_rate", "=", "tf", ".", "train", ".", "polynomial_decay", "(", "\n", "learning_rate", ",", "\n", "global_step", ",", "\n", "num_train_steps", ",", "\n", "end_learning_rate", "=", "0.0", ",", "\n", "power", "=", "1.0", ",", "\n", "cycle", "=", "False", ")", "\n", "\n", "# Implements linear warmup. I.e., if global_step < num_warmup_steps, the", "\n", "# learning rate will be `global_step/num_warmup_steps * init_lr`.", "\n", "if", "num_warmup_steps", ":", "\n", "    ", "global_steps_int", "=", "tf", ".", "cast", "(", "global_step", ",", "tf", ".", "int32", ")", "\n", "warmup_steps_int", "=", "tf", ".", "constant", "(", "num_warmup_steps", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "global_steps_float", "=", "tf", ".", "cast", "(", "global_steps_int", ",", "tf", ".", "float32", ")", "\n", "warmup_steps_float", "=", "tf", ".", "cast", "(", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "\n", "warmup_percent_done", "=", "global_steps_float", "/", "warmup_steps_float", "\n", "warmup_learning_rate", "=", "init_lr", "*", "warmup_percent_done", "\n", "\n", "is_warmup", "=", "tf", ".", "cast", "(", "global_steps_int", "<", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "learning_rate", "=", "(", "\n", "(", "1.0", "-", "is_warmup", ")", "*", "learning_rate", "+", "is_warmup", "*", "warmup_learning_rate", ")", "\n", "\n", "# It is recommended that you use this optimizer for fine tuning, since this", "\n", "# is how the model was trained (note that the Adam m/v variables are NOT", "\n", "# loaded from init_checkpoint.)", "\n", "", "optimizer", "=", "AdamWeightDecayOptimizer", "(", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.01", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "[", "\"LayerNorm\"", ",", "\"layer_norm\"", ",", "\"bias\"", "]", ")", "\n", "\n", "if", "use_tpu", ":", "\n", "    ", "optimizer", "=", "tf", ".", "contrib", ".", "tpu", ".", "CrossShardOptimizer", "(", "optimizer", ")", "\n", "\n", "", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "grads", "=", "tf", ".", "gradients", "(", "loss", ",", "tvars", ")", "\n", "\n", "# This is how the model was pre-trained.", "\n", "(", "grads", ",", "_", ")", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "clip_norm", "=", "1.0", ")", "\n", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "grads", ",", "tvars", ")", ",", "global_step", "=", "global_step", ")", "\n", "\n", "# Normally the global step update is done inside of `apply_gradients`.", "\n", "# However, `AdamWeightDecayOptimizer` doesn't do this. But if you use", "\n", "# a different optimizer, you should probably take this line out.", "\n", "new_global_step", "=", "global_step", "+", "1", "\n", "train_op", "=", "tf", ".", "group", "(", "train_op", ",", "[", "global_step", ".", "assign", "(", "new_global_step", ")", "]", ")", "\n", "return", "train_op", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.FullTokenizer.__init__": [[164, 169], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "tokenization.FullTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.FullTokenizer.tokenize": [[170, 177], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.FullTokenizer.convert_tokens_to_ids": [[178, 180], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.FullTokenizer.convert_ids_to_tokens": [[181, 183], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer.__init__": [[188, 195], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer.tokenize": [[196, 219], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._run_strip_accents": [[220, 230], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._run_split_on_punc": [[231, 250], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._tokenize_chinese_chars": [[251, 263], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._is_chinese_char": [[264, 285], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "    ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "      ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.BasicTokenizer._clean_text": [[286, 298], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.__init__": [[303, 307], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "200", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.WordpieceTokenizer.tokenize": [[308, 360], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "      ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "        ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "          ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "            ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "            ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "          ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "        ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.validate_case_matches_checkpoint": [[28, 76], ["re.match", "re.match.group", "ValueError"], "function", ["None"], ["def", "validate_case_matches_checkpoint", "(", "do_lower_case", ",", "init_checkpoint", ")", ":", "\n", "  ", "\"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"", "\n", "\n", "# The casing has to be passed in by the user and there is no explicit check", "\n", "# as to whether it matches the checkpoint. The casing information probably", "\n", "# should have been stored in the bert_config.json file, but it's not, so", "\n", "# we have to heuristically detect it to validate.", "\n", "\n", "if", "not", "init_checkpoint", ":", "\n", "    ", "return", "\n", "\n", "", "m", "=", "re", ".", "match", "(", "\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\"", ",", "init_checkpoint", ")", "\n", "if", "m", "is", "None", ":", "\n", "    ", "return", "\n", "\n", "", "model_name", "=", "m", ".", "group", "(", "1", ")", "\n", "\n", "lower_models", "=", "[", "\n", "\"uncased_L-24_H-1024_A-16\"", ",", "\"uncased_L-12_H-768_A-12\"", ",", "\n", "\"multilingual_L-12_H-768_A-12\"", ",", "\"chinese_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "cased_models", "=", "[", "\n", "\"cased_L-12_H-768_A-12\"", ",", "\"cased_L-24_H-1024_A-16\"", ",", "\n", "\"multi_cased_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "is_bad_config", "=", "False", "\n", "if", "model_name", "in", "lower_models", "and", "not", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"False\"", "\n", "case_name", "=", "\"lowercased\"", "\n", "opposite_flag", "=", "\"True\"", "\n", "\n", "", "if", "model_name", "in", "cased_models", "and", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"True\"", "\n", "case_name", "=", "\"cased\"", "\n", "opposite_flag", "=", "\"False\"", "\n", "\n", "", "if", "is_bad_config", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"", "\n", "\"However, `%s` seems to be a %s model, so you \"", "\n", "\"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"", "\n", "\"how the model was pre-training. If this error is wrong, please \"", "\n", "\"just comment out this check.\"", "%", "(", "actual_flag", ",", "init_checkpoint", ",", "\n", "model_name", ",", "case_name", ",", "opposite_flag", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_to_unicode": [[78, 96], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "convert_to_unicode", "(", "text", ")", ":", "\n", "  ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.printable_text": [[98, 119], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "  ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.load_vocab": [[121, 134], ["collections.OrderedDict", "tensorflow.gfile.GFile", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "  ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "        ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_by_vocab": [[136, 142], ["output.append"], "function", ["None"], ["", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "  ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "    ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_tokens_to_ids": [[144, 146], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_ids_to_tokens": [[148, 150], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization.whitespace_tokenize": [[152, 159], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization._is_whitespace": [[362, 372], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization._is_control": [[374, 384], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ShannonAI_CorefQA.bert.tokenization._is_punctuation": [[386, 400], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "", ""]]}