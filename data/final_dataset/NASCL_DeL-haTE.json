{"home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.preprocess_datasets.main": [[13, 76], ["pandas.read_csv", "pd.read_csv.rename", "pandas.read_csv", "pd.read_csv.apply", "nltk.stem.PorterStemmer", "utils.load_word_lists", "print", "preprocess_datasets.process_csv_data", "print", "preprocess_datasets.process_csv_data", "pandas.concat", "pandas.concat", "print", "preprocess_datasets.process_gab_data", "print", "datasets.items", "pd.read_csv.index.astype", "pd.read_csv.index.astype", "open", "json.load", "open", "json.dump", "open", "json.dump", "open", "json.dump", "train.to_dict", "test.to_dict"], "function", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.load_word_lists", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.preprocess_datasets.process_csv_data", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.preprocess_datasets.process_csv_data", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.preprocess_datasets.process_gab_data"], ["def", "main", "(", ")", ":", "\n", "    ", "hon_data", "=", "pd", ".", "read_csv", "(", "'data/hon/HON_labeled_data.csv'", ",", "\n", "header", "=", "0", ",", "index_col", "=", "0", ")", "\n", "hon_data", ".", "rename", "(", "columns", "=", "{", "'class'", ":", "'label'", "}", ",", "inplace", "=", "True", ")", "\n", "hon_data", ".", "index", "=", "'h_'", "+", "hon_data", ".", "index", ".", "astype", "(", "str", ")", "\n", "\n", "olid_data", "=", "pd", ".", "read_csv", "(", "'data/olid/olid-training-v1.0.tsv'", ",", "\n", "sep", "=", "'\\t'", ",", "header", "=", "0", ",", "index_col", "=", "0", ")", "\n", "olid_data", "[", "'label'", "]", "=", "olid_data", ".", "apply", "(", "flatten_labels", ",", "axis", "=", "1", ")", "\n", "olid_data", ".", "index", "=", "'o_'", "+", "olid_data", ".", "index", ".", "astype", "(", "str", ")", "\n", "\n", "with", "open", "(", "'data/gab/gab_posts_random_sample_100k.json'", ")", "as", "f", ":", "\n", "        ", "gab_data", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "stemmer", "=", "PorterStemmer", "(", ")", "\n", "addl_stops", "=", "[", "'mt'", ",", "'rt'", "]", "\n", "\n", "hate_words", ",", "offensive_words", ",", "positive_words", "=", "utils", ".", "load_word_lists", "(", "stemmer", ")", "\n", "\n", "print", "(", "'Processing HON data...'", ")", "\n", "hon_train", ",", "hon_test", "=", "process_csv_data", "(", "hon_data", ",", "\n", "stemmer", ",", "\n", "hate_words", ",", "\n", "offensive_words", ",", "\n", "positive_words", ",", "\n", "stops", "=", "addl_stops", ")", "\n", "\n", "print", "(", "'Processing OLID data...'", ")", "\n", "olid_train", ",", "olid_test", "=", "process_csv_data", "(", "olid_data", ",", "\n", "stemmer", ",", "\n", "hate_words", ",", "\n", "offensive_words", ",", "\n", "positive_words", ",", "\n", "stops", "=", "addl_stops", ")", "\n", "\n", "combined_train", "=", "pd", ".", "concat", "(", "[", "hon_train", ",", "olid_train", "]", ")", "\n", "combined_test", "=", "pd", ".", "concat", "(", "[", "hon_test", ",", "olid_test", "]", ")", "\n", "\n", "print", "(", "'Processing Gab data...'", ")", "\n", "gab_train", "=", "process_gab_data", "(", "gab_data", ",", "\n", "stemmer", ",", "\n", "hate_words", ",", "\n", "offensive_words", ",", "\n", "positive_words", ",", "\n", "stops", "=", "addl_stops", ")", "\n", "\n", "cols", "=", "[", "'text'", ",", "'tokens'", ",", "'bounds'", ",", "'label'", "]", "\n", "datasets", "=", "{", "\n", "'hon'", ":", "(", "hon_train", "[", "cols", "]", ",", "hon_test", "[", "cols", "]", ")", ",", "\n", "'olid'", ":", "(", "olid_train", "[", "cols", "]", ",", "olid_test", "[", "cols", "]", ")", ",", "\n", "'combined'", ":", "(", "combined_train", "[", "cols", "]", ",", "combined_test", "[", "cols", "]", ")", "\n", "}", "\n", "\n", "# Save processed text, bounds, and label to json", "\n", "print", "(", "'Saving datasets to JSON...'", ")", "\n", "for", "name", ",", "(", "train", ",", "test", ")", "in", "datasets", ".", "items", "(", ")", ":", "\n", "        ", "with", "open", "(", "f'data/{name}/{name}_train.json'", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "train", ".", "to_dict", "(", "'index'", ")", ",", "f", ",", "indent", "=", "2", ")", "\n", "", "with", "open", "(", "f'data/{name}/{name}_test.json'", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "test", ".", "to_dict", "(", "'index'", ")", ",", "f", ",", "indent", "=", "2", ")", "\n", "\n", "", "", "with", "open", "(", "f'data/gab/gab_train.json'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "gab_train", ",", "f", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.preprocess_datasets.flatten_labels": [[78, 85], ["None"], "function", ["None"], ["", "", "def", "flatten_labels", "(", "row", ")", ":", "\n", "    ", "if", "row", "[", "'subtask_a'", "]", "==", "'NOT'", ":", "\n", "        ", "return", "2", "\n", "", "elif", "row", "[", "'subtask_b'", "]", "==", "'UNT'", ":", "\n", "        ", "return", "1", "\n", "", "else", ":", "\n", "        ", "return", "0", "if", "row", "[", "'subtask_c'", "]", "==", "'GRP'", "else", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.preprocess_datasets.process_csv_data": [[87, 103], ["data[].apply", "data[].apply", "data[].apply", "data.sample", "data.drop"], "function", ["None"], ["", "", "def", "process_csv_data", "(", "data", ",", "stemmer", ",", "hate_words", ",", "offensive_words", ",", "positive_words", ",", "stops", "=", "None", ")", ":", "\n", "# Clean text", "\n", "    ", "data", "[", "'text'", "]", "=", "data", "[", "'tweet'", "]", ".", "apply", "(", "utils", ".", "clean_text", ",", "stemmer", "=", "stemmer", ",", "stops", "=", "stops", ")", "\n", "data", "[", "'tokens'", "]", "=", "data", "[", "'text'", "]", ".", "apply", "(", "word_tokenize", ")", "\n", "\n", "# Calculate class bounds for weak supervision", "\n", "data", "[", "'bounds'", "]", "=", "data", "[", "'tokens'", "]", ".", "apply", "(", "utils", ".", "calculate_bounds", ",", "\n", "hate_words", "=", "hate_words", ",", "\n", "offensive_words", "=", "offensive_words", ",", "\n", "positive_words", "=", "positive_words", ")", "\n", "\n", "# Split dataset into train, val, and test sets", "\n", "test_data", "=", "data", ".", "sample", "(", "frac", "=", "args", ".", "test_frac", ")", "\n", "data", ".", "drop", "(", "test_data", ".", "index", ",", "inplace", "=", "True", ")", "\n", "\n", "return", "data", ",", "test_data", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.preprocess_datasets.process_gab_data": [[105, 120], ["utils.clean_text", "set", "nltk.tokenize.word_tokenize", "set.issubset", "nltk.tokenize.word_tokenize", "utils.calculate_bounds"], "function", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.clean_text", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.calculate_bounds"], ["", "def", "process_gab_data", "(", "data", ",", "stemmer", ",", "hate_words", ",", "offensive_words", ",", "positive_words", ",", "stops", "=", "None", ")", ":", "\n", "    ", "placeholders", "=", "{", "'HASHTAGHERE'", ",", "'MENTIONHERE'", "}", "\n", "clean_data", "=", "{", "}", "\n", "for", "post", "in", "data", ":", "\n", "        ", "text", "=", "utils", ".", "clean_text", "(", "post", "[", "'body'", "]", ",", "stemmer", ",", "stops", "=", "stops", ")", "\n", "words", "=", "set", "(", "word_tokenize", "(", "text", ")", ")", "\n", "\n", "if", "text", "and", "not", "words", ".", "issubset", "(", "placeholders", ")", ":", "\n", "            ", "clean_data", "[", "f'g_{post[\"post_id\"]}'", "]", "=", "{", "\n", "'text'", ":", "text", ",", "\n", "'tokens'", ":", "word_tokenize", "(", "text", ")", ",", "\n", "'bounds'", ":", "utils", ".", "calculate_bounds", "(", "text", ",", "hate_words", ",", "offensive_words", ",", "positive_words", ")", "\n", "}", "\n", "\n", "", "", "return", "clean_data", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.eval_model.main": [[14, 45], ["torch.device", "utils.load_embedding", "utils.load_dataset", "DelhateEnsemble.load_model.to", "DelhateEnsemble.load_model.evaluate", "print", "print", "sklearn.metrics.classification_report", "sklearn.metrics.confusion_matrix", "model_name.replace.replace", "os.makedirs", "args.model_path.split", "model.DelhateEnsemble.load_model", "collections.Counter", "collections.Counter", "open", "f.write", "f.write", "f.write", "f.write", "dataset.upper", "torch.cuda.is_available", "str"], "function", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.load_embedding", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.load_dataset", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.evaluate", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.load_model"], ["def", "main", "(", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "use_gpu", "else", "'cpu'", ")", "\n", "\n", "try", ":", "\n", "        ", "_", ",", "dataset", ",", "embed", ",", "model_type", ",", "model_name", "=", "args", ".", "model_path", ".", "split", "(", "'/'", ")", "\n", "model", "=", "DelhateEnsemble", ".", "load_model", "(", "args", ".", "model_path", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "        ", "raise", "\n", "\n", "", "embedding", ",", "dim", "=", "utils", ".", "load_embedding", "(", "model", ".", "embed_corpus", ")", "\n", "\n", "test_data", "=", "utils", ".", "load_dataset", "(", "args", ".", "dataset", ",", "'test'", ",", "embedding", ",", "labeled", "=", "True", ",", "pad", "=", "model", ".", "seq_length", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "y_pred", ",", "y_true", "=", "model", ".", "evaluate", "(", "test_data", ",", "device", "=", "device", ")", "\n", "\n", "print", "(", "'pred:'", ",", "Counter", "(", "y_pred", ")", ")", "\n", "print", "(", "'true:'", ",", "Counter", "(", "y_true", ")", ")", "\n", "\n", "report", "=", "classification_report", "(", "y_true", ",", "y_pred", ",", "target_names", "=", "[", "'H'", ",", "'O'", ",", "'N'", "]", ",", "digits", "=", "3", ")", "\n", "conf_mat", "=", "confusion_matrix", "(", "y_true", ",", "y_pred", ")", "\n", "\n", "model_name", "=", "model_name", ".", "replace", "(", "'.pt'", ",", "''", ")", "\n", "out_path", "=", "f'metrics/{dataset.upper()}/{embed}/{model_type}'", "\n", "os", ".", "makedirs", "(", "out_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "with", "open", "(", "f'{out_path}/{model_name}_{args.dataset}.txt'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "report", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "f", ".", "write", "(", "'\\n'", ".", "join", "(", "'  '", ".", "join", "(", "str", "(", "x", ")", "for", "x", "in", "y", ")", "for", "y", "in", "conf_mat", ")", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.train_model.main": [[12, 59], ["torch.device", "torch.device", "os.makedirs", "utils.load_embedding", "utils.load_dataset", "model.DelhateEnsemble", "model.DelhateEnsemble.train_models", "model.DelhateEnsemble.save_model", "utils.weak_loss", "torch.cuda.is_available", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.load_embedding", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.load_dataset", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.train_models", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.save_model", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.weak_loss"], ["def", "main", "(", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "use_gpu", "else", "'cpu'", ")", "\n", "\n", "rnn_str", "=", "args", ".", "rnn_type", "if", "args", ".", "rnn_type", "else", "'cnn'", "\n", "weak_str", "=", "'_weak'", "if", "args", ".", "weak_loss", "else", "''", "\n", "\n", "out_path", "=", "f'models/{args.dataset}/{args.embed_corpus}/delhate_{rnn_str}{weak_str}'", "\n", "os", ".", "makedirs", "(", "out_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "embedding", ",", "dim", "=", "utils", ".", "load_embedding", "(", "args", ".", "embed_corpus", ")", "\n", "\n", "labeled", "=", "not", "args", ".", "weak_loss", "\n", "\n", "train_data", "=", "utils", ".", "load_dataset", "(", "args", ".", "dataset", ",", "'train'", ",", "embedding", ",", "labeled", ",", "args", ".", "pad", ")", "\n", "\n", "model", "=", "DelhateEnsemble", "(", "\n", "n_models", "=", "args", ".", "n_models", ",", "\n", "seq_length", "=", "train_data", ".", "padded_seq", ",", "\n", "embed_corpus", "=", "args", ".", "embed_corpus", ",", "\n", "embed_dim", "=", "dim", ",", "\n", "n_classes", "=", "train_data", ".", "n_classes", ",", "\n", "n_filters", "=", "args", ".", "n_filters", ",", "\n", "filter_width", "=", "args", ".", "filter_width", ",", "\n", "pool_size", "=", "args", ".", "pool_size", ",", "\n", "n_hidden", "=", "args", ".", "n_hidden", ",", "\n", "rnn_type", "=", "args", ".", "rnn_type", ",", "\n", "dropout", "=", "args", ".", "dropout", "\n", ")", "\n", "\n", "if", "args", ".", "weak_loss", ":", "\n", "        ", "loss_fn", "=", "lambda", "x", ",", "y", ":", "utils", ".", "weak_loss", "(", "x", ",", "y", ",", "weight", "=", "args", ".", "class_weight", ")", "\n", "", "else", ":", "\n", "        ", "loss_fn", "=", "F", ".", "cross_entropy", "\n", "\n", "", "model", ".", "train_models", "(", "\n", "train_data", ",", "\n", "loss_fn", "=", "loss_fn", ",", "\n", "lr", "=", "args", ".", "learn_rate", ",", "\n", "n_samples", "=", "args", ".", "n_samples", ",", "\n", "use_val", "=", "args", ".", "use_val", ",", "\n", "early_stop", "=", "args", ".", "early_stop", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "EPOCHS", "=", "args", ".", "epochs", ",", "\n", "device", "=", "device", "\n", ")", "\n", "\n", "model", ".", "save_model", "(", "f'{out_path}/{args.model_name}.pt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.dataset.PostDataset.__init__": [[13, 33], ["list", "max", "dataset.PostDataset.set_padding", "posts.keys", "collections.defaultdict", "enumerate", "len", "len", "max", "dataset.PostDataset.label_idx[].append"], "methods", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.dataset.PostDataset.set_padding"], ["    ", "def", "__init__", "(", "self", ",", "posts", ",", "embedding", ",", "labeled", "=", "True", ",", "padded_seq", "=", "'max'", ")", ":", "\n", "        ", "self", ".", "posts", "=", "posts", "\n", "self", ".", "post_ids", "=", "list", "(", "posts", ".", "keys", "(", ")", ")", "\n", "self", ".", "labeled", "=", "labeled", "\n", "\n", "self", ".", "embedding", "=", "embedding", "\n", "\n", "self", ".", "max_seq_length", "=", "max", "(", "len", "(", "self", ".", "posts", "[", "ID", "]", "[", "'tokens'", "]", ")", "for", "ID", "in", "self", ".", "posts", ")", "\n", "\n", "self", ".", "padded_seq", "=", "None", "\n", "self", ".", "set_padding", "(", "padded_seq", ")", "\n", "\n", "if", "self", ".", "labeled", ":", "\n", "            ", "self", ".", "n_classes", "=", "max", "(", "self", ".", "posts", "[", "ID", "]", "[", "'label'", "]", "for", "ID", "in", "self", ".", "posts", ")", "+", "1", "\n", "\n", "self", ".", "label_idx", "=", "defaultdict", "(", "list", ")", "# Dict of post ids per class", "\n", "for", "i", ",", "post", "in", "enumerate", "(", "self", ".", "post_ids", ")", ":", "\n", "                ", "self", ".", "label_idx", "[", "self", ".", "posts", "[", "post", "]", "[", "'label'", "]", "]", ".", "append", "(", "i", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "n_classes", "=", "len", "(", "self", ".", "posts", "[", "self", ".", "post_ids", "[", "0", "]", "]", "[", "'bounds'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.dataset.PostDataset.__len__": [[34, 36], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "posts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.dataset.PostDataset.__getitem__": [[37, 52], ["dataset.PostDataset.embedding.get_vecs_by_tokens", "torch.pad", "torch.pad", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "ID", "=", "self", ".", "post_ids", "[", "idx", "]", "\n", "\n", "# Get embeddings vectors from word tokens", "\n", "X", "=", "self", ".", "embedding", ".", "get_vecs_by_tokens", "(", "self", ".", "posts", "[", "ID", "]", "[", "'tokens'", "]", ")", "\n", "\n", "# Add zero left padding to standardize word embedding matrix size", "\n", "X", "=", "F", ".", "pad", "(", "X", ",", "[", "0", ",", "0", ",", "(", "self", ".", "padded_seq", "-", "X", ".", "shape", "[", "0", "]", ")", ",", "0", "]", ")", "\n", "\n", "if", "self", ".", "labeled", ":", "\n", "            ", "y", "=", "self", ".", "posts", "[", "ID", "]", "[", "'label'", "]", "\n", "", "else", ":", "\n", "            ", "y", "=", "torch", ".", "tensor", "(", "self", ".", "posts", "[", "ID", "]", "[", "'bounds'", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n", "", "return", "X", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.dataset.PostDataset.set_padding": [[53, 64], ["int", "Exception"], "methods", ["None"], ["", "def", "set_padding", "(", "self", ",", "pad", ")", ":", "\n", "        ", "if", "pad", "==", "'max'", ":", "\n", "            ", "pad", "=", "self", ".", "max_seq_length", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "pad", "=", "int", "(", "pad", ")", "\n", "", "except", "TypeError", "as", "e", ":", "\n", "                ", "raise", "Exception", "(", "'Invalid padding. Padding must be an integer or '", "\n", "'\"max\" which will set padding to the length of max input sequence.'", ")", "from", "e", "\n", "\n", "", "", "self", ".", "padded_seq", "=", "pad", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.dataset.PostDataset.sample_classes": [[65, 86], ["min", "torch.utils.data.Subset", "torch.utils.data.Subset", "len", "warnings.warn", "dataset.PostDataset.label_idx.values", "random.sample", "dataset.PostDataset.label_idx.values", "type", "TypeError"], "methods", ["None"], ["", "def", "sample_classes", "(", "self", ",", "n_samples", "=", "1000", ")", ":", "\n", "        ", "assert", "self", ".", "labeled", "\n", "\n", "min_class", "=", "min", "(", "len", "(", "p_ids", ")", "for", "p_ids", "in", "self", ".", "label_idx", ".", "values", "(", ")", ")", "\n", "if", "n_samples", "==", "'all'", ":", "\n", "            ", "n_samples", "=", "min_class", "\n", "", "elif", "n_samples", ">", "min_class", ":", "\n", "            ", "warnings", ".", "warn", "(", "(", "'Number of sample greater than examples of smallest class. '", "\n", "f'Setting n_samples = {min_class:,}'", ")", ")", "\n", "n_samples", "=", "min_class", "\n", "", "elif", "type", "(", "n_samples", ")", "is", "not", "int", ":", "\n", "            ", "raise", "TypeError", "(", "'Invalid n_samples. n_samples must be an integer or '", "\n", "'\"all\" which will set n_samples to the number of examples in the smallest class.'", ")", "\n", "\n", "", "sample_idx", "=", "[", "\n", "p_i", "\n", "for", "p_indices", "in", "self", ".", "label_idx", ".", "values", "(", ")", "\n", "for", "p_i", "in", "sample", "(", "p_indices", ",", "k", "=", "n_samples", ")", "\n", "]", "\n", "\n", "return", "Subset", "(", "self", ",", "sample_idx", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.__init__": [[16, 66], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "range", "model.DelhateEnsemble.sub_models.append", "rnn_type.casefold", "rnn_type.casefold", "model.ComponentModel", "rnn_type.casefold", "ValueError"], "methods", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "n_models", ",", "\n", "seq_length", ",", "\n", "embed_corpus", ",", "\n", "embed_dim", ",", "\n", "n_classes", ",", "\n", "n_filters", ",", "\n", "filter_width", ",", "\n", "pool_size", "=", "4", ",", "\n", "n_hidden", "=", "100", ",", "\n", "rnn_type", "=", "'gru'", ",", "\n", "dropout", "=", "0.2", "\n", ")", ":", "\n", "        ", "super", "(", "DelhateEnsemble", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "n_models", "=", "n_models", "\n", "self", ".", "seq_length", "=", "seq_length", "\n", "self", ".", "embed_corpus", "=", "embed_corpus", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "n_classes", "=", "n_classes", "\n", "self", ".", "n_filters", "=", "n_filters", "\n", "self", ".", "filter_width", "=", "filter_width", "\n", "self", ".", "pool_size", "=", "pool_size", "\n", "self", ".", "n_hidden", "=", "n_hidden", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "if", "rnn_type", "is", "None", "or", "rnn_type", ".", "casefold", "(", ")", "==", "'none'", ":", "\n", "            ", "self", ".", "rnn_type", "=", "None", "\n", "", "elif", "rnn_type", ".", "casefold", "(", ")", "==", "'lstm'", ":", "\n", "            ", "self", ".", "rnn_type", "=", "'lstm'", "\n", "", "elif", "rnn_type", ".", "casefold", "(", ")", "==", "'gru'", ":", "\n", "            ", "self", ".", "rnn_type", "=", "'gru'", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid RNN type selected. Options are gru/lstm/None'", ")", "\n", "\n", "", "self", ".", "sub_models", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "n_models", ")", ":", "\n", "            ", "self", ".", "sub_models", ".", "append", "(", "\n", "ComponentModel", "(", "\n", "self", ".", "seq_length", ",", "\n", "self", ".", "embed_corpus", ",", "\n", "self", ".", "embed_dim", ",", "\n", "self", ".", "n_classes", ",", "\n", "self", ".", "n_filters", ",", "\n", "self", ".", "filter_width", ",", "\n", "self", ".", "pool_size", ",", "\n", "self", ".", "n_hidden", ",", "\n", "self", ".", "rnn_type", ",", "\n", "self", ".", "dropout", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.forward": [[71, 78], ["torch.empty", "torch.empty", "torch.empty", "torch.empty", "enumerate", "model", "x.clone"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Shape: batch size X num models X num classes", "\n", "        ", "logits", "=", "torch", ".", "empty", "(", "(", "x", ".", "shape", "[", "0", "]", ",", "self", ".", "n_models", ",", "self", ".", "n_classes", ")", ")", "\n", "for", "i", ",", "model", "in", "enumerate", "(", "self", ".", "sub_models", ")", ":", "\n", "            ", "logits", "[", ":", ",", "i", ",", ":", "]", "=", "model", "(", "x", ".", "clone", "(", ")", ")", "\n", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.evaluate": [[79, 95], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.DelhateEnsemble.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.DelhateEnsemble.", "torch.softmax().argmax().mode", "torch.softmax().argmax().mode", "torch.softmax().argmax().mode", "torch.softmax().argmax().mode", "y_pred.extend", "y_true.extend", "X.to", "y.to", "preds.tolist", "y.tolist", "torch.softmax().argmax", "torch.softmax().argmax", "torch.softmax().argmax", "torch.softmax().argmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["None"], ["", "def", "evaluate", "(", "self", ",", "data", ",", "batch_size", "=", "64", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "data_loader", "=", "DataLoader", "(", "data", ",", "batch_size", "=", "batch_size", ")", "\n", "y_pred", ",", "y_true", "=", "[", "]", ",", "[", "]", "\n", "\n", "self", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "X", ",", "y", "in", "data_loader", ":", "\n", "                ", "X", ",", "y", "=", "X", ".", "to", "(", "device", ")", ",", "y", ".", "to", "(", "device", ")", "\n", "\n", "logits", "=", "self", "(", "X", ")", "\n", "preds", ",", "_", "=", "torch", ".", "softmax", "(", "logits", ",", "dim", "=", "2", ")", ".", "argmax", "(", "dim", "=", "2", ")", ".", "mode", "(", "dim", "=", "1", ")", "\n", "\n", "y_pred", ".", "extend", "(", "preds", ".", "tolist", "(", ")", ")", "\n", "y_true", ".", "extend", "(", "y", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "return", "y_pred", ",", "y_true", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.train_models": [[96, 149], ["collections.defaultdict", "model.DelhateEnsemble.to", "enumerate", "torch.optim.Adam", "torch.optim.Adam", "model.train_model", "losses[].append", "losses[].append", "model.parameters", "int", "torch.utils.data.random_split", "torch.utils.data.random_split", "dataset.PostDataset", "len", "len"], "methods", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.train_model"], ["", "def", "train_models", "(", "\n", "self", ",", "\n", "train_data", ",", "\n", "loss_fn", ",", "\n", "lr", "=", "1e-3", ",", "\n", "n_samples", "=", "1000", ",", "\n", "use_val", "=", "False", ",", "\n", "early_stop", "=", "False", ",", "\n", "batch_size", "=", "24", ",", "\n", "EPOCHS", "=", "20", ",", "\n", "device", "=", "'cpu'", "\n", ")", ":", "\n", "# TODO: Parallelize?", "\n", "        ", "losses", "=", "defaultdict", "(", "list", ")", "\n", "self", ".", "to", "(", "device", ")", "\n", "for", "i", ",", "model", "in", "enumerate", "(", "self", ".", "sub_models", ")", ":", "\n", "            ", "desc", "=", "f'Training model {i}'", "\n", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "\n", "val_data", "=", "None", "\n", "if", "use_val", "or", "early_stop", ":", "\n", "                ", "val_size", "=", "int", "(", "0.1", "*", "len", "(", "train_data", ")", ")", "\n", "train_sub", ",", "val_data", "=", "random_split", "(", "train_data", ",", "[", "(", "len", "(", "train_data", ")", "-", "val_size", ")", ",", "val_size", "]", ")", "\n", "\n", "# Create training Dataset for use class-balanced sampling", "\n", "train_posts", "=", "{", "\n", "train_data", ".", "post_ids", "[", "p_id", "]", ":", "train_data", ".", "posts", "[", "train_data", ".", "post_ids", "[", "p_id", "]", "]", "\n", "for", "p_id", "in", "train_sub", ".", "indices", "\n", "}", "\n", "\n", "train_sub", "=", "PostDataset", "(", "posts", "=", "train_posts", ",", "\n", "embedding", "=", "train_data", ".", "embedding", ",", "\n", "labeled", "=", "train_data", ".", "labeled", ",", "\n", "padded_seq", "=", "train_data", ".", "padded_seq", ")", "\n", "", "else", ":", "\n", "                ", "train_sub", "=", "train_data", "\n", "\n", "", "t_loss", ",", "v_loss", "=", "model", ".", "train_model", "(", "train_sub", ",", "\n", "loss_fn", ",", "\n", "optimizer", ",", "\n", "val_data", ",", "\n", "n_samples", ",", "\n", "early_stop", ",", "\n", "batch_size", ",", "\n", "EPOCHS", ",", "\n", "device", ",", "\n", "desc", "=", "desc", ")", "\n", "\n", "losses", "[", "'train'", "]", ".", "append", "(", "t_loss", ")", "\n", "losses", "[", "'val'", "]", ".", "append", "(", "v_loss", ")", "\n", "\n", "", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.tune_models": [[150, 165], ["model.DelhateEnsemble.to", "model.tune_model", "losses.append"], "methods", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.tune_model"], ["", "def", "tune_models", "(", "\n", "self", ",", "\n", "data", ",", "\n", "lr", "=", "5e-4", ",", "\n", "batch_size", "=", "16", ",", "\n", "EPOCHS", "=", "10", ",", "\n", "device", "=", "'cpu'", "\n", ")", ":", "\n", "        ", "self", ".", "to", "(", "device", ")", "\n", "losses", "=", "[", "]", "\n", "for", "model", "in", "self", ".", "sub_models", ":", "\n", "            ", "tune_loss", "=", "model", ".", "tune_model", "(", "data", ",", "lr", ",", "batch_size", ",", "EPOCHS", ",", "device", ")", "\n", "losses", ".", "append", "(", "tune_loss", ")", "\n", "\n", "", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.get_model_attributes": [[166, 179], ["None"], "methods", ["None"], ["", "def", "get_model_attributes", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'n_models'", ":", "self", ".", "n_models", ",", "\n", "'seq_length'", ":", "self", ".", "seq_length", ",", "\n", "'embed_corpus'", ":", "self", ".", "embed_corpus", ",", "\n", "'embed_dim'", ":", "self", ".", "embed_dim", ",", "\n", "'n_classes'", ":", "self", ".", "n_classes", ",", "\n", "'n_filters'", ":", "self", ".", "n_filters", ",", "\n", "'filter_width'", ":", "self", ".", "filter_width", ",", "\n", "'pool_size'", ":", "self", ".", "pool_size", ",", "\n", "'n_hidden'", ":", "self", ".", "n_hidden", ",", "\n", "'dropout'", ":", "self", ".", "dropout", ",", "\n", "'rnn_type'", ":", "self", ".", "rnn_type", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.save_model": [[181, 183], ["torch.save", "torch.save", "torch.save", "torch.save", "model.DelhateEnsemble.get_model_attributes", "model.DelhateEnsemble.state_dict"], "methods", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.get_model_attributes"], ["", "def", "save_model", "(", "self", ",", "path", ")", ":", "\n", "        ", "torch", ".", "save", "(", "(", "self", ".", "get_model_attributes", "(", ")", ",", "self", ".", "state_dict", "(", ")", ")", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.DelhateEnsemble.load_model": [[184, 203], ["torch.load", "torch.load", "torch.load", "torch.load", "cls", "cls.load_state_dict"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "load_model", "(", "cls", ",", "model_path", ")", ":", "\n", "        ", "model_attr", ",", "state_dict", "=", "torch", ".", "load", "(", "model_path", ")", "\n", "model", "=", "cls", "(", "\n", "n_models", "=", "model_attr", "[", "'n_models'", "]", ",", "\n", "seq_length", "=", "model_attr", "[", "'seq_length'", "]", ",", "\n", "embed_corpus", "=", "model_attr", "[", "'embed_corpus'", "]", ",", "\n", "embed_dim", "=", "model_attr", "[", "'embed_dim'", "]", ",", "\n", "n_classes", "=", "model_attr", "[", "'n_classes'", "]", ",", "\n", "n_filters", "=", "model_attr", "[", "'n_filters'", "]", ",", "\n", "filter_width", "=", "model_attr", "[", "'filter_width'", "]", ",", "\n", "pool_size", "=", "model_attr", "[", "'pool_size'", "]", ",", "\n", "n_hidden", "=", "model_attr", "[", "'n_hidden'", "]", ",", "\n", "rnn_type", "=", "model_attr", "[", "'rnn_type'", "]", ",", "\n", "dropout", "=", "model_attr", "[", "'dropout'", "]", "\n", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.__init__": [[206, 259], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.AdaptiveMaxPool1d", "torch.nn.AdaptiveMaxPool1d", "torch.nn.Sequential", "torch.nn.Sequential", "collections.OrderedDict", "model.ComponentModel.features.add_module", "collections.OrderedDict", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.GRU", "torch.nn.GRU", "ValueError", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "seq_length", ",", "\n", "embed_corpus", ",", "\n", "embed_dim", ",", "\n", "n_classes", ",", "\n", "n_filters", ",", "\n", "filter_width", ",", "\n", "pool_size", ",", "\n", "n_hidden", ",", "\n", "rnn_type", ",", "\n", "dropout", "\n", ")", ":", "\n", "        ", "super", "(", "ComponentModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "seq_length", "=", "seq_length", "\n", "self", ".", "embed_corpus", "=", "embed_corpus", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "n_classes", "=", "n_classes", "\n", "self", ".", "n_filters", "=", "n_filters", "\n", "self", ".", "filter_width", "=", "filter_width", "\n", "self", ".", "pool_size", "=", "pool_size", "\n", "self", ".", "n_hidden", "=", "n_hidden", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "if", "self", ".", "rnn_type", "is", "None", ":", "\n", "            ", "rnn", "=", "None", "\n", "", "elif", "self", ".", "rnn_type", "==", "'lstm'", ":", "\n", "            ", "rnn", "=", "nn", ".", "LSTM", "(", "self", ".", "embed_dim", "//", "self", ".", "pool_size", ",", "self", ".", "n_hidden", ",", "batch_first", "=", "True", ")", "\n", "", "elif", "self", ".", "rnn_type", "==", "'gru'", ":", "\n", "            ", "rnn", "=", "nn", ".", "GRU", "(", "self", ".", "embed_dim", "//", "self", ".", "pool_size", ",", "self", ".", "n_hidden", ",", "batch_first", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid RNN type selected. Options are gru/lstm/None'", ")", "\n", "\n", "", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'conv1d'", ",", "nn", ".", "Conv1d", "(", "self", ".", "seq_length", ",", "self", ".", "n_filters", ",", "self", ".", "filter_width", ",", "\n", "padding", "=", "(", "self", ".", "filter_width", "-", "1", ")", "//", "2", ")", ")", ",", "\n", "(", "'batchnorm'", ",", "nn", ".", "BatchNorm1d", "(", "self", ".", "n_filters", ")", ")", ",", "\n", "(", "'relu'", ",", "nn", ".", "ReLU", "(", ")", ")", ",", "\n", "(", "'maxpool'", ",", "nn", ".", "MaxPool1d", "(", "self", ".", "pool_size", ")", ")", "\n", "]", ")", ")", "\n", "\n", "if", "rnn", ":", "\n", "            ", "self", ".", "features", ".", "add_module", "(", "'rnn'", ",", "rnn", ")", "\n", "\n", "", "self", ".", "globalpool", "=", "nn", ".", "AdaptiveMaxPool1d", "(", "self", ".", "n_hidden", ")", "\n", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'linear_h'", ",", "nn", ".", "Linear", "(", "self", ".", "n_hidden", ",", "self", ".", "n_hidden", "//", "2", ")", ")", ",", "\n", "(", "'relu'", ",", "nn", ".", "ReLU", "(", ")", ")", ",", "\n", "(", "'dropout'", ",", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout", ")", ")", ",", "\n", "(", "'linear_out'", ",", "nn", ".", "Linear", "(", "self", ".", "n_hidden", "//", "2", ",", "self", ".", "n_classes", ")", ")", "\n", "]", ")", ")", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.forward": [[261, 286], ["model.ComponentModel.globalpool", "model.ComponentModel.classifier", "x.contiguous().view.contiguous().view.squeeze", "model.ComponentModel.features", "x.contiguous().view.contiguous().view.view", "x.contiguous().view.contiguous().view.contiguous().view", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "model.ComponentModel.features", "model.ComponentModel.features", "x.contiguous().view.contiguous().view.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "rnn_type", "is", "None", ":", "\n", "            ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "n_filters", "*", "self", ".", "embed_dim", "//", "self", ".", "pool_size", ")", "\n", "# h = torch.zeros(x.shape[0], 1, self.n_hidden, device=x.device)", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "rnn_type", "==", "'lstm'", ":", "\n", "                ", "x", ",", "(", "h", ",", "c", ")", "=", "self", ".", "features", "(", "x", ")", "\n", "", "elif", "self", ".", "rnn_type", "==", "'gru'", ":", "\n", "                ", "x", ",", "h", "=", "self", ".", "features", "(", "x", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "\n", "\n", "", "x", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "n_filters", "*", "self", ".", "n_hidden", ")", "\n", "h", "=", "torch", ".", "transpose", "(", "h", ",", "0", ",", "1", ")", "\n", "\n", "", "x", "=", "self", ".", "globalpool", "(", "x", ")", "\n", "\n", "# Include hidden state for classifier?", "\n", "# If so, need to change classifier input shape", "\n", "# x = torch.cat([x, h], dim=-1)", "\n", "\n", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "\n", "return", "x", ".", "squeeze", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.evaluate": [[287, 307], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.ComponentModel.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.ComponentModel.", "criterion().item", "torch.softmax().argmax", "torch.softmax().argmax", "torch.softmax().argmax", "torch.softmax().argmax", "y_pred.extend", "y_true.extend", "len", "X.to", "y.to", "torch.softmax().argmax.tolist", "torch.softmax().argmax.tolist", "y.tolist", "criterion", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["None"], ["", "def", "evaluate", "(", "self", ",", "data", ",", "criterion", ",", "batch_size", "=", "64", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "data_loader", "=", "DataLoader", "(", "data", ",", "batch_size", "=", "batch_size", ")", "\n", "\n", "y_pred", ",", "y_true", "=", "[", "]", ",", "[", "]", "\n", "val_loss", "=", "0", "\n", "\n", "self", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "X", ",", "y", "in", "data_loader", ":", "\n", "                ", "X", ",", "y", "=", "X", ".", "to", "(", "device", ")", ",", "y", ".", "to", "(", "device", ")", "\n", "\n", "logits", "=", "self", "(", "X", ")", "\n", "val_loss", "+=", "criterion", "(", "logits", ",", "y", ")", ".", "item", "(", ")", "\n", "\n", "preds", "=", "torch", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "\n", "y_pred", ".", "extend", "(", "preds", ".", "tolist", "(", ")", ")", "\n", "y_true", ".", "extend", "(", "y", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "return", "y_pred", ",", "y_true", ",", "(", "val_loss", "/", "len", "(", "data_loader", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.train_model": [[308, 371], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.ComponentModel.to", "tqdm.tqdm.tqdm", "range", "model.ComponentModel.train", "train_losses.append", "model.ComponentModel.load_state_dict", "train_data.sample_classes", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "optimizer.zero_grad", "model.ComponentModel.", "criterion", "criterion.backward", "optimizer.step", "criterion.item", "model.ComponentModel.evaluate", "val_losses.append", "X.to", "y.to", "len", "model.ComponentModel.state_dict"], "methods", ["home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.dataset.PostDataset.sample_classes", "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.evaluate"], ["", "def", "train_model", "(", "\n", "self", ",", "\n", "train_data", ",", "\n", "criterion", ",", "\n", "optimizer", ",", "\n", "val_data", "=", "None", ",", "\n", "n_samples", "=", "1000", ",", "\n", "early_stop", "=", "False", ",", "\n", "batch_size", "=", "24", ",", "\n", "EPOCHS", "=", "20", ",", "\n", "device", "=", "'cpu'", ",", "\n", "desc", "=", "''", "\n", ")", ":", "\n", "\n", "        ", "best_model", ",", "best_epoch", "=", "None", ",", "-", "1", "\n", "val_loss_min", "=", "np", ".", "Inf", "\n", "\n", "if", "early_stop", ":", "\n", "            ", "assert", "val_data", "\n", "\n", "# If using heuristic bounds and weak loss, use entire training data at each epoch (no class balancing)", "\n", "", "train_loader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "\n", "self", ".", "to", "(", "device", ")", "\n", "train_losses", ",", "val_losses", "=", "[", "]", ",", "[", "]", "\n", "for", "epoch", "in", "tqdm", "(", "range", "(", "EPOCHS", ")", ",", "desc", "=", "desc", ")", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "running_loss", "=", "0", "\n", "\n", "# If using labeled data and cross entropy loss,", "\n", "# sample training data at each epoch to create a class-balanced training subset", "\n", "if", "train_data", ".", "labeled", ":", "\n", "                ", "train_sample", "=", "train_data", ".", "sample_classes", "(", "n_samples", "=", "n_samples", ")", "\n", "train_loader", "=", "DataLoader", "(", "train_sample", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "\n", "", "for", "X", ",", "y", "in", "train_loader", ":", "\n", "                ", "X", ",", "y", "=", "X", ".", "to", "(", "device", ")", ",", "y", ".", "to", "(", "device", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "logits", "=", "self", "(", "X", ")", "\n", "loss", "=", "criterion", "(", "logits", ",", "y", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "running_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "", "train_losses", ".", "append", "(", "running_loss", "/", "len", "(", "train_loader", ")", ")", "\n", "\n", "if", "val_data", ":", "\n", "                ", "_", ",", "_", ",", "val_loss", "=", "self", ".", "evaluate", "(", "val_data", ",", "criterion", ",", "batch_size", "=", "batch_size", ",", "device", "=", "device", ")", "\n", "val_losses", ".", "append", "(", "val_loss", ")", "\n", "\n", "if", "early_stop", "and", "val_loss", "<=", "val_loss_min", ":", "\n", "                    ", "val_loss_min", "=", "val_loss", "\n", "best_model", "=", "self", ".", "state_dict", "(", ")", "\n", "best_epoch", "=", "epoch", "\n", "\n", "", "", "", "if", "early_stop", ":", "\n", "            ", "self", ".", "load_state_dict", "(", "best_model", ")", "\n", "\n", "", "return", "train_losses", ",", "val_losses", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.model.ComponentModel.tune_model": [[372, 411], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.ComponentModel.features.parameters", "torch.optim.Adam", "torch.optim.Adam", "model.ComponentModel.to", "tqdm.tqdm.tqdm", "model.ComponentModel.parameters", "model.ComponentModel.train", "losses.append", "torch.optim.Adam.zero_grad", "model.ComponentModel.", "criterion", "criterion.backward", "torch.optim.Adam.step", "criterion.item", "X.to", "y.to", "len"], "methods", ["None"], ["", "def", "tune_model", "(", "\n", "self", ",", "\n", "data", ",", "\n", "lr", ",", "\n", "batch_size", "=", "32", ",", "\n", "EPOCHS", "=", "20", ",", "\n", "device", "=", "'cpu'", "\n", ")", ":", "\n", "        ", "train_loader", "=", "DataLoader", "(", "data", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "\n", "# Freeze feature extraction component", "\n", "for", "param", "in", "self", ".", "features", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "criterion", "=", "F", ".", "cross_entropy", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "\n", "self", ".", "to", "(", "device", ")", "\n", "losses", "=", "[", "]", "\n", "for", "epoch", "in", "tqdm", "(", "EPOCHS", ")", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "running_loss", "=", "0", "\n", "\n", "for", "X", ",", "y", "in", "train_loader", ":", "\n", "                ", "X", ",", "y", "=", "X", ".", "to", "(", "device", ")", ",", "y", ".", "to", "(", "device", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "logits", "=", "self", "(", "X", ")", "\n", "loss", "=", "criterion", "(", "logits", ",", "y", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "running_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "", "losses", ".", "append", "(", "running_loss", "/", "len", "(", "train_loader", ")", ")", "\n", "\n", "", "return", "losses", "\n", "", "", ""]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.clean_text": [[17, 52], ["re.sub.lower", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "set", "re.sub", "re.sub", "re.sub.strip", "nltk.corpus.stopwords.words", "set().union", "stemmer.stem", "set", "nltk.tokenize.word_tokenize"], "function", ["None"], ["def", "clean_text", "(", "text", ",", "stemmer", ",", "stops", "=", "None", ")", ":", "\n", "    ", "text", "=", "text", ".", "lower", "(", ")", "\n", "\n", "# Remove urls", "\n", "www_exp", "=", "r'www.[^ ]+'", "\n", "http_exp", "=", "r'https?[^\\s]+'", "\n", "text", "=", "re", ".", "sub", "(", "'|'", ".", "join", "(", "(", "www_exp", ",", "http_exp", ")", ")", ",", "''", ",", "text", ")", "\n", "\n", "# Remove encoded Unicode symbols (removes emoticons etc.)", "\n", "text", "=", "re", ".", "sub", "(", "r'&.*?;'", ",", "' '", ",", "text", ")", "\n", "\n", "# Replace user mentions and hashtags with placeholder", "\n", "text", "=", "re", ".", "sub", "(", "r'#([\\w\\-]+)'", ",", "r' HASHTAGHERE \\1'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'@[\\w\\-]+'", ",", "r'MENTIONHERE '", ",", "text", ")", "\n", "\n", "# Remove non-letter chars", "\n", "text", "=", "re", ".", "sub", "(", "r'[^a-zA-Z\\s]'", ",", "' '", ",", "text", ")", "\n", "\n", "# Remove extra whitespace", "\n", "text", "=", "re", ".", "sub", "(", "r'\\s+'", ",", "' '", ",", "text", ")", "\n", "\n", "# Remove stop words and apply stemming", "\n", "nltk_stops", "=", "set", "(", "stopwords", ".", "words", "(", "'english'", ")", ")", "\n", "stops", "=", "set", "(", "stops", ")", ".", "union", "(", "nltk_stops", ")", "if", "stops", "else", "nltk_stops", "\n", "text", "=", "' '", ".", "join", "(", "\n", "stemmer", ".", "stem", "(", "word", ")", "\n", "for", "word", "in", "word_tokenize", "(", "text", ")", "\n", "if", "word", "not", "in", "stops", "\n", ")", "\n", "\n", "# Replace stemmed mention and hashtag placeholders", "\n", "text", "=", "re", ".", "sub", "(", "r'mentionher'", ",", "'MENTIONHERE'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'hashtagher'", ",", "'HASHTAGHERE'", ",", "text", ")", "\n", "\n", "return", "text", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.load_word_lists": [[54, 63], ["open", "set", "open", "set", "open", "set", "stemmer.stem", "stemmer.stem", "stemmer.stem", "word.strip", "word.strip", "word.strip"], "function", ["None"], ["", "def", "load_word_lists", "(", "stemmer", ")", ":", "\n", "    ", "with", "open", "(", "'data/word_lists/hate_words.txt'", ")", "as", "f", ":", "\n", "        ", "hate_words", "=", "set", "(", "stemmer", ".", "stem", "(", "word", ".", "strip", "(", ")", ")", "for", "word", "in", "f", ")", "\n", "", "with", "open", "(", "'data/word_lists/offensive_words.txt'", ")", "as", "f", ":", "\n", "        ", "offensive_words", "=", "set", "(", "stemmer", ".", "stem", "(", "word", ".", "strip", "(", ")", ")", "for", "word", "in", "f", ")", "\n", "", "with", "open", "(", "'data/word_lists/positive_words.txt'", ")", "as", "f", ":", "\n", "        ", "positive_words", "=", "set", "(", "stemmer", ".", "stem", "(", "word", ".", "strip", "(", ")", ")", "for", "word", "in", "f", ")", "\n", "\n", "", "return", "hate_words", ",", "offensive_words", ",", "positive_words", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.load_dataset": [[65, 70], ["dataset.PostDataset", "open", "json.load"], "function", ["None"], ["", "def", "load_dataset", "(", "dataset", ",", "split", ",", "embedding", ",", "labeled", ",", "pad", ")", ":", "\n", "    ", "with", "open", "(", "f'data/{dataset}/{dataset}_{split}.json'", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "return", "PostDataset", "(", "data", ",", "embedding", ",", "labeled", "=", "labeled", ",", "padded_seq", "=", "pad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.load_embedding": [[72, 108], ["os.makedirs", "os.makedirs", "os.makedirs", "torchtext.vocab.GloVe", "torchtext.vocab.GloVe", "torchtext.vocab.FastText", "torchtext.vocab.Vectors", "os.path.isfile", "ValueError", "torchtext.vocab.Vectors", "FileNotFoundError"], "function", ["None"], ["", "def", "load_embedding", "(", "embed_corpus", ")", ":", "\n", "    ", "corpora", "=", "[", "'glove_twitter'", ",", "'glove_commoncrawl'", ",", "'fasttext_wiki'", ",", "'fasttext_commoncrawl'", ",", "'word2vec'", "]", "\n", "dim", "=", "300", "\n", "\n", "os", ".", "makedirs", "(", "'data/glove'", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "'data/fast_text'", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "'data/word2vec'", ",", "exist_ok", "=", "True", ")", "\n", "\n", "if", "embed_corpus", "==", "'glove_twitter'", ":", "\n", "# GloVe trained on Twitter corpus", "\n", "        ", "embedding", "=", "GloVe", "(", "name", "=", "'twitter.27B'", ",", "dim", "=", "200", ",", "cache", "=", "'data/glove/'", ")", "\n", "dim", "=", "200", "\n", "", "elif", "embed_corpus", "==", "'glove_commoncrawl'", ":", "\n", "# GloVe trained on Common Crawl corpus", "\n", "        ", "embedding", "=", "GloVe", "(", "name", "=", "'42B'", ",", "dim", "=", "300", ",", "cache", "=", "'data/glove/'", ")", "\n", "", "elif", "embed_corpus", "==", "'fasttext_wiki'", ":", "\n", "# FastText trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset", "\n", "        ", "embedding", "=", "FastText", "(", "language", "=", "'en'", ",", "cache", "=", "'data/fast_text/'", ")", "\n", "", "elif", "embed_corpus", "==", "'fasttext_commoncrawl'", ":", "\n", "# FastText trained on Common Crawl corpus", "\n", "        ", "embedding", "=", "Vectors", "(", "name", "=", "'crawl-300d-2M.vec'", ",", "\n", "url", "=", "'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'", ",", "\n", "cache", "=", "'data/fast_text/'", ")", "\n", "", "elif", "embed_corpus", "==", "'word2vec'", ":", "\n", "# Word2Vec trained on Google New corpus", "\n", "        ", "name", "=", "'GoogleNews-vectors-negative300.txt'", "\n", "if", "os", ".", "path", ".", "isfile", "(", "f'data/word2vec/{name}.pt'", ")", ":", "\n", "            ", "embedding", "=", "Vectors", "(", "name", "=", "name", ",", "\n", "cache", "=", "'data/word2vec/'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "(", "'No torchtext formatted word2vec vectors file found. '", "\n", "'See load_word2vec.py to create the necessary pt file. Requires gensim.'", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f'Invalid pre-trained word embedding vectors. Options are {\"/\".join(corpora)}.'", ")", "\n", "\n", "", "return", "embedding", ",", "dim", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.weak_loss": [[110, 132], ["torch.tensor", "torch.softmax", "torch.ones_like", "torch.min", "torch.min", "torch.sum", "torch.sum", "torch.log", "torch.log"], "function", ["None"], ["", "def", "weak_loss", "(", "output", ",", "bounds", ",", "weight", "=", "(", "1.", ",", "1.", ",", "1.", ")", ")", ":", "\n", "    ", "weight", "=", "torch", ".", "tensor", "(", "weight", ",", "device", "=", "output", ".", "device", ")", "\n", "\n", "# Convert class scores to probabilities", "\n", "probs", "=", "torch", ".", "softmax", "(", "output", ",", "dim", "=", "1", ")", "\n", "\n", "LBs", "=", "bounds", "[", ":", ",", ":", ",", "0", "]", "\n", "UBs", "=", "bounds", "[", ":", ",", ":", ",", "1", "]", "\n", "\n", "ones", "=", "torch", ".", "ones_like", "(", "probs", ",", "device", "=", "output", ".", "device", ")", "\n", "\n", "# Loss is applied if output probability falls outside the heuristic bounds", "\n", "min_lb", "=", "torch", ".", "min", "(", "ones", ",", "(", "ones", "+", "probs", "-", "LBs", ")", ")", "\n", "min_ub", "=", "torch", ".", "min", "(", "ones", ",", "(", "ones", "+", "UBs", "-", "probs", ")", ")", "\n", "\n", "nll", "=", "-", "torch", ".", "log", "(", "min_lb", ")", "-", "torch", ".", "log", "(", "min_ub", ")", "\n", "\n", "loss", "=", "torch", ".", "sum", "(", "nll", ",", "dim", "=", "0", ")", "\n", "loss", "*=", "weight", "\n", "\n", "# Sum per-class loss", "\n", "return", "torch", ".", "sum", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.calculate_bounds": [[134, 179], ["len", "unique_words.intersection", "unique_words.intersection", "len", "len", "len", "set", "unique_words.intersection", "min", "max"], "function", ["None"], ["", "def", "calculate_bounds", "(", "tokens", ",", "hate_words", ",", "offensive_words", ",", "positive_words", ")", ":", "\n", "# TODO: Simplify bounds logic", "\n", "    ", "placeholders", "=", "{", "'HASHTAGHERE'", ",", "'MENTIONHERE'", "}", "\n", "unique_words", "=", "set", "(", "tokens", ")", "-", "placeholders", "\n", "num_unique", "=", "len", "(", "unique_words", ")", "\n", "\n", "post_hate", "=", "unique_words", ".", "intersection", "(", "hate_words", ")", "\n", "post_offensive", "=", "unique_words", ".", "intersection", "(", "offensive_words", ")", "-", "post_hate", "\n", "post_positive", "=", "unique_words", ".", "intersection", "(", "positive_words", ")", "\n", "\n", "num_hate", "=", "len", "(", "post_hate", ")", "\n", "num_offensive", "=", "len", "(", "post_offensive", ")", "\n", "num_positive", "=", "len", "(", "post_positive", ")", "\n", "try", ":", "\n", "        ", "if", "post_hate", ":", "\n", "            ", "hate_LB", "=", "min", "(", "1.", ",", "(", "num_hate", "+", "num_offensive", ")", "/", "num_unique", ")", "\n", "hate_UB", "=", "1.", "\n", "\n", "offensive_LB", "=", "0.5", "*", "num_offensive", "/", "num_unique", "\n", "offensive_UB", "=", "1", "-", "(", "num_hate", "+", "num_positive", ")", "/", "num_unique", "\n", "\n", "neither_LB", "=", "0.", "\n", "neither_UB", "=", "max", "(", "0.", ",", "1", "-", "(", "num_hate", "+", "num_offensive", ")", "/", "num_unique", ")", "\n", "", "elif", "post_offensive", ":", "\n", "            ", "hate_LB", "=", "0.", "\n", "hate_UB", "=", "1", "-", "(", "num_offensive", "+", "num_positive", ")", "/", "num_unique", "\n", "\n", "offensive_LB", "=", "(", "num_offensive", "+", "0.5", "*", "num_positive", ")", "/", "num_unique", "\n", "offensive_UB", "=", "1", "-", "0.5", "*", "num_positive", "/", "num_unique", "\n", "\n", "neither_LB", "=", "0.5", "*", "num_positive", "/", "num_unique", "\n", "neither_UB", "=", "1", "-", "(", "num_hate", "+", "num_offensive", ")", "/", "num_unique", "\n", "", "else", ":", "\n", "            ", "hate_LB", "=", "0.", "\n", "hate_UB", "=", "1", "-", "num_positive", "/", "num_unique", "\n", "\n", "offensive_LB", "=", "0.", "\n", "offensive_UB", "=", "1", "-", "num_positive", "/", "num_unique", "\n", "\n", "neither_LB", "=", "num_positive", "/", "num_unique", "\n", "neither_UB", "=", "1.", "\n", "", "", "except", "ZeroDivisionError", ":", "\n", "        ", "return", "(", "0.", ",", "1.", ")", ",", "(", "0.", ",", "1.", ")", ",", "(", "0.", ",", "1.", ")", "\n", "\n", "", "return", "(", "hate_LB", ",", "hate_UB", ")", ",", "(", "offensive_LB", ",", "offensive_UB", ")", ",", "(", "neither_LB", ",", "neither_UB", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.parse_train_args": [[181, 251], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "parse_train_args", "(", ")", ":", "\n", "    ", "parser", "=", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'dataset'", ",", "type", "=", "str", ",", "\n", "help", "=", "'Dataset on which to train the model. Options are hon/olid/combined/gab'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'model_name'", ",", "type", "=", "str", ",", "\n", "help", "=", "'Model name. The model attributes and state dict will be saved as <model-name>.pt.'", ")", "\n", "\n", "corpora", "=", "[", "'glove_twitter'", ",", "'glove_commoncrawl'", ",", "'fasttext_wiki'", ",", "'fasttext_commoncrawl'", ",", "'word2vec'", "]", "\n", "parser", ".", "add_argument", "(", "'--embed_corpus'", ",", "type", "=", "str", ",", "default", "=", "'glove_commoncrawl'", ",", "\n", "help", "=", "f'Pre-trained word embedding model/corpus to use. '", "\n", "f'Options are {\"/\".join(corpora)}. Default is glove_commoncrawl'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--pad'", ",", "default", "=", "50", ",", "\n", "help", "=", "'Length of padded input to pass to the model. Default is 50.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--n_models'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "'Number of models in the ensemble. Default is 5.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--n_filters'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Number of filters in the CNN layer. Default is 32.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--filter_width'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "'Width of each filter in the CNN layer. Default is 5.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--pool_size'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "'Kernel size for max pooling. Default is 4.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--n_hidden'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'Size of the hidden layers. Default is 100.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--rnn_type'", ",", "type", "=", "str", ",", "default", "=", "'gru'", ",", "\n", "help", "=", "'Type of RNN to use in the model. Options are gru/lstm/None. Default is gru'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "\n", "help", "=", "'Dropout probability during model training. Default is 0.2.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--use_val'", ",", "default", "=", "True", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Default True. Use validation data to evaluate model after each training epoch.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--early_stop'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Default False. Use early stopping during training to save model state at the epoch '", "\n", "'with minimum validation loss.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--use_weak_loss'", ",", "dest", "=", "'weak_loss'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Default False. Use heuristic class bounds and weak loss for training. '", "\n", "'When False, use true class labels and cross entropy loss.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--class_weight'", ",", "type", "=", "float", ",", "default", "=", "[", "1.", ",", "1.", ",", "1.", "]", ",", "nargs", "=", "'+'", ",", "\n", "help", "=", "'Per-class weight for weak loss calculation. Default (1., 1., 1.).'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--learn_rate'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ",", "\n", "help", "=", "'Learning rate for Adam optimizer during training. Default is 1e-3.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--n_samples'", ",", "default", "=", "1000", ",", "\n", "help", "=", "'Number of observations to sample from each class at each training epoch. '", "\n", "'May be an integer or \"all\" to set sample size equal to the number of observations '", "\n", "'in the smallest class. Default is 1000.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Batch size to use during training. Default is 32.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "'Number of training epochs. Default is 20.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--use_gpu'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "\n", "help", "=", "'Use GPU for model training. Default is True.'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.NASCL_DeL-haTE.None.utils.plot_loss": [[253, 270], ["matplotlib.figure", "matplotlib.plot", "matplotlib.legend", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "matplotlib.close", "matplotlib.plot", "matplotlib.vlines", "val_losses.index", "min", "max", "dataset.upper", "min"], "function", ["None"], ["", "def", "plot_loss", "(", "dataset", ",", "train_losses", ",", "val_losses", ",", "early_stop", "=", "False", ")", ":", "\n", "    ", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "12", ",", "8", ")", ")", "\n", "plt", ".", "plot", "(", "train_losses", ",", "label", "=", "'Training loss'", ")", "\n", "\n", "if", "val_losses", ":", "\n", "        ", "plt", ".", "plot", "(", "val_losses", ",", "label", "=", "'Validation loss'", ")", "\n", "if", "early_stop", ":", "\n", "            ", "plt", ".", "vlines", "(", "val_losses", ".", "index", "(", "min", "(", "val_losses", ")", ")", ",", "\n", "min", "(", "[", "*", "train_losses", ",", "*", "val_losses", "]", ")", ",", "max", "(", "[", "*", "train_losses", ",", "*", "val_losses", "]", ")", ",", "\n", "linestyles", "=", "'dashed'", ")", "\n", "\n", "", "", "plt", ".", "legend", "(", "frameon", "=", "False", ")", "\n", "plt", ".", "title", "(", "f'Model training loss on {dataset.upper()} data'", ")", "\n", "plt", ".", "xlabel", "(", "'Epoch'", ")", "\n", "plt", ".", "ylabel", "(", "'Avg. Loss per Batch'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "plt", ".", "close", "(", ")", "\n", "", ""]]}