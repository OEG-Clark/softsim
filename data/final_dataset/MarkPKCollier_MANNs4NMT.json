{"home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.iterator_utils.get_infer_iterator": [[39, 90], ["tensorflow.cast", "src_dataset.map.map", "src_dataset.map.map", "src_dataset.map.map", "src_dataset.map.apply", "src_dataset.apply.make_initializable_iterator", "batched_dataset.make_initializable_iterator.get_next", "iterator_utils.BatchedInput", "src_vocab_table.lookup", "src_dataset.map.map", "src_dataset.map.map", "tensorflow.contrib.data.padded_batch_and_drop_remainder", "tensorflow.constant", "tensorflow.cast", "tensorflow.string_split", "src_vocab_table.lookup", "tensorflow.reverse", "tensorflow.size", "tensorflow.TensorShape", "tensorflow.TensorShape"], "function", ["None"], ["", "def", "get_infer_iterator", "(", "\n", "src_dataset", ",", "src_vocab_table", ",", "batch_size", ",", "\n", "source_reverse", ",", "sos", ",", "eos", ",", "src_max_len", "=", "None", ")", ":", "\n", "  ", "src_eos_id", "=", "tf", ".", "cast", "(", "src_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "eos", ")", ")", ",", "tf", ".", "int32", ")", "\n", "src_dataset", "=", "src_dataset", ".", "map", "(", "lambda", "src", ":", "tf", ".", "string_split", "(", "[", "src", "]", ")", ".", "values", ")", "\n", "\n", "if", "src_max_len", ":", "\n", "    ", "src_dataset", "=", "src_dataset", ".", "map", "(", "lambda", "src", ":", "src", "[", ":", "src_max_len", "]", ")", "\n", "# Convert the word strings to ids", "\n", "", "src_dataset", "=", "src_dataset", ".", "map", "(", "\n", "lambda", "src", ":", "tf", ".", "cast", "(", "src_vocab_table", ".", "lookup", "(", "src", ")", ",", "tf", ".", "int32", ")", ")", "\n", "if", "source_reverse", ":", "\n", "    ", "src_dataset", "=", "src_dataset", ".", "map", "(", "lambda", "src", ":", "tf", ".", "reverse", "(", "src", ",", "axis", "=", "[", "0", "]", ")", ")", "\n", "# Add in the word counts.", "\n", "", "src_dataset", "=", "src_dataset", ".", "map", "(", "lambda", "src", ":", "(", "src", ",", "tf", ".", "size", "(", "src", ")", ")", ")", "\n", "\n", "# def batching_func(x):", "\n", "#   return x.padded_batch(", "\n", "#       batch_size,", "\n", "#       # The entry is the source line rows;", "\n", "#       # this has unknown-length vectors.  The last entry is", "\n", "#       # the source row size; this is a scalar.", "\n", "#       padded_shapes=(tf.TensorShape([src_max_len]),  # src", "\n", "#                      tf.TensorShape([])),     # src_len", "\n", "#       # Pad the source sequences with eos tokens.", "\n", "#       # (Though notice we don't generally need to do this since", "\n", "#       # later on we will be masking out calculations past the true sequence.", "\n", "#       padding_values=(src_eos_id,  # src", "\n", "#                       0))          # src_len -- unused", "\n", "\n", "# batched_dataset = batching_func(src_dataset)", "\n", "\n", "batched_dataset", "=", "src_dataset", ".", "apply", "(", "tf", ".", "contrib", ".", "data", ".", "padded_batch_and_drop_remainder", "(", "\n", "batch_size", ",", "\n", "padded_shapes", "=", "(", "tf", ".", "TensorShape", "(", "[", "src_max_len", "]", ")", ",", "# src", "\n", "tf", ".", "TensorShape", "(", "[", "]", ")", ")", ",", "# src_len", "\n", "\n", "padding_values", "=", "(", "src_eos_id", ",", "# src", "\n", "0", ")", ")", "# src_len -- unused", "\n", ")", "\n", "\n", "batched_iter", "=", "batched_dataset", ".", "make_initializable_iterator", "(", ")", "\n", "(", "src_ids", ",", "src_seq_len", ")", "=", "batched_iter", ".", "get_next", "(", ")", "\n", "return", "BatchedInput", "(", "\n", "initializer", "=", "batched_iter", ".", "initializer", ",", "\n", "source", "=", "src_ids", ",", "\n", "target_input", "=", "None", ",", "\n", "target_output", "=", "None", ",", "\n", "source_sequence_length", "=", "src_seq_len", ",", "\n", "target_sequence_length", "=", "None", ",", "\n", "handle", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.iterator_utils.get_iterator": [[92, 249], ["tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.contrib.data.Dataset.zip", "src_tgt_dataset.filter.shuffle", "src_tgt_dataset.filter.map", "src_tgt_dataset.filter.filter", "src_tgt_dataset.filter.map", "src_tgt_dataset.filter.map", "src_tgt_dataset.filter.map", "batching_func.make_initializable_iterator", "batched_dataset.make_initializable_iterator.get_next", "iterator_utils.BatchedInput", "src_vocab_table.lookup", "tgt_vocab_table.lookup", "tgt_vocab_table.lookup", "src_tgt_dataset.filter.skip", "src_tgt_dataset.filter.map", "src_tgt_dataset.filter.map", "src_tgt_dataset.filter.map", "src_tgt_dataset.filter.filter", "x.apply", "src_tgt_dataset.filter.group_by_window", "iterator_utils.get_iterator.batching_func"], "function", ["None"], ["", "def", "get_iterator", "(", "src_dataset", ",", "\n", "tgt_dataset", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "batch_size", ",", "\n", "sos", ",", "\n", "eos", ",", "\n", "source_reverse", ",", "\n", "random_seed", ",", "\n", "num_buckets", ",", "\n", "src_max_len", "=", "None", ",", "\n", "tgt_max_len", "=", "None", ",", "\n", "num_threads", "=", "4", ",", "\n", "output_buffer_size", "=", "None", ",", "\n", "skip_count", "=", "None", ",", "\n", "use_curriculum", "=", "False", ",", "\n", "curriculum_point_a", "=", "None", ",", "\n", "curriculum_point_b", "=", "None", ")", ":", "\n", "  ", "if", "not", "output_buffer_size", ":", "output_buffer_size", "=", "batch_size", "*", "1000", "\n", "src_eos_id", "=", "tf", ".", "cast", "(", "\n", "src_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "eos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "tgt_sos_id", "=", "tf", ".", "cast", "(", "\n", "tgt_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "sos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "tgt_eos_id", "=", "tf", ".", "cast", "(", "\n", "tgt_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "eos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "\n", "src_tgt_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "zip", "(", "(", "src_dataset", ",", "tgt_dataset", ")", ")", "\n", "\n", "if", "skip_count", "is", "not", "None", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "skip", "(", "skip_count", ")", "\n", "\n", "", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "shuffle", "(", "\n", "output_buffer_size", ",", "random_seed", ")", "\n", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "\n", "tf", ".", "string_split", "(", "[", "src", "]", ")", ".", "values", ",", "tf", ".", "string_split", "(", "[", "tgt", "]", ")", ".", "values", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "\n", "# Filter zero length input sequences.", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "filter", "(", "\n", "lambda", "src", ",", "tgt", ":", "tf", ".", "logical_and", "(", "tf", ".", "size", "(", "src", ")", ">", "0", ",", "tf", ".", "size", "(", "tgt", ")", ">", "0", ")", ")", "\n", "\n", "if", "src_max_len", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "src", "[", ":", "src_max_len", "]", ",", "tgt", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "", "if", "tgt_max_len", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "src", ",", "tgt", "[", ":", "tgt_max_len", "]", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "", "if", "source_reverse", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "tf", ".", "reverse", "(", "src", ",", "axis", "=", "[", "0", "]", ")", ",", "tgt", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "\n", "", "if", "use_curriculum", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "filter", "(", "\n", "lambda", "src", ",", "tgt", ":", "tf", ".", "logical_and", "(", "tf", ".", "size", "(", "src", ")", ">=", "curriculum_point_a", ",", "tf", ".", "size", "(", "src", ")", "<", "curriculum_point_b", ")", ")", "\n", "\n", "# Convert the word strings to ids.  Word strings that are not in the", "\n", "# vocab get the lookup table's default_value integer.", "\n", "", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "tf", ".", "cast", "(", "src_vocab_table", ".", "lookup", "(", "src", ")", ",", "tf", ".", "int32", ")", ",", "\n", "tf", ".", "cast", "(", "tgt_vocab_table", ".", "lookup", "(", "tgt", ")", ",", "tf", ".", "int32", ")", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "# Create a tgt_input prefixed with <sos> and a tgt_output suffixed with <eos>.", "\n", "# src_tgt_dataset = src_tgt_dataset.map(", "\n", "#     lambda src, tgt: (src,", "\n", "#                       tf.concat(([tgt_sos_id], [tgt_sos_id], tgt), 0),", "\n", "#                       tf.concat(([tgt_sos_id], tgt, [tgt_eos_id]), 0)),", "\n", "#     num_threads=num_threads, output_buffer_size=output_buffer_size)", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "src", ",", "\n", "tf", ".", "concat", "(", "(", "[", "tgt_sos_id", "]", ",", "tgt", ")", ",", "0", ")", ",", "\n", "tf", ".", "concat", "(", "(", "tgt", ",", "[", "tgt_eos_id", "]", ")", ",", "0", ")", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "# Add in sequence lengths.", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt_in", ",", "tgt_out", ":", "(", "\n", "src", ",", "tgt_in", ",", "tgt_out", ",", "tf", ".", "size", "(", "src", ")", ",", "tf", ".", "size", "(", "tgt_in", ")", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "# Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)", "\n", "def", "batching_func", "(", "x", ")", ":", "\n", "    ", "return", "x", ".", "apply", "(", "tf", ".", "contrib", ".", "data", ".", "padded_batch_and_drop_remainder", "(", "\n", "batch_size", ",", "\n", "padded_shapes", "=", "(", "tf", ".", "TensorShape", "(", "[", "src_max_len", "]", ")", ",", "# src", "\n", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", ",", "# tgt_input", "\n", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", ",", "# tgt_output", "\n", "tf", ".", "TensorShape", "(", "[", "]", ")", ",", "# src_len", "\n", "tf", ".", "TensorShape", "(", "[", "]", ")", ")", ",", "# tgt_len", "\n", "\n", "padding_values", "=", "(", "src_eos_id", ",", "# src", "\n", "tgt_eos_id", ",", "# tgt_input", "\n", "tgt_eos_id", ",", "# tgt_output", "\n", "0", ",", "# src_len -- unused", "\n", "0", ")", "\n", ")", ")", "\n", "\n", "# return x.padded_batch(", "\n", "#     batch_size,", "\n", "#     # The first three entries are the source and target line rows;", "\n", "#     # these have unknown-length vectors.  The last two entries are", "\n", "#     # the source and target row sizes; these are scalars.", "\n", "#     padded_shapes=(tf.TensorShape([src_max_len]),  # src", "\n", "#                    tf.TensorShape([None]),  # tgt_input", "\n", "#                    tf.TensorShape([None]),  # tgt_output", "\n", "#                    tf.TensorShape([]),      # src_len", "\n", "#                    tf.TensorShape([])),     # tgt_len", "\n", "#     # Pad the source and target sequences with eos tokens.", "\n", "#     # (Though notice we don't generally need to do this since", "\n", "#     # later on we will be masking out calculations past the true sequence.", "\n", "#     padding_values=(src_eos_id,  # src", "\n", "#                     tgt_eos_id,  # tgt_input", "\n", "#                     tgt_eos_id,  # tgt_output", "\n", "#                     0,           # src_len -- unused", "\n", "#                     0))          # tgt_len -- unused", "\n", "", "if", "num_buckets", ">", "1", ":", "\n", "    ", "def", "key_func", "(", "unused_1", ",", "unused_2", ",", "unused_3", ",", "src_len", ",", "tgt_len", ")", ":", "\n", "# Calculate bucket_width by maximum source sequence length.", "\n", "# Pairs with length [0, bucket_width) go to bucket 0, length", "\n", "# [bucket_width, 2 * bucket_width) go to bucket 1, etc.  Pairs with length", "\n", "# over ((num_bucket-1) * bucket_width) words all go into the last bucket.", "\n", "      ", "if", "src_max_len", ":", "\n", "        ", "bucket_width", "=", "(", "src_max_len", "+", "num_buckets", "-", "1", ")", "//", "num_buckets", "\n", "", "else", ":", "\n", "        ", "bucket_width", "=", "10", "\n", "\n", "# Bucket sentence pairs by the length of their source sentence and target", "\n", "# sentence.", "\n", "", "bucket_id", "=", "tf", ".", "maximum", "(", "src_len", "//", "bucket_width", ",", "tgt_len", "//", "bucket_width", ")", "\n", "return", "tf", ".", "to_int64", "(", "tf", ".", "minimum", "(", "num_buckets", ",", "bucket_id", ")", ")", "\n", "", "def", "reduce_func", "(", "unused_key", ",", "windowed_data", ")", ":", "\n", "      ", "return", "batching_func", "(", "windowed_data", ")", "\n", "", "batched_dataset", "=", "src_tgt_dataset", ".", "group_by_window", "(", "\n", "key_func", "=", "key_func", ",", "reduce_func", "=", "reduce_func", ",", "window_size", "=", "batch_size", ")", "\n", "", "else", ":", "\n", "    ", "batched_dataset", "=", "batching_func", "(", "src_tgt_dataset", ")", "\n", "", "batched_iter", "=", "batched_dataset", ".", "make_initializable_iterator", "(", ")", "\n", "(", "src_ids", ",", "tgt_input_ids", ",", "tgt_output_ids", ",", "src_seq_len", ",", "tgt_seq_len", ")", "=", "(", "\n", "batched_iter", ".", "get_next", "(", ")", ")", "\n", "return", "BatchedInput", "(", "\n", "initializer", "=", "batched_iter", ".", "initializer", ",", "\n", "source", "=", "src_ids", ",", "\n", "target_input", "=", "tgt_input_ids", ",", "\n", "target_output", "=", "tgt_output_ids", ",", "\n", "source_sequence_length", "=", "src_seq_len", ",", "\n", "target_sequence_length", "=", "tgt_seq_len", ",", "\n", "handle", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.iterator_utils.get_feedable_iterator": [[251, 398], ["range", "tensorflow.placeholder", "tensorflow.contrib.data.Iterator.from_string_handle", "tf.contrib.data.Iterator.from_string_handle.get_next", "iterator_utils.BatchedInput", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.contrib.data.Dataset.zip", "src_tgt_dataset.map.shuffle", "src_tgt_dataset.map.map", "src_tgt_dataset.map.filter", "src_tgt_dataset.map.filter", "src_tgt_dataset.map.map", "src_tgt_dataset.map.map", "src_tgt_dataset.map.map", "src_tgt_dataset.map.apply", "iterators.append", "src_vocab_table.lookup", "tgt_vocab_table.lookup", "tgt_vocab_table.lookup", "src_tgt_dataset.map.skip", "src_tgt_dataset.map.map", "src_tgt_dataset.map.map", "src_tgt_dataset.map.map", "tensorflow.contrib.data.padded_batch_and_drop_remainder", "get_dataset().make_initializable_iterator", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.logical_and", "tensorflow.logical_and", "tensorflow.cast", "tensorflow.cast", "tensorflow.concat", "tensorflow.concat", "tensorflow.size", "tensorflow.size", "iterator_utils.get_feedable_iterator.get_dataset"], "function", ["None"], ["", "def", "get_feedable_iterator", "(", "hparams", ",", "\n", "src_dataset", ",", "\n", "tgt_dataset", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "batch_size", ",", "\n", "sos", ",", "\n", "eos", ",", "\n", "source_reverse", ",", "\n", "random_seed", ",", "\n", "num_buckets", ",", "\n", "src_max_len", "=", "None", ",", "\n", "tgt_max_len", "=", "None", ",", "\n", "num_threads", "=", "4", ",", "\n", "output_buffer_size", "=", "None", ",", "\n", "skip_count", "=", "None", ")", ":", "\n", "  ", "if", "not", "output_buffer_size", ":", "output_buffer_size", "=", "batch_size", "*", "1000", "\n", "def", "get_dataset", "(", "curriculum_point_a", ",", "curriculum_point_b", ")", ":", "\n", "    ", "src_eos_id", "=", "tf", ".", "cast", "(", "\n", "src_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "eos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "tgt_sos_id", "=", "tf", ".", "cast", "(", "\n", "tgt_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "sos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "tgt_eos_id", "=", "tf", ".", "cast", "(", "\n", "tgt_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "eos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "\n", "src_tgt_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "zip", "(", "(", "src_dataset", ",", "tgt_dataset", ")", ")", "\n", "\n", "if", "skip_count", "is", "not", "None", ":", "\n", "      ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "skip", "(", "skip_count", ")", "\n", "\n", "", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "shuffle", "(", "\n", "output_buffer_size", ",", "random_seed", ")", "\n", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "\n", "tf", ".", "string_split", "(", "[", "src", "]", ")", ".", "values", ",", "tf", ".", "string_split", "(", "[", "tgt", "]", ")", ".", "values", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "\n", "# Filter zero length input sequences.", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "filter", "(", "\n", "lambda", "src", ",", "tgt", ":", "tf", ".", "logical_and", "(", "tf", ".", "size", "(", "src", ")", ">", "0", ",", "tf", ".", "size", "(", "tgt", ")", ">", "0", ")", ")", "\n", "\n", "if", "src_max_len", ":", "\n", "      ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "src", "[", ":", "src_max_len", "]", ",", "tgt", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "", "if", "tgt_max_len", ":", "\n", "      ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "src", ",", "tgt", "[", ":", "tgt_max_len", "]", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "", "if", "source_reverse", ":", "\n", "      ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "tf", ".", "reverse", "(", "src", ",", "axis", "=", "[", "0", "]", ")", ",", "tgt", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "\n", "", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "filter", "(", "\n", "lambda", "src", ",", "tgt", ":", "tf", ".", "logical_and", "(", "tf", ".", "size", "(", "src", ")", ">=", "curriculum_point_a", ",", "tf", ".", "size", "(", "src", ")", "<", "curriculum_point_b", ")", ")", "\n", "\n", "# Convert the word strings to ids.  Word strings that are not in the", "\n", "# vocab get the lookup table's default_value integer.", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "tf", ".", "cast", "(", "src_vocab_table", ".", "lookup", "(", "src", ")", ",", "tf", ".", "int32", ")", ",", "\n", "tf", ".", "cast", "(", "tgt_vocab_table", ".", "lookup", "(", "tgt", ")", ",", "tf", ".", "int32", ")", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "# Create a tgt_input prefixed with <sos> and a tgt_output suffixed with <eos>.", "\n", "# src_tgt_dataset = src_tgt_dataset.map(", "\n", "#     lambda src, tgt: (src,", "\n", "#                       tf.concat(([tgt_sos_id], [tgt_sos_id], tgt), 0),", "\n", "#                       tf.concat(([tgt_sos_id], tgt, [tgt_eos_id]), 0)),", "\n", "#     num_threads=num_threads, output_buffer_size=output_buffer_size)", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "src", ",", "\n", "tf", ".", "concat", "(", "(", "[", "tgt_sos_id", "]", ",", "tgt", ")", ",", "0", ")", ",", "\n", "tf", ".", "concat", "(", "(", "tgt", ",", "[", "tgt_eos_id", "]", ")", ",", "0", ")", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "# Add in sequence lengths.", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt_in", ",", "tgt_out", ":", "(", "\n", "src", ",", "tgt_in", ",", "tgt_out", ",", "tf", ".", "size", "(", "src", ")", ",", "tf", ".", "size", "(", "tgt_in", ")", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "# Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)", "\n", "# def batching_func(x):", "\n", "#   return x.padded_batch(", "\n", "#       batch_size,", "\n", "#       # The first three entries are the source and target line rows;", "\n", "#       # these have unknown-length vectors.  The last two entries are", "\n", "#       # the source and target row sizes; these are scalars.", "\n", "#       padded_shapes=(tf.TensorShape([src_max_len]),  # src", "\n", "#                      tf.TensorShape([None]),  # tgt_input", "\n", "#                      tf.TensorShape([None]),  # tgt_output", "\n", "#                      tf.TensorShape([]),      # src_len", "\n", "#                      tf.TensorShape([])),     # tgt_len", "\n", "#       # Pad the source and target sequences with eos tokens.", "\n", "#       # (Though notice we don't generally need to do this since", "\n", "#       # later on we will be masking out calculations past the true sequence.", "\n", "#       padding_values=(src_eos_id,  # src", "\n", "#                       tgt_eos_id,  # tgt_input", "\n", "#                       tgt_eos_id,  # tgt_output", "\n", "#                       0,           # src_len -- unused", "\n", "#                       0))          # tgt_len -- unused", "\n", "\n", "# batched_dataset = batching_func(src_tgt_dataset)", "\n", "\n", "batched_dataset", "=", "src_tgt_dataset", ".", "apply", "(", "tf", ".", "contrib", ".", "data", ".", "padded_batch_and_drop_remainder", "(", "\n", "batch_size", ",", "\n", "padded_shapes", "=", "(", "tf", ".", "TensorShape", "(", "[", "src_max_len", "]", ")", ",", "# src", "\n", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", ",", "# tgt_input", "\n", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", ",", "# tgt_output", "\n", "tf", ".", "TensorShape", "(", "[", "]", ")", ",", "# src_len", "\n", "tf", ".", "TensorShape", "(", "[", "]", ")", ")", ",", "# tgt_len", "\n", "\n", "padding_values", "=", "(", "src_eos_id", ",", "# src", "\n", "tgt_eos_id", ",", "# tgt_input", "\n", "tgt_eos_id", ",", "# tgt_output", "\n", "0", ",", "# src_len -- unused", "\n", "0", ")", "\n", ")", ")", "\n", "\n", "return", "batched_dataset", "\n", "\n", "", "iterators", "=", "[", "]", "\n", "for", "lesson", "in", "range", "(", "hparams", ".", "num_curriculum_buckets", ")", ":", "\n", "    ", "curriculum_point_a", "=", "lesson", "*", "(", "hparams", ".", "src_max_len", "//", "hparams", ".", "num_curriculum_buckets", ")", "+", "1", "\n", "curriculum_point_b", "=", "(", "lesson", "+", "1", ")", "*", "(", "hparams", ".", "src_max_len", "//", "hparams", ".", "num_curriculum_buckets", ")", "+", "1", "\n", "iterators", ".", "append", "(", "get_dataset", "(", "curriculum_point_a", ",", "curriculum_point_b", ")", ".", "make_initializable_iterator", "(", ")", ")", "\n", "\n", "", "handle", "=", "tf", ".", "placeholder", "(", "tf", ".", "string", ",", "shape", "=", "[", "]", ")", "\n", "iterator", "=", "tf", ".", "contrib", ".", "data", ".", "Iterator", ".", "from_string_handle", "(", "handle", ",", "iterators", "[", "0", "]", ".", "output_types", ",", "iterators", "[", "0", "]", ".", "output_shapes", ")", "\n", "(", "src_ids", ",", "tgt_input_ids", ",", "tgt_output_ids", ",", "src_seq_len", ",", "tgt_seq_len", ")", "=", "(", "\n", "iterator", ".", "get_next", "(", ")", ")", "\n", "\n", "return", "BatchedInput", "(", "\n", "initializer", "=", "iterators", ",", "\n", "source", "=", "src_ids", ",", "\n", "target_input", "=", "tgt_input_ids", ",", "\n", "target_output", "=", "tgt_output_ids", ",", "\n", "source_sequence_length", "=", "src_seq_len", ",", "\n", "target_sequence_length", "=", "tgt_seq_len", ",", "\n", "handle", "=", "handle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest._createTestInferCheckpoint": [[40, 58], ["model_helper.create_infer_model", "inference_test.InferenceTest.test_session", "model_helper.create_or_load_model", "loaded_model.saver.save", "os.path.join", "ValueError"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_infer_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_or_load_model"], ["  ", "def", "_createTestInferCheckpoint", "(", "self", ",", "hparams", ",", "out_dir", ")", ":", "\n", "    ", "if", "not", "hparams", ".", "attention", ":", "\n", "      ", "model_creator", "=", "nmt_model", ".", "Model", "\n", "", "elif", "hparams", ".", "attention_architecture", "==", "\"standard\"", ":", "\n", "      ", "model_creator", "=", "attention_model", ".", "AttentionModel", "\n", "", "elif", "hparams", ".", "attention_architecture", "in", "[", "\"gnmt\"", ",", "\"gnmt_v2\"", "]", ":", "\n", "      ", "model_creator", "=", "gnmt_model", ".", "GNMTModel", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unknown model architecture\"", ")", "\n", "\n", "", "infer_model", "=", "model_helper", ".", "create_infer_model", "(", "model_creator", ",", "hparams", ")", "\n", "with", "self", ".", "test_session", "(", "graph", "=", "infer_model", ".", "graph", ")", "as", "sess", ":", "\n", "      ", "loaded_model", ",", "global_step", "=", "model_helper", ".", "create_or_load_model", "(", "\n", "infer_model", ".", "model", ",", "out_dir", ",", "sess", ",", "\"infer_name\"", ")", "\n", "ckpt", "=", "loaded_model", ".", "saver", ".", "save", "(", "\n", "sess", ",", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"translate.ckpt\"", ")", ",", "\n", "global_step", "=", "global_step", ")", "\n", "", "return", "ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest.testBasicModel": [[59, 79], ["utils.common_test_utils.create_test_hparams", "utils.common_test_utils.create_test_hparams.add_hparam", "utils.common_test_utils.create_test_hparams.add_hparam", "os.path.join", "utils.common_test_utils.create_test_hparams.add_hparam", "os.makedirs", "os.path.join", "inference_test.InferenceTest._createTestInferCheckpoint", "inference.inference", "tensorflow.test.get_temp_dir", "open", "inference_test.InferenceTest.assertEqual", "len", "list"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest._createTestInferCheckpoint", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.inference"], ["", "def", "testBasicModel", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "\"uni\"", ",", "\n", "num_layers", "=", "1", ",", "\n", "attention", "=", "\"\"", ",", "\n", "attention_architecture", "=", "\"\"", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "vocab_prefix", "=", "\"nmt/testdata/test_infer_vocab\"", "\n", "hparams", ".", "add_hparam", "(", "\"src_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "src", ")", "\n", "hparams", ".", "add_hparam", "(", "\"tgt_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "tgt", ")", "\n", "\n", "infer_file", "=", "\"nmt/testdata/test_infer_file\"", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "tf", ".", "test", ".", "get_temp_dir", "(", ")", ",", "\"basic_infer\"", ")", "\n", "hparams", ".", "add_hparam", "(", "\"out_dir\"", ",", "out_dir", ")", "\n", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "output_infer", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"output_infer\"", ")", "\n", "ckpt", "=", "self", ".", "_createTestInferCheckpoint", "(", "hparams", ",", "out_dir", ")", "\n", "inference", ".", "inference", "(", "ckpt", ",", "infer_file", ",", "output_infer", ",", "hparams", ")", "\n", "with", "open", "(", "output_infer", ")", "as", "f", ":", "\n", "      ", "self", ".", "assertEqual", "(", "5", ",", "len", "(", "list", "(", "f", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest.testAttentionModel": [[80, 100], ["utils.common_test_utils.create_test_hparams", "utils.common_test_utils.create_test_hparams.add_hparam", "utils.common_test_utils.create_test_hparams.add_hparam", "os.path.join", "utils.common_test_utils.create_test_hparams.add_hparam", "os.makedirs", "os.path.join", "inference_test.InferenceTest._createTestInferCheckpoint", "inference.inference", "tensorflow.test.get_temp_dir", "open", "inference_test.InferenceTest.assertEqual", "len", "list"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest._createTestInferCheckpoint", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.inference"], ["", "", "def", "testAttentionModel", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "\"uni\"", ",", "\n", "num_layers", "=", "1", ",", "\n", "attention", "=", "\"scaled_luong\"", ",", "\n", "attention_architecture", "=", "\"standard\"", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "vocab_prefix", "=", "\"nmt/testdata/test_infer_vocab\"", "\n", "hparams", ".", "add_hparam", "(", "\"src_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "src", ")", "\n", "hparams", ".", "add_hparam", "(", "\"tgt_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "tgt", ")", "\n", "\n", "infer_file", "=", "\"nmt/testdata/test_infer_file\"", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "tf", ".", "test", ".", "get_temp_dir", "(", ")", ",", "\"attention_infer\"", ")", "\n", "hparams", ".", "add_hparam", "(", "\"out_dir\"", ",", "out_dir", ")", "\n", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "output_infer", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"output_infer\"", ")", "\n", "ckpt", "=", "self", ".", "_createTestInferCheckpoint", "(", "hparams", ",", "out_dir", ")", "\n", "inference", ".", "inference", "(", "ckpt", ",", "infer_file", ",", "output_infer", ",", "hparams", ")", "\n", "with", "open", "(", "output_infer", ")", "as", "f", ":", "\n", "      ", "self", ".", "assertEqual", "(", "5", ",", "len", "(", "list", "(", "f", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest.testMultiWorkers": [[101, 139], ["utils.common_test_utils.create_test_hparams", "utils.common_test_utils.create_test_hparams.add_hparam", "utils.common_test_utils.create_test_hparams.add_hparam", "os.path.join", "utils.common_test_utils.create_test_hparams.add_hparam", "os.makedirs", "os.path.join", "inference_test.InferenceTest._createTestInferCheckpoint", "inference.inference", "inference.inference", "inference.inference", "tensorflow.test.get_temp_dir", "open", "inference_test.InferenceTest.assertEqual", "len", "list"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest._createTestInferCheckpoint", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.inference", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.inference", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.inference"], ["", "", "def", "testMultiWorkers", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "\"uni\"", ",", "\n", "num_layers", "=", "2", ",", "\n", "attention", "=", "\"scaled_luong\"", ",", "\n", "attention_architecture", "=", "\"standard\"", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "vocab_prefix", "=", "\"nmt/testdata/test_infer_vocab\"", "\n", "hparams", ".", "add_hparam", "(", "\"src_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "src", ")", "\n", "hparams", ".", "add_hparam", "(", "\"tgt_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "tgt", ")", "\n", "\n", "infer_file", "=", "\"nmt/testdata/test_infer_file\"", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "tf", ".", "test", ".", "get_temp_dir", "(", ")", ",", "\"multi_worker_infer\"", ")", "\n", "hparams", ".", "add_hparam", "(", "\"out_dir\"", ",", "out_dir", ")", "\n", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "output_infer", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"output_infer\"", ")", "\n", "\n", "num_workers", "=", "3", "\n", "\n", "# There are 5 examples, make batch_size=3 makes job0 has 3 examples, job1", "\n", "# has 2 examples, and job2 has 0 example. This helps testing some edge", "\n", "# cases.", "\n", "hparams", ".", "batch_size", "=", "3", "\n", "\n", "ckpt", "=", "self", ".", "_createTestInferCheckpoint", "(", "hparams", ",", "out_dir", ")", "\n", "inference", ".", "inference", "(", "\n", "ckpt", ",", "infer_file", ",", "output_infer", ",", "hparams", ",", "num_workers", ",", "jobid", "=", "1", ")", "\n", "\n", "inference", ".", "inference", "(", "\n", "ckpt", ",", "infer_file", ",", "output_infer", ",", "hparams", ",", "num_workers", ",", "jobid", "=", "2", ")", "\n", "\n", "# Note: Need to start job 0 at the end; otherwise, it will block the testing", "\n", "# thread.", "\n", "inference", ".", "inference", "(", "\n", "ckpt", ",", "infer_file", ",", "output_infer", ",", "hparams", ",", "num_workers", ",", "jobid", "=", "0", ")", "\n", "\n", "with", "open", "(", "output_infer", ")", "as", "f", ":", "\n", "      ", "self", ".", "assertEqual", "(", "5", ",", "len", "(", "list", "(", "f", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest.testBasicModelWithInferIndices": [[140, 161], ["utils.common_test_utils.create_test_hparams", "utils.common_test_utils.create_test_hparams.add_hparam", "utils.common_test_utils.create_test_hparams.add_hparam", "os.path.join", "utils.common_test_utils.create_test_hparams.add_hparam", "os.makedirs", "os.path.join", "inference_test.InferenceTest._createTestInferCheckpoint", "inference.inference", "tensorflow.test.get_temp_dir", "open", "inference_test.InferenceTest.assertEqual", "len", "list"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest._createTestInferCheckpoint", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.inference"], ["", "", "def", "testBasicModelWithInferIndices", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "\"uni\"", ",", "\n", "num_layers", "=", "1", ",", "\n", "attention", "=", "\"\"", ",", "\n", "attention_architecture", "=", "\"\"", ",", "\n", "use_residual", "=", "False", ",", "\n", "inference_indices", "=", "[", "0", "]", ")", "\n", "vocab_prefix", "=", "\"nmt/testdata/test_infer_vocab\"", "\n", "hparams", ".", "add_hparam", "(", "\"src_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "src", ")", "\n", "hparams", ".", "add_hparam", "(", "\"tgt_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "tgt", ")", "\n", "\n", "infer_file", "=", "\"nmt/testdata/test_infer_file\"", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "tf", ".", "test", ".", "get_temp_dir", "(", ")", ",", "\"basic_infer_with_indices\"", ")", "\n", "hparams", ".", "add_hparam", "(", "\"out_dir\"", ",", "out_dir", ")", "\n", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "output_infer", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"output_infer\"", ")", "\n", "ckpt", "=", "self", ".", "_createTestInferCheckpoint", "(", "hparams", ",", "out_dir", ")", "\n", "inference", ".", "inference", "(", "ckpt", ",", "infer_file", ",", "output_infer", ",", "hparams", ")", "\n", "with", "open", "(", "output_infer", ")", "as", "f", ":", "\n", "      ", "self", ".", "assertEqual", "(", "1", ",", "len", "(", "list", "(", "f", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest.testAttentionModelWithInferIndices": [[162, 188], ["utils.common_test_utils.create_test_hparams", "utils.common_test_utils.create_test_hparams.add_hparam", "utils.common_test_utils.create_test_hparams.add_hparam", "os.path.join", "utils.common_test_utils.create_test_hparams.add_hparam", "os.makedirs", "os.path.join", "inference_test.InferenceTest._createTestInferCheckpoint", "inference.inference", "inference_test.InferenceTest.assertTrue", "inference_test.InferenceTest.assertTrue", "tensorflow.test.get_temp_dir", "open", "inference_test.InferenceTest.assertEqual", "os.path.exists", "os.path.exists", "len", "list", "str", "str"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference_test.InferenceTest._createTestInferCheckpoint", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.inference"], ["", "", "def", "testAttentionModelWithInferIndices", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "\"uni\"", ",", "\n", "num_layers", "=", "1", ",", "\n", "attention", "=", "\"scaled_luong\"", ",", "\n", "attention_architecture", "=", "\"standard\"", ",", "\n", "use_residual", "=", "False", ",", "\n", "inference_indices", "=", "[", "1", ",", "2", "]", ")", "\n", "# TODO(rzhao): Make infer indices support batch_size > 1.", "\n", "hparams", ".", "infer_batch_size", "=", "1", "\n", "vocab_prefix", "=", "\"nmt/testdata/test_infer_vocab\"", "\n", "hparams", ".", "add_hparam", "(", "\"src_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "src", ")", "\n", "hparams", ".", "add_hparam", "(", "\"tgt_vocab_file\"", ",", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "tgt", ")", "\n", "\n", "infer_file", "=", "\"nmt/testdata/test_infer_file\"", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "tf", ".", "test", ".", "get_temp_dir", "(", ")", ",", "\n", "\"attention_infer_with_indices\"", ")", "\n", "hparams", ".", "add_hparam", "(", "\"out_dir\"", ",", "out_dir", ")", "\n", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "output_infer", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"output_infer\"", ")", "\n", "ckpt", "=", "self", ".", "_createTestInferCheckpoint", "(", "hparams", ",", "out_dir", ")", "\n", "inference", ".", "inference", "(", "ckpt", ",", "infer_file", ",", "output_infer", ",", "hparams", ")", "\n", "with", "open", "(", "output_infer", ")", "as", "f", ":", "\n", "      ", "self", ".", "assertEqual", "(", "2", ",", "len", "(", "list", "(", "f", ")", ")", ")", "\n", "", "self", ".", "assertTrue", "(", "os", ".", "path", ".", "exists", "(", "output_infer", "+", "str", "(", "1", ")", "+", "\".png\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "os", ".", "path", ".", "exists", "(", "output_infer", "+", "str", "(", "2", ")", "+", "\".png\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.expand": [[4, 6], ["tensorflow.tile", "tensorflow.expand_dims", "range"], "function", ["None"], ["def", "expand", "(", "x", ",", "dim", ",", "N", ",", "dims", ")", ":", "\n", "    ", "return", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "x", ",", "dim", ")", ",", "multiples", "=", "[", "N", "]", "+", "[", "1", "for", "_", "in", "range", "(", "dims", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.learned_init": [[9, 12], ["tensorflow.squeeze", "tensorflow.contrib.layers.fully_connected", "tensorflow.ones"], "function", ["None"], ["", "def", "learned_init", "(", "units", ")", ":", "\n", "    ", "return", "tf", ".", "squeeze", "(", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "tf", ".", "ones", "(", "[", "1", ",", "1", "]", ")", ",", "units", ",", "\n", "activation_fn", "=", "None", ",", "biases_initializer", "=", "None", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.create_linear_initializer": [[13, 16], ["tensorflow.truncated_normal_initializer", "numpy.sqrt"], "function", ["None"], ["", "def", "create_linear_initializer", "(", "input_size", ",", "dtype", "=", "tf", ".", "float32", ")", ":", "\n", "    ", "stddev", "=", "1.0", "/", "np", ".", "sqrt", "(", "input_size", ")", "\n", "return", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "stddev", ",", "dtype", "=", "dtype", ")", "", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.vocab_utils.check_vocab": [[37, 70], ["tensorflow.gfile.Exists", "len", "misc_utils.print_out", "ValueError", "len", "misc_utils.print_out", "os.path.join", "codecs.getreader", "tensorflow.gfile.GFile", "vocab.append", "os.path.basename", "word.strip", "codecs.getwriter", "tensorflow.gfile.GFile", "f.write"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["def", "check_vocab", "(", "vocab_file", ",", "out_dir", ",", "sos", "=", "None", ",", "eos", "=", "None", ",", "unk", "=", "None", ")", ":", "\n", "  ", "\"\"\"Check if vocab_file doesn't exist, create from corpus_file.\"\"\"", "\n", "if", "tf", ".", "gfile", ".", "Exists", "(", "vocab_file", ")", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"# Vocab file %s exists\"", "%", "vocab_file", ")", "\n", "vocab", "=", "[", "]", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"rb\"", ")", ")", "as", "f", ":", "\n", "      ", "vocab_size", "=", "0", "\n", "for", "word", "in", "f", ":", "\n", "        ", "vocab_size", "+=", "1", "\n", "vocab", ".", "append", "(", "word", ".", "strip", "(", ")", ")", "\n", "\n", "# Verify if the vocab starts with unk, sos, eos", "\n", "# If not, prepend those tokens & generate a new vocab file", "\n", "", "", "if", "not", "unk", ":", "unk", "=", "UNK", "\n", "if", "not", "sos", ":", "sos", "=", "SOS", "\n", "if", "not", "eos", ":", "eos", "=", "EOS", "\n", "assert", "len", "(", "vocab", ")", ">=", "3", "\n", "if", "vocab", "[", "0", "]", "!=", "unk", "or", "vocab", "[", "1", "]", "!=", "sos", "or", "vocab", "[", "2", "]", "!=", "eos", ":", "\n", "      ", "utils", ".", "print_out", "(", "\"The first 3 vocab words [%s, %s, %s]\"", "\n", "\" are not [%s, %s, %s]\"", "%", "\n", "(", "vocab", "[", "0", "]", ",", "vocab", "[", "1", "]", ",", "vocab", "[", "2", "]", ",", "unk", ",", "sos", ",", "eos", ")", ")", "\n", "vocab", "=", "[", "unk", ",", "sos", ",", "eos", "]", "+", "vocab", "\n", "vocab_size", "+=", "3", "\n", "new_vocab_file", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "os", ".", "path", ".", "basename", "(", "vocab_file", ")", ")", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "new_vocab_file", ",", "\"wb\"", ")", ")", "as", "f", ":", "\n", "        ", "for", "word", "in", "vocab", ":", "\n", "          ", "f", ".", "write", "(", "\"%s\\n\"", "%", "word", ")", "\n", "", "", "vocab_file", "=", "new_vocab_file", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"vocab_file does not exist: \"", "+", "vocab_file", ")", "\n", "\n", "", "vocab_size", "=", "len", "(", "vocab", ")", "\n", "return", "vocab_size", ",", "vocab_file", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.vocab_utils.create_vocab_tables": [[72, 82], ["tensorflow.python.ops.lookup_ops.index_table_from_file", "tensorflow.python.ops.lookup_ops.index_table_from_file"], "function", ["None"], ["", "def", "create_vocab_tables", "(", "src_vocab_file", ",", "tgt_vocab_file", ",", "share_vocab", ")", ":", "\n", "  ", "\"\"\"Creates vocab tables for src_vocab_file and tgt_vocab_file.\"\"\"", "\n", "src_vocab_table", "=", "lookup_ops", ".", "index_table_from_file", "(", "\n", "src_vocab_file", ",", "default_value", "=", "UNK_ID", ")", "\n", "if", "share_vocab", ":", "\n", "    ", "tgt_vocab_table", "=", "src_vocab_table", "\n", "", "else", ":", "\n", "    ", "tgt_vocab_table", "=", "lookup_ops", ".", "index_table_from_file", "(", "\n", "tgt_vocab_file", ",", "default_value", "=", "UNK_ID", ")", "\n", "", "return", "src_vocab_table", ",", "tgt_vocab_table", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.get_initializer": [[26, 40], ["tensorflow.random_uniform_initializer", "tensorflow.contrib.keras.initializers.glorot_normal", "tensorflow.contrib.keras.initializers.glorot_uniform", "ValueError"], "function", ["None"], ["def", "get_initializer", "(", "init_op", ",", "seed", "=", "None", ",", "init_weight", "=", "None", ")", ":", "\n", "  ", "\"\"\"Create an initializer. init_weight is only for uniform.\"\"\"", "\n", "if", "init_op", "==", "\"uniform\"", ":", "\n", "    ", "assert", "init_weight", "\n", "return", "tf", ".", "random_uniform_initializer", "(", "\n", "-", "init_weight", ",", "init_weight", ",", "seed", "=", "seed", ")", "\n", "", "elif", "init_op", "==", "\"glorot_normal\"", ":", "\n", "    ", "return", "tf", ".", "contrib", ".", "keras", ".", "initializers", ".", "glorot_normal", "(", "\n", "seed", "=", "seed", ")", "\n", "", "elif", "init_op", "==", "\"glorot_uniform\"", ":", "\n", "    ", "return", "tf", ".", "contrib", ".", "keras", ".", "initializers", ".", "glorot_uniform", "(", "\n", "seed", "=", "seed", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unknown init_op %s\"", "%", "init_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.get_device_str": [[42, 48], ["None"], "function", ["None"], ["", "", "def", "get_device_str", "(", "device_id", ",", "num_gpus", ")", ":", "\n", "  ", "\"\"\"Return a device string for multi-GPU setup.\"\"\"", "\n", "if", "num_gpus", "==", "0", ":", "\n", "    ", "return", "\"/cpu:0\"", "\n", "", "device_str_output", "=", "\"/gpu:%d\"", "%", "(", "device_id", "%", "num_gpus", ")", "\n", "return", "device_str_output", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_train_model": [[56, 124], ["tensorflow.Graph", "model_helper.TrainModel", "tf.Graph.as_default", "vocab_utils.create_vocab_tables", "tensorflow.contrib.data.TextLineDataset", "tensorflow.contrib.data.TextLineDataset", "tensorflow.placeholder", "iterator_utils.get_iterator", "iterator_utils.get_feedable_iterator", "tensorflow.device", "model_creator"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.vocab_utils.create_vocab_tables", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_iterator", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.iterator_utils.get_feedable_iterator"], ["", "def", "create_train_model", "(", "\n", "model_creator", ",", "hparams", ",", "scope", "=", "None", ",", "\n", "single_cell_fn", "=", "None", ",", "model_device_fn", "=", "None", ")", ":", "\n", "  ", "\"\"\"Create train graph, model, and iterator.\"\"\"", "\n", "src_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "train_prefix", ",", "hparams", ".", "src", ")", "\n", "tgt_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "train_prefix", ",", "hparams", ".", "tgt", ")", "\n", "src_vocab_file", "=", "hparams", ".", "src_vocab_file", "\n", "tgt_vocab_file", "=", "hparams", ".", "tgt_vocab_file", "\n", "\n", "graph", "=", "tf", ".", "Graph", "(", ")", "\n", "\n", "with", "graph", ".", "as_default", "(", ")", ":", "\n", "    ", "src_vocab_table", ",", "tgt_vocab_table", "=", "vocab_utils", ".", "create_vocab_tables", "(", "\n", "src_vocab_file", ",", "tgt_vocab_file", ",", "hparams", ".", "share_vocab", ")", "\n", "\n", "src_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "TextLineDataset", "(", "src_file", ")", "\n", "tgt_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "TextLineDataset", "(", "tgt_file", ")", "\n", "skip_count_placeholder", "=", "tf", ".", "placeholder", "(", "shape", "=", "(", ")", ",", "dtype", "=", "tf", ".", "int64", ")", "\n", "\n", "if", "hparams", ".", "curriculum", "==", "'none'", ":", "\n", "      ", "iterator", "=", "iterator_utils", ".", "get_iterator", "(", "\n", "src_dataset", ",", "\n", "tgt_dataset", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "batch_size", "=", "hparams", ".", "batch_size", ",", "\n", "sos", "=", "hparams", ".", "sos", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "source_reverse", "=", "hparams", ".", "source_reverse", ",", "\n", "random_seed", "=", "hparams", ".", "random_seed", ",", "\n", "num_buckets", "=", "hparams", ".", "num_buckets", ",", "\n", "src_max_len", "=", "hparams", ".", "src_max_len", ",", "\n", "tgt_max_len", "=", "hparams", ".", "tgt_max_len", ",", "\n", "skip_count", "=", "skip_count_placeholder", ")", "\n", "", "else", ":", "\n", "      ", "iterator", "=", "iterator_utils", ".", "get_feedable_iterator", "(", "\n", "hparams", ",", "\n", "src_dataset", ",", "\n", "tgt_dataset", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "batch_size", "=", "hparams", ".", "batch_size", ",", "\n", "sos", "=", "hparams", ".", "sos", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "source_reverse", "=", "hparams", ".", "source_reverse", ",", "\n", "random_seed", "=", "hparams", ".", "random_seed", ",", "\n", "num_buckets", "=", "hparams", ".", "num_buckets", ",", "\n", "src_max_len", "=", "hparams", ".", "src_max_len", ",", "\n", "tgt_max_len", "=", "hparams", ".", "tgt_max_len", ",", "\n", "skip_count", "=", "skip_count_placeholder", ")", "\n", "\n", "# Note: One can set model_device_fn to", "\n", "# `tf.train.replica_device_setter(ps_tasks)` for distributed training.", "\n", "", "with", "tf", ".", "device", "(", "model_device_fn", ")", ":", "\n", "      ", "model", "=", "model_creator", "(", "\n", "hparams", ",", "\n", "iterator", "=", "iterator", ",", "\n", "mode", "=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", ",", "\n", "source_vocab_table", "=", "src_vocab_table", ",", "\n", "target_vocab_table", "=", "tgt_vocab_table", ",", "\n", "scope", "=", "scope", ",", "\n", "single_cell_fn", "=", "single_cell_fn", ")", "\n", "\n", "", "", "return", "TrainModel", "(", "\n", "graph", "=", "graph", ",", "\n", "model", "=", "model", ",", "\n", "iterator", "=", "iterator", ",", "\n", "skip_count_placeholder", "=", "skip_count_placeholder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_eval_model": [[133, 173], ["tensorflow.Graph", "model_helper.EvalModel", "tf.Graph.as_default", "vocab_utils.create_vocab_tables", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.contrib.data.TextLineDataset", "tensorflow.contrib.data.TextLineDataset", "iterator_utils.get_iterator", "model_creator"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.vocab_utils.create_vocab_tables", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_iterator"], ["", "def", "create_eval_model", "(", "model_creator", ",", "hparams", ",", "scope", "=", "None", ",", "single_cell_fn", "=", "None", ")", ":", "\n", "  ", "\"\"\"Create train graph, model, src/tgt file holders, and iterator.\"\"\"", "\n", "src_vocab_file", "=", "hparams", ".", "src_vocab_file", "\n", "tgt_vocab_file", "=", "hparams", ".", "tgt_vocab_file", "\n", "graph", "=", "tf", ".", "Graph", "(", ")", "\n", "\n", "with", "graph", ".", "as_default", "(", ")", ":", "\n", "    ", "src_vocab_table", ",", "tgt_vocab_table", "=", "vocab_utils", ".", "create_vocab_tables", "(", "\n", "src_vocab_file", ",", "tgt_vocab_file", ",", "hparams", ".", "share_vocab", ")", "\n", "src_file_placeholder", "=", "tf", ".", "placeholder", "(", "shape", "=", "(", ")", ",", "dtype", "=", "tf", ".", "string", ")", "\n", "tgt_file_placeholder", "=", "tf", ".", "placeholder", "(", "shape", "=", "(", ")", ",", "dtype", "=", "tf", ".", "string", ")", "\n", "src_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "TextLineDataset", "(", "src_file_placeholder", ")", "\n", "tgt_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "TextLineDataset", "(", "tgt_file_placeholder", ")", "\n", "iterator", "=", "iterator_utils", ".", "get_iterator", "(", "\n", "src_dataset", ",", "\n", "tgt_dataset", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "hparams", ".", "batch_size", ",", "\n", "sos", "=", "hparams", ".", "sos", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "source_reverse", "=", "hparams", ".", "source_reverse", ",", "\n", "random_seed", "=", "hparams", ".", "random_seed", ",", "\n", "num_buckets", "=", "hparams", ".", "num_buckets", ",", "\n", "src_max_len", "=", "hparams", ".", "src_max_len_infer", ",", "\n", "tgt_max_len", "=", "hparams", ".", "tgt_max_len_infer", ")", "\n", "model", "=", "model_creator", "(", "\n", "hparams", ",", "\n", "iterator", "=", "iterator", ",", "\n", "mode", "=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "EVAL", ",", "\n", "source_vocab_table", "=", "src_vocab_table", ",", "\n", "target_vocab_table", "=", "tgt_vocab_table", ",", "\n", "scope", "=", "scope", ",", "\n", "single_cell_fn", "=", "single_cell_fn", ")", "\n", "", "return", "EvalModel", "(", "\n", "graph", "=", "graph", ",", "\n", "model", "=", "model", ",", "\n", "src_file_placeholder", "=", "src_file_placeholder", ",", "\n", "tgt_file_placeholder", "=", "tgt_file_placeholder", ",", "\n", "iterator", "=", "iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_infer_model": [[182, 222], ["tensorflow.Graph", "model_helper.InferModel", "tf.Graph.as_default", "vocab_utils.create_vocab_tables", "tensorflow.python.ops.lookup_ops.index_to_string_table_from_file", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.contrib.data.Dataset.from_tensor_slices", "iterator_utils.get_infer_iterator", "model_creator"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.vocab_utils.create_vocab_tables", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_infer_iterator"], ["", "def", "create_infer_model", "(", "model_creator", ",", "hparams", ",", "scope", "=", "None", ",", "single_cell_fn", "=", "None", ")", ":", "\n", "  ", "\"\"\"Create inference model.\"\"\"", "\n", "graph", "=", "tf", ".", "Graph", "(", ")", "\n", "src_vocab_file", "=", "hparams", ".", "src_vocab_file", "\n", "tgt_vocab_file", "=", "hparams", ".", "tgt_vocab_file", "\n", "\n", "with", "graph", ".", "as_default", "(", ")", ":", "\n", "    ", "src_vocab_table", ",", "tgt_vocab_table", "=", "vocab_utils", ".", "create_vocab_tables", "(", "\n", "src_vocab_file", ",", "tgt_vocab_file", ",", "hparams", ".", "share_vocab", ")", "\n", "reverse_tgt_vocab_table", "=", "lookup_ops", ".", "index_to_string_table_from_file", "(", "\n", "tgt_vocab_file", ",", "default_value", "=", "vocab_utils", ".", "UNK", ")", "\n", "\n", "src_placeholder", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", "]", ",", "dtype", "=", "tf", ".", "string", ")", "\n", "batch_size_placeholder", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "]", ",", "dtype", "=", "tf", ".", "int64", ")", "\n", "\n", "src_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "src_placeholder", ")", "\n", "iterator", "=", "iterator_utils", ".", "get_infer_iterator", "(", "\n", "src_dataset", ",", "\n", "src_vocab_table", ",", "\n", "batch_size", "=", "hparams", ".", "infer_batch_size", ",", "\n", "sos", "=", "hparams", ".", "sos", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "source_reverse", "=", "hparams", ".", "source_reverse", ",", "\n", "src_max_len", "=", "hparams", ".", "src_max_len_infer", ")", "\n", "model", "=", "model_creator", "(", "\n", "hparams", ",", "\n", "iterator", "=", "iterator", ",", "\n", "mode", "=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ",", "\n", "source_vocab_table", "=", "src_vocab_table", ",", "\n", "target_vocab_table", "=", "tgt_vocab_table", ",", "\n", "reverse_target_vocab_table", "=", "reverse_tgt_vocab_table", ",", "\n", "scope", "=", "scope", ",", "\n", "single_cell_fn", "=", "single_cell_fn", ")", "\n", "", "return", "InferModel", "(", "\n", "graph", "=", "graph", ",", "\n", "model", "=", "model", ",", "\n", "src_placeholder", "=", "src_placeholder", ",", "\n", "batch_size_placeholder", "=", "batch_size_placeholder", ",", "\n", "iterator", "=", "iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_emb_for_encoder_and_decoder": [[224, 287], ["tensorflow.fixed_size_partitioner", "tensorflow.variable_scope", "misc_utils.print_out", "tensorflow.get_variable", "ValueError", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.get_variable"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "create_emb_for_encoder_and_decoder", "(", "share_vocab", ",", "\n", "src_vocab_size", ",", "\n", "tgt_vocab_size", ",", "\n", "src_embed_size", ",", "\n", "tgt_embed_size", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "num_partitions", "=", "0", ",", "\n", "scope", "=", "None", ")", ":", "\n", "  ", "\"\"\"Create embedding matrix for both encoder and decoder.\n\n  Args:\n    share_vocab: A boolean. Whether to share embedding matrix for both\n      encoder and decoder.\n    src_vocab_size: An integer. The source vocab size.\n    tgt_vocab_size: An integer. The target vocab size.\n    src_embed_size: An integer. The embedding dimension for the encoder's\n      embedding.\n    tgt_embed_size: An integer. The embedding dimension for the decoder's\n      embedding.\n    dtype: dtype of the embedding matrix. Default to float32.\n    num_partitions: number of partitions used for the embedding vars.\n    scope: VariableScope for the created subgraph. Default to \"embedding\".\n\n  Returns:\n    embedding_encoder: Encoder's embedding matrix.\n    embedding_decoder: Decoder's embedding matrix.\n\n  Raises:\n    ValueError: if use share_vocab but source and target have different vocab\n      size.\n  \"\"\"", "\n", "\n", "if", "num_partitions", "<=", "1", ":", "\n", "    ", "partitioner", "=", "None", "\n", "", "else", ":", "\n", "# Note: num_partitions > 1 is required for distributed training due to", "\n", "# embedding_lookup tries to colocate single partition-ed embedding variable", "\n", "# with lookup ops. This may cause embedding variables being placed on worker", "\n", "# jobs.", "\n", "    ", "partitioner", "=", "tf", ".", "fixed_size_partitioner", "(", "num_partitions", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\n", "scope", "or", "\"embeddings\"", ",", "dtype", "=", "dtype", ",", "partitioner", "=", "partitioner", ")", "as", "scope", ":", "\n", "# Share embedding", "\n", "    ", "if", "share_vocab", ":", "\n", "      ", "if", "src_vocab_size", "!=", "tgt_vocab_size", ":", "\n", "        ", "raise", "ValueError", "(", "\"Share embedding but different src/tgt vocab sizes\"", "\n", "\" %d vs. %d\"", "%", "(", "src_vocab_size", ",", "tgt_vocab_size", ")", ")", "\n", "", "utils", ".", "print_out", "(", "\"# Use the same source embeddings for target\"", ")", "\n", "embedding", "=", "tf", ".", "get_variable", "(", "\n", "\"embedding_share\"", ",", "[", "src_vocab_size", ",", "src_embed_size", "]", ",", "dtype", ")", "\n", "embedding_encoder", "=", "embedding", "\n", "embedding_decoder", "=", "embedding", "\n", "", "else", ":", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ",", "partitioner", "=", "partitioner", ")", ":", "\n", "        ", "embedding_encoder", "=", "tf", ".", "get_variable", "(", "\n", "\"embedding_encoder\"", ",", "[", "src_vocab_size", ",", "src_embed_size", "]", ",", "dtype", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"decoder\"", ",", "partitioner", "=", "partitioner", ")", ":", "\n", "        ", "embedding_decoder", "=", "tf", ".", "get_variable", "(", "\n", "\"embedding_decoder\"", ",", "[", "tgt_vocab_size", ",", "tgt_embed_size", "]", ",", "dtype", ")", "\n", "\n", "", "", "", "return", "embedding_encoder", ",", "embedding_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper._single_cell": [[289, 334], ["misc_utils.print_out", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.DropoutWrapper", "misc_utils.print_out", "tensorflow.contrib.rnn.ResidualWrapper", "misc_utils.print_out", "tensorflow.contrib.rnn.DeviceWrapper", "misc_utils.print_out", "misc_utils.print_out", "tensorflow.contrib.rnn.GRUCell", "misc_utils.print_out", "tensorflow.contrib.rnn.LayerNormBasicLSTMCell", "ValueError", "type", "type", "type"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "_single_cell", "(", "unit_type", ",", "num_units", ",", "forget_bias", ",", "dropout", ",", "\n", "mode", ",", "residual_connection", "=", "False", ",", "device_str", "=", "None", ",", "\n", "num_proj", "=", "None", ",", "num_layers", "=", "None", ")", ":", "\n", "  ", "\"\"\"Create an instance of a single RNN cell.\"\"\"", "\n", "# dropout (= 1 - keep_prob) is set to 0 during eval and infer", "\n", "dropout", "=", "dropout", "if", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", "else", "0.0", "\n", "\n", "# Cell Type", "\n", "if", "unit_type", "==", "\"lstm\"", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"  LSTM, forget_bias=%g\"", "%", "forget_bias", ",", "new_line", "=", "False", ")", "\n", "single_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "\n", "num_units", ",", "\n", "forget_bias", "=", "forget_bias", ")", "\n", "", "elif", "unit_type", "==", "\"gru\"", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"  GRU\"", ",", "new_line", "=", "False", ")", "\n", "single_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "GRUCell", "(", "num_units", ")", "\n", "", "elif", "unit_type", "==", "\"layer_norm_lstm\"", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"  Layer Normalized LSTM, forget_bias=%g\"", "%", "forget_bias", ",", "\n", "new_line", "=", "False", ")", "\n", "single_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "LayerNormBasicLSTMCell", "(", "\n", "num_units", ",", "\n", "forget_bias", "=", "forget_bias", ",", "\n", "layer_norm", "=", "True", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unknown unit type %s!\"", "%", "unit_type", ")", "\n", "\n", "# Dropout (= 1 - keep_prob)", "\n", "", "if", "dropout", ">", "0.0", ":", "\n", "    ", "single_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "\n", "cell", "=", "single_cell", ",", "input_keep_prob", "=", "(", "1.0", "-", "dropout", ")", ")", "\n", "utils", ".", "print_out", "(", "\"  %s, dropout=%g \"", "%", "(", "type", "(", "single_cell", ")", ".", "__name__", ",", "dropout", ")", ",", "\n", "new_line", "=", "False", ")", "\n", "\n", "# Residual", "\n", "", "if", "residual_connection", ":", "\n", "    ", "single_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "ResidualWrapper", "(", "single_cell", ")", "\n", "utils", ".", "print_out", "(", "\"  %s\"", "%", "type", "(", "single_cell", ")", ".", "__name__", ",", "new_line", "=", "False", ")", "\n", "\n", "# Device Wrapper", "\n", "", "if", "device_str", ":", "\n", "    ", "single_cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "DeviceWrapper", "(", "single_cell", ",", "device_str", ")", "\n", "utils", ".", "print_out", "(", "\"  %s, device=%s\"", "%", "\n", "(", "type", "(", "single_cell", ")", ".", "__name__", ",", "device_str", ")", ",", "new_line", "=", "False", ")", "\n", "\n", "", "return", "single_cell", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper._cell_list": [[336, 362], ["range", "misc_utils.print_out", "single_cell_fn", "misc_utils.print_out", "cell_list.append", "model_helper.get_device_str"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.get_device_str"], ["", "def", "_cell_list", "(", "unit_type", ",", "num_units", ",", "num_layers", ",", "num_residual_layers", ",", "\n", "forget_bias", ",", "dropout", ",", "mode", ",", "num_gpus", ",", "base_gpu", "=", "0", ",", "\n", "single_cell_fn", "=", "None", ",", "num_proj", "=", "None", ",", "num_cells", "=", "1", ")", ":", "\n", "  ", "\"\"\"Create a list of RNN cells.\"\"\"", "\n", "if", "not", "single_cell_fn", ":", "\n", "    ", "single_cell_fn", "=", "_single_cell", "\n", "\n", "# Multi-GPU", "\n", "", "cell_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"  cell %d\"", "%", "i", ",", "new_line", "=", "False", ")", "\n", "single_cell", "=", "single_cell_fn", "(", "\n", "unit_type", "=", "unit_type", ",", "\n", "num_units", "=", "num_units", ",", "\n", "forget_bias", "=", "forget_bias", ",", "\n", "dropout", "=", "dropout", ",", "\n", "mode", "=", "mode", ",", "\n", "residual_connection", "=", "(", "i", ">=", "num_layers", "-", "num_residual_layers", ")", ",", "\n", "device_str", "=", "get_device_str", "(", "i", "+", "base_gpu", ",", "num_gpus", ")", ",", "\n", "num_proj", "=", "num_proj", ",", "\n", "num_layers", "=", "num_layers", "\n", ")", "\n", "utils", ".", "print_out", "(", "\"\"", ")", "\n", "cell_list", ".", "append", "(", "single_cell", ")", "\n", "\n", "", "return", "cell_list", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_rnn_cell": [[364, 407], ["model_helper._cell_list", "len", "tensorflow.contrib.rnn.MultiRNNCell"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper._cell_list"], ["", "def", "create_rnn_cell", "(", "unit_type", ",", "num_units", ",", "num_layers", ",", "num_residual_layers", ",", "\n", "forget_bias", ",", "dropout", ",", "mode", ",", "num_gpus", ",", "base_gpu", "=", "0", ",", "\n", "single_cell_fn", "=", "None", ",", "num_proj", "=", "None", ",", "num_cells", "=", "1", ")", ":", "\n", "  ", "\"\"\"Create multi-layer RNN cell.\n\n  Args:\n    unit_type: string representing the unit type, i.e. \"lstm\".\n    num_units: the depth of each unit.\n    num_layers: number of cells.\n    num_residual_layers: Number of residual layers from top to bottom. For\n      example, if `num_layers=4` and `num_residual_layers=2`, the last 2 RNN\n      cells in the returned list will be wrapped with `ResidualWrapper`.\n    forget_bias: the initial forget bias of the RNNCell(s).\n    dropout: floating point value between 0.0 and 1.0:\n      the probability of dropout.  this is ignored if `mode != TRAIN`.\n    mode: either tf.contrib.learn.TRAIN/EVAL/INFER\n    num_gpus: The number of gpus to use when performing round-robin\n      placement of layers.\n    base_gpu: The gpu device id to use for the first RNN cell in the\n      returned list. The i-th RNN cell will use `(base_gpu + i) % num_gpus`\n      as its device id.\n    single_cell_fn: single_cell_fn: allow for adding customized cell.\n      When not specified, we default to model_helper._single_cell\n  Returns:\n    An `RNNCell` instance.\n  \"\"\"", "\n", "cell_list", "=", "_cell_list", "(", "unit_type", "=", "unit_type", ",", "\n", "num_units", "=", "num_units", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "num_residual_layers", "=", "num_residual_layers", ",", "\n", "forget_bias", "=", "forget_bias", ",", "\n", "dropout", "=", "dropout", ",", "\n", "mode", "=", "mode", ",", "\n", "num_gpus", "=", "num_gpus", ",", "\n", "base_gpu", "=", "base_gpu", ",", "\n", "single_cell_fn", "=", "single_cell_fn", ",", "\n", "num_proj", "=", "num_proj", ",", "\n", "num_cells", "=", "num_cells", ")", "\n", "\n", "if", "len", "(", "cell_list", ")", "==", "1", ":", "# Single layer.", "\n", "    ", "return", "cell_list", "[", "0", "]", "\n", "", "else", ":", "# Multi layers", "\n", "    ", "return", "tf", ".", "contrib", ".", "rnn", ".", "MultiRNNCell", "(", "cell_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.gradient_clip": [[409, 418], ["tensorflow.clip_by_global_norm", "gradient_norm_summary.append", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.global_norm"], "function", ["None"], ["", "", "def", "gradient_clip", "(", "gradients", ",", "max_gradient_norm", ")", ":", "\n", "  ", "\"\"\"Clipping gradients of a model.\"\"\"", "\n", "clipped_gradients", ",", "gradient_norm", "=", "tf", ".", "clip_by_global_norm", "(", "\n", "gradients", ",", "max_gradient_norm", ")", "\n", "gradient_norm_summary", "=", "[", "tf", ".", "summary", ".", "scalar", "(", "\"grad_norm\"", ",", "gradient_norm", ")", "]", "\n", "gradient_norm_summary", ".", "append", "(", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"clipped_gradient\"", ",", "tf", ".", "global_norm", "(", "clipped_gradients", ")", ")", ")", "\n", "\n", "return", "clipped_gradients", ",", "gradient_norm_summary", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.load_model": [[420, 428], ["time.time", "model.saver.restore", "session.run", "misc_utils.print_out", "tensorflow.tables_initializer", "time.time"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "load_model", "(", "model", ",", "ckpt", ",", "session", ",", "name", ")", ":", "\n", "  ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "model", ".", "saver", ".", "restore", "(", "session", ",", "ckpt", ")", "\n", "session", ".", "run", "(", "tf", ".", "tables_initializer", "(", ")", ")", "\n", "utils", ".", "print_out", "(", "\n", "\"  loaded %s model parameters from %s, time %.2fs\"", "%", "\n", "(", "name", ",", "ckpt", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_or_load_model": [[430, 444], ["tensorflow.train.latest_checkpoint", "load_model.global_step.eval", "model_helper.load_model", "time.time", "session.run", "session.run", "misc_utils.print_out", "tensorflow.global_variables_initializer", "tensorflow.tables_initializer", "time.time"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.load_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "create_or_load_model", "(", "model", ",", "model_dir", ",", "session", ",", "name", ")", ":", "\n", "  ", "\"\"\"Create translation model and initialize or load parameters in session.\"\"\"", "\n", "latest_ckpt", "=", "tf", ".", "train", ".", "latest_checkpoint", "(", "model_dir", ")", "\n", "if", "latest_ckpt", ":", "\n", "    ", "model", "=", "load_model", "(", "model", ",", "latest_ckpt", ",", "session", ",", "name", ")", "\n", "", "else", ":", "\n", "    ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "session", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "session", ".", "run", "(", "tf", ".", "tables_initializer", "(", ")", ")", "\n", "utils", ".", "print_out", "(", "\"  created %s model with fresh parameters, time %.2fs\"", "%", "\n", "(", "name", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n", "", "global_step", "=", "model", ".", "global_step", ".", "eval", "(", "session", "=", "session", ")", "\n", "return", "model", ",", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.compute_perplexity": [[446, 473], ["time.time", "misc_utils.safe_exp", "misc_utils.print_time", "model.eval"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.safe_exp", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_time", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.eval"], ["", "def", "compute_perplexity", "(", "model", ",", "sess", ",", "name", ")", ":", "\n", "  ", "\"\"\"Compute perplexity of the output of the model.\n\n  Args:\n    model: model for compute perplexity.\n    sess: tensorflow session to use.\n    name: name of the batch.\n\n  Returns:\n    The perplexity of the eval outputs.\n  \"\"\"", "\n", "total_loss", "=", "0", "\n", "total_predict_count", "=", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "while", "True", ":", "\n", "    ", "try", ":", "\n", "      ", "loss", ",", "predict_count", ",", "batch_size", "=", "model", ".", "eval", "(", "sess", ")", "\n", "total_loss", "+=", "loss", "*", "batch_size", "\n", "total_predict_count", "+=", "predict_count", "\n", "", "except", "tf", ".", "errors", ".", "OutOfRangeError", ":", "\n", "      ", "break", "\n", "\n", "", "", "perplexity", "=", "utils", ".", "safe_exp", "(", "total_loss", "/", "total_predict_count", ")", "\n", "utils", ".", "print_time", "(", "\"  eval %s: perplexity %.2f\"", "%", "(", "name", ",", "perplexity", ")", ",", "\n", "start_time", ")", "\n", "return", "perplexity", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.setUpClass": [[40, 200], ["None"], "methods", ["None"], ["  ", "@", "classmethod", "\n", "def", "setUpClass", "(", "cls", ")", ":", "\n", "    ", "cls", ".", "actual_vars_values", "=", "{", "}", "\n", "cls", ".", "expected_vars_values", "=", "{", "\n", "'AttentionMechanismBahdanau/att_layer_weight/shape'", ":", "(", "10", ",", "5", ")", ",", "\n", "'AttentionMechanismBahdanau/att_layer_weight/sum'", ":", "\n", "-", "0.64981574", ",", "\n", "'AttentionMechanismBahdanau/last_dec_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'AttentionMechanismBahdanau/last_dec_weight/sum'", ":", "\n", "0.058069646", ",", "\n", "'AttentionMechanismBahdanau/last_enc_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'AttentionMechanismBahdanau/last_enc_weight/sum'", ":", "\n", "0.058028102", ",", "\n", "'AttentionMechanismLuong/att_layer_weight/shape'", ":", "(", "10", ",", "5", ")", ",", "\n", "'AttentionMechanismLuong/att_layer_weight/sum'", ":", "\n", "-", "0.64981574", ",", "\n", "'AttentionMechanismLuong/last_dec_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'AttentionMechanismLuong/last_dec_weight/sum'", ":", "\n", "0.058069646", ",", "\n", "'AttentionMechanismLuong/last_enc_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'AttentionMechanismLuong/last_enc_weight/sum'", ":", "\n", "0.058028102", ",", "\n", "'AttentionMechanismNormedBahdanau/att_layer_weight/shape'", ":", "(", "10", ",", "5", ")", ",", "\n", "'AttentionMechanismNormedBahdanau/att_layer_weight/sum'", ":", "\n", "-", "0.64981973", ",", "\n", "'AttentionMechanismNormedBahdanau/last_dec_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'AttentionMechanismNormedBahdanau/last_dec_weight/sum'", ":", "\n", "0.058067322", ",", "\n", "'AttentionMechanismNormedBahdanau/last_enc_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'AttentionMechanismNormedBahdanau/last_enc_weight/sum'", ":", "\n", "0.058022559", ",", "\n", "'AttentionMechanismScaledLuong/att_layer_weight/shape'", ":", "(", "10", ",", "5", ")", ",", "\n", "'AttentionMechanismScaledLuong/att_layer_weight/sum'", ":", "\n", "-", "0.64981574", ",", "\n", "'AttentionMechanismScaledLuong/last_dec_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'AttentionMechanismScaledLuong/last_dec_weight/sum'", ":", "\n", "0.058069646", ",", "\n", "'AttentionMechanismScaledLuong/last_enc_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'AttentionMechanismScaledLuong/last_enc_weight/sum'", ":", "\n", "0.058028102", ",", "\n", "'GNMTModel_gnmt/last_dec_weight/shape'", ":", "(", "15", ",", "20", ")", ",", "\n", "'GNMTModel_gnmt/last_dec_weight/sum'", ":", "\n", "-", "0.48634407", ",", "\n", "'GNMTModel_gnmt/last_enc_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'GNMTModel_gnmt/last_enc_weight/sum'", ":", "\n", "0.058025002", ",", "\n", "'GNMTModel_gnmt/mem_layer_weight/shape'", ":", "(", "5", ",", "5", ")", ",", "\n", "'GNMTModel_gnmt/mem_layer_weight/sum'", ":", "\n", "-", "0.44815454", ",", "\n", "'GNMTModel_gnmt_v2/last_dec_weight/shape'", ":", "(", "15", ",", "20", ")", ",", "\n", "'GNMTModel_gnmt_v2/last_dec_weight/sum'", ":", "\n", "-", "0.48634392", ",", "\n", "'GNMTModel_gnmt_v2/last_enc_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'GNMTModel_gnmt_v2/last_enc_weight/sum'", ":", "\n", "0.058024824", ",", "\n", "'GNMTModel_gnmt_v2/mem_layer_weight/shape'", ":", "(", "5", ",", "5", ")", ",", "\n", "'GNMTModel_gnmt_v2/mem_layer_weight/sum'", ":", "\n", "-", "0.44815454", ",", "\n", "'NoAttentionNoResidualUniEncoder/last_dec_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'NoAttentionNoResidualUniEncoder/last_dec_weight/sum'", ":", "\n", "0.057424068", ",", "\n", "'NoAttentionNoResidualUniEncoder/last_enc_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'NoAttentionNoResidualUniEncoder/last_enc_weight/sum'", ":", "\n", "0.058453858", ",", "\n", "'NoAttentionResidualBiEncoder/last_dec_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'NoAttentionResidualBiEncoder/last_dec_weight/sum'", ":", "\n", "0.058025062", ",", "\n", "'NoAttentionResidualBiEncoder/last_enc_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'NoAttentionResidualBiEncoder/last_enc_weight/sum'", ":", "\n", "0.058053195", ",", "\n", "'UniEncoderBottomAttentionArchitecture/last_dec_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'UniEncoderBottomAttentionArchitecture/last_dec_weight/sum'", ":", "\n", "0.058024943", ",", "\n", "'UniEncoderBottomAttentionArchitecture/last_enc_weight/shape'", ":", "(", "10", ",", "20", ")", ",", "\n", "'UniEncoderBottomAttentionArchitecture/last_enc_weight/sum'", ":", "\n", "0.058025122", ",", "\n", "'UniEncoderBottomAttentionArchitecture/mem_layer_weight/shape'", ":", "(", "5", ",", "5", ")", ",", "\n", "'UniEncoderBottomAttentionArchitecture/mem_layer_weight/sum'", ":", "\n", "-", "0.44815454", ",", "\n", "'UniEncoderStandardAttentionArchitecture/last_dec_weight/shape'", ":", "(", "10", ",", "\n", "20", ")", ",", "\n", "'UniEncoderStandardAttentionArchitecture/last_dec_weight/sum'", ":", "\n", "0.058025002", ",", "\n", "'UniEncoderStandardAttentionArchitecture/last_enc_weight/shape'", ":", "(", "10", ",", "\n", "20", ")", ",", "\n", "'UniEncoderStandardAttentionArchitecture/last_enc_weight/sum'", ":", "\n", "0.058024883", ",", "\n", "'UniEncoderStandardAttentionArchitecture/mem_layer_weight/shape'", ":", "(", "5", ",", "\n", "5", ")", ",", "\n", "'UniEncoderStandardAttentionArchitecture/mem_layer_weight/sum'", ":", "\n", "-", "0.44815454", ",", "\n", "}", "\n", "\n", "cls", ".", "actual_train_values", "=", "{", "}", "\n", "cls", ".", "expected_train_values", "=", "{", "\n", "'AttentionMechanismBahdanau/loss'", ":", "8.8519039", ",", "\n", "'AttentionMechanismLuong/loss'", ":", "8.8519039", ",", "\n", "'AttentionMechanismNormedBahdanau/loss'", ":", "8.851902", ",", "\n", "'AttentionMechanismScaledLuong/loss'", ":", "8.8519039", ",", "\n", "'GNMTModel_gnmt/loss'", ":", "8.8519087", ",", "\n", "'GNMTModel_gnmt_v2/loss'", ":", "8.8519087", ",", "\n", "'NoAttentionNoResidualUniEncoder/loss'", ":", "8.8516064", ",", "\n", "'NoAttentionResidualBiEncoder/loss'", ":", "8.851984", ",", "\n", "'UniEncoderStandardAttentionArchitecture/loss'", ":", "8.8519087", ",", "\n", "'InitializerGlorotNormal/loss'", ":", "8.9779415", ",", "\n", "'InitializerGlorotUniform/loss'", ":", "8.7643699", ",", "\n", "}", "\n", "\n", "cls", ".", "actual_eval_values", "=", "{", "}", "\n", "cls", ".", "expected_eval_values", "=", "{", "\n", "'AttentionMechanismBahdanau/loss'", ":", "8.8517132", ",", "\n", "'AttentionMechanismBahdanau/predict_count'", ":", "11.0", ",", "\n", "'AttentionMechanismLuong/loss'", ":", "8.8517132", ",", "\n", "'AttentionMechanismLuong/predict_count'", ":", "11.0", ",", "\n", "'AttentionMechanismNormedBahdanau/loss'", ":", "8.8517132", ",", "\n", "'AttentionMechanismNormedBahdanau/predict_count'", ":", "11.0", ",", "\n", "'AttentionMechanismScaledLuong/loss'", ":", "8.8517132", ",", "\n", "'AttentionMechanismScaledLuong/predict_count'", ":", "11.0", ",", "\n", "'GNMTModel_gnmt/loss'", ":", "8.8443403", ",", "\n", "'GNMTModel_gnmt/predict_count'", ":", "11.0", ",", "\n", "'GNMTModel_gnmt_v2/loss'", ":", "8.8443756", ",", "\n", "'GNMTModel_gnmt_v2/predict_count'", ":", "11.0", ",", "\n", "'NoAttentionNoResidualUniEncoder/loss'", ":", "8.8440113", ",", "\n", "'NoAttentionNoResidualUniEncoder/predict_count'", ":", "11.0", ",", "\n", "'NoAttentionResidualBiEncoder/loss'", ":", "8.8291245", ",", "\n", "'NoAttentionResidualBiEncoder/predict_count'", ":", "11.0", ",", "\n", "'UniEncoderBottomAttentionArchitecture/loss'", ":", "8.844492", ",", "\n", "'UniEncoderBottomAttentionArchitecture/predict_count'", ":", "11.0", ",", "\n", "'UniEncoderStandardAttentionArchitecture/loss'", ":", "8.8517151", ",", "\n", "'UniEncoderStandardAttentionArchitecture/predict_count'", ":", "11.0", "\n", "}", "\n", "\n", "cls", ".", "actual_infer_values", "=", "{", "}", "\n", "cls", ".", "expected_infer_values", "=", "{", "\n", "'AttentionMechanismBahdanau/logits_sum'", ":", "-", "0.026374687", ",", "\n", "'AttentionMechanismLuong/logits_sum'", ":", "-", "0.026374735", ",", "\n", "'AttentionMechanismNormedBahdanau/logits_sum'", ":", "-", "0.026376063", ",", "\n", "'AttentionMechanismScaledLuong/logits_sum'", ":", "-", "0.026374735", ",", "\n", "'GNMTModel_gnmt/logits_sum'", ":", "-", "0.98668635", ",", "\n", "'GNMTModel_gnmt_v2/logits_sum'", ":", "-", "0.98513138", ",", "\n", "'NoAttentionNoResidualUniEncoder/logits_sum'", ":", "-", "1.0808625", ",", "\n", "'NoAttentionResidualBiEncoder/logits_sum'", ":", "-", "2.8147559", ",", "\n", "'UniEncoderBottomAttentionArchitecture/logits_sum'", ":", "-", "0.97026241", ",", "\n", "'UniEncoderStandardAttentionArchitecture/logits_sum'", ":", "-", "0.02665353", "\n", "}", "\n", "\n", "cls", ".", "actual_beam_sentences", "=", "{", "}", "\n", "cls", ".", "expected_beam_sentences", "=", "{", "\n", "'BeamSearchAttentionModel: batch 0 of beam 0'", ":", "''", ",", "\n", "'BeamSearchAttentionModel: batch 0 of beam 1'", ":", "''", ",", "\n", "'BeamSearchAttentionModel: batch 1 of beam 0'", ":", "''", ",", "\n", "'BeamSearchAttentionModel: batch 1 of beam 1'", ":", "''", ",", "\n", "'BeamSearchBasicModel: batch 0 of beam 0'", ":", "'b b b b'", ",", "\n", "'BeamSearchBasicModel: batch 0 of beam 1'", ":", "'b b b sos'", ",", "\n", "'BeamSearchBasicModel: batch 0 of beam 2'", ":", "'b b b c'", ",", "\n", "'BeamSearchBasicModel: batch 1 of beam 0'", ":", "'b b b b'", ",", "\n", "'BeamSearchBasicModel: batch 1 of beam 1'", ":", "'a b b b'", ",", "\n", "'BeamSearchBasicModel: batch 1 of beam 2'", ":", "'b b b sos'", ",", "\n", "'BeamSearchGNMTModel: batch 0 of beam 0'", ":", "''", ",", "\n", "'BeamSearchGNMTModel: batch 1 of beam 0'", ":", "''", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.tearDownClass": [[202, 223], ["print", "pprint.pprint", "sys.stdout.flush", "print", "pprint.pprint", "sys.stdout.flush", "print", "pprint.pprint", "sys.stdout.flush", "print", "pprint.pprint", "sys.stdout.flush", "print", "pprint.pprint", "sys.stdout.flush"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "tearDownClass", "(", "cls", ")", ":", "\n", "    ", "print", "(", "'ModelTest - actual_vars_values: '", ")", "\n", "pprint", ".", "pprint", "(", "cls", ".", "actual_vars_values", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "print", "(", "'ModelTest - actual_train_values: '", ")", "\n", "pprint", ".", "pprint", "(", "cls", ".", "actual_train_values", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "print", "(", "'ModelTest - actual_eval_values: '", ")", "\n", "pprint", ".", "pprint", "(", "cls", ".", "actual_eval_values", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "print", "(", "'ModelTest - actual_infer_values: '", ")", "\n", "pprint", ".", "pprint", "(", "cls", ".", "actual_infer_values", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "print", "(", "'ModelTest - actual_beam_sentences: '", ")", "\n", "pprint", ".", "pprint", "(", "cls", ".", "actual_beam_sentences", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.assertAllClose": [[224, 228], ["super().assertAllClose"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.assertAllClose"], ["", "def", "assertAllClose", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "kwargs", "[", "'atol'", "]", "=", "5e-2", "\n", "kwargs", "[", "'rtol'", "]", "=", "5e-2", "\n", "return", "super", "(", "ModelTest", ",", "self", ")", ".", "assertAllClose", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariableNames": [[229, 236], ["print", "model_test.ModelTest.assertEqual", "model_test.ModelTest.assertEqual", "len", "len", "sorted", "sorted"], "methods", ["None"], ["", "def", "_assertModelVariableNames", "(", "self", ",", "expected_var_names", ",", "model_var_names", ",", "\n", "name", ")", ":", "\n", "\n", "    ", "print", "(", "'{} variable names are: '", ".", "format", "(", "name", ")", ",", "model_var_names", ")", "\n", "\n", "self", ".", "assertEqual", "(", "len", "(", "expected_var_names", ")", ",", "len", "(", "model_var_names", ")", ")", "\n", "self", ".", "assertEqual", "(", "sorted", "(", "expected_var_names", ")", ",", "sorted", "(", "model_var_names", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable": [[237, 250], ["tuple", "sess.run", "numpy.sum", "print", "model_test.ModelTest.assertEqual", "model_test.ModelTest.assertAllClose", "variable.get_shape().as_list", "variable.get_shape"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.assertAllClose"], ["", "def", "_assertModelVariable", "(", "self", ",", "variable", ",", "sess", ",", "name", ")", ":", "\n", "    ", "var_shape", "=", "tuple", "(", "variable", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", ")", "\n", "var_res", "=", "sess", ".", "run", "(", "variable", ")", "\n", "var_weight_sum", "=", "np", ".", "sum", "(", "var_res", ")", "\n", "\n", "print", "(", "'{} weight sum is: '", ".", "format", "(", "name", ")", ",", "var_weight_sum", ")", "\n", "expected_sum", "=", "self", ".", "expected_vars_values", "[", "name", "+", "'/sum'", "]", "\n", "expected_shape", "=", "self", ".", "expected_vars_values", "[", "name", "+", "'/shape'", "]", "\n", "self", ".", "actual_vars_values", "[", "name", "+", "'/sum'", "]", "=", "var_weight_sum", "\n", "self", ".", "actual_vars_values", "[", "name", "+", "'/shape'", "]", "=", "var_shape", "\n", "\n", "self", ".", "assertEqual", "(", "expected_shape", ",", "var_shape", ")", "\n", "self", ".", "assertAllClose", "(", "expected_sum", ",", "var_weight_sum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss": [[251, 260], ["range", "print", "model_test.ModelTest.assertAllClose", "m.train"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.assertAllClose", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.train"], ["", "def", "_assertTrainStepsLoss", "(", "self", ",", "m", ",", "sess", ",", "name", ",", "num_steps", "=", "1", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "num_steps", ")", ":", "\n", "      ", "_", ",", "loss", ",", "_", ",", "_", ",", "_", ",", "_", ",", "_", "=", "m", ".", "train", "(", "sess", ")", "\n", "\n", "", "print", "(", "'{} {}-th step loss is: '", ".", "format", "(", "name", ",", "num_steps", ")", ",", "loss", ")", "\n", "expected_loss", "=", "self", ".", "expected_train_values", "[", "name", "+", "'/loss'", "]", "\n", "self", ".", "actual_train_values", "[", "name", "+", "'/loss'", "]", "=", "loss", "\n", "\n", "self", ".", "assertAllClose", "(", "expected_loss", ",", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertEvalLossAndPredictCount": [[261, 273], ["m.eval", "print", "print", "model_test.ModelTest.assertAllClose", "model_test.ModelTest.assertAllClose"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.assertAllClose", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.assertAllClose"], ["", "def", "_assertEvalLossAndPredictCount", "(", "self", ",", "m", ",", "sess", ",", "name", ")", ":", "\n", "    ", "loss", ",", "predict_count", ",", "_", "=", "m", ".", "eval", "(", "sess", ")", "\n", "\n", "print", "(", "'{} eval loss is: '", ".", "format", "(", "name", ")", ",", "loss", ")", "\n", "print", "(", "'{} predict count is: '", ".", "format", "(", "name", ")", ",", "predict_count", ")", "\n", "expected_loss", "=", "self", ".", "expected_eval_values", "[", "name", "+", "'/loss'", "]", "\n", "expected_predict_count", "=", "self", ".", "expected_eval_values", "[", "name", "+", "'/predict_count'", "]", "\n", "self", ".", "actual_eval_values", "[", "name", "+", "'/loss'", "]", "=", "loss", "\n", "self", ".", "actual_eval_values", "[", "name", "+", "'/predict_count'", "]", "=", "predict_count", "\n", "\n", "self", ".", "assertAllClose", "(", "expected_loss", ",", "loss", ")", "\n", "self", ".", "assertAllClose", "(", "expected_predict_count", ",", "predict_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertInferLogits": [[274, 283], ["m.infer", "numpy.sum", "print", "model_test.ModelTest.assertAllClose"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.infer", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.assertAllClose"], ["", "def", "_assertInferLogits", "(", "self", ",", "m", ",", "sess", ",", "name", ")", ":", "\n", "    ", "results", "=", "m", ".", "infer", "(", "sess", ")", "\n", "logits_sum", "=", "np", ".", "sum", "(", "results", "[", "0", "]", ")", "\n", "\n", "print", "(", "'{} infer logits sum is: '", ".", "format", "(", "name", ")", ",", "logits_sum", ")", "\n", "expected_logits_sum", "=", "self", ".", "expected_infer_values", "[", "name", "+", "'/logits_sum'", "]", "\n", "self", ".", "actual_infer_values", "[", "name", "+", "'/logits_sum'", "]", "=", "logits_sum", "\n", "\n", "self", ".", "assertAllClose", "(", "expected_logits_sum", ",", "logits_sum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertBeamSearchOutputs": [[284, 296], ["m.decode", "range", "range", "utils.nmt_utils.get_translation", "model_test.ModelTest.assertEqual"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.get_translation"], ["", "def", "_assertBeamSearchOutputs", "(", "self", ",", "m", ",", "sess", ",", "assert_top_k_sentence", ",", "name", ")", ":", "\n", "    ", "nmt_outputs", ",", "_", "=", "m", ".", "decode", "(", "sess", ")", "\n", "\n", "for", "i", "in", "range", "(", "assert_top_k_sentence", ")", ":", "\n", "      ", "output_words", "=", "nmt_outputs", "[", "i", "]", "\n", "for", "j", "in", "range", "(", "output_words", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "sentence", "=", "nmt_utils", ".", "get_translation", "(", "\n", "output_words", ",", "j", ",", "tgt_eos", "=", "'eos'", ",", "bpe_delimiter", "=", "None", ")", "\n", "sentence_key", "=", "(", "'%s: batch %d of beam %d'", "%", "(", "name", ",", "j", ",", "i", ")", ")", "\n", "self", ".", "actual_beam_sentences", "[", "sentence_key", "]", "=", "sentence", "\n", "expected_sentence", "=", "self", ".", "expected_beam_sentences", "[", "sentence_key", "]", "\n", "self", ".", "assertEqual", "(", "expected_sentence", ",", "sentence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel": [[297, 312], ["utils.common_test_utils.create_test_iterator", "m_creator", "sess.run", "sess.run", "sess.run", "tensorflow.global_variables_initializer", "tensorflow.tables_initializer"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_iterator"], ["", "", "", "def", "_createTestTrainModel", "(", "self", ",", "m_creator", ",", "hparams", ",", "sess", ")", ":", "\n", "    ", "train_mode", "=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", "\n", "train_iterator", ",", "src_vocab_table", ",", "tgt_vocab_table", "=", "common_test_utils", ".", "create_test_iterator", "(", "\n", "hparams", ",", "train_mode", ")", "\n", "train_m", "=", "m_creator", "(", "\n", "hparams", ",", "\n", "train_mode", ",", "\n", "train_iterator", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "scope", "=", "'dynamic_seq2seq'", ")", "\n", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "sess", ".", "run", "(", "tf", ".", "tables_initializer", "(", ")", ")", "\n", "sess", ".", "run", "(", "train_iterator", ".", "initializer", ")", "\n", "return", "train_m", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestEvalModel": [[313, 327], ["utils.common_test_utils.create_test_iterator", "m_creator", "sess.run", "sess.run", "tensorflow.tables_initializer"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_iterator"], ["", "def", "_createTestEvalModel", "(", "self", ",", "m_creator", ",", "hparams", ",", "sess", ")", ":", "\n", "    ", "eval_mode", "=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "EVAL", "\n", "eval_iterator", ",", "src_vocab_table", ",", "tgt_vocab_table", "=", "common_test_utils", ".", "create_test_iterator", "(", "\n", "hparams", ",", "eval_mode", ")", "\n", "eval_m", "=", "m_creator", "(", "\n", "hparams", ",", "\n", "eval_mode", ",", "\n", "eval_iterator", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "scope", "=", "'dynamic_seq2seq'", ")", "\n", "sess", ".", "run", "(", "tf", ".", "tables_initializer", "(", ")", ")", "\n", "sess", ".", "run", "(", "eval_iterator", ".", "initializer", ")", "\n", "return", "eval_m", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel": [[328, 346], ["utils.common_test_utils.create_test_iterator", "m_creator", "sess.run", "sess.run", "sess.run", "tensorflow.tables_initializer", "tensorflow.global_variables_initializer"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_iterator"], ["", "def", "_createTestInferModel", "(", "\n", "self", ",", "m_creator", ",", "hparams", ",", "sess", ",", "init_global_vars", "=", "False", ")", ":", "\n", "    ", "infer_mode", "=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", "\n", "infer_iterator", ",", "src_vocab_table", ",", "tgt_vocab_table", ",", "reverse_tgt_vocab_table", "=", "(", "\n", "common_test_utils", ".", "create_test_iterator", "(", "hparams", ",", "infer_mode", ")", ")", "\n", "infer_m", "=", "m_creator", "(", "\n", "hparams", ",", "\n", "infer_mode", ",", "\n", "infer_iterator", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "reverse_tgt_vocab_table", ",", "\n", "scope", "=", "'dynamic_seq2seq'", ")", "\n", "if", "init_global_vars", ":", "\n", "      ", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "", "sess", ".", "run", "(", "tf", ".", "tables_initializer", "(", ")", ")", "\n", "sess", ".", "run", "(", "infer_iterator", ".", "initializer", ")", "\n", "return", "infer_m", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config": [[347, 351], ["tensorflow.ConfigProto"], "methods", ["None"], ["", "def", "_get_session_config", "(", "self", ")", ":", "\n", "    ", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "allow_soft_placement", "=", "True", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testNoAttentionNoResidualUniEncoder": [[355, 411], ["utils.common_test_utils.create_test_hparams", "tensorflow.test.create_local_cluster", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Session", "model_test.ModelTest._createTestTrainModel", "tensorflow.trainable_variables", "model_test.ModelTest._assertModelVariableNames", "model_test.ModelTest._assertTrainStepsLoss", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "tensorflow.Session", "model_test.ModelTest._createTestEvalModel", "model_test.ModelTest._assertEvalLossAndPredictCount", "tensorflow.Session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertInferLogits", "tensorflow.Graph", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.Graph", "tensorflow.Graph", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariableNames", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestEvalModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertEvalLossAndPredictCount", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertInferLogits", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config"], ["", "def", "testNoAttentionNoResidualUniEncoder", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "num_layers", "=", "1", ",", "\n", "attention", "=", "''", ",", "\n", "attention_architecture", "=", "''", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "\n", "workers", ",", "_", "=", "tf", ".", "test", ".", "create_local_cluster", "(", "1", ",", "0", ")", "\n", "worker", "=", "workers", "[", "0", "]", "\n", "\n", "# pylint: disable=line-too-long", "\n", "expected_var_names", "=", "[", "\n", "'dynamic_seq2seq/encoder/embedding_encoder:0'", ",", "\n", "'dynamic_seq2seq/decoder/embedding_decoder:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/output_projection/kernel:0'", "\n", "]", "\n", "# pylint: enable=line-too-long", "\n", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "model", ".", "Model", ",", "hparams", ",", "sess", ")", "\n", "\n", "m_vars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "_assertModelVariableNames", "(", "expected_var_names", ",", "\n", "[", "v", ".", "name", "for", "v", "in", "m_vars", "]", ",", "\n", "'NoAttentionNoResidualUniEncoder'", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'dynamic_seq2seq'", ",", "reuse", "=", "True", ")", ":", "\n", "          ", "last_enc_weight", "=", "tf", ".", "get_variable", "(", "\n", "'encoder/rnn/basic_lstm_cell/kernel'", ")", "\n", "last_dec_weight", "=", "tf", ".", "get_variable", "(", "'decoder/basic_lstm_cell/kernel'", ")", "\n", "", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "\n", "'NoAttentionNoResidualUniEncoder'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_enc_weight", ",", "sess", ",", "\n", "'NoAttentionNoResidualUniEncoder/last_enc_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_dec_weight", ",", "sess", ",", "\n", "'NoAttentionNoResidualUniEncoder/last_dec_weight'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "eval_m", "=", "self", ".", "_createTestEvalModel", "(", "model", ".", "Model", ",", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertEvalLossAndPredictCount", "(", "eval_m", ",", "sess", ",", "\n", "'NoAttentionNoResidualUniEncoder'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "model", ".", "Model", ",", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertInferLogits", "(", "infer_m", ",", "sess", ",", "\n", "'NoAttentionNoResidualUniEncoder'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testNoAttentionResidualBiEncoder": [[412, 480], ["utils.common_test_utils.create_test_hparams", "tensorflow.test.create_local_cluster", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Session", "model_test.ModelTest._createTestTrainModel", "tensorflow.trainable_variables", "model_test.ModelTest._assertModelVariableNames", "model_test.ModelTest._assertTrainStepsLoss", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "tensorflow.Session", "model_test.ModelTest._createTestEvalModel", "model_test.ModelTest._assertEvalLossAndPredictCount", "tensorflow.Session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertInferLogits", "tensorflow.Graph", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.Graph", "tensorflow.Graph", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariableNames", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestEvalModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertEvalLossAndPredictCount", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertInferLogits", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config"], ["", "", "", "def", "testNoAttentionResidualBiEncoder", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'bi'", ",", "\n", "num_layers", "=", "4", ",", "\n", "attention", "=", "''", ",", "\n", "attention_architecture", "=", "''", ",", "\n", "use_residual", "=", "True", ",", ")", "\n", "\n", "workers", ",", "_", "=", "tf", ".", "test", ".", "create_local_cluster", "(", "1", ",", "0", ")", "\n", "worker", "=", "workers", "[", "0", "]", "\n", "\n", "# pylint: disable=line-too-long", "\n", "expected_var_names", "=", "[", "\n", "'dynamic_seq2seq/encoder/embedding_encoder:0'", ",", "\n", "'dynamic_seq2seq/decoder/embedding_decoder:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/output_projection/kernel:0'", "\n", "]", "\n", "# pylint: enable=line-too-long", "\n", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "model", ".", "Model", ",", "hparams", ",", "sess", ")", "\n", "\n", "m_vars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "_assertModelVariableNames", "(", "expected_var_names", ",", "\n", "[", "v", ".", "name", "for", "v", "in", "m_vars", "]", ",", "\n", "'NoAttentionResidualBiEncoder'", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'dynamic_seq2seq'", ",", "reuse", "=", "True", ")", ":", "\n", "          ", "last_enc_weight", "=", "tf", ".", "get_variable", "(", "\n", "'encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'", "\n", ")", "\n", "last_dec_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/multi_rnn_cell/cell_3/basic_lstm_cell/kernel'", ")", "\n", "", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "\n", "'NoAttentionResidualBiEncoder'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_enc_weight", ",", "sess", ",", "\n", "'NoAttentionResidualBiEncoder/last_enc_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_dec_weight", ",", "sess", ",", "\n", "'NoAttentionResidualBiEncoder/last_dec_weight'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "eval_m", "=", "self", ".", "_createTestEvalModel", "(", "model", ".", "Model", ",", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertEvalLossAndPredictCount", "(", "eval_m", ",", "sess", ",", "\n", "'NoAttentionResidualBiEncoder'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "model", ".", "Model", ",", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertInferLogits", "(", "infer_m", ",", "sess", ",", "'NoAttentionResidualBiEncoder'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testAttentionMechanismLuong": [[482, 549], ["utils.common_test_utils.create_test_hparams", "tensorflow.test.create_local_cluster", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Session", "model_test.ModelTest._createTestTrainModel", "tensorflow.trainable_variables", "model_test.ModelTest._assertModelVariableNames", "model_test.ModelTest._assertTrainStepsLoss", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "tensorflow.Session", "model_test.ModelTest._createTestEvalModel", "model_test.ModelTest._assertEvalLossAndPredictCount", "tensorflow.Session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertInferLogits", "tensorflow.Graph", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.Graph", "tensorflow.Graph", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariableNames", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestEvalModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertEvalLossAndPredictCount", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertInferLogits", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config"], ["", "", "", "def", "testAttentionMechanismLuong", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "attention", "=", "'luong'", ",", "\n", "attention_architecture", "=", "'standard'", ",", "\n", "num_layers", "=", "2", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "\n", "workers", ",", "_", "=", "tf", ".", "test", ".", "create_local_cluster", "(", "1", ",", "0", ")", "\n", "worker", "=", "workers", "[", "0", "]", "\n", "\n", "# pylint: disable=line-too-long", "\n", "expected_var_names", "=", "[", "\n", "'dynamic_seq2seq/encoder/embedding_encoder:0'", ",", "\n", "'dynamic_seq2seq/decoder/embedding_decoder:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/memory_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/output_projection/kernel:0'", "\n", "]", "\n", "# pylint: enable=line-too-long", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "\n", "m_vars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "_assertModelVariableNames", "(", "\n", "expected_var_names", ",", "[", "v", ".", "name", "\n", "for", "v", "in", "m_vars", "]", ",", "'AttentionMechanismLuong'", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'dynamic_seq2seq'", ",", "reuse", "=", "True", ")", ":", "\n", "# pylint: disable=line-too-long", "\n", "          ", "last_enc_weight", "=", "tf", ".", "get_variable", "(", "\n", "'encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'", ")", "\n", "last_dec_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'", ")", "\n", "att_layer_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/attention/attention_layer/kernel'", ")", "\n", "# pylint: enable=line-too-long", "\n", "", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "'AttentionMechanismLuong'", ")", "\n", "self", ".", "_assertModelVariable", "(", "last_enc_weight", ",", "sess", ",", "\n", "'AttentionMechanismLuong/last_enc_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "last_dec_weight", ",", "sess", ",", "\n", "'AttentionMechanismLuong/last_dec_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "att_layer_weight", ",", "sess", ",", "\n", "'AttentionMechanismLuong/att_layer_weight'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "eval_m", "=", "self", ".", "_createTestEvalModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertEvalLossAndPredictCount", "(", "eval_m", ",", "sess", ",", "\n", "'AttentionMechanismLuong'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertInferLogits", "(", "infer_m", ",", "sess", ",", "'AttentionMechanismLuong'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testAttentionMechanismScaledLuong": [[550, 623], ["utils.common_test_utils.create_test_hparams", "tensorflow.test.create_local_cluster", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Session", "model_test.ModelTest._createTestTrainModel", "tensorflow.trainable_variables", "model_test.ModelTest._assertModelVariableNames", "model_test.ModelTest._assertTrainStepsLoss", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "tensorflow.Session", "model_test.ModelTest._createTestEvalModel", "model_test.ModelTest._assertEvalLossAndPredictCount", "tensorflow.Session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertInferLogits", "tensorflow.Graph", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.Graph", "tensorflow.Graph", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariableNames", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestEvalModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertEvalLossAndPredictCount", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertInferLogits", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config"], ["", "", "", "def", "testAttentionMechanismScaledLuong", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "attention", "=", "'scaled_luong'", ",", "\n", "attention_architecture", "=", "'standard'", ",", "\n", "num_layers", "=", "2", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "\n", "workers", ",", "_", "=", "tf", ".", "test", ".", "create_local_cluster", "(", "1", ",", "0", ")", "\n", "worker", "=", "workers", "[", "0", "]", "\n", "\n", "# pylint: disable=line-too-long", "\n", "expected_var_names", "=", "[", "\n", "'dynamic_seq2seq/encoder/embedding_encoder:0'", ",", "\n", "'dynamic_seq2seq/decoder/embedding_decoder:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/memory_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/output_projection/kernel:0'", "\n", "]", "\n", "# pylint: enable=line-too-long", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "\n", "m_vars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "_assertModelVariableNames", "(", "expected_var_names", ",", "\n", "[", "v", ".", "name", "for", "v", "in", "m_vars", "]", ",", "\n", "'AttentionMechanismScaledLuong'", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'dynamic_seq2seq'", ",", "reuse", "=", "True", ")", ":", "\n", "# pylint: disable=line-too-long", "\n", "          ", "last_enc_weight", "=", "tf", ".", "get_variable", "(", "\n", "'encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'", ")", "\n", "last_dec_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'", ")", "\n", "att_layer_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/attention/attention_layer/kernel'", ")", "\n", "# pylint: enable=line-too-long", "\n", "\n", "", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "\n", "'AttentionMechanismScaledLuong'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_enc_weight", ",", "sess", ",", "\n", "'AttentionMechanismScaledLuong/last_enc_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_dec_weight", ",", "sess", ",", "\n", "'AttentionMechanismScaledLuong/last_dec_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "att_layer_weight", ",", "sess", ",", "\n", "'AttentionMechanismScaledLuong/att_layer_weight'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "eval_m", "=", "self", ".", "_createTestEvalModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertEvalLossAndPredictCount", "(", "eval_m", ",", "sess", ",", "\n", "'AttentionMechanismScaledLuong'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertInferLogits", "(", "infer_m", ",", "sess", ",", "'AttentionMechanismScaledLuong'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testAttentionMechanismBahdanau": [[624, 693], ["utils.common_test_utils.create_test_hparams", "tensorflow.test.create_local_cluster", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Session", "model_test.ModelTest._createTestTrainModel", "tensorflow.trainable_variables", "model_test.ModelTest._assertModelVariableNames", "model_test.ModelTest._assertTrainStepsLoss", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "tensorflow.Session", "model_test.ModelTest._createTestEvalModel", "model_test.ModelTest._assertEvalLossAndPredictCount", "tensorflow.Session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertInferLogits", "tensorflow.Graph", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.Graph", "tensorflow.Graph", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariableNames", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestEvalModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertEvalLossAndPredictCount", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertInferLogits", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config"], ["", "", "", "def", "testAttentionMechanismBahdanau", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "attention", "=", "'bahdanau'", ",", "\n", "attention_architecture", "=", "'standard'", ",", "\n", "num_layers", "=", "2", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "\n", "workers", ",", "_", "=", "tf", ".", "test", ".", "create_local_cluster", "(", "1", ",", "0", ")", "\n", "worker", "=", "workers", "[", "0", "]", "\n", "\n", "# pylint: disable=line-too-long", "\n", "expected_var_names", "=", "[", "\n", "'dynamic_seq2seq/encoder/embedding_encoder:0'", ",", "\n", "'dynamic_seq2seq/decoder/embedding_decoder:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/memory_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/bahdanau_attention/query_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/bahdanau_attention/attention_v:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/output_projection/kernel:0'", "\n", "]", "\n", "# pylint: enable=line-too-long", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "\n", "m_vars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "_assertModelVariableNames", "(", "\n", "expected_var_names", ",", "[", "v", ".", "name", "\n", "for", "v", "in", "m_vars", "]", ",", "'AttentionMechanismBahdanau'", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'dynamic_seq2seq'", ",", "reuse", "=", "True", ")", ":", "\n", "# pylint: disable=line-too-long", "\n", "          ", "last_enc_weight", "=", "tf", ".", "get_variable", "(", "\n", "'encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'", ")", "\n", "last_dec_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'", ")", "\n", "att_layer_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/attention/attention_layer/kernel'", ")", "\n", "# pylint: enable=line-too-long", "\n", "", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "'AttentionMechanismBahdanau'", ")", "\n", "self", ".", "_assertModelVariable", "(", "last_enc_weight", ",", "sess", ",", "\n", "'AttentionMechanismBahdanau/last_enc_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "last_dec_weight", ",", "sess", ",", "\n", "'AttentionMechanismBahdanau/last_dec_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "att_layer_weight", ",", "sess", ",", "\n", "'AttentionMechanismBahdanau/att_layer_weight'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "eval_m", "=", "self", ".", "_createTestEvalModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertEvalLossAndPredictCount", "(", "eval_m", ",", "sess", ",", "\n", "'AttentionMechanismBahdanau'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertInferLogits", "(", "infer_m", ",", "sess", ",", "'AttentionMechanismBahdanau'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testAttentionMechanismNormedBahdanau": [[694, 771], ["utils.common_test_utils.create_test_hparams", "tensorflow.test.create_local_cluster", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Session", "model_test.ModelTest._createTestTrainModel", "tensorflow.trainable_variables", "model_test.ModelTest._assertModelVariableNames", "model_test.ModelTest._assertTrainStepsLoss", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "tensorflow.Session", "model_test.ModelTest._createTestEvalModel", "model_test.ModelTest._assertEvalLossAndPredictCount", "tensorflow.Session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertInferLogits", "tensorflow.Graph", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.Graph", "tensorflow.Graph", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariableNames", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestEvalModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertEvalLossAndPredictCount", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertInferLogits", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config"], ["", "", "", "def", "testAttentionMechanismNormedBahdanau", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "attention", "=", "'normed_bahdanau'", ",", "\n", "attention_architecture", "=", "'standard'", ",", "\n", "num_layers", "=", "2", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "\n", "workers", ",", "_", "=", "tf", ".", "test", ".", "create_local_cluster", "(", "1", ",", "0", ")", "\n", "worker", "=", "workers", "[", "0", "]", "\n", "\n", "# pylint: disable=line-too-long", "\n", "expected_var_names", "=", "[", "\n", "'dynamic_seq2seq/encoder/embedding_encoder:0'", ",", "\n", "'dynamic_seq2seq/decoder/embedding_decoder:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/memory_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/bahdanau_attention/query_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/bahdanau_attention/attention_v:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/bahdanau_attention/attention_g:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/bahdanau_attention/attention_b:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/output_projection/kernel:0'", "\n", "]", "\n", "# pylint: enable=line-too-long", "\n", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "\n", "m_vars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "_assertModelVariableNames", "(", "expected_var_names", ",", "\n", "[", "v", ".", "name", "for", "v", "in", "m_vars", "]", ",", "\n", "'AttentionMechanismNormedBahdanau'", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'dynamic_seq2seq'", ",", "reuse", "=", "True", ")", ":", "\n", "# pylint: disable=line-too-long", "\n", "          ", "last_enc_weight", "=", "tf", ".", "get_variable", "(", "\n", "'encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'", ")", "\n", "last_dec_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'", ")", "\n", "att_layer_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/attention/attention_layer/kernel'", ")", "\n", "# pylint: enable=line-too-long", "\n", "", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "\n", "'AttentionMechanismNormedBahdanau'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_enc_weight", ",", "sess", ",", "\n", "'AttentionMechanismNormedBahdanau/last_enc_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_dec_weight", ",", "sess", ",", "\n", "'AttentionMechanismNormedBahdanau/last_dec_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "att_layer_weight", ",", "sess", ",", "\n", "'AttentionMechanismNormedBahdanau/att_layer_weight'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "eval_m", "=", "self", ".", "_createTestEvalModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertEvalLossAndPredictCount", "(", "eval_m", ",", "sess", ",", "\n", "'AttentionMechanismNormedBahdanau'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertInferLogits", "(", "infer_m", ",", "sess", ",", "\n", "'AttentionMechanismNormedBahdanau'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testUniEncoderStandardAttentionArchitecture": [[774, 851], ["utils.common_test_utils.create_test_hparams", "tensorflow.test.create_local_cluster", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Session", "model_test.ModelTest._createTestTrainModel", "tensorflow.trainable_variables", "model_test.ModelTest._assertModelVariableNames", "model_test.ModelTest._assertTrainStepsLoss", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "tensorflow.Session", "model_test.ModelTest._createTestEvalModel", "model_test.ModelTest._assertEvalLossAndPredictCount", "tensorflow.Session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertInferLogits", "tensorflow.Graph", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.Graph", "tensorflow.Graph", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariableNames", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestEvalModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertEvalLossAndPredictCount", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertInferLogits", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config"], ["", "", "", "def", "testUniEncoderStandardAttentionArchitecture", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "num_layers", "=", "4", ",", "\n", "attention", "=", "'scaled_luong'", ",", "\n", "attention_architecture", "=", "'standard'", ",", ")", "\n", "\n", "workers", ",", "_", "=", "tf", ".", "test", ".", "create_local_cluster", "(", "1", ",", "0", ")", "\n", "worker", "=", "workers", "[", "0", "]", "\n", "\n", "# pylint: disable=line-too-long", "\n", "expected_var_names", "=", "[", "\n", "'dynamic_seq2seq/encoder/embedding_encoder:0'", ",", "\n", "'dynamic_seq2seq/decoder/embedding_decoder:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/memory_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0'", ",", "\n", "'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/output_projection/kernel:0'", "\n", "]", "\n", "# pylint: enable=line-too-long", "\n", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "\n", "m_vars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "_assertModelVariableNames", "(", "expected_var_names", ",", "[", "\n", "v", ".", "name", "for", "v", "in", "m_vars", "\n", "]", ",", "'UniEncoderStandardAttentionArchitecture'", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'dynamic_seq2seq'", ",", "reuse", "=", "True", ")", ":", "\n", "          ", "last_enc_weight", "=", "tf", ".", "get_variable", "(", "\n", "'encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel'", ")", "\n", "last_dec_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel'", ")", "\n", "mem_layer_weight", "=", "tf", ".", "get_variable", "(", "'decoder/memory_layer/kernel'", ")", "\n", "", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "\n", "'UniEncoderStandardAttentionArchitecture'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_enc_weight", ",", "sess", ",", "\n", "'UniEncoderStandardAttentionArchitecture/last_enc_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "last_dec_weight", ",", "sess", ",", "\n", "'UniEncoderStandardAttentionArchitecture/last_dec_weight'", ")", "\n", "self", ".", "_assertModelVariable", "(", "\n", "mem_layer_weight", ",", "sess", ",", "\n", "'UniEncoderStandardAttentionArchitecture/mem_layer_weight'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "eval_m", "=", "self", ".", "_createTestEvalModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertEvalLossAndPredictCount", "(", "\n", "eval_m", ",", "sess", ",", "'UniEncoderStandardAttentionArchitecture'", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "attention_model", ".", "AttentionModel", ",", "\n", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertInferLogits", "(", "infer_m", ",", "sess", ",", "\n", "'UniEncoderStandardAttentionArchitecture'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._testGNMTModel": [[853, 925], ["utils.common_test_utils.create_test_hparams", "tensorflow.test.create_local_cluster", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Graph().as_default", "tensorflow.Session", "model_test.ModelTest._createTestTrainModel", "tensorflow.trainable_variables", "model_test.ModelTest._assertModelVariableNames", "model_test.ModelTest._assertTrainStepsLoss", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "model_test.ModelTest._assertModelVariable", "tensorflow.Session", "model_test.ModelTest._createTestEvalModel", "model_test.ModelTest._assertEvalLossAndPredictCount", "tensorflow.Session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertInferLogits", "tensorflow.Graph", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.Graph", "tensorflow.Graph", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config", "model_test.ModelTest._get_session_config"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariableNames", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertModelVariable", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestEvalModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertEvalLossAndPredictCount", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertInferLogits", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._get_session_config"], ["", "", "", "def", "_testGNMTModel", "(", "self", ",", "architecture", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'gnmt'", ",", "\n", "num_layers", "=", "4", ",", "\n", "attention", "=", "'scaled_luong'", ",", "\n", "attention_architecture", "=", "architecture", ")", "\n", "\n", "workers", ",", "_", "=", "tf", ".", "test", ".", "create_local_cluster", "(", "1", ",", "0", ")", "\n", "worker", "=", "workers", "[", "0", "]", "\n", "\n", "# pylint: disable=line-too-long", "\n", "expected_var_names", "=", "[", "\n", "'dynamic_seq2seq/encoder/embedding_encoder:0'", ",", "\n", "'dynamic_seq2seq/decoder/embedding_decoder:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/memory_layer/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/luong_attention/attention_g:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0'", ",", "\n", "'dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0'", ",", "\n", "'dynamic_seq2seq/decoder/output_projection/kernel:0'", "\n", "]", "\n", "# pylint: enable=line-too-long", "\n", "\n", "test_prefix", "=", "'GNMTModel_%s'", "%", "architecture", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "gnmt_model", ".", "GNMTModel", ",", "hparams", ",", "\n", "sess", ")", "\n", "\n", "m_vars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "self", ".", "_assertModelVariableNames", "(", "expected_var_names", ",", "\n", "[", "v", ".", "name", "for", "v", "in", "m_vars", "]", ",", "test_prefix", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'dynamic_seq2seq'", ",", "reuse", "=", "True", ")", ":", "\n", "          ", "last_enc_weight", "=", "tf", ".", "get_variable", "(", "\n", "'encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel'", ")", "\n", "last_dec_weight", "=", "tf", ".", "get_variable", "(", "\n", "'decoder/multi_rnn_cell/cell_3/basic_lstm_cell/kernel'", ")", "\n", "mem_layer_weight", "=", "tf", ".", "get_variable", "(", "'decoder/memory_layer/kernel'", ")", "\n", "", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "test_prefix", ")", "\n", "\n", "self", ".", "_assertModelVariable", "(", "last_enc_weight", ",", "sess", ",", "\n", "'%s/last_enc_weight'", "%", "test_prefix", ")", "\n", "self", ".", "_assertModelVariable", "(", "last_dec_weight", ",", "sess", ",", "\n", "'%s/last_dec_weight'", "%", "test_prefix", ")", "\n", "self", ".", "_assertModelVariable", "(", "mem_layer_weight", ",", "sess", ",", "\n", "'%s/mem_layer_weight'", "%", "test_prefix", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "eval_m", "=", "self", ".", "_createTestEvalModel", "(", "gnmt_model", ".", "GNMTModel", ",", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertEvalLossAndPredictCount", "(", "eval_m", ",", "sess", ",", "test_prefix", ")", "\n", "\n", "", "", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "Session", "(", "worker", ".", "target", ",", "config", "=", "self", ".", "_get_session_config", "(", ")", ")", "as", "sess", ":", "\n", "        ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "gnmt_model", ".", "GNMTModel", ",", "hparams", ",", "\n", "sess", ")", "\n", "self", ".", "_assertInferLogits", "(", "infer_m", ",", "sess", ",", "test_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testGNMTModel": [[926, 928], ["model_test.ModelTest._testGNMTModel"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._testGNMTModel"], ["", "", "", "def", "testGNMTModel", "(", "self", ")", ":", "\n", "    ", "self", ".", "_testGNMTModel", "(", "'gnmt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testGNMTModelV2": [[929, 931], ["model_test.ModelTest._testGNMTModel"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._testGNMTModel"], ["", "def", "testGNMTModelV2", "(", "self", ")", ":", "\n", "    ", "self", ".", "_testGNMTModel", "(", "'gnmt_v2'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testBeamSearchBasicModel": [[933, 949], ["utils.common_test_utils.create_test_hparams", "model_test.ModelTest.test_session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertBeamSearchOutputs"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertBeamSearchOutputs"], ["", "def", "testBeamSearchBasicModel", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "num_layers", "=", "1", ",", "\n", "attention", "=", "''", ",", "\n", "attention_architecture", "=", "''", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "hparams", ".", "beam_width", "=", "3", "\n", "hparams", ".", "tgt_max_len_infer", "=", "4", "\n", "assert_top_k_sentence", "=", "3", "\n", "\n", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "\n", "model", ".", "Model", ",", "hparams", ",", "sess", ",", "True", ")", "\n", "self", ".", "_assertBeamSearchOutputs", "(", "\n", "infer_m", ",", "sess", ",", "assert_top_k_sentence", ",", "'BeamSearchBasicModel'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testBeamSearchAttentionModel": [[950, 966], ["utils.common_test_utils.create_test_hparams", "model_test.ModelTest.test_session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertBeamSearchOutputs"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertBeamSearchOutputs"], ["", "", "def", "testBeamSearchAttentionModel", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "attention", "=", "'scaled_luong'", ",", "\n", "attention_architecture", "=", "'standard'", ",", "\n", "num_layers", "=", "2", ",", "\n", "use_residual", "=", "False", ",", ")", "\n", "hparams", ".", "beam_width", "=", "3", "\n", "hparams", ".", "tgt_max_len_infer", "=", "4", "\n", "assert_top_k_sentence", "=", "2", "\n", "\n", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "\n", "attention_model", ".", "AttentionModel", ",", "hparams", ",", "sess", ",", "True", ")", "\n", "self", ".", "_assertBeamSearchOutputs", "(", "\n", "infer_m", ",", "sess", ",", "assert_top_k_sentence", ",", "'BeamSearchAttentionModel'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testBeamSearchGNMTModel": [[967, 982], ["utils.common_test_utils.create_test_hparams", "model_test.ModelTest.test_session", "model_test.ModelTest._createTestInferModel", "model_test.ModelTest._assertBeamSearchOutputs"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestInferModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertBeamSearchOutputs"], ["", "", "def", "testBeamSearchGNMTModel", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'gnmt'", ",", "\n", "num_layers", "=", "4", ",", "\n", "attention", "=", "'scaled_luong'", ",", "\n", "attention_architecture", "=", "'gnmt'", ")", "\n", "hparams", ".", "beam_width", "=", "3", "\n", "hparams", ".", "tgt_max_len_infer", "=", "4", "\n", "assert_top_k_sentence", "=", "1", "\n", "\n", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "infer_m", "=", "self", ".", "_createTestInferModel", "(", "\n", "gnmt_model", ".", "GNMTModel", ",", "hparams", ",", "sess", ",", "True", ")", "\n", "self", ".", "_assertBeamSearchOutputs", "(", "\n", "infer_m", ",", "sess", ",", "assert_top_k_sentence", ",", "'BeamSearchGNMTModel'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testInitializerGlorotNormal": [[983, 996], ["utils.common_test_utils.create_test_hparams", "model_test.ModelTest.test_session", "model_test.ModelTest._createTestTrainModel", "model_test.ModelTest._assertTrainStepsLoss"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss"], ["", "", "def", "testInitializerGlorotNormal", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "num_layers", "=", "1", ",", "\n", "attention", "=", "''", ",", "\n", "attention_architecture", "=", "''", ",", "\n", "use_residual", "=", "False", ",", "\n", "init_op", "=", "'glorot_normal'", ")", "\n", "\n", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "model", ".", "Model", ",", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "\n", "'InitializerGlorotNormal'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest.testInitializerGlorotUniform": [[997, 1010], ["utils.common_test_utils.create_test_hparams", "model_test.ModelTest.test_session", "model_test.ModelTest._createTestTrainModel", "model_test.ModelTest._assertTrainStepsLoss"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._createTestTrainModel", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_test.ModelTest._assertTrainStepsLoss"], ["", "", "def", "testInitializerGlorotUniform", "(", "self", ")", ":", "\n", "    ", "hparams", "=", "common_test_utils", ".", "create_test_hparams", "(", "\n", "encoder_type", "=", "'uni'", ",", "\n", "num_layers", "=", "1", ",", "\n", "attention", "=", "''", ",", "\n", "attention_architecture", "=", "''", ",", "\n", "use_residual", "=", "False", ",", "\n", "init_op", "=", "'glorot_uniform'", ")", "\n", "\n", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "train_m", "=", "self", ".", "_createTestTrainModel", "(", "model", ".", "Model", ",", "hparams", ",", "sess", ")", "\n", "self", ".", "_assertTrainStepsLoss", "(", "train_m", ",", "sess", ",", "\n", "'InitializerGlorotUniform'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dense.Dense.__init__": [[51, 74], ["tensorflow.python.ops.init_ops.zeros_initializer", "tensorflow.python.layers.base.Layer.__init__", "tensorflow.python.layers.base.InputSpec", "isinstance"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__"], ["def", "__init__", "(", "self", ",", "units", ",", "\n", "activation", "=", "None", ",", "\n", "use_bias", "=", "True", ",", "\n", "kernel_initializer", "=", "None", ",", "\n", "bias_initializer", "=", "init_ops", ".", "zeros_initializer", "(", ")", ",", "\n", "kernel_regularizer", "=", "None", ",", "\n", "bias_regularizer", "=", "None", ",", "\n", "activity_regularizer", "=", "None", ",", "\n", "trainable", "=", "True", ",", "\n", "name", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "    ", "super", "(", "Dense", ",", "self", ")", ".", "__init__", "(", "trainable", "=", "trainable", ",", "name", "=", "name", ",", "**", "kwargs", ")", "\n", "if", "not", "isinstance", "(", "units", ",", "list", ")", ":", "\n", "      ", "units", "=", "[", "units", "]", "\n", "", "self", ".", "units", "=", "units", "\n", "self", ".", "activation", "=", "activation", "\n", "self", ".", "use_bias", "=", "use_bias", "\n", "self", ".", "kernel_initializer", "=", "kernel_initializer", "\n", "self", ".", "bias_initializer", "=", "bias_initializer", "\n", "self", ".", "kernel_regularizer", "=", "kernel_regularizer", "\n", "self", ".", "bias_regularizer", "=", "bias_regularizer", "\n", "self", ".", "activity_regularizer", "=", "activity_regularizer", "\n", "self", ".", "input_spec", "=", "base", ".", "InputSpec", "(", "min_ndim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dense.Dense.build": [[75, 100], ["tensorflow.python.framework.tensor_shape.TensorShape", "tensorflow.python.layers.base.InputSpec", "ValueError", "dense.Dense.add_variable", "enumerate", "dense.Dense.add_variable", "str", "enumerate", "str"], "methods", ["None"], ["", "def", "build", "(", "self", ",", "input_shape", ")", ":", "\n", "    ", "input_shape", "=", "tensor_shape", ".", "TensorShape", "(", "input_shape", ")", "\n", "if", "input_shape", "[", "-", "1", "]", ".", "value", "is", "None", ":", "\n", "      ", "raise", "ValueError", "(", "'The last dimension of the inputs to `Dense` '", "\n", "'should be defined. Found `None`.'", ")", "\n", "", "self", ".", "input_spec", "=", "base", ".", "InputSpec", "(", "min_ndim", "=", "2", ",", "\n", "axes", "=", "{", "-", "1", ":", "input_shape", "[", "-", "1", "]", ".", "value", "}", ")", "\n", "\n", "self", ".", "kernel", "=", "[", "self", ".", "add_variable", "(", "'kernel_'", "+", "str", "(", "i", ")", ",", "\n", "shape", "=", "[", "input_shape", "[", "-", "1", "]", ".", "value", "if", "i", "==", "0", "else", "self", ".", "units", "[", "i", "-", "1", "]", ",", "num_units", "]", ",", "\n", "initializer", "=", "self", ".", "kernel_initializer", ",", "\n", "regularizer", "=", "self", ".", "kernel_regularizer", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "trainable", "=", "True", ")", "for", "i", ",", "num_units", "in", "enumerate", "(", "self", ".", "units", ")", "]", "\n", "if", "self", ".", "use_bias", ":", "\n", "      ", "self", ".", "bias", "=", "[", "self", ".", "add_variable", "(", "'bias_'", "+", "str", "(", "i", ")", ",", "\n", "shape", "=", "[", "num_units", ",", "]", ",", "\n", "initializer", "=", "self", ".", "bias_initializer", ",", "\n", "regularizer", "=", "self", ".", "bias_regularizer", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", "trainable", "=", "True", ")", "for", "i", ",", "num_units", "in", "enumerate", "(", "self", ".", "units", ")", "]", "\n", "", "else", ":", "\n", "      ", "self", ".", "bias", "=", "None", "\n", "\n", "", "self", ".", "built", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dense.Dense.call": [[101, 122], ["tensorflow.python.framework.ops.convert_to_tensor", "range", "len", "tensorflow.python.framework.ops.convert_to_tensor.get_shape().as_list", "len", "tensorflow.python.ops.standard_ops.tensordot", "tensorflow.python.ops.nn.bias_add.set_shape", "tensorflow.python.ops.standard_ops.matmul", "tensorflow.python.ops.nn.bias_add", "dense.Dense.activation", "tensorflow.python.framework.ops.convert_to_tensor.get_shape", "len"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "inputs", ")", ":", "\n", "    ", "inputs", "=", "ops", ".", "convert_to_tensor", "(", "inputs", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "units", ")", ")", ":", "\n", "      ", "shape", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "output_shape", "=", "shape", "[", ":", "-", "1", "]", "+", "[", "self", ".", "units", "[", "i", "]", "]", "\n", "if", "len", "(", "output_shape", ")", ">", "2", ":", "\n", "# Broadcasting is required for the inputs.", "\n", "        ", "outputs", "=", "standard_ops", ".", "tensordot", "(", "inputs", ",", "self", ".", "kernel", "[", "i", "]", ",", "[", "[", "len", "(", "shape", ")", "-", "1", "]", ",", "\n", "[", "0", "]", "]", ")", "\n", "# Reshape the output back to the original ndim of the input.", "\n", "outputs", ".", "set_shape", "(", "output_shape", ")", "\n", "", "else", ":", "\n", "        ", "outputs", "=", "standard_ops", ".", "matmul", "(", "inputs", ",", "self", ".", "kernel", "[", "i", "]", ")", "\n", "", "if", "self", ".", "use_bias", ":", "\n", "        ", "outputs", "=", "nn", ".", "bias_add", "(", "outputs", ",", "self", ".", "bias", "[", "i", "]", ")", "\n", "", "if", "self", ".", "activation", "is", "not", "None", ":", "\n", "        ", "return", "self", ".", "activation", "(", "outputs", ")", "# pylint: disable=not-callable", "\n", "\n", "", "inputs", "=", "outputs", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dense.Dense._compute_output_shape": [[123, 131], ["tensorflow.python.framework.tensor_shape.TensorShape", "input_shape.with_rank_at_least.with_rank_at_least.with_rank_at_least", "input_shape[].concatenate", "ValueError"], "methods", ["None"], ["", "def", "_compute_output_shape", "(", "self", ",", "input_shape", ")", ":", "\n", "    ", "input_shape", "=", "tensor_shape", ".", "TensorShape", "(", "input_shape", ")", "\n", "input_shape", "=", "input_shape", ".", "with_rank_at_least", "(", "2", ")", "\n", "if", "input_shape", "[", "-", "1", "]", ".", "value", "is", "None", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "'The innermost dimension of input_shape must be defined, but saw: %s'", "\n", "%", "input_shape", ")", "\n", "", "return", "input_shape", "[", ":", "-", "1", "]", ".", "concatenate", "(", "self", ".", "units", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference._decode_inference_indices": [[43, 79], ["misc_utils.print_out", "time.time", "misc_utils.print_time", "trans_f.write", "codecs.getwriter", "tensorflow.gfile.GFile", "model.decode", "nmt_utils.get_translation", "trans_f.write", "misc_utils.print_out", "len", "misc_utils.print_out", "tensorflow.Summary", "tf.Summary.ParseFromString", "tensorflow.gfile.GFile", "img_f.write", "str"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_time", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.get_translation", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["def", "_decode_inference_indices", "(", "model", ",", "sess", ",", "output_infer", ",", "\n", "output_infer_summary_prefix", ",", "\n", "inference_indices", ",", "\n", "tgt_sos", ",", "\n", "tgt_eos", ",", "\n", "bpe_delimiter", ")", ":", "\n", "  ", "\"\"\"Decoding only a specific set of sentences.\"\"\"", "\n", "utils", ".", "print_out", "(", "\"  decoding to output %s , num sents %d.\"", "%", "\n", "(", "output_infer", ",", "len", "(", "inference_indices", ")", ")", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "output_infer", ",", "mode", "=", "\"wb\"", ")", ")", "as", "trans_f", ":", "\n", "    ", "trans_f", ".", "write", "(", "\"\"", ")", "# Write empty string to ensure file is created.", "\n", "for", "decode_id", "in", "inference_indices", ":", "\n", "      ", "nmt_outputs", ",", "infer_summary", "=", "model", ".", "decode", "(", "sess", ")", "\n", "\n", "# get text translation", "\n", "assert", "nmt_outputs", ".", "shape", "[", "0", "]", "==", "1", "\n", "translation", "=", "nmt_utils", ".", "get_translation", "(", "\n", "nmt_outputs", ",", "\n", "sent_id", "=", "0", ",", "\n", "tgt_sos", "=", "tgt_sos", ",", "\n", "tgt_eos", "=", "tgt_eos", ",", "\n", "bpe_delimiter", "=", "bpe_delimiter", ")", "\n", "\n", "if", "infer_summary", "is", "not", "None", ":", "# Attention models", "\n", "        ", "image_file", "=", "output_infer_summary_prefix", "+", "str", "(", "decode_id", ")", "+", "\".png\"", "\n", "utils", ".", "print_out", "(", "\"  save attention image to %s*\"", "%", "image_file", ")", "\n", "image_summ", "=", "tf", ".", "Summary", "(", ")", "\n", "image_summ", ".", "ParseFromString", "(", "infer_summary", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "image_file", ",", "mode", "=", "\"w\"", ")", "as", "img_f", ":", "\n", "          ", "img_f", ".", "write", "(", "image_summ", ".", "value", "[", "0", "]", ".", "image", ".", "encoded_image_string", ")", "\n", "\n", "", "", "trans_f", ".", "write", "(", "\"%s\\n\"", "%", "translation", ")", "\n", "utils", ".", "print_out", "(", "b\"%s\\n\"", "%", "translation", ")", "\n", "", "", "utils", ".", "print_time", "(", "\"  done\"", ",", "start_time", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.load_data": [[81, 91], ["f.read().splitlines", "codecs.getreader", "tensorflow.gfile.GFile", "f.read"], "function", ["None"], ["", "def", "load_data", "(", "inference_input_file", ",", "hparams", "=", "None", ")", ":", "\n", "  ", "\"\"\"Load inference data.\"\"\"", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "inference_input_file", ",", "mode", "=", "\"rb\"", ")", ")", "as", "f", ":", "\n", "    ", "inference_data", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "\n", "", "if", "hparams", "and", "hparams", ".", "inference_indices", ":", "\n", "    ", "inference_data", "=", "[", "inference_data", "[", "i", "]", "for", "i", "in", "hparams", ".", "inference_indices", "]", "\n", "\n", "", "return", "inference_data", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.inference": [[93, 130], ["model_helper.create_infer_model", "inference.single_worker_inference", "inference.multi_worker_inference", "ValueError"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_infer_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.single_worker_inference", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.multi_worker_inference"], ["", "def", "inference", "(", "ckpt", ",", "\n", "inference_input_file", ",", "\n", "inference_output_file", ",", "\n", "hparams", ",", "\n", "num_workers", "=", "1", ",", "\n", "jobid", "=", "0", ",", "\n", "scope", "=", "None", ")", ":", "\n", "  ", "\"\"\"Perform translation.\"\"\"", "\n", "if", "hparams", ".", "inference_indices", ":", "\n", "    ", "assert", "num_workers", "==", "1", "\n", "\n", "", "if", "not", "hparams", ".", "attention", ":", "\n", "    ", "model_creator", "=", "nmt_model", ".", "Model", "\n", "", "elif", "hparams", ".", "attention_architecture", "==", "\"standard\"", ":", "\n", "    ", "model_creator", "=", "attention_model", ".", "AttentionModel", "\n", "", "elif", "hparams", ".", "attention_architecture", "in", "[", "\"gnmt\"", ",", "\"gnmt_v2\"", "]", ":", "\n", "    ", "model_creator", "=", "gnmt_model", ".", "GNMTModel", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unknown model architecture\"", ")", "\n", "", "infer_model", "=", "model_helper", ".", "create_infer_model", "(", "model_creator", ",", "hparams", ",", "scope", ")", "\n", "\n", "if", "num_workers", "==", "1", ":", "\n", "    ", "single_worker_inference", "(", "\n", "infer_model", ",", "\n", "ckpt", ",", "\n", "inference_input_file", ",", "\n", "inference_output_file", ",", "\n", "hparams", ")", "\n", "", "else", ":", "\n", "    ", "multi_worker_inference", "(", "\n", "infer_model", ",", "\n", "ckpt", ",", "\n", "inference_input_file", ",", "\n", "inference_output_file", ",", "\n", "hparams", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "jobid", "=", "jobid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.single_worker_inference": [[132, 177], ["inference.load_data", "tensorflow.Session", "model_helper.load_model", "sess.run", "misc_utils.print_out", "inference._decode_inference_indices", "nmt_utils.decode_and_evaluate", "misc_utils.get_config_proto"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.load_data", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.load_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference._decode_inference_indices", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.decode_and_evaluate", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.get_config_proto"], ["", "", "def", "single_worker_inference", "(", "infer_model", ",", "\n", "ckpt", ",", "\n", "inference_input_file", ",", "\n", "inference_output_file", ",", "\n", "hparams", ")", ":", "\n", "  ", "\"\"\"Inference with a single worker.\"\"\"", "\n", "output_infer", "=", "inference_output_file", "\n", "\n", "# Read data", "\n", "infer_data", "=", "load_data", "(", "inference_input_file", ",", "hparams", ")", "\n", "\n", "with", "tf", ".", "Session", "(", "\n", "graph", "=", "infer_model", ".", "graph", ",", "config", "=", "utils", ".", "get_config_proto", "(", ")", ")", "as", "sess", ":", "\n", "    ", "loaded_infer_model", "=", "model_helper", ".", "load_model", "(", "\n", "infer_model", ".", "model", ",", "ckpt", ",", "sess", ",", "\"infer\"", ")", "\n", "sess", ".", "run", "(", "\n", "infer_model", ".", "iterator", ".", "initializer", ",", "\n", "feed_dict", "=", "{", "\n", "infer_model", ".", "src_placeholder", ":", "infer_data", ",", "\n", "infer_model", ".", "batch_size_placeholder", ":", "hparams", ".", "infer_batch_size", "\n", "}", ")", "\n", "# Decode", "\n", "utils", ".", "print_out", "(", "\"# Start decoding\"", ")", "\n", "if", "hparams", ".", "inference_indices", ":", "\n", "      ", "_decode_inference_indices", "(", "\n", "loaded_infer_model", ",", "\n", "sess", ",", "\n", "output_infer", "=", "output_infer", ",", "\n", "output_infer_summary_prefix", "=", "output_infer", ",", "\n", "inference_indices", "=", "hparams", ".", "inference_indices", ",", "\n", "tgt_sos", "=", "hparams", ".", "sos", ",", "\n", "tgt_eos", "=", "hparams", ".", "eos", ",", "\n", "bpe_delimiter", "=", "hparams", ".", "bpe_delimiter", ")", "\n", "", "else", ":", "\n", "      ", "nmt_utils", ".", "decode_and_evaluate", "(", "\n", "\"infer\"", ",", "\n", "loaded_infer_model", ",", "\n", "sess", ",", "\n", "output_infer", ",", "\n", "ref_file", "=", "None", ",", "\n", "metrics", "=", "hparams", ".", "metrics", ",", "\n", "bpe_delimiter", "=", "hparams", ".", "bpe_delimiter", ",", "\n", "beam_width", "=", "hparams", ".", "beam_width", ",", "\n", "tgt_sos", "=", "hparams", ".", "sos", ",", "\n", "tgt_eos", "=", "hparams", ".", "eos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.multi_worker_inference": [[179, 245], ["inference.load_data", "len", "min", "int", "tensorflow.Session", "model_helper.load_model", "sess.run", "misc_utils.print_out", "nmt_utils.decode_and_evaluate", "tensorflow.gfile.Rename", "range", "misc_utils.get_config_proto", "codecs.getwriter", "tensorflow.gfile.GFile", "tensorflow.gfile.Remove", "tensorflow.gfile.Exists", "misc_utils.print_out", "time.sleep", "codecs.getreader", "tensorflow.gfile.GFile", "final_f.write"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.load_data", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.load_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.decode_and_evaluate", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.get_config_proto", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "", "", "def", "multi_worker_inference", "(", "infer_model", ",", "\n", "ckpt", ",", "\n", "inference_input_file", ",", "\n", "inference_output_file", ",", "\n", "hparams", ",", "\n", "num_workers", ",", "\n", "jobid", ")", ":", "\n", "  ", "\"\"\"Inference using multiple workers.\"\"\"", "\n", "assert", "num_workers", ">", "1", "\n", "\n", "final_output_infer", "=", "inference_output_file", "\n", "output_infer", "=", "\"%s_%d\"", "%", "(", "inference_output_file", ",", "jobid", ")", "\n", "output_infer_done", "=", "\"%s_done_%d\"", "%", "(", "inference_output_file", ",", "jobid", ")", "\n", "\n", "# Read data", "\n", "infer_data", "=", "load_data", "(", "inference_input_file", ",", "hparams", ")", "\n", "\n", "# Split data to multiple workers", "\n", "total_load", "=", "len", "(", "infer_data", ")", "\n", "load_per_worker", "=", "int", "(", "(", "total_load", "-", "1", ")", "/", "num_workers", ")", "+", "1", "\n", "start_position", "=", "jobid", "*", "load_per_worker", "\n", "end_position", "=", "min", "(", "start_position", "+", "load_per_worker", ",", "total_load", ")", "\n", "infer_data", "=", "infer_data", "[", "start_position", ":", "end_position", "]", "\n", "\n", "with", "tf", ".", "Session", "(", "\n", "graph", "=", "infer_model", ".", "graph", ",", "config", "=", "utils", ".", "get_config_proto", "(", ")", ")", "as", "sess", ":", "\n", "    ", "loaded_infer_model", "=", "model_helper", ".", "load_model", "(", "\n", "infer_model", ".", "model", ",", "ckpt", ",", "sess", ",", "\"infer\"", ")", "\n", "sess", ".", "run", "(", "infer_model", ".", "iterator", ".", "initializer", ",", "\n", "{", "\n", "infer_model", ".", "src_placeholder", ":", "infer_data", ",", "\n", "infer_model", ".", "batch_size_placeholder", ":", "hparams", ".", "infer_batch_size", "\n", "}", ")", "\n", "# Decode", "\n", "utils", ".", "print_out", "(", "\"# Start decoding\"", ")", "\n", "nmt_utils", ".", "decode_and_evaluate", "(", "\n", "\"infer\"", ",", "\n", "loaded_infer_model", ",", "\n", "sess", ",", "\n", "output_infer", ",", "\n", "ref_file", "=", "None", ",", "\n", "metrics", "=", "hparams", ".", "metrics", ",", "\n", "bpe_delimiter", "=", "hparams", ".", "bpe_delimiter", ",", "\n", "beam_width", "=", "hparams", ".", "beam_width", ",", "\n", "tgt_eos", "=", "hparams", ".", "eos", ")", "\n", "\n", "# Change file name to indicate the file writing is completed.", "\n", "tf", ".", "gfile", ".", "Rename", "(", "output_infer", ",", "output_infer_done", ",", "overwrite", "=", "True", ")", "\n", "\n", "# Job 0 is responsible for the clean up.", "\n", "if", "jobid", "!=", "0", ":", "return", "\n", "\n", "# Now write all translations", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "final_output_infer", ",", "mode", "=", "\"wb\"", ")", ")", "as", "final_f", ":", "\n", "      ", "for", "worker_id", "in", "range", "(", "num_workers", ")", ":", "\n", "        ", "worker_infer_done", "=", "\"%s_done_%d\"", "%", "(", "inference_output_file", ",", "worker_id", ")", "\n", "while", "not", "tf", ".", "gfile", ".", "Exists", "(", "worker_infer_done", ")", ":", "\n", "          ", "utils", ".", "print_out", "(", "\"  waitting job %d to complete.\"", "%", "worker_id", ")", "\n", "time", ".", "sleep", "(", "10", ")", "\n", "\n", "", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "worker_infer_done", ",", "mode", "=", "\"rb\"", ")", ")", "as", "f", ":", "\n", "          ", "for", "translation", "in", "f", ":", "\n", "            ", "final_f", ".", "write", "(", "\"%s\"", "%", "translation", ")", "\n", "", "", "tf", ".", "gfile", ".", "Remove", "(", "worker_infer_done", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.bleu._get_ngrams": [[28, 46], ["collections.Counter", "range", "range", "tuple", "len"], "function", ["None"], ["def", "_get_ngrams", "(", "segment", ",", "max_order", ")", ":", "\n", "  ", "\"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"", "\n", "ngram_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "order", "in", "range", "(", "1", ",", "max_order", "+", "1", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "segment", ")", "-", "order", "+", "1", ")", ":", "\n", "      ", "ngram", "=", "tuple", "(", "segment", "[", "i", ":", "i", "+", "order", "]", ")", "\n", "ngram_counts", "[", "ngram", "]", "+=", "1", "\n", "", "", "return", "ngram_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.bleu.compute_bleu": [[48, 113], ["zip", "range", "min", "len", "collections.Counter", "bleu._get_ngrams", "range", "min", "sum", "math.exp", "float", "math.exp", "bleu._get_ngrams", "len", "len", "float", "math.log", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_ngrams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_ngrams"], ["", "def", "compute_bleu", "(", "reference_corpus", ",", "translation_corpus", ",", "max_order", "=", "4", ",", "\n", "smooth", "=", "False", ")", ":", "\n", "  ", "\"\"\"Computes BLEU score of translated segments against one or more references.\n\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n  Returns:\n    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n    precisions and brevity penalty.\n  \"\"\"", "\n", "matches_by_order", "=", "[", "0", "]", "*", "max_order", "\n", "possible_matches_by_order", "=", "[", "0", "]", "*", "max_order", "\n", "reference_length", "=", "0", "\n", "translation_length", "=", "0", "\n", "for", "(", "references", ",", "translation", ")", "in", "zip", "(", "reference_corpus", ",", "\n", "translation_corpus", ")", ":", "\n", "    ", "reference_length", "+=", "min", "(", "len", "(", "r", ")", "for", "r", "in", "references", ")", "\n", "translation_length", "+=", "len", "(", "translation", ")", "\n", "\n", "merged_ref_ngram_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "reference", "in", "references", ":", "\n", "      ", "merged_ref_ngram_counts", "|=", "_get_ngrams", "(", "reference", ",", "max_order", ")", "\n", "", "translation_ngram_counts", "=", "_get_ngrams", "(", "translation", ",", "max_order", ")", "\n", "overlap", "=", "translation_ngram_counts", "&", "merged_ref_ngram_counts", "\n", "for", "ngram", "in", "overlap", ":", "\n", "      ", "matches_by_order", "[", "len", "(", "ngram", ")", "-", "1", "]", "+=", "overlap", "[", "ngram", "]", "\n", "", "for", "order", "in", "range", "(", "1", ",", "max_order", "+", "1", ")", ":", "\n", "      ", "possible_matches", "=", "len", "(", "translation", ")", "-", "order", "+", "1", "\n", "if", "possible_matches", ">", "0", ":", "\n", "        ", "possible_matches_by_order", "[", "order", "-", "1", "]", "+=", "possible_matches", "\n", "\n", "", "", "", "precisions", "=", "[", "0", "]", "*", "max_order", "\n", "for", "i", "in", "range", "(", "0", ",", "max_order", ")", ":", "\n", "    ", "if", "smooth", ":", "\n", "      ", "precisions", "[", "i", "]", "=", "(", "(", "matches_by_order", "[", "i", "]", "+", "1.", ")", "/", "\n", "(", "possible_matches_by_order", "[", "i", "]", "+", "1.", ")", ")", "\n", "", "else", ":", "\n", "      ", "if", "possible_matches_by_order", "[", "i", "]", ">", "0", ":", "\n", "        ", "precisions", "[", "i", "]", "=", "(", "float", "(", "matches_by_order", "[", "i", "]", ")", "/", "\n", "possible_matches_by_order", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "        ", "precisions", "[", "i", "]", "=", "0.0", "\n", "\n", "", "", "", "if", "min", "(", "precisions", ")", ">", "0", ":", "\n", "    ", "p_log_sum", "=", "sum", "(", "(", "1.", "/", "max_order", ")", "*", "math", ".", "log", "(", "p", ")", "for", "p", "in", "precisions", ")", "\n", "geo_mean", "=", "math", ".", "exp", "(", "p_log_sum", ")", "\n", "", "else", ":", "\n", "    ", "geo_mean", "=", "0", "\n", "\n", "", "ratio", "=", "float", "(", "translation_length", ")", "/", "reference_length", "\n", "\n", "if", "ratio", ">", "1.0", ":", "\n", "    ", "bp", "=", "1.", "\n", "", "else", ":", "\n", "    ", "bp", "=", "math", ".", "exp", "(", "1", "-", "1.", "/", "ratio", ")", "\n", "\n", "", "bleu", "=", "geo_mean", "*", "bp", "\n", "\n", "return", "(", "bleu", ",", "precisions", ",", "bp", ",", "ratio", ",", "translation_length", ",", "reference_length", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.check_tensorflow_version": [[31, 34], ["EnvironmentError"], "function", ["None"], ["def", "check_tensorflow_version", "(", ")", ":", "\n", "  ", "if", "tf", ".", "__version__", "<", "\"1.2.0\"", ":", "\n", "    ", "raise", "EnvironmentError", "(", "\"Tensorflow version must >= 1.2.0\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.safe_exp": [[36, 43], ["math.exp", "float"], "function", ["None"], ["", "", "def", "safe_exp", "(", "value", ")", ":", "\n", "  ", "\"\"\"Exponentiation with catching of overflow error.\"\"\"", "\n", "try", ":", "\n", "    ", "ans", "=", "math", ".", "exp", "(", "value", ")", "\n", "", "except", "OverflowError", ":", "\n", "    ", "ans", "=", "float", "(", "\"inf\"", ")", "\n", "", "return", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.print_time": [[45, 50], ["print", "sys.stdout.flush", "time.time", "time.ctime", "time.time"], "function", ["None"], ["", "def", "print_time", "(", "s", ",", "start_time", ")", ":", "\n", "  ", "\"\"\"Take a start time, print elapsed duration, and return a new time.\"\"\"", "\n", "print", "(", "\"%s, time %ds, %s.\"", "%", "(", "s", ",", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ",", "time", ".", "ctime", "(", ")", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "return", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.print_out": [[52, 71], ["isinstance", "s.decode.encode", "print", "sys.stdout.flush", "s.decode.decode", "f.write", "isinstance", "out_s.decode.decode", "sys.stdout.write", "s.decode.encode", "f.write"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode"], ["", "def", "print_out", "(", "s", ",", "f", "=", "None", ",", "new_line", "=", "True", ")", ":", "\n", "  ", "\"\"\"Similar to print but with support to flush and output to a file.\"\"\"", "\n", "if", "isinstance", "(", "s", ",", "bytes", ")", ":", "\n", "    ", "s", "=", "s", ".", "decode", "(", "\"utf-8\"", ")", "\n", "\n", "", "if", "f", ":", "\n", "    ", "f", ".", "write", "(", "s", ".", "encode", "(", "\"utf-8\"", ")", ")", "\n", "if", "new_line", ":", "\n", "      ", "f", ".", "write", "(", "b\"\\n\"", ")", "\n", "\n", "# stdout", "\n", "", "", "out_s", "=", "s", ".", "encode", "(", "\"utf-8\"", ")", "\n", "if", "not", "isinstance", "(", "out_s", ",", "str", ")", ":", "\n", "    ", "out_s", "=", "out_s", ".", "decode", "(", "\"utf-8\"", ")", "\n", "", "print", "(", "out_s", ",", "end", "=", "\"\"", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "\n", "if", "new_line", ":", "\n", "    ", "sys", ".", "stdout", ".", "write", "(", "\"\\n\"", ")", "\n", "", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.print_hparams": [[73, 80], ["hparams.values", "sorted", "hparams.values.keys", "all", "misc_utils.print_out", "str"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "print_hparams", "(", "hparams", ",", "skip_patterns", "=", "None", ")", ":", "\n", "  ", "\"\"\"Print hparams, can skip keys based on pattern.\"\"\"", "\n", "values", "=", "hparams", ".", "values", "(", ")", "\n", "for", "key", "in", "sorted", "(", "values", ".", "keys", "(", ")", ")", ":", "\n", "    ", "if", "not", "skip_patterns", "or", "all", "(", "\n", "[", "skip_pattern", "not", "in", "key", "for", "skip_pattern", "in", "skip_patterns", "]", ")", ":", "\n", "      ", "print_out", "(", "\"  %s=%s\"", "%", "(", "key", ",", "str", "(", "values", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.load_hparams": [[82, 97], ["os.path.join", "tensorflow.gfile.Exists", "misc_utils.print_out", "codecs.getreader", "tensorflow.gfile.GFile", "json.load", "tensorflow.contrib.training.HParams", "misc_utils.print_out"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "", "", "def", "load_hparams", "(", "model_dir", ")", ":", "\n", "  ", "\"\"\"Load hparams from an existing model directory.\"\"\"", "\n", "hparams_file", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "\"hparams\"", ")", "\n", "if", "tf", ".", "gfile", ".", "Exists", "(", "hparams_file", ")", ":", "\n", "    ", "print_out", "(", "\"# Loading hparams from %s\"", "%", "hparams_file", ")", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "hparams_file", ",", "\"rb\"", ")", ")", "as", "f", ":", "\n", "      ", "try", ":", "\n", "        ", "hparams_values", "=", "json", ".", "load", "(", "f", ")", "\n", "hparams", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "**", "hparams_values", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "print_out", "(", "\"  can't load hparams file\"", ")", "\n", "return", "None", "\n", "", "", "return", "hparams", "\n", "", "else", ":", "\n", "    ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.maybe_parse_standard_hparams": [[99, 110], ["tensorflow.gfile.Exists", "misc_utils.print_out", "tensorflow.gfile.GFile", "hparams.parse_json", "f.read"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "", "def", "maybe_parse_standard_hparams", "(", "hparams", ",", "hparams_path", ")", ":", "\n", "  ", "\"\"\"Override hparams values with existing standard hparams config.\"\"\"", "\n", "if", "not", "hparams_path", ":", "\n", "    ", "return", "hparams", "\n", "\n", "", "if", "tf", ".", "gfile", ".", "Exists", "(", "hparams_path", ")", ":", "\n", "    ", "print_out", "(", "\"# Loading standard hparams from %s\"", "%", "hparams_path", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "hparams_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "      ", "hparams", ".", "parse_json", "(", "f", ".", "read", "(", ")", ")", "\n", "\n", "", "", "return", "hparams", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.save_hparams": [[112, 118], ["os.path.join", "misc_utils.print_out", "f.write", "codecs.getwriter", "tensorflow.gfile.GFile", "hparams.to_json"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "save_hparams", "(", "out_dir", ",", "hparams", ")", ":", "\n", "  ", "\"\"\"Save hparams.\"\"\"", "\n", "hparams_file", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"hparams\"", ")", "\n", "print_out", "(", "\"  saving hparams to %s\"", "%", "hparams_file", ")", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "hparams_file", ",", "\"wb\"", ")", ")", "as", "f", ":", "\n", "    ", "f", ".", "write", "(", "hparams", ".", "to_json", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.debug_tensor": [[120, 125], ["tensorflow.Print", "tensorflow.shape"], "function", ["None"], ["", "", "def", "debug_tensor", "(", "s", ",", "msg", "=", "None", ",", "summarize", "=", "10", ")", ":", "\n", "  ", "\"\"\"Print the shape and value of a tensor at test time. Return a new tensor.\"\"\"", "\n", "if", "not", "msg", ":", "\n", "    ", "msg", "=", "s", ".", "name", "\n", "", "return", "tf", ".", "Print", "(", "s", ",", "[", "tf", ".", "shape", "(", "s", ")", ",", "s", "]", ",", "msg", "+", "\" \"", ",", "summarize", "=", "summarize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.add_summary": [[127, 133], ["tensorflow.Summary", "summary_writer.add_summary", "tensorflow.Summary.Value"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.add_summary"], ["", "def", "add_summary", "(", "summary_writer", ",", "global_step", ",", "tag", ",", "value", ")", ":", "\n", "  ", "\"\"\"Add a new summary to the current summary_writer.\n  Useful to log things that are not part of the training graph, e.g., tag=BLEU.\n  \"\"\"", "\n", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "[", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "tag", ",", "simple_value", "=", "value", ")", "]", ")", "\n", "summary_writer", ".", "add_summary", "(", "summary", ",", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.get_config_proto": [[135, 143], ["tensorflow.ConfigProto"], "function", ["None"], ["", "def", "get_config_proto", "(", "log_device_placement", "=", "False", ",", "allow_soft_placement", "=", "True", ")", ":", "\n", "# GPU options:", "\n", "# https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html", "\n", "  ", "config_proto", "=", "tf", ".", "ConfigProto", "(", "\n", "log_device_placement", "=", "log_device_placement", ",", "\n", "allow_soft_placement", "=", "allow_soft_placement", ")", "\n", "config_proto", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "return", "config_proto", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.format_text": [[145, 151], ["hasattr", "isinstance"], "function", ["None"], ["", "def", "format_text", "(", "words", ")", ":", "\n", "  ", "\"\"\"Convert a sequence words into sentence.\"\"\"", "\n", "if", "(", "not", "hasattr", "(", "words", ",", "\"__len__\"", ")", "and", "# for numpy array", "\n", "not", "isinstance", "(", "words", ",", "collections", ".", "Iterable", ")", ")", ":", "\n", "    ", "words", "=", "[", "words", "]", "\n", "", "return", "b\" \"", ".", "join", "(", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.misc_utils.format_bpe_text": [[153, 168], ["isinstance", "len", "symbols.encode.encode", "words.append", "len"], "function", ["None"], ["", "def", "format_bpe_text", "(", "symbols", ",", "delimiter", "=", "b\"@@\"", ")", ":", "\n", "  ", "\"\"\"Convert a sequence of bpe words into sentence.\"\"\"", "\n", "words", "=", "[", "]", "\n", "word", "=", "b\"\"", "\n", "if", "isinstance", "(", "symbols", ",", "str", ")", ":", "\n", "    ", "symbols", "=", "symbols", ".", "encode", "(", ")", "\n", "", "delimiter_len", "=", "len", "(", "delimiter", ")", "\n", "for", "symbol", "in", "symbols", ":", "\n", "    ", "if", "len", "(", "symbol", ")", ">=", "delimiter_len", "and", "symbol", "[", "-", "delimiter_len", ":", "]", "==", "delimiter", ":", "\n", "      ", "word", "+=", "symbol", "[", ":", "-", "delimiter_len", "]", "\n", "", "else", ":", "# end of a word", "\n", "      ", "word", "+=", "symbol", "\n", "words", ".", "append", "(", "word", ")", "\n", "word", "=", "b\"\"", "\n", "", "", "return", "b\" \"", ".", "join", "(", "words", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_sample_decode": [[53, 64], ["train._sample_decode", "infer_model.graph.as_default", "model_helper.create_or_load_model"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._sample_decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_or_load_model"], ["def", "run_sample_decode", "(", "infer_model", ",", "infer_sess", ",", "model_dir", ",", "hparams", ",", "\n", "summary_writer", ",", "src_data", ",", "tgt_data", ")", ":", "\n", "  ", "\"\"\"Sample decode a random sentence from src_data.\"\"\"", "\n", "with", "infer_model", ".", "graph", ".", "as_default", "(", ")", ":", "\n", "    ", "loaded_infer_model", ",", "global_step", "=", "model_helper", ".", "create_or_load_model", "(", "\n", "infer_model", ".", "model", ",", "model_dir", ",", "infer_sess", ",", "\"infer\"", ")", "\n", "\n", "", "_sample_decode", "(", "loaded_infer_model", ",", "global_step", ",", "infer_sess", ",", "hparams", ",", "\n", "infer_model", ".", "iterator", ",", "src_data", ",", "tgt_data", ",", "\n", "infer_model", ".", "src_placeholder", ",", "\n", "infer_model", ".", "batch_size_placeholder", ",", "summary_writer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_internal_eval": [[66, 95], ["train._internal_eval", "eval_model.graph.as_default", "model_helper.create_or_load_model", "train._internal_eval"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._internal_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_or_load_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._internal_eval"], ["", "def", "run_internal_eval", "(", "\n", "eval_model", ",", "eval_sess", ",", "model_dir", ",", "hparams", ",", "summary_writer", ")", ":", "\n", "  ", "\"\"\"Compute internal evaluation (perplexity) for both dev / test.\"\"\"", "\n", "with", "eval_model", ".", "graph", ".", "as_default", "(", ")", ":", "\n", "    ", "loaded_eval_model", ",", "global_step", "=", "model_helper", ".", "create_or_load_model", "(", "\n", "eval_model", ".", "model", ",", "model_dir", ",", "eval_sess", ",", "\"eval\"", ")", "\n", "\n", "", "dev_src_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "dev_prefix", ",", "hparams", ".", "src", ")", "\n", "dev_tgt_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "dev_prefix", ",", "hparams", ".", "tgt", ")", "\n", "dev_eval_iterator_feed_dict", "=", "{", "\n", "eval_model", ".", "src_file_placeholder", ":", "dev_src_file", ",", "\n", "eval_model", ".", "tgt_file_placeholder", ":", "dev_tgt_file", "\n", "}", "\n", "\n", "dev_ppl", "=", "_internal_eval", "(", "loaded_eval_model", ",", "global_step", ",", "eval_sess", ",", "\n", "eval_model", ".", "iterator", ",", "dev_eval_iterator_feed_dict", ",", "\n", "summary_writer", ",", "\"dev\"", ")", "\n", "test_ppl", "=", "None", "\n", "if", "hparams", ".", "test_prefix", ":", "\n", "    ", "test_src_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "test_prefix", ",", "hparams", ".", "src", ")", "\n", "test_tgt_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "test_prefix", ",", "hparams", ".", "tgt", ")", "\n", "test_eval_iterator_feed_dict", "=", "{", "\n", "eval_model", ".", "src_file_placeholder", ":", "test_src_file", ",", "\n", "eval_model", ".", "tgt_file_placeholder", ":", "test_tgt_file", "\n", "}", "\n", "test_ppl", "=", "_internal_eval", "(", "loaded_eval_model", ",", "global_step", ",", "eval_sess", ",", "\n", "eval_model", ".", "iterator", ",", "test_eval_iterator_feed_dict", ",", "\n", "summary_writer", ",", "\"test\"", ")", "\n", "", "return", "dev_ppl", ",", "test_ppl", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_external_eval": [[97, 143], ["train._external_eval", "infer_model.graph.as_default", "model_helper.create_or_load_model", "inference.load_data", "train._external_eval", "inference.load_data"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._external_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_or_load_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.load_data", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._external_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.load_data"], ["", "def", "run_external_eval", "(", "infer_model", ",", "infer_sess", ",", "model_dir", ",", "hparams", ",", "\n", "summary_writer", ",", "save_best_dev", "=", "True", ")", ":", "\n", "\n", "  ", "\"\"\"Compute external evaluation (bleu, rouge, etc.) for both dev / test.\"\"\"", "\n", "with", "infer_model", ".", "graph", ".", "as_default", "(", ")", ":", "\n", "    ", "loaded_infer_model", ",", "global_step", "=", "model_helper", ".", "create_or_load_model", "(", "\n", "infer_model", ".", "model", ",", "model_dir", ",", "infer_sess", ",", "\"infer\"", ")", "\n", "\n", "", "dev_src_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "dev_prefix", ",", "hparams", ".", "src", ")", "\n", "dev_tgt_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "dev_prefix", ",", "hparams", ".", "tgt", ")", "\n", "dev_infer_iterator_feed_dict", "=", "{", "\n", "infer_model", ".", "src_placeholder", ":", "inference", ".", "load_data", "(", "dev_src_file", ")", ",", "\n", "infer_model", ".", "batch_size_placeholder", ":", "hparams", ".", "infer_batch_size", ",", "\n", "}", "\n", "dev_scores", "=", "_external_eval", "(", "\n", "loaded_infer_model", ",", "\n", "global_step", ",", "\n", "infer_sess", ",", "\n", "hparams", ",", "\n", "infer_model", ".", "iterator", ",", "\n", "dev_infer_iterator_feed_dict", ",", "\n", "dev_tgt_file", ",", "\n", "\"dev\"", ",", "\n", "summary_writer", ",", "\n", "save_on_best", "=", "save_best_dev", ")", "\n", "\n", "test_scores", "=", "None", "\n", "if", "hparams", ".", "test_prefix", ":", "\n", "    ", "test_src_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "test_prefix", ",", "hparams", ".", "src", ")", "\n", "test_tgt_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "test_prefix", ",", "hparams", ".", "tgt", ")", "\n", "test_infer_iterator_feed_dict", "=", "{", "\n", "infer_model", ".", "src_placeholder", ":", "inference", ".", "load_data", "(", "test_src_file", ")", ",", "\n", "infer_model", ".", "batch_size_placeholder", ":", "hparams", ".", "infer_batch_size", ",", "\n", "}", "\n", "test_scores", "=", "_external_eval", "(", "\n", "loaded_infer_model", ",", "\n", "global_step", ",", "\n", "infer_sess", ",", "\n", "hparams", ",", "\n", "infer_model", ".", "iterator", ",", "\n", "test_infer_iterator_feed_dict", ",", "\n", "test_tgt_file", ",", "\n", "\"test\"", ",", "\n", "summary_writer", ",", "\n", "save_on_best", "=", "False", ")", "\n", "", "return", "dev_scores", ",", "test_scores", ",", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_full_eval": [[145, 161], ["train.run_sample_decode", "train.run_internal_eval", "train.run_external_eval", "train._format_results", "train._format_results"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_sample_decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_internal_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_external_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._format_results", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._format_results"], ["", "def", "run_full_eval", "(", "model_dir", ",", "infer_model", ",", "infer_sess", ",", "eval_model", ",", "eval_sess", ",", "\n", "hparams", ",", "summary_writer", ",", "sample_src_data", ",", "sample_tgt_data", ")", ":", "\n", "  ", "\"\"\"Wrapper for running sample_decode, internal_eval and external_eval.\"\"\"", "\n", "run_sample_decode", "(", "infer_model", ",", "infer_sess", ",", "model_dir", ",", "hparams", ",", "summary_writer", ",", "\n", "sample_src_data", ",", "sample_tgt_data", ")", "\n", "dev_ppl", ",", "test_ppl", "=", "run_internal_eval", "(", "\n", "eval_model", ",", "eval_sess", ",", "model_dir", ",", "hparams", ",", "summary_writer", ")", "\n", "dev_scores", ",", "test_scores", ",", "global_step", "=", "run_external_eval", "(", "\n", "infer_model", ",", "infer_sess", ",", "model_dir", ",", "hparams", ",", "summary_writer", ")", "\n", "\n", "result_summary", "=", "_format_results", "(", "\"dev\"", ",", "dev_ppl", ",", "dev_scores", ",", "hparams", ".", "metrics", ")", "\n", "if", "hparams", ".", "test_prefix", ":", "\n", "    ", "result_summary", "+=", "\", \"", "+", "_format_results", "(", "\"test\"", ",", "test_ppl", ",", "test_scores", ",", "\n", "hparams", ".", "metrics", ")", "\n", "\n", "", "return", "result_summary", ",", "global_step", ",", "dev_scores", ",", "test_scores", ",", "dev_ppl", ",", "test_ppl", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.train": [[163, 468], ["model_helper.create_train_model", "model_helper.create_eval_model", "model_helper.create_infer_model", "inference.load_data", "inference.load_data", "os.path.join", "tensorflow.gfile.GFile", "misc_utils.print_out", "misc_utils.get_config_proto", "tensorflow.Session", "tensorflow.Session", "tensorflow.Session", "tensorflow.summary.FileWriter", "train.run_full_eval", "time.time", "misc_utils.print_out", "misc_utils.print_out", "misc_utils.print_out", "loaded_train_model.saver.save", "train.run_full_eval", "misc_utils.print_out", "misc_utils.print_time", "misc_utils.print_out", "tf.summary.FileWriter.close", "model_helper.create_train_model.graph.as_default", "model_helper.create_or_load_model", "os.path.join", "tf.Session.run", "range", "time.time", "tf.summary.FileWriter.add_summary", "float", "os.path.join", "getattr", "train.run_full_eval", "misc_utils.print_out", "time.time", "exp3S.Exp3S", "tf.Session.run", "tf.Session.run", "time.time", "misc_utils.safe_exp", "misc_utils.print_out", "math.isnan", "misc_utils.print_out", "misc_utils.add_summary", "loaded_train_model.saver.save", "train.run_sample_decode", "train.run_internal_eval", "train.run_external_eval", "ValueError", "loaded_train_model.learning_rate.eval", "time.ctime", "model_helper.create_train_model.iterator.initializer[].string_handle", "range", "loaded_train_model.train", "loaded_train_model.train", "misc_utils.print_out", "misc_utils.print_out", "os.path.join", "loaded_train_model.learning_rate.eval", "time.ctime", "exp3S.Exp3S.draw_task", "tf.Session.run", "exp3S.Exp3S.update_w", "tf.Session.run", "tf.Session.run", "time.ctime", "misc_utils.print_out", "loaded_train_model.learning_rate.eval", "train._get_best_results", "numpy.random.randint", "float", "numpy.random.randint", "numpy.random.random_sample", "float"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_train_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_eval_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_infer_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.load_data", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.inference.load_data", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.get_config_proto", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_full_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_full_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_time", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_or_load_model", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.add_summary", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_full_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.safe_exp", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.add_summary", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_sample_decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_internal_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train.run_external_eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.train", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.train", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.eval", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._get_best_results"], ["", "def", "train", "(", "hparams", ",", "scope", "=", "None", ",", "target_session", "=", "\"\"", ")", ":", "\n", "  ", "\"\"\"Train a translation model.\"\"\"", "\n", "log_device_placement", "=", "hparams", ".", "log_device_placement", "\n", "out_dir", "=", "hparams", ".", "out_dir", "\n", "num_train_steps", "=", "hparams", ".", "num_train_steps", "\n", "steps_per_stats", "=", "hparams", ".", "steps_per_stats", "\n", "steps_per_external_eval", "=", "hparams", ".", "steps_per_external_eval", "\n", "steps_per_eval", "=", "10", "*", "steps_per_stats", "\n", "if", "not", "steps_per_external_eval", ":", "\n", "    ", "steps_per_external_eval", "=", "5", "*", "steps_per_eval", "\n", "\n", "", "if", "not", "hparams", ".", "attention", ":", "\n", "    ", "model_creator", "=", "nmt_model", ".", "Model", "\n", "", "elif", "hparams", ".", "attention_architecture", "==", "\"standard\"", ":", "\n", "    ", "model_creator", "=", "attention_model", ".", "AttentionModel", "\n", "", "elif", "hparams", ".", "attention_architecture", "in", "[", "\"gnmt\"", ",", "\"gnmt_v2\"", "]", ":", "\n", "    ", "model_creator", "=", "gnmt_model", ".", "GNMTModel", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unknown model architecture\"", ")", "\n", "\n", "", "train_model", "=", "model_helper", ".", "create_train_model", "(", "model_creator", ",", "hparams", ",", "scope", ")", "\n", "eval_model", "=", "model_helper", ".", "create_eval_model", "(", "model_creator", ",", "hparams", ",", "scope", ")", "\n", "infer_model", "=", "model_helper", ".", "create_infer_model", "(", "model_creator", ",", "hparams", ",", "scope", ")", "\n", "\n", "# Preload data for sample decoding.", "\n", "dev_src_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "dev_prefix", ",", "hparams", ".", "src", ")", "\n", "dev_tgt_file", "=", "\"%s.%s\"", "%", "(", "hparams", ".", "dev_prefix", ",", "hparams", ".", "tgt", ")", "\n", "sample_src_data", "=", "inference", ".", "load_data", "(", "dev_src_file", ")", "\n", "sample_tgt_data", "=", "inference", ".", "load_data", "(", "dev_tgt_file", ")", "\n", "\n", "summary_name", "=", "\"train_log\"", "\n", "model_dir", "=", "hparams", ".", "out_dir", "\n", "\n", "# Log and output files", "\n", "log_file", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"log_%d\"", "%", "time", ".", "time", "(", ")", ")", "\n", "log_f", "=", "tf", ".", "gfile", ".", "GFile", "(", "log_file", ",", "mode", "=", "\"w\"", ")", "\n", "utils", ".", "print_out", "(", "\"# log_file=%s\"", "%", "log_file", ",", "log_f", ")", "\n", "\n", "avg_step_time", "=", "0.0", "\n", "\n", "# TensorFlow model", "\n", "config_proto", "=", "utils", ".", "get_config_proto", "(", "\n", "log_device_placement", "=", "log_device_placement", ")", "\n", "\n", "train_sess", "=", "tf", ".", "Session", "(", "\n", "target", "=", "target_session", ",", "config", "=", "config_proto", ",", "graph", "=", "train_model", ".", "graph", ")", "\n", "eval_sess", "=", "tf", ".", "Session", "(", "\n", "target", "=", "target_session", ",", "config", "=", "config_proto", ",", "graph", "=", "eval_model", ".", "graph", ")", "\n", "infer_sess", "=", "tf", ".", "Session", "(", "\n", "target", "=", "target_session", ",", "config", "=", "config_proto", ",", "graph", "=", "infer_model", ".", "graph", ")", "\n", "\n", "with", "train_model", ".", "graph", ".", "as_default", "(", ")", ":", "\n", "    ", "loaded_train_model", ",", "global_step", "=", "model_helper", ".", "create_or_load_model", "(", "\n", "train_model", ".", "model", ",", "model_dir", ",", "train_sess", ",", "\"train\"", ")", "\n", "\n", "# Summary writer", "\n", "", "summary_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "summary_name", ")", ",", "train_model", ".", "graph", ")", "\n", "\n", "# First evaluation", "\n", "run_full_eval", "(", "\n", "model_dir", ",", "infer_model", ",", "infer_sess", ",", "\n", "eval_model", ",", "eval_sess", ",", "hparams", ",", "\n", "summary_writer", ",", "sample_src_data", ",", "\n", "sample_tgt_data", ")", "\n", "\n", "last_stats_step", "=", "global_step", "\n", "last_eval_step", "=", "global_step", "\n", "last_external_eval_step", "=", "global_step", "\n", "\n", "# This is the training loop.", "\n", "step_time", ",", "checkpoint_loss", ",", "checkpoint_predict_count", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "checkpoint_total_count", "=", "0.0", "\n", "speed", ",", "train_ppl", "=", "0.0", ",", "0.0", "\n", "start_train_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "utils", ".", "print_out", "(", "\n", "\"# Start step %d, lr %g, %s\"", "%", "\n", "(", "global_step", ",", "loaded_train_model", ".", "learning_rate", ".", "eval", "(", "session", "=", "train_sess", ")", ",", "\n", "time", ".", "ctime", "(", ")", ")", ",", "\n", "log_f", ")", "\n", "\n", "# Initialize all of the iterators", "\n", "skip_count", "=", "hparams", ".", "batch_size", "*", "hparams", ".", "epoch_step", "\n", "utils", ".", "print_out", "(", "\"# Init train iterator, skipping %d elements\"", "%", "skip_count", ")", "\n", "\n", "if", "hparams", ".", "curriculum", "==", "'none'", ":", "\n", "    ", "train_sess", ".", "run", "(", "\n", "train_model", ".", "iterator", ".", "initializer", ",", "\n", "feed_dict", "=", "{", "\n", "train_model", ".", "skip_count_placeholder", ":", "skip_count", "\n", "}", ")", "\n", "", "else", ":", "\n", "    ", "if", "hparams", ".", "curriculum", "==", "'predictive_gain'", ":", "\n", "      ", "exp3s", "=", "Exp3S", "(", "hparams", ".", "num_curriculum_buckets", ",", "0.001", ",", "0", ",", "0.05", ")", "\n", "", "elif", "hparams", ".", "curriculum", "==", "'look_back_and_forward'", ":", "\n", "      ", "curriculum_point", "=", "0", "\n", "\n", "", "handle", "=", "train_model", ".", "iterator", ".", "handle", "\n", "for", "i", "in", "range", "(", "hparams", ".", "num_curriculum_buckets", ")", ":", "\n", "      ", "train_sess", ".", "run", "(", "\n", "train_model", ".", "iterator", ".", "initializer", "[", "i", "]", ".", "initializer", ",", "\n", "feed_dict", "=", "{", "\n", "train_model", ".", "skip_count_placeholder", ":", "skip_count", "\n", "}", ")", "\n", "\n", "", "iterator_handles", "=", "[", "train_sess", ".", "run", "(", "train_model", ".", "iterator", ".", "initializer", "[", "i", "]", ".", "string_handle", "(", ")", ",", "\n", "feed_dict", "=", "{", "\n", "train_model", ".", "skip_count_placeholder", ":", "skip_count", "\n", "}", ")", "\n", "for", "i", "in", "range", "(", "hparams", ".", "num_curriculum_buckets", ")", "]", "\n", "\n", "", "utils", ".", "print_out", "(", "\"Starting training\"", ")", "\n", "\n", "while", "global_step", "<", "num_train_steps", ":", "\n", "### Run a step ###", "\n", "    ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "try", ":", "\n", "      ", "if", "hparams", ".", "curriculum", "!=", "'none'", ":", "\n", "        ", "if", "hparams", ".", "curriculum", "==", "'predictive_gain'", ":", "\n", "          ", "lesson", "=", "exp3s", ".", "draw_task", "(", ")", "\n", "", "elif", "hparams", ".", "curriculum", "==", "'look_back_and_forward'", ":", "\n", "          ", "if", "curriculum_point", "==", "hparams", ".", "num_curriculum_buckets", ":", "\n", "            ", "lesson", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "hparams", ".", "num_curriculum_buckets", ")", "\n", "", "else", ":", "\n", "            ", "lesson", "=", "curriculum_point", "if", "np", ".", "random", ".", "random_sample", "(", ")", "<", "0.8", "else", "np", ".", "random", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "hparams", ".", "num_curriculum_buckets", ")", "\n", "\n", "", "", "step_result", "=", "loaded_train_model", ".", "train", "(", "hparams", ",", "train_sess", ",", "\n", "handle", "=", "handle", ",", "iterator_handle", "=", "iterator_handles", "[", "lesson", "]", ",", "\n", "use_fed_source_placeholder", "=", "loaded_train_model", ".", "use_fed_source", ",", "\n", "fed_source_placeholder", "=", "loaded_train_model", ".", "fed_source", ")", "\n", "\n", "(", "_", ",", "step_loss", ",", "step_predict_count", ",", "step_summary", ",", "global_step", ",", "\n", "step_word_count", ",", "batch_size", ",", "source", ")", "=", "step_result", "\n", "\n", "if", "hparams", ".", "curriculum", "==", "'predictive_gain'", ":", "\n", "          ", "new_loss", "=", "train_sess", ".", "run", "(", "[", "loaded_train_model", ".", "train_loss", "]", ",", "\n", "feed_dict", "=", "{", "\n", "handle", ":", "iterator_handles", "[", "lesson", "]", ",", "\n", "loaded_train_model", ".", "use_fed_source", ":", "True", ",", "\n", "loaded_train_model", ".", "fed_source", ":", "source", "\n", "}", ")", "\n", "\n", "# new_loss = loaded_train_model.train_loss.eval(", "\n", "#   session=train_sess,", "\n", "#   feed_dict={", "\n", "#     handle: iterator_handles[lesson],", "\n", "#     loaded_train_model.use_fed_source: True,", "\n", "#     loaded_train_model.fed_source: source", "\n", "#   })", "\n", "\n", "# utils.print_out(\"lesson: %s, step loss: %s, new_loss: %s\" % (lesson, step_loss, new_loss))", "\n", "# utils.print_out(\"exp3s dist: %s\" % (exp3s.pi, ))", "\n", "\n", "curriculum_point_a", "=", "lesson", "*", "(", "hparams", ".", "src_max_len", "//", "hparams", ".", "num_curriculum_buckets", ")", "+", "1", "\n", "curriculum_point_b", "=", "(", "lesson", "+", "1", ")", "*", "(", "hparams", ".", "src_max_len", "//", "hparams", ".", "num_curriculum_buckets", ")", "+", "1", "\n", "\n", "v", "=", "step_loss", "-", "new_loss", "\n", "exp3s", ".", "update_w", "(", "v", ",", "float", "(", "curriculum_point_a", "+", "curriculum_point_b", ")", "/", "2.0", ")", "\n", "", "elif", "hparams", ".", "curriculum", "==", "'look_back_and_forward'", ":", "\n", "          ", "utils", ".", "print_out", "(", "\"step loss: %s, lesson: %s\"", "%", "(", "step_loss", ",", "lesson", ")", ")", "\n", "curriculum_point_a", "=", "curriculum_point", "*", "(", "hparams", ".", "src_max_len", "//", "hparams", ".", "num_curriculum_buckets", ")", "+", "1", "\n", "curriculum_point_b", "=", "(", "curriculum_point", "+", "1", ")", "*", "(", "hparams", ".", "src_max_len", "//", "hparams", ".", "num_curriculum_buckets", ")", "+", "1", "\n", "\n", "if", "step_loss", "<", "(", "hparams", ".", "curriculum_progress_loss", "*", "(", "float", "(", "curriculum_point_a", "+", "curriculum_point_b", ")", "/", "2.0", ")", ")", ":", "\n", "            ", "curriculum_point", "+=", "1", "\n", "", "", "", "else", ":", "\n", "        ", "step_result", "=", "loaded_train_model", ".", "train", "(", "hparams", ",", "train_sess", ")", "\n", "(", "_", ",", "step_loss", ",", "step_predict_count", ",", "step_summary", ",", "global_step", ",", "\n", "step_word_count", ",", "batch_size", ")", "=", "step_result", "\n", "", "hparams", ".", "epoch_step", "+=", "1", "\n", "", "except", "tf", ".", "errors", ".", "OutOfRangeError", ":", "\n", "# Finished going through the training dataset.  Go to next epoch.", "\n", "      ", "hparams", ".", "epoch_step", "=", "0", "\n", "# utils.print_out(", "\n", "#     \"# Finished an epoch, step %d. Perform external evaluation\" %", "\n", "#     global_step)", "\n", "# run_sample_decode(infer_model, infer_sess,", "\n", "#                   model_dir, hparams, summary_writer, sample_src_data,", "\n", "#                   sample_tgt_data)", "\n", "# dev_scores, test_scores, _ = run_external_eval(", "\n", "#     infer_model, infer_sess, model_dir,", "\n", "#     hparams, summary_writer)", "\n", "if", "hparams", ".", "curriculum", "==", "'none'", ":", "\n", "        ", "train_sess", ".", "run", "(", "\n", "train_model", ".", "iterator", ".", "initializer", ",", "\n", "feed_dict", "=", "{", "\n", "train_model", ".", "skip_count_placeholder", ":", "0", "\n", "}", ")", "\n", "", "else", ":", "\n", "        ", "train_sess", ".", "run", "(", "\n", "train_model", ".", "iterator", ".", "initializer", "[", "lesson", "]", ".", "initializer", ",", "\n", "feed_dict", "=", "{", "\n", "train_model", ".", "skip_count_placeholder", ":", "0", "\n", "}", ")", "\n", "", "continue", "\n", "\n", "# Write step summary.", "\n", "", "summary_writer", ".", "add_summary", "(", "step_summary", ",", "global_step", ")", "\n", "\n", "# update statistics", "\n", "step_time", "+=", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "\n", "checkpoint_loss", "+=", "(", "step_loss", "*", "batch_size", ")", "\n", "checkpoint_predict_count", "+=", "step_predict_count", "\n", "checkpoint_total_count", "+=", "float", "(", "step_word_count", ")", "\n", "\n", "# Once in a while, we print statistics.", "\n", "if", "global_step", "-", "last_stats_step", ">=", "steps_per_stats", ":", "\n", "      ", "if", "hparams", ".", "curriculum", "==", "'predictive_gain'", ":", "\n", "        ", "utils", ".", "print_out", "(", "\"lesson: %s, step loss: %s, new_loss: %s\"", "%", "(", "lesson", ",", "step_loss", ",", "new_loss", ")", ")", "\n", "utils", ".", "print_out", "(", "\"exp3s dist: %s\"", "%", "(", "exp3s", ".", "pi", ",", ")", ")", "\n", "\n", "", "last_stats_step", "=", "global_step", "\n", "\n", "# Print statistics for the previous epoch.", "\n", "avg_step_time", "=", "step_time", "/", "steps_per_stats", "\n", "train_ppl", "=", "utils", ".", "safe_exp", "(", "checkpoint_loss", "/", "checkpoint_predict_count", ")", "\n", "speed", "=", "checkpoint_total_count", "/", "(", "1000", "*", "step_time", ")", "\n", "utils", ".", "print_out", "(", "\n", "\"  global step %d lr %g \"", "\n", "\"step-time %.2fs wps %.2fK ppl %.2f %s\"", "%", "\n", "(", "global_step", ",", "\n", "loaded_train_model", ".", "learning_rate", ".", "eval", "(", "session", "=", "train_sess", ")", ",", "\n", "avg_step_time", ",", "speed", ",", "train_ppl", ",", "_get_best_results", "(", "hparams", ")", ")", ",", "\n", "log_f", ")", "\n", "\n", "if", "math", ".", "isnan", "(", "train_ppl", ")", ":", "\n", "        ", "break", "\n", "\n", "# Reset timer and loss.", "\n", "", "step_time", ",", "checkpoint_loss", ",", "checkpoint_predict_count", "=", "0.0", ",", "0.0", ",", "0.0", "\n", "checkpoint_total_count", "=", "0.0", "\n", "\n", "", "if", "global_step", "-", "last_eval_step", ">=", "steps_per_eval", ":", "\n", "      ", "last_eval_step", "=", "global_step", "\n", "\n", "utils", ".", "print_out", "(", "\"# Save eval, global step %d\"", "%", "global_step", ")", "\n", "utils", ".", "add_summary", "(", "summary_writer", ",", "global_step", ",", "\"train_ppl\"", ",", "train_ppl", ")", "\n", "\n", "# Save checkpoint", "\n", "loaded_train_model", ".", "saver", ".", "save", "(", "\n", "train_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"translate.ckpt\"", ")", ",", "\n", "global_step", "=", "global_step", ")", "\n", "\n", "# Evaluate on dev/test", "\n", "run_sample_decode", "(", "infer_model", ",", "infer_sess", ",", "\n", "model_dir", ",", "hparams", ",", "summary_writer", ",", "sample_src_data", ",", "\n", "sample_tgt_data", ")", "\n", "dev_ppl", ",", "test_ppl", "=", "run_internal_eval", "(", "\n", "eval_model", ",", "eval_sess", ",", "model_dir", ",", "hparams", ",", "summary_writer", ")", "\n", "\n", "dev_scores", ",", "test_scores", ",", "_", "=", "run_external_eval", "(", "\n", "infer_model", ",", "infer_sess", ",", "model_dir", ",", "\n", "hparams", ",", "summary_writer", ")", "\n", "\n", "# if global_step - last_external_eval_step >= steps_per_external_eval:", "\n", "#   last_external_eval_step = global_step", "\n", "\n", "#   # Save checkpoint", "\n", "#   loaded_train_model.saver.save(", "\n", "#       train_sess,", "\n", "#       os.path.join(out_dir, \"translate.ckpt\"),", "\n", "#       global_step=global_step)", "\n", "#   run_sample_decode(infer_model, infer_sess,", "\n", "#                     model_dir, hparams, summary_writer, sample_src_data,", "\n", "#                     sample_tgt_data)", "\n", "#   dev_scores, test_scores, _ = run_external_eval(", "\n", "#       infer_model, infer_sess, model_dir,", "\n", "#       hparams, summary_writer)", "\n", "\n", "# Done training", "\n", "", "", "loaded_train_model", ".", "saver", ".", "save", "(", "\n", "train_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"translate.ckpt\"", ")", ",", "\n", "global_step", "=", "global_step", ")", "\n", "\n", "result_summary", ",", "_", ",", "dev_scores", ",", "test_scores", ",", "dev_ppl", ",", "test_ppl", "=", "run_full_eval", "(", "\n", "model_dir", ",", "infer_model", ",", "infer_sess", ",", "\n", "eval_model", ",", "eval_sess", ",", "hparams", ",", "\n", "summary_writer", ",", "sample_src_data", ",", "\n", "sample_tgt_data", ")", "\n", "\n", "utils", ".", "print_out", "(", "\n", "\"# Final, step %d lr %g \"", "\n", "\"step-time %.2f wps %.2fK ppl %.2f, %s, %s\"", "%", "\n", "(", "global_step", ",", "loaded_train_model", ".", "learning_rate", ".", "eval", "(", "session", "=", "train_sess", ")", ",", "\n", "avg_step_time", ",", "speed", ",", "train_ppl", ",", "result_summary", ",", "time", ".", "ctime", "(", ")", ")", ",", "\n", "log_f", ")", "\n", "utils", ".", "print_time", "(", "\"# Done training!\"", ",", "start_train_time", ")", "\n", "\n", "utils", ".", "print_out", "(", "\"# Start evaluating saved best models.\"", ")", "\n", "for", "metric", "in", "hparams", ".", "metrics", ":", "\n", "    ", "best_model_dir", "=", "getattr", "(", "hparams", ",", "\"best_\"", "+", "metric", "+", "\"_dir\"", ")", "\n", "result_summary", ",", "best_global_step", ",", "_", ",", "_", ",", "_", ",", "_", "=", "run_full_eval", "(", "\n", "best_model_dir", ",", "infer_model", ",", "infer_sess", ",", "eval_model", ",", "eval_sess", ",", "hparams", ",", "\n", "summary_writer", ",", "sample_src_data", ",", "sample_tgt_data", ")", "\n", "utils", ".", "print_out", "(", "\"# Best %s, step %d \"", "\n", "\"step-time %.2f wps %.2fK, %s, %s\"", "%", "\n", "(", "metric", ",", "best_global_step", ",", "avg_step_time", ",", "speed", ",", "\n", "result_summary", ",", "time", ".", "ctime", "(", ")", ")", ",", "log_f", ")", "\n", "\n", "", "summary_writer", ".", "close", "(", ")", "\n", "return", "(", "dev_scores", ",", "test_scores", ",", "dev_ppl", ",", "test_ppl", ",", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._format_results": [[470, 477], ["None"], "function", ["None"], ["", "def", "_format_results", "(", "name", ",", "ppl", ",", "scores", ",", "metrics", ")", ":", "\n", "  ", "\"\"\"Format results.\"\"\"", "\n", "result_str", "=", "\"%s ppl %.2f\"", "%", "(", "name", ",", "ppl", ")", "\n", "if", "scores", ":", "\n", "    ", "for", "metric", "in", "metrics", ":", "\n", "      ", "result_str", "+=", "\", %s %s %.1f\"", "%", "(", "name", ",", "metric", ",", "scores", "[", "metric", "]", ")", "\n", "", "", "return", "result_str", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._get_best_results": [[479, 485], ["tokens.append", "getattr"], "function", ["None"], ["", "def", "_get_best_results", "(", "hparams", ")", ":", "\n", "  ", "\"\"\"Summary of the current best results.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "metric", "in", "hparams", ".", "metrics", ":", "\n", "    ", "tokens", ".", "append", "(", "\"%s %.2f\"", "%", "(", "metric", ",", "getattr", "(", "hparams", ",", "\"best_\"", "+", "metric", ")", ")", ")", "\n", "", "return", "\", \"", ".", "join", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._internal_eval": [[487, 494], ["sess.run", "model_helper.compute_perplexity", "misc_utils.add_summary"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.compute_perplexity", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.add_summary"], ["", "def", "_internal_eval", "(", "model", ",", "global_step", ",", "sess", ",", "iterator", ",", "iterator_feed_dict", ",", "\n", "summary_writer", ",", "label", ")", ":", "\n", "  ", "\"\"\"Computing perplexity.\"\"\"", "\n", "sess", ".", "run", "(", "iterator", ".", "initializer", ",", "feed_dict", "=", "iterator_feed_dict", ")", "\n", "ppl", "=", "model_helper", ".", "compute_perplexity", "(", "model", ",", "sess", ",", "label", ")", "\n", "utils", ".", "add_summary", "(", "summary_writer", ",", "global_step", ",", "\"%s_ppl\"", "%", "label", ",", "ppl", ")", "\n", "return", "ppl", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._sample_decode": [[496, 541], ["sess.run", "model.decode", "numpy.asarray", "range", "nmt_utils.get_translation", "outputs.append", "misc_utils.print_out", "misc_utils.print_out", "misc_utils.print_out", "enumerate", "open", "len", "pickle.dump"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.get_translation", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "_sample_decode", "(", "model", ",", "global_step", ",", "sess", ",", "hparams", ",", "iterator", ",", "src_data", ",", "\n", "tgt_data", ",", "iterator_src_placeholder", ",", "\n", "iterator_batch_size_placeholder", ",", "summary_writer", ")", ":", "\n", "  ", "\"\"\"Pick a sentence and decode.\"\"\"", "\n", "iterator_feed_dict", "=", "{", "\n", "iterator_src_placeholder", ":", "src_data", "[", "-", "hparams", ".", "infer_batch_size", ":", "]", ",", "\n", "iterator_batch_size_placeholder", ":", "hparams", ".", "infer_batch_size", ",", "\n", "}", "\n", "sess", ".", "run", "(", "iterator", ".", "initializer", ",", "feed_dict", "=", "iterator_feed_dict", ")", "\n", "\n", "nmt_outputs", ",", "att_w_history", ",", "ext_w_history", "=", "model", ".", "decode", "(", "sess", ")", "\n", "\n", "if", "hparams", ".", "beam_width", ">", "0", ":", "\n", "# get the top translation.", "\n", "    ", "nmt_outputs", "=", "nmt_outputs", "[", "0", "]", "\n", "\n", "", "nmt_outputs", "=", "np", ".", "asarray", "(", "nmt_outputs", ")", "\n", "\n", "outputs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "hparams", ".", "infer_batch_size", ")", ":", "\n", "    ", "tmp", "=", "{", "}", "\n", "translation", "=", "nmt_utils", ".", "get_translation", "(", "\n", "nmt_outputs", ",", "\n", "sent_id", "=", "i", ",", "\n", "tgt_sos", "=", "hparams", ".", "sos", ",", "\n", "tgt_eos", "=", "hparams", ".", "eos", ",", "\n", "bpe_delimiter", "=", "hparams", ".", "bpe_delimiter", ")", "\n", "if", "i", "<=", "5", ":", "\n", "      ", "utils", ".", "print_out", "(", "\"    src: %s\"", "%", "src_data", "[", "-", "hparams", ".", "infer_batch_size", "+", "i", "]", ")", "\n", "utils", ".", "print_out", "(", "\"    ref: %s\"", "%", "tgt_data", "[", "-", "hparams", ".", "infer_batch_size", "+", "i", "]", ")", "\n", "utils", ".", "print_out", "(", "b\"    nmt: %s\"", "%", "translation", ")", "\n", "", "tmp", "[", "'src'", "]", "=", "src_data", "[", "-", "hparams", ".", "infer_batch_size", "+", "i", "]", "\n", "tmp", "[", "'ref'", "]", "=", "tgt_data", "[", "-", "hparams", ".", "infer_batch_size", "+", "i", "]", "\n", "tmp", "[", "'nmt'", "]", "=", "translation", "\n", "if", "att_w_history", "is", "not", "None", ":", "\n", "      ", "tmp", "[", "'attention_head'", "]", "=", "att_w_history", "[", "-", "hparams", ".", "infer_batch_size", "+", "i", "]", "\n", "", "if", "ext_w_history", "is", "not", "None", ":", "\n", "      ", "for", "j", ",", "ext_head", "in", "enumerate", "(", "ext_w_history", ")", ":", "\n", "        ", "tmp", "[", "'ext_head_{0}'", ".", "format", "(", "j", ")", "]", "=", "ext_head", "[", "-", "hparams", ".", "infer_batch_size", "+", "i", "]", "\n", "", "", "outputs", ".", "append", "(", "tmp", ")", "\n", "\n", "", "if", "hparams", ".", "record_w_history", ":", "\n", "    ", "with", "open", "(", "hparams", ".", "out_dir", "+", "'/heads_step_{0}.pickle'", ".", "format", "(", "global_step", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "      ", "if", "len", "(", "outputs", ")", ">", "0", ":", "\n", "        ", "pickle", ".", "dump", "(", "outputs", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.train._external_eval": [[544, 583], ["sess.run", "os.path.join", "nmt_utils.decode_and_evaluate", "misc_utils.print_out", "misc_utils.save_hparams", "misc_utils.add_summary", "setattr", "model.saver.save", "getattr", "os.path.join", "getattr"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.decode_and_evaluate", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.save_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.add_summary"], ["", "", "", "", "def", "_external_eval", "(", "model", ",", "global_step", ",", "sess", ",", "hparams", ",", "iterator", ",", "\n", "iterator_feed_dict", ",", "tgt_file", ",", "label", ",", "summary_writer", ",", "\n", "save_on_best", ")", ":", "\n", "  ", "\"\"\"External evaluation such as BLEU and ROUGE scores.\"\"\"", "\n", "out_dir", "=", "hparams", ".", "out_dir", "\n", "decode", "=", "global_step", ">", "0", "\n", "if", "decode", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"# External evaluation, global step %d\"", "%", "global_step", ")", "\n", "\n", "", "sess", ".", "run", "(", "iterator", ".", "initializer", ",", "feed_dict", "=", "iterator_feed_dict", ")", "\n", "\n", "output", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"output_%s\"", "%", "label", ")", "\n", "scores", "=", "nmt_utils", ".", "decode_and_evaluate", "(", "\n", "label", ",", "\n", "model", ",", "\n", "sess", ",", "\n", "output", ",", "\n", "ref_file", "=", "tgt_file", ",", "\n", "metrics", "=", "hparams", ".", "metrics", ",", "\n", "bpe_delimiter", "=", "hparams", ".", "bpe_delimiter", ",", "\n", "beam_width", "=", "hparams", ".", "beam_width", ",", "\n", "tgt_sos", "=", "hparams", ".", "sos", ",", "\n", "tgt_eos", "=", "hparams", ".", "eos", ",", "\n", "decode", "=", "decode", ")", "\n", "# Save on best metrics", "\n", "if", "decode", ":", "\n", "    ", "for", "metric", "in", "hparams", ".", "metrics", ":", "\n", "      ", "utils", ".", "add_summary", "(", "summary_writer", ",", "global_step", ",", "\"%s_%s\"", "%", "(", "label", ",", "metric", ")", ",", "\n", "scores", "[", "metric", "]", ")", "\n", "# metric: larger is better", "\n", "if", "save_on_best", "and", "scores", "[", "metric", "]", ">", "getattr", "(", "hparams", ",", "\"best_\"", "+", "metric", ")", ":", "\n", "        ", "setattr", "(", "hparams", ",", "\"best_\"", "+", "metric", ",", "scores", "[", "metric", "]", ")", "\n", "model", ".", "saver", ".", "save", "(", "\n", "sess", ",", "\n", "os", ".", "path", ".", "join", "(", "\n", "getattr", "(", "hparams", ",", "\"best_\"", "+", "metric", "+", "\"_dir\"", ")", ",", "\"translate.ckpt\"", ")", ",", "\n", "global_step", "=", "model", ".", "global_step", ")", "\n", "", "", "utils", ".", "save_hparams", "(", "out_dir", ",", "hparams", ")", "\n", "", "return", "scores", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge._get_ngrams": [[19, 35], ["set", "len", "range", "set.add", "tuple"], "function", ["None"], ["def", "_get_ngrams", "(", "n", ",", "text", ")", ":", "\n", "  ", "\"\"\"Calcualtes n-grams.\n\n  Args:\n    n: which n-grams to calculate\n    text: An array of tokens\n\n  Returns:\n    A set of n-grams\n  \"\"\"", "\n", "ngram_set", "=", "set", "(", ")", "\n", "text_length", "=", "len", "(", "text", ")", "\n", "max_index_ngram_start", "=", "text_length", "-", "n", "\n", "for", "i", "in", "range", "(", "max_index_ngram_start", "+", "1", ")", ":", "\n", "    ", "ngram_set", ".", "add", "(", "tuple", "(", "text", "[", "i", ":", "i", "+", "n", "]", ")", ")", "\n", "", "return", "ngram_set", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge._split_into_words": [[37, 40], ["list", "itertools.chain", "_.split"], "function", ["None"], ["", "def", "_split_into_words", "(", "sentences", ")", ":", "\n", "  ", "\"\"\"Splits multiple sentences into words and flattens the result\"\"\"", "\n", "return", "list", "(", "itertools", ".", "chain", "(", "*", "[", "_", ".", "split", "(", "\" \"", ")", "for", "_", "in", "sentences", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge._get_word_ngrams": [[42, 50], ["rouge._split_into_words", "rouge._get_ngrams", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_ngrams"], ["", "def", "_get_word_ngrams", "(", "n", ",", "sentences", ")", ":", "\n", "  ", "\"\"\"Calculates word n-grams for multiple sentences.\n  \"\"\"", "\n", "assert", "len", "(", "sentences", ")", ">", "0", "\n", "assert", "n", ">", "0", "\n", "\n", "words", "=", "_split_into_words", "(", "sentences", ")", "\n", "return", "_get_ngrams", "(", "n", ",", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge._len_lcs": [[52, 68], ["rouge._lcs", "len", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._lcs"], ["", "def", "_len_lcs", "(", "x", ",", "y", ")", ":", "\n", "  ", "\"\"\"\n  Returns the length of the Longest Common Subsequence between sequences x\n  and y.\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns\n    integer: Length of LCS between x and y\n  \"\"\"", "\n", "table", "=", "_lcs", "(", "x", ",", "y", ")", "\n", "n", ",", "m", "=", "len", "(", "x", ")", ",", "len", "(", "y", ")", "\n", "return", "table", "[", "n", ",", "m", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge._lcs": [[70, 95], ["dict", "range", "len", "len", "range", "max"], "function", ["None"], ["", "def", "_lcs", "(", "x", ",", "y", ")", ":", "\n", "  ", "\"\"\"\n  Computes the length of the longest common subsequence (lcs) between two\n  strings. The implementation below uses a DP programming algorithm and runs\n  in O(nm) time where n = len(x) and m = len(y).\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: collection of words\n    y: collection of words\n\n  Returns:\n    Table of dictionary of coord and len lcs\n  \"\"\"", "\n", "n", ",", "m", "=", "len", "(", "x", ")", ",", "len", "(", "y", ")", "\n", "table", "=", "dict", "(", ")", "\n", "for", "i", "in", "range", "(", "n", "+", "1", ")", ":", "\n", "    ", "for", "j", "in", "range", "(", "m", "+", "1", ")", ":", "\n", "      ", "if", "i", "==", "0", "or", "j", "==", "0", ":", "\n", "        ", "table", "[", "i", ",", "j", "]", "=", "0", "\n", "", "elif", "x", "[", "i", "-", "1", "]", "==", "y", "[", "j", "-", "1", "]", ":", "\n", "        ", "table", "[", "i", ",", "j", "]", "=", "table", "[", "i", "-", "1", ",", "j", "-", "1", "]", "+", "1", "\n", "", "else", ":", "\n", "        ", "table", "[", "i", ",", "j", "]", "=", "max", "(", "table", "[", "i", "-", "1", ",", "j", "]", ",", "table", "[", "i", ",", "j", "-", "1", "]", ")", "\n", "", "", "", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge._recon_lcs": [[97, 125], ["rouge._lcs", "tuple", "len", "len", "map", "rouge._recon_lcs._recon"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._lcs"], ["", "def", "_recon_lcs", "(", "x", ",", "y", ")", ":", "\n", "  ", "\"\"\"\n  Returns the Longest Subsequence between x and y.\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns:\n    sequence: LCS of x and y\n  \"\"\"", "\n", "i", ",", "j", "=", "len", "(", "x", ")", ",", "len", "(", "y", ")", "\n", "table", "=", "_lcs", "(", "x", ",", "y", ")", "\n", "\n", "def", "_recon", "(", "i", ",", "j", ")", ":", "\n", "    ", "\"\"\"private recon calculation\"\"\"", "\n", "if", "i", "==", "0", "or", "j", "==", "0", ":", "\n", "      ", "return", "[", "]", "\n", "", "elif", "x", "[", "i", "-", "1", "]", "==", "y", "[", "j", "-", "1", "]", ":", "\n", "      ", "return", "_recon", "(", "i", "-", "1", ",", "j", "-", "1", ")", "+", "[", "(", "x", "[", "i", "-", "1", "]", ",", "i", ")", "]", "\n", "", "elif", "table", "[", "i", "-", "1", ",", "j", "]", ">", "table", "[", "i", ",", "j", "-", "1", "]", ":", "\n", "      ", "return", "_recon", "(", "i", "-", "1", ",", "j", ")", "\n", "", "else", ":", "\n", "      ", "return", "_recon", "(", "i", ",", "j", "-", "1", ")", "\n", "\n", "", "", "recon_tuple", "=", "tuple", "(", "map", "(", "lambda", "x", ":", "x", "[", "0", "]", ",", "_recon", "(", "i", ",", "j", ")", ")", ")", "\n", "return", "recon_tuple", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge.rouge_n": [[127, 171], ["rouge._get_word_ngrams", "rouge._get_word_ngrams", "len", "len", "_get_word_ngrams.intersection", "len", "ValueError", "len", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_word_ngrams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_word_ngrams"], ["", "def", "rouge_n", "(", "evaluated_sentences", ",", "reference_sentences", ",", "n", "=", "2", ")", ":", "\n", "  ", "\"\"\"\n  Computes ROUGE-N of two text collections of sentences.\n  Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n  papers/rouge-working-note-v1.3.1.pdf\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentences: The sentences from the referene set\n    n: Size of ngram.  Defaults to 2.\n\n  Returns:\n    A tuple (f1, precision, recall) for ROUGE-N\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", "or", "len", "(", "reference_sentences", ")", "<=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "\n", "", "evaluated_ngrams", "=", "_get_word_ngrams", "(", "n", ",", "evaluated_sentences", ")", "\n", "reference_ngrams", "=", "_get_word_ngrams", "(", "n", ",", "reference_sentences", ")", "\n", "reference_count", "=", "len", "(", "reference_ngrams", ")", "\n", "evaluated_count", "=", "len", "(", "evaluated_ngrams", ")", "\n", "\n", "# Gets the overlapping ngrams between evaluated and reference", "\n", "overlapping_ngrams", "=", "evaluated_ngrams", ".", "intersection", "(", "reference_ngrams", ")", "\n", "overlapping_count", "=", "len", "(", "overlapping_ngrams", ")", "\n", "\n", "# Handle edge case. This isn't mathematically correct, but it's good enough", "\n", "if", "evaluated_count", "==", "0", ":", "\n", "    ", "precision", "=", "0.0", "\n", "", "else", ":", "\n", "    ", "precision", "=", "overlapping_count", "/", "evaluated_count", "\n", "\n", "", "if", "reference_count", "==", "0", ":", "\n", "    ", "recall", "=", "0.0", "\n", "", "else", ":", "\n", "    ", "recall", "=", "overlapping_count", "/", "reference_count", "\n", "\n", "", "f1_score", "=", "2.0", "*", "(", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", "+", "1e-8", ")", ")", "\n", "\n", "# return overlapping_count / reference_count", "\n", "return", "f1_score", ",", "precision", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge._f_p_r_lcs": [[173, 194], ["None"], "function", ["None"], ["", "def", "_f_p_r_lcs", "(", "llcs", ",", "m", ",", "n", ")", ":", "\n", "  ", "\"\"\"\n  Computes the LCS-based F-measure score\n  Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Args:\n    llcs: Length of LCS\n    m: number of words in reference summary\n    n: number of words in candidate summary\n\n  Returns:\n    Float. LCS-based F-measure score\n  \"\"\"", "\n", "r_lcs", "=", "llcs", "/", "m", "\n", "p_lcs", "=", "llcs", "/", "n", "\n", "beta", "=", "p_lcs", "/", "(", "r_lcs", "+", "1e-12", ")", "\n", "num", "=", "(", "1", "+", "(", "beta", "**", "2", ")", ")", "*", "r_lcs", "*", "p_lcs", "\n", "denom", "=", "r_lcs", "+", "(", "(", "beta", "**", "2", ")", "*", "p_lcs", ")", "\n", "f_lcs", "=", "num", "/", "(", "denom", "+", "1e-12", ")", "\n", "return", "f_lcs", ",", "p_lcs", ",", "r_lcs", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge.rouge_l_sentence_level": [[196, 231], ["rouge._split_into_words", "rouge._split_into_words", "len", "len", "rouge._len_lcs", "rouge._f_p_r_lcs", "ValueError", "len", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._len_lcs", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._f_p_r_lcs"], ["", "def", "rouge_l_sentence_level", "(", "evaluated_sentences", ",", "reference_sentences", ")", ":", "\n", "  ", "\"\"\"\n  Computes ROUGE-L (sentence level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Calculated according to:\n  R_lcs = LCS(X,Y)/m\n  P_lcs = LCS(X,Y)/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n  where:\n  X = reference summary\n  Y = Candidate summary\n  m = length of reference summary\n  n = length of candidate summary\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentences: The sentences from the referene set\n\n  Returns:\n    A float: F_lcs\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", "or", "len", "(", "reference_sentences", ")", "<=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "", "reference_words", "=", "_split_into_words", "(", "reference_sentences", ")", "\n", "evaluated_words", "=", "_split_into_words", "(", "evaluated_sentences", ")", "\n", "m", "=", "len", "(", "reference_words", ")", "\n", "n", "=", "len", "(", "evaluated_words", ")", "\n", "lcs", "=", "_len_lcs", "(", "evaluated_words", ",", "reference_words", ")", "\n", "return", "_f_p_r_lcs", "(", "lcs", ",", "m", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge._union_lcs": [[233, 268], ["set", "rouge._split_into_words", "len", "len", "ValueError", "rouge._split_into_words", "set", "len", "lcs_union.union.union", "rouge._recon_lcs"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._recon_lcs"], ["", "def", "_union_lcs", "(", "evaluated_sentences", ",", "reference_sentence", ")", ":", "\n", "  ", "\"\"\"\n  Returns LCS_u(r_i, C) which is the LCS score of the union longest common\n  subsequence between reference sentence ri and candidate summary C. For example\n  if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and\n  c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is\n  \"w1 w2\" and the longest common subsequence of r_i and c2 is \"w1 w3 w5\". The\n  union longest common subsequence of r_i, c1, and c2 is \"w1 w2 w3 w5\" and\n  LCS_u(r_i, C) = 4/5.\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentence: One of the sentences in the reference summaries\n\n  Returns:\n    float: LCS_u(r_i, C)\n\n  ValueError:\n    Raises exception if a param has len <= 0\n  \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "\n", "", "lcs_union", "=", "set", "(", ")", "\n", "reference_words", "=", "_split_into_words", "(", "[", "reference_sentence", "]", ")", "\n", "combined_lcs_length", "=", "0", "\n", "for", "eval_s", "in", "evaluated_sentences", ":", "\n", "    ", "evaluated_words", "=", "_split_into_words", "(", "[", "eval_s", "]", ")", "\n", "lcs", "=", "set", "(", "_recon_lcs", "(", "reference_words", ",", "evaluated_words", ")", ")", "\n", "combined_lcs_length", "+=", "len", "(", "lcs", ")", "\n", "lcs_union", "=", "lcs_union", ".", "union", "(", "lcs", ")", "\n", "\n", "", "union_lcs_count", "=", "len", "(", "lcs_union", ")", "\n", "union_lcs_value", "=", "union_lcs_count", "/", "combined_lcs_length", "\n", "return", "union_lcs_value", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge.rouge_l_summary_level": [[270, 312], ["len", "len", "rouge._f_p_r_lcs", "ValueError", "rouge._split_into_words", "rouge._split_into_words", "rouge._union_lcs", "len", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._f_p_r_lcs", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._union_lcs"], ["", "def", "rouge_l_summary_level", "(", "evaluated_sentences", ",", "reference_sentences", ")", ":", "\n", "  ", "\"\"\"\n  Computes ROUGE-L (summary level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Calculated according to:\n  R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m\n  P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n  where:\n  SUM(i,u) = SUM from i through u\n  u = number of sentences in reference summary\n  C = Candidate summary made up of v sentences\n  m = number of words in reference summary\n  n = number of words in candidate summary\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentence: One of the sentences in the reference summaries\n\n  Returns:\n    A float: F_lcs\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", "or", "len", "(", "reference_sentences", ")", "<=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "\n", "# total number of words in reference sentences", "\n", "", "m", "=", "len", "(", "_split_into_words", "(", "reference_sentences", ")", ")", "\n", "\n", "# total number of words in evaluated sentences", "\n", "n", "=", "len", "(", "_split_into_words", "(", "evaluated_sentences", ")", ")", "\n", "\n", "union_lcs_sum_across_all_references", "=", "0", "\n", "for", "ref_s", "in", "reference_sentences", ":", "\n", "    ", "union_lcs_sum_across_all_references", "+=", "_union_lcs", "(", "evaluated_sentences", ",", "\n", "ref_s", ")", "\n", "", "return", "_f_p_r_lcs", "(", "union_lcs_sum_across_all_references", ",", "m", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.rouge.rouge": [[314, 352], ["map", "map", "map", "rouge.rouge_n", "zip", "rouge.rouge_n", "zip", "rouge.rouge_l_sentence_level", "zip", "zip", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge_n", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge_n", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge_l_sentence_level"], ["", "def", "rouge", "(", "hypotheses", ",", "references", ")", ":", "\n", "  ", "\"\"\"Calculates average rouge scores for a list of hypotheses and\n  references\"\"\"", "\n", "\n", "# Filter out hyps that are of 0 length", "\n", "# hyps_and_refs = zip(hypotheses, references)", "\n", "# hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]", "\n", "# hypotheses, references = zip(*hyps_and_refs)", "\n", "\n", "# Calculate ROUGE-1 F1, precision, recall scores", "\n", "rouge_1", "=", "[", "\n", "rouge_n", "(", "[", "hyp", "]", ",", "[", "ref", "]", ",", "1", ")", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "\n", "]", "\n", "rouge_1_f", ",", "rouge_1_p", ",", "rouge_1_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_1", ")", ")", "\n", "\n", "# Calculate ROUGE-2 F1, precision, recall scores", "\n", "rouge_2", "=", "[", "\n", "rouge_n", "(", "[", "hyp", "]", ",", "[", "ref", "]", ",", "2", ")", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "\n", "]", "\n", "rouge_2_f", ",", "rouge_2_p", ",", "rouge_2_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_2", ")", ")", "\n", "\n", "# Calculate ROUGE-L F1, precision, recall scores", "\n", "rouge_l", "=", "[", "\n", "rouge_l_sentence_level", "(", "[", "hyp", "]", ",", "[", "ref", "]", ")", "\n", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "\n", "]", "\n", "rouge_l_f", ",", "rouge_l_p", ",", "rouge_l_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_l", ")", ")", "\n", "\n", "return", "{", "\n", "\"rouge_1/f_score\"", ":", "rouge_1_f", ",", "\n", "\"rouge_1/r_score\"", ":", "rouge_1_r", ",", "\n", "\"rouge_1/p_score\"", ":", "rouge_1_p", ",", "\n", "\"rouge_2/f_score\"", ":", "rouge_2_f", ",", "\n", "\"rouge_2/r_score\"", ":", "rouge_2_r", ",", "\n", "\"rouge_2/p_score\"", ":", "rouge_2_p", ",", "\n", "\"rouge_l/f_score\"", ":", "rouge_l_f", ",", "\n", "\"rouge_l/r_score\"", ":", "rouge_l_r", ",", "\n", "\"rouge_l/p_score\"", ":", "rouge_l_p", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess.__init__": [[87, 115], ["sonnet.RNNCore.__init__", "addressing.CosineWeights", "addressing.CosineWeights", "addressing.TemporalLinkage", "addressing.Freeness"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__"], ["def", "__init__", "(", "self", ",", "\n", "memory_size", "=", "128", ",", "\n", "word_size", "=", "20", ",", "\n", "num_reads", "=", "1", ",", "\n", "num_writes", "=", "1", ",", "\n", "name", "=", "'memory_access'", ")", ":", "\n", "    ", "\"\"\"Creates a MemoryAccess module.\n\n    Args:\n      memory_size: The number of memory slots (N in the DNC paper).\n      word_size: The width of each memory slot (W in the DNC paper)\n      num_reads: The number of read heads (R in the DNC paper).\n      num_writes: The number of write heads (fixed at 1 in the paper).\n      name: The name of the module.\n    \"\"\"", "\n", "super", "(", "MemoryAccess", ",", "self", ")", ".", "__init__", "(", "name", "=", "name", ")", "\n", "self", ".", "_memory_size", "=", "memory_size", "\n", "self", ".", "_word_size", "=", "word_size", "\n", "self", ".", "_num_reads", "=", "num_reads", "\n", "self", ".", "_num_writes", "=", "num_writes", "\n", "\n", "self", ".", "_write_content_weights_mod", "=", "addressing", ".", "CosineWeights", "(", "\n", "num_writes", ",", "word_size", ",", "name", "=", "'write_content_weights'", ")", "\n", "self", ".", "_read_content_weights_mod", "=", "addressing", ".", "CosineWeights", "(", "\n", "num_reads", ",", "word_size", ",", "name", "=", "'read_content_weights'", ")", "\n", "\n", "self", ".", "_linkage", "=", "addressing", ".", "TemporalLinkage", "(", "memory_size", ",", "num_writes", ")", "\n", "self", ".", "_freeness", "=", "addressing", ".", "Freeness", "(", "memory_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess._build": [[116, 162], ["access.MemoryAccess._read_inputs", "access.MemoryAccess._freeness", "access.MemoryAccess._write_weights", "access._erase_and_write", "access.MemoryAccess._linkage", "access.MemoryAccess._read_weights", "tensorflow.matmul", "AccessState"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess._read_inputs", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess._write_weights", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access._erase_and_write", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess._read_weights"], ["", "def", "_build", "(", "self", ",", "inputs", ",", "prev_state", ")", ":", "\n", "    ", "\"\"\"Connects the MemoryAccess module into the graph.\n\n    Args:\n      inputs: tensor of shape `[batch_size, input_size]`. This is used to\n          control this access module.\n      prev_state: Instance of `AccessState` containing the previous state.\n\n    Returns:\n      A tuple `(output, next_state)`, where `output` is a tensor of shape\n      `[batch_size, num_reads, word_size]`, and `next_state` is the new\n      `AccessState` named tuple at the current time t.\n    \"\"\"", "\n", "inputs", "=", "self", ".", "_read_inputs", "(", "inputs", ")", "\n", "\n", "# Update usage using inputs['free_gate'] and previous read & write weights.", "\n", "usage", "=", "self", ".", "_freeness", "(", "\n", "write_weights", "=", "prev_state", ".", "write_weights", ",", "\n", "free_gate", "=", "inputs", ".", "free_gate", ",", "\n", "read_weights", "=", "prev_state", ".", "read_weights", ",", "\n", "prev_usage", "=", "prev_state", ".", "usage", ")", "\n", "\n", "# Write to memory.", "\n", "write_weights", "=", "self", ".", "_write_weights", "(", "inputs", ",", "prev_state", ".", "memory", ",", "usage", ")", "\n", "memory", "=", "_erase_and_write", "(", "\n", "prev_state", ".", "memory", ",", "\n", "address", "=", "write_weights", ",", "\n", "reset_weights", "=", "inputs", ".", "erase_vectors", ",", "\n", "values", "=", "inputs", ".", "write_vectors", ")", "\n", "\n", "linkage_state", "=", "self", ".", "_linkage", "(", "write_weights", ",", "prev_state", ".", "linkage", ")", "\n", "\n", "# Read from memory.", "\n", "read_weights", "=", "self", ".", "_read_weights", "(", "\n", "inputs", ",", "\n", "memory", "=", "memory", ",", "\n", "prev_read_weights", "=", "prev_state", ".", "read_weights", ",", "\n", "link", "=", "linkage_state", ".", "link", ")", "\n", "read_words", "=", "tf", ".", "matmul", "(", "read_weights", ",", "memory", ")", "\n", "\n", "return", "(", "read_words", ",", "AccessState", "(", "\n", "memory", "=", "memory", ",", "\n", "read_weights", "=", "read_weights", ",", "\n", "write_weights", "=", "write_weights", ",", "\n", "linkage", "=", "linkage_state", ",", "\n", "usage", "=", "usage", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess._read_inputs": [[163, 224], ["access.MemoryAccess._read_inputs._linear"], "methods", ["None"], ["", "def", "_read_inputs", "(", "self", ",", "inputs", ")", ":", "\n", "    ", "\"\"\"Applies transformations to `inputs` to get control for this module.\"\"\"", "\n", "\n", "def", "_linear", "(", "first_dim", ",", "second_dim", ",", "name", ",", "activation", "=", "None", ")", ":", "\n", "      ", "\"\"\"Returns a linear transformation of `inputs`, followed by a reshape.\"\"\"", "\n", "linear", "=", "snt", ".", "Linear", "(", "first_dim", "*", "second_dim", ",", "name", "=", "name", ")", "(", "inputs", ")", "\n", "if", "activation", "is", "not", "None", ":", "\n", "        ", "linear", "=", "activation", "(", "linear", ",", "name", "=", "name", "+", "'_activation'", ")", "\n", "", "return", "tf", ".", "reshape", "(", "linear", ",", "[", "-", "1", ",", "first_dim", ",", "second_dim", "]", ")", "\n", "\n", "# v_t^i - The vectors to write to memory, for each write head `i`.", "\n", "", "write_vectors", "=", "_linear", "(", "self", ".", "_num_writes", ",", "self", ".", "_word_size", ",", "'write_vectors'", ")", "\n", "\n", "# e_t^i - Amount to erase the memory by before writing, for each write head.", "\n", "erase_vectors", "=", "_linear", "(", "self", ".", "_num_writes", ",", "self", ".", "_word_size", ",", "'erase_vectors'", ",", "\n", "tf", ".", "sigmoid", ")", "\n", "\n", "# f_t^j - Amount that the memory at the locations read from at the previous", "\n", "# time step can be declared unused, for each read head `j`.", "\n", "free_gate", "=", "tf", ".", "sigmoid", "(", "\n", "snt", ".", "Linear", "(", "self", ".", "_num_reads", ",", "name", "=", "'free_gate'", ")", "(", "inputs", ")", ")", "\n", "\n", "# g_t^{a, i} - Interpolation between writing to unallocated memory and", "\n", "# content-based lookup, for each write head `i`. Note: `a` is simply used to", "\n", "# identify this gate with allocation vs writing (as defined below).", "\n", "allocation_gate", "=", "tf", ".", "sigmoid", "(", "\n", "snt", ".", "Linear", "(", "self", ".", "_num_writes", ",", "name", "=", "'allocation_gate'", ")", "(", "inputs", ")", ")", "\n", "\n", "# g_t^{w, i} - Overall gating of write amount for each write head.", "\n", "write_gate", "=", "tf", ".", "sigmoid", "(", "\n", "snt", ".", "Linear", "(", "self", ".", "_num_writes", ",", "name", "=", "'write_gate'", ")", "(", "inputs", ")", ")", "\n", "\n", "# \\pi_t^j - Mixing between \"backwards\" and \"forwards\" positions (for", "\n", "# each write head), and content-based lookup, for each read head.", "\n", "num_read_modes", "=", "1", "+", "2", "*", "self", ".", "_num_writes", "\n", "# read_mode = snt.BatchApply(tf.nn.softmax)(", "\n", "#     _linear(self._num_reads, num_read_modes, name='read_mode'))", "\n", "read_mode", "=", "tf", ".", "nn", ".", "softmax", "(", "\n", "_linear", "(", "self", ".", "_num_reads", ",", "num_read_modes", ",", "name", "=", "'read_mode'", ")", ")", "\n", "\n", "# Parameters for the (read / write) \"weights by content matching\" modules.", "\n", "write_keys", "=", "_linear", "(", "self", ".", "_num_writes", ",", "self", ".", "_word_size", ",", "'write_keys'", ")", "\n", "write_strengths", "=", "snt", ".", "Linear", "(", "self", ".", "_num_writes", ",", "name", "=", "'write_strengths'", ")", "(", "\n", "inputs", ")", "\n", "\n", "read_keys", "=", "_linear", "(", "self", ".", "_num_reads", ",", "self", ".", "_word_size", ",", "'read_keys'", ")", "\n", "read_strengths", "=", "snt", ".", "Linear", "(", "self", ".", "_num_reads", ",", "name", "=", "'read_strengths'", ")", "(", "inputs", ")", "\n", "\n", "result", "=", "Result", "(", "\n", "read_content_keys", "=", "read_keys", ",", "\n", "read_content_strengths", "=", "read_strengths", ",", "\n", "write_content_keys", "=", "write_keys", ",", "\n", "write_content_strengths", "=", "write_strengths", ",", "\n", "write_vectors", "=", "write_vectors", ",", "\n", "erase_vectors", "=", "erase_vectors", ",", "\n", "free_gate", "=", "free_gate", ",", "\n", "allocation_gate", "=", "allocation_gate", ",", "\n", "write_gate", "=", "write_gate", ",", "\n", "read_mode", "=", "read_mode", "\n", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess._write_weights": [[225, 263], ["tensorflow.name_scope", "access.MemoryAccess._write_content_weights_mod", "access.MemoryAccess._freeness.write_allocation_weights", "tensorflow.expand_dims", "tensorflow.expand_dims"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness.write_allocation_weights"], ["", "def", "_write_weights", "(", "self", ",", "inputs", ",", "memory", ",", "usage", ")", ":", "\n", "    ", "\"\"\"Calculates the memory locations to write to.\n\n    This uses a combination of content-based lookup and finding an unused\n    location in memory, for each write head.\n\n    Args:\n      inputs: Collection of inputs to the access module, including controls for\n          how to chose memory writing, such as the content to look-up and the\n          weighting between content-based and allocation-based addressing.\n      memory: A tensor of shape  `[batch_size, memory_size, word_size]`\n          containing the current memory contents.\n      usage: Current memory usage, which is a tensor of shape `[batch_size,\n          memory_size]`, used for allocation-based addressing.\n\n    Returns:\n      tensor of shape `[batch_size, num_writes, memory_size]` indicating where\n          to write to (if anywhere) for each write head.\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'write_weights'", ",", "values", "=", "[", "inputs", ",", "memory", ",", "usage", "]", ")", ":", "\n", "# c_t^{w, i} - The content-based weights for each write head.", "\n", "      ", "write_content_weights", "=", "self", ".", "_write_content_weights_mod", "(", "\n", "memory", ",", "inputs", ".", "write_content_keys", ",", "\n", "inputs", ".", "write_content_strengths", ")", "\n", "\n", "# a_t^i - The allocation weights for each write head.", "\n", "write_allocation_weights", "=", "self", ".", "_freeness", ".", "write_allocation_weights", "(", "\n", "usage", "=", "usage", ",", "\n", "write_gates", "=", "(", "inputs", ".", "allocation_gate", "*", "inputs", ".", "write_gate", ")", ",", "\n", "num_writes", "=", "self", ".", "_num_writes", ")", "\n", "\n", "# Expands gates over memory locations.", "\n", "allocation_gate", "=", "tf", ".", "expand_dims", "(", "inputs", ".", "allocation_gate", ",", "-", "1", ")", "\n", "write_gate", "=", "tf", ".", "expand_dims", "(", "inputs", ".", "write_gate", ",", "-", "1", ")", "\n", "\n", "# w_t^{w, i} - The write weightings for each write head.", "\n", "return", "write_gate", "*", "(", "allocation_gate", "*", "write_allocation_weights", "+", "\n", "(", "1", "-", "allocation_gate", ")", "*", "write_content_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess._read_weights": [[264, 309], ["tensorflow.name_scope", "access.MemoryAccess._read_content_weights_mod", "access.MemoryAccess._linkage.directional_read_weights", "access.MemoryAccess._linkage.directional_read_weights", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage.directional_read_weights", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage.directional_read_weights"], ["", "", "def", "_read_weights", "(", "self", ",", "inputs", ",", "memory", ",", "prev_read_weights", ",", "link", ")", ":", "\n", "    ", "\"\"\"Calculates read weights for each read head.\n\n    The read weights are a combination of following the link graphs in the\n    forward or backward directions from the previous read position, and doing\n    content-based lookup. The interpolation between these different modes is\n    done by `inputs['read_mode']`.\n\n    Args:\n      inputs: Controls for this access module. This contains the content-based\n          keys to lookup, and the weightings for the different read modes.\n      memory: A tensor of shape `[batch_size, memory_size, word_size]`\n          containing the current memory contents to do content-based lookup.\n      prev_read_weights: A tensor of shape `[batch_size, num_reads,\n          memory_size]` containing the previous read locations.\n      link: A tensor of shape `[batch_size, num_writes, memory_size,\n          memory_size]` containing the temporal write transition graphs.\n\n    Returns:\n      A tensor of shape `[batch_size, num_reads, memory_size]` containing the\n      read weights for each read head.\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "\n", "'read_weights'", ",", "values", "=", "[", "inputs", ",", "memory", ",", "prev_read_weights", ",", "link", "]", ")", ":", "\n", "# c_t^{r, i} - The content weightings for each read head.", "\n", "      ", "content_weights", "=", "self", ".", "_read_content_weights_mod", "(", "\n", "memory", ",", "inputs", ".", "read_content_keys", ",", "inputs", ".", "read_content_strengths", ")", "\n", "\n", "# Calculates f_t^i and b_t^i.", "\n", "forward_weights", "=", "self", ".", "_linkage", ".", "directional_read_weights", "(", "\n", "link", ",", "prev_read_weights", ",", "forward", "=", "True", ")", "\n", "backward_weights", "=", "self", ".", "_linkage", ".", "directional_read_weights", "(", "\n", "link", ",", "prev_read_weights", ",", "forward", "=", "False", ")", "\n", "\n", "backward_mode", "=", "inputs", ".", "read_mode", "[", ":", ",", ":", ",", ":", "self", ".", "_num_writes", "]", "\n", "forward_mode", "=", "(", "\n", "inputs", ".", "read_mode", "[", ":", ",", ":", ",", "self", ".", "_num_writes", ":", "2", "*", "self", ".", "_num_writes", "]", ")", "\n", "content_mode", "=", "inputs", ".", "read_mode", "[", ":", ",", ":", ",", "2", "*", "self", ".", "_num_writes", "]", "\n", "\n", "read_weights", "=", "(", "\n", "tf", ".", "expand_dims", "(", "content_mode", ",", "2", ")", "*", "content_weights", "+", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "expand_dims", "(", "forward_mode", ",", "3", ")", "*", "forward_weights", ",", "2", ")", "+", "\n", "tf", ".", "reduce_sum", "(", "tf", ".", "expand_dims", "(", "backward_mode", ",", "3", ")", "*", "backward_weights", ",", "2", ")", ")", "\n", "\n", "return", "read_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess.state_size": [[310, 319], ["AccessState", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns a tuple of the shape of the state tensors.\"\"\"", "\n", "return", "AccessState", "(", "\n", "memory", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "_memory_size", ",", "self", ".", "_word_size", "]", ")", ",", "\n", "read_weights", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "_num_reads", ",", "self", ".", "_memory_size", "]", ")", ",", "\n", "write_weights", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "_num_writes", ",", "self", ".", "_memory_size", "]", ")", ",", "\n", "linkage", "=", "self", ".", "_linkage", ".", "state_size", ",", "\n", "usage", "=", "self", ".", "_freeness", ".", "state_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access.MemoryAccess.output_size": [[320, 324], ["tensorflow.TensorShape"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns the output shape.\"\"\"", "\n", "return", "tf", ".", "TensorShape", "(", "[", "self", ".", "_num_reads", ",", "self", ".", "_word_size", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.access._erase_and_write": [[35, 67], ["tensorflow.name_scope", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.reduce_prod", "tensorflow.name_scope", "tensorflow.matmul"], "function", ["None"], ["def", "_erase_and_write", "(", "memory", ",", "address", ",", "reset_weights", ",", "values", ")", ":", "\n", "  ", "\"\"\"Module to erase and write in the external memory.\n\n  Erase operation:\n    M_t'(i) = M_{t-1}(i) * (1 - w_t(i) * e_t)\n\n  Add operation:\n    M_t(i) = M_t'(i) + w_t(i) * a_t\n\n  where e are the reset_weights, w the write weights and a the values.\n\n  Args:\n    memory: 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n    address: 3-D tensor `[batch_size, num_writes, memory_size]`.\n    reset_weights: 3-D tensor `[batch_size, num_writes, word_size]`.\n    values: 3-D tensor `[batch_size, num_writes, word_size]`.\n\n  Returns:\n    3-D tensor of shape `[batch_size, num_writes, word_size]`.\n  \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'erase_memory'", ",", "values", "=", "[", "memory", ",", "address", ",", "reset_weights", "]", ")", ":", "\n", "    ", "expand_address", "=", "tf", ".", "expand_dims", "(", "address", ",", "3", ")", "\n", "reset_weights", "=", "tf", ".", "expand_dims", "(", "reset_weights", ",", "2", ")", "\n", "weighted_resets", "=", "expand_address", "*", "reset_weights", "\n", "reset_gate", "=", "tf", ".", "reduce_prod", "(", "1", "-", "weighted_resets", ",", "[", "1", "]", ")", "\n", "memory", "*=", "reset_gate", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "'additive_write'", ",", "values", "=", "[", "memory", ",", "address", ",", "values", "]", ")", ":", "\n", "    ", "add_matrix", "=", "tf", ".", "matmul", "(", "address", ",", "values", ",", "adjoint_a", "=", "True", ")", "\n", "memory", "+=", "add_matrix", "\n", "\n", "", "return", "memory", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt_utils.decode_and_evaluate": [[30, 87], ["misc_utils.print_out", "time.time", "tensorflow.gfile.Exists", "trans_f.write", "evaluation_utils.evaluate", "misc_utils.print_out", "codecs.getwriter", "tensorflow.gfile.GFile", "model.decode", "len", "range", "len", "nmt_utils.get_translation", "trans_f.write", "misc_utils.print_time"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils.evaluate", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.get_translation", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_time"], ["def", "decode_and_evaluate", "(", "name", ",", "\n", "model", ",", "\n", "sess", ",", "\n", "trans_file", ",", "\n", "ref_file", ",", "\n", "metrics", ",", "\n", "bpe_delimiter", ",", "\n", "beam_width", ",", "\n", "tgt_sos", ",", "\n", "tgt_eos", ",", "\n", "decode", "=", "True", ")", ":", "\n", "  ", "\"\"\"Decode a test set and compute a score according to the evaluation task.\"\"\"", "\n", "# Decode", "\n", "if", "decode", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"  decoding to output %s.\"", "%", "trans_file", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "num_sentences", "=", "0", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "trans_file", ",", "mode", "=", "\"wb\"", ")", ")", "as", "trans_f", ":", "\n", "      ", "trans_f", ".", "write", "(", "\"\"", ")", "# Write empty string to ensure file is created.", "\n", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "          ", "nmt_outputs", ",", "_", ",", "_", "=", "model", ".", "decode", "(", "sess", ")", "\n", "\n", "if", "beam_width", ">", "0", ":", "\n", "# get the top translation.", "\n", "            ", "nmt_outputs", "=", "nmt_outputs", "[", "0", "]", "\n", "\n", "", "num_sentences", "+=", "len", "(", "nmt_outputs", ")", "\n", "for", "sent_id", "in", "range", "(", "len", "(", "nmt_outputs", ")", ")", ":", "\n", "            ", "translation", "=", "get_translation", "(", "\n", "nmt_outputs", ",", "\n", "sent_id", ",", "\n", "tgt_sos", "=", "tgt_sos", ",", "\n", "tgt_eos", "=", "tgt_eos", ",", "\n", "bpe_delimiter", "=", "bpe_delimiter", ")", "\n", "trans_f", ".", "write", "(", "(", "translation", "+", "b\"\\n\"", ")", ".", "decode", "(", "\"utf-8\"", ")", ")", "\n", "", "", "except", "tf", ".", "errors", ".", "OutOfRangeError", ":", "\n", "          ", "utils", ".", "print_time", "(", "\"  done, num sentences %d\"", "%", "num_sentences", ",", "\n", "start_time", ")", "\n", "break", "\n", "\n", "# Evaluation", "\n", "", "", "", "", "evaluation_scores", "=", "{", "}", "\n", "if", "ref_file", "and", "tf", ".", "gfile", ".", "Exists", "(", "trans_file", ")", ":", "\n", "    ", "for", "metric", "in", "metrics", ":", "\n", "      ", "score", "=", "evaluation_utils", ".", "evaluate", "(", "\n", "ref_file", ",", "\n", "trans_file", ",", "\n", "metric", ",", "\n", "bpe_delimiter", "=", "bpe_delimiter", ")", "\n", "evaluation_scores", "[", "metric", "]", "=", "score", "\n", "utils", ".", "print_out", "(", "\"  %s %s: %.1f\"", "%", "(", "metric", ",", "name", ",", "score", ")", ")", "\n", "\n", "", "", "return", "evaluation_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt_utils.get_translation": [[89, 110], ["nmt_outputs[].tolist", "tgt_sos.encode.encode", "tgt_eos.encode.encode", "bpe_delimiter.encode.encode", "misc_utils.format_text", "misc_utils.format_bpe_text", "nmt_outputs[].tolist.index"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.format_text", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.format_bpe_text"], ["", "def", "get_translation", "(", "nmt_outputs", ",", "sent_id", ",", "tgt_sos", ",", "tgt_eos", ",", "bpe_delimiter", ")", ":", "\n", "  ", "\"\"\"Given batch decoding outputs, select a sentence and turn to text.\"\"\"", "\n", "if", "tgt_sos", ":", "tgt_sos", "=", "tgt_sos", ".", "encode", "(", "\"utf-8\"", ")", "\n", "if", "tgt_eos", ":", "tgt_eos", "=", "tgt_eos", ".", "encode", "(", "\"utf-8\"", ")", "\n", "if", "bpe_delimiter", ":", "bpe_delimiter", "=", "bpe_delimiter", ".", "encode", "(", "\"utf-8\"", ")", "\n", "# Select a sentence", "\n", "output", "=", "nmt_outputs", "[", "sent_id", ",", ":", "]", ".", "tolist", "(", ")", "\n", "\n", "if", "tgt_sos", "and", "output", "[", "0", "]", "==", "tgt_sos", ":", "\n", "    ", "output", "=", "output", "[", "1", ":", "]", "\n", "\n", "# If there is an eos symbol in outputs, cut them at that point.", "\n", "", "if", "tgt_eos", "and", "tgt_eos", "in", "output", ":", "\n", "    ", "output", "=", "output", "[", ":", "output", ".", "index", "(", "tgt_eos", ")", "]", "\n", "\n", "", "if", "not", "bpe_delimiter", ":", "\n", "    ", "translation", "=", "utils", ".", "format_text", "(", "output", ")", "\n", "", "else", ":", "# BPE", "\n", "    ", "translation", "=", "utils", ".", "format_bpe_text", "(", "output", ",", "delimiter", "=", "bpe_delimiter", ")", "\n", "\n", "", "return", "translation", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.CosineWeights.__init__": [[66, 83], ["sonnet.AbstractModule.__init__"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_heads", ",", "\n", "word_size", ",", "\n", "strength_op", "=", "tf", ".", "nn", ".", "softplus", ",", "\n", "name", "=", "'cosine_weights'", ")", ":", "\n", "    ", "\"\"\"Initializes the CosineWeights module.\n\n    Args:\n      num_heads: number of memory heads.\n      word_size: memory word size.\n      strength_op: operation to apply to strengths (default is tf.nn.softplus).\n      name: module name (default 'cosine_weights')\n    \"\"\"", "\n", "super", "(", "CosineWeights", ",", "self", ")", ".", "__init__", "(", "name", "=", "name", ")", "\n", "self", ".", "_num_heads", "=", "num_heads", "\n", "self", ".", "_word_size", "=", "word_size", "\n", "self", ".", "_strength_op", "=", "strength_op", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.CosineWeights._build": [[84, 107], ["tensorflow.matmul", "addressing._vector_norms", "addressing._vector_norms", "tensorflow.matmul", "addressing.weighted_softmax"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing._vector_norms", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing._vector_norms", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.weighted_softmax"], ["", "def", "_build", "(", "self", ",", "memory", ",", "keys", ",", "strengths", ")", ":", "\n", "    ", "\"\"\"Connects the CosineWeights module into the graph.\n\n    Args:\n      memory: A 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n      keys: A 3-D tensor of shape `[batch_size, num_heads, word_size]`.\n      strengths: A 2-D tensor of shape `[batch_size, num_heads]`.\n\n    Returns:\n      Weights tensor of shape `[batch_size, num_heads, memory_size]`.\n    \"\"\"", "\n", "# Calculates the inner product between the query vector and words in memory.", "\n", "dot", "=", "tf", ".", "matmul", "(", "keys", ",", "memory", ",", "adjoint_b", "=", "True", ")", "\n", "\n", "# Outer product to compute denominator (euclidean norm of query and memory).", "\n", "memory_norms", "=", "_vector_norms", "(", "memory", ")", "\n", "key_norms", "=", "_vector_norms", "(", "keys", ")", "\n", "norm", "=", "tf", ".", "matmul", "(", "key_norms", ",", "memory_norms", ",", "adjoint_b", "=", "True", ")", "\n", "\n", "# Calculates cosine similarity between the query vector and words in memory.", "\n", "similarity", "=", "dot", "/", "(", "norm", "+", "_EPSILON", ")", "\n", "\n", "return", "weighted_softmax", "(", "similarity", ",", "strengths", ",", "self", ".", "_strength_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage.__init__": [[122, 133], ["sonnet.RNNCore.__init__"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__"], ["def", "__init__", "(", "self", ",", "memory_size", ",", "num_writes", ",", "name", "=", "'temporal_linkage'", ")", ":", "\n", "    ", "\"\"\"Construct a TemporalLinkage module.\n\n    Args:\n      memory_size: The number of memory slots.\n      num_writes: The number of write heads.\n      name: Name of the module.\n    \"\"\"", "\n", "super", "(", "TemporalLinkage", ",", "self", ")", ".", "__init__", "(", "name", "=", "name", ")", "\n", "self", ".", "_memory_size", "=", "memory_size", "\n", "self", ".", "_num_writes", "=", "num_writes", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage._build": [[134, 155], ["addressing.TemporalLinkage._link", "addressing.TemporalLinkage._precedence_weights", "TemporalLinkageState"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage._link", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage._precedence_weights"], ["", "def", "_build", "(", "self", ",", "write_weights", ",", "prev_state", ")", ":", "\n", "    ", "\"\"\"Calculate the updated linkage state given the write weights.\n\n    Args:\n      write_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n          containing the memory addresses of the different write heads.\n      prev_state: `TemporalLinkageState` tuple containg a tensor `link` of\n          shape `[batch_size, num_writes, memory_size, memory_size]`, and a\n          tensor `precedence_weights` of shape `[batch_size, num_writes,\n          memory_size]` containing the aggregated history of recent writes.\n\n    Returns:\n      A `TemporalLinkageState` tuple `next_state`, which contains the updated\n      link and precedence weights.\n    \"\"\"", "\n", "link", "=", "self", ".", "_link", "(", "prev_state", ".", "link", ",", "prev_state", ".", "precedence_weights", ",", "\n", "write_weights", ")", "\n", "precedence_weights", "=", "self", ".", "_precedence_weights", "(", "prev_state", ".", "precedence_weights", ",", "\n", "write_weights", ")", "\n", "return", "TemporalLinkageState", "(", "\n", "link", "=", "link", ",", "precedence_weights", "=", "precedence_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage.directional_read_weights": [[156, 183], ["tensorflow.name_scope", "tensorflow.stack", "tensorflow.matmul", "tensorflow.transpose"], "methods", ["None"], ["", "def", "directional_read_weights", "(", "self", ",", "link", ",", "prev_read_weights", ",", "forward", ")", ":", "\n", "    ", "\"\"\"Calculates the forward or the backward read weights.\n\n    For each read head (at a given address), there are `num_writes` link graphs\n    to follow. Thus this function computes a read address for each of the\n    `num_reads * num_writes` pairs of read and write heads.\n\n    Args:\n      link: tensor of shape `[batch_size, num_writes, memory_size,\n          memory_size]` representing the link graphs L_t.\n      prev_read_weights: tensor of shape `[batch_size, num_reads,\n          memory_size]` containing the previous read weights w_{t-1}^r.\n      forward: Boolean indicating whether to follow the \"future\" direction in\n          the link graph (True) or the \"past\" direction (False).\n\n    Returns:\n      tensor of shape `[batch_size, num_reads, num_writes, memory_size]`\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'directional_read_weights'", ")", ":", "\n", "# We calculate the forward and backward directions for each pair of", "\n", "# read and write heads; hence we need to tile the read weights and do a", "\n", "# sort of \"outer product\" to get this.", "\n", "      ", "expanded_read_weights", "=", "tf", ".", "stack", "(", "[", "prev_read_weights", "]", "*", "self", ".", "_num_writes", ",", "\n", "1", ")", "\n", "result", "=", "tf", ".", "matmul", "(", "expanded_read_weights", ",", "link", ",", "adjoint_b", "=", "forward", ")", "\n", "# Swap dimensions 1, 2 so order is [batch, reads, writes, memory]:", "\n", "return", "tf", ".", "transpose", "(", "result", ",", "perm", "=", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage._link": [[184, 220], ["tensorflow.name_scope", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.matrix_set_diag", "tensorflow.zeros", "prev_link.get_shape"], "methods", ["None"], ["", "", "def", "_link", "(", "self", ",", "prev_link", ",", "prev_precedence_weights", ",", "write_weights", ")", ":", "\n", "    ", "\"\"\"Calculates the new link graphs.\n\n    For each write head, the link is a directed graph (represented by a matrix\n    with entries in range [0, 1]) whose vertices are the memory locations, and\n    an edge indicates temporal ordering of writes.\n\n    Args:\n      prev_link: A tensor of shape `[batch_size, num_writes, memory_size,\n          memory_size]` representing the previous link graphs for each write\n          head.\n      prev_precedence_weights: A tensor of shape `[batch_size, num_writes,\n          memory_size]` which is the previous \"aggregated\" write weights for\n          each write head.\n      write_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n          containing the new locations in memory written to.\n\n    Returns:\n      A tensor of shape `[batch_size, num_writes, memory_size, memory_size]`\n      containing the new link graphs for each write head.\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'link'", ")", ":", "\n", "      ", "batch_size", "=", "prev_link", ".", "get_shape", "(", ")", "[", "0", "]", ".", "value", "\n", "write_weights_i", "=", "tf", ".", "expand_dims", "(", "write_weights", ",", "3", ")", "\n", "write_weights_j", "=", "tf", ".", "expand_dims", "(", "write_weights", ",", "2", ")", "\n", "prev_precedence_weights_j", "=", "tf", ".", "expand_dims", "(", "prev_precedence_weights", ",", "2", ")", "\n", "prev_link_scale", "=", "1", "-", "write_weights_i", "-", "write_weights_j", "\n", "new_link", "=", "write_weights_i", "*", "prev_precedence_weights_j", "\n", "link", "=", "prev_link_scale", "*", "prev_link", "+", "new_link", "\n", "# Return the link with the diagonal set to zero, to remove self-looping", "\n", "# edges.", "\n", "return", "tf", ".", "matrix_set_diag", "(", "\n", "link", ",", "\n", "tf", ".", "zeros", "(", "\n", "[", "batch_size", ",", "self", ".", "_num_writes", ",", "self", ".", "_memory_size", "]", ",", "\n", "dtype", "=", "link", ".", "dtype", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage._precedence_weights": [[221, 242], ["tensorflow.name_scope", "tensorflow.reduce_sum"], "methods", ["None"], ["", "", "def", "_precedence_weights", "(", "self", ",", "prev_precedence_weights", ",", "write_weights", ")", ":", "\n", "    ", "\"\"\"Calculates the new precedence weights given the current write weights.\n\n    The precedence weights are the \"aggregated write weights\" for each write\n    head, where write weights with sum close to zero will leave the precedence\n    weights unchanged, but with sum close to one will replace the precedence\n    weights.\n\n    Args:\n      prev_precedence_weights: A tensor of shape `[batch_size, num_writes,\n          memory_size]` containing the previous precedence weights.\n      write_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n          containing the new write weights.\n\n    Returns:\n      A tensor of shape `[batch_size, num_writes, memory_size]` containing the\n      new precedence weights.\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'precedence_weights'", ")", ":", "\n", "      ", "write_sum", "=", "tf", ".", "reduce_sum", "(", "write_weights", ",", "2", ",", "keep_dims", "=", "True", ")", "\n", "return", "(", "1", "-", "write_sum", ")", "*", "prev_precedence_weights", "+", "write_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.TemporalLinkage.state_size": [[243, 251], ["TemporalLinkageState", "tensorflow.TensorShape", "tensorflow.TensorShape"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns a `TemporalLinkageState` tuple of the state tensors' shapes.\"\"\"", "\n", "return", "TemporalLinkageState", "(", "\n", "link", "=", "tf", ".", "TensorShape", "(", "\n", "[", "self", ".", "_num_writes", ",", "self", ".", "_memory_size", ",", "self", ".", "_memory_size", "]", ")", ",", "\n", "precedence_weights", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "_num_writes", ",", "\n", "self", ".", "_memory_size", "]", ")", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness.__init__": [[270, 279], ["sonnet.RNNCore.__init__"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__"], ["def", "__init__", "(", "self", ",", "memory_size", ",", "name", "=", "'freeness'", ")", ":", "\n", "    ", "\"\"\"Creates a Freeness module.\n\n    Args:\n      memory_size: Number of memory slots.\n      name: Name of the module.\n    \"\"\"", "\n", "super", "(", "Freeness", ",", "self", ")", ".", "__init__", "(", "name", "=", "name", ")", "\n", "self", ".", "_memory_size", "=", "memory_size", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness._build": [[280, 307], ["tensorflow.stop_gradient", "addressing.Freeness._usage_after_write", "addressing.Freeness._usage_after_read"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness._usage_after_write", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness._usage_after_read"], ["", "def", "_build", "(", "self", ",", "write_weights", ",", "free_gate", ",", "read_weights", ",", "prev_usage", ")", ":", "\n", "    ", "\"\"\"Calculates the new memory usage u_t.\n\n    Memory that was written to in the previous time step will have its usage\n    increased; memory that was read from and the controller says can be \"freed\"\n    will have its usage decreased.\n\n    Args:\n      write_weights: tensor of shape `[batch_size, num_writes,\n          memory_size]` giving write weights at previous time step.\n      free_gate: tensor of shape `[batch_size, num_reads]` which indicates\n          which read heads read memory that can now be freed.\n      read_weights: tensor of shape `[batch_size, num_reads,\n          memory_size]` giving read weights at previous time step.\n      prev_usage: tensor of shape `[batch_size, memory_size]` giving\n          usage u_{t - 1} at the previous time step, with entries in range\n          [0, 1].\n\n    Returns:\n      tensor of shape `[batch_size, memory_size]` representing updated memory\n      usage.\n    \"\"\"", "\n", "# Calculation of usage is not differentiable with respect to write weights.", "\n", "write_weights", "=", "tf", ".", "stop_gradient", "(", "write_weights", ")", "\n", "usage", "=", "self", ".", "_usage_after_write", "(", "prev_usage", ",", "write_weights", ")", "\n", "usage", "=", "self", ".", "_usage_after_read", "(", "usage", ",", "free_gate", ",", "read_weights", ")", "\n", "return", "usage", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness.write_allocation_weights": [[308, 342], ["tensorflow.name_scope", "tensorflow.expand_dims", "range", "tensorflow.stack", "allocation_weights.append", "addressing.Freeness._allocation"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness._allocation"], ["", "def", "write_allocation_weights", "(", "self", ",", "usage", ",", "write_gates", ",", "num_writes", ")", ":", "\n", "    ", "\"\"\"Calculates freeness-based locations for writing to.\n\n    This finds unused memory by ranking the memory locations by usage, for each\n    write head. (For more than one write head, we use a \"simulated new usage\"\n    which takes into account the fact that the previous write head will increase\n    the usage in that area of the memory.)\n\n    Args:\n      usage: A tensor of shape `[batch_size, memory_size]` representing\n          current memory usage.\n      write_gates: A tensor of shape `[batch_size, num_writes]` with values in\n          the range [0, 1] indicating how much each write head does writing\n          based on the address returned here (and hence how much usage\n          increases).\n      num_writes: The number of write heads to calculate write weights for.\n\n    Returns:\n      tensor of shape `[batch_size, num_writes, memory_size]` containing the\n          freeness-based write locations. Note that this isn't scaled by\n          `write_gate`; this scaling must be applied externally.\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'write_allocation_weights'", ")", ":", "\n", "# expand gatings over memory locations", "\n", "      ", "write_gates", "=", "tf", ".", "expand_dims", "(", "write_gates", ",", "-", "1", ")", "\n", "\n", "allocation_weights", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_writes", ")", ":", "\n", "        ", "allocation_weights", ".", "append", "(", "self", ".", "_allocation", "(", "usage", ")", ")", "\n", "# update usage to take into account writing to this new allocation", "\n", "usage", "+=", "(", "(", "1", "-", "usage", ")", "*", "write_gates", "[", ":", ",", "i", ",", ":", "]", "*", "allocation_weights", "[", "i", "]", ")", "\n", "\n", "# Pack the allocation weights for the write heads into one tensor.", "\n", "", "return", "tf", ".", "stack", "(", "allocation_weights", ",", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness._usage_after_write": [[343, 357], ["tensorflow.name_scope", "tensorflow.reduce_prod"], "methods", ["None"], ["", "", "def", "_usage_after_write", "(", "self", ",", "prev_usage", ",", "write_weights", ")", ":", "\n", "    ", "\"\"\"Calcualtes the new usage after writing to memory.\n\n    Args:\n      prev_usage: tensor of shape `[batch_size, memory_size]`.\n      write_weights: tensor of shape `[batch_size, num_writes, memory_size]`.\n\n    Returns:\n      New usage, a tensor of shape `[batch_size, memory_size]`.\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'usage_after_write'", ")", ":", "\n", "# Calculate the aggregated effect of all write heads", "\n", "      ", "write_weights", "=", "1", "-", "tf", ".", "reduce_prod", "(", "1", "-", "write_weights", ",", "[", "1", "]", ")", "\n", "return", "prev_usage", "+", "(", "1", "-", "prev_usage", ")", "*", "write_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness._usage_after_read": [[358, 376], ["tensorflow.name_scope", "tensorflow.expand_dims", "tensorflow.reduce_prod"], "methods", ["None"], ["", "", "def", "_usage_after_read", "(", "self", ",", "prev_usage", ",", "free_gate", ",", "read_weights", ")", ":", "\n", "    ", "\"\"\"Calcualtes the new usage after reading and freeing from memory.\n\n    Args:\n      prev_usage: tensor of shape `[batch_size, memory_size]`.\n      free_gate: tensor of shape `[batch_size, num_reads]` with entries in the\n          range [0, 1] indicating the amount that locations read from can be\n          freed.\n      read_weights: tensor of shape `[batch_size, num_reads, memory_size]`.\n\n    Returns:\n      New usage, a tensor of shape `[batch_size, memory_size]`.\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'usage_after_read'", ")", ":", "\n", "      ", "free_gate", "=", "tf", ".", "expand_dims", "(", "free_gate", ",", "-", "1", ")", "\n", "free_read_weights", "=", "free_gate", "*", "read_weights", "\n", "phi", "=", "tf", ".", "reduce_prod", "(", "1", "-", "free_read_weights", ",", "[", "1", "]", ",", "name", "=", "'phi'", ")", "\n", "return", "prev_usage", "*", "phi", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness._allocation": [[377, 407], ["tensorflow.name_scope", "tensorflow.nn.top_k", "tensorflow.cumprod", "util.batch_invert_permutation", "util.batch_gather"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.util.batch_invert_permutation", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.util.batch_gather"], ["", "", "def", "_allocation", "(", "self", ",", "usage", ")", ":", "\n", "    ", "r\"\"\"Computes allocation by sorting `usage`.\n\n    This corresponds to the value a = a_t[\\phi_t[j]] in the paper.\n\n    Args:\n      usage: tensor of shape `[batch_size, memory_size]` indicating current\n          memory usage. This is equal to u_t in the paper when we only have one\n          write head, but for multiple write heads, one should update the usage\n          while iterating through the write heads to take into account the\n          allocation returned by this function.\n\n    Returns:\n      Tensor of shape `[batch_size, memory_size]` corresponding to allocation.\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'allocation'", ")", ":", "\n", "# Ensure values are not too small prior to cumprod.", "\n", "      ", "usage", "=", "_EPSILON", "+", "(", "1", "-", "_EPSILON", ")", "*", "usage", "\n", "\n", "nonusage", "=", "1", "-", "usage", "\n", "sorted_nonusage", ",", "indices", "=", "tf", ".", "nn", ".", "top_k", "(", "\n", "nonusage", ",", "k", "=", "self", ".", "_memory_size", ",", "name", "=", "'sort'", ")", "\n", "sorted_usage", "=", "1", "-", "sorted_nonusage", "\n", "prod_sorted_usage", "=", "tf", ".", "cumprod", "(", "sorted_usage", ",", "axis", "=", "1", ",", "exclusive", "=", "True", ")", "\n", "sorted_allocation", "=", "sorted_nonusage", "*", "prod_sorted_usage", "\n", "inverse_indices", "=", "util", ".", "batch_invert_permutation", "(", "indices", ")", "\n", "\n", "# This final line \"unsorts\" sorted_allocation, so that the indexing", "\n", "# corresponds to the original indexing of `usage`.", "\n", "return", "util", ".", "batch_gather", "(", "sorted_allocation", ",", "inverse_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.Freeness.state_size": [[408, 412], ["tensorflow.TensorShape"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns the shape of the state tensor.\"\"\"", "\n", "return", "tf", ".", "TensorShape", "(", "[", "self", ".", "_memory_size", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing._vector_norms": [[34, 37], ["tensorflow.reduce_sum", "tensorflow.sqrt"], "function", ["None"], ["def", "_vector_norms", "(", "m", ")", ":", "\n", "  ", "squared_norms", "=", "tf", ".", "reduce_sum", "(", "m", "*", "m", ",", "axis", "=", "2", ",", "keep_dims", "=", "True", ")", "\n", "return", "tf", ".", "sqrt", "(", "squared_norms", "+", "_EPSILON", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.addressing.weighted_softmax": [[39, 57], ["tensorflow.expand_dims", "softmax", "strengths_op"], "function", ["None"], ["", "def", "weighted_softmax", "(", "activations", ",", "strengths", ",", "strengths_op", ")", ":", "\n", "  ", "\"\"\"Returns softmax over activations multiplied by positive strengths.\n\n  Args:\n    activations: A tensor of shape `[batch_size, num_heads, memory_size]`, of\n      activations to be transformed. Softmax is taken over the last dimension.\n    strengths: A tensor of shape `[batch_size, num_heads]` containing strengths to\n      multiply by the activations prior to the softmax.\n    strengths_op: An operation to transform strengths before softmax.\n\n  Returns:\n    A tensor of same shape as `activations` with weighted softmax applied.\n  \"\"\"", "\n", "transformed_strengths", "=", "tf", ".", "expand_dims", "(", "strengths_op", "(", "strengths", ")", ",", "-", "1", ")", "\n", "sharp_activations", "=", "activations", "*", "transformed_strengths", "\n", "# softmax = snt.BatchApply(module_or_op=tf.nn.softmax)", "\n", "softmax", "=", "tf", ".", "nn", ".", "softmax", "\n", "return", "softmax", "(", "sharp_activations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.add_arguments": [[39, 245], ["parser.register", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "v.lower"], "function", ["None"], ["def", "add_arguments", "(", "parser", ")", ":", "\n", "  ", "\"\"\"Build ArgumentParser.\"\"\"", "\n", "parser", ".", "register", "(", "\"type\"", ",", "\"bool\"", ",", "lambda", "v", ":", "v", ".", "lower", "(", ")", "==", "\"true\"", ")", "\n", "\n", "# MANN + Curriculum", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "type", "=", "str", ",", "default", "=", "\"none\"", ",", "help", "=", "\"none | model0 | model1 | model2 | model3\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mann\"", ",", "type", "=", "str", ",", "default", "=", "\"ntm\"", ",", "help", "=", "\"ntm | dnc\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--read_heads\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Number of read heads in external memory unit\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--write_heads\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Number of write heads in external memory unit\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_memory_locations\"", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "\"Number of memory locations in external memory unit\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--memory_unit_size\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "\"Units per memory location in external memory unit\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--curriculum\"", ",", "type", "=", "str", ",", "default", "=", "\"none\"", ",", "help", "=", "\"none | predictive_gain\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_curriculum_buckets\"", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"Number of lessons in the curriculum\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--curriculum_progress_loss\"", ",", "type", "=", "float", ",", "default", "=", "2.0", ")", "\n", "parser", ".", "add_argument", "(", "\"--record_w_history\"", ",", "type", "=", "\"bool\"", ",", "default", "=", "False", ")", "\n", "\n", "# network", "\n", "parser", ".", "add_argument", "(", "\"--num_units\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "\"Network size.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_proj\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "help", "=", "\"Output projection.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--embedding_size\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "help", "=", "\"Size of encoding/decoding embedding.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_encoder_units\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "help", "=", "\"Network size.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_decoder_units\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "help", "=", "\"Network size.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_layers\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "\"Network depth.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--encoder_type\"", ",", "type", "=", "str", ",", "default", "=", "\"uni\"", ",", "help", "=", "\"\"\"\\\n      uni | bi | gnmt. For bi, we build num_layers/2 bi-directional layers.For\n      gnmt, we build 1 bi-directional layer, and (num_layers - 1) uni-\n      directional layers.\\\n      \"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--residual\"", ",", "type", "=", "\"bool\"", ",", "nargs", "=", "\"?\"", ",", "const", "=", "True", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"Whether to add residual connections.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--time_major\"", ",", "type", "=", "\"bool\"", ",", "nargs", "=", "\"?\"", ",", "const", "=", "True", ",", "\n", "default", "=", "True", ",", "\n", "help", "=", "\"Whether to use time-major mode for dynamic RNN.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_embeddings_partitions\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Number of partitions for embedding vars.\"", ")", "\n", "\n", "# attention mechanisms", "\n", "parser", ".", "add_argument", "(", "\"--attention\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "help", "=", "\"\"\"\\\n      luong | scaled_luong | bahdanau | normed_bahdanau or set to \"\" for no\n      attention\\\n      \"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--attention_architecture\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"standard\"", ",", "\n", "help", "=", "\"\"\"\\\n      standard | gnmt | gnmt_v2.\n      standard: use top layer to compute attention.\n      gnmt: GNMT style of computing attention, use previous bottom layer to\n          compute attention.\n      gnmt_v2: similar to gnmt, but use current bottom layer to compute\n          attention.\\\n      \"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--pass_hidden_state\"", ",", "type", "=", "\"bool\"", ",", "nargs", "=", "\"?\"", ",", "const", "=", "True", ",", "\n", "default", "=", "True", ",", "\n", "help", "=", "\"\"\"\\\n      Whether to pass encoder's hidden state to decoder when using an attention\n      based model.\\\n      \"\"\"", ")", "\n", "\n", "# optimizer", "\n", "parser", ".", "add_argument", "(", "\"--optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"sgd\"", ",", "help", "=", "\"sgd | adam\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"Learning rate. Adam: 0.001 | 0.0001\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--start_decay_step\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"When we start to decay\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--decay_steps\"", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "\"How frequent we decay\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--decay_factor\"", ",", "type", "=", "float", ",", "default", "=", "0.98", ",", "\n", "help", "=", "\"How much we decay.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--num_train_steps\"", ",", "type", "=", "int", ",", "default", "=", "12000", ",", "help", "=", "\"Num steps to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--colocate_gradients_with_ops\"", ",", "type", "=", "\"bool\"", ",", "nargs", "=", "\"?\"", ",", "\n", "const", "=", "True", ",", "\n", "default", "=", "True", ",", "\n", "help", "=", "(", "\"Whether try colocating gradients with \"", "\n", "\"corresponding op\"", ")", ")", "\n", "\n", "# initializer", "\n", "parser", ".", "add_argument", "(", "\"--init_op\"", ",", "type", "=", "str", ",", "default", "=", "\"uniform\"", ",", "\n", "help", "=", "\"uniform | glorot_normal | glorot_uniform\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--init_weight\"", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "(", "\"for uniform init_op, initialize weights \"", "\n", "\"between [-this, this].\"", ")", ")", "\n", "\n", "# data", "\n", "parser", ".", "add_argument", "(", "\"--src\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Source suffix, e.g., en.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Target suffix, e.g., de.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_prefix\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Train prefix, expect files with src/tgt suffixes.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dev_prefix\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Dev prefix, expect files with src/tgt suffixes.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--test_prefix\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Test prefix, expect files with src/tgt suffixes.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--out_dir\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Store log/model files.\"", ")", "\n", "\n", "# Vocab", "\n", "parser", ".", "add_argument", "(", "\"--vocab_prefix\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "\"\"\"\\\n      Vocab prefix, expect files with src/tgt suffixes.If None, extract from\n      train files.\\\n      \"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sos\"", ",", "type", "=", "str", ",", "default", "=", "\"<s>\"", ",", "\n", "help", "=", "\"Start-of-sentence symbol.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eos\"", ",", "type", "=", "str", ",", "default", "=", "\"</s>\"", ",", "\n", "help", "=", "\"End-of-sentence symbol.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--share_vocab\"", ",", "type", "=", "\"bool\"", ",", "nargs", "=", "\"?\"", ",", "const", "=", "True", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"\"\"\\\n      Whether to use the source vocab and embeddings for both source and\n      target.\\\n      \"\"\"", ")", "\n", "\n", "# Sequence lengths", "\n", "parser", ".", "add_argument", "(", "\"--src_max_len\"", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Max length of src sequences during training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_max_len\"", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Max length of tgt sequences during training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--src_max_len_infer\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Max length of src sequences during inference.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_max_len_infer\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"\"\"\\\n      Max length of tgt sequences during inference.  Also use to restrict the\n      maximum decoding length.\\\n      \"\"\"", ")", "\n", "\n", "# Default settings works well (rarely need to change)", "\n", "parser", ".", "add_argument", "(", "\"--unit_type\"", ",", "type", "=", "str", ",", "default", "=", "\"lstm\"", ",", "\n", "help", "=", "\"lstm | gru | layer_norm_lstm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--forget_bias\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "\"Forget bias for BasicLSTMCell.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "\n", "help", "=", "\"Dropout rate (not keep_prob)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_gradient_norm\"", ",", "type", "=", "float", ",", "default", "=", "5.0", ",", "\n", "help", "=", "\"Clip gradients to this norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--source_reverse\"", ",", "type", "=", "\"bool\"", ",", "nargs", "=", "\"?\"", ",", "const", "=", "True", ",", "\n", "default", "=", "False", ",", "help", "=", "\"Reverse source sequence.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "\"Batch size.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--steps_per_stats\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "(", "\"How many training steps to do per stats logging.\"", "\n", "\"Save checkpoint every 10x steps_per_stats\"", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_train\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Limit on the size of training data (0: no limit).\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_buckets\"", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "\"Put data into similar-length buckets.\"", ")", "\n", "\n", "# BPE", "\n", "parser", ".", "add_argument", "(", "\"--bpe_delimiter\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Set to @@ to activate BPE\"", ")", "\n", "\n", "# Misc", "\n", "parser", ".", "add_argument", "(", "\"--num_gpus\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Number of gpus in each worker.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_device_placement\"", ",", "type", "=", "\"bool\"", ",", "nargs", "=", "\"?\"", ",", "\n", "const", "=", "True", ",", "default", "=", "False", ",", "help", "=", "\"Debug GPU allocation.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--metrics\"", ",", "type", "=", "str", ",", "default", "=", "\"bleu\"", ",", "\n", "help", "=", "(", "\"Comma-separated list of evaluations \"", "\n", "\"metrics (bleu,rouge,accuracy)\"", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--steps_per_external_eval\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"\"\"\\\n      How many training steps to do per external evaluation.  Automatically set\n      based on data if None.\\\n      \"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--scope\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"scope to put variables under\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hparams_path\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "(", "\"Path to standard hparams json file that overrides\"", "\n", "\"hparams values from FLAGS.\"", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--random_seed\"", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Random seed (>0, set a specific seed).\"", ")", "\n", "\n", "# Inference", "\n", "parser", ".", "add_argument", "(", "\"--ckpt\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Checkpoint file to load a model for inference.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--inference_input_file\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Set to the text to decode.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--inference_list\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "(", "\"A comma-separated list of sentence indices \"", "\n", "\"(0-based) to decode.\"", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--infer_batch_size\"", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "\"Batch size for inference mode.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--inference_output_file\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Output file to store decoding results.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--inference_ref_file\"", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "(", "\"\"\"\\\n      Reference file to compute evaluation scores (if provided).\\\n      \"\"\"", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--beam_width\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "(", "\"\"\"\\\n      beam width when using beam search decoder. If 0 (default), use standard\n      decoder with greedy helper.\\\n      \"\"\"", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--length_penalty_weight\"", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "\"Length penalty for beam search.\"", ")", "\n", "\n", "# Job info", "\n", "parser", ".", "add_argument", "(", "\"--jobid\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Task id of the worker.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Number of workers (inference only).\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.create_hparams": [[247, 332], ["tensorflow.contrib.training.HParams", "flags.metrics.split"], "function", ["None"], ["", "def", "create_hparams", "(", "flags", ")", ":", "\n", "  ", "\"\"\"Create training hparams.\"\"\"", "\n", "return", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "# Data", "\n", "src", "=", "flags", ".", "src", ",", "\n", "tgt", "=", "flags", ".", "tgt", ",", "\n", "train_prefix", "=", "flags", ".", "train_prefix", ",", "\n", "dev_prefix", "=", "flags", ".", "dev_prefix", ",", "\n", "test_prefix", "=", "flags", ".", "test_prefix", ",", "\n", "vocab_prefix", "=", "flags", ".", "vocab_prefix", ",", "\n", "out_dir", "=", "flags", ".", "out_dir", ",", "\n", "\n", "# MANN + Curriculum", "\n", "model", "=", "flags", ".", "model", ",", "\n", "mann", "=", "flags", ".", "mann", ",", "\n", "read_heads", "=", "flags", ".", "read_heads", ",", "\n", "write_heads", "=", "flags", ".", "write_heads", ",", "\n", "num_memory_locations", "=", "flags", ".", "num_memory_locations", ",", "\n", "memory_unit_size", "=", "flags", ".", "memory_unit_size", ",", "\n", "curriculum", "=", "flags", ".", "curriculum", ",", "\n", "num_curriculum_buckets", "=", "flags", ".", "num_curriculum_buckets", ",", "\n", "curriculum_progress_loss", "=", "flags", ".", "curriculum_progress_loss", ",", "\n", "record_w_history", "=", "flags", ".", "record_w_history", ",", "\n", "\n", "# Networks", "\n", "num_units", "=", "flags", ".", "num_units", ",", "\n", "num_proj", "=", "flags", ".", "num_proj", ",", "\n", "embedding_size", "=", "flags", ".", "embedding_size", ",", "\n", "num_encoder_units", "=", "flags", ".", "num_encoder_units", ",", "\n", "num_decoder_units", "=", "flags", ".", "num_decoder_units", ",", "\n", "num_layers", "=", "flags", ".", "num_layers", ",", "\n", "dropout", "=", "flags", ".", "dropout", ",", "\n", "unit_type", "=", "flags", ".", "unit_type", ",", "\n", "encoder_type", "=", "flags", ".", "encoder_type", ",", "\n", "residual", "=", "flags", ".", "residual", ",", "\n", "time_major", "=", "flags", ".", "time_major", ",", "\n", "num_embeddings_partitions", "=", "flags", ".", "num_embeddings_partitions", ",", "\n", "\n", "# Attention mechanisms", "\n", "attention", "=", "flags", ".", "attention", ",", "\n", "attention_architecture", "=", "flags", ".", "attention_architecture", ",", "\n", "pass_hidden_state", "=", "flags", ".", "pass_hidden_state", ",", "\n", "\n", "# Train", "\n", "optimizer", "=", "flags", ".", "optimizer", ",", "\n", "num_train_steps", "=", "flags", ".", "num_train_steps", ",", "\n", "batch_size", "=", "flags", ".", "batch_size", ",", "\n", "init_op", "=", "flags", ".", "init_op", ",", "\n", "init_weight", "=", "flags", ".", "init_weight", ",", "\n", "max_gradient_norm", "=", "flags", ".", "max_gradient_norm", ",", "\n", "learning_rate", "=", "flags", ".", "learning_rate", ",", "\n", "start_decay_step", "=", "flags", ".", "start_decay_step", ",", "\n", "decay_factor", "=", "flags", ".", "decay_factor", ",", "\n", "decay_steps", "=", "flags", ".", "decay_steps", ",", "\n", "colocate_gradients_with_ops", "=", "flags", ".", "colocate_gradients_with_ops", ",", "\n", "\n", "# Data constraints", "\n", "num_buckets", "=", "flags", ".", "num_buckets", ",", "\n", "max_train", "=", "flags", ".", "max_train", ",", "\n", "src_max_len", "=", "flags", ".", "src_max_len", ",", "\n", "tgt_max_len", "=", "flags", ".", "tgt_max_len", ",", "\n", "source_reverse", "=", "flags", ".", "source_reverse", ",", "\n", "\n", "# Inference", "\n", "src_max_len_infer", "=", "flags", ".", "src_max_len_infer", ",", "\n", "tgt_max_len_infer", "=", "flags", ".", "tgt_max_len_infer", ",", "\n", "infer_batch_size", "=", "flags", ".", "infer_batch_size", ",", "\n", "beam_width", "=", "flags", ".", "beam_width", ",", "\n", "length_penalty_weight", "=", "flags", ".", "length_penalty_weight", ",", "\n", "\n", "# Vocab", "\n", "sos", "=", "flags", ".", "sos", "if", "flags", ".", "sos", "else", "vocab_utils", ".", "SOS", ",", "\n", "eos", "=", "flags", ".", "eos", "if", "flags", ".", "eos", "else", "vocab_utils", ".", "EOS", ",", "\n", "bpe_delimiter", "=", "flags", ".", "bpe_delimiter", ",", "\n", "\n", "# Misc", "\n", "forget_bias", "=", "flags", ".", "forget_bias", ",", "\n", "num_gpus", "=", "flags", ".", "num_gpus", ",", "\n", "epoch_step", "=", "0", ",", "# record where we were within an epoch.", "\n", "steps_per_stats", "=", "flags", ".", "steps_per_stats", ",", "\n", "steps_per_external_eval", "=", "flags", ".", "steps_per_external_eval", ",", "\n", "share_vocab", "=", "flags", ".", "share_vocab", ",", "\n", "metrics", "=", "flags", ".", "metrics", ".", "split", "(", "\",\"", ")", ",", "\n", "log_device_placement", "=", "flags", ".", "log_device_placement", ",", "\n", "random_seed", "=", "flags", ".", "random_seed", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.extend_hparams": [[335, 416], ["misc_utils.print_out", "misc_utils.print_out", "misc_utils.print_out", "misc_utils.print_out", "misc_utils.print_out", "misc_utils.print_out", "misc_utils.print_out", "hparams.add_hparam", "vocab_utils.check_vocab", "hparams.add_hparam", "hparams.add_hparam", "hparams.add_hparam", "hparams.add_hparam", "ValueError", "ValueError", "ValueError", "misc_utils.print_out", "vocab_utils.check_vocab", "tensorflow.gfile.Exists", "misc_utils.print_out", "tensorflow.gfile.MakeDirs", "hparams.add_hparam", "os.path.join", "hparams.add_hparam", "tensorflow.gfile.MakeDirs"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.vocab_utils.check_vocab", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.vocab_utils.check_vocab", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "extend_hparams", "(", "hparams", ")", ":", "\n", "  ", "\"\"\"Extend training hparams.\"\"\"", "\n", "# Sanity checks", "\n", "if", "hparams", ".", "encoder_type", "==", "\"bi\"", "and", "hparams", ".", "num_layers", "%", "2", "!=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\"For bi, num_layers %d should be even\"", "%", "\n", "hparams", ".", "num_layers", ")", "\n", "", "if", "(", "hparams", ".", "attention_architecture", "in", "[", "\"gnmt\"", "]", "and", "\n", "hparams", ".", "num_layers", "<", "2", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\"For gnmt attention architecture, \"", "\n", "\"num_layers %d should be >= 2\"", "%", "hparams", ".", "num_layers", ")", "\n", "\n", "# Flags", "\n", "", "utils", ".", "print_out", "(", "\"# hparams:\"", ")", "\n", "utils", ".", "print_out", "(", "\"  src=%s\"", "%", "hparams", ".", "src", ")", "\n", "utils", ".", "print_out", "(", "\"  tgt=%s\"", "%", "hparams", ".", "tgt", ")", "\n", "utils", ".", "print_out", "(", "\"  train_prefix=%s\"", "%", "hparams", ".", "train_prefix", ")", "\n", "utils", ".", "print_out", "(", "\"  dev_prefix=%s\"", "%", "hparams", ".", "dev_prefix", ")", "\n", "utils", ".", "print_out", "(", "\"  test_prefix=%s\"", "%", "hparams", ".", "test_prefix", ")", "\n", "utils", ".", "print_out", "(", "\"  out_dir=%s\"", "%", "hparams", ".", "out_dir", ")", "\n", "\n", "# Set num_residual_layers", "\n", "if", "hparams", ".", "residual", "and", "hparams", ".", "num_layers", ">", "1", ":", "\n", "    ", "if", "hparams", ".", "encoder_type", "==", "\"gnmt\"", ":", "\n", "# The first unidirectional layer (after the bi-directional layer) in", "\n", "# the GNMT encoder can't have residual connection due to the input is", "\n", "# the concatenation of fw_cell and bw_cell's outputs.", "\n", "      ", "num_residual_layers", "=", "hparams", ".", "num_layers", "-", "2", "\n", "", "else", ":", "\n", "      ", "num_residual_layers", "=", "hparams", ".", "num_layers", "-", "1", "\n", "", "", "else", ":", "\n", "    ", "num_residual_layers", "=", "0", "\n", "", "hparams", ".", "add_hparam", "(", "\"num_residual_layers\"", ",", "num_residual_layers", ")", "\n", "\n", "## Vocab", "\n", "# Get vocab file names first", "\n", "if", "hparams", ".", "vocab_prefix", ":", "\n", "    ", "src_vocab_file", "=", "hparams", ".", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "src", "\n", "tgt_vocab_file", "=", "hparams", ".", "vocab_prefix", "+", "\".\"", "+", "hparams", ".", "tgt", "\n", "# src_vocab_file = hparams.vocab_prefix", "\n", "# tgt_vocab_file = hparams.vocab_prefix", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"hparams.vocab_prefix must be provided.\"", ")", "\n", "\n", "# Source vocab", "\n", "", "src_vocab_size", ",", "src_vocab_file", "=", "vocab_utils", ".", "check_vocab", "(", "\n", "src_vocab_file", ",", "\n", "hparams", ".", "out_dir", ",", "\n", "sos", "=", "hparams", ".", "sos", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "unk", "=", "vocab_utils", ".", "UNK", ")", "\n", "\n", "# Target vocab", "\n", "if", "hparams", ".", "share_vocab", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"  using source vocab for target\"", ")", "\n", "tgt_vocab_file", "=", "src_vocab_file", "\n", "tgt_vocab_size", "=", "src_vocab_size", "\n", "", "else", ":", "\n", "    ", "tgt_vocab_size", ",", "tgt_vocab_file", "=", "vocab_utils", ".", "check_vocab", "(", "\n", "tgt_vocab_file", ",", "\n", "hparams", ".", "out_dir", ",", "\n", "sos", "=", "hparams", ".", "sos", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "unk", "=", "vocab_utils", ".", "UNK", ")", "\n", "", "hparams", ".", "add_hparam", "(", "\"src_vocab_size\"", ",", "src_vocab_size", ")", "\n", "hparams", ".", "add_hparam", "(", "\"tgt_vocab_size\"", ",", "tgt_vocab_size", ")", "\n", "hparams", ".", "add_hparam", "(", "\"src_vocab_file\"", ",", "src_vocab_file", ")", "\n", "hparams", ".", "add_hparam", "(", "\"tgt_vocab_file\"", ",", "tgt_vocab_file", ")", "\n", "\n", "# Check out_dir", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "hparams", ".", "out_dir", ")", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"# Creating output directory %s ...\"", "%", "hparams", ".", "out_dir", ")", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "hparams", ".", "out_dir", ")", "\n", "\n", "# Evaluation", "\n", "", "for", "metric", "in", "hparams", ".", "metrics", ":", "\n", "    ", "hparams", ".", "add_hparam", "(", "\"best_\"", "+", "metric", ",", "0", ")", "# larger is better", "\n", "best_metric_dir", "=", "os", ".", "path", ".", "join", "(", "hparams", ".", "out_dir", ",", "\"best_\"", "+", "metric", ")", "\n", "hparams", ".", "add_hparam", "(", "\"best_\"", "+", "metric", "+", "\"_dir\"", ",", "best_metric_dir", ")", "\n", "tf", ".", "gfile", ".", "MakeDirs", "(", "best_metric_dir", ")", "\n", "\n", "", "return", "hparams", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.ensure_compatible_hparams": [[418, 443], ["misc_utils.maybe_parse_standard_hparams", "utils.maybe_parse_standard_hparams.values", "hparams.values", "hparams.add_hparam", "misc_utils.print_out", "setattr", "getattr", "str", "str", "getattr"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.maybe_parse_standard_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "ensure_compatible_hparams", "(", "hparams", ",", "default_hparams", ",", "hparams_path", ")", ":", "\n", "  ", "\"\"\"Make sure the loaded hparams is compatible with new changes.\"\"\"", "\n", "default_hparams", "=", "utils", ".", "maybe_parse_standard_hparams", "(", "\n", "default_hparams", ",", "hparams_path", ")", "\n", "\n", "# For compatible reason, if there are new fields in default_hparams,", "\n", "#   we add them to the current hparams", "\n", "default_config", "=", "default_hparams", ".", "values", "(", ")", "\n", "config", "=", "hparams", ".", "values", "(", ")", "\n", "for", "key", "in", "default_config", ":", "\n", "    ", "if", "key", "not", "in", "config", ":", "\n", "      ", "hparams", ".", "add_hparam", "(", "key", ",", "default_config", "[", "key", "]", ")", "\n", "\n", "# Make sure that the loaded model has latest values for the below keys", "\n", "", "", "updated_keys", "=", "[", "\n", "\"out_dir\"", ",", "\"num_gpus\"", ",", "\"test_prefix\"", ",", "\"beam_width\"", ",", "\n", "\"length_penalty_weight\"", ",", "\"num_train_steps\"", "\n", "]", "\n", "for", "key", "in", "updated_keys", ":", "\n", "    ", "if", "key", "in", "default_config", "and", "getattr", "(", "hparams", ",", "key", ")", "!=", "default_config", "[", "key", "]", ":", "\n", "      ", "utils", ".", "print_out", "(", "\"# Updating hparams.%s: %s -> %s\"", "%", "\n", "(", "key", ",", "str", "(", "getattr", "(", "hparams", ",", "key", ")", ")", ",", "\n", "str", "(", "default_config", "[", "key", "]", ")", ")", ")", "\n", "setattr", "(", "hparams", ",", "key", ",", "default_config", "[", "key", "]", ")", "\n", "", "", "return", "hparams", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.create_or_load_hparams": [[445, 465], ["misc_utils.load_hparams", "misc_utils.save_hparams", "misc_utils.print_hparams", "misc_utils.maybe_parse_standard_hparams", "nmt.extend_hparams", "nmt.ensure_compatible_hparams", "misc_utils.save_hparams", "getattr"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.load_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.save_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.maybe_parse_standard_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.extend_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.ensure_compatible_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.save_hparams"], ["", "def", "create_or_load_hparams", "(", "out_dir", ",", "default_hparams", ",", "hparams_path", ")", ":", "\n", "  ", "\"\"\"Create hparams or load hparams from out_dir.\"\"\"", "\n", "hparams", "=", "utils", ".", "load_hparams", "(", "out_dir", ")", "\n", "if", "not", "hparams", ":", "\n", "    ", "hparams", "=", "default_hparams", "\n", "hparams", "=", "utils", ".", "maybe_parse_standard_hparams", "(", "\n", "hparams", ",", "hparams_path", ")", "\n", "hparams", "=", "extend_hparams", "(", "hparams", ")", "\n", "", "else", ":", "\n", "    ", "hparams", "=", "ensure_compatible_hparams", "(", "hparams", ",", "default_hparams", ",", "hparams_path", ")", "\n", "\n", "# Save HParams", "\n", "", "utils", ".", "save_hparams", "(", "out_dir", ",", "hparams", ")", "\n", "\n", "for", "metric", "in", "hparams", ".", "metrics", ":", "\n", "    ", "utils", ".", "save_hparams", "(", "getattr", "(", "hparams", ",", "\"best_\"", "+", "metric", "+", "\"_dir\"", ")", ",", "hparams", ")", "\n", "\n", "# Print HParams", "\n", "", "utils", ".", "print_hparams", "(", "hparams", ")", "\n", "return", "hparams", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.run_main": [[467, 516], ["misc_utils.print_out", "nmt.create_or_load_hparams", "misc_utils.print_out", "random.seed", "numpy.random.seed", "tensorflow.gfile.Exists", "tensorflow.gfile.MakeDirs", "inference_fn", "train_fn", "tensorflow.train.latest_checkpoint", "tensorflow.gfile.Exists", "int", "evaluation_utils.evaluate", "misc_utils.print_out", "flags.inference_list.split"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.create_or_load_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils.evaluate", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "run_main", "(", "flags", ",", "default_hparams", ",", "train_fn", ",", "inference_fn", ",", "target_session", "=", "\"\"", ")", ":", "\n", "  ", "\"\"\"Run main.\"\"\"", "\n", "# Job", "\n", "jobid", "=", "flags", ".", "jobid", "\n", "num_workers", "=", "flags", ".", "num_workers", "\n", "utils", ".", "print_out", "(", "\"# Job id %d\"", "%", "jobid", ")", "\n", "\n", "# Random", "\n", "random_seed", "=", "flags", ".", "random_seed", "\n", "if", "random_seed", "is", "not", "None", "and", "random_seed", ">", "0", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"# Set random seed to %d\"", "%", "random_seed", ")", "\n", "random", ".", "seed", "(", "random_seed", "+", "jobid", ")", "\n", "np", ".", "random", ".", "seed", "(", "random_seed", "+", "jobid", ")", "\n", "\n", "## Train / Decode", "\n", "", "out_dir", "=", "flags", ".", "out_dir", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "out_dir", ")", ":", "tf", ".", "gfile", ".", "MakeDirs", "(", "out_dir", ")", "\n", "\n", "# Load hparams.", "\n", "hparams", "=", "create_or_load_hparams", "(", "out_dir", ",", "default_hparams", ",", "flags", ".", "hparams_path", ")", "\n", "\n", "if", "flags", ".", "inference_input_file", ":", "\n", "# Inference indices", "\n", "    ", "hparams", ".", "inference_indices", "=", "None", "\n", "if", "flags", ".", "inference_list", ":", "\n", "      ", "(", "hparams", ".", "inference_indices", ")", "=", "(", "\n", "[", "int", "(", "token", ")", "for", "token", "in", "flags", ".", "inference_list", ".", "split", "(", "\",\"", ")", "]", ")", "\n", "\n", "# Inference", "\n", "", "trans_file", "=", "flags", ".", "inference_output_file", "\n", "ckpt", "=", "flags", ".", "ckpt", "\n", "if", "not", "ckpt", ":", "\n", "      ", "ckpt", "=", "tf", ".", "train", ".", "latest_checkpoint", "(", "out_dir", ")", "\n", "", "inference_fn", "(", "ckpt", ",", "flags", ".", "inference_input_file", ",", "\n", "trans_file", ",", "hparams", ",", "num_workers", ",", "jobid", ")", "\n", "\n", "# Evaluation", "\n", "ref_file", "=", "flags", ".", "inference_ref_file", "\n", "if", "ref_file", "and", "tf", ".", "gfile", ".", "Exists", "(", "trans_file", ")", ":", "\n", "      ", "for", "metric", "in", "hparams", ".", "metrics", ":", "\n", "        ", "score", "=", "evaluation_utils", ".", "evaluate", "(", "\n", "ref_file", ",", "\n", "trans_file", ",", "\n", "metric", ",", "\n", "hparams", ".", "bpe_delimiter", ")", "\n", "utils", ".", "print_out", "(", "\"  %s: %.1f\"", "%", "(", "metric", ",", "score", ")", ")", "\n", "", "", "", "else", ":", "\n", "# Train", "\n", "    ", "train_fn", "(", "hparams", ",", "target_session", "=", "target_session", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.main": [[518, 523], ["nmt.create_hparams", "nmt.run_main"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.create_hparams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.nmt.run_main"], ["", "", "def", "main", "(", "unused_argv", ")", ":", "\n", "  ", "default_hparams", "=", "create_hparams", "(", "FLAGS", ")", "\n", "train_fn", "=", "train", ".", "train", "\n", "inference_fn", "=", "inference", ".", "inference", "\n", "run_main", "(", "FLAGS", ",", "default_hparams", ",", "train_fn", ",", "inference_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.__init__": [[48, 188], ["isinstance", "model_helper.get_initializer", "tensorflow.get_variable_scope().set_initializer", "model.BaseModel.init_embeddings", "tensorflow.size", "model.BaseModel.build_graph", "print", "tensorflow.Variable", "tensorflow.trainable_variables", "tensorflow.train.Saver", "misc_utils.print_out", "misc_utils.print_out", "tensorflow.variable_scope", "tensorflow.reduce_sum", "tensorflow.gradients", "model_helper.gradient_clip", "tensorflow.train.AdamOptimizer.apply_gradients", "tensorflow.summary.merge", "model.BaseModel._get_infer_summary", "tensorflow.global_variables", "param.get_shape", "misc_utils.print_out", "tensorflow.get_variable_scope", "tensorflow.variable_scope", "tensorflow.python.layers.core.Dense", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.cond", "tensorflow.train.GradientDescentOptimizer", "tensorflow.summary.scalar", "zip", "reverse_target_vocab_table.lookup", "tensorflow.constant", "tensorflow.train.AdamOptimizer", "tensorflow.to_int64", "tensorflow.constant", "tensorflow.train.exponential_decay", "float", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "str", "param.get_shape"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.get_initializer", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.init_embeddings", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.build_graph", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.gradient_clip", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model.AttentionModel._get_infer_summary", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["def", "__init__", "(", "self", ",", "\n", "hparams", ",", "\n", "mode", ",", "\n", "iterator", ",", "\n", "source_vocab_table", ",", "\n", "target_vocab_table", ",", "\n", "reverse_target_vocab_table", "=", "None", ",", "\n", "scope", "=", "None", ",", "\n", "single_cell_fn", "=", "None", ")", ":", "\n", "    ", "\"\"\"Create the model.\n\n    Args:\n      hparams: Hyperparameter configurations.\n      mode: TRAIN | EVAL | INFER\n      iterator: Dataset Iterator that feeds data.\n      source_vocab_table: Lookup table mapping source words to ids.\n      target_vocab_table: Lookup table mapping target words to ids.\n      reverse_target_vocab_table: Lookup table mapping ids to target words. Only\n        required in INFER mode. Defaults to None.\n      scope: scope of the model.\n      single_cell_fn: allow for adding customized cell. When not specified,\n        we default to model_helper._single_cell\n    \"\"\"", "\n", "assert", "isinstance", "(", "iterator", ",", "iterator_utils", ".", "BatchedInput", ")", "\n", "self", ".", "iterator", "=", "iterator", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "src_vocab_table", "=", "source_vocab_table", "\n", "self", ".", "tgt_vocab_table", "=", "target_vocab_table", "\n", "\n", "self", ".", "src_vocab_size", "=", "hparams", ".", "src_vocab_size", "\n", "self", ".", "tgt_vocab_size", "=", "hparams", ".", "tgt_vocab_size", "\n", "self", ".", "num_layers", "=", "hparams", ".", "num_layers", "\n", "self", ".", "num_gpus", "=", "hparams", ".", "num_gpus", "\n", "self", ".", "time_major", "=", "hparams", ".", "time_major", "\n", "\n", "# Initializer", "\n", "initializer", "=", "model_helper", ".", "get_initializer", "(", "\n", "hparams", ".", "init_op", ",", "hparams", ".", "random_seed", ",", "hparams", ".", "init_weight", ")", "\n", "tf", ".", "get_variable_scope", "(", ")", ".", "set_initializer", "(", "initializer", ")", "\n", "\n", "# Embeddings", "\n", "# TODO(ebrevdo): Only do this if the mode is TRAIN?", "\n", "self", ".", "init_embeddings", "(", "hparams", ",", "scope", ")", "\n", "self", ".", "batch_size", "=", "tf", ".", "size", "(", "self", ".", "iterator", ".", "source_sequence_length", ")", "\n", "\n", "# Projection", "\n", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"build_network\"", ")", ":", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "\"decoder/output_projection\"", ")", ":", "\n", "        ", "self", ".", "output_layer", "=", "layers_core", ".", "Dense", "(", "\n", "hparams", ".", "tgt_vocab_size", ",", "use_bias", "=", "False", ",", "name", "=", "\"output_projection\"", ")", "\n", "\n", "# To make it flexible for external code to add other cell types", "\n", "# If not specified, we will later use model_helper._single_cell", "\n", "", "", "self", ".", "single_cell_fn", "=", "single_cell_fn", "\n", "\n", "## Train graph", "\n", "res", "=", "self", ".", "build_graph", "(", "hparams", ",", "scope", "=", "scope", ")", "\n", "\n", "if", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "self", ".", "train_loss", "=", "res", "[", "1", "]", "\n", "self", ".", "word_count", "=", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "iterator", ".", "source_sequence_length", ")", "+", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "iterator", ".", "target_sequence_length", ")", "\n", "", "elif", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "EVAL", ":", "\n", "      ", "self", ".", "eval_loss", "=", "res", "[", "1", "]", "\n", "", "elif", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ":", "\n", "      ", "self", ".", "infer_logits", ",", "_", ",", "self", ".", "final_context_state", ",", "self", ".", "sample_id", "=", "res", "\n", "self", ".", "sample_words", "=", "reverse_target_vocab_table", ".", "lookup", "(", "\n", "tf", ".", "to_int64", "(", "self", ".", "sample_id", ")", ")", "\n", "\n", "", "if", "self", ".", "mode", "!=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ":", "\n", "## Count the number of predicted words for compute ppl.", "\n", "      ", "self", ".", "predict_count", "=", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "iterator", ".", "target_sequence_length", ")", "\n", "\n", "## Learning rate", "\n", "", "print", "(", "\"  start_decay_step=%d, learning_rate=%g, decay_steps %d,\"", "\n", "\"decay_factor %g\"", "%", "(", "hparams", ".", "start_decay_step", ",", "hparams", ".", "learning_rate", ",", "\n", "hparams", ".", "decay_steps", ",", "hparams", ".", "decay_factor", ")", ")", "\n", "self", ".", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "trainable", "=", "False", ")", "\n", "\n", "params", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "\n", "# Gradients and SGD update operation for training the model.", "\n", "# Arrage for the embedding vars to appear at the beginning.", "\n", "if", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "if", "hparams", ".", "optimizer", "==", "\"sgd\"", ":", "\n", "        ", "self", ".", "learning_rate", "=", "tf", ".", "cond", "(", "\n", "self", ".", "global_step", "<", "hparams", ".", "start_decay_step", ",", "\n", "lambda", ":", "tf", ".", "constant", "(", "hparams", ".", "learning_rate", ")", ",", "\n", "lambda", ":", "tf", ".", "train", ".", "exponential_decay", "(", "\n", "hparams", ".", "learning_rate", ",", "\n", "(", "self", ".", "global_step", "-", "hparams", ".", "start_decay_step", ")", ",", "\n", "hparams", ".", "decay_steps", ",", "\n", "hparams", ".", "decay_factor", ",", "\n", "staircase", "=", "True", ")", ",", "\n", "name", "=", "\"learning_rate\"", ")", "\n", "opt", "=", "tf", ".", "train", ".", "GradientDescentOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"lr\"", ",", "self", ".", "learning_rate", ")", "\n", "", "elif", "hparams", ".", "optimizer", "==", "\"adam\"", ":", "\n", "        ", "assert", "float", "(", "\n", "hparams", ".", "learning_rate", "\n", ")", "<=", "0.001", ",", "\"! High Adam learning rate %g\"", "%", "hparams", ".", "learning_rate", "\n", "self", ".", "learning_rate", "=", "tf", ".", "constant", "(", "hparams", ".", "learning_rate", ")", "\n", "opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "learning_rate", ")", "\n", "\n", "", "gradients", "=", "tf", ".", "gradients", "(", "\n", "self", ".", "train_loss", ",", "\n", "params", ",", "\n", "colocate_gradients_with_ops", "=", "hparams", ".", "colocate_gradients_with_ops", ")", "\n", "\n", "clipped_gradients", ",", "gradient_norm_summary", "=", "model_helper", ".", "gradient_clip", "(", "\n", "gradients", ",", "max_gradient_norm", "=", "hparams", ".", "max_gradient_norm", ")", "\n", "\n", "self", ".", "update", "=", "opt", ".", "apply_gradients", "(", "\n", "zip", "(", "clipped_gradients", ",", "params", ")", ",", "global_step", "=", "self", ".", "global_step", ")", "\n", "\n", "# Summary", "\n", "self", ".", "train_summary", "=", "tf", ".", "summary", ".", "merge", "(", "[", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"lr\"", ",", "self", ".", "learning_rate", ")", ",", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"train_loss\"", ",", "self", ".", "train_loss", ")", ",", "\n", "]", "+", "gradient_norm_summary", ")", "\n", "\n", "", "if", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ":", "\n", "      ", "self", ".", "infer_summary", "=", "self", ".", "_get_infer_summary", "(", "hparams", ")", "\n", "\n", "# Saver", "\n", "", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "tf", ".", "global_variables", "(", ")", ")", "\n", "\n", "# Print trainable variables", "\n", "utils", ".", "print_out", "(", "\"# Trainable variables\"", ")", "\n", "total_parameters", "=", "0", "\n", "for", "param", "in", "params", ":", "\n", "      ", "variable_parametes", "=", "1", "\n", "for", "dim", "in", "param", ".", "get_shape", "(", ")", ":", "\n", "        ", "variable_parametes", "*=", "dim", ".", "value", "\n", "", "total_parameters", "+=", "variable_parametes", "\n", "utils", ".", "print_out", "(", "\"  %s, %s, %s\"", "%", "(", "param", ".", "name", ",", "str", "(", "param", ".", "get_shape", "(", ")", ")", ",", "\n", "param", ".", "op", ".", "device", ")", ")", "\n", "", "utils", ".", "print_out", "(", "\"Total parameters:  %s\"", "%", "(", "total_parameters", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.init_embeddings": [[189, 200], ["model_helper.create_emb_for_encoder_and_decoder"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_emb_for_encoder_and_decoder"], ["", "def", "init_embeddings", "(", "self", ",", "hparams", ",", "scope", ")", ":", "\n", "    ", "\"\"\"Init embeddings.\"\"\"", "\n", "self", ".", "embedding_encoder", ",", "self", ".", "embedding_decoder", "=", "(", "\n", "model_helper", ".", "create_emb_for_encoder_and_decoder", "(", "\n", "share_vocab", "=", "hparams", ".", "share_vocab", ",", "\n", "src_vocab_size", "=", "self", ".", "src_vocab_size", ",", "\n", "tgt_vocab_size", "=", "self", ".", "tgt_vocab_size", ",", "\n", "src_embed_size", "=", "hparams", ".", "num_units", "if", "hparams", ".", "num_proj", "is", "None", "else", "hparams", ".", "num_proj", ",", "\n", "tgt_embed_size", "=", "hparams", ".", "num_units", "if", "hparams", ".", "num_proj", "is", "None", "else", "hparams", ".", "num_proj", ",", "\n", "num_partitions", "=", "hparams", ".", "num_embeddings_partitions", ",", "\n", "scope", "=", "scope", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.train": [[201, 226], ["sess.run", "sess.run", "range"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "hparams", ",", "sess", ",", "handle", "=", "None", ",", "iterator_handle", "=", "None", ",", "\n", "use_fed_source_placeholder", "=", "None", ",", "fed_source_placeholder", "=", "None", ")", ":", "\n", "    ", "assert", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", "\n", "if", "handle", "!=", "None", ":", "\n", "      ", "return", "sess", ".", "run", "(", "[", "self", ".", "update", ",", "\n", "self", ".", "train_loss", ",", "\n", "self", ".", "predict_count", ",", "\n", "self", ".", "train_summary", ",", "\n", "self", ".", "global_step", ",", "\n", "self", ".", "word_count", ",", "\n", "self", ".", "batch_size", ",", "\n", "self", ".", "source", "]", ",", "\n", "feed_dict", "=", "{", "\n", "handle", ":", "iterator_handle", ",", "\n", "use_fed_source_placeholder", ":", "False", ",", "\n", "fed_source_placeholder", ":", "[", "[", "-", "1", "for", "_", "in", "range", "(", "hparams", ".", "src_max_len", ")", "]", "]", "\n", "}", ")", "\n", "", "else", ":", "\n", "      ", "return", "sess", ".", "run", "(", "[", "self", ".", "update", ",", "\n", "self", ".", "train_loss", ",", "\n", "self", ".", "predict_count", ",", "\n", "self", ".", "train_summary", ",", "\n", "self", ".", "global_step", ",", "\n", "self", ".", "word_count", ",", "\n", "self", ".", "batch_size", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.eval": [[227, 232], ["sess.run"], "methods", ["None"], ["", "", "def", "eval", "(", "self", ",", "sess", ")", ":", "\n", "    ", "assert", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "EVAL", "\n", "return", "sess", ".", "run", "(", "[", "self", ".", "eval_loss", ",", "\n", "self", ".", "predict_count", ",", "\n", "self", ".", "batch_size", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.build_graph": [[233, 300], ["misc_utils.print_out", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.variable_scope", "model.BaseModel._build_encoder", "model.BaseModel._build_decoder", "tensorflow.no_op", "tensorflow.no_op", "tensorflow.device", "model.BaseModel._compute_loss", "print", "ntm.Model1NTMState", "ntm.Model3NTMState.att_w_history.stack", "tensorflow.transpose", "map", "model_helper.get_device_str", "ntm.Model2NTMState", "ntm.Model3NTMState", "tensorflow.transpose", "hist.stack"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.gnmt_model.GNMTModel._build_encoder", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._build_decoder", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._compute_loss", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.get_device_str"], ["", "def", "build_graph", "(", "self", ",", "hparams", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"Subclass must implement this method.\n\n    Creates a sequence-to-sequence model with dynamic RNN decoder API.\n    Args:\n      hparams: Hyperparameter configurations.\n      scope: VariableScope for the created subgraph; default \"dynamic_seq2seq\".\n\n    Returns:\n      A tuple of the form (logits, loss, final_context_state),\n      where:\n        logits: float32 Tensor [batch_size x num_decoder_symbols].\n        loss: the total loss / batch_size.\n        final_context_state: The final state of decoder RNN.\n\n    Raises:\n      ValueError: if encoder_type differs from mono and bi, or\n        attention_option is not (luong | scaled_luong |\n        bahdanau | normed_bahdanau).\n    \"\"\"", "\n", "utils", ".", "print_out", "(", "\"# creating %s graph ...\"", "%", "self", ".", "mode", ")", "\n", "dtype", "=", "tf", ".", "float32", "\n", "num_layers", "=", "hparams", ".", "num_layers", "\n", "num_gpus", "=", "hparams", ".", "num_gpus", "\n", "\n", "if", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "self", ".", "use_fed_source", "=", "tf", ".", "placeholder", "(", "tf", ".", "bool", ")", "\n", "self", ".", "fed_source", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "shape", "=", "(", "None", ",", "hparams", ".", "src_max_len", ")", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"dynamic_seq2seq\"", ",", "dtype", "=", "dtype", ")", ":", "\n", "# Encoder", "\n", "      ", "encoder_outputs", ",", "encoder_state", "=", "self", ".", "_build_encoder", "(", "hparams", ")", "\n", "\n", "## Decoder", "\n", "logits", ",", "sample_id", ",", "final_context_state", "=", "self", ".", "_build_decoder", "(", "encoder_outputs", ",", "encoder_state", ",", "hparams", ")", "\n", "\n", "if", "hparams", ".", "beam_width", ">", "0", "and", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ":", "\n", "        ", "cell_state", "=", "final_context_state", ".", "cell_state", "\n", "", "else", ":", "\n", "        ", "cell_state", "=", "final_context_state", "\n", "\n", "", "if", "hparams", ".", "mann", "==", "'ntm'", ":", "\n", "        ", "if", "hparams", ".", "model", "in", "(", "'model0'", ",", "'model1'", ")", ":", "\n", "          ", "print", "(", "'here'", ",", "final_context_state", ")", "\n", "final_state", "=", "Model1NTMState", "(", "*", "cell_state", ")", "\n", "", "elif", "hparams", ".", "model", "==", "'model2'", ":", "\n", "          ", "final_state", "=", "Model2NTMState", "(", "*", "cell_state", ")", "\n", "", "else", ":", "\n", "          ", "final_state", "=", "Model3NTMState", "(", "*", "cell_state", ")", "\n", "\n", "", "", "self", ".", "att_w_history", "=", "tf", ".", "no_op", "(", ")", "\n", "self", ".", "ext_w_history", "=", "tf", ".", "no_op", "(", ")", "\n", "if", "hparams", ".", "record_w_history", ":", "\n", "        ", "if", "hparams", ".", "mann", "==", "'ntm'", "and", "hparams", ".", "model", "in", "(", "'model0'", ",", "'model1'", ",", "'model2'", ",", "'model3'", ")", ":", "\n", "          ", "att_w_history", "=", "final_state", ".", "att_w_history", ".", "stack", "(", ")", "\n", "self", ".", "att_w_history", "=", "tf", ".", "transpose", "(", "att_w_history", ",", "[", "1", ",", "2", ",", "0", "]", ")", "\n", "", "if", "hparams", ".", "mann", "==", "'ntm'", "and", "hparams", ".", "model", "in", "(", "'model2'", ",", "'model3'", ")", ":", "\n", "          ", "self", ".", "ext_w_history", "=", "map", "(", "lambda", "hist", ":", "tf", ".", "transpose", "(", "hist", ".", "stack", "(", ")", ",", "[", "1", ",", "2", ",", "0", "]", ")", ",", "final_state", ".", "ext_w_history", ")", "\n", "\n", "## Loss", "\n", "", "", "", "if", "self", ".", "mode", "!=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ":", "\n", "      ", "with", "tf", ".", "device", "(", "model_helper", ".", "get_device_str", "(", "num_layers", "-", "1", ",", "num_gpus", ")", ")", ":", "\n", "        ", "loss", "=", "self", ".", "_compute_loss", "(", "logits", ")", "\n", "", "", "else", ":", "\n", "      ", "loss", "=", "None", "\n", "\n", "", "return", "logits", ",", "loss", ",", "final_context_state", ",", "sample_id", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._build_encoder": [[301, 314], ["None"], "methods", ["None"], ["", "@", "abc", ".", "abstractmethod", "\n", "def", "_build_encoder", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "\"\"\"Subclass must implement this.\n\n    Build and run an RNN encoder.\n\n    Args:\n      hparams: Hyperparameters configurations.\n\n    Returns:\n      A tuple of encoder_emb_inp, encoder_outputs and encoder_state.\n    \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._build_encoder_cell": [[315, 354], ["model_helper.create_rnn_cell", "ntm.NTMCell", "dnc.DNC"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_rnn_cell"], ["", "def", "_build_encoder_cell", "(", "self", ",", "hparams", ",", "num_layers", ",", "num_residual_layers", ",", "\n", "base_gpu", "=", "0", ")", ":", "\n", "    ", "\"\"\"Build a multi-layer RNN cell that can be used by encoder.\"\"\"", "\n", "\n", "if", "hparams", ".", "model", "==", "'model3'", ":", "\n", "      ", "if", "hparams", ".", "mann", "==", "'ntm'", ":", "\n", "        ", "return", "NTMCell", "(", "hparams", ".", "num_layers", ",", "hparams", ".", "num_units", ",", "\n", "use_att_memory", "=", "False", ",", "att_memory", "=", "False", ",", "att_memory_size", "=", "None", ",", "att_memory_vector_dim", "=", "None", ",", "\n", "use_ext_memory", "=", "True", ",", "ext_memory_size", "=", "hparams", ".", "num_memory_locations", ",", "ext_memory_vector_dim", "=", "hparams", ".", "memory_unit_size", ",", "\n", "ext_read_head_num", "=", "hparams", ".", "read_heads", ",", "ext_write_head_num", "=", "hparams", ".", "write_heads", ",", "\n", "dropout", "=", "hparams", ".", "dropout", ",", "batch_size", "=", "hparams", ".", "batch_size", ",", "mode", "=", "self", ".", "mode", ",", "\n", "shift_range", "=", "1", ",", "output_dim", "=", "hparams", ".", "num_units", ",", "reuse", "=", "False", ",", "\n", "record_w_history", "=", "hparams", ".", "record_w_history", ")", "\n", "", "elif", "hparams", ".", "mann", "==", "'dnc'", ":", "\n", "        ", "access_config", "=", "{", "\n", "'memory_size'", ":", "hparams", ".", "num_memory_locations", ",", "\n", "'word_size'", ":", "hparams", ".", "memory_unit_size", ",", "\n", "'num_reads'", ":", "hparams", ".", "read_heads", ",", "\n", "'num_writes'", ":", "hparams", ".", "write_heads", "\n", "}", "\n", "controller_config", "=", "{", "\n", "'num_units'", ":", "hparams", ".", "num_units", ",", "\n", "'num_layers'", ":", "hparams", ".", "num_layers", "\n", "}", "\n", "\n", "return", "DNC", "(", "access_config", ",", "controller_config", ",", "hparams", ".", "num_units", ",", "20", ",", "hparams", ".", "dropout", ",", "self", ".", "mode", ",", "hparams", ".", "batch_size", ")", "\n", "", "", "else", ":", "\n", "      ", "return", "model_helper", ".", "create_rnn_cell", "(", "\n", "unit_type", "=", "hparams", ".", "unit_type", ",", "\n", "num_units", "=", "hparams", ".", "num_units", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "num_residual_layers", "=", "num_residual_layers", ",", "\n", "forget_bias", "=", "hparams", ".", "forget_bias", ",", "\n", "dropout", "=", "hparams", ".", "dropout", ",", "\n", "num_gpus", "=", "hparams", ".", "num_gpus", ",", "\n", "mode", "=", "self", ".", "mode", ",", "\n", "base_gpu", "=", "base_gpu", ",", "\n", "single_cell_fn", "=", "self", ".", "single_cell_fn", ",", "\n", "num_proj", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._build_decoder": [[355, 492], ["tensorflow.cast", "tensorflow.cast", "model.BaseModel.tgt_vocab_table.lookup", "model.BaseModel.tgt_vocab_table.lookup", "misc_utils.print_out", "tensorflow.reduce_max", "tensorflow.to_int32", "tensorflow.variable_scope", "tensorflow.constant", "tensorflow.constant", "tensorflow.round", "model.BaseModel._build_decoder_cell", "tensorflow.nn.embedding_lookup", "tensorflow.contrib.seq2seq.TrainingHelper", "tensorflow.contrib.seq2seq.BasicDecoder", "tensorflow.contrib.seq2seq.dynamic_decode", "tensorflow.fill", "tensorflow.contrib.seq2seq.dynamic_decode", "print", "tensorflow.contrib.seq2seq.tile_batch", "tensorflow.transpose", "tensorflow.device", "model.BaseModel.output_layer", "tensorflow.contrib.seq2seq.BeamSearchDecoder", "tensorflow.contrib.seq2seq.GreedyEmbeddingHelper", "tensorflow.contrib.seq2seq.BasicDecoder", "tensorflow.no_op", "tensorflow.to_float", "model_helper.get_device_str"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model.AttentionModel._build_decoder_cell", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.get_device_str"], ["", "", "def", "_build_decoder", "(", "self", ",", "encoder_outputs", ",", "encoder_state", ",", "hparams", ")", ":", "\n", "    ", "\"\"\"Build and run a RNN decoder with a final projection layer.\n\n    Args:\n      encoder_outputs: The outputs of encoder for every time step.\n      encoder_state: The final state of the encoder.\n      hparams: The Hyperparameters configurations.\n\n    Returns:\n      A tuple of final logits and final decoder state:\n        logits: size [time, batch_size, vocab_size] when time_major=True.\n    \"\"\"", "\n", "tgt_sos_id", "=", "tf", ".", "cast", "(", "self", ".", "tgt_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "hparams", ".", "sos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "tgt_eos_id", "=", "tf", ".", "cast", "(", "self", ".", "tgt_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "hparams", ".", "eos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "\n", "num_layers", "=", "hparams", ".", "num_layers", "\n", "num_gpus", "=", "hparams", ".", "num_gpus", "\n", "\n", "iterator", "=", "self", ".", "iterator", "\n", "\n", "# maximum_iteration: The maximum decoding steps.", "\n", "if", "hparams", ".", "tgt_max_len_infer", ":", "\n", "      ", "maximum_iterations", "=", "hparams", ".", "tgt_max_len_infer", "\n", "utils", ".", "print_out", "(", "\"  decoding maximum_iterations %d\"", "%", "maximum_iterations", ")", "\n", "", "else", ":", "\n", "# TODO(thangluong): add decoding_length_factor flag", "\n", "      ", "decoding_length_factor", "=", "2.0", "\n", "max_encoder_length", "=", "tf", ".", "reduce_max", "(", "iterator", ".", "source_sequence_length", ")", "\n", "maximum_iterations", "=", "tf", ".", "to_int32", "(", "tf", ".", "round", "(", "\n", "tf", ".", "to_float", "(", "max_encoder_length", ")", "*", "decoding_length_factor", ")", ")", "\n", "\n", "## Decoder.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"decoder\"", ")", "as", "decoder_scope", ":", "\n", "      ", "if", "hparams", ".", "model", "==", "'model3'", ":", "\n", "          ", "if", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", "and", "hparams", ".", "beam_width", ">", "0", ":", "\n", "            ", "print", "(", "'hello'", ",", "encoder_state", ",", "self", ".", "encoder_cell", ",", "self", ".", "encoder_cell", ".", "state_size", ")", "\n", "decoder_initial_state", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "tile_batch", "(", "\n", "encoder_state", ",", "multiplier", "=", "hparams", ".", "beam_width", ")", "\n", "", "else", ":", "\n", "            ", "decoder_initial_state", "=", "encoder_state", "\n", "", "cell", "=", "self", ".", "encoder_cell", "\n", "\n", "", "else", ":", "\n", "        ", "cell", ",", "decoder_initial_state", "=", "self", ".", "_build_decoder_cell", "(", "\n", "hparams", ",", "encoder_outputs", ",", "encoder_state", ",", "\n", "iterator", ".", "source_sequence_length", ")", "\n", "\n", "## Train or eval", "\n", "", "if", "self", ".", "mode", "!=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ":", "\n", "# decoder_emp_inp: [max_time, batch_size, num_units]", "\n", "        ", "target_input", "=", "iterator", ".", "target_input", "\n", "if", "self", ".", "time_major", ":", "\n", "          ", "target_input", "=", "tf", ".", "transpose", "(", "target_input", ")", "\n", "", "decoder_emb_inp", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "\n", "self", ".", "embedding_decoder", ",", "target_input", ")", "\n", "\n", "# Helper", "\n", "helper", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "TrainingHelper", "(", "\n", "decoder_emb_inp", ",", "iterator", ".", "target_sequence_length", ",", "\n", "time_major", "=", "self", ".", "time_major", ")", "\n", "\n", "# Decoder", "\n", "my_decoder", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "BasicDecoder", "(", "\n", "cell", ",", "\n", "helper", ",", "\n", "decoder_initial_state", ",", ")", "\n", "\n", "# Dynamic decoding", "\n", "outputs", ",", "final_context_state", ",", "_", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "dynamic_decode", "(", "\n", "my_decoder", ",", "\n", "output_time_major", "=", "self", ".", "time_major", ",", "\n", "swap_memory", "=", "True", ",", "\n", "scope", "=", "decoder_scope", ")", "\n", "\n", "sample_id", "=", "outputs", ".", "sample_id", "\n", "\n", "# Note: there's a subtle difference here between train and inference.", "\n", "# We could have set output_layer when create my_decoder", "\n", "#   and shared more code between train and inference.", "\n", "# We chose to apply the output_layer to all timesteps for speed:", "\n", "#   10% improvements for small models & 20% for larger ones.", "\n", "# If memory is a concern, we should apply output_layer per timestep.", "\n", "device_id", "=", "num_layers", "if", "num_layers", "<", "num_gpus", "else", "(", "num_layers", "-", "1", ")", "\n", "with", "tf", ".", "device", "(", "model_helper", ".", "get_device_str", "(", "device_id", ",", "num_gpus", ")", ")", ":", "\n", "          ", "logits", "=", "self", ".", "output_layer", "(", "outputs", ".", "rnn_output", ")", "\n", "\n", "## Inference", "\n", "", "", "else", ":", "\n", "        ", "beam_width", "=", "hparams", ".", "beam_width", "\n", "length_penalty_weight", "=", "hparams", ".", "length_penalty_weight", "\n", "start_tokens", "=", "tf", ".", "fill", "(", "[", "self", ".", "batch_size", "]", ",", "tgt_sos_id", ")", "\n", "end_token", "=", "tgt_eos_id", "\n", "\n", "if", "beam_width", ">", "0", ":", "\n", "          ", "if", "hparams", ".", "model", "==", "'model3'", ":", "\n", "            ", "cell", ".", "batch_size", "=", "cell", ".", "batch_size", "*", "beam_width", "\n", "\n", "", "my_decoder", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "BeamSearchDecoder", "(", "\n", "cell", "=", "cell", ",", "\n", "embedding", "=", "self", ".", "embedding_decoder", ",", "\n", "start_tokens", "=", "start_tokens", ",", "\n", "end_token", "=", "end_token", ",", "\n", "initial_state", "=", "decoder_initial_state", ",", "\n", "beam_width", "=", "beam_width", ",", "\n", "output_layer", "=", "self", ".", "output_layer", ",", "\n", "length_penalty_weight", "=", "length_penalty_weight", ")", "\n", "", "else", ":", "\n", "# Helper", "\n", "          ", "helper", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "GreedyEmbeddingHelper", "(", "\n", "self", ".", "embedding_decoder", ",", "start_tokens", ",", "end_token", ")", "\n", "\n", "# Decoder", "\n", "my_decoder", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "BasicDecoder", "(", "\n", "cell", ",", "\n", "helper", ",", "\n", "decoder_initial_state", ",", "\n", "output_layer", "=", "self", ".", "output_layer", "# applied per timestep", "\n", ")", "\n", "\n", "# Dynamic decoding", "\n", "", "outputs", ",", "final_context_state", ",", "_", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "dynamic_decode", "(", "\n", "my_decoder", ",", "\n", "maximum_iterations", "=", "maximum_iterations", ",", "\n", "output_time_major", "=", "self", ".", "time_major", ",", "\n", "swap_memory", "=", "True", ",", "\n", "scope", "=", "decoder_scope", ")", "\n", "\n", "if", "beam_width", ">", "0", ":", "\n", "          ", "logits", "=", "tf", ".", "no_op", "(", ")", "\n", "sample_id", "=", "outputs", ".", "predicted_ids", "\n", "", "else", ":", "\n", "          ", "logits", "=", "outputs", ".", "rnn_output", "\n", "sample_id", "=", "outputs", ".", "sample_id", "\n", "\n", "", "", "", "return", "logits", ",", "sample_id", ",", "final_context_state", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.get_max_time": [[493, 496], ["tensorflow.shape"], "methods", ["None"], ["", "def", "get_max_time", "(", "self", ",", "tensor", ")", ":", "\n", "    ", "time_axis", "=", "0", "if", "self", ".", "time_major", "else", "1", "\n", "return", "tensor", ".", "shape", "[", "time_axis", "]", ".", "value", "or", "tf", ".", "shape", "(", "tensor", ")", "[", "time_axis", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._build_decoder_cell": [[497, 513], ["None"], "methods", ["None"], ["", "@", "abc", ".", "abstractmethod", "\n", "def", "_build_decoder_cell", "(", "self", ",", "hparams", ",", "encoder_outputs", ",", "encoder_state", ",", "\n", "source_sequence_length", ")", ":", "\n", "    ", "\"\"\"Subclass must implement this.\n\n    Args:\n      hparams: Hyperparameters configurations.\n      encoder_outputs: The outputs of encoder for every time step.\n      encoder_state: The final state of the encoder.\n      source_sequence_length: sequence length of encoder_outputs.\n\n    Returns:\n      A tuple of a multi-layer RNN cell used by decoder\n        and the intial state of the decoder RNN.\n    \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._compute_loss": [[514, 530], ["model.BaseModel.get_max_time", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.sequence_mask", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.reduce_sum", "tensorflow.to_float"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.get_max_time"], ["", "def", "_compute_loss", "(", "self", ",", "logits", ")", ":", "\n", "    ", "\"\"\"Compute optimization loss.\"\"\"", "\n", "self", ".", "target_output", "=", "self", ".", "iterator", ".", "target_output", "\n", "if", "self", ".", "time_major", ":", "\n", "      ", "target_output", "=", "tf", ".", "transpose", "(", "self", ".", "target_output", ")", "\n", "", "max_time", "=", "self", ".", "get_max_time", "(", "target_output", ")", "\n", "crossent", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "labels", "=", "target_output", ",", "logits", "=", "logits", ")", "\n", "target_weights", "=", "tf", ".", "sequence_mask", "(", "\n", "self", ".", "iterator", ".", "target_sequence_length", ",", "max_time", ",", "dtype", "=", "logits", ".", "dtype", ")", "\n", "if", "self", ".", "time_major", ":", "\n", "      ", "target_weights", "=", "tf", ".", "transpose", "(", "target_weights", ")", "\n", "\n", "", "loss", "=", "tf", ".", "reduce_sum", "(", "\n", "crossent", "*", "target_weights", ")", "/", "tf", ".", "to_float", "(", "self", ".", "batch_size", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._get_infer_summary": [[531, 533], ["tensorflow.no_op"], "methods", ["None"], ["", "def", "_get_infer_summary", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "return", "tf", ".", "no_op", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.infer": [[534, 538], ["sess.run"], "methods", ["None"], ["", "def", "infer", "(", "self", ",", "sess", ")", ":", "\n", "    ", "assert", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", "\n", "return", "sess", ".", "run", "(", "[", "\n", "self", ".", "infer_logits", ",", "self", ".", "att_w_history", ",", "self", ".", "ext_w_history", ",", "self", ".", "sample_id", ",", "self", ".", "sample_words", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode": [[540, 556], ["model.BaseModel.infer", "sample_words.transpose.transpose.transpose"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.infer"], ["", "def", "decode", "(", "self", ",", "sess", ")", ":", "\n", "    ", "\"\"\"Decode a batch.\n\n    Args:\n      sess: tensorflow session to use.\n\n    Returns:\n      A tuple consiting of outputs, infer_summary.\n        outputs: of size [batch_size, time]\n    \"\"\"", "\n", "_", ",", "att_w_history", ",", "ext_w_history", ",", "_", ",", "sample_words", "=", "self", ".", "infer", "(", "sess", ")", "\n", "\n", "# make sure outputs is of shape [batch_size, time]", "\n", "if", "self", ".", "time_major", ":", "\n", "      ", "sample_words", "=", "sample_words", ".", "transpose", "(", ")", "\n", "", "return", "sample_words", ",", "att_w_history", ",", "ext_w_history", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.Model._build_encoder": [[565, 628], ["tensorflow.cond", "tensorflow.transpose", "tensorflow.variable_scope", "tensorflow.nn.embedding_lookup", "misc_utils.print_out", "model.Model._build_encoder_cell", "tensorflow.nn.dynamic_rnn", "int", "int", "misc_utils.print_out", "model.Model._build_bidirectional_rnn", "ValueError", "range", "tuple", "tuple.append", "tuple.append"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._build_encoder_cell", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.Model._build_bidirectional_rnn"], ["def", "_build_encoder", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "\"\"\"Build an encoder.\"\"\"", "\n", "num_layers", "=", "hparams", ".", "num_layers", "\n", "num_residual_layers", "=", "hparams", ".", "num_residual_layers", "\n", "\n", "iterator", "=", "self", ".", "iterator", "\n", "\n", "if", "hparams", ".", "curriculum", "!=", "'none'", "and", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "self", ".", "source", "=", "tf", ".", "cond", "(", "self", ".", "use_fed_source", ",", "lambda", ":", "self", ".", "fed_source", ",", "lambda", ":", "iterator", ".", "source", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "source", "=", "iterator", ".", "source", "\n", "\n", "", "source", "=", "self", ".", "source", "\n", "if", "self", ".", "time_major", ":", "\n", "      ", "source", "=", "tf", ".", "transpose", "(", "source", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ")", "as", "scope", ":", "\n", "      ", "dtype", "=", "scope", ".", "dtype", "\n", "# Look up embedding, emp_inp: [max_time, batch_size, num_units]", "\n", "encoder_emb_inp", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "\n", "self", ".", "embedding_encoder", ",", "source", ")", "\n", "\n", "# Encoder_outpus: [max_time, batch_size, num_units]", "\n", "if", "hparams", ".", "encoder_type", "==", "\"uni\"", "or", "hparams", ".", "model", "==", "'model3'", ":", "\n", "        ", "utils", ".", "print_out", "(", "\"  num_layers = %d, num_residual_layers=%d\"", "%", "\n", "(", "num_layers", ",", "num_residual_layers", ")", ")", "\n", "\n", "self", ".", "encoder_cell", "=", "self", ".", "_build_encoder_cell", "(", "\n", "hparams", ",", "num_layers", ",", "num_residual_layers", ")", "\n", "\n", "encoder_outputs", ",", "encoder_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "\n", "self", ".", "encoder_cell", ",", "\n", "encoder_emb_inp", ",", "\n", "dtype", "=", "dtype", ",", "\n", "sequence_length", "=", "iterator", ".", "source_sequence_length", ",", "\n", "time_major", "=", "self", ".", "time_major", ")", "\n", "", "elif", "hparams", ".", "encoder_type", "==", "\"bi\"", ":", "\n", "        ", "num_bi_layers", "=", "int", "(", "num_layers", "/", "2", ")", "\n", "num_bi_residual_layers", "=", "int", "(", "num_residual_layers", "/", "2", ")", "\n", "utils", ".", "print_out", "(", "\"  num_bi_layers = %d, num_bi_residual_layers=%d\"", "%", "\n", "(", "num_bi_layers", ",", "num_bi_residual_layers", ")", ")", "\n", "\n", "encoder_outputs", ",", "bi_encoder_state", "=", "(", "\n", "self", ".", "_build_bidirectional_rnn", "(", "\n", "inputs", "=", "encoder_emb_inp", ",", "\n", "sequence_length", "=", "iterator", ".", "source_sequence_length", ",", "\n", "dtype", "=", "dtype", ",", "\n", "hparams", "=", "hparams", ",", "\n", "num_bi_layers", "=", "num_bi_layers", ",", "\n", "num_bi_residual_layers", "=", "num_bi_residual_layers", ")", ")", "\n", "\n", "if", "num_bi_layers", "==", "1", ":", "\n", "          ", "encoder_state", "=", "bi_encoder_state", "\n", "", "else", ":", "\n", "# alternatively concat forward and backward states", "\n", "          ", "encoder_state", "=", "[", "]", "\n", "for", "layer_id", "in", "range", "(", "num_bi_layers", ")", ":", "\n", "            ", "encoder_state", ".", "append", "(", "bi_encoder_state", "[", "0", "]", "[", "layer_id", "]", ")", "# forward", "\n", "encoder_state", ".", "append", "(", "bi_encoder_state", "[", "1", "]", "[", "layer_id", "]", ")", "# backward", "\n", "", "encoder_state", "=", "tuple", "(", "encoder_state", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown encoder_type %s\"", "%", "hparams", ".", "encoder_type", ")", "\n", "", "", "return", "encoder_outputs", ",", "encoder_state", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.Model._build_bidirectional_rnn": [[629, 668], ["model.Model._build_encoder_cell", "model.Model._build_encoder_cell", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._build_encoder_cell", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel._build_encoder_cell"], ["", "def", "_build_bidirectional_rnn", "(", "self", ",", "inputs", ",", "sequence_length", ",", "\n", "dtype", ",", "hparams", ",", "\n", "num_bi_layers", ",", "\n", "num_bi_residual_layers", ",", "\n", "base_gpu", "=", "0", ")", ":", "\n", "    ", "\"\"\"Create and call biddirectional RNN cells.\n\n    Args:\n      num_residual_layers: Number of residual layers from top to bottom. For\n        example, if `num_bi_layers=4` and `num_residual_layers=2`, the last 2 RNN\n        layers in each RNN cell will be wrapped with `ResidualWrapper`.\n      base_gpu: The gpu device id to use for the first forward RNN layer. The\n        i-th forward RNN layer will use `(base_gpu + i) % num_gpus` as its\n        device id. The `base_gpu` for backward RNN cell is `(base_gpu +\n        num_bi_layers)`.\n\n    Returns:\n      The concatenated bidirectional output and the bidirectional RNN cell\"s\n      state.\n    \"\"\"", "\n", "# Construct forward and backward cells", "\n", "fw_cell", "=", "self", ".", "_build_encoder_cell", "(", "hparams", ",", "\n", "num_bi_layers", ",", "\n", "num_bi_residual_layers", ",", "\n", "base_gpu", "=", "base_gpu", ")", "\n", "bw_cell", "=", "self", ".", "_build_encoder_cell", "(", "hparams", ",", "\n", "num_bi_layers", ",", "\n", "num_bi_residual_layers", ",", "\n", "base_gpu", "=", "(", "base_gpu", "+", "num_bi_layers", ")", ")", "\n", "\n", "bi_outputs", ",", "bi_state", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "fw_cell", ",", "\n", "bw_cell", ",", "\n", "inputs", ",", "\n", "dtype", "=", "dtype", ",", "\n", "sequence_length", "=", "sequence_length", ",", "\n", "time_major", "=", "self", ".", "time_major", ")", "\n", "\n", "return", "tf", ".", "concat", "(", "bi_outputs", ",", "-", "1", ")", ",", "bi_state", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.Model._build_decoder_cell": [[669, 700], ["model_helper.create_rnn_cell", "ValueError", "tensorflow.contrib.seq2seq.tile_batch"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_rnn_cell"], ["", "def", "_build_decoder_cell", "(", "self", ",", "hparams", ",", "encoder_outputs", ",", "encoder_state", ",", "\n", "source_sequence_length", ")", ":", "\n", "    ", "\"\"\"Build an RNN cell that can be used by decoder.\"\"\"", "\n", "# We only make use of encoder_outputs in attention-based models", "\n", "if", "hparams", ".", "attention", ":", "\n", "      ", "raise", "ValueError", "(", "\"BasicModel doesn't support attention.\"", ")", "\n", "\n", "", "num_layers", "=", "hparams", ".", "num_layers", "\n", "num_residual_layers", "=", "hparams", ".", "num_residual_layers", "\n", "\n", "cell", "=", "model_helper", ".", "create_rnn_cell", "(", "\n", "unit_type", "=", "hparams", ".", "unit_type", ",", "\n", "num_units", "=", "hparams", ".", "num_units", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "num_residual_layers", "=", "num_residual_layers", ",", "\n", "forget_bias", "=", "hparams", ".", "forget_bias", ",", "\n", "dropout", "=", "hparams", ".", "dropout", ",", "\n", "num_gpus", "=", "hparams", ".", "num_gpus", ",", "\n", "mode", "=", "self", ".", "mode", ",", "\n", "single_cell_fn", "=", "self", ".", "single_cell_fn", ",", "\n", "num_proj", "=", "None", ",", "\n", "num_cells", "=", "2", "if", "(", "hparams", ".", "encoder_type", "==", "\"bi\"", ")", "else", "1", ")", "\n", "\n", "# For beam search, we need to replicate encoder infos beam_width times", "\n", "if", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", "and", "hparams", ".", "beam_width", ">", "0", ":", "\n", "      ", "decoder_initial_state", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "tile_batch", "(", "\n", "encoder_state", ",", "multiplier", "=", "hparams", ".", "beam_width", ")", "\n", "", "else", ":", "\n", "      ", "decoder_initial_state", "=", "encoder_state", "\n", "\n", "", "return", "cell", ",", "decoder_initial_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.gnmt_model.GNMTModel.__init__": [[40, 58], ["attention_model.AttentionModel.__init__"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__"], ["def", "__init__", "(", "self", ",", "\n", "hparams", ",", "\n", "mode", ",", "\n", "iterator", ",", "\n", "source_vocab_table", ",", "\n", "target_vocab_table", ",", "\n", "reverse_target_vocab_table", "=", "None", ",", "\n", "scope", "=", "None", ",", "\n", "single_cell_fn", "=", "None", ")", ":", "\n", "    ", "super", "(", "GNMTModel", ",", "self", ")", ".", "__init__", "(", "\n", "hparams", "=", "hparams", ",", "\n", "mode", "=", "mode", ",", "\n", "iterator", "=", "iterator", ",", "\n", "source_vocab_table", "=", "source_vocab_table", ",", "\n", "target_vocab_table", "=", "target_vocab_table", ",", "\n", "reverse_target_vocab_table", "=", "reverse_target_vocab_table", ",", "\n", "scope", "=", "scope", ",", "\n", "single_cell_fn", "=", "single_cell_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.gnmt_model.GNMTModel._build_encoder": [[59, 125], ["misc_utils.print_out", "misc_utils.print_out", "super()._build_encoder", "ValueError", "tensorflow.transpose", "tensorflow.variable_scope", "tensorflow.nn.embedding_lookup", "gnmt_model.GNMTModel._build_bidirectional_rnn", "model_helper.create_rnn_cell", "tensorflow.nn.dynamic_rnn"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.gnmt_model.GNMTModel._build_encoder", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.Model._build_bidirectional_rnn", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_rnn_cell"], ["", "def", "_build_encoder", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "\"\"\"Build a GNMT encoder.\"\"\"", "\n", "if", "hparams", ".", "encoder_type", "==", "\"uni\"", "or", "hparams", ".", "encoder_type", "==", "\"bi\"", ":", "\n", "      ", "return", "super", "(", "GNMTModel", ",", "self", ")", ".", "_build_encoder", "(", "hparams", ")", "\n", "\n", "", "if", "hparams", ".", "encoder_type", "!=", "\"gnmt\"", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unknown encoder_type %s\"", "%", "hparams", ".", "encoder_type", ")", "\n", "\n", "# Build GNMT encoder.", "\n", "", "num_layers", "=", "hparams", ".", "num_layers", "\n", "num_residual_layers", "=", "hparams", ".", "num_residual_layers", "\n", "num_bi_layers", "=", "1", "\n", "num_uni_layers", "=", "num_layers", "-", "num_bi_layers", "\n", "utils", ".", "print_out", "(", "\"  num_bi_layers = %d\"", "%", "num_bi_layers", ")", "\n", "utils", ".", "print_out", "(", "\"  num_uni_layers = %d\"", "%", "num_uni_layers", ")", "\n", "\n", "iterator", "=", "self", ".", "iterator", "\n", "source", "=", "iterator", ".", "source", "\n", "if", "self", ".", "time_major", ":", "\n", "      ", "source", "=", "tf", ".", "transpose", "(", "source", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ")", "as", "scope", ":", "\n", "      ", "dtype", "=", "scope", ".", "dtype", "\n", "\n", "# Look up embedding, emp_inp: [max_time, batch_size, num_units]", "\n", "#   when time_major = True", "\n", "encoder_emb_inp", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "embedding_encoder", ",", "\n", "source", ")", "\n", "\n", "# Execute _build_bidirectional_rnn from Model class", "\n", "bi_encoder_outputs", ",", "bi_encoder_state", "=", "self", ".", "_build_bidirectional_rnn", "(", "\n", "inputs", "=", "encoder_emb_inp", ",", "\n", "sequence_length", "=", "iterator", ".", "source_sequence_length", ",", "\n", "dtype", "=", "dtype", ",", "\n", "hparams", "=", "hparams", ",", "\n", "num_bi_layers", "=", "num_bi_layers", ",", "\n", "num_bi_residual_layers", "=", "0", ",", "# no residual connection", "\n", ")", "\n", "\n", "uni_cell", "=", "model_helper", ".", "create_rnn_cell", "(", "\n", "unit_type", "=", "hparams", ".", "unit_type", ",", "\n", "num_units", "=", "hparams", ".", "num_units", ",", "\n", "num_layers", "=", "num_uni_layers", ",", "\n", "num_residual_layers", "=", "num_residual_layers", ",", "\n", "forget_bias", "=", "hparams", ".", "forget_bias", ",", "\n", "dropout", "=", "hparams", ".", "dropout", ",", "\n", "num_gpus", "=", "hparams", ".", "num_gpus", ",", "\n", "base_gpu", "=", "1", ",", "\n", "mode", "=", "self", ".", "mode", ",", "\n", "single_cell_fn", "=", "self", ".", "single_cell_fn", ")", "\n", "\n", "# encoder_outputs: size [max_time, batch_size, num_units]", "\n", "#   when time_major = True", "\n", "encoder_outputs", ",", "encoder_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "\n", "uni_cell", ",", "\n", "bi_encoder_outputs", ",", "\n", "dtype", "=", "dtype", ",", "\n", "sequence_length", "=", "iterator", ".", "source_sequence_length", ",", "\n", "time_major", "=", "self", ".", "time_major", ")", "\n", "\n", "# Pass all encoder state except the first bi-directional layer's state to", "\n", "# decoder.", "\n", "encoder_state", "=", "(", "bi_encoder_state", "[", "1", "]", ",", ")", "+", "(", "\n", "(", "encoder_state", ",", ")", "if", "num_uni_layers", "==", "1", "else", "encoder_state", ")", "\n", "\n", "", "return", "encoder_outputs", ",", "encoder_state", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.gnmt_model.GNMTModel._build_decoder_cell": [[126, 203], ["attention_model.create_attention_mechanism", "model_helper._cell_list", "model_helper._cell_list.pop", "tensorflow.contrib.seq2seq.AttentionWrapper", "tensorflow.transpose", "tensorflow.contrib.seq2seq.tile_batch", "tensorflow.contrib.seq2seq.tile_batch", "tensorflow.contrib.seq2seq.tile_batch", "gnmt_model.GNMTAttentionMultiCell", "tuple", "GNMTAttentionMultiCell.zero_state", "gnmt_model.GNMTAttentionMultiCell", "ValueError", "isinstance", "zs.clone", "zip", "GNMTAttentionMultiCell.zero_state"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model.create_attention_mechanism", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper._cell_list", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.zero_state", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.zero_state"], ["", "def", "_build_decoder_cell", "(", "self", ",", "hparams", ",", "encoder_outputs", ",", "encoder_state", ",", "\n", "source_sequence_length", ")", ":", "\n", "    ", "\"\"\"Build a RNN cell with GNMT attention architecture.\"\"\"", "\n", "attention_option", "=", "hparams", ".", "attention", "\n", "attention_architecture", "=", "hparams", ".", "attention_architecture", "\n", "num_units", "=", "hparams", ".", "num_units", "\n", "num_layers", "=", "hparams", ".", "num_layers", "\n", "num_residual_layers", "=", "hparams", ".", "num_residual_layers", "\n", "beam_width", "=", "hparams", ".", "beam_width", "\n", "\n", "dtype", "=", "tf", ".", "float32", "\n", "\n", "if", "self", ".", "time_major", ":", "\n", "      ", "memory", "=", "tf", ".", "transpose", "(", "encoder_outputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "", "else", ":", "\n", "      ", "memory", "=", "encoder_outputs", "\n", "\n", "", "if", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", "and", "beam_width", ">", "0", ":", "\n", "      ", "memory", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "tile_batch", "(", "\n", "memory", ",", "multiplier", "=", "beam_width", ")", "\n", "source_sequence_length", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "tile_batch", "(", "\n", "source_sequence_length", ",", "multiplier", "=", "beam_width", ")", "\n", "encoder_state", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "tile_batch", "(", "\n", "encoder_state", ",", "multiplier", "=", "beam_width", ")", "\n", "batch_size", "=", "self", ".", "batch_size", "*", "beam_width", "\n", "", "else", ":", "\n", "      ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "attention_mechanism", "=", "attention_model", ".", "create_attention_mechanism", "(", "\n", "attention_option", ",", "num_units", ",", "memory", ",", "source_sequence_length", ")", "\n", "\n", "cell_list", "=", "model_helper", ".", "_cell_list", "(", "# pylint: disable=protected-access", "\n", "unit_type", "=", "hparams", ".", "unit_type", ",", "\n", "num_units", "=", "num_units", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "num_residual_layers", "=", "num_residual_layers", ",", "\n", "forget_bias", "=", "hparams", ".", "forget_bias", ",", "\n", "dropout", "=", "hparams", ".", "dropout", ",", "\n", "num_gpus", "=", "hparams", ".", "num_gpus", ",", "\n", "mode", "=", "self", ".", "mode", ",", "\n", "single_cell_fn", "=", "self", ".", "single_cell_fn", ")", "\n", "\n", "# Only wrap the bottom layer with the attention mechanism.", "\n", "attention_cell", "=", "cell_list", ".", "pop", "(", "0", ")", "\n", "\n", "# Only generate alignment in greedy INFER mode.", "\n", "alignment_history", "=", "(", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", "and", "\n", "beam_width", "==", "0", ")", "\n", "attention_cell", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "AttentionWrapper", "(", "\n", "attention_cell", ",", "\n", "attention_mechanism", ",", "\n", "attention_layer_size", "=", "None", ",", "# don't use attenton layer.", "\n", "output_attention", "=", "False", ",", "\n", "alignment_history", "=", "alignment_history", ",", "\n", "name", "=", "\"attention\"", ")", "\n", "\n", "if", "attention_architecture", "==", "\"gnmt\"", ":", "\n", "      ", "cell", "=", "GNMTAttentionMultiCell", "(", "\n", "attention_cell", ",", "cell_list", ")", "\n", "", "elif", "attention_architecture", "==", "\"gnmt_v2\"", ":", "\n", "      ", "cell", "=", "GNMTAttentionMultiCell", "(", "\n", "attention_cell", ",", "cell_list", ",", "use_new_attention", "=", "True", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"Unknown attention_architecture %s\"", "%", "attention_architecture", ")", "\n", "\n", "\n", "", "if", "hparams", ".", "pass_hidden_state", ":", "\n", "      ", "decoder_initial_state", "=", "tuple", "(", "\n", "zs", ".", "clone", "(", "cell_state", "=", "es", ")", "\n", "if", "isinstance", "(", "zs", ",", "tf", ".", "contrib", ".", "seq2seq", ".", "AttentionWrapperState", ")", "else", "es", "\n", "for", "zs", ",", "es", "in", "zip", "(", "\n", "cell", ".", "zero_state", "(", "batch_size", ",", "dtype", ")", ",", "encoder_state", ")", ")", "\n", "", "else", ":", "\n", "      ", "decoder_initial_state", "=", "cell", ".", "zero_state", "(", "batch_size", ",", "dtype", ")", "\n", "\n", "", "return", "cell", ",", "decoder_initial_state", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.gnmt_model.GNMTModel._get_infer_summary": [[204, 209], ["attention_model._create_attention_images_summary", "tensorflow.no_op"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model._create_attention_images_summary"], ["", "def", "_get_infer_summary", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "if", "hparams", ".", "beam_width", ">", "0", ":", "\n", "      ", "return", "tf", ".", "no_op", "(", ")", "\n", "", "return", "attention_model", ".", "_create_attention_images_summary", "(", "\n", "self", ".", "final_context_state", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.gnmt_model.GNMTAttentionMultiCell.__init__": [[214, 226], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__"], ["def", "__init__", "(", "self", ",", "attention_cell", ",", "cells", ",", "use_new_attention", "=", "False", ")", ":", "\n", "    ", "\"\"\"Creates a GNMTAttentionMultiCell.\n\n    Args:\n      attention_cell: An instance of AttentionWrapper.\n      cells: A list of RNNCell wrapped with AttentionInputWrapper.\n      use_new_attention: Whether to use the attention generated from current\n        step bottom layer's output. Default is False.\n    \"\"\"", "\n", "cells", "=", "[", "attention_cell", "]", "+", "cells", "\n", "self", ".", "use_new_attention", "=", "use_new_attention", "\n", "super", "(", "GNMTAttentionMultiCell", ",", "self", ")", ".", "__init__", "(", "cells", ",", "state_is_tuple", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.gnmt_model.GNMTAttentionMultiCell.__call__": [[227, 263], ["tensorflow.python.util.nest.is_sequence", "ValueError", "tensorflow.variable_scope", "range", "tuple", "tensorflow.variable_scope", "attention_cell", "new_states.append", "len", "tensorflow.variable_scope", "cell", "new_states.append", "len", "isinstance", "TypeError", "cur_state._replace._replace._replace", "cur_state._replace._replace._replace", "tensorflow.concat", "tensorflow.concat"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "state", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"Run the cell with bottom layer's attention copied to all upper layers.\"\"\"", "\n", "if", "not", "nest", ".", "is_sequence", "(", "state", ")", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"Expected state to be a tuple of length %d, but received: %s\"", "\n", "%", "(", "len", "(", "self", ".", "state_size", ")", ",", "state", ")", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", "or", "\"multi_rnn_cell\"", ")", ":", "\n", "      ", "new_states", "=", "[", "]", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"cell_0_attention\"", ")", ":", "\n", "        ", "attention_cell", "=", "self", ".", "_cells", "[", "0", "]", "\n", "attention_state", "=", "state", "[", "0", "]", "\n", "cur_inp", ",", "new_attention_state", "=", "attention_cell", "(", "inputs", ",", "attention_state", ")", "\n", "new_states", ".", "append", "(", "new_attention_state", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "1", ",", "len", "(", "self", ".", "_cells", ")", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"cell_%d\"", "%", "i", ")", ":", "\n", "\n", "          ", "cell", "=", "self", ".", "_cells", "[", "i", "]", "\n", "cur_state", "=", "state", "[", "i", "]", "\n", "\n", "if", "not", "isinstance", "(", "cur_state", ",", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"`state[{}]` must be a LSTMStateTuple\"", ".", "format", "(", "i", ")", ")", "\n", "\n", "", "if", "self", ".", "use_new_attention", ":", "\n", "            ", "cur_state", "=", "cur_state", ".", "_replace", "(", "h", "=", "tf", ".", "concat", "(", "\n", "[", "cur_state", ".", "h", ",", "new_attention_state", ".", "attention", "]", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "cur_state", "=", "cur_state", ".", "_replace", "(", "h", "=", "tf", ".", "concat", "(", "\n", "[", "cur_state", ".", "h", ",", "attention_state", ".", "attention", "]", ",", "1", ")", ")", "\n", "\n", "", "cur_inp", ",", "new_state", "=", "cell", "(", "cur_inp", ",", "cur_state", ")", "\n", "new_states", ".", "append", "(", "new_state", ")", "\n", "\n", "", "", "", "return", "cur_inp", ",", "tuple", "(", "new_states", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model.AttentionModel.__init__": [[41, 61], ["model.Model.__init__", "attention_model.AttentionModel._get_infer_summary"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model.AttentionModel._get_infer_summary"], ["def", "__init__", "(", "self", ",", "\n", "hparams", ",", "\n", "mode", ",", "\n", "iterator", ",", "\n", "source_vocab_table", ",", "\n", "target_vocab_table", ",", "\n", "reverse_target_vocab_table", "=", "None", ",", "\n", "scope", "=", "None", ",", "\n", "single_cell_fn", "=", "None", ")", ":", "\n", "    ", "super", "(", "AttentionModel", ",", "self", ")", ".", "__init__", "(", "\n", "hparams", "=", "hparams", ",", "\n", "mode", "=", "mode", ",", "\n", "iterator", "=", "iterator", ",", "\n", "source_vocab_table", "=", "source_vocab_table", ",", "\n", "target_vocab_table", "=", "target_vocab_table", ",", "\n", "reverse_target_vocab_table", "=", "reverse_target_vocab_table", ",", "\n", "scope", "=", "scope", ",", "\n", "single_cell_fn", "=", "single_cell_fn", ")", "\n", "if", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ":", "\n", "      ", "self", ".", "infer_summary", "=", "self", ".", "_get_infer_summary", "(", "hparams", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model.AttentionModel._build_decoder_cell": [[62, 161], ["ValueError", "tensorflow.transpose", "tensorflow.contrib.seq2seq.tile_batch", "tensorflow.contrib.seq2seq.tile_batch", "tensorflow.contrib.seq2seq.tile_batch", "tensorflow.contrib.layers.fully_connected", "ntm.NTMCell", "tensorflow.contrib.rnn.DeviceWrapper.zero_state", "attention_model.create_attention_mechanism", "model_helper.create_rnn_cell", "tensorflow.contrib.seq2seq.AttentionWrapper", "tensorflow.contrib.rnn.DeviceWrapper", "tuple", "model_helper.get_device_str", "tensorflow.contrib.rnn.DeviceWrapper.zero_state().clone", "tensorflow.contrib.rnn.DeviceWrapper.zero_state", "tensorflow.random_uniform_initializer", "list", "tensorflow.contrib.rnn.DeviceWrapper.zero_state"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.zero_state", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model.create_attention_mechanism", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.create_rnn_cell", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model_helper.get_device_str", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.zero_state", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.zero_state"], ["", "", "def", "_build_decoder_cell", "(", "self", ",", "hparams", ",", "encoder_outputs", ",", "encoder_state", ",", "\n", "source_sequence_length", ")", ":", "\n", "    ", "\"\"\"Build a RNN cell with attention mechanism that can be used by decoder.\"\"\"", "\n", "attention_option", "=", "hparams", ".", "attention", "\n", "attention_architecture", "=", "hparams", ".", "attention_architecture", "\n", "\n", "if", "attention_architecture", "!=", "\"standard\"", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"Unknown attention architecture %s\"", "%", "attention_architecture", ")", "\n", "\n", "", "num_units", "=", "hparams", ".", "num_units", "\n", "num_layers", "=", "hparams", ".", "num_layers", "\n", "num_residual_layers", "=", "hparams", ".", "num_residual_layers", "\n", "num_gpus", "=", "hparams", ".", "num_gpus", "\n", "beam_width", "=", "hparams", ".", "beam_width", "\n", "\n", "dtype", "=", "tf", ".", "float32", "\n", "\n", "if", "self", ".", "time_major", ":", "\n", "      ", "memory", "=", "tf", ".", "transpose", "(", "encoder_outputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "", "else", ":", "\n", "      ", "memory", "=", "encoder_outputs", "\n", "\n", "", "if", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", "and", "beam_width", ">", "0", ":", "\n", "      ", "memory", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "tile_batch", "(", "\n", "memory", ",", "multiplier", "=", "beam_width", ")", "\n", "source_sequence_length", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "tile_batch", "(", "\n", "source_sequence_length", ",", "multiplier", "=", "beam_width", ")", "\n", "encoder_state", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "tile_batch", "(", "\n", "encoder_state", ",", "multiplier", "=", "beam_width", ")", "\n", "batch_size", "=", "self", ".", "batch_size", "*", "beam_width", "\n", "", "else", ":", "\n", "      ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "if", "hparams", ".", "model", "in", "(", "'model0'", ",", "'model1'", ",", "'model2'", ")", ":", "\n", "      ", "att_memory", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "memory", ",", "num_units", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "0.1", ",", "0.1", ")", ")", "\n", "\n", "cell", "=", "NTMCell", "(", "num_layers", ",", "\n", "num_units", ",", "\n", "use_att_memory", "=", "True", ",", "\n", "att_memory", "=", "att_memory", ",", "\n", "att_memory_size", "=", "hparams", ".", "src_max_len", ",", "\n", "att_memory_vector_dim", "=", "num_units", ",", "\n", "use_ext_memory", "=", "(", "hparams", ".", "model", "==", "'model2'", ")", ",", "\n", "ext_memory_size", "=", "hparams", ".", "num_memory_locations", "if", "hparams", ".", "model", "==", "'model2'", "else", "None", ",", "\n", "ext_memory_vector_dim", "=", "hparams", ".", "memory_unit_size", "if", "hparams", ".", "model", "==", "'model2'", "else", "None", ",", "\n", "ext_read_head_num", "=", "hparams", ".", "read_heads", "if", "hparams", ".", "model", "==", "'model2'", "else", "None", ",", "\n", "ext_write_head_num", "=", "hparams", ".", "write_heads", "if", "hparams", ".", "model", "==", "'model2'", "else", "None", ",", "\n", "dropout", "=", "hparams", ".", "dropout", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "mode", "=", "self", ".", "mode", ",", "\n", "output_dim", "=", "num_units", ",", "\n", "addressing_mode", "=", "'content'", "if", "hparams", ".", "model", "==", "'model0'", "else", "'content_and_location'", ")", "\n", "\n", "decoder_initial_state", "=", "cell", ".", "zero_state", "(", "batch_size", ",", "dtype", ")", "\n", "\n", "if", "hparams", ".", "pass_hidden_state", ":", "\n", "        ", "decoder_initial_state", "=", "tuple", "(", "[", "encoder_state", "]", "+", "list", "(", "decoder_initial_state", "[", "1", ":", "]", ")", ")", "\n", "", "", "else", ":", "\n", "      ", "attention_mechanism", "=", "create_attention_mechanism", "(", "\n", "attention_option", ",", "num_units", ",", "memory", ",", "source_sequence_length", ")", "\n", "\n", "cell", "=", "model_helper", ".", "create_rnn_cell", "(", "\n", "unit_type", "=", "hparams", ".", "unit_type", ",", "\n", "num_units", "=", "num_units", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "num_residual_layers", "=", "num_residual_layers", ",", "\n", "forget_bias", "=", "hparams", ".", "forget_bias", ",", "\n", "dropout", "=", "hparams", ".", "dropout", ",", "\n", "num_gpus", "=", "num_gpus", ",", "\n", "mode", "=", "self", ".", "mode", ",", "\n", "single_cell_fn", "=", "self", ".", "single_cell_fn", ",", "\n", "num_proj", "=", "None", ",", "\n", "num_cells", "=", "2", "if", "(", "hparams", ".", "encoder_type", "==", "\"bi\"", ")", "else", "1", ")", "\n", "\n", "# Only generate alignment in greedy INFER mode.", "\n", "alignment_history", "=", "(", "self", ".", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", "and", "\n", "beam_width", "==", "0", ")", "\n", "cell", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "AttentionWrapper", "(", "\n", "cell", ",", "\n", "attention_mechanism", ",", "\n", "attention_layer_size", "=", "num_units", ",", "\n", "alignment_history", "=", "alignment_history", ",", "\n", "name", "=", "\"attention\"", ")", "\n", "\n", "# TODO(thangluong): do we need num_layers, num_gpus?", "\n", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "DeviceWrapper", "(", "cell", ",", "\n", "model_helper", ".", "get_device_str", "(", "\n", "num_layers", "-", "1", ",", "num_gpus", ")", ")", "\n", "\n", "if", "hparams", ".", "pass_hidden_state", ":", "\n", "        ", "decoder_initial_state", "=", "cell", ".", "zero_state", "(", "batch_size", ",", "dtype", ")", ".", "clone", "(", "\n", "cell_state", "=", "encoder_state", ")", "\n", "", "else", ":", "\n", "        ", "decoder_initial_state", "=", "cell", ".", "zero_state", "(", "batch_size", ",", "dtype", ")", "\n", "\n", "", "", "return", "cell", ",", "decoder_initial_state", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model.AttentionModel._get_infer_summary": [[162, 166], ["attention_model._create_attention_images_summary", "tensorflow.no_op"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model._create_attention_images_summary"], ["", "def", "_get_infer_summary", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "if", "hparams", ".", "beam_width", ">", "0", "or", "hparams", ".", "model", "!=", "'none'", ":", "\n", "      ", "return", "tf", ".", "no_op", "(", ")", "\n", "", "return", "_create_attention_images_summary", "(", "self", ".", "final_context_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model.create_attention_mechanism": [[168, 194], ["tensorflow.contrib.seq2seq.LuongAttention", "tensorflow.contrib.seq2seq.LuongAttention", "tensorflow.contrib.seq2seq.BahdanauAttention", "tensorflow.contrib.seq2seq.BahdanauAttention", "ValueError"], "function", ["None"], ["", "", "def", "create_attention_mechanism", "(", "attention_option", ",", "num_units", ",", "memory", ",", "\n", "source_sequence_length", ")", ":", "\n", "  ", "\"\"\"Create attention mechanism based on the attention_option.\"\"\"", "\n", "# Mechanism", "\n", "if", "attention_option", "==", "\"luong\"", ":", "\n", "    ", "attention_mechanism", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "LuongAttention", "(", "\n", "num_units", ",", "memory", ",", "memory_sequence_length", "=", "source_sequence_length", ")", "\n", "", "elif", "attention_option", "==", "\"scaled_luong\"", ":", "\n", "    ", "attention_mechanism", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "LuongAttention", "(", "\n", "num_units", ",", "\n", "memory", ",", "\n", "memory_sequence_length", "=", "source_sequence_length", ",", "\n", "scale", "=", "True", ")", "\n", "", "elif", "attention_option", "==", "\"bahdanau\"", ":", "\n", "    ", "attention_mechanism", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "BahdanauAttention", "(", "\n", "num_units", ",", "memory", ",", "memory_sequence_length", "=", "source_sequence_length", ")", "\n", "", "elif", "attention_option", "==", "\"normed_bahdanau\"", ":", "\n", "    ", "attention_mechanism", "=", "tf", ".", "contrib", ".", "seq2seq", ".", "BahdanauAttention", "(", "\n", "num_units", ",", "\n", "memory", ",", "\n", "memory_sequence_length", "=", "source_sequence_length", ",", "\n", "normalize", "=", "True", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unknown attention option %s\"", "%", "attention_option", ")", "\n", "\n", "", "return", "attention_mechanism", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.attention_model._create_attention_images_summary": [[196, 206], ["final_context_state.alignment_history.stack", "tensorflow.expand_dims", "tensorflow.summary.image", "tensorflow.transpose"], "function", ["None"], ["", "def", "_create_attention_images_summary", "(", "final_context_state", ")", ":", "\n", "  ", "\"\"\"create attention image and attention summary.\"\"\"", "\n", "attention_images", "=", "(", "final_context_state", ".", "alignment_history", ".", "stack", "(", ")", ")", "\n", "# Reshape to (batch, src_seq_len, tgt_seq_len,1)", "\n", "attention_images", "=", "tf", ".", "expand_dims", "(", "\n", "tf", ".", "transpose", "(", "attention_images", ",", "[", "1", ",", "2", ",", "0", "]", ")", ",", "-", "1", ")", "\n", "# Scale to range [0, 255]", "\n", "attention_images", "*=", "255", "\n", "attention_summary", "=", "tf", ".", "summary", ".", "image", "(", "\"attention_images\"", ",", "attention_images", ")", "\n", "return", "attention_summary", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dnc.DNC.__init__": [[44, 107], ["sonnet.RNNCore.__init__", "numpy.prod", "tensorflow.TensorShape", "DNCState", "dnc.DNC._enter_variable_scope", "tensorflow.contrib.rnn.MultiRNNCell", "access.MemoryAccess", "dnc.DNC._access.output_size.as_list", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.TensorShape", "access.AccessState", "tensorflow.contrib.rnn.DropoutWrapper", "dnc.DNC.__init__.single_cell"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__"], ["def", "__init__", "(", "self", ",", "\n", "access_config", ",", "\n", "controller_config", ",", "\n", "output_size", ",", "\n", "clip_value", "=", "None", ",", "\n", "dropout", "=", "0.0", ",", "\n", "mode", "=", "None", ",", "\n", "batch_size", "=", "None", ",", "\n", "name", "=", "'dnc'", ")", ":", "\n", "    ", "\"\"\"Initializes the DNC core.\n\n    Args:\n      access_config: dictionary of access module configurations.\n      controller_config: dictionary of controller (LSTM) module configurations.\n      output_size: output dimension size of core.\n      clip_value: clips controller and core output values to between\n          `[-clip_value, clip_value]` if specified.\n      name: module name (default 'dnc').\n\n    Raises:\n      TypeError: if direct_input_size is not None for any access module other\n        than KeyValueMemory.\n    \"\"\"", "\n", "super", "(", "DNC", ",", "self", ")", ".", "__init__", "(", "name", "=", "name", ")", "\n", "self", ".", "dropout", "=", "dropout", "if", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", "else", "0.0", "\n", "\n", "self", ".", "access_config", "=", "access_config", "\n", "\n", "with", "self", ".", "_enter_variable_scope", "(", ")", ":", "\n", "      ", "def", "single_cell", "(", "num_units", ")", ":", "\n", "        ", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", ",", "forget_bias", "=", "1.0", ")", "\n", "if", "self", ".", "dropout", ">", "0.0", ":", "\n", "          ", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "cell", "=", "cell", ",", "input_keep_prob", "=", "(", "1.0", "-", "self", ".", "dropout", ")", ")", "\n", "", "return", "cell", "\n", "\n", "", "self", ".", "_controller", "=", "tf", ".", "contrib", ".", "rnn", ".", "MultiRNNCell", "(", "[", "single_cell", "(", "controller_config", "[", "'num_units'", "]", ")", "for", "_", "in", "range", "(", "controller_config", "[", "'num_layers'", "]", ")", "]", ")", "\n", "self", ".", "_access", "=", "access", ".", "MemoryAccess", "(", "**", "access_config", ")", "\n", "\n", "", "self", ".", "_access_output_size", "=", "np", ".", "prod", "(", "self", ".", "_access", ".", "output_size", ".", "as_list", "(", ")", ")", "\n", "self", ".", "_output_size", "=", "output_size", "\n", "self", ".", "_clip_value", "=", "clip_value", "or", "0", "\n", "\n", "self", ".", "_output_size", "=", "tf", ".", "TensorShape", "(", "[", "output_size", "]", ")", "\n", "# self._state_size = DNCState(", "\n", "#     access_output=self._access_output_size,", "\n", "#     access_state=self._access.state_size,", "\n", "#     controller_state=self._controller.state_size)", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "self", ".", "_state_size", "=", "DNCState", "(", "\n", "access_output", "=", "tf", ".", "TensorShape", "(", "(", "self", ".", "access_config", "[", "'word_size'", "]", ")", ")", ",", "\n", "access_state", "=", "access", ".", "AccessState", "(", "\n", "memory", "=", "tf", ".", "TensorShape", "(", "(", "self", ".", "access_config", "[", "'memory_size'", "]", "*", "self", ".", "access_config", "[", "'word_size'", "]", ")", ")", ",", "\n", "read_weights", "=", "tf", ".", "TensorShape", "(", "(", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "write_weights", "=", "tf", ".", "TensorShape", "(", "(", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "linkage", "=", "TemporalLinkageState", "(", "\n", "link", "=", "tf", ".", "TensorShape", "(", "(", "self", ".", "access_config", "[", "'memory_size'", "]", "*", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "precedence_weights", "=", "tf", ".", "TensorShape", "(", "(", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", "\n", ")", ",", "\n", "usage", "=", "self", ".", "_access", ".", "state_size", ".", "usage", "\n", ")", ",", "\n", "controller_state", "=", "self", ".", "_controller", ".", "state_size", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dnc.DNC._clip_if_enabled": [[109, 114], ["tensorflow.clip_by_value"], "methods", ["None"], ["", "def", "_clip_if_enabled", "(", "self", ",", "x", ")", ":", "\n", "    ", "if", "self", ".", "_clip_value", ">", "0", ":", "\n", "      ", "return", "tf", ".", "clip_by_value", "(", "x", ",", "-", "self", ".", "_clip_value", ",", "self", ".", "_clip_value", ")", "\n", "", "else", ":", "\n", "      ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dnc.DNC._build": [[115, 193], ["DNCState", "sonnet.BatchFlatten", "tensorflow.concat", "dnc.DNC._controller", "dnc.DNC._clip_if_enabled", "sonnet.nest.map", "dnc.DNC._access", "tensorflow.concat", "dnc.DNC._clip_if_enabled", "DNCState", "DNCState", "sonnet.Linear", "tensorflow.nn.dropout", "tensorflow.reshape", "access.AccessState", "sonnet.BatchFlatten.", "sonnet.BatchFlatten.", "sonnet.BatchFlatten.", "tensorflow.reshape", "access.AccessState", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "addressing.TemporalLinkageState", "dnc.DNC._output_size.as_list", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "addressing.TemporalLinkageState", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dnc.DNC._clip_if_enabled", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dnc.DNC._clip_if_enabled"], ["", "", "def", "_build", "(", "self", ",", "inputs", ",", "prev_state", ")", ":", "\n", "    ", "\"\"\"Connects the DNC core into the graph.\n\n    Args:\n      inputs: Tensor input.\n      prev_state: A `DNCState` tuple containing the fields `access_output`,\n          `access_state` and `controller_state`. `access_state` is a 3-D Tensor\n          of shape `[batch_size, num_reads, word_size]` containing read words.\n          `access_state` is a tuple of the access module's state, and\n          `controller_state` is a tuple of controller module's state.\n\n    Returns:\n      A tuple `(output, next_state)` where `output` is a tensor and `next_state`\n      is a `DNCState` tuple containing the fields `access_output`,\n      `access_state`, and `controller_state`.\n    \"\"\"", "\n", "prev_state", "=", "DNCState", "(", "\n", "access_output", "=", "tf", ".", "reshape", "(", "prev_state", ".", "access_output", ",", "(", "-", "1", ",", "1", ",", "self", ".", "access_config", "[", "'word_size'", "]", ")", ")", ",", "\n", "access_state", "=", "access", ".", "AccessState", "(", "\n", "memory", "=", "tf", ".", "reshape", "(", "prev_state", ".", "access_state", ".", "memory", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ",", "self", ".", "access_config", "[", "'word_size'", "]", ")", ")", ",", "\n", "read_weights", "=", "tf", ".", "reshape", "(", "prev_state", ".", "access_state", ".", "read_weights", ",", "(", "-", "1", ",", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "write_weights", "=", "tf", ".", "reshape", "(", "prev_state", ".", "access_state", ".", "read_weights", ",", "(", "-", "1", ",", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "linkage", "=", "TemporalLinkageState", "(", "\n", "link", "=", "tf", ".", "reshape", "(", "prev_state", ".", "access_state", ".", "linkage", ".", "link", ",", "(", "-", "1", ",", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "precedence_weights", "=", "tf", ".", "reshape", "(", "prev_state", ".", "access_state", ".", "linkage", ".", "precedence_weights", ",", "(", "-", "1", ",", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", "\n", ")", ",", "\n", "usage", "=", "prev_state", ".", "access_state", ".", "usage", "\n", ")", ",", "\n", "controller_state", "=", "prev_state", ".", "controller_state", "\n", ")", "\n", "\n", "prev_access_output", "=", "prev_state", ".", "access_output", "\n", "prev_access_state", "=", "prev_state", ".", "access_state", "\n", "prev_controller_state", "=", "prev_state", ".", "controller_state", "\n", "\n", "batch_flatten", "=", "snt", ".", "BatchFlatten", "(", ")", "\n", "controller_input", "=", "tf", ".", "concat", "(", "\n", "[", "batch_flatten", "(", "inputs", ")", ",", "batch_flatten", "(", "prev_access_output", ")", "]", ",", "1", ")", "\n", "\n", "controller_output", ",", "controller_state", "=", "self", ".", "_controller", "(", "\n", "controller_input", ",", "prev_controller_state", ")", "\n", "\n", "controller_output", "=", "self", ".", "_clip_if_enabled", "(", "controller_output", ")", "\n", "controller_state", "=", "snt", ".", "nest", ".", "map", "(", "self", ".", "_clip_if_enabled", ",", "controller_state", ")", "\n", "\n", "access_output", ",", "access_state", "=", "self", ".", "_access", "(", "controller_output", ",", "\n", "prev_access_state", ")", "\n", "\n", "output", "=", "tf", ".", "concat", "(", "[", "controller_output", ",", "batch_flatten", "(", "access_output", ")", "]", ",", "1", ")", "\n", "output", "=", "snt", ".", "Linear", "(", "\n", "output_size", "=", "self", ".", "_output_size", ".", "as_list", "(", ")", "[", "0", "]", ",", "\n", "name", "=", "'output_linear'", ")", "(", "output", ")", "\n", "output", "=", "self", ".", "_clip_if_enabled", "(", "output", ")", "\n", "\n", "if", "self", ".", "dropout", ">", "0.0", ":", "\n", "      ", "output", "=", "tf", ".", "nn", ".", "dropout", "(", "output", ",", "1", "-", "self", ".", "dropout", ")", "\n", "\n", "", "state", "=", "DNCState", "(", "\n", "access_output", "=", "access_output", ",", "\n", "access_state", "=", "access_state", ",", "\n", "controller_state", "=", "controller_state", ")", "\n", "\n", "state", "=", "DNCState", "(", "\n", "access_output", "=", "tf", ".", "reshape", "(", "state", ".", "access_output", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'word_size'", "]", ")", ")", ",", "\n", "access_state", "=", "access", ".", "AccessState", "(", "\n", "memory", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "memory", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", "*", "self", ".", "access_config", "[", "'word_size'", "]", ")", ")", ",", "\n", "read_weights", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "read_weights", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "write_weights", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "read_weights", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "linkage", "=", "TemporalLinkageState", "(", "\n", "link", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "linkage", ".", "link", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", "*", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "precedence_weights", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "linkage", ".", "precedence_weights", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", "\n", ")", ",", "\n", "usage", "=", "state", ".", "access_state", ".", "usage", "\n", ")", ",", "\n", "controller_state", "=", "state", ".", "controller_state", "\n", ")", "\n", "\n", "return", "output", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dnc.DNC.zero_state": [[194, 218], ["DNCState", "DNCState", "dnc.DNC._access.initial_state", "tensorflow.zeros", "tensorflow.reshape", "access.AccessState", "hasattr", "dnc.DNC._controller.initial_state", "dnc.DNC._controller.zero_state", "dnc.DNC._access.output_size.as_list", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "addressing.TemporalLinkageState", "tensorflow.reshape", "tensorflow.reshape"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.zero_state"], ["", "def", "zero_state", "(", "self", ",", "batch_size", ",", "dtype", "=", "tf", ".", "float32", ")", ":", "\n", "    ", "state", "=", "DNCState", "(", "\n", "controller_state", "=", "self", ".", "_controller", ".", "initial_state", "(", "batch_size", ",", "dtype", ")", "\n", "if", "hasattr", "(", "self", ".", "_controller", ",", "'initial_state'", ")", "else", "self", ".", "_controller", ".", "zero_state", "(", "batch_size", ",", "dtype", ")", ",", "\n", "access_state", "=", "self", ".", "_access", ".", "initial_state", "(", "batch_size", ",", "dtype", ")", ",", "\n", "access_output", "=", "tf", ".", "zeros", "(", "\n", "[", "batch_size", "]", "+", "self", ".", "_access", ".", "output_size", ".", "as_list", "(", ")", ",", "dtype", ")", ")", "\n", "\n", "state", "=", "DNCState", "(", "\n", "access_output", "=", "tf", ".", "reshape", "(", "state", ".", "access_output", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'word_size'", "]", ")", ")", ",", "\n", "access_state", "=", "access", ".", "AccessState", "(", "\n", "memory", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "memory", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", "*", "self", ".", "access_config", "[", "'word_size'", "]", ")", ")", ",", "\n", "read_weights", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "read_weights", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "write_weights", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "read_weights", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "linkage", "=", "TemporalLinkageState", "(", "\n", "link", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "linkage", ".", "link", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", "*", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", ",", "\n", "precedence_weights", "=", "tf", ".", "reshape", "(", "state", ".", "access_state", ".", "linkage", ".", "precedence_weights", ",", "(", "-", "1", ",", "self", ".", "access_config", "[", "'memory_size'", "]", ")", ")", "\n", ")", ",", "\n", "usage", "=", "state", ".", "access_state", ".", "usage", "\n", ")", ",", "\n", "controller_state", "=", "state", ".", "controller_state", "\n", ")", "\n", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dnc.DNC.state_size": [[219, 222], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_state_size", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dnc.DNC.output_size": [[223, 226], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_output_size", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.util.batch_invert_permutation": [[25, 31], ["tensorflow.name_scope", "tensorflow.unstack", "tensorflow.stack", "tensorflow.invert_permutation"], "function", ["None"], ["def", "batch_invert_permutation", "(", "permutations", ")", ":", "\n", "  ", "\"\"\"Returns batched `tf.invert_permutation` for every row in `permutations`.\"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'batch_invert_permutation'", ",", "values", "=", "[", "permutations", "]", ")", ":", "\n", "    ", "unpacked", "=", "tf", ".", "unstack", "(", "permutations", ")", "\n", "inverses", "=", "[", "tf", ".", "invert_permutation", "(", "permutation", ")", "for", "permutation", "in", "unpacked", "]", "\n", "return", "tf", ".", "stack", "(", "inverses", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.util.batch_gather": [[33, 39], ["tensorflow.name_scope", "zip", "tensorflow.stack", "tensorflow.unstack", "tensorflow.unstack", "tensorflow.gather"], "function", ["None"], ["", "", "def", "batch_gather", "(", "values", ",", "indices", ")", ":", "\n", "  ", "\"\"\"Returns batched `tf.gather` for every row in the input.\"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'batch_gather'", ",", "values", "=", "[", "values", ",", "indices", "]", ")", ":", "\n", "    ", "unpacked", "=", "zip", "(", "tf", ".", "unstack", "(", "values", ")", ",", "tf", ".", "unstack", "(", "indices", ")", ")", "\n", "result", "=", "[", "tf", ".", "gather", "(", "value", ",", "index", ")", "for", "value", ",", "index", "in", "unpacked", "]", "\n", "return", "tf", ".", "stack", "(", "result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.util.one_hot": [[41, 46], ["numpy.zeros"], "function", ["None"], ["", "", "def", "one_hot", "(", "length", ",", "index", ")", ":", "\n", "  ", "\"\"\"Return an nd array of given `length` filled with 0s and a 1 at `index`.\"\"\"", "\n", "result", "=", "np", ".", "zeros", "(", "length", ")", "\n", "result", "[", "index", "]", "=", "1", "\n", "return", "result", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.evaluation_utils.evaluate": [[31, 47], ["metric.lower", "evaluation_utils._bleu", "metric.lower", "evaluation_utils._rouge", "metric.lower", "evaluation_utils._accuracy", "ValueError"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._bleu", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._rouge", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._accuracy"], ["def", "evaluate", "(", "ref_file", ",", "trans_file", ",", "metric", ",", "bpe_delimiter", "=", "None", ")", ":", "\n", "  ", "\"\"\"Pick a metric and evaluate depending on task.\"\"\"", "\n", "# BLEU scores for translation task", "\n", "if", "metric", ".", "lower", "(", ")", "==", "\"bleu\"", ":", "\n", "    ", "evaluation_score", "=", "_bleu", "(", "ref_file", ",", "trans_file", ",", "\n", "bpe_delimiter", "=", "bpe_delimiter", ")", "\n", "# ROUGE scores for summarization tasks", "\n", "", "elif", "metric", ".", "lower", "(", ")", "==", "\"rouge\"", ":", "\n", "    ", "evaluation_score", "=", "_rouge", "(", "ref_file", ",", "trans_file", ",", "\n", "bpe_delimiter", "=", "bpe_delimiter", ")", "\n", "", "elif", "metric", ".", "lower", "(", ")", "==", "\"accuracy\"", ":", "\n", "    ", "evaluation_score", "=", "_accuracy", "(", "ref_file", ",", "trans_file", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unknown metric %s\"", "%", "metric", ")", "\n", "\n", "", "return", "evaluation_score", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.evaluation_utils._clean": [[49, 58], ["re.sub.strip", "re.sub"], "function", ["None"], ["", "def", "_clean", "(", "sentence", ",", "bpe_delimiter", ")", ":", "\n", "  ", "\"\"\"Clean and handle BPE delimiter.\"\"\"", "\n", "sentence", "=", "sentence", ".", "strip", "(", ")", "\n", "\n", "# BPE", "\n", "if", "bpe_delimiter", ":", "\n", "    ", "sentence", "=", "re", ".", "sub", "(", "bpe_delimiter", "+", "\" \"", ",", "\"\"", ",", "sentence", ")", "\n", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.evaluation_utils._bleu": [[61, 91], ["zip", "bleu.compute_bleu", "per_segment_references.append", "reference_text.append", "evaluation_utils._clean", "reference_list.append", "codecs.getreader", "tensorflow.gfile.GFile", "evaluation_utils._clean", "translations.append", "codecs.getreader", "tensorflow.gfile.GFile", "fh.readlines", "_clean.split", "_clean.split"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.bleu.compute_bleu", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._clean", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._clean"], ["", "def", "_bleu", "(", "ref_file", ",", "trans_file", ",", "bpe_delimiter", "=", "None", ")", ":", "\n", "  ", "\"\"\"Compute BLEU scores and handling BPE.\"\"\"", "\n", "max_order", "=", "4", "\n", "smooth", "=", "False", "\n", "\n", "ref_files", "=", "[", "ref_file", "]", "\n", "reference_text", "=", "[", "]", "\n", "for", "reference_filename", "in", "ref_files", ":", "\n", "    ", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "reference_filename", ",", "\"rb\"", ")", ")", "as", "fh", ":", "\n", "      ", "reference_text", ".", "append", "(", "fh", ".", "readlines", "(", ")", ")", "\n", "\n", "", "", "per_segment_references", "=", "[", "]", "\n", "for", "references", "in", "zip", "(", "*", "reference_text", ")", ":", "\n", "    ", "reference_list", "=", "[", "]", "\n", "for", "reference", "in", "references", ":", "\n", "      ", "reference", "=", "_clean", "(", "reference", ",", "bpe_delimiter", ")", "\n", "reference_list", ".", "append", "(", "reference", ".", "split", "(", "\" \"", ")", ")", "\n", "", "per_segment_references", ".", "append", "(", "reference_list", ")", "\n", "\n", "", "translations", "=", "[", "]", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "trans_file", ",", "\"rb\"", ")", ")", "as", "fh", ":", "\n", "    ", "for", "line", "in", "fh", ":", "\n", "      ", "line", "=", "_clean", "(", "line", ",", "bpe_delimiter", ")", "\n", "translations", ".", "append", "(", "line", ".", "split", "(", "\" \"", ")", ")", "\n", "\n", "# bleu_score, precisions, bp, ratio, translation_length, reference_length", "\n", "", "", "bleu_score", ",", "_", ",", "_", ",", "_", ",", "_", ",", "_", "=", "bleu", ".", "compute_bleu", "(", "\n", "per_segment_references", ",", "translations", ",", "max_order", ",", "smooth", ")", "\n", "return", "100", "*", "bleu_score", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.evaluation_utils._rouge": [[93, 109], ["rouge.rouge", "codecs.getreader", "tensorflow.gfile.GFile", "references.append", "codecs.getreader", "tensorflow.gfile.GFile", "hypotheses.append", "evaluation_utils._clean", "evaluation_utils._clean"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._clean", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._clean"], ["", "def", "_rouge", "(", "ref_file", ",", "summarization_file", ",", "bpe_delimiter", "=", "None", ")", ":", "\n", "  ", "\"\"\"Compute ROUGE scores and handling BPE.\"\"\"", "\n", "\n", "references", "=", "[", "]", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "ref_file", ",", "\"rb\"", ")", ")", "as", "fh", ":", "\n", "    ", "for", "line", "in", "fh", ":", "\n", "      ", "references", ".", "append", "(", "_clean", "(", "line", ",", "bpe_delimiter", ")", ")", "\n", "\n", "", "", "hypotheses", "=", "[", "]", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "summarization_file", ",", "\"rb\"", ")", ")", "as", "fh", ":", "\n", "    ", "for", "line", "in", "fh", ":", "\n", "      ", "hypotheses", ".", "append", "(", "_clean", "(", "line", ",", "bpe_delimiter", ")", ")", "\n", "\n", "", "", "rouge_score_map", "=", "rouge", ".", "rouge", "(", "hypotheses", ",", "references", ")", "\n", "return", "100", "*", "rouge_score_map", "[", "\"rouge_l/f_score\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.evaluation_utils._accuracy": [[111, 125], ["codecs.getreader", "tensorflow.gfile.GFile", "codecs.getreader", "tensorflow.gfile.GFile", "label.strip.strip", "pred_fh.readline().strip", "pred_fh.readline"], "function", ["None"], ["", "def", "_accuracy", "(", "label_file", ",", "pred_file", ")", ":", "\n", "  ", "\"\"\"Compute accuracy, each line contains a label.\"\"\"", "\n", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "label_file", ",", "\"rb\"", ")", ")", "as", "label_fh", ":", "\n", "    ", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "pred_file", ",", "\"rb\"", ")", ")", "as", "pred_fh", ":", "\n", "      ", "count", "=", "0.0", "\n", "match", "=", "0.0", "\n", "for", "label", "in", "label_fh", ":", "\n", "        ", "label", "=", "label", ".", "strip", "(", ")", "\n", "pred", "=", "pred_fh", ".", "readline", "(", ")", ".", "strip", "(", ")", "\n", "if", "label", "==", "pred", ":", "\n", "          ", "match", "+=", "1", "\n", "", "count", "+=", "1", "\n", "", "", "", "return", "100", "*", "match", "/", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.evaluation_utils._moses_bleu": [[127, 149], ["subprocess.check_output", "re.search", "float", "re.search.group", "os.path.exists", "subprocess.call", "subprocess.call"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dense.Dense.call", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dense.Dense.call"], ["", "def", "_moses_bleu", "(", "multi_bleu_script", ",", "tgt_test", ",", "trans_file", ",", "bpe_delimiter", "=", "None", ")", ":", "\n", "  ", "\"\"\"Compute BLEU scores using Moses multi-bleu.perl script.\"\"\"", "\n", "# BPE", "\n", "if", "bpe_delimiter", ":", "\n", "    ", "debpe_tgt_test", "=", "tgt_test", "+", "\".debpe\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "debpe_tgt_test", ")", ":", "\n", "# TODO(thangluong): not use shell=True, can be a security hazard", "\n", "      ", "subprocess", ".", "call", "(", "\"cp %s %s\"", "%", "(", "tgt_test", ",", "debpe_tgt_test", ")", ",", "shell", "=", "True", ")", "\n", "subprocess", ".", "call", "(", "\"sed s/%s //g %s\"", "%", "(", "bpe_delimiter", ",", "debpe_tgt_test", ")", ",", "\n", "shell", "=", "True", ")", "\n", "", "tgt_test", "=", "debpe_tgt_test", "\n", "\n", "", "cmd", "=", "\"%s %s < %s\"", "%", "(", "multi_bleu_script", ",", "tgt_test", ",", "trans_file", ")", "\n", "\n", "# subprocess", "\n", "bleu_output", "=", "subprocess", ".", "check_output", "(", "cmd", ",", "shell", "=", "True", ")", "\n", "\n", "# extract BLEU score", "\n", "m", "=", "re", ".", "search", "(", "\"BLEU = (.+?),\"", ",", "bleu_output", ")", "\n", "bleu_score", "=", "float", "(", "m", ".", "group", "(", "1", ")", ")", "\n", "\n", "return", "bleu_score", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__init__": [[24, 76], ["tensorflow.contrib.rnn.MultiRNNCell", "ntm_utils.create_linear_initializer", "ntm_utils.create_linear_initializer", "tensorflow.reshape", "tensorflow.contrib.rnn.BasicLSTMCell", "tensorflow.contrib.rnn.DropoutWrapper", "ntm.NTMCell.__init__.single_cell"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.create_linear_initializer", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.create_linear_initializer"], ["    ", "def", "__init__", "(", "self", ",", "controller_layers", ",", "controller_units", ",", "\n", "use_att_memory", "=", "True", ",", "att_memory", "=", "None", ",", "att_memory_size", "=", "None", ",", "att_memory_vector_dim", "=", "None", ",", "\n", "use_ext_memory", "=", "False", ",", "ext_memory_size", "=", "None", ",", "ext_memory_vector_dim", "=", "None", ",", "ext_read_head_num", "=", "None", ",", "ext_write_head_num", "=", "None", ",", "\n", "dropout", "=", "0.0", ",", "batch_size", "=", "None", ",", "mode", "=", "None", ",", "addressing_mode", "=", "'content_and_location'", ",", "\n", "shift_range", "=", "1", ",", "reuse", "=", "False", ",", "output_dim", "=", "None", ",", "clip_value", "=", "20", ",", "record_w_history", "=", "False", ")", ":", "\n", "        ", "self", ".", "controller_layers", "=", "controller_layers", "\n", "self", ".", "controller_units", "=", "controller_units", "\n", "\n", "self", ".", "att_memory_size", "=", "att_memory_size", "\n", "self", ".", "att_memory_vector_dim", "=", "att_memory_vector_dim", "\n", "self", ".", "ext_memory_size", "=", "ext_memory_size", "\n", "self", ".", "ext_memory_vector_dim", "=", "ext_memory_vector_dim", "\n", "\n", "self", ".", "att_read_head_num", "=", "1", "if", "att_memory", "is", "not", "None", "else", "0", "\n", "self", ".", "ext_read_head_num", "=", "ext_read_head_num", "\n", "self", ".", "total_read_head_num", "=", "self", ".", "att_read_head_num", "+", "(", "ext_read_head_num", "if", "ext_read_head_num", "is", "not", "None", "else", "0", ")", "\n", "self", ".", "ext_write_head_num", "=", "ext_write_head_num", "\n", "\n", "self", ".", "use_att_memory", "=", "use_att_memory", "\n", "self", ".", "use_ext_memory", "=", "use_ext_memory", "\n", "\n", "# need to reshape memory in order to get beam search working", "\n", "if", "self", ".", "use_att_memory", ":", "\n", "            ", "self", ".", "att_M", "=", "tf", ".", "reshape", "(", "att_memory", ",", "[", "-", "1", ",", "self", ".", "att_memory_size", "*", "self", ".", "att_memory_vector_dim", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "att_M", "=", "None", "\n", "\n", "", "self", ".", "addressing_mode", "=", "addressing_mode", "\n", "self", ".", "reuse", "=", "reuse", "\n", "self", ".", "clip_value", "=", "clip_value", "\n", "\n", "self", ".", "dropout", "=", "dropout", "if", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "TRAIN", "else", "0.0", "\n", "def", "single_cell", "(", "num_units", ")", ":", "\n", "            ", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "BasicLSTMCell", "(", "num_units", ",", "forget_bias", "=", "1.0", ")", "\n", "if", "self", ".", "dropout", ">", "0.0", ":", "\n", "                ", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "DropoutWrapper", "(", "cell", "=", "cell", ",", "input_keep_prob", "=", "(", "1.0", "-", "self", ".", "dropout", ")", ")", "\n", "", "return", "cell", "\n", "\n", "", "self", ".", "controller", "=", "tf", ".", "contrib", ".", "rnn", ".", "MultiRNNCell", "(", "[", "single_cell", "(", "self", ".", "controller_units", ")", "for", "_", "in", "range", "(", "self", ".", "controller_layers", ")", "]", ")", "\n", "\n", "self", ".", "step", "=", "0", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "self", ".", "shift_range", "=", "shift_range", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "record_w_history", "=", "record_w_history", "\n", "\n", "self", ".", "o2p_initializer", "=", "create_linear_initializer", "(", "self", ".", "controller_units", ")", "\n", "self", ".", "o2o_initializer", "=", "create_linear_initializer", "(", "\n", "self", ".", "controller_units", "+", "(", "self", ".", "att_memory_vector_dim", "if", "self", ".", "use_att_memory", "else", "0", ")", "+", "(", "self", ".", "ext_memory_vector_dim", "*", "self", ".", "ext_read_head_num", "if", "self", ".", "use_ext_memory", "else", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.interact_with_memory": [[77, 136], ["tensorflow.split", "enumerate", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tensorflow.clip_by_value", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.tanh", "tensorflow.nn.softplus", "tensorflow.sigmoid", "tensorflow.nn.softmax", "w_list.append", "range", "tensorflow.split", "range", "tensorflow.nn.softplus", "tensorflow.variable_scope", "ntm.NTMCell.addressing", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "read_vector_list.append", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "str", "tensorflow.sigmoid", "tensorflow.tanh", "tensorflow.matmul", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.ones", "tensorflow.matmul"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.addressing"], ["", "def", "interact_with_memory", "(", "self", ",", "prev_state", ",", "controller_output", ",", "att", "=", "True", ")", ":", "\n", "        ", "num_parameters_per_head", "=", "(", "self", ".", "att_memory_vector_dim", "if", "att", "else", "self", ".", "ext_memory_vector_dim", ")", "+", "1", "+", "1", "+", "(", "self", ".", "shift_range", "*", "2", "+", "1", ")", "+", "1", "\n", "num_heads", "=", "1", "if", "att", "else", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "\n", "total_parameter_num", "=", "num_parameters_per_head", "if", "att", "else", "(", "num_parameters_per_head", "*", "num_heads", "+", "self", ".", "ext_memory_vector_dim", "*", "2", "*", "self", ".", "ext_write_head_num", ")", "\n", "with", "tf", ".", "variable_scope", "(", "\"o2p_att_\"", "+", "str", "(", "att", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "            ", "parameters", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "controller_output", ",", "total_parameter_num", ",", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "o2p_initializer", ")", "\n", "parameters", "=", "tf", ".", "clip_by_value", "(", "parameters", ",", "-", "self", ".", "clip_value", ",", "self", ".", "clip_value", ")", "\n", "", "head_parameter_list", "=", "tf", ".", "split", "(", "parameters", "[", ":", ",", ":", "num_parameters_per_head", "*", "num_heads", "]", ",", "num_heads", ",", "axis", "=", "1", ")", "\n", "\n", "if", "att", ":", "\n", "            ", "prev_w_list", "=", "prev_state", ".", "att_w_list", "\n", "prev_M", "=", "prev_state", ".", "att_M", "\n", "prev_M", "=", "tf", ".", "reshape", "(", "prev_M", ",", "[", "-", "1", ",", "self", ".", "att_memory_size", ",", "self", ".", "att_memory_vector_dim", "]", ")", "\n", "memory_vector_dim", "=", "self", ".", "att_memory_vector_dim", "\n", "", "else", ":", "\n", "            ", "prev_w_list", "=", "prev_state", ".", "ext_w_list", "\n", "prev_M", "=", "prev_state", ".", "ext_M", "\n", "prev_M", "=", "tf", ".", "reshape", "(", "prev_M", ",", "[", "-", "1", ",", "self", ".", "ext_memory_size", ",", "self", ".", "ext_memory_vector_dim", "]", ")", "\n", "memory_vector_dim", "=", "self", ".", "ext_memory_vector_dim", "\n", "\n", "", "w_list", "=", "[", "]", "\n", "for", "i", ",", "head_parameter", "in", "enumerate", "(", "head_parameter_list", ")", ":", "\n", "            ", "k", "=", "tf", ".", "tanh", "(", "head_parameter", "[", ":", ",", "0", ":", "memory_vector_dim", "]", ")", "\n", "beta", "=", "tf", ".", "nn", ".", "softplus", "(", "head_parameter", "[", ":", ",", "memory_vector_dim", "]", ")", "\n", "g", "=", "tf", ".", "sigmoid", "(", "head_parameter", "[", ":", ",", "memory_vector_dim", "+", "1", "]", ")", "\n", "s", "=", "tf", ".", "nn", ".", "softmax", "(", "\n", "head_parameter", "[", ":", ",", "memory_vector_dim", "+", "2", ":", "memory_vector_dim", "+", "2", "+", "(", "self", ".", "shift_range", "*", "2", "+", "1", ")", "]", "\n", ")", "\n", "gamma", "=", "tf", ".", "nn", ".", "softplus", "(", "head_parameter", "[", ":", ",", "-", "1", "]", ")", "+", "1", "\n", "with", "tf", ".", "variable_scope", "(", "'addressing_head_%d'", "%", "i", ")", ":", "\n", "                ", "w", "=", "self", ".", "addressing", "(", "k", ",", "beta", ",", "g", ",", "s", ",", "gamma", ",", "prev_M", ",", "prev_w_list", "[", "i", "]", ",", "att", "=", "att", ")", "\n", "", "w_list", ".", "append", "(", "w", ")", "\n", "\n", "# Reading (Sec 3.1)", "\n", "\n", "", "if", "att", ":", "\n", "            ", "read_vector_list", "=", "[", "tf", ".", "reduce_sum", "(", "tf", ".", "expand_dims", "(", "w_list", "[", "0", "]", ",", "dim", "=", "2", ")", "*", "prev_M", ",", "axis", "=", "1", ")", "]", "\n", "", "else", ":", "\n", "            ", "read_w_list", "=", "w_list", "[", ":", "self", ".", "ext_read_head_num", "]", "\n", "read_vector_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "ext_read_head_num", ")", ":", "\n", "                ", "read_vector", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "expand_dims", "(", "read_w_list", "[", "i", "]", ",", "dim", "=", "2", ")", "*", "prev_M", ",", "axis", "=", "1", ")", "\n", "read_vector_list", ".", "append", "(", "read_vector", ")", "\n", "\n", "# Writing (Sec 3.2)", "\n", "\n", "", "", "M", "=", "prev_M", "\n", "if", "not", "att", ":", "\n", "            ", "erase_add_list", "=", "tf", ".", "split", "(", "parameters", "[", ":", ",", "num_parameters_per_head", "*", "num_heads", ":", "]", ",", "2", "*", "self", ".", "ext_write_head_num", ",", "axis", "=", "1", ")", "\n", "write_w_list", "=", "w_list", "[", "self", ".", "ext_read_head_num", ":", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "ext_write_head_num", ")", ":", "\n", "                ", "w", "=", "tf", ".", "expand_dims", "(", "write_w_list", "[", "i", "]", ",", "axis", "=", "2", ")", "\n", "erase_vector", "=", "tf", ".", "expand_dims", "(", "tf", ".", "sigmoid", "(", "erase_add_list", "[", "i", "*", "2", "]", ")", ",", "axis", "=", "1", ")", "\n", "add_vector", "=", "tf", ".", "expand_dims", "(", "tf", ".", "tanh", "(", "erase_add_list", "[", "i", "*", "2", "+", "1", "]", ")", ",", "axis", "=", "1", ")", "\n", "M", "=", "M", "*", "(", "tf", ".", "ones", "(", "[", "self", ".", "batch_size", ",", "self", ".", "ext_memory_size", ",", "self", ".", "ext_memory_vector_dim", "]", ")", "-", "tf", ".", "matmul", "(", "w", ",", "erase_vector", ")", ")", "+", "tf", ".", "matmul", "(", "w", ",", "add_vector", ")", "\n", "\n", "", "", "return", "read_vector_list", ",", "w_list", ",", "M", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.__call__": [[137, 216], ["tensorflow.concat", "Model2NTMState", "tensorflow.variable_scope", "ntm.NTMCell.controller", "ntm.NTMCell.interact_with_memory", "tensorflow.reshape", "ntm.NTMCell.interact_with_memory", "tensorflow.reshape", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tensorflow.clip_by_value", "tensorflow.nn.dropout", "map", "map", "map", "map", "Model1NTMState", "Model3NTMState", "x.get_shape", "tensorflow.concat", "tuple", "v.set_shape", "v.set_shape", "v.set_shape", "v.set_shape", "Model2NTMState", "tuple", "tuple", "Model1NTMState", "Model3NTMState", "Model3NTMState.att_w_history.write", "Model3NTMState.ext_w_history[].write", "Model3NTMState.att_w_history.write", "range", "Model3NTMState.ext_w_history[].write", "range"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.interact_with_memory", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.interact_with_memory"], ["", "def", "__call__", "(", "self", ",", "x", ",", "prev_state", ")", ":", "\n", "        ", "if", "self", ".", "use_att_memory", "and", "self", ".", "use_ext_memory", ":", "\n", "            ", "prev_state", "=", "Model2NTMState", "(", "*", "prev_state", ")", "\n", "", "elif", "self", ".", "use_att_memory", ":", "\n", "            ", "prev_state", "=", "Model1NTMState", "(", "*", "prev_state", ")", "\n", "", "else", ":", "\n", "            ", "prev_state", "=", "Model3NTMState", "(", "*", "prev_state", ")", "\n", "\n", "", "prev_read_vector_list", "=", "(", "prev_state", ".", "ext_read_vector_list", "if", "self", ".", "use_ext_memory", "else", "[", "]", ")", "+", "(", "prev_state", ".", "att_read_vector_list", "if", "self", ".", "use_att_memory", "else", "[", "]", ")", "\n", "\n", "controller_input", "=", "tf", ".", "concat", "(", "[", "x", "]", "+", "prev_read_vector_list", "+", "[", "prev_state", ".", "prev_output", "]", ",", "axis", "=", "1", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'controller'", ",", "reuse", "=", "self", ".", "reuse", ")", ":", "\n", "            ", "controller_output", ",", "controller_state", "=", "self", ".", "controller", "(", "controller_input", ",", "prev_state", ".", "controller_state", ")", "\n", "\n", "", "if", "self", ".", "use_att_memory", ":", "\n", "            ", "att_read_vector_list", ",", "att_w_list", ",", "att_M", "=", "self", ".", "interact_with_memory", "(", "prev_state", ",", "controller_output", ",", "att", "=", "True", ")", "\n", "att_M", "=", "tf", ".", "reshape", "(", "att_M", ",", "[", "-", "1", ",", "self", ".", "att_memory_size", "*", "self", ".", "att_memory_vector_dim", "]", ")", "\n", "\n", "", "if", "self", ".", "use_ext_memory", ":", "\n", "            ", "ext_read_vector_list", ",", "ext_w_list", ",", "ext_M", "=", "self", ".", "interact_with_memory", "(", "prev_state", ",", "controller_output", ",", "att", "=", "False", ")", "\n", "ext_M", "=", "tf", ".", "reshape", "(", "ext_M", ",", "[", "-", "1", ",", "self", ".", "ext_memory_size", "*", "self", ".", "ext_memory_vector_dim", "]", ")", "\n", "\n", "", "if", "not", "self", ".", "output_dim", ":", "\n", "            ", "output_dim", "=", "x", ".", "get_shape", "(", ")", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "output_dim", "=", "self", ".", "output_dim", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"o2o\"", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "            ", "read_vector_list", "=", "(", "ext_read_vector_list", "if", "self", ".", "use_ext_memory", "else", "[", "]", ")", "+", "(", "att_read_vector_list", "if", "self", ".", "use_att_memory", "else", "[", "]", ")", "\n", "\n", "NTM_output", "=", "tf", ".", "contrib", ".", "layers", ".", "fully_connected", "(", "\n", "tf", ".", "concat", "(", "[", "controller_output", "]", "+", "read_vector_list", ",", "axis", "=", "1", ")", ",", "output_dim", ",", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "self", ".", "o2o_initializer", ")", "\n", "NTM_output", "=", "tf", ".", "clip_by_value", "(", "NTM_output", ",", "-", "self", ".", "clip_value", ",", "self", ".", "clip_value", ")", "\n", "\n", "", "if", "self", ".", "dropout", ">", "0.0", ":", "\n", "            ", "NTM_output", "=", "tf", ".", "nn", ".", "dropout", "(", "NTM_output", ",", "1", "-", "self", ".", "dropout", ")", "\n", "\n", "", "self", ".", "step", "+=", "1", "\n", "\n", "if", "self", ".", "use_att_memory", ":", "\n", "            ", "map", "(", "lambda", "v", ":", "v", ".", "set_shape", "(", "[", "None", ",", "self", ".", "att_memory_vector_dim", "]", ")", ",", "att_read_vector_list", ")", "\n", "map", "(", "lambda", "v", ":", "v", ".", "set_shape", "(", "[", "None", ",", "self", ".", "att_memory_size", "]", ")", ",", "att_w_list", ")", "\n", "", "if", "self", ".", "use_ext_memory", ":", "\n", "            ", "map", "(", "lambda", "v", ":", "v", ".", "set_shape", "(", "[", "None", ",", "self", ".", "ext_memory_vector_dim", "]", ")", ",", "ext_read_vector_list", ")", "\n", "map", "(", "lambda", "v", ":", "v", ".", "set_shape", "(", "[", "None", ",", "self", ".", "ext_memory_size", "]", ")", ",", "ext_w_list", ")", "\n", "\n", "", "if", "self", ".", "use_att_memory", "and", "self", ".", "use_ext_memory", ":", "\n", "            ", "return", "NTM_output", ",", "tuple", "(", "Model2NTMState", "(", "\n", "time", "=", "prev_state", ".", "time", "+", "1", "if", "self", ".", "record_w_history", "else", "prev_state", ".", "time", ",", "\n", "controller_state", "=", "controller_state", ",", "\n", "ext_read_vector_list", "=", "ext_read_vector_list", ",", "\n", "ext_w_list", "=", "ext_w_list", ",", "\n", "ext_w_history", "=", "[", "prev_state", ".", "ext_w_history", "[", "i", "]", ".", "write", "(", "prev_state", ".", "time", ",", "ext_w_list", "[", "i", "]", ")", "for", "i", "in", "range", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "]", "if", "self", ".", "record_w_history", "else", "prev_state", ".", "ext_w_history", ",", "\n", "ext_M", "=", "ext_M", ",", "\n", "att_read_vector_list", "=", "att_read_vector_list", ",", "\n", "att_w_list", "=", "att_w_list", ",", "\n", "att_w_history", "=", "prev_state", ".", "att_w_history", ".", "write", "(", "prev_state", ".", "time", ",", "att_w_list", "[", "0", "]", ")", "if", "self", ".", "record_w_history", "else", "prev_state", ".", "att_w_history", ",", "\n", "att_M", "=", "att_M", ",", "\n", "prev_output", "=", "NTM_output", ")", ")", "\n", "", "elif", "self", ".", "use_att_memory", ":", "\n", "            ", "return", "NTM_output", ",", "tuple", "(", "Model1NTMState", "(", "\n", "time", "=", "prev_state", ".", "time", "+", "1", "if", "self", ".", "record_w_history", "else", "prev_state", ".", "time", ",", "\n", "controller_state", "=", "controller_state", ",", "\n", "att_read_vector_list", "=", "att_read_vector_list", ",", "\n", "att_w_list", "=", "att_w_list", ",", "\n", "att_w_history", "=", "prev_state", ".", "att_w_history", ".", "write", "(", "prev_state", ".", "time", ",", "att_w_list", "[", "0", "]", ")", "if", "self", ".", "record_w_history", "else", "prev_state", ".", "att_w_history", ",", "\n", "att_M", "=", "att_M", ",", "\n", "prev_output", "=", "NTM_output", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "NTM_output", ",", "tuple", "(", "Model3NTMState", "(", "\n", "time", "=", "prev_state", ".", "time", "+", "1", "if", "self", ".", "record_w_history", "else", "prev_state", ".", "time", ",", "\n", "controller_state", "=", "controller_state", ",", "\n", "ext_read_vector_list", "=", "ext_read_vector_list", ",", "\n", "ext_w_list", "=", "ext_w_list", ",", "\n", "ext_w_history", "=", "[", "prev_state", ".", "ext_w_history", "[", "i", "]", ".", "write", "(", "prev_state", ".", "time", ",", "ext_w_list", "[", "i", "]", ")", "for", "i", "in", "range", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "]", "if", "self", ".", "record_w_history", "else", "prev_state", ".", "ext_w_history", ",", "\n", "ext_M", "=", "ext_M", ",", "\n", "prev_output", "=", "NTM_output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.addressing": [[217, 273], ["tensorflow.expand_dims", "tensorflow.matmul", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.pow", "tensorflow.squeeze", "tensorflow.nn.softmax", "tensorflow.sqrt", "tensorflow.sqrt", "tensorflow.squeeze", "tensorflow.exp", "tensorflow.squeeze", "tensorflow.squeeze.set_shape", "tensorflow.squeeze.set_shape", "tensorflow.concat", "tensorflow.concat", "tensorflow.stack", "tensorflow.concat", "tensorflow.concat", "tensorflow.stack", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.square", "tensorflow.square", "tensorflow.expand_dims", "tensorflow.zeros", "tensorflow.reverse", "tensorflow.reverse", "tensorflow.zeros", "tensorflow.reverse", "tensorflow.reverse", "range", "range"], "methods", ["None"], ["", "", "def", "addressing", "(", "self", ",", "k", ",", "beta", ",", "g", ",", "s", ",", "gamma", ",", "prev_M", ",", "prev_w", ",", "att", "=", "True", ")", ":", "\n", "        ", "k", "=", "tf", ".", "expand_dims", "(", "k", ",", "axis", "=", "2", ")", "\n", "inner_product", "=", "tf", ".", "matmul", "(", "prev_M", ",", "k", ")", "\n", "if", "att", ":", "\n", "            ", "inner_product", "=", "tf", ".", "squeeze", "(", "inner_product", ",", "axis", "=", "2", ")", "\n", "w_c", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "expand_dims", "(", "beta", ",", "axis", "=", "1", ")", "*", "inner_product", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "k_norm", "=", "tf", ".", "sqrt", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "k", ")", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", ")", "\n", "M_norm", "=", "tf", ".", "sqrt", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "prev_M", ")", ",", "axis", "=", "2", ",", "keep_dims", "=", "True", ")", ")", "\n", "norm_product", "=", "M_norm", "*", "k_norm", "\n", "K", "=", "tf", ".", "squeeze", "(", "inner_product", "/", "(", "norm_product", "+", "1e-8", ")", ")", "# eq (6)", "\n", "\n", "# Calculating w^c", "\n", "\n", "K_amplified", "=", "tf", ".", "exp", "(", "tf", ".", "expand_dims", "(", "beta", ",", "axis", "=", "1", ")", "*", "K", ")", "\n", "w_c", "=", "K_amplified", "/", "tf", ".", "reduce_sum", "(", "K_amplified", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# eq (5)", "\n", "\n", "w_c", "=", "tf", ".", "squeeze", "(", "w_c", ")", "\n", "\n", "", "if", "att", ":", "\n", "            ", "w_c", ".", "set_shape", "(", "[", "None", ",", "self", ".", "att_memory_size", "]", ")", "\n", "", "else", ":", "\n", "            ", "w_c", ".", "set_shape", "(", "[", "None", ",", "self", ".", "ext_memory_size", "]", ")", "\n", "\n", "", "if", "self", ".", "addressing_mode", "==", "'content'", ":", "# Only focus on content", "\n", "            ", "return", "w_c", "\n", "\n", "# Sec 3.3.2 Focusing by Location", "\n", "\n", "", "g", "=", "tf", ".", "expand_dims", "(", "g", ",", "axis", "=", "1", ")", "\n", "w_g", "=", "g", "*", "w_c", "+", "(", "1", "-", "g", ")", "*", "prev_w", "# eq (7)", "\n", "\n", "if", "att", ":", "\n", "            ", "s", "=", "tf", ".", "concat", "(", "[", "s", "[", ":", ",", ":", "self", ".", "shift_range", "+", "1", "]", ",", "\n", "tf", ".", "zeros", "(", "[", "self", ".", "batch_size", ",", "self", ".", "att_memory_size", "-", "(", "self", ".", "shift_range", "*", "2", "+", "1", ")", "]", ")", ",", "\n", "s", "[", ":", ",", "-", "self", ".", "shift_range", ":", "]", "]", ",", "axis", "=", "1", ")", "\n", "t", "=", "tf", ".", "concat", "(", "[", "tf", ".", "reverse", "(", "s", ",", "axis", "=", "[", "1", "]", ")", ",", "tf", ".", "reverse", "(", "s", ",", "axis", "=", "[", "1", "]", ")", "]", ",", "axis", "=", "1", ")", "\n", "s_matrix", "=", "tf", ".", "stack", "(", "\n", "[", "t", "[", ":", ",", "self", ".", "att_memory_size", "-", "i", "-", "1", ":", "self", ".", "att_memory_size", "*", "2", "-", "i", "-", "1", "]", "for", "i", "in", "range", "(", "self", ".", "att_memory_size", ")", "]", ",", "\n", "axis", "=", "1", "\n", ")", "\n", "", "else", ":", "\n", "            ", "s", "=", "tf", ".", "concat", "(", "[", "s", "[", ":", ",", ":", "self", ".", "shift_range", "+", "1", "]", ",", "\n", "tf", ".", "zeros", "(", "[", "self", ".", "batch_size", ",", "self", ".", "ext_memory_size", "-", "(", "self", ".", "shift_range", "*", "2", "+", "1", ")", "]", ")", ",", "\n", "s", "[", ":", ",", "-", "self", ".", "shift_range", ":", "]", "]", ",", "axis", "=", "1", ")", "\n", "t", "=", "tf", ".", "concat", "(", "[", "tf", ".", "reverse", "(", "s", ",", "axis", "=", "[", "1", "]", ")", ",", "tf", ".", "reverse", "(", "s", ",", "axis", "=", "[", "1", "]", ")", "]", ",", "axis", "=", "1", ")", "\n", "s_matrix", "=", "tf", ".", "stack", "(", "\n", "[", "t", "[", ":", ",", "self", ".", "ext_memory_size", "-", "i", "-", "1", ":", "self", ".", "ext_memory_size", "*", "2", "-", "i", "-", "1", "]", "for", "i", "in", "range", "(", "self", ".", "ext_memory_size", ")", "]", ",", "\n", "axis", "=", "1", "\n", ")", "\n", "\n", "", "w_", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "expand_dims", "(", "w_g", ",", "axis", "=", "1", ")", "*", "s_matrix", ",", "axis", "=", "2", ")", "# eq (8)", "\n", "w_sharpen", "=", "tf", ".", "pow", "(", "w_", ",", "tf", ".", "expand_dims", "(", "gamma", ",", "axis", "=", "1", ")", ")", "\n", "w", "=", "w_sharpen", "/", "tf", ".", "reduce_sum", "(", "w_sharpen", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# eq (9)", "\n", "\n", "return", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.zero_state": [[274, 343], ["tensorflow.variable_scope", "ntm.NTMCell.controller.zero_state", "tensorflow.zeros", "ntm_utils.expand", "tuple", "ntm_utils.expand", "ntm_utils.expand", "tensorflow.get_variable", "ntm_utils.expand", "ntm_utils.expand", "Model2NTMState", "tuple", "tuple", "tensorflow.tanh", "range", "tensorflow.nn.softmax", "range", "tensorflow.tanh", "tensorflow.nn.softmax", "Model1NTMState", "Model3NTMState", "ntm_utils.learned_init", "ntm_utils.learned_init", "tensorflow.constant_initializer", "ntm_utils.learned_init", "ntm_utils.learned_init", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.TensorArray", "tensorflow.zeros", "tensorflow.TensorArray", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.TensorArray", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.zeros", "range", "tensorflow.TensorArray", "range"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.zero_state", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.expand", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.expand", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.expand", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.expand", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.expand", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.learned_init", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.learned_init", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.learned_init", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm_utils.learned_init"], ["", "def", "zero_state", "(", "self", ",", "batch_size", ",", "dtype", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "'init'", ",", "reuse", "=", "self", ".", "reuse", ")", ":", "\n", "            ", "controller_init_state", "=", "self", ".", "controller", ".", "zero_state", "(", "batch_size", ",", "dtype", ")", "\n", "prev_output", "=", "tf", ".", "zeros", "(", "[", "batch_size", ",", "self", ".", "output_dim", "]", ")", "\n", "\n", "if", "self", ".", "use_ext_memory", ":", "\n", "                ", "ext_read_vector_list", "=", "[", "expand", "(", "tf", ".", "tanh", "(", "learned_init", "(", "self", ".", "ext_memory_vector_dim", ")", ")", ",", "dim", "=", "0", ",", "N", "=", "batch_size", ",", "dims", "=", "1", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "ext_read_head_num", ")", "]", "\n", "\n", "ext_w_list", "=", "[", "expand", "(", "tf", ".", "nn", ".", "softmax", "(", "learned_init", "(", "self", ".", "ext_memory_size", ")", ")", ",", "dim", "=", "0", ",", "N", "=", "batch_size", ",", "dims", "=", "1", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "]", "\n", "\n", "# ext_M = expand(tf.tanh(learned_init(self.ext_memory_size * self.ext_memory_vector_dim)), dim=0, N=batch_size, dims=1)", "\n", "\n", "ext_M", "=", "expand", "(", "tf", ".", "get_variable", "(", "'init_M'", ",", "self", ".", "ext_memory_size", "*", "self", ".", "ext_memory_vector_dim", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "1e-6", ")", ")", ",", "\n", "dim", "=", "0", ",", "N", "=", "batch_size", ",", "dims", "=", "1", ")", "\n", "\n", "", "if", "self", ".", "use_att_memory", ":", "\n", "                ", "att_read_vector_list", "=", "[", "expand", "(", "tf", ".", "tanh", "(", "learned_init", "(", "self", ".", "att_memory_vector_dim", ")", ")", ",", "dim", "=", "0", ",", "N", "=", "batch_size", ",", "dims", "=", "1", ")", "]", "\n", "att_w_list", "=", "[", "expand", "(", "tf", ".", "nn", ".", "softmax", "(", "learned_init", "(", "self", ".", "att_memory_size", ")", ")", ",", "dim", "=", "0", ",", "N", "=", "batch_size", ",", "dims", "=", "1", ")", "]", "\n", "\n", "", "if", "self", ".", "use_att_memory", "and", "self", ".", "use_ext_memory", ":", "\n", "# tmp_att_M = tf.reshape(self.att_M, [-1, self.att_memory_size, self.att_memory_vector_dim])", "\n", "# m = tf.contrib.layers.fully_connected(tf.reduce_mean(tmp_att_M, axis=1), self.ext_memory_vector_dim,", "\n", "#     activation_fn=tf.tanh, weights_initializer=create_linear_initializer(self.att_memory_vector_dim))", "\n", "# ext_M = tf.tile(tf.expand_dims(m, 1), multiples=[1, self.ext_memory_size, 1]) + tf.random_normal([batch_size, self.ext_memory_size, self.ext_memory_vector_dim], stddev=0.316)", "\n", "# ext_M = tf.reshape(ext_M, [-1, self.ext_memory_size * self.ext_memory_vector_dim])", "\n", "\n", "# return tuple(Model2NTMState(", "\n", "#     controller_state=controller_init_state,", "\n", "#     ext_read_vector_list=ext_read_vector_list,", "\n", "#     ext_w_list=ext_w_list,", "\n", "#     ext_M=ext_M,", "\n", "#     att_read_vector_list=att_read_vector_list,", "\n", "#     att_w_list=att_w_list,", "\n", "#     att_M=self.att_M,", "\n", "#     prev_output=prev_output))", "\n", "\n", "                ", "return", "tuple", "(", "Model2NTMState", "(", "\n", "time", "=", "tf", ".", "zeros", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "controller_state", "=", "controller_init_state", ",", "\n", "ext_read_vector_list", "=", "ext_read_vector_list", ",", "\n", "ext_w_list", "=", "ext_w_list", ",", "\n", "ext_w_history", "=", "[", "tf", ".", "TensorArray", "(", "dtype", "=", "dtype", ",", "size", "=", "0", ",", "dynamic_size", "=", "True", ")", "for", "_", "in", "range", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "]", "if", "self", ".", "record_w_history", "else", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "ext_M", "=", "ext_M", ",", "\n", "att_read_vector_list", "=", "att_read_vector_list", ",", "\n", "att_w_list", "=", "att_w_list", ",", "\n", "att_w_history", "=", "tf", ".", "TensorArray", "(", "dtype", "=", "dtype", ",", "size", "=", "0", ",", "dynamic_size", "=", "True", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "att_M", "=", "self", ".", "att_M", ",", "\n", "prev_output", "=", "prev_output", ")", ")", "\n", "", "elif", "self", ".", "use_att_memory", ":", "\n", "                ", "return", "tuple", "(", "Model1NTMState", "(", "\n", "time", "=", "tf", ".", "zeros", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "controller_state", "=", "controller_init_state", ",", "\n", "att_read_vector_list", "=", "att_read_vector_list", ",", "\n", "att_w_list", "=", "att_w_list", ",", "\n", "att_w_history", "=", "tf", ".", "TensorArray", "(", "dtype", "=", "dtype", ",", "size", "=", "0", ",", "dynamic_size", "=", "True", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "att_M", "=", "self", ".", "att_M", ",", "\n", "prev_output", "=", "prev_output", ")", ")", "\n", "", "else", ":", "\n", "                ", "return", "tuple", "(", "Model3NTMState", "(", "\n", "time", "=", "tf", ".", "zeros", "(", "[", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "controller_state", "=", "controller_init_state", ",", "\n", "ext_read_vector_list", "=", "ext_read_vector_list", ",", "\n", "ext_w_list", "=", "ext_w_list", ",", "\n", "ext_w_history", "=", "[", "tf", ".", "TensorArray", "(", "dtype", "=", "dtype", ",", "size", "=", "0", ",", "dynamic_size", "=", "True", ")", "for", "_", "in", "range", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "]", "if", "self", ".", "record_w_history", "else", "tf", ".", "zeros", "(", "[", "batch_size", ",", "1", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "ext_M", "=", "ext_M", ",", "\n", "prev_output", "=", "prev_output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.state_size": [[344, 377], ["tuple", "Model2NTMState", "tuple", "tuple", "Model1NTMState", "Model3NTMState", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tuple", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "range", "range", "tuple", "tensorflow.TensorShape", "tensorflow.TensorShape", "tuple", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "range", "range", "range", "tuple", "range"], "methods", ["None"], ["", "", "", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "use_att_memory", "and", "self", ".", "use_ext_memory", ":", "\n", "            ", "return", "tuple", "(", "Model2NTMState", "(", "\n", "time", "=", "tf", ".", "TensorShape", "(", "[", "]", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "TensorShape", "(", "[", "1", "]", ")", ",", "\n", "controller_state", "=", "self", ".", "controller", ".", "state_size", ",", "\n", "ext_read_vector_list", "=", "[", "self", ".", "ext_memory_vector_dim", "for", "_", "in", "range", "(", "self", ".", "ext_read_head_num", ")", "]", ",", "\n", "ext_w_list", "=", "[", "self", ".", "ext_memory_size", "for", "_", "in", "range", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "]", ",", "\n", "ext_w_history", "=", "[", "tuple", "(", ")", "for", "_", "in", "range", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "]", "if", "self", ".", "record_w_history", "else", "tf", ".", "TensorShape", "(", "[", "1", "]", ")", ",", "\n", "ext_M", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "ext_memory_size", "*", "self", ".", "ext_memory_vector_dim", "]", ")", ",", "\n", "att_read_vector_list", "=", "[", "self", ".", "att_memory_vector_dim", "]", ",", "\n", "att_w_list", "=", "[", "self", ".", "att_memory_size", "]", ",", "\n", "att_w_history", "=", "tuple", "(", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "TensorShape", "(", "[", "1", "]", ")", ",", "\n", "att_M", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "att_memory_size", "*", "self", ".", "att_memory_vector_dim", "]", ")", ",", "\n", "prev_output", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "output_dim", "]", ")", ")", ")", "\n", "", "elif", "self", ".", "use_att_memory", ":", "\n", "            ", "return", "tuple", "(", "Model1NTMState", "(", "\n", "time", "=", "tf", ".", "TensorShape", "(", "[", "]", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "TensorShape", "(", "[", "1", "]", ")", ",", "\n", "controller_state", "=", "self", ".", "controller", ".", "state_size", ",", "\n", "att_read_vector_list", "=", "[", "self", ".", "att_memory_vector_dim", "]", ",", "\n", "att_w_list", "=", "[", "self", ".", "att_memory_size", "]", ",", "\n", "att_w_history", "=", "tuple", "(", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "TensorShape", "(", "[", "1", "]", ")", ",", "\n", "att_M", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "att_memory_size", "*", "self", ".", "att_memory_vector_dim", "]", ")", ",", "\n", "prev_output", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "output_dim", "]", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "tuple", "(", "Model3NTMState", "(", "\n", "time", "=", "tf", ".", "TensorShape", "(", "[", "]", ")", "if", "self", ".", "record_w_history", "else", "tf", ".", "TensorShape", "(", "[", "1", "]", ")", ",", "\n", "controller_state", "=", "self", ".", "controller", ".", "state_size", ",", "\n", "ext_read_vector_list", "=", "[", "self", ".", "ext_memory_vector_dim", "for", "_", "in", "range", "(", "self", ".", "ext_read_head_num", ")", "]", ",", "\n", "ext_w_list", "=", "[", "self", ".", "ext_memory_size", "for", "_", "in", "range", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "]", ",", "\n", "ext_w_history", "=", "[", "tuple", "(", ")", "for", "_", "in", "range", "(", "self", ".", "ext_read_head_num", "+", "self", ".", "ext_write_head_num", ")", "]", "if", "self", ".", "record_w_history", "else", "tf", ".", "TensorShape", "(", "[", "1", "]", ")", ",", "\n", "ext_M", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "ext_memory_size", "*", "self", ".", "ext_memory_vector_dim", "]", ")", ",", "\n", "prev_output", "=", "tf", ".", "TensorShape", "(", "[", "self", ".", "output_dim", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.ntm.NTMCell.output_size": [[378, 381], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_infer_iterator": [[38, 78], ["tensorflow.cast", "src_dataset.map.map", "src_dataset.map.map", "src_dataset.map.map", "iterator_utils.get_infer_iterator.batching_func"], "function", ["None"], ["\n", "", "def", "get_infer_iterator", "(", "\n", "src_dataset", ",", "src_vocab_table", ",", "batch_size", ",", "\n", "source_reverse", ",", "sos", ",", "eos", ",", "src_max_len", "=", "None", ")", ":", "\n", "  ", "src_eos_id", "=", "tf", ".", "cast", "(", "src_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "eos", ")", ")", ",", "tf", ".", "int32", ")", "\n", "src_dataset", "=", "src_dataset", ".", "map", "(", "lambda", "src", ":", "tf", ".", "string_split", "(", "[", "src", "]", ")", ".", "values", ")", "\n", "\n", "if", "src_max_len", ":", "\n", "    ", "src_dataset", "=", "src_dataset", ".", "map", "(", "lambda", "src", ":", "src", "[", ":", "src_max_len", "]", ")", "\n", "# Convert the word strings to ids", "\n", "", "src_dataset", "=", "src_dataset", ".", "map", "(", "\n", "lambda", "src", ":", "tf", ".", "cast", "(", "src_vocab_table", ".", "lookup", "(", "src", ")", ",", "tf", ".", "int32", ")", ")", "\n", "if", "source_reverse", ":", "\n", "    ", "src_dataset", "=", "src_dataset", ".", "map", "(", "lambda", "src", ":", "tf", ".", "reverse", "(", "src", ",", "axis", "=", "[", "0", "]", ")", ")", "\n", "# Add in the word counts.", "\n", "", "src_dataset", "=", "src_dataset", ".", "map", "(", "lambda", "src", ":", "(", "src", ",", "tf", ".", "size", "(", "src", ")", ")", ")", "\n", "\n", "# def batching_func(x):", "\n", "#   return x.padded_batch(", "\n", "#       batch_size,", "\n", "#       # The entry is the source line rows;", "\n", "#       # this has unknown-length vectors.  The last entry is", "\n", "#       # the source row size; this is a scalar.", "\n", "#       padded_shapes=(tf.TensorShape([src_max_len]),  # src", "\n", "#                      tf.TensorShape([])),     # src_len", "\n", "#       # Pad the source sequences with eos tokens.", "\n", "#       # (Though notice we don't generally need to do this since", "\n", "#       # later on we will be masking out calculations past the true sequence.", "\n", "#       padding_values=(src_eos_id,  # src", "\n", "#                       0))          # src_len -- unused", "\n", "\n", "# batched_dataset = batching_func(src_dataset)", "\n", "\n", "batched_dataset", "=", "src_dataset", ".", "apply", "(", "tf", ".", "contrib", ".", "data", ".", "padded_batch_and_drop_remainder", "(", "\n", "batch_size", ",", "\n", "padded_shapes", "=", "(", "tf", ".", "TensorShape", "(", "[", "src_max_len", "]", ")", ",", "# src", "\n", "tf", ".", "TensorShape", "(", "[", "]", ")", ")", ",", "# src_len", "\n", "\n", "padding_values", "=", "(", "src_eos_id", ",", "# src", "\n", "0", ")", ")", "# src_len -- unused", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_iterator": [[80, 208], ["tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.contrib.data.Dataset.zip", "src_tgt_dataset.map.shuffle", "src_tgt_dataset.map.map", "src_tgt_dataset.map.filter", "src_tgt_dataset.map.map", "src_tgt_dataset.map.map", "src_tgt_dataset.map.map", "batching_func.make_initializable_iterator", "batched_dataset.make_initializable_iterator.get_next", "iterator_utils.BatchedInput", "src_vocab_table.lookup", "tgt_vocab_table.lookup", "tgt_vocab_table.lookup", "src_tgt_dataset.map.skip", "src_tgt_dataset.map.map", "src_tgt_dataset.map.map", "src_tgt_dataset.map.map", "x.padded_batch", "src_tgt_dataset.map.group_by_window", "iterator_utils.get_infer_iterator.batching_func"], "function", ["None"], ["batched_iter", "=", "batched_dataset", ".", "make_initializable_iterator", "(", ")", "\n", "(", "src_ids", ",", "src_seq_len", ")", "=", "batched_iter", ".", "get_next", "(", ")", "\n", "return", "BatchedInput", "(", "\n", "initializer", "=", "batched_iter", ".", "initializer", ",", "\n", "source", "=", "src_ids", ",", "\n", "target_input", "=", "None", ",", "\n", "target_output", "=", "None", ",", "\n", "source_sequence_length", "=", "src_seq_len", ",", "\n", "target_sequence_length", "=", "None", ",", "\n", "handle", "=", "None", ")", "\n", "\n", "\n", "", "def", "get_iterator", "(", "src_dataset", ",", "\n", "tgt_dataset", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "batch_size", ",", "\n", "sos", ",", "\n", "eos", ",", "\n", "source_reverse", ",", "\n", "random_seed", ",", "\n", "num_buckets", ",", "\n", "src_max_len", "=", "None", ",", "\n", "tgt_max_len", "=", "None", ",", "\n", "num_threads", "=", "4", ",", "\n", "output_buffer_size", "=", "None", ",", "\n", "skip_count", "=", "None", ",", "\n", "use_curriculum", "=", "False", ",", "\n", "curriculum_point_a", "=", "None", ",", "\n", "curriculum_point_b", "=", "None", ")", ":", "\n", "  ", "if", "not", "output_buffer_size", ":", "output_buffer_size", "=", "batch_size", "*", "1000", "\n", "src_eos_id", "=", "tf", ".", "cast", "(", "\n", "src_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "eos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "tgt_sos_id", "=", "tf", ".", "cast", "(", "\n", "tgt_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "sos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "tgt_eos_id", "=", "tf", ".", "cast", "(", "\n", "tgt_vocab_table", ".", "lookup", "(", "tf", ".", "constant", "(", "eos", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "\n", "src_tgt_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "zip", "(", "(", "src_dataset", ",", "tgt_dataset", ")", ")", "\n", "\n", "if", "skip_count", "is", "not", "None", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "skip", "(", "skip_count", ")", "\n", "\n", "", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "shuffle", "(", "\n", "output_buffer_size", ",", "random_seed", ")", "\n", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "\n", "tf", ".", "string_split", "(", "[", "src", "]", ")", ".", "values", ",", "tf", ".", "string_split", "(", "[", "tgt", "]", ")", ".", "values", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "\n", "# Filter zero length input sequences.", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "filter", "(", "\n", "lambda", "src", ",", "tgt", ":", "tf", ".", "logical_and", "(", "tf", ".", "size", "(", "src", ")", ">", "0", ",", "tf", ".", "size", "(", "tgt", ")", ">", "0", ")", ")", "\n", "\n", "if", "src_max_len", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "src", "[", ":", "src_max_len", "]", ",", "tgt", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "", "if", "tgt_max_len", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "src", ",", "tgt", "[", ":", "tgt_max_len", "]", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "", "if", "source_reverse", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "tf", ".", "reverse", "(", "src", ",", "axis", "=", "[", "0", "]", ")", ",", "tgt", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "\n", "", "if", "use_curriculum", ":", "\n", "    ", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "filter", "(", "\n", "lambda", "src", ",", "tgt", ":", "tf", ".", "logical_and", "(", "tf", ".", "size", "(", "src", ")", ">=", "curriculum_point_a", ",", "tf", ".", "size", "(", "src", ")", "<", "curriculum_point_b", ")", ")", "\n", "\n", "# Convert the word strings to ids.  Word strings that are not in the", "\n", "# vocab get the lookup table's default_value integer.", "\n", "", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "tf", ".", "cast", "(", "src_vocab_table", ".", "lookup", "(", "src", ")", ",", "tf", ".", "int32", ")", ",", "\n", "tf", ".", "cast", "(", "tgt_vocab_table", ".", "lookup", "(", "tgt", ")", ",", "tf", ".", "int32", ")", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "# Create a tgt_input prefixed with <sos> and a tgt_output suffixed with <eos>.", "\n", "# src_tgt_dataset = src_tgt_dataset.map(", "\n", "#     lambda src, tgt: (src,", "\n", "#                       tf.concat(([tgt_sos_id], [tgt_sos_id], tgt), 0),", "\n", "#                       tf.concat(([tgt_sos_id], tgt, [tgt_eos_id]), 0)),", "\n", "#     num_threads=num_threads, output_buffer_size=output_buffer_size)", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt", ":", "(", "src", ",", "\n", "tf", ".", "concat", "(", "(", "[", "tgt_sos_id", "]", ",", "tgt", ")", ",", "0", ")", ",", "\n", "tf", ".", "concat", "(", "(", "tgt", ",", "[", "tgt_eos_id", "]", ")", ",", "0", ")", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "# Add in sequence lengths.", "\n", "src_tgt_dataset", "=", "src_tgt_dataset", ".", "map", "(", "\n", "lambda", "src", ",", "tgt_in", ",", "tgt_out", ":", "(", "\n", "src", ",", "tgt_in", ",", "tgt_out", ",", "tf", ".", "size", "(", "src", ")", ",", "tf", ".", "size", "(", "tgt_in", ")", ")", ",", "\n", "num_threads", "=", "num_threads", ",", "\n", "output_buffer_size", "=", "output_buffer_size", ")", "\n", "# Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)", "\n", "def", "batching_func", "(", "x", ")", ":", "\n", "    ", "return", "x", ".", "apply", "(", "tf", ".", "contrib", ".", "data", ".", "padded_batch_and_drop_remainder", "(", "\n", "batch_size", ",", "\n", "padded_shapes", "=", "(", "tf", ".", "TensorShape", "(", "[", "src_max_len", "]", ")", ",", "# src", "\n", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", ",", "# tgt_input", "\n", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", ",", "# tgt_output", "\n", "tf", ".", "TensorShape", "(", "[", "]", ")", ",", "# src_len", "\n", "tf", ".", "TensorShape", "(", "[", "]", ")", ")", ",", "# tgt_len", "\n", "\n", "padding_values", "=", "(", "src_eos_id", ",", "# src", "\n", "tgt_eos_id", ",", "# tgt_input", "\n", "tgt_eos_id", ",", "# tgt_output", "\n", "0", ",", "# src_len -- unused", "\n", "0", ")", "\n", ")", ")", "\n", "\n", "# return x.padded_batch(", "\n", "#     batch_size,", "\n", "#     # The first three entries are the source and target line rows;", "\n", "#     # these have unknown-length vectors.  The last two entries are", "\n", "#     # the source and target row sizes; these are scalars.", "\n", "#     padded_shapes=(tf.TensorShape([src_max_len]),  # src", "\n", "#                    tf.TensorShape([None]),  # tgt_input", "\n", "#                    tf.TensorShape([None]),  # tgt_output", "\n", "#                    tf.TensorShape([]),      # src_len", "\n", "#                    tf.TensorShape([])),     # tgt_len", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.vocab_utils.check_vocab": [[37, 70], ["tensorflow.gfile.Exists", "len", "utils.misc_utils.print_out", "ValueError", "len", "utils.misc_utils.print_out", "os.path.join", "codecs.getreader", "tensorflow.gfile.GFile", "vocab.append", "os.path.basename", "word.strip", "codecs.getwriter", "tensorflow.gfile.GFile", "f.write"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["def", "check_vocab", "(", "vocab_file", ",", "out_dir", ",", "sos", "=", "None", ",", "eos", "=", "None", ",", "unk", "=", "None", ")", ":", "\n", "  ", "\"\"\"Check if vocab_file doesn't exist, create from corpus_file.\"\"\"", "\n", "if", "tf", ".", "gfile", ".", "Exists", "(", "vocab_file", ")", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"# Vocab file %s exists\"", "%", "vocab_file", ")", "\n", "vocab", "=", "[", "]", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"rb\"", ")", ")", "as", "f", ":", "\n", "      ", "vocab_size", "=", "0", "\n", "for", "word", "in", "f", ":", "\n", "        ", "vocab_size", "+=", "1", "\n", "vocab", ".", "append", "(", "word", ".", "strip", "(", ")", ")", "\n", "\n", "# Verify if the vocab starts with unk, sos, eos", "\n", "# If not, prepend those tokens & generate a new vocab file", "\n", "", "", "if", "not", "unk", ":", "unk", "=", "UNK", "\n", "if", "not", "sos", ":", "sos", "=", "SOS", "\n", "if", "not", "eos", ":", "eos", "=", "EOS", "\n", "assert", "len", "(", "vocab", ")", ">=", "3", "\n", "if", "vocab", "[", "0", "]", "!=", "unk", "or", "vocab", "[", "1", "]", "!=", "sos", "or", "vocab", "[", "2", "]", "!=", "eos", ":", "\n", "      ", "utils", ".", "print_out", "(", "\"The first 3 vocab words [%s, %s, %s]\"", "\n", "\" are not [%s, %s, %s]\"", "%", "\n", "(", "vocab", "[", "0", "]", ",", "vocab", "[", "1", "]", ",", "vocab", "[", "2", "]", ",", "unk", ",", "sos", ",", "eos", ")", ")", "\n", "vocab", "=", "[", "unk", ",", "sos", ",", "eos", "]", "+", "vocab", "\n", "vocab_size", "+=", "3", "\n", "new_vocab_file", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "os", ".", "path", ".", "basename", "(", "vocab_file", ")", ")", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "new_vocab_file", ",", "\"wb\"", ")", ")", "as", "f", ":", "\n", "        ", "for", "word", "in", "vocab", ":", "\n", "          ", "f", ".", "write", "(", "\"%s\\n\"", "%", "word", ")", "\n", "", "", "vocab_file", "=", "new_vocab_file", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"vocab_file does not exist: \"", "+", "vocab_file", ")", "\n", "\n", "", "vocab_size", "=", "len", "(", "vocab", ")", "\n", "return", "vocab_size", ",", "vocab_file", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.vocab_utils.create_vocab_tables": [[72, 82], ["tensorflow.python.ops.lookup_ops.index_table_from_file", "tensorflow.python.ops.lookup_ops.index_table_from_file"], "function", ["None"], ["", "def", "create_vocab_tables", "(", "src_vocab_file", ",", "tgt_vocab_file", ",", "share_vocab", ")", ":", "\n", "  ", "\"\"\"Creates vocab tables for src_vocab_file and tgt_vocab_file.\"\"\"", "\n", "src_vocab_table", "=", "lookup_ops", ".", "index_table_from_file", "(", "\n", "src_vocab_file", ",", "default_value", "=", "UNK_ID", ")", "\n", "if", "share_vocab", ":", "\n", "    ", "tgt_vocab_table", "=", "src_vocab_table", "\n", "", "else", ":", "\n", "    ", "tgt_vocab_table", "=", "lookup_ops", ".", "index_table_from_file", "(", "\n", "tgt_vocab_file", ",", "default_value", "=", "UNK_ID", ")", "\n", "", "return", "src_vocab_table", ",", "tgt_vocab_table", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_hparams": [[29, 104], ["tensorflow.contrib.training.HParams"], "function", ["None"], ["def", "create_test_hparams", "(", "unit_type", "=", "\"lstm\"", ",", "\n", "encoder_type", "=", "\"uni\"", ",", "\n", "num_layers", "=", "4", ",", "\n", "attention", "=", "\"\"", ",", "\n", "attention_architecture", "=", "None", ",", "\n", "use_residual", "=", "False", ",", "\n", "inference_indices", "=", "None", ",", "\n", "init_op", "=", "\"uniform\"", ")", ":", "\n", "  ", "\"\"\"Create training and inference test hparams.\"\"\"", "\n", "num_residual_layers", "=", "0", "\n", "if", "use_residual", ":", "\n", "# TODO(rzhao): Put num_residual_layers computation logic into", "\n", "# `model_utils.py`, so we can also test it here.", "\n", "    ", "num_residual_layers", "=", "2", "\n", "\n", "", "return", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "# Networks", "\n", "num_units", "=", "5", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "0.5", ",", "\n", "unit_type", "=", "unit_type", ",", "\n", "encoder_type", "=", "encoder_type", ",", "\n", "num_residual_layers", "=", "num_residual_layers", ",", "\n", "time_major", "=", "True", ",", "\n", "num_embeddings_partitions", "=", "0", ",", "\n", "\n", "# Attention mechanisms", "\n", "attention", "=", "attention", ",", "\n", "attention_architecture", "=", "attention_architecture", ",", "\n", "pass_hidden_state", "=", "True", ",", "\n", "\n", "# Train", "\n", "optimizer", "=", "\"sgd\"", ",", "\n", "init_op", "=", "init_op", ",", "\n", "init_weight", "=", "0.1", ",", "\n", "max_gradient_norm", "=", "5.0", ",", "\n", "max_emb_gradient_norm", "=", "None", ",", "\n", "learning_rate", "=", "1.0", ",", "\n", "start_decay_step", "=", "0", ",", "\n", "decay_factor", "=", "0.98", ",", "\n", "decay_steps", "=", "100", ",", "\n", "colocate_gradients_with_ops", "=", "True", ",", "\n", "batch_size", "=", "128", ",", "\n", "num_buckets", "=", "5", ",", "\n", "\n", "# Infer", "\n", "tgt_max_len_infer", "=", "100", ",", "\n", "infer_batch_size", "=", "32", ",", "\n", "beam_width", "=", "0", ",", "\n", "length_penalty_weight", "=", "0.0", ",", "\n", "\n", "# Misc", "\n", "forget_bias", "=", "0.0", ",", "\n", "num_gpus", "=", "1", ",", "\n", "share_vocab", "=", "False", ",", "\n", "random_seed", "=", "3", ",", "\n", "\n", "# Vocab", "\n", "src_vocab_size", "=", "5", ",", "\n", "tgt_vocab_size", "=", "5", ",", "\n", "eos", "=", "\"eos\"", ",", "\n", "sos", "=", "\"sos\"", ",", "\n", "\n", "# For inference.py test", "\n", "source_reverse", "=", "False", ",", "\n", "bpe_delimiter", "=", "\"@@\"", ",", "\n", "src", "=", "\"src\"", ",", "\n", "tgt", "=", "\"tgt\"", ",", "\n", "src_max_len", "=", "400", ",", "\n", "tgt_eos_id", "=", "0", ",", "\n", "# TODO(rzhao): Remove this after adding in-graph id to string lookup.", "\n", "tgt_vocab", "=", "[", "\"eos\"", ",", "\"test1\"", ",", "\"test2\"", ",", "\"test3\"", ",", "\"test4\"", ",", "\"test5\"", "]", ",", "\n", "src_max_len_infer", "=", "None", ",", "\n", "inference_indices", "=", "inference_indices", ",", "\n", "metrics", "=", "[", "\"bleu\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.common_test_utils.create_test_iterator": [[107, 148], ["tensorflow.python.ops.lookup_ops.index_table_from_tensor", "tensorflow.constant", "tensorflow.python.ops.lookup_ops.index_table_from_tensor", "tensorflow.contrib.data.Dataset.from_tensor_slices", "tensorflow.constant", "tensorflow.python.ops.lookup_ops.index_to_string_table_from_tensor", "tensorflow.constant", "tensorflow.contrib.data.Dataset.from_tensor_slices", "tensorflow.constant", "utils.iterator_utils.get_iterator", "utils.iterator_utils.get_infer_iterator"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_iterator", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_infer_iterator"], ["", "def", "create_test_iterator", "(", "hparams", ",", "mode", ")", ":", "\n", "  ", "\"\"\"Create test iterator.\"\"\"", "\n", "src_vocab_table", "=", "lookup_ops", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "[", "hparams", ".", "eos", ",", "\"a\"", ",", "\"b\"", ",", "\"c\"", ",", "\"d\"", "]", ")", ")", "\n", "tgt_vocab_mapping", "=", "tf", ".", "constant", "(", "[", "hparams", ".", "sos", ",", "hparams", ".", "eos", ",", "\"a\"", ",", "\"b\"", ",", "\"c\"", "]", ")", "\n", "tgt_vocab_table", "=", "lookup_ops", ".", "index_table_from_tensor", "(", "tgt_vocab_mapping", ")", "\n", "if", "mode", "==", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ":", "\n", "    ", "reverse_tgt_vocab_table", "=", "lookup_ops", ".", "index_to_string_table_from_tensor", "(", "\n", "tgt_vocab_mapping", ")", "\n", "\n", "", "src_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "[", "\"a a b b c\"", ",", "\"a b b\"", "]", ")", ")", "\n", "\n", "if", "mode", "!=", "tf", ".", "contrib", ".", "learn", ".", "ModeKeys", ".", "INFER", ":", "\n", "    ", "tgt_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "[", "\"a b c b c\"", ",", "\"a b c b\"", "]", ")", ")", "\n", "return", "(", "\n", "iterator_utils", ".", "get_iterator", "(", "\n", "src_dataset", "=", "src_dataset", ",", "\n", "tgt_dataset", "=", "tgt_dataset", ",", "\n", "src_vocab_table", "=", "src_vocab_table", ",", "\n", "tgt_vocab_table", "=", "tgt_vocab_table", ",", "\n", "batch_size", "=", "hparams", ".", "batch_size", ",", "\n", "sos", "=", "hparams", ".", "sos", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "source_reverse", "=", "hparams", ".", "source_reverse", ",", "\n", "random_seed", "=", "hparams", ".", "random_seed", ",", "\n", "num_buckets", "=", "hparams", ".", "num_buckets", ")", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ")", "\n", "", "else", ":", "\n", "    ", "return", "(", "\n", "iterator_utils", ".", "get_infer_iterator", "(", "\n", "src_dataset", "=", "src_dataset", ",", "\n", "src_vocab_table", "=", "src_vocab_table", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "source_reverse", "=", "hparams", ".", "source_reverse", ",", "\n", "batch_size", "=", "hparams", ".", "batch_size", ")", ",", "\n", "src_vocab_table", ",", "\n", "tgt_vocab_table", ",", "\n", "reverse_tgt_vocab_table", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.vocab_utils_test.VocabUtilsTest.testCheckVocab": [[31, 57], ["os.path.join", "os.makedirs", "os.path.join", "os.path.join", "os.makedirs", "utils.vocab_utils.check_vocab", "vocab_utils_test.VocabUtilsTest.assertEqual", "vocab_utils_test.VocabUtilsTest.assertEqual", "vocab_utils_test.VocabUtilsTest.assertEqual", "tensorflow.test.get_temp_dir", "tensorflow.test.get_temp_dir", "os.path.join", "codecs.getwriter", "tensorflow.gfile.GFile", "f.write", "len", "codecs.getreader", "tensorflow.gfile.GFile", "new_vocab.append", "line.strip"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.vocab_utils.check_vocab"], ["  ", "def", "testCheckVocab", "(", "self", ")", ":", "\n", "# Create a vocab file", "\n", "    ", "vocab_dir", "=", "os", ".", "path", ".", "join", "(", "tf", ".", "test", ".", "get_temp_dir", "(", ")", ",", "\"vocab_dir\"", ")", "\n", "os", ".", "makedirs", "(", "vocab_dir", ")", "\n", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_dir", ",", "\"vocab_file\"", ")", "\n", "vocab", "=", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", "]", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"wb\"", ")", ")", "as", "f", ":", "\n", "      ", "for", "word", "in", "vocab", ":", "\n", "        ", "f", ".", "write", "(", "\"%s\\n\"", "%", "word", ")", "\n", "\n", "# Call vocab_utils", "\n", "", "", "out_dir", "=", "os", ".", "path", ".", "join", "(", "tf", ".", "test", ".", "get_temp_dir", "(", ")", ",", "\"out_dir\"", ")", "\n", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "vocab_size", ",", "new_vocab_file", "=", "vocab_utils", ".", "check_vocab", "(", "\n", "vocab_file", ",", "out_dir", ")", "\n", "\n", "# Assert: we expect the code to add  <unk>, <s>, </s> and", "\n", "# create a new vocab file", "\n", "self", ".", "assertEqual", "(", "len", "(", "vocab", ")", "+", "3", ",", "vocab_size", ")", "\n", "self", ".", "assertEqual", "(", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"vocab_file\"", ")", ",", "new_vocab_file", ")", "\n", "new_vocab", "=", "[", "]", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "new_vocab_file", ",", "\"rb\"", ")", ")", "as", "f", ":", "\n", "      ", "for", "line", "in", "f", ":", "\n", "        ", "new_vocab", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "self", ".", "assertEqual", "(", "\n", "[", "vocab_utils", ".", "UNK", ",", "vocab_utils", ".", "SOS", ",", "vocab_utils", ".", "EOS", "]", "+", "vocab", ",", "new_vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.check_tensorflow_version": [[31, 34], ["EnvironmentError"], "function", ["None"], ["def", "check_tensorflow_version", "(", ")", ":", "\n", "  ", "if", "tf", ".", "__version__", "<", "\"1.2.0\"", ":", "\n", "    ", "raise", "EnvironmentError", "(", "\"Tensorflow version must >= 1.2.0\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.safe_exp": [[36, 43], ["math.exp", "float"], "function", ["None"], ["", "", "def", "safe_exp", "(", "value", ")", ":", "\n", "  ", "\"\"\"Exponentiation with catching of overflow error.\"\"\"", "\n", "try", ":", "\n", "    ", "ans", "=", "math", ".", "exp", "(", "value", ")", "\n", "", "except", "OverflowError", ":", "\n", "    ", "ans", "=", "float", "(", "\"inf\"", ")", "\n", "", "return", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_time": [[45, 50], ["print", "sys.stdout.flush", "time.time", "time.ctime", "time.time"], "function", ["None"], ["", "def", "print_time", "(", "s", ",", "start_time", ")", ":", "\n", "  ", "\"\"\"Take a start time, print elapsed duration, and return a new time.\"\"\"", "\n", "print", "(", "\"%s, time %ds, %s.\"", "%", "(", "s", ",", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ",", "time", ".", "ctime", "(", ")", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "return", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out": [[52, 71], ["isinstance", "s.decode.encode", "print", "sys.stdout.flush", "s.decode.decode", "f.write", "isinstance", "out_s.decode.decode", "sys.stdout.write", "s.decode.encode", "f.write"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode"], ["", "def", "print_out", "(", "s", ",", "f", "=", "None", ",", "new_line", "=", "True", ")", ":", "\n", "  ", "\"\"\"Similar to print but with support to flush and output to a file.\"\"\"", "\n", "if", "isinstance", "(", "s", ",", "bytes", ")", ":", "\n", "    ", "s", "=", "s", ".", "decode", "(", "\"utf-8\"", ")", "\n", "\n", "", "if", "f", ":", "\n", "    ", "f", ".", "write", "(", "s", ".", "encode", "(", "\"utf-8\"", ")", ")", "\n", "if", "new_line", ":", "\n", "      ", "f", ".", "write", "(", "b\"\\n\"", ")", "\n", "\n", "# stdout", "\n", "", "", "out_s", "=", "s", ".", "encode", "(", "\"utf-8\"", ")", "\n", "if", "not", "isinstance", "(", "out_s", ",", "str", ")", ":", "\n", "    ", "out_s", "=", "out_s", ".", "decode", "(", "\"utf-8\"", ")", "\n", "", "print", "(", "out_s", ",", "end", "=", "\"\"", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "\n", "if", "new_line", ":", "\n", "    ", "sys", ".", "stdout", ".", "write", "(", "\"\\n\"", ")", "\n", "", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_hparams": [[73, 80], ["hparams.values", "sorted", "hparams.values.keys", "all", "misc_utils.print_out", "str"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "print_hparams", "(", "hparams", ",", "skip_patterns", "=", "None", ")", ":", "\n", "  ", "\"\"\"Print hparams, can skip keys based on pattern.\"\"\"", "\n", "values", "=", "hparams", ".", "values", "(", ")", "\n", "for", "key", "in", "sorted", "(", "values", ".", "keys", "(", ")", ")", ":", "\n", "    ", "if", "not", "skip_patterns", "or", "all", "(", "\n", "[", "skip_pattern", "not", "in", "key", "for", "skip_pattern", "in", "skip_patterns", "]", ")", ":", "\n", "      ", "print_out", "(", "\"  %s=%s\"", "%", "(", "key", ",", "str", "(", "values", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.load_hparams": [[82, 97], ["os.path.join", "tensorflow.gfile.Exists", "misc_utils.print_out", "codecs.getreader", "tensorflow.gfile.GFile", "json.load", "tensorflow.contrib.training.HParams", "misc_utils.print_out"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "", "", "def", "load_hparams", "(", "model_dir", ")", ":", "\n", "  ", "\"\"\"Load hparams from an existing model directory.\"\"\"", "\n", "hparams_file", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "\"hparams\"", ")", "\n", "if", "tf", ".", "gfile", ".", "Exists", "(", "hparams_file", ")", ":", "\n", "    ", "print_out", "(", "\"# Loading hparams from %s\"", "%", "hparams_file", ")", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "hparams_file", ",", "\"rb\"", ")", ")", "as", "f", ":", "\n", "      ", "try", ":", "\n", "        ", "hparams_values", "=", "json", ".", "load", "(", "f", ")", "\n", "hparams", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "**", "hparams_values", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "print_out", "(", "\"  can't load hparams file\"", ")", "\n", "return", "None", "\n", "", "", "return", "hparams", "\n", "", "else", ":", "\n", "    ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.maybe_parse_standard_hparams": [[99, 110], ["tensorflow.gfile.Exists", "misc_utils.print_out", "tensorflow.gfile.GFile", "hparams.parse_json", "f.read"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "", "def", "maybe_parse_standard_hparams", "(", "hparams", ",", "hparams_path", ")", ":", "\n", "  ", "\"\"\"Override hparams values with existing standard hparams config.\"\"\"", "\n", "if", "not", "hparams_path", ":", "\n", "    ", "return", "hparams", "\n", "\n", "", "if", "tf", ".", "gfile", ".", "Exists", "(", "hparams_path", ")", ":", "\n", "    ", "print_out", "(", "\"# Loading standard hparams from %s\"", "%", "hparams_path", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "hparams_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "      ", "hparams", ".", "parse_json", "(", "f", ".", "read", "(", ")", ")", "\n", "\n", "", "", "return", "hparams", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.save_hparams": [[112, 118], ["os.path.join", "misc_utils.print_out", "f.write", "codecs.getwriter", "tensorflow.gfile.GFile", "hparams.to_json"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out"], ["", "def", "save_hparams", "(", "out_dir", ",", "hparams", ")", ":", "\n", "  ", "\"\"\"Save hparams.\"\"\"", "\n", "hparams_file", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"hparams\"", ")", "\n", "print_out", "(", "\"  saving hparams to %s\"", "%", "hparams_file", ")", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "hparams_file", ",", "\"wb\"", ")", ")", "as", "f", ":", "\n", "    ", "f", ".", "write", "(", "hparams", ".", "to_json", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.debug_tensor": [[120, 125], ["tensorflow.Print", "tensorflow.shape"], "function", ["None"], ["", "", "def", "debug_tensor", "(", "s", ",", "msg", "=", "None", ",", "summarize", "=", "10", ")", ":", "\n", "  ", "\"\"\"Print the shape and value of a tensor at test time. Return a new tensor.\"\"\"", "\n", "if", "not", "msg", ":", "\n", "    ", "msg", "=", "s", ".", "name", "\n", "", "return", "tf", ".", "Print", "(", "s", ",", "[", "tf", ".", "shape", "(", "s", ")", ",", "s", "]", ",", "msg", "+", "\" \"", ",", "summarize", "=", "summarize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.add_summary": [[127, 133], ["tensorflow.Summary", "summary_writer.add_summary", "tensorflow.Summary.Value"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.add_summary"], ["", "def", "add_summary", "(", "summary_writer", ",", "global_step", ",", "tag", ",", "value", ")", ":", "\n", "  ", "\"\"\"Add a new summary to the current summary_writer.\n  Useful to log things that are not part of the training graph, e.g., tag=BLEU.\n  \"\"\"", "\n", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "[", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "tag", ",", "simple_value", "=", "value", ")", "]", ")", "\n", "summary_writer", ".", "add_summary", "(", "summary", ",", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.get_config_proto": [[135, 143], ["tensorflow.ConfigProto"], "function", ["None"], ["", "def", "get_config_proto", "(", "log_device_placement", "=", "False", ",", "allow_soft_placement", "=", "True", ")", ":", "\n", "# GPU options:", "\n", "# https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html", "\n", "  ", "config_proto", "=", "tf", ".", "ConfigProto", "(", "\n", "log_device_placement", "=", "log_device_placement", ",", "\n", "allow_soft_placement", "=", "allow_soft_placement", ")", "\n", "config_proto", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "return", "config_proto", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.format_text": [[145, 151], ["hasattr", "isinstance"], "function", ["None"], ["", "def", "format_text", "(", "words", ")", ":", "\n", "  ", "\"\"\"Convert a sequence words into sentence.\"\"\"", "\n", "if", "(", "not", "hasattr", "(", "words", ",", "\"__len__\"", ")", "and", "# for numpy array", "\n", "not", "isinstance", "(", "words", ",", "collections", ".", "Iterable", ")", ")", ":", "\n", "    ", "words", "=", "[", "words", "]", "\n", "", "return", "b\" \"", ".", "join", "(", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.format_bpe_text": [[153, 168], ["isinstance", "len", "symbols.encode.encode", "words.append", "len"], "function", ["None"], ["", "def", "format_bpe_text", "(", "symbols", ",", "delimiter", "=", "b\"@@\"", ")", ":", "\n", "  ", "\"\"\"Convert a sequence of bpe words into sentence.\"\"\"", "\n", "words", "=", "[", "]", "\n", "word", "=", "b\"\"", "\n", "if", "isinstance", "(", "symbols", ",", "str", ")", ":", "\n", "    ", "symbols", "=", "symbols", ".", "encode", "(", ")", "\n", "", "delimiter_len", "=", "len", "(", "delimiter", ")", "\n", "for", "symbol", "in", "symbols", ":", "\n", "    ", "if", "len", "(", "symbol", ")", ">=", "delimiter_len", "and", "symbol", "[", "-", "delimiter_len", ":", "]", "==", "delimiter", ":", "\n", "      ", "word", "+=", "symbol", "[", ":", "-", "delimiter_len", "]", "\n", "", "else", ":", "# end of a word", "\n", "      ", "word", "+=", "symbol", "\n", "words", ".", "append", "(", "word", ")", "\n", "word", "=", "b\"\"", "\n", "", "", "return", "b\" \"", ".", "join", "(", "words", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.decode_and_evaluate": [[30, 85], ["utils.misc_utils.print_out", "time.time", "tensorflow.gfile.Exists", "trans_f.write", "utils.evaluation_utils.evaluate", "utils.misc_utils.print_out", "codecs.getwriter", "tensorflow.gfile.GFile", "model.decode", "len", "range", "len", "nmt_utils.get_translation", "trans_f.write", "utils.misc_utils.print_time"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils.evaluate", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_out", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.model.BaseModel.decode", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.get_translation", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.print_time"], ["def", "decode_and_evaluate", "(", "name", ",", "\n", "model", ",", "\n", "sess", ",", "\n", "trans_file", ",", "\n", "ref_file", ",", "\n", "metrics", ",", "\n", "bpe_delimiter", ",", "\n", "beam_width", ",", "\n", "tgt_sos", ",", "\n", "tgt_eos", ",", "\n", "decode", "=", "True", ")", ":", "\n", "  ", "\"\"\"Decode a test set and compute a score according to the evaluation task.\"\"\"", "\n", "# Decode", "\n", "if", "decode", ":", "\n", "    ", "utils", ".", "print_out", "(", "\"  decoding to output %s.\"", "%", "trans_file", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "num_sentences", "=", "0", "\n", "with", "codecs", ".", "getwriter", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "trans_file", ",", "mode", "=", "\"wb\"", ")", ")", "as", "trans_f", ":", "\n", "      ", "trans_f", ".", "write", "(", "\"\"", ")", "# Write empty string to ensure file is created.", "\n", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "          ", "nmt_outputs", ",", "_", ",", "_", "=", "model", ".", "decode", "(", "sess", ")", "\n", "\n", "if", "beam_width", ">", "0", ":", "\n", "# get the top translation.", "\n", "            ", "nmt_outputs", "=", "nmt_outputs", "[", "0", "]", "\n", "\n", "", "num_sentences", "+=", "len", "(", "nmt_outputs", ")", "\n", "for", "sent_id", "in", "range", "(", "len", "(", "nmt_outputs", ")", ")", ":", "\n", "            ", "translation", "=", "get_translation", "(", "\n", "nmt_outputs", ",", "\n", "sent_id", ",", "\n", "tgt_sos", "=", "tgt_sos", ",", "\n", "tgt_eos", "=", "tgt_eos", ",", "\n", "bpe_delimiter", "=", "bpe_delimiter", ")", "\n", "trans_f", ".", "write", "(", "(", "translation", "+", "b\"\\n\"", ")", ".", "decode", "(", "\"utf-8\"", ")", ")", "\n", "", "", "except", "tf", ".", "errors", ".", "OutOfRangeError", ":", "\n", "          ", "utils", ".", "print_time", "(", "\"  done, num sentences %d\"", "%", "num_sentences", ",", "\n", "start_time", ")", "\n", "break", "\n", "\n", "# Evaluation", "\n", "", "", "", "", "evaluation_scores", "=", "{", "}", "\n", "if", "ref_file", "and", "tf", ".", "gfile", ".", "Exists", "(", "trans_file", ")", ":", "\n", "    ", "for", "metric", "in", "metrics", ":", "\n", "      ", "score", "=", "evaluation_utils", ".", "evaluate", "(", "\n", "ref_file", ",", "\n", "trans_file", ",", "\n", "metric", ",", "\n", "bpe_delimiter", "=", "bpe_delimiter", ")", "\n", "evaluation_scores", "[", "metric", "]", "=", "score", "\n", "utils", ".", "print_out", "(", "\"  %s %s: %.1f\"", "%", "(", "metric", ",", "name", ",", "score", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.nmt_utils.get_translation": [[87, 104], ["nmt_outputs[].tolist", "tgt_eos.encode.encode", "bpe_delimiter.encode.encode", "utils.misc_utils.format_text", "utils.misc_utils.format_bpe_text", "nmt_outputs[].tolist.index"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.format_text", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.format_bpe_text"], ["\n", "\n", "", "def", "get_translation", "(", "nmt_outputs", ",", "sent_id", ",", "tgt_sos", ",", "tgt_eos", ",", "bpe_delimiter", ")", ":", "\n", "  ", "\"\"\"Given batch decoding outputs, select a sentence and turn to text.\"\"\"", "\n", "if", "tgt_sos", ":", "tgt_sos", "=", "tgt_sos", ".", "encode", "(", "\"utf-8\"", ")", "\n", "if", "tgt_eos", ":", "tgt_eos", "=", "tgt_eos", ".", "encode", "(", "\"utf-8\"", ")", "\n", "if", "bpe_delimiter", ":", "bpe_delimiter", "=", "bpe_delimiter", ".", "encode", "(", "\"utf-8\"", ")", "\n", "# Select a sentence", "\n", "output", "=", "nmt_outputs", "[", "sent_id", ",", ":", "]", ".", "tolist", "(", ")", "\n", "\n", "if", "tgt_sos", "and", "output", "[", "0", "]", "==", "tgt_sos", ":", "\n", "    ", "output", "=", "output", "[", "1", ":", "]", "\n", "\n", "# If there is an eos symbol in outputs, cut them at that point.", "\n", "", "if", "tgt_eos", "and", "tgt_eos", "in", "output", ":", "\n", "    ", "output", "=", "output", "[", ":", "output", ".", "index", "(", "tgt_eos", ")", "]", "\n", "\n", "", "if", "not", "bpe_delimiter", ":", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils_test.MiscUtilsTest.testFormatBpeText": [[30, 41], ["misc_utils_test.MiscUtilsTest.assertEqual", "utils.misc_utils.format_bpe_text", "bpe_line.split"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.misc_utils.format_bpe_text"], ["  ", "def", "testFormatBpeText", "(", "self", ")", ":", "\n", "    ", "bpe_line", "=", "(", "\n", "b\"En@@ ough to make already reluc@@ tant men hesitate to take screening\"", "\n", "b\" tests .\"", "\n", ")", "\n", "expected_result", "=", "(", "\n", "b\"Enough to make already reluctant men hesitate to take screening tests\"", "\n", "b\" .\"", "\n", ")", "\n", "self", ".", "assertEqual", "(", "expected_result", ",", "\n", "misc_utils", ".", "format_bpe_text", "(", "bpe_line", ".", "split", "(", "b\" \"", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils_test.IteratorUtilsTest.testGetIterator": [[31, 108], ["tensorflow.python.ops.lookup_ops.index_table_from_tensor", "tensorflow.contrib.data.Dataset.from_tensor_slices", "tensorflow.contrib.data.Dataset.from_tensor_slices", "tensorflow.contrib.training.HParams", "utils.iterator_utils.get_iterator", "tensorflow.tables_initializer", "iterator_utils_test.IteratorUtilsTest.assertEqual", "iterator_utils_test.IteratorUtilsTest.assertEqual", "iterator_utils_test.IteratorUtilsTest.assertEqual", "iterator_utils_test.IteratorUtilsTest.assertEqual", "iterator_utils_test.IteratorUtilsTest.assertEqual", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "source.shape.as_list", "target_input.shape.as_list", "target_output.shape.as_list", "src_seq_len.shape.as_list", "tgt_seq_len.shape.as_list", "iterator_utils_test.IteratorUtilsTest.test_session", "sess.run", "sess.run", "sess.run", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "sess.run", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertRaisesOpError", "sess.run"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_iterator"], ["  ", "def", "testGetIterator", "(", "self", ")", ":", "\n", "    ", "tgt_vocab_table", "=", "src_vocab_table", "=", "lookup_ops", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", ",", "\"eos\"", ",", "\"sos\"", "]", ")", ")", "\n", "src_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "[", "\"c c a\"", ",", "\"c a\"", ",", "\"d\"", ",", "\"f e a g\"", "]", ")", ")", "\n", "tgt_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "[", "\"a b\"", ",", "\"b c\"", ",", "\"\"", ",", "\"c c\"", "]", ")", ")", "\n", "hparams", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "random_seed", "=", "3", ",", "\n", "num_buckets", "=", "5", ",", "\n", "source_reverse", "=", "False", ",", "\n", "eos", "=", "\"eos\"", ",", "\n", "sos", "=", "\"sos\"", ")", "\n", "batch_size", "=", "2", "\n", "src_max_len", "=", "3", "\n", "iterator", "=", "iterator_utils", ".", "get_iterator", "(", "\n", "src_dataset", "=", "src_dataset", ",", "\n", "tgt_dataset", "=", "tgt_dataset", ",", "\n", "src_vocab_table", "=", "src_vocab_table", ",", "\n", "tgt_vocab_table", "=", "tgt_vocab_table", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "sos", "=", "hparams", ".", "sos", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "source_reverse", "=", "hparams", ".", "source_reverse", ",", "\n", "random_seed", "=", "hparams", ".", "random_seed", ",", "\n", "num_buckets", "=", "hparams", ".", "num_buckets", ",", "\n", "src_max_len", "=", "src_max_len", ")", "\n", "table_initializer", "=", "tf", ".", "tables_initializer", "(", ")", "\n", "source", "=", "iterator", ".", "source", "\n", "target_input", "=", "iterator", ".", "target_input", "\n", "target_output", "=", "iterator", ".", "target_output", "\n", "src_seq_len", "=", "iterator", ".", "source_sequence_length", "\n", "tgt_seq_len", "=", "iterator", ".", "target_sequence_length", "\n", "self", ".", "assertEqual", "(", "[", "None", ",", "None", "]", ",", "source", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "[", "None", ",", "None", "]", ",", "target_input", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "[", "None", ",", "None", "]", ",", "target_output", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "[", "None", "]", ",", "src_seq_len", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "[", "None", "]", ",", "tgt_seq_len", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "sess", ".", "run", "(", "table_initializer", ")", "\n", "sess", ".", "run", "(", "iterator", ".", "initializer", ")", "\n", "\n", "(", "source_v", ",", "src_len_v", ",", "target_input_v", ",", "target_output_v", ",", "tgt_len_v", ")", "=", "(", "\n", "sess", ".", "run", "(", "(", "source", ",", "src_seq_len", ",", "target_input", ",", "target_output", ",", "\n", "tgt_seq_len", ")", ")", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "-", "1", ",", "-", "1", ",", "0", "]", ",", "# \"f\" == unknown, \"e\" == unknown, a", "\n", "[", "2", ",", "0", ",", "3", "]", "]", ",", "# c a eos -- eos is padding", "\n", "source_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", ",", "2", "]", ",", "src_len_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "4", ",", "2", ",", "2", "]", ",", "# sos c c", "\n", "[", "4", ",", "1", ",", "2", "]", "]", ",", "# sos b c", "\n", "target_input_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "2", ",", "2", ",", "3", "]", ",", "# c c eos", "\n", "[", "1", ",", "2", ",", "3", "]", "]", ",", "# b c eos", "\n", "target_output_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", ",", "3", "]", ",", "tgt_len_v", ")", "\n", "\n", "(", "source_v", ",", "src_len_v", ",", "target_input_v", ",", "target_output_v", ",", "tgt_len_v", ")", "=", "(", "\n", "sess", ".", "run", "(", "(", "source", ",", "src_seq_len", ",", "target_input", ",", "target_output", ",", "\n", "tgt_seq_len", ")", ")", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "2", ",", "2", ",", "0", "]", "]", ",", "# c c a", "\n", "source_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", "]", ",", "src_len_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "4", ",", "0", ",", "1", "]", "]", ",", "# sos a b", "\n", "target_input_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "0", ",", "1", ",", "3", "]", "]", ",", "# a b eos", "\n", "target_output_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", "]", ",", "tgt_len_v", ")", "\n", "\n", "with", "self", ".", "assertRaisesOpError", "(", "\"End of sequence\"", ")", ":", "\n", "        ", "sess", ".", "run", "(", "source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils_test.IteratorUtilsTest.testGetIteratorWithSkipCount": [[110, 210], ["tensorflow.python.ops.lookup_ops.index_table_from_tensor", "tensorflow.contrib.data.Dataset.from_tensor_slices", "tensorflow.contrib.data.Dataset.from_tensor_slices", "tensorflow.contrib.training.HParams", "tensorflow.placeholder", "utils.iterator_utils.get_iterator", "tensorflow.tables_initializer", "iterator_utils_test.IteratorUtilsTest.assertEqual", "iterator_utils_test.IteratorUtilsTest.assertEqual", "iterator_utils_test.IteratorUtilsTest.assertEqual", "iterator_utils_test.IteratorUtilsTest.assertEqual", "iterator_utils_test.IteratorUtilsTest.assertEqual", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "source.shape.as_list", "target_input.shape.as_list", "target_output.shape.as_list", "src_seq_len.shape.as_list", "tgt_seq_len.shape.as_list", "iterator_utils_test.IteratorUtilsTest.test_session", "sess.run", "sess.run", "sess.run", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "sess.run", "sess.run", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "sess.run", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertRaisesOpError", "sess.run", "iterator_utils_test.IteratorUtilsTest.assertRaisesOpError", "sess.run"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_iterator"], ["", "", "", "def", "testGetIteratorWithSkipCount", "(", "self", ")", ":", "\n", "    ", "tgt_vocab_table", "=", "src_vocab_table", "=", "lookup_ops", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", ",", "\"eos\"", ",", "\"sos\"", "]", ")", ")", "\n", "src_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "[", "\"c c a\"", ",", "\"c a\"", ",", "\"d\"", ",", "\"f e a g\"", "]", ")", ")", "\n", "tgt_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "[", "\"a b\"", ",", "\"b c\"", ",", "\"\"", ",", "\"c c\"", "]", ")", ")", "\n", "hparams", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "random_seed", "=", "3", ",", "\n", "num_buckets", "=", "5", ",", "\n", "source_reverse", "=", "False", ",", "\n", "eos", "=", "\"eos\"", ",", "\n", "sos", "=", "\"sos\"", ")", "\n", "batch_size", "=", "2", "\n", "src_max_len", "=", "3", "\n", "skip_count", "=", "tf", ".", "placeholder", "(", "shape", "=", "(", ")", ",", "dtype", "=", "tf", ".", "int64", ")", "\n", "iterator", "=", "iterator_utils", ".", "get_iterator", "(", "\n", "src_dataset", "=", "src_dataset", ",", "\n", "tgt_dataset", "=", "tgt_dataset", ",", "\n", "src_vocab_table", "=", "src_vocab_table", ",", "\n", "tgt_vocab_table", "=", "tgt_vocab_table", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "sos", "=", "hparams", ".", "sos", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "source_reverse", "=", "hparams", ".", "source_reverse", ",", "\n", "random_seed", "=", "hparams", ".", "random_seed", ",", "\n", "num_buckets", "=", "hparams", ".", "num_buckets", ",", "\n", "src_max_len", "=", "src_max_len", ",", "\n", "skip_count", "=", "skip_count", ")", "\n", "table_initializer", "=", "tf", ".", "tables_initializer", "(", ")", "\n", "source", "=", "iterator", ".", "source", "\n", "target_input", "=", "iterator", ".", "target_input", "\n", "target_output", "=", "iterator", ".", "target_output", "\n", "src_seq_len", "=", "iterator", ".", "source_sequence_length", "\n", "tgt_seq_len", "=", "iterator", ".", "target_sequence_length", "\n", "self", ".", "assertEqual", "(", "[", "None", ",", "None", "]", ",", "source", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "[", "None", ",", "None", "]", ",", "target_input", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "[", "None", ",", "None", "]", ",", "target_output", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "[", "None", "]", ",", "src_seq_len", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "[", "None", "]", ",", "tgt_seq_len", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "sess", ".", "run", "(", "table_initializer", ")", "\n", "sess", ".", "run", "(", "iterator", ".", "initializer", ",", "feed_dict", "=", "{", "skip_count", ":", "3", "}", ")", "\n", "\n", "(", "source_v", ",", "src_len_v", ",", "target_input_v", ",", "target_output_v", ",", "tgt_len_v", ")", "=", "(", "\n", "sess", ".", "run", "(", "(", "source", ",", "src_seq_len", ",", "target_input", ",", "target_output", ",", "\n", "tgt_seq_len", ")", ")", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "-", "1", ",", "-", "1", ",", "0", "]", "]", ",", "# \"f\" == unknown, \"e\" == unknown, a", "\n", "source_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", "]", ",", "src_len_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "4", ",", "2", ",", "2", "]", "]", ",", "# sos c c", "\n", "target_input_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "2", ",", "2", ",", "3", "]", "]", ",", "# c c eos", "\n", "target_output_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", "]", ",", "tgt_len_v", ")", "\n", "\n", "with", "self", ".", "assertRaisesOpError", "(", "\"End of sequence\"", ")", ":", "\n", "        ", "sess", ".", "run", "(", "source", ")", "\n", "\n", "# Re-init iterator with skip_count=0.", "\n", "", "sess", ".", "run", "(", "iterator", ".", "initializer", ",", "feed_dict", "=", "{", "skip_count", ":", "0", "}", ")", "\n", "\n", "(", "source_v", ",", "src_len_v", ",", "target_input_v", ",", "target_output_v", ",", "tgt_len_v", ")", "=", "(", "\n", "sess", ".", "run", "(", "(", "source", ",", "src_seq_len", ",", "target_input", ",", "target_output", ",", "\n", "tgt_seq_len", ")", ")", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "-", "1", ",", "-", "1", ",", "0", "]", ",", "# \"f\" == unknown, \"e\" == unknown, a", "\n", "[", "2", ",", "0", ",", "3", "]", "]", ",", "# c a eos -- eos is padding", "\n", "source_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", ",", "2", "]", ",", "src_len_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "4", ",", "2", ",", "2", "]", ",", "# sos c c", "\n", "[", "4", ",", "1", ",", "2", "]", "]", ",", "# sos b c", "\n", "target_input_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "2", ",", "2", ",", "3", "]", ",", "# c c eos", "\n", "[", "1", ",", "2", ",", "3", "]", "]", ",", "# b c eos", "\n", "target_output_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", ",", "3", "]", ",", "tgt_len_v", ")", "\n", "\n", "(", "source_v", ",", "src_len_v", ",", "target_input_v", ",", "target_output_v", ",", "tgt_len_v", ")", "=", "(", "\n", "sess", ".", "run", "(", "(", "source", ",", "src_seq_len", ",", "target_input", ",", "target_output", ",", "\n", "tgt_seq_len", ")", ")", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "2", ",", "2", ",", "0", "]", "]", ",", "# c c a", "\n", "source_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", "]", ",", "src_len_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "4", ",", "0", ",", "1", "]", "]", ",", "# sos a b", "\n", "target_input_v", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "0", ",", "1", ",", "3", "]", "]", ",", "# a b eos", "\n", "target_output_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", "]", ",", "tgt_len_v", ")", "\n", "\n", "with", "self", ".", "assertRaisesOpError", "(", "\"End of sequence\"", ")", ":", "\n", "        ", "sess", ".", "run", "(", "source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils_test.IteratorUtilsTest.testGetInferIterator": [[212, 256], ["tensorflow.python.ops.lookup_ops.index_table_from_tensor", "tensorflow.contrib.data.Dataset.from_tensor_slices", "tensorflow.contrib.training.HParams", "utils.iterator_utils.get_infer_iterator", "tensorflow.tables_initializer", "iterator_utils_test.IteratorUtilsTest.assertEqual", "iterator_utils_test.IteratorUtilsTest.assertEqual", "tensorflow.constant", "tensorflow.constant", "source.shape.as_list", "seq_len.shape.as_list", "iterator_utils_test.IteratorUtilsTest.test_session", "sess.run", "sess.run", "sess.run", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "sess.run", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertAllEqual", "iterator_utils_test.IteratorUtilsTest.assertRaisesOpError", "sess.run"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.iterator_utils.get_infer_iterator"], ["", "", "", "def", "testGetInferIterator", "(", "self", ")", ":", "\n", "    ", "src_vocab_table", "=", "lookup_ops", ".", "index_table_from_tensor", "(", "\n", "tf", ".", "constant", "(", "[", "\"a\"", ",", "\"b\"", ",", "\"c\"", ",", "\"eos\"", ",", "\"sos\"", "]", ")", ")", "\n", "src_dataset", "=", "tf", ".", "contrib", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "\n", "tf", ".", "constant", "(", "[", "\"c c a\"", ",", "\"c a\"", ",", "\"d\"", ",", "\"f e a g\"", "]", ")", ")", "\n", "hparams", "=", "tf", ".", "contrib", ".", "training", ".", "HParams", "(", "\n", "random_seed", "=", "3", ",", "\n", "source_reverse", "=", "False", ",", "\n", "eos", "=", "\"eos\"", ",", "\n", "sos", "=", "\"sos\"", ")", "\n", "batch_size", "=", "2", "\n", "src_max_len", "=", "3", "\n", "iterator", "=", "iterator_utils", ".", "get_infer_iterator", "(", "\n", "src_dataset", "=", "src_dataset", ",", "\n", "src_vocab_table", "=", "src_vocab_table", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "eos", "=", "hparams", ".", "eos", ",", "\n", "source_reverse", "=", "hparams", ".", "source_reverse", ",", "\n", "src_max_len", "=", "src_max_len", ")", "\n", "table_initializer", "=", "tf", ".", "tables_initializer", "(", ")", "\n", "source", "=", "iterator", ".", "source", "\n", "seq_len", "=", "iterator", ".", "source_sequence_length", "\n", "self", ".", "assertEqual", "(", "[", "None", ",", "None", "]", ",", "source", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "[", "None", "]", ",", "seq_len", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "with", "self", ".", "test_session", "(", ")", "as", "sess", ":", "\n", "      ", "sess", ".", "run", "(", "table_initializer", ")", "\n", "sess", ".", "run", "(", "iterator", ".", "initializer", ")", "\n", "\n", "(", "source_v", ",", "seq_len_v", ")", "=", "sess", ".", "run", "(", "(", "source", ",", "seq_len", ")", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "2", ",", "2", ",", "0", "]", ",", "# c c a", "\n", "[", "2", ",", "0", ",", "3", "]", "]", ",", "# c a eos", "\n", "source_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "3", ",", "2", "]", ",", "seq_len_v", ")", "\n", "\n", "(", "source_v", ",", "seq_len_v", ")", "=", "sess", ".", "run", "(", "(", "source", ",", "seq_len", ")", ")", "\n", "self", ".", "assertAllEqual", "(", "\n", "[", "[", "-", "1", ",", "3", ",", "3", "]", ",", "# \"d\" == unknown, eos eos", "\n", "[", "-", "1", ",", "-", "1", ",", "0", "]", "]", ",", "# \"f\" == unknown, \"e\" == unknown, a", "\n", "source_v", ")", "\n", "self", ".", "assertAllEqual", "(", "[", "1", ",", "3", "]", ",", "seq_len_v", ")", "\n", "\n", "with", "self", ".", "assertRaisesOpError", "(", "\"End of sequence\"", ")", ":", "\n", "        ", "sess", ".", "run", "(", "(", "source", ",", "seq_len", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils_test.EvaluationUtilsTest.testEvaluate": [[29, 45], ["utils.evaluation_utils.evaluate", "utils.evaluation_utils.evaluate", "evaluation_utils_test.EvaluationUtilsTest.assertAlmostEqual", "evaluation_utils_test.EvaluationUtilsTest.assertAlmostEqual"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils.evaluate", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils.evaluate"], ["  ", "def", "testEvaluate", "(", "self", ")", ":", "\n", "    ", "output", "=", "\"nmt/testdata/deen_output\"", "\n", "ref_bpe", "=", "\"nmt/testdata/deen_ref_bpe\"", "\n", "\n", "bpe_delimiter", "=", "\"@@\"", "\n", "\n", "expected_blue_score", "=", "22.5855084573", "\n", "expected_rouge_score", "=", "50.8429782599", "\n", "\n", "bleu_score", "=", "evaluation_utils", ".", "evaluate", "(", "\n", "ref_bpe", ",", "output", ",", "\"bleu\"", ",", "bpe_delimiter", ")", "\n", "rouge_score", "=", "evaluation_utils", ".", "evaluate", "(", "\n", "ref_bpe", ",", "output", ",", "\"rouge\"", ",", "bpe_delimiter", ")", "\n", "\n", "self", ".", "assertAlmostEqual", "(", "expected_blue_score", ",", "bleu_score", ")", "\n", "self", ".", "assertAlmostEqual", "(", "expected_rouge_score", ",", "rouge_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils_test.EvaluationUtilsTest.testAccuracy": [[46, 55], ["utils.evaluation_utils.evaluate", "evaluation_utils_test.EvaluationUtilsTest.assertAlmostEqual"], "methods", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils.evaluate"], ["", "def", "testAccuracy", "(", "self", ")", ":", "\n", "    ", "pred_output", "=", "\"nmt/testdata/pred_output\"", "\n", "label_ref", "=", "\"nmt/testdata/label_ref\"", "\n", "\n", "expected_accuracy_score", "=", "60.00", "\n", "\n", "accuracy_score", "=", "evaluation_utils", ".", "evaluate", "(", "\n", "label_ref", ",", "pred_output", ",", "\"accuracy\"", ")", "\n", "self", ".", "assertAlmostEqual", "(", "expected_accuracy_score", ",", "accuracy_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils.evaluate": [[31, 47], ["metric.lower", "evaluation_utils._bleu", "metric.lower", "evaluation_utils._rouge", "metric.lower", "evaluation_utils._accuracy", "ValueError"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._bleu", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._rouge", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._accuracy"], ["def", "evaluate", "(", "ref_file", ",", "trans_file", ",", "metric", ",", "bpe_delimiter", "=", "None", ")", ":", "\n", "  ", "\"\"\"Pick a metric and evaluate depending on task.\"\"\"", "\n", "# BLEU scores for translation task", "\n", "if", "metric", ".", "lower", "(", ")", "==", "\"bleu\"", ":", "\n", "    ", "evaluation_score", "=", "_bleu", "(", "ref_file", ",", "trans_file", ",", "\n", "bpe_delimiter", "=", "bpe_delimiter", ")", "\n", "# ROUGE scores for summarization tasks", "\n", "", "elif", "metric", ".", "lower", "(", ")", "==", "\"rouge\"", ":", "\n", "    ", "evaluation_score", "=", "_rouge", "(", "ref_file", ",", "trans_file", ",", "\n", "bpe_delimiter", "=", "bpe_delimiter", ")", "\n", "", "elif", "metric", ".", "lower", "(", ")", "==", "\"accuracy\"", ":", "\n", "    ", "evaluation_score", "=", "_accuracy", "(", "ref_file", ",", "trans_file", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unknown metric %s\"", "%", "metric", ")", "\n", "\n", "", "return", "evaluation_score", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._clean": [[49, 58], ["re.sub.strip", "re.sub"], "function", ["None"], ["", "def", "_clean", "(", "sentence", ",", "bpe_delimiter", ")", ":", "\n", "  ", "\"\"\"Clean and handle BPE delimiter.\"\"\"", "\n", "sentence", "=", "sentence", ".", "strip", "(", ")", "\n", "\n", "# BPE", "\n", "if", "bpe_delimiter", ":", "\n", "    ", "sentence", "=", "re", ".", "sub", "(", "bpe_delimiter", "+", "\" \"", ",", "\"\"", ",", "sentence", ")", "\n", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._bleu": [[61, 91], ["zip", "scripts.bleu.compute_bleu", "per_segment_references.append", "reference_text.append", "evaluation_utils._clean", "reference_list.append", "codecs.getreader", "tensorflow.gfile.GFile", "evaluation_utils._clean", "translations.append", "codecs.getreader", "tensorflow.gfile.GFile", "fh.readlines", "_clean.split", "_clean.split"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.bleu.compute_bleu", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._clean", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._clean"], ["", "def", "_bleu", "(", "ref_file", ",", "trans_file", ",", "bpe_delimiter", "=", "None", ")", ":", "\n", "  ", "\"\"\"Compute BLEU scores and handling BPE.\"\"\"", "\n", "max_order", "=", "4", "\n", "smooth", "=", "False", "\n", "\n", "ref_files", "=", "[", "ref_file", "]", "\n", "reference_text", "=", "[", "]", "\n", "for", "reference_filename", "in", "ref_files", ":", "\n", "    ", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "reference_filename", ",", "\"rb\"", ")", ")", "as", "fh", ":", "\n", "      ", "reference_text", ".", "append", "(", "fh", ".", "readlines", "(", ")", ")", "\n", "\n", "", "", "per_segment_references", "=", "[", "]", "\n", "for", "references", "in", "zip", "(", "*", "reference_text", ")", ":", "\n", "    ", "reference_list", "=", "[", "]", "\n", "for", "reference", "in", "references", ":", "\n", "      ", "reference", "=", "_clean", "(", "reference", ",", "bpe_delimiter", ")", "\n", "reference_list", ".", "append", "(", "reference", ".", "split", "(", "\" \"", ")", ")", "\n", "", "per_segment_references", ".", "append", "(", "reference_list", ")", "\n", "\n", "", "translations", "=", "[", "]", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "trans_file", ",", "\"rb\"", ")", ")", "as", "fh", ":", "\n", "    ", "for", "line", "in", "fh", ":", "\n", "      ", "line", "=", "_clean", "(", "line", ",", "bpe_delimiter", ")", "\n", "translations", ".", "append", "(", "line", ".", "split", "(", "\" \"", ")", ")", "\n", "\n", "# bleu_score, precisions, bp, ratio, translation_length, reference_length", "\n", "", "", "bleu_score", ",", "_", ",", "_", ",", "_", ",", "_", ",", "_", "=", "bleu", ".", "compute_bleu", "(", "\n", "per_segment_references", ",", "translations", ",", "max_order", ",", "smooth", ")", "\n", "return", "100", "*", "bleu_score", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._rouge": [[93, 109], ["scripts.rouge.rouge", "codecs.getreader", "tensorflow.gfile.GFile", "references.append", "codecs.getreader", "tensorflow.gfile.GFile", "hypotheses.append", "evaluation_utils._clean", "evaluation_utils._clean"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._clean", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._clean"], ["", "def", "_rouge", "(", "ref_file", ",", "summarization_file", ",", "bpe_delimiter", "=", "None", ")", ":", "\n", "  ", "\"\"\"Compute ROUGE scores and handling BPE.\"\"\"", "\n", "\n", "references", "=", "[", "]", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "ref_file", ",", "\"rb\"", ")", ")", "as", "fh", ":", "\n", "    ", "for", "line", "in", "fh", ":", "\n", "      ", "references", ".", "append", "(", "_clean", "(", "line", ",", "bpe_delimiter", ")", ")", "\n", "\n", "", "", "hypotheses", "=", "[", "]", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "\n", "tf", ".", "gfile", ".", "GFile", "(", "summarization_file", ",", "\"rb\"", ")", ")", "as", "fh", ":", "\n", "    ", "for", "line", "in", "fh", ":", "\n", "      ", "hypotheses", ".", "append", "(", "_clean", "(", "line", ",", "bpe_delimiter", ")", ")", "\n", "\n", "", "", "rouge_score_map", "=", "rouge", ".", "rouge", "(", "hypotheses", ",", "references", ")", "\n", "return", "100", "*", "rouge_score_map", "[", "\"rouge_l/f_score\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._accuracy": [[111, 125], ["codecs.getreader", "tensorflow.gfile.GFile", "codecs.getreader", "tensorflow.gfile.GFile", "label.strip.strip", "pred_fh.readline().strip", "pred_fh.readline"], "function", ["None"], ["", "def", "_accuracy", "(", "label_file", ",", "pred_file", ")", ":", "\n", "  ", "\"\"\"Compute accuracy, each line contains a label.\"\"\"", "\n", "\n", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "label_file", ",", "\"rb\"", ")", ")", "as", "label_fh", ":", "\n", "    ", "with", "codecs", ".", "getreader", "(", "\"utf-8\"", ")", "(", "tf", ".", "gfile", ".", "GFile", "(", "pred_file", ",", "\"rb\"", ")", ")", "as", "pred_fh", ":", "\n", "      ", "count", "=", "0.0", "\n", "match", "=", "0.0", "\n", "for", "label", "in", "label_fh", ":", "\n", "        ", "label", "=", "label", ".", "strip", "(", ")", "\n", "pred", "=", "pred_fh", ".", "readline", "(", ")", ".", "strip", "(", ")", "\n", "if", "label", "==", "pred", ":", "\n", "          ", "match", "+=", "1", "\n", "", "count", "+=", "1", "\n", "", "", "", "return", "100", "*", "match", "/", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.utils.evaluation_utils._moses_bleu": [[127, 149], ["subprocess.check_output", "re.search", "float", "re.search.group", "os.path.exists", "subprocess.call", "subprocess.call"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dense.Dense.call", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.nmt.dense.Dense.call"], ["", "def", "_moses_bleu", "(", "multi_bleu_script", ",", "tgt_test", ",", "trans_file", ",", "bpe_delimiter", "=", "None", ")", ":", "\n", "  ", "\"\"\"Compute BLEU scores using Moses multi-bleu.perl script.\"\"\"", "\n", "# BPE", "\n", "if", "bpe_delimiter", ":", "\n", "    ", "debpe_tgt_test", "=", "tgt_test", "+", "\".debpe\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "debpe_tgt_test", ")", ":", "\n", "# TODO(thangluong): not use shell=True, can be a security hazard", "\n", "      ", "subprocess", ".", "call", "(", "\"cp %s %s\"", "%", "(", "tgt_test", ",", "debpe_tgt_test", ")", ",", "shell", "=", "True", ")", "\n", "subprocess", ".", "call", "(", "\"sed s/%s //g %s\"", "%", "(", "bpe_delimiter", ",", "debpe_tgt_test", ")", ",", "\n", "shell", "=", "True", ")", "\n", "", "tgt_test", "=", "debpe_tgt_test", "\n", "\n", "", "cmd", "=", "\"%s %s < %s\"", "%", "(", "multi_bleu_script", ",", "tgt_test", ",", "trans_file", ")", "\n", "\n", "# subprocess", "\n", "bleu_output", "=", "subprocess", ".", "check_output", "(", "cmd", ",", "shell", "=", "True", ")", "\n", "\n", "# extract BLEU score", "\n", "m", "=", "re", ".", "search", "(", "\"BLEU = (.+?),\"", ",", "bleu_output", ")", "\n", "bleu_score", "=", "float", "(", "m", ".", "group", "(", "1", ")", ")", "\n", "\n", "return", "bleu_score", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.bleu._get_ngrams": [[28, 46], ["collections.Counter", "range", "range", "tuple", "len"], "function", ["None"], ["def", "_get_ngrams", "(", "segment", ",", "max_order", ")", ":", "\n", "  ", "\"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"", "\n", "ngram_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "order", "in", "range", "(", "1", ",", "max_order", "+", "1", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "segment", ")", "-", "order", "+", "1", ")", ":", "\n", "      ", "ngram", "=", "tuple", "(", "segment", "[", "i", ":", "i", "+", "order", "]", ")", "\n", "ngram_counts", "[", "ngram", "]", "+=", "1", "\n", "", "", "return", "ngram_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.bleu.compute_bleu": [[48, 113], ["zip", "range", "min", "len", "collections.Counter", "bleu._get_ngrams", "range", "min", "sum", "math.exp", "float", "math.exp", "bleu._get_ngrams", "len", "len", "float", "math.log", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_ngrams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_ngrams"], ["", "def", "compute_bleu", "(", "reference_corpus", ",", "translation_corpus", ",", "max_order", "=", "4", ",", "\n", "smooth", "=", "False", ")", ":", "\n", "  ", "\"\"\"Computes BLEU score of translated segments against one or more references.\n\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n  Returns:\n    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n    precisions and brevity penalty.\n  \"\"\"", "\n", "matches_by_order", "=", "[", "0", "]", "*", "max_order", "\n", "possible_matches_by_order", "=", "[", "0", "]", "*", "max_order", "\n", "reference_length", "=", "0", "\n", "translation_length", "=", "0", "\n", "for", "(", "references", ",", "translation", ")", "in", "zip", "(", "reference_corpus", ",", "\n", "translation_corpus", ")", ":", "\n", "    ", "reference_length", "+=", "min", "(", "len", "(", "r", ")", "for", "r", "in", "references", ")", "\n", "translation_length", "+=", "len", "(", "translation", ")", "\n", "\n", "merged_ref_ngram_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "reference", "in", "references", ":", "\n", "      ", "merged_ref_ngram_counts", "|=", "_get_ngrams", "(", "reference", ",", "max_order", ")", "\n", "", "translation_ngram_counts", "=", "_get_ngrams", "(", "translation", ",", "max_order", ")", "\n", "overlap", "=", "translation_ngram_counts", "&", "merged_ref_ngram_counts", "\n", "for", "ngram", "in", "overlap", ":", "\n", "      ", "matches_by_order", "[", "len", "(", "ngram", ")", "-", "1", "]", "+=", "overlap", "[", "ngram", "]", "\n", "", "for", "order", "in", "range", "(", "1", ",", "max_order", "+", "1", ")", ":", "\n", "      ", "possible_matches", "=", "len", "(", "translation", ")", "-", "order", "+", "1", "\n", "if", "possible_matches", ">", "0", ":", "\n", "        ", "possible_matches_by_order", "[", "order", "-", "1", "]", "+=", "possible_matches", "\n", "\n", "", "", "", "precisions", "=", "[", "0", "]", "*", "max_order", "\n", "for", "i", "in", "range", "(", "0", ",", "max_order", ")", ":", "\n", "    ", "if", "smooth", ":", "\n", "      ", "precisions", "[", "i", "]", "=", "(", "(", "matches_by_order", "[", "i", "]", "+", "1.", ")", "/", "\n", "(", "possible_matches_by_order", "[", "i", "]", "+", "1.", ")", ")", "\n", "", "else", ":", "\n", "      ", "if", "possible_matches_by_order", "[", "i", "]", ">", "0", ":", "\n", "        ", "precisions", "[", "i", "]", "=", "(", "float", "(", "matches_by_order", "[", "i", "]", ")", "/", "\n", "possible_matches_by_order", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "        ", "precisions", "[", "i", "]", "=", "0.0", "\n", "\n", "", "", "", "if", "min", "(", "precisions", ")", ">", "0", ":", "\n", "    ", "p_log_sum", "=", "sum", "(", "(", "1.", "/", "max_order", ")", "*", "math", ".", "log", "(", "p", ")", "for", "p", "in", "precisions", ")", "\n", "geo_mean", "=", "math", ".", "exp", "(", "p_log_sum", ")", "\n", "", "else", ":", "\n", "    ", "geo_mean", "=", "0", "\n", "\n", "", "ratio", "=", "float", "(", "translation_length", ")", "/", "reference_length", "\n", "\n", "if", "ratio", ">", "1.0", ":", "\n", "    ", "bp", "=", "1.", "\n", "", "else", ":", "\n", "    ", "bp", "=", "math", ".", "exp", "(", "1", "-", "1.", "/", "ratio", ")", "\n", "\n", "", "bleu", "=", "geo_mean", "*", "bp", "\n", "\n", "return", "(", "bleu", ",", "precisions", ",", "bp", ",", "ratio", ",", "translation_length", ",", "reference_length", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_ngrams": [[19, 35], ["set", "len", "range", "set.add", "tuple"], "function", ["None"], ["def", "_get_ngrams", "(", "n", ",", "text", ")", ":", "\n", "  ", "\"\"\"Calcualtes n-grams.\n\n  Args:\n    n: which n-grams to calculate\n    text: An array of tokens\n\n  Returns:\n    A set of n-grams\n  \"\"\"", "\n", "ngram_set", "=", "set", "(", ")", "\n", "text_length", "=", "len", "(", "text", ")", "\n", "max_index_ngram_start", "=", "text_length", "-", "n", "\n", "for", "i", "in", "range", "(", "max_index_ngram_start", "+", "1", ")", ":", "\n", "    ", "ngram_set", ".", "add", "(", "tuple", "(", "text", "[", "i", ":", "i", "+", "n", "]", ")", ")", "\n", "", "return", "ngram_set", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words": [[37, 40], ["list", "itertools.chain", "_.split"], "function", ["None"], ["", "def", "_split_into_words", "(", "sentences", ")", ":", "\n", "  ", "\"\"\"Splits multiple sentences into words and flattens the result\"\"\"", "\n", "return", "list", "(", "itertools", ".", "chain", "(", "*", "[", "_", ".", "split", "(", "\" \"", ")", "for", "_", "in", "sentences", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_word_ngrams": [[42, 50], ["rouge._split_into_words", "rouge._get_ngrams", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_ngrams"], ["", "def", "_get_word_ngrams", "(", "n", ",", "sentences", ")", ":", "\n", "  ", "\"\"\"Calculates word n-grams for multiple sentences.\n  \"\"\"", "\n", "assert", "len", "(", "sentences", ")", ">", "0", "\n", "assert", "n", ">", "0", "\n", "\n", "words", "=", "_split_into_words", "(", "sentences", ")", "\n", "return", "_get_ngrams", "(", "n", ",", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._len_lcs": [[52, 68], ["rouge._lcs", "len", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._lcs"], ["", "def", "_len_lcs", "(", "x", ",", "y", ")", ":", "\n", "  ", "\"\"\"\n  Returns the length of the Longest Common Subsequence between sequences x\n  and y.\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns\n    integer: Length of LCS between x and y\n  \"\"\"", "\n", "table", "=", "_lcs", "(", "x", ",", "y", ")", "\n", "n", ",", "m", "=", "len", "(", "x", ")", ",", "len", "(", "y", ")", "\n", "return", "table", "[", "n", ",", "m", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._lcs": [[70, 95], ["dict", "range", "len", "len", "range", "max"], "function", ["None"], ["", "def", "_lcs", "(", "x", ",", "y", ")", ":", "\n", "  ", "\"\"\"\n  Computes the length of the longest common subsequence (lcs) between two\n  strings. The implementation below uses a DP programming algorithm and runs\n  in O(nm) time where n = len(x) and m = len(y).\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: collection of words\n    y: collection of words\n\n  Returns:\n    Table of dictionary of coord and len lcs\n  \"\"\"", "\n", "n", ",", "m", "=", "len", "(", "x", ")", ",", "len", "(", "y", ")", "\n", "table", "=", "dict", "(", ")", "\n", "for", "i", "in", "range", "(", "n", "+", "1", ")", ":", "\n", "    ", "for", "j", "in", "range", "(", "m", "+", "1", ")", ":", "\n", "      ", "if", "i", "==", "0", "or", "j", "==", "0", ":", "\n", "        ", "table", "[", "i", ",", "j", "]", "=", "0", "\n", "", "elif", "x", "[", "i", "-", "1", "]", "==", "y", "[", "j", "-", "1", "]", ":", "\n", "        ", "table", "[", "i", ",", "j", "]", "=", "table", "[", "i", "-", "1", ",", "j", "-", "1", "]", "+", "1", "\n", "", "else", ":", "\n", "        ", "table", "[", "i", ",", "j", "]", "=", "max", "(", "table", "[", "i", "-", "1", ",", "j", "]", ",", "table", "[", "i", ",", "j", "-", "1", "]", ")", "\n", "", "", "", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._recon_lcs": [[97, 125], ["rouge._lcs", "tuple", "len", "len", "map", "rouge._recon_lcs._recon"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._lcs"], ["", "def", "_recon_lcs", "(", "x", ",", "y", ")", ":", "\n", "  ", "\"\"\"\n  Returns the Longest Subsequence between x and y.\n  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n  Args:\n    x: sequence of words\n    y: sequence of words\n\n  Returns:\n    sequence: LCS of x and y\n  \"\"\"", "\n", "i", ",", "j", "=", "len", "(", "x", ")", ",", "len", "(", "y", ")", "\n", "table", "=", "_lcs", "(", "x", ",", "y", ")", "\n", "\n", "def", "_recon", "(", "i", ",", "j", ")", ":", "\n", "    ", "\"\"\"private recon calculation\"\"\"", "\n", "if", "i", "==", "0", "or", "j", "==", "0", ":", "\n", "      ", "return", "[", "]", "\n", "", "elif", "x", "[", "i", "-", "1", "]", "==", "y", "[", "j", "-", "1", "]", ":", "\n", "      ", "return", "_recon", "(", "i", "-", "1", ",", "j", "-", "1", ")", "+", "[", "(", "x", "[", "i", "-", "1", "]", ",", "i", ")", "]", "\n", "", "elif", "table", "[", "i", "-", "1", ",", "j", "]", ">", "table", "[", "i", ",", "j", "-", "1", "]", ":", "\n", "      ", "return", "_recon", "(", "i", "-", "1", ",", "j", ")", "\n", "", "else", ":", "\n", "      ", "return", "_recon", "(", "i", ",", "j", "-", "1", ")", "\n", "\n", "", "", "recon_tuple", "=", "tuple", "(", "map", "(", "lambda", "x", ":", "x", "[", "0", "]", ",", "_recon", "(", "i", ",", "j", ")", ")", ")", "\n", "return", "recon_tuple", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge_n": [[127, 171], ["rouge._get_word_ngrams", "rouge._get_word_ngrams", "len", "len", "_get_word_ngrams.intersection", "len", "ValueError", "len", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_word_ngrams", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._get_word_ngrams"], ["", "def", "rouge_n", "(", "evaluated_sentences", ",", "reference_sentences", ",", "n", "=", "2", ")", ":", "\n", "  ", "\"\"\"\n  Computes ROUGE-N of two text collections of sentences.\n  Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n  papers/rouge-working-note-v1.3.1.pdf\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentences: The sentences from the referene set\n    n: Size of ngram.  Defaults to 2.\n\n  Returns:\n    A tuple (f1, precision, recall) for ROUGE-N\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", "or", "len", "(", "reference_sentences", ")", "<=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "\n", "", "evaluated_ngrams", "=", "_get_word_ngrams", "(", "n", ",", "evaluated_sentences", ")", "\n", "reference_ngrams", "=", "_get_word_ngrams", "(", "n", ",", "reference_sentences", ")", "\n", "reference_count", "=", "len", "(", "reference_ngrams", ")", "\n", "evaluated_count", "=", "len", "(", "evaluated_ngrams", ")", "\n", "\n", "# Gets the overlapping ngrams between evaluated and reference", "\n", "overlapping_ngrams", "=", "evaluated_ngrams", ".", "intersection", "(", "reference_ngrams", ")", "\n", "overlapping_count", "=", "len", "(", "overlapping_ngrams", ")", "\n", "\n", "# Handle edge case. This isn't mathematically correct, but it's good enough", "\n", "if", "evaluated_count", "==", "0", ":", "\n", "    ", "precision", "=", "0.0", "\n", "", "else", ":", "\n", "    ", "precision", "=", "overlapping_count", "/", "evaluated_count", "\n", "\n", "", "if", "reference_count", "==", "0", ":", "\n", "    ", "recall", "=", "0.0", "\n", "", "else", ":", "\n", "    ", "recall", "=", "overlapping_count", "/", "reference_count", "\n", "\n", "", "f1_score", "=", "2.0", "*", "(", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", "+", "1e-8", ")", ")", "\n", "\n", "# return overlapping_count / reference_count", "\n", "return", "f1_score", ",", "precision", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._f_p_r_lcs": [[173, 194], ["None"], "function", ["None"], ["", "def", "_f_p_r_lcs", "(", "llcs", ",", "m", ",", "n", ")", ":", "\n", "  ", "\"\"\"\n  Computes the LCS-based F-measure score\n  Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Args:\n    llcs: Length of LCS\n    m: number of words in reference summary\n    n: number of words in candidate summary\n\n  Returns:\n    Float. LCS-based F-measure score\n  \"\"\"", "\n", "r_lcs", "=", "llcs", "/", "m", "\n", "p_lcs", "=", "llcs", "/", "n", "\n", "beta", "=", "p_lcs", "/", "(", "r_lcs", "+", "1e-12", ")", "\n", "num", "=", "(", "1", "+", "(", "beta", "**", "2", ")", ")", "*", "r_lcs", "*", "p_lcs", "\n", "denom", "=", "r_lcs", "+", "(", "(", "beta", "**", "2", ")", "*", "p_lcs", ")", "\n", "f_lcs", "=", "num", "/", "(", "denom", "+", "1e-12", ")", "\n", "return", "f_lcs", ",", "p_lcs", ",", "r_lcs", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge_l_sentence_level": [[196, 231], ["rouge._split_into_words", "rouge._split_into_words", "len", "len", "rouge._len_lcs", "rouge._f_p_r_lcs", "ValueError", "len", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._len_lcs", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._f_p_r_lcs"], ["", "def", "rouge_l_sentence_level", "(", "evaluated_sentences", ",", "reference_sentences", ")", ":", "\n", "  ", "\"\"\"\n  Computes ROUGE-L (sentence level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Calculated according to:\n  R_lcs = LCS(X,Y)/m\n  P_lcs = LCS(X,Y)/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n  where:\n  X = reference summary\n  Y = Candidate summary\n  m = length of reference summary\n  n = length of candidate summary\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentences: The sentences from the referene set\n\n  Returns:\n    A float: F_lcs\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", "or", "len", "(", "reference_sentences", ")", "<=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "", "reference_words", "=", "_split_into_words", "(", "reference_sentences", ")", "\n", "evaluated_words", "=", "_split_into_words", "(", "evaluated_sentences", ")", "\n", "m", "=", "len", "(", "reference_words", ")", "\n", "n", "=", "len", "(", "evaluated_words", ")", "\n", "lcs", "=", "_len_lcs", "(", "evaluated_words", ",", "reference_words", ")", "\n", "return", "_f_p_r_lcs", "(", "lcs", ",", "m", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._union_lcs": [[233, 268], ["set", "rouge._split_into_words", "len", "len", "ValueError", "rouge._split_into_words", "set", "len", "lcs_union.union.union", "rouge._recon_lcs"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._recon_lcs"], ["", "def", "_union_lcs", "(", "evaluated_sentences", ",", "reference_sentence", ")", ":", "\n", "  ", "\"\"\"\n  Returns LCS_u(r_i, C) which is the LCS score of the union longest common\n  subsequence between reference sentence ri and candidate summary C. For example\n  if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and\n  c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is\n  \"w1 w2\" and the longest common subsequence of r_i and c2 is \"w1 w3 w5\". The\n  union longest common subsequence of r_i, c1, and c2 is \"w1 w2 w3 w5\" and\n  LCS_u(r_i, C) = 4/5.\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentence: One of the sentences in the reference summaries\n\n  Returns:\n    float: LCS_u(r_i, C)\n\n  ValueError:\n    Raises exception if a param has len <= 0\n  \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "\n", "", "lcs_union", "=", "set", "(", ")", "\n", "reference_words", "=", "_split_into_words", "(", "[", "reference_sentence", "]", ")", "\n", "combined_lcs_length", "=", "0", "\n", "for", "eval_s", "in", "evaluated_sentences", ":", "\n", "    ", "evaluated_words", "=", "_split_into_words", "(", "[", "eval_s", "]", ")", "\n", "lcs", "=", "set", "(", "_recon_lcs", "(", "reference_words", ",", "evaluated_words", ")", ")", "\n", "combined_lcs_length", "+=", "len", "(", "lcs", ")", "\n", "lcs_union", "=", "lcs_union", ".", "union", "(", "lcs", ")", "\n", "\n", "", "union_lcs_count", "=", "len", "(", "lcs_union", ")", "\n", "union_lcs_value", "=", "union_lcs_count", "/", "combined_lcs_length", "\n", "return", "union_lcs_value", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge_l_summary_level": [[270, 312], ["len", "len", "rouge._f_p_r_lcs", "ValueError", "rouge._split_into_words", "rouge._split_into_words", "rouge._union_lcs", "len", "len"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._f_p_r_lcs", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._split_into_words", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge._union_lcs"], ["", "def", "rouge_l_summary_level", "(", "evaluated_sentences", ",", "reference_sentences", ")", ":", "\n", "  ", "\"\"\"\n  Computes ROUGE-L (summary level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n\n  Calculated according to:\n  R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m\n  P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n  where:\n  SUM(i,u) = SUM from i through u\n  u = number of sentences in reference summary\n  C = Candidate summary made up of v sentences\n  m = number of words in reference summary\n  n = number of words in candidate summary\n\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentence: One of the sentences in the reference summaries\n\n  Returns:\n    A float: F_lcs\n\n  Raises:\n    ValueError: raises exception if a param has len <= 0\n  \"\"\"", "\n", "if", "len", "(", "evaluated_sentences", ")", "<=", "0", "or", "len", "(", "reference_sentences", ")", "<=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\"Collections must contain at least 1 sentence.\"", ")", "\n", "\n", "# total number of words in reference sentences", "\n", "", "m", "=", "len", "(", "_split_into_words", "(", "reference_sentences", ")", ")", "\n", "\n", "# total number of words in evaluated sentences", "\n", "n", "=", "len", "(", "_split_into_words", "(", "evaluated_sentences", ")", ")", "\n", "\n", "union_lcs_sum_across_all_references", "=", "0", "\n", "for", "ref_s", "in", "reference_sentences", ":", "\n", "    ", "union_lcs_sum_across_all_references", "+=", "_union_lcs", "(", "evaluated_sentences", ",", "\n", "ref_s", ")", "\n", "", "return", "_f_p_r_lcs", "(", "union_lcs_sum_across_all_references", ",", "m", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge": [[314, 352], ["map", "map", "map", "rouge.rouge_n", "zip", "rouge.rouge_n", "zip", "rouge.rouge_l_sentence_level", "zip", "zip", "zip", "zip"], "function", ["home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge_n", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge_n", "home.repos.pwc.inspect_result.MarkPKCollier_MANNs4NMT.scripts.rouge.rouge_l_sentence_level"], ["", "def", "rouge", "(", "hypotheses", ",", "references", ")", ":", "\n", "  ", "\"\"\"Calculates average rouge scores for a list of hypotheses and\n  references\"\"\"", "\n", "\n", "# Filter out hyps that are of 0 length", "\n", "# hyps_and_refs = zip(hypotheses, references)", "\n", "# hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]", "\n", "# hypotheses, references = zip(*hyps_and_refs)", "\n", "\n", "# Calculate ROUGE-1 F1, precision, recall scores", "\n", "rouge_1", "=", "[", "\n", "rouge_n", "(", "[", "hyp", "]", ",", "[", "ref", "]", ",", "1", ")", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "\n", "]", "\n", "rouge_1_f", ",", "rouge_1_p", ",", "rouge_1_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_1", ")", ")", "\n", "\n", "# Calculate ROUGE-2 F1, precision, recall scores", "\n", "rouge_2", "=", "[", "\n", "rouge_n", "(", "[", "hyp", "]", ",", "[", "ref", "]", ",", "2", ")", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "\n", "]", "\n", "rouge_2_f", ",", "rouge_2_p", ",", "rouge_2_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_2", ")", ")", "\n", "\n", "# Calculate ROUGE-L F1, precision, recall scores", "\n", "rouge_l", "=", "[", "\n", "rouge_l_sentence_level", "(", "[", "hyp", "]", ",", "[", "ref", "]", ")", "\n", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "references", ")", "\n", "]", "\n", "rouge_l_f", ",", "rouge_l_p", ",", "rouge_l_r", "=", "map", "(", "np", ".", "mean", ",", "zip", "(", "*", "rouge_l", ")", ")", "\n", "\n", "return", "{", "\n", "\"rouge_1/f_score\"", ":", "rouge_1_f", ",", "\n", "\"rouge_1/r_score\"", ":", "rouge_1_r", ",", "\n", "\"rouge_1/p_score\"", ":", "rouge_1_p", ",", "\n", "\"rouge_2/f_score\"", ":", "rouge_2_f", ",", "\n", "\"rouge_2/r_score\"", ":", "rouge_2_r", ",", "\n", "\"rouge_2/p_score\"", ":", "rouge_2_p", ",", "\n", "\"rouge_l/f_score\"", ":", "rouge_l_f", ",", "\n", "\"rouge_l/r_score\"", ":", "rouge_l_r", ",", "\n", "\"rouge_l/p_score\"", ":", "rouge_l_p", ",", "\n", "}", "\n"]]}