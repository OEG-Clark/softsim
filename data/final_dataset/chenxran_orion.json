{"home.repos.pwc.inspect_result.chenxran_orion.None.expbert.ExpBERT.__init__": [[67, 77], ["torch.nn.Module.__init__", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_pretrained", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "exp_num", ")", ":", "\n", "        ", "super", "(", "ExpBERT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "exp_num", "=", "exp_num", "\n", "self", ".", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "args", ".", "model", ")", "\n", "self", ".", "model", "=", "AutoModel", ".", "from_pretrained", "(", "args", ".", "model", ",", "config", "=", "self", ".", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "0.1", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", "*", "exp_num", ",", "2", ")", "\n", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.ExpBERT.forward": [[78, 91], ["inputs[].items", "expbert.ExpBERT.model().last_hidden_state[].reshape", "expbert.ExpBERT.dropout", "expbert.ExpBERT.linear", "expbert.ExpBERT.criterion", "torch.argmax", "v.cuda", "torch.LongTensor().cuda", "torch.LongTensor", "expbert.ExpBERT.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "inputs", "[", "\"encoding\"", "]", ".", "items", "(", ")", ":", "\n", "            ", "inputs", "[", "\"encoding\"", "]", "[", "k", "]", "=", "v", ".", "cuda", "(", ")", "\n", "", "pooler_output", "=", "self", ".", "model", "(", "**", "inputs", "[", "\"encoding\"", "]", ")", ".", "last_hidden_state", "[", ":", ",", "0", ",", ":", "]", ".", "reshape", "(", "1", ",", "self", ".", "exp_num", "*", "self", ".", "config", ".", "hidden_size", ")", "\n", "pooler_output", "=", "self", ".", "dropout", "(", "pooler_output", ")", "\n", "logits", "=", "self", ".", "linear", "(", "pooler_output", ")", "\n", "\n", "loss", "=", "self", ".", "criterion", "(", "logits", ",", "torch", ".", "LongTensor", "(", "[", "inputs", "[", "\"label\"", "]", "]", ")", ".", "cuda", "(", ")", ")", "\n", "prediction", "=", "torch", ".", "argmax", "(", "logits", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "loss", ",", "\n", "\"prediction\"", ":", "prediction", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.REDataset.__init__": [[95, 115], ["torch.utils.data.Dataset.__init__", "logger.info", "open", "file.readlines", "example.strip().split", "expbert.REDataset.sentences.append", "expbert.REDataset.entities.append", "str", "eval", "expbert.REDataset.labels.append", "len", "example.strip", "eval", "expbert.REDataset.labels.append"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "path", ",", "exp", ",", "tokenizer", ")", ":", "\n", "        ", "super", "(", "REDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "exp", "=", "exp", "\n", "self", ".", "sentences", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "self", ".", "entities", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "            ", "data", "=", "file", ".", "readlines", "(", ")", "\n", "for", "example", "in", "data", ":", "\n", "                ", "sentence", ",", "entity1", ",", "entity2", ",", "id", ",", "label", "=", "example", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "self", ".", "sentences", ".", "append", "(", "sentence", ")", "\n", "if", "eval", "(", "label", ")", "==", "1", ":", "\n", "                    ", "self", ".", "labels", ".", "append", "(", "1", ")", "\n", "", "elif", "eval", "(", "label", ")", "==", "-", "1", ":", "\n", "                    ", "self", ".", "labels", ".", "append", "(", "0", ")", "\n", "\n", "", "self", ".", "entities", ".", "append", "(", "[", "entity1", ",", "entity2", "]", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"Number of Example in {}: {}\"", ".", "format", "(", "path", ",", "str", "(", "len", "(", "self", ".", "labels", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.REDataset.__len__": [[116, 118], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.REDataset.__getitem__": [[119, 124], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "{", "\n", "\"sentence\"", ":", "self", ".", "sentences", "[", "index", "]", ",", "\n", "\"entity\"", ":", "self", ".", "entities", "[", "index", "]", ",", "\n", "\"label\"", ":", "self", ".", "labels", "[", "index", "]", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.REDataset.collate_fn": [[126, 154], ["outputs.append", "temp.append", "exp.replace().replace.replace().replace.replace().replace", "expbert.REDataset.tokenizer", "exp.replace().replace.replace().replace.index", "exp.replace().replace.replace().replace.replace", "len", "len"], "methods", ["None"], ["", "def", "collate_fn", "(", "self", ",", "batch", ")", ":", "\n", "        ", "outputs", "=", "[", "]", "\n", "for", "ex", "in", "batch", ":", "\n", "            ", "temp", "=", "[", "]", "\n", "for", "exp", "in", "self", ".", "exp", ":", "\n", "                ", "if", "\"{e1}\"", "in", "exp", "or", "\"{e2}\"", "in", "exp", ":", "\n", "                    ", "exp", "=", "exp", ".", "replace", "(", "\"{e1}\"", ",", "ex", "[", "\"entity\"", "]", "[", "0", "]", ")", ".", "replace", "(", "\"{e2}\"", ",", "ex", "[", "\"entity\"", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                    ", "for", "entity", "in", "ex", "[", "\"entity\"", "]", ":", "\n", "                        ", "index", "=", "exp", ".", "index", "(", "'<mask>'", ")", "\n", "exp", "=", "exp", "[", ":", "index", "]", "+", "entity", "+", "exp", "[", "index", "+", "len", "(", "'<mask>'", ")", ":", "]", "\n", "", "", "temp", ".", "append", "(", "exp", ")", "\n", "", "outputs", ".", "append", "(", "\n", "{", "\n", "\"encoding\"", ":", "self", ".", "tokenizer", "(", "\n", "[", "ex", "[", "\"sentence\"", "]", "]", "*", "len", "(", "temp", ")", ",", "temp", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "padding", "=", "\"longest\"", ",", "\n", "truncation", "=", "True", ",", "\n", "max_length", "=", "156", ",", "\n", "return_tensors", "=", "\"pt\"", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", ")", ",", "\n", "\"label\"", ":", "ex", "[", "\"label\"", "]", ",", "\n", "}", "\n", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.REDataset.collate_fn_": [[155, 176], ["expbert.REDataset.tokenizer", "torch.LongTensor", "texts.append", "labels.append"], "methods", ["None"], ["", "def", "collate_fn_", "(", "self", ",", "batch", ")", ":", "\n", "        ", "texts", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "ex", "in", "batch", ":", "\n", "            ", "texts", ".", "append", "(", "ex", "[", "\"sentence\"", "]", ")", "\n", "labels", ".", "append", "(", "ex", "[", "\"label\"", "]", ")", "\n", "\n", "", "outputs", "=", "self", ".", "tokenizer", "(", "\n", "texts", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "padding", "=", "\"longest\"", ",", "\n", "truncation", "=", "True", ",", "\n", "max_length", "=", "156", ",", "\n", "return_tensors", "=", "\"pt\"", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", ")", "\n", "\n", "outputs", "[", "\"labels\"", "]", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.Trainer.__init__": [[179, 207], ["expbert.print_config", "transformers.AutoTokenizer.from_pretrained", "expbert.REDataset", "expbert.REDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.optim.AdamW", "open", "file.readlines", "transformers.AutoModelForSequenceClassification.from_pretrained().cuda", "ExpBERT().cuda", "expbert.Trainer.model.parameters", "transformers.AutoModelForSequenceClassification.from_pretrained", "expbert.ExpBERT", "len"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.print_config"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "print_config", "(", "args", ")", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "self", ".", "args", ".", "model", ")", "\n", "\n", "TASK2EXP", "=", "GENERATED_EXP", "if", "args", ".", "generated_rules", "else", "ANNOTATED_EXP", "\n", "with", "open", "(", "TASK2EXP", "[", "args", ".", "task", "]", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "\n", "            ", "exp", "=", "file", ".", "readlines", "(", ")", "\n", "\n", "", "self", ".", "train_dataset", "=", "REDataset", "(", "TASK2PATH", "[", "'{}-train'", ".", "format", "(", "args", ".", "task", ")", "]", ",", "exp", ",", "self", ".", "tokenizer", ")", "\n", "self", ".", "test_dataset", "=", "REDataset", "(", "TASK2PATH", "[", "'{}-test'", ".", "format", "(", "args", ".", "task", ")", "]", ",", "exp", ",", "self", ".", "tokenizer", ")", "\n", "self", ".", "model", "=", "AutoModelForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ")", ".", "cuda", "(", ")", "if", "self", ".", "args", ".", "no_exp", "else", "ExpBERT", "(", "args", ",", "len", "(", "exp", ")", ")", ".", "cuda", "(", ")", "\n", "\n", "self", ".", "train_loader", "=", "DataLoader", "(", "\n", "self", ".", "train_dataset", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "args", ".", "shuffle", ",", "\n", "collate_fn", "=", "self", ".", "train_dataset", ".", "collate_fn_", "if", "self", ".", "args", ".", "no_exp", "else", "self", ".", "train_dataset", ".", "collate_fn", ",", "\n", ")", "\n", "\n", "self", ".", "test_loader", "=", "DataLoader", "(", "\n", "self", ".", "test_dataset", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "shuffle", "=", "args", ".", "shuffle", ",", "\n", "collate_fn", "=", "self", ".", "test_dataset", ".", "collate_fn_", "if", "self", ".", "args", ".", "no_exp", "else", "self", ".", "test_dataset", ".", "collate_fn", ",", "\n", ")", "\n", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "AdamW", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "args", ".", "learning_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.Trainer.compute_metrics": [[208, 213], ["sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score"], "methods", ["None"], ["", "def", "compute_metrics", "(", "self", ",", "labels", ",", "predictions", ")", ":", "\n", "        ", "accuracy", "=", "accuracy_score", "(", "y_pred", "=", "predictions", ",", "y_true", "=", "labels", ")", "\n", "f1", "=", "f1_score", "(", "y_pred", "=", "predictions", ",", "y_true", "=", "labels", ")", "\n", "\n", "return", "accuracy", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.Trainer.train": [[214, 237], ["expbert.Trainer.model.train", "expbert.Trainer.test", "range", "expbert.Trainer.test", "tqdm.tqdm.tqdm", "enumerate", "expbert.Trainer.model.zero_grad", "torch.nn.utils.clip_grad_norm_", "expbert.Trainer.optimizer.step", "pbar.update", "len", "examples.items", "expbert.Trainer.model", "expbert.Trainer.loss.backward", "expbert.Trainer.model.parameters", "v.cuda", "expbert.Trainer.model", "len"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.expbert.Trainer.train", "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.Trainer.test", "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.Trainer.test"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "test", "(", "-", "1", ")", "\n", "for", "e", "in", "range", "(", "self", ".", "args", ".", "epochs", ")", ":", "\n", "            ", "with", "tqdm", "(", "total", "=", "len", "(", "self", ".", "train_loader", ")", ")", "as", "pbar", ":", "\n", "                ", "for", "step", ",", "examples", "in", "enumerate", "(", "self", ".", "train_loader", ")", ":", "\n", "                    ", "self", ".", "model", ".", "zero_grad", "(", ")", "\n", "if", "self", ".", "args", ".", "no_exp", ":", "\n", "                        ", "for", "k", ",", "v", "in", "examples", ".", "items", "(", ")", ":", "\n", "                            ", "examples", "[", "k", "]", "=", "v", ".", "cuda", "(", ")", "\n", "", "outputs", "=", "self", ".", "model", "(", "**", "examples", ")", "\n", "outputs", ".", "loss", ".", "backward", "(", ")", "\n", "\n", "", "else", ":", "\n", "                        ", "for", "ex", "in", "examples", ":", "\n", "                            ", "outputs", "=", "self", ".", "model", "(", "ex", ")", "\n", "(", "outputs", "[", "\"loss\"", "]", "/", "len", "(", "examples", ")", ")", ".", "backward", "(", ")", "\n", "\n", "", "", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "", "", "self", ".", "test", "(", "e", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.Trainer.test": [[238, 264], ["expbert.Trainer.model.eval", "torch.no_grad", "logger.info", "tqdm.tqdm.tqdm", "enumerate", "expbert.Trainer.compute_metrics", "pbar.update", "len", "len", "examples.items", "expbert.Trainer.model", "loss.append", "labels.extend", "predictions.extend", "v.cuda", "expbert.Trainer.loss.float", "examples[].tolist", "torch.argmax().tolist", "labels.append", "expbert.Trainer.model", "loss.append", "predictions.append", "outputs[].item", "outputs[].tolist", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.expbert.Trainer.compute_metrics"], ["", "", "def", "test", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "with", "tqdm", "(", "total", "=", "len", "(", "self", ".", "test_loader", ")", ")", "as", "pbar", ":", "\n", "                ", "loss", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "predictions", "=", "[", "]", "\n", "for", "step", ",", "examples", "in", "enumerate", "(", "self", ".", "test_loader", ")", ":", "\n", "                    ", "if", "self", ".", "args", ".", "no_exp", ":", "\n", "                        ", "for", "k", ",", "v", "in", "examples", ".", "items", "(", ")", ":", "\n", "                            ", "examples", "[", "k", "]", "=", "v", ".", "cuda", "(", ")", "\n", "", "outputs", "=", "self", ".", "model", "(", "**", "examples", ")", "\n", "loss", ".", "append", "(", "outputs", ".", "loss", ".", "float", "(", ")", ")", "\n", "labels", ".", "extend", "(", "examples", "[", "\"labels\"", "]", ".", "tolist", "(", ")", ")", "\n", "predictions", ".", "extend", "(", "torch", ".", "argmax", "(", "outputs", ".", "logits", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "else", ":", "\n", "                        ", "for", "ex", "in", "examples", ":", "\n", "                            ", "labels", ".", "append", "(", "ex", "[", "'label'", "]", ")", "\n", "outputs", "=", "self", ".", "model", "(", "ex", ")", "\n", "loss", ".", "append", "(", "outputs", "[", "\"loss\"", "]", ".", "item", "(", ")", ")", "\n", "predictions", ".", "append", "(", "outputs", "[", "'prediction'", "]", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "pbar", ".", "update", "(", "1", ")", "\n", "", "accuracy", ",", "f1", "=", "self", ".", "compute_metrics", "(", "predictions", ",", "labels", ")", "\n", "", "logger", ".", "info", "(", "\"[EPOCH {}] Accuracy: {} | F1-Score: {}. (Number of Data {})\"", ".", "format", "(", "epoch", ",", "accuracy", ",", "f1", ",", "len", "(", "predictions", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.set_random_seed": [[46, 54], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_random_seed", "(", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.expbert.print_config": [[56, 64], ["vars", "logger.info", "sorted", "logger.info", "vars.keys", "logger.info", "len"], "function", ["None"], ["", "def", "print_config", "(", "config", ")", ":", "\n", "    ", "config", "=", "vars", "(", "config", ")", "\n", "logger", ".", "info", "(", "\"**************** MODEL CONFIGURATION ****************\"", ")", "\n", "for", "key", "in", "sorted", "(", "config", ".", "keys", "(", ")", ")", ":", "\n", "        ", "val", "=", "config", "[", "key", "]", "\n", "keystr", "=", "\"{}\"", ".", "format", "(", "key", ")", "+", "(", "\" \"", "*", "(", "25", "-", "len", "(", "key", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"{} -->   {}\"", ".", "format", "(", "keystr", ",", "val", ")", ")", "\n", "", "logger", ".", "info", "(", "\"**************** MODEL CONFIGURATION ****************\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.__init__": [[32, 75], ["transformers.BartForConditionalGeneration.from_pretrained().cuda().eval().half", "transformers.BartTokenizer.from_pretrained", "len", "range", "inductor.BartInductor.tokenizer", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "src.bart_with_group_beam.BartForConditionalGeneration_GroupBeam.from_pretrained().cuda().eval().half", "transformers.BartForConditionalGeneration.from_pretrained().cuda().eval().half", "[].isalpha", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "transformers.BartForConditionalGeneration.from_pretrained().cuda().eval", "inductor.BartInductor.stop_sub_list.append", "inductor.BartInductor.tokenizer.encode", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "src.bart_with_group_beam.BartForConditionalGeneration_GroupBeam.from_pretrained().cuda().eval", "transformers.BartForConditionalGeneration.from_pretrained().cuda().eval", "[].upper", "transformers.BartForConditionalGeneration.from_pretrained().cuda", "src.bart_with_group_beam.BartForConditionalGeneration_GroupBeam.from_pretrained().cuda", "transformers.BartForConditionalGeneration.from_pretrained().cuda", "transformers.BartForConditionalGeneration.from_pretrained", "src.bart_with_group_beam.BartForConditionalGeneration_GroupBeam.from_pretrained", "transformers.BartForConditionalGeneration.from_pretrained"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "group_beam", "=", "True", ",", "\n", "continue_pretrain_instance_generator", "=", "True", ",", "\n", "continue_pretrain_hypo_generator", "=", "True", ",", "\n", "if_then", "=", "False", "\n", ")", ":", "\n", "        ", "self", ".", "if_then", "=", "if_then", "\n", "self", ".", "orion_instance_generator_path", "=", "'facebook/bart-large'", "if", "not", "continue_pretrain_instance_generator", "else", "ORION_INS_GENERATOR", "\n", "self", ".", "orion_hypothesis_generator_path", "=", "'facebook/bart-large'", "if", "not", "continue_pretrain_hypo_generator", "else", "ORION_HYPO_GENERATOR", "\n", "\n", "if", "group_beam", ":", "\n", "            ", "self", ".", "orion_hypothesis_generator", "=", "BartForConditionalGeneration_GroupBeam", ".", "from_pretrained", "(", "self", ".", "orion_hypothesis_generator_path", ")", ".", "cuda", "(", ")", ".", "eval", "(", ")", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "orion_hypothesis_generator", "=", "BartForConditionalGeneration", ".", "from_pretrained", "(", "self", ".", "orion_hypothesis_generator_path", ")", ".", "cuda", "(", ")", ".", "eval", "(", ")", ".", "half", "(", ")", "\n", "\n", "", "self", ".", "orion_instance_generator", "=", "BartForConditionalGeneration", ".", "from_pretrained", "(", "self", ".", "orion_instance_generator_path", ")", ".", "cuda", "(", ")", ".", "eval", "(", ")", ".", "half", "(", ")", "\n", "\n", "self", ".", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "\"facebook/bart-large\"", ")", "\n", "self", ".", "word_length", "=", "2", "\n", "\n", "self", ".", "stop_sub_list", "=", "[", "'he'", ",", "'she'", ",", "'this'", ",", "'that'", ",", "'and'", ",", "'it'", ",", "'which'", ",", "'who'", ",", "'whose'", ",", "'there'", ",", "'they'", ",", "'.'", ",", "'its'", ",", "'one'", ",", "\n", "'i'", ",", "','", ",", "'the'", ",", "'nobody'", ",", "'his'", ",", "'her'", ",", "'also'", ",", "'only'", ",", "'currently'", ",", "'here'", ",", "'()'", ",", "'what'", ",", "'where'", ",", "\n", "'why'", ",", "'a'", ",", "'some'", ",", "'\"'", ",", "')'", ",", "'('", ",", "'now'", ",", "'everyone'", ",", "'everybody'", ",", "'their'", ",", "'often'", ",", "'usually'", ",", "'you'", ",", "\n", "'-'", ",", "'?'", ",", "';'", ",", "'in'", ",", "'on'", ",", "'each'", ",", "'both'", ",", "'him'", ",", "'typically'", ",", "'mostly'", ",", "'sometimes'", ",", "'normally'", ",", "\n", "'always'", ",", "'usually'", ",", "'still'", ",", "'today'", ",", "'was'", ",", "'were'", ",", "'but'", ",", "'although'", ",", "'current'", ",", "'all'", ",", "'have'", ",", "\n", "'has'", ",", "'later'", ",", "'with'", ",", "'most'", ",", "'nowadays'", ",", "'then'", ",", "'every'", ",", "'when'", ",", "'someone'", ",", "'anyone'", ",", "'somebody'", ",", "\n", "'anybody'", ",", "'any'", ",", "'being'", ",", "'get'", ",", "'getting'", ",", "'thus'", ",", "'under'", ",", "'even'", ",", "'for'", ",", "'can'", ",", "'rarely'", ",", "'never'", ",", "\n", "'may'", ",", "'generally'", ",", "'other'", ",", "'another'", ",", "'too'", ",", "'first'", ",", "'second'", ",", "'third'", ",", "'mainly'", ",", "'primarily'", ",", "\n", "'having'", ",", "'have'", ",", "'has'", "]", "\n", "\n", "self", ".", "stop_size", "=", "len", "(", "self", ".", "stop_sub_list", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "stop_size", ")", ":", "\n", "            ", "if", "self", ".", "stop_sub_list", "[", "i", "]", "[", "0", "]", ".", "isalpha", "(", ")", ":", "\n", "                ", "temp", "=", "self", ".", "stop_sub_list", "[", "i", "]", "[", "0", "]", ".", "upper", "(", ")", "+", "self", ".", "stop_sub_list", "[", "i", "]", "[", "1", ":", "]", "\n", "self", ".", "stop_sub_list", ".", "append", "(", "temp", ")", "\n", "\n", "", "", "self", ".", "bad_words_ids", "=", "[", "self", ".", "tokenizer", ".", "encode", "(", "bad_word", ")", "[", "1", ":", "-", "1", "]", "for", "bad_word", "in", "[", "'also'", ",", "' also'", "]", "]", "\n", "stop_index", "=", "self", ".", "tokenizer", "(", "self", ".", "stop_sub_list", ",", "max_length", "=", "4", ",", "padding", "=", "True", ")", "\n", "stop_index", "=", "torch", ".", "tensor", "(", "stop_index", "[", "'input_ids'", "]", ")", "[", ":", ",", "1", "]", "\n", "stop_weight", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "tokenizer", ".", "vocab_size", ")", ".", "cuda", "(", ")", "\n", "stop_weight", "[", "0", ",", "stop_index", "]", "-=", "100", "\n", "self", ".", "stop_weight", "=", "stop_weight", "[", "0", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.clean": [[76, 82], ["text.split", "segments[].startswith", "len"], "methods", ["None"], ["", "def", "clean", "(", "self", ",", "text", ")", ":", "\n", "        ", "segments", "=", "text", ".", "split", "(", "'<mask>'", ")", "\n", "if", "len", "(", "segments", ")", "==", "3", "and", "segments", "[", "2", "]", ".", "startswith", "(", "'.'", ")", ":", "\n", "            ", "return", "'<mask>'", ".", "join", "(", "segments", "[", ":", "2", "]", ")", "+", "'<mask>.'", "\n", "", "else", ":", "\n", "            ", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.generate": [[83, 95], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "inductor.BartInductor.generate_rule", "t[].replace().replace", "inductor.BartInductor.clean", "inductor.BartInductor.strip", "new_ret.append", "t[].replace", "len"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.generate_rule", "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.clean"], ["", "", "def", "generate", "(", "self", ",", "inputs", ",", "k", "=", "10", ",", "topk", "=", "10", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "tB_probs", "=", "self", ".", "generate_rule", "(", "inputs", ",", "k", ")", "\n", "ret", "=", "[", "t", "[", "0", "]", ".", "replace", "(", "'<ent0>'", ",", "'<mask>'", ")", ".", "replace", "(", "'<ent1>'", ",", "'<mask>'", ")", "for", "t", "in", "tB_probs", "]", "\n", "\n", "new_ret", "=", "[", "]", "\n", "for", "temp", "in", "ret", ":", "\n", "                ", "temp", "=", "self", ".", "clean", "(", "temp", ".", "strip", "(", ")", ")", "\n", "if", "len", "(", "new_ret", ")", "<", "topk", "and", "temp", "not", "in", "new_ret", ":", "\n", "                    ", "new_ret", ".", "append", "(", "temp", ")", "\n", "\n", "", "", "return", "new_ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.explore_mask": [[96, 126], ["inductor.BartInductor.tokenizer", "inductor.BartInductor.keys", "torch.where", "torch.where", "torch.where", "torch.where", "inductor.BartInductor.orion_instance_generator", "torch.softmax", "torch.softmax", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "range", "min", "generated_ids[].cuda", "top_k[].size", "[].item", "inductor.BartInductor.tokenizer.decode().strip", "tA.index", "tokens_this.append", "copy.deepcopy", "copy.deepcopy.append", "ret.extend", "inductor.BartInductor.explore_mask", "inductor.BartInductor.tokenizer.decode", "token_this[].isalpha", "len", "len"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.explore_mask"], ["", "", "def", "explore_mask", "(", "self", ",", "tA", ",", "k", ",", "tokens", ",", "prob", ",", "required_token", ",", "probs", ")", ":", "\n", "        ", "if", "required_token", "==", "0", ":", "\n", "            ", "return", "[", "[", "tokens", ",", "prob", ",", "probs", "]", "]", "\n", "", "if", "required_token", "<=", "self", ".", "word_length", ":", "\n", "            ", "k", "=", "min", "(", "k", ",", "2", ")", "\n", "", "ret", "=", "[", "]", "\n", "generated_ids", "=", "self", ".", "tokenizer", "(", "tA", ",", "max_length", "=", "128", ",", "padding", "=", "'longest'", ",", "return_tensors", "=", "'pt'", ")", "# [\"input_ids\"].cuda()", "\n", "for", "key", "in", "generated_ids", ".", "keys", "(", ")", ":", "\n", "            ", "generated_ids", "[", "key", "]", "=", "generated_ids", "[", "key", "]", ".", "cuda", "(", ")", "\n", "", "mask_index", "=", "torch", ".", "where", "(", "generated_ids", "[", "\"input_ids\"", "]", "[", "0", "]", "==", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "generated_ret", "=", "self", ".", "orion_instance_generator", "(", "**", "generated_ids", ")", "\n", "#logits = generated_ret.logits", "\n", "logits", "=", "generated_ret", "[", "0", "]", "\n", "softmax", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "mask_word", "=", "softmax", "[", "0", ",", "mask_index", "[", "0", "]", "[", "0", "]", ",", ":", "]", "+", "self", ".", "stop_weight", "\n", "top_k", "=", "torch", ".", "topk", "(", "mask_word", ",", "k", ",", "dim", "=", "0", ")", "\n", "for", "i", "in", "range", "(", "top_k", "[", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "token_s", "=", "top_k", "[", "1", "]", "[", "i", "]", "\n", "prob_s", "=", "top_k", "[", "0", "]", "[", "i", "]", ".", "item", "(", ")", "\n", "token_this", "=", "self", ".", "tokenizer", ".", "decode", "(", "[", "token_s", "]", ")", ".", "strip", "(", ")", "\n", "if", "token_this", "[", "0", "]", ".", "isalpha", "(", ")", "==", "False", "or", "len", "(", "token_this", ")", "<=", "2", ":", "\n", "                ", "continue", "\n", "", "index_s", "=", "tA", ".", "index", "(", "self", ".", "tokenizer", ".", "mask_token", ")", "\n", "tAs", "=", "tA", "[", ":", "index_s", "]", "+", "token_this", "+", "tA", "[", "index_s", "+", "len", "(", "self", ".", "tokenizer", ".", "mask_token", ")", ":", "]", "\n", "tokens_this", "=", "[", "t", "for", "t", "in", "tokens", "]", "\n", "tokens_this", ".", "append", "(", "token_this", ")", "\n", "probs_new", "=", "deepcopy", "(", "probs", ")", "\n", "probs_new", ".", "append", "(", "prob_s", ")", "\n", "ret", ".", "extend", "(", "self", ".", "explore_mask", "(", "tAs", ",", "1", ",", "tokens_this", ",", "prob_s", "*", "prob", ",", "required_token", "-", "1", ",", "probs_new", ")", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.extract_words_for_tA_bart": [[127, 181], ["[].cuda", "inductor.BartInductor.orion_instance_generator.generate", "torch.softmax", "torch.softmax", "enumerate", "t.lower().strip", "inductor.BartInductor.tokenizer.decode", "tA.endswith", "probs[].item", "range", "ret.append", "sorted", "tA[].split", "max", "max", "txt[].strip.endswith", "t.lower", "inductor.BartInductor.tokenizer", "[].cuda.size", "txt[].strip", "len", "words_i.append", "txt[].strip.lower().index", "len", "txt[].strip.lower().index", "txt[].strip", "txt[].strip.lower", "txt[].strip.lower", "len", "txt[].strip.lower", "len", "txt[].strip.lower"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.generate"], ["", "def", "extract_words_for_tA_bart", "(", "self", ",", "tA", ",", "k", "=", "6", ",", "print_it", "=", "False", ")", ":", "\n", "        ", "spans", "=", "[", "t", ".", "lower", "(", ")", ".", "strip", "(", ")", "for", "t", "in", "tA", "[", ":", "-", "1", "]", ".", "split", "(", "'<mask>'", ")", "]", "\n", "generated_ids", "=", "self", ".", "tokenizer", "(", "[", "tA", "]", ",", "padding", "=", "'longest'", ",", "return_tensors", "=", "'pt'", ")", "[", "'input_ids'", "]", ".", "cuda", "(", ")", "\n", "generated_ret", "=", "self", ".", "orion_instance_generator", ".", "generate", "(", "generated_ids", ",", "num_beams", "=", "max", "(", "120", ",", "k", ")", ",", "\n", "#num_beam_groups=max(120, k),", "\n", "max_length", "=", "generated_ids", ".", "size", "(", "1", ")", "+", "15", ",", "\n", "num_return_sequences", "=", "max", "(", "120", ",", "k", ")", ",", "#min_length=generated_ids.size(1),", "\n", "#diversity_penalty=2.0,", "\n", "#length_penalty= 0.8,", "\n", "#early_stopping=True, bad_words_ids=bad_words_ids, no_repeat_ngram_size=2,", "\n", "output_scores", "=", "True", ",", "\n", "return_dict_in_generate", "=", "True", ")", "\n", "summary_ids", "=", "generated_ret", "[", "'sequences'", "]", "\n", "probs", "=", "F", ".", "softmax", "(", "generated_ret", "[", "'sequences_scores'", "]", ")", "\n", "txts", "=", "[", "self", ".", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "True", ")", "for", "g", "in", "summary_ids", "]", "\n", "ret", "=", "[", "]", "\n", "\n", "for", "i", ",", "txt", "in", "enumerate", "(", "txts", ")", ":", "\n", "            ", "if", "tA", ".", "endswith", "(", "'.'", ")", ":", "\n", "                ", "if", "txt", ".", "endswith", "(", "'.'", ")", ":", "\n", "                    ", "txt", "=", "txt", "[", ":", "-", "1", "]", ".", "strip", "(", ")", "\n", "", "txt", "+=", "'.'", "\n", "", "word_imcomplete", "=", "False", "\n", "prob", "=", "probs", "[", "i", "]", ".", "item", "(", ")", "\n", "words_i", "=", "[", "]", "\n", "\n", "start_index", "=", "0", "\n", "for", "j", "in", "range", "(", "len", "(", "spans", ")", "-", "1", ")", ":", "\n", "                ", "span1", "=", "spans", "[", "j", "]", "\n", "span2", "=", "spans", "[", "j", "+", "1", "]", "\n", "if", "(", "span1", "in", "txt", ".", "lower", "(", ")", "[", "start_index", ":", "]", ")", "and", "(", "span2", "in", "txt", ".", "lower", "(", ")", "[", "start_index", ":", "]", ")", ":", "\n", "                    ", "index1", "=", "txt", ".", "lower", "(", ")", ".", "index", "(", "span1", ",", "start_index", ")", "+", "len", "(", "span1", ")", "\n", "if", "span2", "==", "''", ":", "\n", "                        ", "if", "txt", "[", "-", "1", "]", "==", "'.'", ":", "\n", "                            ", "index2", "=", "len", "(", "txt", ")", "-", "1", "\n", "", "else", ":", "\n", "                            ", "index2", "=", "len", "(", "txt", ")", "\n", "", "", "else", ":", "\n", "                        ", "index2", "=", "txt", ".", "lower", "(", ")", ".", "index", "(", "span2", ",", "start_index", ")", "\n", "\n", "", "words_i", ".", "append", "(", "txt", "[", "index1", ":", "index2", "]", ".", "strip", "(", ")", ")", "\n", "start_index", "=", "index2", "\n", "#if words_i[-1] == '':", "\n", "#    word_imcomplete = True", "\n", "", "else", ":", "\n", "                    ", "word_imcomplete", "=", "True", "\n", "", "", "if", "word_imcomplete", ":", "\n", "# if print_it:", "\n", "# print(txt + '\\t' + tA + '\\t' + '\u00d7')", "\n", "                ", "continue", "\n", "\n", "\n", "", "ret", ".", "append", "(", "[", "words_i", ",", "prob", "]", ")", "\n", "", "return", "sorted", "(", "ret", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "[", ":", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.extract_words_for_tA": [[183, 205], ["tA.replace.replace.replace", "tA.replace.replace.count", "inductor.BartInductor.explore_mask", "range", "ret.append", "sorted", "words.append", "range", "probs_words.append"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.explore_mask"], ["", "def", "extract_words_for_tA", "(", "self", ",", "tA", ",", "k", "=", "6", ")", ":", "\n", "        ", "word_mask_str", "=", "' '", ".", "join", "(", "[", "self", ".", "tokenizer", ".", "mask_token", "]", "*", "self", ".", "word_length", ")", "\n", "tA", "=", "tA", ".", "replace", "(", "'<mask>'", ",", "word_mask_str", ")", "\n", "mask_count", "=", "tA", ".", "count", "(", "self", ".", "tokenizer", ".", "mask_token", ")", "\n", "mask_probs", "=", "self", ".", "explore_mask", "(", "tA", ",", "k", "*", "20", ",", "[", "]", ",", "1.0", ",", "mask_count", ",", "[", "]", ")", "\n", "ret", "=", "[", "]", "\n", "visited_mask_txt", "=", "{", "}", "\n", "for", "mask", ",", "prob", ",", "probs", "in", "mask_probs", ":", "\n", "            ", "mask_txt", "=", "' '", ".", "join", "(", "mask", ")", ".", "lower", "(", ")", "\n", "if", "mask_txt", "in", "visited_mask_txt", ":", "\n", "                ", "continue", "\n", "", "visited_mask_txt", "[", "mask_txt", "]", "=", "1", "\n", "words", "=", "[", "]", "\n", "probs_words", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "mask_count", ",", "self", ".", "word_length", ")", ":", "\n", "                ", "words", ".", "append", "(", "' '", ".", "join", "(", "mask", "[", "i", ":", "i", "+", "self", ".", "word_length", "]", ")", ")", "\n", "prob_word", "=", "1.0", "\n", "for", "j", "in", "range", "(", "i", ",", "i", "+", "self", ".", "word_length", ")", ":", "\n", "                    ", "prob_word", "*=", "probs", "[", "j", "]", "\n", "", "probs_words", ".", "append", "(", "prob_word", ")", "\n", "", "ret", ".", "append", "(", "[", "words", ",", "prob", ",", "probs_words", "]", ")", "\n", "", "return", "sorted", "(", "ret", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "[", ":", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.extract_templateBs_batch": [[206, 276], ["words_prob_sorted.sort", "enumerate", "inductor.BartInductor.tokenizer", "words_prob_sorted.append", "src.utils.construct_template", "templates.extend", "[].cuda", "inductor.BartInductor.orion_hypothesis_generator.generate", "generated_ret[].reshape", "torch.softmax", "torch.softmax", "range", "templates.clear", "index_words.clear", "len", "len", "generated_ret[].reshape", "generated_ret[].reshape.size", "index_words[].split", "enumerate", "len", "len", "len", "inductor.BartInductor.tokenizer.decode", "txt.replace.replace.lower", "src.utils.post_process_template", "ii_template.append", "inductor.BartInductor.tokenizer", "len", "[].item", "word.lower", "enumerate", "txt.replace.replace.count", "txt.replace.replace.replace"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.src.utils.construct_template", "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.generate", "home.repos.pwc.inspect_result.chenxran_orion.src.utils.post_process_template"], ["", "def", "extract_templateBs_batch", "(", "self", ",", "words_prob", ",", "tA", ",", "k", ",", "print_it", "=", "False", ")", ":", "\n", "        ", "words_prob_sorted", "=", "[", "]", "\n", "for", "(", "words", ",", "probA", ",", "*", "_", ")", "in", "words_prob", ":", "\n", "            ", "tokenized_word", "=", "self", ".", "tokenizer", "(", "words", "[", "0", "]", ")", "\n", "words_prob_sorted", ".", "append", "(", "[", "words", ",", "probA", ",", "len", "(", "tokenized_word", "[", "'input_ids'", "]", ")", "]", ")", "\n", "", "words_prob_sorted", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "2", "]", ")", "\n", "\n", "batch_size", "=", "8", "\n", "templates", "=", "[", "]", "\n", "index_words", "=", "{", "}", "\n", "ret", "=", "{", "}", "\n", "num_beams", "=", "k", "\n", "for", "enum", ",", "(", "words", ",", "probA", ",", "*", "_", ")", "in", "enumerate", "(", "words_prob_sorted", ")", ":", "\n", "            ", "template", "=", "construct_template", "(", "words", ",", "tA", ",", "self", ".", "if_then", ")", "\n", "templates", ".", "extend", "(", "template", ")", "\n", "for", "t", "in", "template", ":", "\n", "                ", "index_words", "[", "len", "(", "index_words", ")", "]", "=", "'\\t'", ".", "join", "(", "words", ")", "\n", "# index_words[len(templates)-1] = '\\t'.join(words)", "\n", "", "if", "(", "len", "(", "templates", ")", "==", "batch_size", ")", "or", "enum", "==", "len", "(", "words_prob_sorted", ")", "-", "1", "or", "(", "words_prob_sorted", "[", "enum", "+", "1", "]", "[", "2", "]", "!=", "words_prob_sorted", "[", "enum", "]", "[", "2", "]", ")", ":", "\n", "                ", "generated_ids", "=", "self", ".", "tokenizer", "(", "templates", ",", "padding", "=", "\"longest\"", ",", "return_tensors", "=", "'pt'", ")", "[", "'input_ids'", "]", ".", "cuda", "(", ")", "\n", "generated_ret", "=", "self", ".", "orion_hypothesis_generator", ".", "generate", "(", "generated_ids", ",", "num_beams", "=", "num_beams", ",", "\n", "num_beam_groups", "=", "num_beams", ",", "\n", "max_length", "=", "28", ",", "#template_length+5,", "\n", "num_return_sequences", "=", "num_beams", ",", "min_length", "=", "3", ",", "\n", "diversity_penalty", "=", "1.0", ",", "\n", "early_stopping", "=", "True", ",", "\n", "#length_penalty = 0.1,", "\n", "bad_words_ids", "=", "self", ".", "bad_words_ids", ",", "\n", "#no_repeat_ngram_size=2,", "\n", "output_scores", "=", "True", ",", "\n", "return_dict_in_generate", "=", "True", ",", "decoder_ori_input_ids", "=", "generated_ids", ",", "\n", "top_p", "=", "0.95", ",", "\n", ")", "\n", "summary_ids", "=", "generated_ret", "[", "'sequences'", "]", ".", "reshape", "(", "(", "len", "(", "templates", ")", ",", "num_beams", ",", "-", "1", ")", ")", "\n", "probs", "=", "F", ".", "softmax", "(", "generated_ret", "[", "'sequences_scores'", "]", ".", "reshape", "(", "(", "len", "(", "templates", ")", ",", "num_beams", ")", ")", ",", "dim", "=", "1", ")", "\n", "for", "ii", "in", "range", "(", "summary_ids", ".", "size", "(", "0", ")", ")", ":", "\n", "                    ", "txts", "=", "[", "self", ".", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "True", ")", "for", "g", "in", "\n", "summary_ids", "[", "ii", "]", "]", "\n", "ii_template", "=", "[", "]", "\n", "words_ii", "=", "index_words", "[", "ii", "]", ".", "split", "(", "'\\t'", ")", "\n", "for", "i", ",", "txt", "in", "enumerate", "(", "txts", ")", ":", "\n", "                        ", "prob", "=", "probs", "[", "ii", "]", "[", "i", "]", ".", "item", "(", ")", "*", "probA", "\n", "\n", "txt", "=", "txt", ".", "lower", "(", ")", "\n", "txt", "=", "post_process_template", "(", "txt", ")", "\n", "\n", "words_ii_matched", "=", "[", "word", ".", "lower", "(", ")", "for", "word", "in", "words_ii", "]", "#extract_similar_words(txt, words_ii)", "\n", "if", "words_ii_matched", "is", "None", ":", "\n", "                            ", "prob", "=", "0.0", "\n", "", "else", ":", "\n", "                            ", "for", "j", ",", "word", "in", "enumerate", "(", "words_ii_matched", ")", ":", "\n", "                                ", "if", "word", "not", "in", "txt", ":", "\n", "                                    ", "prob", "=", "0.0", "\n", "", "else", ":", "\n", "                                    ", "txt", "=", "txt", ".", "replace", "(", "word", ",", "'<ent{}>'", ".", "format", "(", "j", ")", ",", "1", ")", "\n", "\n", "", "", "", "if", "txt", ".", "count", "(", "' '", ")", "+", "1", "<=", "3", ":", "\n", "                            ", "continue", "\n", "\n", "", "ii_template", ".", "append", "(", "[", "txt", ",", "prob", "]", ")", "\n", "# if print_it:", "\n", "# print(index_words[ii]+'\\t'+str(convert_for_print(ii_template)))", "\n", "", "for", "template", ",", "prob", "in", "ii_template", ":", "\n", "                        ", "if", "template", "not", "in", "ret", ":", "\n", "                            ", "ret", "[", "template", "]", "=", "0.0", "\n", "", "ret", "[", "template", "]", "+=", "prob", "\n", "", "", "templates", ".", "clear", "(", ")", "\n", "index_words", ".", "clear", "(", ")", "\n", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.generate_rule": [[277, 303], ["src.utils.formalize_tA", "inductor.BartInductor.extract_templateBs_batch", "str().lower", "inductor.BartInductor.extract_words_for_tA_bart", "inductor.BartInductor.extract_words_for_tA", "ret.append", "sorted", "enumerate", "src.utils.filter_words", "src.utils.filter_words", "str", "sentence.replace.replace.replace", "sentence.replace.replace.split"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.src.utils.formalize_tA", "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.extract_templateBs_batch", "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.extract_words_for_tA_bart", "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.BartInductor.extract_words_for_tA", "home.repos.pwc.inspect_result.chenxran_orion.src.utils.filter_words", "home.repos.pwc.inspect_result.chenxran_orion.src.utils.filter_words"], ["", "def", "generate_rule", "(", "self", ",", "tA", ",", "k", "=", "10", ",", "print_it", "=", "False", ")", ":", "\n", "        ", "tA", "=", "formalize_tA", "(", "tA", ")", "\n", "if", "'bart'", "in", "str", "(", "self", ".", "orion_instance_generator", ".", "__class__", ")", ".", "lower", "(", ")", ":", "\n", "            ", "words_prob", "=", "self", ".", "extract_words_for_tA_bart", "(", "tA", ",", "k", ",", "print_it", "=", "print_it", ")", "\n", "words_prob", "=", "filter_words", "(", "words_prob", ")", "[", ":", "k", "]", "\n", "# if print_it:", "\n", "# print(convert_for_print(words_prob))", "\n", "", "else", ":", "\n", "            ", "words_prob", "=", "self", ".", "extract_words_for_tA", "(", "tA", ",", "k", ")", "\n", "words_prob", "=", "filter_words", "(", "words_prob", ")", "[", ":", "k", "]", "\n", "\n", "", "tB_prob", "=", "self", ".", "extract_templateBs_batch", "(", "words_prob", ",", "tA", ",", "k", ",", "print_it", "=", "print_it", ")", "\n", "\n", "ret", "=", "[", "]", "\n", "for", "k1", "in", "tB_prob", ":", "\n", "            ", "ret", ".", "append", "(", "[", "k1", ",", "tB_prob", "[", "k1", "]", "]", ")", "\n", "", "ret", "=", "sorted", "(", "ret", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "[", ":", "k", "]", "\n", "if", "self", ".", "if_then", ":", "\n", "            ", "for", "i", ",", "temp", "in", "enumerate", "(", "ret", ")", ":", "\n", "                ", "sentence", "=", "temp", "[", "0", "]", "\n", "if", "\"then\"", "in", "sentence", ":", "\n", "                    ", "sentence", "=", "sentence", ".", "split", "(", "\"then\"", ")", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                    ", "sentence", "=", "sentence", ".", "replace", "(", "\"if\"", ",", "\"\"", ")", "\n", "", "ret", "[", "i", "]", "[", "0", "]", "=", "sentence", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.__init__": [[306, 312], ["transformers.AutoModelForSeq2SeqLM.from_pretrained().cuda().eval", "transformers.AutoTokenizer.from_pretrained", "inductor.CometInductor.use_task_specific_params", "transformers.AutoModelForSeq2SeqLM.from_pretrained().cuda", "transformers.AutoModelForSeq2SeqLM.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.use_task_specific_params"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", "=", "AutoModelForSeq2SeqLM", ".", "from_pretrained", "(", "\"adamlin/comet-atomic_2020_BART\"", ")", ".", "cuda", "(", ")", ".", "eval", "(", ")", "# .half()", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"adamlin/comet-atomic_2020_BART\"", ")", "\n", "self", ".", "task", "=", "\"summarization\"", "\n", "self", ".", "use_task_specific_params", "(", ")", "\n", "self", ".", "decoder_start_token_id", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.drop_repeat": [[313, 320], ["new_list.append"], "methods", ["None"], ["", "def", "drop_repeat", "(", "self", ",", "old_list", ")", ":", "\n", "        ", "new_list", "=", "[", "]", "\n", "for", "item", "in", "old_list", ":", "\n", "            ", "if", "item", "not", "in", "new_list", ":", "\n", "                ", "new_list", ".", "append", "(", "item", ")", "\n", "\n", "", "", "return", "new_list", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.chunks": [[321, 325], ["range", "len"], "methods", ["None"], ["", "def", "chunks", "(", "self", ",", "lst", ",", "n", ")", ":", "\n", "        ", "\"\"\"Yield successive n-sized chunks from lst.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "lst", ")", ",", "n", ")", ":", "\n", "            ", "yield", "lst", "[", "i", ":", "i", "+", "n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.use_task_specific_params": [[326, 333], ["task_specific_params.get", "inductor.CometInductor.model.config.update"], "methods", ["None"], ["", "", "def", "use_task_specific_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"Update config with summarization specific params.\"\"\"", "\n", "task_specific_params", "=", "self", ".", "model", ".", "config", ".", "task_specific_params", "\n", "\n", "if", "task_specific_params", "is", "not", "None", ":", "\n", "            ", "pars", "=", "task_specific_params", ".", "get", "(", "self", ".", "task", ",", "{", "}", ")", "\n", "self", ".", "model", ".", "config", ".", "update", "(", "pars", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.trim_batch": [[334, 343], ["input_ids.ne().any", "input_ids.ne"], "methods", ["None"], ["", "", "def", "trim_batch", "(", "\n", "self", ",", "input_ids", ",", "pad_token_id", ",", "attention_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"", "\n", "keep_column_mask", "=", "input_ids", ".", "ne", "(", "pad_token_id", ")", ".", "any", "(", "dim", "=", "0", ")", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "return", "input_ids", "[", ":", ",", "keep_column_mask", "]", "\n", "", "else", ":", "\n", "            ", "return", "(", "input_ids", "[", ":", ",", "keep_column_mask", "]", ",", "attention_mask", "[", ":", ",", "keep_column_mask", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.generate": [[344, 376], ["enumerate", "re.findall", "inputs.index", "inductor.CometInductor.generate_", "output.strip.strip.replace().replace", "output.strip.strip.strip", "re.sub", "re.sub.endswith", "re.search", "re.search", "re.sub", "re.sub.endswith", "output.strip.strip.strip", "outputs.append", "outputs.append", "output.strip.strip.replace", "output.strip.strip.strip", "outputs.append", "outputs.append", "len"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.generate_"], ["", "", "def", "generate", "(", "self", ",", "inputs", ",", "k", ",", "topk", ")", ":", "\n", "        ", "outputs", "=", "[", "]", "\n", "words", "=", "[", "'PersonX'", ",", "'PersonY'", "]", "\n", "for", "i", ",", "_", "in", "enumerate", "(", "re", ".", "findall", "(", "\"<mask>\"", ",", "inputs", ")", ")", ":", "\n", "            ", "index", "=", "inputs", ".", "index", "(", "'<mask>'", ")", "\n", "inputs", "=", "inputs", "[", ":", "index", "]", "+", "words", "[", "i", "]", "+", "inputs", "[", "index", "+", "len", "(", "'<mask>'", ")", ":", "]", "\n", "\n", "", "for", "relation", "in", "RELATIONS", ":", "\n", "            ", "inputs", "=", "\"{} {} [GEN]\"", ".", "format", "(", "inputs", "[", ":", "-", "1", "]", ",", "relation", ")", "\n", "gen", "=", "self", ".", "generate_", "(", "inputs", ",", "num_generate", "=", "10", ")", "\n", "switch", "=", "0", "\n", "for", "output", "in", "gen", "[", "0", "]", ":", "\n", "                ", "output", "=", "output", ".", "strip", "(", ")", "\n", "if", "re", ".", "search", "(", "\"PersonX|X\"", ",", "output", ")", "and", "re", ".", "search", "(", "\"PersonY|Y\"", ",", "output", ")", ":", "\n", "                    ", "temp", "=", "re", ".", "sub", "(", "\"PersonX|X|PersonY|Y\"", ",", "\"<mask>\"", ",", "output", ".", "strip", "(", ")", ")", "\n", "if", "temp", ".", "endswith", "(", "\".\"", ")", ":", "\n", "                        ", "outputs", ".", "append", "(", "temp", ")", "\n", "", "else", ":", "\n", "                        ", "outputs", ".", "append", "(", "temp", "+", "\".\"", ")", "\n", "", "switch", "=", "1", "\n", "break", "\n", "\n", "", "", "if", "switch", "==", "0", ":", "\n", "                ", "output", "=", "gen", "[", "0", "]", "[", "0", "]", "\n", "temp", "=", "re", ".", "sub", "(", "\"PersonX|X|PersonY|Y\"", ",", "\"<mask>\"", ",", "output", ".", "strip", "(", ")", ")", "\n", "if", "temp", ".", "endswith", "(", "\".\"", ")", ":", "\n", "                    ", "outputs", ".", "append", "(", "temp", ")", "\n", "", "else", ":", "\n", "                    ", "outputs", ".", "append", "(", "temp", "+", "\".\"", ")", "\n", "\n", "", "", "", "outputs", "=", "[", "output", ".", "replace", "(", "'PersonX'", ",", "'<mask>'", ")", ".", "replace", "(", "'PersonY'", ",", "'<mask>'", ")", "for", "output", "in", "outputs", "]", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.generate_": [[377, 401], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "inductor.CometInductor.tokenizer", "inductor.CometInductor.trim_batch", "inductor.CometInductor.model.generate", "inductor.CometInductor.tokenizer.batch_decode", "decs.append", "input_ids.cuda", "attention_mask.cuda"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.trim_batch", "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.generate"], ["", "def", "generate_", "(", "\n", "self", ",", "\n", "queries", ",", "\n", "decode_method", "=", "\"beam\"", ",", "\n", "num_generate", "=", "5", ",", "\n", ")", ":", "\n", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "decs", "=", "[", "]", "\n", "batch", "=", "self", ".", "tokenizer", "(", "queries", ",", "return_tensors", "=", "\"pt\"", ",", "padding", "=", "\"longest\"", ")", "\n", "input_ids", ",", "attention_mask", "=", "self", ".", "trim_batch", "(", "**", "batch", ",", "pad_token_id", "=", "self", ".", "tokenizer", ".", "pad_token_id", ")", "\n", "\n", "summaries", "=", "self", ".", "model", ".", "generate", "(", "\n", "input_ids", "=", "input_ids", ".", "cuda", "(", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "cuda", "(", ")", ",", "\n", "decoder_start_token_id", "=", "self", ".", "decoder_start_token_id", ",", "\n", "num_beams", "=", "num_generate", ",", "\n", "num_return_sequences", "=", "num_generate", ",", "\n", ")", "\n", "\n", "dec", "=", "self", ".", "tokenizer", ".", "batch_decode", "(", "summaries", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "\n", "decs", ".", "append", "(", "dec", ")", "\n", "\n", "return", "decs", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.__init__": [[64, 75], ["inductor.BartInductor", "inductor.CometInductor"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "if", "self", ".", "args", ".", "inductor", "==", "'rule'", ":", "\n", "            ", "self", ".", "inductor", "=", "BartInductor", "(", "\n", "group_beam", "=", "self", ".", "args", ".", "group_beam", ",", "\n", "continue_pretrain_instance_generator", "=", "self", ".", "args", ".", "mlm_training", ",", "\n", "continue_pretrain_hypo_generator", "=", "self", ".", "args", ".", "bart_training", ",", "\n", "if_then", "=", "self", ".", "args", ".", "if_then", ",", "\n", ")", "\n", "", "elif", "self", ".", "args", ".", "inductor", "==", "'comet'", ":", "\n", "            ", "self", ".", "inductor", "=", "CometInductor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.clean": [[76, 82], ["text.split", "segments[].startswith", "len"], "methods", ["None"], ["", "", "def", "clean", "(", "self", ",", "text", ")", ":", "\n", "        ", "segments", "=", "text", ".", "split", "(", "'<mask>'", ")", "\n", "if", "len", "(", "segments", ")", "==", "3", "and", "segments", "[", "2", "]", ".", "startswith", "(", "'.'", ")", ":", "\n", "            ", "return", "'<mask>'", ".", "join", "(", "segments", "[", ":", "2", "]", ")", "+", "'<mask>.'", "\n", "", "else", ":", "\n", "            ", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.clean_references": [[83, 89], ["enumerate", "text.endswith", "text.replace"], "methods", ["None"], ["", "", "def", "clean_references", "(", "self", ",", "texts", ")", ":", "\n", "        ", "for", "i", ",", "text", "in", "enumerate", "(", "texts", ")", ":", "\n", "            ", "if", "text", ".", "endswith", "(", "\" .\"", ")", ":", "\n", "                ", "texts", "[", "i", "]", "=", "text", ".", "replace", "(", "\" .\"", ",", "\".\"", ")", "\n", "\n", "", "", "return", "texts", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.self_bleu": [[90, 100], ["range", "numpy.mean", "len", "bleus.append", "nltk.bleu"], "methods", ["None"], ["", "def", "self_bleu", "(", "self", ",", "hypothesis", ")", ":", "\n", "        ", "bleus", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "hypothesis", ")", ")", ":", "\n", "            ", "bleus", ".", "append", "(", "bleu", "(", "\n", "hypothesis", "[", ":", "i", "]", "+", "hypothesis", "[", "i", "+", "1", ":", "]", ",", "\n", "hypothesis", "[", "i", "]", ",", "\n", "weights", "=", "(", "0.5", ",", "0.5", ")", ")", ")", "\n", "\n", "", "ret", "=", "np", ".", "mean", "(", "bleus", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.evaluate": [[101, 230], ["torch.no_grad", "evaluation.RelationExtractionEvaluator.print", "open", "file.readlines", "tqdm.tqdm.tqdm", "pbar.update", "row.strip().split.strip().split.strip().split", "re.sub.strip", "evaluation.RelationExtractionEvaluator.clean_references", "evaluation.RelationExtractionEvaluator.inductor.generate", "logger.info", "logger.info", "logger.info", "enumerate", "logger.info", "logger.info", "logger.info", "logger.info", "len", "relations.startswith", "relations.endswith", "re.sub", "evaluation.RelationExtractionEvaluator.clean", "logger.info", "len", "evaluation.RelationExtractionEvaluator.metrics.keys", "row.strip().split.strip().split.strip", "relation.replace().replace().lower().strip", "relations.replace().replace().lower().strip", "hypo.lower().strip", "evaluation.RelationExtractionEvaluator.metrics[].append", "eval", "evaluation.RelationExtractionEvaluator.metrics[].append", "evaluation.RelationExtractionEvaluator.metrics[].append", "evaluation.RelationExtractionEvaluator.metrics[].append", "evaluation.RelationExtractionEvaluator.metrics[].append", "evaluation.RelationExtractionEvaluator.metrics[].append", "evaluation.RelationExtractionEvaluator.metrics[].append", "evaluation.RelationExtractionEvaluator.metrics[].append", "evaluation.RelationExtractionEvaluator.self_bleu", "logger.warning", "relation.replace().replace().lower", "relations.replace().replace().lower", "hypo.lower", "nltk.bleu", "logger.warning", "nltk.bleu", "logger.warning", "nltk.bleu", "logger.warning", "nltk.bleu", "logger.warning", "nltk.meteor", "logger.warning", "evaluation.rouge", "logger.warning", "hypo.split", "hypo.split", "hypo.split", "hypo.split", "relation.replace().replace", "relations.replace().replace", "reference.split", "reference.split", "reference.split", "reference.split", "relation.replace", "relations.replace"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.print", "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.clean_references", "home.repos.pwc.inspect_result.chenxran_orion.None.inductor.CometInductor.generate", "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.clean", "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.self_bleu", "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.rouge"], ["", "def", "evaluate", "(", "self", ",", "task", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "metrics", "=", "{", "\n", "\"bleu-4\"", ":", "[", "]", ",", "\n", "\"bleu-3\"", ":", "[", "]", ",", "\n", "\"bleu-2\"", ":", "[", "]", ",", "\n", "\"bleu-1\"", ":", "[", "]", ",", "\n", "\"METEOR\"", ":", "[", "]", ",", "\n", "\"ROUGE-L\"", ":", "[", "]", ",", "\n", "\"self-BLEU-2\"", ":", "[", "]", ",", "\n", "}", "\n", "with", "open", "(", "FILES", "[", "task", "]", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file", ":", "\n", "                ", "data", "=", "file", ".", "readlines", "(", ")", "\n", "with", "tqdm", "(", "total", "=", "len", "(", "data", ")", ")", "as", "pbar", ":", "\n", "                    ", "for", "row", "in", "data", ":", "\n", "                        ", "pbar", ".", "update", "(", "1", ")", "\n", "row", "=", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "inputs", ",", "head", ",", "tail", ",", "relations", "=", "row", "[", "0", "]", ",", "row", "[", "1", "]", ",", "row", "[", "2", "]", ",", "row", "[", "3", "]", "\n", "inputs", "=", "inputs", ".", "strip", "(", ")", "\n", "\n", "if", "relations", ".", "startswith", "(", "'['", ")", "and", "relations", ".", "endswith", "(", "']'", ")", ":", "\n", "                            ", "inputs", "=", "re", ".", "sub", "(", "\"<A>|<B>\"", ",", "\"<mask>\"", ",", "inputs", ")", "\n", "references", "=", "[", "relation", ".", "replace", "(", "'<A>'", ",", "'<mask>'", ")", ".", "replace", "(", "'<B>'", ",", "'<mask>'", ")", ".", "lower", "(", ")", ".", "strip", "(", ")", "for", "relation", "in", "eval", "(", "relations", ")", "]", "\n", "", "else", ":", "\n", "                            ", "references", "=", "[", "relations", ".", "replace", "(", "'[X]'", ",", "'<mask>'", ")", ".", "replace", "(", "'[Y]'", ",", "'<mask>'", ")", ".", "lower", "(", ")", ".", "strip", "(", ")", "]", "\n", "", "references", "=", "self", ".", "clean_references", "(", "references", ")", "\n", "hypothesis", "=", "self", ".", "inductor", ".", "generate", "(", "inputs", ",", "k", "=", "10", ",", "topk", "=", "10", ")", "\n", "\n", "logger", ".", "info", "(", "\"***********Input************\"", ")", "\n", "logger", ".", "info", "(", "inputs", ")", "\n", "logger", ".", "info", "(", "\"*********Hypothesis*********\"", ")", "\n", "for", "i", ",", "hypo", "in", "enumerate", "(", "hypothesis", ")", ":", "\n", "                            ", "hypothesis", "[", "i", "]", "=", "self", ".", "clean", "(", "hypo", ".", "lower", "(", ")", ".", "strip", "(", ")", ")", "\n", "logger", ".", "info", "(", "hypo", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"****************************\"", ")", "\n", "logger", ".", "info", "(", "\"*********References*********\"", ")", "\n", "logger", ".", "info", "(", "references", ")", "\n", "logger", ".", "info", "(", "\"****************************\"", ")", "\n", "\n", "if", "len", "(", "hypothesis", ")", "==", "0", ":", "\n", "                            ", "for", "k", "in", "self", ".", "metrics", ".", "keys", "(", ")", ":", "\n", "                                ", "if", "k", "!=", "'self-BLEU-2'", ":", "\n", "                                    ", "self", ".", "metrics", "[", "k", "]", ".", "append", "(", "0.", ")", "\n", "\n", "", "", "", "else", ":", "\n", "                            ", "for", "hypo", "in", "hypothesis", ":", "\n", "                                ", "try", ":", "\n", "                                    ", "self", ".", "metrics", "[", "'bleu-4'", "]", ".", "append", "(", "\n", "bleu", "(", "\n", "[", "reference", ".", "split", "(", ")", "for", "reference", "in", "references", "]", ",", "\n", "hypo", ".", "split", "(", ")", ",", "\n", "weights", "=", "(", "0.25", ",", "0.25", ",", "0.25", ",", "0.25", ")", "\n", ")", "\n", ")", "\n", "", "except", "Exception", ":", "\n", "                                    ", "logger", ".", "warning", "(", "\"Skip bleu-4 in example: {}\"", ".", "format", "(", "inputs", ")", ")", "\n", "pass", "\n", "\n", "", "try", ":", "\n", "                                    ", "self", ".", "metrics", "[", "'bleu-3'", "]", ".", "append", "(", "\n", "bleu", "(", "\n", "[", "reference", ".", "split", "(", ")", "for", "reference", "in", "references", "]", ",", "\n", "hypo", ".", "split", "(", ")", ",", "\n", "weights", "=", "(", "1", "/", "3", ",", ")", "*", "3", "\n", ")", "\n", ")", "\n", "", "except", "Exception", ":", "\n", "                                    ", "logger", ".", "warning", "(", "\"Skip bleu-3 in example: {}\"", ".", "format", "(", "inputs", ")", ")", "\n", "pass", "\n", "\n", "", "try", ":", "\n", "                                    ", "self", ".", "metrics", "[", "'bleu-2'", "]", ".", "append", "(", "\n", "bleu", "(", "\n", "[", "reference", ".", "split", "(", ")", "for", "reference", "in", "references", "]", ",", "\n", "hypo", ".", "split", "(", ")", ",", "\n", "weights", "=", "(", "0.5", ",", "0.5", ")", "\n", ")", "\n", ")", "\n", "", "except", "Exception", ":", "\n", "                                    ", "logger", ".", "warning", "(", "\"Skip bleu-2 in example: {}\"", ".", "format", "(", "inputs", ")", ")", "\n", "pass", "\n", "\n", "", "try", ":", "\n", "                                    ", "self", ".", "metrics", "[", "'bleu-1'", "]", ".", "append", "(", "\n", "bleu", "(", "\n", "[", "reference", ".", "split", "(", ")", "for", "reference", "in", "references", "]", ",", "\n", "hypo", ".", "split", "(", ")", ",", "\n", "weights", "=", "(", "1.0", ",", ")", "\n", ")", "\n", ")", "\n", "", "except", "Exception", ":", "\n", "                                    ", "logger", ".", "warning", "(", "\"Skip bleu-1 in example: {}\"", ".", "format", "(", "inputs", ")", ")", "\n", "pass", "\n", "\n", "", "try", ":", "\n", "                                    ", "self", ".", "metrics", "[", "'METEOR'", "]", ".", "append", "(", "\n", "meteor", "(", "\n", "references", ",", "\n", "hypo", ",", "\n", ")", "\n", ")", "\n", "", "except", ":", "\n", "                                    ", "logger", ".", "warning", "(", "\"Skip METEOR in example: {}\"", ".", "format", "(", "inputs", ")", ")", "\n", "pass", "\n", "\n", "\n", "", "try", ":", "\n", "                                    ", "self", ".", "metrics", "[", "'ROUGE-L'", "]", ".", "append", "(", "\n", "rouge", "(", "\n", "references", ",", "\n", "hypo", ",", "\n", ")", "\n", ")", "\n", "", "except", ":", "\n", "                                    ", "logger", ".", "warning", "(", "\"Skip ROUGE-L in example: {}\"", ".", "format", "(", "inputs", ")", ")", "\n", "pass", "\n", "", "", "try", ":", "\n", "                                ", "self", ".", "metrics", "[", "'self-BLEU-2'", "]", ".", "append", "(", "\n", "self", ".", "self_bleu", "(", "\n", "hypothesis", ",", "\n", ")", "\n", ")", "\n", "", "except", ":", "\n", "                                ", "logger", ".", "warning", "(", "\"Skip self-bleu-2 in example: {}.\"", ".", "format", "(", "inputs", ")", ")", "\n", "pass", "\n", "# break", "\n", "\n", "", "", "", "", "", "self", ".", "print", "(", "task", ",", "self", ".", "metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.RelationExtractionEvaluator.print": [[231, 239], ["logger.info", "metrics.items", "logger.info", "logger.info", "logger.info", "logger.info", "str", "str", "numpy.mean"], "methods", ["None"], ["", "", "def", "print", "(", "self", ",", "task", ",", "metrics", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Task: {}\"", ".", "format", "(", "str", "(", "task", ")", ")", ")", "\n", "for", "k", ",", "v", "in", "metrics", ".", "items", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"{}: {}\"", ".", "format", "(", "k", ",", "str", "(", "np", ".", "mean", "(", "v", ")", ")", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"*******************************************************\"", ")", "\n", "logger", ".", "info", "(", "\"*******************************************************\"", ")", "\n", "logger", ".", "info", "(", "\"*******************************************************\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.print_config": [[40, 48], ["vars", "logger.info", "sorted", "logger.info", "vars.keys", "logger.info", "len"], "function", ["None"], ["def", "print_config", "(", "config", ")", ":", "\n", "    ", "config", "=", "vars", "(", "config", ")", "\n", "logger", ".", "info", "(", "\"**************** MODEL CONFIGURATION ****************\"", ")", "\n", "for", "key", "in", "sorted", "(", "config", ".", "keys", "(", ")", ")", ":", "\n", "        ", "val", "=", "config", "[", "key", "]", "\n", "keystr", "=", "\"{}\"", ".", "format", "(", "key", ")", "+", "(", "\" \"", "*", "(", "25", "-", "len", "(", "key", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"{} -->   {}\"", ".", "format", "(", "keystr", ",", "val", ")", ")", "\n", "", "logger", ".", "info", "(", "\"**************** MODEL CONFIGURATION ****************\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.None.evaluation.rouge": [[51, 61], ["max", "scores.append", "scorer.score"], "function", ["None"], ["def", "rouge", "(", "references", ",", "hypothesis", ")", ":", "\n", "    ", "scores", "=", "[", "]", "\n", "for", "reference", "in", "references", ":", "\n", "        ", "scores", ".", "append", "(", "\n", "scorer", ".", "score", "(", "\n", "reference", ",", "\n", "hypothesis", ")", "[", "'rougeL'", "]", "[", "2", "]", "\n", ")", "\n", "\n", "", "return", "max", "(", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.src.bart_with_group_beam.BartForConditionalGeneration_GroupBeam.beam_search": [[19, 295], ["len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "beam_scores.view.view.view", "beam_scorer.finalize", "transformers.generation_logits_process.LogitsProcessorList", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam.prepare_inputs_for_generation", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam.", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam.adjust_logits_during_generation", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "logits_processor", "next_token_scores.view.view.view", "range", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "beam_scorer.process", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam._update_model_kwargs_for_generation", "model_kwargs[].get", "model_kwargs[].get", "beam_scores[].expand_as", "torch.sum().expand", "torch.sum().expand", "torch.sum().expand", "torch.sum().expand", "next_token_scores.view.view.size", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam._reorder_cache", "transformers.generation_utils.BeamSearchEncoderDecoderOutput", "transformers.generation_utils.BeamSearchDecoderOnlyOutput", "range", "beam_next_tokens.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["    ", "def", "beam_search", "(", "\n", "self", ",", "\n", "input_ids", ":", "torch", ".", "LongTensor", ",", "\n", "beam_scorer", ":", "BeamScorer", ",", "\n", "logits_processor", ":", "Optional", "[", "LogitsProcessorList", "]", "=", "None", ",", "\n", "max_length", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "pad_token_id", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "eos_token_id", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "output_attentions", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "output_hidden_states", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "output_scores", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_dict_in_generate", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "**", "model_kwargs", ",", "\n", ")", "->", "Union", "[", "BeamSearchOutput", ",", "torch", ".", "LongTensor", "]", ":", "\n", "        ", "r\"\"\"\n        Generates sequences for models with a language modeling head using beam search decoding.\n\n        Parameters:\n\n            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n            beam_scorer (:obj:`BeamScorer`):\n                An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n                constructed, stored and sorted during generation. For more information, the documentation of\n                :class:`~transformers.BeamScorer` should be read.\n            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n                head applied at each generation step.\n            max_length (:obj:`int`, `optional`, defaults to 20):\n                The maximum length of the sequence to be generated.\n            pad_token_id (:obj:`int`, `optional`):\n                The id of the `padding` token.\n            eos_token_id (:obj:`int`, `optional`):\n                The id of the `end-of-sequence` token.\n            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n                returned tensors for more details.\n            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n                for more details.\n            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n            model_kwargs:\n                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n\n        Return:\n            :class:`~transformers.generation_utilsBeamSearchDecoderOnlyOutput`,\n            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if\n            ``model.config.is_encoder_decoder=True``.\n\n\n        Examples::\n\n            >>> from transformers import (\n            ...    AutoTokenizer,\n            ...    AutoModelForSeq2SeqLM,\n            ...    LogitsProcessorList,\n            ...    MinLengthLogitsProcessor,\n            ...    BeamSearchScorer,\n            ... )\n            >>> import torch\n\n            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n            >>> encoder_input_str = \"translate English to German: How old are you?\"\n            >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n\n            >>> # lets run beam search using 3 beams\n            >>> num_beams = 3\n            >>> # define decoder start token ids\n            >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n            >>> input_ids = input_ids * model.config.decoder_start_token_id\n\n            >>> # add encoder_outputs to model keyword arguments\n            >>> model_kwargs = {\n            ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n            ... }\n\n            >>> # instantiate beam scorer\n            >>> beam_scorer = BeamSearchScorer(\n            ...     batch_size=1,\n            ...     max_length=model.config.max_length,\n            ...     num_beams=num_beams,\n            ...     device=model.device,\n            ... )\n\n            >>> # instantiate logits processors\n            >>> logits_processor = LogitsProcessorList([\n            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n            ... ])\n\n            >>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n\n            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n        \"\"\"", "\n", "\n", "# init values", "\n", "logits_processor", "=", "logits_processor", "if", "logits_processor", "is", "not", "None", "else", "LogitsProcessorList", "(", ")", "\n", "max_length", "=", "max_length", "if", "max_length", "is", "not", "None", "else", "self", ".", "config", ".", "max_length", "\n", "pad_token_id", "=", "pad_token_id", "if", "pad_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "pad_token_id", "\n", "eos_token_id", "=", "eos_token_id", "if", "eos_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "eos_token_id", "\n", "output_scores", "=", "output_scores", "if", "output_scores", "is", "not", "None", "else", "self", ".", "config", ".", "output_scores", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict_in_generate", "=", "(", "\n", "return_dict_in_generate", "if", "return_dict_in_generate", "is", "not", "None", "else", "self", ".", "config", ".", "return_dict_in_generate", "\n", ")", "\n", "\n", "# init attention / hidden states / scores tuples", "\n", "scores", "=", "(", ")", "if", "(", "return_dict_in_generate", "and", "output_scores", ")", "else", "None", "\n", "decoder_attentions", "=", "(", ")", "if", "(", "return_dict_in_generate", "and", "output_attentions", ")", "else", "None", "\n", "decoder_hidden_states", "=", "(", ")", "if", "(", "return_dict_in_generate", "and", "output_hidden_states", ")", "else", "None", "\n", "\n", "# if model is an encoder-decoder, retrieve encoder attention weights and hidden states", "\n", "if", "return_dict_in_generate", "and", "self", ".", "config", ".", "is_encoder_decoder", ":", "\n", "            ", "encoder_attentions", "=", "model_kwargs", "[", "\"encoder_outputs\"", "]", ".", "get", "(", "\"attentions\"", ")", "if", "output_attentions", "else", "None", "\n", "encoder_hidden_states", "=", "(", "\n", "model_kwargs", "[", "\"encoder_outputs\"", "]", ".", "get", "(", "\"hidden_states\"", ")", "if", "output_hidden_states", "else", "None", "\n", ")", "\n", "\n", "", "batch_size", "=", "len", "(", "beam_scorer", ".", "_beam_hyps", ")", "\n", "num_beams", "=", "beam_scorer", ".", "num_beams", "\n", "\n", "batch_beam_size", ",", "cur_len", "=", "input_ids", ".", "shape", "\n", "\n", "assert", "(", "\n", "num_beams", "*", "batch_size", "==", "batch_beam_size", "\n", ")", ",", "\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"", "\n", "\n", "beam_scores", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "num_beams", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "-", "1e9", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "(", "batch_size", "*", "num_beams", ",", ")", ")", "\n", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "            ", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "input_ids", ",", "**", "model_kwargs", ")", "\n", "\n", "outputs", "=", "self", "(", "\n", "**", "model_inputs", ",", "\n", "return_dict", "=", "True", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "next_token_logits", "=", "outputs", ".", "logits", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "\n", "# adjust tokens for Bart, *e.g.*", "\n", "next_token_logits", "=", "self", ".", "adjust_logits_during_generation", "(", "\n", "next_token_logits", ",", "cur_len", "=", "cur_len", ",", "max_length", "=", "max_length", "\n", ")", "\n", "\n", "next_token_scores", "=", "F", ".", "log_softmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", "# (batch_size * num_beams, vocab_size)", "\n", "\n", "next_token_scores", "=", "logits_processor", "(", "input_ids", ",", "next_token_scores", ")", "\n", "next_token_scores", "=", "next_token_scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "next_token_scores", ")", "\n", "\n", "# Store scores, attentions and hidden_states when required", "\n", "if", "return_dict_in_generate", ":", "\n", "                ", "if", "output_scores", ":", "\n", "                    ", "scores", "+=", "(", "next_token_scores", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                    ", "decoder_attentions", "+=", "(", "\n", "(", "outputs", ".", "decoder_attentions", ",", ")", "if", "self", ".", "config", ".", "is_encoder_decoder", "else", "(", "outputs", ".", "attentions", ",", ")", "\n", ")", "\n", "\n", "", "if", "output_hidden_states", ":", "\n", "                    ", "decoder_hidden_states", "+=", "(", "\n", "(", "outputs", ".", "decoder_hidden_states", ",", ")", "\n", "if", "self", ".", "config", ".", "is_encoder_decoder", "\n", "else", "(", "outputs", ".", "hidden_states", ",", ")", "\n", ")", "\n", "\n", "# reshape for beam search", "\n", "", "", "vocab_size", "=", "next_token_scores", ".", "shape", "[", "-", "1", "]", "\n", "next_token_scores", "=", "next_token_scores", ".", "view", "(", "batch_size", ",", "num_beams", "*", "vocab_size", ")", "\n", "#m = torch.nn.LayerNorm(num_beams * vocab_size)", "\n", "#next_token_scores = m(next_token_scores)", "\n", "\n", "next_token_scores_group", "=", "torch", ".", "sum", "(", "next_token_scores", ",", "dim", "=", "0", ",", "keepdim", "=", "True", ")", ".", "expand", "(", "batch_size", ",", "-", "1", ")", "/", "batch_size", "\n", "\n", "for", "i", "in", "range", "(", "next_token_scores", ".", "size", "(", "0", ")", ")", ":", "\n", "                ", "'''tmin = torch.min(next_token_scores_group[i])\n                for j in range(1,len(model_kwargs['decoder_ori_input_ids'][i])):\n                    next_token_scores_group[i][model_kwargs['decoder_ori_input_ids'][i][j]] = tmin'''", "\n", "for", "t", "in", "model_kwargs", "[", "'decoder_ori_input_ids'", "]", "[", "i", "]", ":", "\n", "                    ", "for", "j", "in", "range", "(", "num_beams", ")", ":", "\n", "#if t not in input_ids[i] or t==1:", "\n", "                        ", "next_token_scores_group", "[", "i", "]", "[", "j", "*", "vocab_size", "+", "t", "]", "=", "next_token_scores", "[", "i", "]", "[", "j", "*", "vocab_size", "+", "t", "]", "\n", "\n", "", "", "", "next_token_scores", ",", "next_tokens", "=", "torch", ".", "topk", "(", "\n", "next_token_scores_group", ",", "2", "*", "num_beams", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "\n", "'''next_token_scores_group = next_token_scores_group.expand(batch_size,-1)\n            next_tokens_group = next_tokens_group.expand(batch_size,-1)\n\n            next_token_scores, next_tokens = torch.topk(\n                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n            )\n\n            for i in range(next_token_scores.size(0)):\n                j1 = 0\n                for j in range(next_token_scores.size(1)):\n                    if next_tokens[i][j] not in model_kwargs['decoder_ori_input_ids'][i]:\n                        next_tokens[i][j] = next_tokens_group[i][j1]\n                        j1 += 1\n            next_token_scores = next_token_scores_group\n\n            del next_token_scores_group, next_tokens_group'''", "\n", "\n", "next_indices", "=", "next_tokens", "//", "vocab_size", "\n", "next_tokens", "=", "next_tokens", "%", "vocab_size", "\n", "\n", "# stateless", "\n", "beam_outputs", "=", "beam_scorer", ".", "process", "(", "\n", "input_ids", ",", "\n", "next_token_scores", ",", "\n", "next_tokens", ",", "\n", "next_indices", ",", "\n", "pad_token_id", "=", "pad_token_id", ",", "\n", "eos_token_id", "=", "eos_token_id", ",", "\n", ")", "\n", "beam_scores", "=", "beam_outputs", "[", "\"next_beam_scores\"", "]", "\n", "beam_next_tokens", "=", "beam_outputs", "[", "\"next_beam_tokens\"", "]", "\n", "beam_idx", "=", "beam_outputs", "[", "\"next_beam_indices\"", "]", "\n", "\n", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", "[", "beam_idx", ",", ":", "]", ",", "beam_next_tokens", ".", "unsqueeze", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "model_kwargs", "=", "self", ".", "_update_model_kwargs_for_generation", "(", "\n", "outputs", ",", "model_kwargs", ",", "is_encoder_decoder", "=", "self", ".", "config", ".", "is_encoder_decoder", "\n", ")", "\n", "if", "model_kwargs", "[", "\"past\"", "]", "is", "not", "None", ":", "\n", "                ", "model_kwargs", "[", "\"past\"", "]", "=", "self", ".", "_reorder_cache", "(", "model_kwargs", "[", "\"past\"", "]", ",", "beam_idx", ")", "\n", "\n", "", "if", "beam_scorer", ".", "is_done", ":", "\n", "                ", "break", "\n", "\n", "", "", "sequence_outputs", "=", "beam_scorer", ".", "finalize", "(", "\n", "input_ids", ",", "beam_scores", ",", "next_tokens", ",", "next_indices", ",", "pad_token_id", "=", "pad_token_id", ",", "eos_token_id", "=", "eos_token_id", "\n", ")", "\n", "\n", "if", "return_dict_in_generate", ":", "\n", "            ", "if", "not", "output_scores", ":", "\n", "                ", "sequence_outputs", "[", "\"sequence_scores\"", "]", "=", "None", "\n", "", "if", "self", ".", "config", ".", "is_encoder_decoder", ":", "\n", "                ", "return", "BeamSearchEncoderDecoderOutput", "(", "\n", "sequences", "=", "sequence_outputs", "[", "\"sequences\"", "]", ",", "\n", "sequences_scores", "=", "sequence_outputs", "[", "\"sequence_scores\"", "]", ",", "\n", "scores", "=", "scores", ",", "\n", "encoder_attentions", "=", "encoder_attentions", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "decoder_attentions", "=", "decoder_attentions", ",", "\n", "decoder_hidden_states", "=", "decoder_hidden_states", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "return", "BeamSearchDecoderOnlyOutput", "(", "\n", "sequences", "=", "sequence_outputs", "[", "\"sequences\"", "]", ",", "\n", "sequences_scores", "=", "sequence_outputs", "[", "\"sequence_scores\"", "]", ",", "\n", "scores", "=", "scores", ",", "\n", "attentions", "=", "decoder_attentions", ",", "\n", "hidden_states", "=", "decoder_hidden_states", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "return", "sequence_outputs", "[", "\"sequences\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.src.bart_with_group_beam.BartForConditionalGeneration_GroupBeam.group_beam_search": [[296, 609], ["len", "torch.full", "torch.full", "torch.full", "torch.full", "beam_scores.view.view.view", "beam_scorer.finalize", "transformers.generation_logits_process.LogitsProcessorList", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam.prepare_inputs_for_generation", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam.", "range", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam._update_model_kwargs_for_generation", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_kwargs[].get", "model_kwargs[].get", "min", "range", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam.adjust_logits_during_generation", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "logits_processor", "next_token_scores.view.view.view", "range", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "beam_scorer.process", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bart_with_group_beam.BartForConditionalGeneration_GroupBeam._reorder_cache", "transformers.generation_utils.BeamSearchEncoderDecoderOutput", "transformers.generation_utils.BeamSearchDecoderOnlyOutput", "torch.zeros_like().half", "torch.zeros_like().half", "torch.zeros_like().half", "torch.zeros_like().half", "batch_group_indices.extend", "beam_scores[].unsqueeze().expand_as", "next_token_scores.view.view.half", "torch.sum().expand", "torch.sum().expand", "torch.sum().expand", "torch.sum().expand", "next_token_scores.view.view.size", "torch.zeros.unsqueeze", "torch.zeros.unsqueeze", "range", "beam_next_tokens.unsqueeze", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "beam_scores[].unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "range"], "methods", ["None"], ["", "", "def", "group_beam_search", "(", "\n", "self", ",", "\n", "input_ids", ":", "torch", ".", "LongTensor", ",", "\n", "beam_scorer", ":", "BeamScorer", ",", "\n", "logits_processor", ":", "Optional", "[", "LogitsProcessorList", "]", "=", "None", ",", "\n", "max_length", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "pad_token_id", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "eos_token_id", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "output_attentions", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "output_hidden_states", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "output_scores", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "return_dict_in_generate", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "**", "model_kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        Generates sequences for models with a language modeling head using beam search decoding.\n\n        Parameters:\n\n            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n                The sequence used as a prompt for the generation. If :obj:`None` the method initializes it as an empty\n                :obj:`torch.LongTensor` of shape :obj:`(1,)`.\n            beam_scorer (:obj:`BeamScorer`):\n                An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n                constructed, stored and sorted during generation. For more information, the documentation of\n                :class:`~transformers.BeamScorer` should be read.\n            logits_processor (:obj:`LogitsProcessorList`, `optional`):\n                An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n                :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n                head applied at each generation step.\n            max_length (:obj:`int`, `optional`, defaults to 20):\n                The maximum length of the sequence to be generated.\n            pad_token_id (:obj:`int`, `optional`):\n                The id of the `padding` token.\n            eos_token_id (:obj:`int`, `optional`):\n                The id of the `end-of-sequence` token.\n            output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n                returned tensors for more details.\n            output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n                Whether or not to return trhe hidden states of all layers. See ``hidden_states`` under returned tensors\n                for more details.\n            output_scores (:obj:`bool`, `optional`, defaults to `False`):\n                Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n            return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n            model_kwargs:\n                Additional model specific kwargs that will be forwarded to the :obj:`forward` function of the model. If\n                model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n\n        Return:\n            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n            :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n            :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n            ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n            :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if\n            ``model.config.is_encoder_decoder=True``.\n\n        Examples::\n\n            >>> from transformers import (\n            ...    AutoTokenizer,\n            ...    AutoModelForSeq2SeqLM,\n            ...    LogitsProcessorList,\n            ...    MinLengthLogitsProcessor,\n            ...    HammingDiversityLogitsProcessor,\n            ...    BeamSearchScorer,\n            ... )\n            >>> import torch\n\n            >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n            >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n\n            >>> encoder_input_str = \"translate English to German: How old are you?\"\n            >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n\n            >>> # lets run diverse beam search using 6 beams\n            >>> num_beams = 6\n            >>> # define decoder start token ids\n            >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n            >>> input_ids = input_ids * model.config.decoder_start_token_id\n\n            >>> # add encoder_outputs to model keyword arguments\n            >>> model_kwargs = {\n            ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n            ... }\n\n            >>> # instantiate beam scorer\n            >>> beam_scorer = BeamSearchScorer(\n            ...     batch_size=1,\n            ...     max_length=model.config.max_length,\n            ...     num_beams=num_beams,\n            ...     device=model.device,\n            ...     num_beam_groups=3\n            ... )\n\n            >>> # instantiate logits processors\n            >>> logits_processor = LogitsProcessorList([\n            ...     HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),\n            ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n            ... ])\n\n            >>> outputs = model.group_beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n\n            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n        \"\"\"", "\n", "\n", "# init values", "\n", "logits_processor", "=", "logits_processor", "if", "logits_processor", "is", "not", "None", "else", "LogitsProcessorList", "(", ")", "\n", "max_length", "=", "max_length", "if", "max_length", "is", "not", "None", "else", "self", ".", "config", ".", "max_length", "\n", "pad_token_id", "=", "pad_token_id", "if", "pad_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "pad_token_id", "\n", "eos_token_id", "=", "eos_token_id", "if", "eos_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "eos_token_id", "\n", "output_scores", "=", "output_scores", "if", "output_scores", "is", "not", "None", "else", "self", ".", "config", ".", "output_scores", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict_in_generate", "=", "(", "\n", "return_dict_in_generate", "if", "return_dict_in_generate", "is", "not", "None", "else", "self", ".", "config", ".", "return_dict_in_generate", "\n", ")", "\n", "\n", "# init attention / hidden states / scores tuples", "\n", "scores", "=", "(", ")", "if", "(", "return_dict_in_generate", "and", "output_scores", ")", "else", "None", "\n", "decoder_attentions", "=", "(", ")", "if", "(", "return_dict_in_generate", "and", "output_attentions", ")", "else", "None", "\n", "decoder_hidden_states", "=", "(", ")", "if", "(", "return_dict_in_generate", "and", "output_hidden_states", ")", "else", "None", "\n", "\n", "# if model is an encoder-decoder, retrieve encoder attention weights and hidden states", "\n", "if", "return_dict_in_generate", "and", "self", ".", "config", ".", "is_encoder_decoder", ":", "\n", "            ", "encoder_attentions", "=", "model_kwargs", "[", "\"encoder_outputs\"", "]", ".", "get", "(", "\"attentions\"", ")", "if", "output_attentions", "else", "None", "\n", "encoder_hidden_states", "=", "(", "\n", "model_kwargs", "[", "\"encoder_outputs\"", "]", ".", "get", "(", "\"hidden_states\"", ")", "if", "output_hidden_states", "else", "None", "\n", ")", "\n", "\n", "", "batch_size", "=", "len", "(", "beam_scorer", ".", "_beam_hyps", ")", "\n", "num_beams", "=", "beam_scorer", ".", "num_beams", "\n", "num_beam_groups", "=", "beam_scorer", ".", "num_beam_groups", "\n", "num_sub_beams", "=", "num_beams", "//", "num_beam_groups", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "batch_beam_size", ",", "cur_len", "=", "input_ids", ".", "shape", "\n", "\n", "assert", "(", "\n", "num_beams", "*", "batch_size", "==", "batch_beam_size", "\n", ")", ",", "f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"", "\n", "\n", "beam_scores", "=", "torch", ".", "full", "(", "(", "batch_size", ",", "num_beams", ")", ",", "-", "1e9", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "# initialise score of first beam of each group with 0 and the rest with 1e-9. This ensures that the beams in", "\n", "# the same group don't produce same tokens everytime.", "\n", "beam_scores", "[", ":", ",", ":", ":", "num_sub_beams", "]", "=", "0", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "(", "batch_size", "*", "num_beams", ",", ")", ")", "\n", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "# predicted tokens in cur_len step", "\n", "            ", "current_tokens", "=", "torch", ".", "zeros", "(", "batch_size", "*", "num_beams", ",", "dtype", "=", "input_ids", ".", "dtype", ",", "device", "=", "device", ")", "\n", "\n", "# indices which will form the beams in the next time step", "\n", "reordering_indices", "=", "torch", ".", "zeros", "(", "batch_size", "*", "num_beams", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# do one decoder step on all beams of all sentences in batch", "\n", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "input_ids", ",", "**", "model_kwargs", ")", "\n", "outputs", "=", "self", "(", "\n", "**", "model_inputs", ",", "\n", "return_dict", "=", "True", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "\n", "for", "beam_group_idx", "in", "range", "(", "num_beam_groups", ")", ":", "\n", "                ", "group_start_idx", "=", "beam_group_idx", "*", "num_sub_beams", "\n", "group_end_idx", "=", "min", "(", "group_start_idx", "+", "num_sub_beams", ",", "num_beams", ")", "\n", "group_size", "=", "group_end_idx", "-", "group_start_idx", "\n", "\n", "# indices of beams of current group among all sentences in batch", "\n", "batch_group_indices", "=", "[", "]", "\n", "\n", "if", "output_scores", ":", "\n", "                    ", "processed_score", "=", "torch", ".", "zeros_like", "(", "outputs", ".", "logits", "[", ":", ",", "-", "1", ",", ":", "]", ")", ".", "half", "(", ")", "# .float()", "\n", "\n", "", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "                    ", "batch_group_indices", ".", "extend", "(", "\n", "[", "batch_idx", "*", "num_beams", "+", "idx", "for", "idx", "in", "range", "(", "group_start_idx", ",", "group_end_idx", ")", "]", "\n", ")", "\n", "", "group_input_ids", "=", "input_ids", "[", "batch_group_indices", "]", "\n", "\n", "# select outputs of beams of current group only", "\n", "next_token_logits", "=", "outputs", ".", "logits", "[", "batch_group_indices", ",", "-", "1", ",", ":", "]", "\n", "\n", "# adjust tokens for Bart, *e.g.*", "\n", "next_token_logits", "=", "self", ".", "adjust_logits_during_generation", "(", "\n", "next_token_logits", ",", "cur_len", "=", "cur_len", ",", "max_length", "=", "max_length", "\n", ")", "\n", "\n", "next_token_scores", "=", "F", ".", "log_softmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", "# (batch_size * group_size, vocab_size)", "\n", "vocab_size", "=", "next_token_scores", ".", "shape", "[", "-", "1", "]", "\n", "\n", "next_token_scores", "=", "logits_processor", "(", "\n", "group_input_ids", ",", "next_token_scores", ",", "current_tokens", "=", "current_tokens", ",", "beam_group_idx", "=", "beam_group_idx", "\n", ")", "\n", "next_token_scores", "=", "next_token_scores", "+", "beam_scores", "[", "batch_group_indices", "]", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand_as", "(", "\n", "next_token_scores", "\n", ")", "\n", "\n", "if", "output_scores", ":", "\n", "                    ", "processed_score", "[", "batch_group_indices", "]", "=", "next_token_scores", ".", "half", "(", ")", "# .float()", "\n", "\n", "# reshape for beam search", "\n", "", "next_token_scores", "=", "next_token_scores", ".", "view", "(", "batch_size", ",", "group_size", "*", "vocab_size", ")", "\n", "###", "\n", "\n", "next_token_scores_group", "=", "torch", ".", "sum", "(", "next_token_scores", ",", "dim", "=", "0", ",", "keepdim", "=", "True", ")", ".", "expand", "(", "batch_size", ",", "\n", "-", "1", ")", "/", "batch_size", "\n", "\n", "for", "i", "in", "range", "(", "next_token_scores", ".", "size", "(", "0", ")", ")", ":", "\n", "                    ", "'''tmin = torch.min(next_token_scores_group[i])\n                    for j in range(1,len(model_kwargs['decoder_ori_input_ids'][i])):\n                        next_token_scores_group[i][model_kwargs['decoder_ori_input_ids'][i][j]] = tmin'''", "\n", "for", "t", "in", "model_kwargs", "[", "'decoder_ori_input_ids'", "]", "[", "i", "]", ":", "\n", "                        ", "for", "j", "in", "range", "(", "group_size", ")", ":", "\n", "# if t not in input_ids[i] or t==1:", "\n", "                            ", "next_token_scores_group", "[", "i", "]", "[", "j", "*", "vocab_size", "+", "t", "]", "=", "next_token_scores", "[", "i", "]", "[", "j", "*", "vocab_size", "+", "t", "]", "\n", "\n", "", "", "", "next_token_scores", ",", "next_tokens", "=", "torch", ".", "topk", "(", "\n", "next_token_scores_group", ",", "2", "*", "group_size", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "\n", "\n", "###", "\n", "#next_token_scores, next_tokens = torch.topk(", "\n", "#    next_token_scores, 2 * group_size, dim=1, largest=True, sorted=True", "\n", "#)", "\n", "\n", "next_indices", "=", "next_tokens", "//", "vocab_size", "\n", "next_tokens", "=", "next_tokens", "%", "vocab_size", "\n", "\n", "# stateless", "\n", "beam_outputs", "=", "beam_scorer", ".", "process", "(", "\n", "group_input_ids", ",", "\n", "next_token_scores", ",", "\n", "next_tokens", ",", "\n", "next_indices", ",", "\n", "pad_token_id", "=", "pad_token_id", ",", "\n", "eos_token_id", "=", "eos_token_id", ",", "\n", ")", "\n", "beam_scores", "[", "batch_group_indices", "]", "=", "beam_outputs", "[", "\"next_beam_scores\"", "]", "\n", "beam_next_tokens", "=", "beam_outputs", "[", "\"next_beam_tokens\"", "]", "\n", "beam_idx", "=", "beam_outputs", "[", "\"next_beam_indices\"", "]", "\n", "\n", "input_ids", "[", "batch_group_indices", "]", "=", "group_input_ids", "[", "beam_idx", "]", "\n", "group_input_ids", "=", "torch", ".", "cat", "(", "[", "group_input_ids", "[", "beam_idx", ",", ":", "]", ",", "beam_next_tokens", ".", "unsqueeze", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "current_tokens", "[", "batch_group_indices", "]", "=", "group_input_ids", "[", ":", ",", "-", "1", "]", "\n", "\n", "# (beam_idx // group_size) -> batch_idx", "\n", "# (beam_idx % group_size) -> offset of idx inside the group", "\n", "reordering_indices", "[", "batch_group_indices", "]", "=", "(", "\n", "num_beams", "*", "(", "beam_idx", "//", "group_size", ")", "+", "group_start_idx", "+", "(", "beam_idx", "%", "group_size", ")", "\n", ")", "\n", "\n", "# Store scores, attentions and hidden_states when required", "\n", "", "if", "return_dict_in_generate", ":", "\n", "                ", "if", "output_scores", ":", "\n", "                    ", "scores", "+=", "(", "processed_score", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                    ", "decoder_attentions", "+=", "(", "\n", "(", "outputs", ".", "decoder_attentions", ",", ")", "if", "self", ".", "config", ".", "is_encoder_decoder", "else", "(", "outputs", ".", "attentions", ",", ")", "\n", ")", "\n", "\n", "", "if", "output_hidden_states", ":", "\n", "                    ", "decoder_hidden_states", "+=", "(", "\n", "(", "outputs", ".", "decoder_hidden_states", ",", ")", "\n", "if", "self", ".", "config", ".", "is_encoder_decoder", "\n", "else", "(", "outputs", ".", "hidden_states", ",", ")", "\n", ")", "\n", "\n", "", "", "model_kwargs", "=", "self", ".", "_update_model_kwargs_for_generation", "(", "\n", "outputs", ",", "model_kwargs", ",", "is_encoder_decoder", "=", "self", ".", "config", ".", "is_encoder_decoder", "\n", ")", "\n", "if", "model_kwargs", "[", "\"past\"", "]", "is", "not", "None", ":", "\n", "                ", "model_kwargs", "[", "\"past\"", "]", "=", "self", ".", "_reorder_cache", "(", "model_kwargs", "[", "\"past\"", "]", ",", "reordering_indices", ")", "\n", "\n", "", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "current_tokens", ".", "unsqueeze", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "cur_len", "=", "cur_len", "+", "1", "\n", "if", "beam_scorer", ".", "is_done", ":", "\n", "                ", "break", "\n", "\n", "", "", "sequence_outputs", "=", "beam_scorer", ".", "finalize", "(", "\n", "input_ids", ",", "beam_scores", ",", "next_tokens", ",", "next_indices", ",", "pad_token_id", "=", "pad_token_id", ",", "eos_token_id", "=", "eos_token_id", ",", "max_length", "=", "max_length", ",", "\n", ")", "\n", "\n", "if", "return_dict_in_generate", ":", "\n", "            ", "if", "not", "output_scores", ":", "\n", "                ", "sequence_outputs", "[", "\"sequence_scores\"", "]", "\n", "", "if", "self", ".", "config", ".", "is_encoder_decoder", ":", "\n", "                ", "return", "BeamSearchEncoderDecoderOutput", "(", "\n", "sequences", "=", "sequence_outputs", "[", "\"sequences\"", "]", ",", "\n", "sequences_scores", "=", "sequence_outputs", "[", "\"sequence_scores\"", "]", ",", "\n", "scores", "=", "scores", ",", "\n", "encoder_attentions", "=", "encoder_attentions", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "decoder_attentions", "=", "decoder_attentions", ",", "\n", "decoder_hidden_states", "=", "decoder_hidden_states", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "return", "BeamSearchDecoderOnlyOutput", "(", "\n", "sequences", "=", "sequence_outputs", "[", "\"sequences\"", "]", ",", "\n", "sequences_scores", "=", "sequence_outputs", "[", "\"sequence_scores\"", "]", ",", "\n", "scores", "=", "scores", ",", "\n", "attentions", "=", "decoder_attentions", ",", "\n", "hidden_states", "=", "decoder_hidden_states", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "return", "sequence_outputs", "[", "\"sequences\"", "]", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.chenxran_orion.src.utils.post_process_template": [[4, 8], ["tB.endswith"], "function", ["None"], ["def", "post_process_template", "(", "tB", ")", ":", "\n", "    ", "if", "tB", ".", "endswith", "(", "'.'", ")", "==", "False", ":", "\n", "        ", "tB", "+=", "'.'", "\n", "", "return", "tB", "\n", "# return tB.split('.')[0] + '.'", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.src.utils.construct_template": [[11, 33], ["len", "len", "templateA.index", "len", "len"], "function", ["None"], ["", "def", "construct_template", "(", "words", ",", "templateA", ",", "if_then", "=", "False", ")", ":", "\n", "    ", "if", "len", "(", "words", ")", "==", "2", ":", "\n", "# template = ['{} <mask> {}.'.format(words[0], words[1])]", "\n", "        ", "templates", "=", "[", "\n", "# '{} is <mask> {}.'.format(words[0], words[1]), ", "\n", "'{} <mask> {}.'", ".", "format", "(", "words", "[", "0", "]", ",", "words", "[", "1", "]", ")", ",", "\n", "]", "\n", "", "elif", "len", "(", "words", ")", "==", "1", ":", "\n", "        ", "templates", "=", "[", "\n", "# '{} is <mask>.'.format(words[0]),", "\n", "'{} <mask>.'", ".", "format", "(", "words", "[", "0", "]", ")", "]", "\n", "\n", "", "elif", "len", "(", "words", ")", "==", "0", ":", "\n", "        ", "templates", "=", "[", "]", "\n", "\n", "", "if", "if_then", ":", "\n", "        ", "for", "word", "in", "words", ":", "\n", "            ", "index", "=", "templateA", ".", "index", "(", "'<mask>'", ")", "\n", "templateA", "=", "templateA", "[", ":", "index", "]", "+", "word", "+", "templateA", "[", "index", "+", "len", "(", "'<mask>'", ")", ":", "]", "\n", "", "templates", "=", "[", "'If '", "+", "templateA", "+", "' then '", "+", "template", "for", "template", "in", "templates", "]", "\n", "\n", "", "return", "templates", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.src.utils.filter_words": [[35, 79], ["sorted", "ret.append", "word.split", "words[].split", "len", "len"], "function", ["None"], ["", "def", "filter_words", "(", "words_prob", ")", ":", "\n", "    ", "word_count", "=", "{", "}", "\n", "token1_count", "=", "{", "}", "\n", "word2_count", "=", "{", "}", "\n", "ret", "=", "[", "]", "\n", "for", "words", ",", "prob", ",", "*", "_", "in", "words_prob", ":", "\n", "        ", "filter_this", "=", "False", "\n", "\n", "# filter repetitive token", "\n", "token_count", "=", "{", "}", "\n", "for", "word", "in", "words", ":", "\n", "            ", "for", "token", "in", "word", ".", "split", "(", "' '", ")", ":", "\n", "                ", "if", "token", "in", "token_count", ":", "\n", "                    ", "filter_this", "=", "True", "\n", "", "token_count", "[", "token", "]", "=", "1", "\n", "", "", "if", "filter_this", ":", "\n", "            ", "prob", "*=", "0.5", "\n", "\n", "# filter repetitive words", "\n", "", "if", "len", "(", "words", ")", "==", "2", "and", "words", "[", "0", "]", "==", "words", "[", "1", "]", ":", "\n", "            ", "continue", "\n", "\n", "# filter repetitive first token", "\n", "", "token1", "=", "words", "[", "0", "]", ".", "split", "(", "' '", ")", "[", "0", "]", "\n", "if", "token1", "not", "in", "token1_count", ":", "\n", "            ", "token1_count", "[", "token1", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "token1_count", "[", "token1", "]", "+=", "1", "\n", "prob", "/=", "token1_count", "[", "token1", "]", "\n", "\n", "", "for", "word", "in", "words", ":", "\n", "            ", "if", "word", "not", "in", "word_count", ":", "\n", "                ", "word_count", "[", "word", "]", "=", "0", "\n", "", "word_count", "[", "word", "]", "+=", "1", "\n", "prob", "/=", "word_count", "[", "word", "]", "\n", "\n", "", "if", "len", "(", "words", ")", "==", "2", ":", "\n", "            ", "if", "words", "[", "1", "]", "not", "in", "word2_count", ":", "\n", "                ", "word2_count", "[", "words", "[", "1", "]", "]", "=", "0", "\n", "", "word2_count", "[", "words", "[", "1", "]", "]", "+=", "1", "\n", "prob", "/=", "word2_count", "[", "words", "[", "1", "]", "]", "\n", "\n", "", "ret", ".", "append", "(", "[", "words", ",", "prob", "]", ")", "\n", "", "return", "sorted", "(", "ret", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.src.utils.convert_for_print": [[85, 93], ["copy.deepcopy", "range", "len", "round", "len", "range", "len", "round"], "function", ["None"], ["def", "convert_for_print", "(", "arr", ")", ":", "\n", "    ", "ret", "=", "deepcopy", "(", "arr", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "ret", ")", ")", ":", "\n", "        ", "ret", "[", "i", "]", "[", "1", "]", "=", "round", "(", "ret", "[", "i", "]", "[", "1", "]", ",", "7", ")", "\n", "if", "len", "(", "ret", "[", "i", "]", ")", "==", "3", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "ret", "[", "i", "]", "[", "2", "]", ")", ")", ":", "\n", "                ", "ret", "[", "i", "]", "[", "2", "]", "[", "j", "]", "=", "round", "(", "ret", "[", "i", "]", "[", "2", "]", "[", "j", "]", ",", "7", ")", "\n", "", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.src.utils.formalize_tA": [[95, 104], ["tA.replace.strip", "tA.replace.endswith", "tA.replace.replace", "tA.replace.replace", "tA[].strip"], "function", ["None"], ["", "def", "formalize_tA", "(", "tA", ")", ":", "\n", "    ", "tA", "=", "tA", ".", "strip", "(", ")", "\n", "if", "tA", ".", "endswith", "(", "'.'", ")", ":", "\n", "        ", "tA", "=", "tA", "[", ":", "-", "1", "]", ".", "strip", "(", ")", "+", "'.'", "\n", "", "else", ":", "\n", "        ", "tA", "+=", "'.'", "\n", "", "tA", "=", "tA", ".", "replace", "(", "' ,'", ",", "','", ")", "\n", "tA", "=", "tA", ".", "replace", "(", "\" '\"", ",", "\"'\"", ")", "\n", "return", "tA", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.src.utils.extract_similar_words": [[109, 127], ["range", "ngram.NGram", "len", "range", "ngram.NGram.find", "ret.append", "len", "len", "min", "txt_ngrams.append", "word.lower", "len", "txt[].lower", "x.lower"], "function", ["None"], ["def", "extract_similar_words", "(", "txt", ",", "words", ")", ":", "\n", "    ", "max_word_length", "=", "0", "\n", "for", "word", "in", "words", ":", "\n", "        ", "if", "len", "(", "word", ")", ">", "max_word_length", ":", "\n", "            ", "max_word_length", "=", "len", "(", "word", ")", "\n", "\n", "", "", "txt_ngrams", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "txt", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "i", "+", "ngram_n", ",", "min", "(", "len", "(", "txt", ")", ",", "i", "+", "max_word_length", "+", "5", ")", ")", ":", "\n", "            ", "txt_ngrams", ".", "append", "(", "txt", "[", "i", ":", "j", "]", ".", "lower", "(", ")", ")", "\n", "", "", "n", "=", "NGram", "(", "txt_ngrams", ",", "key", "=", "lambda", "x", ":", "x", ".", "lower", "(", ")", ",", "N", "=", "ngram_n", ")", "\n", "ret", "=", "[", "]", "\n", "for", "word", "in", "words", ":", "\n", "        ", "matched_word", "=", "n", ".", "find", "(", "word", ".", "lower", "(", ")", ",", "0.5", ")", "\n", "if", "matched_word", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "ret", ".", "append", "(", "matched_word", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.src.utils.extract_words": [[129, 134], ["word.lower"], "function", ["None"], ["", "def", "extract_words", "(", "txt", ",", "words", ")", ":", "\n", "    ", "for", "word", "in", "words", ":", "\n", "        ", "if", "word", "not", "in", "txt", ":", "\n", "            ", "return", "None", "\n", "", "", "return", "[", "word", ".", "lower", "(", ")", "for", "word", "in", "words", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.chenxran_orion.distinct_n.metrics.distinct_n_sentence_level": [[6, 18], ["list", "len", "set", "src.distinct_n.distinct_n.utils.ngrams"], "function", ["home.repos.pwc.inspect_result.chenxran_orion.distinct_n.utils.ngrams"], ["def", "distinct_n_sentence_level", "(", "sentence", ",", "n", ")", ":", "\n", "    ", "\"\"\"\n    Compute distinct-N for a single sentence.\n    :param sentence: a list of words.\n    :param n: int, ngram.\n    :return: float, the metric value.\n    \"\"\"", "\n", "if", "len", "(", "sentence", ")", "==", "0", ":", "\n", "        ", "return", "0.0", "# Prevent a zero division", "\n", "# distinct_ngrams = set(ngrams(sentence, n))", "\n", "# print(ngrams(sentence, n))", "\n", "", "return", "list", "(", "set", "(", "ngrams", "(", "sentence", ",", "n", ")", ")", ")", "\n", "# return len(distinct_ngrams) / len(sentence)", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.distinct_n.metrics.distinct_n_corpus_level": [[21, 34], ["len", "temp.extend", "len", "metrics.distinct_n_sentence_level", "set"], "function", ["home.repos.pwc.inspect_result.chenxran_orion.distinct_n.metrics.distinct_n_sentence_level"], ["", "def", "distinct_n_corpus_level", "(", "sentences", ",", "n", ")", ":", "\n", "    ", "\"\"\"\n    Compute average distinct-N of a list of sentences (the corpus).\n    :param sentences: a list of sentence.\n    :param n: int, ngram.\n    :return: float, the average value.\n    \"\"\"", "\n", "temp", "=", "[", "]", "\n", "length", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "length", "+=", "len", "(", "sentence", ")", "\n", "temp", ".", "extend", "(", "distinct_n_sentence_level", "(", "sentence", ",", "n", ")", ")", "\n", "", "return", "len", "(", "set", "(", "temp", ")", ")", "/", "length", "\n", "", ""]], "home.repos.pwc.inspect_result.chenxran_orion.distinct_n.test.TestDistinctN.test_unigram": [[8, 16], ["test.TestDistinctN.assertAlmostEqual", "test.TestDistinctN.assertAlmostEqual", "distinct_n.distinct_n_sentence_level", "distinct_n.distinct_n_sentence_level"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.distinct_n.metrics.distinct_n_sentence_level", "home.repos.pwc.inspect_result.chenxran_orion.distinct_n.metrics.distinct_n_sentence_level"], ["    ", "def", "test_unigram", "(", "self", ")", ":", "\n", "        ", "sentence", "=", "\"the the the the the\"", ".", "split", "(", ")", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "distinct_n_sentence_level", "(", "sentence", ",", "1", ")", ",", "0.2", "\n", ")", "\n", "sentence", "=", "\"the the the the cat\"", ".", "split", "(", ")", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "distinct_n_sentence_level", "(", "sentence", ",", "1", ")", ",", "0.4", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.distinct_n.test.TestDistinctN.test_bigram": [[18, 22], ["test.TestDistinctN.assertAlmostEqual", "distinct_n.distinct_n_sentence_level"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.distinct_n.metrics.distinct_n_sentence_level"], ["", "def", "test_bigram", "(", "self", ")", ":", "\n", "        ", "sentence", "=", "\"the cat sat on the\"", ".", "split", "(", ")", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "distinct_n_sentence_level", "(", "sentence", ",", "2", ")", ",", "0.8", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.distinct_n.test.TestDistinctN.test_corpus_level": [[24, 33], ["test.TestDistinctN.assertAlmostEqual", "test.TestDistinctN.assertAlmostEqual", "distinct_n.distinct_n_corpus_level", "distinct_n.distinct_n_corpus_level"], "methods", ["home.repos.pwc.inspect_result.chenxran_orion.distinct_n.metrics.distinct_n_corpus_level", "home.repos.pwc.inspect_result.chenxran_orion.distinct_n.metrics.distinct_n_corpus_level"], ["", "def", "test_corpus_level", "(", "self", ")", ":", "\n", "        ", "sentences", "=", "[", "\n", "'the cat sat on the mat'", ".", "split", "(", ")", ",", "\n", "'mat the on sat cat the'", ".", "split", "(", ")", ",", "\n", "'i do not know'", ".", "split", "(", ")", ",", "\n", "'Sorry but i do not know'", ".", "split", "(", ")", ",", "\n", "]", "\n", "self", ".", "assertAlmostEqual", "(", "0.916666", ",", "distinct_n_corpus_level", "(", "sentences", ",", "1", ")", ",", "delta", "=", "1e-5", ")", "\n", "self", ".", "assertAlmostEqual", "(", "0.8125", ",", "distinct_n_corpus_level", "(", "sentences", ",", "2", ")", ",", "delta", "=", "1e-5", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.chenxran_orion.distinct_n.utils.pad_sequence": [[9, 41], ["iter", "itertools.chain", "itertools.chain"], "function", ["None"], ["\n", "\n", "", "def", "construct_template", "(", "words", ",", "templateA", ",", "if_then", "=", "False", ")", ":", "\n", "    ", "if", "len", "(", "words", ")", "==", "2", ":", "\n", "# template = ['{} <mask> {}.'.format(words[0], words[1])]", "\n", "        ", "templates", "=", "[", "\n", "# '{} is <mask> {}.'.format(words[0], words[1]), ", "\n", "'{} <mask> {}.'", ".", "format", "(", "words", "[", "0", "]", ",", "words", "[", "1", "]", ")", ",", "\n", "]", "\n", "", "elif", "len", "(", "words", ")", "==", "1", ":", "\n", "        ", "templates", "=", "[", "\n", "# '{} is <mask>.'.format(words[0]),", "\n", "'{} <mask>.'", ".", "format", "(", "words", "[", "0", "]", ")", "]", "\n", "\n", "", "elif", "len", "(", "words", ")", "==", "0", ":", "\n", "        ", "templates", "=", "[", "]", "\n", "\n", "", "if", "if_then", ":", "\n", "        ", "for", "word", "in", "words", ":", "\n", "            ", "index", "=", "templateA", ".", "index", "(", "'<mask>'", ")", "\n", "templateA", "=", "templateA", "[", ":", "index", "]", "+", "word", "+", "templateA", "[", "index", "+", "len", "(", "'<mask>'", ")", ":", "]", "\n", "", "templates", "=", "[", "'If '", "+", "templateA", "+", "' then '", "+", "template", "for", "template", "in", "templates", "]", "\n", "\n", "", "return", "templates", "\n", "\n", "\n", "", "def", "filter_words", "(", "words_prob", ")", ":", "\n", "    ", "word_count", "=", "{", "}", "\n", "token1_count", "=", "{", "}", "\n", "word2_count", "=", "{", "}", "\n", "ret", "=", "[", "]", "\n", "for", "words", ",", "prob", ",", "*", "_", "in", "words_prob", ":", "\n", "        ", "filter_this", "=", "False", "\n"]], "home.repos.pwc.inspect_result.chenxran_orion.distinct_n.utils.ngrams": [[43, 91], ["utils.pad_sequence", "history.append", "history.append", "next", "tuple"], "function", ["home.repos.pwc.inspect_result.chenxran_orion.distinct_n.utils.pad_sequence"], ["# filter repetitive token", "\n", "token_count", "=", "{", "}", "\n", "for", "word", "in", "words", ":", "\n", "            ", "for", "token", "in", "word", ".", "split", "(", "' '", ")", ":", "\n", "                ", "if", "token", "in", "token_count", ":", "\n", "                    ", "filter_this", "=", "True", "\n", "", "token_count", "[", "token", "]", "=", "1", "\n", "", "", "if", "filter_this", ":", "\n", "            ", "prob", "*=", "0.5", "\n", "\n", "# filter repetitive words", "\n", "", "if", "len", "(", "words", ")", "==", "2", "and", "words", "[", "0", "]", "==", "words", "[", "1", "]", ":", "\n", "            ", "continue", "\n", "\n", "# filter repetitive first token", "\n", "", "token1", "=", "words", "[", "0", "]", ".", "split", "(", "' '", ")", "[", "0", "]", "\n", "if", "token1", "not", "in", "token1_count", ":", "\n", "            ", "token1_count", "[", "token1", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "token1_count", "[", "token1", "]", "+=", "1", "\n", "prob", "/=", "token1_count", "[", "token1", "]", "\n", "\n", "", "for", "word", "in", "words", ":", "\n", "            ", "if", "word", "not", "in", "word_count", ":", "\n", "                ", "word_count", "[", "word", "]", "=", "0", "\n", "", "word_count", "[", "word", "]", "+=", "1", "\n", "prob", "/=", "word_count", "[", "word", "]", "\n", "\n", "", "if", "len", "(", "words", ")", "==", "2", ":", "\n", "            ", "if", "words", "[", "1", "]", "not", "in", "word2_count", ":", "\n", "                ", "word2_count", "[", "words", "[", "1", "]", "]", "=", "0", "\n", "", "word2_count", "[", "words", "[", "1", "]", "]", "+=", "1", "\n", "prob", "/=", "word2_count", "[", "words", "[", "1", "]", "]", "\n", "\n", "", "ret", ".", "append", "(", "[", "words", ",", "prob", "]", ")", "\n", "", "return", "sorted", "(", "ret", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "\n", "", "import", "math", "\n", "from", "copy", "import", "deepcopy", "\n", "\n", "\n", "def", "convert_for_print", "(", "arr", ")", ":", "\n", "    ", "ret", "=", "deepcopy", "(", "arr", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "ret", ")", ")", ":", "\n", "        ", "ret", "[", "i", "]", "[", "1", "]", "=", "round", "(", "ret", "[", "i", "]", "[", "1", "]", ",", "7", ")", "\n", "if", "len", "(", "ret", "[", "i", "]", ")", "==", "3", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "ret", "[", "i", "]", "[", "2", "]", ")", ")", ":", "\n", "                ", "ret", "[", "i", "]", "[", "2", "]", "[", "j", "]", "=", "round", "(", "ret", "[", "i", "]", "[", "2", "]", "[", "j", "]", ",", "7", ")", "\n"]]}