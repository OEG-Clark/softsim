{"home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_embedding.train": [[38, 161], ["chengyubert.utils.distributed.broadcast_tensors", "chengyubert.utils.misc.set_dropout", "chengyubert.optim.misc.build_optimizer", "torch.cuda.amp.GradScaler", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.RunningMeter", "model.train", "time.time", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.optim.misc.build_optimizer.step", "chengyubert.utils.save.save_training_meta", "chengyubert.utils.logger.TB_LOGGER.create", "tqdm.tqdm", "chengyubert.utils.save.ModelSaver", "os.makedirs", "chengyubert.utils.logger.add_log_to_file", "chengyubert.utils.misc.NoOp", "chengyubert.utils.misc.NoOp", "len", "enumerate", "chengyubert.utils.logger.LOGGER.info", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "targets.size", "torch.cuda.amp.GradScaler.scale().backward", "chengyubert.utils.logger.RunningMeter.", "model.parameters", "torch.cuda.amp.autocast", "model", "loss.mean.mean", "chengyubert.utils.distributed.all_reduce_and_rescale_tensors", "loss.mean.item", "chengyubert.optim.get_lr_sched", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.logger.RunningMeter", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.logger.TB_LOGGER.step", "torch.cuda.amp.GradScaler.step", "torch.cuda.amp.GradScaler.update", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.utils.misc.NoOp.update", "torch.cuda.amp.GradScaler.scale", "float", "torch.cuda.amp.GradScaler.unscale_", "torch.nn.utils.clip_grad_norm_", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "sum", "int", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "train_embedding.evaluation", "chengyubert.utils.misc.NoOp.save", "model.parameters", "sum", "len", "model.parameters", "chengyubert.utils.distributed.all_gather_list", "dict", "chengyubert.utils.misc.NoOp.set_description", "filter", "time.time", "dataloaders.items", "x[].startswith"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.broadcast_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_dropout", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.misc.build_optimizer", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.save_training_meta", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.create", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.add_log_to_file", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.GatherLayer.backward", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_reduce_and_rescale_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.get_lr_sched", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list"], ["def", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", ":", "\n", "# make sure every process has same model parameters in the beginning", "\n", "    ", "broadcast_tensors", "(", "[", "p", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ",", "0", ")", "\n", "set_dropout", "(", "model", ",", "opts", ".", "dropout", ")", "\n", "\n", "# Prepare optimizer", "\n", "optimizer", "=", "build_optimizer", "(", "model", ",", "opts", ")", "\n", "scaler", "=", "GradScaler", "(", ")", "\n", "\n", "global_step", "=", "0", "\n", "if", "opts", ".", "rank", "==", "0", ":", "\n", "        ", "save_training_meta", "(", "opts", ")", "\n", "TB_LOGGER", ".", "create", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ")", ")", "\n", "pbar", "=", "tqdm", "(", "total", "=", "opts", ".", "num_train_steps", ",", "desc", "=", "opts", ".", "model", ")", "\n", "model_saver", "=", "ModelSaver", "(", "join", "(", "opts", ".", "output_dir", ",", "'ckpt'", ")", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'results'", ")", ",", "exist_ok", "=", "True", ")", "# store val predictions", "\n", "add_log_to_file", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ",", "'log.txt'", ")", ")", "\n", "", "else", ":", "\n", "        ", "LOGGER", ".", "disabled", "=", "True", "\n", "pbar", "=", "NoOp", "(", ")", "\n", "model_saver", "=", "NoOp", "(", ")", "\n", "\n", "", "LOGGER", ".", "info", "(", "f\"***** Running training with {opts.n_gpu} GPUs *****\"", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "dataloaders", "[", "'train'", "]", ".", "dataset", ")", ")", "\n", "LOGGER", ".", "info", "(", "\"  Batch size = %d\"", ",", "opts", ".", "train_batch_size", ")", "\n", "LOGGER", ".", "info", "(", "\"  Accumulate steps = %d\"", ",", "opts", ".", "gradient_accumulation_steps", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num steps = %d\"", ",", "opts", ".", "num_train_steps", ")", "\n", "\n", "running_loss", "=", "RunningMeter", "(", "'loss'", ")", "\n", "model", ".", "train", "(", ")", "\n", "n_examples", "=", "0", "\n", "n_epoch", "=", "0", "\n", "best_ckpt", "=", "0", "\n", "best_eval", "=", "0", "\n", "start", "=", "time", "(", ")", "\n", "# quick hack for amp delay_unscale bug", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "while", "True", ":", "\n", "        ", "for", "step", ",", "batch", "in", "enumerate", "(", "dataloaders", "[", "'train'", "]", ")", ":", "\n", "            ", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'gather_index'", "]", "\n", "n_examples", "+=", "targets", ".", "size", "(", "0", ")", "\n", "\n", "with", "autocast", "(", ")", ":", "\n", "                ", "loss", "=", "model", "(", "**", "batch", ",", "compute_loss", "=", "True", ")", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "delay_unscale", "=", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "!=", "0", "\n", "scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", ")", "\n", "if", "not", "delay_unscale", ":", "\n", "# gather gradients from every processes", "\n", "# do this before unscaling to make sure every process uses", "\n", "# the same gradient scale", "\n", "                ", "grads", "=", "[", "p", ".", "grad", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "\n", "if", "p", ".", "requires_grad", "and", "p", ".", "grad", "is", "not", "None", "]", "\n", "all_reduce_and_rescale_tensors", "(", "grads", ",", "float", "(", "1", ")", ")", "\n", "\n", "", "running_loss", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "global_step", "+=", "1", "\n", "\n", "# learning rate scheduling", "\n", "lr_this_step", "=", "get_lr_sched", "(", "global_step", ",", "opts", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "TB_LOGGER", ".", "add_scalar", "(", "'lr'", ",", "lr_this_step", ",", "global_step", ")", "\n", "\n", "# log loss", "\n", "losses", "=", "all_gather_list", "(", "running_loss", ")", "\n", "running_loss", "=", "RunningMeter", "(", "\n", "'loss'", ",", "sum", "(", "l", ".", "val", "for", "l", "in", "losses", ")", "/", "len", "(", "losses", ")", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'loss'", ",", "running_loss", ".", "val", ",", "global_step", ")", "\n", "TB_LOGGER", ".", "step", "(", ")", "\n", "\n", "# update model params", "\n", "if", "opts", ".", "grad_norm", "!=", "-", "1", ":", "\n", "# Unscales the gradients of optimizer's assigned params in-place", "\n", "                    ", "scaler", ".", "unscale_", "(", "optimizer", ")", "\n", "grad_norm", "=", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "opts", ".", "grad_norm", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'grad_norm'", ",", "grad_norm", ",", "global_step", ")", "\n", "\n", "# scaler.step() first unscales gradients of the optimizer's params.", "\n", "# If gradients don't contain infs/NaNs, optimizer.step() is then called,", "\n", "# otherwise, optimizer.step() is skipped.", "\n", "", "scaler", ".", "step", "(", "optimizer", ")", "\n", "\n", "# Updates the scale for next iteration.", "\n", "scaler", ".", "update", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "if", "global_step", "%", "100", "==", "0", ":", "\n", "# monitor training throughput", "\n", "                    ", "tot_ex", "=", "sum", "(", "all_gather_list", "(", "n_examples", ")", ")", "\n", "ex_per_sec", "=", "int", "(", "tot_ex", "/", "(", "time", "(", ")", "-", "start", ")", ")", "\n", "LOGGER", ".", "info", "(", "f'{opts.model}: {n_epoch}-{global_step}: '", "\n", "f'{tot_ex} examples trained at '", "\n", "f'{ex_per_sec} ex/s '", "\n", "f'best_acc-{best_eval * 100:.2f}'", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'perf/ex_per_s'", ",", "\n", "ex_per_sec", ",", "global_step", ")", "\n", "\n", "", "if", "global_step", "%", "opts", ".", "valid_steps", "==", "0", ":", "\n", "                    ", "log", "=", "evaluation", "(", "model", ",", "\n", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", ".", "startswith", "(", "'val'", ")", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "\n", "opts", ",", "global_step", ")", "\n", "log_eval", "=", "log", "[", "'val/acc'", "]", "\n", "if", "log_eval", ">", "best_eval", ":", "\n", "                        ", "best_ckpt", "=", "global_step", "\n", "best_eval", "=", "log_eval", "\n", "pbar", ".", "set_description", "(", "f'{opts.model}: {n_epoch}-{best_ckpt} best_acc-{best_eval * 100:.2f}'", ")", "\n", "", "model_saver", ".", "save", "(", "model", ",", "global_step", ")", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "                ", "break", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "            ", "break", "\n", "", "n_epoch", "+=", "1", "\n", "LOGGER", ".", "info", "(", "f\"Step {global_step}: finished {n_epoch} epochs\"", ")", "\n", "# if n_epoch >= opts.num_train_epochs:", "\n", "#     break", "\n", "", "return", "best_ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_embedding.validate": [[163, 228], ["torch.no_grad", "horovod.torch.no_grad", "time.time", "sum", "sum", "sum", "sum", "getattr", "chengyubert.data.evaluation.judge", "chengyubert.utils.logger.LOGGER.info", "tqdm.tqdm", "enumerate", "open", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.distributed.all_gather_list", "time.time", "os.path.isfile", "chengyubert.utils.distributed.all_gather_list", "range", "model", "torch.nn.functional.cross_entropy", "F.cross_entropy.item", "scores.max", "max_idx.cpu().tolist", "torch.gather().cpu().numpy", "horovod.torch.gather().cpu().numpy", "enumerate", "results.extend", "len", "tq.update", "f.write", "open", "glob.glob", "zip", "over_logits[].cpu().numpy", "numpy.argsort", "zip", "len", "shutil.copyfileobj", "int", "len", "max_idx.cpu", "torch.gather().cpu", "horovod.torch.gather().cpu", "open", "over_logits[].cpu", "numpy.argwhere().item", "torch.gather", "horovod.torch.gather", "numpy.argwhere", "scores.max", "torch.gather().cpu().numpy.unsqueeze"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "validate", "(", "opts", ",", "model", ",", "val_loader", ",", "split", ",", "global_step", ")", ":", "\n", "    ", "val_loss", "=", "0", "\n", "tot_score", "=", "0", "\n", "n_ex", "=", "0", "\n", "val_mrr", "=", "0", "\n", "st", "=", "time", "(", ")", "\n", "results", "=", "[", "]", "\n", "with", "tqdm", "(", "range", "(", "len", "(", "val_loader", ".", "dataset", ")", "//", "opts", ".", "size", ")", ",", "desc", "=", "f'{split}-{opts.rank}'", ")", "as", "tq", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "            ", "qids", "=", "batch", "[", "'qids'", "]", "\n", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'qids'", "]", "\n", "del", "batch", "[", "'gather_index'", "]", "\n", "\n", "scores", ",", "over_logits", "=", "model", "(", "**", "batch", ",", "targets", "=", "None", ",", "compute_loss", "=", "False", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "scores", ",", "targets", ",", "reduction", "=", "'sum'", ")", "\n", "val_loss", "+=", "loss", ".", "item", "(", ")", "\n", "tot_score", "+=", "(", "scores", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "[", "1", "]", "==", "targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "max_prob", ",", "max_idx", "=", "scores", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "\n", "answers", "=", "max_idx", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "targets", "=", "torch", ".", "gather", "(", "batch", "[", "'option_ids'", "]", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "j", ",", "(", "qid", ",", "target", ")", "in", "enumerate", "(", "zip", "(", "qids", ",", "targets", ")", ")", ":", "\n", "                ", "g", "=", "over_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "top_k", "=", "np", ".", "argsort", "(", "-", "g", ")", "\n", "val_mrr", "+=", "1", "/", "(", "1", "+", "np", ".", "argwhere", "(", "top_k", "==", "target", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "results", ".", "extend", "(", "zip", "(", "qids", ",", "answers", ")", ")", "\n", "n_ex", "+=", "len", "(", "qids", ")", "\n", "tq", ".", "update", "(", "len", "(", "qids", ")", ")", "\n", "\n", "", "", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank{opts.rank}.csv'", "\n", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "id_", ",", "ans", "in", "results", ":", "\n", "            ", "f", ".", "write", "(", "f'{id_},{ans}\\n'", ")", "\n", "\n", "", "", "val_loss", "=", "sum", "(", "all_gather_list", "(", "val_loss", ")", ")", "\n", "val_mrr", "=", "sum", "(", "all_gather_list", "(", "val_mrr", ")", ")", "\n", "# tot_score = sum(all_gather_list(tot_score))", "\n", "n_ex", "=", "sum", "(", "all_gather_list", "(", "n_ex", ")", ")", "\n", "tot_time", "=", "time", "(", ")", "-", "st", "\n", "\n", "val_loss", "/=", "n_ex", "\n", "val_mrr", "=", "val_mrr", "/", "n_ex", "\n", "\n", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}.csv'", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "out_file", ")", ":", "\n", "        ", "with", "open", "(", "out_file", ",", "'wb'", ")", "as", "g", ":", "\n", "            ", "for", "f", "in", "glob", ".", "glob", "(", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank*.csv'", ")", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "open", "(", "f", ",", "'rb'", ")", ",", "g", ")", "\n", "\n", "", "", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "txt_db", "=", "getattr", "(", "opts", ",", "f'{split}_txt_db'", ")", "\n", "val_acc", "=", "judge", "(", "out_file", ",", "f'{txt_db}/answer.csv'", ")", "\n", "val_log", "=", "{", "f'{split}/loss'", ":", "val_loss", ",", "\n", "f'{split}/acc'", ":", "val_acc", ",", "\n", "f'{split}/mrr'", ":", "val_mrr", ",", "\n", "f'{split}/ex_per_s'", ":", "n_ex", "/", "tot_time", "}", "\n", "LOGGER", ".", "info", "(", "f\"validation finished in {int(tot_time)} seconds, \"", "\n", "f\"score: {val_acc * 100:.2f}, \"", "\n", "f\"mrr: {val_mrr:.3f}\"", ")", "\n", "return", "val_log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_embedding.evaluate_embeddings_recall": [[230, 259], ["numpy.array", "len", "tqdm.tqdm", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyu_synonyms_dict.items", "any", "numpy.linalg.norm().argsort", "range", "recall_at_k_cosine.setdefault", "sum", "recall_at_k_norm.setdefault", "sum", "len", "numpy.linalg.norm", "sklearn.metrics.pairwise.cosine_similarity", "embeddings[].reshape"], "function", ["None"], ["", "def", "evaluate_embeddings_recall", "(", "embeddings", ",", "chengyu_vocab", ",", "chengyu_synonyms_dict", ")", ":", "\n", "    ", "iw", "=", "[", "k", "for", "k", "in", "embeddings", "]", "\n", "\n", "vectors", "=", "np", ".", "array", "(", "[", "embeddings", "[", "iw", "[", "i", "]", "]", "for", "i", "in", "range", "(", "len", "(", "iw", ")", ")", "]", ")", "\n", "cnt", "=", "0", "\n", "recall_at_k_cosine", "=", "{", "}", "\n", "recall_at_k_norm", "=", "{", "}", "\n", "k_list", "=", "[", "1", ",", "3", ",", "5", ",", "10", "]", "\n", "total", "=", "len", "(", "chengyu_synonyms_dict", ")", "\n", "for", "w", ",", "wl", "in", "tqdm", "(", "chengyu_synonyms_dict", ".", "items", "(", ")", ")", ":", "\n", "        ", "if", "w", "in", "embeddings", "and", "any", "(", "[", "x", "in", "embeddings", "for", "x", "in", "wl", "]", ")", ":", "\n", "            ", "cnt", "+=", "1", "\n", "\n", "cosine_distances", "=", "(", "1", "-", "cosine_similarity", "(", "embeddings", "[", "w", "]", ".", "reshape", "(", "1", ",", "-", "1", ")", ",", "vectors", ")", "[", "0", "]", ")", ".", "argsort", "(", ")", "\n", "norm_distances", "=", "np", ".", "linalg", ".", "norm", "(", "vectors", "-", "embeddings", "[", "w", "]", ",", "axis", "=", "1", ")", ".", "argsort", "(", ")", "\n", "cids", "=", "[", "idx", "for", "idx", "in", "cosine_distances", "if", "iw", "[", "idx", "]", "in", "chengyu_vocab", "]", "\n", "nids", "=", "[", "idx", "for", "idx", "in", "norm_distances", "if", "iw", "[", "idx", "]", "in", "chengyu_vocab", "]", "\n", "for", "k", "in", "k_list", ":", "\n", "                ", "top_ids", "=", "cids", "[", "1", ":", "k", "+", "1", "]", "\n", "recall_at_k_cosine", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "recall_at_k_cosine", "[", "k", "]", "+=", "sum", "(", "[", "1", "for", "idx", "in", "top_ids", "if", "iw", "[", "idx", "]", "in", "wl", "]", ")", "\n", "\n", "top_ids", "=", "nids", "[", "1", ":", "k", "+", "1", "]", "\n", "recall_at_k_norm", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "recall_at_k_norm", "[", "k", "]", "+=", "sum", "(", "[", "1", "for", "idx", "in", "top_ids", "if", "iw", "[", "idx", "]", "in", "wl", "]", ")", "\n", "", "", "", "LOGGER", ".", "info", "(", "f'{cnt} word pairs appeared in the training dictionary , total word pairs {total}'", ")", "\n", "LOGGER", ".", "info", "(", "recall_at_k_cosine", ")", "\n", "LOGGER", ".", "info", "(", "recall_at_k_norm", ")", "\n", "return", "cnt", ",", "total", ",", "recall_at_k_cosine", ",", "recall_at_k_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_embedding.evaluation": [[261, 276], ["model.eval", "data_loaders.items", "chengyubert.utils.logger.TB_LOGGER.log_scaler_dict", "model.train", "chengyubert.utils.logger.LOGGER.info", "log.update", "train_embedding.validate", "model.idiom_embedding.weight.detach().cpu().numpy", "train_embedding.evaluate_embeddings_recall", "model.idiom_embedding.weight.detach().cpu", "loader.dataset.chengyu_vocab.items", "model.idiom_embedding.weight.detach"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.log_scaler_dict", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.validate", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluate_embeddings_recall"], ["", "def", "evaluation", "(", "model", ",", "data_loaders", ":", "dict", ",", "opts", ",", "global_step", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "log", "=", "{", "}", "\n", "for", "split", ",", "loader", "in", "data_loaders", ".", "items", "(", ")", ":", "\n", "        ", "LOGGER", ".", "info", "(", "f\"Step {global_step}: start running \"", "\n", "f\"validation on {split} split...\"", ")", "\n", "log", ".", "update", "(", "validate", "(", "opts", ",", "model", ",", "loader", ",", "split", ",", "global_step", ")", ")", "\n", "if", "split", "==", "'val'", "and", "opts", ".", "evaluate_embedding", ":", "\n", "            ", "embeddings_np", "=", "model", ".", "idiom_embedding", ".", "weight", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "embeddings", "=", "{", "k", ":", "embeddings_np", "[", "v", "]", "for", "k", ",", "v", "in", "loader", ".", "dataset", ".", "chengyu_vocab", ".", "items", "(", ")", "}", "\n", "evaluate_embeddings_recall", "(", "embeddings", ",", "loader", ".", "dataset", ".", "chengyu_vocab", ",", "\n", "loader", ".", "dataset", ".", "chengyu_synonyms_dict", ")", "\n", "", "", "TB_LOGGER", ".", "log_scaler_dict", "(", "log", ")", "\n", "model", ".", "train", "(", ")", "\n", "return", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_embedding.get_best_ckpt": [[278, 293], ["re.compile", "glob.glob", "collections.Counter", "print", "collections.Counter.most_common", "chengyubert.data.evaluation.judge", "collections.Counter.update", "re.compile.match", "int", "os.path.join", "os.path.basename", "pat.match.group"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge"], ["", "def", "get_best_ckpt", "(", "val_data_dir", ",", "opts", ")", ":", "\n", "    ", "pat", "=", "re", ".", "compile", "(", "r'val_results_(?P<step>\\d+)_rank0.csv'", ")", "\n", "prediction_files", "=", "glob", ".", "glob", "(", "'{}/results/val_results_*_rank0.csv'", ".", "format", "(", "opts", ".", "output_dir", ")", ")", "\n", "\n", "top_files", "=", "Counter", "(", ")", "\n", "for", "f", "in", "prediction_files", ":", "\n", "        ", "acc", "=", "judge", "(", "f", ",", "os", ".", "path", ".", "join", "(", "val_data_dir", ",", "'answer.csv'", ")", ")", "\n", "top_files", ".", "update", "(", "{", "f", ":", "acc", "}", ")", "\n", "\n", "", "print", "(", "top_files", ")", "\n", "\n", "for", "f", ",", "acc", "in", "top_files", ".", "most_common", "(", "1", ")", ":", "\n", "        ", "m", "=", "pat", ".", "match", "(", "os", ".", "path", ".", "basename", "(", "f", ")", ")", "\n", "best_epoch", "=", "int", "(", "m", ".", "group", "(", "'step'", ")", ")", "\n", "return", "best_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_embedding.main": [[295, 342], ["torch.device", "horovod.torch.device", "torch.cuda.set_device", "horovod.torch.cuda.set_device", "horovod.torch.rank", "horovod.torch.size", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.misc.set_random_seed", "chengyubert.data.create_dataloaders", "chengyubert.models.build_model", "chengyubert.models.build_model.to", "sum", "train_embedding.evaluation", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "horovod.torch.local_rank", "horovod.torch.local_rank", "ValueError", "train_embedding.train", "chengyubert.utils.distributed.all_gather_list", "chengyubert.models.build_model.load_state_dict", "dict", "horovod.torch.rank", "train_embedding.get_best_ckpt", "torch.load", "horovod.torch.load", "filter", "itertools.chain", "os.makedirs", "dataloaders.items", "os.path.join", "format", "format"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_random_seed", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_dataloaders", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.__init__.build_model", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.get_best_ckpt"], ["", "", "def", "main", "(", "opts", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "hvd", ".", "local_rank", "(", ")", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "hvd", ".", "local_rank", "(", ")", ")", "\n", "rank", "=", "hvd", ".", "rank", "(", ")", "\n", "opts", ".", "rank", "=", "rank", "\n", "opts", ".", "size", "=", "hvd", ".", "size", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"device: {} n_gpu: {}, rank: {}, \"", "\n", "\"16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "hvd", ".", "rank", "(", ")", ",", "opts", ".", "fp16", ")", ")", "\n", "\n", "if", "opts", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, \"", "\n", "\"should be >= 1\"", ".", "format", "(", "\n", "opts", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "set_random_seed", "(", "opts", ".", "seed", ")", "\n", "\n", "# data loaders", "\n", "DatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "dataset_cls", "]", "\n", "EvalDatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "eval_dataset_cls", "]", "\n", "splits", ",", "dataloaders", "=", "create_dataloaders", "(", "DatasetCls", ",", "EvalDatasetCls", ",", "opts", ")", "\n", "opts", ".", "evaluate_embedding", "=", "True", "\n", "\n", "# Prepare model", "\n", "model", "=", "build_model", "(", "opts", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "opts", ".", "mode", "==", "'train'", ":", "\n", "        ", "best_ckpt", "=", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", "\n", "", "elif", "opts", ".", "mode", "==", "'eval'", ":", "\n", "        ", "best_ckpt", "=", "None", "\n", "if", "opts", ".", "rank", "==", "0", ":", "\n", "            ", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'results'", ")", ",", "exist_ok", "=", "True", ")", "# store val predictions", "\n", "", "", "else", ":", "\n", "        ", "best_ckpt", "=", "get_best_ckpt", "(", "dataloaders", "[", "'val'", "]", ".", "dataset", ".", "db_dir", ",", "opts", ")", "\n", "\n", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "if", "best_ckpt", "is", "not", "None", ":", "\n", "        ", "best_pt", "=", "f'{opts.output_dir}/ckpt/model_step_{best_ckpt}.pt'", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "best_pt", ")", ",", "strict", "=", "False", ")", "\n", "", "log", "=", "evaluation", "(", "model", ",", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", "!=", "'train'", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "opts", ",", "best_ckpt", ")", "\n", "splits", "=", "[", "'val'", ",", "'test'", ",", "'ran'", ",", "'sim'", ",", "'out'", "]", "\n", "LOGGER", ".", "info", "(", "'\\t'", ".", "join", "(", "splits", ")", ")", "\n", "LOGGER", ".", "info", "(", "'\\t'", ".", "join", "(", "chain", "(", "\n", "[", "format", "(", "log", "[", "f'{split}/acc'", "]", ",", "\"0.6f\"", ")", "for", "split", "in", "splits", "]", ",", "\n", "[", "format", "(", "log", "[", "f'{split}/mrr'", "]", ",", "\"0.6f\"", ")", "for", "split", "in", "splits", "]", "\n", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_competition.main": [[35, 195], ["torch.device", "horovod.torch.device", "torch.cuda.set_device", "horovod.torch.cuda.set_device", "horovod.torch.rank", "horovod.torch.size", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.misc.set_random_seed", "chengyubert.data.create_dataloaders", "chengyubert.models.build_model", "chengyubert.models.build_model.to", "chengyubert.utils.distributed.broadcast_tensors", "chengyubert.utils.misc.set_dropout", "chengyubert.optim.misc.build_optimizer", "torch.cuda.amp.GradScaler", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.RunningMeter", "chengyubert.models.build_model.train", "time.time", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.optim.misc.build_optimizer.step", "sum", "chengyubert.models.build_model.load_state_dict", "train_competition.evaluation", "horovod.torch.local_rank", "horovod.torch.local_rank", "ValueError", "chengyubert.utils.save.save_training_meta", "chengyubert.utils.logger.TB_LOGGER.create", "tqdm.tqdm", "chengyubert.utils.save.ModelSaver", "os.makedirs", "chengyubert.utils.logger.add_log_to_file", "chengyubert.utils.misc.NoOp", "chengyubert.utils.misc.NoOp", "len", "enumerate", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.distributed.all_gather_list", "torch.load", "horovod.torch.load", "dict", "horovod.torch.rank", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "targets.size", "torch.cuda.amp.GradScaler.scale().backward", "chengyubert.utils.logger.RunningMeter.", "filter", "chengyubert.models.build_model.parameters", "torch.cuda.amp.autocast", "chengyubert.models.build_model.", "loss.mean.mean", "chengyubert.utils.distributed.all_reduce_and_rescale_tensors", "loss.mean.item", "chengyubert.optim.get_lr_sched", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.logger.RunningMeter", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.logger.TB_LOGGER.step", "torch.cuda.amp.GradScaler.step", "torch.cuda.amp.GradScaler.update", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.utils.misc.NoOp.update", "dataloaders.items", "torch.cuda.amp.GradScaler.scale", "float", "torch.cuda.amp.GradScaler.unscale_", "torch.nn.utils.clip_grad_norm_", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "sum", "int", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "train_competition.evaluation", "chengyubert.utils.misc.NoOp.save", "chengyubert.models.build_model.parameters", "sum", "len", "chengyubert.models.build_model.parameters", "chengyubert.utils.distributed.all_gather_list", "dict", "chengyubert.utils.misc.NoOp.set_description", "AssertionError", "filter", "time.time", "dataloaders.items", "x[].startswith"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_random_seed", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_dataloaders", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.__init__.build_model", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.broadcast_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_dropout", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.misc.build_optimizer", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.save_training_meta", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.create", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.add_log_to_file", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.GatherLayer.backward", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_reduce_and_rescale_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.get_lr_sched", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list"], ["def", "main", "(", "opts", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "hvd", ".", "local_rank", "(", ")", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "hvd", ".", "local_rank", "(", ")", ")", "\n", "rank", "=", "hvd", ".", "rank", "(", ")", "\n", "opts", ".", "rank", "=", "rank", "\n", "opts", ".", "size", "=", "hvd", ".", "size", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"device: {} n_gpu: {}, rank: {}, \"", "\n", "\"16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "hvd", ".", "rank", "(", ")", ",", "opts", ".", "fp16", ")", ")", "\n", "\n", "if", "opts", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, \"", "\n", "\"should be >= 1\"", ".", "format", "(", "\n", "opts", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "set_random_seed", "(", "opts", ".", "seed", ")", "\n", "\n", "# data loaders", "\n", "DatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "dataset_cls", "]", "\n", "EvalDatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "eval_dataset_cls", "]", "\n", "splits", ",", "dataloaders", "=", "create_dataloaders", "(", "DatasetCls", ",", "EvalDatasetCls", ",", "opts", ")", "\n", "\n", "# Prepare model", "\n", "model", "=", "build_model", "(", "opts", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "# make sure every process has same model parameters in the beginning", "\n", "broadcast_tensors", "(", "[", "p", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ",", "0", ")", "\n", "set_dropout", "(", "model", ",", "opts", ".", "dropout", ")", "\n", "\n", "# Prepare optimizer", "\n", "optimizer", "=", "build_optimizer", "(", "model", ",", "opts", ")", "\n", "scaler", "=", "GradScaler", "(", ")", "\n", "\n", "global_step", "=", "0", "\n", "if", "rank", "==", "0", ":", "\n", "        ", "save_training_meta", "(", "opts", ")", "\n", "TB_LOGGER", ".", "create", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ")", ")", "\n", "pbar", "=", "tqdm", "(", "total", "=", "opts", ".", "num_train_steps", ",", "desc", "=", "opts", ".", "model", ")", "\n", "model_saver", "=", "ModelSaver", "(", "join", "(", "opts", ".", "output_dir", ",", "'ckpt'", ")", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'results'", ")", ",", "exist_ok", "=", "True", ")", "# store val predictions", "\n", "add_log_to_file", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ",", "'log.txt'", ")", ")", "\n", "", "else", ":", "\n", "        ", "LOGGER", ".", "disabled", "=", "True", "\n", "pbar", "=", "NoOp", "(", ")", "\n", "model_saver", "=", "NoOp", "(", ")", "\n", "\n", "", "LOGGER", ".", "info", "(", "f\"***** Running training with {n_gpu} GPUs *****\"", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "dataloaders", "[", "'train'", "]", ".", "dataset", ")", ")", "\n", "LOGGER", ".", "info", "(", "\"  Batch size = %d\"", ",", "opts", ".", "train_batch_size", ")", "\n", "LOGGER", ".", "info", "(", "\"  Accumulate steps = %d\"", ",", "opts", ".", "gradient_accumulation_steps", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num steps = %d\"", ",", "opts", ".", "num_train_steps", ")", "\n", "\n", "running_loss", "=", "RunningMeter", "(", "'loss'", ")", "\n", "model", ".", "train", "(", ")", "\n", "n_examples", "=", "0", "\n", "n_epoch", "=", "0", "\n", "best_ckpt", "=", "0", "\n", "best_eval", "=", "0", "\n", "start", "=", "time", "(", ")", "\n", "# quick hack for amp delay_unscale bug", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "while", "True", ":", "\n", "        ", "for", "step", ",", "batch", "in", "enumerate", "(", "dataloaders", "[", "'train'", "]", ")", ":", "\n", "            ", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'gather_index'", "]", "\n", "n_examples", "+=", "targets", ".", "size", "(", "0", ")", "\n", "\n", "with", "autocast", "(", ")", ":", "\n", "                ", "original_loss", ",", "enlarged_loss", "=", "model", "(", "**", "batch", ",", "compute_loss", "=", "True", ")", "\n", "if", "opts", ".", "candidates", "==", "'original'", ":", "\n", "                    ", "loss", "=", "original_loss", "\n", "", "elif", "opts", ".", "candidates", "==", "'enlarged'", ":", "\n", "                    ", "loss", "=", "enlarged_loss", "\n", "", "elif", "opts", ".", "candidates", "==", "'combined'", ":", "\n", "                    ", "loss", "=", "original_loss", "+", "enlarged_loss", "\n", "", "else", ":", "\n", "                    ", "raise", "AssertionError", "(", "\"No such loss!\"", ")", "\n", "\n", "", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "delay_unscale", "=", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "!=", "0", "\n", "scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", ")", "\n", "if", "not", "delay_unscale", ":", "\n", "# gather gradients from every processes", "\n", "# do this before unscaling to make sure every process uses", "\n", "# the same gradient scale", "\n", "                ", "grads", "=", "[", "p", ".", "grad", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "\n", "if", "p", ".", "requires_grad", "and", "p", ".", "grad", "is", "not", "None", "]", "\n", "all_reduce_and_rescale_tensors", "(", "grads", ",", "float", "(", "1", ")", ")", "\n", "\n", "", "running_loss", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "global_step", "+=", "1", "\n", "\n", "# learning rate scheduling", "\n", "lr_this_step", "=", "get_lr_sched", "(", "global_step", ",", "opts", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "TB_LOGGER", ".", "add_scalar", "(", "'lr'", ",", "lr_this_step", ",", "global_step", ")", "\n", "\n", "# log loss", "\n", "losses", "=", "all_gather_list", "(", "running_loss", ")", "\n", "running_loss", "=", "RunningMeter", "(", "\n", "'loss'", ",", "sum", "(", "l", ".", "val", "for", "l", "in", "losses", ")", "/", "len", "(", "losses", ")", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'loss'", ",", "running_loss", ".", "val", ",", "global_step", ")", "\n", "TB_LOGGER", ".", "step", "(", ")", "\n", "\n", "# update model params", "\n", "if", "opts", ".", "grad_norm", "!=", "-", "1", ":", "\n", "# Unscales the gradients of optimizer's assigned params in-place", "\n", "                    ", "scaler", ".", "unscale_", "(", "optimizer", ")", "\n", "grad_norm", "=", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "opts", ".", "grad_norm", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'grad_norm'", ",", "grad_norm", ",", "global_step", ")", "\n", "\n", "# scaler.step() first unscales gradients of the optimizer's params.", "\n", "# If gradients don't contain infs/NaNs, optimizer.step() is then called,", "\n", "# otherwise, optimizer.step() is skipped.", "\n", "", "scaler", ".", "step", "(", "optimizer", ")", "\n", "\n", "# Updates the scale for next iteration.", "\n", "scaler", ".", "update", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "if", "global_step", "%", "100", "==", "0", ":", "\n", "# monitor training throughput", "\n", "                    ", "tot_ex", "=", "sum", "(", "all_gather_list", "(", "n_examples", ")", ")", "\n", "ex_per_sec", "=", "int", "(", "tot_ex", "/", "(", "time", "(", ")", "-", "start", ")", ")", "\n", "LOGGER", ".", "info", "(", "f'{opts.model}: {n_epoch}-{global_step}: '", "\n", "f'{tot_ex} examples trained at '", "\n", "f'{ex_per_sec} ex/s '", "\n", "f'best_acc-{best_eval * 100:.2f}'", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'perf/ex_per_s'", ",", "\n", "ex_per_sec", ",", "global_step", ")", "\n", "\n", "", "if", "global_step", "%", "opts", ".", "valid_steps", "==", "0", ":", "\n", "                    ", "log", "=", "evaluation", "(", "model", ",", "\n", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", ".", "startswith", "(", "'val'", ")", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "\n", "opts", ",", "global_step", ")", "\n", "if", "log", "[", "'val/acc'", "]", ">", "best_eval", ":", "\n", "                        ", "best_ckpt", "=", "global_step", "\n", "best_eval", "=", "log", "[", "'val/acc'", "]", "\n", "pbar", ".", "set_description", "(", "f'{opts.model}: {n_epoch}-{best_ckpt} best_acc-{best_eval * 100:.2f}'", ")", "\n", "", "model_saver", ".", "save", "(", "model", ",", "global_step", ")", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "                ", "break", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "            ", "break", "\n", "", "n_epoch", "+=", "1", "\n", "LOGGER", ".", "info", "(", "f\"Step {global_step}: finished {n_epoch} epochs\"", ")", "\n", "\n", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "best_pt", "=", "f'{opts.output_dir}/ckpt/model_step_{best_ckpt}.pt'", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "best_pt", ")", ",", "strict", "=", "False", ")", "\n", "evaluation", "(", "model", ",", "\n", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", "!=", "'train'", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "\n", "opts", ",", "best_ckpt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_competition.evaluation": [[197, 207], ["model.eval", "data_loaders.items", "chengyubert.utils.logger.TB_LOGGER.log_scaler_dict", "model.train", "chengyubert.utils.logger.LOGGER.info", "log.update", "train_competition.validate"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.log_scaler_dict", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.validate"], ["", "def", "evaluation", "(", "model", ",", "data_loaders", ":", "dict", ",", "opts", ",", "global_step", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "log", "=", "{", "}", "\n", "for", "split", ",", "loader", "in", "data_loaders", ".", "items", "(", ")", ":", "\n", "        ", "LOGGER", ".", "info", "(", "f\"Step {global_step}: start running \"", "\n", "f\"validation on {split} split...\"", ")", "\n", "log", ".", "update", "(", "validate", "(", "opts", ",", "model", ",", "loader", ",", "split", ",", "global_step", ")", ")", "\n", "", "TB_LOGGER", ".", "log_scaler_dict", "(", "log", ")", "\n", "model", ".", "train", "(", ")", "\n", "return", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_competition.optimize_answer": [[209, 220], ["example_logits[].items", "numpy.array", "scipy.optimize.linear_sum_assignment", "zip", "tags.append", "costs.append"], "function", ["None"], ["", "def", "optimize_answer", "(", "example_logits", ")", ":", "\n", "    ", "for", "eid", "in", "example_logits", ":", "\n", "        ", "tags", "=", "[", "]", "\n", "costs", "=", "[", "]", "\n", "for", "tag", ",", "logits", "in", "example_logits", "[", "eid", "]", ".", "items", "(", ")", ":", "\n", "            ", "tags", ".", "append", "(", "tag", ")", "\n", "costs", ".", "append", "(", "logits", ")", "\n", "", "cost_matrix", "=", "np", ".", "array", "(", "costs", ")", "\n", "row_ind", ",", "col_ind", "=", "linear_sum_assignment", "(", "-", "cost_matrix", ")", "\n", "for", "tag", ",", "ind", "in", "zip", "(", "tags", ",", "col_ind", ")", ":", "\n", "            ", "yield", "tag", ",", "ind", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_competition.validate": [[222, 306], ["torch.no_grad", "horovod.torch.no_grad", "time.time", "sum", "sum", "sum", "os.path.join", "chengyubert.data.evaluation.judge", "chengyubert.utils.logger.LOGGER.info", "open", "json.load", "tqdm.tqdm", "enumerate", "open", "train_competition.optimize_answer", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.distributed.all_gather_list", "time.time", "os.path.isfile", "chengyubert.utils.distributed.all_gather_list", "chengyubert.data.intermediate_dir", "getattr", "range", "model", "torch.nn.functional.cross_entropy", "F.cross_entropy.item", "logits.max", "torch.eq().sum().item", "horovod.torch.eq().sum().item", "torch.gather().cpu().numpy", "horovod.torch.gather().cpu().numpy", "enumerate", "len", "tq.update", "f.write", "open", "glob.glob", "len", "zip", "over_logit.cpu().numpy", "numpy.argsort", "example_logits.setdefault", "score.cpu().numpy", "len", "shutil.copyfileobj", "int", "torch.eq().sum", "horovod.torch.eq().sum", "torch.gather().cpu", "horovod.torch.gather().cpu", "open", "AssertionError", "over_logit.cpu", "numpy.argwhere().item", "score.cpu", "torch.eq", "horovod.torch.eq", "torch.gather", "horovod.torch.gather", "numpy.argwhere", "torch.gather().cpu().numpy.unsqueeze"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_competition.optimize_answer", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.intermediate_dir"], ["", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "validate", "(", "opts", ",", "model", ",", "val_loader", ",", "split", ",", "global_step", ")", ":", "\n", "    ", "val_loss", "=", "0", "\n", "tot_score", "=", "0", "\n", "n_ex", "=", "0", "\n", "val_mrr", "=", "0", "\n", "st", "=", "time", "(", ")", "\n", "example_logits", "=", "{", "}", "\n", "with", "open", "(", "f'{val_loader.dataset.db_dir}/id2eid.json'", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "id2eid", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "with", "tqdm", "(", "range", "(", "len", "(", "val_loader", ".", "dataset", ")", ")", ",", "desc", "=", "split", ")", "as", "tq", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "            ", "qids", "=", "batch", "[", "'qids'", "]", "\n", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'gather_index'", "]", "\n", "del", "batch", "[", "'qids'", "]", "\n", "\n", "logits", ",", "over_logits", ",", "cond_logits", "=", "model", "(", "**", "batch", ",", "targets", "=", "None", ",", "compute_loss", "=", "False", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "targets", ",", "reduction", "=", "'sum'", ")", "\n", "val_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "if", "opts", ".", "candidates", "==", "'original'", ":", "\n", "                ", "logits", "=", "logits", "\n", "", "elif", "opts", ".", "candidates", "==", "'enlarged'", ":", "\n", "                ", "logits", "=", "cond_logits", "\n", "", "elif", "opts", ".", "candidates", "==", "'combined'", ":", "\n", "                ", "logits", "=", "logits", "+", "cond_logits", "\n", "", "else", ":", "\n", "                ", "raise", "AssertionError", "(", "\"No such loss!\"", ")", "\n", "\n", "# scores, over_logits = model(**batch, targets=None, compute_loss=False)", "\n", "# loss = F.cross_entropy(scores, targets, reduction='sum')", "\n", "# val_loss += loss.item()", "\n", "", "max_prob", ",", "max_idx", "=", "logits", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "\n", "tot_score", "+=", "torch", ".", "eq", "(", "max_idx", ",", "targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# tot_score += (scores.max(dim=-1, keepdim=False)[1] == targets).sum().item()", "\n", "\n", "targets", "=", "torch", ".", "gather", "(", "batch", "[", "'option_ids'", "]", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "j", ",", "(", "qid", ",", "target", ",", "score", ",", "over_logit", ")", "in", "enumerate", "(", "zip", "(", "qids", ",", "targets", ",", "logits", ",", "over_logits", ")", ")", ":", "\n", "                ", "g", "=", "over_logit", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "top_k", "=", "np", ".", "argsort", "(", "-", "g", ")", "\n", "val_mrr", "+=", "1", "/", "(", "1", "+", "np", ".", "argwhere", "(", "top_k", "==", "target", ")", ".", "item", "(", ")", ")", "\n", "\n", "eid", "=", "id2eid", "[", "qid", "]", "\n", "example_logits", ".", "setdefault", "(", "eid", ",", "{", "}", ")", "\n", "example_logits", "[", "eid", "]", "[", "qid", "]", "=", "score", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "n_ex", "+=", "len", "(", "qids", ")", "\n", "tq", ".", "update", "(", "len", "(", "qids", ")", ")", "\n", "\n", "", "", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank{opts.rank}.csv'", "\n", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "id_", ",", "ans", "in", "optimize_answer", "(", "example_logits", ")", ":", "\n", "            ", "f", ".", "write", "(", "f'{id_},{ans}\\n'", ")", "\n", "\n", "", "", "val_loss", "=", "sum", "(", "all_gather_list", "(", "val_loss", ")", ")", "\n", "n_ex", "=", "sum", "(", "all_gather_list", "(", "n_ex", ")", ")", "\n", "tot_time", "=", "time", "(", ")", "-", "st", "\n", "val_loss", "/=", "n_ex", "\n", "val_mrr", "=", "val_mrr", "/", "n_ex", "\n", "\n", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}.csv'", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "out_file", ")", ":", "\n", "        ", "with", "open", "(", "out_file", ",", "'wb'", ")", "as", "g", ":", "\n", "            ", "for", "f", "in", "glob", ".", "glob", "(", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank*.csv'", ")", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "open", "(", "f", ",", "'rb'", ")", ",", "g", ")", "\n", "\n", "", "", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "txt_db", "=", "os", ".", "path", ".", "join", "(", "'/txt'", ",", "\n", "intermediate_dir", "(", "opts", ".", "pretrained_model_name_or_path", ")", ",", "\n", "getattr", "(", "opts", ",", "f'{split}_txt_db'", ")", ")", "\n", "val_acc", "=", "judge", "(", "out_file", ",", "f'{txt_db}/answer.csv'", ")", "\n", "\n", "val_log", "=", "{", "f'{split}/loss'", ":", "val_loss", ",", "\n", "f'{split}/acc'", ":", "val_acc", ",", "\n", "f'{split}/mrr'", ":", "val_mrr", ",", "\n", "f'{split}/ex_per_s'", ":", "n_ex", "/", "tot_time", "}", "\n", "LOGGER", ".", "info", "(", "f\"validation finished in {int(tot_time)} seconds, \"", "\n", "f\"score: {val_acc * 100:.2f}, \"", "\n", "f\"mrr: {val_mrr:.3f}\"", ")", "\n", "return", "val_log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_composition.train": [[38, 159], ["chengyubert.utils.distributed.broadcast_tensors", "chengyubert.utils.misc.set_dropout", "chengyubert.optim.misc.build_optimizer", "torch.cuda.amp.GradScaler", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.RunningMeter", "model.train", "time.time", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.optim.misc.build_optimizer.step", "chengyubert.utils.save.save_training_meta", "chengyubert.utils.logger.TB_LOGGER.create", "tqdm.tqdm", "chengyubert.utils.save.ModelSaver", "os.makedirs", "chengyubert.utils.logger.add_log_to_file", "chengyubert.utils.misc.NoOp", "chengyubert.utils.misc.NoOp", "len", "enumerate", "chengyubert.utils.logger.LOGGER.info", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "targets.size", "torch.cuda.amp.GradScaler.scale().backward", "chengyubert.utils.logger.RunningMeter.", "model.parameters", "torch.cuda.amp.autocast", "model", "loss.mean.mean", "chengyubert.utils.distributed.all_reduce_and_rescale_tensors", "loss.mean.item", "chengyubert.optim.get_lr_sched", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.logger.RunningMeter", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.logger.TB_LOGGER.step", "torch.cuda.amp.GradScaler.step", "torch.cuda.amp.GradScaler.update", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.utils.misc.NoOp.update", "torch.cuda.amp.GradScaler.scale", "float", "torch.cuda.amp.GradScaler.unscale_", "torch.nn.utils.clip_grad_norm_", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "sum", "int", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "train_composition.evaluation", "chengyubert.utils.misc.NoOp.save", "model.parameters", "sum", "len", "model.parameters", "chengyubert.utils.distributed.all_gather_list", "dict", "chengyubert.utils.misc.NoOp.set_description", "filter", "time.time", "dataloaders.items", "x[].startswith"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.broadcast_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_dropout", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.misc.build_optimizer", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.save_training_meta", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.create", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.add_log_to_file", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.GatherLayer.backward", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_reduce_and_rescale_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.get_lr_sched", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list"], ["def", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", ":", "\n", "# make sure every process has same model parameters in the beginning", "\n", "    ", "broadcast_tensors", "(", "[", "p", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ",", "0", ")", "\n", "set_dropout", "(", "model", ",", "opts", ".", "dropout", ")", "\n", "\n", "# Prepare optimizer", "\n", "optimizer", "=", "build_optimizer", "(", "model", ",", "opts", ")", "\n", "scaler", "=", "GradScaler", "(", ")", "\n", "\n", "global_step", "=", "0", "\n", "if", "opts", ".", "rank", "==", "0", ":", "\n", "        ", "save_training_meta", "(", "opts", ")", "\n", "TB_LOGGER", ".", "create", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ")", ")", "\n", "pbar", "=", "tqdm", "(", "total", "=", "opts", ".", "num_train_steps", ",", "desc", "=", "opts", ".", "model", ")", "\n", "model_saver", "=", "ModelSaver", "(", "join", "(", "opts", ".", "output_dir", ",", "'ckpt'", ")", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'results'", ")", ",", "exist_ok", "=", "True", ")", "# store val predictions", "\n", "add_log_to_file", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ",", "'log.txt'", ")", ")", "\n", "", "else", ":", "\n", "        ", "LOGGER", ".", "disabled", "=", "True", "\n", "pbar", "=", "NoOp", "(", ")", "\n", "model_saver", "=", "NoOp", "(", ")", "\n", "\n", "", "LOGGER", ".", "info", "(", "f\"***** Running training with {opts.n_gpu} GPUs *****\"", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "dataloaders", "[", "'train'", "]", ".", "dataset", ")", ")", "\n", "LOGGER", ".", "info", "(", "\"  Batch size = %d\"", ",", "opts", ".", "train_batch_size", ")", "\n", "LOGGER", ".", "info", "(", "\"  Accumulate steps = %d\"", ",", "opts", ".", "gradient_accumulation_steps", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num steps = %d\"", ",", "opts", ".", "num_train_steps", ")", "\n", "\n", "running_loss", "=", "RunningMeter", "(", "'loss'", ")", "\n", "model", ".", "train", "(", ")", "\n", "n_examples", "=", "0", "\n", "n_epoch", "=", "0", "\n", "best_ckpt", "=", "0", "\n", "best_eval", "=", "0", "\n", "start", "=", "time", "(", ")", "\n", "# quick hack for amp delay_unscale bug", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "while", "True", ":", "\n", "        ", "for", "step", ",", "batch", "in", "enumerate", "(", "dataloaders", "[", "'train'", "]", ")", ":", "\n", "            ", "targets", "=", "batch", "[", "'targets'", "]", "\n", "n_examples", "+=", "targets", ".", "size", "(", "0", ")", "\n", "\n", "with", "autocast", "(", ")", ":", "\n", "                ", "_", ",", "loss", ",", "_", "=", "model", "(", "**", "batch", ",", "compute_loss", "=", "True", ")", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "delay_unscale", "=", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "!=", "0", "\n", "scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", ")", "\n", "if", "not", "delay_unscale", ":", "\n", "# gather gradients from every processes", "\n", "# do this before unscaling to make sure every process uses", "\n", "# the same gradient scale", "\n", "                ", "grads", "=", "[", "p", ".", "grad", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "\n", "if", "p", ".", "requires_grad", "and", "p", ".", "grad", "is", "not", "None", "]", "\n", "all_reduce_and_rescale_tensors", "(", "grads", ",", "float", "(", "1", ")", ")", "\n", "\n", "", "running_loss", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "global_step", "+=", "1", "\n", "\n", "# learning rate scheduling", "\n", "lr_this_step", "=", "get_lr_sched", "(", "global_step", ",", "opts", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "TB_LOGGER", ".", "add_scalar", "(", "'lr'", ",", "lr_this_step", ",", "global_step", ")", "\n", "\n", "# log loss", "\n", "losses", "=", "all_gather_list", "(", "running_loss", ")", "\n", "running_loss", "=", "RunningMeter", "(", "\n", "'loss'", ",", "sum", "(", "l", ".", "val", "for", "l", "in", "losses", ")", "/", "len", "(", "losses", ")", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'loss'", ",", "running_loss", ".", "val", ",", "global_step", ")", "\n", "TB_LOGGER", ".", "step", "(", ")", "\n", "\n", "# update model params", "\n", "if", "opts", ".", "grad_norm", "!=", "-", "1", ":", "\n", "# Unscales the gradients of optimizer's assigned params in-place", "\n", "                    ", "scaler", ".", "unscale_", "(", "optimizer", ")", "\n", "grad_norm", "=", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "opts", ".", "grad_norm", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'grad_norm'", ",", "grad_norm", ",", "global_step", ")", "\n", "\n", "# scaler.step() first unscales gradients of the optimizer's params.", "\n", "# If gradients don't contain infs/NaNs, optimizer.step() is then called,", "\n", "# otherwise, optimizer.step() is skipped.", "\n", "", "scaler", ".", "step", "(", "optimizer", ")", "\n", "\n", "# Updates the scale for next iteration.", "\n", "scaler", ".", "update", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "if", "global_step", "%", "100", "==", "0", ":", "\n", "# monitor training throughput", "\n", "                    ", "tot_ex", "=", "sum", "(", "all_gather_list", "(", "n_examples", ")", ")", "\n", "ex_per_sec", "=", "int", "(", "tot_ex", "/", "(", "time", "(", ")", "-", "start", ")", ")", "\n", "LOGGER", ".", "info", "(", "f'{opts.model}: {n_epoch}-{global_step}: '", "\n", "f'{tot_ex} examples trained at '", "\n", "f'{ex_per_sec} ex/s '", "\n", "f'best_acc-{best_eval * 100:.2f}'", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'perf/ex_per_s'", ",", "\n", "ex_per_sec", ",", "global_step", ")", "\n", "\n", "", "if", "global_step", "%", "opts", ".", "valid_steps", "==", "0", ":", "\n", "                    ", "log", "=", "evaluation", "(", "model", ",", "\n", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", ".", "startswith", "(", "'val'", ")", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "\n", "opts", ",", "global_step", ")", "\n", "if", "log", "[", "'val/acc'", "]", ">", "best_eval", ":", "\n", "                        ", "best_ckpt", "=", "global_step", "\n", "best_eval", "=", "log", "[", "'val/acc'", "]", "\n", "pbar", ".", "set_description", "(", "f'{opts.model}: {n_epoch}-{best_ckpt} best_acc-{best_eval * 100:.2f}'", ")", "\n", "", "model_saver", ".", "save", "(", "model", ",", "global_step", ")", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "                ", "break", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "            ", "break", "\n", "", "n_epoch", "+=", "1", "\n", "LOGGER", ".", "info", "(", "f\"Step {global_step}: finished {n_epoch} epochs\"", ")", "\n", "# if n_epoch >= opts.num_train_epochs:", "\n", "#     break", "\n", "", "return", "best_ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_composition.idiom2tree": [[161, 189], ["enumerate", "enumerate", "len", "ans.pop", "isinstance", "ans.pop", "isinstance", "ans.insert", "ans.pop", "isinstance", "ans.insert", "ans.pop.label", "ans.pop.label", "nltk.Tree", "ans.pop.label", "nltk.Tree"], "function", ["None"], ["", "def", "idiom2tree", "(", "idiom", ",", "select_masks", ")", ":", "\n", "# ans = list(idiom)", "\n", "    ", "ans", "=", "idiom", "\n", "for", "k", ",", "select_mask", "in", "enumerate", "(", "select_masks", ")", ":", "\n", "        ", "for", "idx", ",", "v", "in", "enumerate", "(", "select_mask", ")", ":", "\n", "            ", "if", "v", "==", "1", ":", "\n", "                ", "c0", "=", "ans", ".", "pop", "(", "idx", ")", "\n", "if", "isinstance", "(", "c0", ",", "Tree", ")", ":", "\n", "                    ", "c0_label", "=", "c0", ".", "label", "(", ")", "\n", "", "else", ":", "\n", "                    ", "c0_label", "=", "c0", "\n", "\n", "", "c1", "=", "ans", ".", "pop", "(", "idx", ")", "\n", "if", "isinstance", "(", "c1", ",", "Tree", ")", ":", "\n", "                    ", "c1_label", "=", "c1", ".", "label", "(", ")", "\n", "", "else", ":", "\n", "                    ", "c1_label", "=", "c1", "\n", "\n", "", "ans", ".", "insert", "(", "idx", ",", "Tree", "(", "c0_label", "+", "c1_label", ",", "(", "c0", ",", "c1", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "c", "=", "ans", ".", "pop", "(", "idx", ")", "\n", "if", "isinstance", "(", "c", ",", "Tree", ")", ":", "\n", "                    ", "c_label", "=", "c", ".", "label", "(", ")", "\n", "", "else", ":", "\n", "                    ", "c_label", "=", "c", "\n", "", "ans", ".", "insert", "(", "idx", ",", "Tree", "(", "c_label", ",", "(", "c", ",", ")", ")", ")", "\n", "", "", "", "assert", "len", "(", "ans", ")", "==", "2", "\n", "return", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_composition.validate": [[191, 284], ["torch.no_grad", "horovod.torch.no_grad", "time.time", "sum", "sum", "sum", "sum", "os.path.join", "chengyubert.data.evaluation.judge", "chengyubert.utils.logger.LOGGER.info", "tqdm.tqdm", "enumerate", "open", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.distributed.all_gather_list", "time.time", "os.path.isfile", "chengyubert.utils.distributed.all_gather_list", "chengyubert.data.intermediate_dir", "getattr", "range", "model", "torch.nn.functional.cross_entropy", "F.cross_entropy.item", "scores.max", "torch.gather", "horovod.torch.gather", "torch.gather().cpu().numpy", "horovod.torch.gather().cpu().numpy", "enumerate", "max_idx.cpu().tolist", "results.extend", "len", "tq.update", "f.write", "open", "glob.glob", "len", "zip", "over_logits[].cpu().numpy", "numpy.argsort", "zip", "len", "shutil.copyfileobj", "int", "torch.gather().cpu", "horovod.torch.gather().cpu", "print", "print", "atts[].cpu().numpy().tolist", "list", "print", "max_idx.cpu", "open", "over_logits[].cpu", "numpy.argwhere().item", "len", "atts.size", "select_mask[].long().cpu().numpy().tolist", "composition_gates[].sum", "nltk.Tree", "print", "torch.gather", "horovod.torch.gather", "answer.item", "atts[].cpu().numpy", "train_composition.idiom2tree", "chengyubert.utils.tree.TreePrettyPrinter().text", "numpy.argwhere", "o.item", "target.item", "select_mask[].long().cpu().numpy", "scores.max", "torch.gather().cpu().numpy.unsqueeze", "atts[].cpu", "chengyubert.utils.tree.TreePrettyPrinter", "select_mask[].long().cpu", "select_mask[].long"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.intermediate_dir", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_affection.idiom2tree", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.text"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "validate", "(", "opts", ",", "model", ",", "val_loader", ",", "split", ",", "global_step", ")", ":", "\n", "    ", "val_loss", "=", "0", "\n", "tot_score", "=", "0", "\n", "n_ex", "=", "0", "\n", "val_mrr", "=", "0", "\n", "st", "=", "time", "(", ")", "\n", "results", "=", "[", "]", "\n", "with", "tqdm", "(", "range", "(", "len", "(", "val_loader", ".", "dataset", ")", ")", ",", "desc", "=", "f'{split}-{opts.rank}'", ")", "as", "tq", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "            ", "qids", "=", "batch", "[", "'qids'", "]", "\n", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'qids'", "]", "\n", "\n", "scores", ",", "over_logits", ",", "composition", "=", "model", "(", "**", "batch", ",", "targets", "=", "None", ",", "compute_loss", "=", "False", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "scores", ",", "targets", ",", "reduction", "=", "'sum'", ")", "\n", "val_loss", "+=", "loss", ".", "item", "(", ")", "\n", "tot_score", "+=", "(", "scores", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "[", "1", "]", "==", "targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "max_prob", ",", "max_idx", "=", "scores", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "\n", "\n", "select_masks", ",", "atts", ",", "composition_gates", "=", "composition", "\n", "\n", "input_ids", "=", "torch", ".", "gather", "(", "batch", "[", "'input_ids'", "]", ",", "dim", "=", "1", ",", "index", "=", "batch", "[", "'gather_index'", "]", ")", "\n", "targets", "=", "torch", ".", "gather", "(", "batch", "[", "'option_ids'", "]", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "j", ",", "(", "qid", ",", "target", ",", "inp", ",", "option_ids", ",", "position", ",", "answer", ")", "in", "enumerate", "(", "zip", "(", "qids", ",", "targets", ",", "input_ids", ",", "\n", "batch", "[", "'option_ids'", "]", ",", "\n", "batch", "[", "'positions'", "]", ",", "\n", "max_idx", ")", ")", ":", "\n", "                ", "g", "=", "over_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "top_k", "=", "np", ".", "argsort", "(", "-", "g", ")", "\n", "val_mrr", "+=", "1", "/", "(", "1", "+", "np", ".", "argwhere", "(", "top_k", "==", "target", ")", ".", "item", "(", ")", ")", "\n", "if", "i", "%", "1000", "==", "0", ":", "\n", "                    ", "options", "=", "[", "val_loader", ".", "dataset", ".", "id2idiom", "[", "o", ".", "item", "(", ")", "]", "for", "o", "in", "option_ids", "]", "\n", "idiom", "=", "options", "[", "answer", ".", "item", "(", ")", "]", "\n", "print", "(", "qid", ",", "\n", "val_loader", ".", "dataset", ".", "id2idiom", "[", "target", ".", "item", "(", ")", "]", ",", "\n", "idiom", ",", "\n", "options", ")", "\n", "print", "(", "len", "(", "select_masks", ")", ",", "atts", ".", "size", "(", ")", ")", "\n", "s_masks", "=", "[", "select_mask", "[", "j", "]", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "for", "select_mask", "in", "select_masks", "]", "\n", "s_att", "=", "atts", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "# tokens = val_loader.dataset.tokenizer.convert_ids_to_tokens(inp)", "\n", "# start = tokens.index(val_loader.dataset.tokenizer.mask_token)", "\n", "# tokens[position:position + len(idiom)] = list(idiom)", "\n", "tokens", "=", "list", "(", "idiom", ")", "\n", "print", "(", "tokens", ",", "s_masks", ",", "s_att", ",", "composition_gates", "[", "j", "]", ".", "sum", "(", ")", ")", "\n", "try", ":", "\n", "                        ", "tree", "=", "Tree", "(", "' '", ".", "join", "(", "tokens", ")", ",", "idiom2tree", "(", "tokens", ",", "s_masks", ")", ")", "\n", "print", "(", "TreePrettyPrinter", "(", "tree", ")", ".", "text", "(", "unicodelines", "=", "True", ")", ")", "\n", "", "except", ":", "\n", "                        ", "pass", "\n", "\n", "", "", "", "answers", "=", "max_idx", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "results", ".", "extend", "(", "zip", "(", "qids", ",", "answers", ")", ")", "\n", "n_ex", "+=", "len", "(", "qids", ")", "\n", "tq", ".", "update", "(", "len", "(", "qids", ")", ")", "\n", "\n", "", "", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank{opts.rank}.csv'", "\n", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "id_", ",", "ans", "in", "results", ":", "\n", "            ", "f", ".", "write", "(", "f'{id_},{ans}\\n'", ")", "\n", "\n", "", "", "val_loss", "=", "sum", "(", "all_gather_list", "(", "val_loss", ")", ")", "\n", "val_mrr", "=", "sum", "(", "all_gather_list", "(", "val_mrr", ")", ")", "\n", "# tot_score = sum(all_gather_list(tot_score))", "\n", "n_ex", "=", "sum", "(", "all_gather_list", "(", "n_ex", ")", ")", "\n", "tot_time", "=", "time", "(", ")", "-", "st", "\n", "\n", "val_loss", "/=", "n_ex", "\n", "val_mrr", "=", "val_mrr", "/", "n_ex", "\n", "\n", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}.csv'", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "out_file", ")", ":", "\n", "        ", "with", "open", "(", "out_file", ",", "'wb'", ")", "as", "g", ":", "\n", "            ", "for", "f", "in", "glob", ".", "glob", "(", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank*.csv'", ")", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "open", "(", "f", ",", "'rb'", ")", ",", "g", ")", "\n", "\n", "", "", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "txt_db", "=", "os", ".", "path", ".", "join", "(", "'/txt'", ",", "\n", "intermediate_dir", "(", "opts", ".", "pretrained_model_name_or_path", ")", ",", "\n", "getattr", "(", "opts", ",", "f'{split}_txt_db'", ")", ")", "\n", "val_acc", "=", "judge", "(", "out_file", ",", "f'{txt_db}/answer.csv'", ")", "\n", "val_log", "=", "{", "f'{split}/loss'", ":", "val_loss", ",", "\n", "f'{split}/acc'", ":", "val_acc", ",", "\n", "f'{split}/mrr'", ":", "val_mrr", ",", "\n", "f'{split}/ex_per_s'", ":", "n_ex", "/", "tot_time", "}", "\n", "LOGGER", ".", "info", "(", "f\"validation finished in {int(tot_time)} seconds, \"", "\n", "f\"score: {val_acc * 100:.2f}, \"", "\n", "f\"mrr: {val_mrr:.3f}\"", ")", "\n", "return", "val_log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_composition.evaluation": [[286, 296], ["model.eval", "data_loaders.items", "chengyubert.utils.logger.TB_LOGGER.log_scaler_dict", "model.train", "chengyubert.utils.logger.LOGGER.info", "log.update", "train_composition.validate"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.log_scaler_dict", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.validate"], ["", "def", "evaluation", "(", "model", ",", "data_loaders", ":", "dict", ",", "opts", ",", "global_step", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "log", "=", "{", "}", "\n", "for", "split", ",", "loader", "in", "data_loaders", ".", "items", "(", ")", ":", "\n", "        ", "LOGGER", ".", "info", "(", "f\"Step {global_step}: start running \"", "\n", "f\"validation on {split} split...\"", ")", "\n", "log", ".", "update", "(", "validate", "(", "opts", ",", "model", ",", "loader", ",", "split", ",", "global_step", ")", ")", "\n", "", "TB_LOGGER", ".", "log_scaler_dict", "(", "log", ")", "\n", "model", ".", "train", "(", ")", "\n", "return", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_composition.get_best_ckpt": [[298, 313], ["re.compile", "glob.glob", "collections.Counter", "print", "collections.Counter.most_common", "chengyubert.data.evaluation.judge", "collections.Counter.update", "re.compile.match", "int", "os.path.join", "os.path.basename", "pat.match.group"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge"], ["", "def", "get_best_ckpt", "(", "val_data_dir", ",", "opts", ")", ":", "\n", "    ", "pat", "=", "re", ".", "compile", "(", "r'val_results_(?P<step>\\d+)_rank0.csv'", ")", "\n", "prediction_files", "=", "glob", ".", "glob", "(", "'{}/results/val_results_*_rank0.csv'", ".", "format", "(", "opts", ".", "output_dir", ")", ")", "\n", "\n", "top_files", "=", "Counter", "(", ")", "\n", "for", "f", "in", "prediction_files", ":", "\n", "        ", "acc", "=", "judge", "(", "f", ",", "os", ".", "path", ".", "join", "(", "val_data_dir", ",", "'answer.csv'", ")", ")", "\n", "top_files", ".", "update", "(", "{", "f", ":", "acc", "}", ")", "\n", "\n", "", "print", "(", "top_files", ")", "\n", "\n", "for", "f", ",", "acc", "in", "top_files", ".", "most_common", "(", "1", ")", ":", "\n", "        ", "m", "=", "pat", ".", "match", "(", "os", ".", "path", ".", "basename", "(", "f", ")", ")", "\n", "best_epoch", "=", "int", "(", "m", ".", "group", "(", "'step'", ")", ")", "\n", "return", "best_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_composition.main": [[315, 350], ["torch.device", "horovod.torch.device", "torch.cuda.set_device", "horovod.torch.cuda.set_device", "horovod.torch.rank", "horovod.torch.size", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.misc.set_random_seed", "chengyubert.data.create_dataloaders", "chengyubert.models.build_model", "chengyubert.models.build_model.to", "chengyubert.models.build_model.load_state_dict", "train_composition.evaluation", "horovod.torch.local_rank", "horovod.torch.local_rank", "ValueError", "train_composition.train", "train_composition.get_best_ckpt", "torch.load", "horovod.torch.load", "dict", "horovod.torch.rank", "filter", "dataloaders.items"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_random_seed", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_dataloaders", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.__init__.build_model", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.get_best_ckpt"], ["", "", "def", "main", "(", "opts", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "hvd", ".", "local_rank", "(", ")", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "hvd", ".", "local_rank", "(", ")", ")", "\n", "rank", "=", "hvd", ".", "rank", "(", ")", "\n", "opts", ".", "rank", "=", "rank", "\n", "opts", ".", "size", "=", "hvd", ".", "size", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"device: {} n_gpu: {}, rank: {}, \"", "\n", "\"16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "hvd", ".", "rank", "(", ")", ",", "opts", ".", "fp16", ")", ")", "\n", "\n", "if", "opts", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, \"", "\n", "\"should be >= 1\"", ".", "format", "(", "\n", "opts", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "set_random_seed", "(", "opts", ".", "seed", ")", "\n", "\n", "# data loaders", "\n", "DatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "dataset_cls", "]", "\n", "EvalDatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "eval_dataset_cls", "]", "\n", "splits", ",", "dataloaders", "=", "create_dataloaders", "(", "DatasetCls", ",", "EvalDatasetCls", ",", "opts", ")", "\n", "opts", ".", "evaluate_embedding", "=", "False", "\n", "\n", "# Prepare model", "\n", "model", "=", "build_model", "(", "opts", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "opts", ".", "mode", "==", "'train'", ":", "\n", "        ", "best_ckpt", "=", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", "\n", "", "else", ":", "\n", "        ", "best_ckpt", "=", "get_best_ckpt", "(", "dataloaders", "[", "'val'", "]", ".", "dataset", ".", "db_dir", ",", "opts", ")", "\n", "\n", "", "best_pt", "=", "f'{opts.output_dir}/ckpt/model_step_{best_ckpt}.pt'", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "best_pt", ")", ",", "strict", "=", "False", ")", "\n", "evaluation", "(", "model", ",", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", "!=", "'train'", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "opts", ",", "best_ckpt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_official.train": [[36, 170], ["chengyubert.utils.distributed.broadcast_tensors", "chengyubert.utils.misc.set_dropout", "chengyubert.optim.misc.build_optimizer", "torch.cuda.amp.GradScaler", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.RunningMeter", "model.train", "time.time", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.optim.misc.build_optimizer.step", "chengyubert.utils.save.save_training_meta", "tqdm.tqdm", "chengyubert.utils.save.ModelSaver", "os.makedirs", "chengyubert.utils.misc.NoOp", "chengyubert.utils.misc.NoOp", "len", "enumerate", "chengyubert.utils.logger.LOGGER.info", "os.path.join", "os.path.join", "targets.size", "torch.cuda.amp.GradScaler.scale().backward", "chengyubert.utils.logger.RunningMeter.", "model.parameters", "torch.cuda.amp.autocast", "model", "loss.mean.mean", "chengyubert.utils.distributed.all_reduce_and_rescale_tensors", "loss.mean.item", "chengyubert.optim.get_lr_sched", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.logger.RunningMeter", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.logger.TB_LOGGER.step", "torch.cuda.amp.GradScaler.step", "torch.cuda.amp.GradScaler.update", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.utils.misc.NoOp.update", "torch.cuda.amp.GradScaler.scale", "float", "torch.cuda.amp.GradScaler.unscale_", "torch.nn.utils.clip_grad_norm_", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "sum", "int", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "train_official.evaluation", "chengyubert.utils.misc.NoOp.save", "model.parameters", "sum", "len", "model.parameters", "chengyubert.utils.distributed.all_gather_list", "dict", "chengyubert.utils.misc.NoOp.set_description", "AssertionError", "filter", "time.time", "dataloaders.items", "x[].startswith"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.broadcast_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_dropout", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.misc.build_optimizer", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.save_training_meta", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.GatherLayer.backward", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_reduce_and_rescale_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.get_lr_sched", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list"], ["def", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", ":", "\n", "# make sure every process has same model parameters in the beginning", "\n", "    ", "broadcast_tensors", "(", "[", "p", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ",", "0", ")", "\n", "set_dropout", "(", "model", ",", "opts", ".", "dropout", ")", "\n", "\n", "# Prepare optimizer", "\n", "optimizer", "=", "build_optimizer", "(", "model", ",", "opts", ")", "\n", "scaler", "=", "GradScaler", "(", ")", "\n", "\n", "global_step", "=", "0", "\n", "if", "opts", ".", "rank", "==", "0", ":", "\n", "        ", "save_training_meta", "(", "opts", ")", "\n", "pbar", "=", "tqdm", "(", "total", "=", "opts", ".", "num_train_steps", ",", "desc", "=", "opts", ".", "model", ")", "\n", "model_saver", "=", "ModelSaver", "(", "join", "(", "opts", ".", "output_dir", ",", "'ckpt'", ")", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'results'", ")", ",", "exist_ok", "=", "True", ")", "# store val predictions", "\n", "", "else", ":", "\n", "        ", "LOGGER", ".", "disabled", "=", "True", "\n", "pbar", "=", "NoOp", "(", ")", "\n", "model_saver", "=", "NoOp", "(", ")", "\n", "\n", "", "LOGGER", ".", "info", "(", "f\"***** Running training with {opts.n_gpu} GPUs *****\"", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "dataloaders", "[", "'train'", "]", ".", "dataset", ")", ")", "\n", "LOGGER", ".", "info", "(", "\"  Batch size = %d\"", ",", "opts", ".", "train_batch_size", ")", "\n", "LOGGER", ".", "info", "(", "\"  Accumulate steps = %d\"", ",", "opts", ".", "gradient_accumulation_steps", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num steps = %d\"", ",", "opts", ".", "num_train_steps", ")", "\n", "\n", "running_loss", "=", "RunningMeter", "(", "'loss'", ")", "\n", "model", ".", "train", "(", ")", "\n", "n_examples", "=", "0", "\n", "n_epoch", "=", "0", "\n", "best_ckpt", "=", "0", "\n", "best_eval", "=", "0", "\n", "start", "=", "time", "(", ")", "\n", "\n", "# There are three choices for the loss", "\n", "# 1. enlarged-candidates", "\n", "# 2. original-candidates", "\n", "# 3. combined-candidates", "\n", "\n", "# quick hack for amp delay_unscale bug", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "while", "True", ":", "\n", "        ", "for", "step", ",", "batch", "in", "enumerate", "(", "dataloaders", "[", "'train'", "]", ")", ":", "\n", "            ", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'gather_index'", "]", "\n", "n_examples", "+=", "targets", ".", "size", "(", "0", ")", "\n", "\n", "with", "autocast", "(", ")", ":", "\n", "                ", "original_loss", ",", "enlarged_loss", "=", "model", "(", "**", "batch", ",", "compute_loss", "=", "True", ")", "\n", "if", "opts", ".", "candidates", "==", "'original'", ":", "\n", "                    ", "loss", "=", "original_loss", "\n", "", "elif", "opts", ".", "candidates", "==", "'enlarged'", ":", "\n", "                    ", "loss", "=", "enlarged_loss", "\n", "", "elif", "opts", ".", "candidates", "==", "'combined'", ":", "\n", "                    ", "loss", "=", "original_loss", "+", "enlarged_loss", "\n", "", "else", ":", "\n", "                    ", "raise", "AssertionError", "(", "\"No such loss!\"", ")", "\n", "\n", "", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "delay_unscale", "=", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "!=", "0", "\n", "scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", ")", "\n", "if", "not", "delay_unscale", ":", "\n", "# gather gradients from every processes", "\n", "# do this before unscaling to make sure every process uses", "\n", "# the same gradient scale", "\n", "                ", "grads", "=", "[", "p", ".", "grad", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "\n", "if", "p", ".", "requires_grad", "and", "p", ".", "grad", "is", "not", "None", "]", "\n", "all_reduce_and_rescale_tensors", "(", "grads", ",", "float", "(", "1", ")", ")", "\n", "\n", "", "running_loss", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "global_step", "+=", "1", "\n", "\n", "# learning rate scheduling", "\n", "lr_this_step", "=", "get_lr_sched", "(", "global_step", ",", "opts", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "TB_LOGGER", ".", "add_scalar", "(", "'lr'", ",", "lr_this_step", ",", "global_step", ")", "\n", "\n", "# log loss", "\n", "losses", "=", "all_gather_list", "(", "running_loss", ")", "\n", "running_loss", "=", "RunningMeter", "(", "\n", "'loss'", ",", "sum", "(", "l", ".", "val", "for", "l", "in", "losses", ")", "/", "len", "(", "losses", ")", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'loss'", ",", "running_loss", ".", "val", ",", "global_step", ")", "\n", "TB_LOGGER", ".", "step", "(", ")", "\n", "\n", "# update model params", "\n", "if", "opts", ".", "grad_norm", "!=", "-", "1", ":", "\n", "# Unscales the gradients of optimizer's assigned params in-place", "\n", "                    ", "scaler", ".", "unscale_", "(", "optimizer", ")", "\n", "grad_norm", "=", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "opts", ".", "grad_norm", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'grad_norm'", ",", "grad_norm", ",", "global_step", ")", "\n", "\n", "# scaler.step() first unscales gradients of the optimizer's params.", "\n", "# If gradients don't contain infs/NaNs, optimizer.step() is then called,", "\n", "# otherwise, optimizer.step() is skipped.", "\n", "", "scaler", ".", "step", "(", "optimizer", ")", "\n", "\n", "# Updates the scale for next iteration.", "\n", "scaler", ".", "update", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "if", "global_step", "%", "100", "==", "0", ":", "\n", "# monitor training throughput", "\n", "                    ", "tot_ex", "=", "sum", "(", "all_gather_list", "(", "n_examples", ")", ")", "\n", "ex_per_sec", "=", "int", "(", "tot_ex", "/", "(", "time", "(", ")", "-", "start", ")", ")", "\n", "LOGGER", ".", "info", "(", "f'{opts.model}-{opts.candidates}: {n_epoch}-{global_step}: '", "\n", "f'{tot_ex} examples trained at '", "\n", "f'{ex_per_sec} ex/s '", "\n", "f'best_acc@{best_ckpt}-{best_eval * 100:.2f}'", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'perf/ex_per_s'", ",", "\n", "ex_per_sec", ",", "global_step", ")", "\n", "\n", "", "if", "global_step", "%", "opts", ".", "valid_steps", "==", "0", ":", "\n", "                    ", "log", "=", "evaluation", "(", "model", ",", "\n", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", ".", "startswith", "(", "'val'", ")", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "\n", "opts", ",", "global_step", ")", "\n", "if", "log", "[", "'val/acc'", "]", ">", "best_eval", ":", "\n", "                        ", "best_ckpt", "=", "global_step", "\n", "best_eval", "=", "log", "[", "'val/acc'", "]", "\n", "pbar", ".", "set_description", "(", "\n", "f'{opts.model}-{opts.candidates}: {n_epoch}-{best_ckpt} best_acc-{best_eval * 100:.2f}'", ")", "\n", "", "model_saver", ".", "save", "(", "model", ",", "global_step", ")", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "                ", "break", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "            ", "break", "\n", "", "n_epoch", "+=", "1", "\n", "LOGGER", ".", "info", "(", "f\"Step {global_step}: finished {n_epoch} epochs\"", ")", "\n", "", "return", "best_ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_official.validate": [[172, 248], ["torch.no_grad", "horovod.torch.no_grad", "time.time", "sum", "sum", "sum", "sum", "os.path.join", "chengyubert.data.evaluation.judge", "chengyubert.utils.logger.LOGGER.info", "tqdm.tqdm", "enumerate", "open", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.distributed.all_gather_list", "time.time", "os.path.isfile", "chengyubert.utils.distributed.all_gather_list", "chengyubert.data.intermediate_dir", "getattr", "range", "model", "torch.nn.functional.cross_entropy", "F.cross_entropy.item", "logits.max", "torch.eq().sum().item", "horovod.torch.eq().sum().item", "max_idx.cpu().tolist", "torch.gather().cpu().numpy", "horovod.torch.gather().cpu().numpy", "enumerate", "results.extend", "len", "tq.update", "f.write", "open", "glob.glob", "zip", "over_logits[].cpu().numpy", "numpy.argsort", "zip", "len", "shutil.copyfileobj", "int", "len", "torch.eq().sum", "horovod.torch.eq().sum", "max_idx.cpu", "torch.gather().cpu", "horovod.torch.gather().cpu", "open", "AssertionError", "over_logits[].cpu", "numpy.argwhere().item", "torch.eq", "horovod.torch.eq", "torch.gather", "horovod.torch.gather", "numpy.argwhere", "torch.gather().cpu().numpy.unsqueeze"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.intermediate_dir"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "validate", "(", "opts", ",", "model", ",", "val_loader", ",", "split", ",", "global_step", ")", ":", "\n", "    ", "val_loss", "=", "0", "\n", "tot_score", "=", "0", "\n", "n_ex", "=", "0", "\n", "val_mrr", "=", "0", "\n", "st", "=", "time", "(", ")", "\n", "results", "=", "[", "]", "\n", "with", "tqdm", "(", "range", "(", "len", "(", "val_loader", ".", "dataset", ")", "//", "opts", ".", "size", ")", ",", "desc", "=", "f'{split}-{opts.rank}'", ")", "as", "tq", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "            ", "qids", "=", "batch", "[", "'qids'", "]", "\n", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'gather_index'", "]", "\n", "del", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'qids'", "]", "\n", "\n", "logits", ",", "over_logits", ",", "cond_logits", "=", "model", "(", "**", "batch", ",", "targets", "=", "None", ",", "compute_loss", "=", "False", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "targets", ",", "reduction", "=", "'sum'", ")", "\n", "val_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "if", "opts", ".", "candidates", "==", "'original'", ":", "\n", "                ", "logits", "=", "logits", "\n", "", "elif", "opts", ".", "candidates", "==", "'enlarged'", ":", "\n", "                ", "logits", "=", "cond_logits", "\n", "", "elif", "opts", ".", "candidates", "==", "'combined'", ":", "\n", "                ", "logits", "=", "logits", "+", "cond_logits", "\n", "", "else", ":", "\n", "                ", "raise", "AssertionError", "(", "\"No such loss!\"", ")", "\n", "", "max_prob", ",", "max_idx", "=", "logits", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "\n", "tot_score", "+=", "torch", ".", "eq", "(", "max_idx", ",", "targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "answers", "=", "max_idx", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "targets", "=", "torch", ".", "gather", "(", "batch", "[", "'option_ids'", "]", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "j", ",", "(", "qid", ",", "target", ")", "in", "enumerate", "(", "zip", "(", "qids", ",", "targets", ")", ")", ":", "\n", "                ", "g", "=", "over_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "top_k", "=", "np", ".", "argsort", "(", "-", "g", ")", "\n", "val_mrr", "+=", "1", "/", "(", "1", "+", "np", ".", "argwhere", "(", "top_k", "==", "target", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "results", ".", "extend", "(", "zip", "(", "qids", ",", "answers", ")", ")", "\n", "n_ex", "+=", "len", "(", "qids", ")", "\n", "tq", ".", "update", "(", "len", "(", "qids", ")", ")", "\n", "\n", "", "", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank{opts.rank}.csv'", "\n", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "id_", ",", "ans", "in", "results", ":", "\n", "            ", "f", ".", "write", "(", "f'{id_},{ans}\\n'", ")", "\n", "\n", "", "", "val_loss", "=", "sum", "(", "all_gather_list", "(", "val_loss", ")", ")", "\n", "val_mrr", "=", "sum", "(", "all_gather_list", "(", "val_mrr", ")", ")", "\n", "# tot_score = sum(all_gather_list(tot_score))", "\n", "n_ex", "=", "sum", "(", "all_gather_list", "(", "n_ex", ")", ")", "\n", "tot_time", "=", "time", "(", ")", "-", "st", "\n", "\n", "val_loss", "/=", "n_ex", "\n", "val_mrr", "=", "val_mrr", "/", "n_ex", "\n", "\n", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}.csv'", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "out_file", ")", ":", "\n", "        ", "with", "open", "(", "out_file", ",", "'wb'", ")", "as", "g", ":", "\n", "            ", "for", "f", "in", "glob", ".", "glob", "(", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank*.csv'", ")", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "open", "(", "f", ",", "'rb'", ")", ",", "g", ")", "\n", "\n", "", "", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "txt_db", "=", "os", ".", "path", ".", "join", "(", "'/txt'", ",", "\n", "intermediate_dir", "(", "opts", ".", "pretrained_model_name_or_path", ")", ",", "\n", "getattr", "(", "opts", ",", "f'{split}_txt_db'", ")", ")", "\n", "val_acc", "=", "judge", "(", "out_file", ",", "f'{txt_db}/answer.csv'", ")", "\n", "val_log", "=", "{", "f'{split}/loss'", ":", "val_loss", ",", "\n", "f'{split}/acc'", ":", "val_acc", ",", "\n", "f'{split}/mrr'", ":", "val_mrr", ",", "\n", "f'{split}/ex_per_s'", ":", "n_ex", "/", "tot_time", "}", "\n", "LOGGER", ".", "info", "(", "f\"validation finished in {int(tot_time)} seconds, \"", "\n", "f\"score: {val_acc * 100:.2f}, \"", "\n", "f\"mrr: {val_mrr:.3f}\"", ")", "\n", "return", "val_log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_official.evaluation": [[250, 260], ["model.eval", "data_loaders.items", "chengyubert.utils.logger.TB_LOGGER.log_scaler_dict", "model.train", "chengyubert.utils.logger.LOGGER.info", "log.update", "train_official.validate"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.log_scaler_dict", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.validate"], ["", "def", "evaluation", "(", "model", ",", "data_loaders", ":", "dict", ",", "opts", ",", "global_step", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "log", "=", "{", "}", "\n", "for", "split", ",", "loader", "in", "data_loaders", ".", "items", "(", ")", ":", "\n", "        ", "LOGGER", ".", "info", "(", "f\"Step {global_step}: start running \"", "\n", "f\"validation on {split} split...\"", ")", "\n", "log", ".", "update", "(", "validate", "(", "opts", ",", "model", ",", "loader", ",", "split", ",", "global_step", ")", ")", "\n", "", "TB_LOGGER", ".", "log_scaler_dict", "(", "log", ")", "\n", "model", ".", "train", "(", ")", "\n", "return", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_official.get_best_ckpt": [[262, 279], ["re.compile", "glob.glob", "collections.Counter", "print", "collections.Counter.most_common", "re.compile.match", "re.compile.match", "int", "os.path.basename", "chengyubert.data.evaluation.judge", "collections.Counter.update", "os.path.basename", "pat.match.group", "os.path.join"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge"], ["", "def", "get_best_ckpt", "(", "val_data_dir", ",", "opts", ")", ":", "\n", "    ", "pat", "=", "re", ".", "compile", "(", "r'val_results_(?P<step>\\d+).csv'", ")", "\n", "prediction_files", "=", "glob", ".", "glob", "(", "'{}/results/val_results_*.csv'", ".", "format", "(", "opts", ".", "output_dir", ")", ")", "\n", "\n", "top_files", "=", "Counter", "(", ")", "\n", "for", "f", "in", "prediction_files", ":", "\n", "        ", "m", "=", "pat", ".", "match", "(", "os", ".", "path", ".", "basename", "(", "f", ")", ")", "\n", "if", "m", ":", "\n", "            ", "acc", "=", "judge", "(", "f", ",", "os", ".", "path", ".", "join", "(", "val_data_dir", ",", "'answer.csv'", ")", ")", "\n", "top_files", ".", "update", "(", "{", "f", ":", "acc", "}", ")", "\n", "\n", "", "", "print", "(", "top_files", ")", "\n", "\n", "for", "f", ",", "acc", "in", "top_files", ".", "most_common", "(", "1", ")", ":", "\n", "        ", "m", "=", "pat", ".", "match", "(", "os", ".", "path", ".", "basename", "(", "f", ")", ")", "\n", "best_epoch", "=", "int", "(", "m", ".", "group", "(", "'step'", ")", ")", "\n", "return", "best_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_official.main": [[281, 333], ["torch.device", "horovod.torch.device", "torch.cuda.set_device", "horovod.torch.cuda.set_device", "horovod.torch.rank", "horovod.torch.size", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.misc.set_random_seed", "chengyubert.data.create_dataloaders", "chengyubert.models.build_model", "chengyubert.models.build_model.to", "sum", "sum", "train_official.evaluation", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "horovod.torch.local_rank", "horovod.torch.local_rank", "ValueError", "chengyubert.utils.logger.TB_LOGGER.create", "chengyubert.utils.logger.add_log_to_file", "train_official.train", "chengyubert.utils.distributed.all_gather_list", "chengyubert.models.build_model.load_state_dict", "chengyubert.utils.distributed.all_gather_list", "dict", "horovod.torch.rank", "os.path.join", "os.path.join", "train_official.get_best_ckpt", "torch.load", "horovod.torch.load", "filter", "itertools.chain", "os.makedirs", "dataloaders.items", "os.path.join", "format", "format"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_random_seed", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_dataloaders", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.__init__.build_model", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.create", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.add_log_to_file", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.get_best_ckpt"], ["", "", "def", "main", "(", "opts", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "hvd", ".", "local_rank", "(", ")", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "hvd", ".", "local_rank", "(", ")", ")", "\n", "rank", "=", "hvd", ".", "rank", "(", ")", "\n", "opts", ".", "rank", "=", "rank", "\n", "opts", ".", "size", "=", "hvd", ".", "size", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"device: {} n_gpu: {}, rank: {}, \"", "\n", "\"16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "hvd", ".", "rank", "(", ")", ",", "opts", ".", "fp16", ")", ")", "\n", "\n", "if", "opts", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, \"", "\n", "\"should be >= 1\"", ".", "format", "(", "\n", "opts", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "set_random_seed", "(", "opts", ".", "seed", ")", "\n", "if", "opts", ".", "rank", "==", "0", ":", "\n", "        ", "TB_LOGGER", ".", "create", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ")", ")", "\n", "add_log_to_file", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ",", "f'{opts.mode}.log'", ")", ")", "\n", "\n", "# data loaders", "\n", "", "DatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "dataset_cls", "]", "\n", "EvalDatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "eval_dataset_cls", "]", "\n", "splits", ",", "dataloaders", "=", "create_dataloaders", "(", "DatasetCls", ",", "EvalDatasetCls", ",", "opts", ")", "\n", "\n", "# Prepare model", "\n", "model", "=", "build_model", "(", "opts", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "opts", ".", "mode", "==", "'train'", ":", "\n", "        ", "best_ckpt", "=", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", "\n", "", "elif", "opts", ".", "mode", "==", "'eval'", ":", "\n", "        ", "best_ckpt", "=", "None", "\n", "if", "opts", ".", "rank", "==", "0", ":", "\n", "            ", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'results'", ")", ",", "exist_ok", "=", "True", ")", "# store val predictions", "\n", "", "", "else", ":", "\n", "        ", "best_ckpt", "=", "get_best_ckpt", "(", "dataloaders", "[", "'val'", "]", ".", "dataset", ".", "db_dir", ",", "opts", ")", "\n", "\n", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "if", "best_ckpt", "is", "not", "None", ":", "\n", "        ", "best_pt", "=", "f'{opts.output_dir}/ckpt/model_step_{best_ckpt}.pt'", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "best_pt", ")", ",", "strict", "=", "False", ")", "\n", "\n", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "log", "=", "evaluation", "(", "model", ",", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", "!=", "'train'", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "opts", ",", "best_ckpt", ")", "\n", "splits", "=", "[", "'val'", ",", "'test'", ",", "'ran'", ",", "'sim'", ",", "'out'", "]", "\n", "LOGGER", ".", "info", "(", "'\\t'", ".", "join", "(", "splits", ")", ")", "\n", "LOGGER", ".", "info", "(", "'\\t'", ".", "join", "(", "chain", "(", "\n", "[", "format", "(", "log", "[", "f'{split}/acc'", "]", ",", "\"0.6f\"", ")", "for", "split", "in", "splits", "]", ",", "\n", "[", "format", "(", "log", "[", "f'{split}/mrr'", "]", ",", "\"0.6f\"", ")", "for", "split", "in", "splits", "]", "\n", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.Example.__init__": [[19, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "idx", ",", "\n", "tag", ",", "\n", "context", ",", "\n", "idiom", ",", "\n", "options", ",", "\n", "label", "=", "None", ")", ":", "\n", "        ", "self", ".", "idx", "=", "idx", "\n", "self", ".", "tag", "=", "tag", "\n", "self", ".", "context", "=", "context", "\n", "self", ".", "idiom", "=", "idiom", "\n", "self", ".", "options", "=", "options", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.Example.__str__": [[33, 35], ["preprocess.Example.__repr__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.__repr__"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.Example.__repr__": [[36, 45], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "\"\"", "\n", "s", "+=", "\"idx: %s\"", "%", "(", "self", ".", "idx", ")", "\n", "s", "+=", "\"tag: %s\"", "%", "(", "self", ".", "tag", ")", "\n", "s", "+=", "\", context: %s\"", "%", "(", "self", ".", "context", ")", "\n", "s", "+=", "\", options: [%s]\"", "%", "(", "\", \"", ".", "join", "(", "self", ".", "options", ")", ")", "\n", "if", "self", ".", "label", "is", "not", "None", "and", "self", ".", "options", ":", "\n", "            ", "s", "+=", "\", answer: %s\"", "%", "self", ".", "options", "[", "self", ".", "label", "]", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidParser.__init__": [[98, 103], ["chengyubert.data.chengyu_process"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.chengyu_process"], ["def", "__init__", "(", "self", ",", "split", ",", "len_idiom_vocab", ",", "annotation_dir", "=", "'/annotations'", ")", ":", "\n", "        ", "self", ".", "split", "=", "split", "\n", "self", ".", "vocab", "=", "chengyu_process", "(", "len_idiom_vocab", "=", "len_idiom_vocab", ",", "annotation_dir", "=", "annotation_dir", ")", "\n", "self", ".", "annotation_dir", "=", "annotation_dir", "\n", "self", ".", "reverse_index", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidParser.data_dir": [[104, 108], ["None"], "methods", ["None"], ["", "@", "property", "\n", "@", "abstractmethod", "\n", "def", "data_dir", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidParser.data_file": [[109, 113], ["None"], "methods", ["None"], ["", "@", "property", "\n", "@", "abstractmethod", "\n", "def", "data_file", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidParser.answer_file": [[114, 117], ["os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "answer_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'{}_answer.csv'", ".", "format", "(", "self", ".", "split", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidParser.get_idiom_id": [[118, 120], ["None"], "methods", ["None"], ["", "def", "get_idiom_id", "(", "self", ",", "idiom", ")", ":", "\n", "        ", "return", "self", ".", "vocab", "[", "idiom", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidParser.read_examples": [[121, 156], ["tqdm.tqdm.tqdm", "open", "enumerate", "os.path.getsize", "pbar.update", "eval", "enumerate", "len", "data_str.decode", "zip", "tmp_context.replace.replace.replace", "preprocess.ChidParser.get_idiom_id", "preprocess.ChidParser.reverse_index.setdefault", "preprocess.ChidParser.reverse_index[].append", "re.finditer", "options.index", "preprocess.Example", "len", "print", "len", "tag.start", "tag.end"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.get_idiom_id"], ["", "def", "read_examples", "(", "self", ")", ":", "\n", "        ", "with", "tqdm", "(", "total", "=", "os", ".", "path", ".", "getsize", "(", "self", ".", "data_file", ")", ",", "desc", "=", "self", ".", "split", ",", "\n", "bar_format", "=", "\"{desc}: {percentage:.3f}%|{bar}| {n:.2f}/{total_fmt} [{elapsed}<{remaining}]\"", ")", "as", "pbar", ":", "\n", "            ", "with", "open", "(", "self", ".", "data_file", ",", "mode", "=", "'rb'", ")", "as", "f", ":", "\n", "                ", "for", "idx", ",", "data_str", "in", "enumerate", "(", "f", ")", ":", "\n", "                    ", "pbar", ".", "update", "(", "len", "(", "data_str", ")", ")", "\n", "data", "=", "eval", "(", "data_str", ".", "decode", "(", "'utf8'", ")", ")", "\n", "context", "=", "data", "[", "'content'", "]", "\n", "for", "i", ",", "(", "tag", ",", "idiom", ")", "in", "enumerate", "(", "zip", "(", "re", ".", "finditer", "(", "\"#idiom#\"", ",", "context", ")", ",", "data", "[", "'groundTruth'", "]", ")", ")", ":", "\n", "                        ", "new_tag", "=", "idx", "*", "20", "+", "i", "\n", "tag_str", "=", "\"#idiom%06d#\"", "%", "new_tag", "\n", "\n", "tmp_context", "=", "context", "\n", "tmp_context", "=", "\"\"", ".", "join", "(", "(", "tmp_context", "[", ":", "tag", ".", "start", "(", "0", ")", "]", ",", "tag_str", ",", "tmp_context", "[", "tag", ".", "end", "(", "0", ")", ":", "]", ")", ")", "\n", "tmp_context", "=", "tmp_context", ".", "replace", "(", "\"#idiom#\"", ",", "\"[UNK]\"", ")", "\n", "\n", "if", "'candidates'", "not", "in", "data", ":", "\n", "                            ", "options", ",", "label", "=", "[", "]", ",", "-", "1", "\n", "", "else", ":", "\n", "                            ", "options", "=", "data", "[", "'candidates'", "]", "[", "i", "]", "\n", "if", "len", "(", "options", ")", "!=", "7", ":", "\n", "                                ", "print", "(", "data", ")", "\n", "assert", "len", "(", "options", ")", "==", "7", "\n", "", "label", "=", "options", ".", "index", "(", "idiom", ")", "\n", "\n", "", "idiom_id", "=", "self", ".", "get_idiom_id", "(", "idiom", ")", "\n", "self", ".", "reverse_index", ".", "setdefault", "(", "idiom_id", ",", "[", "]", ")", "\n", "self", ".", "reverse_index", "[", "idiom_id", "]", ".", "append", "(", "tag_str", ")", "\n", "yield", "Example", "(", "\n", "idx", "=", "new_tag", ",", "\n", "tag", "=", "tag_str", ",", "\n", "context", "=", "tmp_context", ",", "\n", "idiom", "=", "idiom", ",", "\n", "options", "=", "options", ",", "\n", "label", "=", "label", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidBalancedParser.__init__": [[169, 173], ["preprocess.ChidParser.__init__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "len_idiom_vocab", ",", "annotation_dir", "=", "'/annotations'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "len_idiom_vocab", ",", "annotation_dir", ")", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "annotation_dir", "=", "annotation_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidBalancedParser.data_dir": [[174, 177], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_dir", "(", "self", ")", ":", "\n", "        ", "return", "f'{self.annotation_dir}/balanced'", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidBalancedParser.data_file": [[178, 181], ["os.path.join", "len"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'balanced{}_{}.txt'", ".", "format", "(", "len", "(", "self", ".", "vocab", ")", ",", "self", ".", "split", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidBalancedFixParser.__init__": [[201, 210], ["preprocess.ChidParser.__init__", "vocab.items", "preprocess.ChidBalancedFixParser.fix_dict.get"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "len_idiom_vocab", ",", "annotation_dir", "=", "'/annotations'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "len_idiom_vocab", ",", "annotation_dir", ")", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "vocab", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "vocab", ".", "items", "(", ")", ":", "\n", "            ", "k", "=", "self", ".", "fix_dict", ".", "get", "(", "k", ",", "k", ")", "\n", "self", ".", "vocab", "[", "k", "]", "=", "v", "\n", "\n", "", "self", ".", "annotation_dir", "=", "annotation_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidBalancedFixParser.data_dir": [[211, 214], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_dir", "(", "self", ")", ":", "\n", "        ", "return", "f'{self.annotation_dir}/balanced'", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidBalancedFixParser.data_file": [[215, 218], ["os.path.join", "len"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'balanced{}fix_{}.txt'", ".", "format", "(", "len", "(", "self", ".", "vocab", ")", ",", "self", ".", "split", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidExternalParser.__init__": [[230, 234], ["preprocess.ChidParser.__init__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "len_idiom_vocab", ",", "annotation_dir", "=", "'/annotations'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "len_idiom_vocab", ",", "annotation_dir", ")", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "annotation_dir", "=", "annotation_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidExternalParser.data_dir": [[235, 238], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_dir", "(", "self", ")", ":", "\n", "        ", "return", "f'{self.annotation_dir}/external'", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidExternalParser.data_file": [[239, 245], ["os.path.join", "os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_file", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "split", "in", "[", "'pretrain'", "]", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'{}_data.txt'", ".", "format", "(", "self", ".", "split", ")", ")", "\n", "", "elif", "self", ".", "split", "in", "[", "'cct7'", ",", "'cct4'", "]", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'chengyu.txt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidOfficialParser.__init__": [[257, 261], ["preprocess.ChidParser.__init__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "len_idiom_vocab", ",", "annotation_dir", "=", "'/annotations'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "len_idiom_vocab", ",", "annotation_dir", ")", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "annotation_dir", "=", "annotation_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidOfficialParser.data_dir": [[262, 265], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_dir", "(", "self", ")", ":", "\n", "        ", "return", "f'/{self.annotation_dir}/official'", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidOfficialParser.data_file": [[266, 276], ["os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_file", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'{}_data.txt'", ".", "format", "(", "self", ".", "split", ")", ")", "\n", "", "elif", "self", ".", "split", "==", "'out'", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'test_out_data.txt'", ")", "\n", "", "elif", "self", ".", "split", "==", "'sim'", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'test_data_sim.txt'", ")", "\n", "", "elif", "self", ".", "split", "==", "'ran'", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'test_data_ord.txt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidCompetitionParser.__init__": [[288, 294], ["chengyubert.data.chengyu_process"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.chengyu_process"], ["def", "__init__", "(", "self", ",", "split", ",", "len_idiom_vocab", ",", "annotation_dir", "=", "'/annotations'", ")", ":", "\n", "        ", "self", ".", "split", "=", "split", "\n", "self", ".", "vocab", "=", "chengyu_process", "(", "len_idiom_vocab", "=", "len_idiom_vocab", ",", "annotation_dir", "=", "annotation_dir", ")", "\n", "self", ".", "annotation_dir", "=", "annotation_dir", "\n", "self", ".", "data_dir", "=", "f'{self.annotation_dir}/competition'", "\n", "self", ".", "reverse_index", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidCompetitionParser.answer_file": [[295, 298], ["os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "answer_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'{}_answer.csv'", ".", "format", "(", "self", ".", "split", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidCompetitionParser.data_file": [[299, 302], ["os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'{}.txt'", ".", "format", "(", "self", ".", "split", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidCompetitionParser.get_idiom_id": [[303, 305], ["None"], "methods", ["None"], ["", "def", "get_idiom_id", "(", "self", ",", "idiom", ")", ":", "\n", "        ", "return", "self", ".", "vocab", "[", "idiom", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidCompetitionParser.read_examples": [[306, 337], ["open", "tqdm.tqdm.tqdm", "ll.strip().split", "int", "open", "enumerate", "os.path.getsize", "pbar.update", "eval", "ll.strip", "len", "data_str.decode", "re.findall", "preprocess.ChidCompetitionParser.get_idiom_id", "preprocess.ChidCompetitionParser.reverse_index.setdefault", "preprocess.ChidCompetitionParser.reverse_index[].append", "preprocess.Example", "tmp_context.replace.replace.replace"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.get_idiom_id"], ["", "def", "read_examples", "(", "self", ")", ":", "\n", "        ", "ans_dict", "=", "{", "}", "\n", "with", "open", "(", "self", ".", "answer_file", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "ll", "in", "f", ":", "\n", "                ", "k", ",", "v", "=", "ll", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "\n", "ans_dict", "[", "k", "]", "=", "int", "(", "v", ")", "\n", "\n", "", "", "with", "tqdm", "(", "total", "=", "os", ".", "path", ".", "getsize", "(", "self", ".", "data_file", ")", ",", "desc", "=", "self", ".", "split", ",", "\n", "bar_format", "=", "\"{desc}: {percentage:.3f}%|{bar}| {n:.2f}/{total_fmt} [{elapsed}<{remaining}]\"", ")", "as", "pbar", ":", "\n", "            ", "with", "open", "(", "self", ".", "data_file", ",", "mode", "=", "'rb'", ")", "as", "f", ":", "\n", "                ", "for", "idx", ",", "data_str", "in", "enumerate", "(", "f", ")", ":", "\n", "                    ", "pbar", ".", "update", "(", "len", "(", "data_str", ")", ")", "\n", "data", "=", "eval", "(", "data_str", ".", "decode", "(", "'utf8'", ")", ")", "\n", "options", "=", "data", "[", "'candidates'", "]", "\n", "for", "context", "in", "data", "[", "'content'", "]", ":", "\n", "                        ", "tags", "=", "re", ".", "findall", "(", "\"#idiom\\d+#\"", ",", "context", ")", "\n", "for", "tag", "in", "tags", ":", "\n", "                            ", "tmp_context", "=", "context", "\n", "for", "other_tag", "in", "tags", ":", "\n", "                                ", "if", "other_tag", "!=", "tag", ":", "\n", "                                    ", "tmp_context", "=", "tmp_context", ".", "replace", "(", "other_tag", ",", "\"[UNK]\"", ")", "\n", "", "", "idiom_id", "=", "self", ".", "get_idiom_id", "(", "options", "[", "ans_dict", "[", "tag", "]", "]", ")", "\n", "self", ".", "reverse_index", ".", "setdefault", "(", "idiom_id", ",", "[", "]", ")", "\n", "self", ".", "reverse_index", "[", "idiom_id", "]", ".", "append", "(", "tag", ")", "\n", "yield", "Example", "(", "\n", "idx", "=", "idx", ",", "\n", "tag", "=", "tag", ",", "\n", "context", "=", "tmp_context", ",", "\n", "idiom", "=", "options", "[", "ans_dict", "[", "tag", "]", "]", ",", "\n", "options", "=", "options", ",", "\n", "label", "=", "ans_dict", "[", "tag", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidAffectionParser.__init__": [[350, 360], ["preprocess.ChidParser.__init__", "open", "json.load", "open", "json.load"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "split", ",", "len_idiom_vocab", ",", "annotation_dir", "=", "'/annotations'", ",", "limit", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "len_idiom_vocab", ",", "annotation_dir", ")", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "annotation_dir", "=", "annotation_dir", "\n", "with", "open", "(", "self", ".", "data_file", ")", "as", "fd", ":", "\n", "            ", "self", ".", "filtered", "=", "json", ".", "load", "(", "fd", ")", "\n", "", "with", "open", "(", "self", ".", "unlabelled_file", ")", "as", "fd", ":", "\n", "            ", "self", ".", "unlabelled", "=", "json", ".", "load", "(", "fd", ")", "\n", "", "self", ".", "limit", "=", "limit", "\n", "self", ".", "reverse_index", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidAffectionParser.data_dir": [[361, 364], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_dir", "(", "self", ")", ":", "\n", "        ", "return", "f'/{self.annotation_dir}/affection'", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidAffectionParser.data_file": [[365, 368], ["os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'{}.json'", ".", "format", "(", "self", ".", "split", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidAffectionParser.unlabelled_file": [[369, 372], ["os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "unlabelled_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'unlabelled.json'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidAffectionParser._read_data_file": [[373, 380], ["tqdm.tqdm.tqdm", "open", "enumerate", "os.path.getsize", "pbar.update", "len"], "methods", ["None"], ["", "def", "_read_data_file", "(", "self", ",", "data_file", ")", ":", "\n", "        ", "with", "tqdm", "(", "total", "=", "os", ".", "path", ".", "getsize", "(", "data_file", ")", ",", "desc", "=", "self", ".", "split", ",", "\n", "bar_format", "=", "\"{desc}: {percentage:.3f}%|{bar}| {n:.2f}/{total_fmt} [{elapsed}<{remaining}]\"", ")", "as", "pbar", ":", "\n", "            ", "with", "open", "(", "data_file", ",", "mode", "=", "'rb'", ")", "as", "f", ":", "\n", "                ", "for", "idx", ",", "data_str", "in", "enumerate", "(", "f", ")", ":", "\n", "                    ", "pbar", ".", "update", "(", "len", "(", "data_str", ")", ")", "\n", "yield", "data_str", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidAffectionParser._construct_example": [[381, 406], ["tmp_context.replace.replace.replace", "preprocess.ChidAffectionParser.get_idiom_id", "preprocess.ChidAffectionParser.reverse_index.setdefault", "preprocess.ChidAffectionParser.reverse_index[].append", "preprocess.Example", "options.index", "len", "print", "len", "tag.start", "tag.end"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.get_idiom_id"], ["", "", "", "", "def", "_construct_example", "(", "self", ",", "i", ",", "idiom", ",", "tag", ",", "new_tag", ",", "context", ",", "data", ")", ":", "\n", "        ", "tag_str", "=", "\"#idiom%06d#\"", "%", "new_tag", "\n", "\n", "tmp_context", "=", "context", "\n", "tmp_context", "=", "\"\"", ".", "join", "(", "(", "tmp_context", "[", ":", "tag", ".", "start", "(", "0", ")", "]", ",", "tag_str", ",", "tmp_context", "[", "tag", ".", "end", "(", "0", ")", ":", "]", ")", ")", "\n", "tmp_context", "=", "tmp_context", ".", "replace", "(", "\"#idiom#\"", ",", "\"[UNK]\"", ")", "\n", "\n", "if", "'candidates'", "not", "in", "data", ":", "\n", "            ", "options", ",", "label", "=", "[", "]", ",", "-", "1", "\n", "", "else", ":", "\n", "            ", "options", "=", "data", "[", "'candidates'", "]", "[", "i", "]", "\n", "if", "len", "(", "options", ")", "!=", "7", ":", "\n", "                ", "print", "(", "data", ")", "\n", "assert", "len", "(", "options", ")", "==", "7", "\n", "", "label", "=", "options", ".", "index", "(", "idiom", ")", "\n", "", "idiom_id", "=", "self", ".", "get_idiom_id", "(", "idiom", ")", "\n", "self", ".", "reverse_index", ".", "setdefault", "(", "idiom_id", ",", "[", "]", ")", "\n", "self", ".", "reverse_index", "[", "idiom_id", "]", ".", "append", "(", "tag_str", ")", "\n", "return", "Example", "(", "\n", "idx", "=", "new_tag", ",", "\n", "tag", "=", "tag_str", ",", "\n", "context", "=", "tmp_context", ",", "\n", "idiom", "=", "idiom", ",", "\n", "options", "=", "options", ",", "\n", "label", "=", "label", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidAffectionParser.read_examples": [[408, 424], ["enumerate", "itertools.chain", "eval", "enumerate", "preprocess.ChidAffectionParser._read_data_file", "preprocess.ChidAffectionParser._read_data_file", "data_str.decode", "zip", "os.path.join", "os.path.join", "re.finditer", "preprocess.ChidAffectionParser._construct_example", "preprocess.ChidAffectionParser._construct_example"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidAffectionParser._read_data_file", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.ChidAffectionParser._read_data_file", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser._construct_example", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser._construct_example"], ["", "def", "read_examples", "(", "self", ")", ":", "\n", "        ", "for", "idx", ",", "data_str", "in", "enumerate", "(", "chain", "(", "self", ".", "_read_data_file", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'data.jsonl'", ")", ")", ",", "\n", "self", ".", "_read_data_file", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'extra.jsonl'", ")", ")", ")", ")", ":", "\n", "            ", "data", "=", "eval", "(", "data_str", ".", "decode", "(", "'utf8'", ")", ")", "\n", "context", "=", "data", "[", "'content'", "]", "\n", "for", "i", ",", "(", "tag", ",", "idiom", ")", "in", "enumerate", "(", "zip", "(", "re", ".", "finditer", "(", "\"#idiom#\"", ",", "context", ")", ",", "data", "[", "'groundTruth'", "]", ")", ")", ":", "\n", "                ", "new_tag", "=", "idx", "*", "20", "+", "i", "\n", "if", "idiom", "not", "in", "self", ".", "filtered", "and", "self", ".", "split", "!=", "'train'", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "self", ".", "split", "==", "'train'", ":", "\n", "                    ", "if", "idiom", "in", "self", ".", "filtered", "or", "idiom", "in", "self", ".", "unlabelled", ":", "\n", "                        ", "yield", "self", ".", "_construct_example", "(", "i", ",", "idiom", ",", "tag", ",", "new_tag", ",", "context", ",", "data", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "idiom", "in", "self", ".", "filtered", ":", "\n", "                        ", "yield", "self", ".", "_construct_example", "(", "i", ",", "idiom", ",", "tag", ",", "new_tag", ",", "context", ",", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.__init__": [[435, 446], ["chengyubert.data.idioms_process", "open", "json.load", "open", "json.load"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.idioms_process"], ["def", "__init__", "(", "self", ",", "split", ",", "len_idiom_vocab", ",", "annotation_dir", "=", "'/annotations'", ",", "limit", "=", "None", ")", ":", "\n", "        ", "self", ".", "split", "=", "split", "\n", "self", ".", "vocab", ",", "_", ",", "self", ".", "mapping", "=", "idioms_process", "(", "len_idiom_vocab", "=", "len_idiom_vocab", ",", "\n", "annotation_dir", "=", "'/annotations'", ")", "\n", "self", ".", "annotation_dir", "=", "annotation_dir", "\n", "with", "open", "(", "self", ".", "data_file", ")", "as", "fd", ":", "\n", "            ", "self", ".", "filtered", "=", "json", ".", "load", "(", "fd", ")", "\n", "", "with", "open", "(", "self", ".", "unlabelled_file", ")", "as", "fd", ":", "\n", "            ", "self", ".", "unlabelled", "=", "json", ".", "load", "(", "fd", ")", "\n", "", "self", ".", "limit", "=", "limit", "\n", "self", ".", "reverse_index", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.data_dir": [[447, 450], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_dir", "(", "self", ")", ":", "\n", "        ", "return", "f'/{self.annotation_dir}/slide'", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.data_file": [[451, 454], ["os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "data_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'{}.json'", ".", "format", "(", "self", ".", "split", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.mapping_file": [[455, 458], ["os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "mapping_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'idiom_span_mapping.json'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.answer_file": [[459, 462], ["os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "answer_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'{}_answer.csv'", ".", "format", "(", "self", ".", "split", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.unlabelled_file": [[463, 466], ["os.path.join"], "methods", ["None"], ["", "@", "property", "\n", "def", "unlabelled_file", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'unlabelled.json'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.get_idiom_id": [[467, 470], ["None"], "methods", ["None"], ["", "def", "get_idiom_id", "(", "self", ",", "idiom", ")", ":", "\n", "        ", "idiom", "=", "self", ".", "mapping", "[", "idiom", "]", "\n", "return", "self", ".", "vocab", "[", "idiom", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser._construct_example": [[471, 496], ["tmp_context.replace.replace.replace", "preprocess.SlideParser.get_idiom_id", "preprocess.SlideParser.reverse_index.setdefault", "preprocess.SlideParser.reverse_index[].append", "preprocess.Example", "options.index", "len", "print", "len", "tag.start", "tag.end"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.get_idiom_id"], ["", "def", "_construct_example", "(", "self", ",", "i", ",", "idiom", ",", "tag", ",", "new_tag", ",", "context", ",", "data", ")", ":", "\n", "        ", "tag_str", "=", "\"#idiom%06d#\"", "%", "new_tag", "\n", "\n", "tmp_context", "=", "context", "\n", "tmp_context", "=", "\"\"", ".", "join", "(", "(", "tmp_context", "[", ":", "tag", ".", "start", "(", "0", ")", "]", ",", "tag_str", ",", "tmp_context", "[", "tag", ".", "end", "(", "0", ")", ":", "]", ")", ")", "\n", "tmp_context", "=", "tmp_context", ".", "replace", "(", "\"#idiom#\"", ",", "\"[UNK]\"", ")", "\n", "\n", "if", "'candidates'", "not", "in", "data", ":", "\n", "            ", "options", ",", "label", "=", "[", "]", ",", "-", "1", "\n", "", "else", ":", "\n", "            ", "options", "=", "data", "[", "'candidates'", "]", "[", "i", "]", "\n", "if", "len", "(", "options", ")", "!=", "7", ":", "\n", "                ", "print", "(", "data", ")", "\n", "assert", "len", "(", "options", ")", "==", "7", "\n", "", "label", "=", "options", ".", "index", "(", "idiom", ")", "\n", "", "idiom_id", "=", "self", ".", "get_idiom_id", "(", "idiom", ")", "\n", "self", ".", "reverse_index", ".", "setdefault", "(", "idiom_id", ",", "[", "]", ")", "\n", "self", ".", "reverse_index", "[", "idiom_id", "]", ".", "append", "(", "tag_str", ")", "\n", "return", "Example", "(", "\n", "idx", "=", "new_tag", ",", "\n", "tag", "=", "tag_str", ",", "\n", "context", "=", "tmp_context", ",", "\n", "idiom", "=", "idiom", ",", "\n", "options", "=", "options", ",", "\n", "label", "=", "label", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.read_examples": [[498, 520], ["tqdm.tqdm.tqdm", "open", "json.load", "json.load.items", "os.path.join", "len", "random.sample", "enumerate", "zip", "len", "re.finditer", "preprocess.SlideParser._construct_example", "preprocess.SlideParser._construct_example"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser._construct_example", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser._construct_example"], ["", "def", "read_examples", "(", "self", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "'data.json'", ")", ")", "as", "f", ":", "\n", "            ", "examples", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "idx", "=", "0", "\n", "for", "idiom", ",", "data_list", "in", "tqdm", "(", "examples", ".", "items", "(", ")", ",", "total", "=", "len", "(", "examples", ")", ")", ":", "\n", "            ", "if", "idiom", "not", "in", "self", ".", "filtered", "and", "self", ".", "split", "!=", "'train'", ":", "\n", "                ", "continue", "\n", "", "data_list", "=", "random", ".", "sample", "(", "data_list", ",", "k", "=", "self", ".", "limit", ")", "if", "self", ".", "limit", "and", "len", "(", "\n", "data_list", ")", ">", "self", ".", "limit", "else", "data_list", "\n", "for", "data", "in", "data_list", ":", "\n", "                ", "idx", "+=", "1", "\n", "context", "=", "data", "[", "'content'", "]", "\n", "for", "i", ",", "(", "tag", ",", "span_text", ")", "in", "enumerate", "(", "zip", "(", "re", ".", "finditer", "(", "\"#idiom#\"", ",", "context", ")", ",", "data", "[", "'groundTruth'", "]", ")", ")", ":", "\n", "                    ", "new_tag", "=", "idx", "*", "20", "+", "i", "\n", "\n", "if", "self", ".", "split", "==", "'train'", ":", "\n", "                        ", "if", "idiom", "in", "self", ".", "filtered", "or", "idiom", "in", "self", ".", "unlabelled", ":", "\n", "                            ", "yield", "self", ".", "_construct_example", "(", "i", ",", "span_text", ",", "tag", ",", "new_tag", ",", "context", ",", "data", ")", "\n", "", "", "else", ":", "\n", "                        ", "if", "idiom", "in", "self", ".", "filtered", ":", "\n", "                            ", "yield", "self", ".", "_construct_example", "(", "i", ",", "span_text", ",", "tag", ",", "new_tag", ",", "context", ",", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.create_cct_dataset": [[47, 74], ["pandas.read_csv", "open", "open", "l.strip().split", "random.shuffle", "f.write", "f.write", "pda.read_csv.ground_truth.sample().to_list", "tmp_context.replace.replace", "tmp_context.replace.replace", "json.dumps", "l.strip", "pda.read_csv.ground_truth.sample"], "function", ["None"], ["", "", "def", "create_cct_dataset", "(", "candidates_num", ")", ":", "\n", "    ", "\"\"\"\n    Create a random Chengyu Cloze Test with a specific candidates number\n    :param candidates_num: Candidate set size\n    :return:\n    \"\"\"", "\n", "df", "=", "pda", ".", "read_csv", "(", "'/txt/chengyu/chengyu_sentence.txt'", ",", "header", "=", "None", ",", "names", "=", "[", "'ground_truth'", ",", "'context'", "]", ")", "\n", "with", "open", "(", "'/txt/chengyu/chengyu_sentence.txt'", ")", "as", "g", ":", "\n", "        ", "with", "open", "(", "self", ".", "data_file", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "for", "l", "in", "g", ":", "\n", "                ", "item", "=", "l", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "\n", "tmp_context", "=", "','", ".", "join", "(", "item", "[", "1", ":", "]", ")", "\n", "label", "=", "item", "[", "0", "]", "\n", "options", "=", "df", ".", "ground_truth", ".", "sample", "(", "candidates_num", "-", "1", ")", ".", "to_list", "(", ")", "+", "[", "label", "]", "\n", "random", ".", "shuffle", "(", "options", ")", "\n", "if", "\"\uff5e\"", "in", "tmp_context", ":", "\n", "                    ", "tmp_context", "=", "tmp_context", ".", "replace", "(", "\"\uff5e\"", ",", "\"#idiom#\"", ")", "\n", "", "else", ":", "\n", "                    ", "tmp_context", "=", "tmp_context", ".", "replace", "(", "\"#{}#\"", ".", "format", "(", "label", ")", ",", "\"#idiom#\"", ")", "\n", "\n", "", "f", ".", "write", "(", "json", ".", "dumps", "(", "{", "\n", "\"groundTruth\"", ":", "[", "label", "]", ",", "\n", "\"candidates\"", ":", "[", "options", "]", ",", "\n", "\"content\"", ":", "tmp_context", ",", "\n", "\"realCount\"", ":", "1", "\n", "}", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.tokenize": [[76, 87], ["re.split", "tokenizer.convert_tokens_to_ids", "tokenizer.convert_tokens_to_ids.index", "len", "tokenizer.tokenize", "tokenizer.tokenize", "len", "len"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.tokenize", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.tokenize"], ["", "", "", "", "def", "tokenize", "(", "tokenizer", ",", "example", ")", ":", "\n", "    ", "tag", "=", "example", ".", "tag", "\n", "parts", "=", "re", ".", "split", "(", "tag", ",", "example", ".", "context", ")", "\n", "assert", "len", "(", "parts", ")", "==", "2", "\n", "before_part", "=", "tokenizer", ".", "tokenize", "(", "parts", "[", "0", "]", ")", "if", "len", "(", "parts", "[", "0", "]", ")", ">", "0", "else", "[", "]", "\n", "after_part", "=", "tokenizer", ".", "tokenize", "(", "parts", "[", "1", "]", ")", "if", "len", "(", "parts", "[", "1", "]", ")", ">", "0", "else", "[", "]", "\n", "\n", "tokens", "=", "before_part", "+", "[", "tokenizer", ".", "mask_token", "]", "+", "after_part", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "position", "=", "input_ids", ".", "index", "(", "tokenizer", ".", "mask_token_id", ")", "\n", "return", "input_ids", ",", "position", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.process": [[522, 606], ["opts.annotation.split", "enumerate", "source.startswith", "source.startswith", "preprocess.ChidOfficialParser", "preprocess.tokenize", "preprocess.SlideParser.read_examples", "preprocess.process.parse_example"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.tokenize", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.SlideParser.read_examples"], ["", "", "", "", "", "", "", "def", "process", "(", "opts", ",", "db", ",", "tokenizer", ")", ":", "\n", "    ", "source", ",", "split", "=", "opts", ".", "annotation", ".", "split", "(", "'_'", ")", "\n", "\n", "if", "source", "==", "'official'", ":", "\n", "        ", "assert", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", ",", "'ran'", ",", "'sim'", ",", "'out'", "]", "\n", "parser", "=", "ChidOfficialParser", "(", "split", ",", "opts", ".", "len_idiom_vocab", ")", "\n", "", "elif", "source", "==", "'external'", ":", "\n", "        ", "assert", "split", "in", "[", "'pretrain'", ",", "'cct7'", ",", "'cct4'", "]", "\n", "parser", "=", "ChidExternalParser", "(", "split", ",", "opts", ".", "len_idiom_vocab", ")", "\n", "", "elif", "source", "==", "'balanced'", ":", "\n", "        ", "assert", "split", "in", "[", "'train'", ",", "'val'", "]", "\n", "parser", "=", "ChidBalancedParser", "(", "split", ",", "opts", ".", "len_idiom_vocab", ")", "\n", "", "elif", "source", "==", "'balancedfix'", ":", "\n", "        ", "assert", "split", "in", "[", "'train'", ",", "'val'", "]", "\n", "parser", "=", "ChidBalancedFixParser", "(", "split", ",", "opts", ".", "len_idiom_vocab", ")", "\n", "", "elif", "source", "==", "'competition'", ":", "\n", "        ", "assert", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", ",", "'out'", "]", "\n", "parser", "=", "ChidCompetitionParser", "(", "split", ",", "opts", ".", "len_idiom_vocab", ")", "\n", "", "elif", "source", ".", "startswith", "(", "'affection'", ")", ":", "\n", "        ", "assert", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "# We only use train split of ChID for affection", "\n", "if", "source", "!=", "'affection'", ":", "\n", "            ", "limit", "=", "int", "(", "source", ".", "replace", "(", "'affection'", ",", "''", ")", ")", "\n", "", "parser", "=", "ChidAffectionParser", "(", "split", ",", "opts", ".", "len_idiom_vocab", ",", "limit", "=", "limit", ")", "\n", "", "elif", "source", ".", "startswith", "(", "'slide'", ")", ":", "\n", "        ", "assert", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "\n", "if", "source", "!=", "'slide'", ":", "\n", "            ", "limit", "=", "int", "(", "source", ".", "replace", "(", "'slide'", ",", "''", ")", ")", "\n", "", "parser", "=", "SlideParser", "(", "split", ",", "opts", ".", "len_idiom_vocab", ",", "limit", "=", "limit", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"No such source!\"", ")", "\n", "\n", "", "def", "parse_example", "(", "example", ")", ":", "\n", "        ", "input_ids", ",", "position", "=", "tokenize", "(", "tokenizer", ",", "example", ")", "\n", "return", "{", "\n", "'input_ids'", ":", "input_ids", ",", "\n", "'position'", ":", "position", ",", "\n", "'idiom'", ":", "parser", ".", "get_idiom_id", "(", "example", ".", "idiom", ")", ",", "\n", "'target'", ":", "example", ".", "label", ",", "\n", "'options'", ":", "[", "parser", ".", "get_idiom_id", "(", "o", ")", "for", "o", "in", "example", ".", "options", "]", "\n", "}", "\n", "\n", "", "id2len", "=", "{", "}", "\n", "ans_dict", "=", "{", "}", "\n", "id2eid", "=", "{", "}", "\n", "span_texts", "=", "{", "}", "\n", "for", "i", ",", "ex", "in", "enumerate", "(", "parser", ".", "read_examples", "(", ")", ")", ":", "\n", "        ", "exa", "=", "parse_example", "(", "ex", ")", "\n", "if", "i", "%", "1000", "==", "0", ":", "\n", "            ", "print", "(", "exa", ")", "\n", "\n", "", "db", "[", "ex", ".", "tag", "]", "=", "exa", "\n", "id2len", "[", "ex", ".", "tag", "]", "=", "len", "(", "exa", "[", "'input_ids'", "]", ")", "\n", "ans_dict", "[", "ex", ".", "tag", "]", "=", "ex", ".", "label", "\n", "id2eid", "[", "ex", ".", "tag", "]", "=", "ex", ".", "idx", "\n", "span_texts", "[", "ex", ".", "tag", "]", "=", "ex", ".", "idiom", "\n", "\n", "", "assert", "len", "(", "id2len", ")", "==", "len", "(", "ans_dict", ")", "\n", "\n", "with", "open", "(", "f'{opts.output}/answer.csv'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "k", ",", "v", "in", "ans_dict", ".", "items", "(", ")", ":", "\n", "            ", "f", ".", "write", "(", "'{},{}\\n'", ".", "format", "(", "k", ",", "v", ")", ")", "\n", "\n", "", "", "with", "open", "(", "f'{opts.output}/id2eid.json'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "id2eid", ",", "f", ")", "\n", "\n", "", "with", "open", "(", "f'{opts.output}/reverse_index.json'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "parser", ".", "reverse_index", ",", "f", ")", "\n", "\n", "", "if", "source", ".", "startswith", "(", "'affection'", ")", ":", "\n", "        ", "with", "open", "(", "f'{opts.output}/{split}.json'", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "[", "parser", ".", "vocab", "[", "v", "]", "for", "v", "in", "parser", ".", "filtered", "]", ",", "f", ")", "\n", "", "with", "open", "(", "f'{opts.output}/unlabelled.json'", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "[", "parser", ".", "vocab", "[", "v", "]", "for", "v", "in", "parser", ".", "unlabelled", "]", ",", "f", ")", "\n", "\n", "", "", "if", "source", ".", "startswith", "(", "'slide'", ")", ":", "\n", "        ", "with", "open", "(", "f'{opts.output}/{split}.json'", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "[", "parser", ".", "get_idiom_id", "(", "v", ")", "for", "v", "in", "parser", ".", "filtered", "]", ",", "f", ")", "\n", "", "with", "open", "(", "f'{opts.output}/unlabelled.json'", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "[", "parser", ".", "get_idiom_id", "(", "v", ")", "for", "v", "in", "parser", ".", "unlabelled", "]", ",", "f", ")", "\n", "", "with", "open", "(", "f'{opts.output}/span_idiom_mapping.json'", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "span_texts", ",", "f", ")", "\n", "\n", "", "", "return", "id2len", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.main": [[608, 634], ["print", "opts.annotation.split", "os.path.join", "os.makedirs", "transformers.AutoTokenizer.from_pretrained", "cytoolz.curry", "chengyubert.data.intermediate_dir", "getattr", "cytoolz.curry.", "preprocess.process", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.intermediate_dir", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.process"], ["", "def", "main", "(", "opts", ")", ":", "\n", "    ", "print", "(", "opts", ")", "\n", "dataset", ",", "split", "=", "opts", ".", "annotation", ".", "split", "(", "'_'", ")", "\n", "if", "split", "==", "'dev'", ":", "\n", "        ", "txt_db", "=", "'val_txt_db'", "\n", "", "elif", "split", "==", "'pretrain'", ":", "\n", "        ", "txt_db", "=", "'train_txt_db'", "\n", "", "else", ":", "\n", "        ", "txt_db", "=", "f'{split}_txt_db'", "\n", "", "opts", ".", "output", "=", "os", ".", "path", ".", "join", "(", "'/txt'", ",", "\n", "intermediate_dir", "(", "opts", ".", "pretrained_model_name_or_path", ")", ",", "\n", "getattr", "(", "opts", ",", "txt_db", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "opts", ".", "output", ")", "\n", "\n", "# train_db_dir = os.path.join(os.path.dirname(opts.output), f'{source}_{split}.db')", "\n", "# meta = vars(opts)", "\n", "# meta['tokenizer'] = opts.toker", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "opts", ".", "pretrained_model_name_or_path", ",", "use_fast", "=", "True", ")", "\n", "\n", "open_db", "=", "curry", "(", "open_lmdb", ",", "opts", ".", "output", ",", "readonly", "=", "False", ")", "\n", "with", "open_db", "(", ")", "as", "db", ":", "\n", "        ", "id2lens", "=", "process", "(", "opts", ",", "db", ",", "tokenizer", ")", "\n", "\n", "", "with", "open", "(", "f'{opts.output}/id2len.json'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "id2lens", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_affection.train": [[40, 179], ["chengyubert.utils.distributed.broadcast_tensors", "chengyubert.utils.misc.set_dropout", "chengyubert.optim.misc.build_optimizer", "torch.cuda.amp.GradScaler", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.RunningMeter", "model.train", "time.time", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.optim.misc.build_optimizer.step", "chengyubert.utils.save.save_training_meta", "chengyubert.utils.logger.TB_LOGGER.create", "tqdm.tqdm", "chengyubert.utils.save.ModelSaver", "os.makedirs", "chengyubert.utils.logger.add_log_to_file", "chengyubert.utils.misc.NoOp", "chengyubert.utils.misc.NoOp", "len", "enumerate", "chengyubert.utils.logger.LOGGER.info", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "targets.size", "torch.cuda.amp.GradScaler.scale().backward", "chengyubert.utils.logger.RunningMeter.", "model.parameters", "torch.cuda.amp.autocast", "model", "chengyubert.utils.distributed.all_reduce_and_rescale_tensors", "sentiment_emotion_loss.mean.item", "chengyubert.optim.get_lr_sched", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.logger.RunningMeter", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.logger.TB_LOGGER.step", "torch.cuda.amp.GradScaler.step", "torch.cuda.amp.GradScaler.update", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.utils.misc.NoOp.update", "torch.cuda.amp.GradScaler.scale", "float", "torch.cuda.amp.GradScaler.unscale_", "torch.nn.utils.clip_grad_norm_", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "sum", "int", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "train_affection.evaluation", "chengyubert.utils.misc.NoOp.save", "sentiment_emotion_loss.mean", "model.parameters", "sum", "len", "model.parameters", "chengyubert.utils.distributed.all_gather_list", "dict", "chengyubert.utils.misc.NoOp.set_description", "filter", "time.time", "dataloaders.items", "over_loss.mean", "x[].startswith"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.broadcast_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_dropout", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.misc.build_optimizer", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.save_training_meta", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.create", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.add_log_to_file", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.GatherLayer.backward", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_reduce_and_rescale_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.get_lr_sched", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list"], ["def", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", ":", "\n", "# make sure every process has same model parameters in the beginning", "\n", "    ", "broadcast_tensors", "(", "[", "p", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ",", "0", ")", "\n", "set_dropout", "(", "model", ",", "opts", ".", "dropout", ")", "\n", "\n", "# Prepare optimizer", "\n", "optimizer", "=", "build_optimizer", "(", "model", ",", "opts", ")", "\n", "scaler", "=", "GradScaler", "(", ")", "\n", "\n", "global_step", "=", "0", "\n", "if", "opts", ".", "rank", "==", "0", ":", "\n", "        ", "save_training_meta", "(", "opts", ")", "\n", "TB_LOGGER", ".", "create", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ")", ")", "\n", "pbar", "=", "tqdm", "(", "total", "=", "opts", ".", "num_train_steps", ",", "desc", "=", "opts", ".", "model", ")", "\n", "model_saver", "=", "ModelSaver", "(", "join", "(", "opts", ".", "output_dir", ",", "'ckpt'", ")", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'results'", ")", ",", "exist_ok", "=", "True", ")", "# store val predictions", "\n", "add_log_to_file", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ",", "'log.txt'", ")", ")", "\n", "", "else", ":", "\n", "        ", "LOGGER", ".", "disabled", "=", "True", "\n", "pbar", "=", "NoOp", "(", ")", "\n", "model_saver", "=", "NoOp", "(", ")", "\n", "\n", "", "LOGGER", ".", "info", "(", "f\"***** Running training with {opts.n_gpu} GPUs *****\"", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "dataloaders", "[", "'train'", "]", ".", "dataset", ")", ")", "\n", "LOGGER", ".", "info", "(", "\"  Batch size = %d\"", ",", "opts", ".", "train_batch_size", ")", "\n", "LOGGER", ".", "info", "(", "\"  Accumulate steps = %d\"", ",", "opts", ".", "gradient_accumulation_steps", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num steps = %d\"", ",", "opts", ".", "num_train_steps", ")", "\n", "\n", "running_loss", "=", "RunningMeter", "(", "'loss'", ")", "\n", "model", ".", "train", "(", ")", "\n", "n_examples", "=", "0", "\n", "n_epoch", "=", "0", "\n", "best_ckpt", "=", "0", "\n", "best_eval", "=", "0", "\n", "start", "=", "time", "(", ")", "\n", "# quick hack for amp delay_unscale bug", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "while", "True", ":", "\n", "        ", "for", "step", ",", "batch", "in", "enumerate", "(", "dataloaders", "[", "'train'", "]", ")", ":", "\n", "            ", "targets", "=", "batch", "[", "'targets'", "]", "\n", "n_examples", "+=", "targets", ".", "size", "(", "0", ")", "\n", "\n", "with", "autocast", "(", ")", ":", "\n", "                ", "(", "_", ",", "over_loss", ",", "\n", "select_masks", ",", "\n", "# coarse_emotion_loss,", "\n", "losses", ")", "=", "model", "(", "**", "batch", ",", "compute_loss", "=", "True", ")", "\n", "if", "over_loss", "is", "not", "None", ":", "\n", "                    ", "if", "opts", ".", "project", "==", "'calo'", ":", "\n", "                        ", "fine_emotion_loss", ",", "sentiment_emotion_loss", "=", "losses", "\n", "loss", "=", "(", "over_loss", "+", "fine_emotion_loss", "+", "sentiment_emotion_loss", ")", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "                        ", "sentiment_emotion_loss", "=", "losses", "\n", "loss", "=", "(", "over_loss", "+", "sentiment_emotion_loss", ")", ".", "mean", "(", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "opts", ".", "project", "==", "'calo'", ":", "\n", "                        ", "fine_emotion_loss", ",", "sentiment_emotion_loss", "=", "losses", "\n", "loss", "=", "(", "fine_emotion_loss", "+", "sentiment_emotion_loss", ")", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "                        ", "sentiment_emotion_loss", "=", "losses", "\n", "loss", "=", "sentiment_emotion_loss", ".", "mean", "(", ")", "\n", "\n", "", "", "", "delay_unscale", "=", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "!=", "0", "\n", "scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", ")", "\n", "if", "not", "delay_unscale", ":", "\n", "# gather gradients from every processes", "\n", "# do this before unscaling to make sure every process uses", "\n", "# the same gradient scale", "\n", "                ", "grads", "=", "[", "p", ".", "grad", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "\n", "if", "p", ".", "requires_grad", "and", "p", ".", "grad", "is", "not", "None", "]", "\n", "all_reduce_and_rescale_tensors", "(", "grads", ",", "float", "(", "1", ")", ")", "\n", "\n", "", "running_loss", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "global_step", "+=", "1", "\n", "\n", "# learning rate scheduling", "\n", "lr_this_step", "=", "get_lr_sched", "(", "global_step", ",", "opts", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "TB_LOGGER", ".", "add_scalar", "(", "'lr'", ",", "lr_this_step", ",", "global_step", ")", "\n", "\n", "# log loss", "\n", "losses", "=", "all_gather_list", "(", "running_loss", ")", "\n", "running_loss", "=", "RunningMeter", "(", "\n", "'loss'", ",", "sum", "(", "l", ".", "val", "for", "l", "in", "losses", ")", "/", "len", "(", "losses", ")", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'loss'", ",", "running_loss", ".", "val", ",", "global_step", ")", "\n", "TB_LOGGER", ".", "step", "(", ")", "\n", "\n", "# update model params", "\n", "if", "opts", ".", "grad_norm", "!=", "-", "1", ":", "\n", "# Unscales the gradients of optimizer's assigned params in-place", "\n", "                    ", "scaler", ".", "unscale_", "(", "optimizer", ")", "\n", "grad_norm", "=", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "opts", ".", "grad_norm", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'grad_norm'", ",", "grad_norm", ",", "global_step", ")", "\n", "\n", "# scaler.step() first unscales gradients of the optimizer's params.", "\n", "# If gradients don't contain infs/NaNs, optimizer.step() is then called,", "\n", "# otherwise, optimizer.step() is skipped.", "\n", "", "scaler", ".", "step", "(", "optimizer", ")", "\n", "\n", "# Updates the scale for next iteration.", "\n", "scaler", ".", "update", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "if", "global_step", "%", "100", "==", "0", ":", "\n", "# monitor training throughput", "\n", "                    ", "tot_ex", "=", "sum", "(", "all_gather_list", "(", "n_examples", ")", ")", "\n", "ex_per_sec", "=", "int", "(", "tot_ex", "/", "(", "time", "(", ")", "-", "start", ")", ")", "\n", "LOGGER", ".", "info", "(", "f'{opts.model}: {n_epoch}-{global_step}: '", "\n", "f'{tot_ex} examples trained at '", "\n", "f'{ex_per_sec} ex/s \\n'", "\n", "f'over loss: {over_loss.mean() if over_loss is not None else over_loss} \\n'", "\n", "# f'coarse emotion loss: {coarse_emotion_loss.mean()} \\n'", "\n", "# f'fine emotion loss: {fine_emotion_loss.mean()} \\n'", "\n", "# f'sentiment loss: {sentiment_emotion_loss.mean()} \\n'", "\n", "f'best_acc-{best_eval * 100:.2f}'", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'perf/ex_per_s'", ",", "\n", "ex_per_sec", ",", "global_step", ")", "\n", "\n", "", "if", "global_step", "%", "opts", ".", "valid_steps", "==", "0", ":", "\n", "                    ", "log", "=", "evaluation", "(", "model", ",", "\n", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", ".", "startswith", "(", "'val'", ")", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "\n", "opts", ",", "global_step", ")", "\n", "if", "log", "and", "log", "[", "'val/acc'", "]", ">", "best_eval", ":", "\n", "                        ", "best_ckpt", "=", "global_step", "\n", "best_eval", "=", "log", "[", "'val/acc'", "]", "\n", "pbar", ".", "set_description", "(", "f'{opts.model}: {n_epoch}-{best_ckpt} best_acc-{best_eval * 100:.2f}'", ")", "\n", "", "model_saver", ".", "save", "(", "model", ",", "global_step", ")", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "                ", "break", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", "or", "global_step", "-", "best_ckpt", ">", "0.3", "*", "opts", ".", "num_train_steps", ":", "\n", "            ", "break", "\n", "", "n_epoch", "+=", "1", "\n", "LOGGER", ".", "info", "(", "f\"Step {global_step}: finished {n_epoch} epochs\"", ")", "\n", "", "return", "best_ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_affection.idiom2tree": [[181, 209], ["enumerate", "enumerate", "len", "ans.pop", "isinstance", "ans.pop", "isinstance", "ans.insert", "ans.pop", "isinstance", "ans.insert", "ans.pop.label", "ans.pop.label", "nltk.Tree", "ans.pop.label", "nltk.Tree"], "function", ["None"], ["", "def", "idiom2tree", "(", "idiom", ",", "select_masks", ")", ":", "\n", "# ans = list(idiom)", "\n", "    ", "ans", "=", "idiom", "\n", "for", "k", ",", "select_mask", "in", "enumerate", "(", "select_masks", ")", ":", "\n", "        ", "for", "idx", ",", "v", "in", "enumerate", "(", "select_mask", ")", ":", "\n", "            ", "if", "v", "==", "1", ":", "\n", "                ", "c0", "=", "ans", ".", "pop", "(", "idx", ")", "\n", "if", "isinstance", "(", "c0", ",", "Tree", ")", ":", "\n", "                    ", "c0_label", "=", "c0", ".", "label", "(", ")", "\n", "", "else", ":", "\n", "                    ", "c0_label", "=", "c0", "\n", "\n", "", "c1", "=", "ans", ".", "pop", "(", "idx", ")", "\n", "if", "isinstance", "(", "c1", ",", "Tree", ")", ":", "\n", "                    ", "c1_label", "=", "c1", ".", "label", "(", ")", "\n", "", "else", ":", "\n", "                    ", "c1_label", "=", "c1", "\n", "\n", "", "ans", ".", "insert", "(", "idx", ",", "Tree", "(", "c0_label", "+", "c1_label", ",", "(", "c0", ",", "c1", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "c", "=", "ans", ".", "pop", "(", "idx", ")", "\n", "if", "isinstance", "(", "c", ",", "Tree", ")", ":", "\n", "                    ", "c_label", "=", "c", ".", "label", "(", ")", "\n", "", "else", ":", "\n", "                    ", "c_label", "=", "c", "\n", "", "ans", ".", "insert", "(", "idx", ",", "Tree", "(", "c_label", ",", "(", "c", ",", ")", ")", ")", "\n", "", "", "", "assert", "len", "(", "ans", ")", "==", "2", "\n", "return", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_affection.validate_calo": [[211, 446], ["torch.no_grad", "horovod.torch.no_grad", "time.time", "sum", "sum", "sum", "sum", "sum", "tqdm.tqdm", "enumerate", "train_affection.validate_calo.get_header"], "function", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "validate_calo", "(", "opts", ",", "model", ",", "val_loader", ",", "split", ",", "global_step", ")", ":", "\n", "    ", "val_loss", "=", "0", "\n", "fine_emotion_score", "=", "0", "\n", "sentiment_score", "=", "0", "\n", "n_ex", "=", "0", "\n", "val_mrr", "=", "0", "\n", "st", "=", "time", "(", ")", "\n", "results", "=", "[", "]", "\n", "\n", "def", "get_header", "(", "key", ")", ":", "\n", "        ", "d", "=", "calo_inverse_mapping", "[", "key", "]", "\n", "return", "[", "f'{key}_{d[v]}_{v}'", "if", "isinstance", "(", "d", "[", "v", "]", ",", "str", ")", "else", "f'{key}_{d[v][-1]}_{v}'", "for", "v", "in", "range", "(", "len", "(", "d", ")", ")", "]", "\n", "\n", "", "affection_results", "=", "[", "]", "\n", "with", "tqdm", "(", "range", "(", "len", "(", "val_loader", ".", "dataset", ")", "//", "opts", ".", "size", ")", ",", "desc", "=", "f'{split}-{opts.rank}'", ")", "as", "tq", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "            ", "qids", "=", "batch", "[", "'qids'", "]", "\n", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'qids'", "]", "\n", "\n", "# select_masks, atts, composition_gates = composition", "\n", "if", "batch", "[", "'input_ids'", "]", ".", "dim", "(", ")", "==", "3", ":", "\n", "                ", "input_ids", "=", "torch", ".", "gather", "(", "batch", "[", "'input_ids'", "]", "[", "1", "]", ",", "dim", "=", "1", ",", "index", "=", "batch", "[", "'gather_index'", "]", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "input_ids", "=", "torch", ".", "gather", "(", "batch", "[", "'input_ids'", "]", ",", "dim", "=", "1", ",", "index", "=", "batch", "[", "'gather_index'", "]", ")", "\n", "\n", "", "_", ",", "over_logits", ",", "select_masks", ",", "(", "fine_emotion_logits", ",", "sentiment_logits", ")", "=", "model", "(", "\n", "**", "batch", ",", "targets", "=", "None", ",", "compute_loss", "=", "False", ")", "\n", "\n", "idiom_targets", "=", "targets", "[", ":", ",", "0", "]", "\n", "coarse_emotion_targets", "=", "targets", "[", ":", ",", "1", "]", "\n", "fine_emotion_targets", "=", "targets", "[", ":", ",", "2", "]", "\n", "sentiment_targets", "=", "targets", "[", ":", ",", "3", "]", "\n", "\n", "fine_emotion_score", "+=", "(", "\n", "fine_emotion_logits", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "[", "1", "]", "==", "fine_emotion_targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "sentiment_score", "+=", "(", "\n", "sentiment_logits", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "[", "1", "]", "==", "sentiment_targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "if", "over_logits", "is", "not", "None", ":", "\n", "                ", "loss", "=", "F", ".", "cross_entropy", "(", "over_logits", ",", "idiom_targets", ",", "reduction", "=", "'sum'", ")", "\n", "val_loss", "+=", "loss", ".", "item", "(", ")", "\n", "# tot_score += (scores.max(dim=-1, keepdim=False)[1] == idiom_targets).sum().item()", "\n", "max_prob", ",", "max_idx", "=", "over_logits", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "\n", "\n", "options", "=", "[", "val_loader", ".", "dataset", ".", "id2idiom", "[", "o", "]", "for", "o", "in", "val_loader", ".", "dataset", ".", "enlarged_candidates", "]", "\n", "for", "j", ",", "(", "qid", ",", "inp", ",", "position", ",", "answer", ")", "in", "enumerate", "(", "zip", "(", "qids", ",", "\n", "# idiom_targets,", "\n", "input_ids", ",", "\n", "# batch['option_ids'],", "\n", "batch", "[", "'positions'", "]", ",", "\n", "max_idx", ")", ")", ":", "\n", "# g = over_logits[j].cpu().numpy()", "\n", "# top_k = np.argsort(-g)", "\n", "# val_mrr += 1 / (1 + np.argwhere(top_k == target.item()).item())", "\n", "\n", "                    ", "example", "=", "val_loader", ".", "dataset", ".", "db", "[", "qid", "]", "\n", "idiom", "=", "val_loader", ".", "dataset", ".", "id2idiom", "[", "example", "[", "'idiom'", "]", "]", "\n", "# idiom = options[target.item()]", "\n", "affection_results", ".", "append", "(", "\n", "[", "idiom", "]", "+", "fine_emotion_logits", "[", "\n", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "+", "sentiment_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", ")", "\n", "if", "i", "%", "1000", "==", "0", "and", "select_masks", "is", "not", "None", ":", "\n", "                        ", "g", "=", "over_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "top_k", "=", "np", ".", "argsort", "(", "-", "g", ")", "[", ":", "5", "]", "\n", "print", "(", "qid", ",", "\n", "[", "options", "[", "k", "]", "for", "k", "in", "top_k", "]", ",", "\n", "idiom", ")", "\n", "# print(len(select_masks), atts.size())", "\n", "s_masks", "=", "[", "select_mask", "[", "j", "]", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "for", "select_mask", "in", "select_masks", "]", "\n", "# s_att = atts[j].cpu().numpy().tolist()", "\n", "\n", "# tokens = val_loader.dataset.tokenizer.convert_ids_to_tokens(inp)", "\n", "# start = tokens.index(val_loader.dataset.tokenizer.mask_token)", "\n", "# tokens[position:position + len(idiom)] = list(idiom)", "\n", "tokens", "=", "list", "(", "idiom", ")", "\n", "# print(tokens, s_masks, s_att, composition_gates[j].sum())", "\n", "print", "(", "tokens", ",", "s_masks", ")", "\n", "try", ":", "\n", "                            ", "tree", "=", "Tree", "(", "' '", ".", "join", "(", "tokens", ")", ",", "idiom2tree", "(", "tokens", ",", "s_masks", ")", ")", "\n", "print", "(", "TreePrettyPrinter", "(", "tree", ")", ".", "text", "(", "unicodelines", "=", "True", ")", ")", "\n", "", "except", ":", "\n", "                            ", "pass", "\n", "\n", "", "predictions", "=", "{", "\n", "# \"coarse emotion\": {", "\n", "#     \"target\": calo_inverse_mapping['coarse_emotion'].get(coarse_emotion_targets[j].item(),", "\n", "#                                                          '\u65e0'),", "\n", "#     \"predictions\": {calo_inverse_mapping['coarse_emotion'][k]: v for k, v in", "\n", "#                     enumerate(coarse_emotion_logits[j].cpu().numpy().tolist())}", "\n", "# },", "\n", "\"fine emotion\"", ":", "{", "\n", "\"target\"", ":", "calo_inverse_mapping", "[", "'fine_emotion'", "]", ".", "get", "(", "fine_emotion_targets", "[", "j", "]", ".", "item", "(", ")", ",", "'\u65e0'", ")", ",", "\n", "\"predictions\"", ":", "{", "calo_inverse_mapping", "[", "'fine_emotion'", "]", "[", "k", "]", ":", "v", "for", "k", ",", "v", "in", "\n", "enumerate", "(", "fine_emotion_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "}", "\n", "}", ",", "\n", "\"sentiment\"", ":", "{", "\n", "\"target\"", ":", "calo_inverse_mapping", "[", "'sentiment'", "]", ".", "get", "(", "sentiment_targets", "[", "j", "]", ".", "item", "(", ")", ",", "'\u65e0'", ")", ",", "\n", "\"predictions\"", ":", "{", "calo_inverse_mapping", "[", "'sentiment'", "]", "[", "k", "]", ":", "v", "for", "k", ",", "v", "in", "\n", "enumerate", "(", "sentiment_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "}", "\n", "}", "\n", "}", "\n", "pprint", "(", "predictions", ")", "\n", "\n", "", "", "answers", "=", "max_idx", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "results", ".", "extend", "(", "zip", "(", "qids", ",", "answers", ")", ")", "\n", "", "else", ":", "\n", "                ", "for", "j", ",", "(", "qid", ",", "inp", ",", "position", ")", "in", "enumerate", "(", "zip", "(", "qids", ",", "input_ids", ",", "\n", "# batch['option_ids'],", "\n", "batch", "[", "'positions'", "]", ",", "\n", ")", ")", ":", "\n", "# options = [val_loader.dataset.id2idiom[o.item()] for o in option_ids]", "\n", "                    ", "example", "=", "val_loader", ".", "dataset", ".", "db", "[", "qid", "]", "\n", "idiom", "=", "val_loader", ".", "dataset", ".", "id2idiom", "[", "example", "[", "'idiom'", "]", "]", "\n", "affection_results", ".", "append", "(", "\n", "[", "idiom", "]", "+", "fine_emotion_logits", "[", "\n", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "+", "sentiment_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", ")", "\n", "if", "i", "%", "1000", "==", "0", "and", "select_masks", "is", "not", "None", ":", "\n", "                        ", "print", "(", "qid", ",", "\n", "idiom", ")", "\n", "s_masks", "=", "[", "select_mask", "[", "j", "]", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "for", "select_mask", "in", "select_masks", "]", "\n", "tokens", "=", "list", "(", "idiom", ")", "\n", "# print(tokens, s_masks, s_att, composition_gates[j].sum())", "\n", "print", "(", "tokens", ",", "s_masks", ")", "\n", "try", ":", "\n", "                            ", "tree", "=", "Tree", "(", "' '", ".", "join", "(", "tokens", ")", ",", "idiom2tree", "(", "tokens", ",", "s_masks", ")", ")", "\n", "print", "(", "TreePrettyPrinter", "(", "tree", ")", ".", "text", "(", "unicodelines", "=", "True", ")", ")", "\n", "", "except", ":", "\n", "                            ", "pass", "\n", "\n", "", "predictions", "=", "{", "\n", "# \"coarse emotion\": {", "\n", "#     \"target\": calo_inverse_mapping['coarse_emotion'].get(coarse_emotion_targets[j].item(),", "\n", "#                                                          '\u65e0'),", "\n", "#     \"predictions\": {calo_inverse_mapping['coarse_emotion'][k]: v for k, v in", "\n", "#                     enumerate(coarse_emotion_logits[j].cpu().numpy().tolist())}", "\n", "# },", "\n", "\"fine emotion\"", ":", "{", "\n", "\"target\"", ":", "calo_inverse_mapping", "[", "'fine_emotion'", "]", ".", "get", "(", "fine_emotion_targets", "[", "j", "]", ".", "item", "(", ")", ",", "'\u65e0'", ")", ",", "\n", "\"predictions\"", ":", "{", "calo_inverse_mapping", "[", "'fine_emotion'", "]", "[", "k", "]", ":", "v", "for", "k", ",", "v", "in", "\n", "enumerate", "(", "fine_emotion_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "}", "\n", "}", ",", "\n", "\"sentiment\"", ":", "{", "\n", "\"target\"", ":", "calo_inverse_mapping", "[", "'sentiment'", "]", ".", "get", "(", "sentiment_targets", "[", "j", "]", ".", "item", "(", ")", ",", "'\u65e0'", ")", ",", "\n", "\"predictions\"", ":", "{", "calo_inverse_mapping", "[", "'sentiment'", "]", "[", "k", "]", ":", "v", "for", "k", ",", "v", "in", "\n", "enumerate", "(", "sentiment_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "}", "\n", "}", "\n", "}", "\n", "pprint", "(", "predictions", ")", "\n", "\n", "", "", "", "n_ex", "+=", "len", "(", "qids", ")", "\n", "tq", ".", "update", "(", "len", "(", "qids", ")", ")", "\n", "\n", "", "", "if", "results", ":", "\n", "        ", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank{opts.rank}.csv'", "\n", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "for", "id_", ",", "ans", "in", "results", ":", "\n", "                ", "f", ".", "write", "(", "f'{id_},{ans}\\n'", ")", "\n", "\n", "", "", "", "header", "=", "[", "'idiom'", "]", "+", "get_header", "(", "'fine_emotion'", ")", "+", "get_header", "(", "'sentiment'", ")", "\n", "if", "affection_results", ":", "\n", "        ", "out_file", "=", "f'{opts.output_dir}/results/{split}_affection_results_{global_step}_rank{opts.rank}.csv'", "\n", "pd", ".", "DataFrame", "(", "affection_results", ",", "columns", "=", "header", ")", ".", "to_csv", "(", "out_file", ")", "\n", "\n", "", "val_loss", "=", "sum", "(", "all_gather_list", "(", "val_loss", ")", ")", "\n", "val_mrr", "=", "sum", "(", "all_gather_list", "(", "val_mrr", ")", ")", "\n", "\n", "# val_coarse_emotion_score = sum(all_gather_list(coarse_emotion_score))", "\n", "val_fine_emotion_score", "=", "sum", "(", "all_gather_list", "(", "fine_emotion_score", ")", ")", "\n", "val_sentiment_score", "=", "sum", "(", "all_gather_list", "(", "sentiment_score", ")", ")", "\n", "\n", "n_ex", "=", "sum", "(", "all_gather_list", "(", "n_ex", ")", ")", "\n", "tot_time", "=", "time", "(", ")", "-", "st", "\n", "\n", "val_loss", "/=", "n_ex", "\n", "val_mrr", "=", "val_mrr", "/", "n_ex", "\n", "# val_coarse_emotion_score = val_coarse_emotion_score / n_ex", "\n", "val_fine_emotion_score", "=", "val_fine_emotion_score", "/", "n_ex", "\n", "val_sentiment_score", "=", "val_sentiment_score", "/", "n_ex", "\n", "\n", "if", "results", ":", "\n", "        ", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}.csv'", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "out_file", ")", ":", "\n", "            ", "with", "open", "(", "out_file", ",", "'wb'", ")", "as", "g", ":", "\n", "                ", "for", "f", "in", "glob", ".", "glob", "(", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank*.csv'", ")", ":", "\n", "                    ", "shutil", ".", "copyfileobj", "(", "open", "(", "f", ",", "'rb'", ")", ",", "g", ")", "\n", "\n", "", "", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "txt_db", "=", "os", ".", "path", ".", "join", "(", "'/txt'", ",", "\n", "intermediate_dir", "(", "opts", ".", "pretrained_model_name_or_path", ")", ",", "\n", "getattr", "(", "opts", ",", "f'{split}_txt_db'", ")", ")", "\n", "val_acc", "=", "judge", "(", "out_file", ",", "f'{txt_db}/answer.csv'", ")", "\n", "\n", "", "if", "opts", ".", "rank", "==", "0", ":", "\n", "        ", "results_files", "=", "glob", ".", "glob", "(", "f'{opts.output_dir}/results/{split}_affection_results_{global_step}_rank*.csv'", ")", "\n", "new_affection_results_df", "=", "pd", ".", "concat", "(", "map", "(", "pd", ".", "read_csv", ",", "results_files", ")", ")", "\n", "idiom_num", "=", "new_affection_results_df", "[", "'idiom'", "]", ".", "unique", "(", ")", ".", "size", "\n", "idiom_wise_accs", "=", "{", "}", "\n", "for", "item", "in", "new_affection_results_df", ".", "groupby", "(", "'idiom'", ")", ".", "mean", "(", ")", ".", "reset_index", "(", ")", ".", "to_dict", "(", "orient", "=", "'records'", ")", ":", "\n", "            ", "idiom", "=", "item", "[", "'idiom'", "]", "\n", "idiom_id", "=", "val_loader", ".", "dataset", ".", "chengyu_vocab", "[", "idiom", "]", "\n", "affections", "=", "val_loader", ".", "dataset", ".", "calo_vocab", "[", "idiom_id", "]", "[", "0", "]", "\n", "for", "sub_type", "in", "[", "'fine_emotion'", ",", "'sentiment'", "]", ":", "\n", "                ", "d", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "item", ".", "items", "(", ")", "if", "k", ".", "startswith", "(", "sub_type", ")", "}", "\n", "key", "=", "max", "(", "d", ",", "key", "=", "d", ".", "get", ")", "\n", "_", ",", "pred", "=", "key", ".", "rsplit", "(", "'_'", ",", "1", ")", "\n", "target", "=", "affections", "[", "sub_type", "]", "\n", "idiom_wise_accs", ".", "setdefault", "(", "sub_type", ",", "0", ")", "\n", "idiom_wise_accs", "[", "sub_type", "]", "+=", "(", "int", "(", "pred", ")", "==", "target", ")", "/", "idiom_num", "*", "100", "\n", "\n", "", "", "val_acc", "=", "(", "val_fine_emotion_score", "+", "val_sentiment_score", ")", "/", "2", "\n", "\n", "val_log", "=", "{", "f'{split}/loss'", ":", "val_loss", ",", "\n", "f'{split}/acc'", ":", "val_acc", ",", "\n", "f'{split}/fine_emotion'", ":", "val_fine_emotion_score", "*", "100", ",", "\n", "f'{split}/sentiment'", ":", "val_sentiment_score", "*", "100", ",", "\n", "f'{split}/mrr'", ":", "val_mrr", ",", "\n", "f'{split}/ex_per_s'", ":", "n_ex", "/", "tot_time", "}", "\n", "\n", "for", "k", ",", "v", "in", "idiom_wise_accs", ".", "items", "(", ")", ":", "\n", "            ", "val_log", "[", "f'{split}/{k}'", "]", "=", "v", "\n", "\n", "", "LOGGER", ".", "info", "(", "f\"validation finished in {int(tot_time)} seconds, \\n\"", "\n", "# f\"coarse emotion score: {val_coarse_emotion_score * 100:.2f}, \\n\"", "\n", "f\"fine emotion score: {val_fine_emotion_score * 100:.2f}, \\n\"", "\n", "f\"sentiment score: {val_sentiment_score * 100:.2f}, \\n\"", "\n", "f\"score: {val_acc * 100:.2f}, \\n\"", "\n", "f\"idiom-wise score: {idiom_wise_accs}, \"", "\n", "f\"mrr: {val_mrr:.3f}\"", ")", "\n", "return", "val_log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_affection.validate_slide": [[448, 657], ["torch.no_grad", "horovod.torch.no_grad", "time.time", "sum", "sum", "sum", "sum", "tqdm.tqdm", "enumerate", "train_affection.validate_calo.get_header"], "function", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "validate_slide", "(", "opts", ",", "model", ",", "val_loader", ",", "split", ",", "global_step", ")", ":", "\n", "    ", "val_loss", "=", "0", "\n", "sentiment_score", "=", "0", "\n", "n_ex", "=", "0", "\n", "val_mrr", "=", "0", "\n", "st", "=", "time", "(", ")", "\n", "results", "=", "[", "]", "\n", "\n", "def", "get_header", "(", "key", ")", ":", "\n", "        ", "d", "=", "idioms_inverse_mapping", "[", "key", "]", "\n", "return", "[", "f'{key}_{d[v]}_{v}'", "if", "isinstance", "(", "d", "[", "v", "]", ",", "str", ")", "else", "f'{key}_{d[v][-1]}_{v}'", "for", "v", "in", "range", "(", "len", "(", "d", ")", ")", "]", "\n", "\n", "", "affection_results", "=", "[", "]", "\n", "with", "tqdm", "(", "range", "(", "len", "(", "val_loader", ".", "dataset", ")", "//", "opts", ".", "size", ")", ",", "desc", "=", "f'{split}-{opts.rank}'", ")", "as", "tq", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "            ", "qids", "=", "batch", "[", "'qids'", "]", "\n", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'qids'", "]", "\n", "\n", "# select_masks, atts, composition_gates = composition", "\n", "if", "batch", "[", "'input_ids'", "]", ".", "dim", "(", ")", "==", "3", ":", "\n", "                ", "input_ids", "=", "torch", ".", "gather", "(", "batch", "[", "'input_ids'", "]", "[", "1", "]", ",", "dim", "=", "1", ",", "index", "=", "batch", "[", "'gather_index'", "]", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "input_ids", "=", "torch", ".", "gather", "(", "batch", "[", "'input_ids'", "]", ",", "dim", "=", "1", ",", "index", "=", "batch", "[", "'gather_index'", "]", ")", "\n", "\n", "", "_", ",", "over_logits", ",", "select_masks", ",", "sentiment_logits", "=", "model", "(", "\n", "**", "batch", ",", "targets", "=", "None", ",", "compute_loss", "=", "False", ")", "\n", "\n", "idiom_targets", "=", "targets", "[", ":", ",", "0", "]", "\n", "sentiment_targets", "=", "targets", "[", ":", ",", "1", "]", "\n", "\n", "sentiment_score", "+=", "(", "\n", "sentiment_logits", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "[", "1", "]", "==", "sentiment_targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "if", "over_logits", "is", "not", "None", ":", "\n", "                ", "loss", "=", "F", ".", "cross_entropy", "(", "over_logits", ",", "idiom_targets", ",", "reduction", "=", "'sum'", ")", "\n", "val_loss", "+=", "loss", ".", "item", "(", ")", "\n", "# tot_score += (scores.max(dim=-1, keepdim=False)[1] == idiom_targets).sum().item()", "\n", "max_prob", ",", "max_idx", "=", "over_logits", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "\n", "\n", "options", "=", "[", "val_loader", ".", "dataset", ".", "id2idiom", "[", "o", "]", "for", "o", "in", "val_loader", ".", "dataset", ".", "enlarged_candidates", "]", "\n", "for", "j", ",", "(", "qid", ",", "inp", ",", "position", ",", "answer", ")", "in", "enumerate", "(", "zip", "(", "qids", ",", "\n", "# idiom_targets,", "\n", "input_ids", ",", "\n", "# batch['option_ids'],", "\n", "batch", "[", "'positions'", "]", ",", "\n", "max_idx", ")", ")", ":", "\n", "# g = over_logits[j].cpu().numpy()", "\n", "# top_k = np.argsort(-g)", "\n", "# val_mrr += 1 / (1 + np.argwhere(top_k == target.item()).item())", "\n", "\n", "                    ", "example", "=", "val_loader", ".", "dataset", ".", "db", "[", "qid", "]", "\n", "idiom", "=", "val_loader", ".", "dataset", ".", "id2idiom", "[", "example", "[", "'idiom'", "]", "]", "\n", "# idiom = options[target.item()]", "\n", "affection_results", ".", "append", "(", "\n", "[", "idiom", "]", "+", "sentiment_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", ")", "\n", "if", "i", "%", "1000", "==", "0", ":", "\n", "                        ", "g", "=", "over_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "top_k", "=", "np", ".", "argsort", "(", "-", "g", ")", "[", ":", "5", "]", "\n", "print", "(", "qid", ",", "\n", "[", "options", "[", "k", "]", "for", "k", "in", "top_k", "]", ",", "\n", "idiom", ")", "\n", "# print(len(select_masks), atts.size())", "\n", "if", "select_masks", "is", "not", "None", ":", "\n", "                            ", "s_masks", "=", "[", "select_mask", "[", "j", "]", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "for", "select_mask", "in", "select_masks", "]", "\n", "# s_att = atts[j].cpu().numpy().tolist()", "\n", "\n", "# tokens = val_loader.dataset.tokenizer.convert_ids_to_tokens(inp)", "\n", "# start = tokens.index(val_loader.dataset.tokenizer.mask_token)", "\n", "# tokens[position:position + len(idiom)] = list(idiom)", "\n", "tokens", "=", "val_loader", ".", "dataset", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "\n", "val_loader", ".", "dataset", ".", "idiom_input_ids", "[", "qid", "]", ")", "\n", "# print(tokens, s_masks, s_att, composition_gates[j].sum())", "\n", "print", "(", "tokens", ",", "s_masks", ")", "\n", "try", ":", "\n", "                                ", "tree", "=", "Tree", "(", "' '", ".", "join", "(", "tokens", ")", ",", "idiom2tree", "(", "tokens", ",", "s_masks", ")", ")", "\n", "print", "(", "TreePrettyPrinter", "(", "tree", ")", ".", "text", "(", "unicodelines", "=", "True", ")", ")", "\n", "", "except", ":", "\n", "                                ", "pass", "\n", "\n", "", "", "predictions", "=", "{", "\n", "# \"coarse emotion\": {", "\n", "#     \"target\": calo_inverse_mapping['coarse_emotion'].get(coarse_emotion_targets[j].item(),", "\n", "#                                                          '\u65e0'),", "\n", "#     \"predictions\": {calo_inverse_mapping['coarse_emotion'][k]: v for k, v in", "\n", "#                     enumerate(coarse_emotion_logits[j].cpu().numpy().tolist())}", "\n", "# },", "\n", "\"sentiment\"", ":", "{", "\n", "\"target\"", ":", "idioms_inverse_mapping", "[", "'sentiment'", "]", ".", "get", "(", "sentiment_targets", "[", "j", "]", ".", "item", "(", ")", ",", "'\u65e0'", ")", ",", "\n", "\"predictions\"", ":", "{", "idioms_inverse_mapping", "[", "'sentiment'", "]", "[", "k", "]", ":", "v", "for", "k", ",", "v", "in", "\n", "enumerate", "(", "sentiment_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "}", "\n", "}", "\n", "}", "\n", "pprint", "(", "predictions", ")", "\n", "\n", "", "", "answers", "=", "max_idx", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "results", ".", "extend", "(", "zip", "(", "qids", ",", "answers", ")", ")", "\n", "", "else", ":", "\n", "                ", "for", "j", ",", "(", "qid", ",", "inp", ",", "position", ")", "in", "enumerate", "(", "zip", "(", "qids", ",", "input_ids", ",", "\n", "# batch['option_ids'],", "\n", "batch", "[", "'positions'", "]", ",", "\n", ")", ")", ":", "\n", "# options = [val_loader.dataset.id2idiom[o.item()] for o in option_ids]", "\n", "                    ", "example", "=", "val_loader", ".", "dataset", ".", "db", "[", "qid", "]", "\n", "idiom", "=", "val_loader", ".", "dataset", ".", "id2idiom", "[", "example", "[", "'idiom'", "]", "]", "\n", "affection_results", ".", "append", "(", "\n", "[", "idiom", "]", "+", "sentiment_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", ")", "\n", "if", "i", "%", "1000", "==", "0", ":", "\n", "                        ", "print", "(", "qid", ",", "\n", "idiom", ")", "\n", "if", "select_masks", "is", "not", "None", ":", "\n", "                            ", "s_masks", "=", "[", "select_mask", "[", "j", "]", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "for", "select_mask", "in", "select_masks", "]", "\n", "tokens", "=", "val_loader", ".", "dataset", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "\n", "val_loader", ".", "dataset", ".", "idiom_input_ids", "[", "qid", "]", ")", "\n", "# print(tokens, s_masks, s_att, composition_gates[j].sum())", "\n", "print", "(", "tokens", ",", "s_masks", ")", "\n", "try", ":", "\n", "                                ", "tree", "=", "Tree", "(", "' '", ".", "join", "(", "tokens", ")", ",", "idiom2tree", "(", "tokens", ",", "s_masks", ")", ")", "\n", "print", "(", "TreePrettyPrinter", "(", "tree", ")", ".", "text", "(", "unicodelines", "=", "True", ")", ")", "\n", "", "except", ":", "\n", "                                ", "pass", "\n", "\n", "", "", "predictions", "=", "{", "\n", "\"sentiment\"", ":", "{", "\n", "\"target\"", ":", "idioms_inverse_mapping", "[", "'sentiment'", "]", ".", "get", "(", "sentiment_targets", "[", "j", "]", ".", "item", "(", ")", ",", "'\u65e0'", ")", ",", "\n", "\"predictions\"", ":", "{", "idioms_inverse_mapping", "[", "'sentiment'", "]", "[", "k", "]", ":", "v", "for", "k", ",", "v", "in", "\n", "enumerate", "(", "sentiment_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", ")", "}", "\n", "}", "\n", "}", "\n", "pprint", "(", "predictions", ")", "\n", "\n", "", "", "", "n_ex", "+=", "len", "(", "qids", ")", "\n", "tq", ".", "update", "(", "len", "(", "qids", ")", ")", "\n", "\n", "", "", "if", "results", ":", "\n", "        ", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank{opts.rank}.csv'", "\n", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "for", "id_", ",", "ans", "in", "results", ":", "\n", "                ", "f", ".", "write", "(", "f'{id_},{ans}\\n'", ")", "\n", "\n", "", "", "", "header", "=", "[", "'idiom'", "]", "+", "get_header", "(", "'sentiment'", ")", "\n", "if", "affection_results", ":", "\n", "        ", "out_file", "=", "f'{opts.output_dir}/results/{split}_affection_results_{global_step}_rank{opts.rank}.csv'", "\n", "pd", ".", "DataFrame", "(", "affection_results", ",", "columns", "=", "header", ")", ".", "to_csv", "(", "out_file", ")", "\n", "\n", "", "val_loss", "=", "sum", "(", "all_gather_list", "(", "val_loss", ")", ")", "\n", "val_mrr", "=", "sum", "(", "all_gather_list", "(", "val_mrr", ")", ")", "\n", "\n", "val_sentiment_score", "=", "sum", "(", "all_gather_list", "(", "sentiment_score", ")", ")", "\n", "\n", "n_ex", "=", "sum", "(", "all_gather_list", "(", "n_ex", ")", ")", "\n", "tot_time", "=", "time", "(", ")", "-", "st", "\n", "\n", "val_loss", "/=", "n_ex", "\n", "val_mrr", "=", "val_mrr", "/", "n_ex", "\n", "val_sentiment_score", "=", "val_sentiment_score", "/", "n_ex", "\n", "\n", "if", "results", ":", "\n", "        ", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}.csv'", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "out_file", ")", ":", "\n", "            ", "with", "open", "(", "out_file", ",", "'wb'", ")", "as", "g", ":", "\n", "                ", "for", "f", "in", "glob", ".", "glob", "(", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank*.csv'", ")", ":", "\n", "                    ", "shutil", ".", "copyfileobj", "(", "open", "(", "f", ",", "'rb'", ")", ",", "g", ")", "\n", "\n", "", "", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "txt_db", "=", "os", ".", "path", ".", "join", "(", "'/txt'", ",", "\n", "intermediate_dir", "(", "opts", ".", "pretrained_model_name_or_path", ")", ",", "\n", "getattr", "(", "opts", ",", "f'{split}_txt_db'", ")", ")", "\n", "val_acc", "=", "judge", "(", "out_file", ",", "f'{txt_db}/answer.csv'", ")", "\n", "\n", "", "if", "opts", ".", "rank", "==", "0", ":", "\n", "        ", "results_files", "=", "glob", ".", "glob", "(", "f'{opts.output_dir}/results/{split}_affection_results_{global_step}_rank*.csv'", ")", "\n", "new_affection_results_df", "=", "pd", ".", "concat", "(", "map", "(", "pd", ".", "read_csv", ",", "results_files", ")", ")", "\n", "idiom_num", "=", "new_affection_results_df", "[", "'idiom'", "]", ".", "unique", "(", ")", ".", "size", "\n", "idiom_wise_accs", "=", "{", "}", "\n", "for", "item", "in", "new_affection_results_df", ".", "groupby", "(", "'idiom'", ")", ".", "mean", "(", ")", ".", "reset_index", "(", ")", ".", "to_dict", "(", "orient", "=", "'records'", ")", ":", "\n", "            ", "idiom", "=", "item", "[", "'idiom'", "]", "\n", "idiom_id", "=", "val_loader", ".", "dataset", ".", "vocab", "[", "idiom", "]", "\n", "for", "sub_type", "in", "[", "'sentiment'", "]", ":", "\n", "                ", "d", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "item", ".", "items", "(", ")", "if", "k", ".", "startswith", "(", "sub_type", ")", "}", "\n", "key", "=", "max", "(", "d", ",", "key", "=", "d", ".", "get", ")", "\n", "_", ",", "pred", "=", "key", ".", "rsplit", "(", "'_'", ",", "1", ")", "\n", "target", "=", "val_loader", ".", "dataset", ".", "sentiments", "[", "idiom_id", "]", "\n", "idiom_wise_accs", ".", "setdefault", "(", "sub_type", ",", "0", ")", "\n", "idiom_wise_accs", "[", "sub_type", "]", "+=", "(", "int", "(", "pred", ")", "==", "target", ")", "/", "idiom_num", "*", "100", "\n", "\n", "", "", "val_acc", "=", "val_sentiment_score", "\n", "\n", "val_log", "=", "{", "f'{split}/loss'", ":", "val_loss", ",", "\n", "f'{split}/acc'", ":", "val_acc", ",", "\n", "f'{split}/sentiment'", ":", "val_sentiment_score", "*", "100", ",", "\n", "f'{split}/mrr'", ":", "val_mrr", ",", "\n", "f'{split}/ex_per_s'", ":", "n_ex", "/", "tot_time", "}", "\n", "\n", "for", "k", ",", "v", "in", "idiom_wise_accs", ".", "items", "(", ")", ":", "\n", "            ", "val_log", "[", "f'{split}/{k}'", "]", "=", "v", "\n", "\n", "", "LOGGER", ".", "info", "(", "f\"validation finished in {int(tot_time)} seconds, \\n\"", "\n", "# f\"coarse emotion score: {val_coarse_emotion_score * 100:.2f}, \\n\"", "\n", "f\"sentiment score: {val_sentiment_score * 100:.2f}, \\n\"", "\n", "f\"score: {val_acc * 100:.2f}, \\n\"", "\n", "f\"idiom-wise score: {idiom_wise_accs}, \"", "\n", "f\"mrr: {val_mrr:.3f}\"", ")", "\n", "return", "val_log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_affection.evaluation": [[665, 675], ["model.eval", "data_loaders.items", "chengyubert.utils.logger.TB_LOGGER.log_scaler_dict", "model.train", "chengyubert.utils.logger.LOGGER.info", "log.update"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.log_scaler_dict", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train"], ["def", "evaluation", "(", "model", ",", "data_loaders", ":", "dict", ",", "opts", ",", "global_step", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "log", "=", "{", "}", "\n", "for", "split", ",", "loader", "in", "data_loaders", ".", "items", "(", ")", ":", "\n", "        ", "LOGGER", ".", "info", "(", "f\"Step {global_step}: start running \"", "\n", "f\"validation on {split} split...\"", ")", "\n", "log", ".", "update", "(", "validate", "[", "opts", ".", "project", "]", "(", "opts", ",", "model", ",", "loader", ",", "split", ",", "global_step", ")", ")", "\n", "", "TB_LOGGER", ".", "log_scaler_dict", "(", "log", ")", "\n", "model", ".", "train", "(", ")", "\n", "return", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_affection.get_best_ckpt": [[677, 692], ["re.compile", "glob.glob", "collections.Counter", "print", "collections.Counter.most_common", "chengyubert.data.evaluation.judge", "collections.Counter.update", "re.compile.match", "int", "os.path.join", "os.path.basename", "pat.match.group"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge"], ["", "def", "get_best_ckpt", "(", "val_data_dir", ",", "opts", ")", ":", "\n", "    ", "pat", "=", "re", ".", "compile", "(", "r'val_results_(?P<step>\\d+)_rank0.csv'", ")", "\n", "prediction_files", "=", "glob", ".", "glob", "(", "'{}/results/val_results_*_rank0.csv'", ".", "format", "(", "opts", ".", "output_dir", ")", ")", "\n", "\n", "top_files", "=", "Counter", "(", ")", "\n", "for", "f", "in", "prediction_files", ":", "\n", "        ", "acc", "=", "judge", "(", "f", ",", "os", ".", "path", ".", "join", "(", "val_data_dir", ",", "'answer.csv'", ")", ")", "\n", "top_files", ".", "update", "(", "{", "f", ":", "acc", "}", ")", "\n", "\n", "", "print", "(", "top_files", ")", "\n", "\n", "for", "f", ",", "acc", "in", "top_files", ".", "most_common", "(", "1", ")", ":", "\n", "        ", "m", "=", "pat", ".", "match", "(", "os", ".", "path", ".", "basename", "(", "f", ")", ")", "\n", "best_epoch", "=", "int", "(", "m", ".", "group", "(", "'step'", ")", ")", "\n", "return", "best_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_affection.main": [[694, 741], ["torch.device", "horovod.torch.device", "torch.cuda.set_device", "horovod.torch.cuda.set_device", "horovod.torch.rank", "horovod.torch.size", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.misc.set_random_seed", "chengyubert.data.create_dataloaders", "chengyubert.models.build_model", "chengyubert.models.build_model.to", "chengyubert.models.build_model.load_state_dict", "train_affection.evaluation", "horovod.torch.local_rank", "horovod.torch.local_rank", "ValueError", "setattr", "setattr", "opts.weights.tolist", "train_affection.train", "train_affection.get_best_ckpt", "torch.load", "horovod.torch.load", "dict", "horovod.torch.rank", "w1.tolist", "w2.tolist", "filter", "dataloaders.items"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_random_seed", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_dataloaders", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.__init__.build_model", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.get_best_ckpt"], ["", "", "def", "main", "(", "opts", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "hvd", ".", "local_rank", "(", ")", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "hvd", ".", "local_rank", "(", ")", ")", "\n", "rank", "=", "hvd", ".", "rank", "(", ")", "\n", "opts", ".", "rank", "=", "rank", "\n", "opts", ".", "size", "=", "hvd", ".", "size", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"device: {} n_gpu: {}, rank: {}, \"", "\n", "\"16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "hvd", ".", "rank", "(", ")", ",", "opts", ".", "fp16", ")", ")", "\n", "\n", "if", "opts", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, \"", "\n", "\"should be >= 1\"", ".", "format", "(", "\n", "opts", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "set_random_seed", "(", "opts", ".", "seed", ")", "\n", "\n", "# data loaders", "\n", "DatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "dataset_cls", "]", "\n", "EvalDatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "eval_dataset_cls", "]", "\n", "opts", ".", "evaluate_embedding", "=", "False", "\n", "splits", ",", "dataloaders", "=", "create_dataloaders", "(", "DatasetCls", ",", "EvalDatasetCls", ",", "opts", ")", "\n", "\n", "if", "opts", ".", "project", "==", "'calo'", ":", "\n", "        ", "setattr", "(", "opts", ",", "'weights'", ",", "(", "dataloaders", "[", "'train'", "]", ".", "dataset", ".", "fine_emotion_weights", ",", "\n", "dataloaders", "[", "'train'", "]", ".", "dataset", ".", "sentiment_weights", ")", ")", "\n", "", "else", ":", "\n", "        ", "setattr", "(", "opts", ",", "'weights'", ",", "dataloaders", "[", "'train'", "]", ".", "dataset", ".", "sentiment_weights", ")", "\n", "\n", "# Prepare model", "\n", "", "model", "=", "build_model", "(", "opts", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "opts", ".", "project", "==", "'calo'", ":", "\n", "        ", "w1", ",", "w2", "=", "opts", ".", "weights", "\n", "opts", ".", "weights", "=", "(", "w1", ".", "tolist", "(", ")", ",", "w2", ".", "tolist", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "opts", ".", "weights", "=", "opts", ".", "weights", ".", "tolist", "(", ")", "\n", "\n", "", "if", "opts", ".", "mode", "==", "'train'", ":", "\n", "        ", "best_ckpt", "=", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", "\n", "", "else", ":", "\n", "        ", "best_ckpt", "=", "get_best_ckpt", "(", "dataloaders", "[", "'val'", "]", ".", "dataset", ".", "db_dir", ",", "opts", ")", "\n", "\n", "", "best_pt", "=", "f'{opts.output_dir}/ckpt/model_step_{best_ckpt}.pt'", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "best_pt", ")", ",", "strict", "=", "False", ")", "\n", "evaluation", "(", "model", ",", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", "!=", "'train'", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "opts", ",", "best_ckpt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train": [[38, 161], ["chengyubert.utils.distributed.broadcast_tensors", "chengyubert.utils.misc.set_dropout", "chengyubert.optim.misc.build_optimizer", "torch.cuda.amp.GradScaler", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.RunningMeter", "model.train", "time.time", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.optim.misc.build_optimizer.step", "chengyubert.utils.save.save_training_meta", "chengyubert.utils.logger.TB_LOGGER.create", "tqdm.tqdm", "chengyubert.utils.save.ModelSaver", "os.makedirs", "chengyubert.utils.logger.add_log_to_file", "chengyubert.utils.misc.NoOp", "chengyubert.utils.misc.NoOp", "len", "enumerate", "chengyubert.utils.logger.LOGGER.info", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "targets.size", "torch.cuda.amp.GradScaler.scale().backward", "chengyubert.utils.logger.RunningMeter.", "model.parameters", "torch.cuda.amp.autocast", "model", "loss.mean.mean", "chengyubert.utils.distributed.all_reduce_and_rescale_tensors", "loss.mean.item", "chengyubert.optim.get_lr_sched", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.logger.RunningMeter", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "chengyubert.utils.logger.TB_LOGGER.step", "torch.cuda.amp.GradScaler.step", "torch.cuda.amp.GradScaler.update", "chengyubert.optim.misc.build_optimizer.zero_grad", "chengyubert.utils.misc.NoOp.update", "torch.cuda.amp.GradScaler.scale", "float", "torch.cuda.amp.GradScaler.unscale_", "torch.nn.utils.clip_grad_norm_", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "sum", "int", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.TB_LOGGER.add_scalar", "train_pretrain.evaluation", "chengyubert.utils.misc.NoOp.save", "model.parameters", "sum", "len", "model.parameters", "chengyubert.utils.distributed.all_gather_list", "dict", "chengyubert.utils.misc.NoOp.set_description", "filter", "time.time", "dataloaders.items", "x[].startswith"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.broadcast_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_dropout", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.misc.build_optimizer", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.save_training_meta", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.create", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.add_log_to_file", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.GatherLayer.backward", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_reduce_and_rescale_tensors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.get_lr_sched", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list"], ["def", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", ":", "\n", "# make sure every process has same model parameters in the beginning", "\n", "    ", "broadcast_tensors", "(", "[", "p", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", ",", "0", ")", "\n", "set_dropout", "(", "model", ",", "opts", ".", "dropout", ")", "\n", "\n", "# Prepare optimizer", "\n", "optimizer", "=", "build_optimizer", "(", "model", ",", "opts", ")", "\n", "scaler", "=", "GradScaler", "(", ")", "\n", "\n", "global_step", "=", "0", "\n", "if", "opts", ".", "rank", "==", "0", ":", "\n", "        ", "save_training_meta", "(", "opts", ")", "\n", "TB_LOGGER", ".", "create", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ")", ")", "\n", "pbar", "=", "tqdm", "(", "total", "=", "opts", ".", "num_train_steps", ",", "desc", "=", "opts", ".", "model", ")", "\n", "model_saver", "=", "ModelSaver", "(", "join", "(", "opts", ".", "output_dir", ",", "'ckpt'", ")", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'results'", ")", ",", "exist_ok", "=", "True", ")", "# store val predictions", "\n", "add_log_to_file", "(", "join", "(", "opts", ".", "output_dir", ",", "'log'", ",", "'log.txt'", ")", ")", "\n", "", "else", ":", "\n", "        ", "LOGGER", ".", "disabled", "=", "True", "\n", "pbar", "=", "NoOp", "(", ")", "\n", "model_saver", "=", "NoOp", "(", ")", "\n", "\n", "", "LOGGER", ".", "info", "(", "f\"***** Running training with {opts.n_gpu} GPUs *****\"", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "dataloaders", "[", "'train'", "]", ".", "dataset", ")", ")", "\n", "LOGGER", ".", "info", "(", "\"  Batch size = %d\"", ",", "opts", ".", "train_batch_size", ")", "\n", "LOGGER", ".", "info", "(", "\"  Accumulate steps = %d\"", ",", "opts", ".", "gradient_accumulation_steps", ")", "\n", "LOGGER", ".", "info", "(", "\"  Num steps = %d\"", ",", "opts", ".", "num_train_steps", ")", "\n", "\n", "running_loss", "=", "RunningMeter", "(", "'loss'", ")", "\n", "model", ".", "train", "(", ")", "\n", "n_examples", "=", "0", "\n", "n_epoch", "=", "0", "\n", "best_ckpt", "=", "0", "\n", "best_eval", "=", "0", "\n", "start", "=", "time", "(", ")", "\n", "# quick hack for amp delay_unscale bug", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "while", "True", ":", "\n", "        ", "for", "step", ",", "batch", "in", "enumerate", "(", "dataloaders", "[", "'train'", "]", ")", ":", "\n", "            ", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'gather_index'", "]", "\n", "n_examples", "+=", "targets", ".", "size", "(", "0", ")", "\n", "\n", "with", "autocast", "(", ")", ":", "\n", "                ", "loss", "=", "model", "(", "**", "batch", ",", "compute_loss", "=", "True", ")", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "delay_unscale", "=", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "!=", "0", "\n", "scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", ")", "\n", "if", "not", "delay_unscale", ":", "\n", "# gather gradients from every processes", "\n", "# do this before unscaling to make sure every process uses", "\n", "# the same gradient scale", "\n", "                ", "grads", "=", "[", "p", ".", "grad", ".", "data", "for", "p", "in", "model", ".", "parameters", "(", ")", "\n", "if", "p", ".", "requires_grad", "and", "p", ".", "grad", "is", "not", "None", "]", "\n", "all_reduce_and_rescale_tensors", "(", "grads", ",", "float", "(", "1", ")", ")", "\n", "\n", "", "running_loss", "(", "loss", ".", "item", "(", ")", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "opts", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "global_step", "+=", "1", "\n", "\n", "# learning rate scheduling", "\n", "lr_this_step", "=", "get_lr_sched", "(", "global_step", ",", "opts", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "TB_LOGGER", ".", "add_scalar", "(", "'lr'", ",", "lr_this_step", ",", "global_step", ")", "\n", "\n", "# log loss", "\n", "losses", "=", "all_gather_list", "(", "running_loss", ")", "\n", "running_loss", "=", "RunningMeter", "(", "\n", "'loss'", ",", "sum", "(", "l", ".", "val", "for", "l", "in", "losses", ")", "/", "len", "(", "losses", ")", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'loss'", ",", "running_loss", ".", "val", ",", "global_step", ")", "\n", "TB_LOGGER", ".", "step", "(", ")", "\n", "\n", "# update model params", "\n", "if", "opts", ".", "grad_norm", "!=", "-", "1", ":", "\n", "# Unscales the gradients of optimizer's assigned params in-place", "\n", "                    ", "scaler", ".", "unscale_", "(", "optimizer", ")", "\n", "grad_norm", "=", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "opts", ".", "grad_norm", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'grad_norm'", ",", "grad_norm", ",", "global_step", ")", "\n", "\n", "# scaler.step() first unscales gradients of the optimizer's params.", "\n", "# If gradients don't contain infs/NaNs, optimizer.step() is then called,", "\n", "# otherwise, optimizer.step() is skipped.", "\n", "", "scaler", ".", "step", "(", "optimizer", ")", "\n", "\n", "# Updates the scale for next iteration.", "\n", "scaler", ".", "update", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "if", "global_step", "%", "100", "==", "0", ":", "\n", "# monitor training throughput", "\n", "                    ", "tot_ex", "=", "sum", "(", "all_gather_list", "(", "n_examples", ")", ")", "\n", "ex_per_sec", "=", "int", "(", "tot_ex", "/", "(", "time", "(", ")", "-", "start", ")", ")", "\n", "LOGGER", ".", "info", "(", "f'{opts.model}: {n_epoch}-{global_step}: '", "\n", "f'{tot_ex} examples trained at '", "\n", "f'{ex_per_sec} ex/s '", "\n", "f'best_acc-{best_eval * 100:.2f}'", ")", "\n", "TB_LOGGER", ".", "add_scalar", "(", "'perf/ex_per_s'", ",", "\n", "ex_per_sec", ",", "global_step", ")", "\n", "\n", "", "if", "global_step", "%", "opts", ".", "valid_steps", "==", "0", ":", "\n", "                    ", "log", "=", "evaluation", "(", "model", ",", "\n", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", ".", "startswith", "(", "'val'", ")", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "\n", "opts", ",", "global_step", ")", "\n", "log_eval", "=", "log", "[", "'val/acc'", "]", "\n", "if", "log_eval", ">", "best_eval", ":", "\n", "                        ", "best_ckpt", "=", "global_step", "\n", "best_eval", "=", "log_eval", "\n", "pbar", ".", "set_description", "(", "f'{opts.model}: {n_epoch}-{best_ckpt} best_acc-{best_eval * 100:.2f}'", ")", "\n", "", "model_saver", ".", "save", "(", "model", ",", "global_step", ")", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "                ", "break", "\n", "", "", "if", "global_step", ">=", "opts", ".", "num_train_steps", ":", "\n", "            ", "break", "\n", "", "n_epoch", "+=", "1", "\n", "LOGGER", ".", "info", "(", "f\"Step {global_step}: finished {n_epoch} epochs\"", ")", "\n", "# if n_epoch >= opts.num_train_epochs:", "\n", "#     break", "\n", "", "return", "best_ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.validate": [[163, 230], ["torch.no_grad", "horovod.torch.no_grad", "time.time", "sum", "sum", "sum", "sum", "os.path.join", "chengyubert.data.evaluation.judge", "chengyubert.utils.logger.LOGGER.info", "tqdm.tqdm", "enumerate", "open", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.distributed.all_gather_list", "chengyubert.utils.distributed.all_gather_list", "time.time", "os.path.isfile", "chengyubert.utils.distributed.all_gather_list", "chengyubert.data.intermediate_dir", "getattr", "range", "model", "torch.nn.functional.cross_entropy", "F.cross_entropy.item", "scores.max", "max_idx.cpu().tolist", "torch.gather().cpu().numpy", "horovod.torch.gather().cpu().numpy", "enumerate", "results.extend", "len", "tq.update", "f.write", "open", "glob.glob", "zip", "over_logits[].cpu().numpy", "numpy.argsort", "zip", "len", "shutil.copyfileobj", "int", "len", "max_idx.cpu", "torch.gather().cpu", "horovod.torch.gather().cpu", "open", "over_logits[].cpu", "numpy.argwhere().item", "torch.gather", "horovod.torch.gather", "numpy.argwhere", "scores.max", "torch.gather().cpu().numpy.unsqueeze"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.intermediate_dir"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "validate", "(", "opts", ",", "model", ",", "val_loader", ",", "split", ",", "global_step", ")", ":", "\n", "    ", "val_loss", "=", "0", "\n", "tot_score", "=", "0", "\n", "n_ex", "=", "0", "\n", "val_mrr", "=", "0", "\n", "st", "=", "time", "(", ")", "\n", "results", "=", "[", "]", "\n", "with", "tqdm", "(", "range", "(", "len", "(", "val_loader", ".", "dataset", ")", "//", "opts", ".", "size", ")", ",", "desc", "=", "f'{split}-{opts.rank}'", ")", "as", "tq", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "            ", "qids", "=", "batch", "[", "'qids'", "]", "\n", "targets", "=", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'targets'", "]", "\n", "del", "batch", "[", "'qids'", "]", "\n", "del", "batch", "[", "'gather_index'", "]", "\n", "\n", "scores", ",", "over_logits", "=", "model", "(", "**", "batch", ",", "targets", "=", "None", ",", "compute_loss", "=", "False", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "scores", ",", "targets", ",", "reduction", "=", "'sum'", ")", "\n", "val_loss", "+=", "loss", ".", "item", "(", ")", "\n", "tot_score", "+=", "(", "scores", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "[", "1", "]", "==", "targets", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "max_prob", ",", "max_idx", "=", "scores", ".", "max", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "\n", "answers", "=", "max_idx", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "targets", "=", "torch", ".", "gather", "(", "batch", "[", "'option_ids'", "]", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "j", ",", "(", "qid", ",", "target", ")", "in", "enumerate", "(", "zip", "(", "qids", ",", "targets", ")", ")", ":", "\n", "                ", "g", "=", "over_logits", "[", "j", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "top_k", "=", "np", ".", "argsort", "(", "-", "g", ")", "\n", "val_mrr", "+=", "1", "/", "(", "1", "+", "np", ".", "argwhere", "(", "top_k", "==", "target", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "results", ".", "extend", "(", "zip", "(", "qids", ",", "answers", ")", ")", "\n", "n_ex", "+=", "len", "(", "qids", ")", "\n", "tq", ".", "update", "(", "len", "(", "qids", ")", ")", "\n", "\n", "", "", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank{opts.rank}.csv'", "\n", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "id_", ",", "ans", "in", "results", ":", "\n", "            ", "f", ".", "write", "(", "f'{id_},{ans}\\n'", ")", "\n", "\n", "", "", "val_loss", "=", "sum", "(", "all_gather_list", "(", "val_loss", ")", ")", "\n", "val_mrr", "=", "sum", "(", "all_gather_list", "(", "val_mrr", ")", ")", "\n", "# tot_score = sum(all_gather_list(tot_score))", "\n", "n_ex", "=", "sum", "(", "all_gather_list", "(", "n_ex", ")", ")", "\n", "tot_time", "=", "time", "(", ")", "-", "st", "\n", "\n", "val_loss", "/=", "n_ex", "\n", "val_mrr", "=", "val_mrr", "/", "n_ex", "\n", "\n", "out_file", "=", "f'{opts.output_dir}/results/{split}_results_{global_step}.csv'", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "out_file", ")", ":", "\n", "        ", "with", "open", "(", "out_file", ",", "'wb'", ")", "as", "g", ":", "\n", "            ", "for", "f", "in", "glob", ".", "glob", "(", "f'{opts.output_dir}/results/{split}_results_{global_step}_rank*.csv'", ")", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "open", "(", "f", ",", "'rb'", ")", ",", "g", ")", "\n", "\n", "", "", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "txt_db", "=", "os", ".", "path", ".", "join", "(", "'/txt'", ",", "\n", "intermediate_dir", "(", "opts", ".", "pretrained_model_name_or_path", ")", ",", "\n", "getattr", "(", "opts", ",", "f'{split}_txt_db'", ")", ")", "\n", "val_acc", "=", "judge", "(", "out_file", ",", "f'{txt_db}/answer.csv'", ")", "\n", "val_log", "=", "{", "f'{split}/loss'", ":", "val_loss", ",", "\n", "f'{split}/acc'", ":", "val_acc", ",", "\n", "f'{split}/mrr'", ":", "val_mrr", ",", "\n", "f'{split}/ex_per_s'", ":", "n_ex", "/", "tot_time", "}", "\n", "LOGGER", ".", "info", "(", "f\"validation finished in {int(tot_time)} seconds, \"", "\n", "f\"score: {val_acc * 100:.2f}, \"", "\n", "f\"mrr: {val_mrr:.3f}\"", ")", "\n", "return", "val_log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluate_embeddings_recall": [[232, 261], ["numpy.array", "len", "tqdm.tqdm", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "chengyu_synonyms_dict.items", "any", "numpy.linalg.norm().argsort", "range", "recall_at_k_cosine.setdefault", "sum", "recall_at_k_norm.setdefault", "sum", "len", "numpy.linalg.norm", "sklearn.metrics.pairwise.cosine_similarity", "embeddings[].reshape"], "function", ["None"], ["", "def", "evaluate_embeddings_recall", "(", "embeddings", ",", "chengyu_vocab", ",", "chengyu_synonyms_dict", ")", ":", "\n", "    ", "iw", "=", "[", "k", "for", "k", "in", "embeddings", "]", "\n", "\n", "vectors", "=", "np", ".", "array", "(", "[", "embeddings", "[", "iw", "[", "i", "]", "]", "for", "i", "in", "range", "(", "len", "(", "iw", ")", ")", "]", ")", "\n", "cnt", "=", "0", "\n", "recall_at_k_cosine", "=", "{", "}", "\n", "recall_at_k_norm", "=", "{", "}", "\n", "k_list", "=", "[", "1", ",", "3", ",", "5", ",", "10", "]", "\n", "total", "=", "len", "(", "chengyu_synonyms_dict", ")", "\n", "for", "w", ",", "wl", "in", "tqdm", "(", "chengyu_synonyms_dict", ".", "items", "(", ")", ")", ":", "\n", "        ", "if", "w", "in", "embeddings", "and", "any", "(", "[", "x", "in", "embeddings", "for", "x", "in", "wl", "]", ")", ":", "\n", "            ", "cnt", "+=", "1", "\n", "\n", "cosine_distances", "=", "(", "1", "-", "cosine_similarity", "(", "embeddings", "[", "w", "]", ".", "reshape", "(", "1", ",", "-", "1", ")", ",", "vectors", ")", "[", "0", "]", ")", ".", "argsort", "(", ")", "\n", "norm_distances", "=", "np", ".", "linalg", ".", "norm", "(", "vectors", "-", "embeddings", "[", "w", "]", ",", "axis", "=", "1", ")", ".", "argsort", "(", ")", "\n", "cids", "=", "[", "idx", "for", "idx", "in", "cosine_distances", "if", "iw", "[", "idx", "]", "in", "chengyu_vocab", "]", "\n", "nids", "=", "[", "idx", "for", "idx", "in", "norm_distances", "if", "iw", "[", "idx", "]", "in", "chengyu_vocab", "]", "\n", "for", "k", "in", "k_list", ":", "\n", "                ", "top_ids", "=", "cids", "[", "1", ":", "k", "+", "1", "]", "\n", "recall_at_k_cosine", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "recall_at_k_cosine", "[", "k", "]", "+=", "sum", "(", "[", "1", "for", "idx", "in", "top_ids", "if", "iw", "[", "idx", "]", "in", "wl", "]", ")", "\n", "\n", "top_ids", "=", "nids", "[", "1", ":", "k", "+", "1", "]", "\n", "recall_at_k_norm", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "recall_at_k_norm", "[", "k", "]", "+=", "sum", "(", "[", "1", "for", "idx", "in", "top_ids", "if", "iw", "[", "idx", "]", "in", "wl", "]", ")", "\n", "", "", "", "LOGGER", ".", "info", "(", "f'{cnt} word pairs appeared in the training dictionary , total word pairs {total}'", ")", "\n", "LOGGER", ".", "info", "(", "recall_at_k_cosine", ")", "\n", "LOGGER", ".", "info", "(", "recall_at_k_norm", ")", "\n", "return", "cnt", ",", "total", ",", "recall_at_k_cosine", ",", "recall_at_k_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation": [[263, 278], ["model.eval", "data_loaders.items", "chengyubert.utils.logger.TB_LOGGER.log_scaler_dict", "model.train", "chengyubert.utils.logger.LOGGER.info", "log.update", "train_pretrain.validate", "model.idiom_embedding.weight.detach().cpu().numpy", "train_pretrain.evaluate_embeddings_recall", "model.idiom_embedding.weight.detach().cpu", "loader.dataset.chengyu_vocab.items", "model.idiom_embedding.weight.detach"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.log_scaler_dict", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.validate", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluate_embeddings_recall"], ["", "def", "evaluation", "(", "model", ",", "data_loaders", ":", "dict", ",", "opts", ",", "global_step", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "log", "=", "{", "}", "\n", "for", "split", ",", "loader", "in", "data_loaders", ".", "items", "(", ")", ":", "\n", "        ", "LOGGER", ".", "info", "(", "f\"Step {global_step}: start running \"", "\n", "f\"validation on {split} split...\"", ")", "\n", "log", ".", "update", "(", "validate", "(", "opts", ",", "model", ",", "loader", ",", "split", ",", "global_step", ")", ")", "\n", "if", "split", "==", "'val'", "and", "opts", ".", "evaluate_embedding", ":", "\n", "            ", "embeddings_np", "=", "model", ".", "idiom_embedding", ".", "weight", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "embeddings", "=", "{", "k", ":", "embeddings_np", "[", "v", "]", "for", "k", ",", "v", "in", "loader", ".", "dataset", ".", "chengyu_vocab", ".", "items", "(", ")", "}", "\n", "evaluate_embeddings_recall", "(", "embeddings", ",", "loader", ".", "dataset", ".", "chengyu_vocab", ",", "\n", "loader", ".", "dataset", ".", "chengyu_synonyms_dict", ")", "\n", "", "", "TB_LOGGER", ".", "log_scaler_dict", "(", "log", ")", "\n", "model", ".", "train", "(", ")", "\n", "return", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.get_best_ckpt": [[280, 295], ["re.compile", "glob.glob", "collections.Counter", "print", "collections.Counter.most_common", "chengyubert.data.evaluation.judge", "collections.Counter.update", "re.compile.match", "int", "os.path.join", "os.path.basename", "pat.match.group"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge"], ["", "def", "get_best_ckpt", "(", "val_data_dir", ",", "opts", ")", ":", "\n", "    ", "pat", "=", "re", ".", "compile", "(", "r'val_results_(?P<step>\\d+)_rank0.csv'", ")", "\n", "prediction_files", "=", "glob", ".", "glob", "(", "'{}/results/val_results_*_rank0.csv'", ".", "format", "(", "opts", ".", "output_dir", ")", ")", "\n", "\n", "top_files", "=", "Counter", "(", ")", "\n", "for", "f", "in", "prediction_files", ":", "\n", "        ", "acc", "=", "judge", "(", "f", ",", "os", ".", "path", ".", "join", "(", "val_data_dir", ",", "'answer.csv'", ")", ")", "\n", "top_files", ".", "update", "(", "{", "f", ":", "acc", "}", ")", "\n", "\n", "", "print", "(", "top_files", ")", "\n", "\n", "for", "f", ",", "acc", "in", "top_files", ".", "most_common", "(", "1", ")", ":", "\n", "        ", "m", "=", "pat", ".", "match", "(", "os", ".", "path", ".", "basename", "(", "f", ")", ")", "\n", "best_epoch", "=", "int", "(", "m", ".", "group", "(", "'step'", ")", ")", "\n", "return", "best_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.main": [[297, 344], ["torch.device", "horovod.torch.device", "torch.cuda.set_device", "horovod.torch.cuda.set_device", "horovod.torch.rank", "horovod.torch.size", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.misc.set_random_seed", "chengyubert.data.create_dataloaders", "chengyubert.models.build_model", "chengyubert.models.build_model.to", "sum", "train_pretrain.evaluation", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "horovod.torch.local_rank", "horovod.torch.local_rank", "ValueError", "train_pretrain.train", "chengyubert.utils.distributed.all_gather_list", "chengyubert.models.build_model.load_state_dict", "dict", "horovod.torch.rank", "train_pretrain.get_best_ckpt", "torch.load", "horovod.torch.load", "filter", "itertools.chain", "os.makedirs", "dataloaders.items", "os.path.join", "format", "format"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_random_seed", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_dataloaders", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.__init__.build_model", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.evaluation", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.train", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.get_best_ckpt"], ["", "", "def", "main", "(", "opts", ")", ":", "\n", "    ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "hvd", ".", "local_rank", "(", ")", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "hvd", ".", "local_rank", "(", ")", ")", "\n", "rank", "=", "hvd", ".", "rank", "(", ")", "\n", "opts", ".", "rank", "=", "rank", "\n", "opts", ".", "size", "=", "hvd", ".", "size", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"device: {} n_gpu: {}, rank: {}, \"", "\n", "\"16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "hvd", ".", "rank", "(", ")", ",", "opts", ".", "fp16", ")", ")", "\n", "\n", "if", "opts", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, \"", "\n", "\"should be >= 1\"", ".", "format", "(", "\n", "opts", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "set_random_seed", "(", "opts", ".", "seed", ")", "\n", "\n", "# data loaders", "\n", "DatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "dataset_cls", "]", "\n", "EvalDatasetCls", "=", "DATA_REGISTRY", "[", "opts", ".", "eval_dataset_cls", "]", "\n", "splits", ",", "dataloaders", "=", "create_dataloaders", "(", "DatasetCls", ",", "EvalDatasetCls", ",", "opts", ")", "\n", "opts", ".", "evaluate_embedding", "=", "False", "\n", "\n", "# Prepare model", "\n", "model", "=", "build_model", "(", "opts", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "opts", ".", "mode", "==", "'train'", ":", "\n", "        ", "best_ckpt", "=", "train", "(", "model", ",", "dataloaders", ",", "opts", ")", "\n", "", "elif", "opts", ".", "mode", "==", "'eval'", ":", "\n", "        ", "best_ckpt", "=", "None", "\n", "if", "opts", ".", "rank", "==", "0", ":", "\n", "            ", "os", ".", "makedirs", "(", "join", "(", "opts", ".", "output_dir", ",", "'results'", ")", ",", "exist_ok", "=", "True", ")", "# store val predictions", "\n", "", "", "else", ":", "\n", "        ", "best_ckpt", "=", "get_best_ckpt", "(", "dataloaders", "[", "'val'", "]", ".", "dataset", ".", "db_dir", ",", "opts", ")", "\n", "\n", "", "sum", "(", "all_gather_list", "(", "opts", ".", "rank", ")", ")", "\n", "\n", "if", "best_ckpt", "is", "not", "None", ":", "\n", "        ", "best_pt", "=", "f'{opts.output_dir}/ckpt/model_step_{best_ckpt}.pt'", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "best_pt", ")", ",", "strict", "=", "False", ")", "\n", "", "log", "=", "evaluation", "(", "model", ",", "dict", "(", "filter", "(", "lambda", "x", ":", "x", "[", "0", "]", "!=", "'train'", ",", "dataloaders", ".", "items", "(", ")", ")", ")", ",", "opts", ",", "best_ckpt", ")", "\n", "splits", "=", "[", "'val'", ",", "'test'", ",", "'ran'", ",", "'sim'", ",", "'out'", "]", "\n", "LOGGER", ".", "info", "(", "'\\t'", ".", "join", "(", "splits", ")", ")", "\n", "LOGGER", ".", "info", "(", "'\\t'", ".", "join", "(", "chain", "(", "\n", "[", "format", "(", "log", "[", "f'{split}/acc'", "]", ",", "\"0.6f\"", ")", "for", "split", "in", "splits", "]", ",", "\n", "[", "format", "(", "log", "[", "f'{split}/mrr'", "]", ",", "\"0.6f\"", ")", "for", "split", "in", "splits", "]", "\n", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.dump_from_es.dump_idiom": [[28, 181], ["enumerate", "tqdm.tqdm", "tmp_context.index", "tmp_context.rindex", "tmp_context[].replace().replace", "records.append", "jsonlines.open", "word.split", "word.split", "word.split", "word.split", "[].append", "nlp", "doc[].tag_.startswith", "elasticsearch.helpers.scan", "dump_from_es.get_contexts", "f.write", "word.split", "word.split", "word.split", "word.split", "nlp", "doc[].tag_.startswith", "tmp_context[].replace", "tqdm.tqdm", "word.split", "word.split", "[].append", "[].append", "toker.tokenize", "word.replace", "[].append", "[].append", "word.replace", "word.replace", "word.replace", "word.replace"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.dump_from_es.get_contexts", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.tokenize"], ["def", "dump_idiom", "(", "word", ",", "dump_file", ",", "index", ")", ":", "\n", "    ", "body", "=", "{", "\n", "\"_source\"", ":", "[", "\"text\"", "]", ",", "\n", "\"highlight\"", ":", "{", "\n", "\"fields\"", ":", "{", "\n", "\"text\"", ":", "{", "\n", "\"require_field_match\"", ":", "False", ",", "\n", "\"fragment_size\"", ":", "400", ",", "\n", "\"number_of_fragments\"", ":", "1", ",", "\n", "\"no_match_size\"", ":", "20", "\n", "}", "\n", "}", "\n", "}", "\n", "}", "\n", "\n", "if", "\"one's\"", "in", "word", ".", "split", "(", ")", "or", "\"someone's\"", "in", "word", ".", "split", "(", ")", "or", "\"anyone's\"", "in", "word", ".", "split", "(", ")", ":", "\n", "        ", "query", "=", "{", "\n", "\"bool\"", ":", "{", "\n", "\"should\"", ":", "[", "\n", "]", "\n", "}", "\n", "}", "\n", "\n", "if", "\"one's\"", "in", "word", ".", "split", "(", ")", ":", "\n", "            ", "key", "=", "\"one's\"", "\n", "", "elif", "\"someone's\"", "in", "word", ".", "split", "(", ")", ":", "\n", "            ", "key", "=", "\"someone's\"", "\n", "", "elif", "\"anyone's\"", "in", "word", ".", "split", "(", ")", ":", "\n", "            ", "key", "=", "\"anyone's\"", "\n", "\n", "", "for", "po", "in", "[", "'my'", ",", "'your'", ",", "'his'", ",", "'her'", ",", "'our'", ",", "'their'", "]", ":", "\n", "            ", "query", "[", "'bool'", "]", "[", "'should'", "]", ".", "append", "(", "{", "\n", "\"match_phrase\"", ":", "{", "\n", "\"text\"", ":", "word", ".", "replace", "(", "key", ",", "po", ")", "\n", "}", "\n", "}", ")", "\n", "", "", "elif", "\"someone\"", "in", "word", ".", "split", "(", ")", "or", "\"anyone\"", "in", "word", ".", "split", "(", ")", ":", "\n", "        ", "query", "=", "{", "\n", "\"bool\"", ":", "{", "\n", "\"should\"", ":", "[", "\n", "]", "\n", "}", "\n", "}", "\n", "\n", "if", "\"someone\"", "in", "word", ".", "split", "(", ")", ":", "\n", "            ", "key", "=", "\"someone\"", "\n", "", "elif", "\"anyone\"", "in", "word", ".", "split", "(", ")", ":", "\n", "            ", "key", "=", "\"anyone\"", "\n", "\n", "", "doc", "=", "nlp", "(", "word", ")", "\n", "idx", "=", "[", "t", ".", "i", "for", "t", "in", "doc", "if", "t", ".", "text", "in", "[", "'someone'", ",", "'anyone'", "]", "]", "[", "0", "]", "\n", "if", "doc", "[", "idx", "-", "1", "]", ".", "tag_", ".", "startswith", "(", "'V'", ")", ":", "\n", "            ", "for", "po", "in", "[", "'me'", ",", "'you'", ",", "'him'", ",", "'her'", ",", "'us'", ",", "'them'", "]", ":", "\n", "                ", "query", "[", "'bool'", "]", "[", "'should'", "]", ".", "append", "(", "{", "\n", "\"match_phrase\"", ":", "{", "\n", "\"text\"", ":", "word", ".", "replace", "(", "key", ",", "po", ")", "\n", "}", "\n", "}", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "po", "in", "[", "'I'", ",", "'you'", ",", "'he'", ",", "'she'", ",", "'we'", ",", "'they'", "]", ":", "\n", "                ", "query", "[", "'bool'", "]", "[", "'should'", "]", ".", "append", "(", "{", "\n", "\"match_phrase\"", ":", "{", "\n", "\"text\"", ":", "word", ".", "replace", "(", "key", ",", "po", ")", "\n", "}", "\n", "}", ")", "\n", "", "", "", "elif", "word", "in", "[", "\"an offer one can't refuse\"", ",", "\n", "\"as best one can\"", ",", "\n", "\"as far as one knows\"", ",", "\n", "\"bite off more than one can chew\"", ",", "\n", "\"know where one stands\"", ",", "\n", "\"last thing one needs\"", ",", "\n", "\"let one go\"", "]", ":", "\n", "        ", "query", "=", "{", "\n", "\"bool\"", ":", "{", "\n", "\"should\"", ":", "[", "\n", "]", "\n", "}", "\n", "}", "\n", "\n", "doc", "=", "nlp", "(", "word", ")", "\n", "idx", "=", "[", "t", ".", "i", "for", "t", "in", "doc", "if", "t", ".", "text", "in", "[", "'one'", "]", "]", "[", "0", "]", "\n", "if", "doc", "[", "idx", "-", "1", "]", ".", "tag_", ".", "startswith", "(", "'V'", ")", ":", "\n", "            ", "for", "po", "in", "[", "'me'", ",", "'you'", ",", "'him'", ",", "'her'", ",", "'us'", ",", "'them'", "]", ":", "\n", "                ", "query", "[", "'bool'", "]", "[", "'should'", "]", ".", "append", "(", "{", "\n", "\"match_phrase\"", ":", "{", "\n", "\"text\"", ":", "word", ".", "replace", "(", "'one'", ",", "po", ")", "\n", "}", "\n", "}", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "po", "in", "[", "'I'", ",", "'you'", ",", "'he'", ",", "'she'", ",", "'we'", ",", "'they'", "]", ":", "\n", "                ", "query", "[", "'bool'", "]", "[", "'should'", "]", ".", "append", "(", "{", "\n", "\"match_phrase\"", ":", "{", "\n", "\"text\"", ":", "word", ".", "replace", "(", "'one'", ",", "po", ")", "\n", "}", "\n", "}", ")", "\n", "", "", "", "else", ":", "\n", "        ", "new_word", "=", "' '", ".", "join", "(", "toker", ".", "tokenize", "(", "word", ")", ")", "\n", "if", "new_word", "!=", "word", ":", "\n", "            ", "query", "=", "{", "\n", "\"bool\"", ":", "{", "\n", "\"should\"", ":", "[", "{", "\n", "\"match_phrase\"", ":", "{", "\n", "\"text\"", ":", "word", "\n", "}", "\n", "}", ",", "\n", "{", "\n", "\"match_phrase\"", ":", "{", "\n", "\"text\"", ":", "new_word", "\n", "}", "\n", "}", "\n", "]", "\n", "}", "\n", "}", "\n", "", "else", ":", "\n", "            ", "query", "=", "{", "\n", "\"match_phrase\"", ":", "{", "\n", "\"text\"", ":", "word", "\n", "}", "\n", "}", "\n", "\n", "", "", "body", "[", "'query'", "]", "=", "query", "\n", "\n", "records", "=", "[", "]", "\n", "for", "i", ",", "hit", "in", "enumerate", "(", "tqdm", "(", "scan", "(", "client", ",", "query", "=", "body", ",", "index", "=", "index", ",", "\n", "scroll", "=", "'10m'", ",", "size", "=", "10000", ",", "\n", "request_timeout", "=", "600", ")", ",", "leave", "=", "False", ",", "desc", "=", "'hitting'", ")", ")", ":", "\n", "        ", "t", "=", "hit", "[", "'highlight'", "]", "\n", "tmp_context", "=", "t", "[", "'text'", "]", "[", "0", "]", "\n", "left", "=", "tmp_context", ".", "index", "(", "'<em>'", ")", "\n", "right", "=", "tmp_context", ".", "rindex", "(", "'</em>'", ")", "\n", "\n", "ground_truth", "=", "tmp_context", "[", "left", ":", "right", "]", ".", "replace", "(", "'<em>'", ",", "''", ")", ".", "replace", "(", "'</em>'", ",", "''", ")", "\n", "if", "','", "not", "in", "word", "and", "','", "in", "ground_truth", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "index", "==", "'bnc'", ":", "\n", "            ", "tmp_context_d", "=", "get_contexts", "(", "hit", "[", "'_id'", "]", ")", "\n", "tmp_context_d", "[", "hit", "[", "'_id'", "]", "]", "=", "tmp_context", "\n", "", "else", ":", "\n", "            ", "tmp_context_d", "=", "{", "\n", "hit", "[", "'_id'", "]", ":", "tmp_context", "\n", "}", "\n", "\n", "", "d", "=", "{", "\n", "'_id'", ":", "hit", "[", "'_id'", "]", ",", "\n", "'idiom'", ":", "word", ",", "\n", "\"groundTruth\"", ":", "[", "ground_truth", "]", ",", "\n", "\"content\"", ":", "tmp_context_d", ",", "\n", "}", "\n", "records", ".", "append", "(", "d", ")", "\n", "\n", "", "with", "jsonlines", ".", "open", "(", "dump_file", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "[", "f", ".", "write", "(", "d", ")", "for", "d", "in", "tqdm", "(", "records", ",", "leave", "=", "False", ",", "desc", "=", "'dumping'", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.dump_from_es.build_values": [[183, 198], ["None"], "function", ["None"], ["", "", "def", "build_values", "(", "book_name", ",", "line_num", ")", ":", "\n", "    ", "if", "line_num", "==", "0", ":", "\n", "        ", "values", "=", "[", "\n", "# f\"{book_name}_{line_num}\",", "\n", "f\"{book_name}_{line_num + 1}\"", ",", "\n", "f\"{book_name}_{line_num + 2}\"", ",", "\n", "]", "\n", "", "else", ":", "\n", "        ", "values", "=", "[", "\n", "f\"{book_name}_{line_num - 1}\"", ",", "\n", "# f\"{book_name}_{line_num}\",", "\n", "f\"{book_name}_{line_num + 1}\"", ",", "\n", "]", "\n", "\n", "", "return", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.dump_from_es.get_contexts": [[200, 224], ["hit_id.rsplit", "int", "dump_from_es.build_values", "tqdm.tqdm", "elasticsearch.helpers.scan", "len", "list"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.dump_from_es.build_values"], ["", "def", "get_contexts", "(", "hit_id", ")", ":", "\n", "    ", "book_name", ",", "line_num", "=", "hit_id", ".", "rsplit", "(", "'_'", ",", "1", ")", "\n", "line_num", "=", "int", "(", "line_num", ")", "\n", "\n", "values", "=", "build_values", "(", "book_name", ",", "line_num", ")", "\n", "\n", "body", "=", "{", "\n", "\"_source\"", ":", "[", "\"text\"", "]", ",", "\n", "\"query\"", ":", "{", "\n", "\"ids\"", ":", "{", "\n", "\"values\"", ":", "list", "(", "values", ")", "\n", "}", "\n", "}", "\n", "}", "\n", "\n", "tmp_context_d", "=", "{", "}", "\n", "for", "hit", "in", "tqdm", "(", "scan", "(", "client", ",", "query", "=", "body", ",", "index", "=", "index", ",", "\n", "scroll", "=", "'10m'", ",", "size", "=", "10000", ",", "\n", "request_timeout", "=", "600", ")", ",", "\n", "leave", "=", "False", ",", "desc", "=", "'windowing'", ",", "total", "=", "len", "(", "values", ")", ")", ":", "\n", "        ", "idx", "=", "hit", "[", "'_id'", "]", "\n", "t", "=", "hit", "[", "'_source'", "]", "\n", "tmp_context_d", "[", "idx", "]", "=", "t", "[", "'text'", "]", "\n", "", "return", "tmp_context_d", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.build_dataset_slide.parse_start_end": [[36, 74], ["nlp", "nlp", "enumerate", "print", "all", "any", "trues.append", "trues.append", "t.text.lower", "tt.text.lower", "t.lemma_.lower", "tt.lemma_.lower", "t.text.lower", "tt.text.lower"], "function", ["None"], ["def", "parse_start_end", "(", "sentence", ",", "span", ")", ":", "\n", "    ", "doc", "=", "nlp", "(", "sentence", ")", "\n", "target_doc", "=", "nlp", "(", "span", ")", "\n", "\n", "start", ",", "end", "=", "None", ",", "None", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "doc", ")", ":", "\n", "        ", "j", "=", "i", "\n", "trues", "=", "[", "]", "\n", "for", "tt", "in", "target_doc", ":", "\n", "            ", "if", "t", ".", "is_punct", ":", "\n", "                ", "j", "+=", "1", "\n", "", "elif", "tt", ".", "is_punct", "or", "tt", ".", "text", "==", "\"'s\"", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "                ", "checks", "=", "[", "\n", "t", ".", "text", ".", "lower", "(", ")", "==", "tt", ".", "text", ".", "lower", "(", ")", ",", "\n", "t", ".", "text", ".", "lower", "(", ")", "in", "possessive_form2", "and", "tt", ".", "text", ".", "lower", "(", ")", "in", "possessive_form1", ",", "\n", "t", ".", "lemma_", ".", "lower", "(", ")", "==", "tt", ".", "lemma_", ".", "lower", "(", ")", "\n", "]", "\n", "#                 print(t.text.lower(), tt.text.lower(), checks)", "\n", "if", "any", "(", "checks", ")", ":", "\n", "                    ", "trues", ".", "append", "(", "True", ")", "\n", "", "else", ":", "\n", "                    ", "trues", ".", "append", "(", "False", ")", "\n", "break", "\n", "", "j", "+=", "1", "\n", "", "t", "=", "doc", "[", "j", "]", "\n", "#         print(trues)", "\n", "", "if", "trues", "and", "all", "(", "trues", ")", ":", "\n", "            ", "start", "=", "i", "\n", "end", "=", "j", "-", "1", "\n", "break", "\n", "\n", "", "", "if", "start", "is", "None", "or", "end", "is", "None", ":", "\n", "        ", "print", "(", "sentence", ",", "span", ")", "\n", "\n", "", "assert", "start", "is", "not", "None", "and", "end", "is", "not", "None", "\n", "return", "doc", ",", "start", ",", "end", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.build_dataset_slide.is_substring": [[76, 79], ["s_text.lower().replace().replace().count", "idiom.lower().replace().replace", "s_text.lower().replace().replace", "idiom.lower().replace", "s_text.lower().replace", "idiom.lower", "s_text.lower"], "function", ["None"], ["", "def", "is_substring", "(", "s_text", ",", "idiom", ")", ":", "\n", "    ", "return", "s_text", ".", "lower", "(", ")", ".", "replace", "(", "'-'", ",", "' '", ")", ".", "replace", "(", "' '", ",", "''", ")", ".", "count", "(", "\n", "idiom", ".", "lower", "(", ")", ".", "replace", "(", "'-'", ",", "' '", ")", ".", "replace", "(", "' '", ",", "''", ")", ")", "==", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.build_dataset_slide.common_idioms": [[81, 97], ["set().intersection", "set", "itertools.chain", "k.lower().replace().replace", "k.lower().replace().replace", "d_1.values", "d_0.items", "d_1.items", "set", "set.add", "k.lower().replace", "k.lower().replace", "d_0.values", "k.lower", "k.lower"], "function", ["None"], ["", "def", "common_idioms", "(", "idioms_0", ",", "idioms_1", ")", ":", "\n", "    ", "d_0", "=", "{", "}", "\n", "for", "k", "in", "idioms_0", ":", "\n", "        ", "d_0", "[", "k", "]", "=", "k", ".", "lower", "(", ")", ".", "replace", "(", "'-'", ",", "' '", ")", ".", "replace", "(", "' '", ",", "''", ")", "\n", "\n", "", "d_1", "=", "{", "}", "\n", "for", "k", "in", "idioms_1", ":", "\n", "        ", "d_1", "[", "k", "]", "=", "k", ".", "lower", "(", ")", ".", "replace", "(", "'-'", ",", "' '", ")", ".", "replace", "(", "' '", ",", "''", ")", "\n", "\n", "", "commons", "=", "set", "(", "d_0", ".", "values", "(", ")", ")", ".", "intersection", "(", "d_1", ".", "values", "(", ")", ")", "\n", "\n", "s", "=", "set", "(", ")", "\n", "for", "k", ",", "v", "in", "chain", "(", "d_0", ".", "items", "(", ")", ",", "d_1", ".", "items", "(", ")", ")", ":", "\n", "        ", "if", "v", "in", "commons", ":", "\n", "            ", "s", ".", "add", "(", "k", ")", "\n", "", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.build_dataset_slide.parse_data": [[99, 134], ["tqdm.tqdm", "os.path.isfile", "jsonlines.open", "os.stat", "len", "sorted", "[].replace().replace", "[].replace().replace.replace", "[].replace().replace.count", "len", "v.replace().replace.replace().replace", "[].replace", "len", "len", "print", "data.setdefault", "data[].append", "v.replace().replace.replace"], "function", ["None"], ["", "def", "parse_data", "(", "all_idioms", ",", "idiom_span_mapping", ")", ":", "\n", "    ", "data", "=", "{", "}", "\n", "# all_idioms = list(idiom_span_mapping.keys())", "\n", "for", "idiom", "in", "tqdm", "(", "all_idioms", ")", ":", "\n", "        ", "dump_files", "=", "[", "\n", "f'{annotation_dir}/bnc_dumped/{idiom}.jsonl'", ",", "\n", "f'{annotation_dir}/1billion_dumped/{idiom}.jsonl'", ",", "\n", "]", "\n", "for", "dump_file", "in", "dump_files", ":", "\n", "            ", "if", "os", ".", "path", ".", "isfile", "(", "dump_file", ")", "and", "os", ".", "stat", "(", "dump_file", ")", ".", "st_size", ">", "0", ":", "\n", "                ", "with", "jsonlines", ".", "open", "(", "dump_file", ")", "as", "f", ":", "\n", "                    ", "for", "d", "in", "f", ":", "\n", "                        ", "if", "len", "(", "d", "[", "'content'", "]", ")", ">", "1", ":", "\n", "                            ", "content", "=", "''", "\n", "for", "k", "in", "sorted", "(", "d", "[", "'content'", "]", ")", ":", "\n", "                                ", "v", "=", "d", "[", "'content'", "]", "[", "k", "]", "\n", "if", "k", "==", "d", "[", "'_id'", "]", ":", "\n", "                                    ", "v", "=", "v", ".", "replace", "(", "'<em>'", ",", "''", ")", ".", "replace", "(", "'</em>'", ",", "''", ")", "\n", "", "content", "+=", "v", "\n", "", "", "else", ":", "\n", "                            ", "content", "=", "d", "[", "'content'", "]", "[", "d", "[", "'_id'", "]", "]", ".", "replace", "(", "'<em>'", ",", "''", ")", ".", "replace", "(", "'</em>'", ",", "''", ")", "\n", "\n", "", "span_text", "=", "d", "[", "'groundTruth'", "]", "[", "0", "]", "\n", "if", "content", ".", "count", "(", "span_text", ")", "==", "1", "and", "len", "(", "span_text", ")", "-", "len", "(", "idiom", ")", "<", "len", "(", "idiom", ")", ":", "\n", "                            ", "idiom", "=", "idiom_span_mapping", "[", "idiom", "]", "\n", "d", "[", "'idiom'", "]", "=", "idiom", "\n", "d", "[", "'content'", "]", "=", "content", ".", "replace", "(", "span_text", ",", "\"#idiom#\"", ")", "\n", "if", "' #idiom# '", "in", "d", "[", "'content'", "]", ":", "\n", "                                ", "if", "span_text", "in", "idiom_span_mapping", "and", "idiom_span_mapping", "[", "span_text", "]", "!=", "d", "[", "'idiom'", "]", ":", "\n", "                                    ", "print", "(", "\"Error:\"", ",", "d", ",", "span_text", ",", "idiom_span_mapping", "[", "span_text", "]", ")", "\n", "", "else", ":", "\n", "                                    ", "data", ".", "setdefault", "(", "idiom", ",", "[", "]", ")", "\n", "data", "[", "idiom", "]", ".", "append", "(", "d", ")", "\n", "idiom_span_mapping", "[", "span_text", "]", "=", "d", "[", "'idiom'", "]", "\n", "", "", "", "", "", "", "", "", "return", "data", ",", "idiom_span_mapping", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.slide.build_dataset_slide.get_form": [[136, 152], ["idiom.lower", "tmp.replace.replace"], "function", ["None"], ["", "def", "get_form", "(", "idiom", ")", ":", "\n", "    ", "tmp", "=", "idiom", ".", "lower", "(", ")", "\n", "for", "a", ",", "b", "in", "[", "\n", "(", "\" someone's \"", ",", "' ones '", ")", ",", "\n", "(", "' someones '", ",", "' ones '", ")", ",", "\n", "(", "' your '", ",", "' ones '", ")", ",", "\n", "(", "' my '", ",", "' ones '", ")", ",", "\n", "(", "' his '", ",", "' ones '", ")", ",", "\n", "(", "' them '", ",", "' someone '", ")", ",", "\n", "(", "\",\"", ",", "' '", ")", ",", "\n", "(", "\"'\"", ",", "' '", ")", ",", "\n", "(", "'-'", ",", "' '", ")", ",", "\n", "(", "' '", ",", "''", ")", "\n", "]", ":", "\n", "        ", "tmp", "=", "tmp", ".", "replace", "(", "a", ",", "b", ")", "\n", "", "return", "tmp", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.notebook.embedded_image": [[8, 13], ["PIL.Image.open", "io.BytesIO", "pil_image.open.save", "base64.b64encode().decode", "base64.b64encode", "io.BytesIO.getvalue"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save"], ["def", "embedded_image", "(", "url", ")", ":", "\n", "    ", "pil_im", "=", "pil_image", ".", "open", "(", "url", ")", "\n", "b", "=", "BytesIO", "(", ")", "\n", "pil_im", ".", "save", "(", "b", ",", "format", "=", "'png'", ")", "\n", "return", "\"data:image/png;base64,{0}\"", ".", "format", "(", "b64encode", "(", "b", ".", "getvalue", "(", ")", ")", ".", "decode", "(", "'utf-8'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.notebook.visualize_attrs": [[15, 22], ["enumerate", "notebook.get_color"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.notebook.get_color"], ["", "def", "visualize_attrs", "(", "tokens", ",", "attrs", ")", ":", "\n", "    ", "html_text", "=", "\"\"", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "r", ",", "g", ",", "b", "=", "get_color", "(", "attrs", "[", "i", "]", ")", "\n", "html_text", "+=", "\" <strong><span style='size:16;color:rgb(%d,%d,%d)'>%s</span></strong>\"", "%", "(", "\n", "r", ",", "g", ",", "b", ",", "tok", ")", "\n", "", "return", "html_text", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.notebook.get_latex": [[24, 30], ["enumerate", "notebook.get_color"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.notebook.get_color"], ["", "def", "get_latex", "(", "tokens", ",", "attrs", ")", ":", "\n", "    ", "ans", "=", "\"\"", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "[", "r", ",", "g", ",", "b", "]", "=", "[", "w", "/", "256.0", "for", "w", "in", "get_color", "(", "attrs", "[", "i", "]", ")", "]", "\n", "ans", "+=", "\" {\\color[rgb]{%f,%f,%f}%s}\"", "%", "(", "r", ",", "g", ",", "b", ",", "tok", ")", "\n", "", "return", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.notebook.normalize_attrs": [[32, 36], ["max", "abs", "abs", "attrs.max", "attrs.min"], "function", ["None"], ["", "def", "normalize_attrs", "(", "attrs", ")", ":", "\n", "    ", "\"\"\" normalize attributions to between -1 and 1 \"\"\"", "\n", "bound", "=", "max", "(", "abs", "(", "attrs", ".", "max", "(", ")", ")", ",", "abs", "(", "attrs", ".", "min", "(", ")", ")", ")", "\n", "return", "attrs", "/", "bound", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.notebook.get_color": [[38, 43], ["int", "int", "int", "int", "int", "int"], "function", ["None"], ["", "def", "get_color", "(", "attr", ")", ":", "\n", "    ", "\"\"\" attr is assumed to be between -1 and 1 \"\"\"", "\n", "if", "attr", ">", "0", ":", "\n", "        ", "return", "int", "(", "128", "*", "attr", ")", "+", "127", ",", "128", "-", "int", "(", "64", "*", "attr", ")", ",", "128", "-", "int", "(", "64", "*", "attr", ")", "\n", "", "return", "128", "+", "int", "(", "64", "*", "attr", ")", ",", "128", "+", "int", "(", "64", "*", "attr", ")", ",", "int", "(", "-", "128", "*", "attr", ")", "+", "127", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.notebook.eval_fix": [[45, 63], ["more_itertools.numeric_range", "bl_acc.append", "gd_acc.append", "threshold_list.append", "sum", "len", "sum", "len", "zip"], "function", ["None"], ["", "def", "eval_fix", "(", "predictions", ",", "targets", ")", ":", "\n", "    ", "gd_acc", "=", "[", "]", "\n", "bl_acc", "=", "[", "]", "\n", "max_acc", "=", "0", "\n", "prediction", "=", "None", "\n", "threshold_list", "=", "[", "]", "\n", "\n", "for", "t", "in", "numeric_range", "(", "0.5", ",", "1", ",", "0.05", ")", ":", "\n", "        ", "bl", "=", "[", "idx", "if", "prob", ">", "t", "else", "1", "for", "(", "prob", ",", "idx", ")", "in", "predictions", "]", "\n", "acc", "=", "sum", "(", "x", "==", "y", "for", "x", ",", "y", "in", "zip", "(", "bl", ",", "targets", ")", ")", "/", "len", "(", "targets", ")", "\n", "bl_acc", ".", "append", "(", "acc", ")", "\n", "gd_acc", ".", "append", "(", "sum", "(", "x", "==", "1", "for", "x", "in", "targets", ")", "/", "len", "(", "targets", ")", ")", "\n", "threshold_list", ".", "append", "(", "t", ")", "\n", "\n", "if", "acc", ">", "max_acc", ":", "\n", "            ", "max_acc", "=", "acc", "\n", "prediction", "=", "bl", "\n", "", "", "return", "max_acc", ",", "prediction", "\n", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_reduce_and_rescale_tensors": [[16, 44], ["sum", "tensors[].new().zero_", "horovod.torch.allreduce_", "tensors[].new().zero_.div_", "t.numel", "buffer_t[].copy_", "t.numel", "t.view().copy_", "t.numel", "tensors[].new", "t.view", "t.view"], "function", ["None"], ["def", "all_reduce_and_rescale_tensors", "(", "tensors", ",", "rescale_denom", ")", ":", "\n", "    ", "\"\"\"All-reduce and rescale tensors at once (as a flattened tensor)\n\n    Args:\n        tensors: list of Tensors to all-reduce\n        rescale_denom: denominator for rescaling summed Tensors\n    \"\"\"", "\n", "# buffer size in bytes, determine equiv. # of elements based on data type", "\n", "sz", "=", "sum", "(", "t", ".", "numel", "(", ")", "for", "t", "in", "tensors", ")", "\n", "buffer_t", "=", "tensors", "[", "0", "]", ".", "new", "(", "sz", ")", ".", "zero_", "(", ")", "\n", "\n", "# copy tensors into buffer_t", "\n", "offset", "=", "0", "\n", "for", "t", "in", "tensors", ":", "\n", "        ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ".", "copy_", "(", "t", ".", "view", "(", "-", "1", ")", ")", "\n", "offset", "+=", "numel", "\n", "\n", "# all-reduce and rescale", "\n", "", "hvd", ".", "allreduce_", "(", "buffer_t", "[", ":", "offset", "]", ")", "\n", "buffer_t", ".", "div_", "(", "rescale_denom", ")", "\n", "\n", "# copy all-reduced buffer back into tensors", "\n", "offset", "=", "0", "\n", "for", "t", "in", "tensors", ":", "\n", "        ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "t", ".", "view", "(", "-", "1", ")", ".", "copy_", "(", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ")", "\n", "offset", "+=", "numel", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_reduce_and_rescale_tensors_chunked": [[46, 98], ["tensors[].new().zero_", "horovod.torch.allreduce_", "tensors[].new().zero_.div_", "len", "distributed.all_reduce_and_rescale_tensors_chunked.all_reduce_buffer"], "function", ["None"], ["", "", "def", "all_reduce_and_rescale_tensors_chunked", "(", "tensors", ",", "rescale_denom", ",", "\n", "buffer_size", "=", "10485760", ")", ":", "\n", "    ", "\"\"\"All-reduce and rescale tensors in chunks of the specified size.\n\n    Args:\n        tensors: list of Tensors to all-reduce\n        rescale_denom: denominator for rescaling summed Tensors\n        buffer_size: all-reduce chunk size in bytes\n    \"\"\"", "\n", "# buffer size in bytes, determine equiv. # of elements based on data type", "\n", "buffer_t", "=", "tensors", "[", "0", "]", ".", "new", "(", "\n", "math", ".", "ceil", "(", "buffer_size", "/", "tensors", "[", "0", "]", ".", "element_size", "(", ")", ")", ")", ".", "zero_", "(", ")", "\n", "buffer", "=", "[", "]", "\n", "\n", "def", "all_reduce_buffer", "(", ")", ":", "\n", "# copy tensors into buffer_t", "\n", "        ", "offset", "=", "0", "\n", "for", "t", "in", "buffer", ":", "\n", "            ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ".", "copy_", "(", "t", ".", "view", "(", "-", "1", ")", ")", "\n", "offset", "+=", "numel", "\n", "\n", "# all-reduce and rescale", "\n", "", "hvd", ".", "allreduce_", "(", "buffer_t", "[", ":", "offset", "]", ")", "\n", "buffer_t", ".", "div_", "(", "rescale_denom", ")", "\n", "\n", "# copy all-reduced buffer back into tensors", "\n", "offset", "=", "0", "\n", "for", "t", "in", "buffer", ":", "\n", "            ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "t", ".", "view", "(", "-", "1", ")", ".", "copy_", "(", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ")", "\n", "offset", "+=", "numel", "\n", "\n", "", "", "filled", "=", "0", "\n", "for", "t", "in", "tensors", ":", "\n", "        ", "sz", "=", "t", ".", "numel", "(", ")", "*", "t", ".", "element_size", "(", ")", "\n", "if", "sz", ">", "buffer_size", ":", "\n", "# tensor is bigger than buffer, all-reduce and rescale directly", "\n", "            ", "hvd", ".", "allreduce_", "(", "t", ")", "\n", "t", ".", "div_", "(", "rescale_denom", ")", "\n", "", "elif", "filled", "+", "sz", ">", "buffer_size", ":", "\n", "# buffer is full, all-reduce and replace buffer with grad", "\n", "            ", "all_reduce_buffer", "(", ")", "\n", "buffer", "=", "[", "t", "]", "\n", "filled", "=", "sz", "\n", "", "else", ":", "\n", "# add tensor to buffer", "\n", "            ", "buffer", ".", "append", "(", "t", ")", "\n", "filled", "+=", "sz", "\n", "\n", "", "", "if", "len", "(", "buffer", ")", ">", "0", ":", "\n", "        ", "all_reduce_buffer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.broadcast_tensors": [[100, 149], ["tensors[].new().zero_", "horovod.torch.broadcast_", "len", "distributed.broadcast_tensors.broadcast_buffer"], "function", ["None"], ["", "", "def", "broadcast_tensors", "(", "tensors", ",", "root_rank", ",", "buffer_size", "=", "10485760", ")", ":", "\n", "    ", "\"\"\"broadcast tensors in chunks of the specified size.\n\n    Args:\n        tensors: list of Tensors to broadcast\n        root_rank: rank to broadcast\n        buffer_size: broadcast chunk size in bytes\n    \"\"\"", "\n", "# buffer size in bytes, determine equiv. # of elements based on data type", "\n", "buffer_t", "=", "tensors", "[", "0", "]", ".", "new", "(", "\n", "math", ".", "ceil", "(", "buffer_size", "/", "tensors", "[", "0", "]", ".", "element_size", "(", ")", ")", ")", ".", "zero_", "(", ")", "\n", "buffer", "=", "[", "]", "\n", "\n", "def", "broadcast_buffer", "(", ")", ":", "\n", "# copy tensors into buffer_t", "\n", "        ", "offset", "=", "0", "\n", "for", "t", "in", "buffer", ":", "\n", "            ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ".", "copy_", "(", "t", ".", "view", "(", "-", "1", ")", ")", "\n", "offset", "+=", "numel", "\n", "\n", "# broadcast", "\n", "", "hvd", ".", "broadcast_", "(", "buffer_t", "[", ":", "offset", "]", ",", "root_rank", ")", "\n", "\n", "# copy all-reduced buffer back into tensors", "\n", "offset", "=", "0", "\n", "for", "t", "in", "buffer", ":", "\n", "            ", "numel", "=", "t", ".", "numel", "(", ")", "\n", "t", ".", "view", "(", "-", "1", ")", ".", "copy_", "(", "buffer_t", "[", "offset", ":", "offset", "+", "numel", "]", ")", "\n", "offset", "+=", "numel", "\n", "\n", "", "", "filled", "=", "0", "\n", "for", "t", "in", "tensors", ":", "\n", "        ", "sz", "=", "t", ".", "numel", "(", ")", "*", "t", ".", "element_size", "(", ")", "\n", "if", "sz", ">", "buffer_size", ":", "\n", "# tensor is bigger than buffer, broadcast directly", "\n", "            ", "hvd", ".", "broadcast_", "(", "t", ",", "root_rank", ")", "\n", "", "elif", "filled", "+", "sz", ">", "buffer_size", ":", "\n", "# buffer is full, broadcast and replace buffer with tensor", "\n", "            ", "broadcast_buffer", "(", ")", "\n", "buffer", "=", "[", "t", "]", "\n", "filled", "=", "sz", "\n", "", "else", ":", "\n", "# add tensor to buffer", "\n", "            ", "buffer", ".", "append", "(", "t", ")", "\n", "filled", "+=", "sz", "\n", "\n", "", "", "if", "len", "(", "buffer", ")", ">", "0", ":", "\n", "        ", "broadcast_buffer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed._encode": [[151, 166], ["len", "max", "range", "torch.ByteTensor", "horovod.torch.ByteTensor", "math.floor", "torch.cuda.ByteTensor", "horovod.torch.cuda.ByteTensor", "torch.cuda.ByteTensor", "horovod.torch.cuda.ByteTensor", "list", "math.log"], "function", ["None"], ["", "", "def", "_encode", "(", "enc", ",", "max_size", ",", "use_max_size", "=", "False", ")", ":", "\n", "    ", "enc_size", "=", "len", "(", "enc", ")", "\n", "enc_byte", "=", "max", "(", "math", ".", "floor", "(", "math", ".", "log", "(", "max_size", ",", "256", ")", "+", "1", ")", ",", "1", ")", "\n", "if", "use_max_size", ":", "\n", "# this is used for broadcasting", "\n", "        ", "buffer_", "=", "torch", ".", "cuda", ".", "ByteTensor", "(", "max_size", "+", "enc_byte", ")", "\n", "", "else", ":", "\n", "        ", "buffer_", "=", "torch", ".", "cuda", ".", "ByteTensor", "(", "enc_size", "+", "enc_byte", ")", "\n", "", "remainder", "=", "enc_size", "\n", "for", "i", "in", "range", "(", "enc_byte", ")", ":", "\n", "        ", "base", "=", "256", "**", "(", "enc_byte", "-", "i", "-", "1", ")", "\n", "buffer_", "[", "i", "]", "=", "remainder", "//", "base", "\n", "remainder", "%=", "base", "\n", "", "buffer_", "[", "enc_byte", ":", "enc_byte", "+", "enc_size", "]", "=", "torch", ".", "ByteTensor", "(", "list", "(", "enc", ")", ")", "\n", "return", "buffer_", ",", "enc_byte", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed._decode": [[168, 174], ["sum", "bytes", "buffer_[].tolist", "buffer_[].item", "range"], "function", ["None"], ["", "def", "_decode", "(", "buffer_", ",", "enc_byte", ")", ":", "\n", "    ", "size", "=", "sum", "(", "256", "**", "(", "enc_byte", "-", "i", "-", "1", ")", "*", "buffer_", "[", "i", "]", ".", "item", "(", ")", "\n", "for", "i", "in", "range", "(", "enc_byte", ")", ")", "\n", "bytes_list", "=", "bytes", "(", "buffer_", "[", "enc_byte", ":", "enc_byte", "+", "size", "]", ".", "tolist", "(", ")", ")", "\n", "shift", "=", "size", "+", "enc_byte", "\n", "return", "bytes_list", ",", "shift", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.all_gather_list": [[179, 196], ["pickle.dumps", "len", "horovod.torch.allgather().max().item", "distributed._encode", "horovod.torch.allgather", "range", "horovod.torch.size", "distributed._decode", "pickle.loads", "results.append", "horovod.torch.allgather().max", "horovod.torch.allgather", "torch.tensor().cuda", "horovod.torch.tensor().cuda", "torch.tensor", "horovod.torch.tensor"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed._encode", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed._decode"], ["def", "all_gather_list", "(", "data", ")", ":", "\n", "    ", "\"\"\"Gathers arbitrary data from all nodes into a list.\"\"\"", "\n", "enc", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "\n", "enc_size", "=", "len", "(", "enc", ")", "\n", "max_size", "=", "hvd", ".", "allgather", "(", "torch", ".", "tensor", "(", "[", "enc_size", "]", ")", ".", "cuda", "(", ")", ")", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "in_buffer", ",", "enc_byte", "=", "_encode", "(", "enc", ",", "max_size", ")", "\n", "\n", "out_buffer", "=", "hvd", ".", "allgather", "(", "in_buffer", "[", ":", "enc_byte", "+", "enc_size", "]", ")", "\n", "\n", "results", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "hvd", ".", "size", "(", ")", ")", ":", "\n", "        ", "bytes_list", ",", "shift", "=", "_decode", "(", "out_buffer", ",", "enc_byte", ")", "\n", "out_buffer", "=", "out_buffer", "[", "shift", ":", "]", "\n", "result", "=", "pickle", ".", "loads", "(", "bytes_list", ")", "\n", "results", ".", "append", "(", "result", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed.any_broadcast": [[198, 210], ["pickle.dumps", "horovod.torch.allgather().max().item", "distributed._encode", "horovod.torch.broadcast_", "distributed._decode", "pickle.loads", "horovod.torch.allgather().max", "horovod.torch.allgather", "torch.tensor().cuda", "horovod.torch.tensor().cuda", "torch.tensor", "horovod.torch.tensor", "len"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed._encode", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.distributed._decode"], ["", "def", "any_broadcast", "(", "data", ",", "root_rank", ")", ":", "\n", "    ", "\"\"\"broadcast arbitrary data from root_rank to all nodes.\"\"\"", "\n", "enc", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "\n", "max_size", "=", "hvd", ".", "allgather", "(", "torch", ".", "tensor", "(", "[", "len", "(", "enc", ")", "]", ")", ".", "cuda", "(", ")", ")", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "buffer_", ",", "enc_byte", "=", "_encode", "(", "enc", ",", "max_size", ",", "use_max_size", "=", "True", ")", "\n", "\n", "hvd", ".", "broadcast_", "(", "buffer_", ",", "root_rank", ")", "\n", "\n", "bytes_list", ",", "_", "=", "_decode", "(", "buffer_", ",", "enc_byte", ")", "\n", "result", "=", "pickle", ".", "loads", "(", "bytes_list", ")", "\n", "return", "result", "\n", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.__init__": [[29, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_logger", "=", "None", "\n", "self", ".", "_global_step", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.create": [[33, 35], ["SummaryWriter"], "methods", ["None"], ["", "def", "create", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "_logger", "=", "SummaryWriter", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.noop": [[36, 38], ["None"], "methods", ["None"], ["", "def", "noop", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.step": [[39, 41], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "self", ".", "_global_step", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.global_step": [[42, 45], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "global_step", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.log_scaler_dict": [[46, 59], ["log_dict.items", "isinstance", "logger.TensorboardLogger.log_scaler_dict", "logger.TensorboardLogger._logger.add_scalar"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.log_scaler_dict"], ["", "def", "log_scaler_dict", "(", "self", ",", "log_dict", ",", "prefix", "=", "''", ")", ":", "\n", "        ", "\"\"\" log a dictionary of scalar values\"\"\"", "\n", "if", "self", ".", "_logger", "is", "None", ":", "\n", "            ", "return", "\n", "", "if", "prefix", ":", "\n", "            ", "prefix", "=", "f'{prefix}_'", "\n", "", "for", "name", ",", "value", "in", "log_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "\n", "                ", "self", ".", "log_scaler_dict", "(", "value", ",", "self", ".", "_global_step", ",", "\n", "prefix", "=", "f'{prefix}{name}'", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_logger", ".", "add_scalar", "(", "f'{prefix}{name}'", ",", "value", ",", "\n", "self", ".", "_global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.TensorboardLogger.__getattr__": [[60, 64], ["logger.TensorboardLogger._logger.__getattribute__"], "methods", ["None"], ["", "", "", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "if", "self", ".", "_logger", "is", "None", ":", "\n", "            ", "return", "self", ".", "noop", "\n", "", "return", "self", ".", "_logger", ".", "__getattribute__", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.RunningMeter.__init__": [[73, 77], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "name", ",", "val", "=", "None", ",", "smooth", "=", "0.99", ")", ":", "\n", "        ", "self", ".", "_name", "=", "name", "\n", "self", ".", "_sm", "=", "smooth", "\n", "self", ".", "_val", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.RunningMeter.__call__": [[78, 81], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_val", "=", "(", "value", "if", "self", ".", "_val", "is", "None", "\n", "else", "value", "*", "(", "1", "-", "self", ".", "_sm", ")", "+", "self", ".", "_val", "*", "self", ".", "_sm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.RunningMeter.__str__": [[82, 84], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "f'{self._name}: {self._val:.4f}'", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.RunningMeter.val": [[85, 88], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "val", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_val", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.RunningMeter.name": [[89, 92], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "name", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_name", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.logger.add_log_to_file": [[21, 26], ["logging.FileHandler", "logging.Formatter", "logging.FileHandler.setFormatter", "LOGGER.addHandler"], "function", ["None"], ["def", "add_log_to_file", "(", "log_path", ")", ":", "\n", "    ", "fh", "=", "logging", ".", "FileHandler", "(", "log_path", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "_LOG_FMT", ",", "datefmt", "=", "_DATE_FMT", ")", "\n", "fh", ".", "setFormatter", "(", "formatter", ")", "\n", "LOGGER", ".", "addHandler", "(", "fh", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.__init__": [[59, 63], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "output_dir", ",", "prefix", "=", "'model_step'", ",", "suffix", "=", "'pt'", ")", ":", "\n", "        ", "self", ".", "output_dir", "=", "output_dir", "\n", "self", ".", "prefix", "=", "prefix", "\n", "self", ".", "suffix", "=", "suffix", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save": [[64, 81], ["os.path.join", "torch.save", "hasattr", "hasattr", "torch.save", "isinstance", "v.cpu", "model.state_dict().items", "optimizer.state_dict", "model.state_dict"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.ModelSaver.save"], ["", "def", "save", "(", "self", ",", "model", ",", "step", ",", "optimizer", "=", "None", ")", ":", "\n", "        ", "output_model_file", "=", "join", "(", "self", ".", "output_dir", ",", "\n", "f\"{self.prefix}_{step}.{self.suffix}\"", ")", "\n", "state_dict", "=", "{", "k", ":", "v", ".", "cpu", "(", ")", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", "else", "v", "\n", "for", "k", ",", "v", "in", "model", ".", "state_dict", "(", ")", ".", "items", "(", ")", "}", "\n", "if", "hasattr", "(", "model", ",", "'vocab_pad'", ")", "and", "model", ".", "vocab_pad", ":", "\n", "# store vocab embeddings before padding", "\n", "            ", "emb_w", "=", "state_dict", "[", "'bert.embeddings.word_embeddings.weight'", "]", "\n", "emb_w", "=", "emb_w", "[", ":", "-", "model", ".", "vocab_pad", ",", ":", "]", "\n", "state_dict", "[", "'bert.embeddings.word_embeddings.weight'", "]", "=", "emb_w", "\n", "state_dict", "[", "'cls.predictions.decoder.weight'", "]", "=", "emb_w", "\n", "", "torch", ".", "save", "(", "state_dict", ",", "output_model_file", ")", "\n", "if", "optimizer", "is", "not", "None", ":", "\n", "            ", "dump", "=", "{", "'step'", ":", "step", ",", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", "}", "\n", "if", "hasattr", "(", "optimizer", ",", "'_amp_stash'", ")", ":", "\n", "                ", "pass", "# TODO fp16 optimizer", "\n", "", "torch", ".", "save", "(", "dump", ",", "f'{self.output_dir}/train_state_{step}.pt'", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.save.save_training_meta": [[17, 56], ["os.makedirs", "os.makedirs", "os.path.join", "os.path.join", "open", "json.dump", "logger.LOGGER.info", "subprocess.run", "subprocess.run.stdout.decode().strip", "logger.LOGGER.info", "subprocess.run", "subprocess.run.stdout.decode().strip", "logger.LOGGER.info", "os.path.abspath", "subprocess.check_output().strip", "os.path.join", "vars", "os.path.dirname", "open", "json.dump", "logger.LOGGER.exception", "logger.LOGGER.warn", "subprocess.run.stdout.decode", "subprocess.run.stdout.decode", "subprocess.check_output", "os.path.join", "bool"], "function", ["None"], ["def", "save_training_meta", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "rank", ">", "0", ":", "\n", "        ", "return", "\n", "\n", "", "os", ".", "makedirs", "(", "join", "(", "args", ".", "output_dir", ",", "'log'", ")", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "join", "(", "args", ".", "output_dir", ",", "'ckpt'", ")", ",", "exist_ok", "=", "True", ")", "\n", "\n", "with", "open", "(", "join", "(", "args", ".", "output_dir", ",", "'log'", ",", "'hps.json'", ")", ",", "'w'", ")", "as", "writer", ":", "\n", "        ", "json", ".", "dump", "(", "vars", "(", "args", ")", ",", "writer", ",", "indent", "=", "4", ")", "\n", "\n", "# if os.path.isfile(args.model_config):", "\n", "#     model_config = json.load(open(args.model_config))", "\n", "#     with open(join(args.output_dir, 'log', 'model.json'), 'w') as writer:", "\n", "#         json.dump(model_config, writer, indent=4)", "\n", "# git info", "\n", "", "try", ":", "\n", "        ", "LOGGER", ".", "info", "(", "\"Waiting on git info....\"", ")", "\n", "c", "=", "subprocess", ".", "run", "(", "[", "\"git\"", ",", "\"rev-parse\"", ",", "\"--abbrev-ref\"", ",", "\"HEAD\"", "]", ",", "\n", "timeout", "=", "10", ",", "stdout", "=", "subprocess", ".", "PIPE", ")", "\n", "git_branch_name", "=", "c", ".", "stdout", ".", "decode", "(", ")", ".", "strip", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"Git branch: %s\"", ",", "git_branch_name", ")", "\n", "c", "=", "subprocess", ".", "run", "(", "[", "\"git\"", ",", "\"rev-parse\"", ",", "\"HEAD\"", "]", ",", "\n", "timeout", "=", "10", ",", "stdout", "=", "subprocess", ".", "PIPE", ")", "\n", "git_sha", "=", "c", ".", "stdout", ".", "decode", "(", ")", ".", "strip", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"Git SHA: %s\"", ",", "git_sha", ")", "\n", "git_dir", "=", "abspath", "(", "dirname", "(", "__file__", ")", ")", "\n", "git_status", "=", "subprocess", ".", "check_output", "(", "\n", "[", "'git'", ",", "'status'", ",", "'--short'", "]", ",", "\n", "cwd", "=", "git_dir", ",", "universal_newlines", "=", "True", ")", ".", "strip", "(", ")", "\n", "with", "open", "(", "join", "(", "args", ".", "output_dir", ",", "'log'", ",", "'git_info.json'", ")", ",", "\n", "'w'", ")", "as", "writer", ":", "\n", "            ", "json", ".", "dump", "(", "{", "'branch'", ":", "git_branch_name", ",", "\n", "'is_dirty'", ":", "bool", "(", "git_status", ")", ",", "\n", "'status'", ":", "git_status", ",", "\n", "'sha'", ":", "git_sha", "}", ",", "\n", "writer", ",", "indent", "=", "4", ")", "\n", "", "", "except", "subprocess", ".", "TimeoutExpired", "as", "e", ":", "\n", "        ", "LOGGER", ".", "exception", "(", "e", ")", "\n", "LOGGER", ".", "warn", "(", "\"Git info not found. Moving right along...\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.NoOp.__getattr__": [[21, 23], ["None"], "methods", ["None"], ["def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "return", "self", ".", "noop", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.NoOp.noop": [[24, 26], ["None"], "methods", ["None"], ["", "def", "noop", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.Struct.__init__": [[70, 72], ["misc.Struct.__dict__.update"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dict_", ")", ":", "\n", "        ", "self", ".", "__dict__", ".", "update", "(", "dict_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.parse_with_config": [[28, 54], ["parser.parse_args", "json.load", "json.load.items", "open", "os.getenv", "arg[].split", "arg.startswith", "setattr", "k.upper", "logger.LOGGER.info", "os.getenv", "isinstance", "isinstance", "isinstance", "setattr", "k.upper", "int", "float", "bool", "k.upper"], "function", ["None"], ["", "", "def", "parse_with_config", "(", "parser", ")", ":", "\n", "    ", "\"\"\"\n    Parse from config files < command lines < system env\n    \"\"\"", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "config", "is", "not", "None", ":", "\n", "        ", "config_args", "=", "json", ".", "load", "(", "open", "(", "args", ".", "config", ")", ")", "\n", "override_keys", "=", "{", "arg", "[", "2", ":", "]", ".", "split", "(", "'='", ")", "[", "0", "]", "for", "arg", "in", "sys", ".", "argv", "[", "1", ":", "]", "\n", "if", "arg", ".", "startswith", "(", "'--'", ")", "}", "\n", "for", "k", ",", "v", "in", "config_args", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "not", "in", "override_keys", ":", "\n", "                ", "setattr", "(", "args", ",", "k", ",", "v", ")", "\n", "", "if", "os", ".", "getenv", "(", "k", ".", "upper", "(", ")", ")", ":", "\n", "                ", "LOGGER", ".", "info", "(", "f\"Replaced {k} from system environment {k.upper()}.\"", ")", "\n", "new_v", "=", "os", ".", "getenv", "(", "k", ".", "upper", "(", ")", ")", "\n", "if", "isinstance", "(", "v", ",", "int", ")", ":", "\n", "                    ", "new_v", "=", "int", "(", "new_v", ")", "\n", "", "if", "isinstance", "(", "v", ",", "float", ")", ":", "\n", "                    ", "new_v", "=", "float", "(", "new_v", ")", "\n", "", "if", "isinstance", "(", "v", ",", "bool", ")", ":", "\n", "                    ", "new_v", "=", "bool", "(", "new_v", ")", "\n", "", "setattr", "(", "args", ",", "k", ",", "new_v", ")", "\n", "\n", "# del args.config", "\n", "# args.model_config = os.path.join(args.pretrained_model_name_or_path, 'config.json')", "\n", "", "", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_dropout": [[74, 81], ["model.named_modules", "isinstance", "logger.LOGGER.info"], "function", ["None"], ["", "", "def", "set_dropout", "(", "model", ",", "drop_p", ")", ":", "\n", "    ", "for", "name", ",", "module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "# we might want to tune dropout for smaller dataset", "\n", "        ", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "Dropout", ")", ":", "\n", "            ", "if", "module", ".", "p", "!=", "drop_p", ":", "\n", "                ", "module", ".", "p", "=", "drop_p", "\n", "LOGGER", ".", "info", "(", "f'{name} set to {drop_p}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.set_random_seed": [[83, 88], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "", "", "", "def", "set_random_seed", "(", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.misc.get_parent_dir": [[90, 92], ["os.path.abspath", "os.path.join"], "function", ["None"], ["", "def", "get_parent_dir", "(", "cur_dir", ")", ":", "\n", "    ", "return", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "os", ".", "path", ".", "pardir", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.scale_input": [[5, 14], ["torch.cat", "delta.squeeze", "padding_embedding.unsqueeze", "torch.add", "range"], "function", ["None"], ["def", "scale_input", "(", "q_emb", ",", "padding_embedding", ",", "num_batches", "=", "10", ")", ":", "\n", "    ", "\"\"\" Create scaled versions of input and stack along batch dimension\n    q_emb shape = (q_length, emb_dim)\n    \"\"\"", "\n", "num_points", "=", "num_batches", "\n", "scale", "=", "1.0", "/", "num_points", "\n", "delta", "=", "(", "q_emb", "-", "padding_embedding", ".", "unsqueeze", "(", "1", ")", ")", "*", "scale", "\n", "batch", "=", "torch", ".", "cat", "(", "[", "torch", ".", "add", "(", "padding_embedding", ",", "delta", "*", "i", ")", "for", "i", "in", "range", "(", "num_points", ")", "]", ",", "dim", "=", "0", ")", "\n", "return", "batch", ",", "delta", ".", "squeeze", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.compute_attributions": [[16, 55], ["tokenizer.encode_plus", "model.bert.embeddings.word_embeddings", "torch.sum", "torch.sum", "input_ids[].tolist", "tokenizer.convert_ids_to_tokens", "torch.autograd.set_grad_enabled", "integrated_gradients.scale_input", "model", "torch.softmax", "torch.autograd.grad", "torch.no_grad", "model", "[].item", "torch.unbind", "prediction[].max"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.scale_input"], ["", "def", "compute_attributions", "(", "sentence1", ",", "sentence2", ",", "model", ",", "padding_embedding", ",", "tokenizer", ",", "target_label_idx", "=", "None", ")", ":", "\n", "    ", "batch_size", "=", "100", "\n", "total_grads", "=", "0", "\n", "inputs", "=", "tokenizer", ".", "encode_plus", "(", "sentence1", ",", "sentence2", ",", "return_tensors", "=", "'pt'", ",", "add_special_tokens", "=", "True", ")", "\n", "token_type_ids", "=", "inputs", "[", "'token_type_ids'", "]", "\n", "input_ids", "=", "inputs", "[", "'input_ids'", "]", "\n", "# diff = 0", "\n", "\n", "if", "target_label_idx", "is", "None", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "prediction", "=", "model", "(", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ")", "\n", "target_label_idx", "=", "prediction", "[", "0", "]", ".", "max", "(", "dim", "=", "1", ")", "[", "1", "]", ".", "item", "(", ")", "\n", "\n", "", "", "emb", "=", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "with", "torch", ".", "autograd", ".", "set_grad_enabled", "(", "True", ")", ":", "\n", "        ", "scaled_q_emb", ",", "delta", "=", "scale_input", "(", "emb", ",", "padding_embedding", ",", "batch_size", ")", "\n", "\n", "scaled_answer", "=", "model", "(", "input_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "scaled_q_emb", ")", "\n", "\n", "output", "=", "torch", ".", "softmax", "(", "scaled_answer", "[", "0", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "gradients", "=", "torch", ".", "autograd", ".", "grad", "(", "torch", ".", "unbind", "(", "output", "[", ":", ",", "target_label_idx", "]", ")", ",", "scaled_q_emb", ")", "\n", "# diff -= output[0, target_label_idx]", "\n", "# baseline_softmax = output[0, :]", "\n", "# diff += output[-1, target_label_idx]", "\n", "\n", "", "total_grads", "+=", "torch", ".", "sum", "(", "gradients", "[", "0", "]", ",", "dim", "=", "0", ")", "\n", "\n", "attributions", "=", "torch", ".", "sum", "(", "total_grads", "*", "delta", ",", "dim", "=", "1", ")", "\n", "\n", "# area = torch.sum(attributions, dim=0)", "\n", "\n", "input_id_list", "=", "input_ids", "[", "0", "]", ".", "tolist", "(", ")", "# Batch index 0", "\n", "tokens", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "input_id_list", ")", "\n", "return", "tokens", ",", "attributions", ",", "target_label_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.calculate_outputs_and_gradients": [[57, 79], ["numpy.array", "integrated_gradients.pre_processing", "model", "F.softmax", "torch.tensor", "output.gather.gather", "model.zero_grad", "output.gather.backward", "np.array.append", "torch.argmax().item", "numpy.ones", "index.cuda.cuda", "pre_processing.grad.detach().cpu().numpy", "torch.argmax", "pre_processing.grad.detach().cpu", "output.gather.size", "pre_processing.grad.detach"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.pre_processing", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.GatherLayer.backward"], ["", "def", "calculate_outputs_and_gradients", "(", "inputs", ",", "model", ",", "target_label_idx", ",", "cuda", "=", "False", ")", ":", "\n", "# do the pre-processing", "\n", "    ", "predict_idx", "=", "None", "\n", "gradients", "=", "[", "]", "\n", "for", "input", "in", "inputs", ":", "\n", "        ", "input", "=", "pre_processing", "(", "input", ",", "cuda", ")", "\n", "output", "=", "model", "(", "input", ")", "\n", "output", "=", "F", ".", "softmax", "(", "output", ",", "dim", "=", "1", ")", "\n", "if", "target_label_idx", "is", "None", ":", "\n", "            ", "target_label_idx", "=", "torch", ".", "argmax", "(", "output", ",", "1", ")", ".", "item", "(", ")", "\n", "", "index", "=", "np", ".", "ones", "(", "(", "output", ".", "size", "(", ")", "[", "0", "]", ",", "1", ")", ")", "*", "target_label_idx", "\n", "index", "=", "torch", ".", "tensor", "(", "index", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "if", "cuda", ":", "\n", "            ", "index", "=", "index", ".", "cuda", "(", ")", "\n", "", "output", "=", "output", ".", "gather", "(", "1", ",", "index", ")", "\n", "# clear grad", "\n", "model", ".", "zero_grad", "(", ")", "\n", "output", ".", "backward", "(", ")", "\n", "gradient", "=", "input", ".", "grad", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "gradients", ".", "append", "(", "gradient", ")", "\n", "", "gradients", "=", "np", ".", "array", "(", "gradients", ")", "\n", "return", "gradients", ",", "target_label_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.pre_processing": [[81, 95], ["numpy.array().reshape", "numpy.array().reshape", "numpy.transpose", "numpy.expand_dims", "numpy.array", "torch.tensor", "torch.device", "torch.device", "numpy.array", "numpy.array"], "function", ["None"], ["", "def", "pre_processing", "(", "obs", ",", "cuda", ")", ":", "\n", "    ", "mean", "=", "np", ".", "array", "(", "[", "0.485", ",", "0.456", ",", "0.406", "]", ")", ".", "reshape", "(", "[", "1", ",", "1", ",", "3", "]", ")", "\n", "std", "=", "np", ".", "array", "(", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", ".", "reshape", "(", "[", "1", ",", "1", ",", "3", "]", ")", "\n", "obs", "=", "obs", "/", "255", "\n", "obs", "=", "(", "obs", "-", "mean", ")", "/", "std", "\n", "obs", "=", "np", ".", "transpose", "(", "obs", ",", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "obs", "=", "np", ".", "expand_dims", "(", "obs", ",", "0", ")", "\n", "obs", "=", "np", ".", "array", "(", "obs", ")", "\n", "if", "cuda", ":", "\n", "        ", "torch_device", "=", "torch", ".", "device", "(", "'cuda:0'", ")", "\n", "", "else", ":", "\n", "        ", "torch_device", "=", "torch", ".", "device", "(", "'cpu'", ")", "\n", "", "obs_tensor", "=", "torch", ".", "tensor", "(", "obs", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "torch_device", ",", "requires_grad", "=", "True", ")", "\n", "return", "obs_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.generate_entrie_images": [[98, 107], ["numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "cv2.resize", "numpy.ones", "numpy.ones"], "function", ["None"], ["", "def", "generate_entrie_images", "(", "img_origin", ",", "img_grad", ",", "img_grad_overlay", ",", "img_integrad", ",", "img_integrad_overlay", ")", ":", "\n", "    ", "blank", "=", "np", ".", "ones", "(", "(", "img_grad", ".", "shape", "[", "0", "]", ",", "10", ",", "3", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "*", "255", "\n", "blank_hor", "=", "np", ".", "ones", "(", "(", "10", ",", "20", "+", "img_grad", ".", "shape", "[", "0", "]", "*", "3", ",", "3", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "*", "255", "\n", "upper", "=", "np", ".", "concatenate", "(", "[", "img_origin", "[", ":", ",", ":", ",", "(", "2", ",", "1", ",", "0", ")", "]", ",", "blank", ",", "img_grad_overlay", ",", "blank", ",", "img_grad", "]", ",", "1", ")", "\n", "down", "=", "np", ".", "concatenate", "(", "[", "img_origin", "[", ":", ",", ":", ",", "(", "2", ",", "1", ",", "0", ")", "]", ",", "blank", ",", "img_integrad_overlay", ",", "blank", ",", "img_integrad", "]", ",", "1", ")", "\n", "total", "=", "np", ".", "concatenate", "(", "[", "upper", ",", "blank_hor", ",", "down", "]", ",", "0", ")", "\n", "total", "=", "cv2", ".", "resize", "(", "total", ",", "(", "550", ",", "364", ")", ")", "\n", "\n", "return", "total", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.integrated_gradients": [[110, 120], ["predict_and_gradients", "numpy.average", "numpy.transpose", "range", "float"], "function", ["None"], ["", "def", "integrated_gradients", "(", "inputs", ",", "model", ",", "target_label_idx", ",", "predict_and_gradients", ",", "baseline", ",", "steps", "=", "50", ",", "cuda", "=", "False", ")", ":", "\n", "    ", "if", "baseline", "is", "None", ":", "\n", "        ", "baseline", "=", "0", "*", "inputs", "\n", "# scale inputs and compute gradients", "\n", "", "scaled_inputs", "=", "[", "baseline", "+", "(", "float", "(", "i", ")", "/", "steps", ")", "*", "(", "inputs", "-", "baseline", ")", "for", "i", "in", "range", "(", "0", ",", "steps", "+", "1", ")", "]", "\n", "grads", ",", "_", "=", "predict_and_gradients", "(", "scaled_inputs", ",", "model", ",", "target_label_idx", ",", "cuda", ")", "\n", "avg_grads", "=", "np", ".", "average", "(", "grads", "[", ":", "-", "1", "]", ",", "axis", "=", "0", ")", "\n", "avg_grads", "=", "np", ".", "transpose", "(", "avg_grads", ",", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "integrated_grad", "=", "(", "inputs", "-", "baseline", ")", "*", "avg_grads", "\n", "return", "integrated_grad", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.random_baseline_integrated_gradients": [[122, 132], ["range", "numpy.average", "integrated_gradients.integrated_gradients", "all_intgrads.append", "print", "numpy.array", "numpy.random.random"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.integrated_gradients.integrated_gradients"], ["", "def", "random_baseline_integrated_gradients", "(", "inputs", ",", "model", ",", "target_label_idx", ",", "predict_and_gradients", ",", "steps", ",", "\n", "num_random_trials", ",", "cuda", ")", ":", "\n", "    ", "all_intgrads", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_random_trials", ")", ":", "\n", "        ", "integrated_grad", "=", "integrated_gradients", "(", "inputs", ",", "model", ",", "target_label_idx", ",", "predict_and_gradients", ",", "baseline", "=", "255.0", "*", "np", ".", "random", ".", "random", "(", "inputs", ".", "shape", ")", ",", "steps", "=", "steps", ",", "cuda", "=", "cuda", ")", "\n", "all_intgrads", ".", "append", "(", "integrated_grad", ")", "\n", "print", "(", "'the trial number is: {}'", ".", "format", "(", "i", ")", ")", "\n", "", "avg_intgrads", "=", "np", ".", "average", "(", "np", ".", "array", "(", "all_intgrads", ")", ",", "axis", "=", "0", ")", "\n", "return", "avg_intgrads", "\n", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.__init__": [[52, 79], ["tree.copy.TreePrettyPrinter.nodecoords", "tree.copy.copy.leaves", "all", "tree.copy.copy.copy", "tree.copy.copy.subtrees", "any", "str", "isinstance", "len", "a.append", "sentence.append", "any", "len", "enumerate", "len", "tree.copy.copy.subtrees", "isinstance", "isinstance", "len", "sentence.append", "type"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.nodecoords"], ["def", "__init__", "(", "self", ",", "tree", ",", "sentence", "=", "None", ",", "highlight", "=", "(", ")", ")", ":", "\n", "        ", "if", "sentence", "is", "None", ":", "\n", "            ", "leaves", "=", "tree", ".", "leaves", "(", ")", "\n", "if", "(", "\n", "leaves", "\n", "and", "not", "any", "(", "len", "(", "a", ")", "==", "0", "for", "a", "in", "tree", ".", "subtrees", "(", ")", ")", "\n", "and", "all", "(", "isinstance", "(", "a", ",", "int", ")", "for", "a", "in", "leaves", ")", "\n", ")", ":", "\n", "                ", "sentence", "=", "[", "str", "(", "a", ")", "for", "a", "in", "leaves", "]", "\n", "", "else", ":", "\n", "# this deals with empty nodes (frontier non-terminals)", "\n", "# and multiple/mixed terminals under non-terminals.", "\n", "                ", "tree", "=", "tree", ".", "copy", "(", "True", ")", "\n", "sentence", "=", "[", "]", "\n", "for", "a", "in", "tree", ".", "subtrees", "(", ")", ":", "\n", "                    ", "if", "len", "(", "a", ")", "==", "0", ":", "\n", "                        ", "a", ".", "append", "(", "len", "(", "sentence", ")", ")", "\n", "sentence", ".", "append", "(", "None", ")", "\n", "", "elif", "any", "(", "not", "isinstance", "(", "b", ",", "Tree", ")", "for", "b", "in", "a", ")", ":", "\n", "                        ", "for", "n", ",", "b", "in", "enumerate", "(", "a", ")", ":", "\n", "                            ", "if", "not", "isinstance", "(", "b", ",", "Tree", ")", ":", "\n", "                                ", "a", "[", "n", "]", "=", "len", "(", "sentence", ")", "\n", "if", "type", "(", "b", ")", "==", "tuple", ":", "\n", "                                    ", "b", "=", "\"/\"", ".", "join", "(", "b", ")", "\n", "", "sentence", ".", "append", "(", "\"%s\"", "%", "b", ")", "\n", "", "", "", "", "", "", "self", ".", "nodes", ",", "self", ".", "coords", ",", "self", ".", "edges", ",", "self", ".", "highlight", "=", "self", ".", "nodecoords", "(", "\n", "tree", ",", "sentence", ",", "highlight", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.__str__": [[81, 83], ["tree.TreePrettyPrinter.text"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.text"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "text", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.__repr__": [[84, 86], ["len"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"<TreePrettyPrinter with %d nodes>\"", "%", "len", "(", "self", ".", "nodes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.nodecoords": [[87, 310], ["tree.copy.copy.leaves", "tree.copy.copy.copy", "tree.copy.copy.subtrees", "set", "tree.copy.copy.treepositions", "collections.defaultdict", "dict", "set", "dict", "terminals.sort", "set", "sorted", "range", "enumerate", "sorted", "nltk.util.OrderedDict", "reversed", "tree[].leaves", "range", "ValueError", "all", "ValueError", "len", "len", "ValueError", "all", "ValueError", "a.sort", "max", "isinstance", "levels[].sort", "sorted.remove", "childcols[].add", "len", "matrix.append", "len", "enumerate", "enumerate", "min", "max", "len", "len", "set", "map", "levels[].append", "terminals.append", "int", "set.discard", "tree.copy.TreePrettyPrinter.nodecoords.findcell"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "nodecoords", "(", "tree", ",", "sentence", ",", "highlight", ")", ":", "\n", "        ", "\"\"\"\n        Produce coordinates of nodes on a grid.\n\n        Objective:\n\n        - Produce coordinates for a non-overlapping placement of nodes and\n            horizontal lines.\n        - Order edges so that crossing edges cross a minimal number of previous\n            horizontal lines (never vertical lines).\n\n        Approach:\n\n        - bottom up level order traversal (start at terminals)\n        - at each level, identify nodes which cannot be on the same row\n        - identify nodes which cannot be in the same column\n        - place nodes into a grid at (row, column)\n        - order child-parent edges with crossing edges last\n\n        Coordinates are (row, column); the origin (0, 0) is at the top left;\n        the root node is on row 0. Coordinates do not consider the size of a\n        node (which depends on font, &c), so the width of a column of the grid\n        should be automatically determined by the element with the greatest\n        width in that column. Alternatively, the integer coordinates could be\n        converted to coordinates in which the distances between adjacent nodes\n        are non-uniform.\n\n        Produces tuple (nodes, coords, edges, highlighted) where:\n\n        - nodes[id]: Tree object for the node with this integer id\n        - coords[id]: (n, m) coordinate where to draw node with id in the grid\n        - edges[id]: parent id of node with this id (ordered dictionary)\n        - highlighted: set of ids that should be highlighted\n        \"\"\"", "\n", "\n", "def", "findcell", "(", "m", ",", "matrix", ",", "startoflevel", ",", "children", ")", ":", "\n", "            ", "\"\"\"\n            Find vacant row, column index for node ``m``.\n            Iterate over current rows for this level (try lowest first)\n            and look for cell between first and last child of this node,\n            add new row to level if no free row available.\n            \"\"\"", "\n", "candidates", "=", "[", "a", "for", "_", ",", "a", "in", "children", "[", "m", "]", "]", "\n", "minidx", ",", "maxidx", "=", "min", "(", "candidates", ")", ",", "max", "(", "candidates", ")", "\n", "leaves", "=", "tree", "[", "m", "]", ".", "leaves", "(", ")", "\n", "center", "=", "scale", "*", "sum", "(", "leaves", ")", "//", "len", "(", "leaves", ")", "# center of gravity", "\n", "if", "minidx", "<", "maxidx", "and", "not", "minidx", "<", "center", "<", "maxidx", ":", "\n", "                ", "center", "=", "sum", "(", "candidates", ")", "//", "len", "(", "candidates", ")", "\n", "", "if", "max", "(", "candidates", ")", "-", "min", "(", "candidates", ")", ">", "2", "*", "scale", ":", "\n", "                ", "center", "-=", "center", "%", "scale", "# round to unscaled coordinate", "\n", "if", "minidx", "<", "maxidx", "and", "not", "minidx", "<", "center", "<", "maxidx", ":", "\n", "                    ", "center", "+=", "scale", "\n", "", "", "if", "ids", "[", "m", "]", "==", "0", ":", "\n", "                ", "startoflevel", "=", "len", "(", "matrix", ")", "\n", "", "for", "rowidx", "in", "range", "(", "startoflevel", ",", "len", "(", "matrix", ")", "+", "1", ")", ":", "\n", "                ", "if", "rowidx", "==", "len", "(", "matrix", ")", ":", "# need to add a new row", "\n", "                    ", "matrix", ".", "append", "(", "\n", "[", "\n", "vertline", "if", "a", "not", "in", "(", "corner", ",", "None", ")", "else", "None", "\n", "for", "a", "in", "matrix", "[", "-", "1", "]", "\n", "]", "\n", ")", "\n", "", "row", "=", "matrix", "[", "rowidx", "]", "\n", "i", "=", "j", "=", "center", "\n", "if", "len", "(", "children", "[", "m", "]", ")", "==", "1", ":", "# place unaries directly above child", "\n", "                    ", "return", "rowidx", ",", "next", "(", "iter", "(", "children", "[", "m", "]", ")", ")", "[", "1", "]", "\n", "", "elif", "all", "(", "\n", "a", "is", "None", "or", "a", "==", "vertline", "\n", "for", "a", "in", "row", "[", "min", "(", "candidates", ")", ":", "max", "(", "candidates", ")", "+", "1", "]", "\n", ")", ":", "\n", "# find free column", "\n", "                    ", "for", "n", "in", "range", "(", "scale", ")", ":", "\n", "                        ", "i", "=", "j", "=", "center", "+", "n", "\n", "while", "j", ">", "minidx", "or", "i", "<", "maxidx", ":", "\n", "                            ", "if", "i", "<", "maxidx", "and", "(", "\n", "matrix", "[", "rowidx", "]", "[", "i", "]", "is", "None", "or", "i", "in", "candidates", "\n", ")", ":", "\n", "                                ", "return", "rowidx", ",", "i", "\n", "", "elif", "j", ">", "minidx", "and", "(", "\n", "matrix", "[", "rowidx", "]", "[", "j", "]", "is", "None", "or", "j", "in", "candidates", "\n", ")", ":", "\n", "                                ", "return", "rowidx", ",", "j", "\n", "", "i", "+=", "scale", "\n", "j", "-=", "scale", "\n", "", "", "", "", "raise", "ValueError", "(", "\n", "\"could not find a free cell for:\\n%s\\n%s\"", "\n", "\"min=%d; max=%d\"", "%", "(", "tree", "[", "m", "]", ",", "minidx", ",", "maxidx", ",", "dumpmatrix", "(", ")", ")", "\n", ")", "\n", "\n", "", "def", "dumpmatrix", "(", ")", ":", "\n", "            ", "\"\"\"Dump matrix contents for debugging purposes.\"\"\"", "\n", "return", "\"\\n\"", ".", "join", "(", "\n", "\"%2d: %s\"", "%", "(", "n", ",", "\" \"", ".", "join", "(", "(", "\"%2r\"", "%", "i", ")", "[", ":", "2", "]", "for", "i", "in", "row", ")", ")", "\n", "for", "n", ",", "row", "in", "enumerate", "(", "matrix", ")", "\n", ")", "\n", "\n", "", "leaves", "=", "tree", ".", "leaves", "(", ")", "\n", "if", "not", "all", "(", "isinstance", "(", "n", ",", "int", ")", "for", "n", "in", "leaves", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"All leaves must be integer indices.\"", ")", "\n", "", "if", "len", "(", "leaves", ")", "!=", "len", "(", "set", "(", "leaves", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Indices must occur at most once.\"", ")", "\n", "", "if", "not", "all", "(", "0", "<=", "n", "<", "len", "(", "sentence", ")", "for", "n", "in", "leaves", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"All leaves must be in the interval 0..n \"", "\n", "\"with n=len(sentence)\\ntokens: %d indices: \"", "\n", "\"%r\\nsentence: %s\"", "%", "(", "len", "(", "sentence", ")", ",", "tree", ".", "leaves", "(", ")", ",", "sentence", ")", "\n", ")", "\n", "", "vertline", ",", "corner", "=", "-", "1", ",", "-", "2", "# constants", "\n", "tree", "=", "tree", ".", "copy", "(", "True", ")", "\n", "for", "a", "in", "tree", ".", "subtrees", "(", ")", ":", "\n", "            ", "a", ".", "sort", "(", "key", "=", "lambda", "n", ":", "min", "(", "n", ".", "leaves", "(", ")", ")", "if", "isinstance", "(", "n", ",", "Tree", ")", "else", "n", ")", "\n", "", "scale", "=", "2", "\n", "crossed", "=", "set", "(", ")", "\n", "# internal nodes and lexical nodes (no frontiers)", "\n", "positions", "=", "tree", ".", "treepositions", "(", ")", "\n", "maxdepth", "=", "max", "(", "map", "(", "len", ",", "positions", ")", ")", "+", "1", "\n", "childcols", "=", "defaultdict", "(", "set", ")", "\n", "matrix", "=", "[", "[", "None", "]", "*", "(", "len", "(", "sentence", ")", "*", "scale", ")", "]", "\n", "nodes", "=", "{", "}", "\n", "ids", "=", "dict", "(", "(", "a", ",", "n", ")", "for", "n", ",", "a", "in", "enumerate", "(", "positions", ")", ")", "\n", "highlighted_nodes", "=", "set", "(", "\n", "n", "for", "a", ",", "n", "in", "ids", ".", "items", "(", ")", "if", "not", "highlight", "or", "tree", "[", "a", "]", "in", "highlight", "\n", ")", "\n", "levels", "=", "dict", "(", "(", "n", ",", "[", "]", ")", "for", "n", "in", "range", "(", "maxdepth", "-", "1", ")", ")", "\n", "terminals", "=", "[", "]", "\n", "for", "a", "in", "positions", ":", "\n", "            ", "node", "=", "tree", "[", "a", "]", "\n", "if", "isinstance", "(", "node", ",", "Tree", ")", ":", "\n", "                ", "levels", "[", "maxdepth", "-", "node", ".", "height", "(", ")", "]", ".", "append", "(", "a", ")", "\n", "", "else", ":", "\n", "                ", "terminals", ".", "append", "(", "a", ")", "\n", "\n", "", "", "for", "n", "in", "levels", ":", "\n", "            ", "levels", "[", "n", "]", ".", "sort", "(", "key", "=", "lambda", "n", ":", "max", "(", "tree", "[", "n", "]", ".", "leaves", "(", ")", ")", "-", "min", "(", "tree", "[", "n", "]", ".", "leaves", "(", ")", ")", ")", "\n", "", "terminals", ".", "sort", "(", ")", "\n", "positions", "=", "set", "(", "positions", ")", "\n", "\n", "for", "m", "in", "terminals", ":", "\n", "            ", "i", "=", "int", "(", "tree", "[", "m", "]", ")", "*", "scale", "\n", "assert", "matrix", "[", "0", "]", "[", "i", "]", "is", "None", ",", "(", "matrix", "[", "0", "]", "[", "i", "]", ",", "m", ",", "i", ")", "\n", "matrix", "[", "0", "]", "[", "i", "]", "=", "ids", "[", "m", "]", "\n", "nodes", "[", "ids", "[", "m", "]", "]", "=", "sentence", "[", "tree", "[", "m", "]", "]", "\n", "if", "nodes", "[", "ids", "[", "m", "]", "]", "is", "None", ":", "\n", "                ", "nodes", "[", "ids", "[", "m", "]", "]", "=", "\"...\"", "\n", "highlighted_nodes", ".", "discard", "(", "ids", "[", "m", "]", ")", "\n", "", "positions", ".", "remove", "(", "m", ")", "\n", "childcols", "[", "m", "[", ":", "-", "1", "]", "]", ".", "add", "(", "(", "0", ",", "i", ")", ")", "\n", "\n", "# add other nodes centered on their children,", "\n", "# if the center is already taken, back off", "\n", "# to the left and right alternately, until an empty cell is found.", "\n", "", "for", "n", "in", "sorted", "(", "levels", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "nodesatdepth", "=", "levels", "[", "n", "]", "\n", "startoflevel", "=", "len", "(", "matrix", ")", "\n", "matrix", ".", "append", "(", "\n", "[", "vertline", "if", "a", "not", "in", "(", "corner", ",", "None", ")", "else", "None", "for", "a", "in", "matrix", "[", "-", "1", "]", "]", "\n", ")", "\n", "for", "m", "in", "nodesatdepth", ":", "# [::-1]:", "\n", "                ", "if", "n", "<", "maxdepth", "-", "1", "and", "childcols", "[", "m", "]", ":", "\n", "                    ", "_", ",", "pivot", "=", "min", "(", "childcols", "[", "m", "]", ",", "key", "=", "itemgetter", "(", "1", ")", ")", "\n", "if", "set", "(", "\n", "a", "[", ":", "-", "1", "]", "\n", "for", "row", "in", "matrix", "[", ":", "-", "1", "]", "\n", "for", "a", "in", "row", "[", ":", "pivot", "]", "\n", "if", "isinstance", "(", "a", ",", "tuple", ")", "\n", ")", "&", "set", "(", "\n", "a", "[", ":", "-", "1", "]", "\n", "for", "row", "in", "matrix", "[", ":", "-", "1", "]", "\n", "for", "a", "in", "row", "[", "pivot", ":", "]", "\n", "if", "isinstance", "(", "a", ",", "tuple", ")", "\n", ")", ":", "\n", "                        ", "crossed", ".", "add", "(", "m", ")", "\n", "\n", "", "", "rowidx", ",", "i", "=", "findcell", "(", "m", ",", "matrix", ",", "startoflevel", ",", "childcols", ")", "\n", "positions", ".", "remove", "(", "m", ")", "\n", "\n", "# block positions where children of this node branch out", "\n", "for", "_", ",", "x", "in", "childcols", "[", "m", "]", ":", "\n", "                    ", "matrix", "[", "rowidx", "]", "[", "x", "]", "=", "corner", "\n", "# assert m == () or matrix[rowidx][i] in (None, corner), (", "\n", "#         matrix[rowidx][i], m, str(tree), ' '.join(sentence))", "\n", "# node itself", "\n", "", "matrix", "[", "rowidx", "]", "[", "i", "]", "=", "ids", "[", "m", "]", "\n", "nodes", "[", "ids", "[", "m", "]", "]", "=", "tree", "[", "m", "]", "\n", "# add column to the set of children for its parent", "\n", "if", "m", "!=", "(", ")", ":", "\n", "                    ", "childcols", "[", "m", "[", ":", "-", "1", "]", "]", ".", "add", "(", "(", "rowidx", ",", "i", ")", ")", "\n", "", "", "", "assert", "len", "(", "positions", ")", "==", "0", "\n", "\n", "# remove unused columns, right to left", "\n", "for", "m", "in", "range", "(", "scale", "*", "len", "(", "sentence", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "if", "not", "any", "(", "isinstance", "(", "row", "[", "m", "]", ",", "(", "Tree", ",", "int", ")", ")", "for", "row", "in", "matrix", ")", ":", "\n", "                ", "for", "row", "in", "matrix", ":", "\n", "                    ", "del", "row", "[", "m", "]", "\n", "\n", "# remove unused rows, reverse", "\n", "", "", "", "matrix", "=", "[", "\n", "row", "\n", "for", "row", "in", "reversed", "(", "matrix", ")", "\n", "if", "not", "all", "(", "a", "is", "None", "or", "a", "==", "vertline", "for", "a", "in", "row", ")", "\n", "]", "\n", "\n", "# collect coordinates of nodes", "\n", "coords", "=", "{", "}", "\n", "for", "n", ",", "_", "in", "enumerate", "(", "matrix", ")", ":", "\n", "            ", "for", "m", ",", "i", "in", "enumerate", "(", "matrix", "[", "n", "]", ")", ":", "\n", "                ", "if", "isinstance", "(", "i", ",", "int", ")", "and", "i", ">=", "0", ":", "\n", "                    ", "coords", "[", "i", "]", "=", "n", ",", "m", "\n", "\n", "# move crossed edges last", "\n", "", "", "", "positions", "=", "sorted", "(", "\n", "[", "a", "for", "level", "in", "levels", ".", "values", "(", ")", "for", "a", "in", "level", "]", ",", "\n", "key", "=", "lambda", "a", ":", "a", "[", ":", "-", "1", "]", "in", "crossed", ",", "\n", ")", "\n", "\n", "# collect edges from node to node", "\n", "edges", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "reversed", "(", "positions", ")", ":", "\n", "            ", "for", "j", ",", "_", "in", "enumerate", "(", "tree", "[", "i", "]", ")", ":", "\n", "                ", "edges", "[", "ids", "[", "i", "+", "(", "j", ",", ")", "]", "]", "=", "ids", "[", "i", "]", "\n", "\n", "", "", "return", "nodes", ",", "coords", ",", "edges", ",", "highlighted_nodes", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.text": [[311, 475], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "re.compile", "sorted", "list", "list", "max", "re.compile.sub().strip.split", "max", "max", "childcols[].add", "min", "max", "result.extend", "isinstance", "tree.TreePrettyPrinter.nodes[].label", "re.compile.sub().strip", "len", "max", "minchildcol.get", "maxchildcol.get", "isinstance", "range", "max", "tree.TreePrettyPrinter.coords.items", "result.append", "reversed", "len", "len", "map", "range", "range", "a.center", "isinstance", "isinstance", "node.label().startswith", "len", "len", "len", "re.compile.sub", "range", "range", "tree.TreePrettyPrinter.text.crosscell"], "methods", ["None"], ["", "def", "text", "(", "\n", "self", ",", "\n", "nodedist", "=", "1", ",", "\n", "unicodelines", "=", "False", ",", "\n", "html", "=", "False", ",", "\n", "ansi", "=", "False", ",", "\n", "nodecolor", "=", "\"blue\"", ",", "\n", "leafcolor", "=", "\"red\"", ",", "\n", "funccolor", "=", "\"green\"", ",", "\n", "abbreviate", "=", "None", ",", "\n", "maxwidth", "=", "16", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        :return: ASCII art for a discontinuous tree.\n\n        :param unicodelines: whether to use Unicode line drawing characters\n            instead of plain (7-bit) ASCII.\n        :param html: whether to wrap output in html code (default plain text).\n        :param ansi: whether to produce colors with ANSI escape sequences\n            (only effective when html==False).\n        :param leafcolor, nodecolor: specify colors of leaves and phrasal\n            nodes; effective when either html or ansi is True.\n        :param abbreviate: if True, abbreviate labels longer than 5 characters.\n            If integer, abbreviate labels longer than `abbr` characters.\n        :param maxwidth: maximum number of characters before a label starts to\n            wrap; pass None to disable.\n        \"\"\"", "\n", "if", "abbreviate", "==", "True", ":", "\n", "            ", "abbreviate", "=", "5", "\n", "", "if", "unicodelines", ":", "\n", "            ", "horzline", "=", "\"\\u2500\"", "\n", "leftcorner", "=", "\"\\u250c\"", "\n", "rightcorner", "=", "\"\\u2510\"", "\n", "vertline", "=", "\" \\u2502 \"", "\n", "tee", "=", "horzline", "+", "\"\\u252C\"", "+", "horzline", "\n", "bottom", "=", "horzline", "+", "\"\\u2534\"", "+", "horzline", "\n", "cross", "=", "horzline", "+", "\"\\u253c\"", "+", "horzline", "\n", "ellipsis", "=", "\"\\u2026\"", "\n", "", "else", ":", "\n", "            ", "horzline", "=", "\"_\"", "\n", "leftcorner", "=", "rightcorner", "=", "\" \"", "\n", "vertline", "=", "\" | \"", "\n", "tee", "=", "3", "*", "horzline", "\n", "cross", "=", "bottom", "=", "\"_|_\"", "\n", "ellipsis", "=", "\".\"", "\n", "\n", "", "def", "crosscell", "(", "cur", ",", "x", "=", "vertline", ")", ":", "\n", "            ", "\"\"\"Overwrite center of this cell with a vertical branch.\"\"\"", "\n", "splitl", "=", "len", "(", "cur", ")", "-", "len", "(", "cur", ")", "//", "2", "-", "len", "(", "x", ")", "//", "2", "-", "1", "\n", "lst", "=", "list", "(", "cur", ")", "\n", "lst", "[", "splitl", ":", "splitl", "+", "len", "(", "x", ")", "]", "=", "list", "(", "x", ")", "\n", "return", "\"\"", ".", "join", "(", "lst", ")", "\n", "\n", "", "result", "=", "[", "]", "\n", "matrix", "=", "defaultdict", "(", "dict", ")", "\n", "maxnodewith", "=", "defaultdict", "(", "lambda", ":", "3", ")", "\n", "maxnodeheight", "=", "defaultdict", "(", "lambda", ":", "1", ")", "\n", "maxcol", "=", "0", "\n", "minchildcol", "=", "{", "}", "\n", "maxchildcol", "=", "{", "}", "\n", "childcols", "=", "defaultdict", "(", "set", ")", "\n", "labels", "=", "{", "}", "\n", "wrapre", "=", "re", ".", "compile", "(", "\n", "\"(.{%d,%d}\\\\b\\\\W*|.{%d})\"", "%", "(", "maxwidth", "-", "4", ",", "maxwidth", ",", "maxwidth", ")", "\n", ")", "\n", "# collect labels and coordinates", "\n", "for", "a", "in", "self", ".", "nodes", ":", "\n", "            ", "row", ",", "column", "=", "self", ".", "coords", "[", "a", "]", "\n", "matrix", "[", "row", "]", "[", "column", "]", "=", "a", "\n", "maxcol", "=", "max", "(", "maxcol", ",", "column", ")", "\n", "label", "=", "(", "\n", "self", ".", "nodes", "[", "a", "]", ".", "label", "(", ")", "\n", "if", "isinstance", "(", "self", ".", "nodes", "[", "a", "]", ",", "Tree", ")", "\n", "else", "self", ".", "nodes", "[", "a", "]", "\n", ")", "\n", "if", "abbreviate", "and", "len", "(", "label", ")", ">", "abbreviate", ":", "\n", "                ", "label", "=", "label", "[", ":", "abbreviate", "]", "+", "ellipsis", "\n", "", "if", "maxwidth", "and", "len", "(", "label", ")", ">", "maxwidth", ":", "\n", "                ", "label", "=", "wrapre", ".", "sub", "(", "r\"\\1\\n\"", ",", "label", ")", ".", "strip", "(", ")", "\n", "", "label", "=", "label", ".", "split", "(", "\"\\n\"", ")", "\n", "maxnodeheight", "[", "row", "]", "=", "max", "(", "maxnodeheight", "[", "row", "]", ",", "len", "(", "label", ")", ")", "\n", "maxnodewith", "[", "column", "]", "=", "max", "(", "maxnodewith", "[", "column", "]", ",", "max", "(", "map", "(", "len", ",", "label", ")", ")", ")", "\n", "labels", "[", "a", "]", "=", "label", "\n", "if", "a", "not", "in", "self", ".", "edges", ":", "\n", "                ", "continue", "# e.g., root", "\n", "", "parent", "=", "self", ".", "edges", "[", "a", "]", "\n", "childcols", "[", "parent", "]", ".", "add", "(", "(", "row", ",", "column", ")", ")", "\n", "minchildcol", "[", "parent", "]", "=", "min", "(", "minchildcol", ".", "get", "(", "parent", ",", "column", ")", ",", "column", ")", "\n", "maxchildcol", "[", "parent", "]", "=", "max", "(", "maxchildcol", ".", "get", "(", "parent", ",", "column", ")", ",", "column", ")", "\n", "# bottom up level order traversal", "\n", "", "for", "row", "in", "sorted", "(", "matrix", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "noderows", "=", "[", "\n", "[", "\"\"", ".", "center", "(", "maxnodewith", "[", "col", "]", ")", "for", "col", "in", "range", "(", "maxcol", "+", "1", ")", "]", "\n", "for", "_", "in", "range", "(", "maxnodeheight", "[", "row", "]", ")", "\n", "]", "\n", "branchrow", "=", "[", "\"\"", ".", "center", "(", "maxnodewith", "[", "col", "]", ")", "for", "col", "in", "range", "(", "maxcol", "+", "1", ")", "]", "\n", "for", "col", "in", "matrix", "[", "row", "]", ":", "\n", "                ", "n", "=", "matrix", "[", "row", "]", "[", "col", "]", "\n", "node", "=", "self", ".", "nodes", "[", "n", "]", "\n", "text", "=", "labels", "[", "n", "]", "\n", "if", "isinstance", "(", "node", ",", "Tree", ")", ":", "\n", "# draw horizontal branch towards children for this node", "\n", "                    ", "if", "n", "in", "minchildcol", "and", "minchildcol", "[", "n", "]", "<", "maxchildcol", "[", "n", "]", ":", "\n", "                        ", "i", ",", "j", "=", "minchildcol", "[", "n", "]", ",", "maxchildcol", "[", "n", "]", "\n", "a", ",", "b", "=", "(", "maxnodewith", "[", "i", "]", "+", "1", ")", "//", "2", "-", "1", ",", "maxnodewith", "[", "j", "]", "//", "2", "\n", "branchrow", "[", "i", "]", "=", "(", "(", "\" \"", "*", "a", ")", "+", "leftcorner", ")", ".", "ljust", "(", "\n", "maxnodewith", "[", "i", "]", ",", "horzline", "\n", ")", "\n", "branchrow", "[", "j", "]", "=", "(", "rightcorner", "+", "(", "\" \"", "*", "b", ")", ")", ".", "rjust", "(", "\n", "maxnodewith", "[", "j", "]", ",", "horzline", "\n", ")", "\n", "for", "i", "in", "range", "(", "minchildcol", "[", "n", "]", "+", "1", ",", "maxchildcol", "[", "n", "]", ")", ":", "\n", "                            ", "if", "i", "==", "col", "and", "any", "(", "a", "==", "i", "for", "_", ",", "a", "in", "childcols", "[", "n", "]", ")", ":", "\n", "                                ", "line", "=", "cross", "\n", "", "elif", "i", "==", "col", ":", "\n", "                                ", "line", "=", "bottom", "\n", "", "elif", "any", "(", "a", "==", "i", "for", "_", ",", "a", "in", "childcols", "[", "n", "]", ")", ":", "\n", "                                ", "line", "=", "tee", "\n", "", "else", ":", "\n", "                                ", "line", "=", "horzline", "\n", "", "branchrow", "[", "i", "]", "=", "line", ".", "center", "(", "maxnodewith", "[", "i", "]", ",", "horzline", ")", "\n", "", "", "else", ":", "# if n and n in minchildcol:", "\n", "                        ", "branchrow", "[", "col", "]", "=", "crosscell", "(", "branchrow", "[", "col", "]", ")", "\n", "", "", "text", "=", "[", "a", ".", "center", "(", "maxnodewith", "[", "col", "]", ")", "for", "a", "in", "text", "]", "\n", "color", "=", "nodecolor", "if", "isinstance", "(", "node", ",", "Tree", ")", "else", "leafcolor", "\n", "if", "isinstance", "(", "node", ",", "Tree", ")", "and", "node", ".", "label", "(", ")", ".", "startswith", "(", "\"-\"", ")", ":", "\n", "                    ", "color", "=", "funccolor", "\n", "", "if", "html", ":", "\n", "                    ", "text", "=", "[", "escape", "(", "a", ",", "quote", "=", "False", ")", "for", "a", "in", "text", "]", "\n", "if", "n", "in", "self", ".", "highlight", ":", "\n", "                        ", "text", "=", "[", "\"<font color=%s>%s</font>\"", "%", "(", "color", ",", "a", ")", "for", "a", "in", "text", "]", "\n", "", "", "elif", "ansi", "and", "n", "in", "self", ".", "highlight", ":", "\n", "                    ", "text", "=", "[", "\"\\x1b[%d;1m%s\\x1b[0m\"", "%", "(", "ANSICOLOR", "[", "color", "]", ",", "a", ")", "for", "a", "in", "text", "]", "\n", "", "for", "x", "in", "range", "(", "maxnodeheight", "[", "row", "]", ")", ":", "\n", "# draw vertical lines in partially filled multiline node", "\n", "# labels, but only if it's not a frontier node.", "\n", "                    ", "noderows", "[", "x", "]", "[", "col", "]", "=", "(", "\n", "text", "[", "x", "]", "\n", "if", "x", "<", "len", "(", "text", ")", "\n", "else", "(", "vertline", "if", "childcols", "[", "n", "]", "else", "\" \"", ")", ".", "center", "(", "\n", "maxnodewith", "[", "col", "]", ",", "\" \"", "\n", ")", "\n", ")", "\n", "#", "\n", "# print(noderows)", "\n", "# print(text)", "\n", "# for each column, if there is a node below us which has a parent", "\n", "# above us, draw a vertical branch in that column.", "\n", "", "", "if", "row", "!=", "max", "(", "matrix", ")", ":", "\n", "                ", "for", "n", ",", "(", "childrow", ",", "col", ")", "in", "self", ".", "coords", ".", "items", "(", ")", ":", "\n", "                    ", "if", "n", ">", "0", "and", "self", ".", "coords", "[", "self", ".", "edges", "[", "n", "]", "]", "[", "0", "]", "<", "row", "<", "childrow", ":", "\n", "                        ", "branchrow", "[", "col", "]", "=", "crosscell", "(", "branchrow", "[", "col", "]", ")", "\n", "if", "col", "not", "in", "matrix", "[", "row", "]", ":", "\n", "                            ", "for", "noderow", "in", "noderows", ":", "\n", "                                ", "noderow", "[", "col", "]", "=", "crosscell", "(", "noderow", "[", "col", "]", ")", "\n", "", "", "", "", "branchrow", "=", "[", "\n", "a", "+", "(", "(", "a", "[", "-", "1", "]", "if", "a", "[", "-", "1", "]", "!=", "\" \"", "else", "b", "[", "0", "]", ")", "*", "nodedist", ")", "\n", "for", "a", ",", "b", "in", "zip", "(", "branchrow", ",", "branchrow", "[", "1", ":", "]", "+", "[", "\" \"", "]", ")", "\n", "]", "\n", "result", ".", "append", "(", "\"\"", ".", "join", "(", "branchrow", ")", ")", "\n", "", "result", ".", "extend", "(", "\n", "(", "\" \"", "*", "nodedist", ")", ".", "join", "(", "noderow", ")", "for", "noderow", "in", "reversed", "(", "noderows", ")", "\n", ")", "\n", "", "return", "\"\\n\"", ".", "join", "(", "reversed", "(", "result", ")", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.TreePrettyPrinter.svg": [[476, 567], ["max", "max", "collections.defaultdict", "tree.TreePrettyPrinter.edges.items", "tree.TreePrettyPrinter.coords.items", "result.append", "result.append", "children[].add", "tree.TreePrettyPrinter.coords.values", "tree.TreePrettyPrinter.coords.values", "min", "max", "isinstance", "isinstance", "node.label().startswith", "escape", "node.label", "isinstance", "node.label"], "methods", ["None"], ["", "def", "svg", "(", "self", ",", "nodecolor", "=", "\"blue\"", ",", "leafcolor", "=", "\"red\"", ",", "funccolor", "=", "\"green\"", ")", ":", "\n", "        ", "\"\"\"\n        :return: SVG representation of a tree.\n        \"\"\"", "\n", "fontsize", "=", "12", "\n", "hscale", "=", "40", "\n", "vscale", "=", "25", "\n", "hstart", "=", "vstart", "=", "20", "\n", "width", "=", "max", "(", "col", "for", "_", ",", "col", "in", "self", ".", "coords", ".", "values", "(", ")", ")", "\n", "height", "=", "max", "(", "row", "for", "row", ",", "_", "in", "self", ".", "coords", ".", "values", "(", ")", ")", "\n", "result", "=", "[", "\n", "'<svg version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" '", "\n", "'width=\"%dem\" height=\"%dem\" viewBox=\"%d %d %d %d\">'", "\n", "%", "(", "\n", "width", "*", "3", ",", "\n", "height", "*", "2.5", ",", "\n", "-", "hstart", ",", "\n", "-", "vstart", ",", "\n", "width", "*", "hscale", "+", "3", "*", "hstart", ",", "\n", "height", "*", "vscale", "+", "3", "*", "vstart", ",", "\n", ")", "\n", "]", "\n", "\n", "children", "=", "defaultdict", "(", "set", ")", "\n", "for", "n", "in", "self", ".", "nodes", ":", "\n", "            ", "if", "n", ":", "\n", "                ", "children", "[", "self", ".", "edges", "[", "n", "]", "]", ".", "add", "(", "n", ")", "\n", "\n", "# horizontal branches from nodes to children", "\n", "", "", "for", "node", "in", "self", ".", "nodes", ":", "\n", "            ", "if", "not", "children", "[", "node", "]", ":", "\n", "                ", "continue", "\n", "", "y", ",", "x", "=", "self", ".", "coords", "[", "node", "]", "\n", "x", "*=", "hscale", "\n", "y", "*=", "vscale", "\n", "x", "+=", "hstart", "\n", "y", "+=", "vstart", "+", "fontsize", "//", "2", "\n", "childx", "=", "[", "self", ".", "coords", "[", "c", "]", "[", "1", "]", "for", "c", "in", "children", "[", "node", "]", "]", "\n", "xmin", "=", "hstart", "+", "hscale", "*", "min", "(", "childx", ")", "\n", "xmax", "=", "hstart", "+", "hscale", "*", "max", "(", "childx", ")", "\n", "result", ".", "append", "(", "\n", "'\\t<polyline style=\"stroke:black; stroke-width:1; fill:none;\" '", "\n", "'points=\"%g,%g %g,%g\" />'", "%", "(", "xmin", ",", "y", ",", "xmax", ",", "y", ")", "\n", ")", "\n", "result", ".", "append", "(", "\n", "'\\t<polyline style=\"stroke:black; stroke-width:1; fill:none;\" '", "\n", "'points=\"%g,%g %g,%g\" />'", "%", "(", "x", ",", "y", ",", "x", ",", "y", "-", "fontsize", "//", "3", ")", "\n", ")", "\n", "\n", "# vertical branches from children to parents", "\n", "", "for", "child", ",", "parent", "in", "self", ".", "edges", ".", "items", "(", ")", ":", "\n", "            ", "y", ",", "_", "=", "self", ".", "coords", "[", "parent", "]", "\n", "y", "*=", "vscale", "\n", "y", "+=", "vstart", "+", "fontsize", "//", "2", "\n", "childy", ",", "childx", "=", "self", ".", "coords", "[", "child", "]", "\n", "childx", "*=", "hscale", "\n", "childy", "*=", "vscale", "\n", "childx", "+=", "hstart", "\n", "childy", "+=", "vstart", "-", "fontsize", "\n", "result", "+=", "[", "\n", "'\\t<polyline style=\"stroke:white; stroke-width:10; fill:none;\"'", "\n", "' points=\"%g,%g %g,%g\" />'", "%", "(", "childx", ",", "childy", ",", "childx", ",", "y", "+", "5", ")", ",", "\n", "'\\t<polyline style=\"stroke:black; stroke-width:1; fill:none;\"'", "\n", "' points=\"%g,%g %g,%g\" />'", "%", "(", "childx", ",", "childy", ",", "childx", ",", "y", ")", ",", "\n", "]", "\n", "\n", "# write nodes with coordinates", "\n", "", "for", "n", ",", "(", "row", ",", "column", ")", "in", "self", ".", "coords", ".", "items", "(", ")", ":", "\n", "            ", "node", "=", "self", ".", "nodes", "[", "n", "]", "\n", "x", "=", "column", "*", "hscale", "+", "hstart", "\n", "y", "=", "row", "*", "vscale", "+", "vstart", "\n", "if", "n", "in", "self", ".", "highlight", ":", "\n", "                ", "color", "=", "nodecolor", "if", "isinstance", "(", "node", ",", "Tree", ")", "else", "leafcolor", "\n", "if", "isinstance", "(", "node", ",", "Tree", ")", "and", "node", ".", "label", "(", ")", ".", "startswith", "(", "\"-\"", ")", ":", "\n", "                    ", "color", "=", "funccolor", "\n", "", "", "else", ":", "\n", "                ", "color", "=", "\"black\"", "\n", "", "result", "+=", "[", "\n", "'\\t<text style=\"text-anchor: middle; fill: %s; '", "\n", "'font-size: %dpx;\" x=\"%g\" y=\"%g\">%s</text>'", "\n", "%", "(", "\n", "color", ",", "\n", "fontsize", ",", "\n", "x", ",", "\n", "y", ",", "\n", "escape", "(", "node", ".", "label", "(", ")", "if", "isinstance", "(", "node", ",", "Tree", ")", "else", "node", ",", "quote", "=", "False", ")", ",", "\n", ")", "\n", "]", "\n", "\n", "", "result", "+=", "[", "\"</svg>\"", "]", "\n", "return", "\"\\n\"", ".", "join", "(", "result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.utils.tree.test": [[569, 603], ["print", "print", "print", "nltk.tree.Tree.fromstring", "tree.test.print_tree"], "function", ["None"], ["", "", "def", "test", "(", ")", ":", "\n", "    ", "\"\"\"Do some tree drawing tests.\"\"\"", "\n", "\n", "def", "print_tree", "(", "n", ",", "tree", ",", "sentence", "=", "None", ",", "ansi", "=", "True", ",", "**", "xargs", ")", ":", "\n", "        ", "print", "(", ")", "\n", "print", "(", "'{0}: \"{1}\"'", ".", "format", "(", "n", ",", "\" \"", ".", "join", "(", "sentence", "or", "tree", ".", "leaves", "(", ")", ")", ")", ")", "\n", "print", "(", "tree", ")", "\n", "print", "(", ")", "\n", "drawtree", "=", "TreePrettyPrinter", "(", "tree", ",", "sentence", ")", "\n", "try", ":", "\n", "            ", "print", "(", "drawtree", ".", "text", "(", "unicodelines", "=", "ansi", ",", "ansi", "=", "ansi", ",", "**", "xargs", ")", ")", "\n", "", "except", "(", "UnicodeDecodeError", ",", "UnicodeEncodeError", ")", ":", "\n", "            ", "print", "(", "drawtree", ".", "text", "(", "unicodelines", "=", "False", ",", "ansi", "=", "False", ",", "**", "xargs", ")", ")", "\n", "\n", "", "", "from", "nltk", ".", "corpus", "import", "treebank", "\n", "\n", "for", "n", "in", "[", "0", ",", "1440", ",", "1591", ",", "2771", ",", "2170", "]", ":", "\n", "        ", "tree", "=", "treebank", ".", "parsed_sents", "(", ")", "[", "n", "]", "\n", "print_tree", "(", "n", ",", "tree", ",", "nodedist", "=", "2", ",", "maxwidth", "=", "8", ")", "\n", "", "print", "(", ")", "\n", "print", "(", "\"ASCII version:\"", ")", "\n", "print", "(", "TreePrettyPrinter", "(", "tree", ")", ".", "text", "(", "nodedist", "=", "2", ")", ")", "\n", "\n", "tree", "=", "Tree", ".", "fromstring", "(", "\n", "\"(top (punct 8) (smain (noun 0) (verb 1) (inf (verb 5) (inf (verb 6) \"", "\n", "\"(conj (inf (pp (prep 2) (np (det 3) (noun 4))) (verb 7)) (inf (verb 9)) \"", "\n", "\"(vg 10) (inf (verb 11)))))) (punct 12))\"", ",", "\n", "read_leaf", "=", "int", ",", "\n", ")", "\n", "sentence", "=", "(", "\n", "\"Ze had met haar moeder kunnen gaan winkelen ,\"", "\n", "\" zwemmen of terrassen .\"", ".", "split", "(", ")", "\n", ")", "\n", "print_tree", "(", "\"Discontinuous tree\"", ",", "tree", ",", "sentence", ",", "nodedist", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.GatherLayer.forward": [[12, 19], ["ctx.save_for_backward", "torch.all_gather", "torch.all_gather", "torch.all_gather", "torch.all_gather", "tuple", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "input", ")", "\n", "output", "=", "[", "torch", ".", "zeros_like", "(", "input", ")", "for", "_", "in", "range", "(", "dist", ".", "get_world_size", "(", ")", ")", "]", "\n", "dist", ".", "all_gather", "(", "output", ",", "input", ")", "\n", "return", "tuple", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.GatherLayer.backward": [[20, 26], ["torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.get_rank", "torch.get_rank", "torch.get_rank", "torch.get_rank"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "*", "grads", ")", ":", "\n", "        ", "input", ",", "=", "ctx", ".", "saved_tensors", "\n", "grad_out", "=", "torch", ".", "zeros_like", "(", "input", ")", "\n", "grad_out", "[", ":", "]", "=", "grads", "[", "dist", ".", "get_rank", "(", ")", "]", "\n", "return", "grad_out", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.NTXentLoss.__init__": [[33, 42], ["torch.Module.__init__", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "loss.NTXentLoss._get_correlated_mask().type", "loss.NTXentLoss._get_similarity_function", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "loss.NTXentLoss._get_correlated_mask"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.NTXentLoss._get_similarity_function", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.NTXentLoss._get_correlated_mask"], ["def", "__init__", "(", "self", ",", "device", ",", "batch_size", ",", "temperature", ",", "use_cosine_similarity", ")", ":", "\n", "        ", "super", "(", "NTXentLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "softmax", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "mask_samples_from_same_repr", "=", "self", ".", "_get_correlated_mask", "(", ")", ".", "type", "(", "torch", ".", "bool", ")", "\n", "self", ".", "similarity_function", "=", "self", ".", "_get_similarity_function", "(", "use_cosine_similarity", ")", "\n", "self", ".", "criterion", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.NTXentLoss._get_similarity_function": [[43, 49], ["torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity", "torch.nn.CosineSimilarity"], "methods", ["None"], ["", "def", "_get_similarity_function", "(", "self", ",", "use_cosine_similarity", ")", ":", "\n", "        ", "if", "use_cosine_similarity", ":", "\n", "            ", "self", ".", "_cosine_similarity", "=", "torch", ".", "nn", ".", "CosineSimilarity", "(", "dim", "=", "-", "1", ")", "\n", "return", "self", ".", "_cosine_simililarity", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_dot_simililarity", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.NTXentLoss._get_correlated_mask": [[50, 57], ["numpy.eye", "numpy.eye", "numpy.eye", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy.to", "torch.from_numpy.to", "torch.from_numpy.to", "torch.from_numpy.to"], "methods", ["None"], ["", "", "def", "_get_correlated_mask", "(", "self", ")", ":", "\n", "        ", "diag", "=", "np", ".", "eye", "(", "2", "*", "self", ".", "batch_size", ")", "\n", "l1", "=", "np", ".", "eye", "(", "(", "2", "*", "self", ".", "batch_size", ")", ",", "2", "*", "self", ".", "batch_size", ",", "k", "=", "-", "self", ".", "batch_size", ")", "\n", "l2", "=", "np", ".", "eye", "(", "(", "2", "*", "self", ".", "batch_size", ")", ",", "2", "*", "self", ".", "batch_size", ",", "k", "=", "self", ".", "batch_size", ")", "\n", "mask", "=", "torch", ".", "from_numpy", "(", "(", "diag", "+", "l1", "+", "l2", ")", ")", "\n", "mask", "=", "(", "1", "-", "mask", ")", ".", "type", "(", "torch", ".", "bool", ")", "\n", "return", "mask", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.NTXentLoss._dot_simililarity": [[58, 65], ["torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "torch.tensordot", "x.unsqueeze", "y.T.unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_dot_simililarity", "(", "x", ",", "y", ")", ":", "\n", "        ", "v", "=", "torch", ".", "tensordot", "(", "x", ".", "unsqueeze", "(", "1", ")", ",", "y", ".", "T", ".", "unsqueeze", "(", "0", ")", ",", "dims", "=", "2", ")", "\n", "# x shape: (N, 1, C)", "\n", "# y shape: (1, C, 2N)", "\n", "# v shape: (N, 2N)", "\n", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.NTXentLoss._cosine_simililarity": [[66, 72], ["loss.NTXentLoss._cosine_similarity", "x.unsqueeze", "y.unsqueeze"], "methods", ["None"], ["", "def", "_cosine_simililarity", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "# x shape: (N, 1, C)", "\n", "# y shape: (1, 2N, C)", "\n", "# v shape: (N, 2N)", "\n", "        ", "v", "=", "self", ".", "_cosine_similarity", "(", "x", ".", "unsqueeze", "(", "1", ")", ",", "y", ".", "unsqueeze", "(", "0", ")", ")", "\n", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.NTXentLoss.forward": [[73, 92], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "loss.NTXentLoss.NTXentLoss.similarity_function", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "similarity_matrix[].view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "torch.zeros().to().long", "loss.NTXentLoss.NTXentLoss.criterion", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "zis", ",", "zjs", ")", ":", "\n", "        ", "representations", "=", "torch", ".", "cat", "(", "[", "zjs", ",", "zis", "]", ",", "dim", "=", "0", ")", "\n", "\n", "similarity_matrix", "=", "self", ".", "similarity_function", "(", "representations", ",", "representations", ")", "\n", "\n", "# filter out the scores from the positive samples", "\n", "l_pos", "=", "torch", ".", "diag", "(", "similarity_matrix", ",", "self", ".", "batch_size", ")", "\n", "r_pos", "=", "torch", ".", "diag", "(", "similarity_matrix", ",", "-", "self", ".", "batch_size", ")", "\n", "positives", "=", "torch", ".", "cat", "(", "[", "l_pos", ",", "r_pos", "]", ")", ".", "view", "(", "2", "*", "self", ".", "batch_size", ",", "1", ")", "\n", "\n", "negatives", "=", "similarity_matrix", "[", "self", ".", "mask_samples_from_same_repr", "]", ".", "view", "(", "2", "*", "self", ".", "batch_size", ",", "-", "1", ")", "\n", "\n", "logits", "=", "torch", ".", "cat", "(", "(", "positives", ",", "negatives", ")", ",", "dim", "=", "1", ")", "\n", "logits", "/=", "self", ".", "temperature", "\n", "\n", "labels", "=", "torch", ".", "zeros", "(", "2", "*", "self", ".", "batch_size", ")", ".", "to", "(", "self", ".", "device", ")", ".", "long", "(", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "logits", ",", "labels", ")", "\n", "\n", "return", "loss", "/", "(", "2", "*", "self", ".", "batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.ContrastiveLoss.__init__": [[96, 99], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tau", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temperature", "=", "tau", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.ContrastiveLoss.forward": [[100, 117], ["out_1.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "sim_matrix.masked_select().view.masked_select().view.masked_select().view", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "sim_matrix.masked_select().view.masked_select().view.masked_select", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cat.t().contiguous", "torch.cat.t().contiguous", "torch.cat.t().contiguous", "torch.cat.t().contiguous", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "sim_matrix.masked_select().view.masked_select().view.sum", "torch.cat.t", "torch.cat.t", "torch.cat.t", "torch.cat.t"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "out_1", ",", "out_2", ")", ":", "\n", "        ", "batch_size", "=", "out_1", ".", "size", "(", "0", ")", "\n", "out_1", ",", "out_2", "=", "F", ".", "normalize", "(", "out_1", ",", "dim", "=", "-", "1", ")", ",", "F", ".", "normalize", "(", "out_2", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# [2*B, D]", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out_1", ",", "out_2", "]", ",", "dim", "=", "0", ")", "\n", "# [2*B, 2*B]", "\n", "sim_matrix", "=", "torch", ".", "exp", "(", "torch", ".", "mm", "(", "out", ",", "out", ".", "t", "(", ")", ".", "contiguous", "(", ")", ")", "/", "self", ".", "temperature", ")", "\n", "mask", "=", "(", "torch", ".", "ones_like", "(", "sim_matrix", ")", "-", "torch", ".", "eye", "(", "2", "*", "batch_size", ",", "device", "=", "sim_matrix", ".", "device", ")", ")", ".", "bool", "(", ")", "\n", "# [2*B, 2*B-1]", "\n", "sim_matrix", "=", "sim_matrix", ".", "masked_select", "(", "mask", ")", ".", "view", "(", "2", "*", "batch_size", ",", "-", "1", ")", "\n", "\n", "# compute loss", "\n", "pos_sim", "=", "torch", ".", "exp", "(", "torch", ".", "sum", "(", "out_1", "*", "out_2", ",", "dim", "=", "-", "1", ")", "/", "self", ".", "temperature", ")", "\n", "# [2*B]", "\n", "pos_sim", "=", "torch", ".", "cat", "(", "[", "pos_sim", ",", "pos_sim", "]", ",", "dim", "=", "0", ")", "\n", "return", "-", "torch", ".", "log", "(", "pos_sim", "/", "sim_matrix", ".", "sum", "(", "dim", "=", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.FocalLoss.__init__": [[220, 248], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "alpha", ":", "float", "=", "0.25", ",", "\n", "gamma", ":", "float", "=", "2", ",", "\n", "reduction", ":", "str", "=", "\"none\"", ")", ":", "\n", "        ", "\"\"\"\n        Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py .\n        Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n        Args:\n            inputs: A float tensor of arbitrary shape.\n                    The predictions for each example.\n            targets: A float tensor with the same shape as inputs. Stores the binary\n                     classification label for each element in inputs\n                    (0 for the negative class and 1 for the positive class).\n            alpha: (optional) Weighting factor in range (0,1) to balance\n                    positive vs negative examples or -1 for ignore. Default = 0.25\n            gamma: Exponent of the modulating factor (1 - p_t) to\n                   balance easy vs hard examples.\n            reduction: 'none' | 'mean' | 'sum'\n                     'none': No reduction will be applied to the output.\n                     'mean': The output will be averaged.\n                     'sum': The output will be summed.\n        Returns:\n            Loss tensor with the reduction option applied.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "reduction", "=", "reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.loss.FocalLoss.forward": [[249, 262], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "loss.mean"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "targets", ")", ":", "\n", "        ", "p", "=", "torch", ".", "sigmoid", "(", "inputs", ")", "\n", "ce_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "inputs", ",", "targets", ",", "reduction", "=", "\"none\"", "\n", ")", "\n", "p_t", "=", "p", "*", "targets", "+", "(", "1", "-", "p", ")", "*", "(", "1", "-", "targets", ")", "\n", "loss", "=", "ce_loss", "*", "(", "(", "1", "-", "p_t", ")", "**", "self", ".", "gamma", ")", "\n", "\n", "if", "self", ".", "alpha", ">=", "0", ":", "\n", "            ", "alpha_t", "=", "self", ".", "alpha", "*", "targets", "+", "(", "1", "-", "self", ".", "alpha", ")", "*", "(", "1", "-", "targets", ")", "\n", "loss", "=", "alpha_t", "*", "loss", "\n", "\n", "", "return", "loss", ".", "mean", "(", ")", "if", "self", ".", "reduction", "==", "\"mean\"", "else", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.misc.build_optimizer": [[11, 31], ["OptimCls", "ValueError", "model.named_parameters", "model.named_parameters", "any", "any"], "function", ["None"], ["\n", "import", "numpy", "as", "np", "\n", "import", "torch", "\n", "\n", "from", ".", "logger", "import", "LOGGER", "\n", "\n", "\n", "class", "NoOp", "(", "object", ")", ":", "\n", "    ", "\"\"\" useful for distributed training No-Ops \"\"\"", "\n", "\n", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "return", "self", ".", "noop", "\n", "\n", "", "def", "noop", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "\n", "\n", "\n", "", "", "def", "parse_with_config", "(", "parser", ")", ":", "\n", "    ", "\"\"\"\n    Parse from config files < command lines < system env\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.noam_schedule": [[10, 15], ["None"], "function", ["None"], ["def", "noam_schedule", "(", "step", ",", "warmup_step", "=", "4000", ")", ":", "\n", "    ", "\"\"\" original Transformer schedule\"\"\"", "\n", "if", "step", "<=", "warmup_step", ":", "\n", "        ", "return", "step", "/", "warmup_step", "\n", "", "return", "(", "warmup_step", "**", "0.5", ")", "*", "(", "step", "**", "-", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.warmup_linear": [[17, 22], ["max"], "function", ["None"], ["", "def", "warmup_linear", "(", "step", ",", "warmup_step", ",", "tot_step", ")", ":", "\n", "    ", "\"\"\" BERT schedule \"\"\"", "\n", "if", "step", "<", "warmup_step", ":", "\n", "        ", "return", "step", "/", "warmup_step", "\n", "", "return", "max", "(", "0", ",", "(", "tot_step", "-", "step", ")", "/", "(", "tot_step", "-", "warmup_step", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.vqa_schedule": [[24, 38], ["math.ceil"], "function", ["None"], ["", "def", "vqa_schedule", "(", "step", ",", "warmup_interval", ",", "decay_interval", ",", "\n", "decay_start", ",", "decay_rate", ")", ":", "\n", "    ", "\"\"\" VQA schedule from MCAN \"\"\"", "\n", "if", "step", "<", "warmup_interval", ":", "\n", "        ", "return", "1", "/", "4", "\n", "", "elif", "step", "<", "2", "*", "warmup_interval", ":", "\n", "        ", "return", "2", "/", "4", "\n", "", "elif", "step", "<", "3", "*", "warmup_interval", ":", "\n", "        ", "return", "3", "/", "4", "\n", "", "elif", "step", ">=", "decay_start", ":", "\n", "        ", "num_decay", "=", "ceil", "(", "(", "step", "-", "decay_start", ")", "/", "decay_interval", ")", "\n", "return", "decay_rate", "**", "num_decay", "\n", "", "else", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.get_lr_sched": [[40, 47], ["sched.warmup_linear"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.optim.sched.warmup_linear"], ["", "", "def", "get_lr_sched", "(", "global_step", ",", "opts", ")", ":", "\n", "# learning rate scheduling", "\n", "    ", "lr_this_step", "=", "opts", ".", "learning_rate", "*", "warmup_linear", "(", "\n", "global_step", ",", "opts", ".", "warmup_steps", ",", "opts", ".", "num_train_steps", ")", "\n", "if", "lr_this_step", "<=", "0", ":", "\n", "        ", "lr_this_step", "=", "1e-8", "\n", "", "return", "lr_this_step", "\n", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.Attention.__init__": [[10, 18], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_hidden_enc", ",", "n_hidden_dec", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "h_hidden_enc", "=", "n_hidden_enc", "\n", "self", ".", "h_hidden_dec", "=", "n_hidden_dec", "\n", "\n", "self", ".", "W", "=", "nn", ".", "Linear", "(", "n_hidden_enc", "+", "n_hidden_dec", ",", "n_hidden_dec", ",", "bias", "=", "False", ")", "\n", "self", ".", "V", "=", "nn", ".", "Parameter", "(", "torch", ".", "rand", "(", "n_hidden_dec", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.Attention.forward": [[19, 45], ["last_layer_enc.size", "last_layer_enc.size", "hidden_dec.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "tanh_W_s_h.permute.permute.permute", "attention.Attention.V.repeat().unsqueeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.softmax", "torch.softmax", "torch.softmax", "attention.Attention.W", "hidden_dec.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attention.Attention.V.repeat", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_dec", ",", "last_layer_enc", ",", "attention_mask", ")", ":", "\n", "        ", "'''\n            PARAMS:\n                hidden_dec:     [b, n_hidden_dec]\n                last_layer_enc: [b, seq_len, n_hidden_enc * 2]\n\n            RETURN:\n                att_weights:    [b, src_seq_len]\n        '''", "\n", "\n", "batch_size", "=", "last_layer_enc", ".", "size", "(", "0", ")", "\n", "src_seq_len", "=", "last_layer_enc", ".", "size", "(", "1", ")", "\n", "\n", "hidden_dec", "=", "hidden_dec", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "src_seq_len", ",", "1", ")", "# [b, src_seq_len, n_hidden_dec]", "\n", "\n", "tanh_W_s_h", "=", "torch", ".", "tanh", "(", "\n", "self", ".", "W", "(", "torch", ".", "cat", "(", "(", "hidden_dec", ",", "last_layer_enc", ")", ",", "dim", "=", "-", "1", ")", ")", ")", "# [b, src_seq_len, n_hidden_dec]", "\n", "tanh_W_s_h", "=", "tanh_W_s_h", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# [b, n_hidde_dec, seq_len]", "\n", "\n", "V", "=", "self", ".", "V", ".", "repeat", "(", "batch_size", ",", "1", ")", ".", "unsqueeze", "(", "1", ")", "# [b, 1, n_hidden_dec]", "\n", "e", "=", "torch", ".", "bmm", "(", "V", ",", "tanh_W_s_h", ")", ".", "squeeze", "(", "1", ")", "# [b, seq_len]", "\n", "e", "=", "attention_mask", "+", "e", "\n", "\n", "att_weights", "=", "F", ".", "softmax", "(", "e", ",", "dim", "=", "1", ")", "# [b, src_seq_len]", "\n", "\n", "return", "att_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.ContrastiveCoAttention.__init__": [[49, 52], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "affinity_linear", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.ContrastiveCoAttention.forward": [[53, 75], ["attention.ContrastiveCoAttention.affinity_linear", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "chengyubert.modules.utils.masked_softmax", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "chengyubert.modules.utils.masked_softmax", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.bmm.max", "torch.bmm.max", "torch.bmm.max", "torch.bmm.max", "torch.bmm.max", "torch.bmm.max"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.masked_softmax", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.masked_softmax"], ["", "def", "forward", "(", "self", ",", "L", ",", "I", ",", "mask_L", ",", "mask_I", ")", ":", "\n", "# L = idiom_states", "\n", "# I = encoded_context_masked", "\n", "# I = encoded_context_masked", "\n", "\n", "# idiom_length = (gather_index > 0).sum(1)", "\n", "# idiom_mask = sequence_mask(idiom_length)", "\n", "\n", "        ", "AI", "=", "self", ".", "affinity_linear", "(", "I", ")", "\n", "\n", "# co attention", "\n", "L_T", "=", "torch", ".", "transpose", "(", "L", ",", "1", ",", "2", ")", "# B x l x m + 1", "\n", "Z", "=", "torch", ".", "bmm", "(", "AI", ",", "L_T", ")", "# L = B x n + 1 x m + 1", "\n", "\n", "# row max", "\n", "A_L_", "=", "masked_softmax", "(", "Z", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", ",", "mask", "=", "mask_L", ")", "# B x n + 1 x m + 1", "\n", "C_L", "=", "torch", ".", "einsum", "(", "'bn,bnd->bd'", ",", "[", "A_L_", ",", "L", "]", ")", "\n", "\n", "# col max", "\n", "A_I_", "=", "masked_softmax", "(", "Z", ".", "max", "(", "dim", "=", "2", ")", "[", "0", "]", ",", "mask", "=", "mask_I", ")", "# B x n + 1 x m + 1", "\n", "C_I", "=", "torch", ".", "einsum", "(", "'bn,bnd->bd'", ",", "[", "A_I_", ",", "I", "]", ")", "\n", "return", "C_L", ",", "C_I", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.BahdanauAttention.__init__": [[78, 85], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "torch.SELU", "torch.SELU", "torch.SELU", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.SELU", "torch.SELU", "torch.SELU", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear_encoder", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "self", ".", "linear_decoder", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "self", ".", "linear_in", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ",", "nn", ".", "SELU", "(", ")", ",", "nn", ".", "Dropout", "(", "p", "=", "0.1", ")", ",", "\n", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ",", "nn", ".", "SELU", "(", ")", ",", "nn", ".", "Dropout", "(", "p", "=", "0.1", ")", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.BahdanauAttention.init_context": [[86, 88], ["context.transpose"], "methods", ["None"], ["", "def", "init_context", "(", "self", ",", "context", ")", ":", "\n", "        ", "self", ".", "context", "=", "context", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.BahdanauAttention.forward": [[89, 95], ["attention.BahdanauAttention.linear_in", "sequence_output.transpose", "attention.BahdanauAttention.tanh", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "graph_output", ",", "sequence_output", ")", ":", "\n", "        ", "gamma_encoder", "=", "self", ".", "linear_in", "(", "graph_output", ")", "\n", "self", ".", "context", "=", "sequence_output", ".", "transpose", "(", "1", ",", "2", ")", "\n", "weights", "=", "self", ".", "tanh", "(", "torch", ".", "bmm", "(", "gamma_encoder", ",", "self", ".", "context", ")", "/", "4", ")", "\n", "\n", "return", "weights", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.bahdanau_attention_3.__init__": [[98, 105], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "torch.SELU", "torch.SELU", "torch.SELU", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.SELU", "torch.SELU", "torch.SELU", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", "bahdanau_attention_3", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear_encoder", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "self", ".", "linear_decoder", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "self", ".", "linear_in", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ",", "nn", ".", "SELU", "(", ")", ",", "nn", ".", "Dropout", "(", "p", "=", "0.1", ")", ",", "\n", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ",", "nn", ".", "SELU", "(", ")", ",", "nn", ".", "Dropout", "(", "p", "=", "0.1", ")", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.bahdanau_attention_3.init_context": [[106, 108], ["context.transpose"], "methods", ["None"], ["", "def", "init_context", "(", "self", ",", "context", ")", ":", "\n", "        ", "self", ".", "context", "=", "context", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.attention.bahdanau_attention_3.forward": [[109, 115], ["attention.bahdanau_attention_3.linear_in", "sequence_output.transpose", "attention.bahdanau_attention_3.tanh", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "graph_output", ",", "sequence_output", ")", ":", "\n", "        ", "gamma_encoder", "=", "self", ".", "linear_in", "(", "graph_output", ")", "\n", "self", ".", "context", "=", "sequence_output", ".", "transpose", "(", "1", ",", "2", ")", "\n", "weights", "=", "self", ".", "tanh", "(", "torch", ".", "bmm", "(", "gamma_encoder", ",", "self", ".", "context", ")", "/", "4", ")", "\n", "\n", "return", "weights", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.BinaryTreeLSTMLayer.__init__": [[121, 126], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_dim", ")", ":", "\n", "        ", "super", "(", "BinaryTreeLSTMLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "comp_linear", "=", "nn", ".", "Linear", "(", "in_features", "=", "2", "*", "hidden_dim", ",", "\n", "out_features", "=", "5", "*", "hidden_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.BinaryTreeLSTMLayer.forward": [[127, 148], ["torch.cat", "utils.BinaryTreeLSTMLayer.comp_linear", "utils.BinaryTreeLSTMLayer.chunk", "o.sigmoid", "c.tanh", "u.tanh", "i.sigmoid"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "l", "=", "None", ",", "r", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            l: A (h_l, c_l) tuple, where each value has the size\n                (batch_size, max_length, hidden_dim).\n            r: A (h_r, c_r) tuple, where each value has the size\n                (batch_size, max_length, hidden_dim).\n        Returns:\n            h, c: The hidden and cell state of the composed parent,\n                each of which has the size\n                (batch_size, max_length - 1, hidden_dim).\n        \"\"\"", "\n", "hl", ",", "cl", "=", "l", "\n", "hr", ",", "cr", "=", "r", "\n", "hlr_cat", "=", "torch", ".", "cat", "(", "[", "hl", ",", "hr", "]", ",", "dim", "=", "2", ")", "\n", "treelstm_vector", "=", "self", ".", "comp_linear", "(", "hlr_cat", ")", "\n", "i", ",", "fl", ",", "fr", ",", "u", ",", "o", "=", "treelstm_vector", ".", "chunk", "(", "chunks", "=", "5", ",", "dim", "=", "2", ")", "\n", "c", "=", "(", "cl", "*", "(", "fl", "+", "1", ")", ".", "sigmoid", "(", ")", "+", "cr", "*", "(", "fr", "+", "1", ")", ".", "sigmoid", "(", ")", "\n", "+", "u", ".", "tanh", "(", ")", "*", "i", ".", "sigmoid", "(", ")", ")", "\n", "h", "=", "o", ".", "sigmoid", "(", ")", "*", "c", ".", "tanh", "(", ")", "\n", "return", "h", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.LatentComposition.__init__": [[152, 163], ["torch.nn.Module.__init__", "torch.nn.LSTMCell", "torch.nn.LSTMCell", "utils.BinaryTreeLSTMLayer", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gumbel_temperature", "=", "1", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "\n", "word_dim", "=", "hidden_size", "\n", "hidden_dim", "=", "hidden_size", "\n", "self", ".", "leaf_rnn_cell", "=", "nn", ".", "LSTMCell", "(", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "self", ".", "leaf_rnn_cell_bw", "=", "nn", ".", "LSTMCell", "(", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "self", ".", "treelstm_layer", "=", "BinaryTreeLSTMLayer", "(", "2", "*", "hidden_dim", ")", "\n", "self", ".", "comp_query_linear", "=", "nn", ".", "Linear", "(", "hidden_dim", "*", "2", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.LatentComposition.update_state": [[164, 172], ["done_mask.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze", "done_mask.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze.type_as().unsqueeze", "done_mask.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze.type_as"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "update_state", "(", "old_state", ",", "new_state", ",", "done_mask", ")", ":", "\n", "        ", "old_h", ",", "old_c", "=", "old_state", "\n", "new_h", ",", "new_c", "=", "new_state", "\n", "done_mask", "=", "done_mask", ".", "type_as", "(", "old_h", ")", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "h", "=", "done_mask", "*", "new_h", "+", "(", "1", "-", "done_mask", ")", "*", "old_h", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "c", "=", "done_mask", "*", "new_c", "+", "(", "1", "-", "done_mask", ")", "*", "old_c", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "return", "h", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.LatentComposition.select_composition": [[173, 204], ["greedy_select.type_as", "greedy_select.unsqueeze().expand_as", "greedy_select.cumsum", "left_mask.unsqueeze().expand_as", "right_mask.unsqueeze().expand_as", "utils.LatentComposition.comp_query_linear().sum().squeeze", "math.sqrt", "torch.nn.functional.gumbel_softmax", "utils.greedy_select", "greedy_select.unsqueeze", "left_mask.unsqueeze", "right_mask.unsqueeze", "utils.LatentComposition.comp_query_linear().sum", "utils.LatentComposition.comp_query_linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.greedy_select"], ["", "def", "select_composition", "(", "self", ",", "old_state", ",", "new_state", ",", "mask", ")", ":", "\n", "        ", "new_h", ",", "new_c", "=", "new_state", "\n", "old_h", ",", "old_c", "=", "old_state", "\n", "old_h_left", ",", "old_h_right", "=", "old_h", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "old_h", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "old_c_left", ",", "old_c_right", "=", "old_c", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "old_c", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "comp_weights", "=", "self", ".", "comp_query_linear", "(", "new_h", ")", ".", "sum", "(", "-", "1", ")", ".", "squeeze", "(", "dim", "=", "-", "1", ")", "/", "math", ".", "sqrt", "(", "self", ".", "hidden_size", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "select_mask", "=", "torch", ".", "nn", ".", "functional", ".", "gumbel_softmax", "(", "logits", "=", "comp_weights", ",", "\n", "tau", "=", "self", ".", "gumbel_temperature", ",", "\n", "hard", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "select_mask", "=", "greedy_select", "(", "logits", "=", "comp_weights", ",", "mask", "=", "mask", ")", "\n", "\n", "", "select_mask", "=", "select_mask", ".", "type_as", "(", "old_h", ")", "\n", "select_mask_expand", "=", "select_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "new_h", ")", "\n", "select_mask_cumsum", "=", "select_mask", ".", "cumsum", "(", "1", ")", "\n", "\n", "left_mask", "=", "1", "-", "select_mask_cumsum", "\n", "left_mask_expand", "=", "left_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "old_h_left", ")", "\n", "right_mask", "=", "select_mask_cumsum", "-", "select_mask", "\n", "right_mask_expand", "=", "right_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "old_h_right", ")", "\n", "\n", "new_h", "=", "(", "select_mask_expand", "*", "new_h", "\n", "+", "left_mask_expand", "*", "old_h_left", "\n", "+", "right_mask_expand", "*", "old_h_right", ")", "\n", "new_c", "=", "(", "select_mask_expand", "*", "new_c", "\n", "+", "left_mask_expand", "*", "old_c_left", "\n", "+", "right_mask_expand", "*", "old_c_right", ")", "\n", "\n", "selected_h", "=", "(", "select_mask_expand", "*", "new_h", ")", ".", "sum", "(", "1", ")", "\n", "return", "new_h", ",", "new_c", ",", "select_mask", ",", "selected_h", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.LatentComposition.forward": [[205, 263], ["idiom_hidden.size", "utils.sequence_mask", "idiom_hidden.size", "idiom_hidden.data.new_zeros().chunk", "range", "torch.stack", "torch.stack", "idiom_hidden.data.new_zeros().chunk", "utils.reverse_padded_sequence", "range", "torch.stack", "torch.stack", "utils.reverse_padded_sequence", "utils.reverse_padded_sequence", "torch.cat", "torch.cat", "range", "utils.LatentComposition.leaf_rnn_cell", "torch.cat.append", "torch.cat.append", "utils.LatentComposition.leaf_rnn_cell_bw", "reverse_padded_sequence.append", "reverse_padded_sequence.append", "utils.LatentComposition.treelstm_layer", "utils.LatentComposition.update_state", "h.squeeze", "c.squeeze", "idiom_hidden.data.new_zeros", "idiom_hidden.data.new_zeros", "utils.LatentComposition.select_composition", "select_masks.append", "h.size", "c.size"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.sequence_mask", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.update_state", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.select_composition"], ["", "def", "forward", "(", "self", ",", "idiom_hidden", ",", "idiom_length", ")", ":", "\n", "        ", "max_depth", "=", "idiom_hidden", ".", "size", "(", "1", ")", "\n", "length_mask", "=", "sequence_mask", "(", "sequence_length", "=", "idiom_length", ",", "\n", "max_length", "=", "max_depth", ")", "\n", "select_masks", "=", "[", "]", "\n", "\n", "hs", "=", "[", "]", "\n", "cs", "=", "[", "]", "\n", "batch_size", ",", "max_length", ",", "_", "=", "idiom_hidden", ".", "size", "(", ")", "\n", "h_prev", ",", "c_prev", "=", "idiom_hidden", ".", "data", ".", "new_zeros", "(", "2", "*", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "chunk", "(", "chunks", "=", "2", ",", "dim", "=", "0", ")", "\n", "for", "i", "in", "range", "(", "max_length", ")", ":", "\n", "            ", "h", ",", "c", "=", "self", ".", "leaf_rnn_cell", "(", "input", "=", "idiom_hidden", "[", ":", ",", "i", ",", ":", "]", ",", "hx", "=", "(", "h_prev", ",", "c_prev", ")", ")", "\n", "hs", ".", "append", "(", "h", ")", "\n", "cs", ".", "append", "(", "c", ")", "\n", "h_prev", "=", "h", "\n", "c_prev", "=", "c", "\n", "", "hs", "=", "torch", ".", "stack", "(", "hs", ",", "dim", "=", "1", ")", "\n", "cs", "=", "torch", ".", "stack", "(", "cs", ",", "dim", "=", "1", ")", "\n", "\n", "hs_bw", "=", "[", "]", "\n", "cs_bw", "=", "[", "]", "\n", "h_bw_prev", ",", "c_bw_prev", "=", "idiom_hidden", ".", "data", ".", "new_zeros", "(", "2", "*", "batch_size", ",", "self", ".", "hidden_size", ")", ".", "chunk", "(", "chunks", "=", "2", ",", "dim", "=", "0", ")", "\n", "input_bw", "=", "reverse_padded_sequence", "(", "inputs", "=", "idiom_hidden", ",", "lengths", "=", "idiom_length", ",", "batch_first", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "max_length", ")", ":", "\n", "            ", "h_bw", ",", "c_bw", "=", "self", ".", "leaf_rnn_cell_bw", "(", "input", "=", "input_bw", "[", ":", ",", "i", ",", ":", "]", ",", "hx", "=", "(", "h_bw_prev", ",", "c_bw_prev", ")", ")", "\n", "hs_bw", ".", "append", "(", "h_bw", ")", "\n", "cs_bw", ".", "append", "(", "c_bw", ")", "\n", "h_bw_prev", "=", "h_bw", "\n", "c_bw_prev", "=", "c_bw", "\n", "", "hs_bw", "=", "torch", ".", "stack", "(", "hs_bw", ",", "dim", "=", "1", ")", "\n", "cs_bw", "=", "torch", ".", "stack", "(", "cs_bw", ",", "dim", "=", "1", ")", "\n", "\n", "hs_bw", "=", "reverse_padded_sequence", "(", "inputs", "=", "hs_bw", ",", "lengths", "=", "idiom_length", ",", "batch_first", "=", "True", ")", "\n", "cs_bw", "=", "reverse_padded_sequence", "(", "inputs", "=", "cs_bw", ",", "lengths", "=", "idiom_length", ",", "batch_first", "=", "True", ")", "\n", "\n", "hs", "=", "torch", ".", "cat", "(", "[", "hs", ",", "hs_bw", "]", ",", "dim", "=", "2", ")", "\n", "cs", "=", "torch", ".", "cat", "(", "[", "cs", ",", "cs_bw", "]", ",", "dim", "=", "2", ")", "\n", "state", "=", "(", "hs", ",", "cs", ")", "\n", "\n", "for", "i", "in", "range", "(", "max_depth", "-", "1", ")", ":", "\n", "            ", "h", ",", "c", "=", "state", "\n", "l", "=", "(", "h", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "c", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "\n", "r", "=", "(", "h", "[", ":", ",", "1", ":", ",", ":", "]", ",", "c", "[", ":", ",", "1", ":", ",", ":", "]", ")", "\n", "new_state", "=", "self", ".", "treelstm_layer", "(", "l", "=", "l", ",", "r", "=", "r", ")", "\n", "if", "i", "<", "max_depth", "-", "2", ":", "\n", "# We don't need to greedily select the composition in the", "\n", "# last iteration, since it has only one option left.", "\n", "                ", "new_h", ",", "new_c", ",", "select_mask", ",", "selected_h", "=", "self", ".", "select_composition", "(", "\n", "old_state", "=", "state", ",", "new_state", "=", "new_state", ",", "\n", "mask", "=", "length_mask", "[", ":", ",", "i", "+", "1", ":", "]", ")", "\n", "new_state", "=", "(", "new_h", ",", "new_c", ")", "\n", "select_masks", ".", "append", "(", "select_mask", ")", "\n", "", "done_mask", "=", "length_mask", "[", ":", ",", "i", "+", "1", "]", "\n", "state", "=", "self", ".", "update_state", "(", "old_state", "=", "state", ",", "new_state", "=", "new_state", ",", "done_mask", "=", "done_mask", ")", "\n", "\n", "", "h", ",", "c", "=", "state", "\n", "assert", "h", ".", "size", "(", "1", ")", "==", "1", "and", "c", ".", "size", "(", "1", ")", "==", "1", "\n", "return", "h", ".", "squeeze", "(", "1", ")", ",", "c", ".", "squeeze", "(", "1", ")", ",", "select_masks", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.GatedTanh.__init__": [[491, 495], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ")", ":", "\n", "        ", "super", "(", "GatedTanh", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ")", "\n", "self", ".", "gate_fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.GatedTanh.forward": [[496, 504], ["torch.tanh", "torch.sigmoid", "utils.GatedTanh.fc", "utils.GatedTanh.gate_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y_tilda", "=", "torch", ".", "tanh", "(", "self", ".", "fc", "(", "x", ")", ")", "\n", "gated", "=", "torch", ".", "sigmoid", "(", "self", ".", "gate_fc", "(", "x", ")", ")", "\n", "\n", "# Element wise multiplication", "\n", "y", "=", "y_tilda", "*", "gated", "\n", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.WeightNormClassifier.__init__": [[507, 516], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.utils.weight_norm", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.utils.weight_norm", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "hidden_dim", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "WeightNormClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layers", "=", "[", "\n", "weight_norm", "(", "nn", ".", "Linear", "(", "in_dim", ",", "hidden_dim", ")", ",", "dim", "=", "None", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ",", "inplace", "=", "True", ")", ",", "\n", "weight_norm", "(", "nn", ".", "Linear", "(", "hidden_dim", ",", "out_dim", ")", ",", "dim", "=", "None", ")", ",", "\n", "]", "\n", "self", ".", "main", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.WeightNormClassifier.forward": [[517, 520], ["utils.WeightNormClassifier.main"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.train_pretrain.main"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "logits", "=", "self", ".", "main", "(", "x", ")", "\n", "return", "logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.TreeComposition.__init__": [[309, 338], ["torch.nn.Module.__init__", "torch.nn.LSTMCell", "torch.nn.Linear", "utils.BinaryTreeLSTMLayer", "torch.nn.Linear", "utils.BinaryTreeLSTMLayer", "torch.nn.Linear", "torch.nn.LSTMCell"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "use_leaf_rnn", "=", "True", "\n", "self", ".", "intra_attention", "=", "False", "\n", "self", ".", "gumbel_temperature", "=", "1", "\n", "self", ".", "bidirectional", "=", "True", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "\n", "assert", "not", "(", "self", ".", "bidirectional", "and", "not", "self", ".", "use_leaf_rnn", ")", "\n", "\n", "word_dim", "=", "hidden_size", "\n", "hidden_dim", "=", "hidden_size", "\n", "if", "self", ".", "use_leaf_rnn", ":", "\n", "            ", "self", ".", "leaf_rnn_cell", "=", "nn", ".", "LSTMCell", "(", "\n", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "if", "self", ".", "bidirectional", ":", "\n", "                ", "self", ".", "leaf_rnn_cell_bw", "=", "nn", ".", "LSTMCell", "(", "\n", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "word_linear", "=", "nn", ".", "Linear", "(", "in_features", "=", "word_dim", ",", "\n", "out_features", "=", "2", "*", "hidden_dim", ")", "\n", "", "if", "self", ".", "bidirectional", ":", "\n", "            ", "self", ".", "treelstm_layer", "=", "BinaryTreeLSTMLayer", "(", "2", "*", "hidden_dim", ")", "\n", "# self.comp_query = nn.Parameter(torch.FloatTensor(2 * hidden_dim))", "\n", "self", ".", "comp_query_linear", "=", "nn", ".", "Linear", "(", "hidden_dim", "*", "2", ",", "1", ",", "bias", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "treelstm_layer", "=", "BinaryTreeLSTMLayer", "(", "hidden_dim", ")", "\n", "# self.comp_query = nn.Parameter(torch.FloatTensor(hidden_dim))", "\n", "self", ".", "comp_query_linear", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.TreeComposition.update_state": [[339, 347], ["done_mask.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze", "done_mask.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze.type_as().unsqueeze", "done_mask.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze.type_as"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "update_state", "(", "old_state", ",", "new_state", ",", "done_mask", ")", ":", "\n", "        ", "old_h", ",", "old_c", "=", "old_state", "\n", "new_h", ",", "new_c", "=", "new_state", "\n", "done_mask", "=", "done_mask", ".", "type_as", "(", "old_h", ")", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "h", "=", "done_mask", "*", "new_h", "+", "(", "1", "-", "done_mask", ")", "*", "old_h", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "c", "=", "done_mask", "*", "new_c", "+", "(", "1", "-", "done_mask", ")", "*", "old_c", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "return", "h", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.TreeComposition.select_composition": [[348, 384], ["utils.TreeComposition.comp_query_linear().sum().squeeze", "greedy_select.type_as", "greedy_select.unsqueeze().expand_as", "greedy_select.cumsum", "left_mask.unsqueeze().expand_as", "right_mask.unsqueeze().expand_as", "torch.nn.functional.gumbel_softmax", "utils.greedy_select", "utils.TreeComposition.comp_query_linear().sum", "greedy_select.unsqueeze", "left_mask.unsqueeze", "right_mask.unsqueeze", "utils.TreeComposition.comp_query_linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.greedy_select"], ["", "def", "select_composition", "(", "self", ",", "old_state", ",", "new_state", ",", "mask", ")", ":", "\n", "        ", "new_h", ",", "new_c", "=", "new_state", "\n", "old_h", ",", "old_c", "=", "old_state", "\n", "old_h_left", ",", "old_h_right", "=", "old_h", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "old_h", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "old_c_left", ",", "old_c_right", "=", "old_c", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "old_c", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "# comp_weights = (self.comp_query * new_h).sum(-1)", "\n", "# comp_weights = comp_weights / math.sqrt(self.config.hidden_size)", "\n", "comp_weights", "=", "self", ".", "comp_query_linear", "(", "new_h", ")", ".", "sum", "(", "-", "1", ")", ".", "squeeze", "(", "dim", "=", "-", "1", ")", "\n", "if", "self", ".", "training", ":", "\n", "# select_mask = st_gumbel_softmax(", "\n", "#     logits=comp_weights, temperature=self.gumbel_temperature,", "\n", "#     mask=mask)", "\n", "            ", "select_mask", "=", "torch", ".", "nn", ".", "functional", ".", "gumbel_softmax", "(", "logits", "=", "comp_weights", ",", "\n", "tau", "=", "self", ".", "gumbel_temperature", ",", "\n", "hard", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "select_mask", "=", "greedy_select", "(", "logits", "=", "comp_weights", ",", "mask", "=", "mask", ")", "\n", "\n", "", "select_mask", "=", "select_mask", ".", "type_as", "(", "old_h", ")", "\n", "select_mask_expand", "=", "select_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "new_h", ")", "\n", "select_mask_cumsum", "=", "select_mask", ".", "cumsum", "(", "1", ")", "\n", "\n", "left_mask", "=", "1", "-", "select_mask_cumsum", "\n", "left_mask_expand", "=", "left_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "old_h_left", ")", "\n", "right_mask", "=", "select_mask_cumsum", "-", "select_mask", "\n", "right_mask_expand", "=", "right_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "old_h_right", ")", "\n", "\n", "new_h", "=", "(", "select_mask_expand", "*", "new_h", "\n", "+", "left_mask_expand", "*", "old_h_left", "\n", "+", "right_mask_expand", "*", "old_h_right", ")", "\n", "new_c", "=", "(", "select_mask_expand", "*", "new_c", "\n", "+", "left_mask_expand", "*", "old_c_left", "\n", "+", "right_mask_expand", "*", "old_c_right", ")", "\n", "\n", "selected_h", "=", "(", "select_mask_expand", "*", "new_h", ")", ".", "sum", "(", "1", ")", "\n", "return", "new_h", ",", "new_c", ",", "select_mask", ",", "selected_h", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.TreeComposition.forward": [[385, 478], ["idiom_hidden.size", "utils.sequence_mask", "range", "idiom_hidden.size", "idiom_hidden.data.new_zeros", "range", "torch.stack", "torch.stack", "utils.TreeComposition.word_linear", "utils.TreeComposition.chunk", "torch.cat.append", "utils.TreeComposition.treelstm_layer", "utils.TreeComposition.update_state", "torch.cat", "att_mask.type_as.type_as.type_as", "torch.cat", "att_mask.type_as.type_as.unsqueeze().expand_as", "torch.cat.mean().squeeze().unsqueeze", "torch.bmm().squeeze", "utils.masked_softmax", "masked_softmax.unsqueeze().expand_as", "h.squeeze", "c.squeeze", "utils.TreeComposition.leaf_rnn_cell", "torch.cat.append", "torch.cat.append", "utils.reverse_padded_sequence", "range", "torch.stack", "torch.stack", "utils.reverse_padded_sequence", "utils.reverse_padded_sequence", "torch.cat", "torch.cat", "utils.TreeComposition.select_composition", "select_masks.append", "torch.cat.append", "h.size", "c.size", "utils.TreeComposition.leaf_rnn_cell_bw", "reverse_padded_sequence.append", "reverse_padded_sequence.append", "torch.cat.append", "att_mask.type_as.type_as.unsqueeze", "torch.cat.mean().squeeze", "torch.bmm", "masked_softmax.unsqueeze", "torch.cat.mean"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.sequence_mask", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.update_state", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.masked_softmax", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.select_composition"], ["", "def", "forward", "(", "self", ",", "idiom_hidden", ",", "idiom_length", ")", ":", "\n", "        ", "max_depth", "=", "idiom_hidden", ".", "size", "(", "1", ")", "\n", "length_mask", "=", "sequence_mask", "(", "sequence_length", "=", "idiom_length", ",", "\n", "max_length", "=", "max_depth", ")", "\n", "select_masks", "=", "[", "]", "\n", "\n", "if", "self", ".", "use_leaf_rnn", ":", "\n", "            ", "hs", "=", "[", "]", "\n", "cs", "=", "[", "]", "\n", "batch_size", ",", "max_length", ",", "_", "=", "idiom_hidden", ".", "size", "(", ")", "\n", "zero_state", "=", "idiom_hidden", ".", "data", ".", "new_zeros", "(", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "# input.data.new_zeros(batch_size, self.config.hidden_size)", "\n", "h_prev", "=", "c_prev", "=", "zero_state", "\n", "for", "i", "in", "range", "(", "max_length", ")", ":", "\n", "                ", "h", ",", "c", "=", "self", ".", "leaf_rnn_cell", "(", "input", "=", "idiom_hidden", "[", ":", ",", "i", ",", ":", "]", ",", "hx", "=", "(", "h_prev", ",", "c_prev", ")", ")", "\n", "hs", ".", "append", "(", "h", ")", "\n", "cs", ".", "append", "(", "c", ")", "\n", "h_prev", "=", "h", "\n", "c_prev", "=", "c", "\n", "", "hs", "=", "torch", ".", "stack", "(", "hs", ",", "dim", "=", "1", ")", "\n", "cs", "=", "torch", ".", "stack", "(", "cs", ",", "dim", "=", "1", ")", "\n", "\n", "if", "self", ".", "bidirectional", ":", "\n", "                ", "hs_bw", "=", "[", "]", "\n", "cs_bw", "=", "[", "]", "\n", "h_bw_prev", "=", "c_bw_prev", "=", "zero_state", "\n", "# lengths_list = list(length.data)", "\n", "input_bw", "=", "reverse_padded_sequence", "(", "\n", "inputs", "=", "idiom_hidden", ",", "lengths", "=", "idiom_length", ",", "batch_first", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "max_length", ")", ":", "\n", "                    ", "h_bw", ",", "c_bw", "=", "self", ".", "leaf_rnn_cell_bw", "(", "\n", "input", "=", "input_bw", "[", ":", ",", "i", ",", ":", "]", ",", "hx", "=", "(", "h_bw_prev", ",", "c_bw_prev", ")", ")", "\n", "hs_bw", ".", "append", "(", "h_bw", ")", "\n", "cs_bw", ".", "append", "(", "c_bw", ")", "\n", "h_bw_prev", "=", "h_bw", "\n", "c_bw_prev", "=", "c_bw", "\n", "", "hs_bw", "=", "torch", ".", "stack", "(", "hs_bw", ",", "dim", "=", "1", ")", "\n", "cs_bw", "=", "torch", ".", "stack", "(", "cs_bw", ",", "dim", "=", "1", ")", "\n", "hs_bw", "=", "reverse_padded_sequence", "(", "\n", "inputs", "=", "hs_bw", ",", "lengths", "=", "idiom_length", ",", "batch_first", "=", "True", ")", "\n", "cs_bw", "=", "reverse_padded_sequence", "(", "\n", "inputs", "=", "cs_bw", ",", "lengths", "=", "idiom_length", ",", "batch_first", "=", "True", ")", "\n", "hs", "=", "torch", ".", "cat", "(", "[", "hs", ",", "hs_bw", "]", ",", "dim", "=", "2", ")", "\n", "cs", "=", "torch", ".", "cat", "(", "[", "cs", ",", "cs_bw", "]", ",", "dim", "=", "2", ")", "\n", "", "state", "=", "(", "hs", ",", "cs", ")", "\n", "", "else", ":", "\n", "            ", "state", "=", "self", ".", "word_linear", "(", "idiom_hidden", ")", "\n", "state", "=", "state", ".", "chunk", "(", "chunks", "=", "2", ",", "dim", "=", "2", ")", "\n", "\n", "", "nodes", "=", "[", "]", "\n", "if", "self", ".", "intra_attention", ":", "\n", "            ", "nodes", ".", "append", "(", "state", "[", "0", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "max_depth", "-", "1", ")", ":", "\n", "            ", "h", ",", "c", "=", "state", "\n", "l", "=", "(", "h", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "c", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "\n", "r", "=", "(", "h", "[", ":", ",", "1", ":", ",", ":", "]", ",", "c", "[", ":", ",", "1", ":", ",", ":", "]", ")", "\n", "new_state", "=", "self", ".", "treelstm_layer", "(", "l", "=", "l", ",", "r", "=", "r", ")", "\n", "if", "i", "<", "max_depth", "-", "2", ":", "\n", "# We don't need to greedily select the composition in the", "\n", "# last iteration, since it has only one option left.", "\n", "                ", "new_h", ",", "new_c", ",", "select_mask", ",", "selected_h", "=", "self", ".", "select_composition", "(", "\n", "old_state", "=", "state", ",", "new_state", "=", "new_state", ",", "\n", "mask", "=", "length_mask", "[", ":", ",", "i", "+", "1", ":", "]", ")", "\n", "new_state", "=", "(", "new_h", ",", "new_c", ")", "\n", "select_masks", ".", "append", "(", "select_mask", ")", "\n", "if", "self", ".", "intra_attention", ":", "\n", "                    ", "nodes", ".", "append", "(", "selected_h", ")", "\n", "", "", "done_mask", "=", "length_mask", "[", ":", ",", "i", "+", "1", "]", "\n", "state", "=", "self", ".", "update_state", "(", "old_state", "=", "state", ",", "new_state", "=", "new_state", ",", "\n", "done_mask", "=", "done_mask", ")", "\n", "if", "self", ".", "intra_attention", "and", "i", ">=", "max_depth", "-", "2", ":", "\n", "                ", "nodes", ".", "append", "(", "state", "[", "0", "]", ")", "\n", "\n", "", "", "h", ",", "c", "=", "state", "\n", "if", "self", ".", "intra_attention", ":", "\n", "            ", "att_mask", "=", "torch", ".", "cat", "(", "[", "length_mask", ",", "length_mask", "[", ":", ",", "1", ":", "]", "]", ",", "dim", "=", "1", ")", "\n", "att_mask", "=", "att_mask", ".", "type_as", "(", "h", ")", "\n", "# nodes: (batch_size, num_tree_nodes, hidden_dim)", "\n", "nodes", "=", "torch", ".", "cat", "(", "nodes", ",", "dim", "=", "1", ")", "\n", "att_mask_expand", "=", "att_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "nodes", ")", "\n", "nodes", "=", "nodes", "*", "att_mask_expand", "\n", "# nodes_mean: (batch_size, hidden_dim, 1)", "\n", "nodes_mean", "=", "nodes", ".", "mean", "(", "1", ")", ".", "squeeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "# att_weights: (batch_size, num_tree_nodes)", "\n", "att_weights", "=", "torch", ".", "bmm", "(", "nodes", ",", "nodes_mean", ")", ".", "squeeze", "(", "2", ")", "\n", "att_weights", "=", "masked_softmax", "(", "logits", "=", "att_weights", ",", "mask", "=", "att_mask", ")", "\n", "# att_weights_expand: (batch_size, num_tree_nodes, hidden_dim)", "\n", "att_weights_expand", "=", "att_weights", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "nodes", ")", "\n", "# h: (batch_size, 1, 2 * hidden_dim)", "\n", "h", "=", "(", "att_weights_expand", "*", "nodes", ")", ".", "sum", "(", "1", ")", "\n", "", "assert", "h", ".", "size", "(", "1", ")", "==", "1", "and", "c", ".", "size", "(", "1", ")", "==", "1", "\n", "return", "h", ".", "squeeze", "(", "1", ")", ",", "c", ".", "squeeze", "(", "1", ")", ",", "select_masks", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.convert_to_one_hot": [[7, 23], ["indices.unsqueeze.size", "indices.unsqueeze.unsqueeze", "indices.unsqueeze.new_zeros().scatter_", "indices.unsqueeze.new_zeros"], "function", ["None"], ["def", "convert_to_one_hot", "(", "indices", ",", "num_classes", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        indices (tensor): A vector containing indices,\n            whose size is (batch_size,).\n        num_classes (tensor): The number of classes, which would be\n            the second dimension of the resulting one-hot matrix.\n\n    Returns:\n        result: The one-hot matrix of size (batch_size, num_classes).\n    \"\"\"", "\n", "\n", "batch_size", "=", "indices", ".", "size", "(", "0", ")", "\n", "indices", "=", "indices", ".", "unsqueeze", "(", "1", ")", "\n", "one_hot", "=", "indices", ".", "new_zeros", "(", "batch_size", ",", "num_classes", ")", ".", "scatter_", "(", "1", ",", "indices", ",", "1", ")", "\n", "return", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.masked_softmax": [[25, 34], ["logits.softmax", "mask.type_as"], "function", ["None"], ["", "def", "masked_softmax", "(", "logits", ",", "mask", "=", "None", ")", ":", "\n", "# eps = 1e-20", "\n", "# probs = torch.softmax(logits, dim=1)", "\n", "# if mask is not None:", "\n", "#     mask = mask.float()", "\n", "#     probs = probs * mask + eps", "\n", "#     probs = probs / probs.sum(1, keepdim=True)", "\n", "    ", "logits", "+=", "(", "1.0", "-", "mask", ".", "type_as", "(", "logits", ")", ")", "*", "-", "10000.0", "\n", "return", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.greedy_select": [[36, 41], ["utils.masked_softmax", "utils.convert_to_one_hot", "logits.size", "masked_softmax.max"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.masked_softmax", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.convert_to_one_hot"], ["", "def", "greedy_select", "(", "logits", ",", "mask", "=", "None", ")", ":", "\n", "    ", "probs", "=", "masked_softmax", "(", "logits", "=", "logits", ",", "mask", "=", "mask", ")", "\n", "one_hot", "=", "convert_to_one_hot", "(", "indices", "=", "probs", ".", "max", "(", "1", ")", "[", "1", "]", ",", "\n", "num_classes", "=", "logits", ".", "size", "(", "1", ")", ")", "\n", "return", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.st_gumbel_softmax": [[43, 74], ["logits.data.new().uniform_", "utils.masked_softmax", "convert_to_one_hot().float", "torch.log", "masked_softmax.max", "logits.data.new", "utils.convert_to_one_hot", "logits.size", "torch.log", "masked_softmax.size"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.masked_softmax", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.convert_to_one_hot"], ["", "def", "st_gumbel_softmax", "(", "logits", ",", "temperature", "=", "1.0", ",", "mask", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the result of Straight-Through Gumbel-Softmax Estimation.\n    It approximates the discrete sampling via Gumbel-Softmax trick\n    and applies the biased ST estimator.\n    In the forward propagation, it emits the discrete one-hot result,\n    and in the backward propagation it approximates the categorical\n    distribution via smooth Gumbel-Softmax distribution.\n\n    Args:\n        logits (tensor): A un-normalized probability values,\n            which has the size (batch_size, num_classes)\n        temperature (float): A temperature parameter. The higher\n            the value is, the smoother the distribution is.\n        mask (tensor, optional): If given, it masks the softmax\n            so that indices of '0' mask values are not selected.\n            The size is (batch_size, num_classes).\n\n    Returns:\n        y: The sampled output, which has the property explained above.\n    \"\"\"", "\n", "\n", "eps", "=", "1e-10", "\n", "u", "=", "logits", ".", "data", ".", "new", "(", "*", "logits", ".", "size", "(", ")", ")", ".", "uniform_", "(", ")", "\n", "gumbel_noise", "=", "-", "torch", ".", "log", "(", "-", "torch", ".", "log", "(", "u", "+", "eps", ")", "+", "eps", ")", "\n", "y", "=", "logits", "+", "gumbel_noise", "\n", "y", "=", "masked_softmax", "(", "logits", "=", "y", "/", "temperature", ",", "mask", "=", "mask", ")", "\n", "y_argmax", "=", "y", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "y_hard", "=", "convert_to_one_hot", "(", "indices", "=", "y_argmax", ",", "num_classes", "=", "y", ".", "size", "(", "1", ")", ")", ".", "float", "(", ")", "\n", "y", "=", "(", "y_hard", "-", "y", ")", ".", "detach", "(", ")", "+", "y", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.sequence_mask": [[76, 85], ["sequence_length.size", "torch.arange().long().type_as", "torch.arange().long().type_as.unsqueeze().expand", "seq_range_expand.to.to", "sequence_length.unsqueeze().expand_as", "sequence_length.data.max", "torch.arange().long", "torch.arange().long().type_as.unsqueeze", "sequence_length.unsqueeze", "torch.arange"], "function", ["None"], ["", "def", "sequence_mask", "(", "sequence_length", ",", "max_length", "=", "None", ")", ":", "\n", "    ", "if", "max_length", "is", "None", ":", "\n", "        ", "max_length", "=", "sequence_length", ".", "data", ".", "max", "(", ")", "\n", "", "batch_size", "=", "sequence_length", ".", "size", "(", "0", ")", "\n", "seq_range", "=", "torch", ".", "arange", "(", "0", ",", "max_length", ")", ".", "long", "(", ")", ".", "type_as", "(", "sequence_length", ")", "\n", "seq_range_expand", "=", "seq_range", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "max_length", ")", "\n", "seq_range_expand", "=", "seq_range_expand", ".", "to", "(", "sequence_length", ")", "\n", "seq_length_expand", "=", "sequence_length", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "seq_range_expand", ")", "\n", "return", "seq_range_expand", "<", "seq_length_expand", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.modules.utils.reverse_padded_sequence": [[87, 117], ["enumerate", "torch.LongTensor().unsqueeze().expand_as", "torch.gather", "inputs.transpose.transpose", "inputs.transpose.size", "len", "ValueError", "list", "torch.LongTensor().unsqueeze().expand_as.type_as", "reversed_inputs.transpose.transpose", "range", "range", "torch.LongTensor().unsqueeze", "inputs.transpose.size", "inputs.transpose.size", "torch.LongTensor"], "function", ["None"], ["", "def", "reverse_padded_sequence", "(", "inputs", ",", "lengths", ",", "batch_first", "=", "False", ")", ":", "\n", "    ", "\"\"\"Reverses sequences according to their lengths.\n    Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n    ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n    B is the batch size, and * is any number of dimensions (including 0).\n    Arguments:\n        inputs (tensor): padded batch of variable length sequences.\n        lengths (list[int]): list of sequence lengths\n        batch_first (bool, optional): if True, inputs should be B x T x *.\n    Returns:\n        A tensor with the same size as inputs, but with each sequence\n        reversed according to its length.\n    \"\"\"", "\n", "\n", "if", "not", "batch_first", ":", "\n", "        ", "inputs", "=", "inputs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "inputs", ".", "size", "(", "0", ")", "!=", "len", "(", "lengths", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'inputs incompatible with lengths.'", ")", "\n", "", "reversed_indices", "=", "[", "list", "(", "range", "(", "inputs", ".", "size", "(", "1", ")", ")", ")", "\n", "for", "_", "in", "range", "(", "inputs", ".", "size", "(", "0", ")", ")", "]", "\n", "for", "i", ",", "length", "in", "enumerate", "(", "lengths", ")", ":", "\n", "        ", "if", "length", ">", "0", ":", "\n", "            ", "reversed_indices", "[", "i", "]", "[", ":", "length", "]", "=", "reversed_indices", "[", "i", "]", "[", "length", "-", "1", ":", ":", "-", "1", "]", "\n", "", "", "reversed_indices", "=", "(", "torch", ".", "LongTensor", "(", "reversed_indices", ")", ".", "unsqueeze", "(", "2", ")", "\n", ".", "expand_as", "(", "inputs", ")", ")", "\n", "# reversed_indices = reversed_indices.to(inputs)", "\n", "reversed_inputs", "=", "torch", ".", "gather", "(", "inputs", ",", "1", ",", "reversed_indices", ".", "type_as", "(", "lengths", ")", ")", "\n", "if", "not", "batch_first", ":", "\n", "        ", "reversed_inputs", "=", "reversed_inputs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "return", "reversed_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_dual.ChengyuBertSingle.__init__": [[12, 23], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "modeling_dual.ChengyuBertSingle.register_buffer", "torch.Embedding", "torch.Embedding", "modeling_dual.ChengyuBertSingle.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "emb_hidden_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "emb_hidden_size", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_dual.ChengyuBertSingle.vocab": [[24, 27], ["modeling_dual.ChengyuBertSingle.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_dual.ChengyuBertSingle.forward": [[28, 63], ["modeling_dual.ChengyuBertSingle.bert", "modeling_dual.ChengyuBertSingle.vocab", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.max", "torch.max", "torch.max", "torch.max", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "ValueError", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "modeling_dual.ChengyuBertSingle.idiom_embedding", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "\n", "encoded_context", "=", "encoded_layer", "\n", "blank_states", "=", "encoded_context", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "encoded_options", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "encoded_options", "=", "self", ".", "idiom_embedding", "(", "option_ids", ")", "# (b, 10, 768)", "\n", "\n", "", "over_logits", "=", "self", ".", "vocab", "(", "blank_states", ")", "\n", "\n", "mo_logits", "=", "torch", ".", "einsum", "(", "'bld,bnd->bln'", ",", "[", "encoded_context", ",", "encoded_options", "]", ")", "# (b, 256, 10)", "\n", "c_mo_logits", ",", "_", "=", "torch", ".", "max", "(", "mo_logits", ",", "dim", "=", "1", ")", "\n", "\n", "c_fo_logits", "=", "torch", ".", "einsum", "(", "'bd,bnd->bn'", ",", "[", "blank_states", ",", "encoded_options", "]", ")", "# (b, 10)", "\n", "logits", "=", "c_mo_logits", "+", "c_fo_logits", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "logits", ",", "over_logits", ",", "cond_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_dual.ChengyuBertDual.__init__": [[67, 79], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "modeling_dual.ChengyuBertDual.register_buffer", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "modeling_dual.ChengyuBertDual.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "emb_hidden_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_facial_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "emb_hidden_size", ")", "\n", "self", ".", "idiom_meaning_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "emb_hidden_size", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_dual.ChengyuBertDual.vocab": [[80, 86], ["modeling_dual.ChengyuBertDual.idiom_facial_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_dual.ChengyuBertDual.idiom_meaning_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_facial_embeddings", "=", "self", ".", "idiom_facial_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "c_fo_logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_facial_embeddings", "]", ")", "# (b, 256, 10)", "\n", "idiom_meaning_embeddings", "=", "self", ".", "idiom_meaning_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "c_mo_logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_meaning_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "c_mo_logits", "+", "c_fo_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_dual.ChengyuBertDual.forward": [[87, 123], ["modeling_dual.ChengyuBertDual.bert", "modeling_dual.ChengyuBertDual.vocab", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.max", "torch.max", "torch.max", "torch.max", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "ValueError", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "modeling_dual.ChengyuBertDual.idiom_facial_embedding", "modeling_dual.ChengyuBertDual.idiom_meaning_embedding", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "\n", "encoded_context", "=", "encoded_layer", "\n", "blank_states", "=", "encoded_context", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "facial_state", ",", "meaning_state", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "facial_state", "=", "self", ".", "idiom_facial_embedding", "(", "option_ids", ")", "# (b, 10, 768)", "\n", "meaning_state", "=", "self", ".", "idiom_meaning_embedding", "(", "option_ids", ")", "# (b, 10, 768)", "\n", "\n", "", "over_logits", "=", "self", ".", "vocab", "(", "blank_states", ")", "\n", "\n", "mo_logits", "=", "torch", ".", "einsum", "(", "'bld,bnd->bln'", ",", "[", "encoded_context", ",", "meaning_state", "]", ")", "# (b, 256, 10)", "\n", "c_mo_logits", ",", "_", "=", "torch", ".", "max", "(", "mo_logits", ",", "dim", "=", "1", ")", "\n", "\n", "c_fo_logits", "=", "torch", ".", "einsum", "(", "'bd,bnd->bn'", ",", "[", "blank_states", ",", "facial_state", "]", ")", "# (b, 10)", "\n", "logits", "=", "c_mo_logits", "+", "c_fo_logits", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "logits", ",", "over_logits", ",", "cond_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.BinaryTreeLSTMLayer.__init__": [[122, 127], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_dim", ")", ":", "\n", "        ", "super", "(", "BinaryTreeLSTMLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "comp_linear", "=", "nn", ".", "Linear", "(", "in_features", "=", "2", "*", "hidden_dim", ",", "\n", "out_features", "=", "5", "*", "hidden_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.BinaryTreeLSTMLayer.forward": [[128, 149], ["torch.cat", "modeling_composition.BinaryTreeLSTMLayer.comp_linear", "modeling_composition.BinaryTreeLSTMLayer.chunk", "o.sigmoid", "c.tanh", "u.tanh", "i.sigmoid"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "l", "=", "None", ",", "r", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            l: A (h_l, c_l) tuple, where each value has the size\n                (batch_size, max_length, hidden_dim).\n            r: A (h_r, c_r) tuple, where each value has the size\n                (batch_size, max_length, hidden_dim).\n        Returns:\n            h, c: The hidden and cell state of the composed parent,\n                each of which has the size\n                (batch_size, max_length - 1, hidden_dim).\n        \"\"\"", "\n", "hl", ",", "cl", "=", "l", "\n", "hr", ",", "cr", "=", "r", "\n", "hlr_cat", "=", "torch", ".", "cat", "(", "[", "hl", ",", "hr", "]", ",", "dim", "=", "2", ")", "\n", "treelstm_vector", "=", "self", ".", "comp_linear", "(", "hlr_cat", ")", "\n", "i", ",", "fl", ",", "fr", ",", "u", ",", "o", "=", "treelstm_vector", ".", "chunk", "(", "chunks", "=", "5", ",", "dim", "=", "2", ")", "\n", "c", "=", "(", "cl", "*", "(", "fl", "+", "1", ")", ".", "sigmoid", "(", ")", "+", "cr", "*", "(", "fr", "+", "1", ")", ".", "sigmoid", "(", ")", "\n", "+", "u", ".", "tanh", "(", ")", "*", "i", ".", "sigmoid", "(", ")", ")", "\n", "h", "=", "o", ".", "sigmoid", "(", ")", "*", "c", ".", "tanh", "(", ")", "\n", "return", "h", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.GatedTanh.__init__": [[162, 166], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ")", ":", "\n", "        ", "super", "(", "GatedTanh", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ")", "\n", "self", ".", "gate_fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.GatedTanh.forward": [[167, 175], ["torch.tanh", "torch.sigmoid", "modeling_composition.GatedTanh.fc", "modeling_composition.GatedTanh.gate_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y_tilda", "=", "torch", ".", "tanh", "(", "self", ".", "fc", "(", "x", ")", ")", "\n", "gated", "=", "torch", ".", "sigmoid", "(", "self", ".", "gate_fc", "(", "x", ")", ")", "\n", "\n", "# Element wise multiplication", "\n", "y", "=", "y_tilda", "*", "gated", "\n", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.__init__": [[180, 221], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling_composition.ChengyuBertComposition.register_buffer", "torch.nn.Embedding", "torch.nn.LayerNorm", "modeling_composition.ChengyuBertComposition.init_weights", "torch.nn.LSTMCell", "torch.nn.Linear", "modeling_composition.BinaryTreeLSTMLayer", "torch.nn.Linear", "modeling_composition.BinaryTreeLSTMLayer", "torch.nn.Linear", "torch.arange", "torch.nn.LSTMCell"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "use_leaf_rnn", "=", "True", "\n", "self", ".", "intra_attention", "=", "False", "\n", "self", ".", "gumbel_temperature", "=", "1", "\n", "self", ".", "bidirectional", "=", "True", "\n", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "assert", "not", "(", "self", ".", "bidirectional", "and", "not", "self", ".", "use_leaf_rnn", ")", "\n", "\n", "word_dim", "=", "config", ".", "hidden_size", "\n", "hidden_dim", "=", "config", ".", "hidden_size", "\n", "if", "self", ".", "use_leaf_rnn", ":", "\n", "            ", "self", ".", "leaf_rnn_cell", "=", "nn", ".", "LSTMCell", "(", "\n", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "if", "self", ".", "bidirectional", ":", "\n", "                ", "self", ".", "leaf_rnn_cell_bw", "=", "nn", ".", "LSTMCell", "(", "\n", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "word_linear", "=", "nn", ".", "Linear", "(", "in_features", "=", "word_dim", ",", "\n", "out_features", "=", "2", "*", "hidden_dim", ")", "\n", "", "if", "self", ".", "bidirectional", ":", "\n", "            ", "self", ".", "treelstm_layer", "=", "BinaryTreeLSTMLayer", "(", "2", "*", "hidden_dim", ")", "\n", "# self.comp_query = nn.Parameter(torch.FloatTensor(2 * hidden_dim))", "\n", "self", ".", "comp_query_linear", "=", "nn", ".", "Linear", "(", "hidden_dim", "*", "2", ",", "1", ",", "bias", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "treelstm_layer", "=", "BinaryTreeLSTMLayer", "(", "hidden_dim", ")", "\n", "# self.comp_query = nn.Parameter(torch.FloatTensor(hidden_dim))", "\n", "self", ".", "comp_query_linear", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n", "", "self", ".", "over_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "emb_hidden_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "emb_hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "emb_hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.update_state": [[222, 230], ["done_mask.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze", "done_mask.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze.type_as().unsqueeze", "done_mask.type_as().unsqueeze().unsqueeze.type_as().unsqueeze().unsqueeze.type_as"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "update_state", "(", "old_state", ",", "new_state", ",", "done_mask", ")", ":", "\n", "        ", "old_h", ",", "old_c", "=", "old_state", "\n", "new_h", ",", "new_c", "=", "new_state", "\n", "done_mask", "=", "done_mask", ".", "type_as", "(", "old_h", ")", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "h", "=", "done_mask", "*", "new_h", "+", "(", "1", "-", "done_mask", ")", "*", "old_h", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "c", "=", "done_mask", "*", "new_c", "+", "(", "1", "-", "done_mask", ")", "*", "old_c", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "return", "h", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.select_composition": [[231, 267], ["modeling_composition.ChengyuBertComposition.comp_query_linear().sum().squeeze", "greedy_select.type_as", "greedy_select.unsqueeze().expand_as", "greedy_select.cumsum", "left_mask.unsqueeze().expand_as", "right_mask.unsqueeze().expand_as", "torch.nn.functional.gumbel_softmax", "modeling_composition.greedy_select", "modeling_composition.ChengyuBertComposition.comp_query_linear().sum", "greedy_select.unsqueeze", "left_mask.unsqueeze", "right_mask.unsqueeze", "modeling_composition.ChengyuBertComposition.comp_query_linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.greedy_select"], ["", "def", "select_composition", "(", "self", ",", "old_state", ",", "new_state", ",", "mask", ")", ":", "\n", "        ", "new_h", ",", "new_c", "=", "new_state", "\n", "old_h", ",", "old_c", "=", "old_state", "\n", "old_h_left", ",", "old_h_right", "=", "old_h", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "old_h", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "old_c_left", ",", "old_c_right", "=", "old_c", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "old_c", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "# comp_weights = (self.comp_query * new_h).sum(-1)", "\n", "# comp_weights = comp_weights / math.sqrt(self.config.hidden_size)", "\n", "comp_weights", "=", "self", ".", "comp_query_linear", "(", "new_h", ")", ".", "sum", "(", "-", "1", ")", ".", "squeeze", "(", "dim", "=", "-", "1", ")", "\n", "if", "self", ".", "training", ":", "\n", "# select_mask = st_gumbel_softmax(", "\n", "#     logits=comp_weights, temperature=self.gumbel_temperature,", "\n", "#     mask=mask)", "\n", "            ", "select_mask", "=", "torch", ".", "nn", ".", "functional", ".", "gumbel_softmax", "(", "logits", "=", "comp_weights", ",", "\n", "tau", "=", "self", ".", "gumbel_temperature", ",", "\n", "hard", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "select_mask", "=", "greedy_select", "(", "logits", "=", "comp_weights", ",", "mask", "=", "mask", ")", "\n", "\n", "", "select_mask", "=", "select_mask", ".", "type_as", "(", "old_h", ")", "\n", "select_mask_expand", "=", "select_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "new_h", ")", "\n", "select_mask_cumsum", "=", "select_mask", ".", "cumsum", "(", "1", ")", "\n", "\n", "left_mask", "=", "1", "-", "select_mask_cumsum", "\n", "left_mask_expand", "=", "left_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "old_h_left", ")", "\n", "right_mask", "=", "select_mask_cumsum", "-", "select_mask", "\n", "right_mask_expand", "=", "right_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "old_h_right", ")", "\n", "\n", "new_h", "=", "(", "select_mask_expand", "*", "new_h", "\n", "+", "left_mask_expand", "*", "old_h_left", "\n", "+", "right_mask_expand", "*", "old_h_right", ")", "\n", "new_c", "=", "(", "select_mask_expand", "*", "new_c", "\n", "+", "left_mask_expand", "*", "old_c_left", "\n", "+", "right_mask_expand", "*", "old_c_right", ")", "\n", "\n", "selected_h", "=", "(", "select_mask_expand", "*", "new_h", ")", ".", "sum", "(", "1", ")", "\n", "return", "new_h", ",", "new_c", ",", "select_mask", ",", "selected_h", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.idiom_compose": [[268, 361], ["input.size", "modeling_composition.sequence_mask", "range", "input.size", "torch.zeros().type_as", "range", "torch.stack", "torch.stack", "modeling_composition.ChengyuBertComposition.word_linear", "modeling_composition.ChengyuBertComposition.chunk", "torch.cat.append", "modeling_composition.ChengyuBertComposition.treelstm_layer", "modeling_composition.ChengyuBertComposition.update_state", "torch.cat", "att_mask.type_as.type_as.type_as", "torch.cat", "att_mask.type_as.type_as.unsqueeze().expand_as", "torch.cat.mean().squeeze().unsqueeze", "torch.bmm().squeeze", "modeling_composition.masked_softmax", "masked_softmax.unsqueeze().expand_as", "h.squeeze", "c.squeeze", "modeling_composition.ChengyuBertComposition.leaf_rnn_cell", "torch.cat.append", "torch.cat.append", "modeling_composition.reverse_padded_sequence", "range", "torch.stack", "torch.stack", "modeling_composition.reverse_padded_sequence", "modeling_composition.reverse_padded_sequence", "torch.cat", "torch.cat", "modeling_composition.ChengyuBertComposition.select_composition", "select_masks.append", "torch.cat.append", "h.size", "c.size", "torch.zeros", "modeling_composition.ChengyuBertComposition.leaf_rnn_cell_bw", "reverse_padded_sequence.append", "reverse_padded_sequence.append", "torch.cat.append", "att_mask.type_as.type_as.unsqueeze", "torch.cat.mean().squeeze", "torch.bmm", "masked_softmax.unsqueeze", "torch.cat.mean"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.sequence_mask", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.update_state", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.masked_softmax", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.select_composition"], ["", "def", "idiom_compose", "(", "self", ",", "input", ",", "length", ")", ":", "\n", "        ", "max_depth", "=", "input", ".", "size", "(", "1", ")", "\n", "length_mask", "=", "sequence_mask", "(", "sequence_length", "=", "length", ",", "\n", "max_length", "=", "max_depth", ")", "\n", "select_masks", "=", "[", "]", "\n", "\n", "if", "self", ".", "use_leaf_rnn", ":", "\n", "            ", "hs", "=", "[", "]", "\n", "cs", "=", "[", "]", "\n", "batch_size", ",", "max_length", ",", "_", "=", "input", ".", "size", "(", ")", "\n", "zero_state", "=", "torch", ".", "zeros", "(", "batch_size", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input", ")", "\n", "# input.data.new_zeros(batch_size, self.config.hidden_size)", "\n", "h_prev", "=", "c_prev", "=", "zero_state", "\n", "for", "i", "in", "range", "(", "max_length", ")", ":", "\n", "                ", "h", ",", "c", "=", "self", ".", "leaf_rnn_cell", "(", "input", "=", "input", "[", ":", ",", "i", ",", ":", "]", ",", "hx", "=", "(", "h_prev", ",", "c_prev", ")", ")", "\n", "hs", ".", "append", "(", "h", ")", "\n", "cs", ".", "append", "(", "c", ")", "\n", "h_prev", "=", "h", "\n", "c_prev", "=", "c", "\n", "", "hs", "=", "torch", ".", "stack", "(", "hs", ",", "dim", "=", "1", ")", "\n", "cs", "=", "torch", ".", "stack", "(", "cs", ",", "dim", "=", "1", ")", "\n", "\n", "if", "self", ".", "bidirectional", ":", "\n", "                ", "hs_bw", "=", "[", "]", "\n", "cs_bw", "=", "[", "]", "\n", "h_bw_prev", "=", "c_bw_prev", "=", "zero_state", "\n", "# lengths_list = list(length.data)", "\n", "input_bw", "=", "reverse_padded_sequence", "(", "\n", "inputs", "=", "input", ",", "lengths", "=", "length", ",", "batch_first", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "max_length", ")", ":", "\n", "                    ", "h_bw", ",", "c_bw", "=", "self", ".", "leaf_rnn_cell_bw", "(", "\n", "input", "=", "input_bw", "[", ":", ",", "i", ",", ":", "]", ",", "hx", "=", "(", "h_bw_prev", ",", "c_bw_prev", ")", ")", "\n", "hs_bw", ".", "append", "(", "h_bw", ")", "\n", "cs_bw", ".", "append", "(", "c_bw", ")", "\n", "h_bw_prev", "=", "h_bw", "\n", "c_bw_prev", "=", "c_bw", "\n", "", "hs_bw", "=", "torch", ".", "stack", "(", "hs_bw", ",", "dim", "=", "1", ")", "\n", "cs_bw", "=", "torch", ".", "stack", "(", "cs_bw", ",", "dim", "=", "1", ")", "\n", "hs_bw", "=", "reverse_padded_sequence", "(", "\n", "inputs", "=", "hs_bw", ",", "lengths", "=", "length", ",", "batch_first", "=", "True", ")", "\n", "cs_bw", "=", "reverse_padded_sequence", "(", "\n", "inputs", "=", "cs_bw", ",", "lengths", "=", "length", ",", "batch_first", "=", "True", ")", "\n", "hs", "=", "torch", ".", "cat", "(", "[", "hs", ",", "hs_bw", "]", ",", "dim", "=", "2", ")", "\n", "cs", "=", "torch", ".", "cat", "(", "[", "cs", ",", "cs_bw", "]", ",", "dim", "=", "2", ")", "\n", "", "state", "=", "(", "hs", ",", "cs", ")", "\n", "", "else", ":", "\n", "            ", "state", "=", "self", ".", "word_linear", "(", "input", ")", "\n", "state", "=", "state", ".", "chunk", "(", "chunks", "=", "2", ",", "dim", "=", "2", ")", "\n", "\n", "", "nodes", "=", "[", "]", "\n", "if", "self", ".", "intra_attention", ":", "\n", "            ", "nodes", ".", "append", "(", "state", "[", "0", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "max_depth", "-", "1", ")", ":", "\n", "            ", "h", ",", "c", "=", "state", "\n", "l", "=", "(", "h", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "c", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "\n", "r", "=", "(", "h", "[", ":", ",", "1", ":", ",", ":", "]", ",", "c", "[", ":", ",", "1", ":", ",", ":", "]", ")", "\n", "new_state", "=", "self", ".", "treelstm_layer", "(", "l", "=", "l", ",", "r", "=", "r", ")", "\n", "if", "i", "<", "max_depth", "-", "2", ":", "\n", "# We don't need to greedily select the composition in the", "\n", "# last iteration, since it has only one option left.", "\n", "                ", "new_h", ",", "new_c", ",", "select_mask", ",", "selected_h", "=", "self", ".", "select_composition", "(", "\n", "old_state", "=", "state", ",", "new_state", "=", "new_state", ",", "\n", "mask", "=", "length_mask", "[", ":", ",", "i", "+", "1", ":", "]", ")", "\n", "new_state", "=", "(", "new_h", ",", "new_c", ")", "\n", "select_masks", ".", "append", "(", "select_mask", ")", "\n", "if", "self", ".", "intra_attention", ":", "\n", "                    ", "nodes", ".", "append", "(", "selected_h", ")", "\n", "", "", "done_mask", "=", "length_mask", "[", ":", ",", "i", "+", "1", "]", "\n", "state", "=", "self", ".", "update_state", "(", "old_state", "=", "state", ",", "new_state", "=", "new_state", ",", "\n", "done_mask", "=", "done_mask", ")", "\n", "if", "self", ".", "intra_attention", "and", "i", ">=", "max_depth", "-", "2", ":", "\n", "                ", "nodes", ".", "append", "(", "state", "[", "0", "]", ")", "\n", "\n", "", "", "h", ",", "c", "=", "state", "\n", "if", "self", ".", "intra_attention", ":", "\n", "            ", "att_mask", "=", "torch", ".", "cat", "(", "[", "length_mask", ",", "length_mask", "[", ":", ",", "1", ":", "]", "]", ",", "dim", "=", "1", ")", "\n", "att_mask", "=", "att_mask", ".", "type_as", "(", "h", ")", "\n", "# nodes: (batch_size, num_tree_nodes, hidden_dim)", "\n", "nodes", "=", "torch", ".", "cat", "(", "nodes", ",", "dim", "=", "1", ")", "\n", "att_mask_expand", "=", "att_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "nodes", ")", "\n", "nodes", "=", "nodes", "*", "att_mask_expand", "\n", "# nodes_mean: (batch_size, hidden_dim, 1)", "\n", "nodes_mean", "=", "nodes", ".", "mean", "(", "1", ")", ".", "squeeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "# att_weights: (batch_size, num_tree_nodes)", "\n", "att_weights", "=", "torch", ".", "bmm", "(", "nodes", ",", "nodes_mean", ")", ".", "squeeze", "(", "2", ")", "\n", "att_weights", "=", "masked_softmax", "(", "logits", "=", "att_weights", ",", "mask", "=", "att_mask", ")", "\n", "# att_weights_expand: (batch_size, num_tree_nodes, hidden_dim)", "\n", "att_weights_expand", "=", "att_weights", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "nodes", ")", "\n", "# h: (batch_size, 1, 2 * hidden_dim)", "\n", "h", "=", "(", "att_weights_expand", "*", "nodes", ")", ".", "sum", "(", "1", ")", "\n", "", "assert", "h", ".", "size", "(", "1", ")", "==", "1", "and", "c", ".", "size", "(", "1", ")", "==", "1", "\n", "return", "h", ".", "squeeze", "(", "1", ")", ",", "c", ".", "squeeze", "(", "1", ")", ",", "select_masks", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.vocab": [[362, 365], ["modeling_composition.ChengyuBertComposition.LayerNorm", "torch.einsum", "modeling_composition.ChengyuBertComposition.idiom_embedding"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.forward": [[366, 408], ["modeling_composition.ChengyuBertComposition.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.size", "modeling_composition.ChengyuBertComposition.idiom_compose", "modeling_composition.ChengyuBertComposition.vocab", "torch.gather", "modeling_composition.ChengyuBertComposition.model_name.endswith", "torch.tensor().type_as", "ValueError", "modeling_composition.ChengyuBertComposition.over_linear", "torch.einsum", "torch.max", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.gather", "torch.nn.CrossEntropyLoss.", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "modeling_composition.ChengyuBertComposition.idiom_embedding", "torch.gather.squeeze", "torch.tensor", "targets.unsqueeze", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.idiom_compose", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "gather_index", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "batch_size", ",", "length", "=", "input_ids", ".", "shape", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", "\n", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "# idiom_states = encoded_context[[i for i in range(len(positions))], positions]  # [batch, hidden_state]", "\n", "\n", "span", "=", "gather_index", ".", "size", "(", "1", ")", "\n", "blank_states", ",", "_", ",", "select_masks", "=", "self", ".", "idiom_compose", "(", "idiom_states", ",", "\n", "torch", ".", "tensor", "(", "[", "span", "]", "*", "batch_size", ")", ".", "type_as", "(", "input_ids", ")", ")", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "encoded_options", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "encoded_options", "=", "self", ".", "idiom_embedding", "(", "option_ids", ")", "# (b, 10, 768)", "\n", "\n", "", "over_logits", "=", "self", ".", "vocab", "(", "self", ".", "over_linear", "(", "blank_states", ")", ")", "\n", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "\n", "if", "self", ".", "model_name", ".", "endswith", "(", "'context'", ")", ":", "\n", "            ", "mo_logits", "=", "torch", ".", "einsum", "(", "'bld,bnd->bln'", ",", "[", "encoded_context", ",", "encoded_options", "]", ")", "# (b, 256, 10)", "\n", "c_mo_logits", ",", "_", "=", "torch", ".", "max", "(", "mo_logits", ",", "dim", "=", "1", ")", "\n", "logits", "=", "c_mo_logits", "+", "cond_logits", "\n", "", "else", ":", "\n", "            ", "logits", "=", "cond_logits", "\n", "\n", "", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", ",", "select_masks", "\n", "", "else", ":", "\n", "            ", "return", "logits", ",", "over_logits", ",", "select_masks", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.AttentionPool.__init__": [[413, 417], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "hidden_size", ",", "drop", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "1", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.AttentionPool.forward": [[418, 427], ["modeling_composition.AttentionPool.fc().squeeze", "modeling_composition.AttentionPool.dropout", "modeling_composition.AttentionPool.unsqueeze().matmul().squeeze", "torch.softmax", "modeling_composition.AttentionPool.fc", "mask.to", "modeling_composition.AttentionPool.unsqueeze().matmul", "modeling_composition.AttentionPool.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_", ",", "mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"input: [B, T, D], mask = [B, T]\"\"\"", "\n", "score", "=", "self", ".", "fc", "(", "input_", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "mask", "=", "mask", ".", "to", "(", "dtype", "=", "input_", ".", "dtype", ")", "*", "-", "1e4", "\n", "score", "=", "score", "+", "mask", "\n", "", "norm_score", "=", "self", ".", "dropout", "(", "torch", ".", "softmax", "(", "score", ",", "dim", "=", "1", ")", ")", "\n", "output", "=", "norm_score", ".", "unsqueeze", "(", "1", ")", ".", "matmul", "(", "input_", ")", ".", "squeeze", "(", "1", ")", "\n", "return", "output", ",", "norm_score", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.CompositionGate.__init__": [[432, 439], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Sigmoid", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "output_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gate", "=", "nn", ".", "Linear", "(", "input_size", "*", "3", ",", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "sig", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "literal_proj", "=", "nn", ".", "Linear", "(", "input_size", "*", "2", ",", "output_size", ")", "\n", "self", ".", "contextual_proj", "=", "nn", ".", "Linear", "(", "input_size", ",", "output_size", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.CompositionGate.forward": [[440, 445], ["modeling_composition.CompositionGate.sig", "modeling_composition.CompositionGate.literal_proj", "modeling_composition.CompositionGate.contextual_proj", "modeling_composition.CompositionGate.gate", "modeling_composition.CompositionGate.tanh", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "literal_state", ",", "contextual_state", ")", ":", "\n", "        ", "z", "=", "self", ".", "sig", "(", "self", ".", "gate", "(", "torch", ".", "cat", "(", "(", "literal_state", ",", "contextual_state", ")", ",", "dim", "=", "1", ")", ")", ")", "\n", "proj_literal", "=", "self", ".", "literal_proj", "(", "literal_state", ")", "\n", "proj_contextual", "=", "self", ".", "contextual_proj", "(", "contextual_state", ")", "\n", "return", "self", ".", "tanh", "(", "(", "1.", "-", "z", ")", "*", "proj_contextual", "+", "z", "*", "proj_literal", ")", ",", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertCompositionPaired.__init__": [[450, 495], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling_composition.ChengyuBertCompositionPaired.register_buffer", "torch.nn.Embedding", "torch.nn.LayerNorm", "modeling_composition.AttentionPool", "modeling_composition.CompositionGate", "modeling_composition.ChengyuBertCompositionPaired.init_weights", "torch.nn.LSTMCell", "torch.nn.Linear", "modeling_composition.BinaryTreeLSTMLayer", "torch.nn.Linear", "modeling_composition.BinaryTreeLSTMLayer", "torch.nn.Linear", "torch.arange", "torch.nn.LSTMCell"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "BertPreTrainedModel", ".", "__init__", "(", "self", ",", "config", ")", "\n", "self", ".", "use_leaf_rnn", "=", "True", "\n", "self", ".", "intra_attention", "=", "False", "\n", "self", ".", "gumbel_temperature", "=", "1", "\n", "self", ".", "bidirectional", "=", "True", "\n", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "assert", "not", "(", "self", ".", "bidirectional", "and", "not", "self", ".", "use_leaf_rnn", ")", "\n", "\n", "word_dim", "=", "config", ".", "hidden_size", "\n", "hidden_dim", "=", "config", ".", "hidden_size", "\n", "if", "self", ".", "use_leaf_rnn", ":", "\n", "            ", "self", ".", "leaf_rnn_cell", "=", "nn", ".", "LSTMCell", "(", "\n", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "if", "self", ".", "bidirectional", ":", "\n", "                ", "self", ".", "leaf_rnn_cell_bw", "=", "nn", ".", "LSTMCell", "(", "\n", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "word_linear", "=", "nn", ".", "Linear", "(", "in_features", "=", "word_dim", ",", "\n", "out_features", "=", "2", "*", "hidden_dim", ")", "\n", "", "if", "self", ".", "bidirectional", ":", "\n", "            ", "self", ".", "treelstm_layer", "=", "BinaryTreeLSTMLayer", "(", "2", "*", "hidden_dim", ")", "\n", "# self.comp_query = nn.Parameter(torch.FloatTensor(2 * hidden_dim))", "\n", "self", ".", "comp_query_linear", "=", "nn", ".", "Linear", "(", "hidden_dim", "*", "2", ",", "1", ",", "bias", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "treelstm_layer", "=", "BinaryTreeLSTMLayer", "(", "hidden_dim", ")", "\n", "# self.comp_query = nn.Parameter(torch.FloatTensor(hidden_dim))", "\n", "self", ".", "comp_query_linear", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n", "", "self", ".", "over_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "emb_hidden_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "emb_hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "emb_hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n", "self", ".", "context_pool", "=", "AttentionPool", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "composition_gate", "=", "CompositionGate", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertCompositionPaired.vocab": [[496, 499], ["modeling_composition.ChengyuBertCompositionPaired.LayerNorm", "torch.einsum", "modeling_composition.ChengyuBertCompositionPaired.idiom_embedding"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "over_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "over_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertCompositionPaired.forward": [[500, 551], ["modeling_composition.ChengyuBertCompositionPaired.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.size", "modeling_composition.ChengyuBertCompositionPaired.idiom_compose", "context_gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "modeling_composition.ChengyuBertCompositionPaired.context_pool", "modeling_composition.ChengyuBertCompositionPaired.composition_gate", "modeling_composition.ChengyuBertCompositionPaired.vocab", "torch.gather", "modeling_composition.ChengyuBertCompositionPaired.model_name.endswith", "torch.tensor().type_as", "ValueError", "torch.einsum", "torch.max", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.gather", "torch.nn.CrossEntropyLoss.", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "context_gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "modeling_composition.ChengyuBertCompositionPaired.idiom_embedding", "torch.gather.squeeze", "torch.tensor", "targets.unsqueeze", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze", "context_gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.idiom_compose", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "gather_index", ",", "context_gather_index", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "batch_size", ",", "length", "=", "input_ids", ".", "shape", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", "\n", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "# idiom_states = encoded_context[[i for i in range(len(positions))], positions]  # [batch, hidden_state]", "\n", "\n", "span", "=", "gather_index", ".", "size", "(", "1", ")", "\n", "literal_states", ",", "_", ",", "select_masks", "=", "self", ".", "idiom_compose", "(", "idiom_states", ",", "\n", "torch", ".", "tensor", "(", "[", "span", "]", "*", "batch_size", ")", ".", "type_as", "(", "input_ids", ")", ")", "\n", "\n", "context_gather_index", "=", "context_gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "context_windowed", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "context_gather_index", ")", "\n", "\n", "contextual_states", ",", "att", "=", "self", ".", "context_pool", "(", "context_windowed", ")", "\n", "\n", "over_states", ",", "composition_gates", "=", "self", ".", "composition_gate", "(", "literal_states", ",", "contextual_states", ")", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "encoded_options", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "encoded_options", "=", "self", ".", "idiom_embedding", "(", "option_ids", ")", "# (b, 10, 768)", "\n", "\n", "", "over_logits", "=", "self", ".", "vocab", "(", "over_states", ")", "\n", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "\n", "if", "self", ".", "model_name", ".", "endswith", "(", "'context'", ")", ":", "\n", "            ", "mo_logits", "=", "torch", ".", "einsum", "(", "'bld,bnd->bln'", ",", "[", "encoded_context", ",", "encoded_options", "]", ")", "# (b, 256, 10)", "\n", "c_mo_logits", ",", "_", "=", "torch", ".", "max", "(", "mo_logits", ",", "dim", "=", "1", ")", "\n", "logits", "=", "c_mo_logits", "+", "cond_logits", "\n", "", "else", ":", "\n", "            ", "logits", "=", "cond_logits", "\n", "\n", "", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", ",", "(", "select_masks", ",", "att", ",", "composition_gates", ")", "\n", "", "else", ":", "\n", "            ", "return", "logits", ",", "over_logits", ",", "(", "select_masks", ",", "att", ",", "composition_gates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertCompositionDual.__init__": [[556, 601], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling_composition.ChengyuBertCompositionDual.register_buffer", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "modeling_composition.AttentionPool", "modeling_composition.ChengyuBertCompositionDual.init_weights", "torch.nn.LSTMCell", "torch.nn.Linear", "modeling_composition.BinaryTreeLSTMLayer", "torch.nn.Linear", "modeling_composition.BinaryTreeLSTMLayer", "torch.nn.Linear", "torch.arange", "torch.nn.LSTMCell"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "BertPreTrainedModel", ".", "__init__", "(", "self", ",", "config", ")", "\n", "self", ".", "use_leaf_rnn", "=", "True", "\n", "self", ".", "intra_attention", "=", "False", "\n", "self", ".", "gumbel_temperature", "=", "1", "\n", "self", ".", "bidirectional", "=", "True", "\n", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "assert", "not", "(", "self", ".", "bidirectional", "and", "not", "self", ".", "use_leaf_rnn", ")", "\n", "\n", "word_dim", "=", "config", ".", "hidden_size", "\n", "hidden_dim", "=", "config", ".", "hidden_size", "\n", "if", "self", ".", "use_leaf_rnn", ":", "\n", "            ", "self", ".", "leaf_rnn_cell", "=", "nn", ".", "LSTMCell", "(", "\n", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "if", "self", ".", "bidirectional", ":", "\n", "                ", "self", ".", "leaf_rnn_cell_bw", "=", "nn", ".", "LSTMCell", "(", "\n", "input_size", "=", "word_dim", ",", "hidden_size", "=", "hidden_dim", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "word_linear", "=", "nn", ".", "Linear", "(", "in_features", "=", "word_dim", ",", "\n", "out_features", "=", "2", "*", "hidden_dim", ")", "\n", "", "if", "self", ".", "bidirectional", ":", "\n", "            ", "self", ".", "treelstm_layer", "=", "BinaryTreeLSTMLayer", "(", "2", "*", "hidden_dim", ")", "\n", "# self.comp_query = nn.Parameter(torch.FloatTensor(2 * hidden_dim))", "\n", "self", ".", "comp_query_linear", "=", "nn", ".", "Linear", "(", "hidden_dim", "*", "2", ",", "1", ",", "bias", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "treelstm_layer", "=", "BinaryTreeLSTMLayer", "(", "hidden_dim", ")", "\n", "# self.comp_query = nn.Parameter(torch.FloatTensor(hidden_dim))", "\n", "self", ".", "comp_query_linear", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n", "", "self", ".", "v_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "emb_hidden_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_embedding_u", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "emb_hidden_size", ")", "\n", "self", ".", "idiom_embedding_v", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "emb_hidden_size", ")", "\n", "self", ".", "LayerNorm_u", "=", "nn", ".", "LayerNorm", "(", "emb_hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "LayerNorm_v", "=", "nn", ".", "LayerNorm", "(", "emb_hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n", "self", ".", "context_pool", "=", "AttentionPool", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertCompositionDual.forward": [[602, 647], ["modeling_composition.ChengyuBertCompositionDual.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.size", "modeling_composition.ChengyuBertCompositionDual.idiom_compose", "context_gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "modeling_composition.ChengyuBertCompositionDual.context_pool", "modeling_composition.ChengyuBertCompositionDual.LayerNorm_u", "torch.einsum", "modeling_composition.ChengyuBertCompositionDual.v_linear", "modeling_composition.ChengyuBertCompositionDual.LayerNorm_v", "torch.einsum", "torch.einsum", "torch.gather", "torch.tensor().type_as", "modeling_composition.ChengyuBertCompositionDual.idiom_embedding_u", "modeling_composition.ChengyuBertCompositionDual.idiom_embedding_v", "torch.nn.CrossEntropyLoss", "torch.gather().squeeze", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "context_gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "torch.tensor", "torch.gather", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze", "context_gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze", "targets.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.idiom_compose"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "gather_index", ",", "context_gather_index", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "batch_size", ",", "length", "=", "input_ids", ".", "shape", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", "\n", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "# idiom_states = encoded_context[[i for i in range(len(positions))], positions]  # [batch, hidden_state]", "\n", "\n", "span", "=", "gather_index", ".", "size", "(", "1", ")", "\n", "literal_states", ",", "_", ",", "select_masks", "=", "self", ".", "idiom_compose", "(", "idiom_states", ",", "\n", "torch", ".", "tensor", "(", "[", "span", "]", "*", "batch_size", ")", ".", "type_as", "(", "input_ids", ")", ")", "\n", "\n", "context_gather_index", "=", "context_gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "context_windowed", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "context_gather_index", ")", "\n", "\n", "contextual_states", ",", "att", "=", "self", ".", "context_pool", "(", "context_windowed", ")", "\n", "\n", "idiom_embeddings_u", "=", "self", ".", "LayerNorm_u", "(", "self", ".", "idiom_embedding_u", "(", "self", ".", "enlarged_candidates", ")", ")", "\n", "u_logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "contextual_states", ",", "idiom_embeddings_u", "]", ")", "# (b, 256, 10)", "\n", "\n", "literal_states", "=", "self", ".", "v_linear", "(", "literal_states", ")", "\n", "idiom_embeddings_v", "=", "self", ".", "LayerNorm_v", "(", "self", ".", "idiom_embedding_v", "(", "self", ".", "enlarged_candidates", ")", ")", "\n", "v_logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "literal_states", ",", "\n", "idiom_embeddings_v", "]", ")", "# (b, 256, 10)", "\n", "\n", "over_logits", "=", "u_logits", "+", "v_logits", "\n", "composition_similarity", "=", "torch", ".", "einsum", "(", "'bd,bd->b'", ",", "[", "literal_states", ",", "contextual_states", "]", ")", "# (b, 256, 10)", "\n", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "u_loss", "=", "loss_fct", "(", "u_logits", ",", "target", ")", "\n", "v_loss", "=", "loss_fct", "(", "v_logits", ",", "target", ")", "\n", "over_loss", "=", "u_loss", "+", "v_loss", "\n", "return", "None", ",", "over_loss", ",", "(", "select_masks", ",", "att", ",", "composition_similarity", ")", "\n", "", "else", ":", "\n", "            ", "return", "cond_logits", ",", "over_logits", ",", "(", "select_masks", ",", "att", ",", "composition_similarity", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.convert_to_one_hot": [[8, 24], ["indices.unsqueeze.size", "indices.unsqueeze.unsqueeze", "indices.unsqueeze.new_zeros().scatter_", "indices.unsqueeze.new_zeros"], "function", ["None"], ["def", "convert_to_one_hot", "(", "indices", ",", "num_classes", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        indices (tensor): A vector containing indices,\n            whose size is (batch_size,).\n        num_classes (tensor): The number of classes, which would be\n            the second dimension of the resulting one-hot matrix.\n\n    Returns:\n        result: The one-hot matrix of size (batch_size, num_classes).\n    \"\"\"", "\n", "\n", "batch_size", "=", "indices", ".", "size", "(", "0", ")", "\n", "indices", "=", "indices", ".", "unsqueeze", "(", "1", ")", "\n", "one_hot", "=", "indices", ".", "new_zeros", "(", "batch_size", ",", "num_classes", ")", ".", "scatter_", "(", "1", ",", "indices", ",", "1", ")", "\n", "return", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.masked_softmax": [[26, 35], ["logits.softmax", "mask.type_as"], "function", ["None"], ["", "def", "masked_softmax", "(", "logits", ",", "mask", "=", "None", ")", ":", "\n", "# eps = 1e-20", "\n", "# probs = torch.softmax(logits, dim=1)", "\n", "# if mask is not None:", "\n", "#     mask = mask.float()", "\n", "#     probs = probs * mask + eps", "\n", "#     probs = probs / probs.sum(1, keepdim=True)", "\n", "    ", "logits", "+=", "(", "1.0", "-", "mask", ".", "type_as", "(", "logits", ")", ")", "*", "-", "10000.0", "\n", "return", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.greedy_select": [[37, 42], ["modeling_composition.masked_softmax", "modeling_composition.convert_to_one_hot", "logits.size", "masked_softmax.max"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.masked_softmax", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.convert_to_one_hot"], ["", "def", "greedy_select", "(", "logits", ",", "mask", "=", "None", ")", ":", "\n", "    ", "probs", "=", "masked_softmax", "(", "logits", "=", "logits", ",", "mask", "=", "mask", ")", "\n", "one_hot", "=", "convert_to_one_hot", "(", "indices", "=", "probs", ".", "max", "(", "1", ")", "[", "1", "]", ",", "\n", "num_classes", "=", "logits", ".", "size", "(", "1", ")", ")", "\n", "return", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.st_gumbel_softmax": [[44, 75], ["logits.data.new().uniform_", "modeling_composition.masked_softmax", "convert_to_one_hot().float", "torch.log", "masked_softmax.max", "logits.data.new", "modeling_composition.convert_to_one_hot", "logits.size", "torch.log", "masked_softmax.size"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.masked_softmax", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.convert_to_one_hot"], ["", "def", "st_gumbel_softmax", "(", "logits", ",", "temperature", "=", "1.0", ",", "mask", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the result of Straight-Through Gumbel-Softmax Estimation.\n    It approximates the discrete sampling via Gumbel-Softmax trick\n    and applies the biased ST estimator.\n    In the forward propagation, it emits the discrete one-hot result,\n    and in the backward propagation it approximates the categorical\n    distribution via smooth Gumbel-Softmax distribution.\n\n    Args:\n        logits (tensor): A un-normalized probability values,\n            which has the size (batch_size, num_classes)\n        temperature (float): A temperature parameter. The higher\n            the value is, the smoother the distribution is.\n        mask (tensor, optional): If given, it masks the softmax\n            so that indices of '0' mask values are not selected.\n            The size is (batch_size, num_classes).\n\n    Returns:\n        y: The sampled output, which has the property explained above.\n    \"\"\"", "\n", "\n", "eps", "=", "1e-10", "\n", "u", "=", "logits", ".", "data", ".", "new", "(", "*", "logits", ".", "size", "(", ")", ")", ".", "uniform_", "(", ")", "\n", "gumbel_noise", "=", "-", "torch", ".", "log", "(", "-", "torch", ".", "log", "(", "u", "+", "eps", ")", "+", "eps", ")", "\n", "y", "=", "logits", "+", "gumbel_noise", "\n", "y", "=", "masked_softmax", "(", "logits", "=", "y", "/", "temperature", ",", "mask", "=", "mask", ")", "\n", "y_argmax", "=", "y", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "y_hard", "=", "convert_to_one_hot", "(", "indices", "=", "y_argmax", ",", "num_classes", "=", "y", ".", "size", "(", "1", ")", ")", ".", "float", "(", ")", "\n", "y", "=", "(", "y_hard", "-", "y", ")", ".", "detach", "(", ")", "+", "y", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.sequence_mask": [[77, 86], ["sequence_length.size", "torch.arange().long().type_as", "torch.arange().long().type_as.unsqueeze().expand", "seq_range_expand.to.to", "sequence_length.unsqueeze().expand_as", "sequence_length.data.max", "torch.arange().long", "torch.arange().long().type_as.unsqueeze", "sequence_length.unsqueeze", "torch.arange"], "function", ["None"], ["", "def", "sequence_mask", "(", "sequence_length", ",", "max_length", "=", "None", ")", ":", "\n", "    ", "if", "max_length", "is", "None", ":", "\n", "        ", "max_length", "=", "sequence_length", ".", "data", ".", "max", "(", ")", "\n", "", "batch_size", "=", "sequence_length", ".", "size", "(", "0", ")", "\n", "seq_range", "=", "torch", ".", "arange", "(", "0", ",", "max_length", ")", ".", "long", "(", ")", ".", "type_as", "(", "sequence_length", ")", "\n", "seq_range_expand", "=", "seq_range", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "max_length", ")", "\n", "seq_range_expand", "=", "seq_range_expand", ".", "to", "(", "sequence_length", ")", "\n", "seq_length_expand", "=", "sequence_length", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "seq_range_expand", ")", "\n", "return", "seq_range_expand", "<", "seq_length_expand", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.reverse_padded_sequence": [[88, 118], ["enumerate", "torch.LongTensor().unsqueeze().expand_as", "torch.gather", "inputs.transpose.transpose", "inputs.transpose.size", "len", "ValueError", "list", "torch.LongTensor().unsqueeze().expand_as.type_as", "reversed_inputs.transpose.transpose", "range", "range", "torch.LongTensor().unsqueeze", "inputs.transpose.size", "inputs.transpose.size", "torch.LongTensor"], "function", ["None"], ["", "def", "reverse_padded_sequence", "(", "inputs", ",", "lengths", ",", "batch_first", "=", "False", ")", ":", "\n", "    ", "\"\"\"Reverses sequences according to their lengths.\n    Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n    ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n    B is the batch size, and * is any number of dimensions (including 0).\n    Arguments:\n        inputs (tensor): padded batch of variable length sequences.\n        lengths (list[int]): list of sequence lengths\n        batch_first (bool, optional): if True, inputs should be B x T x *.\n    Returns:\n        A tensor with the same size as inputs, but with each sequence\n        reversed according to its length.\n    \"\"\"", "\n", "\n", "if", "not", "batch_first", ":", "\n", "        ", "inputs", "=", "inputs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "inputs", ".", "size", "(", "0", ")", "!=", "len", "(", "lengths", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'inputs incompatible with lengths.'", ")", "\n", "", "reversed_indices", "=", "[", "list", "(", "range", "(", "inputs", ".", "size", "(", "1", ")", ")", ")", "\n", "for", "_", "in", "range", "(", "inputs", ".", "size", "(", "0", ")", ")", "]", "\n", "for", "i", ",", "length", "in", "enumerate", "(", "lengths", ")", ":", "\n", "        ", "if", "length", ">", "0", ":", "\n", "            ", "reversed_indices", "[", "i", "]", "[", ":", "length", "]", "=", "reversed_indices", "[", "i", "]", "[", "length", "-", "1", ":", ":", "-", "1", "]", "\n", "", "", "reversed_indices", "=", "(", "torch", ".", "LongTensor", "(", "reversed_indices", ")", ".", "unsqueeze", "(", "2", ")", "\n", ".", "expand_as", "(", "inputs", ")", ")", "\n", "# reversed_indices = reversed_indices.to(inputs)", "\n", "reversed_inputs", "=", "torch", ".", "gather", "(", "inputs", ",", "1", ",", "reversed_indices", ".", "type_as", "(", "lengths", ")", ")", "\n", "if", "not", "batch_first", ":", "\n", "        ", "reversed_inputs", "=", "reversed_inputs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "return", "reversed_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_bert.ChengyuBertForClozeChid.__init__": [[40, 48], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "modeling_bert.ChengyuBertForClozeChid.init_weights"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_bert.ChengyuBertForClozeChid.vocab": [[49, 51], ["torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "self", ".", "idiom_embedding", ".", "weight", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_bert.ChengyuBertForClozeChid.forward": [[52, 86], ["modeling_bert.ChengyuBertForClozeChid.bert", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_bert.ChengyuBertForClozeChid.vocab", "modeling_bert.ChengyuBertForClozeChid.dropout", "modeling_bert.ChengyuBertForClozeChid.classifier", "modeling_bert.ChengyuBertForClozeChid.view", "ValueError", "len", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "modeling_bert.ChengyuBertForClozeChid.idiom_embedding", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "encoded_options", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "encoded_options", "=", "self", ".", "idiom_embedding", "(", "option_ids", ")", "\n", "\n", "", "multiply_result", "=", "torch", ".", "einsum", "(", "'abc,ac->abc'", ",", "encoded_options", ",", "blank_states", ")", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "blank_states", ")", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "multiply_result", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "len", "(", "positions", ")", ",", "-", "1", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "reshaped_logits", ",", "over_logits", ",", "cond_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_bert.BertForClozeChoice.__init__": [[122, 131], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "modeling_bert.BertForClozeChoice.init_weights"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "use_kld", "=", "opts", ".", "use_kld", "\n", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_bert.BertForClozeChoice.forward": [[132, 152], ["input_ids.view.view.view", "modeling_bert.BertForClozeChoice.bert", "modeling_bert.BertForClozeChoice.dropout", "modeling_bert.BertForClozeChoice.classifier", "modeling_bert.BertForClozeChoice.view", "input_ids.view.view.size", "attention_mask.view", "token_type_ids.view", "attention_mask.size", "token_type_ids.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "option_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "idiom_ids", ",", "\n", "labels", "=", "None", ")", ":", "\n", "        ", "num_choices", "=", "input_ids", ".", "shape", "[", "1", "]", "\n", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "\n", "outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ")", "\n", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "num_choices", ")", "\n", "return", "reshaped_logits", ",", "None", ",", "None", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_embedding.ChengyuBertEmb.__init__": [[12, 23], ["transformers.BertPreTrainedModel.__init__", "int", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "modeling_embedding.ChengyuBertEmb.register_buffer", "modeling_embedding.ChengyuBertEmb.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "modeling_embedding.ChengyuBertEmb.model_name.split"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "chengyu_emb_dim", "=", "int", "(", "self", ".", "model_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "chengyu_emb_dim", ")", "\n", "self", ".", "project_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "chengyu_emb_dim", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_embedding.ChengyuBertEmb.vocab": [[24, 28], ["modeling_embedding.ChengyuBertEmb.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "over_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "c_mo_logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "over_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "c_mo_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_embedding.ChengyuBertEmb.forward": [[29, 47], ["modeling_embedding.ChengyuBertEmb.bert", "modeling_embedding.ChengyuBertEmb.vocab", "modeling_embedding.ChengyuBertEmb.project_linear", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "over_logits", "=", "self", ".", "vocab", "(", "self", ".", "project_linear", "(", "blank_states", ")", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "cond_logits", ",", "over_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_embedding.ChengyuBertSGNS.__init__": [[51, 73], ["transformers.BertPreTrainedModel.__init__", "opts.model.startswith", "int", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "torch.LayerNorm", "torch.LayerNorm", "modeling_embedding.ChengyuBertSGNS.model_name.startswith", "modeling_embedding.ChengyuBertSGNS.register_buffer", "modeling_embedding.ChengyuBertSGNS.init_weights", "torch.Linear", "torch.Linear", "modeling_embedding.ChengyuBertSGNS.model_name.startswith", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "modeling_embedding.ChengyuBertSGNS.model_name.split", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "assert", "opts", ".", "model", ".", "startswith", "(", "(", "'chengyubert-ns-mask'", ",", "\n", "'chengyubert-ns-cls-mask'", ",", "\n", "'chengyubert-ns-element-wise'", ",", "\n", ")", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "chengyu_emb_dim", "=", "int", "(", "self", ".", "model_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", ")", "\n", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "chengyu_emb_dim", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "chengyu_emb_dim", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "if", "self", ".", "model_name", ".", "startswith", "(", "'chengyubert-ns-mask'", ")", ":", "\n", "            ", "self", ".", "project_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "chengyu_emb_dim", ")", "\n", "", "elif", "self", ".", "model_name", ".", "startswith", "(", "'chengyubert-ns-cls-mask'", ")", ":", "\n", "            ", "self", ".", "project_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "chengyu_emb_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "project_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "4", ",", "chengyu_emb_dim", ")", "\n", "", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_embedding.ChengyuBertSGNS.project": [[74, 84], ["modeling_embedding.ChengyuBertSGNS.model_name.startswith", "modeling_embedding.ChengyuBertSGNS.project_linear", "modeling_embedding.ChengyuBertSGNS.model_name.startswith", "modeling_embedding.ChengyuBertSGNS.project_linear", "modeling_embedding.ChengyuBertSGNS.project_linear", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "project", "(", "self", ",", "cls_states", ",", "blank_states", ")", ":", "\n", "        ", "if", "self", ".", "model_name", ".", "startswith", "(", "'chengyubert-ns-mask'", ")", ":", "\n", "            ", "return", "self", ".", "project_linear", "(", "blank_states", ")", "\n", "", "elif", "self", ".", "model_name", ".", "startswith", "(", "'chengyubert-ns-cls-mask'", ")", ":", "\n", "            ", "return", "self", ".", "project_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "project_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "\n", "cls_states", ",", "\n", "blank_states", "*", "cls_states", ",", "\n", "blank_states", "-", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_embedding.ChengyuBertSGNS.vocab": [[85, 89], ["modeling_embedding.ChengyuBertSGNS.LayerNorm", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_embedding.ChengyuBertSGNS.idiom_embedding"], "methods", ["None"], ["", "", "def", "vocab", "(", "self", ",", "over_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", ")", "\n", "c_mo_logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "over_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "c_mo_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_embedding.ChengyuBertSGNS.forward": [[90, 124], ["modeling_embedding.ChengyuBertSGNS.bert", "modeling_embedding.ChengyuBertSGNS.project", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "modeling_embedding.ChengyuBertSGNS.LayerNorm", "option_ids.size", "torch.masked_select().view", "torch.masked_select().view", "torch.masked_select().view", "torch.masked_select().view", "modeling_embedding.ChengyuBertSGNS.LayerNorm", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "modeling_embedding.ChengyuBertSGNS.vocab", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "modeling_embedding.ChengyuBertSGNS.idiom_embedding", "modeling_embedding.ChengyuBertSGNS.idiom_embedding", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.functional.logsigmoid", "torch.functional.logsigmoid", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "targets.unsqueeze", "torch.gather.squeeze", "torch.gather.squeeze", "torch.masked_select", "torch.masked_select", "torch.masked_select", "torch.masked_select", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.functional.logsigmoid", "torch.functional.logsigmoid", "modeling_embedding.ChengyuBertSGNS.unsqueeze", "range", "torch.gather.repeat", "torch.gather.repeat", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.project", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "emb_u", "=", "self", ".", "project", "(", "cls_states", ",", "blank_states", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "emb_v", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "target", ".", "squeeze", "(", "1", ")", ")", ")", "\n", "\n", "bs", ",", "num", "=", "option_ids", ".", "size", "(", ")", "\n", "\n", "negative_samples", "=", "torch", ".", "masked_select", "(", "option_ids", ",", "\n", "option_ids", "!=", "target", ".", "repeat", "(", "[", "1", ",", "num", "]", ")", ")", ".", "view", "(", "bs", ",", "-", "1", ")", "\n", "emb_neg_v", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "negative_samples", ")", ")", "\n", "\n", "score", "=", "torch", ".", "sum", "(", "torch", ".", "mul", "(", "emb_u", ",", "emb_v", ")", ",", "dim", "=", "1", ")", "\n", "# score = torch.clamp(score, max=10, min=-10)", "\n", "score", "=", "-", "nn", ".", "functional", ".", "logsigmoid", "(", "score", ")", "\n", "\n", "neg_score", "=", "torch", ".", "bmm", "(", "emb_neg_v", ",", "emb_u", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", ")", "\n", "# neg_score = torch.clamp(neg_score, max=10, min=-10)", "\n", "neg_score", "=", "-", "torch", ".", "sum", "(", "nn", ".", "functional", ".", "logsigmoid", "(", "-", "neg_score", ")", ",", "dim", "=", "1", ")", "\n", "\n", "return", "torch", ".", "mean", "(", "score", "+", "neg_score", ")", "\n", "", "else", ":", "\n", "            ", "over_logits", "=", "self", ".", "vocab", "(", "emb_u", ")", "\n", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "cond_logits", ",", "over_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.CaloClassifier.__init__": [[15, 28], ["torch.nn.Module.__init__", "chengyubert.modules.utils.WeightNormClassifier", "chengyubert.modules.utils.WeightNormClassifier"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "hidden_dropout_prob", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Emotion-7 Predictor", "\n", "self", ".", "fine_emotion_classifier", "=", "WeightNormClassifier", "(", "hidden_size", ",", "\n", "21", ",", "\n", "hidden_size", ",", "\n", "hidden_dropout_prob", ")", "\n", "\n", "# Sentiment Predictor", "\n", "self", ".", "sentiment_classifier", "=", "WeightNormClassifier", "(", "hidden_size", ",", "\n", "4", ",", "\n", "hidden_size", ",", "\n", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.CaloClassifier.forward": [[29, 35], ["modeling_affection.CaloClassifier.fine_emotion_classifier", "modeling_affection.CaloClassifier.sentiment_classifier"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "emotion_state", ")", "->", "Tuple", "[", "Any", ",", "Any", "]", ":", "\n", "# affection prediction", "\n", "        ", "fine_emotion_logits", "=", "self", ".", "fine_emotion_classifier", "(", "emotion_state", ")", "\n", "# coarse_emotion_logits = self.coarse_emotion_classifier(emotion_state)", "\n", "sentiment_logits", "=", "self", ".", "sentiment_classifier", "(", "emotion_state", ")", "\n", "return", "fine_emotion_logits", ",", "sentiment_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.SlideClassifier.__init__": [[39, 45], ["torch.nn.Module.__init__", "chengyubert.modules.utils.WeightNormClassifier"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "hidden_dropout_prob", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sentiment_classifier", "=", "WeightNormClassifier", "(", "hidden_size", ",", "\n", "3", ",", "\n", "hidden_size", ",", "\n", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.SlideClassifier.forward": [[46, 49], ["modeling_affection.SlideClassifier.sentiment_classifier"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "emotion_state", ")", "->", "None", ":", "\n", "# slide prediction", "\n", "        ", "return", "self", ".", "sentiment_classifier", "(", "emotion_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.CaloLoss.__init__": [[59, 67], ["torch.nn.Module.__init__", "chengyubert.optim.loss.FocalLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "use_focal", ",", "weights", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "fine_emotion_weights", ",", "sentiment_weights", "=", "weights", "\n", "if", "use_focal", ":", "\n", "            ", "self", ".", "loss_fct", "=", "FocalLoss", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "fine_emotion_loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "fine_emotion_weights", ",", "reduction", "=", "'none'", ")", "\n", "self", ".", "sentiment_loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "sentiment_weights", ",", "reduction", "=", "'none'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.CaloLoss.forward": [[68, 78], ["modeling_affection.CaloLoss.fine_emotion_loss_fct", "modeling_affection.CaloLoss.sentiment_loss_fct", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "logits", ",", "targets", ")", "->", "Tuple", "[", "Optional", "[", "Any", "]", ",", "Tuple", "[", "Any", ",", "Any", "]", "]", ":", "\n", "        ", "over_logits", ",", "(", "fine_emotion_logits", ",", "sentiment_logits", ")", "=", "logits", "\n", "if", "over_logits", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "targets", "[", ":", ",", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "over_loss", "=", "None", "\n", "", "fine_emotion_loss", "=", "self", ".", "fine_emotion_loss_fct", "(", "fine_emotion_logits", ",", "targets", "[", ":", ",", "2", "]", ")", "\n", "sentiment_loss", "=", "self", ".", "sentiment_loss_fct", "(", "sentiment_logits", ",", "targets", "[", ":", ",", "3", "]", ")", "\n", "return", "over_loss", ",", "(", "fine_emotion_loss", ",", "sentiment_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.SlideLoss.__init__": [[82, 88], ["torch.nn.Module.__init__", "chengyubert.optim.loss.FocalLoss", "torch.nn.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "use_focal", ",", "weights", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "use_focal", ":", "\n", "            ", "self", ".", "loss_fct", "=", "FocalLoss", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "weights", ",", "reduction", "=", "'none'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.SlideLoss.forward": [[89, 98], ["modeling_affection.SlideLoss.loss_fct", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "logits", ",", "targets", ")", "->", "Tuple", "[", "Any", ",", "Any", "]", ":", "\n", "        ", "over_logits", ",", "sentiment_logits", "=", "logits", "\n", "if", "over_logits", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "targets", "[", ":", ",", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "over_loss", "=", "None", "\n", "", "sentiment_emotion_loss", "=", "self", ".", "loss_fct", "(", "sentiment_logits", ",", "targets", "[", ":", ",", "1", "]", ")", "\n", "return", "over_loss", ",", "sentiment_emotion_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPooling.__init__": [[109, 121], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling_affection.AffectionMaxPooling.init_weights"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPooling.forward": [[122, 146], ["modeling_affection.AffectionMaxPooling.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "torch.gather.max", "modeling_affection.AffectionMaxPooling.channel1_linear().tanh", "modeling_affection.AffectionMaxPooling.classifier", "modeling_affection.AffectionMaxPooling.loss_fct", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "modeling_affection.AffectionMaxPooling.channel1_linear", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "gather_index", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "# n, batch_size, seq_len = input_ids.size()", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", "\n", "\n", "# idiom_length = (gather_index > 0).sum(1)", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "\n", "composed_states", ",", "_", "=", "idiom_states", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "emotion_state", "=", "self", ".", "channel1_linear", "(", "composed_states", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "_", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "None", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "None", ",", "None", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "None", ",", "None", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPoolingMasked.__init__": [[151, 166], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionMaxPoolingMasked.init_weights"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPoolingMasked.forward": [[167, 204], ["input_ids.size", "modeling_affection.AffectionMaxPoolingMasked.bert", "gather_index.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as", "torch.gather", "torch.gather.max", "torch.gather.max", "modeling_affection.AffectionMaxPoolingMasked.channel1_linear().tanh", "modeling_affection.AffectionMaxPoolingMasked.channel2_linear().tanh", "modeling_affection.AffectionMaxPoolingMasked.compose_linear().tanh", "modeling_affection.AffectionMaxPoolingMasked.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionMaxPoolingMasked.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand", "gather_index_masked.unsqueeze().expand", "modeling_affection.AffectionMaxPoolingMasked.channel1_linear", "modeling_affection.AffectionMaxPoolingMasked.channel2_linear", "modeling_affection.AffectionMaxPoolingMasked.compose_linear", "torch.cat", "gather_index.unsqueeze", "gather_index_masked.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "gather_index", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "idiom_length", "=", "(", "gather_index", ">", "0", ")", ".", "sum", "(", "1", ")", "\n", "\n", "gather_index_unsqueezed", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index_unsqueezed", ")", "\n", "\n", "gather_index_masked_unsqueezed", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "\n", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked_unsqueezed", ")", "\n", "\n", "composed_states", ",", "_", "=", "idiom_states", ".", "max", "(", "dim", "=", "1", ")", "\n", "composed_states_masked", ",", "_", "=", "idiom_states_masked", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "composed_states", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "composed_states_masked", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "torch", ".", "cat", "(", "[", "channel1", ",", "channel2", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "_", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "None", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "None", ",", "None", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "None", ",", "None", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.__init__": [[209, 232], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "print", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.init_weights", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.register_buffer", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.register_buffer", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.enlarged_candidates.size", "torch.tensor", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "opts", ".", "enlarged_candidates", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "tensor", "(", "opts", ".", "enlarged_candidates", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "", "print", "(", "self", ".", "enlarged_candidates", ".", "size", "(", ")", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.vocab": [[233, 238], ["modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum.softmax"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "state", "=", "torch", ".", "einsum", "(", "'bn,nd->bd'", ",", "[", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "logits", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.forward": [[239, 275], ["input_ids.size", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as", "torch.gather", "torch.gather.max", "torch.gather.max", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.vocab", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.channel1_linear().tanh", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.channel2_linear().tanh", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.compose_linear().tanh", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "gather_index_masked.unsqueeze().expand", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.channel1_linear", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.channel2_linear", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiom.compose_linear", "torch.cat", "torch.cat", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze", "gather_index_masked.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "gather_index", ",", "option_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "gather_index_masked_unsqueezed", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "\n", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked_unsqueezed", ")", "\n", "\n", "composed_states", ",", "_", "=", "idiom_states", ".", "max", "(", "dim", "=", "1", ")", "\n", "composed_states_masked", ",", "_", "=", "idiom_states_masked", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "over_logits", ",", "idiom_attn_state", "=", "self", ".", "vocab", "(", "composed_states_masked", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "composed_states", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "torch", ".", "cat", "(", "[", "composed_states_masked", ",", "idiom_attn_state", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "torch", ".", "cat", "(", "[", "channel1", ",", "channel2", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "over_loss", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "over_logits", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "over_loss", ",", "None", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "over_logits", ",", "None", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.__init__": [[280, 306], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "print", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.register_parameter", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.init_weights", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.register_buffer", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.register_buffer", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.enlarged_candidates.size", "torch.tensor", "torch.arange", "torch.nn.Parameter", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "opts", ".", "enlarged_candidates", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "tensor", "(", "opts", ".", "enlarged_candidates", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "", "print", "(", "self", ".", "enlarged_candidates", ".", "size", "(", ")", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "register_parameter", "(", "name", "=", "'g'", ",", "\n", "param", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "config", ".", "hidden_size", ")", "/", "config", ".", "hidden_size", ")", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.vocab": [[307, 312], ["modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum.softmax"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "state", "=", "torch", ".", "einsum", "(", "'bn,nd->bd'", ",", "[", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "logits", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.forward": [[313, 352], ["input_ids.size", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as", "torch.gather", "torch.gather.max", "torch.gather.max", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.vocab", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.channel1_linear().tanh", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.channel2_linear().tanh", "torch.sigmoid", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.compose_linear().tanh", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "gather_index_masked.unsqueeze().expand", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.channel1_linear", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.channel2_linear", "modeling_affection.AffectionMaxPoolingMaskedLatentIdiomWithGate.compose_linear", "torch.cat", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze", "gather_index_masked.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "gather_index", ",", "option_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "gather_index_masked_unsqueezed", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "\n", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked_unsqueezed", ")", "\n", "\n", "composed_states", ",", "_", "=", "idiom_states", ".", "max", "(", "dim", "=", "1", ")", "\n", "composed_states_masked", ",", "_", "=", "idiom_states_masked", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "over_logits", ",", "idiom_attn_state", "=", "self", ".", "vocab", "(", "composed_states_masked", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "composed_states", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "torch", ".", "cat", "(", "[", "composed_states_masked", ",", "idiom_attn_state", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "gate", "=", "torch", ".", "sigmoid", "(", "self", ".", "g", "*", "channel1", ")", "\n", "s", "=", "gate", "*", "channel1", "+", "(", "1", "-", "gate", ")", "*", "channel2", "\n", "\n", "# affection prediction", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "s", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "over_loss", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "over_logits", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "over_loss", ",", "None", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "over_logits", ",", "None", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCompose.__init__": [[357, 370], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "chengyubert.modules.utils.LatentComposition", "torch.nn.Linear", "modeling_affection.AffectionCompose.init_weights"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "idiom_compose", "=", "LatentComposition", "(", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCompose.forward": [[371, 395], ["modeling_affection.AffectionCompose.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "modeling_affection.AffectionCompose.idiom_compose", "modeling_affection.AffectionCompose.channel1_linear().tanh", "modeling_affection.AffectionCompose.classifier", "modeling_affection.AffectionCompose.loss_fct", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "modeling_affection.AffectionCompose.channel1_linear", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.idiom_compose"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "gather_index", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "# n, batch_size, seq_len = input_ids.size()", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", "\n", "\n", "idiom_length", "=", "(", "gather_index", ">", "0", ")", ".", "sum", "(", "1", ")", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "\n", "composed_states", ",", "_", ",", "select_masks", "=", "self", ".", "idiom_compose", "(", "idiom_states", ",", "idiom_length", ")", "\n", "\n", "emotion_state", "=", "self", ".", "channel1_linear", "(", "composed_states", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "_", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "None", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "None", ",", "select_masks", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "None", ",", "select_masks", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionComposeMasked.__init__": [[400, 416], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "chengyubert.modules.utils.LatentComposition", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionComposeMasked.init_weights"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "idiom_compose", "=", "LatentComposition", "(", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionComposeMasked.forward": [[417, 453], ["input_ids.size", "modeling_affection.AffectionComposeMasked.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "modeling_affection.AffectionComposeMasked.idiom_compose", "torch.gather.max", "modeling_affection.AffectionComposeMasked.channel1_linear().tanh", "modeling_affection.AffectionComposeMasked.channel2_linear().tanh", "modeling_affection.AffectionComposeMasked.compose_linear().tanh", "modeling_affection.AffectionComposeMasked.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionComposeMasked.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "modeling_affection.AffectionComposeMasked.channel1_linear", "modeling_affection.AffectionComposeMasked.channel2_linear", "modeling_affection.AffectionComposeMasked.compose_linear", "torch.cat", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.idiom_compose"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "gather_index", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "idiom_length", "=", "(", "gather_index", ">", "0", ")", ".", "sum", "(", "1", ")", "\n", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "\n", "gather_index_masked", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked", ")", "\n", "\n", "composed_states", ",", "_", ",", "select_masks", "=", "self", ".", "idiom_compose", "(", "idiom_states", ",", "idiom_length", ")", "\n", "# composed_states_masked, _, select_masks_masked = self.idiom_compose(idiom_states_masked, idiom_length)", "\n", "composed_states_masked", ",", "_", "=", "idiom_states_masked", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "composed_states", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "composed_states_masked", ")", ".", "tanh", "(", ")", "\n", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "torch", ".", "cat", "(", "[", "channel1", ",", "channel2", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "_", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "None", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "None", ",", "select_masks", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "None", ",", "select_masks", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionComposeMaskedLatentIdiom.__init__": [[458, 483], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "print", "torch.nn.Embedding", "chengyubert.modules.utils.LatentComposition", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionComposeMaskedLatentIdiom.init_weights", "modeling_affection.AffectionComposeMaskedLatentIdiom.register_buffer", "modeling_affection.AffectionComposeMaskedLatentIdiom.register_buffer", "modeling_affection.AffectionComposeMaskedLatentIdiom.enlarged_candidates.size", "torch.tensor", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "opts", ".", "enlarged_candidates", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "tensor", "(", "opts", ".", "enlarged_candidates", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "", "print", "(", "self", ".", "enlarged_candidates", ".", "size", "(", ")", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "idiom_compose", "=", "LatentComposition", "(", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionComposeMaskedLatentIdiom.vocab": [[484, 489], ["modeling_affection.AffectionComposeMaskedLatentIdiom.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum.softmax"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "state", "=", "torch", ".", "einsum", "(", "'bn,nd->bd'", ",", "[", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "logits", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionComposeMaskedLatentIdiom.forward": [[490, 528], ["input_ids.size", "modeling_affection.AffectionComposeMaskedLatentIdiom.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "modeling_affection.AffectionComposeMaskedLatentIdiom.idiom_compose", "torch.gather.max", "modeling_affection.AffectionComposeMaskedLatentIdiom.vocab", "modeling_affection.AffectionComposeMaskedLatentIdiom.channel1_linear().tanh", "modeling_affection.AffectionComposeMaskedLatentIdiom.channel2_linear().tanh", "modeling_affection.AffectionComposeMaskedLatentIdiom.compose_linear().tanh", "modeling_affection.AffectionComposeMaskedLatentIdiom.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionComposeMaskedLatentIdiom.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "modeling_affection.AffectionComposeMaskedLatentIdiom.channel1_linear", "modeling_affection.AffectionComposeMaskedLatentIdiom.channel2_linear", "modeling_affection.AffectionComposeMaskedLatentIdiom.compose_linear", "torch.cat", "torch.cat", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.idiom_compose", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "gather_index", ",", "option_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "idiom_length", "=", "(", "gather_index", ">", "0", ")", ".", "sum", "(", "1", ")", "\n", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "\n", "gather_index_masked", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked", ")", "\n", "# idiom_states = encoded_context[[i for i in range(len(positions))], positions]  # [batch, hidden_state]", "\n", "\n", "composed_states", ",", "_", ",", "select_masks", "=", "self", ".", "idiom_compose", "(", "idiom_states", ",", "idiom_length", ")", "\n", "composed_states_masked", ",", "_", "=", "idiom_states_masked", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "over_logits", ",", "idiom_attn_state", "=", "self", ".", "vocab", "(", "composed_states_masked", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "composed_states", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "torch", ".", "cat", "(", "[", "composed_states_masked", ",", "idiom_attn_state", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "torch", ".", "cat", "(", "[", "channel1", ",", "channel2", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "over_loss", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "over_logits", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "over_loss", ",", "select_masks", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "over_logits", ",", "select_masks", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.__init__": [[533, 566], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "print", "torch.nn.Embedding", "chengyubert.modules.utils.LatentComposition", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.register_parameter", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.init_weights", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.register_buffer", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.register_buffer", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.enlarged_candidates.size", "torch.tensor", "torch.arange", "torch.nn.Parameter", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "use_leaf_rnn", "=", "True", "\n", "self", ".", "intra_attention", "=", "False", "\n", "self", ".", "gumbel_temperature", "=", "1", "\n", "self", ".", "bidirectional", "=", "True", "\n", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "opts", ".", "enlarged_candidates", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "tensor", "(", "opts", ".", "enlarged_candidates", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "", "print", "(", "self", ".", "enlarged_candidates", ".", "size", "(", ")", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "idiom_compose", "=", "LatentComposition", "(", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "register_parameter", "(", "name", "=", "'g'", ",", "\n", "param", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "config", ".", "hidden_size", ")", "/", "config", ".", "hidden_size", ")", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.vocab": [[567, 572], ["modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum.softmax"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "state", "=", "torch", ".", "einsum", "(", "'bn,nd->bd'", ",", "[", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "logits", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.forward": [[573, 615], ["input_ids.size", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.bert", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.idiom_compose", "torch.gather.max", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.vocab", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.channel1_linear().tanh", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.channel2_linear().tanh", "torch.sigmoid", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.compose_linear().tanh", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.channel1_linear", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.channel2_linear", "modeling_affection.AffectionComposeMaskedLatentIdiomWithGate.compose_linear", "torch.cat", "gather_index.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.ChengyuBertComposition.idiom_compose", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "gather_index", ",", "option_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "idiom_length", "=", "(", "gather_index", ">", "0", ")", ".", "sum", "(", "1", ")", "\n", "\n", "gather_index", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "\n", "gather_index_masked", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked", ")", "\n", "# idiom_states = encoded_context[[i for i in range(len(positions))], positions]  # [batch, hidden_state]", "\n", "\n", "composed_states", ",", "_", ",", "select_masks", "=", "self", ".", "idiom_compose", "(", "idiom_states", ",", "idiom_length", ")", "\n", "composed_states_masked", ",", "_", "=", "idiom_states_masked", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "over_logits", ",", "idiom_attn_state", "=", "self", ".", "vocab", "(", "composed_states_masked", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "composed_states", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "torch", ".", "cat", "(", "[", "composed_states_masked", ",", "idiom_attn_state", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "gate", "=", "torch", ".", "sigmoid", "(", "self", ".", "g", "*", "channel1", ")", "\n", "s", "=", "gate", "*", "channel1", "+", "(", "1", "-", "gate", ")", "*", "channel2", "\n", "\n", "# affection prediction", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "s", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "over_loss", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "over_logits", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "over_loss", ",", "select_masks", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "over_logits", ",", "select_masks", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMasked.__init__": [[620, 643], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "print", "chengyubert.modules.attention.ContrastiveCoAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionCoAttentionMasked.init_weights", "modeling_affection.AffectionCoAttentionMasked.register_buffer", "modeling_affection.AffectionCoAttentionMasked.register_buffer", "modeling_affection.AffectionCoAttentionMasked.enlarged_candidates.size", "torch.tensor", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "opts", ".", "enlarged_candidates", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "tensor", "(", "opts", ".", "enlarged_candidates", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "", "print", "(", "self", ".", "enlarged_candidates", ".", "size", "(", ")", ")", "\n", "\n", "self", ".", "coattention", "=", "ContrastiveCoAttention", "(", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMasked.forward": [[644, 687], ["input_ids.size", "modeling_affection.AffectionCoAttentionMasked.bert", "gather_index.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as", "torch.gather", "chengyubert.modules.utils.sequence_mask", "modeling_affection.AffectionCoAttentionMasked.coattention", "modeling_affection.AffectionCoAttentionMasked.channel1_linear().tanh", "modeling_affection.AffectionCoAttentionMasked.channel2_linear().tanh", "modeling_affection.AffectionCoAttentionMasked.compose_linear().tanh", "modeling_affection.AffectionCoAttentionMasked.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionCoAttentionMasked.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand", "gather_index_masked.unsqueeze().expand", "modeling_affection.AffectionCoAttentionMasked.channel1_linear", "modeling_affection.AffectionCoAttentionMasked.channel2_linear", "modeling_affection.AffectionCoAttentionMasked.compose_linear", "torch.cat", "gather_index.unsqueeze", "gather_index_masked.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.sequence_mask"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "gather_index", ",", "option_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "\n", "gather_index_unsqueezed", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index_unsqueezed", ")", "\n", "\n", "gather_index_masked_unsqueezed", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "\n", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked_unsqueezed", ")", "\n", "\n", "# composed_states_masked, _ = idiom_states_masked.max(dim=1)", "\n", "\n", "L", "=", "idiom_states", "\n", "I", "=", "idiom_states_masked", "\n", "\n", "idiom_length", "=", "(", "gather_index", ">", "0", ")", ".", "sum", "(", "1", ")", "\n", "idiom_mask", "=", "sequence_mask", "(", "idiom_length", ")", "\n", "\n", "C_L", ",", "C_I", "=", "self", ".", "coattention", "(", "L", ",", "I", ",", "idiom_mask", ",", "idiom_mask", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "C_L", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "C_I", ")", ".", "tanh", "(", ")", "\n", "\n", "# slide prediction", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "torch", ".", "cat", "(", "[", "channel1", ",", "channel2", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "_", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "None", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "None", ",", "None", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "None", ",", "None", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedLatentIdiom.__init__": [[692, 717], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "print", "torch.nn.Embedding", "chengyubert.modules.attention.ContrastiveCoAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.init_weights", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.register_buffer", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.register_buffer", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.enlarged_candidates.size", "torch.tensor", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "opts", ".", "enlarged_candidates", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "tensor", "(", "opts", ".", "enlarged_candidates", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "", "print", "(", "self", ".", "enlarged_candidates", ".", "size", "(", ")", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "coattention", "=", "ContrastiveCoAttention", "(", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedLatentIdiom.vocab": [[718, 723], ["modeling_affection.AffectionCoAttentionMaskedLatentIdiom.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum.softmax"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "state", "=", "torch", ".", "einsum", "(", "'bn,nd->bd'", ",", "[", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "logits", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedLatentIdiom.forward": [[724, 769], ["input_ids.size", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.bert", "gather_index.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as", "torch.gather", "chengyubert.modules.utils.sequence_mask", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.coattention", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.vocab", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.channel1_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.channel2_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.compose_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand", "gather_index_masked.unsqueeze().expand", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.channel1_linear", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.channel2_linear", "modeling_affection.AffectionCoAttentionMaskedLatentIdiom.compose_linear", "torch.cat", "torch.cat", "gather_index.unsqueeze", "gather_index_masked.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.sequence_mask", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "gather_index", ",", "option_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "\n", "gather_index_unsqueezed", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index_unsqueezed", ")", "\n", "\n", "gather_index_masked_unsqueezed", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "\n", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked_unsqueezed", ")", "\n", "\n", "# composed_states_masked, _ = idiom_states_masked.max(dim=1)", "\n", "\n", "L", "=", "idiom_states", "\n", "I", "=", "idiom_states_masked", "\n", "\n", "idiom_length", "=", "(", "gather_index", ">", "0", ")", ".", "sum", "(", "1", ")", "\n", "idiom_mask", "=", "sequence_mask", "(", "idiom_length", ")", "\n", "\n", "C_L", ",", "C_I", "=", "self", ".", "coattention", "(", "L", ",", "I", ",", "idiom_mask", ",", "idiom_mask", ")", "\n", "\n", "over_logits", ",", "idiom_attn_state", "=", "self", ".", "vocab", "(", "C_I", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "C_L", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "torch", ".", "cat", "(", "[", "C_I", ",", "idiom_attn_state", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# slide prediction", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "torch", ".", "cat", "(", "[", "channel1", ",", "channel2", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "over_loss", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "over_logits", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "over_loss", ",", "None", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "over_logits", ",", "None", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.__init__": [[774, 802], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "print", "torch.nn.Embedding", "chengyubert.modules.attention.ContrastiveCoAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.register_parameter", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.init_weights", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.register_buffer", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.register_buffer", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.enlarged_candidates.size", "torch.tensor", "torch.arange", "torch.nn.Parameter", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "opts", ".", "enlarged_candidates", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "tensor", "(", "opts", ".", "enlarged_candidates", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "", "print", "(", "self", ".", "enlarged_candidates", ".", "size", "(", ")", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "coattention", "=", "ContrastiveCoAttention", "(", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "register_parameter", "(", "name", "=", "'g'", ",", "\n", "param", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "config", ".", "hidden_size", ")", "/", "config", ".", "hidden_size", ")", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.vocab": [[803, 808], ["modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum.softmax"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "state", "=", "torch", ".", "einsum", "(", "'bn,nd->bd'", ",", "[", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "logits", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.forward": [[809, 857], ["input_ids.size", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.bert", "gather_index.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as", "torch.gather", "chengyubert.modules.utils.sequence_mask", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.coattention", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.vocab", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.channel1_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.channel2_linear().tanh", "torch.sigmoid", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.compose_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand", "gather_index_masked.unsqueeze().expand", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.channel1_linear", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.channel2_linear", "modeling_affection.AffectionCoAttentionMaskedLatentIdiomWithGate.compose_linear", "torch.cat", "gather_index.unsqueeze", "gather_index_masked.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_composition.sequence_mask", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "gather_index", ",", "option_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "\n", "gather_index_unsqueezed", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index_unsqueezed", ")", "\n", "\n", "gather_index_masked_unsqueezed", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "\n", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked_unsqueezed", ")", "\n", "\n", "# composed_states_masked, _ = idiom_states_masked.max(dim=1)", "\n", "\n", "L", "=", "idiom_states", "\n", "I", "=", "idiom_states_masked", "\n", "\n", "idiom_length", "=", "(", "gather_index", ">", "0", ")", ".", "sum", "(", "1", ")", "\n", "idiom_mask", "=", "sequence_mask", "(", "idiom_length", ")", "\n", "\n", "C_L", ",", "C_I", "=", "self", ".", "coattention", "(", "L", ",", "I", ",", "idiom_mask", ",", "idiom_mask", ")", "\n", "\n", "over_logits", ",", "idiom_attn_state", "=", "self", ".", "vocab", "(", "C_I", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "C_L", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "torch", ".", "cat", "(", "[", "C_I", ",", "idiom_attn_state", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "gate", "=", "torch", ".", "sigmoid", "(", "self", ".", "g", "*", "channel1", ")", "\n", "s", "=", "gate", "*", "channel1", "+", "(", "1", "-", "gate", ")", "*", "channel2", "\n", "\n", "# affection prediction", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "s", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "over_loss", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "over_logits", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "over_loss", ",", "None", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "over_logits", ",", "None", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedFull.__init__": [[862, 885], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "print", "chengyubert.modules.attention.ContrastiveCoAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionCoAttentionMaskedFull.init_weights", "modeling_affection.AffectionCoAttentionMaskedFull.register_buffer", "modeling_affection.AffectionCoAttentionMaskedFull.register_buffer", "modeling_affection.AffectionCoAttentionMaskedFull.enlarged_candidates.size", "torch.tensor", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "opts", ".", "enlarged_candidates", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "tensor", "(", "opts", ".", "enlarged_candidates", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "", "print", "(", "self", ".", "enlarged_candidates", ".", "size", "(", ")", ")", "\n", "\n", "self", ".", "coattention", "=", "ContrastiveCoAttention", "(", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedFull.forward": [[886, 921], ["input_ids.size", "modeling_affection.AffectionCoAttentionMaskedFull.bert", "gather_index.unsqueeze().expand().type_as", "torch.gather", "torch.gather", "modeling_affection.AffectionCoAttentionMaskedFull.coattention", "modeling_affection.AffectionCoAttentionMaskedFull.channel1_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedFull.channel2_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedFull.compose_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedFull.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionCoAttentionMaskedFull.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand", "modeling_affection.AffectionCoAttentionMaskedFull.channel1_linear", "modeling_affection.AffectionCoAttentionMaskedFull.channel2_linear", "modeling_affection.AffectionCoAttentionMaskedFull.compose_linear", "torch.cat", "gather_index.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "gather_index", ",", "option_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "\n", "gather_index_unsqueezed", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index_unsqueezed", ")", "\n", "\n", "L", "=", "idiom_states", "\n", "mask_L", "=", "torch", ".", "gather", "(", "attention_mask", "[", "0", "]", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "I", "=", "encoded_context_masked", "\n", "mask_I", "=", "attention_mask", "[", "1", "]", "\n", "\n", "C_L", ",", "C_I", "=", "self", ".", "coattention", "(", "L", ",", "I", ",", "mask_L", ",", "mask_I", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "C_L", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "C_I", ")", ".", "tanh", "(", ")", "\n", "\n", "# slide prediction", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "torch", ".", "cat", "(", "[", "channel1", ",", "channel2", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "_", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "None", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "None", ",", "None", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "None", ",", "None", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.__init__": [[926, 952], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "print", "torch.nn.Embedding", "chengyubert.modules.attention.ContrastiveCoAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.init_weights", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.register_buffer", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.register_buffer", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.enlarged_candidates.size", "torch.tensor", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "project", "=", "opts", ".", "project", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "if", "opts", ".", "enlarged_candidates", "is", "not", "None", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "tensor", "(", "opts", ".", "enlarged_candidates", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "", "print", "(", "self", ".", "enlarged_candidates", ".", "size", "(", ")", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.idiom_compose = LatentComposition(config.hidden_size)", "\n", "self", ".", "coattention", "=", "ContrastiveCoAttention", "(", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "channel1_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "channel2_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "compose_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "\n", "self", ".", "classifier", "=", "classifiers", "[", "self", ".", "project", "]", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "loss_fct", "=", "loss_calculators", "[", "self", ".", "project", "]", "(", "opts", ".", "use_focal", ",", "opts", ".", "weights", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.vocab": [[953, 958], ["modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum.softmax"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "state", "=", "torch", ".", "einsum", "(", "'bn,nd->bd'", ",", "[", "logits", ".", "softmax", "(", "dim", "=", "-", "1", ")", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "logits", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.forward": [[959, 1004], ["input_ids.size", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.bert", "gather_index.unsqueeze().expand().type_as", "torch.gather", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand().type_as", "torch.gather", "torch.gather.max", "torch.gather", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.coattention", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.vocab", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.channel1_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.channel2_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.compose_linear().tanh", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.classifier", "input_ids.view", "encoded_outputs[].view", "encoded_outputs[].view", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.loss_fct", "token_type_ids.view", "attention_mask.view", "gather_index.unsqueeze().expand", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze().expand", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.channel1_linear", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.channel2_linear", "modeling_affection.AffectionCoAttentionMaskedFullLatentIdiom.compose_linear", "torch.cat", "torch.cat", "gather_index.unsqueeze", "gather_index_masked.unsqueeze().expand().type_as.unsqueeze().expand().type_as.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "gather_index", ",", "option_ids", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "n", ",", "batch_size", ",", "seq_len", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "n", "*", "batch_size", ",", "seq_len", ")", ")", "\n", "encoded_context", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "0", "]", "\n", "encoded_context_masked", "=", "encoded_outputs", "[", "0", "]", ".", "view", "(", "n", ",", "batch_size", ",", "seq_len", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "gather_index", ",", "gather_index_masked", "=", "gather_index", "\n", "\n", "gather_index_unsqueezed", "=", "gather_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "input_ids", ")", "\n", "idiom_states", "=", "torch", ".", "gather", "(", "encoded_context", ",", "dim", "=", "1", ",", "index", "=", "gather_index_unsqueezed", ")", "\n", "\n", "gather_index_masked", "=", "gather_index_masked", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "config", ".", "hidden_size", ")", ".", "type_as", "(", "\n", "input_ids", ")", "\n", "idiom_states_masked", "=", "torch", ".", "gather", "(", "encoded_context_masked", ",", "dim", "=", "1", ",", "index", "=", "gather_index_masked", ")", "\n", "# idiom_states = encoded_context[[i for i in range(len(positions))], positions]  # [batch, hidden_state]", "\n", "\n", "# composed_states, _, select_masks = self.idiom_compose(idiom_states, idiom_length)", "\n", "composed_states_masked", ",", "_", "=", "idiom_states_masked", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "L", "=", "idiom_states", "\n", "mask_L", "=", "torch", ".", "gather", "(", "attention_mask", "[", "0", "]", ",", "dim", "=", "1", ",", "index", "=", "gather_index", ")", "\n", "I", "=", "encoded_context_masked", "\n", "mask_I", "=", "attention_mask", "[", "1", "]", "\n", "\n", "C_L", ",", "C_I", "=", "self", ".", "coattention", "(", "L", ",", "I", ",", "mask_L", ",", "mask_I", ")", "\n", "\n", "over_logits", ",", "idiom_attn_state", "=", "self", ".", "vocab", "(", "C_I", ")", "\n", "\n", "channel1", "=", "self", ".", "channel1_linear", "(", "C_L", ")", ".", "tanh", "(", ")", "\n", "channel2", "=", "self", ".", "channel2_linear", "(", "torch", ".", "cat", "(", "[", "C_I", ",", "idiom_attn_state", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# slide prediction", "\n", "emotion_state", "=", "self", ".", "compose_linear", "(", "torch", ".", "cat", "(", "[", "channel1", ",", "channel2", "]", ",", "dim", "=", "-", "1", ")", ")", ".", "tanh", "(", ")", "\n", "\n", "# affection prediction", "\n", "logits", "=", "self", ".", "classifier", "(", "emotion_state", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "over_loss", ",", "losses", "=", "self", ".", "loss_fct", "(", "[", "over_logits", ",", "logits", "]", ",", "targets", ")", "\n", "return", "None", ",", "over_loss", ",", "None", ",", "losses", "\n", "", "else", ":", "\n", "            ", "return", "None", ",", "over_logits", ",", "None", ",", "logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStagePretrain.__init__": [[12, 22], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "modeling_2stage.ChengyuBertTwoStagePretrain.register_buffer", "modeling_2stage.ChengyuBertTwoStagePretrain.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "over_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "4", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStagePretrain.vocab": [[23, 26], ["modeling_2stage.ChengyuBertTwoStagePretrain.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStagePretrain.forward": [[27, 52], ["modeling_2stage.ChengyuBertTwoStagePretrain.bert", "modeling_2stage.ChengyuBertTwoStagePretrain.over_linear", "modeling_2stage.ChengyuBertTwoStagePretrain.vocab", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "\n", "over_states", "=", "self", ".", "over_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "\n", "cls_states", ",", "\n", "blank_states", "*", "cls_states", ",", "\n", "blank_states", "-", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "over_states", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "cond_logits", ",", "over_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageMaskPretrain.__init__": [[56, 65], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "modeling_2stage.ChengyuBertTwoStageMaskPretrain.register_buffer", "modeling_2stage.ChengyuBertTwoStageMaskPretrain.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageMaskPretrain.vocab": [[66, 69], ["modeling_2stage.ChengyuBertTwoStageMaskPretrain.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageMaskPretrain.forward": [[70, 89], ["modeling_2stage.ChengyuBertTwoStageMaskPretrain.bert", "modeling_2stage.ChengyuBertTwoStageMaskPretrain.vocab", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "blank_states", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "cond_logits", ",", "over_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageCLSPretrain.__init__": [[93, 103], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "modeling_2stage.ChengyuBertTwoStageCLSPretrain.register_buffer", "modeling_2stage.ChengyuBertTwoStageCLSPretrain.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "over_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageCLSPretrain.vocab": [[104, 107], ["modeling_2stage.ChengyuBertTwoStageCLSPretrain.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageCLSPretrain.forward": [[108, 131], ["modeling_2stage.ChengyuBertTwoStageCLSPretrain.bert", "modeling_2stage.ChengyuBertTwoStageCLSPretrain.over_linear", "modeling_2stage.ChengyuBertTwoStageCLSPretrain.vocab", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "\n", "over_states", "=", "self", ".", "over_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "\n", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "over_states", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "cond_logits", ",", "over_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.__init__": [[135, 146], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "torch.LayerNorm", "torch.LayerNorm", "modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.register_buffer", "modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "over_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "4", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.vocab": [[147, 150], ["modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.LayerNorm", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.idiom_embedding"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.forward": [[151, 176], ["modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.bert", "modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.over_linear", "modeling_2stage.ChengyuBertLayerNormTwoStagePretrain.vocab", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "\n", "over_states", "=", "self", ".", "over_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "\n", "cls_states", ",", "\n", "blank_states", "*", "cls_states", ",", "\n", "blank_states", "-", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "over_states", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "cond_logits", ",", "over_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertLayerNormTwoStageMaskPretrain.__init__": [[180, 190], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "torch.LayerNorm", "torch.LayerNorm", "modeling_2stage.ChengyuBertLayerNormTwoStageMaskPretrain.register_buffer", "modeling_2stage.ChengyuBertLayerNormTwoStageMaskPretrain.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertLayerNormTwoStageMaskPretrain.vocab": [[191, 194], ["modeling_2stage.ChengyuBertLayerNormTwoStageMaskPretrain.LayerNorm", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_2stage.ChengyuBertLayerNormTwoStageMaskPretrain.idiom_embedding"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertLayerNormTwoStageMaskPretrain.forward": [[195, 214], ["modeling_2stage.ChengyuBertLayerNormTwoStageMaskPretrain.bert", "modeling_2stage.ChengyuBertLayerNormTwoStageMaskPretrain.vocab", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "blank_states", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "cond_logits", ",", "over_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.__init__": [[218, 229], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding", "torch.LayerNorm", "torch.LayerNorm", "modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.register_buffer", "modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "over_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.vocab": [[230, 233], ["modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.LayerNorm", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.idiom_embedding"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.forward": [[234, 257], ["modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.bert", "modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.over_linear", "modeling_2stage.ChengyuBertLayerNormTwoStageCLSPretrain.vocab", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "\n", "over_states", "=", "self", ".", "over_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "\n", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "over_states", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "cond_logits", ",", "over_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageFinetune.__init__": [[262, 272], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "modeling_2stage.ChengyuBertTwoStageFinetune.register_buffer", "torch.Embedding", "torch.Embedding", "modeling_2stage.ChengyuBertTwoStageFinetune.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "over_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "4", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageFinetune.vocab": [[273, 276], ["modeling_2stage.ChengyuBertTwoStageFinetune.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageFinetune.forward": [[277, 314], ["modeling_2stage.ChengyuBertTwoStageFinetune.bert", "modeling_2stage.ChengyuBertTwoStageFinetune.over_linear", "modeling_2stage.ChengyuBertTwoStageFinetune.vocab", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "ValueError", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "modeling_2stage.ChengyuBertTwoStageFinetune.idiom_embedding", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "encoded_options", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "encoded_options", "=", "self", ".", "idiom_embedding", "(", "option_ids", ")", "\n", "\n", "", "over_states", "=", "self", ".", "over_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "\n", "cls_states", ",", "\n", "blank_states", "*", "cls_states", ",", "\n", "blank_states", "-", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "over_states", ")", "\n", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "\n", "# encoded_context = encoded_layer", "\n", "# mo_logits = torch.einsum('bld,bnd->bln', [encoded_context, encoded_options])  # (b, 256, 10)", "\n", "# logits, _ = torch.max(mo_logits, dim=1)", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "cond_logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", "\n", "", "else", ":", "\n", "            ", "return", "cond_logits", ",", "over_logits", ",", "cond_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageMaskFinetune.__init__": [[319, 328], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "modeling_2stage.ChengyuBertTwoStageMaskFinetune.register_buffer", "torch.Embedding", "torch.Embedding", "modeling_2stage.ChengyuBertTwoStageMaskFinetune.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageMaskFinetune.vocab": [[329, 332], ["modeling_2stage.ChengyuBertTwoStageMaskFinetune.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageMaskFinetune.forward": [[333, 365], ["modeling_2stage.ChengyuBertTwoStageMaskFinetune.bert", "modeling_2stage.ChengyuBertTwoStageMaskFinetune.vocab", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "ValueError", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "modeling_2stage.ChengyuBertTwoStageMaskFinetune.idiom_embedding", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "encoded_options", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "encoded_options", "=", "self", ".", "idiom_embedding", "(", "option_ids", ")", "\n", "\n", "", "over_logits", "=", "self", ".", "vocab", "(", "blank_states", ")", "\n", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "\n", "# encoded_context = encoded_layer", "\n", "# mo_logits = torch.einsum('bld,bnd->bln', [encoded_context, encoded_options])  # (b, 256, 10)", "\n", "# logits, _ = torch.max(mo_logits, dim=1)", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "cond_logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", "\n", "", "else", ":", "\n", "            ", "return", "cond_logits", ",", "over_logits", ",", "cond_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageCLSFinetune.__init__": [[370, 380], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "modeling_2stage.ChengyuBertTwoStageCLSFinetune.register_buffer", "torch.Embedding", "torch.Embedding", "modeling_2stage.ChengyuBertTwoStageCLSFinetune.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "over_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageCLSFinetune.vocab": [[381, 384], ["modeling_2stage.ChengyuBertTwoStageCLSFinetune.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "blank_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "return", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "blank_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageCLSFinetune.forward": [[385, 420], ["modeling_2stage.ChengyuBertTwoStageCLSFinetune.bert", "modeling_2stage.ChengyuBertTwoStageCLSFinetune.over_linear", "modeling_2stage.ChengyuBertTwoStageCLSFinetune.vocab", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "ValueError", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "modeling_2stage.ChengyuBertTwoStageCLSFinetune.idiom_embedding", "torch.gather.squeeze", "torch.gather.squeeze", "targets.unsqueeze", "range", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "encoded_options", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "encoded_options", "=", "self", ".", "idiom_embedding", "(", "option_ids", ")", "\n", "\n", "", "over_states", "=", "self", ".", "over_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "\n", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "over_states", ")", "\n", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "\n", "# encoded_context = encoded_layer", "\n", "# mo_logits = torch.einsum('bld,bnd->bln', [encoded_context, encoded_options])  # (b, 256, 10)", "\n", "# logits, _ = torch.max(mo_logits, dim=1)", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "cond_logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", "\n", "", "else", ":", "\n", "            ", "return", "cond_logits", ",", "over_logits", ",", "cond_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageWindow.__init__": [[455, 466], ["transformers.BertPreTrainedModel.__init__", "int", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "modeling_2stage.ChengyuBertTwoStageWindow.register_buffer", "torch.Embedding", "torch.Embedding", "modeling_2stage.ChengyuBertTwoStageWindow.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "modeling_2stage.ChengyuBertTwoStageWindow.model_name.split"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "window_size", "=", "int", "(", "self", ".", "model_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "over_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "4", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageWindow.vocab": [[467, 471], ["modeling_2stage.ChengyuBertTwoStageWindow.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "over_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "c_mo_logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "over_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "c_mo_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageWindow.forward": [[472, 528], ["input_ids.size", "modeling_2stage.ChengyuBertTwoStageWindow.bert", "modeling_2stage.ChengyuBertTwoStageWindow.over_linear", "modeling_2stage.ChengyuBertTwoStageWindow.vocab", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "ValueError", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.max", "torch.max", "torch.max", "torch.max", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "modeling_2stage.ChengyuBertTwoStageWindow.idiom_embedding", "enumerate", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "enumerate", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.gather.squeeze", "torch.gather.squeeze", "new_logits.append", "targets.unsqueeze", "new_logits.append", "range", "new_logits.append", "len", "torch.max", "torch.max", "torch.max", "torch.max", "new_logits.append", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "batch_size", ",", "length", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "encoded_options", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "encoded_options", "=", "self", ".", "idiom_embedding", "(", "option_ids", ")", "\n", "\n", "", "over_states", "=", "self", ".", "over_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "\n", "cls_states", ",", "\n", "blank_states", "*", "cls_states", ",", "\n", "blank_states", "-", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "over_logits", "=", "self", ".", "vocab", "(", "over_states", ")", "\n", "\n", "encoded_context", "=", "encoded_layer", "\n", "mo_logits", "=", "torch", ".", "einsum", "(", "'bld,bnd->bln'", ",", "[", "encoded_context", ",", "encoded_options", "]", ")", "# (b, 256, 10)", "\n", "\n", "if", "self", ".", "window_size", ">", "length", ":", "\n", "            ", "logits", ",", "_", "=", "torch", ".", "max", "(", "mo_logits", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "window_size", "==", "0", ":", "\n", "            ", "new_logits", "=", "[", "]", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "positions", ")", ":", "\n", "                ", "new_logits", ".", "append", "(", "mo_logits", "[", "i", ",", "p", "]", ")", "\n", "", "logits", "=", "torch", ".", "stack", "(", "new_logits", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "window_size", "=", "self", ".", "window_size", "\n", "new_logits", "=", "[", "]", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "positions", ")", ":", "\n", "                ", "if", "p", ">=", "window_size", "and", "p", "+", "window_size", ">=", "length", ":", "\n", "                    ", "new_logits", ".", "append", "(", "torch", ".", "max", "(", "mo_logits", "[", "i", ",", "p", "-", "window_size", ":", "]", ",", "dim", "=", "0", ")", "[", "0", "]", ")", "\n", "", "elif", "p", ">=", "window_size", "and", "p", "+", "window_size", "<", "length", ":", "\n", "                    ", "new_logits", ".", "append", "(", "torch", ".", "max", "(", "mo_logits", "[", "i", ",", "(", "p", "-", "window_size", ")", ":", "(", "p", "+", "window_size", ")", "+", "1", "]", ",", "dim", "=", "0", ")", "[", "0", "]", ")", "\n", "", "elif", "p", "<", "window_size", ":", "\n", "                    ", "new_logits", ".", "append", "(", "torch", ".", "max", "(", "mo_logits", "[", "i", ",", ":", "(", "p", "+", "window_size", ")", "+", "1", "]", ",", "dim", "=", "0", ")", "[", "0", "]", ")", "\n", "", "", "logits", "=", "torch", ".", "stack", "(", "new_logits", ",", "dim", "=", "0", ")", "\n", "\n", "", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "logits", ",", "over_logits", ",", "cond_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageMaskWindow.__init__": [[563, 573], ["transformers.BertPreTrainedModel.__init__", "int", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "modeling_2stage.ChengyuBertTwoStageMaskWindow.register_buffer", "torch.Embedding", "torch.Embedding", "modeling_2stage.ChengyuBertTwoStageMaskWindow.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "modeling_2stage.ChengyuBertTwoStageMaskWindow.model_name.split"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "self", ".", "window_size", "=", "int", "(", "self", ".", "model_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageMaskWindow.vocab": [[574, 578], ["modeling_2stage.ChengyuBertTwoStageMaskWindow.idiom_embedding", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["None"], ["", "def", "vocab", "(", "self", ",", "over_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", "\n", "c_mo_logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "over_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "c_mo_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_2stage.ChengyuBertTwoStageMaskWindow.forward": [[579, 629], ["input_ids.size", "modeling_2stage.ChengyuBertTwoStageMaskWindow.bert", "modeling_2stage.ChengyuBertTwoStageMaskWindow.vocab", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "ValueError", "torch.max", "torch.max", "torch.max", "torch.max", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.CrossEntropyLoss.", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "modeling_2stage.ChengyuBertTwoStageMaskWindow.idiom_embedding", "enumerate", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "enumerate", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.gather.squeeze", "torch.gather.squeeze", "new_logits.append", "targets.unsqueeze", "new_logits.append", "range", "new_logits.append", "len", "torch.max", "torch.max", "torch.max", "torch.max", "new_logits.append", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "        ", "batch_size", ",", "length", "=", "input_ids", ".", "size", "(", ")", "\n", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "blank_states", "=", "encoded_layer", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "\n", "if", "option_ids", "is", "None", "and", "options_embeds", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Either option_ids or options_embeds should be given.'", ")", "\n", "", "elif", "options_embeds", "is", "not", "None", ":", "\n", "            ", "encoded_options", "=", "options_embeds", "\n", "", "else", ":", "\n", "            ", "encoded_options", "=", "self", ".", "idiom_embedding", "(", "option_ids", ")", "\n", "\n", "", "over_logits", "=", "self", ".", "vocab", "(", "blank_states", ")", "\n", "\n", "encoded_context", "=", "encoded_layer", "\n", "mo_logits", "=", "torch", ".", "einsum", "(", "'bld,bnd->bln'", ",", "[", "encoded_context", ",", "encoded_options", "]", ")", "# (b, 256, 10)", "\n", "\n", "if", "self", ".", "window_size", ">", "length", ":", "\n", "            ", "logits", ",", "_", "=", "torch", ".", "max", "(", "mo_logits", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "window_size", "==", "0", ":", "\n", "            ", "new_logits", "=", "[", "]", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "positions", ")", ":", "\n", "                ", "new_logits", ".", "append", "(", "mo_logits", "[", "i", ",", "p", "]", ")", "\n", "", "logits", "=", "torch", ".", "stack", "(", "new_logits", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "window_size", "=", "self", ".", "window_size", "\n", "new_logits", "=", "[", "]", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "positions", ")", ":", "\n", "                ", "if", "p", ">=", "window_size", "and", "p", "+", "window_size", ">=", "length", ":", "\n", "                    ", "new_logits", ".", "append", "(", "torch", ".", "max", "(", "mo_logits", "[", "i", ",", "p", "-", "window_size", ":", "]", ",", "dim", "=", "0", ")", "[", "0", "]", ")", "\n", "", "elif", "p", ">=", "window_size", "and", "p", "+", "window_size", "<", "length", ":", "\n", "                    ", "new_logits", ".", "append", "(", "torch", ".", "max", "(", "mo_logits", "[", "i", ",", "(", "p", "-", "window_size", ")", ":", "(", "p", "+", "window_size", ")", "+", "1", "]", ",", "dim", "=", "0", ")", "[", "0", "]", ")", "\n", "", "elif", "p", "<", "window_size", ":", "\n", "                    ", "new_logits", ".", "append", "(", "torch", ".", "max", "(", "mo_logits", "[", "i", ",", ":", "(", "p", "+", "window_size", ")", "+", "1", "]", ",", "dim", "=", "0", ")", "[", "0", "]", ")", "\n", "", "", "logits", "=", "torch", ".", "stack", "(", "new_logits", ",", "dim", "=", "0", ")", "\n", "\n", "", "if", "compute_loss", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ",", "targets", ")", "\n", "target", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", "\n", "over_loss", "=", "loss_fct", "(", "over_logits", ",", "target", ".", "squeeze", "(", "1", ")", ")", "\n", "return", "loss", ",", "over_loss", "\n", "", "else", ":", "\n", "            ", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "logits", ",", "over_logits", ",", "cond_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.__init__.build_model": [[7, 24], ["opts.model.startswith", "ModelCls.from_pretrained", "opts.model.startswith", "opts.model.startswith", "opts.model.startswith", "opts.model.startswith"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.__init__.register_model": [[26, 53], ["ValueError"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.__init__": [[13, 40], ["transformers.BertPreTrainedModel.__init__", "opts.model.startswith", "int", "int", "transformers.BertModel", "torch.Dropout", "torch.Dropout", "torch.Embedding", "torch.Embedding", "torch.LayerNorm", "torch.LayerNorm", "modeling_contrastive.ChengyuBertContrastive.register_buffer", "modeling_contrastive.ChengyuBertContrastive.model_name.startswith", "torch.Sequential", "torch.Sequential", "modeling_contrastive.ChengyuBertContrastive.init_weights", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "modeling_contrastive.ChengyuBertContrastive.model_name.split", "modeling_contrastive.ChengyuBertContrastive.model_name.split"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "assert", "opts", ".", "model", ".", "startswith", "(", "(", "'chengyubert-contrastive-mask'", ",", "'chengyubert-contrastive-cls'", ")", ")", "\n", "self", ".", "model_name", "=", "opts", ".", "model", "\n", "chengyu_emb_dim", "=", "int", "(", "self", ".", "model_name", ".", "split", "(", "'-'", ")", "[", "-", "2", "]", ")", "\n", "contrastive_dim", "=", "int", "(", "self", ".", "model_name", ".", "split", "(", "'-'", ")", "[", "-", "1", "]", ")", "\n", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "self", ".", "idiom_embedding", "=", "nn", ".", "Embedding", "(", "opts", ".", "len_idiom_vocab", ",", "chengyu_emb_dim", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "chengyu_emb_dim", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "register_buffer", "(", "'enlarged_candidates'", ",", "torch", ".", "arange", "(", "opts", ".", "len_idiom_vocab", ")", ")", "\n", "\n", "# projection MLP", "\n", "if", "self", ".", "model_name", ".", "startswith", "(", "'chengyubert-mask-contrastive'", ")", ":", "\n", "            ", "self", ".", "project_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "chengyu_emb_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "project_linear", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "4", ",", "chengyu_emb_dim", ")", "\n", "\n", "# projection MLP", "\n", "", "self", ".", "projection", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "chengyu_emb_dim", ",", "chengyu_emb_dim", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "LayerNorm", "(", "chengyu_emb_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Linear", "(", "chengyu_emb_dim", ",", "contrastive_dim", ",", "bias", "=", "True", ")", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.project": [[41, 49], ["modeling_contrastive.ChengyuBertContrastive.model_name.startswith", "modeling_contrastive.ChengyuBertContrastive.project_linear", "modeling_contrastive.ChengyuBertContrastive.project_linear", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "project", "(", "self", ",", "cls_states", ",", "blank_states", ")", ":", "\n", "        ", "if", "self", ".", "model_name", ".", "startswith", "(", "'chengyubert-contrastive-mask'", ")", ":", "\n", "            ", "return", "self", ".", "project_linear", "(", "blank_states", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "project_linear", "(", "torch", ".", "cat", "(", "[", "blank_states", ",", "\n", "cls_states", ",", "\n", "blank_states", "*", "cls_states", ",", "\n", "blank_states", "-", "cls_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab": [[50, 54], ["modeling_contrastive.ChengyuBertContrastive.LayerNorm", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "modeling_contrastive.ChengyuBertContrastive.idiom_embedding"], "methods", ["None"], ["", "", "def", "vocab", "(", "self", ",", "over_states", ")", ":", "\n", "        ", "idiom_embeddings", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "self", ".", "enlarged_candidates", ")", ")", "\n", "c_mo_logits", "=", "torch", ".", "einsum", "(", "'bd,nd->bn'", ",", "[", "over_states", ",", "idiom_embeddings", "]", ")", "# (b, 256, 10)", "\n", "return", "c_mo_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.forward": [[55, 79], ["modeling_contrastive.ChengyuBertContrastive.bert", "modeling_contrastive.ChengyuBertContrastive.project", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "modeling_contrastive.ChengyuBertContrastive.LayerNorm", "chengyubert.optim.loss.ContrastiveLoss", "chengyubert.optim.loss.ContrastiveLoss.", "modeling_contrastive.ChengyuBertContrastive.vocab", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "modeling_contrastive.ChengyuBertContrastive.idiom_embedding", "modeling_contrastive.ChengyuBertContrastive.projection", "modeling_contrastive.ChengyuBertContrastive.projection", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "range", "targets.unsqueeze", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.project", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.models.modeling_contrastive.ChengyuBertContrastive.vocab"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "option_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "options_embeds", "=", "None", ",", "compute_loss", "=", "False", ",", "targets", "=", "None", ")", ":", "\n", "# batch_size, sequence_num, length = input_ids.shape", "\n", "        ", "encoded_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "encoded_layer", "=", "encoded_outputs", "[", "0", "]", "\n", "\n", "encoded_context", "=", "encoded_layer", "\n", "blank_states", "=", "encoded_context", "[", "[", "i", "for", "i", "in", "range", "(", "len", "(", "positions", ")", ")", "]", ",", "positions", "]", "# [batch, hidden_state]", "\n", "cls_states", "=", "encoded_layer", "[", ":", ",", "0", "]", "\n", "\n", "emb_u", "=", "self", ".", "project", "(", "cls_states", ",", "blank_states", ")", "\n", "\n", "if", "compute_loss", ":", "\n", "            ", "target_ids", "=", "torch", ".", "gather", "(", "option_ids", ",", "dim", "=", "1", ",", "index", "=", "targets", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "emb_v", "=", "self", ".", "LayerNorm", "(", "self", ".", "idiom_embedding", "(", "target_ids", ")", ")", "# (b, 768)", "\n", "contrastive_loss_fct", "=", "ContrastiveLoss", "(", "tau", "=", "1", ")", "\n", "return", "contrastive_loss_fct", "(", "self", ".", "projection", "(", "emb_u", ")", ",", "self", ".", "projection", "(", "emb_v", ")", ")", "\n", "", "else", ":", "\n", "            ", "over_logits", "=", "self", ".", "vocab", "(", "emb_u", ")", "\n", "cond_logits", "=", "torch", ".", "gather", "(", "over_logits", ",", "dim", "=", "1", ",", "index", "=", "option_ids", ")", "\n", "return", "cond_logits", ",", "over_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.TokenBucketSampler.__init__": [[15, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "lens", ",", "bucket_size", ",", "batch_size", ",", "\n", "droplast", "=", "False", ",", "size_multiple", "=", "8", ")", ":", "\n", "        ", "self", ".", "_lens", "=", "lens", "\n", "self", ".", "_max_tok", "=", "batch_size", "\n", "self", ".", "_bucket_size", "=", "bucket_size", "\n", "self", ".", "_droplast", "=", "droplast", "\n", "self", ".", "_size_mul", "=", "size_multiple", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.TokenBucketSampler._create_ids": [[23, 25], ["list", "range", "len"], "methods", ["None"], ["", "def", "_create_ids", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "range", "(", "len", "(", "self", ".", "_lens", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.TokenBucketSampler._sort_fn": [[26, 28], ["None"], "methods", ["None"], ["", "def", "_sort_fn", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "_lens", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.TokenBucketSampler.__iter__": [[29, 56], ["sampler.TokenBucketSampler._create_ids", "random.shuffle", "random.shuffle", "iter", "sorted", "cytoolz.partition_all", "range", "max", "batches.append", "len", "max", "batches.append", "list", "list.extend", "ValueError", "len", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.DistributedTokenBucketSampler._create_ids"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "ids", "=", "self", ".", "_create_ids", "(", ")", "\n", "random", ".", "shuffle", "(", "ids", ")", "\n", "buckets", "=", "[", "sorted", "(", "ids", "[", "i", ":", "i", "+", "self", ".", "_bucket_size", "]", ",", "\n", "key", "=", "self", ".", "_sort_fn", ",", "reverse", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "ids", ")", ",", "self", ".", "_bucket_size", ")", "]", "\n", "batches", "=", "[", "]", "\n", "for", "bucket", "in", "buckets", ":", "\n", "            ", "max_len", "=", "0", "\n", "batch_indices", "=", "[", "]", "\n", "\n", "# Partition the bucket into tuples of length at most self._size_mul", "\n", "for", "indices", "in", "partition_all", "(", "self", ".", "_size_mul", ",", "bucket", ")", ":", "\n", "                ", "max_len", "=", "max", "(", "max_len", ",", "max", "(", "self", ".", "_lens", "[", "i", "]", "for", "i", "in", "indices", ")", ")", "\n", "# fill batches until max_token (include padding)", "\n", "if", "max_len", "*", "(", "len", "(", "batch_indices", ")", "+", "self", ".", "_size_mul", ")", ">", "self", ".", "_max_tok", ":", "\n", "                    ", "if", "not", "batch_indices", ":", "\n", "                        ", "raise", "ValueError", "(", "\"max_tokens too small / max_seq_len too long\"", ")", "\n", "", "assert", "len", "(", "batch_indices", ")", "%", "self", ".", "_size_mul", "==", "0", "\n", "batches", ".", "append", "(", "batch_indices", ")", "\n", "batch_indices", "=", "list", "(", "indices", ")", "\n", "", "else", ":", "\n", "                    ", "batch_indices", ".", "extend", "(", "indices", ")", "\n", "", "", "if", "not", "self", ".", "_droplast", "and", "batch_indices", ":", "\n", "                ", "batches", ".", "append", "(", "batch_indices", ")", "\n", "", "", "random", ".", "shuffle", "(", "batches", ")", "\n", "return", "iter", "(", "batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.TokenBucketSampler.__len__": [[57, 59], ["ValueError"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"NOT supported. \"", "\n", "\"This has some randomness across epochs\"", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.DistributedTokenBucketSampler.__init__": [[63, 67], ["sampler.TokenBucketSampler.__init__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_replicas", ",", "rank", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "_rank", "=", "rank", "\n", "self", ".", "_num_replicas", "=", "num_replicas", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.DistributedTokenBucketSampler._create_ids": [[68, 70], ["sampler.TokenBucketSampler._create_ids"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.DistributedTokenBucketSampler._create_ids"], ["", "def", "_create_ids", "(", "self", ")", ":", "\n", "        ", "return", "super", "(", ")", ".", "_create_ids", "(", ")", "[", "self", ".", "_rank", ":", ":", "self", ".", "_num_replicas", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastiveSampler.__init__": [[73, 84], ["collections.deque", "enumerate"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_replicas", ",", "rank", ",", "lens", ",", "ids", ",", "batch_size", ",", "reverse_index", ",", "\n", "droplast", "=", "False", ",", "size_multiple", "=", "8", ")", ":", "\n", "        ", "self", ".", "_rank", "=", "rank", "\n", "self", ".", "_num_replicas", "=", "num_replicas", "\n", "self", ".", "_lens", "=", "lens", "\n", "self", ".", "_ids", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "enumerate", "(", "ids", ")", "}", "\n", "self", ".", "_max_tok", "=", "batch_size", "\n", "self", ".", "_droplast", "=", "droplast", "\n", "self", ".", "_size_mul", "=", "size_multiple", "\n", "self", ".", "reverse_index", "=", "reverse_index", "\n", "self", ".", "contrastive_deque", "=", "deque", "(", "maxlen", "=", "500", "*", "self", ".", "_num_replicas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastiveSampler._sort_fn": [[85, 87], ["None"], "methods", ["None"], ["", "def", "_sort_fn", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "_lens", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastiveSampler._contrastive_bucket": [[88, 103], ["sampler.ContrastiveSampler.contrastive_deque.pop", "random.choices", "len", "sampler.ContrastiveSampler.contrastive_deque.append", "random.choice", "bucket.append", "random.choices", "list", "sampler.ContrastiveSampler.reverse_index.keys"], "methods", ["None"], ["", "def", "_contrastive_bucket", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "contrastive_deque", ")", "==", "0", ":", "\n", "            ", "self", ".", "contrastive_deque", ".", "append", "(", "random", ".", "choices", "(", "list", "(", "self", ".", "reverse_index", ".", "keys", "(", ")", ")", ",", "\n", "k", "=", "500", "*", "self", ".", "_num_replicas", ")", "[", "self", ".", "_rank", ":", ":", "self", ".", "_num_replicas", "]", ")", "\n", "\n", "", "idiom_ids", "=", "self", ".", "contrastive_deque", ".", "pop", "(", ")", "\n", "\n", "bucket", "=", "[", "]", "\n", "for", "idx", "in", "random", ".", "choices", "(", "idiom_ids", ",", "k", "=", "500", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "qid", "=", "random", ".", "choice", "(", "self", ".", "reverse_index", "[", "idx", "]", ")", "\n", "bucket", ".", "append", "(", "self", ".", "_ids", "[", "qid", "]", ")", "\n", "", "except", ":", "\n", "                ", "continue", "\n", "", "", "return", "bucket", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastiveSampler.__iter__": [[104, 121], ["sampler.ContrastiveSampler._contrastive_bucket", "cytoolz.partition_all", "max", "max", "batch_indices.extend", "ValueError", "len", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastivePairSampler._contrastive_bucket"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "bucket", "=", "self", ".", "_contrastive_bucket", "(", ")", "\n", "max_len", "=", "0", "\n", "batch_indices", "=", "[", "]", "\n", "for", "indices", "in", "partition_all", "(", "self", ".", "_size_mul", ",", "bucket", ")", ":", "\n", "                ", "max_len", "=", "max", "(", "max_len", ",", "max", "(", "self", ".", "_lens", "[", "i", "]", "for", "i", "in", "indices", ")", ")", "\n", "if", "(", "max_len", "*", "(", "len", "(", "batch_indices", ")", "+", "self", ".", "_size_mul", ")", "\n", ">", "self", ".", "_max_tok", ")", ":", "\n", "                    ", "if", "not", "batch_indices", ":", "\n", "                        ", "raise", "ValueError", "(", "\n", "\"max_tokens too small / max_seq_len too long\"", ")", "\n", "", "assert", "len", "(", "batch_indices", ")", "%", "self", ".", "_size_mul", "==", "0", "\n", "break", "\n", "", "else", ":", "\n", "                    ", "batch_indices", ".", "extend", "(", "indices", ")", "\n", "", "", "yield", "batch_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastiveSampler.__len__": [[122, 124], ["ValueError"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"NOT supported. \"", "\n", "\"This has some randomness across epochs\"", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastivePairSampler.__init__": [[128, 140], ["collections.Counter", "collections.deque", "enumerate"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_replicas", ",", "rank", ",", "lens", ",", "ids", ",", "batch_size", ",", "reverse_index", ",", "\n", "droplast", "=", "False", ",", "size_multiple", "=", "8", ")", ":", "\n", "        ", "self", ".", "_rank", "=", "rank", "\n", "self", ".", "_num_replicas", "=", "num_replicas", "\n", "self", ".", "_lens", "=", "lens", "\n", "self", ".", "_ids", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "enumerate", "(", "ids", ")", "}", "# from example to example index", "\n", "self", ".", "_max_tok", "=", "batch_size", "\n", "self", ".", "_droplast", "=", "droplast", "\n", "self", ".", "_size_mul", "=", "size_multiple", "\n", "self", ".", "idiom_counter", "=", "Counter", "(", ")", "\n", "self", ".", "reverse_index", "=", "reverse_index", "# from idiom id to example list", "\n", "self", ".", "contrastive_deque", "=", "deque", "(", "maxlen", "=", "500", "*", "self", ".", "_num_replicas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastivePairSampler._sort_fn": [[141, 143], ["None"], "methods", ["None"], ["", "def", "_sort_fn", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "_lens", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastivePairSampler._contrastive_bucket": [[144, 164], ["sampler.ContrastivePairSampler.contrastive_deque.pop", "random.choices", "len", "random.choices", "sampler.ContrastivePairSampler.contrastive_deque.append", "sum", "range", "random.choices", "bucket.append", "sampler.ContrastivePairSampler.idiom_counter.values", "range", "len", "len"], "methods", ["None"], ["", "def", "_contrastive_bucket", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "contrastive_deque", ")", "==", "0", ":", "\n", "            ", "total", "=", "sum", "(", "self", ".", "idiom_counter", ".", "values", "(", ")", ")", "+", "1", "\n", "weights", "=", "[", "1", "-", "self", ".", "idiom_counter", "[", "i", "]", "/", "total", "for", "i", "in", "range", "(", "len", "(", "self", ".", "reverse_index", ")", ")", "]", "\n", "c", "=", "random", ".", "choices", "(", "range", "(", "len", "(", "self", ".", "reverse_index", ")", ")", ",", "\n", "weights", "=", "weights", ",", "\n", "k", "=", "500", "*", "self", ".", "_num_replicas", ")", "\n", "q", "=", "c", "[", "self", ".", "_rank", ":", ":", "self", ".", "_num_replicas", "]", "\n", "self", ".", "contrastive_deque", ".", "append", "(", "q", ")", "# add sampled idiom ids", "\n", "\n", "", "idiom_ids", "=", "self", ".", "contrastive_deque", ".", "pop", "(", ")", "# pop idiom ids", "\n", "\n", "bucket", "=", "[", "]", "\n", "for", "idx", "in", "random", ".", "choices", "(", "idiom_ids", ",", "k", "=", "500", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "qid_pair", "=", "random", ".", "choices", "(", "self", ".", "reverse_index", "[", "idx", "]", ",", "k", "=", "2", ")", "# sample two examples given one idiom id", "\n", "bucket", ".", "append", "(", "[", "(", "idx", ",", "self", ".", "_ids", "[", "qid", "]", ")", "for", "qid", "in", "qid_pair", "]", ")", "# convert example to example index", "\n", "", "except", ":", "\n", "                ", "continue", "\n", "", "", "return", "bucket", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastivePairSampler.__iter__": [[165, 186], ["sampler.ContrastivePairSampler._contrastive_bucket", "cytoolz.partition_all", "max", "max", "batch_indices.extend", "sampler.ContrastivePairSampler.idiom_counter.update", "len", "ValueError", "len", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastivePairSampler._contrastive_bucket"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "bucket", "=", "self", ".", "_contrastive_bucket", "(", ")", "\n", "max_len", "=", "0", "\n", "batch_indices", "=", "[", "]", "\n", "for", "indices", "in", "partition_all", "(", "self", ".", "_size_mul", ",", "bucket", ")", ":", "\n", "                ", "idiom_ids", "=", "[", "j", "for", "i", "in", "indices", "for", "j", ",", "_", "in", "i", "]", "\n", "indices", "=", "[", "j", "for", "i", "in", "indices", "for", "_", ",", "j", "in", "i", "]", "\n", "max_len", "=", "max", "(", "max_len", ",", "max", "(", "self", ".", "_lens", "[", "i", "]", "for", "i", "in", "indices", ")", ")", "\n", "if", "(", "max_len", "*", "(", "len", "(", "batch_indices", ")", "+", "self", ".", "_size_mul", ")", "*", "2", "\n", ">", "self", ".", "_max_tok", ")", ":", "\n", "                    ", "if", "not", "batch_indices", ":", "\n", "                        ", "raise", "ValueError", "(", "\n", "\"max_tokens too small / max_seq_len too long\"", ")", "\n", "", "assert", "len", "(", "batch_indices", ")", "%", "self", ".", "_size_mul", "==", "0", "\n", "break", "\n", "", "else", ":", "\n", "                    ", "batch_indices", ".", "extend", "(", "indices", ")", "\n", "self", ".", "idiom_counter", ".", "update", "(", "idiom_ids", ")", "\n", "", "", "assert", "len", "(", "batch_indices", ")", "%", "2", "==", "0", "\n", "yield", "batch_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.sampler.ContrastivePairSampler.__len__": [[187, 189], ["ValueError"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"NOT supported. \"", "\n", "\"This has some randomness across epochs\"", ")", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.read_vectors_from_bz2": [[14, 21], ["bz2.BZ2File", "enumerate", "print", "line.decode().rstrip", "line.decode"], "function", ["None"], ["def", "read_vectors_from_bz2", "(", "path", ")", ":", "\n", "    ", "with", "bz2", ".", "BZ2File", "(", "path", ")", "as", "f", ":", "\n", "        ", "for", "line_count", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "yield", "line_count", ",", "line", ".", "decode", "(", ")", ".", "rstrip", "(", ")", "\n", "", "except", ":", "\n", "                ", "print", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.read_vectors_from_txt": [[23, 30], ["tqdm.tqdm", "open", "enumerate", "os.path.getsize", "pbar.update", "len", "line.decode().rstrip", "line.decode"], "function", ["None"], ["", "", "", "", "def", "read_vectors_from_txt", "(", "path", ")", ":", "\n", "    ", "with", "tqdm", "(", "total", "=", "os", ".", "path", ".", "getsize", "(", "path", ")", ",", "\n", "bar_format", "=", "\"{desc}: {percentage:.3f}%|{bar}| {n:.2f}/{total_fmt} [{elapsed}<{remaining}]\"", ")", "as", "pbar", ":", "\n", "        ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "for", "line_count", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "pbar", ".", "update", "(", "len", "(", "line", ")", ")", "\n", "yield", "line_count", ",", "line", ".", "decode", "(", ")", ".", "rstrip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.read_vectors": [[32, 69], ["path.endswith", "embeddings.read_vectors_from_bz2", "embeddings.read_vectors_from_txt", "tqdm.tqdm", "enumerate", "list", "pbar.reset", "line.rstrip().split", "pbar.update", "map", "len", "line.rstrip().split", "line.rstrip", "numpy.asarray", "iw.append", "len", "print", "print", "numpy.asarray", "iw.append", "line.rstrip", "float", "float", "len", "len"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.read_vectors_from_bz2", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.read_vectors_from_txt"], ["", "", "", "", "def", "read_vectors", "(", "path", ",", "top_k", "=", "0", ")", ":", "# read top n word vectors, i.e. top is 10000", "\n", "    ", "lines_num", ",", "dim", "=", "0", ",", "0", "\n", "vectors", "=", "{", "}", "\n", "iw", "=", "[", "]", "\n", "\n", "if", "path", ".", "endswith", "(", "'.bz2'", ")", ":", "\n", "        ", "itr", "=", "read_vectors_from_bz2", "(", "path", ")", "\n", "", "else", ":", "\n", "        ", "itr", "=", "read_vectors_from_txt", "(", "path", ")", "\n", "\n", "", "with", "tqdm", "(", "total", "=", "100", ")", "as", "pbar", ":", "\n", "        ", "for", "i", ",", "line", "in", "itr", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "lines_num", ",", "dim", "=", "list", "(", "map", "(", "int", ",", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", ")", ")", "\n", "pbar", ".", "reset", "(", "total", "=", "lines_num", ")", "\n", "", "else", ":", "\n", "                ", "tokens", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "tokens", ")", "==", "dim", "+", "1", ":", "\n", "                    ", "word", "=", "tokens", "[", "0", "]", "\n", "if", "word", "not", "in", "vectors", ":", "\n", "                        ", "vectors", "[", "word", "]", "=", "np", ".", "asarray", "(", "[", "float", "(", "x", ")", "for", "x", "in", "tokens", "[", "1", ":", "]", "]", ")", "\n", "iw", ".", "append", "(", "word", ")", "\n", "", "", "elif", "len", "(", "tokens", ")", ">", "dim", "+", "1", ":", "\n", "                    ", "word", "=", "' '", ".", "join", "(", "tokens", "[", ":", "len", "(", "tokens", ")", "-", "dim", "]", ")", "\n", "if", "word", "not", "in", "vectors", ":", "\n", "                        ", "vectors", "[", "word", "]", "=", "np", ".", "asarray", "(", "[", "float", "(", "x", ")", "for", "x", "in", "tokens", "[", "len", "(", "tokens", ")", "-", "dim", ":", "]", "]", ")", "\n", "iw", ".", "append", "(", "word", ")", "\n", "", "print", "(", "word", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "f\"Skip a line of all spaces! {tokens[:2]}\"", ")", "\n", "\n", "", "if", "top_k", "!=", "0", "and", "i", ">", "top_k", ":", "\n", "                    ", "break", "\n", "", "pbar", ".", "update", "(", "1", ")", "\n", "\n", "", "", "", "wi", "=", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "iw", ")", "}", "\n", "return", "vectors", ",", "iw", ",", "wi", ",", "dim", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.get_bert_embedding": [[71, 85], ["tokenizer", "torch.no_grad", "model", "[].mean().squeeze().cpu().numpy", "input_ids.cuda", "attention_mask.cuda", "token_type_ids.cuda", "[].mean().squeeze().cpu", "[].mean().squeeze", "[].mean"], "function", ["None"], ["", "def", "get_bert_embedding", "(", "model", ",", "tokenizer", ",", "text", ")", ":", "\n", "    ", "input_dict", "=", "tokenizer", "(", "text", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "input_ids", "=", "input_dict", "[", "'input_ids'", "]", "\n", "token_type_ids", "=", "input_dict", "[", "'token_type_ids'", "]", "\n", "attention_mask", "=", "input_dict", "[", "'attention_mask'", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "encoded_layers", "=", "model", "(", "\n", "input_ids", ".", "cuda", "(", ")", ",", "\n", "attention_mask", "=", "attention_mask", ".", "cuda", "(", ")", ",", "\n", "token_type_ids", "=", "token_type_ids", ".", "cuda", "(", ")", "\n", ")", "\n", "#         return encoded_layers[0][:, 0].squeeze().cpu().numpy()", "\n", "return", "encoded_layers", "[", "0", "]", "[", ":", ",", "1", ":", "-", "1", "]", ".", "mean", "(", "1", ")", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.load_embeddings": [[87, 103], ["embeddings.read_vectors", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModel.from_pretrained().cuda", "embeddings.get_bert_embedding", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModel.from_pretrained().cuda", "ValueError", "transformers.AutoModel.from_pretrained", "chengyu_vocab.items", "embeddings.get_bert_embedding", "transformers.AutoModel.from_pretrained", "chengyu_vocab.items"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.read_vectors", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.get_bert_embedding", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.embeddings.get_bert_embedding"], ["", "", "def", "load_embeddings", "(", "chengyu_vocab", ",", "emb_path", ",", "embedding_type", ",", "wi", ")", ":", "\n", "    ", "if", "embedding_type", "in", "[", "'word'", ",", "'char'", ",", "'bigram'", ",", "'bigram-char'", "]", ":", "\n", "        ", "embeddings", ",", "_", ",", "_", ",", "dim", "=", "read_vectors", "(", "f'{emb_path}/Literature/sgns.literature.{embedding_type}.bz2'", ")", "\n", "", "else", ":", "\n", "        ", "if", "embedding_type", "==", "'ERNIE'", ":", "\n", "            ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\"nghuyong/ernie-1.0\"", ",", "use_fast", "=", "True", ")", "\n", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"nghuyong/ernie-1.0\"", ")", ".", "cuda", "(", ")", "\n", "embeddings", "=", "{", "k", ":", "get_bert_embedding", "(", "model", ",", "tokenizer", ",", "k", ")", "for", "k", ",", "v", "in", "chengyu_vocab", ".", "items", "(", ")", "if", "k", "in", "wi", "}", "\n", "", "elif", "embedding_type", "==", "'BERT'", ":", "\n", "            ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "'hfl/chinese-bert-wwm-ext'", ",", "use_fast", "=", "True", ")", "\n", "model", "=", "AutoModel", ".", "from_pretrained", "(", "'hfl/chinese-bert-wwm-ext'", ")", ".", "cuda", "(", ")", "\n", "embeddings", "=", "{", "k", ":", "get_bert_embedding", "(", "model", ",", "tokenizer", ",", "k", ")", "for", "k", ",", "v", "in", "chengyu_vocab", ".", "items", "(", ")", "if", "k", "in", "wi", "}", "\n", "", "else", ":", "\n", "            ", "embeddings", "=", "None", "\n", "assert", "ValueError", "(", "\"Unsupported embedding type!\"", ")", "\n", "", "", "return", "embeddings", "\n", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.TxtLmdb.__init__": [[32, 47], ["lmdb.open", "__init__.TxtLmdb.env.begin", "lmdb.open", "__init__.TxtLmdb.env.begin", "__init__._check_distributed"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__._check_distributed"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.TxtLmdb.__del__": [[48, 52], ["__init__.TxtLmdb.env.close", "__init__.TxtLmdb.txn.commit"], "methods", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.TxtLmdb.__getitem__": [[53, 56], ["msgpack.loads", "lz4.frame.decompress", "__init__.TxtLmdb.txn.get", "key.encode"], "methods", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.TxtLmdb.__setitem__": [[57, 69], ["__init__.TxtLmdb.txn.put", "ValueError", "key.encode", "lz4.frame.compress", "__init__.TxtLmdb.txn.commit", "__init__.TxtLmdb.env.begin", "msgpack.dumps"], "methods", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.ChengyuLmdb.__init__": [[91, 110], ["os.path.join", "chengyubert.utils.logger.LOGGER.info", "json.load", "__init__.TxtLmdb", "__init__.chengyu_process", "list", "transformers.AutoTokenizer.from_pretrained", "__init__.intermediate_dir", "getattr", "open", "range", "__init__.ChengyuLmdb.chengyu_vocab.items"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.chengyu_process", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.intermediate_dir"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.ChengyuLmdb.__getitem__": [[111, 114], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.IdiomsLmdb.__init__": [[290, 310], ["os.path.join", "chengyubert.utils.logger.LOGGER.info", "json.load", "__init__.TxtLmdb", "__init__.idioms_process", "list", "transformers.AutoTokenizer.from_pretrained", "__init__.intermediate_dir", "getattr", "open", "range", "__init__.IdiomsLmdb.vocab.items"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.idioms_process", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.intermediate_dir"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.IdiomsLmdb.__getitem__": [[311, 314], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.open_lmdb": [[22, 29], ["__init__.TxtLmdb"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__._check_distributed": [[71, 79], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.intermediate_dir": [[81, 88], ["model_name.startswith", "model_name.replace"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.chengyu_process": [[116, 145], ["pandas.read_csv", "pda.read_csv.fillna", "pda.read_csv.itertuples", "print", "enumerate", "len", "chengyu_vocab.items", "len", "eval", "open().readline", "open"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.calo_process": [[181, 201], ["pandas.read_excel", "df_sentiment[].str.strip", "pda.read_excel.itertuples", "getattr", "getattr", "calo_vocab.setdefault", "calo_vocab[].append", "df_sentiment[].isin", "chengyu_vocab.keys", "getattr", "getattr"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.get_form": [[211, 227], ["idiom.lower", "tmp.replace.replace"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.idioms_process": [[229, 287], ["pandas.read_csv", "df_sentiment.assign.assign", "pandas.read_csv", "pda.read_csv.idiom.tolist", "more_itertools.chunked", "print", "print", "df_sentiment.assign.itertuples", "open().read().split", "idioms_extra.append", "len", "open", "json.load", "getattr", "df_sentiment[].map", "df_sentiment.assign.Idiom.tolist", "idioms_vocab.get", "len", "idioms_vocab.items", "len", "open().read", "__init__.get_form", "idioms_forms[].append", "len", "idioms_forms.setdefault", "idioms_forms[].append", "open"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.get_form"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_dataloader": [[316, 325], ["dset_cls", "chengyubert.data.sampler.DistributedTokenBucketSampler", "torch.utils.data.DataLoader", "chengyubert.data.loader.PrefetchLoader"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_contrastive_dataloader": [[327, 335], ["dset_cls", "chengyubert.data.sampler.ContrastiveSampler", "torch.utils.data.DataLoader", "chengyubert.data.loader.PrefetchLoader"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_contrastive_pair_dataloader": [[337, 346], ["dset_cls", "chengyubert.data.sampler.ContrastivePairSampler", "torch.utils.data.DataLoader", "chengyubert.data.loader.PrefetchLoader"], "function", ["None"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_dataloaders": [[348, 366], ["print", "dir", "chengyubert.utils.logger.LOGGER.info", "__init__.create_dataloader", "k.endswith", "getattr", "splits.append", "k.replace"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.create_dataloader"], []], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge": [[8, 36], ["isinstance", "open().readlines", "open().readlines", "pred_file.readlines", "len", "len", "line.strip().split.strip().split", "int", "line.strip().split.strip().split", "int", "open", "open", "line.strip().split.strip", "line.strip().split.strip"], "function", ["None"], ["def", "judge", "(", "pred_file", ",", "answer_file", ")", ":", "\n", "    ", "if", "isinstance", "(", "pred_file", ",", "str", ")", ":", "\n", "        ", "pred", "=", "open", "(", "pred_file", ")", ".", "readlines", "(", ")", "\n", "", "else", ":", "\n", "        ", "pred", "=", "pred_file", ".", "readlines", "(", ")", "\n", "\n", "", "ans", "=", "open", "(", "answer_file", ")", ".", "readlines", "(", ")", "\n", "assert", "len", "(", "pred", ")", "==", "len", "(", "ans", ")", "\n", "\n", "ans_dict", "=", "{", "}", "\n", "for", "line", "in", "ans", ":", "\n", "        ", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "\n", "ans_dict", "[", "line", "[", "0", "]", "]", "=", "int", "(", "line", "[", "1", "]", ")", "\n", "\n", "", "pred_dict", "=", "{", "}", "\n", "for", "line", "in", "pred", ":", "\n", "        ", "line", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "\n", "pred_dict", "[", "line", "[", "0", "]", "]", "=", "int", "(", "line", "[", "1", "]", ")", "\n", "\n", "", "cnt", "=", "0", "\n", "acc", "=", "0", "\n", "for", "key", "in", "ans_dict", ":", "\n", "        ", "assert", "key", "in", "pred_dict", "\n", "cnt", "+=", "1", "\n", "if", "ans_dict", "[", "key", "]", "==", "pred_dict", "[", "key", "]", ":", "\n", "            ", "acc", "+=", "1", "\n", "\n", "", "", "return", "acc", "/", "cnt", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.judge_by_idiom": [[38, 66], ["isinstance", "open().readlines", "pred_file.readlines", "line.strip().split", "int", "mrr_dict.setdefault", "mrr_dict[].append", "float", "float", "open", "mrr_dict.items", "open", "line.strip", "f.write", "len", "sum", "len", "print"], "function", ["None"], ["", "def", "judge_by_idiom", "(", "pred_file", ",", "id2idiom", ",", "out_file", "=", "None", ")", ":", "\n", "    ", "if", "isinstance", "(", "pred_file", ",", "str", ")", ":", "\n", "        ", "pred", "=", "open", "(", "pred_file", ")", ".", "readlines", "(", ")", "\n", "", "else", ":", "\n", "        ", "pred", "=", "pred_file", ".", "readlines", "(", ")", "\n", "\n", "", "cnt", "=", "0", "\n", "mrr_total", "=", "0", "\n", "mrr_dict", "=", "{", "}", "\n", "for", "line", "in", "pred", ":", "\n", "        ", "qid", ",", "ans", ",", "mrr", ",", "target", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "\n", "target", "=", "int", "(", "target", ")", "\n", "mrr_dict", ".", "setdefault", "(", "target", ",", "[", "]", ")", "\n", "mrr_dict", "[", "target", "]", ".", "append", "(", "float", "(", "mrr", ")", ")", "\n", "cnt", "+=", "1", "\n", "mrr_total", "+=", "float", "(", "mrr", ")", "\n", "\n", "", "if", "out_file", "is", "not", "None", ":", "\n", "        ", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "for", "idiom_id", ",", "mrrs", "in", "mrr_dict", ".", "items", "(", ")", ":", "\n", "                ", "if", "len", "(", "mrrs", ")", ">", "0", ":", "\n", "                    ", "mean_mrr", "=", "sum", "(", "mrrs", ")", "/", "len", "(", "mrrs", ")", "\n", "if", "idiom_id", "%", "1000", "==", "0", ":", "\n", "                        ", "print", "(", "id2idiom", "[", "idiom_id", "]", ",", "mean_mrr", ")", "\n", "", "", "else", ":", "\n", "                    ", "mean_mrr", "=", "0", "\n", "", "f", ".", "write", "(", "f'{id2idiom[idiom_id]},{mean_mrr:.3f}\\n'", ")", "\n", "", "", "", "return", "mrr_total", "/", "cnt", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.evaluation.evaluate_embeddings": [[68, 202], ["list", "len", "tqdm.tqdm", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "chengyu_synonyms_dict.items", "itertools.chain", "neighbourhood_cosine.append", "neighbourhood_norm.append", "set_recall_at_k_cosine.setdefault", "set_recall_at_k_norm.setdefault", "set_recall_at_delta_norm.setdefault", "set_recall_at_cosine_norm.setdefault", "recall_at_k_cosine.setdefault", "antonyms_at_k_cosine.setdefault", "sum", "recall_at_k_norm.setdefault", "antonyms_at_k_norm.setdefault", "sum", "sum", "len", "sum", "len", "numpy.linalg.norm", "neighbourhood_delta_norm.setdefault", "neighbourhood_delta_norm[].append", "neighbourhood_delta_cosine.setdefault", "neighbourhood_delta_cosine[].append", "len", "len", "len", "len", "len", "len", "len", "len", "sum", "len", "sum", "len", "format", "format", "format", "format", "format", "format", "format", "format", "format", "cosine_distances.argsort", "np.linalg.norm.argsort", "set.intersection", "set.union", "set.intersection", "set.union", "set.intersection", "set.union", "set.intersection", "set.union", "sklearn.metrics.pairwise.cosine_similarity", "idx.item", "idx.item", "embedding_x.reshape", "numpy.argwhere", "numpy.argwhere", "chengyu_antonyms_dict.get", "chengyu_antonyms_dict.get", "cids.index", "nids.index", "set", "set", "set", "set", "set", "set", "set", "set"], "function", ["None"], ["", "def", "evaluate_embeddings", "(", "iw", ",", "wi", ",", "vectors_np", ",", "\n", "chengyu_synonyms_dict", ",", "\n", "chengyu_antonyms_dict", ",", "\n", "cached", "\n", ")", ":", "\n", "    ", "cnt", "=", "0", "\n", "recall_at_k_cosine", "=", "{", "}", "\n", "antonyms_at_k_cosine", "=", "{", "}", "\n", "recall_at_k_norm", "=", "{", "}", "\n", "antonyms_at_k_norm", "=", "{", "}", "\n", "set_recall_at_k_cosine", "=", "{", "}", "\n", "set_recall_at_k_norm", "=", "{", "}", "\n", "\n", "set_recall_at_delta_norm", "=", "{", "}", "\n", "set_recall_at_cosine_norm", "=", "{", "}", "\n", "\n", "mrr_cosine", "=", "0", "\n", "mrr_norm", "=", "0", "\n", "\n", "k_list", "=", "[", "1", ",", "3", ",", "5", ",", "10", "]", "\n", "\n", "delta_list", "=", "[", "0.5", ",", "1", ",", "2", ",", "3", ",", "5", "]", "\n", "cosine_delta_list", "=", "list", "(", "[", "0.1", ",", "0.2", ",", "0.3", ",", "0.4", ",", "0.5", ",", "0.6", ",", "0.7", ",", "0.8", ",", "0.9", ",", "1", "]", ")", "\n", "\n", "total", "=", "len", "(", "chengyu_synonyms_dict", ")", "\n", "top_k", "=", "{", "}", "\n", "for", "w", ",", "wll", "in", "tqdm", "(", "chengyu_synonyms_dict", ".", "items", "(", ")", ")", ":", "\n", "        ", "wl_new", "=", "[", "x", "for", "x", "in", "wll", "if", "x", "in", "wi", "]", "\n", "if", "w", "in", "wi", "and", "wl_new", ":", "\n", "            ", "cnt", "+=", "1", "\n", "\n", "neighbourhood_cosine", "=", "[", "]", "\n", "neighbourhood_norm", "=", "[", "]", "\n", "neighbourhood_delta_cosine", "=", "{", "}", "\n", "neighbourhood_delta_norm", "=", "{", "}", "\n", "\n", "for", "x", "in", "[", "w", "]", "+", "wl_new", ":", "\n", "                ", "embedding_x", "=", "vectors_np", "[", "wi", "[", "x", "]", "]", "\n", "if", "x", "not", "in", "cached", ":", "\n", "                    ", "cosine_distances", "=", "(", "1", "-", "cosine_similarity", "(", "embedding_x", ".", "reshape", "(", "1", ",", "-", "1", ")", ",", "vectors_np", ")", "[", "0", "]", ")", "\n", "norm_distances", "=", "np", ".", "linalg", ".", "norm", "(", "vectors_np", "-", "embedding_x", ",", "axis", "=", "1", ")", "\n", "cached", "[", "x", "]", "=", "(", "\n", "cosine_distances", ".", "argsort", "(", ")", ",", "norm_distances", ".", "argsort", "(", ")", ",", "cosine_distances", ",", "norm_distances", ")", "\n", "\n", "", "cosine_distances_sort", ",", "norm_distances_sort", ",", "cosine_distances", ",", "norm_distances", "=", "cached", "[", "x", "]", "\n", "\n", "neighbourhood_cosine", ".", "append", "(", "[", "idx", "for", "idx", "in", "cosine_distances_sort", "]", ")", "\n", "neighbourhood_norm", ".", "append", "(", "[", "idx", "for", "idx", "in", "norm_distances_sort", "]", ")", "\n", "\n", "for", "delta", "in", "delta_list", ":", "\n", "                    ", "neighbourhood_delta_norm", ".", "setdefault", "(", "delta", ",", "[", "]", ")", "\n", "neighbourhood_delta_norm", "[", "delta", "]", ".", "append", "(", "\n", "[", "idx", ".", "item", "(", ")", "for", "idx", "in", "np", ".", "argwhere", "(", "norm_distances", "<=", "delta", ")", "]", ")", "\n", "\n", "", "for", "delta", "in", "cosine_delta_list", ":", "\n", "                    ", "neighbourhood_delta_cosine", ".", "setdefault", "(", "delta", ",", "[", "]", ")", "\n", "neighbourhood_delta_cosine", "[", "delta", "]", ".", "append", "(", "\n", "[", "idx", ".", "item", "(", ")", "for", "idx", "in", "np", ".", "argwhere", "(", "cosine_distances", "<=", "delta", ")", "]", ")", "\n", "\n", "", "", "for", "k", "in", "k_list", ":", "\n", "                ", "set_recall_at_k_cosine", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "set_recall_at_k_cosine", "[", "k", "]", "+=", "len", "(", "set", ".", "intersection", "(", "*", "[", "set", "(", "ns", "[", ":", "k", "]", ")", "for", "ns", "in", "neighbourhood_cosine", "]", ")", ")", "/", "len", "(", "\n", "set", ".", "union", "(", "*", "[", "set", "(", "ns", "[", ":", "k", "]", ")", "for", "ns", "in", "neighbourhood_cosine", "]", ")", ")", "\n", "\n", "set_recall_at_k_norm", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "set_recall_at_k_norm", "[", "k", "]", "+=", "len", "(", "set", ".", "intersection", "(", "*", "[", "set", "(", "ns", "[", ":", "k", "]", ")", "for", "ns", "in", "neighbourhood_norm", "]", ")", ")", "/", "len", "(", "\n", "set", ".", "union", "(", "*", "[", "set", "(", "ns", "[", ":", "k", "]", ")", "for", "ns", "in", "neighbourhood_norm", "]", ")", ")", "\n", "\n", "", "for", "delta", "in", "delta_list", ":", "\n", "                ", "set_recall_at_delta_norm", ".", "setdefault", "(", "delta", ",", "0", ")", "\n", "set_recall_at_delta_norm", "[", "delta", "]", "+=", "len", "(", "\n", "set", ".", "intersection", "(", "*", "[", "set", "(", "ns", ")", "for", "ns", "in", "neighbourhood_delta_norm", "[", "delta", "]", "]", ")", ")", "/", "len", "(", "\n", "set", ".", "union", "(", "*", "[", "set", "(", "ns", ")", "for", "ns", "in", "neighbourhood_delta_norm", "[", "delta", "]", "]", ")", ")", "\n", "\n", "", "for", "delta", "in", "cosine_delta_list", ":", "\n", "                ", "set_recall_at_cosine_norm", ".", "setdefault", "(", "delta", ",", "0", ")", "\n", "set_recall_at_cosine_norm", "[", "delta", "]", "+=", "len", "(", "\n", "set", ".", "intersection", "(", "*", "[", "set", "(", "ns", ")", "for", "ns", "in", "neighbourhood_delta_cosine", "[", "delta", "]", "]", ")", ")", "/", "len", "(", "\n", "set", ".", "union", "(", "*", "[", "set", "(", "ns", ")", "for", "ns", "in", "neighbourhood_delta_cosine", "[", "delta", "]", "]", ")", ")", "\n", "\n", "", "cosine_distances_sort", ",", "norm_distances_sort", ",", "cosine_distances", ",", "norm_distances", "=", "cached", "[", "w", "]", "\n", "\n", "cids", "=", "[", "idx", "for", "idx", "in", "cosine_distances_sort", "if", "w", "!=", "iw", "[", "idx", "]", "]", "\n", "nids", "=", "[", "idx", "for", "idx", "in", "norm_distances_sort", "if", "w", "!=", "iw", "[", "idx", "]", "]", "\n", "\n", "for", "k", "in", "k_list", ":", "\n", "                ", "top_ids", "=", "cids", "[", ":", "k", "]", "\n", "recall_at_k_cosine", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "recall_at_k_cosine", "[", "k", "]", "+=", "sum", "(", "[", "1", "for", "idx", "in", "top_ids", "if", "iw", "[", "idx", "]", "in", "wl_new", "]", ")", "/", "len", "(", "wl_new", ")", "\n", "\n", "antonyms_at_k_cosine", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "antonyms_at_k_cosine", "[", "k", "]", "+=", "sum", "(", "[", "1", "for", "idx", "in", "top_ids", "if", "iw", "[", "idx", "]", "in", "chengyu_antonyms_dict", ".", "get", "(", "w", ",", "[", "]", ")", "]", ")", "\n", "\n", "top_ids", "=", "nids", "[", ":", "k", "]", "\n", "recall_at_k_norm", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "recall_at_k_norm", "[", "k", "]", "+=", "sum", "(", "[", "1", "for", "idx", "in", "top_ids", "if", "iw", "[", "idx", "]", "in", "wl_new", "]", ")", "/", "len", "(", "wl_new", ")", "\n", "\n", "antonyms_at_k_norm", ".", "setdefault", "(", "k", ",", "0", ")", "\n", "antonyms_at_k_norm", "[", "k", "]", "+=", "sum", "(", "[", "1", "for", "idx", "in", "top_ids", "if", "iw", "[", "idx", "]", "in", "chengyu_antonyms_dict", ".", "get", "(", "w", ",", "[", "]", ")", "]", ")", "\n", "\n", "", "mrr_cosine", "+=", "sum", "(", "[", "1", "/", "(", "1", "+", "cids", ".", "index", "(", "wi", "[", "x", "]", ")", ")", "for", "x", "in", "wl_new", "if", "x", "in", "wi", "]", ")", "/", "len", "(", "wl_new", ")", "\n", "mrr_norm", "+=", "sum", "(", "[", "1", "/", "(", "1", "+", "nids", ".", "index", "(", "wi", "[", "x", "]", ")", ")", "for", "x", "in", "wl_new", "if", "x", "in", "wi", "]", ")", "/", "len", "(", "wl_new", ")", "\n", "\n", "", "", "print", "(", "cnt", ",", "' entries appeared in the training dictionary , total word pairs '", ",", "total", ")", "\n", "print", "(", "recall_at_k_cosine", ")", "\n", "print", "(", "recall_at_k_norm", ")", "\n", "print", "(", "antonyms_at_k_cosine", ")", "\n", "for", "k", "in", "recall_at_k_cosine", ":", "\n", "        ", "recall_at_k_cosine", "[", "k", "]", "/=", "cnt", "\n", "recall_at_k_norm", "[", "k", "]", "/=", "cnt", "\n", "set_recall_at_k_cosine", "[", "k", "]", "/=", "cnt", "\n", "set_recall_at_k_norm", "[", "k", "]", "/=", "cnt", "\n", "\n", "", "for", "delta", "in", "delta_list", ":", "\n", "        ", "set_recall_at_delta_norm", "[", "delta", "]", "/=", "cnt", "\n", "", "for", "delta", "in", "cosine_delta_list", ":", "\n", "        ", "set_recall_at_cosine_norm", "[", "delta", "]", "/=", "cnt", "\n", "\n", "", "print", "(", "'\\t'", ".", "join", "(", "chain", "(", "[", "format", "(", "recall_at_k_cosine", "[", "k", "]", ",", "\"0.6f\"", ")", "for", "k", "in", "k_list", "]", ",", "\n", "[", "format", "(", "antonyms_at_k_cosine", "[", "k", "]", ",", "\"d\"", ")", "for", "k", "in", "k_list", "]", ",", "\n", "[", "format", "(", "recall_at_k_norm", "[", "k", "]", ",", "\"0.6f\"", ")", "for", "k", "in", "k_list", "]", ",", "\n", "[", "format", "(", "antonyms_at_k_norm", "[", "k", "]", ",", "\"d\"", ")", "for", "k", "in", "k_list", "]", ",", "\n", "[", "format", "(", "set_recall_at_delta_norm", "[", "k", "]", ",", "\"0.6f\"", ")", "for", "k", "in", "delta_list", "]", ",", "\n", "[", "format", "(", "set_recall_at_k_cosine", "[", "k", "]", ",", "\"0.6f\"", ")", "for", "k", "in", "k_list", "]", ",", "\n", "[", "format", "(", "set_recall_at_k_norm", "[", "k", "]", ",", "\"0.6f\"", ")", "for", "k", "in", "k_list", "]", ",", "\n", "[", "format", "(", "mrr", ",", "\"0.6f\"", ")", "for", "mrr", "in", "[", "mrr_cosine", "/", "cnt", ",", "mrr_norm", "/", "cnt", "]", "]", ",", "\n", "[", "format", "(", "set_recall_at_cosine_norm", "[", "k", "]", ",", "\"0.6f\"", ")", "for", "k", "in", "cosine_delta_list", "]", ",", "\n", ")", ")", ")", "\n", "print", "(", "recall_at_k_cosine", ")", "\n", "print", "(", "recall_at_k_norm", ")", "\n", "print", "(", "set_recall_at_k_cosine", ")", "\n", "print", "(", "set_recall_at_k_norm", ")", "\n", "print", "(", "set_recall_at_delta_norm", ")", "\n", "return", "cnt", ",", "total", ",", "recall_at_k_cosine", ",", "recall_at_k_norm", ",", "top_k", "\n", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.__init__": [[45, 48], ["torch.cuda.Stream"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "loader", ")", ":", "\n", "        ", "self", ".", "loader", "=", "loader", "\n", "self", ".", "stream", "=", "torch", ".", "cuda", ".", "Stream", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.__iter__": [[49, 56], ["iter", "loader.PrefetchLoader.preload", "loader.PrefetchLoader.next", "loader.PrefetchLoader.next"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.preload", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.next", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.next"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "loader_it", "=", "iter", "(", "self", ".", "loader", ")", "\n", "self", ".", "preload", "(", "loader_it", ")", "\n", "batch", "=", "self", ".", "next", "(", "loader_it", ")", "\n", "while", "batch", "is", "not", "None", ":", "\n", "            ", "yield", "batch", "\n", "batch", "=", "self", ".", "next", "(", "loader_it", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.__len__": [[57, 59], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.preload": [[60, 77], ["loader.PrefetchLoader.next"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.next"], ["", "def", "preload", "(", "self", ",", "it", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "self", ".", "batch", "=", "next", "(", "it", ")", "\n", "", "except", "StopIteration", ":", "\n", "            ", "self", ".", "batch", "=", "None", "\n", "return", "\n", "# if record_stream() doesn't work, another option is to make sure", "\n", "# device inputs are created on the main stream.", "\n", "# self.next_input_gpu = torch.empty_like(self.next_input,", "\n", "#                                        device='cuda')", "\n", "# self.next_target_gpu = torch.empty_like(self.next_target,", "\n", "#                                         device='cuda')", "\n", "# Need to make sure the memory allocated for next_* is not still in use", "\n", "# by the main stream at the time we start copying to next_*:", "\n", "# self.stream.wait_stream(torch.cuda.current_stream())", "\n", "", "with", "torch", ".", "cuda", ".", "stream", "(", "self", ".", "stream", ")", ":", "\n", "            ", "self", ".", "batch", "=", "move_to_cuda", "(", "self", ".", "batch", ")", "\n", "# more code for the alternative if record_stream() doesn't work:", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.next": [[85, 92], ["torch.cuda.current_stream().wait_stream", "loader.PrefetchLoader.preload", "loader.record_cuda_stream", "torch.cuda.current_stream"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.preload", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.record_cuda_stream"], ["", "", "def", "next", "(", "self", ",", "it", ")", ":", "\n", "        ", "torch", ".", "cuda", ".", "current_stream", "(", ")", ".", "wait_stream", "(", "self", ".", "stream", ")", "\n", "batch", "=", "self", ".", "batch", "\n", "if", "batch", "is", "not", "None", ":", "\n", "            ", "record_cuda_stream", "(", "batch", ")", "\n", "", "self", ".", "preload", "(", "it", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.PrefetchLoader.__getattr__": [[93, 96], ["loader.PrefetchLoader.loader.__getattribute__"], "methods", ["None"], ["", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "method", "=", "self", ".", "loader", ".", "__getattribute__", "(", "name", ")", "\n", "return", "method", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.move_to_cuda": [[12, 24], ["isinstance", "batch.cuda", "isinstance", "isinstance", "loader.move_to_cuda", "tuple", "isinstance", "loader.move_to_cuda", "loader.move_to_cuda", "batch.items"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.move_to_cuda", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.move_to_cuda", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.move_to_cuda"], ["def", "move_to_cuda", "(", "batch", ")", ":", "\n", "    ", "if", "isinstance", "(", "batch", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "batch", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "", "elif", "isinstance", "(", "batch", ",", "list", ")", ":", "\n", "        ", "new_batch", "=", "[", "move_to_cuda", "(", "t", ")", "for", "t", "in", "batch", "]", "\n", "", "elif", "isinstance", "(", "batch", ",", "tuple", ")", ":", "\n", "        ", "new_batch", "=", "tuple", "(", "move_to_cuda", "(", "t", ")", "for", "t", "in", "batch", ")", "\n", "", "elif", "isinstance", "(", "batch", ",", "dict", ")", ":", "\n", "        ", "new_batch", "=", "{", "n", ":", "move_to_cuda", "(", "t", ")", "for", "n", ",", "t", "in", "batch", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "        ", "return", "batch", "\n", "", "return", "new_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.record_cuda_stream": [[26, 37], ["isinstance", "batch.record_stream", "torch.cuda.current_stream", "isinstance", "isinstance", "isinstance", "loader.record_cuda_stream", "batch.values", "loader.record_cuda_stream"], "function", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.record_cuda_stream", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.loader.record_cuda_stream"], ["", "def", "record_cuda_stream", "(", "batch", ")", ":", "\n", "    ", "if", "isinstance", "(", "batch", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "batch", ".", "record_stream", "(", "torch", ".", "cuda", ".", "current_stream", "(", ")", ")", "\n", "", "elif", "isinstance", "(", "batch", ",", "list", ")", "or", "isinstance", "(", "batch", ",", "tuple", ")", ":", "\n", "        ", "for", "t", "in", "batch", ":", "\n", "            ", "record_cuda_stream", "(", "t", ")", "\n", "", "", "elif", "isinstance", "(", "batch", ",", "dict", ")", ":", "\n", "        ", "for", "t", "in", "batch", ".", "values", "(", ")", ":", "\n", "            ", "record_cuda_stream", "(", "t", ")", "\n", "", "", "else", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALODataset.__init__": [[18, 36], ["chengyubert.data.ChengyuLmdb.__init__", "chengyubert.data.calo_process", "affection.ChengyuCALODataset.get_allowed_examples", "affection.ChengyuCALODataset.tokenize_idioms", "affection.ChengyuCALODataset.get_ids_and_lens", "chengyubert.utils.logger.LOGGER.info", "chengyubert.utils.logger.LOGGER.info", "open", "json.load", "open", "json.load", "affection.ChengyuCALODataset.get_label_weights", "affection.ChengyuCALODataset.get_label_weights", "str", "str"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.data.__init__.calo_process", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedDataset.get_allowed_examples", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.tokenize_idioms", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.get_ids_and_lens", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset.get_label_weights", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset.get_label_weights"], ["    ", "def", "__init__", "(", "self", ",", "split", ",", "max_txt_len", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "max_txt_len", ",", "opts", ")", "\n", "self", ".", "use_context", "=", "opts", ".", "use_context", "\n", "# load labelled idioms for the split", "\n", "with", "open", "(", "f'{self.db_dir}/{\"dev\" if split == \"val\" else split}.json'", ")", "as", "f", ":", "\n", "            ", "self", ".", "filtered", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "f'{self.db_dir}/unlabelled.json'", ")", "as", "f", ":", "\n", "            ", "self", ".", "unlabeled", "=", "json", ".", "load", "(", "f", ")", "\n", "", "self", ".", "calo_vocab", "=", "calo_process", "(", "self", ".", "chengyu_vocab", ",", "self", ".", "config", ".", "calo_file", ")", "\n", "self", ".", "allowed", ",", "self", ".", "reverse_index", "=", "self", ".", "get_allowed_examples", "(", "split", ",", "opts", ")", "\n", "\n", "self", ".", "idiom_input_ids", "=", "self", ".", "tokenize_idioms", "(", ")", "\n", "self", ".", "lens", ",", "self", ".", "ids", ",", "self", ".", "st_ed", ",", "fine_emotion_counter", ",", "sentiment_counter", "=", "self", ".", "get_ids_and_lens", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"Fine emotion counter: \"", "+", "str", "(", "fine_emotion_counter", ")", ")", "\n", "LOGGER", ".", "info", "(", "\"Sentiment counter: \"", "+", "str", "(", "sentiment_counter", ")", ")", "\n", "if", "split", "==", "'train'", ":", "\n", "            ", "self", ".", "fine_emotion_weights", "=", "self", ".", "get_label_weights", "(", "fine_emotion_counter", ",", "num_classes", "=", "21", ")", "\n", "self", ".", "sentiment_weights", "=", "self", ".", "get_label_weights", "(", "sentiment_counter", ",", "num_classes", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALODataset.get_label_weights": [[37, 41], ["torch.tensor", "counter.most_common", "range"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "get_label_weights", "(", "counter", ",", "num_classes", ")", ":", "\n", "        ", "_", ",", "max_num", "=", "counter", ".", "most_common", "(", "1", ")", "[", "0", "]", "\n", "return", "torch", ".", "tensor", "(", "[", "counter", "[", "i", "]", "/", "max_num", "for", "i", "in", "range", "(", "num_classes", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALODataset.get_allowed_examples": [[42, 45], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "get_allowed_examples", "(", "self", ",", "split", ",", "opts", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALODataset.__len__": [[46, 48], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALODataset.tokenize_idioms": [[49, 56], ["affection.ChengyuCALODataset.chengyu_vocab.items", "affection.ChengyuCALODataset.tokenizer.tokenize", "affection.ChengyuCALODataset.tokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.tokenize"], ["", "def", "tokenize_idioms", "(", "self", ")", ":", "\n", "        ", "idiom_ids", "=", "{", "}", "\n", "for", "idiom", ",", "idiom_id", "in", "self", ".", "chengyu_vocab", ".", "items", "(", ")", ":", "\n", "            ", "tokens", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "idiom", ")", "\n", "input_ids", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "idiom_ids", "[", "idiom_id", "]", "=", "input_ids", "\n", "", "return", "idiom_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALODataset.get_ids_and_lens": [[57, 91], ["collections.Counter", "collections.Counter", "affection.ChengyuCALODataset.id2len.items", "st_ed.append", "lens.append", "ids.append", "collections.Counter.update", "collections.Counter.update", "min", "len", "max", "len", "len"], "methods", ["None"], ["", "def", "get_ids_and_lens", "(", "self", ")", ":", "\n", "        ", "lens", "=", "[", "]", "\n", "ids", "=", "[", "]", "\n", "st_ed", "=", "[", "]", "\n", "fine_emotion_counter", "=", "Counter", "(", ")", "\n", "sentiment_counter", "=", "Counter", "(", ")", "\n", "for", "id_", ",", "len_", "in", "self", ".", "id2len", ".", "items", "(", ")", ":", "\n", "            ", "if", "id_", "not", "in", "self", ".", "allowed", ":", "\n", "                ", "continue", "\n", "\n", "", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "idiom", "=", "example", "[", "'idiom'", "]", "\n", "if", "idiom", "in", "self", ".", "calo_vocab", "and", "idiom", "in", "self", ".", "filtered", ":", "\n", "                ", "affections", "=", "self", ".", "calo_vocab", "[", "idiom", "]", "[", "0", "]", "\n", "fine_emotion_counter", ".", "update", "(", "[", "affections", "[", "'fine_emotion'", "]", "]", ")", "\n", "sentiment_counter", ".", "update", "(", "[", "affections", "[", "'sentiment'", "]", "]", ")", "\n", "", "position", "=", "example", "[", "'position'", "]", "\n", "input_ids", "=", "example", "[", "'input_ids'", "]", "\n", "half_length", "=", "self", ".", "max_txt_len", "//", "2", "\n", "if", "position", "<", "half_length", ":", "# cut at tail", "\n", "                ", "st", "=", "0", "\n", "ed", "=", "min", "(", "len", "(", "input_ids", ")", "+", "1", ",", "self", ".", "max_txt_len", "-", "2", ")", "\n", "", "elif", "len", "(", "input_ids", ")", "-", "position", "<", "half_length", ":", "# cut at head", "\n", "                ", "ed", "=", "len", "(", "input_ids", ")", "\n", "st", "=", "max", "(", "0", ",", "ed", "-", "(", "self", ".", "max_txt_len", "-", "2", ")", ")", "\n", "", "else", ":", "# cut at both sides", "\n", "                ", "st", "=", "position", "-", "(", "half_length", "-", "2", ")", "\n", "ed", "=", "position", "+", "half_length", "\n", "\n", "", "assert", "ed", "-", "st", "<=", "self", ".", "max_txt_len", "-", "2", "\n", "st_ed", ".", "append", "(", "(", "st", ",", "ed", ")", ")", "\n", "lens", ".", "append", "(", "ed", "-", "st", "+", "1", ")", "\n", "ids", ".", "append", "(", "id_", ")", "\n", "", "return", "lens", ",", "ids", ",", "st_ed", ",", "fine_emotion_counter", ",", "sentiment_counter", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALODataset._decide_target": [[92, 106], ["None"], "methods", ["None"], ["", "def", "_decide_target", "(", "self", ",", "idiom", ",", "idx", ")", ":", "\n", "        ", "target", "=", "[", "\n", "idx", ",", "-", "100", ",", "-", "100", ",", "-", "100", ",", "0", "\n", "]", "\n", "if", "idiom", "in", "self", ".", "calo_vocab", "and", "idiom", "in", "self", ".", "filtered", ":", "\n", "            ", "affections", "=", "self", ".", "calo_vocab", "[", "idiom", "]", "[", "0", "]", "\n", "target", "=", "[", "\n", "idx", ",", "\n", "affections", "[", "'coarse_emotion'", "]", ",", "\n", "affections", "[", "'fine_emotion'", "]", ",", "\n", "affections", "[", "'sentiment'", "]", ",", "\n", "affections", "[", "'strength'", "]", ",", "\n", "]", "\n", "", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyDataset.__init__": [[110, 117], ["affection.ChengyuCALODataset.__init__", "range"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "split", ",", "max_txt_len", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "max_txt_len", ",", "opts", ")", "\n", "if", "split", "==", "'train'", ":", "\n", "            ", "self", ".", "enlarged_candidates", "=", "[", "i", "for", "i", "in", "range", "(", "opts", ".", "len_idiom_vocab", ")", "if", "i", "in", "self", ".", "filtered", "]", "\n", "opts", ".", "enlarged_candidates", "=", "self", ".", "enlarged_candidates", "\n", "", "else", ":", "\n", "            ", "self", ".", "enlarged_candidates", "=", "opts", ".", "enlarged_candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyDataset.get_allowed_examples": [[118, 136], ["json.load().items", "set", "int", "set.update", "json.load", "reverse_index.items", "open"], "methods", ["None"], ["", "", "def", "get_allowed_examples", "(", "self", ",", "split", ",", "opts", ")", ":", "\n", "        ", "\"\"\"\n        For training dataset:\n        1) we can choose whether to add unlabelled data\n        2) we can choose whether to add labelled but not in training\n        :param opts:\n        :return:\n        \"\"\"", "\n", "reverse_index", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "json", ".", "load", "(", "open", "(", "f'{self.db_dir}/reverse_index.json'", ")", ")", ".", "items", "(", ")", ":", "\n", "            ", "k", "=", "int", "(", "k", ")", "\n", "if", "k", "<", "opts", ".", "len_idiom_vocab", ":", "\n", "                ", "if", "k", "in", "self", ".", "filtered", ":", "\n", "                    ", "reverse_index", "[", "k", "]", "=", "v", "\n", "\n", "", "", "", "allowed", "=", "set", "(", ")", "\n", "[", "allowed", ".", "update", "(", "v", ")", "for", "_", ",", "v", "in", "reverse_index", ".", "items", "(", ")", "]", "\n", "return", "allowed", ",", "reverse_index", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyDataset.__getitem__": [[137, 185], ["context_ids.index", "len", "affection.ChengyuCALOComposeOnlyDataset._decide_target", "torch.tensor", "torch.tensor", "torch.tensor", "len", "random.sample", "random.shuffle", "affection.ChengyuCALOComposeOnlyDataset.enlarged_candidates.index", "functools.reduce", "functools.reduce", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset._decide_target"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "id_", "=", "self", ".", "ids", "[", "i", "]", "\n", "st", ",", "ed", "=", "self", ".", "st_ed", "[", "i", "]", "\n", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "options", "=", "example", "[", "'options'", "]", "\n", "idiom", "=", "example", "[", "'idiom'", "]", "\n", "if", "len", "(", "options", ")", "==", "0", ":", "\n", "            ", "options", "=", "random", ".", "sample", "(", "self", ".", "idiom_ids", ",", "k", "=", "7", ")", "\n", "if", "idiom", "not", "in", "options", ":", "\n", "                ", "options", "[", "-", "1", "]", "=", "idiom", "\n", "", "random", ".", "shuffle", "(", "options", ")", "\n", "#     target = options.index(idiom)", "\n", "\n", "", "context_ids", "=", "example", "[", "'input_ids'", "]", "[", "st", ":", "ed", "]", "\n", "idiom_start", "=", "context_ids", ".", "index", "(", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "idiom_input_ids", "=", "self", ".", "idiom_input_ids", "[", "idiom", "]", "\n", "idiom_len", "=", "len", "(", "idiom_input_ids", ")", "\n", "# target = idiom", "\n", "\n", "if", "idiom", "in", "self", ".", "enlarged_candidates", ":", "\n", "            ", "idx", "=", "self", ".", "enlarged_candidates", ".", "index", "(", "idiom", ")", "\n", "", "else", ":", "\n", "            ", "idx", "=", "-", "100", "\n", "", "target", "=", "self", ".", "_decide_target", "(", "idiom", ",", "idx", ")", "\n", "\n", "if", "self", ".", "use_context", ":", "\n", "            ", "input_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "context_ids", "[", ":", "idiom_start", "]", ",", "\n", "idiom_input_ids", ",", "\n", "context_ids", "[", "idiom_start", "+", "1", ":", "]", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "position", "=", "idiom_start", "+", "1", "\n", "", "else", ":", "\n", "            ", "input_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "idiom_input_ids", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "position", "=", "1", "\n", "", "assert", "len", "(", "input_ids", ")", "<=", "self", ".", "max_txt_len", "+", "idiom_len", "\n", "\n", "token_type_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "attention_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "token_type_ids", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "attention_mask", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "position", ",", "idiom_len", ",", "options", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyDataset.collate_fn": [[186, 208], ["map", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "max", "torch.arange().unsqueeze().repeat().clone", "enumerate", "more_itertools.unzip", "zip", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.arange().unsqueeze().repeat", "torch.arange", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange().unsqueeze", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "widths", ",", "options", ",", "targets", ")", "=", "map", "(", "list", ",", "\n", "unzip", "(", "inputs", ")", ")", "\n", "\n", "input_ids", "=", "pad_sequence", "(", "input_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "token_type_ids", "=", "pad_sequence", "(", "token_type_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "attn_masks", "=", "pad_sequence", "(", "attention_mask", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "\n", "width_max", "=", "max", "(", "widths", ")", "\n", "gather_index", "=", "torch", ".", "arange", "(", "0", ",", "width_max", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "1", ")", ".", "clone", "(", ")", "\n", "for", "i", ",", "(", "p", ",", "w", ")", "in", "enumerate", "(", "zip", "(", "positions", ",", "widths", ")", ")", ":", "\n", "            ", "gather_index", ".", "data", "[", "i", ",", ":", "w", "]", "=", "torch", ".", "arange", "(", "p", ",", "p", "+", "w", ",", "dtype", "=", "torch", ".", "long", ")", ".", "data", "\n", "\n", "", "batch", "=", "{", "'input_ids'", ":", "input_ids", ",", "\n", "'token_type_ids'", ":", "token_type_ids", ",", "\n", "'attention_mask'", ":", "attn_masks", ",", "\n", "'gather_index'", ":", "gather_index", ",", "\n", "'positions'", ":", "torch", ".", "tensor", "(", "positions", ")", ".", "long", "(", ")", ",", "\n", "'option_ids'", ":", "torch", ".", "tensor", "(", "options", ")", ".", "long", "(", ")", ",", "\n", "'targets'", ":", "torch", ".", "tensor", "(", "targets", ")", ".", "long", "(", ")", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyEvalDataset.__getitem__": [[212, 216], ["affection.ChengyuCALOComposeOnlyDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.__getitem__"], ["    ", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "qid", "=", "self", ".", "ids", "[", "i", "]", "\n", "tensors", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "return", "(", "qid", ",", "*", "tensors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyEvalDataset.collate_fn": [[217, 226], ["affection.ChengyuCALOComposeOnlyDataset.collate_fn", "qids.append", "affection.ChengyuCALOComposeOnlyDataset.collate_fn"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "qids", ",", "batch", "=", "[", "]", ",", "[", "]", "\n", "for", "id_", ",", "*", "tensors", "in", "inputs", ":", "\n", "            ", "qids", ".", "append", "(", "id_", ")", "\n", "batch", ".", "append", "(", "tensors", ")", "\n", "", "batch", "=", "ChengyuCALOComposeOnlyDataset", ".", "collate_fn", "(", "batch", ")", "\n", "batch", "[", "'qids'", "]", "=", "qids", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyMaskedDataset.__init__": [[230, 241], ["affection.ChengyuCALODataset.__init__", "range", "range"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "split", ",", "max_txt_len", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "max_txt_len", ",", "opts", ")", "\n", "if", "split", "==", "'train'", ":", "\n", "            ", "if", "not", "opts", ".", "use_unlabeled", ":", "\n", "                ", "self", ".", "enlarged_candidates", "=", "[", "i", "for", "i", "in", "range", "(", "opts", ".", "len_idiom_vocab", ")", "if", "i", "in", "self", ".", "filtered", "]", "\n", "", "else", ":", "\n", "# all the idioms without CALO labels are all considered", "\n", "                ", "self", ".", "enlarged_candidates", "=", "[", "i", "for", "i", "in", "range", "(", "opts", ".", "len_idiom_vocab", ")", "if", "i", "in", "self", ".", "reverse_index", "]", "\n", "", "opts", ".", "enlarged_candidates", "=", "self", ".", "enlarged_candidates", "\n", "", "else", ":", "\n", "            ", "self", ".", "enlarged_candidates", "=", "opts", ".", "enlarged_candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyMaskedDataset.get_allowed_examples": [[242, 268], ["json.load().items", "set", "int", "set.update", "json.load", "reverse_index.items", "open"], "methods", ["None"], ["", "", "def", "get_allowed_examples", "(", "self", ",", "split", ",", "opts", ")", ":", "\n", "        ", "\"\"\"\n        For training dataset:\n        1) we can choose whether to add unlabelled data\n        2) we can choose whether to add labelled but not in training\n        :param opts:\n        :return:\n        \"\"\"", "\n", "reverse_index", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "json", ".", "load", "(", "open", "(", "f'{self.db_dir}/reverse_index.json'", ")", ")", ".", "items", "(", ")", ":", "\n", "            ", "k", "=", "int", "(", "k", ")", "\n", "if", "k", "<", "opts", ".", "len_idiom_vocab", ":", "\n", "                ", "if", "split", "==", "'train'", ":", "\n", "                    ", "if", "opts", ".", "use_unlabeled", ":", "\n", "                        ", "if", "k", "in", "self", ".", "filtered", "or", "k", "in", "self", ".", "unlabeled", ":", "\n", "                            ", "reverse_index", "[", "k", "]", "=", "v", "\n", "", "", "else", ":", "\n", "                        ", "if", "k", "in", "self", ".", "filtered", ":", "\n", "                            ", "reverse_index", "[", "k", "]", "=", "v", "\n", "", "", "", "else", ":", "\n", "                    ", "if", "k", "in", "self", ".", "filtered", ":", "\n", "                        ", "reverse_index", "[", "k", "]", "=", "v", "\n", "\n", "", "", "", "", "allowed", "=", "set", "(", ")", "\n", "[", "allowed", ".", "update", "(", "v", ")", "for", "_", ",", "v", "in", "reverse_index", ".", "items", "(", ")", "]", "\n", "return", "allowed", ",", "reverse_index", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyMaskedDataset.__getitem__": [[269, 331], ["context_ids.index", "len", "affection.ChengyuCALOComposeOnlyMaskedDataset._decide_target", "functools.reduce", "torch.tensor", "torch.tensor", "torch.tensor", "len", "random.sample", "random.shuffle", "affection.ChengyuCALOComposeOnlyMaskedDataset.enlarged_candidates.index", "functools.reduce", "functools.reduce", "len", "len", "len", "len", "len", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset._decide_target"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "id_", "=", "self", ".", "ids", "[", "i", "]", "\n", "st", ",", "ed", "=", "self", ".", "st_ed", "[", "i", "]", "\n", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "options", "=", "example", "[", "'options'", "]", "\n", "idiom", "=", "example", "[", "'idiom'", "]", "\n", "if", "len", "(", "options", ")", "==", "0", ":", "\n", "            ", "options", "=", "random", ".", "sample", "(", "self", ".", "idiom_ids", ",", "k", "=", "7", ")", "\n", "if", "idiom", "not", "in", "options", ":", "\n", "                ", "options", "[", "-", "1", "]", "=", "idiom", "\n", "", "random", ".", "shuffle", "(", "options", ")", "\n", "#     target = options.index(idiom)", "\n", "\n", "", "context_ids", "=", "example", "[", "'input_ids'", "]", "[", "st", ":", "ed", "]", "\n", "idiom_start", "=", "context_ids", ".", "index", "(", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "idiom_input_ids", "=", "self", ".", "idiom_input_ids", "[", "idiom", "]", "\n", "idiom_len", "=", "len", "(", "idiom_input_ids", ")", "\n", "# target = idiom", "\n", "\n", "idiom_masked_input_ids", "=", "[", "self", ".", "tokenizer", ".", "mask_token_id", "]", "*", "idiom_len", "\n", "if", "idiom", "in", "self", ".", "enlarged_candidates", ":", "\n", "            ", "idx", "=", "self", ".", "enlarged_candidates", ".", "index", "(", "idiom", ")", "\n", "", "else", ":", "\n", "            ", "idx", "=", "-", "100", "\n", "", "target", "=", "self", ".", "_decide_target", "(", "idiom", ",", "idx", ")", "\n", "\n", "input_masked_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "context_ids", "[", ":", "idiom_start", "]", ",", "\n", "idiom_masked_input_ids", ",", "\n", "context_ids", "[", "idiom_start", "+", "1", ":", "]", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "if", "self", ".", "use_context", ":", "\n", "            ", "input_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "context_ids", "[", ":", "idiom_start", "]", ",", "\n", "idiom_input_ids", ",", "\n", "context_ids", "[", "idiom_start", "+", "1", ":", "]", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "position", "=", "(", "idiom_start", "+", "1", ",", "idiom_start", "+", "1", ")", "\n", "attention_mask_literal", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "", "else", ":", "\n", "            ", "input_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "idiom_input_ids", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "input_ids_len", "=", "len", "(", "input_ids", ")", "\n", "input_ids", "=", "input_ids", "+", "[", "self", ".", "tokenizer", ".", "pad_token_id", "]", "*", "(", "len", "(", "input_masked_ids", ")", "-", "input_ids_len", ")", "\n", "position", "=", "(", "1", ",", "idiom_start", "+", "1", ")", "\n", "attention_mask_literal", "=", "[", "1", "]", "*", "input_ids_len", "+", "[", "0", "]", "*", "(", "len", "(", "input_masked_ids", ")", "-", "input_ids_len", ")", "\n", "", "assert", "len", "(", "input_ids", ")", "<=", "self", ".", "max_txt_len", "+", "idiom_len", "\n", "\n", "attention_mask_idiomatic", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "token_type_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", "\n", "input_masked_ids", "=", "torch", ".", "tensor", "(", "input_masked_ids", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "token_type_ids", ")", "\n", "return", "(", "input_ids", ",", "input_masked_ids", ")", ",", "token_type_ids", ",", "(", "torch", ".", "tensor", "(", "attention_mask_literal", ")", ",", "\n", "torch", ".", "tensor", "(", "attention_mask_idiomatic", ")", ")", ",", "position", ",", "idiom_len", ",", "options", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyMaskedDataset.collate_fn": [[332, 360], ["map", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "max", "torch.arange().unsqueeze().repeat().clone", "enumerate", "torch.arange().unsqueeze().repeat().clone", "enumerate", "more_itertools.unzip", "zip", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.arange().unsqueeze().repeat", "torch.arange", "torch.arange().unsqueeze().repeat", "torch.arange", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "(", "input_ids_tuple", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "widths", ",", "options", ",", "targets", ")", "=", "map", "(", "list", ",", "\n", "unzip", "(", "inputs", ")", ")", "\n", "\n", "input_ids", "=", "pad_sequence", "(", "[", "item", "[", "0", "]", "for", "item", "in", "input_ids_tuple", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "input_masked_ids", "=", "pad_sequence", "(", "[", "item", "[", "1", "]", "for", "item", "in", "input_ids_tuple", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "token_type_ids", "=", "pad_sequence", "(", "token_type_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "attn_masks_literal", "=", "pad_sequence", "(", "[", "item", "[", "0", "]", "for", "item", "in", "attention_mask", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "attn_masks_idiomatic", "=", "pad_sequence", "(", "[", "item", "[", "1", "]", "for", "item", "in", "attention_mask", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "\n", "width_max", "=", "max", "(", "widths", ")", "\n", "gather_index", "=", "torch", ".", "arange", "(", "0", ",", "width_max", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "1", ")", ".", "clone", "(", ")", "\n", "for", "i", ",", "(", "p", ",", "w", ")", "in", "enumerate", "(", "zip", "(", "positions", ",", "widths", ")", ")", ":", "\n", "            ", "gather_index", ".", "data", "[", "i", ",", ":", "w", "]", "=", "torch", ".", "arange", "(", "p", "[", "0", "]", ",", "p", "[", "0", "]", "+", "w", ",", "dtype", "=", "torch", ".", "long", ")", ".", "data", "\n", "\n", "", "gather_index_masked", "=", "torch", ".", "arange", "(", "0", ",", "width_max", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "1", ")", ".", "clone", "(", ")", "\n", "for", "i", ",", "(", "p", ",", "w", ")", "in", "enumerate", "(", "zip", "(", "positions", ",", "widths", ")", ")", ":", "\n", "            ", "gather_index_masked", ".", "data", "[", "i", ",", ":", "w", "]", "=", "torch", ".", "arange", "(", "p", "[", "1", "]", ",", "p", "[", "1", "]", "+", "w", ",", "dtype", "=", "torch", ".", "long", ")", ".", "data", "\n", "\n", "", "batch", "=", "{", "'input_ids'", ":", "torch", ".", "stack", "(", "[", "input_ids", ",", "input_masked_ids", "]", ")", ",", "\n", "'token_type_ids'", ":", "torch", ".", "stack", "(", "[", "token_type_ids", ",", "token_type_ids", "]", ")", ",", "\n", "'attention_mask'", ":", "torch", ".", "stack", "(", "[", "attn_masks_literal", ",", "attn_masks_idiomatic", "]", ")", ",", "\n", "'gather_index'", ":", "(", "gather_index", ",", "gather_index_masked", ")", ",", "\n", "'positions'", ":", "torch", ".", "tensor", "(", "positions", ")", ".", "long", "(", ")", ",", "\n", "'option_ids'", ":", "torch", ".", "tensor", "(", "options", ")", ".", "long", "(", ")", ",", "\n", "'targets'", ":", "torch", ".", "tensor", "(", "targets", ")", ".", "long", "(", ")", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyMaskedEvalDataset.__getitem__": [[364, 368], ["affection.ChengyuCALOComposeOnlyMaskedDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.__getitem__"], ["    ", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "qid", "=", "self", ".", "ids", "[", "i", "]", "\n", "tensors", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "return", "(", "qid", ",", "*", "tensors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.affection.ChengyuCALOComposeOnlyMaskedEvalDataset.collate_fn": [[369, 378], ["affection.ChengyuCALOComposeOnlyMaskedDataset.collate_fn", "qids.append", "affection.ChengyuCALOComposeOnlyMaskedDataset.collate_fn"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "qids", ",", "batch", "=", "[", "]", ",", "[", "]", "\n", "for", "id_", ",", "*", "tensors", "in", "inputs", ":", "\n", "            ", "qids", ".", "append", "(", "id_", ")", "\n", "batch", ".", "append", "(", "tensors", ")", "\n", "", "batch", "=", "ChengyuCALOComposeOnlyMaskedDataset", ".", "collate_fn", "(", "batch", ")", "\n", "batch", "[", "'qids'", "]", "=", "qids", "\n", "return", "batch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset.__init__": [[18, 35], ["chengyubert.data.IdiomsLmdb.__init__", "slide.ChengyuSlideDataset.get_allowed_examples", "slide.ChengyuSlideDataset.tokenize_idioms", "slide.ChengyuSlideDataset.get_ids_and_lens", "chengyubert.utils.logger.LOGGER.info", "open", "json.load", "open", "json.load", "open", "json.load", "slide.ChengyuSlideDataset.get_label_weights", "str"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedDataset.get_allowed_examples", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.tokenize_idioms", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.get_ids_and_lens", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset.get_label_weights"], ["    ", "def", "__init__", "(", "self", ",", "split", ",", "max_txt_len", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "max_txt_len", ",", "opts", ")", "\n", "self", ".", "use_context", "=", "opts", ".", "use_context", "\n", "# load labelled idioms for the split", "\n", "with", "open", "(", "f'{self.db_dir}/{\"dev\" if split == \"val\" else split}.json'", ")", "as", "f", ":", "\n", "            ", "self", ".", "filtered", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "f'{self.db_dir}/span_idiom_mapping.json'", ")", "as", "f", ":", "\n", "            ", "self", ".", "span_idiom_mapping", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "f'{self.db_dir}/unlabelled.json'", ")", "as", "f", ":", "\n", "            ", "self", ".", "unlabeled", "=", "json", ".", "load", "(", "f", ")", "\n", "", "self", ".", "allowed", ",", "self", ".", "reverse_index", "=", "self", ".", "get_allowed_examples", "(", "split", ",", "opts", ")", "\n", "\n", "self", ".", "idiom_input_ids", "=", "self", ".", "tokenize_idioms", "(", ")", "\n", "self", ".", "lens", ",", "self", ".", "ids", ",", "self", ".", "st_ed", ",", "sentiment_counter", "=", "self", ".", "get_ids_and_lens", "(", ")", "\n", "LOGGER", ".", "info", "(", "\"Sentiment counter: \"", "+", "str", "(", "sentiment_counter", ")", ")", "\n", "if", "split", "==", "'train'", ":", "\n", "            ", "self", ".", "sentiment_weights", "=", "self", ".", "get_label_weights", "(", "sentiment_counter", ",", "num_classes", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset.get_label_weights": [[36, 40], ["torch.tensor", "counter.most_common", "range"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "get_label_weights", "(", "counter", ",", "num_classes", ")", ":", "\n", "        ", "_", ",", "max_num", "=", "counter", ".", "most_common", "(", "1", ")", "[", "0", "]", "\n", "return", "torch", ".", "tensor", "(", "[", "counter", "[", "i", "]", "/", "max_num", "for", "i", "in", "range", "(", "num_classes", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset.get_allowed_examples": [[41, 44], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "get_allowed_examples", "(", "self", ",", "split", ",", "opts", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset.__len__": [[45, 47], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset.tokenize_idioms": [[48, 56], ["slide.ChengyuSlideDataset.tokenizer.tokenize", "slide.ChengyuSlideDataset.tokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.tokenize"], ["", "def", "tokenize_idioms", "(", "self", ")", ":", "\n", "        ", "idiom_ids", "=", "{", "}", "\n", "for", "k", "in", "self", ".", "allowed", ":", "\n", "            ", "idiom", "=", "self", ".", "span_idiom_mapping", "[", "k", "]", "\n", "tokens", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "idiom", ")", "\n", "input_ids", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "idiom_ids", "[", "k", "]", "=", "input_ids", "\n", "", "return", "idiom_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset.get_ids_and_lens": [[57, 89], ["collections.Counter", "slide.ChengyuSlideDataset.id2len.items", "st_ed.append", "lens.append", "ids.append", "collections.Counter.update", "min", "len", "max", "len", "len"], "methods", ["None"], ["", "def", "get_ids_and_lens", "(", "self", ")", ":", "\n", "        ", "lens", "=", "[", "]", "\n", "ids", "=", "[", "]", "\n", "st_ed", "=", "[", "]", "\n", "sentiment_counter", "=", "Counter", "(", ")", "\n", "for", "id_", ",", "len_", "in", "self", ".", "id2len", ".", "items", "(", ")", ":", "\n", "            ", "if", "id_", "not", "in", "self", ".", "allowed", ":", "\n", "                ", "continue", "\n", "\n", "", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "idiom", "=", "example", "[", "'idiom'", "]", "\n", "if", "idiom", "in", "self", ".", "sentiments", "and", "idiom", "in", "self", ".", "filtered", ":", "\n", "                ", "sentiment", "=", "self", ".", "sentiments", "[", "idiom", "]", "\n", "sentiment_counter", ".", "update", "(", "[", "sentiment", "]", ")", "\n", "", "position", "=", "example", "[", "'position'", "]", "\n", "input_ids", "=", "example", "[", "'input_ids'", "]", "\n", "half_length", "=", "self", ".", "max_txt_len", "//", "2", "\n", "if", "position", "<", "half_length", ":", "# cut at tail", "\n", "                ", "st", "=", "0", "\n", "ed", "=", "min", "(", "len", "(", "input_ids", ")", "+", "1", ",", "self", ".", "max_txt_len", "-", "2", ")", "\n", "", "elif", "len", "(", "input_ids", ")", "-", "position", "<", "half_length", ":", "# cut at head", "\n", "                ", "ed", "=", "len", "(", "input_ids", ")", "\n", "st", "=", "max", "(", "0", ",", "ed", "-", "(", "self", ".", "max_txt_len", "-", "2", ")", ")", "\n", "", "else", ":", "# cut at both sides", "\n", "                ", "st", "=", "position", "-", "(", "half_length", "-", "2", ")", "\n", "ed", "=", "position", "+", "half_length", "\n", "\n", "", "assert", "ed", "-", "st", "<=", "self", ".", "max_txt_len", "-", "2", "\n", "st_ed", ".", "append", "(", "(", "st", ",", "ed", ")", ")", "\n", "lens", ".", "append", "(", "ed", "-", "st", "+", "1", ")", "\n", "ids", ".", "append", "(", "id_", ")", "\n", "", "return", "lens", ",", "ids", ",", "st_ed", ",", "sentiment_counter", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset._decide_target": [[90, 96], ["None"], "methods", ["None"], ["", "def", "_decide_target", "(", "self", ",", "idiom", ",", "idx", ")", ":", "\n", "        ", "target", "=", "[", "idx", ",", "-", "100", "]", "\n", "if", "idiom", "in", "self", ".", "sentiments", "and", "idiom", "in", "self", ".", "filtered", ":", "\n", "            ", "sentiment", "=", "self", ".", "sentiments", "[", "idiom", "]", "\n", "target", "=", "[", "idx", ",", "sentiment", "]", "\n", "", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyDataset.__init__": [[100, 108], ["slide.ChengyuSlideDataset.__init__", "range"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "split", ",", "max_txt_len", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "max_txt_len", ",", "opts", ")", "\n", "\n", "if", "split", "==", "'train'", ":", "\n", "            ", "self", ".", "enlarged_candidates", "=", "[", "i", "for", "i", "in", "range", "(", "opts", ".", "len_idiom_vocab", ")", "if", "i", "in", "self", ".", "filtered", "]", "\n", "opts", ".", "enlarged_candidates", "=", "self", ".", "enlarged_candidates", "\n", "", "else", ":", "\n", "            ", "self", ".", "enlarged_candidates", "=", "opts", ".", "enlarged_candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyDataset.get_allowed_examples": [[109, 127], ["json.load().items", "set", "int", "set.update", "json.load", "reverse_index.items", "open"], "methods", ["None"], ["", "", "def", "get_allowed_examples", "(", "self", ",", "split", ",", "opts", ")", ":", "\n", "        ", "\"\"\"\n        For training dataset:\n        1) we can choose whether to add unlabelled data\n        2) we can choose whether to add labelled but not in training\n        :param opts:\n        :return:\n        \"\"\"", "\n", "reverse_index", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "json", ".", "load", "(", "open", "(", "f'{self.db_dir}/reverse_index.json'", ")", ")", ".", "items", "(", ")", ":", "\n", "            ", "k", "=", "int", "(", "k", ")", "\n", "if", "k", "<", "opts", ".", "len_idiom_vocab", ":", "\n", "                ", "if", "k", "in", "self", ".", "filtered", ":", "\n", "                    ", "reverse_index", "[", "k", "]", "=", "v", "\n", "\n", "", "", "", "allowed", "=", "set", "(", ")", "\n", "[", "allowed", ".", "update", "(", "v", ")", "for", "_", ",", "v", "in", "reverse_index", ".", "items", "(", ")", "]", "\n", "return", "allowed", ",", "reverse_index", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyDataset.__getitem__": [[128, 171], ["context_ids.index", "len", "slide.ChengyuSlideComposeOnlyDataset._decide_target", "torch.tensor", "torch.tensor", "torch.tensor", "len", "random.sample", "random.shuffle", "slide.ChengyuSlideComposeOnlyDataset.enlarged_candidates.index", "functools.reduce", "functools.reduce", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset._decide_target"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "id_", "=", "self", ".", "ids", "[", "i", "]", "\n", "st", ",", "ed", "=", "self", ".", "st_ed", "[", "i", "]", "\n", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "options", "=", "example", "[", "'options'", "]", "\n", "idiom", "=", "example", "[", "'idiom'", "]", "\n", "if", "len", "(", "options", ")", "==", "0", ":", "\n", "            ", "options", "=", "random", ".", "sample", "(", "self", ".", "idiom_ids", ",", "k", "=", "7", ")", "\n", "if", "idiom", "not", "in", "options", ":", "\n", "                ", "options", "[", "-", "1", "]", "=", "idiom", "\n", "", "random", ".", "shuffle", "(", "options", ")", "\n", "\n", "", "context_ids", "=", "example", "[", "'input_ids'", "]", "[", "st", ":", "ed", "]", "\n", "idiom_start", "=", "context_ids", ".", "index", "(", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "idiom_input_ids", "=", "self", ".", "idiom_input_ids", "[", "id_", "]", "\n", "idiom_len", "=", "len", "(", "idiom_input_ids", ")", "\n", "\n", "idx", "=", "-", "100", "if", "idiom", "not", "in", "self", ".", "enlarged_candidates", "else", "self", ".", "enlarged_candidates", ".", "index", "(", "idiom", ")", "\n", "target", "=", "self", ".", "_decide_target", "(", "idiom", ",", "idx", ")", "\n", "\n", "if", "self", ".", "use_context", ":", "\n", "            ", "input_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "context_ids", "[", ":", "idiom_start", "]", ",", "\n", "idiom_input_ids", ",", "\n", "context_ids", "[", "idiom_start", "+", "1", ":", "]", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "position", "=", "idiom_start", "+", "1", "\n", "", "else", ":", "\n", "            ", "input_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "idiom_input_ids", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "position", "=", "1", "\n", "", "assert", "len", "(", "input_ids", ")", "<=", "self", ".", "max_txt_len", "+", "idiom_len", "\n", "\n", "token_type_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "attention_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "token_type_ids", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "attention_mask", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "position", ",", "idiom_len", ",", "options", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyDataset.collate_fn": [[172, 194], ["map", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "max", "torch.arange().unsqueeze().repeat().clone", "enumerate", "more_itertools.unzip", "zip", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.arange().unsqueeze().repeat", "torch.arange", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange().unsqueeze", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "widths", ",", "options", ",", "targets", ")", "=", "map", "(", "list", ",", "\n", "unzip", "(", "inputs", ")", ")", "\n", "\n", "input_ids", "=", "pad_sequence", "(", "input_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "token_type_ids", "=", "pad_sequence", "(", "token_type_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "attn_masks", "=", "pad_sequence", "(", "attention_mask", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "\n", "width_max", "=", "max", "(", "widths", ")", "\n", "gather_index", "=", "torch", ".", "arange", "(", "0", ",", "width_max", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "1", ")", ".", "clone", "(", ")", "\n", "for", "i", ",", "(", "p", ",", "w", ")", "in", "enumerate", "(", "zip", "(", "positions", ",", "widths", ")", ")", ":", "\n", "            ", "gather_index", ".", "data", "[", "i", ",", ":", "w", "]", "=", "torch", ".", "arange", "(", "p", ",", "p", "+", "w", ",", "dtype", "=", "torch", ".", "long", ")", ".", "data", "\n", "\n", "", "batch", "=", "{", "'input_ids'", ":", "input_ids", ",", "\n", "'token_type_ids'", ":", "token_type_ids", ",", "\n", "'attention_mask'", ":", "attn_masks", ",", "\n", "'gather_index'", ":", "gather_index", ",", "\n", "'positions'", ":", "torch", ".", "tensor", "(", "positions", ")", ".", "long", "(", ")", ",", "\n", "'option_ids'", ":", "torch", ".", "tensor", "(", "options", ")", ".", "long", "(", ")", ",", "\n", "'targets'", ":", "torch", ".", "tensor", "(", "targets", ")", ".", "long", "(", ")", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyEvalDataset.__getitem__": [[198, 202], ["slide.ChengyuSlideComposeOnlyDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.__getitem__"], ["    ", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "qid", "=", "self", ".", "ids", "[", "i", "]", "\n", "tensors", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "return", "(", "qid", ",", "*", "tensors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyEvalDataset.collate_fn": [[203, 212], ["slide.ChengyuSlideComposeOnlyDataset.collate_fn", "qids.append", "slide.ChengyuSlideComposeOnlyDataset.collate_fn"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "qids", ",", "batch", "=", "[", "]", ",", "[", "]", "\n", "for", "id_", ",", "*", "tensors", "in", "inputs", ":", "\n", "            ", "qids", ".", "append", "(", "id_", ")", "\n", "batch", ".", "append", "(", "tensors", ")", "\n", "", "batch", "=", "ChengyuSlideComposeOnlyDataset", ".", "collate_fn", "(", "batch", ")", "\n", "batch", "[", "'qids'", "]", "=", "qids", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedDataset.__init__": [[216, 229], ["slide.ChengyuSlideDataset.__init__", "slide.ChengyuSlideComposeOnlyMaskedDataset.get_allowed_examples", "range", "range"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedDataset.get_allowed_examples"], ["    ", "def", "__init__", "(", "self", ",", "split", ",", "max_txt_len", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "max_txt_len", ",", "opts", ")", "\n", "self", ".", "allowed", ",", "self", ".", "reverse_index", "=", "self", ".", "get_allowed_examples", "(", "split", ",", "opts", ")", "\n", "\n", "if", "split", "==", "'train'", ":", "\n", "            ", "if", "not", "opts", ".", "use_unlabeled", ":", "\n", "                ", "self", ".", "enlarged_candidates", "=", "[", "i", "for", "i", "in", "range", "(", "opts", ".", "len_idiom_vocab", ")", "if", "i", "in", "self", ".", "filtered", "]", "\n", "", "else", ":", "\n", "# all the idioms without Slide labels are all considered", "\n", "                ", "self", ".", "enlarged_candidates", "=", "[", "i", "for", "i", "in", "range", "(", "opts", ".", "len_idiom_vocab", ")", "if", "i", "in", "self", ".", "reverse_index", "]", "\n", "", "opts", ".", "enlarged_candidates", "=", "self", ".", "enlarged_candidates", "\n", "", "else", ":", "\n", "            ", "self", ".", "enlarged_candidates", "=", "opts", ".", "enlarged_candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedDataset.get_allowed_examples": [[230, 256], ["json.load().items", "set", "int", "set.update", "json.load", "reverse_index.items", "open"], "methods", ["None"], ["", "", "def", "get_allowed_examples", "(", "self", ",", "split", ",", "opts", ")", ":", "\n", "        ", "\"\"\"\n        For training dataset:\n        1) we can choose whether to add unlabelled data\n        2) we can choose whether to add labelled but not in training\n        :param opts:\n        :return:\n        \"\"\"", "\n", "reverse_index", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "json", ".", "load", "(", "open", "(", "f'{self.db_dir}/reverse_index.json'", ")", ")", ".", "items", "(", ")", ":", "\n", "            ", "k", "=", "int", "(", "k", ")", "\n", "if", "k", "<", "opts", ".", "len_idiom_vocab", ":", "\n", "                ", "if", "split", "==", "'train'", ":", "\n", "                    ", "if", "opts", ".", "use_unlabeled", ":", "\n", "                        ", "if", "k", "in", "self", ".", "filtered", "or", "k", "in", "self", ".", "unlabeled", ":", "\n", "                            ", "reverse_index", "[", "k", "]", "=", "v", "\n", "", "", "else", ":", "\n", "                        ", "if", "k", "in", "self", ".", "filtered", ":", "\n", "                            ", "reverse_index", "[", "k", "]", "=", "v", "\n", "", "", "", "else", ":", "\n", "                    ", "if", "k", "in", "self", ".", "filtered", ":", "\n", "                        ", "reverse_index", "[", "k", "]", "=", "v", "\n", "\n", "", "", "", "", "allowed", "=", "set", "(", ")", "\n", "[", "allowed", ".", "update", "(", "v", ")", "for", "_", ",", "v", "in", "reverse_index", ".", "items", "(", ")", "]", "\n", "return", "allowed", ",", "reverse_index", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedDataset.__len__": [[257, 259], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedDataset.__getitem__": [[260, 322], ["context_ids.index", "len", "slide.ChengyuSlideComposeOnlyMaskedDataset._decide_target", "functools.reduce", "torch.tensor", "torch.tensor", "torch.tensor", "len", "random.sample", "random.shuffle", "slide.ChengyuSlideComposeOnlyMaskedDataset.enlarged_candidates.index", "functools.reduce", "functools.reduce", "len", "len", "len", "len", "len", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideDataset._decide_target"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "id_", "=", "self", ".", "ids", "[", "i", "]", "\n", "st", ",", "ed", "=", "self", ".", "st_ed", "[", "i", "]", "\n", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "options", "=", "example", "[", "'options'", "]", "\n", "idiom", "=", "example", "[", "'idiom'", "]", "\n", "if", "len", "(", "options", ")", "==", "0", ":", "\n", "            ", "options", "=", "random", ".", "sample", "(", "self", ".", "idiom_ids", ",", "k", "=", "7", ")", "\n", "if", "idiom", "not", "in", "options", ":", "\n", "                ", "options", "[", "-", "1", "]", "=", "idiom", "\n", "", "random", ".", "shuffle", "(", "options", ")", "\n", "#     target = options.index(idiom)", "\n", "\n", "", "context_ids", "=", "example", "[", "'input_ids'", "]", "[", "st", ":", "ed", "]", "\n", "idiom_start", "=", "context_ids", ".", "index", "(", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "idiom_input_ids", "=", "self", ".", "idiom_input_ids", "[", "id_", "]", "\n", "idiom_len", "=", "len", "(", "idiom_input_ids", ")", "\n", "# target = idiom", "\n", "\n", "idiom_masked_input_ids", "=", "[", "self", ".", "tokenizer", ".", "mask_token_id", "]", "*", "idiom_len", "\n", "\n", "idx", "=", "-", "100", "if", "idiom", "not", "in", "self", ".", "enlarged_candidates", "else", "self", ".", "enlarged_candidates", ".", "index", "(", "idiom", ")", "\n", "target", "=", "self", ".", "_decide_target", "(", "idiom", ",", "idx", ")", "\n", "\n", "input_masked_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "context_ids", "[", ":", "idiom_start", "]", ",", "\n", "idiom_masked_input_ids", ",", "\n", "context_ids", "[", "idiom_start", "+", "1", ":", "]", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "if", "self", ".", "use_context", ":", "\n", "            ", "input_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "context_ids", "[", ":", "idiom_start", "]", ",", "\n", "idiom_input_ids", ",", "\n", "context_ids", "[", "idiom_start", "+", "1", ":", "]", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "position", "=", "(", "idiom_start", "+", "1", ",", "idiom_start", "+", "1", ")", "\n", "attention_mask_literal", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "", "else", ":", "\n", "            ", "input_ids", "=", "reduce", "(", "operator", ".", "add", ",", "[", "\n", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", ",", "\n", "idiom_input_ids", ",", "\n", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "]", ")", "\n", "input_ids_len", "=", "len", "(", "input_ids", ")", "\n", "input_ids", "=", "input_ids", "+", "[", "self", ".", "tokenizer", ".", "pad_token_id", "]", "*", "(", "len", "(", "input_masked_ids", ")", "-", "input_ids_len", ")", "\n", "position", "=", "(", "1", ",", "idiom_start", "+", "1", ")", "\n", "attention_mask_literal", "=", "[", "1", "]", "*", "input_ids_len", "+", "[", "0", "]", "*", "(", "len", "(", "input_masked_ids", ")", "-", "input_ids_len", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "<=", "self", ".", "max_txt_len", "+", "idiom_len", "\n", "attention_mask_idiomatic", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "token_type_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", "\n", "input_masked_ids", "=", "torch", ".", "tensor", "(", "input_masked_ids", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "token_type_ids", ")", "\n", "# attention_mask = torch.tensor(attention_mask)", "\n", "return", "(", "input_ids", ",", "input_masked_ids", ")", ",", "token_type_ids", ",", "(", "torch", ".", "tensor", "(", "attention_mask_literal", ")", ",", "\n", "torch", ".", "tensor", "(", "attention_mask_idiomatic", ")", ")", ",", "position", ",", "idiom_len", ",", "options", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedDataset.collate_fn": [[323, 351], ["map", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "max", "torch.arange().unsqueeze().repeat().clone", "enumerate", "torch.arange().unsqueeze().repeat().clone", "enumerate", "more_itertools.unzip", "zip", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.arange().unsqueeze().repeat", "torch.arange", "torch.arange().unsqueeze().repeat", "torch.arange", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "(", "input_ids_tuple", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "widths", ",", "options", ",", "targets", ")", "=", "map", "(", "list", ",", "\n", "unzip", "(", "inputs", ")", ")", "\n", "\n", "input_ids", "=", "pad_sequence", "(", "[", "item", "[", "0", "]", "for", "item", "in", "input_ids_tuple", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "input_masked_ids", "=", "pad_sequence", "(", "[", "item", "[", "1", "]", "for", "item", "in", "input_ids_tuple", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "token_type_ids", "=", "pad_sequence", "(", "token_type_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "attn_masks_literal", "=", "pad_sequence", "(", "[", "item", "[", "0", "]", "for", "item", "in", "attention_mask", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "attn_masks_idiomatic", "=", "pad_sequence", "(", "[", "item", "[", "1", "]", "for", "item", "in", "attention_mask", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "\n", "width_max", "=", "max", "(", "widths", ")", "\n", "gather_index", "=", "torch", ".", "arange", "(", "0", ",", "width_max", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "1", ")", ".", "clone", "(", ")", "\n", "for", "i", ",", "(", "p", ",", "w", ")", "in", "enumerate", "(", "zip", "(", "positions", ",", "widths", ")", ")", ":", "\n", "            ", "gather_index", ".", "data", "[", "i", ",", ":", "w", "]", "=", "torch", ".", "arange", "(", "p", "[", "0", "]", ",", "p", "[", "0", "]", "+", "w", ",", "dtype", "=", "torch", ".", "long", ")", ".", "data", "\n", "\n", "", "gather_index_masked", "=", "torch", ".", "arange", "(", "0", ",", "width_max", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "1", ")", ".", "clone", "(", ")", "\n", "for", "i", ",", "(", "p", ",", "w", ")", "in", "enumerate", "(", "zip", "(", "positions", ",", "widths", ")", ")", ":", "\n", "            ", "gather_index_masked", ".", "data", "[", "i", ",", ":", "w", "]", "=", "torch", ".", "arange", "(", "p", "[", "1", "]", ",", "p", "[", "1", "]", "+", "w", ",", "dtype", "=", "torch", ".", "long", ")", ".", "data", "\n", "\n", "", "batch", "=", "{", "'input_ids'", ":", "torch", ".", "stack", "(", "[", "input_ids", ",", "input_masked_ids", "]", ")", ",", "\n", "'token_type_ids'", ":", "torch", ".", "stack", "(", "[", "token_type_ids", ",", "token_type_ids", "]", ")", ",", "\n", "'attention_mask'", ":", "torch", ".", "stack", "(", "[", "attn_masks_literal", ",", "attn_masks_idiomatic", "]", ")", ",", "\n", "'gather_index'", ":", "(", "gather_index", ",", "gather_index_masked", ")", ",", "\n", "'positions'", ":", "torch", ".", "tensor", "(", "positions", ")", ".", "long", "(", ")", ",", "\n", "'option_ids'", ":", "torch", ".", "tensor", "(", "options", ")", ".", "long", "(", ")", ",", "\n", "'targets'", ":", "torch", ".", "tensor", "(", "targets", ")", ".", "long", "(", ")", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedEvalDataset.__getitem__": [[355, 359], ["slide.ChengyuSlideComposeOnlyMaskedDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.__getitem__"], ["    ", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "qid", "=", "self", ".", "ids", "[", "i", "]", "\n", "tensors", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "return", "(", "qid", ",", "*", "tensors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.slide.ChengyuSlideComposeOnlyMaskedEvalDataset.collate_fn": [[360, 369], ["slide.ChengyuSlideComposeOnlyMaskedDataset.collate_fn", "qids.append", "slide.ChengyuSlideComposeOnlyMaskedDataset.collate_fn"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "qids", ",", "batch", "=", "[", "]", ",", "[", "]", "\n", "for", "id_", ",", "*", "tensors", "in", "inputs", ":", "\n", "            ", "qids", ".", "append", "(", "id_", ")", "\n", "batch", ".", "append", "(", "tensors", ")", "\n", "", "batch", "=", "ChengyuSlideComposeOnlyMaskedDataset", ".", "collate_fn", "(", "batch", ")", "\n", "batch", "[", "'qids'", "]", "=", "qids", "\n", "return", "batch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.masked.ChengyuMaskedDataset.__init__": [[14, 21], ["chengyubert.data.ChengyuLmdb.__init__", "set", "masked.ChengyuMaskedDataset.get_ids_and_lens", "int", "masked.ChengyuMaskedDataset.allowed.update", "json.load().items", "masked.ChengyuMaskedDataset.reverse_index.items", "int", "json.load", "open"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.get_ids_and_lens"], ["    ", "def", "__init__", "(", "self", ",", "split", ",", "max_txt_len", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "max_txt_len", ",", "opts", ")", "\n", "self", ".", "reverse_index", "=", "{", "int", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "json", ".", "load", "(", "open", "(", "f'{self.db_dir}/reverse_index.json'", ")", ")", ".", "items", "(", ")", "if", "\n", "int", "(", "k", ")", "<", "opts", ".", "len_idiom_vocab", "}", "\n", "self", ".", "allowed", "=", "set", "(", ")", "\n", "[", "self", ".", "allowed", ".", "update", "(", "v", ")", "for", "_", ",", "v", "in", "self", ".", "reverse_index", ".", "items", "(", ")", "]", "\n", "self", ".", "lens", ",", "self", ".", "ids", ",", "self", ".", "st_ed", "=", "self", ".", "get_ids_and_lens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.masked.ChengyuMaskedDataset.__len__": [[22, 24], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.masked.ChengyuMaskedDataset.get_ids_and_lens": [[25, 51], ["masked.ChengyuMaskedDataset.id2len.items", "st_ed.append", "lens.append", "ids.append", "min", "len", "max", "len", "len"], "methods", ["None"], ["", "def", "get_ids_and_lens", "(", "self", ")", ":", "\n", "        ", "lens", "=", "[", "]", "\n", "ids", "=", "[", "]", "\n", "st_ed", "=", "[", "]", "\n", "for", "id_", ",", "len_", "in", "self", ".", "id2len", ".", "items", "(", ")", ":", "\n", "            ", "if", "id_", "not", "in", "self", ".", "allowed", ":", "\n", "                ", "continue", "\n", "", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "position", "=", "example", "[", "'position'", "]", "\n", "input_ids", "=", "example", "[", "'input_ids'", "]", "\n", "half_length", "=", "self", ".", "max_txt_len", "//", "2", "\n", "if", "position", "<", "half_length", ":", "# cut at tail", "\n", "                ", "st", "=", "0", "\n", "ed", "=", "min", "(", "len", "(", "input_ids", ")", "+", "1", ",", "self", ".", "max_txt_len", "-", "2", ")", "\n", "", "elif", "len", "(", "input_ids", ")", "-", "position", "<", "half_length", ":", "# cut at head", "\n", "                ", "ed", "=", "len", "(", "input_ids", ")", "\n", "st", "=", "max", "(", "0", ",", "ed", "-", "(", "self", ".", "max_txt_len", "-", "2", ")", ")", "\n", "", "else", ":", "# cut at both sides", "\n", "                ", "st", "=", "position", "-", "(", "half_length", "-", "2", ")", "\n", "ed", "=", "position", "+", "half_length", "\n", "\n", "", "assert", "ed", "-", "st", "<=", "self", ".", "max_txt_len", "-", "2", "\n", "st_ed", ".", "append", "(", "(", "st", ",", "ed", ")", ")", "\n", "lens", ".", "append", "(", "ed", "-", "st", "+", "1", ")", "\n", "ids", ".", "append", "(", "id_", ")", "\n", "", "return", "lens", ",", "ids", ",", "st_ed", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.masked.ChengyuMaskedDataset.__getitem__": [[52, 95], ["torch.tensor.index", "torch.tensor", "torch.tensor", "torch.tensor", "len", "random.sample", "random.shuffle", "random.sample.index", "hasattr", "context_ids.index", "range", "len", "len", "len", "context_ids.insert", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "\"\"\"\n        [[txt, img1],\n         [txt, img2]]\n        \"\"\"", "\n", "id_", "=", "self", ".", "ids", "[", "i", "]", "\n", "st", ",", "ed", "=", "self", ".", "st_ed", "[", "i", "]", "\n", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "options", "=", "example", "[", "'options'", "]", "\n", "target", "=", "example", "[", "'target'", "]", "\n", "if", "len", "(", "options", ")", "==", "0", ":", "\n", "            ", "idiom", "=", "example", "[", "'idiom'", "]", "\n", "options", "=", "random", ".", "sample", "(", "self", ".", "idiom_ids", ",", "k", "=", "7", ")", "\n", "if", "idiom", "not", "in", "options", ":", "\n", "                ", "options", "[", "-", "1", "]", "=", "idiom", "\n", "", "random", ".", "shuffle", "(", "options", ")", "\n", "target", "=", "options", ".", "index", "(", "idiom", ")", "\n", "\n", "", "context_ids", "=", "example", "[", "'input_ids'", "]", "[", "st", ":", "ed", "]", "\n", "if", "hasattr", "(", "self", ".", "config", ",", "'structured'", ")", "and", "self", ".", "config", ".", "structured", ":", "\n", "            ", "idiom_start", "=", "context_ids", ".", "index", "(", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "for", "_", "in", "range", "(", "3", ")", ":", "\n", "                ", "context_ids", ".", "insert", "(", "idiom_start", ",", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "\n", "", "if", "len", "(", "context_ids", ")", ">", "self", ".", "max_txt_len", "-", "2", ":", "\n", "                ", "half_length", "=", "self", ".", "max_txt_len", "//", "2", "\n", "pop_length", "=", "len", "(", "context_ids", ")", "-", "(", "self", ".", "max_txt_len", "-", "2", ")", "\n", "if", "idiom_start", ">", "half_length", ":", "\n", "                    ", "context_ids", "=", "context_ids", "[", "pop_length", ":", "]", "\n", "", "else", ":", "\n", "                    ", "context_ids", "=", "context_ids", "[", ":", "-", "pop_length", "]", "\n", "\n", "", "", "", "input_ids", "=", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", "+", "context_ids", "+", "[", "self", ".", "tokenizer", ".", "sep_token_id", "]", "\n", "assert", "len", "(", "input_ids", ")", "<=", "self", ".", "max_txt_len", "\n", "\n", "position", "=", "input_ids", ".", "index", "(", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "token_type_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "attention_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "token_type_ids", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "attention_mask", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "position", ",", "options", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.masked.ChengyuMaskedDataset.collate_fn": [[96, 125], ["map", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence.sum().long", "torch.arange().unsqueeze().repeat().clone", "enumerate", "more_itertools.unzip", "zip", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.nn.utils.rnn.pad_sequence.sum", "torch.arange().unsqueeze().repeat", "torch.arange", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange().unsqueeze", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "options", ",", "targets", ")", "=", "map", "(", "list", ",", "unzip", "(", "inputs", ")", ")", "\n", "\n", "input_ids", "=", "pad_sequence", "(", "input_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "token_type_ids", "=", "pad_sequence", "(", "token_type_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "attn_masks", "=", "pad_sequence", "(", "attention_mask", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "\n", "lengths", "=", "attn_masks", ".", "sum", "(", "-", "1", ")", ".", "long", "(", ")", "\n", "width", "=", "5", "\n", "span", "=", "2", "*", "width", "+", "4", "\n", "gather_index", "=", "torch", ".", "arange", "(", "0", ",", "span", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "1", ")", ".", "clone", "(", ")", "\n", "for", "i", ",", "(", "p", ",", "l", ")", "in", "enumerate", "(", "zip", "(", "positions", ",", "lengths", ")", ")", ":", "\n", "            ", "if", "p", "<=", "width", ":", "\n", "                ", "left", ",", "right", "=", "1", ",", "1", "+", "span", "\n", "", "elif", "p", "+", "4", "+", "width", ">=", "l", ":", "\n", "                ", "left", ",", "right", "=", "l", "-", "span", ",", "l", "\n", "", "else", ":", "\n", "                ", "left", ",", "right", "=", "p", "-", "width", ",", "p", "+", "4", "+", "width", "\n", "", "gather_index", ".", "data", "[", "i", ",", ":", "]", "=", "torch", ".", "arange", "(", "left", ",", "right", ",", "dtype", "=", "torch", ".", "long", ")", ".", "data", "\n", "\n", "", "batch", "=", "{", "'input_ids'", ":", "input_ids", ",", "\n", "'token_type_ids'", ":", "token_type_ids", ",", "\n", "'attention_mask'", ":", "attn_masks", ",", "\n", "'gather_index'", ":", "gather_index", ",", "\n", "'positions'", ":", "torch", ".", "tensor", "(", "positions", ")", ".", "long", "(", ")", ",", "\n", "'option_ids'", ":", "torch", ".", "tensor", "(", "options", ")", ".", "long", "(", ")", ",", "\n", "'targets'", ":", "torch", ".", "tensor", "(", "targets", ")", ".", "long", "(", ")", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.masked.ChengyuMaskedEvalDataset.__getitem__": [[129, 133], ["masked.ChengyuMaskedDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.__getitem__"], ["    ", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "qid", "=", "self", ".", "ids", "[", "i", "]", "\n", "tensors", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "return", "(", "qid", ",", "*", "tensors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.masked.ChengyuMaskedEvalDataset.collate_fn": [[134, 143], ["masked.ChengyuMaskedDataset.collate_fn", "qids.append", "masked.ChengyuMaskedDataset.collate_fn"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "qids", ",", "batch", "=", "[", "]", ",", "[", "]", "\n", "for", "id_", ",", "*", "tensors", "in", "inputs", ":", "\n", "            ", "qids", ".", "append", "(", "id_", ")", "\n", "batch", ".", "append", "(", "tensors", ")", "\n", "", "batch", "=", "ChengyuMaskedDataset", ".", "collate_fn", "(", "batch", ")", "\n", "batch", "[", "'qids'", "]", "=", "qids", "\n", "return", "batch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.sequential.ChengyuSequentialDataset.__init__": [[14, 22], ["chengyubert.data.ChengyuLmdb.__init__", "set", "sequential.ChengyuSequentialDataset.tokenize_idioms", "sequential.ChengyuSequentialDataset.get_ids_and_lens", "int", "sequential.ChengyuSequentialDataset.allowed.update", "json.load().items", "sequential.ChengyuSequentialDataset.reverse_index.items", "int", "json.load", "open"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.tokenize_idioms", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.get_ids_and_lens"], ["    ", "def", "__init__", "(", "self", ",", "split", ",", "max_txt_len", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "max_txt_len", ",", "opts", ")", "\n", "self", ".", "reverse_index", "=", "{", "int", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "json", ".", "load", "(", "open", "(", "f'{self.db_dir}/reverse_index.json'", ")", ")", ".", "items", "(", ")", "if", "\n", "int", "(", "k", ")", "<", "opts", ".", "len_idiom_vocab", "}", "\n", "self", ".", "allowed", "=", "set", "(", ")", "\n", "[", "self", ".", "allowed", ".", "update", "(", "v", ")", "for", "_", ",", "v", "in", "self", ".", "reverse_index", ".", "items", "(", ")", "]", "\n", "self", ".", "idiom_input_ids", "=", "self", ".", "tokenize_idioms", "(", ")", "\n", "self", ".", "lens", ",", "self", ".", "ids", ",", "self", ".", "st_ed", "=", "self", ".", "get_ids_and_lens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.sequential.ChengyuSequentialDataset.__len__": [[23, 25], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.sequential.ChengyuSequentialDataset.tokenize_idioms": [[26, 33], ["sequential.ChengyuSequentialDataset.chengyu_vocab.items", "sequential.ChengyuSequentialDataset.tokenizer.tokenize", "sequential.ChengyuSequentialDataset.tokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.tokenize"], ["", "def", "tokenize_idioms", "(", "self", ")", ":", "\n", "        ", "idiom_ids", "=", "{", "}", "\n", "for", "idiom", ",", "idiom_id", "in", "self", ".", "chengyu_vocab", ".", "items", "(", ")", ":", "\n", "            ", "tokens", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "idiom", ")", "\n", "input_ids", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "idiom_ids", "[", "idiom_id", "]", "=", "input_ids", "\n", "", "return", "idiom_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.sequential.ChengyuSequentialDataset.get_ids_and_lens": [[34, 60], ["sequential.ChengyuSequentialDataset.id2len.items", "st_ed.append", "lens.append", "ids.append", "min", "len", "max", "len", "len"], "methods", ["None"], ["", "def", "get_ids_and_lens", "(", "self", ")", ":", "\n", "        ", "lens", "=", "[", "]", "\n", "ids", "=", "[", "]", "\n", "st_ed", "=", "[", "]", "\n", "for", "id_", ",", "len_", "in", "self", ".", "id2len", ".", "items", "(", ")", ":", "\n", "            ", "if", "id_", "not", "in", "self", ".", "allowed", ":", "\n", "                ", "continue", "\n", "", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "position", "=", "example", "[", "'position'", "]", "\n", "input_ids", "=", "example", "[", "'input_ids'", "]", "\n", "half_length", "=", "self", ".", "max_txt_len", "//", "2", "\n", "if", "position", "<", "half_length", ":", "# cut at tail", "\n", "                ", "st", "=", "0", "\n", "ed", "=", "min", "(", "len", "(", "input_ids", ")", "+", "1", ",", "self", ".", "max_txt_len", "-", "2", ")", "\n", "", "elif", "len", "(", "input_ids", ")", "-", "position", "<", "half_length", ":", "# cut at head", "\n", "                ", "ed", "=", "len", "(", "input_ids", ")", "\n", "st", "=", "max", "(", "0", ",", "ed", "-", "(", "self", ".", "max_txt_len", "-", "2", ")", ")", "\n", "", "else", ":", "# cut at both sides", "\n", "                ", "st", "=", "position", "-", "(", "half_length", "-", "2", ")", "\n", "ed", "=", "position", "+", "half_length", "\n", "\n", "", "assert", "ed", "-", "st", "<=", "self", ".", "max_txt_len", "-", "2", "\n", "st_ed", ".", "append", "(", "(", "st", ",", "ed", ")", ")", "\n", "lens", ".", "append", "(", "ed", "-", "st", "+", "1", ")", "\n", "ids", ".", "append", "(", "id_", ")", "\n", "", "return", "lens", ",", "ids", ",", "st_ed", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.sequential.ChengyuSequentialDataset.__getitem__": [[61, 97], ["context_ids.index", "len", "torch.tensor", "torch.tensor", "torch.tensor", "len", "random.sample", "random.shuffle", "random.sample.index", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "\"\"\"\n        [[txt, img1],\n         [txt, img2]]\n        \"\"\"", "\n", "id_", "=", "self", ".", "ids", "[", "i", "]", "\n", "st", ",", "ed", "=", "self", ".", "st_ed", "[", "i", "]", "\n", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "options", "=", "example", "[", "'options'", "]", "\n", "target", "=", "example", "[", "'target'", "]", "\n", "idiom", "=", "example", "[", "'idiom'", "]", "\n", "if", "len", "(", "options", ")", "==", "0", ":", "\n", "            ", "options", "=", "random", ".", "sample", "(", "self", ".", "idiom_ids", ",", "k", "=", "7", ")", "\n", "if", "idiom", "not", "in", "options", ":", "\n", "                ", "options", "[", "-", "1", "]", "=", "idiom", "\n", "", "random", ".", "shuffle", "(", "options", ")", "\n", "target", "=", "options", ".", "index", "(", "idiom", ")", "\n", "\n", "", "context_ids", "=", "example", "[", "'input_ids'", "]", "[", "st", ":", "ed", "]", "\n", "idiom_start", "=", "context_ids", ".", "index", "(", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "idiom_input_ids", "=", "self", ".", "idiom_input_ids", "[", "idiom", "]", "\n", "input_ids", "=", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", "+", "context_ids", "[", ":", "idiom_start", "]", "+", "idiom_input_ids", "+", "context_ids", "[", "\n", "idiom_start", "+", "1", ":", "]", "+", "[", "\n", "self", ".", "tokenizer", ".", "sep_token_id", "]", "\n", "assert", "len", "(", "input_ids", ")", "<=", "self", ".", "max_txt_len", "+", "len", "(", "idiom_input_ids", ")", "\n", "\n", "position", "=", "idiom_start", "+", "1", "\n", "width", "=", "len", "(", "idiom_input_ids", ")", "\n", "\n", "token_type_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "attention_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "token_type_ids", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "attention_mask", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "position", ",", "width", ",", "options", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.sequential.ChengyuSequentialDataset.collate_fn": [[98, 119], ["map", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "max", "torch.arange().unsqueeze().repeat().clone", "enumerate", "more_itertools.unzip", "zip", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.arange().unsqueeze().repeat", "torch.arange", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange().unsqueeze", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "positions", ",", "widths", ",", "options", ",", "targets", ")", "=", "map", "(", "list", ",", "unzip", "(", "inputs", ")", ")", "\n", "\n", "input_ids", "=", "pad_sequence", "(", "input_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "token_type_ids", "=", "pad_sequence", "(", "token_type_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "attn_masks", "=", "pad_sequence", "(", "attention_mask", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "\n", "width_max", "=", "max", "(", "widths", ")", "\n", "gather_index", "=", "torch", ".", "arange", "(", "0", ",", "width_max", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "1", ")", ".", "clone", "(", ")", "\n", "for", "i", ",", "(", "p", ",", "w", ")", "in", "enumerate", "(", "zip", "(", "positions", ",", "widths", ")", ")", ":", "\n", "            ", "gather_index", ".", "data", "[", "i", ",", ":", "w", "]", "=", "torch", ".", "arange", "(", "p", ",", "p", "+", "w", ",", "dtype", "=", "torch", ".", "long", ")", ".", "data", "\n", "\n", "", "batch", "=", "{", "'input_ids'", ":", "input_ids", ",", "\n", "'token_type_ids'", ":", "token_type_ids", ",", "\n", "'attention_mask'", ":", "attn_masks", ",", "\n", "'gather_index'", ":", "gather_index", ",", "\n", "'positions'", ":", "torch", ".", "tensor", "(", "positions", ")", ".", "long", "(", ")", ",", "\n", "'option_ids'", ":", "torch", ".", "tensor", "(", "options", ")", ".", "long", "(", ")", ",", "\n", "'targets'", ":", "torch", ".", "tensor", "(", "targets", ")", ".", "long", "(", ")", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.sequential.ChengyuSequentialEvalDataset.__getitem__": [[123, 127], ["sequential.ChengyuSequentialDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.__getitem__"], ["    ", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "qid", "=", "self", ".", "ids", "[", "i", "]", "\n", "tensors", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "return", "(", "qid", ",", "*", "tensors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.sequential.ChengyuSequentialEvalDataset.collate_fn": [[128, 137], ["sequential.ChengyuSequentialDataset.collate_fn", "qids.append", "sequential.ChengyuSequentialDataset.collate_fn"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "qids", ",", "batch", "=", "[", "]", ",", "[", "]", "\n", "for", "id_", ",", "*", "tensors", "in", "inputs", ":", "\n", "            ", "qids", ".", "append", "(", "id_", ")", "\n", "batch", ".", "append", "(", "tensors", ")", "\n", "", "batch", "=", "ChengyuSequentialDataset", ".", "collate_fn", "(", "batch", ")", "\n", "batch", "[", "'qids'", "]", "=", "qids", "\n", "return", "batch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__": [[14, 23], ["chengyubert.data.ChengyuLmdb.__init__", "set", "contextual.ChengyuContextualDataset.tokenize_idioms", "contextual.ChengyuContextualDataset.get_ids_and_lens", "int", "contextual.ChengyuContextualDataset.allowed.update", "json.load().items", "contextual.ChengyuContextualDataset.reverse_index.items", "int", "json.load", "open"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__init__", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.tokenize_idioms", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.get_ids_and_lens"], ["    ", "def", "__init__", "(", "self", ",", "split", ",", "max_txt_len", ",", "opts", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "split", ",", "max_txt_len", ",", "opts", ")", "\n", "self", ".", "reverse_index", "=", "{", "int", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "json", ".", "load", "(", "open", "(", "f'{self.db_dir}/reverse_index.json'", ")", ")", ".", "items", "(", ")", "if", "\n", "int", "(", "k", ")", "<", "opts", ".", "len_idiom_vocab", "}", "\n", "self", ".", "allowed", "=", "set", "(", ")", "\n", "[", "self", ".", "allowed", ".", "update", "(", "v", ")", "for", "_", ",", "v", "in", "self", ".", "reverse_index", ".", "items", "(", ")", "]", "\n", "self", ".", "idiom_input_ids", "=", "self", ".", "tokenize_idioms", "(", ")", "\n", "self", ".", "lens", ",", "self", ".", "ids", ",", "self", ".", "st_ed", "=", "self", ".", "get_ids_and_lens", "(", ")", "\n", "self", ".", "window_size", "=", "opts", ".", "window_size", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__len__": [[24, 26], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.tokenize_idioms": [[27, 34], ["contextual.ChengyuContextualDataset.chengyu_vocab.items", "contextual.ChengyuContextualDataset.tokenizer.tokenize", "contextual.ChengyuContextualDataset.tokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.None.preprocess.tokenize"], ["", "def", "tokenize_idioms", "(", "self", ")", ":", "\n", "        ", "idiom_ids", "=", "{", "}", "\n", "for", "idiom", ",", "idiom_id", "in", "self", ".", "chengyu_vocab", ".", "items", "(", ")", ":", "\n", "            ", "tokens", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "idiom", ")", "\n", "input_ids", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "idiom_ids", "[", "idiom_id", "]", "=", "input_ids", "\n", "", "return", "idiom_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.get_ids_and_lens": [[35, 61], ["contextual.ChengyuContextualDataset.id2len.items", "st_ed.append", "lens.append", "ids.append", "min", "len", "max", "len", "len"], "methods", ["None"], ["", "def", "get_ids_and_lens", "(", "self", ")", ":", "\n", "        ", "lens", "=", "[", "]", "\n", "ids", "=", "[", "]", "\n", "st_ed", "=", "[", "]", "\n", "for", "id_", ",", "len_", "in", "self", ".", "id2len", ".", "items", "(", ")", ":", "\n", "            ", "if", "id_", "not", "in", "self", ".", "allowed", ":", "\n", "                ", "continue", "\n", "", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "position", "=", "example", "[", "'position'", "]", "\n", "input_ids", "=", "example", "[", "'input_ids'", "]", "\n", "half_length", "=", "self", ".", "max_txt_len", "//", "2", "\n", "if", "position", "<", "half_length", ":", "# cut at tail", "\n", "                ", "st", "=", "0", "\n", "ed", "=", "min", "(", "len", "(", "input_ids", ")", "+", "1", ",", "self", ".", "max_txt_len", "-", "2", ")", "\n", "", "elif", "len", "(", "input_ids", ")", "-", "position", "<", "half_length", ":", "# cut at head", "\n", "                ", "ed", "=", "len", "(", "input_ids", ")", "\n", "st", "=", "max", "(", "0", ",", "ed", "-", "(", "self", ".", "max_txt_len", "-", "2", ")", ")", "\n", "", "else", ":", "# cut at both sides", "\n", "                ", "st", "=", "position", "-", "(", "half_length", "-", "2", ")", "\n", "ed", "=", "position", "+", "half_length", "\n", "\n", "", "assert", "ed", "-", "st", "<=", "self", ".", "max_txt_len", "-", "2", "\n", "st_ed", ".", "append", "(", "(", "st", ",", "ed", ")", ")", "\n", "lens", ".", "append", "(", "ed", "-", "st", "+", "1", ")", "\n", "ids", ".", "append", "(", "id_", ")", "\n", "", "return", "lens", ",", "ids", ",", "st_ed", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.__getitem__": [[62, 105], ["context_ids.index", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "len", "random.sample", "random.shuffle", "random.sample.index", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "\"\"\"\n        [[txt, img1],\n         [txt, img2]]\n        \"\"\"", "\n", "id_", "=", "self", ".", "ids", "[", "i", "]", "\n", "st", ",", "ed", "=", "self", ".", "st_ed", "[", "i", "]", "\n", "example", "=", "self", ".", "db", "[", "id_", "]", "\n", "options", "=", "example", "[", "'options'", "]", "\n", "target", "=", "example", "[", "'target'", "]", "\n", "idiom", "=", "example", "[", "'idiom'", "]", "\n", "if", "len", "(", "options", ")", "==", "0", ":", "\n", "            ", "options", "=", "random", ".", "sample", "(", "self", ".", "idiom_ids", ",", "k", "=", "7", ")", "\n", "if", "idiom", "not", "in", "options", ":", "\n", "                ", "options", "[", "-", "1", "]", "=", "idiom", "\n", "", "random", ".", "shuffle", "(", "options", ")", "\n", "target", "=", "options", ".", "index", "(", "idiom", ")", "\n", "\n", "", "context_ids", "=", "example", "[", "'input_ids'", "]", "[", "st", ":", "ed", "]", "\n", "idiom_start", "=", "context_ids", ".", "index", "(", "self", ".", "tokenizer", ".", "mask_token_id", ")", "\n", "idiom_input_ids", "=", "self", ".", "idiom_input_ids", "[", "idiom", "]", "\n", "width", "=", "len", "(", "idiom_input_ids", ")", "\n", "input_ids", "=", "[", "self", ".", "tokenizer", ".", "cls_token_id", "]", "+", "context_ids", "[", ":", "idiom_start", "]", "+", "idiom_input_ids", "+", "context_ids", "[", "\n", "idiom_start", "+", "1", ":", "]", "+", "[", "\n", "self", ".", "tokenizer", ".", "sep_token_id", "]", "\n", "assert", "len", "(", "input_ids", ")", "<=", "self", ".", "max_txt_len", "+", "width", "\n", "\n", "input_len", "=", "len", "(", "input_ids", ")", "\n", "position", "=", "idiom_start", "+", "1", "\n", "left_start", "=", "position", "-", "self", ".", "window_size", "if", "self", ".", "window_size", "<", "position", "else", "1", "\n", "left_end", "=", "position", "\n", "right_start", "=", "position", "+", "width", "\n", "right_end", "=", "right_start", "+", "self", ".", "window_size", "if", "right_start", "+", "self", ".", "window_size", "<", "input_len", "-", "1", "else", "input_len", "\n", "\n", "boundary_pairs", "=", "(", "(", "left_start", ",", "left_end", ")", ",", "(", "right_start", ",", "right_end", ")", ")", "\n", "\n", "token_type_ids", "=", "[", "0", "]", "*", "input_len", "\n", "attention_mask", "=", "[", "1", "]", "*", "input_len", "\n", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "token_type_ids", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "attention_mask", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "position", ",", "width", ",", "boundary_pairs", ",", "options", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualDataset.collate_fn": [[106, 140], ["map", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "max", "max", "torch.arange().unsqueeze().repeat().clone", "torch.arange().unsqueeze().repeat().clone", "enumerate", "more_itertools.unzip", "zip", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.arange().unsqueeze().repeat", "torch.arange().unsqueeze().repeat", "torch.arange", "torch.cat", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "positions", ",", "widths", ",", "boundary_pairs", ",", "\n", "options", ",", "targets", ")", "=", "map", "(", "list", ",", "unzip", "(", "inputs", ")", ")", "\n", "\n", "input_ids", "=", "pad_sequence", "(", "input_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "token_type_ids", "=", "pad_sequence", "(", "token_type_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "attn_masks", "=", "pad_sequence", "(", "attention_mask", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "\n", "width_max", "=", "max", "(", "widths", ")", "\n", "context_width_max", "=", "max", "(", "\n", "[", "left_end", "-", "left_start", "+", "right_end", "-", "right_start", "for", "(", "(", "left_start", ",", "left_end", ")", ",", "(", "right_start", ",", "right_end", ")", ")", "in", "\n", "boundary_pairs", "]", ")", "\n", "gather_index", "=", "torch", ".", "arange", "(", "0", ",", "width_max", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "1", ")", ".", "clone", "(", ")", "\n", "context_gather_index", "=", "torch", ".", "arange", "(", "0", ",", "context_width_max", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "len", "(", "inputs", ")", ",", "\n", "1", ")", ".", "clone", "(", ")", "\n", "for", "i", ",", "(", "p", ",", "w", ",", "(", "(", "left_start", ",", "left_end", ")", ",", "(", "right_start", ",", "right_end", ")", ")", ")", "in", "enumerate", "(", "\n", "zip", "(", "positions", ",", "widths", ",", "boundary_pairs", ")", ")", ":", "\n", "            ", "gather_index", ".", "data", "[", "i", ",", ":", "w", "]", "=", "torch", ".", "arange", "(", "p", ",", "p", "+", "w", ",", "dtype", "=", "torch", ".", "long", ")", ".", "data", "\n", "\n", "cw", "=", "left_end", "-", "left_start", "+", "right_end", "-", "right_start", "\n", "context_gather_index", ".", "data", "[", "i", ",", ":", "cw", "]", "=", "torch", ".", "cat", "(", "[", "torch", ".", "arange", "(", "left_start", ",", "left_end", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "arange", "(", "right_start", ",", "right_end", ",", "dtype", "=", "torch", ".", "long", ")", "]", ")", ".", "data", "\n", "\n", "", "batch", "=", "{", "'input_ids'", ":", "input_ids", ",", "\n", "'token_type_ids'", ":", "token_type_ids", ",", "\n", "'attention_mask'", ":", "attn_masks", ",", "\n", "'gather_index'", ":", "gather_index", ",", "\n", "'context_gather_index'", ":", "context_gather_index", ",", "\n", "'positions'", ":", "torch", ".", "tensor", "(", "positions", ")", ".", "long", "(", ")", ",", "\n", "'option_ids'", ":", "torch", ".", "tensor", "(", "options", ")", ".", "long", "(", ")", ",", "\n", "'targets'", ":", "torch", ".", "tensor", "(", "targets", ")", ".", "long", "(", ")", "}", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.__getitem__": [[144, 148], ["contextual.ChengyuContextualDataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.__getitem__"], ["    ", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "qid", "=", "self", ".", "ids", "[", "i", "]", "\n", "tensors", "=", "super", "(", ")", ".", "__getitem__", "(", "i", ")", "\n", "return", "(", "qid", ",", "*", "tensors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn": [[149, 158], ["contextual.ChengyuContextualDataset.collate_fn", "qids.append", "contextual.ChengyuContextualDataset.collate_fn"], "methods", ["home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn", "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.contextual.ChengyuContextualEvalDataset.collate_fn"], ["", "@", "staticmethod", "\n", "def", "collate_fn", "(", "inputs", ")", ":", "\n", "        ", "qids", ",", "batch", "=", "[", "]", ",", "[", "]", "\n", "for", "id_", ",", "*", "tensors", "in", "inputs", ":", "\n", "            ", "qids", ".", "append", "(", "id_", ")", "\n", "batch", ".", "append", "(", "tensors", ")", "\n", "", "batch", "=", "ChengyuContextualDataset", ".", "collate_fn", "(", "batch", ")", "\n", "batch", "[", "'qids'", "]", "=", "qids", "\n", "return", "batch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.VisualJoyce_ChengyuBERT.dataset.__init__.register_dataset": [[7, 34], ["ValueError"], "function", ["None"], []]}