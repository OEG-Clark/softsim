{"home.repos.pwc.inspect_result.microsoft_regionclip.None.setup.get_version": [[18, 39], ["os.path.join", "open().readlines", "[].strip().strip", "os.getenv", "os.path.abspath", "os.getenv", "datetime.today().strftime", "new_init_py.append", "os.path.dirname", "open", "l.strip", "[].strip", "open", "f.write", "l.startswith", "datetime.today", "l.startswith", "version_line.split"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["def", "get_version", "(", ")", ":", "\n", "    ", "init_py_path", "=", "path", ".", "join", "(", "path", ".", "abspath", "(", "path", ".", "dirname", "(", "__file__", ")", ")", ",", "\"detectron2\"", ",", "\"__init__.py\"", ")", "\n", "init_py", "=", "open", "(", "init_py_path", ",", "\"r\"", ")", ".", "readlines", "(", ")", "\n", "version_line", "=", "[", "l", ".", "strip", "(", ")", "for", "l", "in", "init_py", "if", "l", ".", "startswith", "(", "\"__version__\"", ")", "]", "[", "0", "]", "\n", "version", "=", "version_line", ".", "split", "(", "\"=\"", ")", "[", "-", "1", "]", ".", "strip", "(", ")", ".", "strip", "(", "\"'\\\"\"", ")", "\n", "\n", "# The following is used to build release packages.", "\n", "# Users should never use it.", "\n", "suffix", "=", "os", ".", "getenv", "(", "\"D2_VERSION_SUFFIX\"", ",", "\"\"", ")", "\n", "version", "=", "version", "+", "suffix", "\n", "if", "os", ".", "getenv", "(", "\"BUILD_NIGHTLY\"", ",", "\"0\"", ")", "==", "\"1\"", ":", "\n", "        ", "from", "datetime", "import", "datetime", "\n", "\n", "date_str", "=", "datetime", ".", "today", "(", ")", ".", "strftime", "(", "\"%y%m%d\"", ")", "\n", "version", "=", "version", "+", "\".dev\"", "+", "date_str", "\n", "\n", "new_init_py", "=", "[", "l", "for", "l", "in", "init_py", "if", "not", "l", ".", "startswith", "(", "\"__version__\"", ")", "]", "\n", "new_init_py", ".", "append", "(", "'__version__ = \"{}\"\\n'", ".", "format", "(", "version", ")", ")", "\n", "with", "open", "(", "init_py_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"\"", ".", "join", "(", "new_init_py", ")", ")", "\n", "", "", "return", "version", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.None.setup.get_extensions": [[41, 150], ["os.path.dirname", "os.path.join", "os.path.join", "glob.glob", "os.path.abspath", "os.path.join", "hasattr", "torch.utils.hipify.hipify_python.hipify", "shutil.copy", "shutil.copy", "extension", "int", "glob.glob", "glob.glob", "glob.glob", "glob.glob", "torch.cuda.is_available", "os.getenv", "os.environ.get", "torch.utils.hipify.__version__.split", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "extra_compile_args[].append", "s.endswith"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "get_extensions", "(", ")", ":", "\n", "    ", "this_dir", "=", "path", ".", "dirname", "(", "path", ".", "abspath", "(", "__file__", ")", ")", "\n", "extensions_dir", "=", "path", ".", "join", "(", "this_dir", ",", "\"detectron2\"", ",", "\"layers\"", ",", "\"csrc\"", ")", "\n", "\n", "main_source", "=", "path", ".", "join", "(", "extensions_dir", ",", "\"vision.cpp\"", ")", "\n", "sources", "=", "glob", ".", "glob", "(", "path", ".", "join", "(", "extensions_dir", ",", "\"**\"", ",", "\"*.cpp\"", ")", ")", "\n", "\n", "from", "torch", ".", "utils", ".", "cpp_extension", "import", "ROCM_HOME", "\n", "\n", "is_rocm_pytorch", "=", "(", "\n", "True", "if", "(", "(", "torch", ".", "version", ".", "hip", "is", "not", "None", ")", "and", "(", "ROCM_HOME", "is", "not", "None", ")", ")", "else", "False", "\n", ")", "\n", "\n", "hipify_ver", "=", "(", "\n", "[", "int", "(", "x", ")", "for", "x", "in", "torch", ".", "utils", ".", "hipify", ".", "__version__", ".", "split", "(", "\".\"", ")", "]", "\n", "if", "hasattr", "(", "torch", ".", "utils", ".", "hipify", ",", "\"__version__\"", ")", "\n", "else", "[", "0", ",", "0", ",", "0", "]", "\n", ")", "\n", "\n", "if", "is_rocm_pytorch", "and", "hipify_ver", "<", "[", "1", ",", "0", ",", "0", "]", ":", "# TODO not needed since pt1.8", "\n", "\n", "# Earlier versions of hipification and extension modules were not", "\n", "# transparent, i.e. would require an explicit call to hipify, and the", "\n", "# hipification would introduce \"hip\" subdirectories, possibly changing", "\n", "# the relationship between source and header files.", "\n", "# This path is maintained for backwards compatibility.", "\n", "\n", "        ", "hipify_python", ".", "hipify", "(", "\n", "project_directory", "=", "this_dir", ",", "\n", "output_directory", "=", "this_dir", ",", "\n", "includes", "=", "\"/detectron2/layers/csrc/*\"", ",", "\n", "show_detailed", "=", "True", ",", "\n", "is_pytorch_extension", "=", "True", ",", "\n", ")", "\n", "\n", "source_cuda", "=", "glob", ".", "glob", "(", "path", ".", "join", "(", "extensions_dir", ",", "\"**\"", ",", "\"hip\"", ",", "\"*.hip\"", ")", ")", "+", "glob", ".", "glob", "(", "\n", "path", ".", "join", "(", "extensions_dir", ",", "\"hip\"", ",", "\"*.hip\"", ")", "\n", ")", "\n", "\n", "shutil", ".", "copy", "(", "\n", "\"detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_utils.h\"", ",", "\n", "\"detectron2/layers/csrc/box_iou_rotated/hip/box_iou_rotated_utils.h\"", ",", "\n", ")", "\n", "shutil", ".", "copy", "(", "\n", "\"detectron2/layers/csrc/deformable/deform_conv.h\"", ",", "\n", "\"detectron2/layers/csrc/deformable/hip/deform_conv.h\"", ",", "\n", ")", "\n", "\n", "sources", "=", "[", "main_source", "]", "+", "sources", "\n", "sources", "=", "[", "\n", "s", "\n", "for", "s", "in", "sources", "\n", "if", "not", "is_rocm_pytorch", "or", "torch_ver", "<", "[", "1", ",", "7", "]", "or", "not", "s", ".", "endswith", "(", "\"hip/vision.cpp\"", ")", "\n", "]", "\n", "\n", "", "else", ":", "\n", "\n", "# common code between cuda and rocm platforms,", "\n", "# for hipify version [1,0,0] and later.", "\n", "\n", "        ", "source_cuda", "=", "glob", ".", "glob", "(", "path", ".", "join", "(", "extensions_dir", ",", "\"**\"", ",", "\"*.cu\"", ")", ")", "+", "glob", ".", "glob", "(", "\n", "path", ".", "join", "(", "extensions_dir", ",", "\"*.cu\"", ")", "\n", ")", "\n", "\n", "sources", "=", "[", "main_source", "]", "+", "sources", "\n", "\n", "", "extension", "=", "CppExtension", "\n", "\n", "extra_compile_args", "=", "{", "\"cxx\"", ":", "[", "]", "}", "\n", "define_macros", "=", "[", "]", "\n", "\n", "if", "(", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "(", "(", "CUDA_HOME", "is", "not", "None", ")", "or", "is_rocm_pytorch", ")", ")", "or", "os", ".", "getenv", "(", "\n", "\"FORCE_CUDA\"", ",", "\"0\"", "\n", ")", "==", "\"1\"", ":", "\n", "        ", "extension", "=", "CUDAExtension", "\n", "sources", "+=", "source_cuda", "\n", "\n", "if", "not", "is_rocm_pytorch", ":", "\n", "            ", "define_macros", "+=", "[", "(", "\"WITH_CUDA\"", ",", "None", ")", "]", "\n", "extra_compile_args", "[", "\"nvcc\"", "]", "=", "[", "\n", "\"-O3\"", ",", "\n", "\"-DCUDA_HAS_FP16=1\"", ",", "\n", "\"-D__CUDA_NO_HALF_OPERATORS__\"", ",", "\n", "\"-D__CUDA_NO_HALF_CONVERSIONS__\"", ",", "\n", "\"-D__CUDA_NO_HALF2_OPERATORS__\"", ",", "\n", "]", "\n", "", "else", ":", "\n", "            ", "define_macros", "+=", "[", "(", "\"WITH_HIP\"", ",", "None", ")", "]", "\n", "extra_compile_args", "[", "\"nvcc\"", "]", "=", "[", "]", "\n", "\n", "", "if", "torch_ver", "<", "[", "1", ",", "7", "]", ":", "\n", "# supported by https://github.com/pytorch/pytorch/pull/43931", "\n", "            ", "CC", "=", "os", ".", "environ", ".", "get", "(", "\"CC\"", ",", "None", ")", "\n", "if", "CC", "is", "not", "None", ":", "\n", "                ", "extra_compile_args", "[", "\"nvcc\"", "]", ".", "append", "(", "\"-ccbin={}\"", ".", "format", "(", "CC", ")", ")", "\n", "\n", "", "", "", "include_dirs", "=", "[", "extensions_dir", "]", "\n", "\n", "ext_modules", "=", "[", "\n", "extension", "(", "\n", "\"detectron2._C\"", ",", "\n", "sources", ",", "\n", "include_dirs", "=", "include_dirs", ",", "\n", "define_macros", "=", "define_macros", ",", "\n", "extra_compile_args", "=", "extra_compile_args", ",", "\n", ")", "\n", "]", "\n", "\n", "return", "ext_modules", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.None.setup.get_model_zoo_configs": [[152, 183], ["os.path.join", "os.path.join", "os.path.exists", "os.path.dirname", "os.path.dirname", "os.path.islink", "os.path.exists", "glob.glob", "glob.glob", "os.path.realpath", "os.path.realpath", "os.unlink", "os.path.isdir", "os.symlink", "shutil.rmtree", "shutil.copytree"], "function", ["None"], ["", "def", "get_model_zoo_configs", "(", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Return a list of configs to include in package for model zoo. Copy over these configs inside\n    detectron2/model_zoo.\n    \"\"\"", "\n", "\n", "# Use absolute paths while symlinking.", "\n", "source_configs_dir", "=", "path", ".", "join", "(", "path", ".", "dirname", "(", "path", ".", "realpath", "(", "__file__", ")", ")", ",", "\"configs\"", ")", "\n", "destination", "=", "path", ".", "join", "(", "\n", "path", ".", "dirname", "(", "path", ".", "realpath", "(", "__file__", ")", ")", ",", "\"detectron2\"", ",", "\"model_zoo\"", ",", "\"configs\"", "\n", ")", "\n", "# Symlink the config directory inside package to have a cleaner pip install.", "\n", "\n", "# Remove stale symlink/directory from a previous build.", "\n", "if", "path", ".", "exists", "(", "source_configs_dir", ")", ":", "\n", "        ", "if", "path", ".", "islink", "(", "destination", ")", ":", "\n", "            ", "os", ".", "unlink", "(", "destination", ")", "\n", "", "elif", "path", ".", "isdir", "(", "destination", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "destination", ")", "\n", "\n", "", "", "if", "not", "path", ".", "exists", "(", "destination", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "os", ".", "symlink", "(", "source_configs_dir", ",", "destination", ")", "\n", "", "except", "OSError", ":", "\n", "# Fall back to copying if symlink fails: ex. on Windows.", "\n", "            ", "shutil", ".", "copytree", "(", "source_configs_dir", ",", "destination", ")", "\n", "\n", "", "", "config_paths", "=", "glob", ".", "glob", "(", "\"configs/**/*.yaml\"", ",", "recursive", "=", "True", ")", "+", "glob", ".", "glob", "(", "\n", "\"configs/**/*.py\"", ",", "recursive", "=", "True", "\n", ")", "\n", "return", "config_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.lazyconfig_train_net.do_test": [[35, 42], ["detectron2.evaluation.inference_on_dataset", "detectron2.evaluation.print_csv_format", "detectron2.config.instantiate", "detectron2.config.instantiate"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.inference_on_dataset", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.print_csv_format", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate"], ["def", "do_test", "(", "cfg", ",", "model", ")", ":", "\n", "    ", "if", "\"evaluator\"", "in", "cfg", ".", "dataloader", ":", "\n", "        ", "ret", "=", "inference_on_dataset", "(", "\n", "model", ",", "instantiate", "(", "cfg", ".", "dataloader", ".", "test", ")", ",", "instantiate", "(", "cfg", ".", "dataloader", ".", "evaluator", ")", "\n", ")", "\n", "print_csv_format", "(", "ret", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.lazyconfig_train_net.do_train": [[44, 120], ["detectron2.config.instantiate", "logging.getLogger", "logging.getLogger.info", "detectron2.engine.defaults.create_ddp_model.to", "detectron2.config.instantiate", "detectron2.config.instantiate", "detectron2.engine.defaults.create_ddp_model", "detectron2.checkpoint.DetectionCheckpointer", "trainer.register_hooks", "detectron2.checkpoint.DetectionCheckpointer.resume_or_load", "detectron2.checkpoint.DetectionCheckpointer.resume_or_load", "trainer.train", "detectron2.checkpoint.DetectionCheckpointer.has_checkpoint", "detectron2.engine.hooks.IterationTimer", "detectron2.engine.hooks.LRScheduler", "detectron2.engine.hooks.EvalHook", "detectron2.utils.comm.is_main_process", "detectron2.engine.hooks.PeriodicCheckpointer", "detectron2.utils.comm.is_main_process", "detectron2.engine.hooks.PeriodicWriter", "detectron2.config.instantiate", "lazyconfig_train_net.do_test", "detectron2.engine.default_writers"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.create_ddp_model", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.register_hooks", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.do_test", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.default_writers"], ["", "", "def", "do_train", "(", "args", ",", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        cfg: an object with the following attributes:\n            model: instantiate to a module\n            dataloader.{train,test}: instantiate to dataloaders\n            dataloader.evaluator: instantiate to evaluator for test set\n            optimizer: instantaite to an optimizer\n            lr_multiplier: instantiate to a fvcore scheduler\n            train: other misc config defined in `common_train.py`, including:\n                output_dir (str)\n                init_checkpoint (str)\n                amp.enabled (bool)\n                max_iter (int)\n                eval_period, log_period (int)\n                device (str)\n                checkpointer (dict)\n                ddp (dict)\n    \"\"\"", "\n", "model", "=", "instantiate", "(", "cfg", ".", "model", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", "\"detectron2\"", ")", "\n", "logger", ".", "info", "(", "\"Model:\\n{}\"", ".", "format", "(", "model", ")", ")", "\n", "model", ".", "to", "(", "cfg", ".", "train", ".", "device", ")", "\n", "\n", "cfg", ".", "optimizer", ".", "params", ".", "model", "=", "model", "\n", "optim", "=", "instantiate", "(", "cfg", ".", "optimizer", ")", "\n", "\n", "train_loader", "=", "instantiate", "(", "cfg", ".", "dataloader", ".", "train", ")", "\n", "\n", "model", "=", "create_ddp_model", "(", "model", ",", "**", "cfg", ".", "train", ".", "ddp", ")", "\n", "trainer", "=", "(", "AMPTrainer", "if", "cfg", ".", "train", ".", "amp", ".", "enabled", "else", "SimpleTrainer", ")", "(", "model", ",", "train_loader", ",", "optim", ")", "\n", "checkpointer", "=", "DetectionCheckpointer", "(", "\n", "model", ",", "\n", "cfg", ".", "train", ".", "output_dir", ",", "\n", "optimizer", "=", "optim", ",", "\n", "trainer", "=", "trainer", ",", "\n", "# bb_rpn_weights=True, ", "\n", ")", "\n", "trainer", ".", "register_hooks", "(", "\n", "[", "\n", "hooks", ".", "IterationTimer", "(", ")", ",", "\n", "hooks", ".", "LRScheduler", "(", "scheduler", "=", "instantiate", "(", "cfg", ".", "lr_multiplier", ")", ")", ",", "\n", "hooks", ".", "PeriodicCheckpointer", "(", "checkpointer", ",", "**", "cfg", ".", "train", ".", "checkpointer", ")", "\n", "if", "comm", ".", "is_main_process", "(", ")", "\n", "else", "None", ",", "\n", "hooks", ".", "EvalHook", "(", "cfg", ".", "train", ".", "eval_period", ",", "lambda", ":", "do_test", "(", "cfg", ",", "model", ")", ")", ",", "\n", "hooks", ".", "PeriodicWriter", "(", "\n", "default_writers", "(", "cfg", ".", "train", ".", "output_dir", ",", "cfg", ".", "train", ".", "max_iter", ")", ",", "\n", "period", "=", "cfg", ".", "train", ".", "log_period", ",", "\n", ")", "\n", "if", "comm", ".", "is_main_process", "(", ")", "\n", "else", "None", ",", "\n", "]", "\n", ")", "\n", "\n", "checkpointer", ".", "resume_or_load", "(", "\n", "cfg", ".", "train", ".", "init_checkpoint", ",", "resume", "=", "args", ".", "resume", ")", "\n", "\n", "checkpointer", ".", "bb_rpn_weights", "=", "True", "\n", "checkpointer", ".", "resume_or_load", "(", "\n", "cfg", ".", "train", ".", "bb_rpn_checkpoint", ",", "resume", "=", "False", ")", "\n", "\n", "checkpointer", ".", "bb_rpn_weights", "=", "False", "\n", "\n", "# DetectionCheckpointer(model, cfg.train.output_dir).resume_or_load(", "\n", "#     cfg.train.init_checkpoint, resume=args.resume)", "\n", "# DetectionCheckpointer(model, cfg.train.output_dir, bb_rpn_weights=True).resume_or_load(", "\n", "#     cfg.train.bb_rpn_checkpoint, resume=False)", "\n", "\n", "if", "args", ".", "resume", "and", "checkpointer", ".", "has_checkpoint", "(", ")", ":", "\n", "# The checkpoint stores the training iteration that just finished, thus we start", "\n", "# at the next iteration", "\n", "        ", "start_iter", "=", "trainer", ".", "iter", "+", "1", "\n", "", "else", ":", "\n", "        ", "start_iter", "=", "0", "\n", "", "trainer", ".", "train", "(", "start_iter", ",", "cfg", ".", "train", ".", "max_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.lazyconfig_train_net.main": [[122, 144], ["detectron2.config.LazyConfig.load", "detectron2.config.LazyConfig.apply_overrides", "detectron2.engine.default_setup", "detectron2.config.instantiate", "detectron2.engine.defaults.create_ddp_model.to", "detectron2.engine.defaults.create_ddp_model", "detectron2.checkpoint.DetectionCheckpointer().load", "detectron2.checkpoint.DetectionCheckpointer().load", "print", "lazyconfig_train_net.do_train", "lazyconfig_train_net.do_test", "detectron2.checkpoint.DetectionCheckpointer", "detectron2.checkpoint.DetectionCheckpointer"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.apply_overrides", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.default_setup", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.create_ddp_model", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.do_train", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.do_test"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "LazyConfig", ".", "load", "(", "args", ".", "config_file", ")", "\n", "cfg", "=", "LazyConfig", ".", "apply_overrides", "(", "cfg", ",", "args", ".", "opts", ")", "\n", "default_setup", "(", "cfg", ",", "args", ")", "\n", "\n", "if", "args", ".", "eval_only", ":", "\n", "        ", "model", "=", "instantiate", "(", "cfg", ".", "model", ")", "\n", "model", ".", "to", "(", "cfg", ".", "train", ".", "device", ")", "\n", "model", "=", "create_ddp_model", "(", "model", ")", "\n", "DetectionCheckpointer", "(", "model", ")", ".", "load", "(", "cfg", ".", "train", ".", "init_checkpoint", ")", "\n", "DetectionCheckpointer", "(", "model", ",", "bb_rpn_weights", "=", "True", ")", ".", "load", "(", "cfg", ".", "train", ".", "bb_rpn_checkpoint", ")", "\n", "print", "(", "do_test", "(", "cfg", ",", "model", ")", ")", "\n", "", "else", ":", "\n", "# eval at beginning", "\n", "# model = instantiate(cfg.model)", "\n", "# model.to(cfg.train.device)", "\n", "# model = create_ddp_model(model)", "\n", "# DetectionCheckpointer(model).load(cfg.train.init_checkpoint)", "\n", "# DetectionCheckpointer(model, bb_rpn_weights=True).load(cfg.train.bb_rpn_checkpoint)", "\n", "# print(do_test(cfg, model))", "\n", "\n", "        ", "do_train", "(", "args", ",", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.get_evaluator": [[56, 100], ["detectron2.evaluation.DatasetEvaluators", "os.path.join", "detectron2.data.MetadataCatalog.get", "evaluator_list.append", "evaluator_list.append", "evaluator_list.append", "detectron2.evaluation.CityscapesInstanceEvaluator", "detectron2.evaluation.CityscapesSemSegEvaluator", "detectron2.evaluation.PascalVOCDetectionEvaluator", "detectron2.evaluation.LVISEvaluator", "len", "NotImplementedError", "len", "detectron2.evaluation.SemSegEvaluator", "detectron2.evaluation.COCOEvaluator", "detectron2.evaluation.COCOPanopticEvaluator", "torch.cuda.device_count", "detectron2.get_rank", "torch.cuda.device_count", "detectron2.get_rank"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["def", "get_evaluator", "(", "cfg", ",", "dataset_name", ",", "output_folder", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Create evaluator(s) for a given dataset.\n    This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n    For your own dataset, you can simply create an evaluator manually in your\n    script and do not have to worry about the hacky if-else logic here.\n    \"\"\"", "\n", "if", "output_folder", "is", "None", ":", "\n", "        ", "output_folder", "=", "os", ".", "path", ".", "join", "(", "cfg", ".", "OUTPUT_DIR", ",", "\"inference\"", ")", "\n", "", "evaluator_list", "=", "[", "]", "\n", "evaluator_type", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", ".", "evaluator_type", "\n", "if", "evaluator_type", "in", "[", "\"sem_seg\"", ",", "\"coco_panoptic_seg\"", "]", ":", "\n", "        ", "evaluator_list", ".", "append", "(", "\n", "SemSegEvaluator", "(", "\n", "dataset_name", ",", "\n", "distributed", "=", "True", ",", "\n", "output_dir", "=", "output_folder", ",", "\n", ")", "\n", ")", "\n", "", "if", "evaluator_type", "in", "[", "\"coco\"", ",", "\"coco_panoptic_seg\"", "]", ":", "\n", "        ", "evaluator_list", ".", "append", "(", "COCOEvaluator", "(", "dataset_name", ",", "output_dir", "=", "output_folder", ")", ")", "\n", "", "if", "evaluator_type", "==", "\"coco_panoptic_seg\"", ":", "\n", "        ", "evaluator_list", ".", "append", "(", "COCOPanopticEvaluator", "(", "dataset_name", ",", "output_folder", ")", ")", "\n", "", "if", "evaluator_type", "==", "\"cityscapes_instance\"", ":", "\n", "        ", "assert", "(", "\n", "torch", ".", "cuda", ".", "device_count", "(", ")", ">=", "comm", ".", "get_rank", "(", ")", "\n", ")", ",", "\"CityscapesEvaluator currently do not work with multiple machines.\"", "\n", "return", "CityscapesInstanceEvaluator", "(", "dataset_name", ")", "\n", "", "if", "evaluator_type", "==", "\"cityscapes_sem_seg\"", ":", "\n", "        ", "assert", "(", "\n", "torch", ".", "cuda", ".", "device_count", "(", ")", ">=", "comm", ".", "get_rank", "(", ")", "\n", ")", ",", "\"CityscapesEvaluator currently do not work with multiple machines.\"", "\n", "return", "CityscapesSemSegEvaluator", "(", "dataset_name", ")", "\n", "", "if", "evaluator_type", "==", "\"pascal_voc\"", ":", "\n", "        ", "return", "PascalVOCDetectionEvaluator", "(", "dataset_name", ")", "\n", "", "if", "evaluator_type", "==", "\"lvis\"", ":", "\n", "        ", "return", "LVISEvaluator", "(", "dataset_name", ",", "cfg", ",", "True", ",", "output_folder", ")", "\n", "", "if", "len", "(", "evaluator_list", ")", "==", "0", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"no Evaluator for the dataset {} with the type {}\"", ".", "format", "(", "dataset_name", ",", "evaluator_type", ")", "\n", ")", "\n", "", "if", "len", "(", "evaluator_list", ")", "==", "1", ":", "\n", "        ", "return", "evaluator_list", "[", "0", "]", "\n", "", "return", "DatasetEvaluators", "(", "evaluator_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.do_test": [[102, 117], ["collections.OrderedDict", "detectron2.data.build_detection_test_loader", "plain_train_net.get_evaluator", "detectron2.evaluation.inference_on_dataset", "detectron2.is_main_process", "len", "os.path.join", "logger.info", "detectron2.evaluation.print_csv_format", "list", "collections.OrderedDict.values"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_test_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.get_evaluator", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.inference_on_dataset", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.print_csv_format", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "do_test", "(", "cfg", ",", "model", ")", ":", "\n", "    ", "results", "=", "OrderedDict", "(", ")", "\n", "for", "dataset_name", "in", "cfg", ".", "DATASETS", ".", "TEST", ":", "\n", "        ", "data_loader", "=", "build_detection_test_loader", "(", "cfg", ",", "dataset_name", ")", "\n", "evaluator", "=", "get_evaluator", "(", "\n", "cfg", ",", "dataset_name", ",", "os", ".", "path", ".", "join", "(", "cfg", ".", "OUTPUT_DIR", ",", "\"inference\"", ",", "dataset_name", ")", "\n", ")", "\n", "results_i", "=", "inference_on_dataset", "(", "model", ",", "data_loader", ",", "evaluator", ")", "\n", "results", "[", "dataset_name", "]", "=", "results_i", "\n", "if", "comm", ".", "is_main_process", "(", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Evaluation results for {} in csv format:\"", ".", "format", "(", "dataset_name", ")", ")", "\n", "print_csv_format", "(", "results_i", ")", "\n", "", "", "if", "len", "(", "results", ")", "==", "1", ":", "\n", "        ", "results", "=", "list", "(", "results", ".", "values", "(", ")", ")", "[", "0", "]", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.do_train": [[119, 176], ["model.train", "detectron2.solver.build_optimizer", "detectron2.solver.build_lr_scheduler", "detectron2.checkpoint.DetectionCheckpointer", "detectron2.checkpoint.PeriodicCheckpointer", "detectron2.data.build_detection_train_loader", "logger.info", "detectron2.checkpoint.DetectionCheckpointer.resume_or_load().get", "detectron2.is_main_process", "detectron2.engine.default_writers", "detectron2.utils.events.EventStorage", "zip", "range", "model", "sum", "torch.isfinite().all", "sum", "detectron2.is_main_process", "detectron2.solver.build_optimizer.zero_grad", "sum.backward", "detectron2.solver.build_optimizer.step", "storage.put_scalar", "detectron2.solver.build_lr_scheduler.step", "detectron2.checkpoint.PeriodicCheckpointer.step", "detectron2.checkpoint.DetectionCheckpointer.resume_or_load", "model.values", "v.item", "storage.put_scalars", "plain_train_net.do_test", "detectron2.synchronize", "torch.isfinite", "detectron2.reduce_dict().items", "writer.write", "loss_dict_reduced.values", "detectron2.reduce_dict"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.build_optimizer", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.build_lr_scheduler", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_train_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.default_writers", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalars", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.do_test", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.reduce_dict"], ["", "def", "do_train", "(", "cfg", ",", "model", ",", "resume", "=", "False", ")", ":", "\n", "    ", "model", ".", "train", "(", ")", "\n", "optimizer", "=", "build_optimizer", "(", "cfg", ",", "model", ")", "\n", "scheduler", "=", "build_lr_scheduler", "(", "cfg", ",", "optimizer", ")", "\n", "\n", "checkpointer", "=", "DetectionCheckpointer", "(", "\n", "model", ",", "cfg", ".", "OUTPUT_DIR", ",", "optimizer", "=", "optimizer", ",", "scheduler", "=", "scheduler", "\n", ")", "\n", "start_iter", "=", "(", "\n", "checkpointer", ".", "resume_or_load", "(", "cfg", ".", "MODEL", ".", "WEIGHTS", ",", "resume", "=", "resume", ")", ".", "get", "(", "\"iteration\"", ",", "-", "1", ")", "+", "1", "\n", ")", "\n", "max_iter", "=", "cfg", ".", "SOLVER", ".", "MAX_ITER", "\n", "\n", "periodic_checkpointer", "=", "PeriodicCheckpointer", "(", "\n", "checkpointer", ",", "cfg", ".", "SOLVER", ".", "CHECKPOINT_PERIOD", ",", "max_iter", "=", "max_iter", "\n", ")", "\n", "\n", "writers", "=", "default_writers", "(", "cfg", ".", "OUTPUT_DIR", ",", "max_iter", ")", "if", "comm", ".", "is_main_process", "(", ")", "else", "[", "]", "\n", "\n", "# compared to \"train_net.py\", we do not support accurate timing and", "\n", "# precise BN here, because they are not trivial to implement in a small training loop", "\n", "data_loader", "=", "build_detection_train_loader", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "\"Starting training from iteration {}\"", ".", "format", "(", "start_iter", ")", ")", "\n", "with", "EventStorage", "(", "start_iter", ")", "as", "storage", ":", "\n", "        ", "for", "data", ",", "iteration", "in", "zip", "(", "data_loader", ",", "range", "(", "start_iter", ",", "max_iter", ")", ")", ":", "\n", "            ", "storage", ".", "iter", "=", "iteration", "\n", "\n", "loss_dict", "=", "model", "(", "data", ")", "\n", "losses", "=", "sum", "(", "loss_dict", ".", "values", "(", ")", ")", "\n", "assert", "torch", ".", "isfinite", "(", "losses", ")", ".", "all", "(", ")", ",", "loss_dict", "\n", "\n", "loss_dict_reduced", "=", "{", "k", ":", "v", ".", "item", "(", ")", "for", "k", ",", "v", "in", "comm", ".", "reduce_dict", "(", "loss_dict", ")", ".", "items", "(", ")", "}", "\n", "losses_reduced", "=", "sum", "(", "loss", "for", "loss", "in", "loss_dict_reduced", ".", "values", "(", ")", ")", "\n", "if", "comm", ".", "is_main_process", "(", ")", ":", "\n", "                ", "storage", ".", "put_scalars", "(", "total_loss", "=", "losses_reduced", ",", "**", "loss_dict_reduced", ")", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "losses", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "storage", ".", "put_scalar", "(", "\"lr\"", ",", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", ",", "smoothing_hint", "=", "False", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "\n", "if", "(", "\n", "cfg", ".", "TEST", ".", "EVAL_PERIOD", ">", "0", "\n", "and", "(", "iteration", "+", "1", ")", "%", "cfg", ".", "TEST", ".", "EVAL_PERIOD", "==", "0", "\n", "and", "iteration", "!=", "max_iter", "-", "1", "\n", ")", ":", "\n", "                ", "do_test", "(", "cfg", ",", "model", ")", "\n", "# Compared to \"train_net.py\", the test results are not dumped to EventStorage", "\n", "comm", ".", "synchronize", "(", ")", "\n", "\n", "", "if", "iteration", "-", "start_iter", ">", "5", "and", "(", "\n", "(", "iteration", "+", "1", ")", "%", "20", "==", "0", "or", "iteration", "==", "max_iter", "-", "1", "\n", ")", ":", "\n", "                ", "for", "writer", "in", "writers", ":", "\n", "                    ", "writer", ".", "write", "(", ")", "\n", "", "", "periodic_checkpointer", ".", "step", "(", "iteration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.setup": [[178, 190], ["detectron2.config.get_cfg", "detectron2.config.get_cfg.merge_from_file", "detectron2.config.get_cfg.merge_from_list", "detectron2.config.get_cfg.freeze", "detectron2.engine.default_setup"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.default_setup"], ["", "", "", "def", "setup", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Create configs and perform basic setups.\n    \"\"\"", "\n", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "merge_from_file", "(", "args", ".", "config_file", ")", "\n", "cfg", ".", "merge_from_list", "(", "args", ".", "opts", ")", "\n", "cfg", ".", "freeze", "(", ")", "\n", "default_setup", "(", "\n", "cfg", ",", "args", "\n", ")", "# if you don't like any of the default setup, write your own setup code", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.main": [[192, 211], ["plain_train_net.setup", "detectron2.modeling.build_model", "logger.info", "plain_train_net.do_train", "plain_train_net.do_test", "detectron2.checkpoint.DetectionCheckpointer().resume_or_load", "plain_train_net.do_test", "detectron2.get_world_size", "torch.nn.parallel.DistributedDataParallel", "detectron2.checkpoint.DetectionCheckpointer", "detectron2.get_local_rank"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.setup", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.do_train", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.do_test", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.plain_train_net.do_test", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_local_rank"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "setup", "(", "args", ")", "\n", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "\"Model:\\n{}\"", ".", "format", "(", "model", ")", ")", "\n", "if", "args", ".", "eval_only", ":", "\n", "        ", "DetectionCheckpointer", "(", "model", ",", "save_dir", "=", "cfg", ".", "OUTPUT_DIR", ")", ".", "resume_or_load", "(", "\n", "cfg", ".", "MODEL", ".", "WEIGHTS", ",", "resume", "=", "args", ".", "resume", "\n", ")", "\n", "return", "do_test", "(", "cfg", ",", "model", ")", "\n", "\n", "", "distributed", "=", "comm", ".", "get_world_size", "(", ")", ">", "1", "\n", "if", "distributed", ":", "\n", "        ", "model", "=", "DistributedDataParallel", "(", "\n", "model", ",", "device_ids", "=", "[", "comm", ".", "get_local_rank", "(", ")", "]", ",", "broadcast_buffers", "=", "False", "\n", ")", "\n", "\n", "", "do_train", "(", "cfg", ",", "model", ",", "resume", "=", "args", ".", "resume", ")", "\n", "return", "do_test", "(", "cfg", ",", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.benchmark.setup": [[35, 43], ["detectron2.config.get_cfg", "detectron2.config.get_cfg.merge_from_file", "detectron2.config.get_cfg.merge_from_list", "detectron2.config.get_cfg.freeze", "detectron2.utils.logger.setup_logger", "detectron2.utils.comm.get_rank"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["def", "setup", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "merge_from_file", "(", "args", ".", "config_file", ")", "\n", "cfg", ".", "SOLVER", ".", "BASE_LR", "=", "0.001", "# Avoid NaNs. Not useful in this script anyway.", "\n", "cfg", ".", "merge_from_list", "(", "args", ".", "opts", ")", "\n", "cfg", ".", "freeze", "(", ")", "\n", "setup_logger", "(", "distributed_rank", "=", "comm", ".", "get_rank", "(", ")", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.benchmark.RAM_msg": [[45, 49], ["psutil.virtual_memory"], "function", ["None"], ["", "def", "RAM_msg", "(", ")", ":", "\n", "    ", "vram", "=", "psutil", ".", "virtual_memory", "(", ")", "\n", "return", "\"RAM Usage: {:.2f}/{:.2f} GB\"", ".", "format", "(", "\n", "(", "vram", ".", "total", "-", "vram", ".", "available", ")", "/", "1024", "**", "3", ",", "vram", ".", "total", "/", "1024", "**", "3", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.benchmark.benchmark_data": [[52, 87], ["benchmark.setup", "logger.info", "fvcore.common.timer.Timer", "detectron2.data.build_detection_train_loader", "logger.info", "fvcore.common.timer.Timer.reset", "iter", "range", "logger.info", "fvcore.common.timer.Timer", "tqdm.trange", "logger.info", "range", "next", "next", "logger.info", "fvcore.common.timer.Timer", "tqdm.trange", "logger.info", "benchmark.RAM_msg", "fvcore.common.timer.Timer.seconds", "fvcore.common.timer.Timer.seconds", "fvcore.common.timer.Timer.seconds", "next", "benchmark.RAM_msg", "fvcore.common.timer.Timer.seconds"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.setup", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_train_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.reset", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.benchmark.RAM_msg", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.benchmark.RAM_msg"], ["", "def", "benchmark_data", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "setup", "(", "args", ")", "\n", "\n", "logger", ".", "info", "(", "\"After spawning \"", "+", "RAM_msg", "(", ")", ")", "\n", "timer", "=", "Timer", "(", ")", "\n", "dataloader", "=", "build_detection_train_loader", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "\"Initialize loader using {} seconds.\"", ".", "format", "(", "timer", ".", "seconds", "(", ")", ")", ")", "\n", "\n", "timer", ".", "reset", "(", ")", "\n", "itr", "=", "iter", "(", "dataloader", ")", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "# warmup", "\n", "        ", "next", "(", "itr", ")", "\n", "if", "i", "==", "0", ":", "\n", "            ", "startup_time", "=", "timer", ".", "seconds", "(", ")", "\n", "", "", "logger", ".", "info", "(", "\"Startup time: {} seconds\"", ".", "format", "(", "startup_time", ")", ")", "\n", "timer", "=", "Timer", "(", ")", "\n", "max_iter", "=", "1000", "\n", "for", "_", "in", "tqdm", ".", "trange", "(", "max_iter", ")", ":", "\n", "        ", "next", "(", "itr", ")", "\n", "", "logger", ".", "info", "(", "\n", "\"{} iters ({} images) in {} seconds.\"", ".", "format", "(", "\n", "max_iter", ",", "max_iter", "*", "cfg", ".", "SOLVER", ".", "IMS_PER_BATCH", ",", "timer", ".", "seconds", "(", ")", "\n", ")", "\n", ")", "\n", "\n", "# test for a few more rounds", "\n", "for", "k", "in", "range", "(", "10", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f\"Iteration {k} \"", "+", "RAM_msg", "(", ")", ")", "\n", "timer", "=", "Timer", "(", ")", "\n", "max_iter", "=", "1000", "\n", "for", "_", "in", "tqdm", ".", "trange", "(", "max_iter", ")", ":", "\n", "            ", "next", "(", "itr", ")", "\n", "", "logger", ".", "info", "(", "\n", "\"{} iters ({} images) in {} seconds.\"", ".", "format", "(", "\n", "max_iter", ",", "max_iter", "*", "cfg", ".", "SOLVER", ".", "IMS_PER_BATCH", ",", "timer", ".", "seconds", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.benchmark.benchmark_train": [[91, 119], ["benchmark.setup", "detectron2.modeling.build_model", "logger.info", "detectron2.solver.build_optimizer", "detectron2.checkpoint.DetectionCheckpointer", "detectron2.checkpoint.DetectionCheckpointer.load", "setup.defrost", "detectron2.data.build_detection_train_loader", "list", "trainer.register_hooks", "trainer.train", "detectron2.utils.comm.get_world_size", "torch.nn.parallel.DistributedDataParallel", "itertools.islice", "detectron2.data.DatasetFromList", "benchmark.benchmark_train.f"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.setup", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.build_optimizer", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_train_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.register_hooks", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], ["", "", "def", "benchmark_train", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "setup", "(", "args", ")", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "\"Model:\\n{}\"", ".", "format", "(", "model", ")", ")", "\n", "if", "comm", ".", "get_world_size", "(", ")", ">", "1", ":", "\n", "        ", "model", "=", "DistributedDataParallel", "(", "\n", "model", ",", "device_ids", "=", "[", "comm", ".", "get_local_rank", "(", ")", "]", ",", "broadcast_buffers", "=", "False", "\n", ")", "\n", "", "optimizer", "=", "build_optimizer", "(", "cfg", ",", "model", ")", "\n", "checkpointer", "=", "DetectionCheckpointer", "(", "model", ",", "optimizer", "=", "optimizer", ")", "\n", "checkpointer", ".", "load", "(", "cfg", ".", "MODEL", ".", "WEIGHTS", ")", "\n", "\n", "cfg", ".", "defrost", "(", ")", "\n", "cfg", ".", "DATALOADER", ".", "NUM_WORKERS", "=", "2", "\n", "data_loader", "=", "build_detection_train_loader", "(", "cfg", ")", "\n", "dummy_data", "=", "list", "(", "itertools", ".", "islice", "(", "data_loader", ",", "100", ")", ")", "\n", "\n", "def", "f", "(", ")", ":", "\n", "        ", "data", "=", "DatasetFromList", "(", "dummy_data", ",", "copy", "=", "False", ",", "serialize", "=", "False", ")", "\n", "while", "True", ":", "\n", "            ", "yield", "from", "data", "\n", "\n", "", "", "max_iter", "=", "400", "\n", "trainer", "=", "(", "AMPTrainer", "if", "cfg", ".", "SOLVER", ".", "AMP", ".", "ENABLED", "else", "SimpleTrainer", ")", "(", "model", ",", "f", "(", ")", ",", "optimizer", ")", "\n", "trainer", ".", "register_hooks", "(", "\n", "[", "hooks", ".", "IterationTimer", "(", ")", ",", "hooks", ".", "PeriodicWriter", "(", "[", "CommonMetricPrinter", "(", "max_iter", ")", "]", ")", "]", "\n", ")", "\n", "trainer", ".", "train", "(", "1", ",", "max_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.benchmark.benchmark_eval": [[121, 150], ["torch.no_grad", "benchmark.setup", "detectron2.modeling.build_model", "detectron2.modeling.build_model.eval", "logger.info", "detectron2.checkpoint.DetectionCheckpointer().load", "setup.defrost", "detectron2.data.build_detection_test_loader", "detectron2.data.DatasetFromList", "range", "fvcore.common.timer.Timer", "logger.info", "list", "detectron2.modeling.build_model.", "tqdm.tqdm", "enumerate", "detectron2.checkpoint.DetectionCheckpointer", "itertools.islice", "benchmark.benchmark_train.f"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.setup", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_test_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "benchmark_eval", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "setup", "(", "args", ")", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "model", ".", "eval", "(", ")", "\n", "logger", ".", "info", "(", "\"Model:\\n{}\"", ".", "format", "(", "model", ")", ")", "\n", "DetectionCheckpointer", "(", "model", ")", ".", "load", "(", "cfg", ".", "MODEL", ".", "WEIGHTS", ")", "\n", "\n", "cfg", ".", "defrost", "(", ")", "\n", "cfg", ".", "DATALOADER", ".", "NUM_WORKERS", "=", "0", "\n", "data_loader", "=", "build_detection_test_loader", "(", "cfg", ",", "cfg", ".", "DATASETS", ".", "TEST", "[", "0", "]", ")", "\n", "dummy_data", "=", "DatasetFromList", "(", "list", "(", "itertools", ".", "islice", "(", "data_loader", ",", "100", ")", ")", ",", "copy", "=", "False", ")", "\n", "\n", "def", "f", "(", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "yield", "from", "dummy_data", "\n", "\n", "", "", "for", "k", "in", "range", "(", "5", ")", ":", "# warmup", "\n", "        ", "model", "(", "dummy_data", "[", "k", "]", ")", "\n", "\n", "", "max_iter", "=", "300", "\n", "timer", "=", "Timer", "(", ")", "\n", "with", "tqdm", ".", "tqdm", "(", "total", "=", "max_iter", ")", "as", "pbar", ":", "\n", "        ", "for", "idx", ",", "d", "in", "enumerate", "(", "f", "(", ")", ")", ":", "\n", "            ", "if", "idx", "==", "max_iter", ":", "\n", "                ", "break", "\n", "", "model", "(", "d", ")", "\n", "pbar", ".", "update", "(", ")", "\n", "", "", "logger", ".", "info", "(", "\"{} iters in {} seconds.\"", ".", "format", "(", "max_iter", ",", "timer", ".", "seconds", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.analyze_model.setup": [[25, 34], ["detectron2.config.get_cfg", "detectron2.config.get_cfg.merge_from_file", "detectron2.config.get_cfg.merge_from_list", "detectron2.config.get_cfg.freeze", "detectron2.utils.logger.setup_logger", "detectron2.utils.logger.setup_logger"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger"], ["def", "setup", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "merge_from_file", "(", "args", ".", "config_file", ")", "\n", "cfg", ".", "DATALOADER", ".", "NUM_WORKERS", "=", "0", "\n", "cfg", ".", "merge_from_list", "(", "args", ".", "opts", ")", "\n", "cfg", ".", "freeze", "(", ")", "\n", "setup_logger", "(", "name", "=", "\"fvcore\"", ")", "\n", "setup_logger", "(", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.analyze_model.do_flop": [[36, 58], ["detectron2.data.build_detection_test_loader", "detectron2.modeling.build_model", "detectron2.checkpoint.DetectionCheckpointer().load", "detectron2.modeling.build_model.eval", "collections.Counter", "zip", "logger.info", "logger.info", "logger.info", "tqdm.trange", "detectron2.utils.analysis.FlopCountAnalysis", "detectron2.utils.analysis.FlopCountAnalysis.by_operator", "total_flops.append", "detectron2.checkpoint.DetectionCheckpointer", "detectron2.utils.analysis.FlopCountAnalysis.unsupported_ops_warnings().uncalled_modules_warnings", "detectron2.utils.analysis.FlopCountAnalysis.total", "fvcore.nn.flop_count_table", "str", "numpy.mean", "numpy.std", "detectron2.utils.analysis.FlopCountAnalysis.unsupported_ops_warnings", "collections.Counter.items"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_test_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], ["", "def", "do_flop", "(", "cfg", ")", ":", "\n", "    ", "data_loader", "=", "build_detection_test_loader", "(", "cfg", ",", "cfg", ".", "DATASETS", ".", "TEST", "[", "0", "]", ")", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "DetectionCheckpointer", "(", "model", ")", ".", "load", "(", "cfg", ".", "MODEL", ".", "WEIGHTS", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "counts", "=", "Counter", "(", ")", "\n", "total_flops", "=", "[", "]", "\n", "for", "idx", ",", "data", "in", "zip", "(", "tqdm", ".", "trange", "(", "args", ".", "num_inputs", ")", ",", "data_loader", ")", ":", "# noqa", "\n", "        ", "flops", "=", "FlopCountAnalysis", "(", "model", ",", "data", ")", "\n", "if", "idx", ">", "0", ":", "\n", "            ", "flops", ".", "unsupported_ops_warnings", "(", "False", ")", ".", "uncalled_modules_warnings", "(", "False", ")", "\n", "", "counts", "+=", "flops", ".", "by_operator", "(", ")", "\n", "total_flops", ".", "append", "(", "flops", ".", "total", "(", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Flops table computed from only one input sample:\\n\"", "+", "flop_count_table", "(", "flops", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"Average GFlops for each type of operators:\\n\"", "\n", "+", "str", "(", "[", "(", "k", ",", "v", "/", "(", "idx", "+", "1", ")", "/", "1e9", ")", "for", "k", ",", "v", "in", "counts", ".", "items", "(", ")", "]", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"Total GFlops: {:.1f}\u00b1{:.1f}\"", ".", "format", "(", "np", ".", "mean", "(", "total_flops", ")", "/", "1e9", ",", "np", ".", "std", "(", "total_flops", ")", "/", "1e9", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.analyze_model.do_activation": [[61, 80], ["detectron2.data.build_detection_test_loader", "detectron2.modeling.build_model", "detectron2.checkpoint.DetectionCheckpointer().load", "detectron2.modeling.build_model.eval", "collections.Counter", "zip", "logger.info", "logger.info", "tqdm.trange", "detectron2.utils.analysis.activation_count_operators", "total_activations.append", "detectron2.checkpoint.DetectionCheckpointer", "sum", "str", "numpy.mean", "numpy.std", "detectron2.utils.analysis.activation_count_operators.values", "collections.Counter.items"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_test_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.analysis.activation_count_operators"], ["", "def", "do_activation", "(", "cfg", ")", ":", "\n", "    ", "data_loader", "=", "build_detection_test_loader", "(", "cfg", ",", "cfg", ".", "DATASETS", ".", "TEST", "[", "0", "]", ")", "\n", "model", "=", "build_model", "(", "cfg", ")", "\n", "DetectionCheckpointer", "(", "model", ")", ".", "load", "(", "cfg", ".", "MODEL", ".", "WEIGHTS", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "counts", "=", "Counter", "(", ")", "\n", "total_activations", "=", "[", "]", "\n", "for", "idx", ",", "data", "in", "zip", "(", "tqdm", ".", "trange", "(", "args", ".", "num_inputs", ")", ",", "data_loader", ")", ":", "# noqa", "\n", "        ", "count", "=", "activation_count_operators", "(", "model", ",", "data", ")", "\n", "counts", "+=", "count", "\n", "total_activations", ".", "append", "(", "sum", "(", "count", ".", "values", "(", ")", ")", ")", "\n", "", "logger", ".", "info", "(", "\n", "\"(Million) Activations for Each Type of Operators:\\n\"", "\n", "+", "str", "(", "[", "(", "k", ",", "v", "/", "idx", ")", "for", "k", ",", "v", "in", "counts", ".", "items", "(", ")", "]", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"Total (Million) Activations: {}\u00b1{}\"", ".", "format", "(", "\n", "np", ".", "mean", "(", "total_activations", ")", ",", "np", ".", "std", "(", "total_activations", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.analyze_model.do_parameter": [[84, 87], ["detectron2.modeling.build_model", "logger.info", "detectron2.utils.analysis.parameter_count_table"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model"], ["", "def", "do_parameter", "(", "cfg", ")", ":", "\n", "    ", "model", "=", "build_model", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "\"Parameter Count:\\n\"", "+", "parameter_count_table", "(", "model", ",", "max_depth", "=", "5", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.analyze_model.do_structure": [[89, 92], ["detectron2.modeling.build_model", "logger.info", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model"], ["", "def", "do_structure", "(", "cfg", ")", ":", "\n", "    ", "model", "=", "build_model", "(", "cfg", ")", "\n", "logger", ".", "info", "(", "\"Model Structure:\\n\"", "+", "str", "(", "model", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.visualize_data.setup": [[17, 25], ["detectron2.config.get_cfg", "detectron2.config.get_cfg.merge_from_list", "detectron2.config.get_cfg.freeze", "detectron2.config.get_cfg.merge_from_file"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file"], ["def", "setup", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "get_cfg", "(", ")", "\n", "if", "args", ".", "config_file", ":", "\n", "        ", "cfg", ".", "merge_from_file", "(", "args", ".", "config_file", ")", "\n", "", "cfg", ".", "merge_from_list", "(", "args", ".", "opts", ")", "\n", "cfg", ".", "DATALOADER", ".", "NUM_WORKERS", "=", "0", "\n", "cfg", ".", "freeze", "(", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.visualize_data.parse_args": [[27, 45], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.tools.visualize_data.parse_args"], ["", "def", "parse_args", "(", "in_args", "=", "None", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Visualize ground-truth data\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--source\"", ",", "\n", "choices", "=", "[", "\"annotation\"", ",", "\"dataloader\"", "]", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"visualize the annotations or the data loader (with pre-processing)\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--config-file\"", ",", "metavar", "=", "\"FILE\"", ",", "help", "=", "\"path to config file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output-dir\"", ",", "default", "=", "\"./\"", ",", "help", "=", "\"path to output directory\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--show\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"show output in a window\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"opts\"", ",", "\n", "help", "=", "\"Modify config options using the command-line\"", ",", "\n", "default", "=", "None", ",", "\n", "nargs", "=", "argparse", ".", "REMAINDER", ",", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", "in_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.visualize_json_results.create_instances": [[19, 39], ["detectron2.structures.Instances", "numpy.asarray", "numpy.asarray().reshape", "detectron2.structures.BoxMode.convert", "numpy.asarray", "detectron2.structures.Boxes", "numpy.asarray", "dataset_id_map"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["def", "create_instances", "(", "predictions", ",", "image_size", ")", ":", "\n", "    ", "ret", "=", "Instances", "(", "image_size", ")", "\n", "\n", "score", "=", "np", ".", "asarray", "(", "[", "x", "[", "\"score\"", "]", "for", "x", "in", "predictions", "]", ")", "\n", "chosen", "=", "(", "score", ">", "args", ".", "conf_threshold", ")", ".", "nonzero", "(", ")", "[", "0", "]", "\n", "score", "=", "score", "[", "chosen", "]", "\n", "bbox", "=", "np", ".", "asarray", "(", "[", "predictions", "[", "i", "]", "[", "\"bbox\"", "]", "for", "i", "in", "chosen", "]", ")", ".", "reshape", "(", "-", "1", ",", "4", ")", "\n", "bbox", "=", "BoxMode", ".", "convert", "(", "bbox", ",", "BoxMode", ".", "XYWH_ABS", ",", "BoxMode", ".", "XYXY_ABS", ")", "\n", "\n", "labels", "=", "np", ".", "asarray", "(", "[", "dataset_id_map", "(", "predictions", "[", "i", "]", "[", "\"category_id\"", "]", ")", "for", "i", "in", "chosen", "]", ")", "\n", "\n", "ret", ".", "scores", "=", "score", "\n", "ret", ".", "pred_boxes", "=", "Boxes", "(", "bbox", ")", "\n", "ret", ".", "pred_classes", "=", "labels", "\n", "\n", "try", ":", "\n", "        ", "ret", ".", "pred_masks", "=", "[", "predictions", "[", "i", "]", "[", "\"segmentation\"", "]", "for", "i", "in", "chosen", "]", "\n", "", "except", "KeyError", ":", "\n", "        ", "pass", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.Trainer.build_evaluator": [[54, 101], ["detectron2.evaluation.DatasetEvaluators", "os.path.join", "detectron2.data.MetadataCatalog.get", "evaluator_list.append", "evaluator_list.append", "evaluator_list.append", "detectron2.evaluation.CityscapesInstanceEvaluator", "detectron2.evaluation.CityscapesSemSegEvaluator", "len", "NotImplementedError", "detectron2.evaluation.SemSegEvaluator", "detectron2.evaluation.COCOEvaluator", "detectron2.evaluation.COCOPanopticEvaluator", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "detectron2.get_rank", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "detectron2.get_rank", "detectron2.evaluation.PascalVOCDetectionEvaluator", "len", "detectron2.evaluation.LVISEvaluator"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["@", "classmethod", "\n", "def", "build_evaluator", "(", "cls", ",", "cfg", ",", "dataset_name", ",", "output_folder", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Create evaluator(s) for a given dataset.\n        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n        For your own dataset, you can simply create an evaluator manually in your\n        script and do not have to worry about the hacky if-else logic here.\n        \"\"\"", "\n", "if", "output_folder", "is", "None", ":", "\n", "            ", "output_folder", "=", "os", ".", "path", ".", "join", "(", "cfg", ".", "OUTPUT_DIR", ",", "\"inference\"", ")", "\n", "", "evaluator_list", "=", "[", "]", "\n", "evaluator_type", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", ".", "evaluator_type", "\n", "if", "evaluator_type", "in", "[", "\"sem_seg\"", ",", "\"coco_panoptic_seg\"", "]", ":", "\n", "            ", "evaluator_list", ".", "append", "(", "\n", "SemSegEvaluator", "(", "\n", "dataset_name", ",", "\n", "distributed", "=", "True", ",", "\n", "output_dir", "=", "output_folder", ",", "\n", ")", "\n", ")", "\n", "", "if", "evaluator_type", "in", "[", "\"coco\"", ",", "\"coco_panoptic_seg\"", "]", ":", "\n", "            ", "evaluator_list", ".", "append", "(", "COCOEvaluator", "(", "dataset_name", ",", "output_dir", "=", "output_folder", ")", ")", "\n", "", "if", "evaluator_type", "==", "\"coco_panoptic_seg\"", ":", "\n", "            ", "evaluator_list", ".", "append", "(", "COCOPanopticEvaluator", "(", "dataset_name", ",", "output_folder", ")", ")", "\n", "", "if", "evaluator_type", "==", "\"cityscapes_instance\"", ":", "\n", "            ", "assert", "(", "\n", "torch", ".", "cuda", ".", "device_count", "(", ")", ">=", "comm", ".", "get_rank", "(", ")", "\n", ")", ",", "\"CityscapesEvaluator currently do not work with multiple machines.\"", "\n", "return", "CityscapesInstanceEvaluator", "(", "dataset_name", ")", "\n", "", "if", "evaluator_type", "==", "\"cityscapes_sem_seg\"", ":", "\n", "            ", "assert", "(", "\n", "torch", ".", "cuda", ".", "device_count", "(", ")", ">=", "comm", ".", "get_rank", "(", ")", "\n", ")", ",", "\"CityscapesEvaluator currently do not work with multiple machines.\"", "\n", "return", "CityscapesSemSegEvaluator", "(", "dataset_name", ")", "\n", "", "elif", "evaluator_type", "==", "\"pascal_voc\"", ":", "\n", "            ", "return", "PascalVOCDetectionEvaluator", "(", "dataset_name", ")", "\n", "", "elif", "evaluator_type", "==", "\"lvis\"", ":", "\n", "            ", "return", "LVISEvaluator", "(", "dataset_name", ",", "output_dir", "=", "output_folder", ")", "\n", "", "if", "len", "(", "evaluator_list", ")", "==", "0", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"no Evaluator for the dataset {} with the type {}\"", ".", "format", "(", "\n", "dataset_name", ",", "evaluator_type", "\n", ")", "\n", ")", "\n", "", "elif", "len", "(", "evaluator_list", ")", "==", "1", ":", "\n", "            ", "return", "evaluator_list", "[", "0", "]", "\n", "", "return", "DatasetEvaluators", "(", "evaluator_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.Trainer.test_with_TTA": [[102, 118], ["logging.getLogger", "logging.getLogger.info", "detectron2.modeling.GeneralizedRCNNWithTTA", "cls.test", "collections.OrderedDict", "cls.build_evaluator", "os.path.join", "collections.OrderedDict.items"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_packaging.TestCollectEnv.test", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_evaluator"], ["", "@", "classmethod", "\n", "def", "test_with_TTA", "(", "cls", ",", "cfg", ",", "model", ")", ":", "\n", "        ", "logger", "=", "logging", ".", "getLogger", "(", "\"detectron2.trainer\"", ")", "\n", "# In the end of training, run an evaluation with TTA", "\n", "# Only support some R-CNN models.", "\n", "logger", ".", "info", "(", "\"Running inference with test-time augmentation ...\"", ")", "\n", "model", "=", "GeneralizedRCNNWithTTA", "(", "cfg", ",", "model", ")", "\n", "evaluators", "=", "[", "\n", "cls", ".", "build_evaluator", "(", "\n", "cfg", ",", "name", ",", "output_folder", "=", "os", ".", "path", ".", "join", "(", "cfg", ".", "OUTPUT_DIR", ",", "\"inference_TTA\"", ")", "\n", ")", "\n", "for", "name", "in", "cfg", ".", "DATASETS", ".", "TEST", "\n", "]", "\n", "res", "=", "cls", ".", "test", "(", "cfg", ",", "model", ",", "evaluators", ")", "\n", "res", "=", "OrderedDict", "(", "{", "k", "+", "\"_TTA\"", ":", "v", "for", "k", ",", "v", "in", "res", ".", "items", "(", ")", "}", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.setup": [[120, 130], ["detectron2.config.get_cfg", "detectron2.config.get_cfg.merge_from_file", "detectron2.config.get_cfg.merge_from_list", "detectron2.config.get_cfg.freeze", "detectron2.engine.default_setup"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.default_setup"], ["", "", "def", "setup", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Create configs and perform basic setups.\n    \"\"\"", "\n", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "merge_from_file", "(", "args", ".", "config_file", ")", "\n", "cfg", ".", "merge_from_list", "(", "args", ".", "opts", ")", "\n", "cfg", ".", "freeze", "(", ")", "\n", "default_setup", "(", "cfg", ",", "args", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.main": [[132, 165], ["train_net.setup", "train_net.Trainer", "Trainer.resume_or_load", "Trainer.train", "Trainer.build_model", "detectron2.checkpoint.DetectionCheckpointer().resume_or_load", "Trainer.test", "detectron2.is_main_process", "Trainer.register_hooks", "detectron2.checkpoint.DetectionCheckpointer().resume_or_load", "Trainer.test.update", "detectron2.evaluation.verify_results", "detectron2.checkpoint.DetectionCheckpointer", "train_net.Trainer.test_with_TTA", "detectron2.engine.hooks.EvalHook", "detectron2.checkpoint.DetectionCheckpointer", "train_net.Trainer.test_with_TTA"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.setup", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load", "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_packaging.TestCollectEnv.test", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.register_hooks", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.verify_results", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.Trainer.test_with_TTA", "home.repos.pwc.inspect_result.microsoft_regionclip.tools.train_net.Trainer.test_with_TTA"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "setup", "(", "args", ")", "\n", "\n", "if", "args", ".", "eval_only", ":", "\n", "        ", "model", "=", "Trainer", ".", "build_model", "(", "cfg", ")", "\n", "DetectionCheckpointer", "(", "model", ",", "save_dir", "=", "cfg", ".", "OUTPUT_DIR", ")", ".", "resume_or_load", "(", "\n", "cfg", ".", "MODEL", ".", "WEIGHTS", ",", "resume", "=", "args", ".", "resume", "\n", ")", "\n", "if", "cfg", ".", "MODEL", ".", "META_ARCHITECTURE", "in", "[", "'CLIPRCNN'", ",", "'CLIPFastRCNN'", ",", "'PretrainFastRCNN'", "]", "and", "cfg", ".", "MODEL", ".", "CLIP", ".", "BB_RPN_WEIGHTS", "is", "not", "None", "and", "cfg", ".", "MODEL", ".", "CLIP", ".", "CROP_REGION_TYPE", "==", "'RPN'", ":", "# load 2nd pretrained model", "\n", "            ", "DetectionCheckpointer", "(", "model", ",", "save_dir", "=", "cfg", ".", "OUTPUT_DIR", ",", "bb_rpn_weights", "=", "True", ")", ".", "resume_or_load", "(", "\n", "cfg", ".", "MODEL", ".", "CLIP", ".", "BB_RPN_WEIGHTS", ",", "resume", "=", "False", "\n", ")", "\n", "", "res", "=", "Trainer", ".", "test", "(", "cfg", ",", "model", ")", "\n", "if", "cfg", ".", "TEST", ".", "AUG", ".", "ENABLED", ":", "\n", "            ", "res", ".", "update", "(", "Trainer", ".", "test_with_TTA", "(", "cfg", ",", "model", ")", ")", "\n", "", "if", "comm", ".", "is_main_process", "(", ")", ":", "\n", "            ", "verify_results", "(", "cfg", ",", "res", ")", "\n", "", "return", "res", "\n", "\n", "", "\"\"\"\n    If you'd like to do anything fancier than the standard training logic,\n    consider writing your own training loop (see plain_train_net.py) or\n    subclassing the trainer.\n    \"\"\"", "\n", "trainer", "=", "Trainer", "(", "cfg", ")", "\n", "trainer", ".", "resume_or_load", "(", "resume", "=", "args", ".", "resume", ")", "\n", "if", "cfg", ".", "TEST", ".", "AUG", ".", "ENABLED", ":", "\n", "        ", "trainer", ".", "register_hooks", "(", "\n", "[", "hooks", ".", "EvalHook", "(", "0", ",", "lambda", ":", "trainer", ".", "test_with_TTA", "(", "cfg", ",", "trainer", ".", "model", ")", ")", "]", "\n", ")", "\n", "", "return", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.deploy.export_model.setup_cfg": [[30, 40], ["detectron2.config.get_cfg", "detectron2.export.add_export_config", "detectron2.projects.point_rend.add_pointrend_config", "detectron2.export.add_export_config.merge_from_file", "detectron2.export.add_export_config.merge_from_list", "detectron2.export.add_export_config.freeze"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.add_export_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze"], ["def", "setup_cfg", "(", "args", ")", ":", "\n", "    ", "cfg", "=", "get_cfg", "(", ")", "\n", "# cuda context is initialized before creating dataloader, so we don't fork anymore", "\n", "cfg", ".", "DATALOADER", ".", "NUM_WORKERS", "=", "0", "\n", "cfg", "=", "add_export_config", "(", "cfg", ")", "\n", "add_pointrend_config", "(", "cfg", ")", "\n", "cfg", ".", "merge_from_file", "(", "args", ".", "config_file", ")", "\n", "cfg", ".", "merge_from_list", "(", "args", ".", "opts", ")", "\n", "cfg", ".", "freeze", "(", ")", "\n", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.deploy.export_model.export_caffe2_tracing": [[42, 60], ["detectron2.export.Caffe2Tracer", "detectron2.export.Caffe2Tracer.export_caffe2", "tracer.export_caffe2.save_protobuf", "tracer.export_caffe2.save_graph", "os.path.join", "detectron2.export.Caffe2Tracer.export_onnx", "onnx.save", "os.path.join", "detectron2.export.Caffe2Tracer.export_torchscript", "detectron2.export.dump_torchscript_IR", "detectron2.utils.file_io.PathManager.open", "torch.jit.save", "os.path.join"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_caffe2", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.save_protobuf", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.save_graph", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_onnx", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_torchscript", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.dump_torchscript_IR", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save"], ["", "def", "export_caffe2_tracing", "(", "cfg", ",", "torch_model", ",", "inputs", ")", ":", "\n", "    ", "tracer", "=", "Caffe2Tracer", "(", "cfg", ",", "torch_model", ",", "inputs", ")", "\n", "if", "args", ".", "format", "==", "\"caffe2\"", ":", "\n", "        ", "caffe2_model", "=", "tracer", ".", "export_caffe2", "(", ")", "\n", "caffe2_model", ".", "save_protobuf", "(", "args", ".", "output", ")", "\n", "# draw the caffe2 graph", "\n", "caffe2_model", ".", "save_graph", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output", ",", "\"model.svg\"", ")", ",", "inputs", "=", "inputs", ")", "\n", "return", "caffe2_model", "\n", "", "elif", "args", ".", "format", "==", "\"onnx\"", ":", "\n", "        ", "import", "onnx", "\n", "\n", "onnx_model", "=", "tracer", ".", "export_onnx", "(", ")", "\n", "onnx", ".", "save", "(", "onnx_model", ",", "os", ".", "path", ".", "join", "(", "args", ".", "output", ",", "\"model.onnx\"", ")", ")", "\n", "", "elif", "args", ".", "format", "==", "\"torchscript\"", ":", "\n", "        ", "ts_model", "=", "tracer", ".", "export_torchscript", "(", ")", "\n", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output", ",", "\"model.ts\"", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "torch", ".", "jit", ".", "save", "(", "ts_model", ",", "f", ")", "\n", "", "dump_torchscript_IR", "(", "ts_model", ",", "args", ".", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.deploy.export_model.export_scripting": [[63, 105], ["isinstance", "detectron2.export.scripting_with_instances", "detectron2.export.dump_torchscript_IR", "ScriptableAdapter", "detectron2.utils.file_io.PathManager.open", "torch.jit.save", "super().__init__", "export_model..eval", "os.path.join", "export_model..model.inference", "export_model..model", "i.get_fields", "i.get_fields"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.scripting_with_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.dump_torchscript_IR", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields"], ["", "", "def", "export_scripting", "(", "torch_model", ")", ":", "\n", "    ", "assert", "TORCH_VERSION", ">=", "(", "1", ",", "8", ")", "\n", "fields", "=", "{", "\n", "\"proposal_boxes\"", ":", "Boxes", ",", "\n", "\"objectness_logits\"", ":", "Tensor", ",", "\n", "\"pred_boxes\"", ":", "Boxes", ",", "\n", "\"scores\"", ":", "Tensor", ",", "\n", "\"pred_classes\"", ":", "Tensor", ",", "\n", "\"pred_masks\"", ":", "Tensor", ",", "\n", "\"pred_keypoints\"", ":", "torch", ".", "Tensor", ",", "\n", "\"pred_keypoint_heatmaps\"", ":", "torch", ".", "Tensor", ",", "\n", "}", "\n", "assert", "args", ".", "format", "==", "\"torchscript\"", ",", "\"Scripting only supports torchscript format.\"", "\n", "\n", "class", "ScriptableAdapterBase", "(", "nn", ".", "Module", ")", ":", "\n", "# Use this adapter to workaround https://github.com/pytorch/pytorch/issues/46944", "\n", "# by not retuning instances but dicts. Otherwise the exported model is not deployable", "\n", "        ", "def", "__init__", "(", "self", ")", ":", "\n", "            ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "torch_model", "\n", "self", ".", "eval", "(", ")", "\n", "\n", "", "", "if", "isinstance", "(", "torch_model", ",", "GeneralizedRCNN", ")", ":", "\n", "\n", "        ", "class", "ScriptableAdapter", "(", "ScriptableAdapterBase", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "inputs", ":", "Tuple", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", "->", "List", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ":", "\n", "                ", "instances", "=", "self", ".", "model", ".", "inference", "(", "inputs", ",", "do_postprocess", "=", "False", ")", "\n", "return", "[", "i", ".", "get_fields", "(", ")", "for", "i", "in", "instances", "]", "\n", "\n", "", "", "", "else", ":", "\n", "\n", "        ", "class", "ScriptableAdapter", "(", "ScriptableAdapterBase", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "inputs", ":", "Tuple", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", "->", "List", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ":", "\n", "                ", "instances", "=", "self", ".", "model", "(", "inputs", ")", "\n", "return", "[", "i", ".", "get_fields", "(", ")", "for", "i", "in", "instances", "]", "\n", "\n", "", "", "", "ts_model", "=", "scripting_with_instances", "(", "ScriptableAdapter", "(", ")", ",", "fields", ")", "\n", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output", ",", "\"model.ts\"", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "        ", "torch", ".", "jit", ".", "save", "(", "ts_model", ",", "f", ")", "\n", "", "dump_torchscript_IR", "(", "ts_model", ",", "args", ".", "output", ")", "\n", "# TODO inference in Python now missing postprocessing glue code", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.deploy.export_model.export_tracing": [[108, 153], ["isinstance", "detectron2.export.TracingAdapter", "logger.info", "logger.info", "torch.jit.trace", "detectron2.export.dump_torchscript_IR", "isinstance", "detectron2.modeling.postprocessing.detector_postprocess", "detectron2.utils.file_io.PathManager.open", "torch.jit.save", "str", "str", "model.inference", "os.path.join", "detectron2.utils.file_io.PathManager.open", "torch.onnx.export", "detectron2.export.TracingAdapter.outputs_schema", "os.path.join", "torch.jit.trace."], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.dump_torchscript_IR", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference"], ["", "def", "export_tracing", "(", "torch_model", ",", "inputs", ")", ":", "\n", "    ", "assert", "TORCH_VERSION", ">=", "(", "1", ",", "8", ")", "\n", "image", "=", "inputs", "[", "0", "]", "[", "\"image\"", "]", "\n", "inputs", "=", "[", "{", "\"image\"", ":", "image", "}", "]", "# remove other unused keys", "\n", "\n", "if", "isinstance", "(", "torch_model", ",", "GeneralizedRCNN", ")", ":", "\n", "\n", "        ", "def", "inference", "(", "model", ",", "inputs", ")", ":", "\n", "# use do_postprocess=False so it returns ROI mask", "\n", "            ", "inst", "=", "model", ".", "inference", "(", "inputs", ",", "do_postprocess", "=", "False", ")", "[", "0", "]", "\n", "return", "[", "{", "\"instances\"", ":", "inst", "}", "]", "\n", "\n", "", "", "else", ":", "\n", "        ", "inference", "=", "None", "# assume that we just call the model directly", "\n", "\n", "", "traceable_model", "=", "TracingAdapter", "(", "torch_model", ",", "inputs", ",", "inference", ")", "\n", "\n", "if", "args", ".", "format", "==", "\"torchscript\"", ":", "\n", "        ", "ts_model", "=", "torch", ".", "jit", ".", "trace", "(", "traceable_model", ",", "(", "image", ",", ")", ")", "\n", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output", ",", "\"model.ts\"", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "torch", ".", "jit", ".", "save", "(", "ts_model", ",", "f", ")", "\n", "", "dump_torchscript_IR", "(", "ts_model", ",", "args", ".", "output", ")", "\n", "", "elif", "args", ".", "format", "==", "\"onnx\"", ":", "\n", "# NOTE onnx export currently failing in pytorch", "\n", "        ", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output", ",", "\"model.onnx\"", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "torch", ".", "onnx", ".", "export", "(", "traceable_model", ",", "(", "image", ",", ")", ",", "f", ")", "\n", "", "", "logger", ".", "info", "(", "\"Inputs schema: \"", "+", "str", "(", "traceable_model", ".", "inputs_schema", ")", ")", "\n", "logger", ".", "info", "(", "\"Outputs schema: \"", "+", "str", "(", "traceable_model", ".", "outputs_schema", ")", ")", "\n", "\n", "if", "args", ".", "format", "!=", "\"torchscript\"", ":", "\n", "        ", "return", "None", "\n", "", "if", "not", "isinstance", "(", "torch_model", ",", "(", "GeneralizedRCNN", ",", "RetinaNet", ")", ")", ":", "\n", "        ", "return", "None", "\n", "\n", "", "def", "eval_wrapper", "(", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        The exported model does not contain the final resize step, which is typically\n        unused in deployment but needed for evaluation. We add it manually here.\n        \"\"\"", "\n", "input", "=", "inputs", "[", "0", "]", "\n", "instances", "=", "traceable_model", ".", "outputs_schema", "(", "ts_model", "(", "input", "[", "\"image\"", "]", ")", ")", "[", "0", "]", "[", "\"instances\"", "]", "\n", "postprocessed", "=", "detector_postprocess", "(", "instances", ",", "input", "[", "\"height\"", "]", ",", "input", "[", "\"width\"", "]", ")", "\n", "return", "[", "{", "\"instances\"", ":", "postprocessed", "}", "]", "\n", "\n", "", "return", "eval_wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.deploy.export_model.get_sample_inputs": [[155, 178], ["detectron2.data.build_detection_test_loader", "next", "detectron2.data.detection_utils.read_image", "detectron2.ResizeShortestEdge", "T.ResizeShortestEdge.get_transform().apply_image", "torch.as_tensor", "iter", "torch.as_tensor.astype().transpose", "T.ResizeShortestEdge.get_transform", "torch.as_tensor.astype"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_test_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.read_image", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.get_transform"], ["", "def", "get_sample_inputs", "(", "args", ")", ":", "\n", "\n", "    ", "if", "args", ".", "sample_image", "is", "None", ":", "\n", "# get a first batch from dataset", "\n", "        ", "data_loader", "=", "build_detection_test_loader", "(", "cfg", ",", "cfg", ".", "DATASETS", ".", "TEST", "[", "0", "]", ")", "\n", "first_batch", "=", "next", "(", "iter", "(", "data_loader", ")", ")", "\n", "return", "first_batch", "\n", "", "else", ":", "\n", "# get a sample data", "\n", "        ", "original_image", "=", "detection_utils", ".", "read_image", "(", "args", ".", "sample_image", ",", "format", "=", "cfg", ".", "INPUT", ".", "FORMAT", ")", "\n", "# Do same preprocessing as DefaultPredictor", "\n", "aug", "=", "T", ".", "ResizeShortestEdge", "(", "\n", "[", "cfg", ".", "INPUT", ".", "MIN_SIZE_TEST", ",", "cfg", ".", "INPUT", ".", "MIN_SIZE_TEST", "]", ",", "cfg", ".", "INPUT", ".", "MAX_SIZE_TEST", "\n", ")", "\n", "height", ",", "width", "=", "original_image", ".", "shape", "[", ":", "2", "]", "\n", "image", "=", "aug", ".", "get_transform", "(", "original_image", ")", ".", "apply_image", "(", "original_image", ")", "\n", "image", "=", "torch", ".", "as_tensor", "(", "image", ".", "astype", "(", "\"float32\"", ")", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "\n", "inputs", "=", "{", "\"image\"", ":", "image", ",", "\"height\"", ":", "height", ",", "\"width\"", ":", "width", "}", "\n", "\n", "# Sample ready", "\n", "sample_inputs", "=", "[", "inputs", "]", "\n", "return", "sample_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.serialize.PicklableWrapper.__init__": [[15, 17], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "obj", ")", ":", "\n", "        ", "self", ".", "_obj", "=", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.serialize.PicklableWrapper.__reduce__": [[18, 21], ["cloudpickle.dumps"], "methods", ["None"], ["", "def", "__reduce__", "(", "self", ")", ":", "\n", "        ", "s", "=", "cloudpickle", ".", "dumps", "(", "self", ".", "_obj", ")", "\n", "return", "cloudpickle", ".", "loads", ",", "(", "s", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.serialize.PicklableWrapper.__call__": [[22, 24], ["serialize.PicklableWrapper._obj"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "_obj", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.serialize.PicklableWrapper.__getattr__": [[25, 30], ["getattr", "getattr"], "methods", ["None"], ["", "def", "__getattr__", "(", "self", ",", "attr", ")", ":", "\n", "# Ensure that the wrapped object can be used seamlessly as the previous object.", "\n", "        ", "if", "attr", "not", "in", "[", "\"_obj\"", "]", ":", "\n", "            ", "return", "getattr", "(", "self", ".", "_obj", ",", "attr", ")", "\n", "", "return", "getattr", "(", "self", ",", "attr", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.colormap.colormap": [[95, 109], ["None"], "function", ["None"], ["def", "colormap", "(", "rgb", "=", "False", ",", "maximum", "=", "255", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        rgb (bool): whether to return RGB colors or BGR colors.\n        maximum (int): either 255 or 1\n\n    Returns:\n        ndarray: a float32 array of Nx3 colors, in range [0, 255] or [0, 1]\n    \"\"\"", "\n", "assert", "maximum", "in", "[", "255", ",", "1", "]", ",", "maximum", "\n", "c", "=", "_COLORS", "*", "maximum", "\n", "if", "not", "rgb", ":", "\n", "        ", "c", "=", "c", "[", ":", ",", ":", ":", "-", "1", "]", "\n", "", "return", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.colormap.random_color": [[111, 125], ["numpy.random.randint", "len"], "function", ["None"], ["", "def", "random_color", "(", "rgb", "=", "False", ",", "maximum", "=", "255", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        rgb (bool): whether to return RGB colors or BGR colors.\n        maximum (int): either 255 or 1\n\n    Returns:\n        ndarray: a vector of 3 numbers\n    \"\"\"", "\n", "idx", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "_COLORS", ")", ")", "\n", "ret", "=", "_COLORS", "[", "idx", "]", "*", "maximum", "\n", "if", "not", "rgb", ":", "\n", "        ", "ret", "=", "ret", "[", ":", ":", "-", "1", "]", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventWriter.write": [[43, 45], ["None"], "methods", ["None"], ["def", "write", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventWriter.close": [[46, 48], ["None"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.JSONWriter.__init__": [[94, 104], ["detectron2.utils.file_io.PathManager.open"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "json_file", ",", "window_size", "=", "20", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            json_file (str): path to the json file. New data will be appended if the file exists.\n            window_size (int): the window size of median smoothing for the scalars whose\n                `smoothing_hint` are True.\n        \"\"\"", "\n", "self", ".", "_file_handle", "=", "PathManager", ".", "open", "(", "json_file", ",", "\"a\"", ")", "\n", "self", ".", "_window_size", "=", "window_size", "\n", "self", ".", "_last_write", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.JSONWriter.write": [[105, 126], ["events.get_event_storage", "collections.defaultdict", "get_event_storage.latest_with_smoothing_hint().items", "len", "collections.defaultdict.items", "events.JSONWriter._file_handle.flush", "sorted", "max", "events.JSONWriter._file_handle.write", "os.fsync", "get_event_storage.latest_with_smoothing_hint", "collections.defaultdict.keys", "events.JSONWriter._file_handle.fileno", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.latest_with_smoothing_hint"], ["", "def", "write", "(", "self", ")", ":", "\n", "        ", "storage", "=", "get_event_storage", "(", ")", "\n", "to_save", "=", "defaultdict", "(", "dict", ")", "\n", "\n", "for", "k", ",", "(", "v", ",", "iter", ")", "in", "storage", ".", "latest_with_smoothing_hint", "(", "self", ".", "_window_size", ")", ".", "items", "(", ")", ":", "\n", "# keep scalars that have not been written", "\n", "            ", "if", "iter", "<=", "self", ".", "_last_write", ":", "\n", "                ", "continue", "\n", "", "to_save", "[", "iter", "]", "[", "k", "]", "=", "v", "\n", "", "if", "len", "(", "to_save", ")", ":", "\n", "            ", "all_iters", "=", "sorted", "(", "to_save", ".", "keys", "(", ")", ")", "\n", "self", ".", "_last_write", "=", "max", "(", "all_iters", ")", "\n", "\n", "", "for", "itr", ",", "scalars_per_iter", "in", "to_save", ".", "items", "(", ")", ":", "\n", "            ", "scalars_per_iter", "[", "\"iteration\"", "]", "=", "itr", "\n", "self", ".", "_file_handle", ".", "write", "(", "json", ".", "dumps", "(", "scalars_per_iter", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", ")", "\n", "", "self", ".", "_file_handle", ".", "flush", "(", ")", "\n", "try", ":", "\n", "            ", "os", ".", "fsync", "(", "self", ".", "_file_handle", ".", "fileno", "(", ")", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.JSONWriter.close": [[127, 129], ["events.JSONWriter._file_handle.close"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close"], ["", "", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "_file_handle", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.__init__": [[136, 149], ["SummaryWriter"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "log_dir", ":", "str", ",", "window_size", ":", "int", "=", "20", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            log_dir (str): the directory to save the output events\n            window_size (int): the scalars will be median-smoothed by this window size\n\n            kwargs: other arguments passed to `torch.utils.tensorboard.SummaryWriter(...)`\n        \"\"\"", "\n", "self", ".", "_window_size", "=", "window_size", "\n", "from", "torch", ".", "utils", ".", "tensorboard", "import", "SummaryWriter", "\n", "\n", "self", ".", "_writer", "=", "SummaryWriter", "(", "log_dir", ",", "**", "kwargs", ")", "\n", "self", ".", "_last_write", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.write": [[150, 175], ["events.get_event_storage", "get_event_storage.latest_with_smoothing_hint().items", "len", "get_event_storage.clear_images", "len", "get_event_storage.clear_histograms", "get_event_storage.latest_with_smoothing_hint", "events.TensorboardXWriter._writer.add_scalar", "max", "events.TensorboardXWriter._writer.add_image", "events.TensorboardXWriter._writer.add_histogram_raw"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.clear_images", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.clear_histograms", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.latest_with_smoothing_hint"], ["", "def", "write", "(", "self", ")", ":", "\n", "        ", "storage", "=", "get_event_storage", "(", ")", "\n", "new_last_write", "=", "self", ".", "_last_write", "\n", "for", "k", ",", "(", "v", ",", "iter", ")", "in", "storage", ".", "latest_with_smoothing_hint", "(", "self", ".", "_window_size", ")", ".", "items", "(", ")", ":", "\n", "            ", "if", "iter", ">", "self", ".", "_last_write", ":", "\n", "                ", "self", ".", "_writer", ".", "add_scalar", "(", "k", ",", "v", ",", "iter", ")", "\n", "new_last_write", "=", "max", "(", "new_last_write", ",", "iter", ")", "\n", "", "", "self", ".", "_last_write", "=", "new_last_write", "\n", "\n", "# storage.put_{image,histogram} is only meant to be used by", "\n", "# tensorboard writer. So we access its internal fields directly from here.", "\n", "if", "len", "(", "storage", ".", "_vis_data", ")", ">=", "1", ":", "\n", "            ", "for", "img_name", ",", "img", ",", "step_num", "in", "storage", ".", "_vis_data", ":", "\n", "                ", "self", ".", "_writer", ".", "add_image", "(", "img_name", ",", "img", ",", "step_num", ")", "\n", "# Storage stores all image data and rely on this writer to clear them.", "\n", "# As a result it assumes only one writer will use its image data.", "\n", "# An alternative design is to let storage store limited recent", "\n", "# data (e.g. only the most recent image) that all writers can access.", "\n", "# In that case a writer may not see all image data if its period is long.", "\n", "", "storage", ".", "clear_images", "(", ")", "\n", "\n", "", "if", "len", "(", "storage", ".", "_histograms", ")", ">=", "1", ":", "\n", "            ", "for", "params", "in", "storage", ".", "_histograms", ":", "\n", "                ", "self", ".", "_writer", ".", "add_histogram_raw", "(", "**", "params", ")", "\n", "", "storage", ".", "clear_histograms", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close": [[176, 179], ["hasattr", "events.TensorboardXWriter._writer.close"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close"], ["", "", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ",", "\"_writer\"", ")", ":", "# doesn't exist when the code fails at import", "\n", "            ", "self", ".", "_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.__init__": [[191, 202], ["logging.getLogger"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_iter", ":", "Optional", "[", "int", "]", "=", "None", ",", "window_size", ":", "int", "=", "20", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            max_iter: the maximum number of iterations to train.\n                Used to compute ETA. If not given, ETA will not be printed.\n            window_size (int): the losses will be median-smoothed by this window size\n        \"\"\"", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "self", ".", "_max_iter", "=", "max_iter", "\n", "self", ".", "_window_size", "=", "window_size", "\n", "self", ".", "_last_write", "=", "None", "# (step, time) of last call to write(). Used to compute ETA", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter._get_eta": [[203, 222], ["storage.put_scalar", "str", "storage.history().median", "datetime.timedelta", "str", "time.perf_counter", "storage.history", "int", "datetime.timedelta", "time.perf_counter", "int"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.history"], ["", "def", "_get_eta", "(", "self", ",", "storage", ")", "->", "Optional", "[", "str", "]", ":", "\n", "        ", "if", "self", ".", "_max_iter", "is", "None", ":", "\n", "            ", "return", "\"\"", "\n", "", "iteration", "=", "storage", ".", "iter", "\n", "try", ":", "\n", "            ", "eta_seconds", "=", "storage", ".", "history", "(", "\"time\"", ")", ".", "median", "(", "1000", ")", "*", "(", "self", ".", "_max_iter", "-", "iteration", "-", "1", ")", "\n", "storage", ".", "put_scalar", "(", "\"eta_seconds\"", ",", "eta_seconds", ",", "smoothing_hint", "=", "False", ")", "\n", "return", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_seconds", ")", ")", ")", "\n", "", "except", "KeyError", ":", "\n", "# estimate eta on our own - more noisy", "\n", "            ", "eta_string", "=", "None", "\n", "if", "self", ".", "_last_write", "is", "not", "None", ":", "\n", "                ", "estimate_iter_time", "=", "(", "time", ".", "perf_counter", "(", ")", "-", "self", ".", "_last_write", "[", "1", "]", ")", "/", "(", "\n", "iteration", "-", "self", ".", "_last_write", "[", "0", "]", "\n", ")", "\n", "eta_seconds", "=", "estimate_iter_time", "*", "(", "self", ".", "_max_iter", "-", "iteration", "-", "1", ")", "\n", "eta_string", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_seconds", ")", ")", ")", "\n", "", "self", ".", "_last_write", "=", "(", "iteration", ",", "time", ".", "perf_counter", "(", ")", ")", "\n", "return", "eta_string", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write": [[223, 270], ["events.get_event_storage", "events.CommonMetricPrinter._get_eta", "torch.cuda.is_available", "events.CommonMetricPrinter.logger.info", "get_event_storage.history().avg", "get_event_storage.history().global_avg", "get_event_storage.history().latest", "get_event_storage.history", "get_event_storage.history", "torch.cuda.max_memory_allocated", "get_event_storage.history", "v.median", "get_event_storage.histories().items", "get_event_storage.histories"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter._get_eta", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.latest", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.history", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.history", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.history", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.histories"], ["", "", "def", "write", "(", "self", ")", ":", "\n", "        ", "storage", "=", "get_event_storage", "(", ")", "\n", "iteration", "=", "storage", ".", "iter", "\n", "if", "iteration", "==", "self", ".", "_max_iter", ":", "\n", "# This hook only reports training progress (loss, ETA, etc) but not other data,", "\n", "# therefore do not write anything after training succeeds, even if this method", "\n", "# is called.", "\n", "            ", "return", "\n", "\n", "", "try", ":", "\n", "            ", "data_time", "=", "storage", ".", "history", "(", "\"data_time\"", ")", ".", "avg", "(", "20", ")", "\n", "", "except", "KeyError", ":", "\n", "# they may not exist in the first few iterations (due to warmup)", "\n", "# or when SimpleTrainer is not used", "\n", "            ", "data_time", "=", "None", "\n", "", "try", ":", "\n", "            ", "iter_time", "=", "storage", ".", "history", "(", "\"time\"", ")", ".", "global_avg", "(", ")", "\n", "", "except", "KeyError", ":", "\n", "            ", "iter_time", "=", "None", "\n", "", "try", ":", "\n", "            ", "lr", "=", "\"{:.5g}\"", ".", "format", "(", "storage", ".", "history", "(", "\"lr\"", ")", ".", "latest", "(", ")", ")", "\n", "", "except", "KeyError", ":", "\n", "            ", "lr", "=", "\"N/A\"", "\n", "\n", "", "eta_string", "=", "self", ".", "_get_eta", "(", "storage", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "max_mem_mb", "=", "torch", ".", "cuda", ".", "max_memory_allocated", "(", ")", "/", "1024.0", "/", "1024.0", "\n", "", "else", ":", "\n", "            ", "max_mem_mb", "=", "None", "\n", "\n", "# NOTE: max_mem is parsed by grep in \"dev/parse_results.sh\"", "\n", "", "self", ".", "logger", ".", "info", "(", "\n", "\" {eta}iter: {iter}  {losses}  {time}{data_time}lr: {lr}  {memory}\"", ".", "format", "(", "\n", "eta", "=", "f\"eta: {eta_string}  \"", "if", "eta_string", "else", "\"\"", ",", "\n", "iter", "=", "iteration", ",", "\n", "losses", "=", "\"  \"", ".", "join", "(", "\n", "[", "\n", "\"{}: {:.4g}\"", ".", "format", "(", "k", ",", "v", ".", "median", "(", "self", ".", "_window_size", ")", ")", "\n", "for", "k", ",", "v", "in", "storage", ".", "histories", "(", ")", ".", "items", "(", ")", "\n", "if", "\"loss\"", "in", "k", "\n", "]", "\n", ")", ",", "\n", "time", "=", "\"time: {:.4f}  \"", ".", "format", "(", "iter_time", ")", "if", "iter_time", "is", "not", "None", "else", "\"\"", ",", "\n", "data_time", "=", "\"data_time: {:.4f}  \"", ".", "format", "(", "data_time", ")", "if", "data_time", "is", "not", "None", "else", "\"\"", ",", "\n", "lr", "=", "lr", ",", "\n", "memory", "=", "\"max_mem: {:.0f}M\"", ".", "format", "(", "max_mem_mb", ")", "if", "max_mem_mb", "is", "not", "None", "else", "\"\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.__init__": [[281, 293], ["collections.defaultdict"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "start_iter", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            start_iter (int): the iteration number to start with\n        \"\"\"", "\n", "self", ".", "_history", "=", "defaultdict", "(", "HistoryBuffer", ")", "\n", "self", ".", "_smoothing_hints", "=", "{", "}", "\n", "self", ".", "_latest_scalars", "=", "{", "}", "\n", "self", ".", "_iter", "=", "start_iter", "\n", "self", ".", "_current_prefix", "=", "\"\"", "\n", "self", ".", "_vis_data", "=", "[", "]", "\n", "self", ".", "_histograms", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_image": [[294, 308], ["events.EventStorage._vis_data.append"], "methods", ["None"], ["", "def", "put_image", "(", "self", ",", "img_name", ",", "img_tensor", ")", ":", "\n", "        ", "\"\"\"\n        Add an `img_tensor` associated with `img_name`, to be shown on\n        tensorboard.\n\n        Args:\n            img_name (str): The name of the image to put into tensorboard.\n            img_tensor (torch.Tensor or numpy.array): An `uint8` or `float`\n                Tensor of shape `[channel, height, width]` where `channel` is\n                3. The image format should be RGB. The elements in img_tensor\n                can either have values in [0, 1] (float32) or [0, 255] (uint8).\n                The `img_tensor` will be visualized in tensorboard.\n        \"\"\"", "\n", "self", ".", "_vis_data", ".", "append", "(", "(", "img_name", ",", "img_tensor", ",", "self", ".", "_iter", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar": [[309, 335], ["float", "history.update", "events.EventStorage._smoothing_hints.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "put_scalar", "(", "self", ",", "name", ",", "value", ",", "smoothing_hint", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Add a scalar `value` to the `HistoryBuffer` associated with `name`.\n\n        Args:\n            smoothing_hint (bool): a 'hint' on whether this scalar is noisy and should be\n                smoothed when logged. The hint will be accessible through\n                :meth:`EventStorage.smoothing_hints`.  A writer may ignore the hint\n                and apply custom smoothing rule.\n\n                It defaults to True because most scalars we save need to be smoothed to\n                provide any useful signal.\n        \"\"\"", "\n", "name", "=", "self", ".", "_current_prefix", "+", "name", "\n", "history", "=", "self", ".", "_history", "[", "name", "]", "\n", "value", "=", "float", "(", "value", ")", "\n", "history", ".", "update", "(", "value", ",", "self", ".", "_iter", ")", "\n", "self", ".", "_latest_scalars", "[", "name", "]", "=", "(", "value", ",", "self", ".", "_iter", ")", "\n", "\n", "existing_hint", "=", "self", ".", "_smoothing_hints", ".", "get", "(", "name", ")", "\n", "if", "existing_hint", "is", "not", "None", ":", "\n", "            ", "assert", "(", "\n", "existing_hint", "==", "smoothing_hint", "\n", ")", ",", "\"Scalar {} was put with a different smoothing_hint!\"", ".", "format", "(", "name", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_smoothing_hints", "[", "name", "]", "=", "smoothing_hint", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalars": [[336, 346], ["kwargs.items", "events.EventStorage.put_scalar"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar"], ["", "", "def", "put_scalars", "(", "self", ",", "*", ",", "smoothing_hint", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Put multiple scalars from keyword arguments.\n\n        Examples:\n\n            storage.put_scalars(loss=my_loss, accuracy=my_accuracy, smoothing_hint=True)\n        \"\"\"", "\n", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "put_scalar", "(", "k", ",", "v", ",", "smoothing_hint", "=", "smoothing_hint", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_histogram": [[347, 376], ["torch.histc", "torch.linspace", "dict", "events.EventStorage._histograms.append", "hist_tensor.min().item", "hist_tensor.max().item", "len", "float", "float", "hist_edges[].tolist", "torch.histc.tolist", "hist_tensor.min", "hist_tensor.max", "hist_tensor.sum", "torch.sum"], "methods", ["None"], ["", "", "def", "put_histogram", "(", "self", ",", "hist_name", ",", "hist_tensor", ",", "bins", "=", "1000", ")", ":", "\n", "        ", "\"\"\"\n        Create a histogram from a tensor.\n\n        Args:\n            hist_name (str): The name of the histogram to put into tensorboard.\n            hist_tensor (torch.Tensor): A Tensor of arbitrary shape to be converted\n                into a histogram.\n            bins (int): Number of histogram bins.\n        \"\"\"", "\n", "ht_min", ",", "ht_max", "=", "hist_tensor", ".", "min", "(", ")", ".", "item", "(", ")", ",", "hist_tensor", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "\n", "# Create a histogram with PyTorch", "\n", "hist_counts", "=", "torch", ".", "histc", "(", "hist_tensor", ",", "bins", "=", "bins", ")", "\n", "hist_edges", "=", "torch", ".", "linspace", "(", "start", "=", "ht_min", ",", "end", "=", "ht_max", ",", "steps", "=", "bins", "+", "1", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "# Parameter for the add_histogram_raw function of SummaryWriter", "\n", "hist_params", "=", "dict", "(", "\n", "tag", "=", "hist_name", ",", "\n", "min", "=", "ht_min", ",", "\n", "max", "=", "ht_max", ",", "\n", "num", "=", "len", "(", "hist_tensor", ")", ",", "\n", "sum", "=", "float", "(", "hist_tensor", ".", "sum", "(", ")", ")", ",", "\n", "sum_squares", "=", "float", "(", "torch", ".", "sum", "(", "hist_tensor", "**", "2", ")", ")", ",", "\n", "bucket_limits", "=", "hist_edges", "[", "1", ":", "]", ".", "tolist", "(", ")", ",", "\n", "bucket_counts", "=", "hist_counts", ".", "tolist", "(", ")", ",", "\n", "global_step", "=", "self", ".", "_iter", ",", "\n", ")", "\n", "self", ".", "_histograms", ".", "append", "(", "hist_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.history": [[377, 386], ["events.EventStorage._history.get", "KeyError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "history", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            HistoryBuffer: the scalar history for name\n        \"\"\"", "\n", "ret", "=", "self", ".", "_history", ".", "get", "(", "name", ",", "None", ")", "\n", "if", "ret", "is", "None", ":", "\n", "            ", "raise", "KeyError", "(", "\"No history metric available for {}!\"", ".", "format", "(", "name", ")", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.histories": [[387, 393], ["None"], "methods", ["None"], ["", "def", "histories", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            dict[name -> HistoryBuffer]: the HistoryBuffer for all scalars\n        \"\"\"", "\n", "return", "self", ".", "_history", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.latest": [[394, 401], ["None"], "methods", ["None"], ["", "def", "latest", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            dict[str -> (float, int)]: mapping from the name of each scalar to the most\n                recent value and the iteration number its added.\n        \"\"\"", "\n", "return", "self", ".", "_latest_scalars", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.latest_with_smoothing_hint": [[402, 418], ["events.EventStorage._latest_scalars.items", "events.EventStorage._history[].median"], "methods", ["None"], ["", "def", "latest_with_smoothing_hint", "(", "self", ",", "window_size", "=", "20", ")", ":", "\n", "        ", "\"\"\"\n        Similar to :meth:`latest`, but the returned values\n        are either the un-smoothed original latest value,\n        or a median of the given window_size,\n        depend on whether the smoothing_hint is True.\n\n        This provides a default behavior that other writers can use.\n        \"\"\"", "\n", "result", "=", "{", "}", "\n", "for", "k", ",", "(", "v", ",", "itr", ")", "in", "self", ".", "_latest_scalars", ".", "items", "(", ")", ":", "\n", "            ", "result", "[", "k", "]", "=", "(", "\n", "self", ".", "_history", "[", "k", "]", ".", "median", "(", "window_size", ")", "if", "self", ".", "_smoothing_hints", "[", "k", "]", "else", "v", ",", "\n", "itr", ",", "\n", ")", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.smoothing_hints": [[419, 426], ["None"], "methods", ["None"], ["", "def", "smoothing_hints", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            dict[name -> bool]: the user-provided hint on whether the scalar\n                is noisy and needs smoothing.\n        \"\"\"", "\n", "return", "self", ".", "_smoothing_hints", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step": [[427, 435], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        User should either: (1) Call this function to increment storage.iter when needed. Or\n        (2) Set `storage.iter` to the correct iteration number before each iteration.\n\n        The storage will then be able to associate the new data with an iteration number.\n        \"\"\"", "\n", "self", ".", "_iter", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter": [[445, 448], ["int"], "methods", ["None"], ["", "@", "iter", ".", "setter", "\n", "def", "iter", "(", "self", ",", "val", ")", ":", "\n", "        ", "self", ".", "_iter", "=", "int", "(", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iteration": [[449, 453], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "iteration", "(", "self", ")", ":", "\n", "# for backward compatibility", "\n", "        ", "return", "self", ".", "_iter", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.__enter__": [[454, 457], ["_CURRENT_STORAGE_STACK.append"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "_CURRENT_STORAGE_STACK", ".", "append", "(", "self", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.__exit__": [[458, 461], ["_CURRENT_STORAGE_STACK.pop"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "exc_type", ",", "exc_val", ",", "exc_tb", ")", ":", "\n", "        ", "assert", "_CURRENT_STORAGE_STACK", "[", "-", "1", "]", "==", "self", "\n", "_CURRENT_STORAGE_STACK", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.name_scope": [[462, 473], ["name.rstrip"], "methods", ["None"], ["", "@", "contextmanager", "\n", "def", "name_scope", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Yields:\n            A context within which all the events added to this storage\n            will be prefixed by the name scope.\n        \"\"\"", "\n", "old_prefix", "=", "self", ".", "_current_prefix", "\n", "self", ".", "_current_prefix", "=", "name", ".", "rstrip", "(", "\"/\"", ")", "+", "\"/\"", "\n", "yield", "\n", "self", ".", "_current_prefix", "=", "old_prefix", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.clear_images": [[474, 480], ["None"], "methods", ["None"], ["", "def", "clear_images", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Delete all the stored images for visualization. This should be called\n        after images are written to tensorboard.\n        \"\"\"", "\n", "self", ".", "_vis_data", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.clear_histograms": [[481, 487], ["None"], "methods", ["None"], ["", "def", "clear_histograms", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Delete all the stored histograms for visualization.\n        This should be called after histograms are written to tensorboard.\n        \"\"\"", "\n", "self", ".", "_histograms", "=", "[", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage": [[26, 36], ["len"], "function", ["None"], ["def", "get_event_storage", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns:\n        The :class:`EventStorage` object that's currently being used.\n        Throws an error if no :class:`EventStorage` is currently enabled.\n    \"\"\"", "\n", "assert", "len", "(", "\n", "_CURRENT_STORAGE_STACK", "\n", ")", ",", "\"get_event_storage() has to be called inside a 'with EventStorage(...)' context!\"", "\n", "return", "_CURRENT_STORAGE_STACK", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.SoftTargetCrossEntropy.__init__": [[325, 327], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "SoftTargetCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.SoftTargetCrossEntropy.forward": [[328, 331], ["loss.mean", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "target", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "loss", "=", "torch", ".", "sum", "(", "-", "target", "*", "F", ".", "log_softmax", "(", "x", ",", "dim", "=", "dim", ")", ",", "dim", "=", "dim", ")", "/", "(", "torch", ".", "sum", "(", "target", ",", "dim", "=", "dim", ")", "+", "1e-6", ")", "\n", "return", "loss", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.MILCrossEntropy.__init__": [[336, 338], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "MILCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.MILCrossEntropy.forward": [[339, 356], ["torch.max", "torch.max", "torch.max", "torch.max", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "loss.mean", "logits_max.detach", "torch.exp.sum", "torch.exp.sum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "target", ",", "dim", "=", "-", "1", ",", "weights", "=", "None", ",", "avg_positives", "=", "False", ")", ":", "\n", "# for numerical stability", "\n", "        ", "logits_max", ",", "_", "=", "torch", ".", "max", "(", "x", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "logits", "=", "x", "-", "logits_max", ".", "detach", "(", ")", "\n", "exp_logits", "=", "torch", ".", "exp", "(", "logits", ")", "\n", "\n", "# get non-zero entries off-diagonal", "\n", "# identity = torch.eye(target.shape[0]).type_as(target)", "\n", "# laplacian = 1 - (target - identity)", "\n", "probs", "=", "exp_logits", "/", "(", "exp_logits", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "if", "avg_positives", ":", "# average the logits over positive targets", "\n", "            ", "loss", "=", "-", "torch", ".", "log", "(", "torch", ".", "sum", "(", "target", "*", "probs", ",", "dim", "=", "dim", ")", "/", "(", "torch", ".", "sum", "(", "target", ",", "dim", "=", "dim", ")", "+", "1e-6", ")", ")", "\n", "", "else", ":", "# sum the logits over positive targets", "\n", "            ", "loss", "=", "-", "torch", ".", "log", "(", "torch", ".", "sum", "(", "target", "*", "probs", ",", "dim", "=", "dim", ")", ")", "\n", "", "if", "weights", "is", "not", "None", ":", "\n", "            ", "return", "(", "loss", "*", "weights", ")", ".", "mean", "(", ")", "\n", "", "return", "loss", ".", "mean", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size": [[24, 30], ["torch.get_world_size", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], ["def", "get_world_size", "(", ")", "->", "int", ":", "\n", "    ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "return", "dist", ".", "get_world_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank": [[32, 38], ["torch.get_rank", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["", "def", "get_rank", "(", ")", "->", "int", ":", "\n", "    ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "return", "dist", ".", "get_rank", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_local_rank": [[40, 51], ["torch.get_rank", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["", "def", "get_local_rank", "(", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Returns:\n        The rank of the current process within the local (per-machine) process group.\n    \"\"\"", "\n", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "assert", "_LOCAL_PROCESS_GROUP", "is", "not", "None", "\n", "return", "dist", ".", "get_rank", "(", "group", "=", "_LOCAL_PROCESS_GROUP", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_local_size": [[53, 64], ["torch.get_world_size", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], ["", "def", "get_local_size", "(", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Returns:\n        The size of the per-machine process group,\n        i.e. the number of processes per machine.\n    \"\"\"", "\n", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "return", "dist", ".", "get_world_size", "(", "group", "=", "_LOCAL_PROCESS_GROUP", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process": [[66, 68], ["comm.get_rank"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["", "def", "is_main_process", "(", ")", "->", "bool", ":", "\n", "    ", "return", "get_rank", "(", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize": [[70, 83], ["torch.get_world_size", "torch.barrier", "torch.is_available", "torch.is_initialized"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], ["", "def", "synchronize", "(", ")", ":", "\n", "    ", "\"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"", "\n", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "\n", "", "world_size", "=", "dist", ".", "get_world_size", "(", ")", "\n", "if", "world_size", "==", "1", ":", "\n", "        ", "return", "\n", "", "dist", ".", "barrier", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm._get_global_gloo_group": [[85, 95], ["functools.lru_cache", "torch.get_backend", "torch.new_group"], "function", ["None"], ["", "@", "functools", ".", "lru_cache", "(", ")", "\n", "def", "_get_global_gloo_group", "(", ")", ":", "\n", "    ", "\"\"\"\n    Return a process group based on gloo backend, containing all the ranks\n    The result is cached.\n    \"\"\"", "\n", "if", "dist", ".", "get_backend", "(", ")", "==", "\"nccl\"", ":", "\n", "        ", "return", "dist", ".", "new_group", "(", "backend", "=", "\"gloo\"", ")", "\n", "", "else", ":", "\n", "        ", "return", "dist", ".", "group", ".", "WORLD", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm._serialize_to_tensor": [[97, 113], ["torch.get_backend", "torch.device", "torch.device", "pickle.dumps", "torch.ByteStorage.from_buffer", "torch.ByteStorage.from_buffer", "torch.ByteTensor().to", "torch.ByteTensor().to", "len", "logging.getLogger", "logging.getLogger.warning", "torch.ByteTensor", "torch.ByteTensor", "comm.get_rank", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["", "", "def", "_serialize_to_tensor", "(", "data", ",", "group", ")", ":", "\n", "    ", "backend", "=", "dist", ".", "get_backend", "(", "group", ")", "\n", "assert", "backend", "in", "[", "\"gloo\"", ",", "\"nccl\"", "]", "\n", "device", "=", "torch", ".", "device", "(", "\"cpu\"", "if", "backend", "==", "\"gloo\"", "else", "\"cuda\"", ")", "\n", "\n", "buffer", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "if", "len", "(", "buffer", ")", ">", "1024", "**", "3", ":", "\n", "        ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Rank {} trying to all-gather {:.2f} GB of data on device {}\"", ".", "format", "(", "\n", "get_rank", "(", ")", ",", "len", "(", "buffer", ")", "/", "(", "1024", "**", "3", ")", ",", "device", "\n", ")", "\n", ")", "\n", "", "storage", "=", "torch", ".", "ByteStorage", ".", "from_buffer", "(", "buffer", ")", "\n", "tensor", "=", "torch", ".", "ByteTensor", "(", "storage", ")", ".", "to", "(", "device", "=", "device", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm._pad_to_largest_tensor": [[115, 140], ["torch.get_world_size", "torch.tensor", "torch.tensor", "torch.all_gather", "max", "torch.zeros", "torch.zeros", "int", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat.numel", "range", "size.item"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "_pad_to_largest_tensor", "(", "tensor", ",", "group", ")", ":", "\n", "    ", "\"\"\"\n    Returns:\n        list[int]: size of the tensor, on each rank\n        Tensor: padded tensor that has the max size\n    \"\"\"", "\n", "world_size", "=", "dist", ".", "get_world_size", "(", "group", "=", "group", ")", "\n", "assert", "(", "\n", "world_size", ">=", "1", "\n", ")", ",", "\"comm.gather/all_gather must be called from ranks within the given group!\"", "\n", "local_size", "=", "torch", ".", "tensor", "(", "[", "tensor", ".", "numel", "(", ")", "]", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "tensor", ".", "device", ")", "\n", "size_list", "=", "[", "\n", "torch", ".", "zeros", "(", "[", "1", "]", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "tensor", ".", "device", ")", "for", "_", "in", "range", "(", "world_size", ")", "\n", "]", "\n", "dist", ".", "all_gather", "(", "size_list", ",", "local_size", ",", "group", "=", "group", ")", "\n", "size_list", "=", "[", "int", "(", "size", ".", "item", "(", ")", ")", "for", "size", "in", "size_list", "]", "\n", "\n", "max_size", "=", "max", "(", "size_list", ")", "\n", "\n", "# we pad the tensor because torch all_gather does not support", "\n", "# gathering tensors of different shapes", "\n", "if", "local_size", "!=", "max_size", ":", "\n", "        ", "padding", "=", "torch", ".", "zeros", "(", "(", "max_size", "-", "local_size", ",", ")", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "tensor", ".", "device", ")", "\n", "tensor", "=", "torch", ".", "cat", "(", "(", "tensor", ",", "padding", ")", ",", "dim", "=", "0", ")", "\n", "", "return", "size_list", ",", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather": [[142, 178], ["comm._serialize_to_tensor", "comm._pad_to_largest_tensor", "max", "torch.all_gather", "zip", "comm.get_world_size", "comm._get_global_gloo_group", "torch.get_world_size", "torch.empty", "torch.empty", "data_list.append", "_serialize_to_tensor.cpu().numpy().tobytes", "pickle.loads", "_serialize_to_tensor.cpu().numpy", "_serialize_to_tensor.cpu"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm._serialize_to_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm._pad_to_largest_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm._get_global_gloo_group", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], ["", "def", "all_gather", "(", "data", ",", "group", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors).\n\n    Args:\n        data: any picklable object\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"", "\n", "if", "get_world_size", "(", ")", "==", "1", ":", "\n", "        ", "return", "[", "data", "]", "\n", "", "if", "group", "is", "None", ":", "\n", "        ", "group", "=", "_get_global_gloo_group", "(", ")", "\n", "", "if", "dist", ".", "get_world_size", "(", "group", ")", "==", "1", ":", "\n", "        ", "return", "[", "data", "]", "\n", "\n", "", "tensor", "=", "_serialize_to_tensor", "(", "data", ",", "group", ")", "\n", "\n", "size_list", ",", "tensor", "=", "_pad_to_largest_tensor", "(", "tensor", ",", "group", ")", "\n", "max_size", "=", "max", "(", "size_list", ")", "\n", "\n", "# receiving Tensor from all ranks", "\n", "tensor_list", "=", "[", "\n", "torch", ".", "empty", "(", "(", "max_size", ",", ")", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "tensor", ".", "device", ")", "for", "_", "in", "size_list", "\n", "]", "\n", "dist", ".", "all_gather", "(", "tensor_list", ",", "tensor", ",", "group", "=", "group", ")", "\n", "\n", "data_list", "=", "[", "]", "\n", "for", "size", ",", "tensor", "in", "zip", "(", "size_list", ",", "tensor_list", ")", ":", "\n", "        ", "buffer", "=", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tobytes", "(", ")", "[", ":", "size", "]", "\n", "data_list", ".", "append", "(", "pickle", ".", "loads", "(", "buffer", ")", ")", "\n", "\n", "", "return", "data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather": [[180, 221], ["torch.get_rank", "comm._serialize_to_tensor", "comm._pad_to_largest_tensor", "comm.get_world_size", "comm._get_global_gloo_group", "torch.get_world_size", "max", "torch.gather", "zip", "torch.gather", "torch.empty", "torch.empty", "data_list.append", "_serialize_to_tensor.cpu().numpy().tobytes", "pickle.loads", "_serialize_to_tensor.cpu().numpy", "_serialize_to_tensor.cpu"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm._serialize_to_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm._pad_to_largest_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm._get_global_gloo_group", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather"], ["", "def", "gather", "(", "data", ",", "dst", "=", "0", ",", "group", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Run gather on arbitrary picklable data (not necessarily tensors).\n\n    Args:\n        data: any picklable object\n        dst (int): destination rank\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n\n    Returns:\n        list[data]: on dst, a list of data gathered from each rank. Otherwise,\n            an empty list.\n    \"\"\"", "\n", "if", "get_world_size", "(", ")", "==", "1", ":", "\n", "        ", "return", "[", "data", "]", "\n", "", "if", "group", "is", "None", ":", "\n", "        ", "group", "=", "_get_global_gloo_group", "(", ")", "\n", "", "if", "dist", ".", "get_world_size", "(", "group", "=", "group", ")", "==", "1", ":", "\n", "        ", "return", "[", "data", "]", "\n", "", "rank", "=", "dist", ".", "get_rank", "(", "group", "=", "group", ")", "\n", "\n", "tensor", "=", "_serialize_to_tensor", "(", "data", ",", "group", ")", "\n", "size_list", ",", "tensor", "=", "_pad_to_largest_tensor", "(", "tensor", ",", "group", ")", "\n", "\n", "# receiving Tensor from all ranks", "\n", "if", "rank", "==", "dst", ":", "\n", "        ", "max_size", "=", "max", "(", "size_list", ")", "\n", "tensor_list", "=", "[", "\n", "torch", ".", "empty", "(", "(", "max_size", ",", ")", ",", "dtype", "=", "torch", ".", "uint8", ",", "device", "=", "tensor", ".", "device", ")", "for", "_", "in", "size_list", "\n", "]", "\n", "dist", ".", "gather", "(", "tensor", ",", "tensor_list", ",", "dst", "=", "dst", ",", "group", "=", "group", ")", "\n", "\n", "data_list", "=", "[", "]", "\n", "for", "size", ",", "tensor", "in", "zip", "(", "size_list", ",", "tensor_list", ")", ":", "\n", "            ", "buffer", "=", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tobytes", "(", ")", "[", ":", "size", "]", "\n", "data_list", ".", "append", "(", "pickle", ".", "loads", "(", "buffer", ")", ")", "\n", "", "return", "data_list", "\n", "", "else", ":", "\n", "        ", "dist", ".", "gather", "(", "tensor", ",", "[", "]", ",", "dst", "=", "dst", ",", "group", "=", "group", ")", "\n", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.shared_random_seed": [[223, 235], ["numpy.random.randint", "comm.all_gather"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather"], ["", "", "def", "shared_random_seed", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns:\n        int: a random number that is the same across all workers.\n        If workers need a shared RNG, they can use this shared seed to\n        create one.\n\n    All workers must call this function, otherwise it will deadlock.\n    \"\"\"", "\n", "ints", "=", "np", ".", "random", ".", "randint", "(", "2", "**", "31", ")", "\n", "all_ints", "=", "all_gather", "(", "ints", ")", "\n", "return", "all_ints", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.reduce_dict": [[237, 267], ["comm.get_world_size", "torch.no_grad", "torch.no_grad", "sorted", "torch.stack", "torch.stack", "torch.reduce", "input_dict.keys", "names.append", "torch.stack.append", "torch.get_rank", "zip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["", "def", "reduce_dict", "(", "input_dict", ",", "average", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Reduce the values in the dictionary from all processes so that process with rank\n    0 has the reduced results.\n\n    Args:\n        input_dict (dict): inputs to be reduced. All the values must be scalar CUDA Tensor.\n        average (bool): whether to do average or sum\n\n    Returns:\n        a dict with the same keys as input_dict, after reduction.\n    \"\"\"", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "if", "world_size", "<", "2", ":", "\n", "        ", "return", "input_dict", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "names", "=", "[", "]", "\n", "values", "=", "[", "]", "\n", "# sort the keys so that they are consistent across processes", "\n", "for", "k", "in", "sorted", "(", "input_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "names", ".", "append", "(", "k", ")", "\n", "values", ".", "append", "(", "input_dict", "[", "k", "]", ")", "\n", "", "values", "=", "torch", ".", "stack", "(", "values", ",", "dim", "=", "0", ")", "\n", "dist", ".", "reduce", "(", "values", ",", "dst", "=", "0", ")", "\n", "if", "dist", ".", "get_rank", "(", ")", "==", "0", "and", "average", ":", "\n", "# only main process gets accumulated, so only divide by", "\n", "# world_size in this case", "\n", "            ", "values", "/=", "world_size", "\n", "", "reduced_dict", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "names", ",", "values", ")", "}", "\n", "", "return", "reduced_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather_tensors": [[268, 323], ["comm.get_world_size", "comm.get_rank", "torch.tensor", "torch.tensor", "torch.all_gather", "min", "torch.zeros_like", "torch.zeros_like", "torch.bmm", "torch.bmm", "torch.cat.view", "range", "bs.item", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.svd_lowrank", "torch.svd_lowrank", "_gather_tensor", "_gather_tensor", "_gather_tensor", "torch.bmm", "torch.bmm", "diffdist.functional.all_gather", "torch.cat", "torch.cat", "tensor.cpu", "U.to", "torch.diag", "torch.diag", "torch.ones_like", "torch.ones_like", "int", "Sig.to", "V.to", "range", "method.split"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "gather_tensors", "(", "tensor", ",", "method", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\"\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "rank", "=", "get_rank", "(", ")", "\n", "\n", "if", "world_size", "<=", "1", ":", "\n", "        ", "return", "tensor", ",", "tensor", ".", "shape", "[", "0", "]", "\n", "\n", "", "batch_size", "=", "torch", ".", "tensor", "(", "tensor", ".", "shape", "[", "0", "]", ",", "device", "=", "tensor", ".", "device", ")", "\n", "batch_size_full", "=", "[", "torch", ".", "zeros_like", "(", "batch_size", ")", "\n", "for", "_", "in", "range", "(", "world_size", ")", "]", "\n", "dist", ".", "all_gather", "(", "batch_size_full", ",", "batch_size", ")", "\n", "\n", "# cutting all data to min batch size across all GPUs", "\n", "min_bs", "=", "min", "(", "[", "bs", ".", "item", "(", ")", "for", "bs", "in", "batch_size_full", "]", ")", "\n", "if", "min_bs", "<", "batch_size", ":", "\n", "        ", "tensor", "=", "tensor", "[", ":", "min_bs", "]", "\n", "\n", "", "if", "\"svd\"", "in", "method", ":", "\n", "# curently, svd does not support half-precision   ", "\n", "# convert tenosr back to full-precision ", "\n", "        ", "with", "torch", ".", "cuda", ".", "amp", ".", "autocast", "(", "enabled", "=", "False", ")", ":", "\n", "            ", "U", ",", "Sig", ",", "V", "=", "torch", ".", "svd_lowrank", "(", "tensor", ".", "cpu", "(", ")", ",", "q", "=", "int", "(", "method", ".", "split", "(", "\"_\"", ")", "[", "1", "]", ")", ")", "\n", "# gather U", "\n", "Us", "=", "_gather_tensor", "(", "U", ".", "to", "(", "tensor", ".", "device", ")", ",", "world_size", ")", "# N x B x LR", "\n", "Sigs", "=", "_gather_tensor", "(", "torch", ".", "diag", "(", "Sig", ".", "to", "(", "tensor", ".", "device", ")", ")", ",", "world_size", ")", "# N x LR x LR", "\n", "Vs", "=", "_gather_tensor", "(", "V", ".", "to", "(", "tensor", ".", "device", ")", ".", "T", ",", "world_size", ")", "# N x LR x D", "\n", "# perform batch mm", "\n", "# outputs = []", "\n", "# for k in range(Us.shape[0]):", "\n", "#     temp = torch.mm(Us[k], Sigs[k]) # B x LR", "\n", "#     output = torch.mm(temp, Vs[k])  # B x D", "\n", "#     outputs.append(output)", "\n", "# outputs[rank] = tensor", "\n", "# output = torch.cat(outputs, 0)", "\n", "\n", "", "output", "=", "torch", ".", "bmm", "(", "torch", ".", "bmm", "(", "Us", ",", "Sigs", ")", ",", "Vs", ")", "\n", "output", "[", "rank", "]", "=", "tensor", "\n", "output", "=", "output", ".", "view", "(", "-", "1", ",", "output", ".", "shape", "[", "-", "1", "]", ")", "\n", "", "elif", "\"pca\"", "in", "method", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "        ", "tensors_gather", "=", "[", "\n", "torch", ".", "ones_like", "(", "tensor", ")", "\n", "for", "_", "in", "range", "(", "world_size", ")", "\n", "]", "\n", "# dist.all_gather(tensors_gather, tensor, async_op=False)", "\n", "# need to do this to restore propagation of the gradients", "\n", "# tensors_gather[rank] = tensor        ", "\n", "tensors_gather", "=", "diffdist", ".", "functional", ".", "all_gather", "(", "tensors_gather", ",", "tensor", ")", "\n", "output", "=", "torch", ".", "cat", "(", "tensors_gather", ",", "dim", "=", "0", ")", "\n", "", "return", "output", ",", "min_bs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.__init__": [[67, 94], ["isinstance", "isinstance", "isinstance", "ValueError", "isinstance", "pycocotools.frPyObjects.astype", "pycocotools.frPyObjects", "pycocotools.decode", "numpy.asarray().reshape", "type", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode"], ["def", "__init__", "(", "self", ",", "mask_or_polygons", ",", "height", ",", "width", ")", ":", "\n", "        ", "self", ".", "_mask", "=", "self", ".", "_polygons", "=", "self", ".", "_has_holes", "=", "None", "\n", "self", ".", "height", "=", "height", "\n", "self", ".", "width", "=", "width", "\n", "\n", "m", "=", "mask_or_polygons", "\n", "if", "isinstance", "(", "m", ",", "dict", ")", ":", "\n", "# RLEs", "\n", "            ", "assert", "\"counts\"", "in", "m", "and", "\"size\"", "in", "m", "\n", "if", "isinstance", "(", "m", "[", "\"counts\"", "]", ",", "list", ")", ":", "# uncompressed RLEs", "\n", "                ", "h", ",", "w", "=", "m", "[", "\"size\"", "]", "\n", "assert", "h", "==", "height", "and", "w", "==", "width", "\n", "m", "=", "mask_util", ".", "frPyObjects", "(", "m", ",", "h", ",", "w", ")", "\n", "", "self", ".", "_mask", "=", "mask_util", ".", "decode", "(", "m", ")", "[", ":", ",", ":", "]", "\n", "return", "\n", "\n", "", "if", "isinstance", "(", "m", ",", "list", ")", ":", "# list[ndarray]", "\n", "            ", "self", ".", "_polygons", "=", "[", "np", ".", "asarray", "(", "x", ")", ".", "reshape", "(", "-", "1", ")", "for", "x", "in", "m", "]", "\n", "return", "\n", "\n", "", "if", "isinstance", "(", "m", ",", "np", ".", "ndarray", ")", ":", "# assumed to be a binary mask", "\n", "            ", "assert", "m", ".", "shape", "[", "1", "]", "!=", "2", ",", "m", ".", "shape", "\n", "assert", "m", ".", "shape", "==", "(", "height", ",", "width", ")", ",", "m", ".", "shape", "\n", "self", ".", "_mask", "=", "m", ".", "astype", "(", "\"uint8\"", ")", "\n", "return", "\n", "\n", "", "raise", "ValueError", "(", "\"GenericMask cannot handle object {} of type '{}'\"", ".", "format", "(", "m", ",", "type", "(", "m", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.mask": [[95, 100], ["visualizer.GenericMask.polygons_to_mask"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.polygons_to_mask"], ["", "@", "property", "\n", "def", "mask", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_mask", "is", "None", ":", "\n", "            ", "self", ".", "_mask", "=", "self", ".", "polygons_to_mask", "(", "self", ".", "_polygons", ")", "\n", "", "return", "self", ".", "_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.polygons": [[101, 106], ["visualizer.GenericMask.mask_to_polygons"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.mask_to_polygons"], ["", "@", "property", "\n", "def", "polygons", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_polygons", "is", "None", ":", "\n", "            ", "self", ".", "_polygons", ",", "self", ".", "_has_holes", "=", "self", ".", "mask_to_polygons", "(", "self", ".", "_mask", ")", "\n", "", "return", "self", ".", "_polygons", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.has_holes": [[107, 115], ["visualizer.GenericMask.mask_to_polygons"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.mask_to_polygons"], ["", "@", "property", "\n", "def", "has_holes", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_has_holes", "is", "None", ":", "\n", "            ", "if", "self", ".", "_mask", "is", "not", "None", ":", "\n", "                ", "self", ".", "_polygons", ",", "self", ".", "_has_holes", "=", "self", ".", "mask_to_polygons", "(", "self", ".", "_mask", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_has_holes", "=", "False", "# if original format is polygon, does not have holes", "\n", "", "", "return", "self", ".", "_has_holes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.mask_to_polygons": [[116, 134], ["numpy.ascontiguousarray", "cv2.findContours", "numpy.ascontiguousarray.astype", "x.flatten", "len", "hierarchy.reshape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "def", "mask_to_polygons", "(", "self", ",", "mask", ")", ":", "\n", "# cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level", "\n", "# hierarchy. External contours (boundary) of the object are placed in hierarchy-1.", "\n", "# Internal contours (holes) are placed in hierarchy-2.", "\n", "# cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.", "\n", "        ", "mask", "=", "np", ".", "ascontiguousarray", "(", "mask", ")", "# some versions of cv2 does not support incontiguous arr", "\n", "res", "=", "cv2", ".", "findContours", "(", "mask", ".", "astype", "(", "\"uint8\"", ")", ",", "cv2", ".", "RETR_CCOMP", ",", "cv2", ".", "CHAIN_APPROX_NONE", ")", "\n", "hierarchy", "=", "res", "[", "-", "1", "]", "\n", "if", "hierarchy", "is", "None", ":", "# empty mask", "\n", "            ", "return", "[", "]", ",", "False", "\n", "", "has_holes", "=", "(", "hierarchy", ".", "reshape", "(", "-", "1", ",", "4", ")", "[", ":", ",", "3", "]", ">=", "0", ")", ".", "sum", "(", ")", ">", "0", "\n", "res", "=", "res", "[", "-", "2", "]", "\n", "res", "=", "[", "x", ".", "flatten", "(", ")", "for", "x", "in", "res", "]", "\n", "# These coordinates from OpenCV are integers in range [0, W-1 or H-1].", "\n", "# We add 0.5 to turn them into real-value coordinate space. A better solution", "\n", "# would be to first +0.5 and then dilate the returned polygon by 0.5.", "\n", "res", "=", "[", "x", "+", "0.5", "for", "x", "in", "res", "if", "len", "(", "x", ")", ">=", "6", "]", "\n", "return", "res", ",", "has_holes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.polygons_to_mask": [[135, 139], ["pycocotools.frPyObjects", "pycocotools.merge", "pycocotools.decode"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode"], ["", "def", "polygons_to_mask", "(", "self", ",", "polygons", ")", ":", "\n", "        ", "rle", "=", "mask_util", ".", "frPyObjects", "(", "polygons", ",", "self", ".", "height", ",", "self", ".", "width", ")", "\n", "rle", "=", "mask_util", ".", "merge", "(", "rle", ")", "\n", "return", "mask_util", ".", "decode", "(", "rle", ")", "[", ":", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.area": [[140, 142], ["visualizer.GenericMask.mask.sum"], "methods", ["None"], ["", "def", "area", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "mask", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.bbox": [[143, 150], ["pycocotools.frPyObjects", "pycocotools.merge", "pycocotools.toBbox"], "methods", ["None"], ["", "def", "bbox", "(", "self", ")", ":", "\n", "        ", "p", "=", "mask_util", ".", "frPyObjects", "(", "self", ".", "polygons", ",", "self", ".", "height", ",", "self", ".", "width", ")", "\n", "p", "=", "mask_util", ".", "merge", "(", "p", ")", "\n", "bbox", "=", "mask_util", ".", "toBbox", "(", "p", ")", "\n", "bbox", "[", "2", "]", "+=", "bbox", "[", "0", "]", "\n", "bbox", "[", "3", "]", "+=", "bbox", "[", "1", "]", "\n", "return", "bbox", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.__init__": [[157, 192], ["torch.unique", "areas.numpy.numpy.numpy", "numpy.argsort", "visualizer._PanopticPrediction._seg_ids.tolist", "zip", "numpy.unique", "panoptic_seg.numpy", "segments_info.append", "float", "metadata.thing_dataset_id_to_contiguous_id.values", "int", "int", "bool"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "panoptic_seg", ",", "segments_info", ",", "metadata", "=", "None", ")", ":", "\n", "        ", "if", "segments_info", "is", "None", ":", "\n", "            ", "assert", "metadata", "is", "not", "None", "\n", "# If \"segments_info\" is None, we assume \"panoptic_img\" is a", "\n", "# H*W int32 image storing the panoptic_id in the format of", "\n", "# category_id * label_divisor + instance_id. We reserve -1 for", "\n", "# VOID label.", "\n", "label_divisor", "=", "metadata", ".", "label_divisor", "\n", "segments_info", "=", "[", "]", "\n", "for", "panoptic_label", "in", "np", ".", "unique", "(", "panoptic_seg", ".", "numpy", "(", ")", ")", ":", "\n", "                ", "if", "panoptic_label", "==", "-", "1", ":", "\n", "# VOID region.", "\n", "                    ", "continue", "\n", "", "pred_class", "=", "panoptic_label", "//", "label_divisor", "\n", "isthing", "=", "pred_class", "in", "metadata", ".", "thing_dataset_id_to_contiguous_id", ".", "values", "(", ")", "\n", "segments_info", ".", "append", "(", "\n", "{", "\n", "\"id\"", ":", "int", "(", "panoptic_label", ")", ",", "\n", "\"category_id\"", ":", "int", "(", "pred_class", ")", ",", "\n", "\"isthing\"", ":", "bool", "(", "isthing", ")", ",", "\n", "}", "\n", ")", "\n", "", "", "del", "metadata", "\n", "\n", "self", ".", "_seg", "=", "panoptic_seg", "\n", "\n", "self", ".", "_sinfo", "=", "{", "s", "[", "\"id\"", "]", ":", "s", "for", "s", "in", "segments_info", "}", "# seg id -> seg info", "\n", "segment_ids", ",", "areas", "=", "torch", ".", "unique", "(", "panoptic_seg", ",", "sorted", "=", "True", ",", "return_counts", "=", "True", ")", "\n", "areas", "=", "areas", ".", "numpy", "(", ")", "\n", "sorted_idxs", "=", "np", ".", "argsort", "(", "-", "areas", ")", "\n", "self", ".", "_seg_ids", ",", "self", ".", "_seg_areas", "=", "segment_ids", "[", "sorted_idxs", "]", ",", "areas", "[", "sorted_idxs", "]", "\n", "self", ".", "_seg_ids", "=", "self", ".", "_seg_ids", ".", "tolist", "(", ")", "\n", "for", "sid", ",", "area", "in", "zip", "(", "self", ".", "_seg_ids", ",", "self", ".", "_seg_areas", ")", ":", "\n", "            ", "if", "sid", "in", "self", ".", "_sinfo", ":", "\n", "                ", "self", ".", "_sinfo", "[", "sid", "]", "[", "\"area\"", "]", "=", "float", "(", "area", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.non_empty_mask": [[193, 208], ["len", "numpy.zeros", "len", "empty_ids.append"], "methods", ["None"], ["", "", "", "def", "non_empty_mask", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            (H, W) array, a mask for all pixels that have a prediction\n        \"\"\"", "\n", "empty_ids", "=", "[", "]", "\n", "for", "id", "in", "self", ".", "_seg_ids", ":", "\n", "            ", "if", "id", "not", "in", "self", ".", "_sinfo", ":", "\n", "                ", "empty_ids", ".", "append", "(", "id", ")", "\n", "", "", "if", "len", "(", "empty_ids", ")", "==", "0", ":", "\n", "            ", "return", "np", ".", "zeros", "(", "self", ".", "_seg", ".", "shape", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "", "assert", "(", "\n", "len", "(", "empty_ids", ")", "==", "1", "\n", ")", ",", "\">1 ids corresponds to no labels. This is currently not supported\"", "\n", "return", "(", "self", ".", "_seg", "!=", "empty_ids", "[", "0", "]", ")", ".", "numpy", "(", ")", ".", "astype", "(", "np", ".", "bool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.semantic_masks": [[209, 216], ["visualizer._PanopticPrediction._sinfo.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "semantic_masks", "(", "self", ")", ":", "\n", "        ", "for", "sid", "in", "self", ".", "_seg_ids", ":", "\n", "            ", "sinfo", "=", "self", ".", "_sinfo", ".", "get", "(", "sid", ")", "\n", "if", "sinfo", "is", "None", "or", "sinfo", "[", "\"isthing\"", "]", ":", "\n", "# Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.", "\n", "                ", "continue", "\n", "", "yield", "(", "self", ".", "_seg", "==", "sid", ")", ".", "numpy", "(", ")", ".", "astype", "(", "np", ".", "bool", ")", ",", "sinfo", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.instance_masks": [[217, 225], ["visualizer._PanopticPrediction._sinfo.get", "mask.sum"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "instance_masks", "(", "self", ")", ":", "\n", "        ", "for", "sid", "in", "self", ".", "_seg_ids", ":", "\n", "            ", "sinfo", "=", "self", ".", "_sinfo", ".", "get", "(", "sid", ")", "\n", "if", "sinfo", "is", "None", "or", "not", "sinfo", "[", "\"isthing\"", "]", ":", "\n", "                ", "continue", "\n", "", "mask", "=", "(", "self", ".", "_seg", "==", "sid", ")", ".", "numpy", "(", ")", ".", "astype", "(", "np", ".", "bool", ")", "\n", "if", "mask", ".", "sum", "(", ")", ">", "0", ":", "\n", "                ", "yield", "mask", ",", "sinfo", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.__init__": [[255, 265], ["visualizer.VisImage._setup_figure"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage._setup_figure"], ["    ", "def", "__init__", "(", "self", ",", "img", ",", "scale", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (ndarray): an RGB image of shape (H, W, 3).\n            scale (float): scale the input image\n        \"\"\"", "\n", "self", ".", "img", "=", "img", "\n", "self", ".", "scale", "=", "scale", "\n", "self", ".", "width", ",", "self", ".", "height", "=", "img", ".", "shape", "[", "1", "]", ",", "img", ".", "shape", "[", "0", "]", "\n", "self", ".", "_setup_figure", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage._setup_figure": [[266, 292], ["matplotlib.Figure", "matplotlib.Figure", "matplotlib.Figure", "matplotlib.Figure.get_dpi", "matplotlib.Figure.set_size_inches", "matplotlib.backends.backend_agg.FigureCanvasAgg", "matplotlib.backends.backend_agg.FigureCanvasAgg", "matplotlib.backends.backend_agg.FigureCanvasAgg", "matplotlib.Figure.add_axes", "mplfigure.Figure.add_axes.axis", "mplfigure.Figure.add_axes.imshow"], "methods", ["None"], ["", "def", "_setup_figure", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            Same as in :meth:`__init__()`.\n\n        Returns:\n            fig (matplotlib.pyplot.figure): top level container for all the image plot elements.\n            ax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n        \"\"\"", "\n", "fig", "=", "mplfigure", ".", "Figure", "(", "frameon", "=", "False", ")", "\n", "self", ".", "dpi", "=", "fig", ".", "get_dpi", "(", ")", "\n", "# add a small 1e-2 to avoid precision lost due to matplotlib's truncation", "\n", "# (https://github.com/matplotlib/matplotlib/issues/15363)", "\n", "fig", ".", "set_size_inches", "(", "\n", "(", "self", ".", "width", "*", "self", ".", "scale", "+", "1e-2", ")", "/", "self", ".", "dpi", ",", "\n", "(", "self", ".", "height", "*", "self", ".", "scale", "+", "1e-2", ")", "/", "self", ".", "dpi", ",", "\n", ")", "\n", "self", ".", "canvas", "=", "FigureCanvasAgg", "(", "fig", ")", "\n", "# self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)", "\n", "ax", "=", "fig", ".", "add_axes", "(", "[", "0.0", ",", "0.0", ",", "1.0", ",", "1.0", "]", ")", "\n", "ax", ".", "axis", "(", "\"off\"", ")", "\n", "# Need to imshow this first so that other patches can be drawn on top", "\n", "ax", ".", "imshow", "(", "img", ",", "extent", "=", "(", "0", ",", "self", ".", "width", ",", "self", ".", "height", ",", "0", ")", ",", "interpolation", "=", "\"nearest\"", ")", "\n", "\n", "self", ".", "fig", "=", "fig", "\n", "self", ".", "ax", "=", "ax", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.save": [[293, 300], ["visualizer.VisImage.fig.savefig"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "filepath", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            filepath (str): a string that contains the absolute path, including the file name, where\n                the visualized image will be saved.\n        \"\"\"", "\n", "self", ".", "fig", ".", "savefig", "(", "filepath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image": [[301, 320], ["canvas.print_to_buffer", "numpy.frombuffer", "numpy.frombuffer.reshape", "numpy.split", "rgb.astype"], "methods", ["None"], ["", "def", "get_image", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            ndarray:\n                the visualized image of shape (H, W, 3) (RGB) in uint8 type.\n                The shape is scaled w.r.t the input image using the given `scale` argument.\n        \"\"\"", "\n", "canvas", "=", "self", ".", "canvas", "\n", "s", ",", "(", "width", ",", "height", ")", "=", "canvas", ".", "print_to_buffer", "(", ")", "\n", "# buf = io.BytesIO()  # works for cairo backend", "\n", "# canvas.print_rgba(buf)", "\n", "# width, height = self.width, self.height", "\n", "# s = buf.getvalue()", "\n", "\n", "buffer", "=", "np", ".", "frombuffer", "(", "s", ",", "dtype", "=", "\"uint8\"", ")", "\n", "\n", "img_rgba", "=", "buffer", ".", "reshape", "(", "height", ",", "width", ",", "4", ")", "\n", "rgb", ",", "alpha", "=", "np", ".", "split", "(", "img_rgba", ",", "[", "3", "]", ",", "axis", "=", "2", ")", "\n", "return", "rgb", ".", "astype", "(", "\"uint8\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.__init__": [[348, 372], ["numpy.asarray().clip().astype", "visualizer.VisImage", "torch.device", "max", "detectron2.data.MetadataCatalog.get", "numpy.asarray().clip", "numpy.sqrt", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip"], ["def", "__init__", "(", "self", ",", "img_rgb", ",", "metadata", "=", "None", ",", "scale", "=", "1.0", ",", "instance_mode", "=", "ColorMode", ".", "IMAGE", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img_rgb: a numpy array of shape (H, W, C), where H and W correspond to\n                the height and width of the image respectively. C is the number of\n                color channels. The image is required to be in RGB format since that\n                is a requirement of the Matplotlib library. The image is also expected\n                to be in the range [0, 255].\n            metadata (Metadata): dataset metadata (e.g. class names and colors)\n            instance_mode (ColorMode): defines one of the pre-defined style for drawing\n                instances on an image.\n        \"\"\"", "\n", "self", ".", "img", "=", "np", ".", "asarray", "(", "img_rgb", ")", ".", "clip", "(", "0", ",", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "if", "metadata", "is", "None", ":", "\n", "            ", "metadata", "=", "MetadataCatalog", ".", "get", "(", "\"__nonexist__\"", ")", "\n", "", "self", ".", "metadata", "=", "metadata", "\n", "self", ".", "output", "=", "VisImage", "(", "self", ".", "img", ",", "scale", "=", "scale", ")", "\n", "self", ".", "cpu_device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "\n", "# too small texts are useless, therefore clamp to 9", "\n", "self", ".", "_default_font_size", "=", "max", "(", "\n", "np", ".", "sqrt", "(", "self", ".", "output", ".", "height", "*", "self", ".", "output", ".", "width", ")", "//", "90", ",", "10", "//", "scale", "\n", ")", "\n", "self", ".", "_instance_mode", "=", "instance_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_instance_predictions": [[373, 423], ["visualizer._create_text_labels", "predictions.has", "visualizer.Visualizer.overlay_instances", "predictions.has", "predictions.has", "predictions.has", "predictions.pred_classes.tolist", "visualizer.Visualizer.metadata.get", "predictions.has", "numpy.asarray", "visualizer.Visualizer.metadata.get", "visualizer.Visualizer._create_grayscale_image", "visualizer.GenericMask", "visualizer.Visualizer._jitter", "predictions.has", "predictions.pred_masks.any"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._create_text_labels", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._create_grayscale_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._jitter", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has"], ["", "def", "draw_instance_predictions", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "\"\"\"\n        Draw instance-level prediction results on an image.\n\n        Args:\n            predictions (Instances): the output of an instance detection/segmentation\n                model. Following fields will be used to draw:\n                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\n\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"", "\n", "boxes", "=", "predictions", ".", "pred_boxes", "if", "predictions", ".", "has", "(", "\"pred_boxes\"", ")", "else", "None", "\n", "scores", "=", "predictions", ".", "scores", "if", "predictions", ".", "has", "(", "\"scores\"", ")", "else", "None", "\n", "classes", "=", "predictions", ".", "pred_classes", ".", "tolist", "(", ")", "if", "predictions", ".", "has", "(", "\"pred_classes\"", ")", "else", "None", "\n", "labels", "=", "_create_text_labels", "(", "classes", ",", "scores", ",", "self", ".", "metadata", ".", "get", "(", "\"thing_classes\"", ",", "None", ")", ")", "\n", "keypoints", "=", "predictions", ".", "pred_keypoints", "if", "predictions", ".", "has", "(", "\"pred_keypoints\"", ")", "else", "None", "\n", "\n", "if", "predictions", ".", "has", "(", "\"pred_masks\"", ")", ":", "\n", "            ", "masks", "=", "np", ".", "asarray", "(", "predictions", ".", "pred_masks", ")", "\n", "masks", "=", "[", "GenericMask", "(", "x", ",", "self", ".", "output", ".", "height", ",", "self", ".", "output", ".", "width", ")", "for", "x", "in", "masks", "]", "\n", "", "else", ":", "\n", "            ", "masks", "=", "None", "\n", "\n", "", "if", "self", ".", "_instance_mode", "==", "ColorMode", ".", "SEGMENTATION", "and", "self", ".", "metadata", ".", "get", "(", "\"thing_colors\"", ")", ":", "\n", "            ", "colors", "=", "[", "\n", "self", ".", "_jitter", "(", "[", "x", "/", "255", "for", "x", "in", "self", ".", "metadata", ".", "thing_colors", "[", "c", "]", "]", ")", "for", "c", "in", "classes", "\n", "]", "\n", "alpha", "=", "0.8", "\n", "", "else", ":", "\n", "            ", "colors", "=", "None", "\n", "alpha", "=", "0.5", "\n", "\n", "", "if", "self", ".", "_instance_mode", "==", "ColorMode", ".", "IMAGE_BW", ":", "\n", "            ", "self", ".", "output", ".", "img", "=", "self", ".", "_create_grayscale_image", "(", "\n", "(", "predictions", ".", "pred_masks", ".", "any", "(", "dim", "=", "0", ")", ">", "0", ")", ".", "numpy", "(", ")", "\n", "if", "predictions", ".", "has", "(", "\"pred_masks\"", ")", "\n", "else", "None", "\n", ")", "\n", "alpha", "=", "0.3", "\n", "\n", "", "self", ".", "overlay_instances", "(", "\n", "masks", "=", "masks", ",", "\n", "boxes", "=", "boxes", ",", "\n", "labels", "=", "labels", ",", "\n", "keypoints", "=", "keypoints", ",", "\n", "assigned_colors", "=", "colors", ",", "\n", "alpha", "=", "alpha", ",", "\n", ")", "\n", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_sem_seg": [[424, 459], ["isinstance", "numpy.unique", "numpy.argsort().tolist", "filter", "sem_seg.numpy.numpy.numpy", "visualizer.Visualizer.draw_binary_mask", "numpy.argsort", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_binary_mask"], ["", "def", "draw_sem_seg", "(", "self", ",", "sem_seg", ",", "area_threshold", "=", "None", ",", "alpha", "=", "0.8", ")", ":", "\n", "        ", "\"\"\"\n        Draw semantic segmentation predictions/labels.\n\n        Args:\n            sem_seg (Tensor or ndarray): the segmentation of shape (H, W).\n                Each value is the integer label of the pixel.\n            area_threshold (int): segments with less than `area_threshold` are not drawn.\n            alpha (float): the larger it is, the more opaque the segmentations are.\n\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"", "\n", "if", "isinstance", "(", "sem_seg", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "sem_seg", "=", "sem_seg", ".", "numpy", "(", ")", "\n", "", "labels", ",", "areas", "=", "np", ".", "unique", "(", "sem_seg", ",", "return_counts", "=", "True", ")", "\n", "sorted_idxs", "=", "np", ".", "argsort", "(", "-", "areas", ")", ".", "tolist", "(", ")", "\n", "labels", "=", "labels", "[", "sorted_idxs", "]", "\n", "for", "label", "in", "filter", "(", "lambda", "l", ":", "l", "<", "len", "(", "self", ".", "metadata", ".", "stuff_classes", ")", ",", "labels", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "mask_color", "=", "[", "x", "/", "255", "for", "x", "in", "self", ".", "metadata", ".", "stuff_colors", "[", "label", "]", "]", "\n", "", "except", "(", "AttributeError", ",", "IndexError", ")", ":", "\n", "                ", "mask_color", "=", "None", "\n", "\n", "", "binary_mask", "=", "(", "sem_seg", "==", "label", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "text", "=", "self", ".", "metadata", ".", "stuff_classes", "[", "label", "]", "\n", "self", ".", "draw_binary_mask", "(", "\n", "binary_mask", ",", "\n", "color", "=", "mask_color", ",", "\n", "edge_color", "=", "_OFF_WHITE", ",", "\n", "text", "=", "text", ",", "\n", "alpha", "=", "alpha", ",", "\n", "area_threshold", "=", "area_threshold", ",", "\n", ")", "\n", "", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_panoptic_seg": [[460, 523], ["visualizer._PanopticPrediction", "visualizer._PanopticPrediction.semantic_masks", "list", "list", "visualizer._create_text_labels", "visualizer.Visualizer.overlay_instances", "visualizer.Visualizer._create_grayscale_image", "visualizer.Visualizer.draw_binary_mask", "visualizer._PanopticPrediction.instance_masks", "len", "zip", "visualizer._PanopticPrediction.non_empty_mask", "x.get", "visualizer.Visualizer._jitter"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.semantic_masks", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._create_text_labels", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._create_grayscale_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_binary_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.instance_masks", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.non_empty_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._jitter"], ["", "def", "draw_panoptic_seg", "(", "self", ",", "panoptic_seg", ",", "segments_info", ",", "area_threshold", "=", "None", ",", "alpha", "=", "0.7", ")", ":", "\n", "        ", "\"\"\"\n        Draw panoptic prediction annotations or results.\n\n        Args:\n            panoptic_seg (Tensor): of shape (height, width) where the values are ids for each\n                segment.\n            segments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\n                If it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\n                If None, category id of each pixel is computed by\n                ``pixel // metadata.label_divisor``.\n            area_threshold (int): stuff segments with less than `area_threshold` are not drawn.\n\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"", "\n", "pred", "=", "_PanopticPrediction", "(", "panoptic_seg", ",", "segments_info", ",", "self", ".", "metadata", ")", "\n", "\n", "if", "self", ".", "_instance_mode", "==", "ColorMode", ".", "IMAGE_BW", ":", "\n", "            ", "self", ".", "output", ".", "img", "=", "self", ".", "_create_grayscale_image", "(", "pred", ".", "non_empty_mask", "(", ")", ")", "\n", "\n", "# draw mask for all semantic segments first i.e. \"stuff\"", "\n", "", "for", "mask", ",", "sinfo", "in", "pred", ".", "semantic_masks", "(", ")", ":", "\n", "            ", "category_idx", "=", "sinfo", "[", "\"category_id\"", "]", "\n", "try", ":", "\n", "                ", "mask_color", "=", "[", "x", "/", "255", "for", "x", "in", "self", ".", "metadata", ".", "stuff_colors", "[", "category_idx", "]", "]", "\n", "", "except", "AttributeError", ":", "\n", "                ", "mask_color", "=", "None", "\n", "\n", "", "text", "=", "self", ".", "metadata", ".", "stuff_classes", "[", "category_idx", "]", "\n", "self", ".", "draw_binary_mask", "(", "\n", "mask", ",", "\n", "color", "=", "mask_color", ",", "\n", "edge_color", "=", "_OFF_WHITE", ",", "\n", "text", "=", "text", ",", "\n", "alpha", "=", "alpha", ",", "\n", "area_threshold", "=", "area_threshold", ",", "\n", ")", "\n", "\n", "# draw mask for all instances second", "\n", "", "all_instances", "=", "list", "(", "pred", ".", "instance_masks", "(", ")", ")", "\n", "if", "len", "(", "all_instances", ")", "==", "0", ":", "\n", "            ", "return", "self", ".", "output", "\n", "", "masks", ",", "sinfo", "=", "list", "(", "zip", "(", "*", "all_instances", ")", ")", "\n", "category_ids", "=", "[", "x", "[", "\"category_id\"", "]", "for", "x", "in", "sinfo", "]", "\n", "\n", "try", ":", "\n", "            ", "scores", "=", "[", "x", "[", "\"score\"", "]", "for", "x", "in", "sinfo", "]", "\n", "", "except", "KeyError", ":", "\n", "            ", "scores", "=", "None", "\n", "", "labels", "=", "_create_text_labels", "(", "\n", "category_ids", ",", "scores", ",", "self", ".", "metadata", ".", "thing_classes", ",", "[", "x", ".", "get", "(", "\"iscrowd\"", ",", "0", ")", "for", "x", "in", "sinfo", "]", "\n", ")", "\n", "\n", "try", ":", "\n", "            ", "colors", "=", "[", "\n", "self", ".", "_jitter", "(", "[", "x", "/", "255", "for", "x", "in", "self", ".", "metadata", ".", "thing_colors", "[", "c", "]", "]", ")", "for", "c", "in", "category_ids", "\n", "]", "\n", "", "except", "AttributeError", ":", "\n", "            ", "colors", "=", "None", "\n", "", "self", ".", "overlay_instances", "(", "masks", "=", "masks", ",", "labels", "=", "labels", ",", "assigned_colors", "=", "colors", ",", "alpha", "=", "alpha", ")", "\n", "\n", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_dataset_dict": [[526, 594], ["dic.get", "dic.get", "dic.get", "visualizer.Visualizer.metadata.get", "visualizer._create_text_labels", "visualizer.Visualizer.overlay_instances", "visualizer.Visualizer.draw_sem_seg", "torch.tensor", "visualizer.Visualizer.draw_panoptic_seg", "numpy.array().reshape", "visualizer.Visualizer.metadata.get", "detectron2.utils.file_io.PathManager.open", "PIL.Image.open", "numpy.asarray", "detectron2.utils.file_io.PathManager.open", "PIL.Image.open", "numpy.asarray", "rgb2id", "len", "detectron2.structures.BoxMode.convert", "visualizer.Visualizer._jitter", "numpy.array", "len", "x.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._create_text_labels", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_sem_seg", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_panoptic_seg", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._jitter", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "draw_dataset_dict", "(", "self", ",", "dic", ")", ":", "\n", "        ", "\"\"\"\n        Draw annotations/segmentaions in Detectron2 Dataset format.\n\n        Args:\n            dic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\n\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"", "\n", "annos", "=", "dic", ".", "get", "(", "\"annotations\"", ",", "None", ")", "\n", "if", "annos", ":", "\n", "            ", "if", "\"segmentation\"", "in", "annos", "[", "0", "]", ":", "\n", "                ", "masks", "=", "[", "x", "[", "\"segmentation\"", "]", "for", "x", "in", "annos", "]", "\n", "", "else", ":", "\n", "                ", "masks", "=", "None", "\n", "", "if", "\"keypoints\"", "in", "annos", "[", "0", "]", ":", "\n", "                ", "keypts", "=", "[", "x", "[", "\"keypoints\"", "]", "for", "x", "in", "annos", "]", "\n", "keypts", "=", "np", ".", "array", "(", "keypts", ")", ".", "reshape", "(", "len", "(", "annos", ")", ",", "-", "1", ",", "3", ")", "\n", "", "else", ":", "\n", "                ", "keypts", "=", "None", "\n", "\n", "", "boxes", "=", "[", "\n", "BoxMode", ".", "convert", "(", "x", "[", "\"bbox\"", "]", ",", "x", "[", "\"bbox_mode\"", "]", ",", "BoxMode", ".", "XYXY_ABS", ")", "\n", "if", "len", "(", "x", "[", "\"bbox\"", "]", ")", "==", "4", "\n", "else", "x", "[", "\"bbox\"", "]", "\n", "for", "x", "in", "annos", "\n", "]", "\n", "\n", "colors", "=", "None", "\n", "category_ids", "=", "[", "x", "[", "\"category_id\"", "]", "for", "x", "in", "annos", "]", "\n", "if", "self", ".", "_instance_mode", "==", "ColorMode", ".", "SEGMENTATION", "and", "self", ".", "metadata", ".", "get", "(", "\"thing_colors\"", ")", ":", "\n", "                ", "colors", "=", "[", "\n", "self", ".", "_jitter", "(", "[", "x", "/", "255", "for", "x", "in", "self", ".", "metadata", ".", "thing_colors", "[", "c", "]", "]", ")", "\n", "for", "c", "in", "category_ids", "\n", "]", "\n", "", "names", "=", "self", ".", "metadata", ".", "get", "(", "\"thing_classes\"", ",", "None", ")", "\n", "labels", "=", "_create_text_labels", "(", "\n", "category_ids", ",", "\n", "scores", "=", "None", ",", "\n", "class_names", "=", "names", ",", "\n", "is_crowd", "=", "[", "x", ".", "get", "(", "\"iscrowd\"", ",", "0", ")", "for", "x", "in", "annos", "]", ",", "\n", ")", "\n", "self", ".", "overlay_instances", "(", "\n", "labels", "=", "labels", ",", "boxes", "=", "boxes", ",", "masks", "=", "masks", ",", "keypoints", "=", "keypts", ",", "assigned_colors", "=", "colors", "\n", ")", "\n", "\n", "", "sem_seg", "=", "dic", ".", "get", "(", "\"sem_seg\"", ",", "None", ")", "\n", "if", "sem_seg", "is", "None", "and", "\"sem_seg_file_name\"", "in", "dic", ":", "\n", "            ", "with", "PathManager", ".", "open", "(", "dic", "[", "\"sem_seg_file_name\"", "]", ",", "\"rb\"", ")", "as", "f", ":", "\n", "                ", "sem_seg", "=", "Image", ".", "open", "(", "f", ")", "\n", "sem_seg", "=", "np", ".", "asarray", "(", "sem_seg", ",", "dtype", "=", "\"uint8\"", ")", "\n", "", "", "if", "sem_seg", "is", "not", "None", ":", "\n", "            ", "self", ".", "draw_sem_seg", "(", "sem_seg", ",", "area_threshold", "=", "0", ",", "alpha", "=", "0.5", ")", "\n", "\n", "", "pan_seg", "=", "dic", ".", "get", "(", "\"pan_seg\"", ",", "None", ")", "\n", "if", "pan_seg", "is", "None", "and", "\"pan_seg_file_name\"", "in", "dic", ":", "\n", "            ", "with", "PathManager", ".", "open", "(", "dic", "[", "\"pan_seg_file_name\"", "]", ",", "\"rb\"", ")", "as", "f", ":", "\n", "                ", "pan_seg", "=", "Image", ".", "open", "(", "f", ")", "\n", "pan_seg", "=", "np", ".", "asarray", "(", "pan_seg", ")", "\n", "from", "panopticapi", ".", "utils", "import", "rgb2id", "\n", "\n", "pan_seg", "=", "rgb2id", "(", "pan_seg", ")", "\n", "", "", "if", "pan_seg", "is", "not", "None", ":", "\n", "            ", "segments_info", "=", "dic", "[", "\"segments_info\"", "]", "\n", "pan_seg", "=", "torch", ".", "tensor", "(", "pan_seg", ")", "\n", "self", ".", "draw_panoptic_seg", "(", "pan_seg", ",", "segments_info", ",", "area_threshold", "=", "0", ",", "alpha", "=", "0.5", ")", "\n", "", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances": [[595, 737], ["range", "visualizer.Visualizer._convert_boxes", "len", "visualizer.Visualizer._convert_masks", "visualizer.Visualizer._convert_keypoints", "visualizer.Visualizer.overlay_rotated_instances", "numpy.prod", "numpy.argsort().tolist", "len", "len", "len", "colormap.random_color", "numpy.asarray", "visualizer.Visualizer.draw_box", "visualizer.Visualizer._change_color_brightness", "visualizer.Visualizer.draw_text", "visualizer.Visualizer.draw_and_connect_keypoints", "len", "len", "range", "numpy.argsort", "visualizer.Visualizer.draw_polygon", "numpy.sqrt", "x.area", "segment.reshape", "masks[].bbox", "numpy.clip", "len", "numpy.median", "masks[].mask.nonzero"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._convert_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._convert_masks", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._convert_keypoints", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_rotated_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.colormap.random_color", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_box", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._change_color_brightness", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_text", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_and_connect_keypoints", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_polygon", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.GenericMask.bbox", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip"], ["", "def", "overlay_instances", "(", "\n", "self", ",", "\n", "*", ",", "\n", "boxes", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "masks", "=", "None", ",", "\n", "keypoints", "=", "None", ",", "\n", "assigned_colors", "=", "None", ",", "\n", "alpha", "=", "0.5", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            boxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\n                or an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\n                or a :class:`RotatedBoxes`,\n                or an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\n                for the N objects in a single image,\n            labels (list[str]): the text to be displayed for each instance.\n            masks (masks-like object): Supported types are:\n\n                * :class:`detectron2.structures.PolygonMasks`,\n                  :class:`detectron2.structures.BitMasks`.\n                * list[list[ndarray]]: contains the segmentation masks for all objects in one image.\n                  The first level of the list corresponds to individual instances. The second\n                  level to all the polygon that compose the instance, and the third level\n                  to the polygon coordinates. The third level should have the format of\n                  [x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n                * list[ndarray]: each ndarray is a binary mask of shape (H, W).\n                * list[dict]: each dict is a COCO-style RLE.\n            keypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\n                where the N is the number of instances and K is the number of keypoints.\n                The last dimension corresponds to (x, y, visibility or score).\n            assigned_colors (list[matplotlib.colors]): a list of colors, where each color\n                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'\n                for full list of formats that the colors are accepted in.\n\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"", "\n", "num_instances", "=", "0", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "            ", "boxes", "=", "self", ".", "_convert_boxes", "(", "boxes", ")", "\n", "num_instances", "=", "len", "(", "boxes", ")", "\n", "", "if", "masks", "is", "not", "None", ":", "\n", "            ", "masks", "=", "self", ".", "_convert_masks", "(", "masks", ")", "\n", "if", "num_instances", ":", "\n", "                ", "assert", "len", "(", "masks", ")", "==", "num_instances", "\n", "", "else", ":", "\n", "                ", "num_instances", "=", "len", "(", "masks", ")", "\n", "", "", "if", "keypoints", "is", "not", "None", ":", "\n", "            ", "if", "num_instances", ":", "\n", "                ", "assert", "len", "(", "keypoints", ")", "==", "num_instances", "\n", "", "else", ":", "\n", "                ", "num_instances", "=", "len", "(", "keypoints", ")", "\n", "", "keypoints", "=", "self", ".", "_convert_keypoints", "(", "keypoints", ")", "\n", "", "if", "labels", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "labels", ")", "==", "num_instances", "\n", "", "if", "assigned_colors", "is", "None", ":", "\n", "            ", "assigned_colors", "=", "[", "random_color", "(", "rgb", "=", "True", ",", "maximum", "=", "1", ")", "for", "_", "in", "range", "(", "num_instances", ")", "]", "\n", "", "if", "num_instances", "==", "0", ":", "\n", "            ", "return", "self", ".", "output", "\n", "", "if", "boxes", "is", "not", "None", "and", "boxes", ".", "shape", "[", "1", "]", "==", "5", ":", "\n", "            ", "return", "self", ".", "overlay_rotated_instances", "(", "\n", "boxes", "=", "boxes", ",", "labels", "=", "labels", ",", "assigned_colors", "=", "assigned_colors", "\n", ")", "\n", "\n", "# Display in largest to smallest order to reduce occlusion.", "\n", "", "areas", "=", "None", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "            ", "areas", "=", "np", ".", "prod", "(", "boxes", "[", ":", ",", "2", ":", "]", "-", "boxes", "[", ":", ",", ":", "2", "]", ",", "axis", "=", "1", ")", "\n", "", "elif", "masks", "is", "not", "None", ":", "\n", "            ", "areas", "=", "np", ".", "asarray", "(", "[", "x", ".", "area", "(", ")", "for", "x", "in", "masks", "]", ")", "\n", "\n", "", "if", "areas", "is", "not", "None", ":", "\n", "            ", "sorted_idxs", "=", "np", ".", "argsort", "(", "-", "areas", ")", ".", "tolist", "(", ")", "\n", "# Re-order overlapped instances in descending order.", "\n", "boxes", "=", "boxes", "[", "sorted_idxs", "]", "if", "boxes", "is", "not", "None", "else", "None", "\n", "labels", "=", "[", "labels", "[", "k", "]", "for", "k", "in", "sorted_idxs", "]", "if", "labels", "is", "not", "None", "else", "None", "\n", "masks", "=", "[", "masks", "[", "idx", "]", "for", "idx", "in", "sorted_idxs", "]", "if", "masks", "is", "not", "None", "else", "None", "\n", "assigned_colors", "=", "[", "assigned_colors", "[", "idx", "]", "for", "idx", "in", "sorted_idxs", "]", "\n", "keypoints", "=", "keypoints", "[", "sorted_idxs", "]", "if", "keypoints", "is", "not", "None", "else", "None", "\n", "\n", "", "for", "i", "in", "range", "(", "num_instances", ")", ":", "\n", "            ", "color", "=", "assigned_colors", "[", "i", "]", "\n", "if", "boxes", "is", "not", "None", ":", "\n", "                ", "self", ".", "draw_box", "(", "boxes", "[", "i", "]", ",", "edge_color", "=", "color", ")", "\n", "\n", "", "if", "masks", "is", "not", "None", ":", "\n", "                ", "for", "segment", "in", "masks", "[", "i", "]", ".", "polygons", ":", "\n", "                    ", "self", ".", "draw_polygon", "(", "segment", ".", "reshape", "(", "-", "1", ",", "2", ")", ",", "color", ",", "alpha", "=", "alpha", ")", "\n", "\n", "", "", "if", "labels", "is", "not", "None", ":", "\n", "# first get a box", "\n", "                ", "if", "boxes", "is", "not", "None", ":", "\n", "                    ", "x0", ",", "y0", ",", "x1", ",", "y1", "=", "boxes", "[", "i", "]", "\n", "text_pos", "=", "(", "x0", ",", "y0", ")", "# if drawing boxes, put text on the box corner.", "\n", "horiz_align", "=", "\"left\"", "\n", "", "elif", "masks", "is", "not", "None", ":", "\n", "# skip small mask without polygon", "\n", "                    ", "if", "len", "(", "masks", "[", "i", "]", ".", "polygons", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "\n", "", "x0", ",", "y0", ",", "x1", ",", "y1", "=", "masks", "[", "i", "]", ".", "bbox", "(", ")", "\n", "\n", "# draw text in the center (defined by median) when box is not drawn", "\n", "# median is less sensitive to outliers.", "\n", "text_pos", "=", "np", ".", "median", "(", "masks", "[", "i", "]", ".", "mask", ".", "nonzero", "(", ")", ",", "axis", "=", "1", ")", "[", ":", ":", "-", "1", "]", "\n", "horiz_align", "=", "\"center\"", "\n", "", "else", ":", "\n", "                    ", "continue", "# drawing the box confidence for keypoints isn't very useful.", "\n", "# for small objects, draw text at the side to avoid occlusion", "\n", "", "instance_area", "=", "(", "y1", "-", "y0", ")", "*", "(", "x1", "-", "x0", ")", "\n", "if", "(", "\n", "instance_area", "<", "_SMALL_OBJECT_AREA_THRESH", "*", "self", ".", "output", ".", "scale", "\n", "or", "y1", "-", "y0", "<", "40", "*", "self", ".", "output", ".", "scale", "\n", ")", ":", "\n", "                    ", "if", "y1", ">=", "self", ".", "output", ".", "height", "-", "5", ":", "\n", "                        ", "text_pos", "=", "(", "x1", ",", "y0", ")", "\n", "", "else", ":", "\n", "                        ", "text_pos", "=", "(", "x0", ",", "y1", ")", "\n", "\n", "", "", "height_ratio", "=", "(", "y1", "-", "y0", ")", "/", "np", ".", "sqrt", "(", "self", ".", "output", ".", "height", "*", "self", ".", "output", ".", "width", ")", "\n", "lighter_color", "=", "self", ".", "_change_color_brightness", "(", "color", ",", "brightness_factor", "=", "0.7", ")", "\n", "font_size", "=", "(", "\n", "np", ".", "clip", "(", "(", "height_ratio", "-", "0.02", ")", "/", "0.08", "+", "1", ",", "1.2", ",", "2", ")", "\n", "*", "0.5", "\n", "*", "self", ".", "_default_font_size", "\n", ")", "\n", "self", ".", "draw_text", "(", "\n", "labels", "[", "i", "]", ",", "\n", "text_pos", ",", "\n", "color", "=", "lighter_color", ",", "\n", "horizontal_alignment", "=", "horiz_align", ",", "\n", "font_size", "=", "font_size", ",", "\n", ")", "\n", "\n", "# draw keypoints", "\n", "", "", "if", "keypoints", "is", "not", "None", ":", "\n", "            ", "for", "keypoints_per_instance", "in", "keypoints", ":", "\n", "                ", "self", ".", "draw_and_connect_keypoints", "(", "keypoints_per_instance", ")", "\n", "\n", "", "", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_rotated_instances": [[738, 775], ["len", "numpy.argsort().tolist", "range", "visualizer.Visualizer.draw_rotated_box_with_label", "colormap.random_color", "numpy.argsort", "range"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_rotated_box_with_label", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.colormap.random_color"], ["", "def", "overlay_rotated_instances", "(", "self", ",", "boxes", "=", "None", ",", "labels", "=", "None", ",", "assigned_colors", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            boxes (ndarray): an Nx5 numpy array of\n                (x_center, y_center, width, height, angle_degrees) format\n                for the N objects in a single image.\n            labels (list[str]): the text to be displayed for each instance.\n            assigned_colors (list[matplotlib.colors]): a list of colors, where each color\n                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'\n                for full list of formats that the colors are accepted in.\n\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"", "\n", "num_instances", "=", "len", "(", "boxes", ")", "\n", "\n", "if", "assigned_colors", "is", "None", ":", "\n", "            ", "assigned_colors", "=", "[", "random_color", "(", "rgb", "=", "True", ",", "maximum", "=", "1", ")", "for", "_", "in", "range", "(", "num_instances", ")", "]", "\n", "", "if", "num_instances", "==", "0", ":", "\n", "            ", "return", "self", ".", "output", "\n", "\n", "# Display in largest to smallest order to reduce occlusion.", "\n", "", "if", "boxes", "is", "not", "None", ":", "\n", "            ", "areas", "=", "boxes", "[", ":", ",", "2", "]", "*", "boxes", "[", ":", ",", "3", "]", "\n", "\n", "", "sorted_idxs", "=", "np", ".", "argsort", "(", "-", "areas", ")", ".", "tolist", "(", ")", "\n", "# Re-order overlapped instances in descending order.", "\n", "boxes", "=", "boxes", "[", "sorted_idxs", "]", "\n", "labels", "=", "[", "labels", "[", "k", "]", "for", "k", "in", "sorted_idxs", "]", "if", "labels", "is", "not", "None", "else", "None", "\n", "colors", "=", "[", "assigned_colors", "[", "idx", "]", "for", "idx", "in", "sorted_idxs", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_instances", ")", ":", "\n", "            ", "self", ".", "draw_rotated_box_with_label", "(", "\n", "boxes", "[", "i", "]", ",", "edge_color", "=", "colors", "[", "i", "]", ",", "label", "=", "labels", "[", "i", "]", "if", "labels", "is", "not", "None", "else", "None", "\n", ")", "\n", "\n", "", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_and_connect_keypoints": [[776, 833], ["visualizer.Visualizer.metadata.get", "enumerate", "visualizer.Visualizer.metadata.get", "visible.get", "visualizer.Visualizer.draw_circle", "visualizer.Visualizer.draw_line", "visualizer.Visualizer.draw_line", "tuple", "visualizer.Visualizer.draw_line"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_circle", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_line", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_line", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_line"], ["", "def", "draw_and_connect_keypoints", "(", "self", ",", "keypoints", ")", ":", "\n", "        ", "\"\"\"\n        Draws keypoints of an instance and follows the rules for keypoint connections\n        to draw lines between appropriate keypoints. This follows color heuristics for\n        line color.\n\n        Args:\n            keypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\n                and the last dimension corresponds to (x, y, probability).\n\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"", "\n", "visible", "=", "{", "}", "\n", "keypoint_names", "=", "self", ".", "metadata", ".", "get", "(", "\"keypoint_names\"", ")", "\n", "for", "idx", ",", "keypoint", "in", "enumerate", "(", "keypoints", ")", ":", "\n", "# draw keypoint", "\n", "            ", "x", ",", "y", ",", "prob", "=", "keypoint", "\n", "if", "prob", ">", "_KEYPOINT_THRESHOLD", ":", "\n", "                ", "self", ".", "draw_circle", "(", "(", "x", ",", "y", ")", ",", "color", "=", "_RED", ")", "\n", "if", "keypoint_names", ":", "\n", "                    ", "keypoint_name", "=", "keypoint_names", "[", "idx", "]", "\n", "visible", "[", "keypoint_name", "]", "=", "(", "x", ",", "y", ")", "\n", "\n", "", "", "", "if", "self", ".", "metadata", ".", "get", "(", "\"keypoint_connection_rules\"", ")", ":", "\n", "            ", "for", "kp0", ",", "kp1", ",", "color", "in", "self", ".", "metadata", ".", "keypoint_connection_rules", ":", "\n", "                ", "if", "kp0", "in", "visible", "and", "kp1", "in", "visible", ":", "\n", "                    ", "x0", ",", "y0", "=", "visible", "[", "kp0", "]", "\n", "x1", ",", "y1", "=", "visible", "[", "kp1", "]", "\n", "color", "=", "tuple", "(", "x", "/", "255.0", "for", "x", "in", "color", ")", "\n", "self", ".", "draw_line", "(", "[", "x0", ",", "x1", "]", ",", "[", "y0", ",", "y1", "]", ",", "color", "=", "color", ")", "\n", "\n", "# draw lines from nose to mid-shoulder and mid-shoulder to mid-hip", "\n", "# Note that this strategy is specific to person keypoints.", "\n", "# For other keypoints, it should just do nothing", "\n", "", "", "", "try", ":", "\n", "            ", "ls_x", ",", "ls_y", "=", "visible", "[", "\"left_shoulder\"", "]", "\n", "rs_x", ",", "rs_y", "=", "visible", "[", "\"right_shoulder\"", "]", "\n", "mid_shoulder_x", ",", "mid_shoulder_y", "=", "(", "ls_x", "+", "rs_x", ")", "/", "2", ",", "(", "ls_y", "+", "rs_y", ")", "/", "2", "\n", "", "except", "KeyError", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "# draw line from nose to mid-shoulder", "\n", "            ", "nose_x", ",", "nose_y", "=", "visible", ".", "get", "(", "\"nose\"", ",", "(", "None", ",", "None", ")", ")", "\n", "if", "nose_x", "is", "not", "None", ":", "\n", "                ", "self", ".", "draw_line", "(", "[", "nose_x", ",", "mid_shoulder_x", "]", ",", "[", "nose_y", ",", "mid_shoulder_y", "]", ",", "color", "=", "_RED", ")", "\n", "\n", "", "try", ":", "\n", "# draw line from mid-shoulder to mid-hip", "\n", "                ", "lh_x", ",", "lh_y", "=", "visible", "[", "\"left_hip\"", "]", "\n", "rh_x", ",", "rh_y", "=", "visible", "[", "\"right_hip\"", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "mid_hip_x", ",", "mid_hip_y", "=", "(", "lh_x", "+", "rh_x", ")", "/", "2", ",", "(", "lh_y", "+", "rh_y", ")", "/", "2", "\n", "self", ".", "draw_line", "(", "[", "mid_hip_x", ",", "mid_shoulder_x", "]", ",", "[", "mid_hip_y", ",", "mid_shoulder_y", "]", ",", "color", "=", "_RED", ")", "\n", "", "", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_text": [[838, 884], ["numpy.maximum", "max", "visualizer.Visualizer.output.ax.text", "list", "numpy.max", "matplotlib.to_rgb", "matplotlib.to_rgb", "matplotlib.to_rgb", "numpy.argmax"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["def", "draw_text", "(", "\n", "self", ",", "\n", "text", ",", "\n", "position", ",", "\n", "*", ",", "\n", "font_size", "=", "None", ",", "\n", "color", "=", "\"g\"", ",", "\n", "horizontal_alignment", "=", "\"center\"", ",", "\n", "rotation", "=", "0", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            text (str): class label\n            position (tuple): a tuple of the x and y coordinates to place text on image.\n            font_size (int, optional): font of the text. If not provided, a font size\n                proportional to the image width is calculated and used.\n            color: color of the text. Refer to `matplotlib.colors` for full list\n                of formats that are accepted.\n            horizontal_alignment (str): see `matplotlib.text.Text`\n            rotation: rotation angle in degrees CCW\n\n        Returns:\n            output (VisImage): image object with text drawn.\n        \"\"\"", "\n", "if", "not", "font_size", ":", "\n", "            ", "font_size", "=", "self", ".", "_default_font_size", "\n", "\n", "# since the text background is dark, we don't want the text to be dark", "\n", "", "color", "=", "np", ".", "maximum", "(", "list", "(", "mplc", ".", "to_rgb", "(", "color", ")", ")", ",", "0.2", ")", "\n", "color", "[", "np", ".", "argmax", "(", "color", ")", "]", "=", "max", "(", "0.8", ",", "np", ".", "max", "(", "color", ")", ")", "\n", "\n", "x", ",", "y", "=", "position", "\n", "self", ".", "output", ".", "ax", ".", "text", "(", "\n", "x", ",", "\n", "y", ",", "\n", "text", ",", "\n", "size", "=", "font_size", "*", "self", ".", "output", ".", "scale", ",", "\n", "family", "=", "\"sans-serif\"", ",", "\n", "bbox", "=", "{", "\"facecolor\"", ":", "\"black\"", ",", "\"alpha\"", ":", "0.8", ",", "\"pad\"", ":", "0.7", ",", "\"edgecolor\"", ":", "\"none\"", "}", ",", "\n", "verticalalignment", "=", "\"top\"", ",", "\n", "horizontalalignment", "=", "horizontal_alignment", ",", "\n", "color", "=", "color", ",", "\n", "zorder", "=", "10", ",", "\n", "rotation", "=", "rotation", ",", "\n", ")", "\n", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_box": [[885, 918], ["max", "visualizer.Visualizer.output.ax.add_patch", "matplotlib.patches.Rectangle", "matplotlib.patches.Rectangle", "matplotlib.patches.Rectangle"], "methods", ["None"], ["", "def", "draw_box", "(", "self", ",", "box_coord", ",", "alpha", "=", "0.5", ",", "edge_color", "=", "\"g\"", ",", "line_style", "=", "\"-\"", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            box_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\n                are the coordinates of the image's top left corner. x1 and y1 are the\n                coordinates of the image's bottom right corner.\n            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n            edge_color: color of the outline of the box. Refer to `matplotlib.colors`\n                for full list of formats that are accepted.\n            line_style (string): the string to use to create the outline of the boxes.\n\n        Returns:\n            output (VisImage): image object with box drawn.\n        \"\"\"", "\n", "x0", ",", "y0", ",", "x1", ",", "y1", "=", "box_coord", "\n", "width", "=", "x1", "-", "x0", "\n", "height", "=", "y1", "-", "y0", "\n", "\n", "linewidth", "=", "max", "(", "self", ".", "_default_font_size", "/", "4", ",", "1", ")", "\n", "\n", "self", ".", "output", ".", "ax", ".", "add_patch", "(", "\n", "mpl", ".", "patches", ".", "Rectangle", "(", "\n", "(", "x0", ",", "y0", ")", ",", "\n", "width", ",", "\n", "height", ",", "\n", "fill", "=", "False", ",", "\n", "edgecolor", "=", "edge_color", ",", "\n", "linewidth", "=", "linewidth", "*", "self", ".", "output", ".", "scale", ",", "\n", "alpha", "=", "alpha", ",", "\n", "linestyle", "=", "line_style", ",", "\n", ")", "\n", ")", "\n", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_rotated_box_with_label": [[919, 973], ["math.cos", "math.sin", "range", "visualizer.Visualizer.draw_line", "visualizer.Visualizer._change_color_brightness", "visualizer.Visualizer.draw_text", "numpy.sqrt", "numpy.clip"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_line", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._change_color_brightness", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_text", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip"], ["", "def", "draw_rotated_box_with_label", "(", "\n", "self", ",", "rotated_box", ",", "alpha", "=", "0.5", ",", "edge_color", "=", "\"g\"", ",", "line_style", "=", "\"-\"", ",", "label", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Draw a rotated box with label on its top-left corner.\n\n        Args:\n            rotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\n                where cnt_x and cnt_y are the center coordinates of the box.\n                w and h are the width and height of the box. angle represents how\n                many degrees the box is rotated CCW with regard to the 0-degree box.\n            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n            edge_color: color of the outline of the box. Refer to `matplotlib.colors`\n                for full list of formats that are accepted.\n            line_style (string): the string to use to create the outline of the boxes.\n            label (string): label for rotated box. It will not be rendered when set to None.\n\n        Returns:\n            output (VisImage): image object with box drawn.\n        \"\"\"", "\n", "cnt_x", ",", "cnt_y", ",", "w", ",", "h", ",", "angle", "=", "rotated_box", "\n", "area", "=", "w", "*", "h", "\n", "# use thinner lines when the box is small", "\n", "linewidth", "=", "self", ".", "_default_font_size", "/", "(", "\n", "6", "if", "area", "<", "_SMALL_OBJECT_AREA_THRESH", "*", "self", ".", "output", ".", "scale", "else", "3", "\n", ")", "\n", "\n", "theta", "=", "angle", "*", "math", ".", "pi", "/", "180.0", "\n", "c", "=", "math", ".", "cos", "(", "theta", ")", "\n", "s", "=", "math", ".", "sin", "(", "theta", ")", "\n", "rect", "=", "[", "(", "-", "w", "/", "2", ",", "h", "/", "2", ")", ",", "(", "-", "w", "/", "2", ",", "-", "h", "/", "2", ")", ",", "(", "w", "/", "2", ",", "-", "h", "/", "2", ")", ",", "(", "w", "/", "2", ",", "h", "/", "2", ")", "]", "\n", "# x: left->right ; y: top->down", "\n", "rotated_rect", "=", "[", "(", "s", "*", "yy", "+", "c", "*", "xx", "+", "cnt_x", ",", "c", "*", "yy", "-", "s", "*", "xx", "+", "cnt_y", ")", "for", "(", "xx", ",", "yy", ")", "in", "rect", "]", "\n", "for", "k", "in", "range", "(", "4", ")", ":", "\n", "            ", "j", "=", "(", "k", "+", "1", ")", "%", "4", "\n", "self", ".", "draw_line", "(", "\n", "[", "rotated_rect", "[", "k", "]", "[", "0", "]", ",", "rotated_rect", "[", "j", "]", "[", "0", "]", "]", ",", "\n", "[", "rotated_rect", "[", "k", "]", "[", "1", "]", ",", "rotated_rect", "[", "j", "]", "[", "1", "]", "]", ",", "\n", "color", "=", "edge_color", ",", "\n", "linestyle", "=", "\"--\"", "if", "k", "==", "1", "else", "line_style", ",", "\n", "linewidth", "=", "linewidth", ",", "\n", ")", "\n", "\n", "", "if", "label", "is", "not", "None", ":", "\n", "            ", "text_pos", "=", "rotated_rect", "[", "1", "]", "# topleft corner", "\n", "\n", "height_ratio", "=", "h", "/", "np", ".", "sqrt", "(", "self", ".", "output", ".", "height", "*", "self", ".", "output", ".", "width", ")", "\n", "label_color", "=", "self", ".", "_change_color_brightness", "(", "edge_color", ",", "brightness_factor", "=", "0.7", ")", "\n", "font_size", "=", "(", "\n", "np", ".", "clip", "(", "(", "height_ratio", "-", "0.02", ")", "/", "0.08", "+", "1", ",", "1.2", ",", "2", ")", "*", "0.5", "*", "self", ".", "_default_font_size", "\n", ")", "\n", "self", ".", "draw_text", "(", "label", ",", "text_pos", ",", "color", "=", "label_color", ",", "font_size", "=", "font_size", ",", "rotation", "=", "angle", ")", "\n", "\n", "", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_circle": [[974, 991], ["visualizer.Visualizer.output.ax.add_patch", "matplotlib.patches.Circle", "matplotlib.patches.Circle", "matplotlib.patches.Circle"], "methods", ["None"], ["", "def", "draw_circle", "(", "self", ",", "circle_coord", ",", "color", ",", "radius", "=", "3", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            circle_coord (list(int) or tuple(int)): contains the x and y coordinates\n                of the center of the circle.\n            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted.\n            radius (int): radius of the circle.\n\n        Returns:\n            output (VisImage): image object with box drawn.\n        \"\"\"", "\n", "x", ",", "y", "=", "circle_coord", "\n", "self", ".", "output", ".", "ax", ".", "add_patch", "(", "\n", "mpl", ".", "patches", ".", "Circle", "(", "circle_coord", ",", "radius", "=", "radius", ",", "fill", "=", "True", ",", "color", "=", "color", ")", "\n", ")", "\n", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_line": [[992, 1022], ["max", "visualizer.Visualizer.output.ax.add_line", "matplotlib.lines.Line2D", "matplotlib.lines.Line2D", "matplotlib.lines.Line2D"], "methods", ["None"], ["", "def", "draw_line", "(", "self", ",", "x_data", ",", "y_data", ",", "color", ",", "linestyle", "=", "\"-\"", ",", "linewidth", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x_data (list[int]): a list containing x values of all the points being drawn.\n                Length of list should match the length of y_data.\n            y_data (list[int]): a list containing y values of all the points being drawn.\n                Length of list should match the length of x_data.\n            color: color of the line. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted.\n            linestyle: style of the line. Refer to `matplotlib.lines.Line2D`\n                for a full list of formats that are accepted.\n            linewidth (float or None): width of the line. When it's None,\n                a default value will be computed and used.\n\n        Returns:\n            output (VisImage): image object with line drawn.\n        \"\"\"", "\n", "if", "linewidth", "is", "None", ":", "\n", "            ", "linewidth", "=", "self", ".", "_default_font_size", "/", "3", "\n", "", "linewidth", "=", "max", "(", "linewidth", ",", "1", ")", "\n", "self", ".", "output", ".", "ax", ".", "add_line", "(", "\n", "mpl", ".", "lines", ".", "Line2D", "(", "\n", "x_data", ",", "\n", "y_data", ",", "\n", "linewidth", "=", "linewidth", "*", "self", ".", "output", ".", "scale", ",", "\n", "color", "=", "color", ",", "\n", "linestyle", "=", "linestyle", ",", "\n", ")", "\n", ")", "\n", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_binary_mask": [[1023, 1083], ["matplotlib.to_rgb", "matplotlib.to_rgb", "matplotlib.to_rgb", "binary_mask.astype.astype.astype", "visualizer.GenericMask", "colormap.random_color", "numpy.zeros", "visualizer.Visualizer.output.ax.imshow", "visualizer.Visualizer._change_color_brightness", "cv2.connectedComponentsWithStats", "range", "pycocotools.area", "segment.reshape.reshape.reshape", "visualizer.Visualizer.draw_polygon", "numpy.argmax", "pycocotools.frPyObjects", "visualizer.Visualizer.draw_text", "numpy.median"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.colormap.random_color", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._change_color_brightness", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_polygon", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_text"], ["", "def", "draw_binary_mask", "(", "\n", "self", ",", "binary_mask", ",", "color", "=", "None", ",", "*", ",", "edge_color", "=", "None", ",", "text", "=", "None", ",", "alpha", "=", "0.5", ",", "area_threshold", "=", "0", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            binary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\n                W is the image width. Each value in the array is either a 0 or 1 value of uint8\n                type.\n            color: color of the mask. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted. If None, will pick a random color.\n            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n                full list of formats that are accepted.\n            text (str): if None, will be drawn in the object's center of mass.\n            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n            area_threshold (float): a connected component small than this will not be shown.\n\n        Returns:\n            output (VisImage): image object with mask drawn.\n        \"\"\"", "\n", "if", "color", "is", "None", ":", "\n", "            ", "color", "=", "random_color", "(", "rgb", "=", "True", ",", "maximum", "=", "1", ")", "\n", "", "color", "=", "mplc", ".", "to_rgb", "(", "color", ")", "\n", "\n", "has_valid_segment", "=", "False", "\n", "binary_mask", "=", "binary_mask", ".", "astype", "(", "\"uint8\"", ")", "# opencv needs uint8", "\n", "mask", "=", "GenericMask", "(", "binary_mask", ",", "self", ".", "output", ".", "height", ",", "self", ".", "output", ".", "width", ")", "\n", "shape2d", "=", "(", "binary_mask", ".", "shape", "[", "0", "]", ",", "binary_mask", ".", "shape", "[", "1", "]", ")", "\n", "\n", "if", "not", "mask", ".", "has_holes", ":", "\n", "# draw polygons for regular masks", "\n", "            ", "for", "segment", "in", "mask", ".", "polygons", ":", "\n", "                ", "area", "=", "mask_util", ".", "area", "(", "mask_util", ".", "frPyObjects", "(", "[", "segment", "]", ",", "shape2d", "[", "0", "]", ",", "shape2d", "[", "1", "]", ")", ")", "\n", "if", "area", "<", "(", "area_threshold", "or", "0", ")", ":", "\n", "                    ", "continue", "\n", "", "has_valid_segment", "=", "True", "\n", "segment", "=", "segment", ".", "reshape", "(", "-", "1", ",", "2", ")", "\n", "self", ".", "draw_polygon", "(", "segment", ",", "color", "=", "color", ",", "edge_color", "=", "edge_color", ",", "alpha", "=", "alpha", ")", "\n", "", "", "else", ":", "\n", "# TODO: Use Path/PathPatch to draw vector graphics:", "\n", "# https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon", "\n", "            ", "rgba", "=", "np", ".", "zeros", "(", "shape2d", "+", "(", "4", ",", ")", ",", "dtype", "=", "\"float32\"", ")", "\n", "rgba", "[", ":", ",", ":", ",", ":", "3", "]", "=", "color", "\n", "rgba", "[", ":", ",", ":", ",", "3", "]", "=", "(", "mask", ".", "mask", "==", "1", ")", ".", "astype", "(", "\"float32\"", ")", "*", "alpha", "\n", "has_valid_segment", "=", "True", "\n", "self", ".", "output", ".", "ax", ".", "imshow", "(", "rgba", ",", "extent", "=", "(", "0", ",", "self", ".", "output", ".", "width", ",", "self", ".", "output", ".", "height", ",", "0", ")", ")", "\n", "\n", "", "if", "text", "is", "not", "None", "and", "has_valid_segment", ":", "\n", "# TODO sometimes drawn on wrong objects. the heuristics here can improve.", "\n", "            ", "lighter_color", "=", "self", ".", "_change_color_brightness", "(", "color", ",", "brightness_factor", "=", "0.7", ")", "\n", "_num_cc", ",", "cc_labels", ",", "stats", ",", "centroids", "=", "cv2", ".", "connectedComponentsWithStats", "(", "binary_mask", ",", "8", ")", "\n", "largest_component_id", "=", "np", ".", "argmax", "(", "stats", "[", "1", ":", ",", "-", "1", "]", ")", "+", "1", "\n", "\n", "# draw text on the largest component, as well as other very large components.", "\n", "for", "cid", "in", "range", "(", "1", ",", "_num_cc", ")", ":", "\n", "                ", "if", "cid", "==", "largest_component_id", "or", "stats", "[", "cid", ",", "-", "1", "]", ">", "_LARGE_MASK_AREA_THRESH", ":", "\n", "# median is more stable than centroid", "\n", "# center = centroids[largest_component_id]", "\n", "                    ", "center", "=", "np", ".", "median", "(", "(", "cc_labels", "==", "cid", ")", ".", "nonzero", "(", ")", ",", "axis", "=", "1", ")", "[", ":", ":", "-", "1", "]", "\n", "self", ".", "draw_text", "(", "text", ",", "center", ",", "color", "=", "lighter_color", ")", "\n", "", "", "", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_polygon": [[1084, 1115], ["matplotlib.patches.Polygon", "matplotlib.patches.Polygon", "matplotlib.patches.Polygon", "visualizer.Visualizer.output.ax.add_patch", "matplotlib.to_rgb", "matplotlib.to_rgb", "matplotlib.to_rgb", "visualizer.Visualizer._change_color_brightness", "max", "matplotlib.to_rgb", "matplotlib.to_rgb", "matplotlib.to_rgb"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._change_color_brightness"], ["", "def", "draw_polygon", "(", "self", ",", "segment", ",", "color", ",", "edge_color", "=", "None", ",", "alpha", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            segment: numpy array of shape Nx2, containing all the points in the polygon.\n            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted.\n            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n                full list of formats that are accepted. If not provided, a darker shade\n                of the polygon color will be used instead.\n            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n\n        Returns:\n            output (VisImage): image object with polygon drawn.\n        \"\"\"", "\n", "if", "edge_color", "is", "None", ":", "\n", "# make edge color darker than the polygon color", "\n", "            ", "if", "alpha", ">", "0.8", ":", "\n", "                ", "edge_color", "=", "self", ".", "_change_color_brightness", "(", "color", ",", "brightness_factor", "=", "-", "0.7", ")", "\n", "", "else", ":", "\n", "                ", "edge_color", "=", "color", "\n", "", "", "edge_color", "=", "mplc", ".", "to_rgb", "(", "edge_color", ")", "+", "(", "1", ",", ")", "\n", "\n", "polygon", "=", "mpl", ".", "patches", ".", "Polygon", "(", "\n", "segment", ",", "\n", "fill", "=", "True", ",", "\n", "facecolor", "=", "mplc", ".", "to_rgb", "(", "color", ")", "+", "(", "alpha", ",", ")", ",", "\n", "edgecolor", "=", "edge_color", ",", "\n", "linewidth", "=", "max", "(", "self", ".", "_default_font_size", "//", "15", "*", "self", ".", "output", ".", "scale", ",", "1", ")", ",", "\n", ")", "\n", "self", ".", "output", ".", "ax", ".", "add_patch", "(", "polygon", ")", "\n", "return", "self", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._jitter": [[1120, 1138], ["matplotlib.to_rgb", "matplotlib.to_rgb", "matplotlib.to_rgb", "numpy.random.rand", "numpy.clip", "tuple", "numpy.linalg.norm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip"], ["def", "_jitter", "(", "self", ",", "color", ")", ":", "\n", "        ", "\"\"\"\n        Randomly modifies given color to produce a slightly different color than the color given.\n\n        Args:\n            color (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\n                picked. The values in the list are in the [0.0, 1.0] range.\n\n        Returns:\n            jittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\n                color after being jittered. The values in the list are in the [0.0, 1.0] range.\n        \"\"\"", "\n", "color", "=", "mplc", ".", "to_rgb", "(", "color", ")", "\n", "vec", "=", "np", ".", "random", ".", "rand", "(", "3", ")", "\n", "# better to do it in another color space", "\n", "vec", "=", "vec", "/", "np", ".", "linalg", ".", "norm", "(", "vec", ")", "*", "0.5", "\n", "res", "=", "np", ".", "clip", "(", "vec", "+", "color", ",", "0", ",", "1", ")", "\n", "return", "tuple", "(", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._create_grayscale_image": [[1139, 1149], ["visualizer.Visualizer.img.astype().mean", "numpy.stack", "visualizer.Visualizer.img.astype"], "methods", ["None"], ["", "def", "_create_grayscale_image", "(", "self", ",", "mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Create a grayscale version of the original image.\n        The colors in masked area, if given, will be kept.\n        \"\"\"", "\n", "img_bw", "=", "self", ".", "img", ".", "astype", "(", "\"f4\"", ")", ".", "mean", "(", "axis", "=", "2", ")", "\n", "img_bw", "=", "np", ".", "stack", "(", "[", "img_bw", "]", "*", "3", ",", "axis", "=", "2", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "img_bw", "[", "mask", "]", "=", "self", ".", "img", "[", "mask", "]", "\n", "", "return", "img_bw", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._change_color_brightness": [[1150, 1174], ["matplotlib.to_rgb", "matplotlib.to_rgb", "matplotlib.to_rgb", "colorsys.rgb_to_hls", "colorsys.hls_to_rgb", "matplotlib.to_rgb", "matplotlib.to_rgb", "matplotlib.to_rgb"], "methods", ["None"], ["", "def", "_change_color_brightness", "(", "self", ",", "color", ",", "brightness_factor", ")", ":", "\n", "        ", "\"\"\"\n        Depending on the brightness_factor, gives a lighter or darker color i.e. a color with\n        less or more saturation than the original color.\n\n        Args:\n            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted.\n            brightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n                0 will correspond to no change, a factor in [-1.0, 0) range will result in\n                a darker color and a factor in (0, 1.0] range will result in a lighter color.\n\n        Returns:\n            modified_color (tuple[double]): a tuple containing the RGB values of the\n                modified color. Each value in the tuple is in the [0.0, 1.0] range.\n        \"\"\"", "\n", "assert", "brightness_factor", ">=", "-", "1.0", "and", "brightness_factor", "<=", "1.0", "\n", "color", "=", "mplc", ".", "to_rgb", "(", "color", ")", "\n", "polygon_color", "=", "colorsys", ".", "rgb_to_hls", "(", "*", "mplc", ".", "to_rgb", "(", "color", ")", ")", "\n", "modified_lightness", "=", "polygon_color", "[", "1", "]", "+", "(", "brightness_factor", "*", "polygon_color", "[", "1", "]", ")", "\n", "modified_lightness", "=", "0.0", "if", "modified_lightness", "<", "0.0", "else", "modified_lightness", "\n", "modified_lightness", "=", "1.0", "if", "modified_lightness", ">", "1.0", "else", "modified_lightness", "\n", "modified_color", "=", "colorsys", ".", "hls_to_rgb", "(", "polygon_color", "[", "0", "]", ",", "modified_lightness", ",", "polygon_color", "[", "2", "]", ")", "\n", "return", "modified_color", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._convert_boxes": [[1175, 1183], ["isinstance", "isinstance", "boxes.tensor.numpy", "numpy.asarray"], "methods", ["None"], ["", "def", "_convert_boxes", "(", "self", ",", "boxes", ")", ":", "\n", "        ", "\"\"\"\n        Convert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n        \"\"\"", "\n", "if", "isinstance", "(", "boxes", ",", "Boxes", ")", "or", "isinstance", "(", "boxes", ",", "RotatedBoxes", ")", ":", "\n", "            ", "return", "boxes", ".", "tensor", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "asarray", "(", "boxes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._convert_masks": [[1184, 1206], ["isinstance", "isinstance", "isinstance", "m.numpy.numpy.tensor.numpy", "m.numpy.numpy.numpy", "isinstance", "ret.append", "ret.append", "visualizer.GenericMask"], "methods", ["None"], ["", "", "def", "_convert_masks", "(", "self", ",", "masks_or_polygons", ")", ":", "\n", "        ", "\"\"\"\n        Convert different format of masks or polygons to a tuple of masks and polygons.\n\n        Returns:\n            list[GenericMask]:\n        \"\"\"", "\n", "\n", "m", "=", "masks_or_polygons", "\n", "if", "isinstance", "(", "m", ",", "PolygonMasks", ")", ":", "\n", "            ", "m", "=", "m", ".", "polygons", "\n", "", "if", "isinstance", "(", "m", ",", "BitMasks", ")", ":", "\n", "            ", "m", "=", "m", ".", "tensor", ".", "numpy", "(", ")", "\n", "", "if", "isinstance", "(", "m", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "m", "=", "m", ".", "numpy", "(", ")", "\n", "", "ret", "=", "[", "]", "\n", "for", "x", "in", "m", ":", "\n", "            ", "if", "isinstance", "(", "x", ",", "GenericMask", ")", ":", "\n", "                ", "ret", ".", "append", "(", "x", ")", "\n", "", "else", ":", "\n", "                ", "ret", ".", "append", "(", "GenericMask", "(", "x", ",", "self", ".", "output", ".", "height", ",", "self", ".", "output", ".", "width", ")", ")", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._convert_keypoints": [[1207, 1212], ["isinstance", "numpy.asarray"], "methods", ["None"], ["", "def", "_convert_keypoints", "(", "self", ",", "keypoints", ")", ":", "\n", "        ", "if", "isinstance", "(", "keypoints", ",", "Keypoints", ")", ":", "\n", "            ", "keypoints", "=", "keypoints", ".", "tensor", "\n", "", "keypoints", "=", "np", ".", "asarray", "(", "keypoints", ")", "\n", "return", "keypoints", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.get_output": [[1213, 1220], ["None"], "methods", ["None"], ["", "def", "get_output", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            output (VisImage): the image output containing the visualizations added\n            to the image.\n        \"\"\"", "\n", "return", "self", ".", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._create_text_labels": [[227, 252], ["len", "str", "zip", "zip"], "function", ["None"], ["", "", "", "", "def", "_create_text_labels", "(", "classes", ",", "scores", ",", "class_names", ",", "is_crowd", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        classes (list[int] or None):\n        scores (list[float] or None):\n        class_names (list[str] or None):\n        is_crowd (list[bool] or None):\n\n    Returns:\n        list[str] or None\n    \"\"\"", "\n", "labels", "=", "None", "\n", "if", "classes", "is", "not", "None", ":", "\n", "        ", "if", "class_names", "is", "not", "None", "and", "len", "(", "class_names", ")", ">", "0", ":", "\n", "            ", "labels", "=", "[", "class_names", "[", "i", "]", "for", "i", "in", "classes", "]", "\n", "", "else", ":", "\n", "            ", "labels", "=", "[", "str", "(", "i", ")", "for", "i", "in", "classes", "]", "\n", "", "", "if", "scores", "is", "not", "None", ":", "\n", "        ", "if", "labels", "is", "None", ":", "\n", "            ", "labels", "=", "[", "\"{:.0f}%\"", ".", "format", "(", "s", "*", "100", ")", "for", "s", "in", "scores", "]", "\n", "", "else", ":", "\n", "            ", "labels", "=", "[", "\"{} {:.0f}%\"", ".", "format", "(", "l", ",", "s", "*", "100", ")", "for", "l", ",", "s", "in", "zip", "(", "labels", ",", "scores", ")", "]", "\n", "", "", "if", "labels", "is", "not", "None", "and", "is_crowd", "is", "not", "None", ":", "\n", "        ", "labels", "=", "[", "l", "+", "(", "\"|crowd\"", "if", "crowd", "else", "\"\"", ")", "for", "l", ",", "crowd", "in", "zip", "(", "labels", ",", "is_crowd", ")", "]", "\n", "", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer._DetectedInstance.__init__": [[31, 37], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "label", ",", "bbox", ",", "mask_rle", ",", "color", ",", "ttl", ")", ":", "\n", "        ", "self", ".", "label", "=", "label", "\n", "self", ".", "bbox", "=", "bbox", "\n", "self", ".", "mask_rle", "=", "mask_rle", "\n", "self", ".", "color", "=", "color", "\n", "self", ".", "ttl", "=", "ttl", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.__init__": [[40, 52], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "metadata", ",", "instance_mode", "=", "ColorMode", ".", "IMAGE", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            metadata (MetadataCatalog): image metadata.\n        \"\"\"", "\n", "self", ".", "metadata", "=", "metadata", "\n", "self", ".", "_old_instances", "=", "[", "]", "\n", "assert", "instance_mode", "in", "[", "\n", "ColorMode", ".", "IMAGE", ",", "\n", "ColorMode", ".", "IMAGE_BW", ",", "\n", "]", ",", "\"Other mode not supported yet.\"", "\n", "self", ".", "_instance_mode", "=", "instance_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_instance_predictions": [[53, 111], ["detectron2.utils.visualizer.Visualizer", "len", "predictions.has", "video_visualizer.VideoVisualizer._assign_colors", "detectron2.utils.visualizer._create_text_labels", "detectron2.utils.visualizer.Visualizer.overlay_instances", "predictions.has", "predictions.pred_boxes.tensor.numpy", "predictions.has", "predictions.has", "predictions.pred_classes.numpy", "predictions.has", "video_visualizer._DetectedInstance", "video_visualizer.VideoVisualizer.metadata.get", "detectron2.utils.visualizer.Visualizer._create_grayscale_image", "range", "masks.any"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer._assign_colors", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._create_text_labels", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._create_grayscale_image"], ["", "def", "draw_instance_predictions", "(", "self", ",", "frame", ",", "predictions", ")", ":", "\n", "        ", "\"\"\"\n        Draw instance-level prediction results on an image.\n\n        Args:\n            frame (ndarray): an RGB image of shape (H, W, C), in the range [0, 255].\n            predictions (Instances): the output of an instance detection/segmentation\n                model. Following fields will be used to draw:\n                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\n\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"", "\n", "frame_visualizer", "=", "Visualizer", "(", "frame", ",", "self", ".", "metadata", ")", "\n", "num_instances", "=", "len", "(", "predictions", ")", "\n", "if", "num_instances", "==", "0", ":", "\n", "            ", "return", "frame_visualizer", ".", "output", "\n", "\n", "", "boxes", "=", "predictions", ".", "pred_boxes", ".", "tensor", ".", "numpy", "(", ")", "if", "predictions", ".", "has", "(", "\"pred_boxes\"", ")", "else", "None", "\n", "scores", "=", "predictions", ".", "scores", "if", "predictions", ".", "has", "(", "\"scores\"", ")", "else", "None", "\n", "classes", "=", "predictions", ".", "pred_classes", ".", "numpy", "(", ")", "if", "predictions", ".", "has", "(", "\"pred_classes\"", ")", "else", "None", "\n", "keypoints", "=", "predictions", ".", "pred_keypoints", "if", "predictions", ".", "has", "(", "\"pred_keypoints\"", ")", "else", "None", "\n", "\n", "if", "predictions", ".", "has", "(", "\"pred_masks\"", ")", ":", "\n", "            ", "masks", "=", "predictions", ".", "pred_masks", "\n", "# mask IOU is not yet enabled", "\n", "# masks_rles = mask_util.encode(np.asarray(masks.permute(1, 2, 0), order=\"F\"))", "\n", "# assert len(masks_rles) == num_instances", "\n", "", "else", ":", "\n", "            ", "masks", "=", "None", "\n", "\n", "", "detected", "=", "[", "\n", "_DetectedInstance", "(", "classes", "[", "i", "]", ",", "boxes", "[", "i", "]", ",", "mask_rle", "=", "None", ",", "color", "=", "None", ",", "ttl", "=", "8", ")", "\n", "for", "i", "in", "range", "(", "num_instances", ")", "\n", "]", "\n", "colors", "=", "self", ".", "_assign_colors", "(", "detected", ")", "\n", "\n", "labels", "=", "_create_text_labels", "(", "classes", ",", "scores", ",", "self", ".", "metadata", ".", "get", "(", "\"thing_classes\"", ",", "None", ")", ")", "\n", "\n", "if", "self", ".", "_instance_mode", "==", "ColorMode", ".", "IMAGE_BW", ":", "\n", "# any() returns uint8 tensor", "\n", "            ", "frame_visualizer", ".", "output", ".", "img", "=", "frame_visualizer", ".", "_create_grayscale_image", "(", "\n", "(", "masks", ".", "any", "(", "dim", "=", "0", ")", ">", "0", ")", ".", "numpy", "(", ")", "if", "masks", "is", "not", "None", "else", "None", "\n", ")", "\n", "alpha", "=", "0.3", "\n", "", "else", ":", "\n", "            ", "alpha", "=", "0.5", "\n", "\n", "", "frame_visualizer", ".", "overlay_instances", "(", "\n", "boxes", "=", "None", "if", "masks", "is", "not", "None", "else", "boxes", ",", "# boxes are a bit distracting", "\n", "masks", "=", "masks", ",", "\n", "labels", "=", "labels", ",", "\n", "keypoints", "=", "keypoints", ",", "\n", "assigned_colors", "=", "colors", ",", "\n", "alpha", "=", "alpha", ",", "\n", ")", "\n", "\n", "return", "frame_visualizer", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_sem_seg": [[112, 123], ["detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_sem_seg"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_sem_seg"], ["", "def", "draw_sem_seg", "(", "self", ",", "frame", ",", "sem_seg", ",", "area_threshold", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            sem_seg (ndarray or Tensor): semantic segmentation of shape (H, W),\n                each value is the integer label.\n            area_threshold (Optional[int]): only draw segmentations larger than the threshold\n        \"\"\"", "\n", "# don't need to do anything special", "\n", "frame_visualizer", "=", "Visualizer", "(", "frame", ",", "self", ".", "metadata", ")", "\n", "frame_visualizer", ".", "draw_sem_seg", "(", "sem_seg", ",", "area_threshold", "=", "None", ")", "\n", "return", "frame_visualizer", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_panoptic_seg_predictions": [[124, 179], ["detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer._PanopticPrediction", "detectron2.utils.visualizer._PanopticPrediction.semantic_masks", "list", "list", "len", "pycocotools.encode", "video_visualizer.VideoVisualizer._assign_colors", "detectron2.utils.visualizer.Visualizer.overlay_instances", "detectron2.utils.visualizer.Visualizer._create_grayscale_image", "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "detectron2.utils.visualizer._PanopticPrediction.instance_masks", "len", "zip", "numpy.asarray", "len", "video_visualizer._DetectedInstance", "detectron2.utils.visualizer._PanopticPrediction.non_empty_mask", "numpy.asarray().transpose", "range", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.semantic_masks", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer._assign_colors", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer._create_grayscale_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_binary_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.instance_masks", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer._PanopticPrediction.non_empty_mask"], ["", "def", "draw_panoptic_seg_predictions", "(", "\n", "self", ",", "frame", ",", "panoptic_seg", ",", "segments_info", ",", "area_threshold", "=", "None", ",", "alpha", "=", "0.5", "\n", ")", ":", "\n", "        ", "frame_visualizer", "=", "Visualizer", "(", "frame", ",", "self", ".", "metadata", ")", "\n", "pred", "=", "_PanopticPrediction", "(", "panoptic_seg", ",", "segments_info", ",", "self", ".", "metadata", ")", "\n", "\n", "if", "self", ".", "_instance_mode", "==", "ColorMode", ".", "IMAGE_BW", ":", "\n", "            ", "frame_visualizer", ".", "output", ".", "img", "=", "frame_visualizer", ".", "_create_grayscale_image", "(", "\n", "pred", ".", "non_empty_mask", "(", ")", "\n", ")", "\n", "\n", "# draw mask for all semantic segments first i.e. \"stuff\"", "\n", "", "for", "mask", ",", "sinfo", "in", "pred", ".", "semantic_masks", "(", ")", ":", "\n", "            ", "category_idx", "=", "sinfo", "[", "\"category_id\"", "]", "\n", "try", ":", "\n", "                ", "mask_color", "=", "[", "x", "/", "255", "for", "x", "in", "self", ".", "metadata", ".", "stuff_colors", "[", "category_idx", "]", "]", "\n", "", "except", "AttributeError", ":", "\n", "                ", "mask_color", "=", "None", "\n", "\n", "", "frame_visualizer", ".", "draw_binary_mask", "(", "\n", "mask", ",", "\n", "color", "=", "mask_color", ",", "\n", "text", "=", "self", ".", "metadata", ".", "stuff_classes", "[", "category_idx", "]", ",", "\n", "alpha", "=", "alpha", ",", "\n", "area_threshold", "=", "area_threshold", ",", "\n", ")", "\n", "\n", "", "all_instances", "=", "list", "(", "pred", ".", "instance_masks", "(", ")", ")", "\n", "if", "len", "(", "all_instances", ")", "==", "0", ":", "\n", "            ", "return", "frame_visualizer", ".", "output", "\n", "# draw mask for all instances second", "\n", "", "masks", ",", "sinfo", "=", "list", "(", "zip", "(", "*", "all_instances", ")", ")", "\n", "num_instances", "=", "len", "(", "masks", ")", "\n", "masks_rles", "=", "mask_util", ".", "encode", "(", "\n", "np", ".", "asarray", "(", "np", ".", "asarray", "(", "masks", ")", ".", "transpose", "(", "1", ",", "2", ",", "0", ")", ",", "dtype", "=", "np", ".", "uint8", ",", "order", "=", "\"F\"", ")", "\n", ")", "\n", "assert", "len", "(", "masks_rles", ")", "==", "num_instances", "\n", "\n", "category_ids", "=", "[", "x", "[", "\"category_id\"", "]", "for", "x", "in", "sinfo", "]", "\n", "detected", "=", "[", "\n", "_DetectedInstance", "(", "category_ids", "[", "i", "]", ",", "bbox", "=", "None", ",", "mask_rle", "=", "masks_rles", "[", "i", "]", ",", "color", "=", "None", ",", "ttl", "=", "8", ")", "\n", "for", "i", "in", "range", "(", "num_instances", ")", "\n", "]", "\n", "colors", "=", "self", ".", "_assign_colors", "(", "detected", ")", "\n", "labels", "=", "[", "self", ".", "metadata", ".", "thing_classes", "[", "k", "]", "for", "k", "in", "category_ids", "]", "\n", "\n", "frame_visualizer", ".", "overlay_instances", "(", "\n", "boxes", "=", "None", ",", "\n", "masks", "=", "masks", ",", "\n", "labels", "=", "labels", ",", "\n", "keypoints", "=", "None", ",", "\n", "assigned_colors", "=", "colors", ",", "\n", "alpha", "=", "alpha", ",", "\n", ")", "\n", "return", "frame_visualizer", ".", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer._assign_colors": [[180, 236], ["numpy.zeros", "enumerate", "numpy.asarray().argmax", "numpy.asarray().max", "enumerate", "pycocotools.iou", "pycocotools.iou", "len", "numpy.zeros", "enumerate", "len", "numpy.asarray", "numpy.asarray", "extra_instances.append", "colormap.random_color", "len", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.colormap.random_color"], ["", "def", "_assign_colors", "(", "self", ",", "instances", ")", ":", "\n", "        ", "\"\"\"\n        Naive tracking heuristics to assign same color to the same instance,\n        will update the internal state of tracked instances.\n\n        Returns:\n            list[tuple[float]]: list of colors.\n        \"\"\"", "\n", "\n", "# Compute iou with either boxes or masks:", "\n", "is_crowd", "=", "np", ".", "zeros", "(", "(", "len", "(", "instances", ")", ",", ")", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "if", "instances", "[", "0", "]", ".", "bbox", "is", "None", ":", "\n", "            ", "assert", "instances", "[", "0", "]", ".", "mask_rle", "is", "not", "None", "\n", "# use mask iou only when box iou is None", "\n", "# because box seems good enough", "\n", "rles_old", "=", "[", "x", ".", "mask_rle", "for", "x", "in", "self", ".", "_old_instances", "]", "\n", "rles_new", "=", "[", "x", ".", "mask_rle", "for", "x", "in", "instances", "]", "\n", "ious", "=", "mask_util", ".", "iou", "(", "rles_old", ",", "rles_new", ",", "is_crowd", ")", "\n", "threshold", "=", "0.5", "\n", "", "else", ":", "\n", "            ", "boxes_old", "=", "[", "x", ".", "bbox", "for", "x", "in", "self", ".", "_old_instances", "]", "\n", "boxes_new", "=", "[", "x", ".", "bbox", "for", "x", "in", "instances", "]", "\n", "ious", "=", "mask_util", ".", "iou", "(", "boxes_old", ",", "boxes_new", ",", "is_crowd", ")", "\n", "threshold", "=", "0.6", "\n", "", "if", "len", "(", "ious", ")", "==", "0", ":", "\n", "            ", "ious", "=", "np", ".", "zeros", "(", "(", "len", "(", "self", ".", "_old_instances", ")", ",", "len", "(", "instances", ")", ")", ",", "dtype", "=", "\"float32\"", ")", "\n", "\n", "# Only allow matching instances of the same label:", "\n", "", "for", "old_idx", ",", "old", "in", "enumerate", "(", "self", ".", "_old_instances", ")", ":", "\n", "            ", "for", "new_idx", ",", "new", "in", "enumerate", "(", "instances", ")", ":", "\n", "                ", "if", "old", ".", "label", "!=", "new", ".", "label", ":", "\n", "                    ", "ious", "[", "old_idx", ",", "new_idx", "]", "=", "0", "\n", "\n", "", "", "", "matched_new_per_old", "=", "np", ".", "asarray", "(", "ious", ")", ".", "argmax", "(", "axis", "=", "1", ")", "\n", "max_iou_per_old", "=", "np", ".", "asarray", "(", "ious", ")", ".", "max", "(", "axis", "=", "1", ")", "\n", "\n", "# Try to find match for each old instance:", "\n", "extra_instances", "=", "[", "]", "\n", "for", "idx", ",", "inst", "in", "enumerate", "(", "self", ".", "_old_instances", ")", ":", "\n", "            ", "if", "max_iou_per_old", "[", "idx", "]", ">", "threshold", ":", "\n", "                ", "newidx", "=", "matched_new_per_old", "[", "idx", "]", "\n", "if", "instances", "[", "newidx", "]", ".", "color", "is", "None", ":", "\n", "                    ", "instances", "[", "newidx", "]", ".", "color", "=", "inst", ".", "color", "\n", "continue", "\n", "# If an old instance does not match any new instances,", "\n", "# keep it for the next frame in case it is just missed by the detector", "\n", "", "", "inst", ".", "ttl", "-=", "1", "\n", "if", "inst", ".", "ttl", ">", "0", ":", "\n", "                ", "extra_instances", ".", "append", "(", "inst", ")", "\n", "\n", "# Assign random color to newly-detected instances:", "\n", "", "", "for", "inst", "in", "instances", ":", "\n", "            ", "if", "inst", ".", "color", "is", "None", ":", "\n", "                ", "inst", ".", "color", "=", "random_color", "(", "rgb", "=", "True", ",", "maximum", "=", "1", ")", "\n", "", "", "self", ".", "_old_instances", "=", "instances", "[", ":", "]", "+", "extra_instances", "\n", "return", "[", "d", ".", "color", "for", "d", "in", "instances", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.analysis.FlopCountAnalysis.__init__": [[57, 66], ["detectron2.export.TracingAdapter", "super().__init__", "analysis.FlopCountAnalysis.set_op_handle"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            model (nn.Module):\n            inputs (Any): inputs of the given model. Does not have to be tuple of tensors.\n        \"\"\"", "\n", "wrapper", "=", "TracingAdapter", "(", "model", ",", "inputs", ",", "allow_non_tensor", "=", "True", ")", "\n", "super", "(", ")", ".", "__init__", "(", "wrapper", ",", "wrapper", ".", "flattened_inputs", ")", "\n", "self", ".", "set_op_handle", "(", "**", "{", "k", ":", "None", "for", "k", "in", "_IGNORED_OPS", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.analysis.flop_count_operators": [[68, 98], ["model.eval", "FlopCountAnalysis().by_operator", "model.train", "analysis.FlopCountAnalysis", "FlopCountAnalysis().by_operator.items"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train"], ["", "", "def", "flop_count_operators", "(", "model", ":", "nn", ".", "Module", ",", "inputs", ":", "list", ")", "->", "typing", ".", "DefaultDict", "[", "str", ",", "float", "]", ":", "\n", "    ", "\"\"\"\n    Implement operator-level flops counting using jit.\n    This is a wrapper of :func:`fvcore.nn.flop_count` and adds supports for standard\n    detection models in detectron2.\n    Please use :class:`FlopCountAnalysis` for more advanced functionalities.\n\n    Note:\n        The function runs the input through the model to compute flops.\n        The flops of a detection model is often input-dependent, for example,\n        the flops of box & mask head depends on the number of proposals &\n        the number of detected objects.\n        Therefore, the flops counting using a single input may not accurately\n        reflect the computation cost of a model. It's recommended to average\n        across a number of inputs.\n\n    Args:\n        model: a detectron2 model that takes `list[dict]` as input.\n        inputs (list[dict]): inputs to model, in detectron2's standard format.\n            Only \"image\" key will be used.\n        supported_ops (dict[str, Handle]): see documentation of :func:`fvcore.nn.flop_count`\n\n    Returns:\n        Counter: Gflop count per operator\n    \"\"\"", "\n", "old_train", "=", "model", ".", "training", "\n", "model", ".", "eval", "(", ")", "\n", "ret", "=", "FlopCountAnalysis", "(", "model", ",", "inputs", ")", ".", "by_operator", "(", ")", "\n", "model", ".", "train", "(", "old_train", ")", "\n", "return", "{", "k", ":", "v", "/", "1e9", "for", "k", ",", "v", "in", "ret", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.analysis.activation_count_operators": [[100, 123], ["analysis._wrapper_count_operators"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.analysis._wrapper_count_operators"], ["", "def", "activation_count_operators", "(", "\n", "model", ":", "nn", ".", "Module", ",", "inputs", ":", "list", ",", "**", "kwargs", "\n", ")", "->", "typing", ".", "DefaultDict", "[", "str", ",", "float", "]", ":", "\n", "    ", "\"\"\"\n    Implement operator-level activations counting using jit.\n    This is a wrapper of fvcore.nn.activation_count, that supports standard detection models\n    in detectron2.\n\n    Note:\n        The function runs the input through the model to compute activations.\n        The activations of a detection model is often input-dependent, for example,\n        the activations of box & mask head depends on the number of proposals &\n        the number of detected objects.\n\n    Args:\n        model: a detectron2 model that takes `list[dict]` as input.\n        inputs (list[dict]): inputs to model, in detectron2's standard format.\n            Only \"image\" key will be used.\n\n    Returns:\n        Counter: activation count per operator\n    \"\"\"", "\n", "return", "_wrapper_count_operators", "(", "model", "=", "model", ",", "inputs", "=", "inputs", ",", "mode", "=", "ACTIVATIONS_MODE", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.analysis._wrapper_count_operators": [[125, 153], ["supported_ops.update", "isinstance", "detectron2.export.TracingAdapter", "detectron2.export.TracingAdapter.eval", "isinstance", "model.train", "kwargs.pop", "len", "fvcore.nn.flop_count", "fvcore.nn.activation_count", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train"], ["", "def", "_wrapper_count_operators", "(", "\n", "model", ":", "nn", ".", "Module", ",", "inputs", ":", "list", ",", "mode", ":", "str", ",", "**", "kwargs", "\n", ")", "->", "typing", ".", "DefaultDict", "[", "str", ",", "float", "]", ":", "\n", "# ignore some ops", "\n", "    ", "supported_ops", "=", "{", "k", ":", "lambda", "*", "args", ",", "**", "kwargs", ":", "{", "}", "for", "k", "in", "_IGNORED_OPS", "}", "\n", "supported_ops", ".", "update", "(", "kwargs", ".", "pop", "(", "\"supported_ops\"", ",", "{", "}", ")", ")", "\n", "kwargs", "[", "\"supported_ops\"", "]", "=", "supported_ops", "\n", "\n", "assert", "len", "(", "inputs", ")", "==", "1", ",", "\"Please use batch size=1\"", "\n", "tensor_input", "=", "inputs", "[", "0", "]", "[", "\"image\"", "]", "\n", "inputs", "=", "[", "{", "\"image\"", ":", "tensor_input", "}", "]", "# remove other keys, in case there are any", "\n", "\n", "old_train", "=", "model", ".", "training", "\n", "if", "isinstance", "(", "model", ",", "(", "nn", ".", "parallel", ".", "distributed", ".", "DistributedDataParallel", ",", "nn", ".", "DataParallel", ")", ")", ":", "\n", "        ", "model", "=", "model", ".", "module", "\n", "", "wrapper", "=", "TracingAdapter", "(", "model", ",", "inputs", ")", "\n", "wrapper", ".", "eval", "(", ")", "\n", "if", "mode", "==", "FLOPS_MODE", ":", "\n", "        ", "ret", "=", "flop_count", "(", "wrapper", ",", "(", "tensor_input", ",", ")", ",", "**", "kwargs", ")", "\n", "", "elif", "mode", "==", "ACTIVATIONS_MODE", ":", "\n", "        ", "ret", "=", "activation_count", "(", "wrapper", ",", "(", "tensor_input", ",", ")", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Count for mode {} is not supported yet.\"", ".", "format", "(", "mode", ")", ")", "\n", "# compatible with change in fvcore", "\n", "", "if", "isinstance", "(", "ret", ",", "tuple", ")", ":", "\n", "        ", "ret", "=", "ret", "[", "0", "]", "\n", "", "model", ".", "train", "(", "old_train", ")", "\n", "return", "ret", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._ColorfulFormatter.__init__": [[19, 25], ["kwargs.pop", "len", "logging.Formatter.__init__", "kwargs.pop"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_root_name", "=", "kwargs", ".", "pop", "(", "\"root_name\"", ")", "+", "\".\"", "\n", "self", ".", "_abbrev_name", "=", "kwargs", ".", "pop", "(", "\"abbrev_name\"", ",", "\"\"", ")", "\n", "if", "len", "(", "self", ".", "_abbrev_name", ")", ":", "\n", "            ", "self", ".", "_abbrev_name", "=", "self", ".", "_abbrev_name", "+", "\".\"", "\n", "", "super", "(", "_ColorfulFormatter", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._ColorfulFormatter.formatMessage": [[26, 36], ["record.name.replace", "super().formatMessage", "termcolor.colored", "termcolor.colored"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._ColorfulFormatter.formatMessage"], ["", "def", "formatMessage", "(", "self", ",", "record", ")", ":", "\n", "        ", "record", ".", "name", "=", "record", ".", "name", ".", "replace", "(", "self", ".", "_root_name", ",", "self", ".", "_abbrev_name", ")", "\n", "log", "=", "super", "(", "_ColorfulFormatter", ",", "self", ")", ".", "formatMessage", "(", "record", ")", "\n", "if", "record", ".", "levelno", "==", "logging", ".", "WARNING", ":", "\n", "            ", "prefix", "=", "colored", "(", "\"WARNING\"", ",", "\"red\"", ",", "attrs", "=", "[", "\"blink\"", "]", ")", "\n", "", "elif", "record", ".", "levelno", "==", "logging", ".", "ERROR", "or", "record", ".", "levelno", "==", "logging", ".", "CRITICAL", ":", "\n", "            ", "prefix", "=", "colored", "(", "\"ERROR\"", ",", "\"red\"", ",", "attrs", "=", "[", "\"blink\"", ",", "\"underline\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "log", "\n", "", "return", "prefix", "+", "\" \"", "+", "log", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger": [[38, 100], ["functools.lru_cache", "logging.getLogger", "logging.getLogger.setLevel", "logging.Formatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "detectron2.utils.file_io.PathManager.mkdirs", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "logger._ColorfulFormatter", "output.endswith", "output.endswith", "os.path.join", "os.path.dirname", "logger._cached_log_stream", "termcolor.colored", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._cached_log_stream"], ["", "", "@", "functools", ".", "lru_cache", "(", ")", "# so that calling setup_logger multiple times won't add many handlers", "\n", "def", "setup_logger", "(", "\n", "output", "=", "None", ",", "distributed_rank", "=", "0", ",", "*", ",", "color", "=", "True", ",", "name", "=", "\"detectron2\"", ",", "abbrev_name", "=", "None", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Initialize the detectron2 logger and set its verbosity level to \"DEBUG\".\n\n    Args:\n        output (str): a file name or a directory to save log. If None, will not save log file.\n            If ends with \".txt\" or \".log\", assumed to be a file name.\n            Otherwise, logs will be saved to `output/log.txt`.\n        name (str): the root module name of this logger\n        abbrev_name (str): an abbreviation of the module, to avoid long names in logs.\n            Set to \"\" to not log the root module in logs.\n            By default, will abbreviate \"detectron2\" to \"d2\" and leave other\n            modules unchanged.\n\n    Returns:\n        logging.Logger: a logger\n    \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "name", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "logger", ".", "propagate", "=", "False", "\n", "\n", "if", "abbrev_name", "is", "None", ":", "\n", "        ", "abbrev_name", "=", "\"d2\"", "if", "name", "==", "\"detectron2\"", "else", "name", "\n", "\n", "", "plain_formatter", "=", "logging", ".", "Formatter", "(", "\n", "\"[%(asctime)s] %(name)s %(levelname)s: %(message)s\"", ",", "datefmt", "=", "\"%m/%d %H:%M:%S\"", "\n", ")", "\n", "# stdout logging: master only", "\n", "if", "distributed_rank", "==", "0", ":", "\n", "        ", "ch", "=", "logging", ".", "StreamHandler", "(", "stream", "=", "sys", ".", "stdout", ")", "\n", "ch", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "if", "color", ":", "\n", "            ", "formatter", "=", "_ColorfulFormatter", "(", "\n", "colored", "(", "\"[%(asctime)s %(name)s]: \"", ",", "\"green\"", ")", "+", "\"%(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d %H:%M:%S\"", ",", "\n", "root_name", "=", "name", ",", "\n", "abbrev_name", "=", "str", "(", "abbrev_name", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "formatter", "=", "plain_formatter", "\n", "", "ch", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "ch", ")", "\n", "\n", "# file logging: all workers", "\n", "", "if", "output", "is", "not", "None", ":", "\n", "        ", "if", "output", ".", "endswith", "(", "\".txt\"", ")", "or", "output", ".", "endswith", "(", "\".log\"", ")", ":", "\n", "            ", "filename", "=", "output", "\n", "", "else", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "join", "(", "output", ",", "\"log.txt\"", ")", "\n", "", "if", "distributed_rank", ">", "0", ":", "\n", "            ", "filename", "=", "filename", "+", "\".rank{}\"", ".", "format", "(", "distributed_rank", ")", "\n", "", "PathManager", ".", "mkdirs", "(", "os", ".", "path", ".", "dirname", "(", "filename", ")", ")", "\n", "\n", "fh", "=", "logging", ".", "StreamHandler", "(", "_cached_log_stream", "(", "filename", ")", ")", "\n", "fh", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "fh", ".", "setFormatter", "(", "plain_formatter", ")", "\n", "logger", ".", "addHandler", "(", "fh", ")", "\n", "\n", "", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._cached_log_stream": [[104, 110], ["functools.lru_cache", "detectron2.utils.file_io.PathManager.open", "atexit.register"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register"], ["", "@", "functools", ".", "lru_cache", "(", "maxsize", "=", "None", ")", "\n", "def", "_cached_log_stream", "(", "filename", ")", ":", "\n", "# use 1K buffer if writing to cloud storage", "\n", "    ", "io", "=", "PathManager", ".", "open", "(", "filename", ",", "\"a\"", ",", "buffering", "=", "1024", "if", "\"://\"", "in", "filename", "else", "-", "1", ")", "\n", "atexit", ".", "register", "(", "io", ".", "close", ")", "\n", "return", "io", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._find_caller": [[119, 134], ["sys._getframe", "os.path.join"], "function", ["None"], ["def", "_find_caller", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns:\n        str: module name of the caller\n        tuple: a hashable key to be used to identify different callers\n    \"\"\"", "\n", "frame", "=", "sys", ".", "_getframe", "(", "2", ")", "\n", "while", "frame", ":", "\n", "        ", "code", "=", "frame", ".", "f_code", "\n", "if", "os", ".", "path", ".", "join", "(", "\"utils\"", ",", "\"logger.\"", ")", "not", "in", "code", ".", "co_filename", ":", "\n", "            ", "mod_name", "=", "frame", ".", "f_globals", "[", "\"__name__\"", "]", "\n", "if", "mod_name", "==", "\"__main__\"", ":", "\n", "                ", "mod_name", "=", "\"detectron2\"", "\n", "", "return", "mod_name", ",", "(", "code", ".", "co_filename", ",", "frame", ".", "f_lineno", ",", "code", ".", "co_name", ")", "\n", "", "frame", "=", "frame", ".", "f_back", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.log_first_n": [[140, 173], ["isinstance", "logger._find_caller", "len", "logging.getLogger().log", "logging.getLogger"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._find_caller"], ["def", "log_first_n", "(", "lvl", ",", "msg", ",", "n", "=", "1", ",", "*", ",", "name", "=", "None", ",", "key", "=", "\"caller\"", ")", ":", "\n", "    ", "\"\"\"\n    Log only for the first n times.\n\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.\n        key (str or tuple[str]): the string(s) can be one of \"caller\" or\n            \"message\", which defines how to identify duplicated logs.\n            For example, if called with `n=1, key=\"caller\"`, this function\n            will only log the first call from the same caller, regardless of\n            the message content.\n            If called with `n=1, key=\"message\"`, this function will log the\n            same content only once, even if they are called from different places.\n            If called with `n=1, key=(\"caller\", \"message\")`, this function\n            will not log only if the same caller has logged the same message before.\n    \"\"\"", "\n", "if", "isinstance", "(", "key", ",", "str", ")", ":", "\n", "        ", "key", "=", "(", "key", ",", ")", "\n", "", "assert", "len", "(", "key", ")", ">", "0", "\n", "\n", "caller_module", ",", "caller_key", "=", "_find_caller", "(", ")", "\n", "hash_key", "=", "(", ")", "\n", "if", "\"caller\"", "in", "key", ":", "\n", "        ", "hash_key", "=", "hash_key", "+", "caller_key", "\n", "", "if", "\"message\"", "in", "key", ":", "\n", "        ", "hash_key", "=", "hash_key", "+", "(", "msg", ",", ")", "\n", "\n", "", "_LOG_COUNTER", "[", "hash_key", "]", "+=", "1", "\n", "if", "_LOG_COUNTER", "[", "hash_key", "]", "<=", "n", ":", "\n", "        ", "logging", ".", "getLogger", "(", "name", "or", "caller_module", ")", ".", "log", "(", "lvl", ",", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.log_every_n": [[175, 189], ["logger._find_caller", "logging.getLogger().log", "logging.getLogger"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._find_caller"], ["", "", "def", "log_every_n", "(", "lvl", ",", "msg", ",", "n", "=", "1", ",", "*", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Log once per n times.\n\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.\n    \"\"\"", "\n", "caller_module", ",", "key", "=", "_find_caller", "(", ")", "\n", "_LOG_COUNTER", "[", "key", "]", "+=", "1", "\n", "if", "n", "==", "1", "or", "_LOG_COUNTER", "[", "key", "]", "%", "n", "==", "1", ":", "\n", "        ", "logging", ".", "getLogger", "(", "name", "or", "caller_module", ")", ".", "log", "(", "lvl", ",", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.log_every_n_seconds": [[191, 207], ["logger._find_caller", "_LOG_TIMER.get", "time.time", "logging.getLogger().log", "logging.getLogger"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._find_caller", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "log_every_n_seconds", "(", "lvl", ",", "msg", ",", "n", "=", "1", ",", "*", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Log no more than once per n seconds.\n\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.\n    \"\"\"", "\n", "caller_module", ",", "key", "=", "_find_caller", "(", ")", "\n", "last_logged", "=", "_LOG_TIMER", ".", "get", "(", "key", ",", "None", ")", "\n", "current_time", "=", "time", ".", "time", "(", ")", "\n", "if", "last_logged", "is", "None", "or", "current_time", "-", "last_logged", ">=", "n", ":", "\n", "        ", "logging", ".", "getLogger", "(", "name", "or", "caller_module", ")", ".", "log", "(", "lvl", ",", "msg", ")", "\n", "_LOG_TIMER", "[", "key", "]", "=", "current_time", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.create_small_table": [[209, 230], ["tuple", "tabulate.tabulate", "zip", "small_dict.items"], "function", ["None"], ["", "", "def", "create_small_table", "(", "small_dict", ")", ":", "\n", "    ", "\"\"\"\n    Create a small table using the keys of small_dict as headers. This is only\n    suitable for small dictionaries.\n\n    Args:\n        small_dict (dict): a result dictionary of only a few items.\n\n    Returns:\n        str: the table as a string.\n    \"\"\"", "\n", "keys", ",", "values", "=", "tuple", "(", "zip", "(", "*", "small_dict", ".", "items", "(", ")", ")", ")", "\n", "table", "=", "tabulate", "(", "\n", "[", "values", "]", ",", "\n", "headers", "=", "keys", ",", "\n", "tablefmt", "=", "\"pipe\"", ",", "\n", "floatfmt", "=", "\".3f\"", ",", "\n", "stralign", "=", "\"center\"", ",", "\n", "numalign", "=", "\"center\"", ",", "\n", ")", "\n", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._log_api_usage": [[232, 238], ["torch._C._log_api_usage_once"], "function", ["None"], ["", "def", "_log_api_usage", "(", "identifier", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    Internal function used to log the usage of different detectron2 components\n    inside facebook's infra.\n    \"\"\"", "\n", "torch", ".", "_C", ".", "_log_api_usage_once", "(", "\"detectron2.\"", "+", "identifier", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry._convert_target_to_string": [[15, 38], ["module.split", "range", "len", "registry.locate"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry.locate"], ["def", "_convert_target_to_string", "(", "t", ":", "Any", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Inverse of ``locate()``.\n\n    Args:\n        t: any object with ``__module__`` and ``__qualname__``\n    \"\"\"", "\n", "module", ",", "qualname", "=", "t", ".", "__module__", ",", "t", ".", "__qualname__", "\n", "\n", "# Compress the path to this object, e.g. ``module.submodule._impl.class``", "\n", "# may become ``module.submodule.class``, if the later also resolves to the same", "\n", "# object. This simplifies the string, and also is less affected by moving the", "\n", "# class implementation.", "\n", "module_parts", "=", "module", ".", "split", "(", "\".\"", ")", "\n", "for", "k", "in", "range", "(", "1", ",", "len", "(", "module_parts", ")", ")", ":", "\n", "        ", "prefix", "=", "\".\"", ".", "join", "(", "module_parts", "[", ":", "k", "]", ")", "\n", "candidate", "=", "f\"{prefix}.{qualname}\"", "\n", "try", ":", "\n", "            ", "if", "locate", "(", "candidate", ")", "is", "t", ":", "\n", "                ", "return", "candidate", "\n", "", "", "except", "ImportError", ":", "\n", "            ", "pass", "\n", "", "", "return", "f\"{module}.{qualname}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry.locate": [[40, 61], ["pydoc.locate", "_locate", "ImportError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry.locate"], ["", "def", "locate", "(", "name", ":", "str", ")", "->", "Any", ":", "\n", "    ", "\"\"\"\n    Locate and return an object ``x`` using an input string ``{x.__module__}.{x.__qualname__}``,\n    such as \"module.submodule.class_name\".\n\n    Raise Exception if it cannot be found.\n    \"\"\"", "\n", "obj", "=", "pydoc", ".", "locate", "(", "name", ")", "\n", "\n", "# Some cases (e.g. torch.optim.sgd.SGD) not handled correctly", "\n", "# by pydoc.locate. Try a private function from hydra.", "\n", "if", "obj", "is", "None", ":", "\n", "        ", "try", ":", "\n", "# from hydra.utils import get_method - will print many errors", "\n", "            ", "from", "hydra", ".", "utils", "import", "_locate", "\n", "", "except", "ImportError", "as", "e", ":", "\n", "            ", "raise", "ImportError", "(", "f\"Cannot dynamically locate object {name}!\"", ")", "from", "e", "\n", "", "else", ":", "\n", "            ", "obj", "=", "_locate", "(", "name", ")", "# it raises if fails", "\n", "\n", "", "", "return", "obj", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.collect_torch_env": [[17, 27], ["torch.__config__.show", "get_pretty_env_info"], "function", ["None"], ["def", "collect_torch_env", "(", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "import", "torch", ".", "__config__", "\n", "\n", "return", "torch", ".", "__config__", ".", "show", "(", ")", "\n", "", "except", "ImportError", ":", "\n", "# compatible with older versions of pytorch", "\n", "        ", "from", "torch", ".", "utils", ".", "collect_env", "import", "get_pretty_env_info", "\n", "\n", "return", "get_pretty_env_info", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.get_env_module": [[29, 32], ["os.environ.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "get_env_module", "(", ")", ":", "\n", "    ", "var_name", "=", "\"DETECTRON2_ENV_MODULE\"", "\n", "return", "var_name", ",", "os", ".", "environ", ".", "get", "(", "var_name", ",", "\"<not set>\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.detect_compute_compatibility": [[34, 53], ["os.path.join", "os.path.isfile", "subprocess.check_output", "output.decode().strip().split.decode().strip().split", "sorted", "sorted.append", "set", "output.decode().strip().split.decode().strip", "re.findall", "output.decode().strip().split.decode"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode"], ["", "def", "detect_compute_compatibility", "(", "CUDA_HOME", ",", "so_file", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "cuobjdump", "=", "os", ".", "path", ".", "join", "(", "CUDA_HOME", ",", "\"bin\"", ",", "\"cuobjdump\"", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "cuobjdump", ")", ":", "\n", "            ", "output", "=", "subprocess", ".", "check_output", "(", "\n", "\"'{}' --list-elf '{}'\"", ".", "format", "(", "cuobjdump", ",", "so_file", ")", ",", "shell", "=", "True", "\n", ")", "\n", "output", "=", "output", ".", "decode", "(", "\"utf-8\"", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "arch", "=", "[", "]", "\n", "for", "line", "in", "output", ":", "\n", "                ", "line", "=", "re", ".", "findall", "(", "r\"\\.sm_([0-9]*)\\.\"", ",", "line", ")", "[", "0", "]", "\n", "arch", ".", "append", "(", "\".\"", ".", "join", "(", "line", ")", ")", "\n", "", "arch", "=", "sorted", "(", "set", "(", "arch", ")", ")", "\n", "return", "\", \"", ".", "join", "(", "arch", ")", "\n", "", "else", ":", "\n", "            ", "return", "so_file", "+", "\"; cannot find cuobjdump\"", "\n", "", "", "except", "Exception", ":", "\n", "# unhandled failure", "\n", "        ", "return", "so_file", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.collect_env_info": [[55, 191], ["torch.cuda.is_available", "data.append", "data.append", "data.append", "data.append", "data.append", "data.append", "data.append", "data.append", "collect_env.collect_torch_env", "data.append", "data.append", "data.append", "collect_env.get_env_module", "collections.defaultdict", "range", "collections.defaultdict.items", "data.append", "data.append", "data.append", "data.append", "tabulate.tabulate", "getattr", "sys.version.replace", "data.append", "data.append", "data.append", "data.append", "torch.cuda.device_count", "devices[].append", "data.append", "data.append", "data.append", "os.environ.get", "data.append", "data.append", "data.append", "_C.get_compiler_version", "_C.get_cuda_version", "getattr", "os.path.dirname", "torch.cuda.get_device_name", "str", "data.append", "collect_env.detect_compute_compatibility", "data.append", "os.path.dirname", "os.environ.get", "subprocess.check_output", "data.append", "data.append", "collect_env.detect_compute_compatibility", "str", "os.path.dirname", "importlib.util.find_spec", "data.append", "subprocess.check_output.decode().strip().split", "os.path.join", "subprocess.check_output", "importlib.util.find_spec", "torch.cuda.get_device_capability", "os.path.isdir", "str", "os.path.isdir", "str", "str", "subprocess.check_output.decode().strip().split", "collect_env.detect_compute_compatibility", "subprocess.check_output.decode().strip", "subprocess.check_output.decode().strip", "subprocess.check_output.decode", "subprocess.check_output.decode"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.collect_torch_env", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.get_env_module", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.detect_compute_compatibility", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.detect_compute_compatibility", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.detect_compute_compatibility", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode"], ["", "", "def", "collect_env_info", "(", ")", ":", "\n", "    ", "has_gpu", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "# true for both CUDA & ROCM", "\n", "torch_version", "=", "torch", ".", "__version__", "\n", "\n", "# NOTE that CUDA_HOME/ROCM_HOME could be None even when CUDA runtime libs are functional", "\n", "from", "torch", ".", "utils", ".", "cpp_extension", "import", "CUDA_HOME", ",", "ROCM_HOME", "\n", "\n", "has_rocm", "=", "False", "\n", "if", "(", "getattr", "(", "torch", ".", "version", ",", "\"hip\"", ",", "None", ")", "is", "not", "None", ")", "and", "(", "ROCM_HOME", "is", "not", "None", ")", ":", "\n", "        ", "has_rocm", "=", "True", "\n", "", "has_cuda", "=", "has_gpu", "and", "(", "not", "has_rocm", ")", "\n", "\n", "data", "=", "[", "]", "\n", "data", ".", "append", "(", "(", "\"sys.platform\"", ",", "sys", ".", "platform", ")", ")", "# check-template.yml depends on it", "\n", "data", ".", "append", "(", "(", "\"Python\"", ",", "sys", ".", "version", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", ")", ")", "\n", "data", ".", "append", "(", "(", "\"numpy\"", ",", "np", ".", "__version__", ")", ")", "\n", "\n", "try", ":", "\n", "        ", "import", "detectron2", "# noqa", "\n", "\n", "data", ".", "append", "(", "\n", "(", "\"detectron2\"", ",", "detectron2", ".", "__version__", "+", "\" @\"", "+", "os", ".", "path", ".", "dirname", "(", "detectron2", ".", "__file__", ")", ")", "\n", ")", "\n", "", "except", "ImportError", ":", "\n", "        ", "data", ".", "append", "(", "(", "\"detectron2\"", ",", "\"failed to import\"", ")", ")", "\n", "", "except", "AttributeError", ":", "\n", "        ", "data", ".", "append", "(", "(", "\"detectron2\"", ",", "\"imported a wrong installation\"", ")", ")", "\n", "\n", "", "try", ":", "\n", "        ", "import", "detectron2", ".", "_C", "as", "_C", "\n", "", "except", "ImportError", "as", "e", ":", "\n", "        ", "data", ".", "append", "(", "(", "\"detectron2._C\"", ",", "f\"not built correctly: {e}\"", ")", ")", "\n", "\n", "# print system compilers when extension fails to build", "\n", "if", "sys", ".", "platform", "!=", "\"win32\"", ":", "# don't know what to do for windows", "\n", "            ", "try", ":", "\n", "# this is how torch/utils/cpp_extensions.py choose compiler", "\n", "                ", "cxx", "=", "os", ".", "environ", ".", "get", "(", "\"CXX\"", ",", "\"c++\"", ")", "\n", "cxx", "=", "subprocess", ".", "check_output", "(", "\"'{}' --version\"", ".", "format", "(", "cxx", ")", ",", "shell", "=", "True", ")", "\n", "cxx", "=", "cxx", ".", "decode", "(", "\"utf-8\"", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "[", "0", "]", "\n", "", "except", "subprocess", ".", "SubprocessError", ":", "\n", "                ", "cxx", "=", "\"Not found\"", "\n", "", "data", ".", "append", "(", "(", "\"Compiler ($CXX)\"", ",", "cxx", ")", ")", "\n", "\n", "if", "has_cuda", "and", "CUDA_HOME", "is", "not", "None", ":", "\n", "                ", "try", ":", "\n", "                    ", "nvcc", "=", "os", ".", "path", ".", "join", "(", "CUDA_HOME", ",", "\"bin\"", ",", "\"nvcc\"", ")", "\n", "nvcc", "=", "subprocess", ".", "check_output", "(", "\"'{}' -V\"", ".", "format", "(", "nvcc", ")", ",", "shell", "=", "True", ")", "\n", "nvcc", "=", "nvcc", ".", "decode", "(", "\"utf-8\"", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "[", "-", "1", "]", "\n", "", "except", "subprocess", ".", "SubprocessError", ":", "\n", "                    ", "nvcc", "=", "\"Not found\"", "\n", "", "data", ".", "append", "(", "(", "\"CUDA compiler\"", ",", "nvcc", ")", ")", "\n", "", "", "if", "has_cuda", "and", "sys", ".", "platform", "!=", "\"win32\"", ":", "\n", "            ", "try", ":", "\n", "                ", "so_file", "=", "importlib", ".", "util", ".", "find_spec", "(", "\"detectron2._C\"", ")", ".", "origin", "\n", "", "except", "(", "ImportError", ",", "AttributeError", ")", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "data", ".", "append", "(", "\n", "(", "\"detectron2 arch flags\"", ",", "detect_compute_compatibility", "(", "CUDA_HOME", ",", "so_file", ")", ")", "\n", ")", "\n", "", "", "", "else", ":", "\n", "# print compilers that are used to build extension", "\n", "        ", "data", ".", "append", "(", "(", "\"Compiler\"", ",", "_C", ".", "get_compiler_version", "(", ")", ")", ")", "\n", "data", ".", "append", "(", "(", "\"CUDA compiler\"", ",", "_C", ".", "get_cuda_version", "(", ")", ")", ")", "# cuda or hip", "\n", "if", "has_cuda", "and", "getattr", "(", "_C", ",", "\"has_cuda\"", ",", "lambda", ":", "True", ")", "(", ")", ":", "\n", "            ", "data", ".", "append", "(", "\n", "(", "\"detectron2 arch flags\"", ",", "detect_compute_compatibility", "(", "CUDA_HOME", ",", "_C", ".", "__file__", ")", ")", "\n", ")", "\n", "\n", "", "", "data", ".", "append", "(", "get_env_module", "(", ")", ")", "\n", "data", ".", "append", "(", "(", "\"PyTorch\"", ",", "torch_version", "+", "\" @\"", "+", "os", ".", "path", ".", "dirname", "(", "torch", ".", "__file__", ")", ")", ")", "\n", "data", ".", "append", "(", "(", "\"PyTorch debug build\"", ",", "torch", ".", "version", ".", "debug", ")", ")", "\n", "\n", "data", ".", "append", "(", "(", "\"GPU available\"", ",", "has_gpu", ")", ")", "\n", "if", "has_gpu", ":", "\n", "        ", "devices", "=", "defaultdict", "(", "list", ")", "\n", "for", "k", "in", "range", "(", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ":", "\n", "            ", "cap", "=", "\".\"", ".", "join", "(", "(", "str", "(", "x", ")", "for", "x", "in", "torch", ".", "cuda", ".", "get_device_capability", "(", "k", ")", ")", ")", "\n", "name", "=", "torch", ".", "cuda", ".", "get_device_name", "(", "k", ")", "+", "f\" (arch={cap})\"", "\n", "devices", "[", "name", "]", ".", "append", "(", "str", "(", "k", ")", ")", "\n", "", "for", "name", ",", "devids", "in", "devices", ".", "items", "(", ")", ":", "\n", "            ", "data", ".", "append", "(", "(", "\"GPU \"", "+", "\",\"", ".", "join", "(", "devids", ")", ",", "name", ")", ")", "\n", "\n", "", "if", "has_rocm", ":", "\n", "            ", "msg", "=", "\" - invalid!\"", "if", "not", "(", "ROCM_HOME", "and", "os", ".", "path", ".", "isdir", "(", "ROCM_HOME", ")", ")", "else", "\"\"", "\n", "data", ".", "append", "(", "(", "\"ROCM_HOME\"", ",", "str", "(", "ROCM_HOME", ")", "+", "msg", ")", ")", "\n", "", "else", ":", "\n", "            ", "msg", "=", "\" - invalid!\"", "if", "not", "(", "CUDA_HOME", "and", "os", ".", "path", ".", "isdir", "(", "CUDA_HOME", ")", ")", "else", "\"\"", "\n", "data", ".", "append", "(", "(", "\"CUDA_HOME\"", ",", "str", "(", "CUDA_HOME", ")", "+", "msg", ")", ")", "\n", "\n", "cuda_arch_list", "=", "os", ".", "environ", ".", "get", "(", "\"TORCH_CUDA_ARCH_LIST\"", ",", "None", ")", "\n", "if", "cuda_arch_list", ":", "\n", "                ", "data", ".", "append", "(", "(", "\"TORCH_CUDA_ARCH_LIST\"", ",", "cuda_arch_list", ")", ")", "\n", "", "", "", "data", ".", "append", "(", "(", "\"Pillow\"", ",", "PIL", ".", "__version__", ")", ")", "\n", "\n", "try", ":", "\n", "        ", "data", ".", "append", "(", "\n", "(", "\n", "\"torchvision\"", ",", "\n", "str", "(", "torchvision", ".", "__version__", ")", "+", "\" @\"", "+", "os", ".", "path", ".", "dirname", "(", "torchvision", ".", "__file__", ")", ",", "\n", ")", "\n", ")", "\n", "if", "has_cuda", ":", "\n", "            ", "try", ":", "\n", "                ", "torchvision_C", "=", "importlib", ".", "util", ".", "find_spec", "(", "\"torchvision._C\"", ")", ".", "origin", "\n", "msg", "=", "detect_compute_compatibility", "(", "CUDA_HOME", ",", "torchvision_C", ")", "\n", "data", ".", "append", "(", "(", "\"torchvision arch flags\"", ",", "msg", ")", ")", "\n", "", "except", "(", "ImportError", ",", "AttributeError", ")", ":", "\n", "                ", "data", ".", "append", "(", "(", "\"torchvision._C\"", ",", "\"Not found\"", ")", ")", "\n", "", "", "", "except", "AttributeError", ":", "\n", "        ", "data", ".", "append", "(", "(", "\"torchvision\"", ",", "\"unknown\"", ")", ")", "\n", "\n", "", "try", ":", "\n", "        ", "import", "fvcore", "\n", "\n", "data", ".", "append", "(", "(", "\"fvcore\"", ",", "fvcore", ".", "__version__", ")", ")", "\n", "", "except", "(", "ImportError", ",", "AttributeError", ")", ":", "\n", "        ", "pass", "\n", "\n", "", "try", ":", "\n", "        ", "import", "iopath", "\n", "\n", "data", ".", "append", "(", "(", "\"iopath\"", ",", "iopath", ".", "__version__", ")", ")", "\n", "", "except", "(", "ImportError", ",", "AttributeError", ")", ":", "\n", "        ", "pass", "\n", "\n", "", "try", ":", "\n", "        ", "import", "cv2", "\n", "\n", "data", ".", "append", "(", "(", "\"cv2\"", ",", "cv2", ".", "__version__", ")", ")", "\n", "", "except", "(", "ImportError", ",", "AttributeError", ")", ":", "\n", "        ", "data", ".", "append", "(", "(", "\"cv2\"", ",", "\"Not found\"", ")", ")", "\n", "", "env_str", "=", "tabulate", "(", "data", ")", "+", "\"\\n\"", "\n", "env_str", "+=", "collect_torch_env", "(", ")", "\n", "return", "env_str", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.get_model_no_weights": [[19, 27], ["detectron2.model_zoo.get_config", "detectron2.modeling.build_model", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model"], ["def", "get_model_no_weights", "(", "config_path", ")", ":", "\n", "    ", "\"\"\"\n    Like model_zoo.get, but do not load any weights (even pretrained)\n    \"\"\"", "\n", "cfg", "=", "model_zoo", ".", "get_config", "(", "config_path", ")", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "cfg", ".", "MODEL", ".", "DEVICE", "=", "\"cpu\"", "\n", "", "return", "build_model", "(", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes": [[29, 41], ["boxes.clamp_", "torch.rand"], "function", ["None"], ["", "def", "random_boxes", "(", "num_boxes", ",", "max_coord", "=", "100", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "    ", "\"\"\"\n    Create a random Nx4 boxes tensor, with coordinates < max_coord.\n    \"\"\"", "\n", "boxes", "=", "torch", ".", "rand", "(", "num_boxes", ",", "4", ",", "device", "=", "device", ")", "*", "(", "max_coord", "*", "0.5", ")", "\n", "boxes", ".", "clamp_", "(", "min", "=", "1.0", ")", "# tiny boxes cause numerical instability in box regression", "\n", "# Note: the implementation of this function in torchvision is:", "\n", "# boxes[:, 2:] += torch.rand(N, 2) * 100", "\n", "# but it does not guarantee non-negative widths/heights constraints:", "\n", "# boxes[:, 2] >= boxes[:, 0] and boxes[:, 3] >= boxes[:, 1]:", "\n", "boxes", "[", ":", ",", "2", ":", "]", "+=", "boxes", "[", ":", ",", ":", "2", "]", "\n", "return", "boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.get_sample_coco_image": [[43, 63], ["detectron2.data.detection_utils.read_image", "torch.from_numpy", "detectron2.utils.file_io.PathManager.exists", "FileNotFoundError", "numpy.ascontiguousarray", "detectron2.data.DatasetCatalog.get", "torch.from_numpy.transpose"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.read_image", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "get_sample_coco_image", "(", "tensor", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        tensor (bool): if True, returns 3xHxW tensor.\n            else, returns a HxWx3 numpy array.\n\n    Returns:\n        an image, in BGR color.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "file_name", "=", "DatasetCatalog", ".", "get", "(", "\"coco_2017_val_100\"", ")", "[", "0", "]", "[", "\"file_name\"", "]", "\n", "if", "not", "PathManager", ".", "exists", "(", "file_name", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", ")", "\n", "", "", "except", "IOError", ":", "\n", "# for public CI to run", "\n", "        ", "file_name", "=", "\"http://images.cocodataset.org/train2017/000000000009.jpg\"", "\n", "", "ret", "=", "read_image", "(", "file_name", ",", "format", "=", "\"BGR\"", ")", "\n", "if", "tensor", ":", "\n", "        ", "ret", "=", "torch", ".", "from_numpy", "(", "np", ".", "ascontiguousarray", "(", "ret", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ")", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.convert_scripted_instances": [[65, 75], ["detectron2.structures.Instances", "getattr", "detectron2.structures.Instances.set"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "convert_scripted_instances", "(", "instances", ")", ":", "\n", "    ", "\"\"\"\n    Convert a scripted Instances object to a regular :class:`Instances` object\n    \"\"\"", "\n", "ret", "=", "Instances", "(", "instances", ".", "image_size", ")", "\n", "for", "name", "in", "instances", ".", "_field_names", ":", "\n", "        ", "val", "=", "getattr", "(", "instances", ",", "\"_\"", "+", "name", ",", "None", ")", "\n", "if", "val", "is", "not", "None", ":", "\n", "            ", "ret", ".", "set", "(", "name", ",", "val", ")", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.assert_instances_allclose": [[77, 122], ["sorted", "sorted", "isinstance", "testing.convert_scripted_instances", "isinstance", "testing.convert_scripted_instances", "torch.equal", "convert_scripted_instances.get_fields().keys", "convert_scripted_instances.get_fields().keys", "isinstance", "msg.rstrip", "torch.tensor", "torch.tensor", "convert_scripted_instances.get", "convert_scripted_instances.get", "torch.allclose", "isinstance", "convert_scripted_instances.get_fields", "convert_scripted_instances.get_fields", "ValueError", "torch.abs().max().cpu().item", "torch.allclose", "torch.equal", "torch.abs().max().cpu", "type", "torch.abs().max", "torch.abs"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.convert_scripted_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.convert_scripted_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields"], ["", "def", "assert_instances_allclose", "(", "input", ",", "other", ",", "*", ",", "rtol", "=", "1e-5", ",", "msg", "=", "\"\"", ",", "size_as_tensor", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        input, other (Instances):\n        size_as_tensor: compare image_size of the Instances as tensors (instead of tuples).\n             Useful for comparing outputs of tracing.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "input", ",", "Instances", ")", ":", "\n", "        ", "input", "=", "convert_scripted_instances", "(", "input", ")", "\n", "", "if", "not", "isinstance", "(", "other", ",", "Instances", ")", ":", "\n", "        ", "other", "=", "convert_scripted_instances", "(", "other", ")", "\n", "\n", "", "if", "not", "msg", ":", "\n", "        ", "msg", "=", "\"Two Instances are different! \"", "\n", "", "else", ":", "\n", "        ", "msg", "=", "msg", ".", "rstrip", "(", ")", "+", "\" \"", "\n", "\n", "", "size_error_msg", "=", "msg", "+", "f\"image_size is {input.image_size} vs. {other.image_size}!\"", "\n", "if", "size_as_tensor", ":", "\n", "        ", "assert", "torch", ".", "equal", "(", "\n", "torch", ".", "tensor", "(", "input", ".", "image_size", ")", ",", "torch", ".", "tensor", "(", "other", ".", "image_size", ")", "\n", ")", ",", "size_error_msg", "\n", "", "else", ":", "\n", "        ", "assert", "input", ".", "image_size", "==", "other", ".", "image_size", ",", "size_error_msg", "\n", "", "fields", "=", "sorted", "(", "input", ".", "get_fields", "(", ")", ".", "keys", "(", ")", ")", "\n", "fields_other", "=", "sorted", "(", "other", ".", "get_fields", "(", ")", ".", "keys", "(", ")", ")", "\n", "assert", "fields", "==", "fields_other", ",", "msg", "+", "f\"Fields are {fields} vs {fields_other}!\"", "\n", "\n", "for", "f", "in", "fields", ":", "\n", "        ", "val1", ",", "val2", "=", "input", ".", "get", "(", "f", ")", ",", "other", ".", "get", "(", "f", ")", "\n", "if", "isinstance", "(", "val1", ",", "(", "Boxes", ",", "ROIMasks", ")", ")", ":", "\n", "# boxes in the range of O(100) and can have a larger tolerance", "\n", "            ", "assert", "torch", ".", "allclose", "(", "val1", ".", "tensor", ",", "val2", ".", "tensor", ",", "atol", "=", "100", "*", "rtol", ")", ",", "(", "\n", "msg", "+", "f\"Field {f} differs too much!\"", "\n", ")", "\n", "", "elif", "isinstance", "(", "val1", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "if", "val1", ".", "dtype", ".", "is_floating_point", ":", "\n", "                ", "mag", "=", "torch", ".", "abs", "(", "val1", ")", ".", "max", "(", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "assert", "torch", ".", "allclose", "(", "val1", ",", "val2", ",", "atol", "=", "mag", "*", "rtol", ")", ",", "(", "\n", "msg", "+", "f\"Field {f} differs too much!\"", "\n", ")", "\n", "", "else", ":", "\n", "                ", "assert", "torch", ".", "equal", "(", "val1", ",", "val2", ")", ",", "msg", "+", "f\"Field {f} is different!\"", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Don't know how to compare type {type(val1)}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.reload_script_model": [[124, 133], ["io.BytesIO", "torch.jit.save", "io.BytesIO.seek", "torch.jit.load"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], ["", "", "", "def", "reload_script_model", "(", "module", ")", ":", "\n", "    ", "\"\"\"\n    Save a jit module and load it back.\n    Similar to the `getExportImportCopy` function in torch/testing/\n    \"\"\"", "\n", "buffer", "=", "io", ".", "BytesIO", "(", ")", "\n", "torch", ".", "jit", ".", "save", "(", "module", ",", "buffer", ")", "\n", "buffer", ".", "seek", "(", "0", ")", "\n", "return", "torch", ".", "jit", ".", "load", "(", "buffer", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.file_io.Detectron2Handler._get_supported_prefixes": [[24, 26], ["None"], "methods", ["None"], ["def", "_get_supported_prefixes", "(", "self", ")", ":", "\n", "        ", "return", "[", "self", ".", "PREFIX", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.file_io.Detectron2Handler._get_local_path": [[27, 30], ["iopath.common.file_io.PathManager.get_local_path", "len"], "methods", ["None"], ["", "def", "_get_local_path", "(", "self", ",", "path", ",", "**", "kwargs", ")", ":", "\n", "        ", "name", "=", "path", "[", "len", "(", "self", ".", "PREFIX", ")", ":", "]", "\n", "return", "PathManager", ".", "get_local_path", "(", "self", ".", "S3_DETECTRON2_PREFIX", "+", "name", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.file_io.Detectron2Handler._open": [[31, 33], ["iopath.common.file_io.PathManager.open", "file_io.Detectron2Handler._get_local_path"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalogHandler._get_local_path"], ["", "def", "_open", "(", "self", ",", "path", ",", "mode", "=", "\"r\"", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "PathManager", ".", "open", "(", "self", ".", "_get_local_path", "(", "path", ")", ",", "mode", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.seed_all_rng": [[27, 46], ["numpy.random.seed", "torch.manual_seed", "random.seed", "str", "logging.getLogger", "logging.getLogger.info", "int.from_bytes", "os.getpid", "int", "os.urandom", "datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["None"], ["def", "seed_all_rng", "(", "seed", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Set the random seed for the RNG in torch, numpy and python.\n\n    Args:\n        seed (int): if None, will use a strong random seed.\n    \"\"\"", "\n", "if", "seed", "is", "None", ":", "\n", "        ", "seed", "=", "(", "\n", "os", ".", "getpid", "(", ")", "\n", "+", "int", "(", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%S%f\"", ")", ")", "\n", "+", "int", ".", "from_bytes", "(", "os", ".", "urandom", "(", "2", ")", ",", "\"big\"", ")", "\n", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"Using a generated random seed {}\"", ".", "format", "(", "seed", ")", ")", "\n", "", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "os", ".", "environ", "[", "\"PYTHONHASHSEED\"", "]", "=", "str", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env._import_file": [[49, 56], ["importlib.util.spec_from_file_location", "importlib.util.spec_from_file_location", "importlib.util.module_from_spec", "importlib.util.module_from_spec", "importlib.util.spec_from_file_location.loader.exec_module"], "function", ["None"], ["", "def", "_import_file", "(", "module_name", ",", "file_path", ",", "make_importable", "=", "False", ")", ":", "\n", "    ", "spec", "=", "importlib", ".", "util", ".", "spec_from_file_location", "(", "module_name", ",", "file_path", ")", "\n", "module", "=", "importlib", ".", "util", ".", "module_from_spec", "(", "spec", ")", "\n", "spec", ".", "loader", ".", "exec_module", "(", "module", ")", "\n", "if", "make_importable", ":", "\n", "        ", "sys", ".", "modules", "[", "module_name", "]", "=", "module", "\n", "", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env._configure_libraries": [[58, 91], ["int", "os.environ.get", "tuple", "env._configure_libraries.get_version"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.None.setup.get_version"], ["", "def", "_configure_libraries", "(", ")", ":", "\n", "    ", "\"\"\"\n    Configurations for some libraries.\n    \"\"\"", "\n", "# An environment option to disable `import cv2` globally,", "\n", "# in case it leads to negative performance impact", "\n", "disable_cv2", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "\"DETECTRON2_DISABLE_CV2\"", ",", "False", ")", ")", "\n", "if", "disable_cv2", ":", "\n", "        ", "sys", ".", "modules", "[", "\"cv2\"", "]", "=", "None", "\n", "", "else", ":", "\n", "# Disable opencl in opencv since its interaction with cuda often has negative effects", "\n", "# This envvar is supported after OpenCV 3.4.0", "\n", "        ", "os", ".", "environ", "[", "\"OPENCV_OPENCL_RUNTIME\"", "]", "=", "\"disabled\"", "\n", "try", ":", "\n", "            ", "import", "cv2", "\n", "\n", "if", "int", "(", "cv2", ".", "__version__", ".", "split", "(", "\".\"", ")", "[", "0", "]", ")", ">=", "3", ":", "\n", "                ", "cv2", ".", "ocl", ".", "setUseOpenCL", "(", "False", ")", "\n", "", "", "except", "ModuleNotFoundError", ":", "\n", "# Other types of ImportError, if happened, should not be ignored.", "\n", "# Because a failed opencv import could mess up address space", "\n", "# https://github.com/skvark/opencv-python/issues/381", "\n", "            ", "pass", "\n", "\n", "", "", "def", "get_version", "(", "module", ",", "digit", "=", "2", ")", ":", "\n", "        ", "return", "tuple", "(", "map", "(", "int", ",", "module", ".", "__version__", ".", "split", "(", "\".\"", ")", "[", ":", "digit", "]", ")", ")", "\n", "\n", "# fmt: off", "\n", "", "assert", "get_version", "(", "torch", ")", ">=", "(", "1", ",", "4", ")", ",", "\"Requires torch>=1.4\"", "\n", "import", "fvcore", "\n", "assert", "get_version", "(", "fvcore", ",", "3", ")", ">=", "(", "0", ",", "1", ",", "2", ")", ",", "\"Requires fvcore>=0.1.2\"", "\n", "import", "yaml", "\n", "assert", "get_version", "(", "yaml", ")", ">=", "(", "5", ",", "1", ")", ",", "\"Requires pyyaml>=5.1\"", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.setup_environment": [[97, 117], ["env._configure_libraries", "os.environ.get", "env.setup_custom_environment"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.env._configure_libraries", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.setup_custom_environment"], ["def", "setup_environment", "(", ")", ":", "\n", "    ", "\"\"\"Perform environment setup work. The default setup is a no-op, but this\n    function allows the user to specify a Python source file or a module in\n    the $DETECTRON2_ENV_MODULE environment variable, that performs\n    custom setup work that may be necessary to their computing environment.\n    \"\"\"", "\n", "global", "_ENV_SETUP_DONE", "\n", "if", "_ENV_SETUP_DONE", ":", "\n", "        ", "return", "\n", "", "_ENV_SETUP_DONE", "=", "True", "\n", "\n", "_configure_libraries", "(", ")", "\n", "\n", "custom_module_path", "=", "os", ".", "environ", ".", "get", "(", "\"DETECTRON2_ENV_MODULE\"", ")", "\n", "\n", "if", "custom_module_path", ":", "\n", "        ", "setup_custom_environment", "(", "custom_module_path", ")", "\n", "", "else", ":", "\n", "# The default setup is a no-op", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.setup_custom_environment": [[119, 133], ["custom_module.endswith", "importlib.import_module.setup_environment", "env._import_file", "importlib.import_module", "importlib.import_module", "hasattr", "callable"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.setup_environment", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env._import_file"], ["", "", "def", "setup_custom_environment", "(", "custom_module", ")", ":", "\n", "    ", "\"\"\"\n    Load custom environment setup by importing a Python source file or a\n    module, and run the setup function.\n    \"\"\"", "\n", "if", "custom_module", ".", "endswith", "(", "\".py\"", ")", ":", "\n", "        ", "module", "=", "_import_file", "(", "\"detectron2.utils.env.custom_module\"", ",", "custom_module", ")", "\n", "", "else", ":", "\n", "        ", "module", "=", "importlib", ".", "import_module", "(", "custom_module", ")", "\n", "", "assert", "hasattr", "(", "module", ",", "\"setup_environment\"", ")", "and", "callable", "(", "module", ".", "setup_environment", ")", ",", "(", "\n", "\"Custom environment module defined in {} does not have the \"", "\n", "\"required callable attribute 'setup_environment'.\"", "\n", ")", ".", "format", "(", "custom_module", ")", "\n", "module", ".", "setup_environment", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.fixup_module_metadata": [[135, 171], ["set", "set.add", "getattr", "namespace.keys", "id", "id", "isinstance", "objname.startswith", "env.fixup_module_metadata.fix_one"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "fixup_module_metadata", "(", "module_name", ",", "namespace", ",", "keys", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Fix the __qualname__ of module members to be their exported api name, so\n    when they are referenced in docs, sphinx can find them. Reference:\n    https://github.com/python-trio/trio/blob/6754c74eacfad9cc5c92d5c24727a2f3b620624e/trio/_util.py#L216-L241\n    \"\"\"", "\n", "if", "not", "DOC_BUILDING", ":", "\n", "        ", "return", "\n", "", "seen_ids", "=", "set", "(", ")", "\n", "\n", "def", "fix_one", "(", "qualname", ",", "name", ",", "obj", ")", ":", "\n", "# avoid infinite recursion (relevant when using", "\n", "# typing.Generic, for example)", "\n", "        ", "if", "id", "(", "obj", ")", "in", "seen_ids", ":", "\n", "            ", "return", "\n", "", "seen_ids", ".", "add", "(", "id", "(", "obj", ")", ")", "\n", "\n", "mod", "=", "getattr", "(", "obj", ",", "\"__module__\"", ",", "None", ")", "\n", "if", "mod", "is", "not", "None", "and", "(", "mod", ".", "startswith", "(", "module_name", ")", "or", "mod", ".", "startswith", "(", "\"fvcore.\"", ")", ")", ":", "\n", "            ", "obj", ".", "__module__", "=", "module_name", "\n", "# Modules, unlike everything else in Python, put fully-qualitied", "\n", "# names into their __name__ attribute. We check for \".\" to avoid", "\n", "# rewriting these.", "\n", "if", "hasattr", "(", "obj", ",", "\"__name__\"", ")", "and", "\".\"", "not", "in", "obj", ".", "__name__", ":", "\n", "                ", "obj", ".", "__name__", "=", "name", "\n", "obj", ".", "__qualname__", "=", "qualname", "\n", "", "if", "isinstance", "(", "obj", ",", "type", ")", ":", "\n", "                ", "for", "attr_name", ",", "attr_value", "in", "obj", ".", "__dict__", ".", "items", "(", ")", ":", "\n", "                    ", "fix_one", "(", "objname", "+", "\".\"", "+", "attr_name", ",", "attr_name", ",", "attr_value", ")", "\n", "\n", "", "", "", "", "if", "keys", "is", "None", ":", "\n", "        ", "keys", "=", "namespace", ".", "keys", "(", ")", "\n", "", "for", "objname", "in", "keys", ":", "\n", "        ", "if", "not", "objname", ".", "startswith", "(", "\"_\"", ")", ":", "\n", "            ", "obj", "=", "namespace", "[", "objname", "]", "\n", "fix_one", "(", "objname", ",", "objname", ",", "obj", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.memory._ignore_torch_cuda_oom": [[11, 24], ["str"], "function", ["None"], ["@", "contextmanager", "\n", "def", "_ignore_torch_cuda_oom", "(", ")", ":", "\n", "    ", "\"\"\"\n    A context which ignores CUDA OOM exception from pytorch.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "yield", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "# NOTE: the string may change?", "\n", "        ", "if", "\"CUDA out of memory. \"", "in", "str", "(", "e", ")", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "raise", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.utils.memory.retry_if_cuda_oom": [[26, 85], ["functools.wraps", "torch.cuda.empty_cache", "logging.getLogger", "logging.getLogger.info", "func", "x.to", "memory._ignore_torch_cuda_oom", "func", "memory._ignore_torch_cuda_oom", "func", "memory.retry_if_cuda_oom.maybe_to_cpu"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.memory._ignore_torch_cuda_oom", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.memory._ignore_torch_cuda_oom"], ["", "", "", "def", "retry_if_cuda_oom", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Makes a function retry itself after encountering\n    pytorch's CUDA OOM error.\n    It will first retry after calling `torch.cuda.empty_cache()`.\n\n    If that still fails, it will then retry by trying to convert inputs to CPUs.\n    In this case, it expects the function to dispatch to CPU implementation.\n    The return values may become CPU tensors as well and it's user's\n    responsibility to convert it back to CUDA tensor if needed.\n\n    Args:\n        func: a stateless callable that takes tensor-like objects as arguments\n\n    Returns:\n        a callable which retries `func` if OOM is encountered.\n\n    Examples:\n    ::\n        output = retry_if_cuda_oom(some_torch_function)(input1, input2)\n        # output may be on CPU even if inputs are on GPU\n\n    Note:\n        1. When converting inputs to CPU, it will only look at each argument and check\n           if it has `.device` and `.to` for conversion. Nested structures of tensors\n           are not supported.\n\n        2. Since the function might be called more than once, it has to be\n           stateless.\n    \"\"\"", "\n", "\n", "def", "maybe_to_cpu", "(", "x", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "like_gpu_tensor", "=", "x", ".", "device", ".", "type", "==", "\"cuda\"", "and", "hasattr", "(", "x", ",", "\"to\"", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "like_gpu_tensor", "=", "False", "\n", "", "if", "like_gpu_tensor", ":", "\n", "            ", "return", "x", ".", "to", "(", "device", "=", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n", "", "", "@", "wraps", "(", "func", ")", "\n", "def", "wrapped", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "_ignore_torch_cuda_oom", "(", ")", ":", "\n", "            ", "return", "func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "# Clear cache and retry", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "with", "_ignore_torch_cuda_oom", "(", ")", ":", "\n", "            ", "return", "func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "# Try on CPU. This slows down the code significantly, therefore print a notice.", "\n", "", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"Attempting to copy inputs of {} to CPU due to CUDA OOM\"", ".", "format", "(", "str", "(", "func", ")", ")", ")", "\n", "new_args", "=", "(", "maybe_to_cpu", "(", "x", ")", "for", "x", "in", "args", ")", "\n", "new_kwargs", "=", "{", "k", ":", "maybe_to_cpu", "(", "v", ")", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", "}", "\n", "return", "func", "(", "*", "new_args", ",", "**", "new_kwargs", ")", "\n", "\n", "", "return", "wrapped", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess": [[9, 76], ["isinstance", "detectron2.structures.Instances", "detectron2.structures.Instances.has", "output_boxes.scale", "output_boxes.clip", "detectron2.structures.Instances.has", "detectron2.structures.Instances.has", "output_width.float", "output_height.float", "torch.stack", "detectron2.structures.Instances.has", "isinstance", "detectron2.structures.Instances.get_fields", "output_boxes.nonempty", "detectron2.structures.ROIMasks", "detectron2.structures.ROIMasks.to_bitmasks"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.scale", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.nonempty", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.ROIMasks.to_bitmasks"], ["def", "detector_postprocess", "(", "\n", "results", ":", "Instances", ",", "output_height", ":", "int", ",", "output_width", ":", "int", ",", "mask_threshold", ":", "float", "=", "0.5", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Resize the output instances.\n    The input images are often resized when entering an object detector.\n    As a result, we often need the outputs of the detector in a different\n    resolution from its inputs.\n\n    This function will resize the raw outputs of an R-CNN detector\n    to produce outputs according to the desired output resolution.\n\n    Args:\n        results (Instances): the raw outputs from the detector.\n            `results.image_size` contains the input image resolution the detector sees.\n            This object might be modified in-place.\n        output_height, output_width: the desired output resolution.\n\n    Returns:\n        Instances: the resized output from the model, based on the output resolution\n    \"\"\"", "\n", "# Change to 'if is_tracing' after PT1.7", "\n", "if", "isinstance", "(", "output_height", ",", "torch", ".", "Tensor", ")", ":", "\n", "# Converts integer tensors to float temporaries to ensure true", "\n", "# division is performed when computing scale_x and scale_y.", "\n", "        ", "output_width_tmp", "=", "output_width", ".", "float", "(", ")", "\n", "output_height_tmp", "=", "output_height", ".", "float", "(", ")", "\n", "new_size", "=", "torch", ".", "stack", "(", "[", "output_height", ",", "output_width", "]", ")", "\n", "", "else", ":", "\n", "        ", "new_size", "=", "(", "output_height", ",", "output_width", ")", "\n", "output_width_tmp", "=", "output_width", "\n", "output_height_tmp", "=", "output_height", "\n", "\n", "", "scale_x", ",", "scale_y", "=", "(", "\n", "output_width_tmp", "/", "results", ".", "image_size", "[", "1", "]", ",", "\n", "output_height_tmp", "/", "results", ".", "image_size", "[", "0", "]", ",", "\n", ")", "\n", "results", "=", "Instances", "(", "new_size", ",", "**", "results", ".", "get_fields", "(", ")", ")", "\n", "\n", "if", "results", ".", "has", "(", "\"pred_boxes\"", ")", ":", "\n", "        ", "output_boxes", "=", "results", ".", "pred_boxes", "\n", "", "elif", "results", ".", "has", "(", "\"proposal_boxes\"", ")", ":", "\n", "        ", "output_boxes", "=", "results", ".", "proposal_boxes", "\n", "", "else", ":", "\n", "        ", "output_boxes", "=", "None", "\n", "", "assert", "output_boxes", "is", "not", "None", ",", "\"Predictions must contain boxes!\"", "\n", "\n", "output_boxes", ".", "scale", "(", "scale_x", ",", "scale_y", ")", "\n", "output_boxes", ".", "clip", "(", "results", ".", "image_size", ")", "\n", "\n", "results", "=", "results", "[", "output_boxes", ".", "nonempty", "(", ")", "]", "\n", "\n", "if", "results", ".", "has", "(", "\"pred_masks\"", ")", ":", "\n", "        ", "if", "isinstance", "(", "results", ".", "pred_masks", ",", "ROIMasks", ")", ":", "\n", "            ", "roi_masks", "=", "results", ".", "pred_masks", "\n", "", "else", ":", "\n", "# pred_masks is a tensor of shape (N, 1, M, M)", "\n", "            ", "roi_masks", "=", "ROIMasks", "(", "results", ".", "pred_masks", "[", ":", ",", "0", ",", ":", ",", ":", "]", ")", "\n", "", "results", ".", "pred_masks", "=", "roi_masks", ".", "to_bitmasks", "(", "\n", "results", ".", "pred_boxes", ",", "output_height", ",", "output_width", ",", "mask_threshold", "\n", ")", ".", "tensor", "# TODO return ROIMasks/BitMask object in the future", "\n", "\n", "", "if", "results", ".", "has", "(", "\"pred_keypoints\"", ")", ":", "\n", "        ", "results", ".", "pred_keypoints", "[", ":", ",", ":", ",", "0", "]", "*=", "scale_x", "\n", "results", ".", "pred_keypoints", "[", ":", ",", ":", ",", "1", "]", "*=", "scale_y", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.sem_seg_postprocess": [[78, 102], ["result[].expand", "torch.nn.functional.interpolate"], "function", ["None"], ["", "def", "sem_seg_postprocess", "(", "result", ",", "img_size", ",", "output_height", ",", "output_width", ")", ":", "\n", "    ", "\"\"\"\n    Return semantic segmentation predictions in the original resolution.\n\n    The input images are often resized when entering semantic segmentor. Moreover, in same\n    cases, they also padded inside segmentor to be divisible by maximum network stride.\n    As a result, we often need the predictions of the segmentor in a different\n    resolution from its inputs.\n\n    Args:\n        result (Tensor): semantic segmentation prediction logits. A tensor of shape (C, H, W),\n            where C is the number of classes, and H, W are the height and width of the prediction.\n        img_size (tuple): image size that segmentor is taking as input.\n        output_height, output_width: the desired output resolution.\n\n    Returns:\n        semantic segmentation prediction (Tensor): A tensor of the shape\n            (C, output_height, output_width) that contains per-pixel soft predictions.\n    \"\"\"", "\n", "result", "=", "result", "[", ":", ",", ":", "img_size", "[", "0", "]", ",", ":", "img_size", "[", "1", "]", "]", ".", "expand", "(", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "result", "=", "F", ".", "interpolate", "(", "\n", "result", ",", "size", "=", "(", "output_height", ",", "output_width", ")", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", "\n", ")", "[", "0", "]", "\n", "return", "result", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.matcher.Matcher.__init__": [[24, 60], ["thresholds.insert", "thresholds.append", "all", "all", "float", "len", "float", "len", "zip"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "thresholds", ":", "List", "[", "float", "]", ",", "labels", ":", "List", "[", "int", "]", ",", "allow_low_quality_matches", ":", "bool", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            thresholds (list): a list of thresholds used to stratify predictions\n                into levels.\n            labels (list): a list of values to label predictions belonging at\n                each level. A label can be one of {-1, 0, 1} signifying\n                {ignore, negative class, positive class}, respectively.\n            allow_low_quality_matches (bool): if True, produce additional matches\n                for predictions with maximum match quality lower than high_threshold.\n                See set_low_quality_matches_ for more details.\n\n            For example,\n                thresholds = [0.3, 0.5]\n                labels = [0, -1, 1]\n                All predictions with iou < 0.3 will be marked with 0 and\n                thus will be considered as false positives while training.\n                All predictions with 0.3 <= iou < 0.5 will be marked with -1 and\n                thus will be ignored.\n                All predictions with 0.5 <= iou will be marked with 1 and\n                thus will be considered as true positives.\n        \"\"\"", "\n", "# Add -inf and +inf to first and last position in thresholds", "\n", "thresholds", "=", "thresholds", "[", ":", "]", "\n", "assert", "thresholds", "[", "0", "]", ">", "0", "\n", "thresholds", ".", "insert", "(", "0", ",", "-", "float", "(", "\"inf\"", ")", ")", "\n", "thresholds", ".", "append", "(", "float", "(", "\"inf\"", ")", ")", "\n", "# Currently torchscript does not support all + generator", "\n", "assert", "all", "(", "[", "low", "<=", "high", "for", "(", "low", ",", "high", ")", "in", "zip", "(", "thresholds", "[", ":", "-", "1", "]", ",", "thresholds", "[", "1", ":", "]", ")", "]", ")", "\n", "assert", "all", "(", "[", "l", "in", "[", "-", "1", ",", "0", ",", "1", "]", "for", "l", "in", "labels", "]", ")", "\n", "assert", "len", "(", "labels", ")", "==", "len", "(", "thresholds", ")", "-", "1", "\n", "self", ".", "thresholds", "=", "thresholds", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "allow_low_quality_matches", "=", "allow_low_quality_matches", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.matcher.Matcher.__call__": [[61, 104], ["torch.all", "match_quality_matrix.max", "matches.new_full", "zip", "match_quality_matrix.dim", "match_quality_matrix.numel", "match_quality_matrix.new_full", "match_quality_matrix.new_full", "matches.size", "matcher.Matcher.set_low_quality_matches_", "match_quality_matrix.size", "match_quality_matrix.size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.matcher.Matcher.set_low_quality_matches_"], ["", "def", "__call__", "(", "self", ",", "match_quality_matrix", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            match_quality_matrix (Tensor[float]): an MxN tensor, containing the\n                pairwise quality between M ground-truth elements and N predicted\n                elements. All elements must be >= 0 (due to the us of `torch.nonzero`\n                for selecting indices in :meth:`set_low_quality_matches_`).\n\n        Returns:\n            matches (Tensor[int64]): a vector of length N, where matches[i] is a matched\n                ground-truth index in [0, M)\n            match_labels (Tensor[int8]): a vector of length N, where pred_labels[i] indicates\n                whether a prediction is a true or false positive or ignored\n        \"\"\"", "\n", "assert", "match_quality_matrix", ".", "dim", "(", ")", "==", "2", "\n", "if", "match_quality_matrix", ".", "numel", "(", ")", "==", "0", ":", "\n", "            ", "default_matches", "=", "match_quality_matrix", ".", "new_full", "(", "\n", "(", "match_quality_matrix", ".", "size", "(", "1", ")", ",", ")", ",", "0", ",", "dtype", "=", "torch", ".", "int64", "\n", ")", "\n", "# When no gt boxes exist, we define IOU = 0 and therefore set labels", "\n", "# to `self.labels[0]`, which usually defaults to background class 0", "\n", "# To choose to ignore instead, can make labels=[-1,0,-1,1] + set appropriate thresholds", "\n", "default_match_labels", "=", "match_quality_matrix", ".", "new_full", "(", "\n", "(", "match_quality_matrix", ".", "size", "(", "1", ")", ",", ")", ",", "self", ".", "labels", "[", "0", "]", ",", "dtype", "=", "torch", ".", "int8", "\n", ")", "\n", "return", "default_matches", ",", "default_match_labels", "\n", "\n", "", "assert", "torch", ".", "all", "(", "match_quality_matrix", ">=", "0", ")", "\n", "\n", "# match_quality_matrix is M (gt) x N (predicted)", "\n", "# Max over gt elements (dim 0) to find best gt candidate for each prediction", "\n", "matched_vals", ",", "matches", "=", "match_quality_matrix", ".", "max", "(", "dim", "=", "0", ")", "\n", "\n", "match_labels", "=", "matches", ".", "new_full", "(", "matches", ".", "size", "(", ")", ",", "1", ",", "dtype", "=", "torch", ".", "int8", ")", "\n", "\n", "for", "(", "l", ",", "low", ",", "high", ")", "in", "zip", "(", "self", ".", "labels", ",", "self", ".", "thresholds", "[", ":", "-", "1", "]", ",", "self", ".", "thresholds", "[", "1", ":", "]", ")", ":", "\n", "            ", "low_high", "=", "(", "matched_vals", ">=", "low", ")", "&", "(", "matched_vals", "<", "high", ")", "\n", "match_labels", "[", "low_high", "]", "=", "l", "\n", "\n", "", "if", "self", ".", "allow_low_quality_matches", ":", "\n", "            ", "self", ".", "set_low_quality_matches_", "(", "match_labels", ",", "match_quality_matrix", ")", "\n", "\n", "", "return", "matches", ",", "match_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.matcher.Matcher.set_low_quality_matches_": [[105, 127], ["match_quality_matrix.max", "detectron2.layers.nonzero_tuple"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple"], ["", "def", "set_low_quality_matches_", "(", "self", ",", "match_labels", ",", "match_quality_matrix", ")", ":", "\n", "        ", "\"\"\"\n        Produce additional matches for predictions that have only low-quality matches.\n        Specifically, for each ground-truth G find the set of predictions that have\n        maximum overlap with it (including ties); for each prediction in that set, if\n        it is unmatched, then match it to the ground-truth G.\n\n        This function implements the RPN assignment case (i) in Sec. 3.1.2 of\n        :paper:`Faster R-CNN`.\n        \"\"\"", "\n", "# For each gt, find the prediction with which it has highest quality", "\n", "highest_quality_foreach_gt", ",", "_", "=", "match_quality_matrix", ".", "max", "(", "dim", "=", "1", ")", "\n", "# Find the highest quality match available, even if it is low, including ties.", "\n", "# Note that the matches qualities must be positive due to the use of", "\n", "# `torch.nonzero`.", "\n", "_", ",", "pred_inds_with_highest_quality", "=", "nonzero_tuple", "(", "\n", "match_quality_matrix", "==", "highest_quality_foreach_gt", "[", ":", ",", "None", "]", "\n", ")", "\n", "# If an anchor was labeled positive only due to a low-quality match", "\n", "# with gt_A, but it has larger overlap with gt_B, it's matched index will still be gt_B.", "\n", "# This follows the implementation in Detectron, and is found to have no significant impact.", "\n", "match_labels", "[", "pred_inds_with_highest_quality", "]", "=", "1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.BufferList.__init__": [[26, 31], ["torch.nn.Module.__init__", "enumerate", "anchor_generator.BufferList.register_buffer", "str"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "buffers", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "for", "i", ",", "buffer", "in", "enumerate", "(", "buffers", ")", ":", "\n", "# Use non-persistent buffer so the values are not saved in checkpoint", "\n", "            ", "self", ".", "register_buffer", "(", "str", "(", "i", ")", ",", "buffer", ",", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.BufferList.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_buffers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.BufferList.__iter__": [[35, 37], ["iter", "anchor_generator.BufferList._buffers.values"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "_buffers", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.DefaultAnchorGenerator.__init__": [[93, 122], ["torch.nn.Module.__init__", "len", "anchor_generator._broadcast_params", "anchor_generator._broadcast_params", "anchor_generator.DefaultAnchorGenerator._calculate_anchors"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator._broadcast_params", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator._broadcast_params", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator._calculate_anchors"], ["@", "configurable", "\n", "def", "__init__", "(", "self", ",", "*", ",", "sizes", ",", "aspect_ratios", ",", "strides", ",", "offset", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"\n        This interface is experimental.\n\n        Args:\n            sizes (list[list[float]] or list[float]):\n                If ``sizes`` is list[list[float]], ``sizes[i]`` is the list of anchor sizes\n                (i.e. sqrt of anchor area) to use for the i-th feature map.\n                If ``sizes`` is list[float], ``sizes`` is used for all feature maps.\n                Anchor sizes are given in absolute lengths in units of\n                the input image; they do not dynamically scale if the input image size changes.\n            aspect_ratios (list[list[float]] or list[float]): list of aspect ratios\n                (i.e. height / width) to use for anchors. Same \"broadcast\" rule for `sizes` applies.\n            strides (list[int]): stride of each input feature.\n            offset (float): Relative offset between the center of the first anchor and the top-left\n                corner of the image. Value has to be in [0, 1).\n                Recommend to use 0.5, which means half stride.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "strides", "=", "strides", "\n", "self", ".", "num_features", "=", "len", "(", "self", ".", "strides", ")", "\n", "sizes", "=", "_broadcast_params", "(", "sizes", ",", "self", ".", "num_features", ",", "\"sizes\"", ")", "\n", "aspect_ratios", "=", "_broadcast_params", "(", "aspect_ratios", ",", "self", ".", "num_features", ",", "\"aspect_ratios\"", ")", "\n", "self", ".", "cell_anchors", "=", "self", ".", "_calculate_anchors", "(", "sizes", ",", "aspect_ratios", ")", "\n", "\n", "self", ".", "offset", "=", "offset", "\n", "assert", "0.0", "<=", "self", ".", "offset", "<", "1.0", ",", "self", ".", "offset", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.DefaultAnchorGenerator.from_config": [[123, 130], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ":", "List", "[", "ShapeSpec", "]", ")", ":", "\n", "        ", "return", "{", "\n", "\"sizes\"", ":", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "SIZES", ",", "\n", "\"aspect_ratios\"", ":", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ASPECT_RATIOS", ",", "\n", "\"strides\"", ":", "[", "x", ".", "stride", "for", "x", "in", "input_shape", "]", ",", "\n", "\"offset\"", ":", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "OFFSET", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.DefaultAnchorGenerator._calculate_anchors": [[132, 137], ["anchor_generator.BufferList", "anchor_generator.DefaultAnchorGenerator.generate_cell_anchors().float", "zip", "anchor_generator.DefaultAnchorGenerator.generate_cell_anchors"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator.generate_cell_anchors"], ["", "def", "_calculate_anchors", "(", "self", ",", "sizes", ",", "aspect_ratios", ")", ":", "\n", "        ", "cell_anchors", "=", "[", "\n", "self", ".", "generate_cell_anchors", "(", "s", ",", "a", ")", ".", "float", "(", ")", "for", "s", ",", "a", "in", "zip", "(", "sizes", ",", "aspect_ratios", ")", "\n", "]", "\n", "return", "BufferList", "(", "cell_anchors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.DefaultAnchorGenerator.num_cell_anchors": [[138, 145], ["None"], "methods", ["None"], ["", "@", "property", "\n", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "num_cell_anchors", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Alias of `num_anchors`.\n        \"\"\"", "\n", "return", "self", ".", "num_anchors", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.DefaultAnchorGenerator.num_anchors": [[146, 160], ["len"], "methods", ["None"], ["", "@", "property", "\n", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "num_anchors", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            list[int]: Each int is the number of anchors at every pixel\n                location, on that feature map.\n                For example, if at every pixel we use anchors of 3 aspect\n                ratios and 5 sizes, the number of anchors is 15.\n                (See also ANCHOR_GENERATOR.SIZES and ANCHOR_GENERATOR.ASPECT_RATIOS in config)\n\n                In standard RPN models, `num_anchors` on every feature map is the same.\n        \"\"\"", "\n", "return", "[", "len", "(", "cell_anchors", ")", "for", "cell_anchors", "in", "self", ".", "cell_anchors", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.DefaultAnchorGenerator._grid_anchors": [[161, 176], ["zip", "anchor_generator._create_grid_offsets", "torch.stack", "anchors.append", "anchor_generator.DefaultAnchorGenerator.cell_anchors.named_buffers", "torch.stack.view", "base_anchors.view"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator._create_grid_offsets"], ["", "def", "_grid_anchors", "(", "self", ",", "grid_sizes", ":", "List", "[", "List", "[", "int", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            list[Tensor]: #featuremap tensors, each is (#locations x #cell_anchors) x 4\n        \"\"\"", "\n", "anchors", "=", "[", "]", "\n", "# buffers() not supported by torchscript. use named_buffers() instead", "\n", "buffers", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "x", "[", "1", "]", "for", "x", "in", "self", ".", "cell_anchors", ".", "named_buffers", "(", ")", "]", "\n", "for", "size", ",", "stride", ",", "base_anchors", "in", "zip", "(", "grid_sizes", ",", "self", ".", "strides", ",", "buffers", ")", ":", "\n", "            ", "shift_x", ",", "shift_y", "=", "_create_grid_offsets", "(", "size", ",", "stride", ",", "self", ".", "offset", ",", "base_anchors", ".", "device", ")", "\n", "shifts", "=", "torch", ".", "stack", "(", "(", "shift_x", ",", "shift_y", ",", "shift_x", ",", "shift_y", ")", ",", "dim", "=", "1", ")", "\n", "\n", "anchors", ".", "append", "(", "(", "shifts", ".", "view", "(", "-", "1", ",", "1", ",", "4", ")", "+", "base_anchors", ".", "view", "(", "1", ",", "-", "1", ",", "4", ")", ")", ".", "reshape", "(", "-", "1", ",", "4", ")", ")", "\n", "\n", "", "return", "anchors", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.DefaultAnchorGenerator.generate_cell_anchors": [[177, 213], ["torch.tensor", "math.sqrt", "anchors.append"], "methods", ["None"], ["", "def", "generate_cell_anchors", "(", "self", ",", "sizes", "=", "(", "32", ",", "64", ",", "128", ",", "256", ",", "512", ")", ",", "aspect_ratios", "=", "(", "0.5", ",", "1", ",", "2", ")", ")", ":", "\n", "        ", "\"\"\"\n        Generate a tensor storing canonical anchor boxes, which are all anchor\n        boxes of different sizes and aspect_ratios centered at (0, 0).\n        We can later build the set of anchors for a full feature map by\n        shifting and tiling these tensors (see `meth:_grid_anchors`).\n\n        Args:\n            sizes (tuple[float]):\n            aspect_ratios (tuple[float]]):\n\n        Returns:\n            Tensor of shape (len(sizes) * len(aspect_ratios), 4) storing anchor boxes\n                in XYXY format.\n        \"\"\"", "\n", "\n", "# This is different from the anchor generator defined in the original Faster R-CNN", "\n", "# code or Detectron. They yield the same AP, however the old version defines cell", "\n", "# anchors in a less natural way with a shift relative to the feature grid and", "\n", "# quantization that results in slightly different sizes for different aspect ratios.", "\n", "# See also https://github.com/facebookresearch/Detectron/issues/227", "\n", "\n", "anchors", "=", "[", "]", "\n", "for", "size", "in", "sizes", ":", "\n", "            ", "area", "=", "size", "**", "2.0", "\n", "for", "aspect_ratio", "in", "aspect_ratios", ":", "\n", "# s * s = w * h", "\n", "# a = h / w", "\n", "# ... some algebra ...", "\n", "# w = sqrt(s * s / a)", "\n", "# h = a * w", "\n", "                ", "w", "=", "math", ".", "sqrt", "(", "area", "/", "aspect_ratio", ")", "\n", "h", "=", "aspect_ratio", "*", "w", "\n", "x0", ",", "y0", ",", "x1", ",", "y1", "=", "-", "w", "/", "2.0", ",", "-", "h", "/", "2.0", ",", "w", "/", "2.0", ",", "h", "/", "2.0", "\n", "anchors", ".", "append", "(", "[", "x0", ",", "y0", ",", "x1", ",", "y1", "]", ")", "\n", "", "", "return", "torch", ".", "tensor", "(", "anchors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.DefaultAnchorGenerator.forward": [[214, 228], ["anchor_generator.DefaultAnchorGenerator._grid_anchors", "detectron2.structures.Boxes"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator._grid_anchors"], ["", "def", "forward", "(", "self", ",", "features", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            features (list[Tensor]): list of backbone feature maps on which to generate anchors.\n\n        Returns:\n            list[Boxes]: a list of Boxes containing all the anchors for each feature map\n                (i.e. the cell anchors repeated over all locations in the feature map).\n                The number of anchors of each feature map is Hi x Wi x num_cell_anchors,\n                where Hi, Wi are resolution of the feature map divided by anchor stride.\n        \"\"\"", "\n", "grid_sizes", "=", "[", "feature_map", ".", "shape", "[", "-", "2", ":", "]", "for", "feature_map", "in", "features", "]", "\n", "anchors_over_all_feature_maps", "=", "self", ".", "_grid_anchors", "(", "grid_sizes", ")", "\n", "return", "[", "Boxes", "(", "x", ")", "for", "x", "in", "anchors_over_all_feature_maps", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator.__init__": [[242, 274], ["torch.nn.Module.__init__", "len", "anchor_generator._broadcast_params", "anchor_generator._broadcast_params", "anchor_generator._broadcast_params", "anchor_generator.RotatedAnchorGenerator._calculate_anchors"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator._broadcast_params", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator._broadcast_params", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator._broadcast_params", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator._calculate_anchors"], ["@", "configurable", "\n", "def", "__init__", "(", "self", ",", "*", ",", "sizes", ",", "aspect_ratios", ",", "strides", ",", "angles", ",", "offset", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"\n        This interface is experimental.\n\n        Args:\n            sizes (list[list[float]] or list[float]):\n                If sizes is list[list[float]], sizes[i] is the list of anchor sizes\n                (i.e. sqrt of anchor area) to use for the i-th feature map.\n                If sizes is list[float], the sizes are used for all feature maps.\n                Anchor sizes are given in absolute lengths in units of\n                the input image; they do not dynamically scale if the input image size changes.\n            aspect_ratios (list[list[float]] or list[float]): list of aspect ratios\n                (i.e. height / width) to use for anchors. Same \"broadcast\" rule for `sizes` applies.\n            strides (list[int]): stride of each input feature.\n            angles (list[list[float]] or list[float]): list of angles (in degrees CCW)\n                to use for anchors. Same \"broadcast\" rule for `sizes` applies.\n            offset (float): Relative offset between the center of the first anchor and the top-left\n                corner of the image. Value has to be in [0, 1).\n                Recommend to use 0.5, which means half stride.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "strides", "=", "strides", "\n", "self", ".", "num_features", "=", "len", "(", "self", ".", "strides", ")", "\n", "sizes", "=", "_broadcast_params", "(", "sizes", ",", "self", ".", "num_features", ",", "\"sizes\"", ")", "\n", "aspect_ratios", "=", "_broadcast_params", "(", "aspect_ratios", ",", "self", ".", "num_features", ",", "\"aspect_ratios\"", ")", "\n", "angles", "=", "_broadcast_params", "(", "angles", ",", "self", ".", "num_features", ",", "\"angles\"", ")", "\n", "self", ".", "cell_anchors", "=", "self", ".", "_calculate_anchors", "(", "sizes", ",", "aspect_ratios", ",", "angles", ")", "\n", "\n", "self", ".", "offset", "=", "offset", "\n", "assert", "0.0", "<=", "self", ".", "offset", "<", "1.0", ",", "self", ".", "offset", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator.from_config": [[275, 283], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ":", "List", "[", "ShapeSpec", "]", ")", ":", "\n", "        ", "return", "{", "\n", "\"sizes\"", ":", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "SIZES", ",", "\n", "\"aspect_ratios\"", ":", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ASPECT_RATIOS", ",", "\n", "\"strides\"", ":", "[", "x", ".", "stride", "for", "x", "in", "input_shape", "]", ",", "\n", "\"offset\"", ":", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "OFFSET", ",", "\n", "\"angles\"", ":", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ANGLES", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator._calculate_anchors": [[285, 291], ["anchor_generator.BufferList", "anchor_generator.RotatedAnchorGenerator.generate_cell_anchors().float", "zip", "anchor_generator.RotatedAnchorGenerator.generate_cell_anchors"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator.generate_cell_anchors"], ["", "def", "_calculate_anchors", "(", "self", ",", "sizes", ",", "aspect_ratios", ",", "angles", ")", ":", "\n", "        ", "cell_anchors", "=", "[", "\n", "self", ".", "generate_cell_anchors", "(", "size", ",", "aspect_ratio", ",", "angle", ")", ".", "float", "(", ")", "\n", "for", "size", ",", "aspect_ratio", ",", "angle", "in", "zip", "(", "sizes", ",", "aspect_ratios", ",", "angles", ")", "\n", "]", "\n", "return", "BufferList", "(", "cell_anchors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator.num_cell_anchors": [[292, 298], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_cell_anchors", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Alias of `num_anchors`.\n        \"\"\"", "\n", "return", "self", ".", "num_anchors", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator.num_anchors": [[299, 313], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_anchors", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            list[int]: Each int is the number of anchors at every pixel\n                location, on that feature map.\n                For example, if at every pixel we use anchors of 3 aspect\n                ratios, 2 sizes and 5 angles, the number of anchors is 30.\n                (See also ANCHOR_GENERATOR.SIZES, ANCHOR_GENERATOR.ASPECT_RATIOS\n                and ANCHOR_GENERATOR.ANGLES in config)\n\n                In standard RRPN models, `num_anchors` on every feature map is the same.\n        \"\"\"", "\n", "return", "[", "len", "(", "cell_anchors", ")", "for", "cell_anchors", "in", "self", ".", "cell_anchors", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator._grid_anchors": [[314, 324], ["zip", "anchor_generator._create_grid_offsets", "torch.zeros_like", "torch.stack", "anchors.append", "torch.stack.view", "base_anchors.view"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator._create_grid_offsets"], ["", "def", "_grid_anchors", "(", "self", ",", "grid_sizes", ")", ":", "\n", "        ", "anchors", "=", "[", "]", "\n", "for", "size", ",", "stride", ",", "base_anchors", "in", "zip", "(", "grid_sizes", ",", "self", ".", "strides", ",", "self", ".", "cell_anchors", ")", ":", "\n", "            ", "shift_x", ",", "shift_y", "=", "_create_grid_offsets", "(", "size", ",", "stride", ",", "self", ".", "offset", ",", "base_anchors", ".", "device", ")", "\n", "zeros", "=", "torch", ".", "zeros_like", "(", "shift_x", ")", "\n", "shifts", "=", "torch", ".", "stack", "(", "(", "shift_x", ",", "shift_y", ",", "zeros", ",", "zeros", ",", "zeros", ")", ",", "dim", "=", "1", ")", "\n", "\n", "anchors", ".", "append", "(", "(", "shifts", ".", "view", "(", "-", "1", ",", "1", ",", "5", ")", "+", "base_anchors", ".", "view", "(", "1", ",", "-", "1", ",", "5", ")", ")", ".", "reshape", "(", "-", "1", ",", "5", ")", ")", "\n", "\n", "", "return", "anchors", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator.generate_cell_anchors": [[325, 360], ["torch.tensor", "math.sqrt", "anchors.extend"], "methods", ["None"], ["", "def", "generate_cell_anchors", "(", "\n", "self", ",", "\n", "sizes", "=", "(", "32", ",", "64", ",", "128", ",", "256", ",", "512", ")", ",", "\n", "aspect_ratios", "=", "(", "0.5", ",", "1", ",", "2", ")", ",", "\n", "angles", "=", "(", "-", "90", ",", "-", "60", ",", "-", "30", ",", "0", ",", "30", ",", "60", ",", "90", ")", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Generate a tensor storing canonical anchor boxes, which are all anchor\n        boxes of different sizes, aspect_ratios, angles centered at (0, 0).\n        We can later build the set of anchors for a full feature map by\n        shifting and tiling these tensors (see `meth:_grid_anchors`).\n\n        Args:\n            sizes (tuple[float]):\n            aspect_ratios (tuple[float]]):\n            angles (tuple[float]]):\n\n        Returns:\n            Tensor of shape (len(sizes) * len(aspect_ratios) * len(angles), 5)\n                storing anchor boxes in (x_ctr, y_ctr, w, h, angle) format.\n        \"\"\"", "\n", "anchors", "=", "[", "]", "\n", "for", "size", "in", "sizes", ":", "\n", "            ", "area", "=", "size", "**", "2.0", "\n", "for", "aspect_ratio", "in", "aspect_ratios", ":", "\n", "# s * s = w * h", "\n", "# a = h / w", "\n", "# ... some algebra ...", "\n", "# w = sqrt(s * s / a)", "\n", "# h = a * w", "\n", "                ", "w", "=", "math", ".", "sqrt", "(", "area", "/", "aspect_ratio", ")", "\n", "h", "=", "aspect_ratio", "*", "w", "\n", "anchors", ".", "extend", "(", "[", "0", ",", "0", ",", "w", ",", "h", ",", "a", "]", "for", "a", "in", "angles", ")", "\n", "\n", "", "", "return", "torch", ".", "tensor", "(", "anchors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator.forward": [[361, 375], ["anchor_generator.RotatedAnchorGenerator._grid_anchors", "detectron2.structures.RotatedBoxes"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.RotatedAnchorGenerator._grid_anchors"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            features (list[Tensor]): list of backbone feature maps on which to generate anchors.\n\n        Returns:\n            list[RotatedBoxes]: a list of Boxes containing all the anchors for each feature map\n                (i.e. the cell anchors repeated over all locations in the feature map).\n                The number of anchors of each feature map is Hi x Wi x num_cell_anchors,\n                where Hi, Wi are resolution of the feature map divided by anchor stride.\n        \"\"\"", "\n", "grid_sizes", "=", "[", "feature_map", ".", "shape", "[", "-", "2", ":", "]", "for", "feature_map", "in", "features", "]", "\n", "anchors_over_all_feature_maps", "=", "self", ".", "_grid_anchors", "(", "grid_sizes", ")", "\n", "return", "[", "RotatedBoxes", "(", "x", ")", "for", "x", "in", "anchors_over_all_feature_maps", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator._create_grid_offsets": [[39, 52], ["torch.arange", "torch.arange", "torch.meshgrid", "shift_x.reshape.reshape", "shift_y.reshape.reshape"], "function", ["None"], ["", "", "def", "_create_grid_offsets", "(", "size", ":", "List", "[", "int", "]", ",", "stride", ":", "int", ",", "offset", ":", "float", ",", "device", ":", "torch", ".", "device", ")", ":", "\n", "    ", "grid_height", ",", "grid_width", "=", "size", "\n", "shifts_x", "=", "torch", ".", "arange", "(", "\n", "offset", "*", "stride", ",", "grid_width", "*", "stride", ",", "step", "=", "stride", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", "\n", ")", "\n", "shifts_y", "=", "torch", ".", "arange", "(", "\n", "offset", "*", "stride", ",", "grid_height", "*", "stride", ",", "step", "=", "stride", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", "\n", ")", "\n", "\n", "shift_y", ",", "shift_x", "=", "torch", ".", "meshgrid", "(", "shifts_y", ",", "shifts_x", ")", "\n", "shift_x", "=", "shift_x", ".", "reshape", "(", "-", "1", ")", "\n", "shift_y", "=", "shift_y", ".", "reshape", "(", "-", "1", ")", "\n", "return", "shift_x", ",", "shift_y", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator._broadcast_params": [[54, 79], ["isinstance", "len", "isinstance", "len", "len", "list", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "_broadcast_params", "(", "params", ",", "num_features", ",", "name", ")", ":", "\n", "    ", "\"\"\"\n    If one size (or aspect ratio) is specified and there are multiple feature\n    maps, we \"broadcast\" anchors of that single size (or aspect ratio)\n    over all feature maps.\n\n    If params is list[float], or list[list[float]] with len(params) == 1, repeat\n    it num_features time.\n\n    Returns:\n        list[list[float]]: param for each feature\n    \"\"\"", "\n", "assert", "isinstance", "(", "\n", "params", ",", "collections", ".", "abc", ".", "Sequence", "\n", ")", ",", "f\"{name} in anchor generator has to be a list! Got {params}.\"", "\n", "assert", "len", "(", "params", ")", ",", "f\"{name} in anchor generator cannot be empty!\"", "\n", "if", "not", "isinstance", "(", "params", "[", "0", "]", ",", "collections", ".", "abc", ".", "Sequence", ")", ":", "# params is list[float]", "\n", "        ", "return", "[", "params", "]", "*", "num_features", "\n", "", "if", "len", "(", "params", ")", "==", "1", ":", "\n", "        ", "return", "list", "(", "params", ")", "*", "num_features", "\n", "", "assert", "len", "(", "params", ")", "==", "num_features", ",", "(", "\n", "f\"Got {name} of length {len(params)} in anchor generator, \"", "\n", "f\"but the number of input features is {num_features}!\"", "\n", ")", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.build_anchor_generator": [[377, 383], ["ANCHOR_GENERATOR_REGISTRY.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "build_anchor_generator", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Built an anchor generator from `cfg.MODEL.ANCHOR_GENERATOR.NAME`.\n    \"\"\"", "\n", "anchor_generator", "=", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "NAME", "\n", "return", "ANCHOR_GENERATOR_REGISTRY", ".", "get", "(", "anchor_generator", ")", "(", "cfg", ",", "input_shape", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper.MMDetBackbone.__init__": [[44, 104], ["backbone.Backbone.__init__", "isinstance", "isinstance", "logger.info", "mmdet_wrapper.MMDetBackbone.backbone.init_weights", "mmdet_wrapper.MMDetBackbone.backbone.train", "build_backbone", "build_neck", "logger.info", "isinstance", "mmdet_wrapper.MMDetBackbone.neck.train", "mmdet_wrapper._to_container", "mmdet_wrapper._to_container", "mmdet_wrapper.MMDetBackbone.neck.init_weights", "m.init_weights", "range", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.init_weights", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper._to_container", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper._to_container", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.init_weights", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.init_weights"], ["def", "__init__", "(", "\n", "self", ",", "\n", "backbone", ":", "Union", "[", "nn", ".", "Module", ",", "Mapping", "]", ",", "\n", "neck", ":", "Union", "[", "nn", ".", "Module", ",", "Mapping", ",", "None", "]", "=", "None", ",", "\n", "*", ",", "\n", "pretrained_backbone", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "output_shapes", ":", "List", "[", "ShapeSpec", "]", ",", "\n", "output_names", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            backbone: either a backbone module or a mmdet config dict that defines a\n                backbone. The backbone takes a 4D image tensor and returns a\n                sequence of tensors.\n            neck: either a backbone module or a mmdet config dict that defines a\n                neck. The neck takes outputs of backbone and returns a\n                sequence of tensors. If None, no neck is used.\n            pretrained_backbone: defines the backbone weights that can be loaded by\n                mmdet, such as \"torchvision://resnet50\".\n            output_shapes: shape for every output of the backbone (or neck, if given).\n                stride and channels are often needed.\n            output_names: names for every output of the backbone (or neck, if given).\n                By default, will use \"out0\", \"out1\", ...\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "isinstance", "(", "backbone", ",", "Mapping", ")", ":", "\n", "            ", "from", "mmdet", ".", "models", "import", "build_backbone", "\n", "\n", "backbone", "=", "build_backbone", "(", "_to_container", "(", "backbone", ")", ")", "\n", "", "self", ".", "backbone", "=", "backbone", "\n", "\n", "if", "isinstance", "(", "neck", ",", "Mapping", ")", ":", "\n", "            ", "from", "mmdet", ".", "models", "import", "build_neck", "\n", "\n", "neck", "=", "build_neck", "(", "_to_container", "(", "neck", ")", ")", "\n", "", "self", ".", "neck", "=", "neck", "\n", "\n", "# It's confusing that backbone weights are given as a separate argument,", "\n", "# but \"neck\" weights, if any, are part of neck itself. This is the interface", "\n", "# of mmdet so we follow it. Reference:", "\n", "# https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/detectors/two_stage.py", "\n", "logger", ".", "info", "(", "f\"Initializing mmdet backbone weights: {pretrained_backbone} ...\"", ")", "\n", "self", ".", "backbone", ".", "init_weights", "(", "pretrained_backbone", ")", "\n", "# train() in mmdet modules is non-trivial, and has to be explicitly", "\n", "# called. Reference:", "\n", "# https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/backbones/resnet.py", "\n", "self", ".", "backbone", ".", "train", "(", ")", "\n", "if", "self", ".", "neck", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\"Initializing mmdet neck weights ...\"", ")", "\n", "if", "isinstance", "(", "self", ".", "neck", ",", "nn", ".", "Sequential", ")", ":", "\n", "                ", "for", "m", "in", "self", ".", "neck", ":", "\n", "                    ", "m", ".", "init_weights", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "neck", ".", "init_weights", "(", ")", "\n", "", "self", ".", "neck", ".", "train", "(", ")", "\n", "\n", "", "self", ".", "_output_shapes", "=", "output_shapes", "\n", "if", "not", "output_names", ":", "\n", "            ", "output_names", "=", "[", "f\"out{i}\"", "for", "i", "in", "range", "(", "len", "(", "output_shapes", ")", ")", "]", "\n", "", "self", ".", "_output_names", "=", "output_names", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper.MMDetBackbone.forward": [[105, 118], ["mmdet_wrapper.MMDetBackbone.backbone", "isinstance", "mmdet_wrapper.MMDetBackbone.neck", "len", "len", "ValueError", "zip", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "Dict", "[", "str", ",", "Tensor", "]", ":", "\n", "        ", "outs", "=", "self", ".", "backbone", "(", "x", ")", "\n", "if", "self", ".", "neck", "is", "not", "None", ":", "\n", "            ", "outs", "=", "self", ".", "neck", "(", "outs", ")", "\n", "", "assert", "isinstance", "(", "\n", "outs", ",", "(", "list", ",", "tuple", ")", "\n", ")", ",", "\"mmdet backbone should return a list/tuple of tensors!\"", "\n", "if", "len", "(", "outs", ")", "!=", "len", "(", "self", ".", "_output_shapes", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Length of output_shapes does not match outputs from the mmdet backbone: \"", "\n", "f\"{len(outs)} != {len(self._output_shapes)}\"", "\n", ")", "\n", "", "return", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "self", ".", "_output_names", ",", "outs", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper.MMDetBackbone.output_shape": [[119, 121], ["zip"], "methods", ["None"], ["", "def", "output_shape", "(", "self", ")", "->", "Dict", "[", "str", ",", "ShapeSpec", "]", ":", "\n", "        ", "return", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "self", ".", "_output_names", ",", "self", ".", "_output_shapes", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper.MMDetDetector.__init__": [[130, 160], ["torch.nn.Module.__init__", "isinstance", "mmdet_wrapper.MMDetDetector.register_buffer", "mmdet_wrapper.MMDetDetector.register_buffer", "build_detector", "torch.tensor().view", "torch.tensor().view", "mmdet_wrapper._to_container", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper._to_container"], ["def", "__init__", "(", "\n", "self", ",", "\n", "detector", ":", "Union", "[", "nn", ".", "Module", ",", "Mapping", "]", ",", "\n", "*", ",", "\n", "# Default is 32 regardless of model:", "\n", "# https://github.com/open-mmlab/mmdetection/tree/master/configs/_base_/datasets", "\n", "size_divisibility", "=", "32", ",", "\n", "pixel_mean", ":", "Tuple", "[", "float", "]", ",", "\n", "pixel_std", ":", "Tuple", "[", "float", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            detector: a mmdet detector, or a mmdet config dict that defines a detector.\n            size_divisibility: pad input images to multiple of this number\n            pixel_mean: per-channel mean to normalize input image\n            pixel_std: per-channel stddev to normalize input image\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "isinstance", "(", "detector", ",", "Mapping", ")", ":", "\n", "            ", "from", "mmdet", ".", "models", "import", "build_detector", "\n", "\n", "detector", "=", "build_detector", "(", "_to_container", "(", "detector", ")", ")", "\n", "", "self", ".", "detector", "=", "detector", "\n", "self", ".", "size_divisibility", "=", "size_divisibility", "\n", "\n", "self", ".", "register_buffer", "(", "\"pixel_mean\"", ",", "torch", ".", "tensor", "(", "pixel_mean", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "self", ".", "register_buffer", "(", "\"pixel_std\"", ",", "torch", ".", "tensor", "(", "pixel_std", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "assert", "(", "\n", "self", ".", "pixel_mean", ".", "shape", "==", "self", ".", "pixel_std", ".", "shape", "\n", ")", ",", "f\"{self.pixel_mean} and {self.pixel_std} have different shapes!\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper.MMDetDetector.forward": [[161, 226], ["x[].to", "detectron2.structures.ImageList.from_tensors", "len", "ValueError", "list", "metas.append", "gt_instances[].has", "mmdet_wrapper._parse_losses", "mmdet_wrapper.MMDetDetector.detector.simple_test", "numpy.array", "output_shapes.append", "output_shapes.append", "x[].to", "mmdet_wrapper.MMDetDetector.detector.forward_train", "mmdet_wrapper.MMDetDetector.detector.forward_train", "isinstance", "mmdet_wrapper.MMDetDetector.forward.convert_mask"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper._parse_losses", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "forward", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "images", "=", "[", "x", "[", "\"image\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "images", "=", "[", "(", "x", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "size_divisibility", "=", "self", ".", "size_divisibility", ")", ".", "tensor", "\n", "metas", "=", "[", "]", "\n", "rescale", "=", "{", "\"height\"", "in", "x", "for", "x", "in", "batched_inputs", "}", "\n", "if", "len", "(", "rescale", ")", "!=", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Some inputs have original height/width, but some don't!\"", ")", "\n", "", "rescale", "=", "list", "(", "rescale", ")", "[", "0", "]", "\n", "output_shapes", "=", "[", "]", "\n", "for", "input", "in", "batched_inputs", ":", "\n", "            ", "meta", "=", "{", "}", "\n", "c", ",", "h", ",", "w", "=", "input", "[", "\"image\"", "]", ".", "shape", "\n", "meta", "[", "\"img_shape\"", "]", "=", "meta", "[", "\"ori_shape\"", "]", "=", "(", "h", ",", "w", ",", "c", ")", "\n", "if", "rescale", ":", "\n", "                ", "scale_factor", "=", "np", ".", "array", "(", "\n", "[", "w", "/", "input", "[", "\"width\"", "]", ",", "h", "/", "input", "[", "\"height\"", "]", "]", "*", "2", ",", "dtype", "=", "\"float32\"", "\n", ")", "\n", "ori_shape", "=", "(", "input", "[", "\"height\"", "]", ",", "input", "[", "\"width\"", "]", ")", "\n", "output_shapes", ".", "append", "(", "ori_shape", ")", "\n", "meta", "[", "\"ori_shape\"", "]", "=", "ori_shape", "+", "(", "c", ",", ")", "\n", "", "else", ":", "\n", "                ", "scale_factor", "=", "1.0", "\n", "output_shapes", ".", "append", "(", "(", "h", ",", "w", ")", ")", "\n", "", "meta", "[", "\"scale_factor\"", "]", "=", "scale_factor", "\n", "meta", "[", "\"flip\"", "]", "=", "False", "\n", "padh", ",", "padw", "=", "images", ".", "shape", "[", "-", "2", ":", "]", "\n", "meta", "[", "\"pad_shape\"", "]", "=", "(", "padh", ",", "padw", ",", "c", ")", "\n", "metas", ".", "append", "(", "meta", ")", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "            ", "gt_instances", "=", "[", "x", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "if", "gt_instances", "[", "0", "]", ".", "has", "(", "\"gt_masks\"", ")", ":", "\n", "                ", "from", "mmdet", ".", "core", "import", "PolygonMasks", "as", "mm_PolygonMasks", ",", "BitmapMasks", "as", "mm_BitMasks", "\n", "\n", "def", "convert_mask", "(", "m", ",", "shape", ")", ":", "\n", "# mmdet mask format", "\n", "                    ", "if", "isinstance", "(", "m", ",", "BitMasks", ")", ":", "\n", "                        ", "return", "mm_BitMasks", "(", "m", ".", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "shape", "[", "0", "]", ",", "shape", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                        ", "return", "mm_PolygonMasks", "(", "m", ".", "polygons", ",", "shape", "[", "0", "]", ",", "shape", "[", "1", "]", ")", "\n", "\n", "", "", "gt_masks", "=", "[", "convert_mask", "(", "x", ".", "gt_masks", ",", "x", ".", "image_size", ")", "for", "x", "in", "gt_instances", "]", "\n", "losses_and_metrics", "=", "self", ".", "detector", ".", "forward_train", "(", "\n", "images", ",", "\n", "metas", ",", "\n", "[", "x", ".", "gt_boxes", ".", "tensor", "for", "x", "in", "gt_instances", "]", ",", "\n", "[", "x", ".", "gt_classes", "for", "x", "in", "gt_instances", "]", ",", "\n", "gt_masks", "=", "gt_masks", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "losses_and_metrics", "=", "self", ".", "detector", ".", "forward_train", "(", "\n", "images", ",", "\n", "metas", ",", "\n", "[", "x", ".", "gt_boxes", ".", "tensor", "for", "x", "in", "gt_instances", "]", ",", "\n", "[", "x", ".", "gt_classes", "for", "x", "in", "gt_instances", "]", ",", "\n", ")", "\n", "", "return", "_parse_losses", "(", "losses_and_metrics", ")", "\n", "", "else", ":", "\n", "            ", "results", "=", "self", ".", "detector", ".", "simple_test", "(", "images", ",", "metas", ",", "rescale", "=", "rescale", ")", "\n", "results", "=", "[", "\n", "{", "\"instances\"", ":", "_convert_mmdet_result", "(", "r", ",", "shape", ")", "}", "\n", "for", "r", ",", "shape", "in", "zip", "(", "results", ",", "output_shapes", ")", "\n", "]", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper.MMDetDetector.device": [[227, 230], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pixel_mean", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper._to_container": [[22, 32], ["isinstance", "ConfigDict", "omegaconf.OmegaConf.to_container"], "function", ["None"], ["def", "_to_container", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    mmdet will assert the type of dict/list.\n    So convert omegaconf objects to dict/list.\n    \"\"\"", "\n", "if", "isinstance", "(", "cfg", ",", "DictConfig", ")", ":", "\n", "        ", "cfg", "=", "OmegaConf", ".", "to_container", "(", "cfg", ",", "resolve", "=", "True", ")", "\n", "", "from", "mmcv", ".", "utils", "import", "ConfigDict", "\n", "\n", "return", "ConfigDict", "(", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper._convert_mmdet_result": [[234, 259], ["isinstance", "torch.from_numpy", "torch.cat", "detectron2.structures.Instances", "detectron2.structures.Boxes", "isinstance", "numpy.vstack", "torch.full", "list", "torch.stack", "enumerate", "len", "itertools.chain", "isinstance", "torch.from_numpy"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "", "def", "_convert_mmdet_result", "(", "result", ",", "shape", ":", "Tuple", "[", "int", ",", "int", "]", ")", "->", "Instances", ":", "\n", "    ", "if", "isinstance", "(", "result", ",", "tuple", ")", ":", "\n", "        ", "bbox_result", ",", "segm_result", "=", "result", "\n", "if", "isinstance", "(", "segm_result", ",", "tuple", ")", ":", "\n", "            ", "segm_result", "=", "segm_result", "[", "0", "]", "\n", "", "", "else", ":", "\n", "        ", "bbox_result", ",", "segm_result", "=", "result", ",", "None", "\n", "\n", "", "bboxes", "=", "torch", ".", "from_numpy", "(", "np", ".", "vstack", "(", "bbox_result", ")", ")", "# Nx5", "\n", "bboxes", ",", "scores", "=", "bboxes", "[", ":", ",", ":", "4", "]", ",", "bboxes", "[", ":", ",", "-", "1", "]", "\n", "labels", "=", "[", "\n", "torch", ".", "full", "(", "(", "bbox", ".", "shape", "[", "0", "]", ",", ")", ",", "i", ",", "dtype", "=", "torch", ".", "int32", ")", "for", "i", ",", "bbox", "in", "enumerate", "(", "bbox_result", ")", "\n", "]", "\n", "labels", "=", "torch", ".", "cat", "(", "labels", ")", "\n", "inst", "=", "Instances", "(", "shape", ")", "\n", "inst", ".", "pred_boxes", "=", "Boxes", "(", "bboxes", ")", "\n", "inst", ".", "scores", "=", "scores", "\n", "inst", ".", "pred_classes", "=", "labels", "\n", "\n", "if", "segm_result", "is", "not", "None", "and", "len", "(", "labels", ")", ">", "0", ":", "\n", "        ", "segm_result", "=", "list", "(", "itertools", ".", "chain", "(", "*", "segm_result", ")", ")", "\n", "segm_result", "=", "[", "torch", ".", "from_numpy", "(", "x", ")", "if", "isinstance", "(", "x", ",", "np", ".", "ndarray", ")", "else", "x", "for", "x", "in", "segm_result", "]", "\n", "segm_result", "=", "torch", ".", "stack", "(", "segm_result", ",", "dim", "=", "0", ")", "\n", "inst", ".", "pred_masks", "=", "segm_result", "\n", "", "return", "inst", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.mmdet_wrapper._parse_losses": [[262, 278], ["collections.OrderedDict", "losses.items", "isinstance", "loss_value.mean", "isinstance", "detectron2.utils.events.get_event_storage", "collections.OrderedDict.pop().cpu().item", "detectron2.utils.events.get_event_storage.put_scalar", "sum", "TypeError", "collections.OrderedDict.pop().cpu", "_loss.mean", "collections.OrderedDict.pop"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar"], ["", "def", "_parse_losses", "(", "losses", ":", "Dict", "[", "str", ",", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "Tensor", "]", ":", "\n", "    ", "log_vars", "=", "OrderedDict", "(", ")", "\n", "for", "loss_name", ",", "loss_value", "in", "losses", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "loss_value", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "log_vars", "[", "loss_name", "]", "=", "loss_value", ".", "mean", "(", ")", "\n", "", "elif", "isinstance", "(", "loss_value", ",", "list", ")", ":", "\n", "            ", "log_vars", "[", "loss_name", "]", "=", "sum", "(", "_loss", ".", "mean", "(", ")", "for", "_loss", "in", "loss_value", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "f\"{loss_name} is not a tensor or list of tensors\"", ")", "\n", "\n", "", "if", "\"loss\"", "not", "in", "loss_name", ":", "\n", "# put metrics to storage; don't return them", "\n", "            ", "storage", "=", "get_event_storage", "(", ")", "\n", "value", "=", "log_vars", ".", "pop", "(", "loss_name", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "storage", ".", "put_scalar", "(", "loss_name", ",", "value", ")", "\n", "", "", "return", "log_vars", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.DatasetMapperTTA.__init__": [[38, 49], ["None"], "methods", ["None"], ["@", "configurable", "\n", "def", "__init__", "(", "self", ",", "min_sizes", ":", "List", "[", "int", "]", ",", "max_size", ":", "int", ",", "flip", ":", "bool", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            min_sizes: list of short-edge size to resize the image to\n            max_size: maximum height or width of resized images\n            flip: whether to apply flipping augmentation\n        \"\"\"", "\n", "self", ".", "min_sizes", "=", "min_sizes", "\n", "self", ".", "max_size", "=", "max_size", "\n", "self", ".", "flip", "=", "flip", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.DatasetMapperTTA.from_config": [[50, 56], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "return", "{", "\n", "\"min_sizes\"", ":", "cfg", ".", "TEST", ".", "AUG", ".", "MIN_SIZES", ",", "\n", "\"max_size\"", ":", "cfg", ".", "TEST", ".", "AUG", ".", "MAX_SIZE", ",", "\n", "\"flip\"", ":", "cfg", ".", "TEST", ".", "AUG", ".", "FLIP", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.DatasetMapperTTA.__call__": [[58, 99], ["dataset_dict[].permute().numpy", "detectron2.data.transforms.ResizeTransform", "fvcore.transforms.NoOpTransform", "detectron2.data.transforms.ResizeShortestEdge", "aug_candidates.append", "detectron2.data.transforms.apply_augmentations", "torch.from_numpy", "copy.deepcopy", "ret.append", "dataset_dict[].permute", "detectron2.data.transforms.RandomFlip", "aug_candidates.append", "numpy.copy", "numpy.ascontiguousarray", "new_image.transpose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.apply_augmentations"], ["", "def", "__call__", "(", "self", ",", "dataset_dict", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dict: a dict in standard model input format. See tutorials for details.\n\n        Returns:\n            list[dict]:\n                a list of dicts, which contain augmented version of the input image.\n                The total number of dicts is ``len(min_sizes) * (2 if flip else 1)``.\n                Each dict has field \"transforms\" which is a TransformList,\n                containing the transforms that are used to generate this image.\n        \"\"\"", "\n", "numpy_image", "=", "dataset_dict", "[", "\"image\"", "]", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "numpy", "(", ")", "\n", "shape", "=", "numpy_image", ".", "shape", "\n", "orig_shape", "=", "(", "dataset_dict", "[", "\"height\"", "]", ",", "dataset_dict", "[", "\"width\"", "]", ")", "\n", "if", "shape", "[", ":", "2", "]", "!=", "orig_shape", ":", "\n", "# It transforms the \"original\" image in the dataset to the input image", "\n", "            ", "pre_tfm", "=", "ResizeTransform", "(", "orig_shape", "[", "0", "]", ",", "orig_shape", "[", "1", "]", ",", "shape", "[", "0", "]", ",", "shape", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "pre_tfm", "=", "NoOpTransform", "(", ")", "\n", "\n", "# Create all combinations of augmentations to use", "\n", "", "aug_candidates", "=", "[", "]", "# each element is a list[Augmentation]", "\n", "for", "min_size", "in", "self", ".", "min_sizes", ":", "\n", "            ", "resize", "=", "ResizeShortestEdge", "(", "min_size", ",", "self", ".", "max_size", ")", "\n", "aug_candidates", ".", "append", "(", "[", "resize", "]", ")", "# resize only", "\n", "if", "self", ".", "flip", ":", "\n", "                ", "flip", "=", "RandomFlip", "(", "prob", "=", "1.0", ")", "\n", "aug_candidates", ".", "append", "(", "[", "resize", ",", "flip", "]", ")", "# resize + flip", "\n", "\n", "# Apply all the augmentations", "\n", "", "", "ret", "=", "[", "]", "\n", "for", "aug", "in", "aug_candidates", ":", "\n", "            ", "new_image", ",", "tfms", "=", "apply_augmentations", "(", "aug", ",", "np", ".", "copy", "(", "numpy_image", ")", ")", "\n", "torch_image", "=", "torch", ".", "from_numpy", "(", "np", ".", "ascontiguousarray", "(", "new_image", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ")", ")", "\n", "\n", "dic", "=", "copy", ".", "deepcopy", "(", "dataset_dict", ")", "\n", "dic", "[", "\"transforms\"", "]", "=", "pre_tfm", "+", "tfms", "\n", "dic", "[", "\"image\"", "]", "=", "torch_image", "\n", "ret", ".", "append", "(", "dic", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA.__init__": [[107, 135], ["torch.nn.Module.__init__", "isinstance", "isinstance", "cfg.clone", "type", "test_time_augmentation.DatasetMapperTTA"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["def", "__init__", "(", "self", ",", "cfg", ",", "model", ",", "tta_mapper", "=", "None", ",", "batch_size", "=", "3", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            cfg (CfgNode):\n            model (GeneralizedRCNN): a GeneralizedRCNN to apply TTA on.\n            tta_mapper (callable): takes a dataset dict and returns a list of\n                augmented versions of the dataset dict. Defaults to\n                `DatasetMapperTTA(cfg)`.\n            batch_size (int): batch the augmented images into this batch size for inference.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "isinstance", "(", "model", ",", "DistributedDataParallel", ")", ":", "\n", "            ", "model", "=", "model", ".", "module", "\n", "", "assert", "isinstance", "(", "\n", "model", ",", "GeneralizedRCNN", "\n", ")", ",", "\"TTA is only supported on GeneralizedRCNN. Got a model of type {}\"", ".", "format", "(", "type", "(", "model", ")", ")", "\n", "self", ".", "cfg", "=", "cfg", ".", "clone", "(", ")", "\n", "assert", "not", "self", ".", "cfg", ".", "MODEL", ".", "KEYPOINT_ON", ",", "\"TTA for keypoint is not supported yet\"", "\n", "assert", "(", "\n", "not", "self", ".", "cfg", ".", "MODEL", ".", "LOAD_PROPOSALS", "\n", ")", ",", "\"TTA for pre-computed proposals is not supported yet\"", "\n", "\n", "self", ".", "model", "=", "model", "\n", "\n", "if", "tta_mapper", "is", "None", ":", "\n", "            ", "tta_mapper", "=", "DatasetMapperTTA", "(", "cfg", ")", "\n", "", "self", ".", "tta_mapper", "=", "tta_mapper", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._turn_off_roi_heads": [[136, 161], ["len", "old.keys", "old.keys", "getattr", "old.keys", "setattr", "setattr"], "methods", ["None"], ["", "@", "contextmanager", "\n", "def", "_turn_off_roi_heads", "(", "self", ",", "attrs", ")", ":", "\n", "        ", "\"\"\"\n        Open a context where some heads in `model.roi_heads` are temporarily turned off.\n        Args:\n            attr (list[str]): the attribute in `model.roi_heads` which can be used\n                to turn off a specific head, e.g., \"mask_on\", \"keypoint_on\".\n        \"\"\"", "\n", "roi_heads", "=", "self", ".", "model", ".", "roi_heads", "\n", "old", "=", "{", "}", "\n", "for", "attr", "in", "attrs", ":", "\n", "            ", "try", ":", "\n", "                ", "old", "[", "attr", "]", "=", "getattr", "(", "roi_heads", ",", "attr", ")", "\n", "", "except", "AttributeError", ":", "\n", "# The head may not be implemented in certain ROIHeads", "\n", "                ", "pass", "\n", "\n", "", "", "if", "len", "(", "old", ".", "keys", "(", ")", ")", "==", "0", ":", "\n", "            ", "yield", "\n", "", "else", ":", "\n", "            ", "for", "attr", "in", "old", ".", "keys", "(", ")", ":", "\n", "                ", "setattr", "(", "roi_heads", ",", "attr", ",", "False", ")", "\n", "", "yield", "\n", "for", "attr", "in", "old", ".", "keys", "(", ")", ":", "\n", "                ", "setattr", "(", "roi_heads", ",", "attr", ",", "old", "[", "attr", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._batch_inference": [[162, 187], ["zip", "itertools.count", "inputs.append", "instances.append", "len", "outputs.extend", "len", "test_time_augmentation.GeneralizedRCNNWithTTA.model.inference", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference"], ["", "", "", "def", "_batch_inference", "(", "self", ",", "batched_inputs", ",", "detected_instances", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Execute inference on a list of inputs,\n        using batch size = self.batch_size, instead of the length of the list.\n\n        Inputs & outputs have the same format as :meth:`GeneralizedRCNN.inference`\n        \"\"\"", "\n", "if", "detected_instances", "is", "None", ":", "\n", "            ", "detected_instances", "=", "[", "None", "]", "*", "len", "(", "batched_inputs", ")", "\n", "\n", "", "outputs", "=", "[", "]", "\n", "inputs", ",", "instances", "=", "[", "]", ",", "[", "]", "\n", "for", "idx", ",", "input", ",", "instance", "in", "zip", "(", "count", "(", ")", ",", "batched_inputs", ",", "detected_instances", ")", ":", "\n", "            ", "inputs", ".", "append", "(", "input", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "if", "len", "(", "inputs", ")", "==", "self", ".", "batch_size", "or", "idx", "==", "len", "(", "batched_inputs", ")", "-", "1", ":", "\n", "                ", "outputs", ".", "extend", "(", "\n", "self", ".", "model", ".", "inference", "(", "\n", "inputs", ",", "\n", "instances", "if", "instances", "[", "0", "]", "is", "not", "None", "else", "None", ",", "\n", "do_postprocess", "=", "False", ",", "\n", ")", "\n", ")", "\n", "inputs", ",", "instances", "=", "[", "]", ",", "[", "]", "\n", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA.__call__": [[188, 205], ["copy.copy", "test_time_augmentation.GeneralizedRCNNWithTTA._inference_one_image", "detectron2.data.detection_utils.read_image", "torch.from_numpy", "test_time_augmentation.GeneralizedRCNNWithTTA.__call__._maybe_read_image"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._inference_one_image", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.read_image"], ["", "def", "__call__", "(", "self", ",", "batched_inputs", ")", ":", "\n", "        ", "\"\"\"\n        Same input/output format as :meth:`GeneralizedRCNN.forward`\n        \"\"\"", "\n", "\n", "def", "_maybe_read_image", "(", "dataset_dict", ")", ":", "\n", "            ", "ret", "=", "copy", ".", "copy", "(", "dataset_dict", ")", "\n", "if", "\"image\"", "not", "in", "ret", ":", "\n", "                ", "image", "=", "read_image", "(", "ret", ".", "pop", "(", "\"file_name\"", ")", ",", "self", ".", "model", ".", "input_format", ")", "\n", "image", "=", "torch", ".", "from_numpy", "(", "np", ".", "ascontiguousarray", "(", "image", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ")", ")", "# CHW", "\n", "ret", "[", "\"image\"", "]", "=", "image", "\n", "", "if", "\"height\"", "not", "in", "ret", "and", "\"width\"", "not", "in", "ret", ":", "\n", "                ", "ret", "[", "\"height\"", "]", "=", "image", ".", "shape", "[", "1", "]", "\n", "ret", "[", "\"width\"", "]", "=", "image", ".", "shape", "[", "2", "]", "\n", "", "return", "ret", "\n", "\n", "", "return", "[", "self", ".", "_inference_one_image", "(", "_maybe_read_image", "(", "x", ")", ")", "for", "x", "in", "batched_inputs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._inference_one_image": [[206, 238], ["test_time_augmentation.GeneralizedRCNNWithTTA._get_augmented_inputs", "test_time_augmentation.GeneralizedRCNNWithTTA._merge_detections", "test_time_augmentation.GeneralizedRCNNWithTTA._turn_off_roi_heads", "test_time_augmentation.GeneralizedRCNNWithTTA._get_augmented_boxes", "test_time_augmentation.GeneralizedRCNNWithTTA._rescale_detected_boxes", "test_time_augmentation.GeneralizedRCNNWithTTA._batch_inference", "test_time_augmentation.GeneralizedRCNNWithTTA._reduce_pred_masks", "postprocessing.detector_postprocess"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._get_augmented_inputs", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._merge_detections", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._turn_off_roi_heads", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._get_augmented_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._rescale_detected_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._batch_inference", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._reduce_pred_masks", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess"], ["", "def", "_inference_one_image", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input (dict): one dataset dict with \"image\" field being a CHW tensor\n\n        Returns:\n            dict: one output dict\n        \"\"\"", "\n", "orig_shape", "=", "(", "input", "[", "\"height\"", "]", ",", "input", "[", "\"width\"", "]", ")", "\n", "augmented_inputs", ",", "tfms", "=", "self", ".", "_get_augmented_inputs", "(", "input", ")", "\n", "# Detect boxes from all augmented versions", "\n", "with", "self", ".", "_turn_off_roi_heads", "(", "[", "\"mask_on\"", ",", "\"keypoint_on\"", "]", ")", ":", "\n", "# temporarily disable roi heads", "\n", "            ", "all_boxes", ",", "all_scores", ",", "all_classes", "=", "self", ".", "_get_augmented_boxes", "(", "augmented_inputs", ",", "tfms", ")", "\n", "# merge all detected boxes to obtain final predictions for boxes", "\n", "", "merged_instances", "=", "self", ".", "_merge_detections", "(", "all_boxes", ",", "all_scores", ",", "all_classes", ",", "orig_shape", ")", "\n", "\n", "if", "self", ".", "cfg", ".", "MODEL", ".", "MASK_ON", ":", "\n", "# Use the detected boxes to obtain masks", "\n", "            ", "augmented_instances", "=", "self", ".", "_rescale_detected_boxes", "(", "\n", "augmented_inputs", ",", "merged_instances", ",", "tfms", "\n", ")", "\n", "# run forward on the detected boxes", "\n", "outputs", "=", "self", ".", "_batch_inference", "(", "augmented_inputs", ",", "augmented_instances", ")", "\n", "# Delete now useless variables to avoid being out of memory", "\n", "del", "augmented_inputs", ",", "augmented_instances", "\n", "# average the predictions", "\n", "merged_instances", ".", "pred_masks", "=", "self", ".", "_reduce_pred_masks", "(", "outputs", ",", "tfms", ")", "\n", "merged_instances", "=", "detector_postprocess", "(", "merged_instances", ",", "*", "orig_shape", ")", "\n", "return", "{", "\"instances\"", ":", "merged_instances", "}", "\n", "", "else", ":", "\n", "            ", "return", "{", "\"instances\"", ":", "merged_instances", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._get_augmented_inputs": [[239, 243], ["test_time_augmentation.GeneralizedRCNNWithTTA.tta_mapper", "x.pop"], "methods", ["None"], ["", "", "def", "_get_augmented_inputs", "(", "self", ",", "input", ")", ":", "\n", "        ", "augmented_inputs", "=", "self", ".", "tta_mapper", "(", "input", ")", "\n", "tfms", "=", "[", "x", ".", "pop", "(", "\"transforms\"", ")", "for", "x", "in", "augmented_inputs", "]", "\n", "return", "augmented_inputs", ",", "tfms", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._get_augmented_boxes": [[244, 261], ["test_time_augmentation.GeneralizedRCNNWithTTA._batch_inference", "zip", "torch.cat", "tfm.inverse().apply_box", "torch.cat.append", "all_scores.extend", "all_classes.extend", "pred_boxes.cpu().numpy", "torch.from_numpy().to", "tfm.inverse", "pred_boxes.cpu", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._batch_inference", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.inverse"], ["", "def", "_get_augmented_boxes", "(", "self", ",", "augmented_inputs", ",", "tfms", ")", ":", "\n", "# 1: forward with all augmented images", "\n", "        ", "outputs", "=", "self", ".", "_batch_inference", "(", "augmented_inputs", ")", "\n", "# 2: union the results", "\n", "all_boxes", "=", "[", "]", "\n", "all_scores", "=", "[", "]", "\n", "all_classes", "=", "[", "]", "\n", "for", "output", ",", "tfm", "in", "zip", "(", "outputs", ",", "tfms", ")", ":", "\n", "# Need to inverse the transforms on boxes, to obtain results on original image", "\n", "            ", "pred_boxes", "=", "output", ".", "pred_boxes", ".", "tensor", "\n", "original_pred_boxes", "=", "tfm", ".", "inverse", "(", ")", ".", "apply_box", "(", "pred_boxes", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "all_boxes", ".", "append", "(", "torch", ".", "from_numpy", "(", "original_pred_boxes", ")", ".", "to", "(", "pred_boxes", ".", "device", ")", ")", "\n", "\n", "all_scores", ".", "extend", "(", "output", ".", "scores", ")", "\n", "all_classes", ".", "extend", "(", "output", ".", "pred_classes", ")", "\n", "", "all_boxes", "=", "torch", ".", "cat", "(", "all_boxes", ",", "dim", "=", "0", ")", "\n", "return", "all_boxes", ",", "all_scores", ",", "all_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._merge_detections": [[262, 281], ["len", "torch.zeros", "zip", "roi_heads.fast_rcnn.fast_rcnn_inference_single_image", "itertools.count"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.fast_rcnn_inference_single_image"], ["", "def", "_merge_detections", "(", "self", ",", "all_boxes", ",", "all_scores", ",", "all_classes", ",", "shape_hw", ")", ":", "\n", "# select from the union of all results", "\n", "        ", "num_boxes", "=", "len", "(", "all_boxes", ")", "\n", "num_classes", "=", "self", ".", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NUM_CLASSES", "\n", "# +1 because fast_rcnn_inference expects background scores as well", "\n", "all_scores_2d", "=", "torch", ".", "zeros", "(", "num_boxes", ",", "num_classes", "+", "1", ",", "device", "=", "all_boxes", ".", "device", ")", "\n", "for", "idx", ",", "cls", ",", "score", "in", "zip", "(", "count", "(", ")", ",", "all_classes", ",", "all_scores", ")", ":", "\n", "            ", "all_scores_2d", "[", "idx", ",", "cls", "]", "=", "score", "\n", "\n", "", "merged_instances", ",", "_", "=", "fast_rcnn_inference_single_image", "(", "\n", "all_boxes", ",", "\n", "all_scores_2d", ",", "\n", "shape_hw", ",", "\n", "1e-8", ",", "\n", "self", ".", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NMS_THRESH_TEST", ",", "\n", "self", ".", "cfg", ".", "TEST", ".", "DETECTIONS_PER_IMAGE", ",", "\n", ")", "\n", "\n", "return", "merged_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._rescale_detected_boxes": [[282, 297], ["zip", "merged_instances.pred_boxes.tensor.cpu().numpy", "torch.from_numpy", "detectron2.structures.Instances", "augmented_instances.append", "tfm.apply_box", "merged_instances.pred_boxes.tensor.cpu", "detectron2.structures.Boxes"], "methods", ["None"], ["", "def", "_rescale_detected_boxes", "(", "self", ",", "augmented_inputs", ",", "merged_instances", ",", "tfms", ")", ":", "\n", "        ", "augmented_instances", "=", "[", "]", "\n", "for", "input", ",", "tfm", "in", "zip", "(", "augmented_inputs", ",", "tfms", ")", ":", "\n", "# Transform the target box to the augmented image's coordinate space", "\n", "            ", "pred_boxes", "=", "merged_instances", ".", "pred_boxes", ".", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "pred_boxes", "=", "torch", ".", "from_numpy", "(", "tfm", ".", "apply_box", "(", "pred_boxes", ")", ")", "\n", "\n", "aug_instances", "=", "Instances", "(", "\n", "image_size", "=", "input", "[", "\"image\"", "]", ".", "shape", "[", "1", ":", "3", "]", ",", "\n", "pred_boxes", "=", "Boxes", "(", "pred_boxes", ")", ",", "\n", "pred_classes", "=", "merged_instances", ".", "pred_classes", ",", "\n", "scores", "=", "merged_instances", ".", "scores", ",", "\n", ")", "\n", "augmented_instances", ".", "append", "(", "aug_instances", ")", "\n", "", "return", "augmented_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_time_augmentation.GeneralizedRCNNWithTTA._reduce_pred_masks": [[298, 308], ["zip", "torch.stack", "torch.mean", "any", "output.pred_masks.flip", "isinstance"], "methods", ["None"], ["", "def", "_reduce_pred_masks", "(", "self", ",", "outputs", ",", "tfms", ")", ":", "\n", "# Should apply inverse transforms on masks.", "\n", "# We assume only resize & flip are used. pred_masks is a scale-invariant", "\n", "# representation, so we handle flip specially", "\n", "        ", "for", "output", ",", "tfm", "in", "zip", "(", "outputs", ",", "tfms", ")", ":", "\n", "            ", "if", "any", "(", "isinstance", "(", "t", ",", "HFlipTransform", ")", "for", "t", "in", "tfm", ".", "transforms", ")", ":", "\n", "                ", "output", ".", "pred_masks", "=", "output", ".", "pred_masks", ".", "flip", "(", "dims", "=", "[", "3", "]", ")", "\n", "", "", "all_pred_masks", "=", "torch", ".", "stack", "(", "[", "o", ".", "pred_masks", "for", "o", "in", "outputs", "]", ",", "dim", "=", "0", ")", "\n", "avg_pred_masks", "=", "torch", ".", "mean", "(", "all_pred_masks", ",", "dim", "=", "0", ")", "\n", "return", "avg_pred_masks", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers.ROIPooler.__init__": [[104, 189], ["torch.nn.Module.__init__", "isinstance", "int", "int", "len", "isinstance", "isinstance", "torch.nn.ModuleList", "math.log2", "math.log2", "math.isclose", "math.isclose", "len", "torch.nn.ModuleList", "int", "int", "detectron2.layers.ROIAlign", "torch.nn.ModuleList", "detectron2.layers.ROIAlign", "torch.nn.ModuleList", "ValueError", "torchvision.ops.RoIPool", "detectron2.layers.ROIAlignRotated"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "output_size", ",", "\n", "scales", ",", "\n", "sampling_ratio", ",", "\n", "pooler_type", ",", "\n", "canonical_box_size", "=", "224", ",", "\n", "canonical_level", "=", "4", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            output_size (int, tuple[int] or list[int]): output size of the pooled region,\n                e.g., 14 x 14. If tuple or list is given, the length must be 2.\n            scales (list[float]): The scale for each low-level pooling op relative to\n                the input image. For a feature map with stride s relative to the input\n                image, scale is defined as 1/s. The stride must be power of 2.\n                When there are multiple scales, they must form a pyramid, i.e. they must be\n                a monotically decreasing geometric sequence with a factor of 1/2.\n            sampling_ratio (int): The `sampling_ratio` parameter for the ROIAlign op.\n            pooler_type (string): Name of the type of pooling operation that should be applied.\n                For instance, \"ROIPool\" or \"ROIAlignV2\".\n            canonical_box_size (int): A canonical box size in pixels (sqrt(box area)). The default\n                is heuristically defined as 224 pixels in the FPN paper (based on ImageNet\n                pre-training).\n            canonical_level (int): The feature map level index from which a canonically-sized box\n                should be placed. The default is defined as level 4 (stride=16) in the FPN paper,\n                i.e., a box of size 224x224 will be placed on the feature with stride=16.\n                The box placement for all boxes will be determined from their sizes w.r.t\n                canonical_box_size. For example, a box whose area is 4x that of a canonical box\n                should be used to pool features from feature level ``canonical_level+1``.\n\n                Note that the actual input feature maps given to this module may not have\n                sufficiently many levels for the input boxes. If the boxes are too large or too\n                small for the input feature maps, the closest level will be used.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "isinstance", "(", "output_size", ",", "int", ")", ":", "\n", "            ", "output_size", "=", "(", "output_size", ",", "output_size", ")", "\n", "", "assert", "len", "(", "output_size", ")", "==", "2", "\n", "assert", "isinstance", "(", "output_size", "[", "0", "]", ",", "int", ")", "and", "isinstance", "(", "output_size", "[", "1", "]", ",", "int", ")", "\n", "self", ".", "output_size", "=", "output_size", "\n", "\n", "if", "pooler_type", "==", "\"ROIAlign\"", ":", "\n", "            ", "self", ".", "level_poolers", "=", "nn", ".", "ModuleList", "(", "\n", "ROIAlign", "(", "\n", "output_size", ",", "spatial_scale", "=", "scale", ",", "sampling_ratio", "=", "sampling_ratio", ",", "aligned", "=", "False", "\n", ")", "\n", "for", "scale", "in", "scales", "\n", ")", "\n", "", "elif", "pooler_type", "==", "\"ROIAlignV2\"", ":", "\n", "            ", "self", ".", "level_poolers", "=", "nn", ".", "ModuleList", "(", "\n", "ROIAlign", "(", "\n", "output_size", ",", "spatial_scale", "=", "scale", ",", "sampling_ratio", "=", "sampling_ratio", ",", "aligned", "=", "True", "\n", ")", "\n", "for", "scale", "in", "scales", "\n", ")", "\n", "", "elif", "pooler_type", "==", "\"ROIPool\"", ":", "\n", "            ", "self", ".", "level_poolers", "=", "nn", ".", "ModuleList", "(", "\n", "RoIPool", "(", "output_size", ",", "spatial_scale", "=", "scale", ")", "for", "scale", "in", "scales", "\n", ")", "\n", "", "elif", "pooler_type", "==", "\"ROIAlignRotated\"", ":", "\n", "            ", "self", ".", "level_poolers", "=", "nn", ".", "ModuleList", "(", "\n", "ROIAlignRotated", "(", "output_size", ",", "spatial_scale", "=", "scale", ",", "sampling_ratio", "=", "sampling_ratio", ")", "\n", "for", "scale", "in", "scales", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown pooler type: {}\"", ".", "format", "(", "pooler_type", ")", ")", "\n", "\n", "# Map scale (defined as 1 / stride) to its feature map level under the", "\n", "# assumption that stride is a power of 2.", "\n", "", "min_level", "=", "-", "(", "math", ".", "log2", "(", "scales", "[", "0", "]", ")", ")", "\n", "max_level", "=", "-", "(", "math", ".", "log2", "(", "scales", "[", "-", "1", "]", ")", ")", "\n", "assert", "math", ".", "isclose", "(", "min_level", ",", "int", "(", "min_level", ")", ")", "and", "math", ".", "isclose", "(", "\n", "max_level", ",", "int", "(", "max_level", ")", "\n", ")", ",", "\"Featuremap stride is not power of 2!\"", "\n", "self", ".", "min_level", "=", "int", "(", "min_level", ")", "\n", "self", ".", "max_level", "=", "int", "(", "max_level", ")", "\n", "assert", "(", "\n", "len", "(", "scales", ")", "==", "self", ".", "max_level", "-", "self", ".", "min_level", "+", "1", "\n", ")", ",", "\"[ROIPooler] Sizes of input featuremaps do not form a pyramid!\"", "\n", "assert", "0", "<=", "self", ".", "min_level", "and", "self", ".", "min_level", "<=", "self", ".", "max_level", "\n", "self", ".", "canonical_level", "=", "canonical_level", "\n", "assert", "canonical_box_size", ">", "0", "\n", "self", ".", "canonical_box_size", "=", "canonical_box_size", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers.ROIPooler.forward": [[190, 251], ["len", "poolers.convert_boxes_to_pooler_format", "poolers.assign_boxes_to_levels", "convert_boxes_to_pooler_format.size", "torch.zeros", "enumerate", "isinstance", "isinstance", "len", "len", "len", "x[].size", "x[].size", "len", "len", "torch.zeros", "torch.zeros.index_put_", "detectron2.layers.nonzero_tuple", "pooler"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers.convert_boxes_to_pooler_format", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers.assign_boxes_to_levels", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple"], ["", "def", "forward", "(", "self", ",", "x", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "box_lists", ":", "List", "[", "Boxes", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (list[Tensor]): A list of feature maps of NCHW shape, with scales matching those\n                used to construct this module.\n            box_lists (list[Boxes] | list[RotatedBoxes]):\n                A list of N Boxes or N RotatedBoxes, where N is the number of images in the batch.\n                The box coordinates are defined on the original image and\n                will be scaled by the `scales` argument of :class:`ROIPooler`.\n\n        Returns:\n            Tensor:\n                A tensor of shape (M, C, output_size, output_size) where M is the total number of\n                boxes aggregated over all N batch images and C is the number of channels in `x`.\n        \"\"\"", "\n", "num_level_assignments", "=", "len", "(", "self", ".", "level_poolers", ")", "\n", "\n", "assert", "isinstance", "(", "x", ",", "list", ")", "and", "isinstance", "(", "\n", "box_lists", ",", "list", "\n", ")", ",", "\"Arguments to pooler must be lists\"", "\n", "assert", "(", "\n", "len", "(", "x", ")", "==", "num_level_assignments", "\n", ")", ",", "\"unequal value, num_level_assignments={}, but x is list of {} Tensors\"", ".", "format", "(", "\n", "num_level_assignments", ",", "len", "(", "x", ")", "\n", ")", "\n", "\n", "assert", "len", "(", "box_lists", ")", "==", "x", "[", "0", "]", ".", "size", "(", "\n", "0", "\n", ")", ",", "\"unequal value, x[0] batch dim 0 is {}, but box_list has length {}\"", ".", "format", "(", "\n", "x", "[", "0", "]", ".", "size", "(", "0", ")", ",", "len", "(", "box_lists", ")", "\n", ")", "\n", "if", "len", "(", "box_lists", ")", "==", "0", ":", "\n", "            ", "return", "torch", ".", "zeros", "(", "\n", "(", "0", ",", "x", "[", "0", "]", ".", "shape", "[", "1", "]", ")", "+", "self", ".", "output_size", ",", "device", "=", "x", "[", "0", "]", ".", "device", ",", "dtype", "=", "x", "[", "0", "]", ".", "dtype", "\n", ")", "\n", "\n", "", "pooler_fmt_boxes", "=", "convert_boxes_to_pooler_format", "(", "box_lists", ")", "\n", "\n", "if", "num_level_assignments", "==", "1", ":", "\n", "            ", "return", "self", ".", "level_poolers", "[", "0", "]", "(", "x", "[", "0", "]", ",", "pooler_fmt_boxes", ")", "\n", "\n", "", "level_assignments", "=", "assign_boxes_to_levels", "(", "\n", "box_lists", ",", "self", ".", "min_level", ",", "self", ".", "max_level", ",", "self", ".", "canonical_box_size", ",", "self", ".", "canonical_level", "\n", ")", "\n", "\n", "num_boxes", "=", "pooler_fmt_boxes", ".", "size", "(", "0", ")", "\n", "num_channels", "=", "x", "[", "0", "]", ".", "shape", "[", "1", "]", "\n", "output_size", "=", "self", ".", "output_size", "[", "0", "]", "\n", "\n", "dtype", ",", "device", "=", "x", "[", "0", "]", ".", "dtype", ",", "x", "[", "0", "]", ".", "device", "\n", "output", "=", "torch", ".", "zeros", "(", "\n", "(", "num_boxes", ",", "num_channels", ",", "output_size", ",", "output_size", ")", ",", "dtype", "=", "dtype", ",", "device", "=", "device", "\n", ")", "\n", "\n", "for", "level", ",", "pooler", "in", "enumerate", "(", "self", ".", "level_poolers", ")", ":", "\n", "            ", "inds", "=", "nonzero_tuple", "(", "level_assignments", "==", "level", ")", "[", "0", "]", "\n", "pooler_fmt_boxes_level", "=", "pooler_fmt_boxes", "[", "inds", "]", "\n", "# Use index_put_ instead of advance indexing, to avoid pytorch/issues/49852", "\n", "output", ".", "index_put_", "(", "(", "inds", ",", ")", ",", "pooler", "(", "x", "[", "level", "]", ",", "pooler_fmt_boxes_level", ")", ")", "\n", "\n", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers.assign_boxes_to_levels": [[22, 59], ["torch.sqrt", "torch.floor", "torch.clamp", "detectron2.layers.cat", "torch.clamp.to", "torch.log2", "boxes.area"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area"], ["def", "assign_boxes_to_levels", "(", "\n", "box_lists", ":", "List", "[", "Boxes", "]", ",", "\n", "min_level", ":", "int", ",", "\n", "max_level", ":", "int", ",", "\n", "canonical_box_size", ":", "int", ",", "\n", "canonical_level", ":", "int", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Map each box in `box_lists` to a feature map level index and return the assignment\n    vector.\n\n    Args:\n        box_lists (list[Boxes] | list[RotatedBoxes]): A list of N Boxes or N RotatedBoxes,\n            where N is the number of images in the batch.\n        min_level (int): Smallest feature map level index. The input is considered index 0,\n            the output of stage 1 is index 1, and so.\n        max_level (int): Largest feature map level index.\n        canonical_box_size (int): A canonical box size in pixels (sqrt(box area)).\n        canonical_level (int): The feature map level index on which a canonically-sized box\n            should be placed.\n\n    Returns:\n        A tensor of length M, where M is the total number of boxes aggregated over all\n            N batch images. The memory layout corresponds to the concatenation of boxes\n            from all images. Each element is the feature map index, as an offset from\n            `self.min_level`, for the corresponding box (so value i means the box is at\n            `self.min_level + i`).\n    \"\"\"", "\n", "box_sizes", "=", "torch", ".", "sqrt", "(", "cat", "(", "[", "boxes", ".", "area", "(", ")", "for", "boxes", "in", "box_lists", "]", ")", ")", "\n", "# Eqn.(1) in FPN paper", "\n", "level_assignments", "=", "torch", ".", "floor", "(", "\n", "canonical_level", "+", "torch", ".", "log2", "(", "box_sizes", "/", "canonical_box_size", "+", "1e-8", ")", "\n", ")", "\n", "# clamp level to (min, max), in case the box size is too large or too small", "\n", "# for the available feature maps", "\n", "level_assignments", "=", "torch", ".", "clamp", "(", "level_assignments", ",", "min", "=", "min_level", ",", "max", "=", "max_level", ")", "\n", "return", "level_assignments", ".", "to", "(", "torch", ".", "int64", ")", "-", "min_level", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers._fmt_box_list": [[61, 66], ["torch.full_like", "detectron2.layers.cat"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "_fmt_box_list", "(", "box_tensor", ",", "batch_index", ":", "int", ")", ":", "\n", "    ", "repeated_index", "=", "torch", ".", "full_like", "(", "\n", "box_tensor", "[", ":", ",", ":", "1", "]", ",", "batch_index", ",", "dtype", "=", "box_tensor", ".", "dtype", ",", "device", "=", "box_tensor", ".", "device", "\n", ")", "\n", "return", "cat", "(", "(", "repeated_index", ",", "box_tensor", ")", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers.convert_boxes_to_pooler_format": [[68, 96], ["detectron2.layers.cat", "poolers._fmt_box_list", "enumerate"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers._fmt_box_list"], ["", "def", "convert_boxes_to_pooler_format", "(", "box_lists", ":", "List", "[", "Boxes", "]", ")", ":", "\n", "    ", "\"\"\"\n    Convert all boxes in `box_lists` to the low-level format used by ROI pooling ops\n    (see description under Returns).\n\n    Args:\n        box_lists (list[Boxes] | list[RotatedBoxes]):\n            A list of N Boxes or N RotatedBoxes, where N is the number of images in the batch.\n\n    Returns:\n        When input is list[Boxes]:\n            A tensor of shape (M, 5), where M is the total number of boxes aggregated over all\n            N batch images.\n            The 5 columns are (batch index, x0, y0, x1, y1), where batch index\n            is the index in [0, N) identifying which batch image the box with corners at\n            (x0, y0, x1, y1) comes from.\n        When input is list[RotatedBoxes]:\n            A tensor of shape (M, 6), where M is the total number of boxes aggregated over all\n            N batch images.\n            The 6 columns are (batch index, x_ctr, y_ctr, width, height, angle_degrees),\n            where batch index is the index in [0, N) identifying which batch image the\n            rotated box (x_ctr, y_ctr, width, height, angle_degrees) comes from.\n    \"\"\"", "\n", "pooler_fmt_boxes", "=", "cat", "(", "\n", "[", "_fmt_box_list", "(", "box_list", ".", "tensor", ",", "i", ")", "for", "i", ",", "box_list", "in", "enumerate", "(", "box_lists", ")", "]", ",", "dim", "=", "0", "\n", ")", "\n", "\n", "return", "pooler_fmt_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransform.__init__": [[27, 41], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "weights", ":", "Tuple", "[", "float", ",", "float", ",", "float", ",", "float", "]", ",", "scale_clamp", ":", "float", "=", "_DEFAULT_SCALE_CLAMP", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            weights (4-element tuple): Scaling factors that are applied to the\n                (dx, dy, dw, dh) deltas. In Fast R-CNN, these were originally set\n                such that the deltas have unit variance; now they are treated as\n                hyperparameters of the system.\n            scale_clamp (float): When predicting deltas, the predicted box scaling\n                factors (dw and dh) are clamped such that they are <= scale_clamp.\n        \"\"\"", "\n", "self", ".", "weights", "=", "weights", "\n", "self", ".", "scale_clamp", "=", "scale_clamp", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransform.get_deltas": [[42, 76], ["isinstance", "type", "isinstance", "type", "torch.stack", "torch.log", "torch.log"], "methods", ["None"], ["", "def", "get_deltas", "(", "self", ",", "src_boxes", ",", "target_boxes", ")", ":", "\n", "        ", "\"\"\"\n        Get box regression transformation deltas (dx, dy, dw, dh) that can be used\n        to transform the `src_boxes` into the `target_boxes`. That is, the relation\n        ``target_boxes == self.apply_deltas(deltas, src_boxes)`` is true (unless\n        any delta is too large and is clamped).\n\n        Args:\n            src_boxes (Tensor): source boxes, e.g., object proposals\n            target_boxes (Tensor): target of the transformation, e.g., ground-truth\n                boxes.\n        \"\"\"", "\n", "assert", "isinstance", "(", "src_boxes", ",", "torch", ".", "Tensor", ")", ",", "type", "(", "src_boxes", ")", "\n", "assert", "isinstance", "(", "target_boxes", ",", "torch", ".", "Tensor", ")", ",", "type", "(", "target_boxes", ")", "\n", "\n", "src_widths", "=", "src_boxes", "[", ":", ",", "2", "]", "-", "src_boxes", "[", ":", ",", "0", "]", "\n", "src_heights", "=", "src_boxes", "[", ":", ",", "3", "]", "-", "src_boxes", "[", ":", ",", "1", "]", "\n", "src_ctr_x", "=", "src_boxes", "[", ":", ",", "0", "]", "+", "0.5", "*", "src_widths", "\n", "src_ctr_y", "=", "src_boxes", "[", ":", ",", "1", "]", "+", "0.5", "*", "src_heights", "\n", "\n", "target_widths", "=", "target_boxes", "[", ":", ",", "2", "]", "-", "target_boxes", "[", ":", ",", "0", "]", "\n", "target_heights", "=", "target_boxes", "[", ":", ",", "3", "]", "-", "target_boxes", "[", ":", ",", "1", "]", "\n", "target_ctr_x", "=", "target_boxes", "[", ":", ",", "0", "]", "+", "0.5", "*", "target_widths", "\n", "target_ctr_y", "=", "target_boxes", "[", ":", ",", "1", "]", "+", "0.5", "*", "target_heights", "\n", "\n", "wx", ",", "wy", ",", "ww", ",", "wh", "=", "self", ".", "weights", "\n", "dx", "=", "wx", "*", "(", "target_ctr_x", "-", "src_ctr_x", ")", "/", "src_widths", "\n", "dy", "=", "wy", "*", "(", "target_ctr_y", "-", "src_ctr_y", ")", "/", "src_heights", "\n", "dw", "=", "ww", "*", "torch", ".", "log", "(", "target_widths", "/", "src_widths", ")", "\n", "dh", "=", "wh", "*", "torch", ".", "log", "(", "target_heights", "/", "src_heights", ")", "\n", "\n", "deltas", "=", "torch", ".", "stack", "(", "(", "dx", ",", "dy", ",", "dw", ",", "dh", ")", ",", "dim", "=", "1", ")", "\n", "assert", "(", "src_widths", ">", "0", ")", ".", "all", "(", ")", ".", "item", "(", ")", ",", "\"Input boxes to Box2BoxTransform are not valid!\"", "\n", "return", "deltas", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransform.apply_deltas": [[77, 116], ["deltas.float.float.float", "boxes.to.to.to", "torch.clamp", "torch.clamp", "torch.stack", "torch.stack.reshape", "torch.exp", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "apply_deltas", "(", "self", ",", "deltas", ",", "boxes", ")", ":", "\n", "        ", "\"\"\"\n        Apply transformation `deltas` (dx, dy, dw, dh) to `boxes`.\n\n        Args:\n            deltas (Tensor): transformation deltas of shape (N, k*4), where k >= 1.\n                deltas[i] represents k potentially different class-specific\n                box transformations for the single box boxes[i].\n            boxes (Tensor): boxes to transform, of shape (N, 4)\n        \"\"\"", "\n", "deltas", "=", "deltas", ".", "float", "(", ")", "# ensure fp32 for decoding precision", "\n", "boxes", "=", "boxes", ".", "to", "(", "deltas", ".", "dtype", ")", "\n", "\n", "widths", "=", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", "\n", "heights", "=", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", "\n", "ctr_x", "=", "boxes", "[", ":", ",", "0", "]", "+", "0.5", "*", "widths", "\n", "ctr_y", "=", "boxes", "[", ":", ",", "1", "]", "+", "0.5", "*", "heights", "\n", "\n", "wx", ",", "wy", ",", "ww", ",", "wh", "=", "self", ".", "weights", "\n", "dx", "=", "deltas", "[", ":", ",", "0", ":", ":", "4", "]", "/", "wx", "\n", "dy", "=", "deltas", "[", ":", ",", "1", ":", ":", "4", "]", "/", "wy", "\n", "dw", "=", "deltas", "[", ":", ",", "2", ":", ":", "4", "]", "/", "ww", "\n", "dh", "=", "deltas", "[", ":", ",", "3", ":", ":", "4", "]", "/", "wh", "\n", "\n", "# Prevent sending too large values into torch.exp()", "\n", "dw", "=", "torch", ".", "clamp", "(", "dw", ",", "max", "=", "self", ".", "scale_clamp", ")", "\n", "dh", "=", "torch", ".", "clamp", "(", "dh", ",", "max", "=", "self", ".", "scale_clamp", ")", "\n", "\n", "pred_ctr_x", "=", "dx", "*", "widths", "[", ":", ",", "None", "]", "+", "ctr_x", "[", ":", ",", "None", "]", "\n", "pred_ctr_y", "=", "dy", "*", "heights", "[", ":", ",", "None", "]", "+", "ctr_y", "[", ":", ",", "None", "]", "\n", "pred_w", "=", "torch", ".", "exp", "(", "dw", ")", "*", "widths", "[", ":", ",", "None", "]", "\n", "pred_h", "=", "torch", ".", "exp", "(", "dh", ")", "*", "heights", "[", ":", ",", "None", "]", "\n", "\n", "x1", "=", "pred_ctr_x", "-", "0.5", "*", "pred_w", "\n", "y1", "=", "pred_ctr_y", "-", "0.5", "*", "pred_h", "\n", "x2", "=", "pred_ctr_x", "+", "0.5", "*", "pred_w", "\n", "y2", "=", "pred_ctr_y", "+", "0.5", "*", "pred_h", "\n", "pred_boxes", "=", "torch", ".", "stack", "(", "(", "x1", ",", "y1", ",", "x2", ",", "y2", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "pred_boxes", ".", "reshape", "(", "deltas", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.__init__": [[128, 143], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "weights", ":", "Tuple", "[", "float", ",", "float", ",", "float", ",", "float", ",", "float", "]", ",", "\n", "scale_clamp", ":", "float", "=", "_DEFAULT_SCALE_CLAMP", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            weights (5-element tuple): Scaling factors that are applied to the\n                (dx, dy, dw, dh, da) deltas. These are treated as\n                hyperparameters of the system.\n            scale_clamp (float): When predicting deltas, the predicted box scaling\n                factors (dw and dh) are clamped such that they are <= scale_clamp.\n        \"\"\"", "\n", "self", ".", "weights", "=", "weights", "\n", "self", ".", "scale_clamp", "=", "scale_clamp", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.get_deltas": [[144, 181], ["isinstance", "type", "isinstance", "type", "torch.unbind", "torch.unbind", "torch.stack", "torch.log", "torch.log"], "methods", ["None"], ["", "def", "get_deltas", "(", "self", ",", "src_boxes", ",", "target_boxes", ")", ":", "\n", "        ", "\"\"\"\n        Get box regression transformation deltas (dx, dy, dw, dh, da) that can be used\n        to transform the `src_boxes` into the `target_boxes`. That is, the relation\n        ``target_boxes == self.apply_deltas(deltas, src_boxes)`` is true (unless\n        any delta is too large and is clamped).\n\n        Args:\n            src_boxes (Tensor): Nx5 source boxes, e.g., object proposals\n            target_boxes (Tensor): Nx5 target of the transformation, e.g., ground-truth\n                boxes.\n        \"\"\"", "\n", "assert", "isinstance", "(", "src_boxes", ",", "torch", ".", "Tensor", ")", ",", "type", "(", "src_boxes", ")", "\n", "assert", "isinstance", "(", "target_boxes", ",", "torch", ".", "Tensor", ")", ",", "type", "(", "target_boxes", ")", "\n", "\n", "src_ctr_x", ",", "src_ctr_y", ",", "src_widths", ",", "src_heights", ",", "src_angles", "=", "torch", ".", "unbind", "(", "src_boxes", ",", "dim", "=", "1", ")", "\n", "\n", "target_ctr_x", ",", "target_ctr_y", ",", "target_widths", ",", "target_heights", ",", "target_angles", "=", "torch", ".", "unbind", "(", "\n", "target_boxes", ",", "dim", "=", "1", "\n", ")", "\n", "\n", "wx", ",", "wy", ",", "ww", ",", "wh", ",", "wa", "=", "self", ".", "weights", "\n", "dx", "=", "wx", "*", "(", "target_ctr_x", "-", "src_ctr_x", ")", "/", "src_widths", "\n", "dy", "=", "wy", "*", "(", "target_ctr_y", "-", "src_ctr_y", ")", "/", "src_heights", "\n", "dw", "=", "ww", "*", "torch", ".", "log", "(", "target_widths", "/", "src_widths", ")", "\n", "dh", "=", "wh", "*", "torch", ".", "log", "(", "target_heights", "/", "src_heights", ")", "\n", "# Angles of deltas are in radians while angles of boxes are in degrees.", "\n", "# the conversion to radians serve as a way to normalize the values", "\n", "da", "=", "target_angles", "-", "src_angles", "\n", "da", "=", "(", "da", "+", "180.0", ")", "%", "360.0", "-", "180.0", "# make it in [-180, 180)", "\n", "da", "*=", "wa", "*", "math", ".", "pi", "/", "180.0", "\n", "\n", "deltas", "=", "torch", ".", "stack", "(", "(", "dx", ",", "dy", ",", "dw", ",", "dh", ",", "da", ")", ",", "dim", "=", "1", ")", "\n", "assert", "(", "\n", "(", "src_widths", ">", "0", ")", ".", "all", "(", ")", ".", "item", "(", ")", "\n", ")", ",", "\"Input boxes to Box2BoxTransformRotated are not valid!\"", "\n", "return", "deltas", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas": [[182, 227], ["boxes.to().unsqueeze.to().unsqueeze.to().unsqueeze", "torch.clamp", "torch.clamp", "torch.zeros_like", "torch.exp", "torch.exp", "boxes.to().unsqueeze.to().unsqueeze.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "apply_deltas", "(", "self", ",", "deltas", ",", "boxes", ")", ":", "\n", "        ", "\"\"\"\n        Apply transformation `deltas` (dx, dy, dw, dh, da) to `boxes`.\n\n        Args:\n            deltas (Tensor): transformation deltas of shape (N, k*5).\n                deltas[i] represents box transformation for the single box boxes[i].\n            boxes (Tensor): boxes to transform, of shape (N, 5)\n        \"\"\"", "\n", "assert", "deltas", ".", "shape", "[", "1", "]", "%", "5", "==", "0", "and", "boxes", ".", "shape", "[", "1", "]", "==", "5", "\n", "\n", "boxes", "=", "boxes", ".", "to", "(", "deltas", ".", "dtype", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "ctr_x", "=", "boxes", "[", ":", ",", "0", "]", "\n", "ctr_y", "=", "boxes", "[", ":", ",", "1", "]", "\n", "widths", "=", "boxes", "[", ":", ",", "2", "]", "\n", "heights", "=", "boxes", "[", ":", ",", "3", "]", "\n", "angles", "=", "boxes", "[", ":", ",", "4", "]", "\n", "\n", "wx", ",", "wy", ",", "ww", ",", "wh", ",", "wa", "=", "self", ".", "weights", "\n", "\n", "dx", "=", "deltas", "[", ":", ",", "0", ":", ":", "5", "]", "/", "wx", "\n", "dy", "=", "deltas", "[", ":", ",", "1", ":", ":", "5", "]", "/", "wy", "\n", "dw", "=", "deltas", "[", ":", ",", "2", ":", ":", "5", "]", "/", "ww", "\n", "dh", "=", "deltas", "[", ":", ",", "3", ":", ":", "5", "]", "/", "wh", "\n", "da", "=", "deltas", "[", ":", ",", "4", ":", ":", "5", "]", "/", "wa", "\n", "\n", "# Prevent sending too large values into torch.exp()", "\n", "dw", "=", "torch", ".", "clamp", "(", "dw", ",", "max", "=", "self", ".", "scale_clamp", ")", "\n", "dh", "=", "torch", ".", "clamp", "(", "dh", ",", "max", "=", "self", ".", "scale_clamp", ")", "\n", "\n", "pred_boxes", "=", "torch", ".", "zeros_like", "(", "deltas", ")", "\n", "pred_boxes", "[", ":", ",", "0", ":", ":", "5", "]", "=", "dx", "*", "widths", "+", "ctr_x", "# x_ctr", "\n", "pred_boxes", "[", ":", ",", "1", ":", ":", "5", "]", "=", "dy", "*", "heights", "+", "ctr_y", "# y_ctr", "\n", "pred_boxes", "[", ":", ",", "2", ":", ":", "5", "]", "=", "torch", ".", "exp", "(", "dw", ")", "*", "widths", "# width", "\n", "pred_boxes", "[", ":", ",", "3", ":", ":", "5", "]", "=", "torch", ".", "exp", "(", "dh", ")", "*", "heights", "# height", "\n", "\n", "# Following original RRPN implementation,", "\n", "# angles of deltas are in radians while angles of boxes are in degrees.", "\n", "pred_angle", "=", "da", "*", "180.0", "/", "math", ".", "pi", "+", "angles", "\n", "pred_angle", "=", "(", "pred_angle", "+", "180.0", ")", "%", "360.0", "-", "180.0", "# make it in [-180, 180)", "\n", "\n", "pred_boxes", "[", ":", ",", "4", ":", ":", "5", "]", "=", "pred_angle", "\n", "\n", "return", "pred_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression._dense_box_regression_loss": [[229, 271], ["type().cat", "torch.stack", "fvcore.nn.smooth_l1_loss", "box2box_transform.get_deltas", "fvcore.nn.giou_loss", "ValueError", "type", "detectron2.layers.cat", "box2box_transform.apply_deltas", "detectron2.layers.cat", "torch.stack", "torch.stack"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.get_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "", "def", "_dense_box_regression_loss", "(", "\n", "anchors", ":", "List", "[", "Boxes", "]", ",", "\n", "box2box_transform", ":", "Box2BoxTransform", ",", "\n", "pred_anchor_deltas", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "gt_boxes", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "fg_mask", ":", "torch", ".", "Tensor", ",", "\n", "box_reg_loss_type", "=", "\"smooth_l1\"", ",", "\n", "smooth_l1_beta", "=", "0.0", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Compute loss for dense multi-level box regression.\n    Loss is accumulated over ``fg_mask``.\n\n    Args:\n        anchors: #lvl anchor boxes, each is (HixWixA, 4)\n        pred_anchor_deltas: #lvl predictions, each is (N, HixWixA, 4)\n        gt_boxes: N ground truth boxes, each has shape (R, 4) (R = sum(Hi * Wi * A))\n        fg_mask: the foreground boolean mask of shape (N, R) to compute loss on\n        box_reg_loss_type (str): Loss type to use. Supported losses: \"smooth_l1\", \"giou\".\n        smooth_l1_beta (float): beta parameter for the smooth L1 regression loss. Default to\n            use L1 loss. Only used when `box_reg_loss_type` is \"smooth_l1\"\n    \"\"\"", "\n", "anchors", "=", "type", "(", "anchors", "[", "0", "]", ")", ".", "cat", "(", "anchors", ")", ".", "tensor", "# (R, 4)", "\n", "if", "box_reg_loss_type", "==", "\"smooth_l1\"", ":", "\n", "        ", "gt_anchor_deltas", "=", "[", "box2box_transform", ".", "get_deltas", "(", "anchors", ",", "k", ")", "for", "k", "in", "gt_boxes", "]", "\n", "gt_anchor_deltas", "=", "torch", ".", "stack", "(", "gt_anchor_deltas", ")", "# (N, R, 4)", "\n", "loss_box_reg", "=", "smooth_l1_loss", "(", "\n", "cat", "(", "pred_anchor_deltas", ",", "dim", "=", "1", ")", "[", "fg_mask", "]", ",", "\n", "gt_anchor_deltas", "[", "fg_mask", "]", ",", "\n", "beta", "=", "smooth_l1_beta", ",", "\n", "reduction", "=", "\"sum\"", ",", "\n", ")", "\n", "", "elif", "box_reg_loss_type", "==", "\"giou\"", ":", "\n", "        ", "pred_boxes", "=", "[", "\n", "box2box_transform", ".", "apply_deltas", "(", "k", ",", "anchors", ")", "for", "k", "in", "cat", "(", "pred_anchor_deltas", ",", "dim", "=", "1", ")", "\n", "]", "\n", "loss_box_reg", "=", "giou_loss", "(", "\n", "torch", ".", "stack", "(", "pred_boxes", ")", "[", "fg_mask", "]", ",", "torch", ".", "stack", "(", "gt_boxes", ")", "[", "fg_mask", "]", ",", "reduction", "=", "\"sum\"", "\n", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Invalid dense box regression loss type '{box_reg_loss_type}'\"", ")", "\n", "", "return", "loss_box_reg", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.sampling.subsample_labels": [[9, 55], ["int", "min", "min", "[].to", "detectron2.layers.nonzero_tuple", "detectron2.layers.nonzero_tuple", "positive.numel", "negative.numel", "torch.randperm", "positive.numel", "torch.randperm", "negative.numel"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple"], ["def", "subsample_labels", "(", "\n", "labels", ":", "torch", ".", "Tensor", ",", "num_samples", ":", "int", ",", "positive_fraction", ":", "float", ",", "bg_label", ":", "int", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Return `num_samples` (or fewer, if not enough found)\n    random samples from `labels` which is a mixture of positives & negatives.\n    It will try to return as many positives as possible without\n    exceeding `positive_fraction * num_samples`, and then try to\n    fill the remaining slots with negatives.\n\n    Args:\n        labels (Tensor): (N, ) label vector with values:\n            * -1: ignore\n            * bg_label: background (\"negative\") class\n            * otherwise: one or more foreground (\"positive\") classes\n        num_samples (int): The total number of labels with value >= 0 to return.\n            Values that are not sampled will be filled with -1 (ignore).\n        positive_fraction (float): The number of subsampled labels with values > 0\n            is `min(num_positives, int(positive_fraction * num_samples))`. The number\n            of negatives sampled is `min(num_negatives, num_samples - num_positives_sampled)`.\n            In order words, if there are not enough positives, the sample is filled with\n            negatives. If there are also not enough negatives, then as many elements are\n            sampled as is possible.\n        bg_label (int): label index of background (\"negative\") class.\n\n    Returns:\n        pos_idx, neg_idx (Tensor):\n            1D vector of indices. The total length of both is `num_samples` or fewer.\n    \"\"\"", "\n", "positive", "=", "nonzero_tuple", "(", "(", "labels", "!=", "-", "1", ")", "&", "(", "labels", "!=", "bg_label", ")", ")", "[", "0", "]", "\n", "negative", "=", "nonzero_tuple", "(", "labels", "==", "bg_label", ")", "[", "0", "]", "\n", "\n", "num_pos", "=", "int", "(", "num_samples", "*", "positive_fraction", ")", "\n", "# protect against not enough positive examples", "\n", "num_pos", "=", "min", "(", "positive", ".", "numel", "(", ")", ",", "num_pos", ")", "\n", "num_neg", "=", "num_samples", "-", "num_pos", "\n", "# protect against not enough negative examples", "\n", "num_neg", "=", "min", "(", "negative", ".", "numel", "(", ")", ",", "num_neg", ")", "\n", "\n", "# randomly select positive and negative examples", "\n", "perm1", "=", "torch", ".", "randperm", "(", "positive", ".", "numel", "(", ")", ",", "device", "=", "positive", ".", "device", ")", "[", ":", "num_pos", "]", "\n", "perm2", "=", "torch", ".", "randperm", "(", "negative", ".", "numel", "(", ")", ")", "[", ":", "num_neg", "]", ".", "to", "(", "negative", ".", "device", ")", "# torch.randperm(negative.numel(), device=negative.device)[:num_neg]", "\n", "\n", "pos_idx", "=", "positive", "[", "perm1", "]", "\n", "neg_idx", "=", "negative", "[", "perm2", "]", "\n", "return", "pos_idx", ",", "neg_idx", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_backbone.TestBackBone.test_resnet_scriptability": [[16, 27], ["unittest.skipIf", "detectron2.config.get_cfg", "detectron2.modeling.backbone.build_resnet_backbone", "torch.jit.script", "torch.rand", "test_backbone.TestBackBone.assertTrue", "detectron2.layers.ShapeSpec", "detectron2.modeling.backbone.build_resnet_backbone.", "torch.jit.script.", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.build_resnet_backbone"], ["    ", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_resnet_scriptability", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "resnet", "=", "build_resnet_backbone", "(", "cfg", ",", "ShapeSpec", "(", "channels", "=", "3", ")", ")", "\n", "\n", "scripted_resnet", "=", "torch", ".", "jit", ".", "script", "(", "resnet", ")", "\n", "\n", "inp", "=", "torch", ".", "rand", "(", "2", ",", "3", ",", "100", ",", "100", ")", "\n", "out1", "=", "resnet", "(", "inp", ")", "[", "\"res4\"", "]", "\n", "out2", "=", "scripted_resnet", "(", "inp", ")", "[", "\"res4\"", "]", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "out1", ",", "out2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_backbone.TestBackBone.test_fpn_scriptability": [[28, 38], ["unittest.skipIf", "detectron2.model_zoo.get_config", "detectron2.modeling.backbone.fpn.build_resnet_fpn_backbone", "torch.jit.script", "torch.rand", "test_backbone.TestBackBone.assertTrue", "detectron2.layers.ShapeSpec", "detectron2.modeling.backbone.fpn.build_resnet_fpn_backbone.", "torch.jit.script.", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.build_resnet_fpn_backbone"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_fpn_scriptability", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "model_zoo", ".", "get_config", "(", "\"Misc/scratch_mask_rcnn_R_50_FPN_3x_gn.yaml\"", ")", "\n", "bb", "=", "build_resnet_fpn_backbone", "(", "cfg", ",", "ShapeSpec", "(", "channels", "=", "3", ")", ")", "\n", "bb_s", "=", "torch", ".", "jit", ".", "script", "(", "bb", ")", "\n", "\n", "inp", "=", "torch", ".", "rand", "(", "2", ",", "3", ",", "128", ",", "128", ")", "\n", "out1", "=", "bb", "(", "inp", ")", "[", "\"p5\"", "]", "\n", "out2", "=", "bb_s", "(", "inp", ")", "[", "\"p5\"", "]", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "out1", ",", "out2", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_mmdet.TestMMDetWrapper.test_backbone": [[16, 41], ["detectron2.modeling.mmdet_wrapper.MMDetBackbone", "dict", "dict", "detectron2.layers.ShapeSpec", "dict", "dict", "dict"], "methods", ["None"], ["    ", "def", "test_backbone", "(", "self", ")", ":", "\n", "        ", "MMDetBackbone", "(", "\n", "backbone", "=", "dict", "(", "\n", "type", "=", "\"DetectoRS_ResNet\"", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "\"ConvAWS\"", ")", ",", "\n", "sac", "=", "dict", "(", "type", "=", "\"SAC\"", ",", "use_deform", "=", "True", ")", ",", "\n", "stage_with_sac", "=", "(", "False", ",", "True", ",", "True", ",", "True", ")", ",", "\n", "depth", "=", "50", ",", "\n", "num_stages", "=", "4", ",", "\n", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ")", ",", "\n", "frozen_stages", "=", "1", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "\"BN\"", ",", "requires_grad", "=", "True", ")", ",", "\n", "norm_eval", "=", "True", ",", "\n", "style", "=", "\"pytorch\"", ",", "\n", ")", ",", "\n", "neck", "=", "dict", "(", "\n", "type", "=", "\"FPN\"", ",", "\n", "in_channels", "=", "[", "256", ",", "512", ",", "1024", ",", "2048", "]", ",", "\n", "out_channels", "=", "256", ",", "\n", "num_outs", "=", "5", ",", "\n", ")", ",", "\n", "# skip pretrained model for tests", "\n", "# pretrained_backbone=\"torchvision://resnet50\",", "\n", "output_shapes", "=", "[", "ShapeSpec", "(", "channels", "=", "256", ",", "stride", "=", "s", ")", "for", "s", "in", "[", "4", ",", "8", ",", "16", ",", "32", ",", "64", "]", "]", ",", "\n", "output_names", "=", "[", "\"p2\"", ",", "\"p3\"", ",", "\"p4\"", ",", "\"p5\"", ",", "\"p6\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_mmdet.TestMMDetWrapper.test_detector": [[43, 185], ["detectron2.modeling.mmdet_wrapper.MMDetDetector", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict"], "methods", ["None"], ["", "def", "test_detector", "(", "self", ")", ":", "\n", "# a basic R50 Mask R-CNN", "\n", "        ", "MMDetDetector", "(", "\n", "detector", "=", "dict", "(", "\n", "type", "=", "\"MaskRCNN\"", ",", "\n", "# skip pretrained model for tests", "\n", "# pretrained=\"torchvision://resnet50\",", "\n", "backbone", "=", "dict", "(", "\n", "type", "=", "\"ResNet\"", ",", "\n", "depth", "=", "50", ",", "\n", "num_stages", "=", "4", ",", "\n", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ")", ",", "\n", "frozen_stages", "=", "1", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "\"BN\"", ",", "requires_grad", "=", "True", ")", ",", "\n", "norm_eval", "=", "True", ",", "\n", "style", "=", "\"pytorch\"", ",", "\n", ")", ",", "\n", "neck", "=", "dict", "(", "\n", "type", "=", "\"FPN\"", ",", "in_channels", "=", "[", "256", ",", "512", ",", "1024", ",", "2048", "]", ",", "out_channels", "=", "256", ",", "num_outs", "=", "5", "\n", ")", ",", "\n", "rpn_head", "=", "dict", "(", "\n", "type", "=", "\"RPNHead\"", ",", "\n", "in_channels", "=", "256", ",", "\n", "feat_channels", "=", "256", ",", "\n", "anchor_generator", "=", "dict", "(", "\n", "type", "=", "\"AnchorGenerator\"", ",", "\n", "scales", "=", "[", "8", "]", ",", "\n", "ratios", "=", "[", "0.5", ",", "1.0", ",", "2.0", "]", ",", "\n", "strides", "=", "[", "4", ",", "8", ",", "16", ",", "32", ",", "64", "]", ",", "\n", ")", ",", "\n", "bbox_coder", "=", "dict", "(", "\n", "type", "=", "\"DeltaXYWHBBoxCoder\"", ",", "\n", "target_means", "=", "[", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "]", ",", "\n", "target_stds", "=", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", "]", ",", "\n", ")", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "\"CrossEntropyLoss\"", ",", "use_sigmoid", "=", "True", ",", "loss_weight", "=", "1.0", ")", ",", "\n", "loss_bbox", "=", "dict", "(", "type", "=", "\"L1Loss\"", ",", "loss_weight", "=", "1.0", ")", ",", "\n", ")", ",", "\n", "roi_head", "=", "dict", "(", "\n", "type", "=", "\"StandardRoIHead\"", ",", "\n", "bbox_roi_extractor", "=", "dict", "(", "\n", "type", "=", "\"SingleRoIExtractor\"", ",", "\n", "roi_layer", "=", "dict", "(", "type", "=", "\"RoIAlign\"", ",", "output_size", "=", "7", ",", "sampling_ratio", "=", "0", ")", ",", "\n", "out_channels", "=", "256", ",", "\n", "featmap_strides", "=", "[", "4", ",", "8", ",", "16", ",", "32", "]", ",", "\n", ")", ",", "\n", "bbox_head", "=", "dict", "(", "\n", "type", "=", "\"Shared2FCBBoxHead\"", ",", "\n", "in_channels", "=", "256", ",", "\n", "fc_out_channels", "=", "1024", ",", "\n", "roi_feat_size", "=", "7", ",", "\n", "num_classes", "=", "80", ",", "\n", "bbox_coder", "=", "dict", "(", "\n", "type", "=", "\"DeltaXYWHBBoxCoder\"", ",", "\n", "target_means", "=", "[", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "]", ",", "\n", "target_stds", "=", "[", "0.1", ",", "0.1", ",", "0.2", ",", "0.2", "]", ",", "\n", ")", ",", "\n", "reg_class_agnostic", "=", "False", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "\"CrossEntropyLoss\"", ",", "use_sigmoid", "=", "False", ",", "loss_weight", "=", "1.0", ")", ",", "\n", "loss_bbox", "=", "dict", "(", "type", "=", "\"L1Loss\"", ",", "loss_weight", "=", "1.0", ")", ",", "\n", ")", ",", "\n", "mask_roi_extractor", "=", "dict", "(", "\n", "type", "=", "\"SingleRoIExtractor\"", ",", "\n", "roi_layer", "=", "dict", "(", "type", "=", "\"RoIAlign\"", ",", "output_size", "=", "14", ",", "sampling_ratio", "=", "0", ")", ",", "\n", "out_channels", "=", "256", ",", "\n", "featmap_strides", "=", "[", "4", ",", "8", ",", "16", ",", "32", "]", ",", "\n", ")", ",", "\n", "mask_head", "=", "dict", "(", "\n", "type", "=", "\"FCNMaskHead\"", ",", "\n", "num_convs", "=", "4", ",", "\n", "in_channels", "=", "256", ",", "\n", "conv_out_channels", "=", "256", ",", "\n", "num_classes", "=", "80", ",", "\n", "loss_mask", "=", "dict", "(", "type", "=", "\"CrossEntropyLoss\"", ",", "use_mask", "=", "True", ",", "loss_weight", "=", "1.0", ")", ",", "\n", ")", ",", "\n", ")", ",", "\n", "# model training and testing settings", "\n", "train_cfg", "=", "dict", "(", "\n", "rpn", "=", "dict", "(", "\n", "assigner", "=", "dict", "(", "\n", "type", "=", "\"MaxIoUAssigner\"", ",", "\n", "pos_iou_thr", "=", "0.7", ",", "\n", "neg_iou_thr", "=", "0.3", ",", "\n", "min_pos_iou", "=", "0.3", ",", "\n", "match_low_quality", "=", "True", ",", "\n", "ignore_iof_thr", "=", "-", "1", ",", "\n", ")", ",", "\n", "sampler", "=", "dict", "(", "\n", "type", "=", "\"RandomSampler\"", ",", "\n", "num", "=", "256", ",", "\n", "pos_fraction", "=", "0.5", ",", "\n", "neg_pos_ub", "=", "-", "1", ",", "\n", "add_gt_as_proposals", "=", "False", ",", "\n", ")", ",", "\n", "allowed_border", "=", "-", "1", ",", "\n", "pos_weight", "=", "-", "1", ",", "\n", "debug", "=", "False", ",", "\n", ")", ",", "\n", "rpn_proposal", "=", "dict", "(", "\n", "nms_pre", "=", "2000", ",", "\n", "max_per_img", "=", "1000", ",", "\n", "nms", "=", "dict", "(", "type", "=", "\"nms\"", ",", "iou_threshold", "=", "0.7", ")", ",", "\n", "min_bbox_size", "=", "0", ",", "\n", ")", ",", "\n", "rcnn", "=", "dict", "(", "\n", "assigner", "=", "dict", "(", "\n", "type", "=", "\"MaxIoUAssigner\"", ",", "\n", "pos_iou_thr", "=", "0.5", ",", "\n", "neg_iou_thr", "=", "0.5", ",", "\n", "min_pos_iou", "=", "0.5", ",", "\n", "match_low_quality", "=", "True", ",", "\n", "ignore_iof_thr", "=", "-", "1", ",", "\n", ")", ",", "\n", "sampler", "=", "dict", "(", "\n", "type", "=", "\"RandomSampler\"", ",", "\n", "num", "=", "512", ",", "\n", "pos_fraction", "=", "0.25", ",", "\n", "neg_pos_ub", "=", "-", "1", ",", "\n", "add_gt_as_proposals", "=", "True", ",", "\n", ")", ",", "\n", "mask_size", "=", "28", ",", "\n", "pos_weight", "=", "-", "1", ",", "\n", "debug", "=", "False", ",", "\n", ")", ",", "\n", ")", ",", "\n", "test_cfg", "=", "dict", "(", "\n", "rpn", "=", "dict", "(", "\n", "nms_pre", "=", "1000", ",", "\n", "max_per_img", "=", "1000", ",", "\n", "nms", "=", "dict", "(", "type", "=", "\"nms\"", ",", "iou_threshold", "=", "0.7", ")", ",", "\n", "min_bbox_size", "=", "0", ",", "\n", ")", ",", "\n", "rcnn", "=", "dict", "(", "\n", "score_thr", "=", "0.05", ",", "\n", "nms", "=", "dict", "(", "type", "=", "\"nms\"", ",", "iou_threshold", "=", "0.5", ")", ",", "\n", "max_per_img", "=", "100", ",", "\n", "mask_thr_binary", "=", "0.5", ",", "\n", ")", ",", "\n", ")", ",", "\n", ")", ",", "\n", "pixel_mean", "=", "[", "1", ",", "2", ",", "3", "]", ",", "\n", "pixel_std", "=", "[", "1", ",", "2", ",", "3", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler._test_roialignv2_roialignrotated_match": [[15, 61], ["range", "detectron2.modeling.poolers.ROIPooler", "detectron2.modeling.poolers.ROIPooler.", "detectron2.modeling.poolers.ROIPooler", "detectron2.modeling.poolers.ROIPooler.", "test_roi_pooler.TestROIPooler.assertTrue", "feature.to", "detectron2.utils.testing.random_boxes", "torch.zeros", "rois.append", "rois_rotated.append", "torch.allclose", "detectron2.structures.Boxes().to", "detectron2.structures.RotatedBoxes().to", "torch.rand", "detectron2.structures.Boxes", "detectron2.structures.RotatedBoxes"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["    ", "def", "_test_roialignv2_roialignrotated_match", "(", "self", ",", "device", ")", ":", "\n", "        ", "pooler_resolution", "=", "14", "\n", "canonical_level", "=", "4", "\n", "canonical_scale_factor", "=", "2", "**", "canonical_level", "\n", "pooler_scales", "=", "(", "1.0", "/", "canonical_scale_factor", ",", ")", "\n", "sampling_ratio", "=", "0", "\n", "\n", "N", ",", "C", ",", "H", ",", "W", "=", "2", ",", "4", ",", "10", ",", "8", "\n", "N_rois", "=", "10", "\n", "std", "=", "11", "\n", "mean", "=", "0", "\n", "feature", "=", "(", "torch", ".", "rand", "(", "N", ",", "C", ",", "H", ",", "W", ")", "-", "0.5", ")", "*", "2", "*", "std", "+", "mean", "\n", "\n", "features", "=", "[", "feature", ".", "to", "(", "device", ")", "]", "\n", "\n", "rois", "=", "[", "]", "\n", "rois_rotated", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "N", ")", ":", "\n", "            ", "boxes", "=", "random_boxes", "(", "N_rois", ",", "W", "*", "canonical_scale_factor", ")", "\n", "rotated_boxes", "=", "torch", ".", "zeros", "(", "N_rois", ",", "5", ")", "\n", "rotated_boxes", "[", ":", ",", "0", "]", "=", "(", "boxes", "[", ":", ",", "0", "]", "+", "boxes", "[", ":", ",", "2", "]", ")", "/", "2.0", "\n", "rotated_boxes", "[", ":", ",", "1", "]", "=", "(", "boxes", "[", ":", ",", "1", "]", "+", "boxes", "[", ":", ",", "3", "]", ")", "/", "2.0", "\n", "rotated_boxes", "[", ":", ",", "2", "]", "=", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", "\n", "rotated_boxes", "[", ":", ",", "3", "]", "=", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", "\n", "rois", ".", "append", "(", "Boxes", "(", "boxes", ")", ".", "to", "(", "device", ")", ")", "\n", "rois_rotated", ".", "append", "(", "RotatedBoxes", "(", "rotated_boxes", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "", "roialignv2_pooler", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "\"ROIAlignV2\"", ",", "\n", ")", "\n", "\n", "roialignv2_out", "=", "roialignv2_pooler", "(", "features", ",", "rois", ")", "\n", "\n", "roialignrotated_pooler", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "\"ROIAlignRotated\"", ",", "\n", ")", "\n", "\n", "roialignrotated_out", "=", "roialignrotated_pooler", "(", "features", ",", "rois_rotated", ")", "\n", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "roialignv2_out", ",", "roialignrotated_out", ",", "atol", "=", "1e-4", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler.test_roialignv2_roialignrotated_match_cpu": [[62, 64], ["test_roi_pooler.TestROIPooler._test_roialignv2_roialignrotated_match"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler._test_roialignv2_roialignrotated_match"], ["", "def", "test_roialignv2_roialignrotated_match_cpu", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_roialignv2_roialignrotated_match", "(", "device", "=", "\"cpu\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler.test_roialignv2_roialignrotated_match_cuda": [[65, 68], ["unittest.skipIf", "test_roi_pooler.TestROIPooler._test_roialignv2_roialignrotated_match", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler._test_roialignv2_roialignrotated_match"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_roialignv2_roialignrotated_match_cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_roialignv2_roialignrotated_match", "(", "device", "=", "\"cuda\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler._test_scriptability": [[69, 100], ["range", "detectron2.modeling.poolers.ROIPooler", "detectron2.modeling.poolers.ROIPooler.", "test_roi_pooler.TestROIPooler.assertTrue", "feature.to", "detectron2.utils.testing.random_boxes", "rois.append", "torch.jit.script", "torch.equal", "detectron2.structures.Boxes().to", "torch.rand", "detectron2.structures.Boxes"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "_test_scriptability", "(", "self", ",", "device", ")", ":", "\n", "        ", "pooler_resolution", "=", "14", "\n", "canonical_level", "=", "4", "\n", "canonical_scale_factor", "=", "2", "**", "canonical_level", "\n", "pooler_scales", "=", "(", "1.0", "/", "canonical_scale_factor", ",", ")", "\n", "sampling_ratio", "=", "0", "\n", "\n", "N", ",", "C", ",", "H", ",", "W", "=", "2", ",", "4", ",", "10", ",", "8", "\n", "N_rois", "=", "10", "\n", "std", "=", "11", "\n", "mean", "=", "0", "\n", "feature", "=", "(", "torch", ".", "rand", "(", "N", ",", "C", ",", "H", ",", "W", ")", "-", "0.5", ")", "*", "2", "*", "std", "+", "mean", "\n", "\n", "features", "=", "[", "feature", ".", "to", "(", "device", ")", "]", "\n", "\n", "rois", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "N", ")", ":", "\n", "            ", "boxes", "=", "random_boxes", "(", "N_rois", ",", "W", "*", "canonical_scale_factor", ")", "\n", "\n", "rois", ".", "append", "(", "Boxes", "(", "boxes", ")", ".", "to", "(", "device", ")", ")", "\n", "\n", "", "roialignv2_pooler", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "\"ROIAlignV2\"", ",", "\n", ")", "\n", "\n", "roialignv2_out", "=", "roialignv2_pooler", "(", "features", ",", "rois", ")", "\n", "scripted_roialignv2_out", "=", "torch", ".", "jit", ".", "script", "(", "roialignv2_pooler", ")", "(", "features", ",", "rois", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "roialignv2_out", ",", "scripted_roialignv2_out", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler.test_scriptability_cpu": [[101, 104], ["unittest.skipIf", "test_roi_pooler.TestROIPooler._test_scriptability"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler._test_scriptability"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_scriptability_cpu", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_scriptability", "(", "device", "=", "\"cpu\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler.test_scriptability_gpu": [[105, 109], ["unittest.skipIf", "unittest.skipIf", "test_roi_pooler.TestROIPooler._test_scriptability", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler._test_scriptability"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_scriptability_gpu", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_scriptability", "(", "device", "=", "\"cuda\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler.test_no_images": [[110, 119], ["detectron2.modeling.poolers.ROIPooler", "detectron2.modeling.poolers.ROIPooler.forward", "test_roi_pooler.TestROIPooler.assertEqual", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward"], ["", "def", "test_no_images", "(", "self", ")", ":", "\n", "        ", "N", ",", "C", ",", "H", ",", "W", "=", "0", ",", "32", ",", "32", ",", "32", "\n", "feature", "=", "torch", ".", "rand", "(", "N", ",", "C", ",", "H", ",", "W", ")", "-", "0.5", "\n", "features", "=", "[", "feature", "]", "\n", "pooler", "=", "ROIPooler", "(", "\n", "output_size", "=", "14", ",", "scales", "=", "(", "1.0", ",", ")", ",", "sampling_ratio", "=", "0.0", ",", "pooler_type", "=", "\"ROIAlignV2\"", "\n", ")", "\n", "output", "=", "pooler", ".", "forward", "(", "features", ",", "[", "]", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "shape", ",", "(", "0", ",", "C", ",", "14", ",", "14", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler.test_fmt_box_list_tracing": [[120, 131], ["torch.no_grad", "torch.jit.trace", "test_roi_pooler.TestROIPooler.assertEqual", "test_roi_pooler.TestROIPooler.assertEqual", "test_roi_pooler.TestROIPooler.assertEqual", "detectron2.modeling.poolers._fmt_box_list", "Model", "torch.ones", "torch.jit.trace.", "torch.jit.trace.", "torch.jit.trace.", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers._fmt_box_list"], ["", "def", "test_fmt_box_list_tracing", "(", "self", ")", ":", "\n", "        ", "class", "Model", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "box_tensor", ")", ":", "\n", "                ", "return", "_fmt_box_list", "(", "box_tensor", ",", "0", ")", "\n", "\n", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "func", "=", "torch", ".", "jit", ".", "trace", "(", "Model", "(", ")", ",", "torch", ".", "ones", "(", "10", ",", "4", ")", ")", "\n", "\n", "self", ".", "assertEqual", "(", "func", "(", "torch", ".", "ones", "(", "10", ",", "4", ")", ")", ".", "shape", ",", "(", "10", ",", "5", ")", ")", "\n", "self", ".", "assertEqual", "(", "func", "(", "torch", ".", "ones", "(", "5", ",", "4", ")", ")", ".", "shape", ",", "(", "5", ",", "5", ")", ")", "\n", "self", ".", "assertEqual", "(", "func", "(", "torch", ".", "ones", "(", "20", ",", "4", ")", ")", ".", "shape", ",", "(", "20", ",", "5", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_pooler.TestROIPooler.test_roi_pooler_tracing": [[132, 177], ["detectron2.utils.testing.random_boxes", "torch.cat", "Model", "detectron2.modeling.poolers.ROIPooler", "torch.no_grad", "torch.jit.trace", "torch.jit.trace.", "test_roi_pooler.TestROIPooler.assertEqual", "torch.jit.trace.", "test_roi_pooler.TestROIPooler.assertEqual", "torch.jit.trace.", "test_roi_pooler.TestROIPooler.assertEqual", "unittest.TestCase.__init__", "test_roi_pooler.TestROIPooler.roi", "torch.tensor", "detectron2.utils.testing.random_boxes", "detectron2.structures.Boxes", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes"], ["", "", "def", "test_roi_pooler_tracing", "(", "self", ")", ":", "\n", "        ", "class", "Model", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "__init__", "(", "self", ",", "roi", ")", ":", "\n", "                ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "roi", "=", "roi", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "boxes", ")", ":", "\n", "                ", "return", "self", ".", "roi", "(", "x", ",", "[", "Boxes", "(", "boxes", ")", "]", ")", "\n", "\n", "", "", "pooler_resolution", "=", "14", "\n", "canonical_level", "=", "4", "\n", "canonical_scale_factor", "=", "2", "**", "canonical_level", "\n", "pooler_scales", "=", "(", "1.0", "/", "canonical_scale_factor", ",", "0.5", "/", "canonical_scale_factor", ")", "\n", "sampling_ratio", "=", "0", "\n", "\n", "N", ",", "C", ",", "H", ",", "W", "=", "1", ",", "4", ",", "10", ",", "8", "\n", "N_rois", "=", "10", "\n", "std", "=", "11", "\n", "mean", "=", "0", "\n", "feature", "=", "(", "torch", ".", "rand", "(", "N", ",", "C", ",", "H", ",", "W", ")", "-", "0.5", ")", "*", "2", "*", "std", "+", "mean", "\n", "feature", "=", "[", "feature", ",", "feature", "]", "\n", "\n", "rois", "=", "random_boxes", "(", "N_rois", ",", "W", "*", "canonical_scale_factor", ")", "\n", "# Add one larger box so that this level has only one box.", "\n", "# This may trigger the bug https://github.com/pytorch/pytorch/issues/49852", "\n", "# that we shall workaround.", "\n", "rois", "=", "torch", ".", "cat", "(", "[", "rois", ",", "torch", ".", "tensor", "(", "[", "[", "0", ",", "0", ",", "448", ",", "448", "]", "]", ")", "]", ")", "\n", "\n", "model", "=", "Model", "(", "\n", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "\"ROIAlign\"", ",", "\n", ")", "\n", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "func", "=", "torch", ".", "jit", ".", "trace", "(", "model", ",", "(", "feature", ",", "rois", ")", ")", "\n", "o", "=", "func", "(", "feature", ",", "rois", ")", "\n", "self", ".", "assertEqual", "(", "o", ".", "shape", ",", "(", "11", ",", "4", ",", "14", ",", "14", ")", ")", "\n", "o", "=", "func", "(", "feature", ",", "rois", "[", ":", "5", "]", ")", "\n", "self", ".", "assertEqual", "(", "o", ".", "shape", ",", "(", "5", ",", "4", ",", "14", ",", "14", ")", ")", "\n", "o", "=", "func", "(", "feature", ",", "random_boxes", "(", "20", ",", "W", "*", "canonical_scale_factor", ")", ")", "\n", "self", ".", "assertEqual", "(", "o", ".", "shape", ",", "(", "20", ",", "4", ",", "14", ",", "14", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_matcher.TestMatcher.test_scriptability": [[11, 39], ["detectron2.config.get_cfg", "detectron2.modeling.matcher.Matcher", "torch.tensor", "torch.tensor", "torch.tensor", "detectron2.modeling.matcher.Matcher.", "test_matcher.TestMatcher.assertTrue", "test_matcher.TestMatcher.assertTrue", "scripted_anchor_matcher", "test_matcher.TestMatcher.assertTrue", "test_matcher.TestMatcher.assertTrue", "torch.allclose", "torch.allclose", "detectron2.modeling.matcher.Matcher", "torch.jit.script", "torch.allclose", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg"], ["    ", "def", "test_scriptability", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "anchor_matcher", "=", "Matcher", "(", "\n", "cfg", ".", "MODEL", ".", "RPN", ".", "IOU_THRESHOLDS", ",", "cfg", ".", "MODEL", ".", "RPN", ".", "IOU_LABELS", ",", "allow_low_quality_matches", "=", "True", "\n", ")", "\n", "match_quality_matrix", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "0.15", ",", "0.45", ",", "0.2", ",", "0.6", "]", ",", "[", "0.3", ",", "0.65", ",", "0.05", ",", "0.1", "]", ",", "[", "0.05", ",", "0.4", ",", "0.25", ",", "0.4", "]", "]", "\n", ")", "\n", "expected_matches", "=", "torch", ".", "tensor", "(", "[", "1", ",", "1", ",", "2", ",", "0", "]", ")", "\n", "expected_match_labels", "=", "torch", ".", "tensor", "(", "[", "-", "1", ",", "1", ",", "0", ",", "1", "]", ",", "dtype", "=", "torch", ".", "int8", ")", "\n", "\n", "matches", ",", "match_labels", "=", "anchor_matcher", "(", "match_quality_matrix", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "matches", ",", "expected_matches", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "match_labels", ",", "expected_match_labels", ")", ")", "\n", "\n", "# nonzero_tuple must be import explicitly to let jit know what it is.", "\n", "# https://github.com/pytorch/pytorch/issues/38964", "\n", "from", "detectron2", ".", "layers", "import", "nonzero_tuple", "# noqa F401", "\n", "\n", "def", "f", "(", "thresholds", ":", "List", "[", "float", "]", ",", "labels", ":", "List", "[", "int", "]", ")", ":", "\n", "            ", "return", "Matcher", "(", "thresholds", ",", "labels", ",", "allow_low_quality_matches", "=", "True", ")", "\n", "\n", "", "scripted_anchor_matcher", "=", "torch", ".", "jit", ".", "script", "(", "f", ")", "(", "\n", "cfg", ".", "MODEL", ".", "RPN", ".", "IOU_THRESHOLDS", ",", "cfg", ".", "MODEL", ".", "RPN", ".", "IOU_LABELS", "\n", ")", "\n", "matches", ",", "match_labels", "=", "scripted_anchor_matcher", "(", "match_quality_matrix", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "matches", ",", "expected_matches", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "match_labels", ",", "expected_match_labels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_fast_rcnn.FastRCNNTest.test_fast_rcnn": [[18, 47], ["torch.manual_seed", "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers", "torch.rand", "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.", "torch.tensor", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.Boxes", "detectron2.structures.Boxes", "torch.tensor", "expected_losses.keys", "detectron2.layers.ShapeSpec", "detectron2.utils.events.EventStorage", "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "torch.tensor", "torch.tensor", "torch.allclose", "detectron2.modeling.box_regression.Box2BoxTransform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses"], ["    ", "def", "test_fast_rcnn", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "132", ")", "\n", "\n", "box_head_output_size", "=", "8", "\n", "\n", "box_predictor", "=", "FastRCNNOutputLayers", "(", "\n", "ShapeSpec", "(", "channels", "=", "box_head_output_size", ")", ",", "\n", "box2box_transform", "=", "Box2BoxTransform", "(", "weights", "=", "(", "10", ",", "10", ",", "5", ",", "5", ")", ")", ",", "\n", "num_classes", "=", "5", ",", "\n", ")", "\n", "feature_pooled", "=", "torch", ".", "rand", "(", "2", ",", "box_head_output_size", ")", "\n", "predictions", "=", "box_predictor", "(", "feature_pooled", ")", "\n", "\n", "proposal_boxes", "=", "torch", ".", "tensor", "(", "[", "[", "0.8", ",", "1.1", ",", "3.2", ",", "2.8", "]", ",", "[", "2.3", ",", "2.5", ",", "7", ",", "8", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "gt_boxes", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "1", ",", "3", ",", "3", "]", ",", "[", "2", ",", "2", ",", "6", ",", "6", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "proposal", "=", "Instances", "(", "(", "10", ",", "10", ")", ")", "\n", "proposal", ".", "proposal_boxes", "=", "Boxes", "(", "proposal_boxes", ")", "\n", "proposal", ".", "gt_boxes", "=", "Boxes", "(", "gt_boxes", ")", "\n", "proposal", ".", "gt_classes", "=", "torch", ".", "tensor", "(", "[", "1", ",", "2", "]", ")", "\n", "\n", "with", "EventStorage", "(", ")", ":", "# capture events in a new storage to discard them", "\n", "            ", "losses", "=", "box_predictor", ".", "losses", "(", "predictions", ",", "[", "proposal", "]", ")", "\n", "\n", "", "expected_losses", "=", "{", "\n", "\"loss_cls\"", ":", "torch", ".", "tensor", "(", "1.7951188087", ")", ",", "\n", "\"loss_box_reg\"", ":", "torch", ".", "tensor", "(", "4.0357131958", ")", ",", "\n", "}", "\n", "for", "name", "in", "expected_losses", ".", "keys", "(", ")", ":", "\n", "            ", "assert", "torch", ".", "allclose", "(", "losses", "[", "name", "]", ",", "expected_losses", "[", "name", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_fast_rcnn.FastRCNNTest.test_fast_rcnn_empty_batch": [[48, 66], ["detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers().to", "torch.randn", "torch.randn", "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers().to.losses", "FastRCNNOutputLayers().to.losses.values", "sum().backward", "test_fast_rcnn.FastRCNNTest.assertTrue", "test_fast_rcnn.FastRCNNTest.assertTrue", "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers().to.inference", "test_fast_rcnn.FastRCNNTest.assertEqual", "test_fast_rcnn.FastRCNNTest.assertTrue", "len", "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers", "torch.allclose", "sum", "detectron2.layers.ShapeSpec", "torch.zeros_like", "FastRCNNOutputLayers().to.losses.values", "detectron2.modeling.box_regression.Box2BoxTransform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference"], ["", "", "def", "test_fast_rcnn_empty_batch", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "box_predictor", "=", "FastRCNNOutputLayers", "(", "\n", "ShapeSpec", "(", "channels", "=", "10", ")", ",", "\n", "box2box_transform", "=", "Box2BoxTransform", "(", "weights", "=", "(", "10", ",", "10", ",", "5", ",", "5", ")", ")", ",", "\n", "num_classes", "=", "8", ",", "\n", ")", ".", "to", "(", "device", "=", "device", ")", "\n", "\n", "logits", "=", "torch", ".", "randn", "(", "0", ",", "100", ",", "requires_grad", "=", "True", ",", "device", "=", "device", ")", "\n", "deltas", "=", "torch", ".", "randn", "(", "0", ",", "4", ",", "requires_grad", "=", "True", ",", "device", "=", "device", ")", "\n", "losses", "=", "box_predictor", ".", "losses", "(", "[", "logits", ",", "deltas", "]", ",", "[", "]", ")", "\n", "for", "value", "in", "losses", ".", "values", "(", ")", ":", "\n", "            ", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "value", ",", "torch", ".", "zeros_like", "(", "value", ")", ")", ")", "\n", "", "sum", "(", "losses", ".", "values", "(", ")", ")", ".", "backward", "(", ")", "\n", "self", ".", "assertTrue", "(", "logits", ".", "grad", "is", "not", "None", ")", "\n", "self", ".", "assertTrue", "(", "deltas", ".", "grad", "is", "not", "None", ")", "\n", "\n", "predictions", ",", "_", "=", "box_predictor", ".", "inference", "(", "[", "logits", ",", "deltas", "]", ",", "[", "]", ")", "\n", "self", ".", "assertEqual", "(", "len", "(", "predictions", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_fast_rcnn.FastRCNNTest.test_fast_rcnn_empty_batch_cuda": [[67, 70], ["unittest.skipIf", "test_fast_rcnn.FastRCNNTest.test_fast_rcnn_empty_batch", "torch.cuda.is_available", "torch.device"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_fast_rcnn.FastRCNNTest.test_fast_rcnn_empty_batch", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_fast_rcnn_empty_batch_cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "test_fast_rcnn_empty_batch", "(", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_fast_rcnn.FastRCNNTest.test_fast_rcnn_rotated": [[71, 104], ["torch.manual_seed", "detectron2.modeling.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers", "torch.rand", "detectron2.modeling.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.", "torch.tensor", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.RotatedBoxes", "detectron2.structures.RotatedBoxes", "torch.tensor", "expected_losses.keys", "detectron2.layers.ShapeSpec", "detectron2.utils.events.EventStorage", "detectron2.modeling.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.losses", "torch.tensor", "torch.tensor", "torch.allclose", "detectron2.modeling.box_regression.Box2BoxTransformRotated"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses"], ["", "def", "test_fast_rcnn_rotated", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "132", ")", "\n", "box_head_output_size", "=", "8", "\n", "\n", "box_predictor", "=", "RotatedFastRCNNOutputLayers", "(", "\n", "ShapeSpec", "(", "channels", "=", "box_head_output_size", ")", ",", "\n", "box2box_transform", "=", "Box2BoxTransformRotated", "(", "weights", "=", "(", "10", ",", "10", ",", "5", ",", "5", ",", "1", ")", ")", ",", "\n", "num_classes", "=", "5", ",", "\n", ")", "\n", "feature_pooled", "=", "torch", ".", "rand", "(", "2", ",", "box_head_output_size", ")", "\n", "predictions", "=", "box_predictor", "(", "feature_pooled", ")", "\n", "proposal_boxes", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "2", ",", "1.95", ",", "2.4", ",", "1.7", ",", "0", "]", ",", "[", "4.65", ",", "5.25", ",", "4.7", ",", "5.5", ",", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "gt_boxes", "=", "torch", ".", "tensor", "(", "[", "[", "2", ",", "2", ",", "2", ",", "2", ",", "0", "]", ",", "[", "4", ",", "4", ",", "4", ",", "4", ",", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "proposal", "=", "Instances", "(", "(", "10", ",", "10", ")", ")", "\n", "proposal", ".", "proposal_boxes", "=", "RotatedBoxes", "(", "proposal_boxes", ")", "\n", "proposal", ".", "gt_boxes", "=", "RotatedBoxes", "(", "gt_boxes", ")", "\n", "proposal", ".", "gt_classes", "=", "torch", ".", "tensor", "(", "[", "1", ",", "2", "]", ")", "\n", "\n", "with", "EventStorage", "(", ")", ":", "# capture events in a new storage to discard them", "\n", "            ", "losses", "=", "box_predictor", ".", "losses", "(", "predictions", ",", "[", "proposal", "]", ")", "\n", "\n", "# Note: the expected losses are slightly different even if", "\n", "# the boxes are essentially the same as in the FastRCNNOutput test, because", "\n", "# bbox_pred in FastRCNNOutputLayers have different Linear layers/initialization", "\n", "# between the two cases.", "\n", "", "expected_losses", "=", "{", "\n", "\"loss_cls\"", ":", "torch", ".", "tensor", "(", "1.7920907736", ")", ",", "\n", "\"loss_box_reg\"", ":", "torch", ".", "tensor", "(", "4.0410838127", ")", ",", "\n", "}", "\n", "for", "name", "in", "expected_losses", ".", "keys", "(", ")", ":", "\n", "            ", "assert", "torch", ".", "allclose", "(", "losses", "[", "name", "]", ",", "expected_losses", "[", "name", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_fast_rcnn.FastRCNNTest.test_predict_boxes_tracing": [[105, 138], ["unittest.skipIf", "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers", "Model", "detectron2.layers.ShapeSpec", "torch.no_grad", "patch_builtin_len", "torch.jit.trace", "torch.jit.trace.", "test_fast_rcnn.FastRCNNTest.assertEqual", "torch.jit.trace.", "test_fast_rcnn.FastRCNNTest.assertEqual", "torch.jit.trace.", "test_fast_rcnn.FastRCNNTest.assertEqual", "unittest.TestCase.__init__", "detectron2.structures.Instances", "detectron2.structures.Boxes", "test_fast_rcnn.FastRCNNTest._output_layer.predict_boxes", "detectron2.modeling.box_regression.Box2BoxTransform", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_builtin_len", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_boxes"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_predict_boxes_tracing", "(", "self", ")", ":", "\n", "        ", "class", "Model", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "__init__", "(", "self", ",", "output_layer", ")", ":", "\n", "                ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_output_layer", "=", "output_layer", "\n", "\n", "", "def", "forward", "(", "self", ",", "proposal_deltas", ",", "proposal_boxes", ")", ":", "\n", "                ", "instances", "=", "Instances", "(", "(", "10", ",", "10", ")", ")", "\n", "instances", ".", "proposal_boxes", "=", "Boxes", "(", "proposal_boxes", ")", "\n", "return", "self", ".", "_output_layer", ".", "predict_boxes", "(", "(", "None", ",", "proposal_deltas", ")", ",", "[", "instances", "]", ")", "\n", "\n", "", "", "box_head_output_size", "=", "8", "\n", "\n", "box_predictor", "=", "FastRCNNOutputLayers", "(", "\n", "ShapeSpec", "(", "channels", "=", "box_head_output_size", ")", ",", "\n", "box2box_transform", "=", "Box2BoxTransform", "(", "weights", "=", "(", "10", ",", "10", ",", "5", ",", "5", ")", ")", ",", "\n", "num_classes", "=", "5", ",", "\n", ")", "\n", "\n", "model", "=", "Model", "(", "box_predictor", ")", "\n", "\n", "from", "detectron2", ".", "export", ".", "torchscript_patch", "import", "patch_builtin_len", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ",", "patch_builtin_len", "(", ")", ":", "\n", "            ", "func", "=", "torch", ".", "jit", ".", "trace", "(", "model", ",", "(", "torch", ".", "randn", "(", "10", ",", "20", ")", ",", "torch", ".", "randn", "(", "10", ",", "4", ")", ")", ")", "\n", "\n", "o", "=", "func", "(", "torch", ".", "randn", "(", "10", ",", "20", ")", ",", "torch", ".", "randn", "(", "10", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "o", "[", "0", "]", ".", "shape", ",", "(", "10", ",", "20", ")", ")", "\n", "o", "=", "func", "(", "torch", ".", "randn", "(", "5", ",", "20", ")", ",", "torch", ".", "randn", "(", "5", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "o", "[", "0", "]", ".", "shape", ",", "(", "5", ",", "20", ")", ")", "\n", "o", "=", "func", "(", "torch", ".", "randn", "(", "20", ",", "20", ")", ",", "torch", ".", "randn", "(", "20", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "o", "[", "0", "]", ".", "shape", ",", "(", "20", ",", "20", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_fast_rcnn.FastRCNNTest.test_predict_probs_tracing": [[139, 170], ["detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers", "Model", "detectron2.layers.ShapeSpec", "torch.no_grad", "patch_builtin_len", "torch.jit.trace", "torch.jit.trace.", "test_fast_rcnn.FastRCNNTest.assertEqual", "torch.jit.trace.", "test_fast_rcnn.FastRCNNTest.assertEqual", "torch.jit.trace.", "test_fast_rcnn.FastRCNNTest.assertEqual", "unittest.TestCase.__init__", "detectron2.structures.Instances", "detectron2.structures.Boxes", "test_fast_rcnn.FastRCNNTest._output_layer.predict_probs", "detectron2.modeling.box_regression.Box2BoxTransform", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_builtin_len", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_probs"], ["", "", "def", "test_predict_probs_tracing", "(", "self", ")", ":", "\n", "        ", "class", "Model", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "__init__", "(", "self", ",", "output_layer", ")", ":", "\n", "                ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_output_layer", "=", "output_layer", "\n", "\n", "", "def", "forward", "(", "self", ",", "scores", ",", "proposal_boxes", ")", ":", "\n", "                ", "instances", "=", "Instances", "(", "(", "10", ",", "10", ")", ")", "\n", "instances", ".", "proposal_boxes", "=", "Boxes", "(", "proposal_boxes", ")", "\n", "return", "self", ".", "_output_layer", ".", "predict_probs", "(", "(", "scores", ",", "None", ")", ",", "[", "instances", "]", ")", "\n", "\n", "", "", "box_head_output_size", "=", "8", "\n", "\n", "box_predictor", "=", "FastRCNNOutputLayers", "(", "\n", "ShapeSpec", "(", "channels", "=", "box_head_output_size", ")", ",", "\n", "box2box_transform", "=", "Box2BoxTransform", "(", "weights", "=", "(", "10", ",", "10", ",", "5", ",", "5", ")", ")", ",", "\n", "num_classes", "=", "5", ",", "\n", ")", "\n", "\n", "model", "=", "Model", "(", "box_predictor", ")", "\n", "\n", "from", "detectron2", ".", "export", ".", "torchscript_patch", "import", "patch_builtin_len", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ",", "patch_builtin_len", "(", ")", ":", "\n", "            ", "func", "=", "torch", ".", "jit", ".", "trace", "(", "model", ",", "(", "torch", ".", "randn", "(", "10", ",", "6", ")", ",", "torch", ".", "rand", "(", "10", ",", "4", ")", ")", ")", "\n", "o", "=", "func", "(", "torch", ".", "randn", "(", "10", ",", "6", ")", ",", "torch", ".", "randn", "(", "10", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "o", "[", "0", "]", ".", "shape", ",", "(", "10", ",", "6", ")", ")", "\n", "o", "=", "func", "(", "torch", ".", "randn", "(", "5", ",", "6", ")", ",", "torch", ".", "randn", "(", "5", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "o", "[", "0", "]", ".", "shape", ",", "(", "5", ",", "6", ")", ")", "\n", "o", "=", "func", "(", "torch", ".", "randn", "(", "20", ",", "6", ")", ",", "torch", ".", "randn", "(", "20", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "o", "[", "0", "]", ".", "shape", ",", "(", "20", ",", "6", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_heads.ROIHeadsTest.test_roi_heads": [[40, 92], ["torch.manual_seed", "detectron2.config.get_cfg", "torch.rand", "detectron2.structures.ImageList", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.tensor", "detectron2.structures.BitMasks", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.tensor", "detectron2.structures.BitMasks", "detectron2.modeling.proposal_generator.build.build_proposal_generator", "detectron2.modeling.roi_heads.StandardROIHeads", "detector_losses.update", "all", "test_roi_heads.ROIHeadsTest.assertTrue", "torch.rand", "detectron2.layers.ShapeSpec", "detectron2.utils.events.EventStorage", "detectron2.modeling.proposal_generator.build.build_proposal_generator.", "detectron2.modeling.roi_heads.StandardROIHeads.", "torch.rand", "torch.rand", "torch.allclose", "torch.tensor", "detector_losses.keys", "v.item", "expected_losses.get", "detector_losses.items"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.build.build_proposal_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["    ", "def", "test_roi_heads", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "121", ")", "\n", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NAME", "=", "\"FastRCNNConvFCHead\"", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NUM_FC", "=", "2", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "=", "\"ROIAlignV2\"", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_WEIGHTS", "=", "(", "10", ",", "10", ",", "5", ",", "5", ")", "\n", "cfg", ".", "MODEL", ".", "MASK_ON", "=", "True", "\n", "num_images", "=", "2", "\n", "images_tensor", "=", "torch", ".", "rand", "(", "num_images", ",", "20", ",", "30", ")", "\n", "image_sizes", "=", "[", "(", "10", ",", "10", ")", ",", "(", "20", ",", "30", ")", "]", "\n", "images", "=", "ImageList", "(", "images_tensor", ",", "image_sizes", ")", "\n", "num_channels", "=", "1024", "\n", "features", "=", "{", "\"res4\"", ":", "torch", ".", "rand", "(", "num_images", ",", "num_channels", ",", "1", ",", "2", ")", "}", "\n", "feature_shape", "=", "{", "\"res4\"", ":", "ShapeSpec", "(", "channels", "=", "num_channels", ",", "stride", "=", "16", ")", "}", "\n", "\n", "image_shape", "=", "(", "15", ",", "15", ")", "\n", "gt_boxes0", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "1", ",", "3", ",", "3", "]", ",", "[", "2", ",", "2", ",", "6", ",", "6", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "gt_instance0", "=", "Instances", "(", "image_shape", ")", "\n", "gt_instance0", ".", "gt_boxes", "=", "Boxes", "(", "gt_boxes0", ")", "\n", "gt_instance0", ".", "gt_classes", "=", "torch", ".", "tensor", "(", "[", "2", ",", "1", "]", ")", "\n", "gt_instance0", ".", "gt_masks", "=", "BitMasks", "(", "torch", ".", "rand", "(", "(", "2", ",", ")", "+", "image_shape", ")", ">", "0.5", ")", "\n", "gt_boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "5", ",", "2", ",", "8", "]", ",", "[", "7", ",", "3", ",", "10", ",", "5", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "gt_instance1", "=", "Instances", "(", "image_shape", ")", "\n", "gt_instance1", ".", "gt_boxes", "=", "Boxes", "(", "gt_boxes1", ")", "\n", "gt_instance1", ".", "gt_classes", "=", "torch", ".", "tensor", "(", "[", "1", ",", "2", "]", ")", "\n", "gt_instance1", ".", "gt_masks", "=", "BitMasks", "(", "torch", ".", "rand", "(", "(", "2", ",", ")", "+", "image_shape", ")", ">", "0.5", ")", "\n", "gt_instances", "=", "[", "gt_instance0", ",", "gt_instance1", "]", "\n", "\n", "proposal_generator", "=", "build_proposal_generator", "(", "cfg", ",", "feature_shape", ")", "\n", "roi_heads", "=", "StandardROIHeads", "(", "cfg", ",", "feature_shape", ")", "\n", "\n", "with", "EventStorage", "(", ")", ":", "# capture events in a new storage to discard them", "\n", "            ", "proposals", ",", "proposal_losses", "=", "proposal_generator", "(", "images", ",", "features", ",", "gt_instances", ")", "\n", "_", ",", "detector_losses", "=", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ")", "\n", "\n", "", "detector_losses", ".", "update", "(", "proposal_losses", ")", "\n", "expected_losses", "=", "{", "\n", "\"loss_cls\"", ":", "4.5253729820251465", ",", "\n", "\"loss_box_reg\"", ":", "0.009785720147192478", ",", "\n", "\"loss_mask\"", ":", "0.693184494972229", ",", "\n", "\"loss_rpn_cls\"", ":", "0.08186662942171097", ",", "\n", "\"loss_rpn_loc\"", ":", "0.1104838103055954", ",", "\n", "}", "\n", "succ", "=", "all", "(", "\n", "torch", ".", "allclose", "(", "detector_losses", "[", "name", "]", ",", "torch", ".", "tensor", "(", "expected_losses", ".", "get", "(", "name", ",", "0.0", ")", ")", ")", "\n", "for", "name", "in", "detector_losses", ".", "keys", "(", ")", "\n", ")", "\n", "self", ".", "assertTrue", "(", "\n", "succ", ",", "\n", "\"Losses has changed! New losses: {}\"", ".", "format", "(", "\n", "{", "k", ":", "v", ".", "item", "(", ")", "for", "k", ",", "v", "in", "detector_losses", ".", "items", "(", ")", "}", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_heads.ROIHeadsTest.test_rroi_heads": [[95, 148], ["torch.manual_seed", "detectron2.config.get_cfg", "torch.rand", "detectron2.structures.ImageList", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.RotatedBoxes", "torch.tensor", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.RotatedBoxes", "torch.tensor", "detectron2.modeling.proposal_generator.build.build_proposal_generator", "detectron2.modeling.roi_heads.build_roi_heads", "detector_losses.update", "all", "test_roi_heads.ROIHeadsTest.assertTrue", "torch.rand", "detectron2.layers.ShapeSpec", "detectron2.utils.events.EventStorage", "detectron2.modeling.proposal_generator.build.build_proposal_generator.", "detectron2.modeling.roi_heads.build_roi_heads.", "torch.allclose", "torch.tensor", "detector_losses.keys", "v.item", "expected_losses.get", "detector_losses.items"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.build.build_proposal_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.build_roi_heads", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "test_rroi_heads", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "121", ")", "\n", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "MODEL", ".", "PROPOSAL_GENERATOR", ".", "NAME", "=", "\"RRPN\"", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "NAME", "=", "\"RotatedAnchorGenerator\"", "\n", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NAME", "=", "\"RROIHeads\"", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NAME", "=", "\"FastRCNNConvFCHead\"", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NUM_FC", "=", "2", "\n", "cfg", ".", "MODEL", ".", "RPN", ".", "BBOX_REG_WEIGHTS", "=", "(", "1", ",", "1", ",", "1", ",", "1", ",", "1", ")", "\n", "cfg", ".", "MODEL", ".", "RPN", ".", "HEAD_NAME", "=", "\"StandardRPNHead\"", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "=", "\"ROIAlignRotated\"", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_WEIGHTS", "=", "(", "10", ",", "10", ",", "5", ",", "5", ",", "1", ")", "\n", "num_images", "=", "2", "\n", "images_tensor", "=", "torch", ".", "rand", "(", "num_images", ",", "20", ",", "30", ")", "\n", "image_sizes", "=", "[", "(", "10", ",", "10", ")", ",", "(", "20", ",", "30", ")", "]", "\n", "images", "=", "ImageList", "(", "images_tensor", ",", "image_sizes", ")", "\n", "num_channels", "=", "1024", "\n", "features", "=", "{", "\"res4\"", ":", "torch", ".", "rand", "(", "num_images", ",", "num_channels", ",", "1", ",", "2", ")", "}", "\n", "feature_shape", "=", "{", "\"res4\"", ":", "ShapeSpec", "(", "channels", "=", "num_channels", ",", "stride", "=", "16", ")", "}", "\n", "\n", "image_shape", "=", "(", "15", ",", "15", ")", "\n", "gt_boxes0", "=", "torch", ".", "tensor", "(", "[", "[", "2", ",", "2", ",", "2", ",", "2", ",", "30", "]", ",", "[", "4", ",", "4", ",", "4", ",", "4", ",", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "gt_instance0", "=", "Instances", "(", "image_shape", ")", "\n", "gt_instance0", ".", "gt_boxes", "=", "RotatedBoxes", "(", "gt_boxes0", ")", "\n", "gt_instance0", ".", "gt_classes", "=", "torch", ".", "tensor", "(", "[", "2", ",", "1", "]", ")", "\n", "gt_boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "1.5", ",", "5.5", ",", "1", ",", "3", ",", "0", "]", ",", "[", "8.5", ",", "4", ",", "3", ",", "2", ",", "-", "50", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "gt_instance1", "=", "Instances", "(", "image_shape", ")", "\n", "gt_instance1", ".", "gt_boxes", "=", "RotatedBoxes", "(", "gt_boxes1", ")", "\n", "gt_instance1", ".", "gt_classes", "=", "torch", ".", "tensor", "(", "[", "1", ",", "2", "]", ")", "\n", "gt_instances", "=", "[", "gt_instance0", ",", "gt_instance1", "]", "\n", "\n", "proposal_generator", "=", "build_proposal_generator", "(", "cfg", ",", "feature_shape", ")", "\n", "roi_heads", "=", "build_roi_heads", "(", "cfg", ",", "feature_shape", ")", "\n", "\n", "with", "EventStorage", "(", ")", ":", "# capture events in a new storage to discard them", "\n", "            ", "proposals", ",", "proposal_losses", "=", "proposal_generator", "(", "images", ",", "features", ",", "gt_instances", ")", "\n", "_", ",", "detector_losses", "=", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ")", "\n", "\n", "", "detector_losses", ".", "update", "(", "proposal_losses", ")", "\n", "expected_losses", "=", "{", "\n", "\"loss_cls\"", ":", "4.365657806396484", ",", "\n", "\"loss_box_reg\"", ":", "0.0015851043863222003", ",", "\n", "\"loss_rpn_cls\"", ":", "0.2427729219198227", ",", "\n", "\"loss_rpn_loc\"", ":", "0.3646621108055115", ",", "\n", "}", "\n", "succ", "=", "all", "(", "\n", "torch", ".", "allclose", "(", "detector_losses", "[", "name", "]", ",", "torch", ".", "tensor", "(", "expected_losses", ".", "get", "(", "name", ",", "0.0", ")", ")", ")", "\n", "for", "name", "in", "detector_losses", ".", "keys", "(", ")", "\n", ")", "\n", "self", ".", "assertTrue", "(", "\n", "succ", ",", "\n", "\"Losses has changed! New losses: {}\"", ".", "format", "(", "\n", "{", "k", ":", "v", ".", "item", "(", ")", "for", "k", ",", "v", "in", "detector_losses", ".", "items", "(", ")", "}", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_heads.ROIHeadsTest.test_box_head_scriptability": [[151, 164], ["unittest.skipIf", "detectron2.layers.ShapeSpec", "torch.randn", "detectron2.modeling.roi_heads.FastRCNNConvFCHead().eval", "torch.jit.script", "detectron2.modeling.roi_heads.FastRCNNConvFCHead().eval.", "torch.jit.script.", "test_roi_heads.ROIHeadsTest.assertTrue", "torch.equal", "detectron2.modeling.roi_heads.FastRCNNConvFCHead"], "methods", ["None"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_box_head_scriptability", "(", "self", ")", ":", "\n", "        ", "input_shape", "=", "ShapeSpec", "(", "channels", "=", "1024", ",", "height", "=", "14", ",", "width", "=", "14", ")", "\n", "box_features", "=", "torch", ".", "randn", "(", "4", ",", "1024", ",", "14", ",", "14", ")", "\n", "\n", "box_head", "=", "FastRCNNConvFCHead", "(", "\n", "input_shape", ",", "conv_dims", "=", "[", "512", ",", "512", "]", ",", "fc_dims", "=", "[", "1024", ",", "1024", "]", "\n", ")", ".", "eval", "(", ")", "\n", "script_box_head", "=", "torch", ".", "jit", ".", "script", "(", "box_head", ")", "\n", "\n", "origin_output", "=", "box_head", "(", "box_features", ")", "\n", "script_output", "=", "script_box_head", "(", "box_features", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "origin_output", ",", "script_output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_heads.ROIHeadsTest.test_mask_head_scriptability": [[165, 194], ["unittest.skipIf", "detectron2.layers.ShapeSpec", "torch.randn", "detectron2.structures.Instances", "torch.tensor", "detectron2.structures.Instances", "torch.tensor", "detectron2.modeling.roi_heads.MaskRCNNConvUpsampleHead().eval", "detectron2.modeling.roi_heads.MaskRCNNConvUpsampleHead().eval.", "zip", "copy.deepcopy", "detectron2.export.torchscript_patch.freeze_training_mode", "detectron2.export.torchscript_patch.patch_instances", "torch.jit.script", "NewInstances.from_instances", "NewInstances.from_instances", "torch.jit.script.", "detectron2.utils.testing.assert_instances_allclose", "detectron2.modeling.roi_heads.MaskRCNNConvUpsampleHead"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.freeze_training_mode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.assert_instances_allclose"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_mask_head_scriptability", "(", "self", ")", ":", "\n", "        ", "input_shape", "=", "ShapeSpec", "(", "channels", "=", "1024", ")", "\n", "mask_features", "=", "torch", ".", "randn", "(", "4", ",", "1024", ",", "14", ",", "14", ")", "\n", "\n", "image_shapes", "=", "[", "(", "10", ",", "10", ")", ",", "(", "15", ",", "15", ")", "]", "\n", "pred_instance0", "=", "Instances", "(", "image_shapes", "[", "0", "]", ")", "\n", "pred_classes0", "=", "torch", ".", "tensor", "(", "[", "1", ",", "2", ",", "3", "]", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "pred_instance0", ".", "pred_classes", "=", "pred_classes0", "\n", "pred_instance1", "=", "Instances", "(", "image_shapes", "[", "1", "]", ")", "\n", "pred_classes1", "=", "torch", ".", "tensor", "(", "[", "4", "]", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "pred_instance1", ".", "pred_classes", "=", "pred_classes1", "\n", "\n", "mask_head", "=", "MaskRCNNConvUpsampleHead", "(", "\n", "input_shape", ",", "num_classes", "=", "80", ",", "conv_dims", "=", "[", "256", ",", "256", "]", "\n", ")", ".", "eval", "(", ")", "\n", "# pred_instance will be in-place changed during the inference", "\n", "# process of `MaskRCNNConvUpsampleHead`", "\n", "origin_outputs", "=", "mask_head", "(", "mask_features", ",", "deepcopy", "(", "[", "pred_instance0", ",", "pred_instance1", "]", ")", ")", "\n", "\n", "fields", "=", "{", "\"pred_masks\"", ":", "torch", ".", "Tensor", ",", "\"pred_classes\"", ":", "torch", ".", "Tensor", "}", "\n", "with", "freeze_training_mode", "(", "mask_head", ")", ",", "patch_instances", "(", "fields", ")", "as", "NewInstances", ":", "\n", "            ", "sciript_mask_head", "=", "torch", ".", "jit", ".", "script", "(", "mask_head", ")", "\n", "pred_instance0", "=", "NewInstances", ".", "from_instances", "(", "pred_instance0", ")", "\n", "pred_instance1", "=", "NewInstances", ".", "from_instances", "(", "pred_instance1", ")", "\n", "script_outputs", "=", "sciript_mask_head", "(", "mask_features", ",", "[", "pred_instance0", ",", "pred_instance1", "]", ")", "\n", "\n", "", "for", "origin_ins", ",", "script_ins", "in", "zip", "(", "origin_outputs", ",", "script_outputs", ")", ":", "\n", "            ", "assert_instances_allclose", "(", "origin_ins", ",", "script_ins", ",", "rtol", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_heads.ROIHeadsTest.test_keypoint_head_scriptability": [[195, 230], ["unittest.skipIf", "detectron2.layers.ShapeSpec", "torch.randn", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.Boxes", "detectron2.modeling.roi_heads.KRCNNConvDeconvUpsampleHead().eval", "detectron2.modeling.roi_heads.KRCNNConvDeconvUpsampleHead().eval.", "zip", "copy.deepcopy", "detectron2.export.torchscript_patch.freeze_training_mode", "detectron2.export.torchscript_patch.patch_instances", "torch.jit.script", "NewInstances.from_instances", "NewInstances.from_instances", "torch.jit.script.", "detectron2.utils.testing.assert_instances_allclose", "detectron2.modeling.roi_heads.KRCNNConvDeconvUpsampleHead"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.freeze_training_mode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.assert_instances_allclose"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_keypoint_head_scriptability", "(", "self", ")", ":", "\n", "        ", "input_shape", "=", "ShapeSpec", "(", "channels", "=", "1024", ",", "height", "=", "14", ",", "width", "=", "14", ")", "\n", "keypoint_features", "=", "torch", ".", "randn", "(", "4", ",", "1024", ",", "14", ",", "14", ")", "\n", "\n", "image_shapes", "=", "[", "(", "10", ",", "10", ")", ",", "(", "15", ",", "15", ")", "]", "\n", "pred_boxes0", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "1", ",", "3", ",", "3", "]", ",", "[", "2", ",", "2", ",", "6", ",", "6", "]", ",", "[", "1", ",", "5", ",", "2", ",", "8", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "pred_instance0", "=", "Instances", "(", "image_shapes", "[", "0", "]", ")", "\n", "pred_instance0", ".", "pred_boxes", "=", "Boxes", "(", "pred_boxes0", ")", "\n", "pred_boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "7", ",", "3", ",", "10", ",", "5", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "pred_instance1", "=", "Instances", "(", "image_shapes", "[", "1", "]", ")", "\n", "pred_instance1", ".", "pred_boxes", "=", "Boxes", "(", "pred_boxes1", ")", "\n", "\n", "keypoint_head", "=", "KRCNNConvDeconvUpsampleHead", "(", "\n", "input_shape", ",", "num_keypoints", "=", "17", ",", "conv_dims", "=", "[", "512", ",", "512", "]", "\n", ")", ".", "eval", "(", ")", "\n", "origin_outputs", "=", "keypoint_head", "(", "\n", "keypoint_features", ",", "deepcopy", "(", "[", "pred_instance0", ",", "pred_instance1", "]", ")", "\n", ")", "\n", "\n", "fields", "=", "{", "\n", "\"pred_boxes\"", ":", "Boxes", ",", "\n", "\"pred_keypoints\"", ":", "torch", ".", "Tensor", ",", "\n", "\"pred_keypoint_heatmaps\"", ":", "torch", ".", "Tensor", ",", "\n", "}", "\n", "with", "freeze_training_mode", "(", "keypoint_head", ")", ",", "patch_instances", "(", "fields", ")", "as", "NewInstances", ":", "\n", "            ", "sciript_keypoint_head", "=", "torch", ".", "jit", ".", "script", "(", "keypoint_head", ")", "\n", "pred_instance0", "=", "NewInstances", ".", "from_instances", "(", "pred_instance0", ")", "\n", "pred_instance1", "=", "NewInstances", ".", "from_instances", "(", "pred_instance1", ")", "\n", "script_outputs", "=", "sciript_keypoint_head", "(", "\n", "keypoint_features", ",", "[", "pred_instance0", ",", "pred_instance1", "]", "\n", ")", "\n", "\n", "", "for", "origin_ins", ",", "script_ins", "in", "zip", "(", "origin_outputs", ",", "script_outputs", ")", ":", "\n", "            ", "assert_instances_allclose", "(", "origin_ins", ",", "script_ins", ",", "rtol", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_heads.ROIHeadsTest.test_StandardROIHeads_scriptability": [[231, 282], ["unittest.skipIf", "detectron2.config.get_cfg", "torch.rand", "detectron2.structures.ImageList", "detectron2.modeling.roi_heads.StandardROIHeads().eval", "detectron2.structures.Instances", "torch.tensor", "detectron2.structures.Boxes", "torch.tensor", "detectron2.structures.Instances", "torch.tensor", "detectron2.structures.Boxes", "torch.tensor", "detectron2.modeling.roi_heads.StandardROIHeads().eval.", "zip", "torch.rand", "detectron2.layers.ShapeSpec", "detectron2.export.torchscript_patch.freeze_training_mode", "detectron2.export.torchscript_patch.patch_instances", "new_instances.from_instances", "new_instances.from_instances", "torch.jit.script", "torch.jit.script.", "detectron2.utils.testing.assert_instances_allclose", "detectron2.modeling.roi_heads.StandardROIHeads"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.freeze_training_mode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.assert_instances_allclose"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_StandardROIHeads_scriptability", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NAME", "=", "\"FastRCNNConvFCHead\"", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NUM_FC", "=", "2", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "=", "\"ROIAlignV2\"", "\n", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_WEIGHTS", "=", "(", "10", ",", "10", ",", "5", ",", "5", ")", "\n", "cfg", ".", "MODEL", ".", "MASK_ON", "=", "True", "\n", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NMS_THRESH_TEST", "=", "0.01", "\n", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "SCORE_THRESH_TEST", "=", "0.01", "\n", "num_images", "=", "2", "\n", "images_tensor", "=", "torch", ".", "rand", "(", "num_images", ",", "20", ",", "30", ")", "\n", "image_sizes", "=", "[", "(", "10", ",", "10", ")", ",", "(", "20", ",", "30", ")", "]", "\n", "images", "=", "ImageList", "(", "images_tensor", ",", "image_sizes", ")", "\n", "num_channels", "=", "1024", "\n", "features", "=", "{", "\"res4\"", ":", "torch", ".", "rand", "(", "num_images", ",", "num_channels", ",", "1", ",", "2", ")", "}", "\n", "feature_shape", "=", "{", "\"res4\"", ":", "ShapeSpec", "(", "channels", "=", "num_channels", ",", "stride", "=", "16", ")", "}", "\n", "\n", "roi_heads", "=", "StandardROIHeads", "(", "cfg", ",", "feature_shape", ")", ".", "eval", "(", ")", "\n", "\n", "proposal0", "=", "Instances", "(", "image_sizes", "[", "0", "]", ")", "\n", "proposal_boxes0", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "1", ",", "3", ",", "3", "]", ",", "[", "2", ",", "2", ",", "6", ",", "6", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "proposal0", ".", "proposal_boxes", "=", "Boxes", "(", "proposal_boxes0", ")", "\n", "proposal0", ".", "objectness_logits", "=", "torch", ".", "tensor", "(", "[", "0.5", ",", "0.7", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "proposal1", "=", "Instances", "(", "image_sizes", "[", "1", "]", ")", "\n", "proposal_boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "5", ",", "2", ",", "8", "]", ",", "[", "7", ",", "3", ",", "10", ",", "5", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "proposal1", ".", "proposal_boxes", "=", "Boxes", "(", "proposal_boxes1", ")", "\n", "proposal1", ".", "objectness_logits", "=", "torch", ".", "tensor", "(", "[", "0.1", ",", "0.9", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "proposals", "=", "[", "proposal0", ",", "proposal1", "]", "\n", "\n", "pred_instances", ",", "_", "=", "roi_heads", "(", "images", ",", "features", ",", "proposals", ")", "\n", "fields", "=", "{", "\n", "\"objectness_logits\"", ":", "torch", ".", "Tensor", ",", "\n", "\"proposal_boxes\"", ":", "Boxes", ",", "\n", "\"pred_classes\"", ":", "torch", ".", "Tensor", ",", "\n", "\"scores\"", ":", "torch", ".", "Tensor", ",", "\n", "\"pred_masks\"", ":", "torch", ".", "Tensor", ",", "\n", "\"pred_boxes\"", ":", "Boxes", ",", "\n", "\"pred_keypoints\"", ":", "torch", ".", "Tensor", ",", "\n", "\"pred_keypoint_heatmaps\"", ":", "torch", ".", "Tensor", ",", "\n", "}", "\n", "with", "freeze_training_mode", "(", "roi_heads", ")", ",", "patch_instances", "(", "fields", ")", "as", "new_instances", ":", "\n", "            ", "proposal0", "=", "new_instances", ".", "from_instances", "(", "proposal0", ")", "\n", "proposal1", "=", "new_instances", ".", "from_instances", "(", "proposal1", ")", "\n", "proposals", "=", "[", "proposal0", ",", "proposal1", "]", "\n", "scripted_rot_heads", "=", "torch", ".", "jit", ".", "script", "(", "roi_heads", ")", "\n", "scripted_pred_instances", ",", "_", "=", "scripted_rot_heads", "(", "images", ",", "features", ",", "proposals", ")", "\n", "\n", "", "for", "instance", ",", "scripted_instance", "in", "zip", "(", "pred_instances", ",", "scripted_pred_instances", ")", ":", "\n", "            ", "assert_instances_allclose", "(", "instance", ",", "scripted_instance", ",", "rtol", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_roi_heads.ROIHeadsTest.test_PointRend_mask_head_tracing": [[283, 326], ["unittest.skipIf", "detectron2.model_zoo.get_config", "detectron2.projects.point_rend.add_pointrend_config", "detectron2.projects.point_rend.PointRendMaskHead", "Wrap", "Wrap.eval", "torch.rand", "torch.rand", "detectron2.utils.testing.random_boxes", "torch.no_grad", "detectron2.export.torchscript_patch.patch_builtin_len", "torch.jit.trace", "test_roi_heads.ROIHeadsTest.test_PointRend_mask_head_tracing.gen_inputs"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_builtin_len"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_PointRend_mask_head_tracing", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "model_zoo", ".", "get_config", "(", "\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"", ")", "\n", "point_rend", ".", "add_pointrend_config", "(", "cfg", ")", "\n", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "=", "[", "\"p2\"", ",", "\"p3\"", "]", "\n", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "NAME", "=", "\"PointRendMaskHead\"", "\n", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_TYPE", "=", "\"\"", "\n", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POINT_HEAD_ON", "=", "True", "\n", "chan", "=", "256", "\n", "head", "=", "point_rend", ".", "PointRendMaskHead", "(", "\n", "cfg", ",", "\n", "{", "\n", "\"p2\"", ":", "ShapeSpec", "(", "channels", "=", "chan", ",", "stride", "=", "4", ")", ",", "\n", "\"p3\"", ":", "ShapeSpec", "(", "channels", "=", "chan", ",", "stride", "=", "8", ")", ",", "\n", "}", ",", "\n", ")", "\n", "\n", "def", "gen_inputs", "(", "h", ",", "w", ",", "N", ")", ":", "\n", "            ", "p2", "=", "torch", ".", "rand", "(", "1", ",", "chan", ",", "h", ",", "w", ")", "\n", "p3", "=", "torch", ".", "rand", "(", "1", ",", "chan", ",", "h", "//", "2", ",", "w", "//", "2", ")", "\n", "boxes", "=", "random_boxes", "(", "N", ",", "max_coord", "=", "h", ")", "\n", "return", "p2", ",", "p3", ",", "boxes", "\n", "\n", "", "class", "Wrap", "(", "nn", ".", "ModuleDict", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "p2", ",", "p3", ",", "boxes", ")", ":", "\n", "                ", "features", "=", "{", "\n", "\"p2\"", ":", "p2", ",", "\n", "\"p3\"", ":", "p3", ",", "\n", "}", "\n", "inst", "=", "Instances", "(", "(", "p2", ".", "shape", "[", "2", "]", "*", "4", ",", "p2", ".", "shape", "[", "3", "]", "*", "4", ")", ")", "\n", "inst", ".", "pred_boxes", "=", "Boxes", "(", "boxes", ")", "\n", "inst", ".", "pred_classes", "=", "torch", ".", "zeros", "(", "inst", ".", "__len__", "(", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "out", "=", "self", ".", "head", "(", "features", ",", "[", "inst", "]", ")", "[", "0", "]", "\n", "return", "out", ".", "pred_masks", "\n", "\n", "", "", "model", "=", "Wrap", "(", "{", "\"head\"", ":", "head", "}", ")", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ",", "patch_builtin_len", "(", ")", ":", "\n", "            ", "traced", "=", "torch", ".", "jit", ".", "trace", "(", "model", ",", "gen_inputs", "(", "302", ",", "208", ",", "20", ")", ")", "\n", "inputs", "=", "gen_inputs", "(", "100", ",", "120", ",", "30", ")", "\n", "out_eager", "=", "model", "(", "*", "inputs", ")", "\n", "out_trace", "=", "traced", "(", "*", "inputs", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "out_eager", ",", "out_trace", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest.setUp": [[77, 80], ["torch.manual_seed", "detectron2.utils.testing.get_model_no_weights"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.get_model_no_weights"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "43", ")", "\n", "self", ".", "model", "=", "get_model_no_weights", "(", "self", ".", "CONFIG_PATH", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest._test_eval": [[81, 85], ["test_model_e2e.ModelE2ETest.model.eval", "test_model_e2e.ModelE2ETest.model", "test_model_e2e.create_model_input", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.create_model_input"], ["", "def", "_test_eval", "(", "self", ",", "input_sizes", ")", ":", "\n", "        ", "inputs", "=", "[", "create_model_input", "(", "torch", ".", "rand", "(", "3", ",", "s", "[", "0", "]", ",", "s", "[", "1", "]", ")", ")", "for", "s", "in", "input_sizes", "]", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "model", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest._test_train": [[86, 97], ["test_model_e2e.ModelE2ETest.model.train", "len", "len", "test_model_e2e.create_model_input", "detectron2.utils.events.EventStorage", "test_model_e2e.ModelE2ETest.model", "sum().backward", "torch.rand", "zip", "sum", "test_model_e2e.ModelE2ETest.values"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.create_model_input", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward"], ["", "def", "_test_train", "(", "self", ",", "input_sizes", ",", "instances", ")", ":", "\n", "        ", "assert", "len", "(", "input_sizes", ")", "==", "len", "(", "instances", ")", "\n", "inputs", "=", "[", "\n", "create_model_input", "(", "torch", ".", "rand", "(", "3", ",", "s", "[", "0", "]", ",", "s", "[", "1", "]", ")", ",", "inst", ")", "\n", "for", "s", ",", "inst", "in", "zip", "(", "input_sizes", ",", "instances", ")", "\n", "]", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "with", "EventStorage", "(", ")", ":", "\n", "            ", "losses", "=", "self", ".", "model", "(", "inputs", ")", "\n", "sum", "(", "losses", ".", "values", "(", ")", ")", ".", "backward", "(", ")", "\n", "del", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest._inf_tensor": [[98, 100], ["torch.zeros"], "methods", ["None"], ["", "", "def", "_inf_tensor", "(", "self", ",", "*", "shape", ")", ":", "\n", "        ", "return", "1.0", "/", "torch", ".", "zeros", "(", "*", "shape", ",", "device", "=", "self", ".", "model", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest._nan_tensor": [[101, 103], ["torch.zeros().fill_", "float", "torch.zeros"], "methods", ["None"], ["", "def", "_nan_tensor", "(", "self", ",", "*", "shape", ")", ":", "\n", "        ", "return", "torch", ".", "zeros", "(", "*", "shape", ",", "device", "=", "self", ".", "model", ".", "device", ")", ".", "fill_", "(", "float", "(", "\"nan\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest.test_empty_data": [[104, 108], ["test_model_e2e.ModelE2ETest._test_eval", "test_model_e2e.ModelE2ETest._test_train", "test_model_e2e.get_empty_instance", "test_model_e2e.get_empty_instance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest._test_eval", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest._test_train", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.get_empty_instance", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.get_empty_instance"], ["", "def", "test_empty_data", "(", "self", ")", ":", "\n", "        ", "instances", "=", "[", "get_empty_instance", "(", "200", ",", "250", ")", ",", "get_empty_instance", "(", "200", ",", "249", ")", "]", "\n", "self", ".", "_test_eval", "(", "[", "(", "200", ",", "250", ")", ",", "(", "200", ",", "249", ")", "]", ")", "\n", "self", ".", "_test_train", "(", "[", "(", "200", ",", "250", ")", ",", "(", "200", ",", "249", ")", "]", ",", "instances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest.test_eval_tocpu": [[109, 116], ["unittest.skipIf", "copy.deepcopy().cpu", "copy.deepcopy().cpu.eval", "copy.deepcopy().cpu.", "test_model_e2e.create_model_input", "torch.cuda.is_available", "copy.deepcopy", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.create_model_input"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA unavailable\"", ")", "\n", "def", "test_eval_tocpu", "(", "self", ")", ":", "\n", "        ", "model", "=", "deepcopy", "(", "self", ".", "model", ")", ".", "cpu", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "input_sizes", "=", "[", "(", "200", ",", "250", ")", ",", "(", "200", ",", "249", ")", "]", "\n", "inputs", "=", "[", "create_model_input", "(", "torch", ".", "rand", "(", "3", ",", "s", "[", "0", "]", ",", "s", "[", "1", "]", ")", ")", "for", "s", "in", "input_sizes", "]", "\n", "model", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.MaskRCNNE2ETest.test_half_empty_data": [[121, 124], ["test_model_e2e.MaskRCNNE2ETest._test_train", "test_model_e2e.get_empty_instance", "test_model_e2e.get_regular_bitmask_instances"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.ModelE2ETest._test_train", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.get_empty_instance", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.get_regular_bitmask_instances"], ["def", "test_half_empty_data", "(", "self", ")", ":", "\n", "        ", "instances", "=", "[", "get_empty_instance", "(", "200", ",", "250", ")", ",", "get_regular_bitmask_instances", "(", "200", ",", "249", ")", "]", "\n", "self", ".", "_test_train", "(", "[", "(", "200", ",", "250", ")", ",", "(", "200", ",", "249", ")", "]", ",", "instances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.MaskRCNNE2ETest.test_roiheads_inf_nan_data": [[140, 156], ["test_model_e2e.MaskRCNNE2ETest.model.eval", "detectron2.structures.ImageList", "detectron2.structures.Boxes().to", "torch.tensor().reshape", "test_model_e2e.MaskRCNNE2ETest.model.roi_heads", "test_model_e2e.MaskRCNNE2ETest.assertEqual", "tensor", "tensor", "tensor", "tensor", "tensor", "tensor", "detectron2.structures.Instances", "len", "detectron2.structures.Boxes", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "test_roiheads_inf_nan_data", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "for", "tensor", "in", "[", "self", ".", "_inf_tensor", ",", "self", ".", "_nan_tensor", "]", ":", "\n", "            ", "images", "=", "ImageList", "(", "tensor", "(", "1", ",", "3", ",", "512", ",", "512", ")", ",", "[", "(", "510", ",", "510", ")", "]", ")", "\n", "features", "=", "{", "\n", "\"p2\"", ":", "tensor", "(", "1", ",", "256", ",", "256", ",", "256", ")", ",", "\n", "\"p3\"", ":", "tensor", "(", "1", ",", "256", ",", "128", ",", "128", ")", ",", "\n", "\"p4\"", ":", "tensor", "(", "1", ",", "256", ",", "64", ",", "64", ")", ",", "\n", "\"p5\"", ":", "tensor", "(", "1", ",", "256", ",", "32", ",", "32", ")", ",", "\n", "\"p6\"", ":", "tensor", "(", "1", ",", "256", ",", "16", ",", "16", ")", ",", "\n", "}", "\n", "props", "=", "[", "Instances", "(", "(", "510", ",", "510", ")", ")", "]", "\n", "props", "[", "0", "]", ".", "proposal_boxes", "=", "Boxes", "(", "[", "[", "10", ",", "10", ",", "20", ",", "20", "]", "]", ")", ".", "to", "(", "device", "=", "self", ".", "model", ".", "device", ")", "\n", "props", "[", "0", "]", ".", "objectness_logits", "=", "torch", ".", "tensor", "(", "[", "1.0", "]", ")", ".", "reshape", "(", "1", ",", "1", ")", "\n", "det", ",", "_", "=", "self", ".", "model", ".", "roi_heads", "(", "images", ",", "features", ",", "props", ")", "\n", "self", ".", "assertEqual", "(", "len", "(", "det", "[", "0", "]", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.MaskRCNNE2ETest.test_autocast": [[157, 172], ["unittest.skipIf", "test_model_e2e.MaskRCNNE2ETest.model.eval", "autocast", "test_model_e2e.typecheck_hook", "test_model_e2e.typecheck_hook", "test_model_e2e.MaskRCNNE2ETest.assertEqual", "test_model_e2e.MaskRCNNE2ETest.assertEqual", "test_model_e2e.MaskRCNNE2ETest.assertEqual", "torch.cuda.is_available", "torch.rand", "test_model_e2e.MaskRCNNE2ETest.model.inference"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.typecheck_hook", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.typecheck_hook", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference"], ["", "", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_autocast", "(", "self", ")", ":", "\n", "        ", "from", "torch", ".", "cuda", ".", "amp", "import", "autocast", "\n", "\n", "inputs", "=", "[", "{", "\"image\"", ":", "torch", ".", "rand", "(", "3", ",", "100", ",", "100", ")", "}", "]", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "with", "autocast", "(", ")", ",", "typecheck_hook", "(", "\n", "self", ".", "model", ".", "backbone", ",", "in_dtype", "=", "torch", ".", "float32", ",", "out_dtype", "=", "torch", ".", "float16", "\n", ")", ",", "typecheck_hook", "(", "\n", "self", ".", "model", ".", "roi_heads", ".", "box_predictor", ",", "in_dtype", "=", "torch", ".", "float16", ",", "out_dtype", "=", "torch", ".", "float16", "\n", ")", ":", "\n", "            ", "out", "=", "self", ".", "model", ".", "inference", "(", "inputs", ",", "do_postprocess", "=", "False", ")", "[", "0", "]", "\n", "self", ".", "assertEqual", "(", "out", ".", "pred_boxes", ".", "tensor", ".", "dtype", ",", "torch", ".", "float32", ")", "\n", "self", ".", "assertEqual", "(", "out", ".", "pred_masks", ".", "dtype", ",", "torch", ".", "float16", ")", "\n", "self", ".", "assertEqual", "(", "out", ".", "scores", ".", "dtype", ",", "torch", ".", "float32", ")", "# scores comes from softmax", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.RetinaNetE2ETest.test_inf_nan_data": [[177, 199], ["test_model_e2e.RetinaNetE2ETest.model.eval", "detectron2.structures.ImageList", "test_model_e2e.RetinaNetE2ETest.model.anchor_generator", "test_model_e2e.RetinaNetE2ETest.model.head", "test_model_e2e.RetinaNetE2ETest.model.inference", "len", "tensor", "tensor", "tensor", "tensor", "tensor", "tensor", "tensor", "tensor", "test_model_e2e.RetinaNetE2ETest.assertTrue", "numpy.prod", "torch.isfinite().sum", "torch.isfinite"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference"], ["def", "test_inf_nan_data", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "model", ".", "score_threshold", "=", "-", "999999999", "\n", "for", "tensor", "in", "[", "self", ".", "_inf_tensor", ",", "self", ".", "_nan_tensor", "]", ":", "\n", "            ", "images", "=", "ImageList", "(", "tensor", "(", "1", ",", "3", ",", "512", ",", "512", ")", ",", "[", "(", "510", ",", "510", ")", "]", ")", "\n", "features", "=", "[", "\n", "tensor", "(", "1", ",", "256", ",", "128", ",", "128", ")", ",", "\n", "tensor", "(", "1", ",", "256", ",", "64", ",", "64", ")", ",", "\n", "tensor", "(", "1", ",", "256", ",", "32", ",", "32", ")", ",", "\n", "tensor", "(", "1", ",", "256", ",", "16", ",", "16", ")", ",", "\n", "tensor", "(", "1", ",", "256", ",", "8", ",", "8", ")", ",", "\n", "]", "\n", "anchors", "=", "self", ".", "model", ".", "anchor_generator", "(", "features", ")", "\n", "_", ",", "pred_anchor_deltas", "=", "self", ".", "model", ".", "head", "(", "features", ")", "\n", "HWAs", "=", "[", "np", ".", "prod", "(", "x", ".", "shape", "[", "-", "3", ":", "]", ")", "//", "4", "for", "x", "in", "pred_anchor_deltas", "]", "\n", "\n", "pred_logits", "=", "[", "tensor", "(", "1", ",", "HWA", ",", "self", ".", "model", ".", "num_classes", ")", "for", "HWA", "in", "HWAs", "]", "\n", "pred_anchor_deltas", "=", "[", "tensor", "(", "1", ",", "HWA", ",", "4", ")", "for", "HWA", "in", "HWAs", "]", "\n", "det", "=", "self", ".", "model", ".", "inference", "(", "anchors", ",", "pred_logits", ",", "pred_anchor_deltas", ",", "images", ".", "image_sizes", ")", "\n", "# all predictions (if any) are infinite or nan", "\n", "if", "len", "(", "det", "[", "0", "]", ")", ":", "\n", "                ", "self", ".", "assertTrue", "(", "torch", ".", "isfinite", "(", "det", "[", "0", "]", ".", "pred_boxes", ".", "tensor", ")", ".", "sum", "(", ")", "==", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.RetinaNetE2ETest.test_autocast": [[200, 212], ["unittest.skipIf", "test_model_e2e.RetinaNetE2ETest.model.eval", "autocast", "test_model_e2e.typecheck_hook", "test_model_e2e.typecheck_hook", "test_model_e2e.RetinaNetE2ETest.assertEqual", "test_model_e2e.RetinaNetE2ETest.assertEqual", "torch.cuda.is_available", "torch.rand", "test_model_e2e.RetinaNetE2ETest.model"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.typecheck_hook", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.typecheck_hook"], ["", "", "", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_autocast", "(", "self", ")", ":", "\n", "        ", "from", "torch", ".", "cuda", ".", "amp", "import", "autocast", "\n", "\n", "inputs", "=", "[", "{", "\"image\"", ":", "torch", ".", "rand", "(", "3", ",", "100", ",", "100", ")", "}", "]", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "with", "autocast", "(", ")", ",", "typecheck_hook", "(", "\n", "self", ".", "model", ".", "backbone", ",", "in_dtype", "=", "torch", ".", "float32", ",", "out_dtype", "=", "torch", ".", "float16", "\n", ")", ",", "typecheck_hook", "(", "self", ".", "model", ".", "head", ",", "in_dtype", "=", "torch", ".", "float16", ",", "out_dtype", "=", "torch", ".", "float16", ")", ":", "\n", "            ", "out", "=", "self", ".", "model", "(", "inputs", ")", "[", "0", "]", "[", "\"instances\"", "]", "\n", "self", ".", "assertEqual", "(", "out", ".", "pred_boxes", ".", "tensor", ".", "dtype", ",", "torch", ".", "float32", ")", "\n", "self", ".", "assertEqual", "(", "out", ".", "scores", ".", "dtype", ",", "torch", ".", "float16", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.typecheck_hook": [[16, 50], ["isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "model.register_forward_hook", "list", "test_model_e2e.typecheck_hook.flatten"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["@", "contextmanager", "\n", "def", "typecheck_hook", "(", "model", ",", "*", ",", "in_dtype", "=", "None", ",", "out_dtype", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Check that the model must be called with the given input/output dtype\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "in_dtype", ",", "set", ")", ":", "\n", "        ", "in_dtype", "=", "{", "in_dtype", "}", "\n", "", "if", "not", "isinstance", "(", "out_dtype", ",", "set", ")", ":", "\n", "        ", "out_dtype", "=", "{", "out_dtype", "}", "\n", "\n", "", "def", "flatten", "(", "x", ")", ":", "\n", "        ", "if", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "return", "[", "x", "]", "\n", "", "if", "isinstance", "(", "x", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "return", "list", "(", "itertools", ".", "chain", "(", "*", "[", "flatten", "(", "t", ")", "for", "t", "in", "x", "]", ")", ")", "\n", "", "if", "isinstance", "(", "x", ",", "dict", ")", ":", "\n", "            ", "return", "flatten", "(", "list", "(", "x", ".", "values", "(", ")", ")", ")", "\n", "", "return", "[", "]", "\n", "\n", "", "def", "hook", "(", "module", ",", "input", ",", "output", ")", ":", "\n", "        ", "if", "in_dtype", "is", "not", "None", ":", "\n", "            ", "dtypes", "=", "{", "x", ".", "dtype", "for", "x", "in", "flatten", "(", "input", ")", "}", "\n", "assert", "(", "\n", "dtypes", "==", "in_dtype", "\n", ")", ",", "f\"Expected input dtype of {type(module)} is {in_dtype}. Got {dtypes} instead!\"", "\n", "\n", "", "if", "out_dtype", "is", "not", "None", ":", "\n", "            ", "dtypes", "=", "{", "x", ".", "dtype", "for", "x", "in", "flatten", "(", "output", ")", "}", "\n", "assert", "(", "\n", "dtypes", "==", "out_dtype", "\n", ")", ",", "f\"Expected output dtype of {type(module)} is {out_dtype}. Got {dtypes} instead!\"", "\n", "\n", "", "", "with", "model", ".", "register_forward_hook", "(", "hook", ")", ":", "\n", "        ", "yield", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.create_model_input": [[52, 57], ["None"], "function", ["None"], ["", "", "def", "create_model_input", "(", "img", ",", "inst", "=", "None", ")", ":", "\n", "    ", "if", "inst", "is", "not", "None", ":", "\n", "        ", "return", "{", "\"image\"", ":", "img", ",", "\"instances\"", ":", "inst", "}", "\n", "", "else", ":", "\n", "        ", "return", "{", "\"image\"", ":", "img", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.get_empty_instance": [[59, 65], ["detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.tensor().to", "detectron2.structures.BitMasks", "torch.rand", "torch.rand", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "", "def", "get_empty_instance", "(", "h", ",", "w", ")", ":", "\n", "    ", "inst", "=", "Instances", "(", "(", "h", ",", "w", ")", ")", "\n", "inst", ".", "gt_boxes", "=", "Boxes", "(", "torch", ".", "rand", "(", "0", ",", "4", ")", ")", "\n", "inst", ".", "gt_classes", "=", "torch", ".", "tensor", "(", "[", "]", ")", ".", "to", "(", "dtype", "=", "torch", ".", "int64", ")", "\n", "inst", ".", "gt_masks", "=", "BitMasks", "(", "torch", ".", "rand", "(", "0", ",", "h", ",", "w", ")", ")", "\n", "return", "inst", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_model_e2e.get_regular_bitmask_instances": [[67, 74], ["detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.tensor().to", "detectron2.structures.BitMasks", "torch.rand", "torch.tensor", "torch.rand"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "get_regular_bitmask_instances", "(", "h", ",", "w", ")", ":", "\n", "    ", "inst", "=", "Instances", "(", "(", "h", ",", "w", ")", ")", "\n", "inst", ".", "gt_boxes", "=", "Boxes", "(", "torch", ".", "rand", "(", "3", ",", "4", ")", ")", "\n", "inst", ".", "gt_boxes", ".", "tensor", "[", ":", ",", "2", ":", "]", "+=", "inst", ".", "gt_boxes", ".", "tensor", "[", ":", ",", ":", "2", "]", "\n", "inst", ".", "gt_classes", "=", "torch", ".", "tensor", "(", "[", "3", ",", "4", ",", "5", "]", ")", ".", "to", "(", "dtype", "=", "torch", ".", "int64", ")", "\n", "inst", ".", "gt_masks", "=", "BitMasks", "(", "(", "torch", ".", "rand", "(", "3", ",", "h", ",", "w", ")", ">", "0.5", ")", ")", "\n", "return", "inst", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_anchor_generator.TestAnchorGenerator.test_default_anchor_generator": [[14, 43], ["detectron2.config.get_cfg", "detectron2.modeling.anchor_generator.DefaultAnchorGenerator", "detectron2.modeling.anchor_generator.DefaultAnchorGenerator.", "torch.tensor", "test_anchor_generator.TestAnchorGenerator.assertTrue", "torch.rand", "torch.allclose", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg"], ["    ", "def", "test_default_anchor_generator", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "SIZES", "=", "[", "[", "32", ",", "64", "]", "]", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ASPECT_RATIOS", "=", "[", "[", "0.25", ",", "1", ",", "4", "]", "]", "\n", "\n", "anchor_generator", "=", "DefaultAnchorGenerator", "(", "cfg", ",", "[", "ShapeSpec", "(", "stride", "=", "4", ")", "]", ")", "\n", "\n", "# only the last two dimensions of features matter here", "\n", "num_images", "=", "2", "\n", "features", "=", "{", "\"stage3\"", ":", "torch", ".", "rand", "(", "num_images", ",", "96", ",", "1", ",", "2", ")", "}", "\n", "anchors", "=", "anchor_generator", "(", "[", "features", "[", "\"stage3\"", "]", "]", ")", "\n", "expected_anchor_tensor", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "-", "32.0", ",", "-", "8.0", ",", "32.0", ",", "8.0", "]", ",", "\n", "[", "-", "16.0", ",", "-", "16.0", ",", "16.0", ",", "16.0", "]", ",", "\n", "[", "-", "8.0", ",", "-", "32.0", ",", "8.0", ",", "32.0", "]", ",", "\n", "[", "-", "64.0", ",", "-", "16.0", ",", "64.0", ",", "16.0", "]", ",", "\n", "[", "-", "32.0", ",", "-", "32.0", ",", "32.0", ",", "32.0", "]", ",", "\n", "[", "-", "16.0", ",", "-", "64.0", ",", "16.0", ",", "64.0", "]", ",", "\n", "[", "-", "28.0", ",", "-", "8.0", ",", "36.0", ",", "8.0", "]", ",", "# -28.0 == -32.0 + STRIDE (4)", "\n", "[", "-", "12.0", ",", "-", "16.0", ",", "20.0", ",", "16.0", "]", ",", "\n", "[", "-", "4.0", ",", "-", "32.0", ",", "12.0", ",", "32.0", "]", ",", "\n", "[", "-", "60.0", ",", "-", "16.0", ",", "68.0", ",", "16.0", "]", ",", "\n", "[", "-", "28.0", ",", "-", "32.0", ",", "36.0", ",", "32.0", "]", ",", "\n", "[", "-", "12.0", ",", "-", "64.0", ",", "20.0", ",", "64.0", "]", ",", "\n", "]", "\n", ")", "\n", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "anchors", "[", "0", "]", ".", "tensor", ",", "expected_anchor_tensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_anchor_generator.TestAnchorGenerator.test_default_anchor_generator_centered": [[44, 75], ["detectron2.modeling.anchor_generator.DefaultAnchorGenerator", "torch.tensor", "detectron2.modeling.anchor_generator.DefaultAnchorGenerator.", "test_anchor_generator.TestAnchorGenerator.assertTrue", "test_anchor_generator.TestAnchorGenerator.assertTrue", "torch.rand", "torch.allclose", "torch.jit.script", "torch.allclose"], "methods", ["None"], ["", "def", "test_default_anchor_generator_centered", "(", "self", ")", ":", "\n", "# test explicit args", "\n", "        ", "anchor_generator", "=", "DefaultAnchorGenerator", "(", "\n", "sizes", "=", "[", "32", ",", "64", "]", ",", "aspect_ratios", "=", "[", "0.25", ",", "1", ",", "4", "]", ",", "strides", "=", "[", "4", "]", "\n", ")", "\n", "\n", "# only the last two dimensions of features matter here", "\n", "num_images", "=", "2", "\n", "features", "=", "{", "\"stage3\"", ":", "torch", ".", "rand", "(", "num_images", ",", "96", ",", "1", ",", "2", ")", "}", "\n", "expected_anchor_tensor", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "-", "30.0", ",", "-", "6.0", ",", "34.0", ",", "10.0", "]", ",", "\n", "[", "-", "14.0", ",", "-", "14.0", ",", "18.0", ",", "18.0", "]", ",", "\n", "[", "-", "6.0", ",", "-", "30.0", ",", "10.0", ",", "34.0", "]", ",", "\n", "[", "-", "62.0", ",", "-", "14.0", ",", "66.0", ",", "18.0", "]", ",", "\n", "[", "-", "30.0", ",", "-", "30.0", ",", "34.0", ",", "34.0", "]", ",", "\n", "[", "-", "14.0", ",", "-", "62.0", ",", "18.0", ",", "66.0", "]", ",", "\n", "[", "-", "26.0", ",", "-", "6.0", ",", "38.0", ",", "10.0", "]", ",", "\n", "[", "-", "10.0", ",", "-", "14.0", ",", "22.0", ",", "18.0", "]", ",", "\n", "[", "-", "2.0", ",", "-", "30.0", ",", "14.0", ",", "34.0", "]", ",", "\n", "[", "-", "58.0", ",", "-", "14.0", ",", "70.0", ",", "18.0", "]", ",", "\n", "[", "-", "26.0", ",", "-", "30.0", ",", "38.0", ",", "34.0", "]", ",", "\n", "[", "-", "10.0", ",", "-", "62.0", ",", "22.0", ",", "66.0", "]", ",", "\n", "]", "\n", ")", "\n", "\n", "anchors", "=", "anchor_generator", "(", "[", "features", "[", "\"stage3\"", "]", "]", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "anchors", "[", "0", "]", ".", "tensor", ",", "expected_anchor_tensor", ")", ")", "\n", "\n", "anchors", "=", "torch", ".", "jit", ".", "script", "(", "anchor_generator", ")", "(", "[", "features", "[", "\"stage3\"", "]", "]", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "anchors", "[", "0", "]", ".", "tensor", ",", "expected_anchor_tensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_anchor_generator.TestAnchorGenerator.test_rrpn_anchor_generator": [[76, 117], ["detectron2.config.get_cfg", "detectron2.modeling.anchor_generator.RotatedAnchorGenerator", "detectron2.modeling.anchor_generator.RotatedAnchorGenerator.", "torch.tensor", "test_anchor_generator.TestAnchorGenerator.assertTrue", "torch.rand", "torch.allclose", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg"], ["", "def", "test_rrpn_anchor_generator", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "SIZES", "=", "[", "[", "32", ",", "64", "]", "]", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ASPECT_RATIOS", "=", "[", "[", "0.25", ",", "1", ",", "4", "]", "]", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ANGLES", "=", "[", "0", ",", "45", "]", "# test single list[float]", "\n", "anchor_generator", "=", "RotatedAnchorGenerator", "(", "cfg", ",", "[", "ShapeSpec", "(", "stride", "=", "4", ")", "]", ")", "\n", "\n", "# only the last two dimensions of features matter here", "\n", "num_images", "=", "2", "\n", "features", "=", "{", "\"stage3\"", ":", "torch", ".", "rand", "(", "num_images", ",", "96", ",", "1", ",", "2", ")", "}", "\n", "anchors", "=", "anchor_generator", "(", "[", "features", "[", "\"stage3\"", "]", "]", ")", "\n", "expected_anchor_tensor", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "0.0", ",", "0.0", ",", "64.0", ",", "16.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "64.0", ",", "16.0", ",", "45.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "32.0", ",", "32.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "32.0", ",", "32.0", ",", "45.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "16.0", ",", "64.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "16.0", ",", "64.0", ",", "45.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "128.0", ",", "32.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "128.0", ",", "32.0", ",", "45.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "64.0", ",", "64.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "64.0", ",", "64.0", ",", "45.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "32.0", ",", "128.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "32.0", ",", "128.0", ",", "45.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "64.0", ",", "16.0", ",", "0.0", "]", ",", "# 4.0 == 0.0 + STRIDE (4)", "\n", "[", "4.0", ",", "0.0", ",", "64.0", ",", "16.0", ",", "45.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "32.0", ",", "32.0", ",", "0.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "32.0", ",", "32.0", ",", "45.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "16.0", ",", "64.0", ",", "0.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "16.0", ",", "64.0", ",", "45.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "128.0", ",", "32.0", ",", "0.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "128.0", ",", "32.0", ",", "45.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "64.0", ",", "64.0", ",", "0.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "64.0", ",", "64.0", ",", "45.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "32.0", ",", "128.0", ",", "0.0", "]", ",", "\n", "[", "4.0", ",", "0.0", ",", "32.0", ",", "128.0", ",", "45.0", "]", ",", "\n", "]", "\n", ")", "\n", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "anchors", "[", "0", "]", ".", "tensor", ",", "expected_anchor_tensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_box2box_transform.TestBox2BoxTransform.test_reconstruction": [[14, 29], ["detectron2.modeling.box_regression.Box2BoxTransform", "detectron2.utils.testing.random_boxes", "detectron2.utils.testing.random_boxes", "torch.cuda.is_available", "torch.device", "devices.append", "src_boxes.to.to.to", "dst_boxes.to.to.to", "detectron2.modeling.box_regression.Box2BoxTransform.get_deltas", "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "test_box2box_transform.TestBox2BoxTransform.assertTrue", "torch.device", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.get_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["    ", "def", "test_reconstruction", "(", "self", ")", ":", "\n", "        ", "weights", "=", "(", "5", ",", "5", ",", "10", ",", "10", ")", "\n", "b2b_tfm", "=", "Box2BoxTransform", "(", "weights", "=", "weights", ")", "\n", "src_boxes", "=", "random_boxes", "(", "10", ")", "\n", "dst_boxes", "=", "random_boxes", "(", "10", ")", "\n", "\n", "devices", "=", "[", "torch", ".", "device", "(", "\"cpu\"", ")", "]", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "devices", ".", "append", "(", "torch", ".", "device", "(", "\"cuda\"", ")", ")", "\n", "", "for", "device", "in", "devices", ":", "\n", "            ", "src_boxes", "=", "src_boxes", ".", "to", "(", "device", "=", "device", ")", "\n", "dst_boxes", "=", "dst_boxes", ".", "to", "(", "device", "=", "device", ")", "\n", "deltas", "=", "b2b_tfm", ".", "get_deltas", "(", "src_boxes", ",", "dst_boxes", ")", "\n", "dst_boxes_reconstructed", "=", "b2b_tfm", ".", "apply_deltas", "(", "deltas", ",", "src_boxes", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "dst_boxes", ",", "dst_boxes_reconstructed", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_box2box_transform.TestBox2BoxTransform.test_apply_deltas_tracing": [[30, 42], ["unittest.skipIf", "detectron2.modeling.box_regression.Box2BoxTransform", "torch.no_grad", "torch.jit.trace", "torch.jit.trace.", "test_box2box_transform.TestBox2BoxTransform.assertEqual", "torch.jit.trace.", "test_box2box_transform.TestBox2BoxTransform.assertEqual", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["None"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_apply_deltas_tracing", "(", "self", ")", ":", "\n", "        ", "weights", "=", "(", "5", ",", "5", ",", "10", ",", "10", ")", "\n", "b2b_tfm", "=", "Box2BoxTransform", "(", "weights", "=", "weights", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "func", "=", "torch", ".", "jit", ".", "trace", "(", "b2b_tfm", ".", "apply_deltas", ",", "(", "torch", ".", "randn", "(", "10", ",", "20", ")", ",", "torch", ".", "randn", "(", "10", ",", "4", ")", ")", ")", "\n", "\n", "o", "=", "func", "(", "torch", ".", "randn", "(", "10", ",", "20", ")", ",", "torch", ".", "randn", "(", "10", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "o", ".", "shape", ",", "(", "10", ",", "20", ")", ")", "\n", "o", "=", "func", "(", "torch", ".", "randn", "(", "5", ",", "20", ")", ",", "torch", ".", "randn", "(", "5", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "o", ".", "shape", ",", "(", "5", ",", "20", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_box2box_transform.TestBox2BoxTransformRotated.test_reconstruction": [[51, 71], ["detectron2.modeling.box_regression.Box2BoxTransformRotated", "test_box2box_transform.random_rotated_boxes", "test_box2box_transform.random_rotated_boxes", "torch.cuda.is_available", "torch.device", "devices.append", "src_boxes.to.to.to", "dst_boxes.to.to.to", "detectron2.modeling.box_regression.Box2BoxTransformRotated.get_deltas", "detectron2.modeling.box_regression.Box2BoxTransformRotated.apply_deltas", "torch.allclose", "torch.allclose", "torch.device", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_box2box_transform.random_rotated_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_box2box_transform.random_rotated_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.get_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["    ", "def", "test_reconstruction", "(", "self", ")", ":", "\n", "        ", "weights", "=", "(", "5", ",", "5", ",", "10", ",", "10", ",", "1", ")", "\n", "b2b_transform", "=", "Box2BoxTransformRotated", "(", "weights", "=", "weights", ")", "\n", "src_boxes", "=", "random_rotated_boxes", "(", "[", "10", ",", "10", ",", "20", ",", "20", ",", "-", "30", "]", ",", "5", ",", "60.0", ",", "10", ")", "\n", "dst_boxes", "=", "random_rotated_boxes", "(", "[", "10", ",", "10", ",", "20", ",", "20", ",", "-", "30", "]", ",", "5", ",", "60.0", ",", "10", ")", "\n", "\n", "devices", "=", "[", "torch", ".", "device", "(", "\"cpu\"", ")", "]", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "devices", ".", "append", "(", "torch", ".", "device", "(", "\"cuda\"", ")", ")", "\n", "", "for", "device", "in", "devices", ":", "\n", "            ", "src_boxes", "=", "src_boxes", ".", "to", "(", "device", "=", "device", ")", "\n", "dst_boxes", "=", "dst_boxes", ".", "to", "(", "device", "=", "device", ")", "\n", "deltas", "=", "b2b_transform", ".", "get_deltas", "(", "src_boxes", ",", "dst_boxes", ")", "\n", "dst_boxes_reconstructed", "=", "b2b_transform", ".", "apply_deltas", "(", "deltas", ",", "src_boxes", ")", "\n", "assert", "torch", ".", "allclose", "(", "dst_boxes", "[", ":", ",", ":", "4", "]", ",", "dst_boxes_reconstructed", "[", ":", ",", ":", "4", "]", ",", "atol", "=", "1e-5", ")", "\n", "# angle difference has to be normalized", "\n", "assert", "torch", ".", "allclose", "(", "\n", "(", "dst_boxes", "[", ":", ",", "4", "]", "-", "dst_boxes_reconstructed", "[", ":", ",", "4", "]", "+", "180.0", ")", "%", "360.0", "-", "180.0", ",", "\n", "torch", ".", "zeros_like", "(", "dst_boxes", "[", ":", ",", "4", "]", ")", ",", "\n", "atol", "=", "1e-4", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_box2box_transform.random_rotated_boxes": [[44, 48], ["torch.cat", "torch.tensor", "torch.rand", "torch.rand"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "", "", "def", "random_rotated_boxes", "(", "mean_box", ",", "std_length", ",", "std_angle", ",", "N", ")", ":", "\n", "    ", "return", "torch", ".", "cat", "(", "\n", "[", "torch", ".", "rand", "(", "N", ",", "4", ")", "*", "std_length", ",", "torch", ".", "rand", "(", "N", ",", "1", ")", "*", "std_angle", "]", ",", "dim", "=", "1", "\n", ")", "+", "torch", ".", "tensor", "(", "mean_box", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.get_gt_and_features": [[24, 36], ["torch.rand", "detectron2.structures.ImageList", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.rand"], "methods", ["None"], ["    ", "def", "get_gt_and_features", "(", "self", ")", ":", "\n", "        ", "num_images", "=", "2", "\n", "images_tensor", "=", "torch", ".", "rand", "(", "num_images", ",", "20", ",", "30", ")", "\n", "image_sizes", "=", "[", "(", "10", ",", "10", ")", ",", "(", "20", ",", "30", ")", "]", "\n", "images", "=", "ImageList", "(", "images_tensor", ",", "image_sizes", ")", "\n", "image_shape", "=", "(", "15", ",", "15", ")", "\n", "num_channels", "=", "1024", "\n", "features", "=", "{", "\"res4\"", ":", "torch", ".", "rand", "(", "num_images", ",", "num_channels", ",", "1", ",", "2", ")", "}", "\n", "gt_boxes", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "1", ",", "3", ",", "3", "]", ",", "[", "2", ",", "2", ",", "6", ",", "6", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "gt_instances", "=", "Instances", "(", "image_shape", ")", "\n", "gt_instances", ".", "gt_boxes", "=", "Boxes", "(", "gt_boxes", ")", "\n", "return", "(", "gt_instances", ",", "features", ",", "images", ",", "image_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.test_rpn": [[37, 69], ["torch.manual_seed", "detectron2.config.get_cfg", "detectron2.modeling.backbone.build_backbone", "detectron2.modeling.proposal_generator.RPN", "test_rpn.RPNTest.get_gt_and_features", "expected_losses.keys", "test_rpn.RPNTest.assertEqual", "zip", "torch.tensor", "torch.tensor", "test_rpn.RPNTest.assertTrue", "test_rpn.RPNTest.assertTrue", "detectron2.modeling.backbone.build_backbone.output_shape", "detectron2.utils.events.EventStorage", "detectron2.modeling.proposal_generator.RPN.", "torch.tensor", "torch.tensor", "test_rpn.RPNTest.assertTrue", "len", "len", "test_rpn.RPNTest.assertEqual", "torch.allclose", "torch.allclose", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.get_gt_and_features", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "def", "test_rpn", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "121", ")", "\n", "cfg", "=", "get_cfg", "(", ")", "\n", "backbone", "=", "build_backbone", "(", "cfg", ")", "\n", "proposal_generator", "=", "RPN", "(", "cfg", ",", "backbone", ".", "output_shape", "(", ")", ")", "\n", "(", "gt_instances", ",", "features", ",", "images", ",", "image_sizes", ")", "=", "self", ".", "get_gt_and_features", "(", ")", "\n", "with", "EventStorage", "(", ")", ":", "# capture events in a new storage to discard them", "\n", "            ", "proposals", ",", "proposal_losses", "=", "proposal_generator", "(", "\n", "images", ",", "features", ",", "[", "gt_instances", "[", "0", "]", ",", "gt_instances", "[", "1", "]", "]", "\n", ")", "\n", "\n", "", "expected_losses", "=", "{", "\n", "\"loss_rpn_cls\"", ":", "torch", ".", "tensor", "(", "0.08011703193", ")", ",", "\n", "\"loss_rpn_loc\"", ":", "torch", ".", "tensor", "(", "0.101470276", ")", ",", "\n", "}", "\n", "for", "name", "in", "expected_losses", ".", "keys", "(", ")", ":", "\n", "            ", "err_msg", "=", "\"proposal_losses[{}] = {}, expected losses = {}\"", ".", "format", "(", "\n", "name", ",", "proposal_losses", "[", "name", "]", ",", "expected_losses", "[", "name", "]", "\n", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "proposal_losses", "[", "name", "]", ",", "expected_losses", "[", "name", "]", ")", ",", "err_msg", ")", "\n", "\n", "", "self", ".", "assertEqual", "(", "len", "(", "proposals", ")", ",", "len", "(", "image_sizes", ")", ")", "\n", "for", "proposal", ",", "im_size", "in", "zip", "(", "proposals", ",", "image_sizes", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "proposal", ".", "image_size", ",", "im_size", ")", "\n", "\n", "", "expected_proposal_box", "=", "torch", ".", "tensor", "(", "[", "[", "0", ",", "0", ",", "10", ",", "10", "]", ",", "[", "7.2702", ",", "0", ",", "10", ",", "10", "]", "]", ")", "\n", "expected_objectness_logit", "=", "torch", ".", "tensor", "(", "[", "0.1596", ",", "-", "0.0007", "]", ")", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "allclose", "(", "proposals", "[", "0", "]", ".", "proposal_boxes", ".", "tensor", ",", "expected_proposal_box", ",", "atol", "=", "1e-4", ")", "\n", ")", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "allclose", "(", "proposals", "[", "0", "]", ".", "objectness_logits", ",", "expected_objectness_logit", ",", "atol", "=", "1e-4", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.verify_rpn": [[71, 80], ["torch.manual_seed", "detectron2.config.get_cfg", "detectron2.modeling.backbone.build_backbone", "detectron2.modeling.proposal_generator.RPN", "enumerate", "detectron2.modeling.backbone.build_backbone.output_shape", "test_rpn.RPNTest.assertEqual"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "def", "verify_rpn", "(", "self", ",", "conv_dims", ",", "expected_conv_dims", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "121", ")", "\n", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "MODEL", ".", "RPN", ".", "CONV_DIMS", "=", "conv_dims", "\n", "backbone", "=", "build_backbone", "(", "cfg", ")", "\n", "proposal_generator", "=", "RPN", "(", "cfg", ",", "backbone", ".", "output_shape", "(", ")", ")", "\n", "for", "k", ",", "conv", "in", "enumerate", "(", "proposal_generator", ".", "rpn_head", ".", "conv", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "expected_conv_dims", "[", "k", "]", ",", "conv", ".", "out_channels", ")", "\n", "", "return", "proposal_generator", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.test_rpn_larger_num_convs": [[81, 98], ["test_rpn.RPNTest.verify_rpn", "test_rpn.RPNTest.get_gt_and_features", "expected_losses.keys", "detectron2.utils.events.EventStorage", "test_rpn.RPNTest.", "torch.tensor", "torch.tensor", "test_rpn.RPNTest.assertTrue", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.verify_rpn", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.get_gt_and_features"], ["", "def", "test_rpn_larger_num_convs", "(", "self", ")", ":", "\n", "        ", "conv_dims", "=", "[", "64", ",", "64", ",", "64", ",", "64", ",", "64", "]", "\n", "proposal_generator", "=", "self", ".", "verify_rpn", "(", "conv_dims", ",", "conv_dims", ")", "\n", "(", "gt_instances", ",", "features", ",", "images", ",", "image_sizes", ")", "=", "self", ".", "get_gt_and_features", "(", ")", "\n", "with", "EventStorage", "(", ")", ":", "# capture events in a new storage to discard them", "\n", "            ", "proposals", ",", "proposal_losses", "=", "proposal_generator", "(", "\n", "images", ",", "features", ",", "[", "gt_instances", "[", "0", "]", ",", "gt_instances", "[", "1", "]", "]", "\n", ")", "\n", "", "expected_losses", "=", "{", "\n", "\"loss_rpn_cls\"", ":", "torch", ".", "tensor", "(", "0.08122821152", ")", ",", "\n", "\"loss_rpn_loc\"", ":", "torch", ".", "tensor", "(", "0.10064548254", ")", ",", "\n", "}", "\n", "for", "name", "in", "expected_losses", ".", "keys", "(", ")", ":", "\n", "            ", "err_msg", "=", "\"proposal_losses[{}] = {}, expected losses = {}\"", ".", "format", "(", "\n", "name", ",", "proposal_losses", "[", "name", "]", ",", "expected_losses", "[", "name", "]", "\n", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "proposal_losses", "[", "name", "]", ",", "expected_losses", "[", "name", "]", ")", ",", "err_msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.test_rpn_conv_dims_not_set": [[99, 103], ["test_rpn.RPNTest.verify_rpn"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.verify_rpn"], ["", "", "def", "test_rpn_conv_dims_not_set", "(", "self", ")", ":", "\n", "        ", "conv_dims", "=", "[", "-", "1", ",", "-", "1", ",", "-", "1", "]", "\n", "expected_conv_dims", "=", "[", "1024", ",", "1024", ",", "1024", "]", "\n", "self", ".", "verify_rpn", "(", "conv_dims", ",", "expected_conv_dims", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.test_rpn_scriptability": [[105, 129], ["unittest.skipIf", "detectron2.config.get_cfg", "detectron2.modeling.proposal_generator.RPN().eval", "torch.rand", "detectron2.structures.ImageList", "detectron2.export.scripting_with_instances", "detectron2.modeling.proposal_generator.RPN().eval.", "detectron2.export.scripting_with_instances.", "zip", "torch.rand", "test_rpn.RPNTest.assertEqual", "test_rpn.RPNTest.assertTrue", "test_rpn.RPNTest.assertTrue", "detectron2.modeling.proposal_generator.RPN", "torch.equal", "torch.equal", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.scripting_with_instances"], ["", "@", "unittest", ".", "skipIf", "(", "\n", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", "or", "sys", ".", "version_info", ".", "minor", "<=", "6", ",", "\"Insufficient pytorch version\"", "\n", ")", "\n", "def", "test_rpn_scriptability", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "proposal_generator", "=", "RPN", "(", "cfg", ",", "{", "\"res4\"", ":", "ShapeSpec", "(", "channels", "=", "1024", ",", "stride", "=", "16", ")", "}", ")", ".", "eval", "(", ")", "\n", "num_images", "=", "2", "\n", "images_tensor", "=", "torch", ".", "rand", "(", "num_images", ",", "30", ",", "40", ")", "\n", "image_sizes", "=", "[", "(", "32", ",", "32", ")", ",", "(", "30", ",", "40", ")", "]", "\n", "images", "=", "ImageList", "(", "images_tensor", ",", "image_sizes", ")", "\n", "features", "=", "{", "\"res4\"", ":", "torch", ".", "rand", "(", "num_images", ",", "1024", ",", "1", ",", "2", ")", "}", "\n", "\n", "fields", "=", "{", "\"proposal_boxes\"", ":", "Boxes", ",", "\"objectness_logits\"", ":", "torch", ".", "Tensor", "}", "\n", "proposal_generator_ts", "=", "scripting_with_instances", "(", "proposal_generator", ",", "fields", ")", "\n", "\n", "proposals", ",", "_", "=", "proposal_generator", "(", "images", ",", "features", ")", "\n", "proposals_ts", ",", "_", "=", "proposal_generator_ts", "(", "images", ",", "features", ")", "\n", "\n", "for", "proposal", ",", "proposal_ts", "in", "zip", "(", "proposals", ",", "proposals_ts", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "proposal", ".", "image_size", ",", "proposal_ts", ".", "image_size", ")", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "equal", "(", "proposal", ".", "proposal_boxes", ".", "tensor", ",", "proposal_ts", ".", "proposal_boxes", ".", "tensor", ")", "\n", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "proposal", ".", "objectness_logits", ",", "proposal_ts", ".", "objectness_logits", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.test_rrpn": [[130, 201], ["torch.manual_seed", "detectron2.config.get_cfg", "detectron2.modeling.backbone.build_backbone", "detectron2.modeling.proposal_generator.build_proposal_generator", "torch.rand", "detectron2.structures.ImageList", "torch.tensor", "detectron2.structures.Instances", "detectron2.structures.RotatedBoxes", "expected_losses.keys", "torch.tensor", "torch.tensor", "torch.set_printoptions", "test_rpn.RPNTest.assertEqual", "test_rpn.RPNTest.assertTrue", "test_rpn.RPNTest.assertTrue", "detectron2.modeling.backbone.build_backbone.output_shape", "torch.rand", "detectron2.utils.events.EventStorage", "detectron2.modeling.proposal_generator.build_proposal_generator.", "torch.tensor", "torch.tensor", "test_rpn.RPNTest.assertTrue", "len", "len", "torch.allclose", "torch.allclose", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.build.build_proposal_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "", "def", "test_rrpn", "(", "self", ")", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "121", ")", "\n", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "MODEL", ".", "PROPOSAL_GENERATOR", ".", "NAME", "=", "\"RRPN\"", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "NAME", "=", "\"RotatedAnchorGenerator\"", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "SIZES", "=", "[", "[", "32", ",", "64", "]", "]", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ASPECT_RATIOS", "=", "[", "[", "0.25", ",", "1", "]", "]", "\n", "cfg", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ANGLES", "=", "[", "[", "0", ",", "60", "]", "]", "\n", "cfg", ".", "MODEL", ".", "RPN", ".", "BBOX_REG_WEIGHTS", "=", "(", "1", ",", "1", ",", "1", ",", "1", ",", "1", ")", "\n", "cfg", ".", "MODEL", ".", "RPN", ".", "HEAD_NAME", "=", "\"StandardRPNHead\"", "\n", "backbone", "=", "build_backbone", "(", "cfg", ")", "\n", "proposal_generator", "=", "build_proposal_generator", "(", "cfg", ",", "backbone", ".", "output_shape", "(", ")", ")", "\n", "num_images", "=", "2", "\n", "images_tensor", "=", "torch", ".", "rand", "(", "num_images", ",", "20", ",", "30", ")", "\n", "image_sizes", "=", "[", "(", "10", ",", "10", ")", ",", "(", "20", ",", "30", ")", "]", "\n", "images", "=", "ImageList", "(", "images_tensor", ",", "image_sizes", ")", "\n", "image_shape", "=", "(", "15", ",", "15", ")", "\n", "num_channels", "=", "1024", "\n", "features", "=", "{", "\"res4\"", ":", "torch", ".", "rand", "(", "num_images", ",", "num_channels", ",", "1", ",", "2", ")", "}", "\n", "gt_boxes", "=", "torch", ".", "tensor", "(", "[", "[", "2", ",", "2", ",", "2", ",", "2", ",", "0", "]", ",", "[", "4", ",", "4", ",", "4", ",", "4", ",", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "gt_instances", "=", "Instances", "(", "image_shape", ")", "\n", "gt_instances", ".", "gt_boxes", "=", "RotatedBoxes", "(", "gt_boxes", ")", "\n", "with", "EventStorage", "(", ")", ":", "# capture events in a new storage to discard them", "\n", "            ", "proposals", ",", "proposal_losses", "=", "proposal_generator", "(", "\n", "images", ",", "features", ",", "[", "gt_instances", "[", "0", "]", ",", "gt_instances", "[", "1", "]", "]", "\n", ")", "\n", "\n", "", "expected_losses", "=", "{", "\n", "\"loss_rpn_cls\"", ":", "torch", ".", "tensor", "(", "0.04291602224", ")", ",", "\n", "\"loss_rpn_loc\"", ":", "torch", ".", "tensor", "(", "0.145077362", ")", ",", "\n", "}", "\n", "for", "name", "in", "expected_losses", ".", "keys", "(", ")", ":", "\n", "            ", "err_msg", "=", "\"proposal_losses[{}] = {}, expected losses = {}\"", ".", "format", "(", "\n", "name", ",", "proposal_losses", "[", "name", "]", ",", "expected_losses", "[", "name", "]", "\n", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "proposal_losses", "[", "name", "]", ",", "expected_losses", "[", "name", "]", ")", ",", "err_msg", ")", "\n", "\n", "", "expected_proposal_box", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "-", "1.77999556", ",", "0.78155339", ",", "68.04367828", ",", "14.78156471", ",", "60.59333801", "]", ",", "\n", "[", "13.82740974", ",", "-", "1.50282836", ",", "34.67269897", ",", "29.19676590", ",", "-", "3.81942749", "]", ",", "\n", "[", "8.10392570", ",", "-", "0.99071521", ",", "145.39100647", ",", "32.13126373", ",", "3.67242432", "]", ",", "\n", "[", "5.00000000", ",", "4.57370186", ",", "10.00000000", ",", "9.14740372", ",", "0.89196777", "]", ",", "\n", "]", "\n", ")", "\n", "\n", "expected_objectness_logit", "=", "torch", ".", "tensor", "(", "[", "0.10924313", ",", "0.09881870", ",", "0.07649877", ",", "0.05858029", "]", ")", "\n", "\n", "torch", ".", "set_printoptions", "(", "precision", "=", "8", ",", "sci_mode", "=", "False", ")", "\n", "\n", "self", ".", "assertEqual", "(", "len", "(", "proposals", ")", ",", "len", "(", "image_sizes", ")", ")", "\n", "\n", "proposal", "=", "proposals", "[", "0", "]", "\n", "# It seems that there's some randomness in the result across different machines:", "\n", "# This test can be run on a local machine for 100 times with exactly the same result,", "\n", "# However, a different machine might produce slightly different results,", "\n", "# thus the atol here.", "\n", "err_msg", "=", "\"computed proposal boxes = {}, expected {}\"", ".", "format", "(", "\n", "proposal", ".", "proposal_boxes", ".", "tensor", ",", "expected_proposal_box", "\n", ")", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "allclose", "(", "proposal", ".", "proposal_boxes", ".", "tensor", "[", ":", "4", "]", ",", "expected_proposal_box", ",", "atol", "=", "1e-5", ")", ",", "\n", "err_msg", ",", "\n", ")", "\n", "\n", "err_msg", "=", "\"computed objectness logits = {}, expected {}\"", ".", "format", "(", "\n", "proposal", ".", "objectness_logits", ",", "expected_objectness_logit", "\n", ")", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "allclose", "(", "proposal", ".", "objectness_logits", "[", ":", "4", "]", ",", "expected_objectness_logit", ",", "atol", "=", "1e-5", ")", ",", "\n", "err_msg", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.test_find_rpn_proposals_inf": [[203, 209], ["[].fill_", "detectron2.modeling.proposal_generator.proposal_utils.find_top_rpn_proposals", "torch.rand", "torch.rand", "float"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.find_top_rpn_proposals"], ["", "def", "test_find_rpn_proposals_inf", "(", "self", ")", ":", "\n", "        ", "N", ",", "Hi", ",", "Wi", ",", "A", "=", "3", ",", "3", ",", "3", ",", "3", "\n", "proposals", "=", "[", "torch", ".", "rand", "(", "N", ",", "Hi", "*", "Wi", "*", "A", ",", "4", ")", "]", "\n", "pred_logits", "=", "[", "torch", ".", "rand", "(", "N", ",", "Hi", "*", "Wi", "*", "A", ")", "]", "\n", "pred_logits", "[", "0", "]", "[", "1", "]", "[", "3", ":", "5", "]", ".", "fill_", "(", "float", "(", "\"inf\"", ")", ")", "\n", "find_top_rpn_proposals", "(", "proposals", ",", "pred_logits", ",", "[", "(", "10", ",", "10", ")", "]", ",", "0.5", ",", "1000", ",", "1000", ",", "0", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.test_find_rpn_proposals_tracing": [[210, 237], ["unittest.skipIf", "torch.rand", "torch.rand", "torch.jit.trace", "other_inputs.append", "detectron2.modeling.proposal_generator.proposal_utils.find_top_rpn_proposals", "isinstance", "torch.tensor", "torch.tensor", "torch.rand", "torch.rand", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.find_top_rpn_proposals"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_find_rpn_proposals_tracing", "(", "self", ")", ":", "\n", "        ", "N", ",", "Hi", ",", "Wi", ",", "A", "=", "3", ",", "50", ",", "50", ",", "9", "\n", "proposal", "=", "torch", ".", "rand", "(", "N", ",", "Hi", "*", "Wi", "*", "A", ",", "4", ")", "\n", "pred_logit", "=", "torch", ".", "rand", "(", "N", ",", "Hi", "*", "Wi", "*", "A", ")", "\n", "\n", "def", "func", "(", "proposal", ",", "logit", ",", "image_size", ")", ":", "\n", "            ", "r", "=", "find_top_rpn_proposals", "(", "\n", "[", "proposal", "]", ",", "[", "logit", "]", ",", "[", "image_size", "]", ",", "0.7", ",", "1000", ",", "1000", ",", "0", ",", "False", "\n", ")", "[", "0", "]", "\n", "size", "=", "r", ".", "image_size", "\n", "if", "not", "isinstance", "(", "size", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "size", "=", "torch", ".", "tensor", "(", "size", ")", "\n", "", "return", "(", "size", ",", "r", ".", "proposal_boxes", ".", "tensor", ",", "r", ".", "objectness_logits", ")", "\n", "\n", "", "other_inputs", "=", "[", "]", "\n", "# test that it generalizes to other shapes", "\n", "for", "Hi", ",", "Wi", ",", "shp", "in", "[", "(", "30", ",", "30", ",", "60", ")", ",", "(", "10", ",", "10", ",", "800", ")", "]", ":", "\n", "            ", "other_inputs", ".", "append", "(", "\n", "(", "\n", "torch", ".", "rand", "(", "N", ",", "Hi", "*", "Wi", "*", "A", ",", "4", ")", ",", "\n", "torch", ".", "rand", "(", "N", ",", "Hi", "*", "Wi", "*", "A", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "shp", ",", "shp", "]", ")", ",", "\n", ")", "\n", ")", "\n", "", "torch", ".", "jit", ".", "trace", "(", "\n", "func", ",", "(", "proposal", ",", "pred_logit", ",", "torch", ".", "tensor", "(", "[", "100", ",", "100", "]", ")", ")", ",", "check_inputs", "=", "other_inputs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.test_rpn.RPNTest.test_append_gt_to_proposal": [[239, 266], ["detectron2.structures.Instances", "detectron2.structures.Boxes", "test_rpn.RPNTest.assertRaises", "detectron2.structures.Instances", "test_rpn.RPNTest.assertRaises", "torch.tensor", "torch.tensor", "test_rpn.RPNTest.assertEqual", "test_rpn.RPNTest.assertRaises", "torch.tensor", "detectron2.modeling.proposal_generator.proposal_utils.add_ground_truth_to_proposals", "detectron2.structures.Boxes", "torch.tensor", "torch.tensor", "torch.empty"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.add_ground_truth_to_proposals"], ["", "def", "test_append_gt_to_proposal", "(", "self", ")", ":", "\n", "        ", "proposals", "=", "Instances", "(", "\n", "(", "10", ",", "10", ")", ",", "\n", "**", "{", "\n", "\"proposal_boxes\"", ":", "Boxes", "(", "torch", ".", "empty", "(", "(", "0", ",", "4", ")", ")", ")", ",", "\n", "\"objectness_logits\"", ":", "torch", ".", "tensor", "(", "[", "]", ")", ",", "\n", "\"custom_attribute\"", ":", "torch", ".", "tensor", "(", "[", "]", ")", ",", "\n", "}", "\n", ")", "\n", "gt_boxes", "=", "Boxes", "(", "torch", ".", "tensor", "(", "[", "[", "0", ",", "0", ",", "1", ",", "1", "]", "]", ")", ")", "\n", "\n", "self", ".", "assertRaises", "(", "AssertionError", ",", "add_ground_truth_to_proposals", ",", "[", "gt_boxes", "]", ",", "[", "proposals", "]", ")", "\n", "\n", "gt_instances", "=", "Instances", "(", "(", "10", ",", "10", ")", ")", "\n", "gt_instances", ".", "gt_boxes", "=", "gt_boxes", "\n", "\n", "self", ".", "assertRaises", "(", "\n", "AssertionError", ",", "add_ground_truth_to_proposals", ",", "[", "gt_instances", "]", ",", "[", "proposals", "]", "\n", ")", "\n", "\n", "gt_instances", ".", "custom_attribute", "=", "torch", ".", "tensor", "(", "[", "1", "]", ")", "\n", "gt_instances", ".", "custom_attribute2", "=", "torch", ".", "tensor", "(", "[", "1", "]", ")", "\n", "new_proposals", "=", "add_ground_truth_to_proposals", "(", "[", "gt_instances", "]", ",", "[", "proposals", "]", ")", "[", "0", "]", "\n", "\n", "self", ".", "assertEqual", "(", "new_proposals", ".", "custom_attribute", "[", "0", "]", ",", "1", ")", "\n", "# new proposals should only include the attributes in proposals", "\n", "self", ".", "assertRaises", "(", "AttributeError", ",", "lambda", ":", "new_proposals", ".", "custom_attribute2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.GeneralizedRCNN.__init__": [[34, 80], ["torch.nn.Module.__init__", "rcnn.GeneralizedRCNN.register_buffer", "rcnn.GeneralizedRCNN.register_buffer", "torch.tensor().view", "torch.tensor().view", "numpy.sum", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "backbone", ":", "Backbone", ",", "\n", "proposal_generator", ":", "nn", ".", "Module", ",", "\n", "roi_heads", ":", "nn", ".", "Module", ",", "\n", "pixel_mean", ":", "Tuple", "[", "float", "]", ",", "\n", "pixel_std", ":", "Tuple", "[", "float", "]", ",", "\n", "input_format", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "vis_period", ":", "int", "=", "0", ",", "\n", "use_clip_c4", ":", "False", ",", "\n", "use_clip_attpool", ":", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            backbone: a backbone module, must follow detectron2's backbone interface\n            proposal_generator: a module that generates proposals using backbone features\n            roi_heads: a ROI head that performs per-region computation\n            pixel_mean, pixel_std: list or tuple with #channels element, representing\n                the per-channel mean and std to be used to normalize the input image\n            input_format: describe the meaning of channels of input. Needed by visualization\n            vis_period: the period to run visualization. Set to 0 to disable.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "backbone", "=", "backbone", "\n", "self", ".", "proposal_generator", "=", "proposal_generator", "\n", "self", ".", "roi_heads", "=", "roi_heads", "\n", "\n", "self", ".", "input_format", "=", "input_format", "\n", "self", ".", "vis_period", "=", "vis_period", "\n", "if", "vis_period", ">", "0", ":", "\n", "            ", "assert", "input_format", "is", "not", "None", ",", "\"input_format is required for visualization!\"", "\n", "\n", "", "self", ".", "register_buffer", "(", "\"pixel_mean\"", ",", "torch", ".", "tensor", "(", "pixel_mean", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "self", ".", "register_buffer", "(", "\"pixel_std\"", ",", "torch", ".", "tensor", "(", "pixel_std", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "assert", "(", "\n", "self", ".", "pixel_mean", ".", "shape", "==", "self", ".", "pixel_std", ".", "shape", "\n", ")", ",", "f\"{self.pixel_mean} and {self.pixel_std} have different shapes!\"", "\n", "if", "np", ".", "sum", "(", "pixel_mean", ")", "<", "3.0", ":", "# converrt pixel value to range [0.0, 1.0] by dividing 255.0", "\n", "            ", "assert", "input_format", "==", "'RGB'", "\n", "self", ".", "div_pixel", "=", "True", "\n", "", "else", ":", "# default setting", "\n", "            ", "self", ".", "div_pixel", "=", "False", "\n", "", "self", ".", "use_clip_c4", "=", "use_clip_c4", "# if True, use C4 mode where roi_head uses the last resnet layer from backbone ", "\n", "self", ".", "use_clip_attpool", "=", "use_clip_attpool", "# if True (C4+text_emb_as_classifier), use att_pool to replace default mean pool", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.GeneralizedRCNN.from_config": [[81, 94], ["backbone.build_backbone.build_backbone", "proposal_generator.build_proposal_generator", "roi_heads.build_roi_heads", "backbone.build_backbone.build_backbone.output_shape", "backbone.build_backbone.build_backbone.output_shape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.build.build_proposal_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.build_roi_heads", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "backbone", "=", "build_backbone", "(", "cfg", ")", "\n", "return", "{", "\n", "\"backbone\"", ":", "backbone", ",", "\n", "\"proposal_generator\"", ":", "build_proposal_generator", "(", "cfg", ",", "backbone", ".", "output_shape", "(", ")", ")", ",", "\n", "\"roi_heads\"", ":", "build_roi_heads", "(", "cfg", ",", "backbone", ".", "output_shape", "(", ")", ")", ",", "\n", "\"input_format\"", ":", "cfg", ".", "INPUT", ".", "FORMAT", ",", "\n", "\"vis_period\"", ":", "cfg", ".", "VIS_PERIOD", ",", "\n", "\"pixel_mean\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_MEAN", ",", "\n", "\"pixel_std\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_STD", ",", "\n", "\"use_clip_c4\"", ":", "cfg", ".", "MODEL", ".", "BACKBONE", ".", "NAME", "==", "\"build_clip_resnet_backbone\"", ",", "\n", "\"use_clip_attpool\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NAME", "==", "'CLIPRes5ROIHeads'", "and", "cfg", ".", "MODEL", ".", "CLIP", ".", "USE_TEXT_EMB_CLASSIFIER", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.GeneralizedRCNN.device": [[96, 99], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pixel_mean", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.GeneralizedRCNN.visualize_training": [[100, 134], ["detectron2.utils.events.get_event_storage", "zip", "detectron2.data.detection_utils.convert_image_to_rgb", "Visualizer", "v_gt.overlay_instances.overlay_instances.overlay_instances", "v_gt.overlay_instances.overlay_instances.get_image", "min", "Visualizer", "v_pred.overlay_instances.overlay_instances.overlay_instances", "v_pred.overlay_instances.overlay_instances.get_image", "numpy.concatenate", "vis_img.transpose.transpose.transpose", "detectron2.utils.events.get_event_storage.put_image", "detectron2.data.detection_utils.convert_image_to_rgb.permute", "len", "prop.proposal_boxes[].tensor.cpu().numpy", "prop.proposal_boxes[].tensor.cpu"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.convert_image_to_rgb", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_image"], ["", "def", "visualize_training", "(", "self", ",", "batched_inputs", ",", "proposals", ")", ":", "\n", "        ", "\"\"\"\n        A function used to visualize images and proposals. It shows ground truth\n        bounding boxes on the original image and up to 20 top-scoring predicted\n        object proposals on the original image. Users can implement different\n        visualization functions for different models.\n\n        Args:\n            batched_inputs (list): a list that contains input to the model.\n            proposals (list): a list that contains predicted proposals. Both\n                batched_inputs and proposals should have the same length.\n        \"\"\"", "\n", "from", "detectron2", ".", "utils", ".", "visualizer", "import", "Visualizer", "\n", "\n", "storage", "=", "get_event_storage", "(", ")", "\n", "max_vis_prop", "=", "20", "\n", "\n", "for", "input", ",", "prop", "in", "zip", "(", "batched_inputs", ",", "proposals", ")", ":", "\n", "            ", "img", "=", "input", "[", "\"image\"", "]", "\n", "img", "=", "convert_image_to_rgb", "(", "img", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "self", ".", "input_format", ")", "\n", "v_gt", "=", "Visualizer", "(", "img", ",", "None", ")", "\n", "v_gt", "=", "v_gt", ".", "overlay_instances", "(", "boxes", "=", "input", "[", "\"instances\"", "]", ".", "gt_boxes", ")", "\n", "anno_img", "=", "v_gt", ".", "get_image", "(", ")", "\n", "box_size", "=", "min", "(", "len", "(", "prop", ".", "proposal_boxes", ")", ",", "max_vis_prop", ")", "\n", "v_pred", "=", "Visualizer", "(", "img", ",", "None", ")", "\n", "v_pred", "=", "v_pred", ".", "overlay_instances", "(", "\n", "boxes", "=", "prop", ".", "proposal_boxes", "[", "0", ":", "box_size", "]", ".", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", ")", "\n", "prop_img", "=", "v_pred", ".", "get_image", "(", ")", "\n", "vis_img", "=", "np", ".", "concatenate", "(", "(", "anno_img", ",", "prop_img", ")", ",", "axis", "=", "1", ")", "\n", "vis_img", "=", "vis_img", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "\n", "vis_name", "=", "\"Left: GT bounding boxes;  Right: Predicted proposals\"", "\n", "storage", ".", "put_image", "(", "vis_name", ",", "vis_img", ")", "\n", "break", "# only visualize one image in a batch", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.GeneralizedRCNN.forward": [[135, 192], ["rcnn.GeneralizedRCNN.preprocess_image", "rcnn.GeneralizedRCNN.backbone", "losses.update", "losses.update", "rcnn.GeneralizedRCNN.inference", "rcnn.GeneralizedRCNN.proposal_generator", "rcnn.GeneralizedRCNN.roi_heads", "detectron2.utils.events.get_event_storage", "x[].to", "x[].to", "rcnn.GeneralizedRCNN.roi_heads", "rcnn.GeneralizedRCNN.roi_heads", "rcnn.GeneralizedRCNN.visualize_training"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.visualize_training"], ["", "", "def", "forward", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\n                Each item in the list contains the inputs for one image.\n                For now, each item in the list is a dict that contains:\n\n                * image: Tensor, image in (C, H, W) format.\n                * instances (optional): groundtruth :class:`Instances`\n                * proposals (optional): :class:`Instances`, precomputed proposals.\n\n                Other information that's included in the original dicts, such as:\n\n                * \"height\", \"width\" (int): the output resolution of the model, used in inference.\n                  See :meth:`postprocess` for details.\n\n        Returns:\n            list[dict]:\n                Each dict is the output for one input image.\n                The dict contains one key \"instances\" whose value is a :class:`Instances`.\n                The :class:`Instances` object has the following keys:\n                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\n        \"\"\"", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "return", "self", ".", "inference", "(", "batched_inputs", ")", "\n", "\n", "", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "if", "\"instances\"", "in", "batched_inputs", "[", "0", "]", ":", "\n", "            ", "gt_instances", "=", "[", "x", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "", "else", ":", "\n", "            ", "gt_instances", "=", "None", "\n", "# eg: {'p2': torch.Size([b, c, 200, 304]), 'p3': torch.Size([b, c, 100, 152]), 'p4': torch.Size([b, c, 50, 76]), 'p5': torch.Size([b, c, 25, 38]), 'p6': torch.Size([b, c, 13, 19])}", "\n", "", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "\n", "if", "self", ".", "proposal_generator", "is", "not", "None", ":", "\n", "            ", "proposals", ",", "proposal_losses", "=", "self", ".", "proposal_generator", "(", "images", ",", "features", ",", "gt_instances", ")", "\n", "", "else", ":", "\n", "            ", "assert", "\"proposals\"", "in", "batched_inputs", "[", "0", "]", "\n", "proposals", "=", "[", "x", "[", "\"proposals\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "proposal_losses", "=", "{", "}", "\n", "\n", "", "if", "self", ".", "use_clip_c4", ":", "# use C4 + resnet weights from CLIP", "\n", "            ", "if", "self", ".", "use_clip_attpool", ":", "# use att_pool from CLIP to match dimension", "\n", "                ", "_", ",", "detector_losses", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ",", "attnpool", "=", "self", ".", "backbone", ".", "attnpool", ")", "\n", "", "else", ":", "# use default mean pool", "\n", "                ", "_", ",", "detector_losses", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ")", "\n", "", "", "else", ":", "# default setting", "\n", "            ", "_", ",", "detector_losses", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ")", "\n", "", "if", "self", ".", "vis_period", ">", "0", ":", "\n", "            ", "storage", "=", "get_event_storage", "(", ")", "\n", "if", "storage", ".", "iter", "%", "self", ".", "vis_period", "==", "0", ":", "\n", "                ", "self", ".", "visualize_training", "(", "batched_inputs", ",", "proposals", ")", "\n", "\n", "", "", "losses", "=", "{", "}", "\n", "losses", ".", "update", "(", "detector_losses", ")", "\n", "losses", ".", "update", "(", "proposal_losses", ")", "\n", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.GeneralizedRCNN.inference": [[193, 252], ["rcnn.GeneralizedRCNN.preprocess_image", "rcnn.GeneralizedRCNN.backbone", "rcnn.GeneralizedRCNN._postprocess", "rcnn.GeneralizedRCNN.proposal_generator", "rcnn.GeneralizedRCNN.roi_heads", "x.to", "rcnn.GeneralizedRCNN.roi_heads.forward_with_given_boxes", "torch.jit.is_scripting", "x[].to", "rcnn.GeneralizedRCNN.roi_heads", "rcnn.GeneralizedRCNN.roi_heads", "rcnn.GeneralizedRCNN.roi_heads.forward_with_given_boxes", "rcnn.GeneralizedRCNN.roi_heads.forward_with_given_boxes"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN._postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward_with_given_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward_with_given_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward_with_given_boxes"], ["", "def", "inference", "(", "\n", "self", ",", "\n", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ",", "\n", "detected_instances", ":", "Optional", "[", "List", "[", "Instances", "]", "]", "=", "None", ",", "\n", "do_postprocess", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Run inference on the given inputs.\n\n        Args:\n            batched_inputs (list[dict]): same as in :meth:`forward`\n            detected_instances (None or list[Instances]): if not None, it\n                contains an `Instances` object per image. The `Instances`\n                object contains \"pred_boxes\" and \"pred_classes\" which are\n                known boxes in the image.\n                The inference will then skip the detection of bounding boxes,\n                and only predict other per-ROI outputs.\n            do_postprocess (bool): whether to apply post-processing on the outputs.\n\n        Returns:\n            When do_postprocess=True, same as in :meth:`forward`.\n            Otherwise, a list[Instances] containing raw network outputs.\n        \"\"\"", "\n", "assert", "not", "self", ".", "training", "\n", "\n", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "\n", "if", "detected_instances", "is", "None", ":", "\n", "            ", "if", "self", ".", "proposal_generator", "is", "not", "None", ":", "\n", "                ", "proposals", ",", "_", "=", "self", ".", "proposal_generator", "(", "images", ",", "features", ",", "None", ")", "\n", "", "else", ":", "\n", "                ", "assert", "\"proposals\"", "in", "batched_inputs", "[", "0", "]", "\n", "proposals", "=", "[", "x", "[", "\"proposals\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "\n", "", "if", "self", ".", "use_clip_c4", ":", "# use C4 + resnet weights from CLIP", "\n", "                ", "if", "self", ".", "use_clip_attpool", ":", "# use att_pool from CLIP to match dimension", "\n", "                    ", "results", ",", "_", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "None", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ",", "attnpool", "=", "self", ".", "backbone", ".", "attnpool", ")", "\n", "", "else", ":", "# use default mean pool", "\n", "                    ", "results", ",", "_", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "None", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ")", "\n", "", "", "else", ":", "# default setting", "\n", "                ", "results", ",", "_", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "None", ")", "\n", "", "", "else", ":", "\n", "            ", "detected_instances", "=", "[", "x", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "detected_instances", "]", "\n", "\n", "if", "self", ".", "use_clip_c4", ":", "# use C4 + resnet weights from CLIP", "\n", "                ", "if", "self", ".", "use_clip_attpool", ":", "# use att_pool from CLIP to match dimension", "\n", "                    ", "results", "=", "self", ".", "roi_heads", ".", "forward_with_given_boxes", "(", "features", ",", "detected_instances", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ",", "attnpool", "=", "self", ".", "backbone", ".", "attnpool", ")", "\n", "", "else", ":", "# use default mean pool", "\n", "                    ", "results", "=", "self", ".", "roi_heads", ".", "forward_with_given_boxes", "(", "features", ",", "detected_instances", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ")", "\n", "", "", "else", ":", "# default setting", "\n", "                ", "results", "=", "self", ".", "roi_heads", ".", "forward_with_given_boxes", "(", "features", ",", "detected_instances", ")", "\n", "\n", "#visualize_proposals(batched_inputs, proposals, self.input_format)", "\n", "", "", "if", "do_postprocess", ":", "\n", "            ", "assert", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ",", "\"Scripting is not supported for postprocess.\"", "\n", "return", "GeneralizedRCNN", ".", "_postprocess", "(", "results", ",", "batched_inputs", ",", "images", ".", "image_sizes", ")", "\n", "", "else", ":", "\n", "            ", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.GeneralizedRCNN.preprocess_image": [[253, 264], ["detectron2.structures.ImageList.from_tensors", "x[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "", "def", "preprocess_image", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Normalize, pad and batch the input images.\n        \"\"\"", "\n", "images", "=", "[", "x", "[", "\"image\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "if", "self", ".", "div_pixel", ":", "\n", "            ", "images", "=", "[", "(", "(", "x", "/", "255.0", ")", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "", "else", ":", "\n", "            ", "images", "=", "[", "(", "x", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "self", ".", "backbone", ".", "size_divisibility", ")", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.GeneralizedRCNN._postprocess": [[265, 280], ["zip", "input_per_image.get", "input_per_image.get", "postprocessing.detector_postprocess", "processed_results.append"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess"], ["", "@", "staticmethod", "\n", "def", "_postprocess", "(", "instances", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ",", "image_sizes", ")", ":", "\n", "        ", "\"\"\"\n        Rescale the output instances to the target size.\n        \"\"\"", "\n", "# note: private function; subject to changes", "\n", "processed_results", "=", "[", "]", "\n", "for", "results_per_image", ",", "input_per_image", ",", "image_size", "in", "zip", "(", "\n", "instances", ",", "batched_inputs", ",", "image_sizes", "\n", ")", ":", "\n", "            ", "height", "=", "input_per_image", ".", "get", "(", "\"height\"", ",", "image_size", "[", "0", "]", ")", "\n", "width", "=", "input_per_image", ".", "get", "(", "\"width\"", ",", "image_size", "[", "1", "]", ")", "\n", "r", "=", "detector_postprocess", "(", "results_per_image", ",", "height", ",", "width", ")", "\n", "processed_results", ".", "append", "(", "{", "\"instances\"", ":", "r", "}", ")", "\n", "", "return", "processed_results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.ProposalNetwork.__init__": [[288, 315], ["torch.nn.Module.__init__", "rcnn.ProposalNetwork.register_buffer", "rcnn.ProposalNetwork.register_buffer", "torch.tensor().view", "torch.tensor().view", "numpy.sum", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "backbone", ":", "Backbone", ",", "\n", "proposal_generator", ":", "nn", ".", "Module", ",", "\n", "pixel_mean", ":", "Tuple", "[", "float", "]", ",", "\n", "pixel_std", ":", "Tuple", "[", "float", "]", ",", "\n", "input_format", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            backbone: a backbone module, must follow detectron2's backbone interface\n            proposal_generator: a module that generates proposals using backbone features\n            pixel_mean, pixel_std: list or tuple with #channels element, representing\n                the per-channel mean and std to be used to normalize the input image\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "backbone", "=", "backbone", "\n", "self", ".", "proposal_generator", "=", "proposal_generator", "\n", "self", ".", "register_buffer", "(", "\"pixel_mean\"", ",", "torch", ".", "tensor", "(", "pixel_mean", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "self", ".", "register_buffer", "(", "\"pixel_std\"", ",", "torch", ".", "tensor", "(", "pixel_std", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "if", "np", ".", "sum", "(", "pixel_mean", ")", "<", "3.0", ":", "# converrt pixel value to range [0.0, 1.0] by dividing 255.0", "\n", "            ", "assert", "input_format", "==", "'RGB'", "\n", "self", ".", "div_pixel", "=", "True", "\n", "", "else", ":", "# default setting", "\n", "            ", "self", ".", "div_pixel", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.ProposalNetwork.from_config": [[316, 325], ["backbone.build_backbone.build_backbone", "proposal_generator.build_proposal_generator", "backbone.build_backbone.build_backbone.output_shape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.build.build_proposal_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "backbone", "=", "build_backbone", "(", "cfg", ")", "\n", "return", "{", "\n", "\"backbone\"", ":", "backbone", ",", "\n", "\"proposal_generator\"", ":", "build_proposal_generator", "(", "cfg", ",", "backbone", ".", "output_shape", "(", ")", ")", ",", "\n", "\"input_format\"", ":", "cfg", ".", "INPUT", ".", "FORMAT", ",", "\n", "\"pixel_mean\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_MEAN", ",", "\n", "\"pixel_std\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_STD", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.ProposalNetwork.device": [[327, 330], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pixel_mean", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.rcnn.ProposalNetwork.forward": [[331, 374], ["detectron2.structures.ImageList.from_tensors", "rcnn.ProposalNetwork.backbone", "rcnn.ProposalNetwork.proposal_generator", "zip", "x[].to", "input_per_image.get", "input_per_image.get", "postprocessing.detector_postprocess", "processed_results.append", "x[].to", "detectron2.utils.logger.log_first_n", "x[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.log_first_n", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "forward", "(", "self", ",", "batched_inputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            Same as in :class:`GeneralizedRCNN.forward`\n\n        Returns:\n            list[dict]:\n                Each dict is the output for one input image.\n                The dict contains one key \"proposals\" whose value is a\n                :class:`Instances` with keys \"proposal_boxes\" and \"objectness_logits\".\n        \"\"\"", "\n", "images", "=", "[", "x", "[", "\"image\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "if", "self", ".", "div_pixel", ":", "\n", "            ", "images", "=", "[", "(", "(", "x", "/", "255.0", ")", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "", "else", ":", "\n", "            ", "images", "=", "[", "(", "x", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "self", ".", "backbone", ".", "size_divisibility", ")", "\n", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "\n", "if", "\"instances\"", "in", "batched_inputs", "[", "0", "]", ":", "\n", "            ", "gt_instances", "=", "[", "x", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "", "elif", "\"targets\"", "in", "batched_inputs", "[", "0", "]", ":", "\n", "            ", "log_first_n", "(", "\n", "logging", ".", "WARN", ",", "\"'targets' in the model inputs is now renamed to 'instances'!\"", ",", "n", "=", "10", "\n", ")", "\n", "gt_instances", "=", "[", "x", "[", "\"targets\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "", "else", ":", "\n", "            ", "gt_instances", "=", "None", "\n", "", "proposals", ",", "proposal_losses", "=", "self", ".", "proposal_generator", "(", "images", ",", "features", ",", "gt_instances", ")", "\n", "# In training, the proposals are not useful at all but we generate them anyway.", "\n", "# This makes RPN-only models about 5% slower.", "\n", "if", "self", ".", "training", ":", "\n", "            ", "return", "proposal_losses", "\n", "\n", "", "processed_results", "=", "[", "]", "\n", "for", "results_per_image", ",", "input_per_image", ",", "image_size", "in", "zip", "(", "\n", "proposals", ",", "batched_inputs", ",", "images", ".", "image_sizes", "\n", ")", ":", "\n", "            ", "height", "=", "input_per_image", ".", "get", "(", "\"height\"", ",", "image_size", "[", "0", "]", ")", "\n", "width", "=", "input_per_image", ".", "get", "(", "\"width\"", ",", "image_size", "[", "1", "]", ")", "\n", "r", "=", "detector_postprocess", "(", "results_per_image", ",", "height", ",", "width", ")", "\n", "processed_results", ".", "append", "(", "{", "\"proposals\"", ":", "r", "}", ")", "\n", "", "return", "processed_results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.__init__": [[48, 156], ["torch.nn.Module.__init__", "retinanet.RetinaNet.register_buffer", "retinanet.RetinaNet.register_buffer", "len", "len", "logger.warning", "torch.tensor().view", "torch.tensor().view", "retinanet.RetinaNet.backbone.output_shape", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "backbone", ":", "Backbone", ",", "\n", "head", ":", "nn", ".", "Module", ",", "\n", "head_in_features", ",", "\n", "anchor_generator", ",", "\n", "box2box_transform", ",", "\n", "anchor_matcher", ",", "\n", "num_classes", ",", "\n", "focal_loss_alpha", "=", "0.25", ",", "\n", "focal_loss_gamma", "=", "2.0", ",", "\n", "smooth_l1_beta", "=", "0.0", ",", "\n", "box_reg_loss_type", "=", "\"smooth_l1\"", ",", "\n", "test_score_thresh", "=", "0.05", ",", "\n", "test_topk_candidates", "=", "1000", ",", "\n", "test_nms_thresh", "=", "0.5", ",", "\n", "max_detections_per_image", "=", "100", ",", "\n", "pixel_mean", ",", "\n", "pixel_std", ",", "\n", "vis_period", "=", "0", ",", "\n", "input_format", "=", "\"BGR\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            backbone: a backbone module, must follow detectron2's backbone interface\n            head (nn.Module): a module that predicts logits and regression deltas\n                for each level from a list of per-level features\n            head_in_features (Tuple[str]): Names of the input feature maps to be used in head\n            anchor_generator (nn.Module): a module that creates anchors from a\n                list of features. Usually an instance of :class:`AnchorGenerator`\n            box2box_transform (Box2BoxTransform): defines the transform from anchors boxes to\n                instance boxes\n            anchor_matcher (Matcher): label the anchors by matching them with ground truth.\n            num_classes (int): number of classes. Used to label background proposals.\n\n            # Loss parameters:\n            focal_loss_alpha (float): focal_loss_alpha\n            focal_loss_gamma (float): focal_loss_gamma\n            smooth_l1_beta (float): smooth_l1_beta\n            box_reg_loss_type (str): Options are \"smooth_l1\", \"giou\"\n\n            # Inference parameters:\n            test_score_thresh (float): Inference cls score threshold, only anchors with\n                score > INFERENCE_TH are considered for inference (to improve speed)\n            test_topk_candidates (int): Select topk candidates before NMS\n            test_nms_thresh (float): Overlap threshold used for non-maximum suppression\n                (suppress boxes with IoU >= this threshold)\n            max_detections_per_image (int):\n                Maximum number of detections to return per image during inference\n                (100 is based on the limit established for the COCO dataset).\n\n            # Input parameters\n            pixel_mean (Tuple[float]):\n                Values to be used for image normalization (BGR order).\n                To train on images of different number of channels, set different mean & std.\n                Default values are the mean pixel value from ImageNet: [103.53, 116.28, 123.675]\n            pixel_std (Tuple[float]):\n                When using pre-trained models in Detectron1 or any MSRA models,\n                std has been absorbed into its conv1 weights, so the std needs to be set 1.\n                Otherwise, you can use [57.375, 57.120, 58.395] (ImageNet std)\n            vis_period (int):\n                The period (in terms of steps) for minibatch visualization at train time.\n                Set to 0 to disable.\n            input_format (str): Whether the model needs RGB, YUV, HSV etc.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "backbone", "=", "backbone", "\n", "self", ".", "head", "=", "head", "\n", "self", ".", "head_in_features", "=", "head_in_features", "\n", "if", "len", "(", "self", ".", "backbone", ".", "output_shape", "(", ")", ")", "!=", "len", "(", "self", ".", "head_in_features", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "\"[RetinaNet] Backbone produces unused features.\"", ")", "\n", "\n", "# Anchors", "\n", "", "self", ".", "anchor_generator", "=", "anchor_generator", "\n", "self", ".", "box2box_transform", "=", "box2box_transform", "\n", "self", ".", "anchor_matcher", "=", "anchor_matcher", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "# Loss parameters:", "\n", "self", ".", "focal_loss_alpha", "=", "focal_loss_alpha", "\n", "self", ".", "focal_loss_gamma", "=", "focal_loss_gamma", "\n", "self", ".", "smooth_l1_beta", "=", "smooth_l1_beta", "\n", "self", ".", "box_reg_loss_type", "=", "box_reg_loss_type", "\n", "# Inference parameters:", "\n", "self", ".", "test_score_thresh", "=", "test_score_thresh", "\n", "self", ".", "test_topk_candidates", "=", "test_topk_candidates", "\n", "self", ".", "test_nms_thresh", "=", "test_nms_thresh", "\n", "self", ".", "max_detections_per_image", "=", "max_detections_per_image", "\n", "# Vis parameters", "\n", "self", ".", "vis_period", "=", "vis_period", "\n", "self", ".", "input_format", "=", "input_format", "\n", "\n", "self", ".", "register_buffer", "(", "\"pixel_mean\"", ",", "torch", ".", "tensor", "(", "pixel_mean", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "self", ".", "register_buffer", "(", "\"pixel_std\"", ",", "torch", ".", "tensor", "(", "pixel_std", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "\n", "\"\"\"\n        In Detectron1, loss is normalized by number of foreground samples in the batch.\n        When batch size is 1 per GPU, #foreground has a large variance and\n        using it lead to lower performance. Here we maintain an EMA of #foreground to\n        stabilize the normalizer.\n        \"\"\"", "\n", "self", ".", "loss_normalizer", "=", "100", "# initialize with any reasonable #fg that's not too small", "\n", "self", ".", "loss_normalizer_momentum", "=", "0.9", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.from_config": [[157, 191], ["backbone.build_backbone.build_backbone", "backbone.build_backbone.build_backbone.output_shape", "retinanet.RetinaNetHead", "anchor_generator.build_anchor_generator.build_anchor_generator", "box_regression.Box2BoxTransform", "matcher.Matcher"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.build_anchor_generator"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "backbone", "=", "build_backbone", "(", "cfg", ")", "\n", "backbone_shape", "=", "backbone", ".", "output_shape", "(", ")", "\n", "feature_shapes", "=", "[", "backbone_shape", "[", "f", "]", "for", "f", "in", "cfg", ".", "MODEL", ".", "RETINANET", ".", "IN_FEATURES", "]", "\n", "head", "=", "RetinaNetHead", "(", "cfg", ",", "feature_shapes", ")", "\n", "anchor_generator", "=", "build_anchor_generator", "(", "cfg", ",", "feature_shapes", ")", "\n", "return", "{", "\n", "\"backbone\"", ":", "backbone", ",", "\n", "\"head\"", ":", "head", ",", "\n", "\"anchor_generator\"", ":", "anchor_generator", ",", "\n", "\"box2box_transform\"", ":", "Box2BoxTransform", "(", "weights", "=", "cfg", ".", "MODEL", ".", "RETINANET", ".", "BBOX_REG_WEIGHTS", ")", ",", "\n", "\"anchor_matcher\"", ":", "Matcher", "(", "\n", "cfg", ".", "MODEL", ".", "RETINANET", ".", "IOU_THRESHOLDS", ",", "\n", "cfg", ".", "MODEL", ".", "RETINANET", ".", "IOU_LABELS", ",", "\n", "allow_low_quality_matches", "=", "True", ",", "\n", ")", ",", "\n", "\"pixel_mean\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_MEAN", ",", "\n", "\"pixel_std\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_STD", ",", "\n", "\"num_classes\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "NUM_CLASSES", ",", "\n", "\"head_in_features\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "IN_FEATURES", ",", "\n", "# Loss parameters:", "\n", "\"focal_loss_alpha\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "FOCAL_LOSS_ALPHA", ",", "\n", "\"focal_loss_gamma\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "FOCAL_LOSS_GAMMA", ",", "\n", "\"smooth_l1_beta\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "SMOOTH_L1_LOSS_BETA", ",", "\n", "\"box_reg_loss_type\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "BBOX_REG_LOSS_TYPE", ",", "\n", "# Inference parameters:", "\n", "\"test_score_thresh\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "SCORE_THRESH_TEST", ",", "\n", "\"test_topk_candidates\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "TOPK_CANDIDATES_TEST", ",", "\n", "\"test_nms_thresh\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "NMS_THRESH_TEST", ",", "\n", "\"max_detections_per_image\"", ":", "cfg", ".", "TEST", ".", "DETECTIONS_PER_IMAGE", ",", "\n", "# Vis parameters", "\n", "\"vis_period\"", ":", "cfg", ".", "VIS_PERIOD", ",", "\n", "\"input_format\"", ":", "cfg", ".", "INPUT", ".", "FORMAT", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.device": [[193, 196], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pixel_mean", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.visualize_training": [[197, 231], ["detectron2.utils.events.get_event_storage", "detectron2.data.detection_utils.convert_image_to_rgb", "Visualizer", "v_gt.overlay_instances.overlay_instances.overlay_instances", "v_gt.overlay_instances.overlay_instances.get_image", "postprocessing.detector_postprocess", "postprocessing.detector_postprocess.pred_boxes.tensor.detach().cpu().numpy", "Visualizer", "v_pred.overlay_instances.overlay_instances.overlay_instances", "v_pred.overlay_instances.overlay_instances.get_image", "numpy.vstack", "vis_img.transpose.transpose.transpose", "detectron2.utils.events.get_event_storage.put_image", "len", "len", "detectron2.data.detection_utils.convert_image_to_rgb.permute", "postprocessing.detector_postprocess.pred_boxes.tensor.detach().cpu", "postprocessing.detector_postprocess.pred_boxes.tensor.detach"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.convert_image_to_rgb", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_image"], ["", "def", "visualize_training", "(", "self", ",", "batched_inputs", ",", "results", ")", ":", "\n", "        ", "\"\"\"\n        A function used to visualize ground truth images and final network predictions.\n        It shows ground truth bounding boxes on the original image and up to 20\n        predicted object bounding boxes on the original image.\n\n        Args:\n            batched_inputs (list): a list that contains input to the model.\n            results (List[Instances]): a list of #images elements.\n        \"\"\"", "\n", "from", "detectron2", ".", "utils", ".", "visualizer", "import", "Visualizer", "\n", "\n", "assert", "len", "(", "batched_inputs", ")", "==", "len", "(", "\n", "results", "\n", ")", ",", "\"Cannot visualize inputs and results of different sizes\"", "\n", "storage", "=", "get_event_storage", "(", ")", "\n", "max_boxes", "=", "20", "\n", "\n", "image_index", "=", "0", "# only visualize a single image", "\n", "img", "=", "batched_inputs", "[", "image_index", "]", "[", "\"image\"", "]", "\n", "img", "=", "convert_image_to_rgb", "(", "img", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "self", ".", "input_format", ")", "\n", "v_gt", "=", "Visualizer", "(", "img", ",", "None", ")", "\n", "v_gt", "=", "v_gt", ".", "overlay_instances", "(", "boxes", "=", "batched_inputs", "[", "image_index", "]", "[", "\"instances\"", "]", ".", "gt_boxes", ")", "\n", "anno_img", "=", "v_gt", ".", "get_image", "(", ")", "\n", "processed_results", "=", "detector_postprocess", "(", "results", "[", "image_index", "]", ",", "img", ".", "shape", "[", "0", "]", ",", "img", ".", "shape", "[", "1", "]", ")", "\n", "predicted_boxes", "=", "processed_results", ".", "pred_boxes", ".", "tensor", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "v_pred", "=", "Visualizer", "(", "img", ",", "None", ")", "\n", "v_pred", "=", "v_pred", ".", "overlay_instances", "(", "boxes", "=", "predicted_boxes", "[", "0", ":", "max_boxes", "]", ")", "\n", "prop_img", "=", "v_pred", ".", "get_image", "(", ")", "\n", "vis_img", "=", "np", ".", "vstack", "(", "(", "anno_img", ",", "prop_img", ")", ")", "\n", "vis_img", "=", "vis_img", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "\n", "vis_name", "=", "f\"Top: GT bounding boxes; Bottom: {max_boxes} Highest Scoring Results\"", "\n", "storage", ".", "put_image", "(", "vis_name", ",", "vis_img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.forward": [[232, 291], ["retinanet.RetinaNet.preprocess_image", "retinanet.RetinaNet.backbone", "retinanet.RetinaNet.anchor_generator", "retinanet.RetinaNet.head", "retinanet.permute_to_N_HWA_K", "retinanet.permute_to_N_HWA_K", "retinanet.RetinaNet.label_anchors", "retinanet.RetinaNet.losses", "retinanet.RetinaNet.inference", "torch.jit.is_scripting", "zip", "torch.jit.is_scripting", "x[].to", "detectron2.utils.events.get_event_storage", "input_per_image.get", "input_per_image.get", "postprocessing.detector_postprocess", "processed_results.append", "retinanet.RetinaNet.inference", "retinanet.RetinaNet.visualize_training"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.permute_to_N_HWA_K", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.permute_to_N_HWA_K", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.label_anchors", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.visualize_training"], ["", "def", "forward", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\n                Each item in the list contains the inputs for one image.\n                For now, each item in the list is a dict that contains:\n\n                * image: Tensor, image in (C, H, W) format.\n                * instances: Instances\n\n                Other information that's included in the original dicts, such as:\n\n                * \"height\", \"width\" (int): the output resolution of the model, used in inference.\n                  See :meth:`postprocess` for details.\n        Returns:\n            In training, dict[str, Tensor]: mapping from a named loss to a tensor storing the\n            loss. Used during training only. In inference, the standard output format, described\n            in :doc:`/tutorials/models`.\n        \"\"\"", "\n", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "head_in_features", "]", "\n", "\n", "anchors", "=", "self", ".", "anchor_generator", "(", "features", ")", "\n", "pred_logits", ",", "pred_anchor_deltas", "=", "self", ".", "head", "(", "features", ")", "\n", "# Transpose the Hi*Wi*A dimension to the middle:", "\n", "pred_logits", "=", "[", "permute_to_N_HWA_K", "(", "x", ",", "self", ".", "num_classes", ")", "for", "x", "in", "pred_logits", "]", "\n", "pred_anchor_deltas", "=", "[", "permute_to_N_HWA_K", "(", "x", ",", "4", ")", "for", "x", "in", "pred_anchor_deltas", "]", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "assert", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ",", "\"Not supported\"", "\n", "assert", "\"instances\"", "in", "batched_inputs", "[", "0", "]", ",", "\"Instance annotations are missing in training!\"", "\n", "gt_instances", "=", "[", "x", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "\n", "gt_labels", ",", "gt_boxes", "=", "self", ".", "label_anchors", "(", "anchors", ",", "gt_instances", ")", "\n", "losses", "=", "self", ".", "losses", "(", "anchors", ",", "pred_logits", ",", "gt_labels", ",", "pred_anchor_deltas", ",", "gt_boxes", ")", "\n", "\n", "if", "self", ".", "vis_period", ">", "0", ":", "\n", "                ", "storage", "=", "get_event_storage", "(", ")", "\n", "if", "storage", ".", "iter", "%", "self", ".", "vis_period", "==", "0", ":", "\n", "                    ", "results", "=", "self", ".", "inference", "(", "\n", "anchors", ",", "pred_logits", ",", "pred_anchor_deltas", ",", "images", ".", "image_sizes", "\n", ")", "\n", "self", ".", "visualize_training", "(", "batched_inputs", ",", "results", ")", "\n", "\n", "", "", "return", "losses", "\n", "", "else", ":", "\n", "            ", "results", "=", "self", ".", "inference", "(", "anchors", ",", "pred_logits", ",", "pred_anchor_deltas", ",", "images", ".", "image_sizes", ")", "\n", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "                ", "return", "results", "\n", "", "processed_results", "=", "[", "]", "\n", "for", "results_per_image", ",", "input_per_image", ",", "image_size", "in", "zip", "(", "\n", "results", ",", "batched_inputs", ",", "images", ".", "image_sizes", "\n", ")", ":", "\n", "                ", "height", "=", "input_per_image", ".", "get", "(", "\"height\"", ",", "image_size", "[", "0", "]", ")", "\n", "width", "=", "input_per_image", ".", "get", "(", "\"width\"", ",", "image_size", "[", "1", "]", ")", "\n", "r", "=", "detector_postprocess", "(", "results_per_image", ",", "height", ",", "width", ")", "\n", "processed_results", ".", "append", "(", "{", "\"instances\"", ":", "r", "}", ")", "\n", "", "return", "processed_results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.losses": [[292, 345], ["len", "torch.stack", "pos_mask.sum().item", "detectron2.utils.events.get_event_storage().put_scalar", "fvcore.nn.sigmoid_focal_loss_jit", "box_regression._dense_box_regression_loss", "torch.nn.functional.one_hot", "gt_labels_target.to", "pos_mask.sum", "detectron2.utils.events.get_event_storage", "max", "detectron2.layers.cat"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression._dense_box_regression_loss", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "", "def", "losses", "(", "self", ",", "anchors", ",", "pred_logits", ",", "gt_labels", ",", "pred_anchor_deltas", ",", "gt_boxes", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            anchors (list[Boxes]): a list of #feature level Boxes\n            gt_labels, gt_boxes: see output of :meth:`RetinaNet.label_anchors`.\n                Their shapes are (N, R) and (N, R, 4), respectively, where R is\n                the total number of anchors across levels, i.e. sum(Hi x Wi x Ai)\n            pred_logits, pred_anchor_deltas: both are list[Tensor]. Each element in the\n                list corresponds to one level and has shape (N, Hi * Wi * Ai, K or 4).\n                Where K is the number of classes used in `pred_logits`.\n\n        Returns:\n            dict[str, Tensor]:\n                mapping from a named loss to a scalar tensor\n                storing the loss. Used during training only. The dict keys are:\n                \"loss_cls\" and \"loss_box_reg\"\n        \"\"\"", "\n", "num_images", "=", "len", "(", "gt_labels", ")", "\n", "gt_labels", "=", "torch", ".", "stack", "(", "gt_labels", ")", "# (N, R)", "\n", "\n", "valid_mask", "=", "gt_labels", ">=", "0", "\n", "pos_mask", "=", "(", "gt_labels", ">=", "0", ")", "&", "(", "gt_labels", "!=", "self", ".", "num_classes", ")", "\n", "num_pos_anchors", "=", "pos_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "get_event_storage", "(", ")", ".", "put_scalar", "(", "\"num_pos_anchors\"", ",", "num_pos_anchors", "/", "num_images", ")", "\n", "self", ".", "loss_normalizer", "=", "self", ".", "loss_normalizer_momentum", "*", "self", ".", "loss_normalizer", "+", "(", "\n", "1", "-", "self", ".", "loss_normalizer_momentum", "\n", ")", "*", "max", "(", "num_pos_anchors", ",", "1", ")", "\n", "\n", "# classification and regression loss", "\n", "gt_labels_target", "=", "F", ".", "one_hot", "(", "gt_labels", "[", "valid_mask", "]", ",", "num_classes", "=", "self", ".", "num_classes", "+", "1", ")", "[", "\n", ":", ",", ":", "-", "1", "\n", "]", "# no loss for the last (background) class", "\n", "loss_cls", "=", "sigmoid_focal_loss_jit", "(", "\n", "cat", "(", "pred_logits", ",", "dim", "=", "1", ")", "[", "valid_mask", "]", ",", "\n", "gt_labels_target", ".", "to", "(", "pred_logits", "[", "0", "]", ".", "dtype", ")", ",", "\n", "alpha", "=", "self", ".", "focal_loss_alpha", ",", "\n", "gamma", "=", "self", ".", "focal_loss_gamma", ",", "\n", "reduction", "=", "\"sum\"", ",", "\n", ")", "\n", "\n", "loss_box_reg", "=", "_dense_box_regression_loss", "(", "\n", "anchors", ",", "\n", "self", ".", "box2box_transform", ",", "\n", "pred_anchor_deltas", ",", "\n", "gt_boxes", ",", "\n", "pos_mask", ",", "\n", "box_reg_loss_type", "=", "self", ".", "box_reg_loss_type", ",", "\n", "smooth_l1_beta", "=", "self", ".", "smooth_l1_beta", ",", "\n", ")", "\n", "\n", "return", "{", "\n", "\"loss_cls\"", ":", "loss_cls", "/", "self", ".", "loss_normalizer", ",", "\n", "\"loss_box_reg\"", ":", "loss_box_reg", "/", "self", ".", "loss_normalizer", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.label_anchors": [[347, 391], ["torch.no_grad", "detectron2.structures.Boxes.cat", "detectron2.structures.pairwise_iou", "retinanet.RetinaNet.anchor_matcher", "gt_labels.append", "matched_gt_boxes.append", "len", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "label_anchors", "(", "self", ",", "anchors", ",", "gt_instances", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            anchors (list[Boxes]): A list of #feature level Boxes.\n                The Boxes contains anchors of this image on the specific feature level.\n            gt_instances (list[Instances]): a list of N `Instances`s. The i-th\n                `Instances` contains the ground-truth per-instance annotations\n                for the i-th input image.\n\n        Returns:\n            list[Tensor]: List of #img tensors. i-th element is a vector of labels whose length is\n            the total number of anchors across all feature maps (sum(Hi * Wi * A)).\n            Label values are in {-1, 0, ..., K}, with -1 means ignore, and K means background.\n\n            list[Tensor]: i-th element is a Rx4 tensor, where R is the total number of anchors\n            across feature maps. The values are the matched gt boxes for each anchor.\n            Values are undefined for those anchors not labeled as foreground.\n        \"\"\"", "\n", "anchors", "=", "Boxes", ".", "cat", "(", "anchors", ")", "# Rx4", "\n", "\n", "gt_labels", "=", "[", "]", "\n", "matched_gt_boxes", "=", "[", "]", "\n", "for", "gt_per_image", "in", "gt_instances", ":", "\n", "            ", "match_quality_matrix", "=", "pairwise_iou", "(", "gt_per_image", ".", "gt_boxes", ",", "anchors", ")", "\n", "matched_idxs", ",", "anchor_labels", "=", "self", ".", "anchor_matcher", "(", "match_quality_matrix", ")", "\n", "del", "match_quality_matrix", "\n", "\n", "if", "len", "(", "gt_per_image", ")", ">", "0", ":", "\n", "                ", "matched_gt_boxes_i", "=", "gt_per_image", ".", "gt_boxes", ".", "tensor", "[", "matched_idxs", "]", "\n", "\n", "gt_labels_i", "=", "gt_per_image", ".", "gt_classes", "[", "matched_idxs", "]", "\n", "# Anchors with label 0 are treated as background.", "\n", "gt_labels_i", "[", "anchor_labels", "==", "0", "]", "=", "self", ".", "num_classes", "\n", "# Anchors with label -1 are ignored.", "\n", "gt_labels_i", "[", "anchor_labels", "==", "-", "1", "]", "=", "-", "1", "\n", "", "else", ":", "\n", "                ", "matched_gt_boxes_i", "=", "torch", ".", "zeros_like", "(", "anchors", ".", "tensor", ")", "\n", "gt_labels_i", "=", "torch", ".", "zeros_like", "(", "matched_idxs", ")", "+", "self", ".", "num_classes", "\n", "\n", "", "gt_labels", ".", "append", "(", "gt_labels_i", ")", "\n", "matched_gt_boxes", ".", "append", "(", "matched_gt_boxes_i", ")", "\n", "\n", "", "return", "gt_labels", ",", "matched_gt_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.inference": [[392, 419], ["enumerate", "retinanet.RetinaNet.inference_single_image", "results.append"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.inference_single_image"], ["", "def", "inference", "(", "\n", "self", ",", "\n", "anchors", ":", "List", "[", "Boxes", "]", ",", "\n", "pred_logits", ":", "List", "[", "Tensor", "]", ",", "\n", "pred_anchor_deltas", ":", "List", "[", "Tensor", "]", ",", "\n", "image_sizes", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Arguments:\n            anchors (list[Boxes]): A list of #feature level Boxes.\n                The Boxes contain anchors of this image on the specific feature level.\n            pred_logits, pred_anchor_deltas: list[Tensor], one per level. Each\n                has shape (N, Hi * Wi * Ai, K or 4)\n            image_sizes (List[(h, w)]): the input image sizes\n\n        Returns:\n            results (List[Instances]): a list of #images elements.\n        \"\"\"", "\n", "results", ":", "List", "[", "Instances", "]", "=", "[", "]", "\n", "for", "img_idx", ",", "image_size", "in", "enumerate", "(", "image_sizes", ")", ":", "\n", "            ", "pred_logits_per_image", "=", "[", "x", "[", "img_idx", "]", "for", "x", "in", "pred_logits", "]", "\n", "deltas_per_image", "=", "[", "x", "[", "img_idx", "]", "for", "x", "in", "pred_anchor_deltas", "]", "\n", "results_per_image", "=", "self", ".", "inference_single_image", "(", "\n", "anchors", ",", "pred_logits_per_image", ",", "deltas_per_image", ",", "image_size", "\n", ")", "\n", "results", ".", "append", "(", "results_per_image", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.inference_single_image": [[420, 487], ["zip", "detectron2.layers.batched_nms", "detectron2.structures.Instances", "detectron2.structures.Boxes", "box_cls_i.flatten().sigmoid_", "min", "box_cls_i.flatten().sigmoid_.sort", "retinanet.RetinaNet.box2box_transform.apply_deltas", "boxes_all.append", "scores_all.append", "class_idxs_all.append", "detectron2.layers.cat", "detectron2.layers.nonzero_tuple", "topk_idxs.size", "box_cls_i.flatten"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "def", "inference_single_image", "(", "\n", "self", ",", "\n", "anchors", ":", "List", "[", "Boxes", "]", ",", "\n", "box_cls", ":", "List", "[", "Tensor", "]", ",", "\n", "box_delta", ":", "List", "[", "Tensor", "]", ",", "\n", "image_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Single-image inference. Return bounding-box detection results by thresholding\n        on scores and applying non-maximum suppression (NMS).\n\n        Arguments:\n            anchors (list[Boxes]): list of #feature levels. Each entry contains\n                a Boxes object, which contains all the anchors in that feature level.\n            box_cls (list[Tensor]): list of #feature levels. Each entry contains\n                tensor of size (H x W x A, K)\n            box_delta (list[Tensor]): Same shape as 'box_cls' except that K becomes 4.\n            image_size (tuple(H, W)): a tuple of the image height and width.\n\n        Returns:\n            Same as `inference`, but for only one image.\n        \"\"\"", "\n", "boxes_all", "=", "[", "]", "\n", "scores_all", "=", "[", "]", "\n", "class_idxs_all", "=", "[", "]", "\n", "\n", "# Iterate over every feature level", "\n", "for", "box_cls_i", ",", "box_reg_i", ",", "anchors_i", "in", "zip", "(", "box_cls", ",", "box_delta", ",", "anchors", ")", ":", "\n", "# (HxWxAxK,)", "\n", "            ", "predicted_prob", "=", "box_cls_i", ".", "flatten", "(", ")", ".", "sigmoid_", "(", ")", "\n", "\n", "# Apply two filtering below to make NMS faster.", "\n", "# 1. Keep boxes with confidence score higher than threshold", "\n", "keep_idxs", "=", "predicted_prob", ">", "self", ".", "test_score_thresh", "\n", "predicted_prob", "=", "predicted_prob", "[", "keep_idxs", "]", "\n", "topk_idxs", "=", "nonzero_tuple", "(", "keep_idxs", ")", "[", "0", "]", "\n", "\n", "# 2. Keep top k top scoring boxes only", "\n", "num_topk", "=", "min", "(", "self", ".", "test_topk_candidates", ",", "topk_idxs", ".", "size", "(", "0", ")", ")", "\n", "# torch.sort is actually faster than .topk (at least on GPUs)", "\n", "predicted_prob", ",", "idxs", "=", "predicted_prob", ".", "sort", "(", "descending", "=", "True", ")", "\n", "predicted_prob", "=", "predicted_prob", "[", ":", "num_topk", "]", "\n", "topk_idxs", "=", "topk_idxs", "[", "idxs", "[", ":", "num_topk", "]", "]", "\n", "\n", "anchor_idxs", "=", "topk_idxs", "//", "self", ".", "num_classes", "\n", "classes_idxs", "=", "topk_idxs", "%", "self", ".", "num_classes", "\n", "\n", "box_reg_i", "=", "box_reg_i", "[", "anchor_idxs", "]", "\n", "anchors_i", "=", "anchors_i", "[", "anchor_idxs", "]", "\n", "# predict boxes", "\n", "predicted_boxes", "=", "self", ".", "box2box_transform", ".", "apply_deltas", "(", "box_reg_i", ",", "anchors_i", ".", "tensor", ")", "\n", "\n", "boxes_all", ".", "append", "(", "predicted_boxes", ")", "\n", "scores_all", ".", "append", "(", "predicted_prob", ")", "\n", "class_idxs_all", ".", "append", "(", "classes_idxs", ")", "\n", "\n", "", "boxes_all", ",", "scores_all", ",", "class_idxs_all", "=", "[", "\n", "cat", "(", "x", ")", "for", "x", "in", "[", "boxes_all", ",", "scores_all", ",", "class_idxs_all", "]", "\n", "]", "\n", "keep", "=", "batched_nms", "(", "boxes_all", ",", "scores_all", ",", "class_idxs_all", ",", "self", ".", "test_nms_thresh", ")", "\n", "keep", "=", "keep", "[", ":", "self", ".", "max_detections_per_image", "]", "\n", "\n", "result", "=", "Instances", "(", "image_size", ")", "\n", "result", ".", "pred_boxes", "=", "Boxes", "(", "boxes_all", "[", "keep", "]", ")", "\n", "result", ".", "scores", "=", "scores_all", "[", "keep", "]", "\n", "result", ".", "pred_classes", "=", "class_idxs_all", "[", "keep", "]", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.preprocess_image": [[488, 496], ["detectron2.structures.ImageList.from_tensors", "x[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "preprocess_image", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Normalize, pad and batch the input images.\n        \"\"\"", "\n", "images", "=", "[", "x", "[", "\"image\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "images", "=", "[", "(", "x", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "self", ".", "backbone", ".", "size_divisibility", ")", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNetHead.__init__": [[504, 570], ["torch.nn.Module.__init__", "zip", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.init.constant_", "logger.warning", "cls_subnet.append", "cls_subnet.append", "bbox_subnet.append", "bbox_subnet.append", "modules.modules", "math.log", "list", "torch.nn.Conv2d", "cls_subnet.append", "torch.nn.ReLU", "torch.nn.Conv2d", "bbox_subnet.append", "torch.nn.ReLU", "isinstance", "detectron2.layers.get_norm", "detectron2.layers.get_norm", "torch.nn.init.normal_", "torch.nn.init.constant_"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "input_shape", ":", "List", "[", "ShapeSpec", "]", ",", "\n", "num_classes", ",", "\n", "num_anchors", ",", "\n", "conv_dims", ":", "List", "[", "int", "]", ",", "\n", "norm", "=", "\"\"", ",", "\n", "prior_prob", "=", "0.01", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            input_shape (List[ShapeSpec]): input shape\n            num_classes (int): number of classes. Used to label background proposals.\n            num_anchors (int): number of generated anchors\n            conv_dims (List[int]): dimensions for each convolution layer\n            norm (str or callable):\n                    Normalization for conv layers except for the two output layers.\n                    See :func:`detectron2.layers.get_norm` for supported types.\n            prior_prob (float): Prior weight for computing bias\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "norm", "==", "\"BN\"", "or", "norm", "==", "\"SyncBN\"", ":", "\n", "            ", "logger", ".", "warning", "(", "\"Shared norm does not work well for BN, SyncBN, expect poor results\"", ")", "\n", "\n", "", "cls_subnet", "=", "[", "]", "\n", "bbox_subnet", "=", "[", "]", "\n", "for", "in_channels", ",", "out_channels", "in", "zip", "(", "\n", "[", "input_shape", "[", "0", "]", ".", "channels", "]", "+", "list", "(", "conv_dims", ")", ",", "conv_dims", "\n", ")", ":", "\n", "            ", "cls_subnet", ".", "append", "(", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", ")", "\n", "if", "norm", ":", "\n", "                ", "cls_subnet", ".", "append", "(", "get_norm", "(", "norm", ",", "out_channels", ")", ")", "\n", "", "cls_subnet", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "bbox_subnet", ".", "append", "(", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", ")", "\n", "if", "norm", ":", "\n", "                ", "bbox_subnet", ".", "append", "(", "get_norm", "(", "norm", ",", "out_channels", ")", ")", "\n", "", "bbox_subnet", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "\n", "", "self", ".", "cls_subnet", "=", "nn", ".", "Sequential", "(", "*", "cls_subnet", ")", "\n", "self", ".", "bbox_subnet", "=", "nn", ".", "Sequential", "(", "*", "bbox_subnet", ")", "\n", "self", ".", "cls_score", "=", "nn", ".", "Conv2d", "(", "\n", "conv_dims", "[", "-", "1", "]", ",", "num_anchors", "*", "num_classes", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", "\n", ")", "\n", "self", ".", "bbox_pred", "=", "nn", ".", "Conv2d", "(", "\n", "conv_dims", "[", "-", "1", "]", ",", "num_anchors", "*", "4", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", "\n", ")", "\n", "\n", "# Initialization", "\n", "for", "modules", "in", "[", "self", ".", "cls_subnet", ",", "self", ".", "bbox_subnet", ",", "self", ".", "cls_score", ",", "self", ".", "bbox_pred", "]", ":", "\n", "            ", "for", "layer", "in", "modules", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "layer", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "layer", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "0.01", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "layer", ".", "bias", ",", "0", ")", "\n", "\n", "# Use prior in model initialization to improve stability", "\n", "", "", "", "bias_value", "=", "-", "(", "math", ".", "log", "(", "(", "1", "-", "prior_prob", ")", "/", "prior_prob", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "self", ".", "cls_score", ".", "bias", ",", "bias_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNetHead.from_config": [[571, 586], ["anchor_generator.build_anchor_generator", "len", "set"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.build_anchor_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ":", "List", "[", "ShapeSpec", "]", ")", ":", "\n", "        ", "num_anchors", "=", "build_anchor_generator", "(", "cfg", ",", "input_shape", ")", ".", "num_cell_anchors", "\n", "assert", "(", "\n", "len", "(", "set", "(", "num_anchors", ")", ")", "==", "1", "\n", ")", ",", "\"Using different number of anchors between levels is not currently supported!\"", "\n", "num_anchors", "=", "num_anchors", "[", "0", "]", "\n", "\n", "return", "{", "\n", "\"input_shape\"", ":", "input_shape", ",", "\n", "\"num_classes\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "NUM_CLASSES", ",", "\n", "\"conv_dims\"", ":", "[", "input_shape", "[", "0", "]", ".", "channels", "]", "*", "cfg", ".", "MODEL", ".", "RETINANET", ".", "NUM_CONVS", ",", "\n", "\"prior_prob\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "PRIOR_PROB", ",", "\n", "\"norm\"", ":", "cfg", ".", "MODEL", ".", "RETINANET", ".", "NORM", ",", "\n", "\"num_anchors\"", ":", "num_anchors", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNetHead.forward": [[588, 610], ["logits.append", "bbox_reg.append", "retinanet.RetinaNetHead.cls_score", "retinanet.RetinaNetHead.bbox_pred", "retinanet.RetinaNetHead.cls_subnet", "retinanet.RetinaNetHead.bbox_subnet"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ":", "List", "[", "Tensor", "]", ")", ":", "\n", "        ", "\"\"\"\n        Arguments:\n            features (list[Tensor]): FPN feature map tensors in high to low resolution.\n                Each tensor in the list correspond to different feature levels.\n\n        Returns:\n            logits (list[Tensor]): #lvl tensors, each has shape (N, AxK, Hi, Wi).\n                The tensor predicts the classification probability\n                at each spatial position for each of the A anchors and K object\n                classes.\n            bbox_reg (list[Tensor]): #lvl tensors, each has shape (N, Ax4, Hi, Wi).\n                The tensor predicts 4-vector (dx,dy,dw,dh) box\n                regression values for every anchor. These values are the\n                relative offset between the anchor and the ground truth box.\n        \"\"\"", "\n", "logits", "=", "[", "]", "\n", "bbox_reg", "=", "[", "]", "\n", "for", "feature", "in", "features", ":", "\n", "            ", "logits", ".", "append", "(", "self", ".", "cls_score", "(", "self", ".", "cls_subnet", "(", "feature", ")", ")", ")", "\n", "bbox_reg", ".", "append", "(", "self", ".", "bbox_pred", "(", "self", ".", "bbox_subnet", "(", "feature", ")", ")", ")", "\n", "", "return", "logits", ",", "bbox_reg", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.permute_to_N_HWA_K": [[30, 40], ["tensor.reshape.view", "tensor.reshape.permute", "tensor.reshape.reshape", "tensor.reshape.dim"], "function", ["None"], ["def", "permute_to_N_HWA_K", "(", "tensor", ",", "K", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Transpose/reshape a tensor from (N, (Ai x K), H, W) to (N, (HxWxAi), K)\n    \"\"\"", "\n", "assert", "tensor", ".", "dim", "(", ")", "==", "4", ",", "tensor", ".", "shape", "\n", "N", ",", "_", ",", "H", ",", "W", "=", "tensor", ".", "shape", "\n", "tensor", "=", "tensor", ".", "view", "(", "N", ",", "-", "1", ",", "K", ",", "H", ",", "W", ")", "\n", "tensor", "=", "tensor", ".", "permute", "(", "0", ",", "3", ",", "4", ",", "1", ",", "2", ")", "\n", "tensor", "=", "tensor", ".", "reshape", "(", "N", ",", "-", "1", ",", "K", ")", "# Size=(N,HWA,K)", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.build.build_model": [[16, 26], ["model.to", "detectron2.utils.logger._log_api_usage", "META_ARCH_REGISTRY.get", "torch.device"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._log_api_usage", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["def", "build_model", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Build the whole model architecture, defined by ``cfg.MODEL.META_ARCHITECTURE``.\n    Note that it does not load any weights from ``cfg``.\n    \"\"\"", "\n", "meta_arch", "=", "cfg", ".", "MODEL", ".", "META_ARCHITECTURE", "\n", "model", "=", "META_ARCH_REGISTRY", ".", "get", "(", "meta_arch", ")", "(", "cfg", ")", "\n", "model", ".", "to", "(", "torch", ".", "device", "(", "cfg", ".", "MODEL", ".", "DEVICE", ")", ")", "\n", "_log_api_usage", "(", "\"modeling.meta_arch.\"", "+", "meta_arch", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.CLIPFastRCNN.__init__": [[39, 104], ["torch.nn.Module.__init__", "clip_rcnn.CLIPFastRCNN.register_buffer", "clip_rcnn.CLIPFastRCNN.register_buffer", "torch.tensor().view", "torch.tensor().view", "numpy.sum", "clip_rcnn.CLIPFastRCNN.register_buffer", "clip_rcnn.CLIPFastRCNN.register_buffer", "torch.tensor().view", "torch.tensor().view", "numpy.sum", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "offline_backbone", ":", "Backbone", ",", "\n", "backbone", ":", "Backbone", ",", "\n", "offline_proposal_generator", ":", "nn", ".", "Module", ",", "\n", "roi_heads", ":", "nn", ".", "Module", ",", "\n", "pixel_mean", ":", "Tuple", "[", "float", "]", ",", "\n", "pixel_std", ":", "Tuple", "[", "float", "]", ",", "\n", "input_format", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "vis_period", ":", "int", "=", "0", ",", "\n", "clip_crop_region_type", ":", "str", "=", "'GT'", ",", "\n", "use_clip_c4", ":", "False", ",", "\n", "use_clip_attpool", ":", "False", ",", "\n", "offline_input_format", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "offline_pixel_mean", ":", "Tuple", "[", "float", "]", ",", "\n", "offline_pixel_std", ":", "Tuple", "[", "float", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            backbone: a backbone module, must follow detectron2's backbone interface\n            proposal_generator: a module that generates proposals using backbone features\n            roi_heads: a ROI head that performs per-region computation\n            pixel_mean, pixel_std: list or tuple with #channels element, representing\n                the per-channel mean and std to be used to normalize the input image\n            input_format: describe the meaning of channels of input. Needed by visualization\n            vis_period: the period to run visualization. Set to 0 to disable.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "offline_backbone", "=", "offline_backbone", "\n", "self", ".", "backbone", "=", "backbone", "\n", "self", ".", "offline_proposal_generator", "=", "offline_proposal_generator", "\n", "self", ".", "roi_heads", "=", "roi_heads", "\n", "\n", "self", ".", "input_format", "=", "input_format", "\n", "self", ".", "vis_period", "=", "vis_period", "\n", "if", "vis_period", ">", "0", ":", "\n", "            ", "assert", "input_format", "is", "not", "None", ",", "\"input_format is required for visualization!\"", "\n", "\n", "# input format, pixel mean and std for offline modules", "\n", "", "self", ".", "register_buffer", "(", "\"pixel_mean\"", ",", "torch", ".", "tensor", "(", "pixel_mean", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "self", ".", "register_buffer", "(", "\"pixel_std\"", ",", "torch", ".", "tensor", "(", "pixel_std", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "assert", "(", "\n", "self", ".", "pixel_mean", ".", "shape", "==", "self", ".", "pixel_std", ".", "shape", "\n", ")", ",", "f\"{self.pixel_mean} and {self.pixel_std} have different shapes!\"", "\n", "if", "np", ".", "sum", "(", "pixel_mean", ")", "<", "3.0", ":", "# converrt pixel value to range [0.0, 1.0] by dividing 255.0", "\n", "            ", "assert", "input_format", "==", "'RGB'", "\n", "self", ".", "div_pixel", "=", "True", "\n", "", "else", ":", "\n", "            ", "self", ".", "div_pixel", "=", "False", "\n", "\n", "", "if", "offline_input_format", "and", "offline_pixel_mean", "and", "offline_pixel_std", ":", "\n", "            ", "self", ".", "offline_input_format", "=", "offline_input_format", "\n", "self", ".", "register_buffer", "(", "\"offline_pixel_mean\"", ",", "torch", ".", "tensor", "(", "offline_pixel_mean", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "self", ".", "register_buffer", "(", "\"offline_pixel_std\"", ",", "torch", ".", "tensor", "(", "offline_pixel_std", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "if", "np", ".", "sum", "(", "offline_pixel_mean", ")", "<", "3.0", ":", "# converrt pixel value to range [0.0, 1.0] by dividing 255.0", "\n", "                ", "assert", "offline_input_format", "==", "'RGB'", "\n", "self", ".", "offline_div_pixel", "=", "True", "\n", "", "else", ":", "\n", "                ", "self", ".", "offline_div_pixel", "=", "False", "\n", "\n", "", "", "self", ".", "clip_crop_region_type", "=", "clip_crop_region_type", "\n", "self", ".", "use_clip_c4", "=", "use_clip_c4", "# if True, use C4 mode where roi_head uses the last resnet layer from backbone ", "\n", "self", ".", "use_clip_attpool", "=", "use_clip_attpool", "# if True (C4+text_emb_as_classifier), use att_pool to replace default mean pool", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.CLIPFastRCNN.from_config": [[105, 154], ["backbone.build_backbone.build_backbone", "roi_heads.build_roi_heads.build_roi_heads", "get_cfg", "get_cfg.merge_from_file", "backbone.build_backbone.build_backbone", "proposal_generator.build_proposal_generator", "backbone.build_backbone.build_backbone.parameters", "proposal_generator.build_proposal_generator.parameters", "backbone.build_backbone.build_backbone.eval", "proposal_generator.build_proposal_generator.eval", "backbone.build_backbone.build_backbone.output_shape", "backbone.build_backbone.build_backbone.output_shape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.build_roi_heads", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.build.build_proposal_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "# create independent backbone & RPN", "\n", "        ", "if", "cfg", ".", "MODEL", ".", "CLIP", ".", "CROP_REGION_TYPE", "==", "\"RPN\"", ":", "\n", "# create offline cfg for the pretrained backbone & RPN", "\n", "            ", "from", "detectron2", ".", "config", "import", "get_cfg", "\n", "offline_cfg", "=", "get_cfg", "(", ")", "\n", "offline_cfg", ".", "merge_from_file", "(", "cfg", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_CONFIG", ")", "\n", "if", "cfg", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_LSJ_PRETRAINED", ":", "# large-scale jittering (LSJ) pretrained RPN", "\n", "                ", "offline_cfg", ".", "MODEL", ".", "BACKBONE", ".", "FREEZE_AT", "=", "0", "# make all fronzon layers to \"SyncBN\"", "\n", "offline_cfg", ".", "MODEL", ".", "RESNETS", ".", "NORM", "=", "\"SyncBN\"", "# 5 resnet layers", "\n", "offline_cfg", ".", "MODEL", ".", "FPN", ".", "NORM", "=", "\"SyncBN\"", "# fpn layers", "\n", "offline_cfg", ".", "MODEL", ".", "RPN", ".", "CONV_DIMS", "=", "[", "-", "1", ",", "-", "1", "]", "# rpn layers", "\n", "", "if", "cfg", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_NMS_THRESH", ":", "\n", "                ", "offline_cfg", ".", "MODEL", ".", "RPN", ".", "NMS_THRESH", "=", "cfg", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_NMS_THRESH", "# 0.9", "\n", "\n", "# create offline backbone and RPN", "\n", "", "offline_backbone", "=", "build_backbone", "(", "offline_cfg", ")", "\n", "offline_rpn", "=", "build_proposal_generator", "(", "offline_cfg", ",", "offline_backbone", ".", "output_shape", "(", ")", ")", "\n", "\n", "# convert to evaluation mode", "\n", "for", "p", "in", "offline_backbone", ".", "parameters", "(", ")", ":", "p", ".", "requires_grad", "=", "False", "\n", "for", "p", "in", "offline_rpn", ".", "parameters", "(", ")", ":", "p", ".", "requires_grad", "=", "False", "\n", "offline_backbone", ".", "eval", "(", ")", "\n", "offline_rpn", ".", "eval", "(", ")", "\n", "# region proposals are ground-truth boxes", "\n", "", "elif", "cfg", ".", "MODEL", ".", "CLIP", ".", "CROP_REGION_TYPE", "==", "\"GT\"", ":", "\n", "            ", "offline_backbone", "=", "None", "\n", "offline_rpn", "=", "None", "\n", "offline_cfg", "=", "None", "\n", "\n", "", "backbone", "=", "build_backbone", "(", "cfg", ")", "\n", "roi_heads", "=", "build_roi_heads", "(", "cfg", ",", "backbone", ".", "output_shape", "(", ")", ")", "\n", "\n", "return", "{", "\n", "\"offline_backbone\"", ":", "offline_backbone", ",", "\n", "\"offline_proposal_generator\"", ":", "offline_rpn", ",", "\n", "\"backbone\"", ":", "backbone", ",", "\n", "\"roi_heads\"", ":", "roi_heads", ",", "\n", "\"input_format\"", ":", "cfg", ".", "INPUT", ".", "FORMAT", ",", "\n", "\"vis_period\"", ":", "cfg", ".", "VIS_PERIOD", ",", "\n", "\"pixel_mean\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_MEAN", ",", "\n", "\"pixel_std\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_STD", ",", "\n", "\"clip_crop_region_type\"", ":", "cfg", ".", "MODEL", ".", "CLIP", ".", "CROP_REGION_TYPE", ",", "\n", "\"use_clip_c4\"", ":", "cfg", ".", "MODEL", ".", "BACKBONE", ".", "NAME", "==", "\"build_clip_resnet_backbone\"", ",", "\n", "\"use_clip_attpool\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NAME", "in", "[", "'CLIPRes5ROIHeads'", ",", "'CLIPStandardROIHeads'", "]", "and", "cfg", ".", "MODEL", ".", "CLIP", ".", "USE_TEXT_EMB_CLASSIFIER", ",", "\n", "\"offline_input_format\"", ":", "offline_cfg", ".", "INPUT", ".", "FORMAT", "if", "offline_cfg", "else", "None", ",", "\n", "\"offline_pixel_mean\"", ":", "offline_cfg", ".", "MODEL", ".", "PIXEL_MEAN", "if", "offline_cfg", "else", "None", ",", "\n", "\"offline_pixel_std\"", ":", "offline_cfg", ".", "MODEL", ".", "PIXEL_STD", "if", "offline_cfg", "else", "None", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.CLIPFastRCNN.device": [[156, 159], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pixel_mean", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.CLIPFastRCNN.forward": [[160, 232], ["clip_rcnn.CLIPFastRCNN.preprocess_image", "clip_rcnn.CLIPFastRCNN.backbone", "losses.update", "clip_rcnn.CLIPFastRCNN.inference", "torch.no_grad", "detectron2.utils.events.get_event_storage", "x[].to", "enumerate", "clip_rcnn.CLIPFastRCNN.roi_heads", "clip_rcnn.CLIPFastRCNN.roi_heads", "clip_rcnn.CLIPFastRCNN.roi_heads", "clip_rcnn.CLIPFastRCNN.roi_heads", "clip_rcnn.CLIPFastRCNN.visualize_training", "copy.deepcopy", "copy.deepcopy._fields[].to", "proposals.append", "clip_rcnn.CLIPFastRCNN.offline_preprocess_image", "clip_rcnn.CLIPFastRCNN.offline_backbone", "torch.ones().to", "clip_rcnn.CLIPFastRCNN.offline_backbone.eval", "clip_rcnn.CLIPFastRCNN.offline_proposal_generator.eval", "clip_rcnn.CLIPFastRCNN.offline_proposal_generator", "torch.ones", "copy.deepcopy._fields[].to.tensor.size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.RetinaNet.visualize_training", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.offline_preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "forward", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\n                Each item in the list contains the inputs for one image.\n                For now, each item in the list is a dict that contains:\n\n                * image: Tensor, image in (C, H, W) format.\n                * instances (optional): groundtruth :class:`Instances`\n                * proposals (optional): :class:`Instances`, precomputed proposals.\n\n                Other information that's included in the original dicts, such as:\n\n                * \"height\", \"width\" (int): the output resolution of the model, used in inference.\n                  See :meth:`postprocess` for details.\n\n        Returns:\n            list[dict]:\n                Each dict is the output for one input image.\n                The dict contains one key \"instances\" whose value is a :class:`Instances`.\n                The :class:`Instances` object has the following keys:\n                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\n        \"\"\"", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "return", "self", ".", "inference", "(", "batched_inputs", ")", "\n", "", "if", "\"instances\"", "in", "batched_inputs", "[", "0", "]", ":", "\n", "            ", "gt_instances", "=", "[", "x", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "", "else", ":", "\n", "            ", "gt_instances", "=", "None", "\n", "\n", "# localization branch: offline modules to get the region proposals", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "self", ".", "clip_crop_region_type", "==", "\"GT\"", ":", "# from ground-truth", "\n", "                ", "proposals", "=", "[", "]", "\n", "for", "r_i", ",", "b_input", "in", "enumerate", "(", "batched_inputs", ")", ":", "\n", "                    ", "this_gt", "=", "copy", ".", "deepcopy", "(", "b_input", "[", "\"instances\"", "]", ")", "# Instance", "\n", "gt_boxes", "=", "this_gt", ".", "_fields", "[", "'gt_boxes'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "this_gt", ".", "_fields", "=", "{", "'proposal_boxes'", ":", "gt_boxes", ",", "'objectness_logits'", ":", "torch", ".", "ones", "(", "gt_boxes", ".", "tensor", ".", "size", "(", "0", ")", ")", ".", "to", "(", "self", ".", "device", ")", "}", "\n", "proposals", ".", "append", "(", "this_gt", ")", "\n", "", "", "elif", "self", ".", "clip_crop_region_type", "==", "\"RPN\"", ":", "# from the backbone & RPN of standard Mask-RCNN, trained on base classes", "\n", "                ", "if", "self", ".", "offline_backbone", ".", "training", "or", "self", ".", "offline_proposal_generator", ".", "training", ":", "#  was set to True in training script", "\n", "                    ", "self", ".", "offline_backbone", ".", "eval", "(", ")", "\n", "self", ".", "offline_proposal_generator", ".", "eval", "(", ")", "\n", "", "images", "=", "self", ".", "offline_preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "offline_backbone", "(", "images", ".", "tensor", ")", "\n", "if", "self", ".", "offline_proposal_generator", "is", "not", "None", ":", "\n", "                    ", "proposals", ",", "_", "=", "self", ".", "offline_proposal_generator", "(", "images", ",", "features", ",", "None", ")", "\n", "\n", "# recognition branch: get 2D feature maps using the backbone of recognition branch", "\n", "", "", "", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "\n", "# Given the proposals, crop region features from 2D image features and classify the regions", "\n", "if", "self", ".", "use_clip_c4", ":", "# use C4 + resnet weights from CLIP", "\n", "            ", "if", "self", ".", "use_clip_attpool", ":", "# use att_pool from CLIP to match dimension", "\n", "                ", "_", ",", "detector_losses", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ",", "attnpool", "=", "self", ".", "backbone", ".", "attnpool", ")", "\n", "", "else", ":", "# use mean pool", "\n", "                ", "_", ",", "detector_losses", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ")", "\n", "", "", "else", ":", "# regular detector setting", "\n", "            ", "if", "self", ".", "use_clip_attpool", ":", "# use att_pool from CLIP to match dimension", "\n", "                ", "_", ",", "detector_losses", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ",", "attnpool", "=", "self", ".", "backbone", ".", "bottom_up", ".", "attnpool", ")", "\n", "", "else", ":", "# use mean pool", "\n", "                ", "_", ",", "detector_losses", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ")", "\n", "", "", "if", "self", ".", "vis_period", ">", "0", ":", "\n", "            ", "storage", "=", "get_event_storage", "(", ")", "\n", "if", "storage", ".", "iter", "%", "self", ".", "vis_period", "==", "0", ":", "\n", "                ", "self", ".", "visualize_training", "(", "batched_inputs", ",", "proposals", ")", "\n", "#visualize_proposals(batched_inputs, proposals, self.input_format)", "\n", "\n", "", "", "losses", "=", "{", "}", "\n", "losses", ".", "update", "(", "detector_losses", ")", "\n", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.CLIPFastRCNN.inference": [[233, 295], ["clip_rcnn.CLIPFastRCNN.preprocess_image", "clip_rcnn.CLIPFastRCNN.backbone", "enumerate", "clip_rcnn.CLIPFastRCNN._postprocess", "copy.deepcopy", "copy.deepcopy._fields[].to", "proposals.append", "clip_rcnn.CLIPFastRCNN.offline_preprocess_image", "clip_rcnn.CLIPFastRCNN.offline_backbone", "clip_rcnn.CLIPFastRCNN.roi_heads", "clip_rcnn.CLIPFastRCNN.roi_heads", "clip_rcnn.CLIPFastRCNN.roi_heads", "clip_rcnn.CLIPFastRCNN.roi_heads", "torch.jit.is_scripting", "clip_rcnn.CLIPFastRCNN.offline_proposal_generator"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN._postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.offline_preprocess_image"], ["", "def", "inference", "(", "\n", "self", ",", "\n", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ",", "\n", "detected_instances", ":", "Optional", "[", "List", "[", "Instances", "]", "]", "=", "None", ",", "\n", "do_postprocess", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Run inference on the given inputs.\n\n        Args:\n            batched_inputs (list[dict]): same as in :meth:`forward`\n            detected_instances (None or list[Instances]): if not None, it\n                contains an `Instances` object per image. The `Instances`\n                object contains \"pred_boxes\" and \"pred_classes\" which are\n                known boxes in the image.\n                The inference will then skip the detection of bounding boxes,\n                and only predict other per-ROI outputs.\n            do_postprocess (bool): whether to apply post-processing on the outputs.\n\n        Returns:\n            When do_postprocess=True, same as in :meth:`forward`.\n            Otherwise, a list[Instances] containing raw network outputs.\n        \"\"\"", "\n", "assert", "not", "self", ".", "training", "\n", "\n", "# localization branch: offline modules to get the region proposals", "\n", "if", "self", ".", "clip_crop_region_type", "==", "\"GT\"", ":", "# from ground-truth", "\n", "            ", "proposals", "=", "[", "]", "\n", "for", "r_i", ",", "b_input", "in", "enumerate", "(", "batched_inputs", ")", ":", "\n", "                ", "this_gt", "=", "copy", ".", "deepcopy", "(", "b_input", "[", "\"instances\"", "]", ")", "# Instance", "\n", "gt_boxes", "=", "this_gt", ".", "_fields", "[", "'gt_boxes'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "this_gt", ".", "_fields", "=", "{", "'proposal_boxes'", ":", "gt_boxes", "}", "#, 'objectness_logits': None}", "\n", "proposals", ".", "append", "(", "this_gt", ")", "\n", "", "", "elif", "self", ".", "clip_crop_region_type", "==", "\"RPN\"", ":", "# from the backbone & RPN of standard Mask-RCNN, trained on base classes", "\n", "            ", "images", "=", "self", ".", "offline_preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "offline_backbone", "(", "images", ".", "tensor", ")", "\n", "if", "detected_instances", "is", "None", ":", "\n", "                ", "if", "self", ".", "offline_proposal_generator", "is", "not", "None", ":", "\n", "                    ", "proposals", ",", "_", "=", "self", ".", "offline_proposal_generator", "(", "images", ",", "features", ",", "None", ")", "\n", "\n", "# recognition branch: get 2D feature maps using the backbone of recognition branch", "\n", "", "", "", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "\n", "# Given the proposals, crop region features from 2D image features and classify the regions", "\n", "if", "self", ".", "use_clip_c4", ":", "# use C4 + resnet weights from CLIP", "\n", "            ", "if", "self", ".", "use_clip_attpool", ":", "# use att_pool from CLIP to match dimension", "\n", "                ", "results", ",", "_", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "None", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ",", "attnpool", "=", "self", ".", "backbone", ".", "attnpool", ")", "\n", "", "else", ":", "# use mean pool", "\n", "                ", "results", ",", "_", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "None", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ")", "\n", "", "", "else", ":", "# regular detector setting", "\n", "            ", "if", "self", ".", "use_clip_attpool", ":", "# use att_pool from CLIP to match dimension", "\n", "                ", "results", ",", "_", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "None", ",", "attnpool", "=", "self", ".", "backbone", ".", "bottom_up", ".", "attnpool", ")", "\n", "", "else", ":", "\n", "                ", "results", ",", "_", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "None", ")", "\n", "\n", "#visualize_proposals(batched_inputs, proposals, self.input_format)", "\n", "", "", "if", "do_postprocess", ":", "\n", "            ", "assert", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ",", "\"Scripting is not supported for postprocess.\"", "\n", "return", "CLIPFastRCNN", ".", "_postprocess", "(", "results", ",", "batched_inputs", ")", "\n", "", "else", ":", "\n", "            ", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.CLIPFastRCNN.offline_preprocess_image": [[296, 311], ["detectron2.structures.ImageList.from_tensors", "x[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "", "def", "offline_preprocess_image", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Normalize, pad and batch the input images. Use detectron2 default processing (pixel mean & std).\n        Note: Due to FPN size_divisibility, images are padded by right/bottom border. So FPN is consistent with C4 and GT boxes.\n        \"\"\"", "\n", "images", "=", "[", "x", "[", "\"image\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "if", "(", "self", ".", "input_format", "==", "'RGB'", "and", "self", ".", "offline_input_format", "==", "'BGR'", ")", "or", "(", "self", ".", "input_format", "==", "'BGR'", "and", "self", ".", "offline_input_format", "==", "'RGB'", ")", ":", "\n", "            ", "images", "=", "[", "x", "[", "[", "2", ",", "1", ",", "0", "]", ",", ":", ",", ":", "]", "for", "x", "in", "images", "]", "\n", "", "if", "self", ".", "offline_div_pixel", ":", "\n", "            ", "images", "=", "[", "(", "(", "x", "/", "255.0", ")", "-", "self", ".", "offline_pixel_mean", ")", "/", "self", ".", "offline_pixel_std", "for", "x", "in", "images", "]", "\n", "", "else", ":", "\n", "            ", "images", "=", "[", "(", "x", "-", "self", ".", "offline_pixel_mean", ")", "/", "self", ".", "offline_pixel_std", "for", "x", "in", "images", "]", "\n", "", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "self", ".", "offline_backbone", ".", "size_divisibility", ")", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.CLIPFastRCNN.preprocess_image": [[312, 324], ["detectron2.structures.ImageList.from_tensors", "x[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "preprocess_image", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Normalize, pad and batch the input images. Use CLIP default processing (pixel mean & std).\n        Note: Due to FPN size_divisibility, images are padded by right/bottom border. So FPN is consistent with C4 and GT boxes.\n        \"\"\"", "\n", "images", "=", "[", "x", "[", "\"image\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "if", "self", ".", "div_pixel", ":", "\n", "            ", "images", "=", "[", "(", "(", "x", "/", "255.0", ")", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "", "else", ":", "\n", "            ", "images", "=", "[", "(", "x", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "self", ".", "backbone", ".", "size_divisibility", ")", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.CLIPFastRCNN._postprocess": [[325, 339], ["zip", "postprocessing.detector_postprocess", "processed_results.append"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess"], ["", "@", "staticmethod", "\n", "def", "_postprocess", "(", "instances", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Rescale the output instances to the target size.\n        \"\"\"", "\n", "# note: private function; subject to changes", "\n", "processed_results", "=", "[", "]", "\n", "for", "results_per_image", ",", "input_per_image", "in", "zip", "(", "\n", "instances", ",", "batched_inputs", ")", ":", "\n", "            ", "height", "=", "input_per_image", "[", "\"height\"", "]", "# original image size, before resizing", "\n", "width", "=", "input_per_image", "[", "\"width\"", "]", "# original image size, before resizing", "\n", "r", "=", "detector_postprocess", "(", "results_per_image", ",", "height", ",", "width", ")", "\n", "processed_results", ".", "append", "(", "{", "\"instances\"", ":", "r", "}", ")", "\n", "", "return", "processed_results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.__init__": [[347, 447], ["torch.nn.Module.__init__", "clip_rcnn.PretrainFastRCNN.register_buffer", "clip_rcnn.PretrainFastRCNN.register_buffer", "torch.tensor().view", "torch.tensor().view", "numpy.sum", "clip_rcnn.PretrainFastRCNN.register_buffer", "clip_rcnn.PretrainFastRCNN.register_buffer", "clip_rcnn.PretrainFastRCNN.lang_encoder.parameters", "clip_rcnn.PretrainFastRCNN.register_buffer", "clip_rcnn.PretrainFastRCNN.teacher_backbone.parameters", "torch.tensor().view", "torch.tensor().view", "numpy.sum", "torch.load", "clip_rcnn.PretrainFastRCNN.register_buffer", "clip_rcnn.PretrainFastRCNN.register_buffer", "torch.tensor", "torch.tensor", "torch.load", "torch.load", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "offline_backbone", ":", "Backbone", ",", "\n", "backbone", ":", "Backbone", ",", "\n", "offline_proposal_generator", ":", "nn", ".", "Module", ",", "\n", "roi_heads", ":", "nn", ".", "Module", ",", "\n", "teacher_backbone", ":", "nn", ".", "Module", ",", "\n", "teacher_roi_heads", ":", "nn", ".", "Module", ",", "\n", "pixel_mean", ":", "Tuple", "[", "float", "]", ",", "\n", "pixel_std", ":", "Tuple", "[", "float", "]", ",", "\n", "input_format", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "vis_period", ":", "int", "=", "0", ",", "\n", "clip_crop_region_type", ":", "str", "=", "'GT'", ",", "\n", "use_clip_c4", ":", "False", ",", "\n", "use_clip_attpool", ":", "False", ",", "\n", "offline_input_format", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "offline_pixel_mean", ":", "Tuple", "[", "float", "]", ",", "\n", "offline_pixel_std", ":", "Tuple", "[", "float", "]", ",", "\n", "language_encoder", ":", "nn", ".", "Module", ",", "\n", "matching_temp", ":", "None", ",", "\n", "num_regions_per_img", ":", "int", "=", "0", ",", "\n", "img_txt_level", ":", "None", ",", "\n", "gather_gpus", ":", "False", ",", "\n", "concept_emb", ":", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            backbone: a backbone module, must follow detectron2's backbone interface\n            proposal_generator: a module that generates proposals using backbone features\n            roi_heads: a ROI head that performs per-region computation\n            pixel_mean, pixel_std: list or tuple with #channels element, representing\n                the per-channel mean and std to be used to normalize the input image\n            input_format: describe the meaning of channels of input. Needed by visualization\n            vis_period: the period to run visualization. Set to 0 to disable.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "offline_backbone", "=", "offline_backbone", "\n", "self", ".", "backbone", "=", "backbone", "\n", "self", ".", "offline_proposal_generator", "=", "offline_proposal_generator", "\n", "self", ".", "roi_heads", "=", "roi_heads", "\n", "\n", "self", ".", "input_format", "=", "input_format", "\n", "self", ".", "vis_period", "=", "vis_period", "\n", "if", "vis_period", ">", "0", ":", "\n", "            ", "assert", "input_format", "is", "not", "None", ",", "\"input_format is required for visualization!\"", "\n", "\n", "# input format, pixel mean and std for offline modules", "\n", "", "self", ".", "register_buffer", "(", "\"pixel_mean\"", ",", "torch", ".", "tensor", "(", "pixel_mean", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "self", ".", "register_buffer", "(", "\"pixel_std\"", ",", "torch", ".", "tensor", "(", "pixel_std", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "assert", "(", "\n", "self", ".", "pixel_mean", ".", "shape", "==", "self", ".", "pixel_std", ".", "shape", "\n", ")", ",", "f\"{self.pixel_mean} and {self.pixel_std} have different shapes!\"", "\n", "if", "np", ".", "sum", "(", "pixel_mean", ")", "<", "3.0", ":", "# converrt pixel value to range [0.0, 1.0] by dividing 255.0", "\n", "            ", "assert", "input_format", "==", "'RGB'", "\n", "self", ".", "div_pixel", "=", "True", "\n", "", "else", ":", "\n", "            ", "self", ".", "div_pixel", "=", "False", "\n", "\n", "", "if", "offline_input_format", "and", "offline_pixel_mean", "and", "offline_pixel_std", ":", "\n", "            ", "self", ".", "offline_input_format", "=", "offline_input_format", "\n", "self", ".", "register_buffer", "(", "\"offline_pixel_mean\"", ",", "torch", ".", "tensor", "(", "offline_pixel_mean", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "self", ".", "register_buffer", "(", "\"offline_pixel_std\"", ",", "torch", ".", "tensor", "(", "offline_pixel_std", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "if", "np", ".", "sum", "(", "offline_pixel_mean", ")", "<", "3.0", ":", "# converrt pixel value to range [0.0, 1.0] by dividing 255.0", "\n", "                ", "assert", "offline_input_format", "==", "'RGB'", "\n", "self", ".", "offline_div_pixel", "=", "True", "\n", "", "else", ":", "\n", "                ", "self", ".", "offline_div_pixel", "=", "False", "\n", "\n", "", "", "self", ".", "clip_crop_region_type", "=", "clip_crop_region_type", "\n", "self", ".", "use_clip_c4", "=", "use_clip_c4", "# if True, use C4 mode where roi_head uses the last resnet layer from backbone ", "\n", "self", ".", "use_clip_attpool", "=", "use_clip_attpool", "# if True (C4+text_emb_as_classifier), use att_pool to replace default mean pool", "\n", "\n", "# image-text level pretraining", "\n", "self", ".", "img_txt_level", "=", "img_txt_level", "[", "0", "]", "\n", "self", ".", "only_eot", "=", "img_txt_level", "[", "1", "]", "\n", "if", "self", ".", "img_txt_level", ":", "\n", "            ", "self", ".", "lang_encoder", "=", "language_encoder", "\n", "for", "p", "in", "self", ".", "lang_encoder", ".", "parameters", "(", ")", ":", "# freeze language encoder", "\n", "                ", "p", ".", "requires_grad", "=", "False", "\n", "", "", "self", ".", "matching_temp", "=", "matching_temp", "\n", "self", ".", "context_length", "=", "77", "# defined in clip_img_txt_pair_tsv class", "\n", "self", ".", "num_regions_per_img", "=", "num_regions_per_img", "\n", "self", ".", "gather_gpus", "=", "gather_gpus", "\n", "\n", "# region-token level pretraining", "\n", "if", "concept_emb", "[", "0", "]", ":", "\n", "            ", "self", ".", "register_buffer", "(", "\"concept_emb\"", ",", "torch", ".", "load", "(", "concept_emb", "[", "0", "]", ")", ",", "False", ")", "# [#concepts, d]", "\n", "self", ".", "concept_thres", "=", "concept_emb", "[", "1", "]", "\n", "self", ".", "teacher_backbone", "=", "teacher_backbone", "\n", "for", "p", "in", "self", ".", "teacher_backbone", ".", "parameters", "(", ")", ":", "# freeze visual encoder of teacher model", "\n", "                ", "p", ".", "requires_grad", "=", "False", "\n", "", "if", "concept_emb", "[", "2", "]", "is", "None", ":", "# teacher model uses the same concept embedding as student model", "\n", "                ", "self", ".", "register_buffer", "(", "\"teacher_concept_emb\"", ",", "torch", ".", "load", "(", "concept_emb", "[", "0", "]", ")", ",", "False", ")", "\n", "", "else", ":", "# teacher model uses a seperate concept embedding", "\n", "                ", "self", ".", "register_buffer", "(", "\"teacher_concept_emb\"", ",", "torch", ".", "load", "(", "concept_emb", "[", "2", "]", ")", ",", "False", ")", "\n", "", "self", ".", "teacher_roi_heads", "=", "teacher_roi_heads", "\n", "", "else", ":", "\n", "            ", "self", ".", "concept_emb", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.from_config": [[448, 514], ["backbone.build_backbone.build_backbone", "roi_heads.build_roi_heads.build_roi_heads", "backbone.build_backbone.clip_backbone.build_clip_language_encoder", "copy.deepcopy", "copy.deepcopy.defrost", "backbone.build_backbone.build_backbone", "roi_heads.build_roi_heads.build_roi_heads", "get_cfg", "get_cfg.merge_from_file", "backbone.build_backbone.build_backbone", "proposal_generator.build_proposal_generator", "backbone.build_backbone.build_backbone.parameters", "proposal_generator.build_proposal_generator.parameters", "backbone.build_backbone.build_backbone.eval", "proposal_generator.build_proposal_generator.eval", "backbone.build_backbone.build_backbone.output_shape", "backbone.build_backbone.build_backbone.output_shape", "backbone.build_backbone.build_backbone.output_shape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.build_roi_heads", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.build_clip_language_encoder", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.build_roi_heads", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.build.build_proposal_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "if", "cfg", ".", "MODEL", ".", "CLIP", ".", "CROP_REGION_TYPE", "==", "\"RPN\"", ":", "# create isolated backbone & RPN", "\n", "# create offline cfg for the pretrained backbone & RPN", "\n", "            ", "from", "detectron2", ".", "config", "import", "get_cfg", "\n", "offline_cfg", "=", "get_cfg", "(", ")", "\n", "offline_cfg", ".", "merge_from_file", "(", "cfg", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_CONFIG", ")", "\n", "if", "cfg", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_LSJ_PRETRAINED", ":", "# large-scale jittering (LSJ) pretrained RPN", "\n", "                ", "offline_cfg", ".", "MODEL", ".", "BACKBONE", ".", "FREEZE_AT", "=", "0", "# make all fronzon layers to \"SyncBN\"", "\n", "offline_cfg", ".", "MODEL", ".", "RESNETS", ".", "NORM", "=", "\"SyncBN\"", "# 5 resnet layers", "\n", "offline_cfg", ".", "MODEL", ".", "FPN", ".", "NORM", "=", "\"SyncBN\"", "# fpn layers", "\n", "offline_cfg", ".", "MODEL", ".", "RPN", ".", "CONV_DIMS", "=", "[", "-", "1", ",", "-", "1", "]", "# rpn layers", "\n", "", "if", "cfg", ".", "MODEL", ".", "CLIP", ".", "PRETRAIN_RPN_REGIONS", ":", "\n", "                ", "offline_cfg", ".", "MODEL", ".", "RPN", ".", "POST_NMS_TOPK_TEST", "=", "cfg", ".", "MODEL", ".", "CLIP", ".", "PRETRAIN_RPN_REGIONS", "\n", "", "if", "cfg", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_NMS_THRESH", ":", "\n", "                ", "offline_cfg", ".", "MODEL", ".", "RPN", ".", "NMS_THRESH", "=", "cfg", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_NMS_THRESH", "\n", "\n", "# create offline backbone and RPN", "\n", "", "offline_backbone", "=", "build_backbone", "(", "offline_cfg", ")", "# build_resnet_fpn_backbone(cfg, ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))", "\n", "offline_rpn", "=", "build_proposal_generator", "(", "offline_cfg", ",", "offline_backbone", ".", "output_shape", "(", ")", ")", "\n", "# convert to evaluation mode", "\n", "for", "p", "in", "offline_backbone", ".", "parameters", "(", ")", ":", "p", ".", "requires_grad", "=", "False", "\n", "for", "p", "in", "offline_rpn", ".", "parameters", "(", ")", ":", "p", ".", "requires_grad", "=", "False", "\n", "offline_backbone", ".", "eval", "(", ")", "\n", "offline_rpn", ".", "eval", "(", ")", "\n", "", "elif", "cfg", ".", "MODEL", ".", "CLIP", ".", "CROP_REGION_TYPE", "in", "[", "\"GRID\"", ",", "\"RANDOM\"", "]", ":", "\n", "            ", "offline_backbone", "=", "None", "\n", "offline_rpn", "=", "None", "\n", "offline_cfg", "=", "None", "\n", "\n", "# visual encoder and roi_heads of student model", "\n", "", "backbone", "=", "build_backbone", "(", "cfg", ")", "\n", "roi_heads", "=", "build_roi_heads", "(", "cfg", ",", "backbone", ".", "output_shape", "(", ")", ")", "\n", "# language encoder of student model", "\n", "language_encoder", "=", "build_clip_language_encoder", "(", "cfg", ")", "\n", "# visual encoder of teacher model", "\n", "teacher_cfg", "=", "copy", ".", "deepcopy", "(", "cfg", ")", "\n", "teacher_cfg", ".", "defrost", "(", ")", "\n", "teacher_cfg", ".", "MODEL", ".", "RESNETS", ".", "DEPTH", "=", "teacher_cfg", ".", "MODEL", ".", "CLIP", ".", "TEACHER_RESNETS_DEPTH", "\n", "teacher_backbone", "=", "build_backbone", "(", "teacher_cfg", ")", "\n", "teacher_cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_RESOLUTION", "=", "teacher_cfg", ".", "MODEL", ".", "CLIP", ".", "TEACHER_POOLER_RESOLUTION", "\n", "teacher_roi_heads", "=", "build_roi_heads", "(", "teacher_cfg", ",", "teacher_backbone", ".", "output_shape", "(", ")", ")", "\n", "\n", "return", "{", "\n", "\"offline_backbone\"", ":", "offline_backbone", ",", "\n", "\"offline_proposal_generator\"", ":", "offline_rpn", ",", "\n", "\"backbone\"", ":", "backbone", ",", "\n", "\"roi_heads\"", ":", "roi_heads", ",", "\n", "\"teacher_backbone\"", ":", "teacher_backbone", ",", "\n", "\"teacher_roi_heads\"", ":", "teacher_roi_heads", ",", "\n", "\"input_format\"", ":", "cfg", ".", "INPUT", ".", "FORMAT", ",", "\n", "\"vis_period\"", ":", "cfg", ".", "VIS_PERIOD", ",", "\n", "\"pixel_mean\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_MEAN", ",", "\n", "\"pixel_std\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_STD", ",", "\n", "\"clip_crop_region_type\"", ":", "cfg", ".", "MODEL", ".", "CLIP", ".", "CROP_REGION_TYPE", ",", "\n", "\"use_clip_c4\"", ":", "cfg", ".", "MODEL", ".", "BACKBONE", ".", "NAME", "==", "\"build_clip_resnet_backbone\"", ",", "\n", "\"use_clip_attpool\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NAME", "==", "'PretrainRes5ROIHeads'", ",", "\n", "\"offline_input_format\"", ":", "offline_cfg", ".", "INPUT", ".", "FORMAT", "if", "offline_cfg", "else", "None", ",", "\n", "\"offline_pixel_mean\"", ":", "offline_cfg", ".", "MODEL", ".", "PIXEL_MEAN", "if", "offline_cfg", "else", "None", ",", "\n", "\"offline_pixel_std\"", ":", "offline_cfg", ".", "MODEL", ".", "PIXEL_STD", "if", "offline_cfg", "else", "None", ",", "\n", "\"language_encoder\"", ":", "language_encoder", ",", "\n", "\"matching_temp\"", ":", "cfg", ".", "MODEL", ".", "CLIP", ".", "CLSS_TEMP", ",", "\n", "\"num_regions_per_img\"", ":", "cfg", ".", "MODEL", ".", "CLIP", ".", "PRETRAIN_SAMPLE_REGIONS", ",", "\n", "\"img_txt_level\"", ":", "(", "cfg", ".", "MODEL", ".", "CLIP", ".", "PRETRAIN_IMG_TXT_LEVEL", ",", "cfg", ".", "MODEL", ".", "CLIP", ".", "PRETRAIN_ONLY_EOT", ")", ",", "\n", "\"gather_gpus\"", ":", "cfg", ".", "MODEL", ".", "CLIP", ".", "GATHER_GPUS", ",", "\n", "\"concept_emb\"", ":", "(", "cfg", ".", "MODEL", ".", "CLIP", ".", "CONCEPT_POOL_EMB", ",", "cfg", ".", "MODEL", ".", "CLIP", ".", "CONCEPT_THRES", ",", "cfg", ".", "MODEL", ".", "CLIP", ".", "TEACHER_CONCEPT_POOL_EMB", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.device": [[516, 519], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pixel_mean", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.forward": [[520, 567], ["clip_rcnn.PretrainFastRCNN.get_region_proposals", "clip_rcnn.PretrainFastRCNN.create_global_proposals", "clip_rcnn.PretrainFastRCNN.preprocess_image", "clip_rcnn.PretrainFastRCNN.backbone", "clip_rcnn.PretrainFastRCNN.get_region_features", "clip_rcnn.PretrainFastRCNN.get_region_features", "clip_rcnn.PretrainFastRCNN.inference", "clip_rcnn.PretrainFastRCNN.image_text_matching", "clip_rcnn.PretrainFastRCNN.region_concept_matching"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.get_region_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.create_global_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.get_region_features", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.get_region_features", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.image_text_matching", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.region_concept_matching"], ["", "def", "forward", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\n                Each item in the list contains the inputs for one image.\n                For now, each item in the list is a dict that contains:\n\n                * image: Tensor, image in (C, H, W) format.\n                * instances (optional): groundtruth :class:`Instances`\n                * proposals (optional): :class:`Instances`, precomputed proposals.\n\n                Other information that's included in the original dicts, such as:\n\n                * \"height\", \"width\" (int): the output resolution of the model, used in inference.\n                  See :meth:`postprocess` for details.\n\n        Returns:\n            list[dict]:\n                Each dict is the output for one input image.\n                The dict contains one key \"instances\" whose value is a :class:`Instances`.\n                The :class:`Instances` object has the following keys:\n                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\n        \"\"\"", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "return", "self", ".", "inference", "(", "batched_inputs", ")", "\n", "", "gt_instances", "=", "None", "\n", "losses", "=", "{", "}", "\n", "\n", "# localization branch: offline modules to get the region proposals", "\n", "proposals", "=", "self", ".", "get_region_proposals", "(", "batched_inputs", ")", "\n", "global_proposals", "=", "self", ".", "create_global_proposals", "(", "batched_inputs", ")", "\n", "\n", "# recognition branch: get 2D feature maps using the backbone of recognition branch and extract region features", "\n", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "region_feats", "=", "self", ".", "get_region_features", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ")", "\n", "global_feats", "=", "self", ".", "get_region_features", "(", "images", ",", "features", ",", "global_proposals", ",", "gt_instances", ")", "\n", "\n", "# image-text level matching", "\n", "if", "self", ".", "img_txt_level", ":", "\n", "            ", "self", ".", "image_text_matching", "(", "batched_inputs", ",", "proposals", ",", "region_feats", ",", "losses", ",", "global_feats", "=", "global_feats", ")", "\n", "\n", "# region-concept level matching", "\n", "", "if", "self", ".", "concept_emb", "is", "not", "None", ":", "\n", "            ", "self", ".", "region_concept_matching", "(", "images", ",", "proposals", ",", "gt_instances", ",", "region_feats", ",", "losses", ")", "\n", "\n", "", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.region_concept_matching": [[568, 596], ["clip_rcnn.PretrainFastRCNN.get_psuedo_concept_labels", "keep_region_feats.norm", "torch.nn.functional.kl_div", "losses.update", "losses.update", "clip_rcnn.PretrainFastRCNN.concept_emb.norm", "concept_emb.t", "torch.nn.functional.softmax().log", "target_embs.norm", "target_embs.t", "detectron2.utils.comm.MILCrossEntropy", "torch.nn.functional.softmax"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.get_psuedo_concept_labels"], ["", "def", "region_concept_matching", "(", "self", ",", "images", ",", "proposals", ",", "gt_instances", ",", "region_feats", ",", "losses", ",", "use_distill", "=", "True", ",", "use_contrastive", "=", "True", ")", ":", "\n", "# get psuedo concept labels from teacher model", "\n", "        ", "concept_scores", ",", "target_inds", ",", "keep_regions", ",", "target_embs", ",", "label_mtx", "=", "self", ".", "get_psuedo_concept_labels", "(", "images", ",", "proposals", ",", "gt_instances", ")", "\n", "\n", "# prepare region features for the kept regions", "\n", "keep_region_feats", "=", "region_feats", "[", "keep_regions", "]", "\n", "keep_region_feats", "=", "keep_region_feats", "/", "keep_region_feats", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "if", "use_distill", ":", "\n", "# distillation learning: learns from the predictions of teacher model", "\n", "            ", "concept_emb", "=", "self", ".", "concept_emb", "/", "self", ".", "concept_emb", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "cls_scores", "=", "keep_region_feats", "@", "concept_emb", ".", "t", "(", ")", "# [#kept_regions, #concepts]", "\n", "cls_scores_temp", "=", "cls_scores", "/", "self", ".", "matching_temp", "\n", "\n", "# calculate loss", "\n", "cls_loss", "=", "F", ".", "kl_div", "(", "F", ".", "softmax", "(", "cls_scores_temp", ",", "dim", "=", "1", ")", ".", "log", "(", ")", ",", "concept_scores", ",", "reduction", "=", "'batchmean'", ")", "# input is log-probabilities, target is probabilities", "\n", "losses", ".", "update", "(", "{", "\"loss_region_distill\"", ":", "cls_loss", "}", ")", "#  * 0.8})", "\n", "\n", "", "if", "use_contrastive", ":", "\n", "# contrastive learning: matching student visual features with target concept embs", "\n", "            ", "target_embs", "=", "target_embs", "/", "target_embs", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "match_scores", "=", "keep_region_feats", "@", "target_embs", ".", "t", "(", ")", "# [#kept_regions, #kept_regions]", "\n", "match_scores_temp", "=", "match_scores", "/", "self", ".", "matching_temp", "\n", "\n", "# calculate loss given matching scores and label matrix", "\n", "contrastive_loss", "=", "MILCrossEntropy", "(", ")", "(", "match_scores_temp", ",", "label_mtx", ",", "weights", "=", "None", ",", "avg_positives", "=", "False", ")", "\n", "losses", ".", "update", "(", "{", "\"loss_concept_contrastive\"", ":", "contrastive_loss", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.image_text_matching": [[597, 626], ["int", "torch.cat", "clip_rcnn.PretrainFastRCNN.lang_encoder.encode_text", "int", "torch.arange().to", "torch.nn.functional.cross_entropy", "torch.nn.functional.cross_entropy", "losses.update", "region_feats.norm", "clip_rcnn.PretrainFastRCNN.norm", "detectron2.utils.comm.gather_tensors", "detectron2.utils.comm.gather_tensors", "text_embs_full.view().t", "region_feats_full.size", "pooled_score.t", "[].size", "x[].view().to", "random.randint", "[].to", "torch.arange", "range", "enumerate", "text_embs_full.view", "x[].view", "len", "text_embs_full.size", "x[].view"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.encode_text", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "", "def", "image_text_matching", "(", "self", ",", "batched_inputs", ",", "proposals", ",", "region_feats", ",", "losses", ",", "global_feats", ")", ":", "\n", "# encode text", "\n", "        ", "num_cap", "=", "int", "(", "batched_inputs", "[", "0", "]", "[", "1", "]", ".", "size", "(", "0", ")", "/", "self", ".", "context_length", ")", "\n", "if", "num_cap", "==", "1", ":", "# one caption per image", "\n", "            ", "text", "=", "[", "x", "[", "1", "]", ".", "view", "(", "1", ",", "-", "1", ")", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "", "else", ":", "# multiple caption pers image, then randomly pick one", "\n", "            ", "rand_ind", "=", "[", "randint", "(", "0", ",", "num_cap", "-", "1", ")", "for", "_", "in", "range", "(", "len", "(", "batched_inputs", ")", ")", "]", "\n", "text", "=", "[", "x", "[", "1", "]", ".", "view", "(", "-", "1", ",", "self", ".", "context_length", ")", "[", "rand_ind", "[", "i", "]", ":", "rand_ind", "[", "i", "]", "+", "1", "]", ".", "to", "(", "self", ".", "device", ")", "for", "i", ",", "x", "in", "enumerate", "(", "batched_inputs", ")", "]", "\n", "", "text", "=", "torch", ".", "cat", "(", "text", ",", "dim", "=", "0", ")", "\n", "text_embs", "=", "self", ".", "lang_encoder", ".", "encode_text", "(", "text", ",", "only_eot", "=", "self", ".", "only_eot", ")", "# [img_batch, n_ctx, transformer.width] or [img_batch, transformer.width]", "\n", "\n", "# prepare region features and text embeddings", "\n", "region_feats", "=", "global_feats", "\n", "region_feats", "=", "region_feats", "/", "region_feats", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "text_embs", "=", "text_embs", "/", "text_embs", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "region_feats_full", ",", "min_bs", "=", "gather_tensors", "(", "region_feats", ")", "if", "self", ".", "gather_gpus", "else", "(", "region_feats", ",", "None", ")", "#  gather across GPUs", "\n", "text_embs_full", ",", "min_bs", "=", "gather_tensors", "(", "text_embs", ")", "if", "self", ".", "gather_gpus", "else", "(", "text_embs", ",", "None", ")", "#  gather across GPUs", "\n", "\n", "# matching visual features with text embs", "\n", "match_scores", "=", "region_feats_full", "@", "text_embs_full", ".", "view", "(", "-", "1", ",", "text_embs_full", ".", "size", "(", "-", "1", ")", ")", ".", "t", "(", ")", "# [#regions, img_batch * n_ctx]", "\n", "img_b", "=", "int", "(", "region_feats_full", ".", "size", "(", "0", ")", ")", "\n", "pooled_score", "=", "match_scores", "\n", "\n", "pooled_score", "=", "pooled_score", "/", "self", ".", "matching_temp", "\n", "contrast_target", "=", "torch", ".", "arange", "(", "img_b", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "row_loss", "=", "F", ".", "cross_entropy", "(", "pooled_score", ",", "contrast_target", ")", "\n", "col_loss", "=", "F", ".", "cross_entropy", "(", "pooled_score", ".", "t", "(", ")", ",", "contrast_target", ")", "\n", "losses", ".", "update", "(", "{", "\"loss_img_txt_level\"", ":", "(", "row_loss", "+", "col_loss", ")", "/", "2.0", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.get_psuedo_concept_labels": [[627, 652], ["torch.no_grad", "clip_rcnn.PretrainFastRCNN.teacher_backbone", "clip_rcnn.PretrainFastRCNN.teacher_roi_heads", "torch.nn.functional.softmax", "torch.max", "clip_rcnn.PretrainFastRCNN.norm", "clip_rcnn.PretrainFastRCNN.teacher_concept_emb.norm", "teacher_concept_emb.t", "keep_regions.nonzero().size", "print", "keep_regions.nonzero", "target_inds.view", "target_inds.view"], "methods", ["None"], ["", "def", "get_psuedo_concept_labels", "(", "self", ",", "images", ",", "proposals", ",", "gt_instances", ",", "s_temp", "=", "0.01", ")", ":", "\n", "        ", "\"\"\" Input images and region proposals, return matching results from teacher model\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# extract visual features from teacher model", "\n", "            ", "features", "=", "self", ".", "teacher_backbone", "(", "images", ".", "tensor", ")", "\n", "teacher_region_feats", "=", "self", ".", "teacher_roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ",", "res5", "=", "self", ".", "teacher_backbone", ".", "layer4", ",", "attnpool", "=", "self", ".", "teacher_backbone", ".", "attnpool", ")", "\n", "\n", "# match teacher visual features with teacher concept embs to create pseudo labels", "\n", "teacher_region_feats", "=", "teacher_region_feats", "/", "teacher_region_feats", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "teacher_concept_emb", "=", "self", ".", "teacher_concept_emb", "/", "self", ".", "teacher_concept_emb", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "concept_scores", "=", "teacher_region_feats", "@", "teacher_concept_emb", ".", "t", "(", ")", "# [#regions, #concepts]", "\n", "concept_scores", "=", "F", ".", "softmax", "(", "concept_scores", "/", "s_temp", ",", "dim", "=", "1", ")", "\n", "\n", "max_scores", ",", "max_inds", "=", "torch", ".", "max", "(", "concept_scores", ",", "dim", "=", "1", ")", "\n", "keep_regions", "=", "max_scores", ">", "self", ".", "concept_thres", "# only keep the regions that have high matching score with a concept", "\n", "if", "keep_regions", ".", "nonzero", "(", ")", ".", "size", "(", "0", ")", "==", "0", ":", "# if all regions can't match to any concept", "\n", "                ", "print", "(", "\"all regions can't match to any concept!\"", ")", "\n", "keep_regions", "=", "max_scores", ">", "0.0", "\n", "", "target_inds", "=", "max_inds", "[", "keep_regions", "]", "\n", "target_embs", "=", "self", ".", "concept_emb", "[", "target_inds", "]", "# the target embedding of student model", "\n", "label_mtx", "=", "(", "target_inds", ".", "view", "(", "-", "1", ",", "1", ")", "==", "target_inds", ".", "view", "(", "1", ",", "-", "1", ")", ")", ".", "type_as", "(", "teacher_region_feats", ")", "\n", "concept_scores", "=", "concept_scores", "[", "keep_regions", "]", "\n", "\n", "", "return", "concept_scores", ",", "target_inds", ",", "keep_regions", ",", "target_embs", ",", "label_mtx", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.get_region_features": [[653, 665], ["clip_rcnn.PretrainFastRCNN.roi_heads", "clip_rcnn.PretrainFastRCNN.roi_heads", "clip_rcnn.PretrainFastRCNN.roi_heads"], "methods", ["None"], ["", "def", "get_region_features", "(", "self", ",", "images", ",", "features", ",", "proposals", ",", "gt_instances", ")", ":", "\n", "        ", "\"\"\" Input images and region proposals, return region features\n        \"\"\"", "\n", "# Given the proposals, crop region features from 2D image features", "\n", "if", "self", ".", "use_clip_c4", ":", "# use C4 + resnet weights from CLIP", "\n", "            ", "if", "self", ".", "use_clip_attpool", ":", "# use att_pool from CLIP to match dimension", "\n", "                ", "region_feats", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ",", "attnpool", "=", "self", ".", "backbone", ".", "attnpool", ")", "\n", "", "else", ":", "# use mean pool", "\n", "                ", "region_feats", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ",", "res5", "=", "self", ".", "backbone", ".", "layer4", ")", "\n", "", "", "else", ":", "# regular detector setting", "\n", "            ", "region_feats", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "gt_instances", ")", "\n", "", "return", "region_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.get_region_proposals": [[666, 687], ["torch.no_grad", "clip_rcnn.PretrainFastRCNN.create_rand_boxes", "[].to", "clip_rcnn.PretrainFastRCNN.offline_preprocess_image", "clip_rcnn.PretrainFastRCNN.offline_backbone", "enumerate", "clip_rcnn.PretrainFastRCNN.offline_backbone.eval", "clip_rcnn.PretrainFastRCNN.offline_proposal_generator.eval", "clip_rcnn.PretrainFastRCNN.offline_proposal_generator", "torch.randperm", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.create_rand_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.offline_preprocess_image"], ["", "def", "get_region_proposals", "(", "self", ",", "batched_inputs", ")", ":", "\n", "        ", "\"\"\" Given image, return object proposals\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "self", ".", "clip_crop_region_type", "==", "\"RANDOM\"", ":", "# from random proposals", "\n", "                ", "proposals", "=", "self", ".", "create_rand_boxes", "(", "batched_inputs", ")", "\n", "", "elif", "self", ".", "clip_crop_region_type", "==", "\"RPN\"", ":", "# from the backbone & RPN of standard Mask-RCNN, trained on base classes", "\n", "                ", "if", "self", ".", "offline_backbone", ".", "training", "or", "self", ".", "offline_proposal_generator", ".", "training", ":", "#  was set to True in training script", "\n", "                    ", "self", ".", "offline_backbone", ".", "eval", "(", ")", "\n", "self", ".", "offline_proposal_generator", ".", "eval", "(", ")", "\n", "", "images", "=", "self", ".", "offline_preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "offline_backbone", "(", "images", ".", "tensor", ")", "\n", "if", "self", ".", "offline_proposal_generator", "is", "not", "None", ":", "\n", "                    ", "proposals", ",", "_", "=", "self", ".", "offline_proposal_generator", "(", "images", ",", "features", ",", "None", ")", "\n", "#visualize_proposals(batched_inputs, proposals, self.input_format, vis_pretrain=True)", "\n", "\n", "# randomly select proposals", "\n", "", "", "", "if", "self", ".", "training", ":", "\n", "            ", "rand_inds", "=", "[", "torch", ".", "randperm", "(", "len", "(", "p", ")", ")", "[", ":", "self", ".", "num_regions_per_img", "]", ".", "to", "(", "self", ".", "device", ")", "for", "p", "in", "proposals", "]", "\n", "proposals", "=", "[", "p", "[", "rand_inds", "[", "i", "]", "]", "for", "i", ",", "p", "in", "enumerate", "(", "proposals", ")", "]", "\n", "", "return", "proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.offline_preprocess_image": [[688, 704], ["detectron2.structures.ImageList.from_tensors", "x[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "offline_preprocess_image", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Normalize, pad and batch the input images. Use detectron2 default processing (pixel mean & std).\n        Note: the image tsv in pretraining are already normalized pixel values and thus opposite to Detectron2 default input.\n        Note: Due to FPN size_divisibility, images are padded by right/bottom border. So FPN is consistent with C4 and GT boxes.\n        \"\"\"", "\n", "images", "=", "[", "x", "[", "0", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "if", "(", "self", ".", "input_format", "==", "'RGB'", "and", "self", ".", "offline_input_format", "==", "'BGR'", ")", "or", "(", "self", ".", "input_format", "==", "'BGR'", "and", "self", ".", "offline_input_format", "==", "'RGB'", ")", ":", "\n", "            ", "images", "=", "[", "x", "[", "[", "2", ",", "1", ",", "0", "]", ",", ":", ",", ":", "]", "for", "x", "in", "images", "]", "\n", "", "if", "self", ".", "offline_div_pixel", ":", "\n", "            ", "images", "=", "[", "(", "x", "-", "self", ".", "offline_pixel_mean", ")", "/", "self", ".", "offline_pixel_std", "for", "x", "in", "images", "]", "\n", "", "else", ":", "\n", "            ", "images", "=", "[", "(", "(", "x", "*", "255.0", ")", "-", "self", ".", "offline_pixel_mean", ")", "/", "self", ".", "offline_pixel_std", "for", "x", "in", "images", "]", "\n", "", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "self", ".", "offline_backbone", ".", "size_divisibility", ")", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image": [[705, 718], ["detectron2.structures.ImageList.from_tensors", "x[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "preprocess_image", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Normalize, pad and batch the input images. Use CLIP default processing (pixel mean & std).\n        Note: the image tsv in pretraining are already normalized pixel values and thus opposite to Detectron2 default input.\n        Note: Due to FPN size_divisibility, images are padded by right/bottom border. So FPN is consistent with C4 and GT boxes.\n        \"\"\"", "\n", "images", "=", "[", "x", "[", "0", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "if", "self", ".", "div_pixel", ":", "\n", "            ", "images", "=", "[", "(", "x", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "", "else", ":", "\n", "            ", "images", "=", "[", "(", "(", "x", "*", "255.0", ")", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "self", ".", "backbone", ".", "size_divisibility", ")", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.create_rand_boxes": [[719, 745], ["clip_rcnn.PretrainFastRCNN.preprocess_image", "clip_rcnn.PretrainFastRCNN.tensor.size", "clip_rcnn.PretrainFastRCNN.tensor.size", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.randint", "torch.randint", "range", "torch.cat().float().to", "torch.tensor.size", "torch.tensor.size", "torch.cat", "torch.cat().float().to.append", "detectron2.structures.Boxes", "torch.cat().float", "range", "range", "range", "range", "range", "left_top_x[].view", "left_top_y[].view", "rb_x.view", "rb_y.view", "len", "torch.randperm", "torch.randperm", "torch.cat", "rb_x_candidates.size", "rb_y_candidates.size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "create_rand_boxes", "(", "self", ",", "batched_inputs", ",", "grid_length", "=", "8", ")", ":", "\n", "        ", "\"\"\" create random boxes within an image, output random self.num_regions_per_img boxes\n        return a list of Boxes\n        \"\"\"", "\n", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "image_height", "=", "images", ".", "tensor", ".", "size", "(", "2", ")", "\n", "image_width", "=", "images", ".", "tensor", ".", "size", "(", "3", ")", "\n", "\n", "left_top_x", "=", "torch", ".", "tensor", "(", "[", "i", "*", "(", "grid_length", ")", "for", "i", "in", "range", "(", "image_width", "//", "grid_length", ")", "]", ")", "\n", "left_top_y", "=", "torch", ".", "tensor", "(", "[", "i", "*", "(", "grid_length", ")", "for", "i", "in", "range", "(", "image_height", "//", "grid_length", ")", "]", ")", "\n", "right_bot_x", "=", "torch", ".", "tensor", "(", "[", "(", "i", "+", "1", ")", "*", "(", "grid_length", ")", "for", "i", "in", "range", "(", "image_width", "//", "grid_length", ")", "]", ")", "\n", "right_bot_y", "=", "torch", ".", "tensor", "(", "[", "(", "i", "+", "1", ")", "*", "(", "grid_length", ")", "for", "i", "in", "range", "(", "image_height", "//", "grid_length", ")", "]", ")", "\n", "x_inds", "=", "torch", ".", "randint", "(", "0", ",", "left_top_x", ".", "size", "(", "0", ")", ",", "(", "self", ".", "num_regions_per_img", ",", ")", ")", "\n", "y_inds", "=", "torch", ".", "randint", "(", "0", ",", "left_top_y", ".", "size", "(", "0", ")", ",", "(", "self", ".", "num_regions_per_img", ",", ")", ")", "\n", "\n", "proposals", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_regions_per_img", ")", ":", "\n", "            ", "rb_x_candidates", "=", "right_bot_x", "[", "x_inds", "[", "i", "]", ":", "]", "\n", "rb_x", "=", "rb_x_candidates", "[", "torch", ".", "randperm", "(", "rb_x_candidates", ".", "size", "(", "0", ")", ")", "[", "0", "]", "]", "\n", "rb_y_candidates", "=", "right_bot_y", "[", "y_inds", "[", "i", "]", ":", "]", "\n", "rb_y", "=", "rb_y_candidates", "[", "torch", ".", "randperm", "(", "rb_y_candidates", ".", "size", "(", "0", ")", ")", "[", "0", "]", "]", "\n", "this_box", "=", "torch", ".", "cat", "(", "(", "left_top_x", "[", "x_inds", "[", "i", "]", "]", ".", "view", "(", "1", ",", "1", ")", ",", "left_top_y", "[", "y_inds", "[", "i", "]", "]", ".", "view", "(", "1", ",", "1", ")", ",", "rb_x", ".", "view", "(", "1", ",", "1", ")", ",", "rb_y", ".", "view", "(", "1", ",", "1", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "proposals", ".", "append", "(", "this_box", ")", "\n", "", "proposals", "=", "torch", ".", "cat", "(", "proposals", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "proposals", "=", "[", "Boxes", "(", "proposals", ")", "for", "i", "in", "range", "(", "len", "(", "batched_inputs", ")", ")", "]", "# a list of Boxes", "\n", "return", "proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.create_global_proposals": [[746, 756], ["clip_rcnn.PretrainFastRCNN.preprocess_image", "clip_rcnn.PretrainFastRCNN.tensor.size", "clip_rcnn.PretrainFastRCNN.tensor.size", "torch.tensor().view().float().to", "detectron2.structures.Boxes", "torch.tensor().view().float", "range", "len", "torch.tensor().view", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "create_global_proposals", "(", "self", ",", "batched_inputs", ")", ":", "\n", "        ", "\"\"\" create a single global box for an image, so as to extract global image features with RoIAlign on high-resolution images.\n        \"\"\"", "\n", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "image_height", "=", "images", ".", "tensor", ".", "size", "(", "2", ")", "\n", "image_width", "=", "images", ".", "tensor", ".", "size", "(", "3", ")", "\n", "\n", "global_box", "=", "torch", ".", "tensor", "(", "[", "0", ",", "0", ",", "image_width", ",", "image_height", "]", ")", ".", "view", "(", "1", ",", "4", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "proposals", "=", "[", "Boxes", "(", "global_box", ")", "for", "i", "in", "range", "(", "len", "(", "batched_inputs", ")", ")", "]", "# a list of Boxes", "\n", "return", "proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.inference": [[757, 759], ["None"], "methods", ["None"], ["", "def", "inference", "(", "self", ",", "batched_inputs", ",", "detected_instances", "=", "None", ",", "do_postprocess", "=", "True", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN._postprocess": [[760, 772], ["zip", "postprocessing.detector_postprocess", "processed_results.append"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess"], ["", "@", "staticmethod", "\n", "def", "_postprocess", "(", "instances", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Rescale the output instances to the target size.\n        \"\"\"", "\n", "# note: private function; subject to changes", "\n", "processed_results", "=", "[", "]", "\n", "for", "results_per_image", ",", "input_per_image", "in", "zip", "(", "instances", ",", "batched_inputs", ")", ":", "\n", "            ", "height", ",", "width", "=", "input_per_image", "[", "-", "1", "]", "[", "2", "]", "# original image size, before resizing", "\n", "r", "=", "detector_postprocess", "(", "results_per_image", ",", "height", ",", "width", ")", "\n", "processed_results", ".", "append", "(", "{", "\"instances\"", ":", "r", "}", ")", "\n", "", "return", "processed_results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.visualize_proposals": [[774, 822], ["enumerate", "zip", "zip", "detectron2.data.detection_utils.convert_image_to_rgb", "min", "Visualizer", "v_pred.overlay_instances.overlay_instances", "v_pred.overlay_instances.get_image", "PIL.Image.fromarray", "Image.fromarray.save", "detectron2.data.detection_utils.convert_image_to_rgb", "Visualizer", "v_gt.overlay_instances.overlay_instances", "v_gt.overlay_instances.get_image", "min", "Visualizer", "v_pred.overlay_instances.overlay_instances", "v_pred.overlay_instances.get_image", "numpy.concatenate", "PIL.Image.fromarray", "Image.fromarray.save", "detectron2.data.detection_utils.convert_image_to_rgb.permute", "len", "numpy.array", "detectron2.data.detection_utils.convert_image_to_rgb.permute", "len", "numpy.array", "prop.proposal_boxes[].tensor.cpu().numpy", "prop.proposal_boxes[].tensor.cpu().numpy", "str", "prop.proposal_boxes[].tensor.cpu", "prop.proposal_boxes[].tensor.cpu", "[].split", "f_n.split"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.convert_image_to_rgb", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.convert_image_to_rgb", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save"], ["", "", "def", "visualize_proposals", "(", "batched_inputs", ",", "proposals", ",", "input_format", ",", "vis_pretrain", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    A function used to visualize images and proposals. It shows ground truth\n    bounding boxes on the original image and up to 20 top-scoring predicted\n    object proposals on the original image. Users can implement different\n    visualization functions for different models.\n\n    Args:\n        batched_inputs (list): a list that contains input to the model.\n        proposals (list): a list that contains predicted proposals. Both\n            batched_inputs and proposals should have the same length.\n    \"\"\"", "\n", "from", "detectron2", ".", "utils", ".", "visualizer", "import", "Visualizer", "\n", "\n", "max_vis_prop", "=", "50", "\n", "if", "vis_pretrain", ":", "\n", "        ", "for", "i", ",", "(", "input", ",", "prop", ")", "in", "enumerate", "(", "zip", "(", "batched_inputs", ",", "proposals", ")", ")", ":", "\n", "            ", "img", "=", "input", "[", "0", "]", "*", "255.0", "\n", "img", "=", "convert_image_to_rgb", "(", "img", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "input_format", ")", "\n", "box_size", "=", "min", "(", "len", "(", "prop", ".", "proposal_boxes", ")", ",", "max_vis_prop", ")", "\n", "v_pred", "=", "Visualizer", "(", "img", ",", "None", ")", "\n", "v_pred", "=", "v_pred", ".", "overlay_instances", "(", "\n", "boxes", "=", "prop", ".", "proposal_boxes", "[", "0", ":", "box_size", "]", ".", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", ")", "\n", "prop_img", "=", "v_pred", ".", "get_image", "(", ")", "\n", "vis_img", "=", "prop_img", "\n", "to_save", "=", "Image", ".", "fromarray", "(", "np", ".", "array", "(", "vis_img", ",", "np", ".", "uint8", ")", ")", "\n", "to_save", ".", "save", "(", "\"output/regions/\"", "+", "str", "(", "i", ")", "+", "\".png\"", ")", "\n", "#break  # only visualize one image in a batch", "\n", "", "", "else", ":", "\n", "        ", "for", "input", ",", "prop", "in", "zip", "(", "batched_inputs", ",", "proposals", ")", ":", "\n", "            ", "img", "=", "input", "[", "\"image\"", "]", "\n", "img", "=", "convert_image_to_rgb", "(", "img", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "input_format", ")", "\n", "v_gt", "=", "Visualizer", "(", "img", ",", "None", ")", "\n", "v_gt", "=", "v_gt", ".", "overlay_instances", "(", "boxes", "=", "input", "[", "\"instances\"", "]", ".", "gt_boxes", ")", "\n", "anno_img", "=", "v_gt", ".", "get_image", "(", ")", "\n", "box_size", "=", "min", "(", "len", "(", "prop", ".", "proposal_boxes", ")", ",", "max_vis_prop", ")", "\n", "v_pred", "=", "Visualizer", "(", "img", ",", "None", ")", "\n", "v_pred", "=", "v_pred", ".", "overlay_instances", "(", "\n", "boxes", "=", "prop", ".", "proposal_boxes", "[", "0", ":", "box_size", "]", ".", "tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", ")", "\n", "prop_img", "=", "v_pred", ".", "get_image", "(", ")", "\n", "vis_img", "=", "np", ".", "concatenate", "(", "(", "anno_img", ",", "prop_img", ")", ",", "axis", "=", "1", ")", "\n", "#vis_img = vis_img.transpose(2, 0, 1)", "\n", "vis_name", "=", "\"Left: GT bounding boxes;  Right: Predicted proposals\"", "\n", "f_n", "=", "input", "[", "'file_name'", "]", "\n", "to_save", "=", "Image", ".", "fromarray", "(", "np", ".", "array", "(", "vis_img", ",", "np", ".", "uint8", ")", ")", "\n", "to_save", ".", "save", "(", "\"output/regions/\"", "+", "f_n", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "split", "(", "\".\"", ")", "[", "0", "]", "+", "\".png\"", ")", "\n", "#break  # only visualize one image in a batch", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.panoptic_fpn.PanopticFPN.__init__": [[26, 55], ["rcnn.GeneralizedRCNN.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "sem_seg_head", ":", "nn", ".", "Module", ",", "\n", "combine_overlap_thresh", ":", "float", "=", "0.5", ",", "\n", "combine_stuff_area_thresh", ":", "float", "=", "4096", ",", "\n", "combine_instances_score_thresh", ":", "float", "=", "0.5", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            sem_seg_head: a module for the semantic segmentation head.\n            combine_overlap_thresh: combine masks into one instances if\n                they have enough overlap\n            combine_stuff_area_thresh: ignore stuff areas smaller than this threshold\n            combine_instances_score_thresh: ignore instances whose score is\n                smaller than this threshold\n\n        Other arguments are the same as :class:`GeneralizedRCNN`.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "sem_seg_head", "=", "sem_seg_head", "\n", "# options when combining instance & semantic outputs", "\n", "self", ".", "combine_overlap_thresh", "=", "combine_overlap_thresh", "\n", "self", ".", "combine_stuff_area_thresh", "=", "combine_stuff_area_thresh", "\n", "self", ".", "combine_instances_score_thresh", "=", "combine_instances_score_thresh", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.panoptic_fpn.PanopticFPN.from_config": [[56, 89], ["super().from_config", "super().from_config.update", "semantic_seg.build_sem_seg_head", "logging.getLogger", "ret[].output_shape", "logging.getLogger.warning", "logging.getLogger.warning", "panoptic_fpn.PanopticFPN.from_config.update_weight"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.build_sem_seg_head", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ")", "\n", "ret", ".", "update", "(", "\n", "{", "\n", "\"combine_overlap_thresh\"", ":", "cfg", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "COMBINE", ".", "OVERLAP_THRESH", ",", "\n", "\"combine_stuff_area_thresh\"", ":", "cfg", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "COMBINE", ".", "STUFF_AREA_LIMIT", ",", "\n", "\"combine_instances_score_thresh\"", ":", "cfg", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "COMBINE", ".", "INSTANCES_CONFIDENCE_THRESH", ",", "# noqa", "\n", "}", "\n", ")", "\n", "ret", "[", "\"sem_seg_head\"", "]", "=", "build_sem_seg_head", "(", "cfg", ",", "ret", "[", "\"backbone\"", "]", ".", "output_shape", "(", ")", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "if", "not", "cfg", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "COMBINE", ".", "ENABLED", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"PANOPTIC_FPN.COMBINED.ENABLED is no longer used. \"", "\n", "\" model.inference(do_postprocess=) should be used to toggle postprocessing.\"", "\n", ")", "\n", "", "if", "cfg", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "INSTANCE_LOSS_WEIGHT", "!=", "1.0", ":", "\n", "            ", "w", "=", "cfg", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "INSTANCE_LOSS_WEIGHT", "\n", "logger", ".", "warning", "(", "\n", "\"PANOPTIC_FPN.INSTANCE_LOSS_WEIGHT should be replaced by weights on each ROI head.\"", "\n", ")", "\n", "\n", "def", "update_weight", "(", "x", ")", ":", "\n", "                ", "if", "isinstance", "(", "x", ",", "dict", ")", ":", "\n", "                    ", "return", "{", "k", ":", "v", "*", "w", "for", "k", ",", "v", "in", "x", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "                    ", "return", "x", "*", "w", "\n", "\n", "", "", "roi_heads", "=", "ret", "[", "\"roi_heads\"", "]", "\n", "roi_heads", ".", "box_predictor", ".", "loss_weight", "=", "update_weight", "(", "roi_heads", ".", "box_predictor", ".", "loss_weight", ")", "\n", "roi_heads", ".", "mask_head", ".", "loss_weight", "=", "update_weight", "(", "roi_heads", ".", "mask_head", ".", "loss_weight", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.panoptic_fpn.PanopticFPN.forward": [[90, 136], ["panoptic_fpn.PanopticFPN.preprocess_image", "panoptic_fpn.PanopticFPN.backbone", "panoptic_fpn.PanopticFPN.sem_seg_head", "panoptic_fpn.PanopticFPN.proposal_generator", "panoptic_fpn.PanopticFPN.roi_heads", "losses.update", "losses.update", "panoptic_fpn.PanopticFPN.inference", "x[].to", "detectron2.structures.ImageList.from_tensors", "x[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "forward", "(", "self", ",", "batched_inputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            batched_inputs: a list, batched outputs of :class:`DatasetMapper`.\n                Each item in the list contains the inputs for one image.\n\n                For now, each item in the list is a dict that contains:\n\n                * \"image\": Tensor, image in (C, H, W) format.\n                * \"instances\": Instances\n                * \"sem_seg\": semantic segmentation ground truth.\n                * Other information that's included in the original dicts, such as:\n                  \"height\", \"width\" (int): the output resolution of the model, used in inference.\n                  See :meth:`postprocess` for details.\n\n        Returns:\n            list[dict]:\n                each dict has the results for one image. The dict contains the following keys:\n\n                * \"instances\": see :meth:`GeneralizedRCNN.forward` for its format.\n                * \"sem_seg\": see :meth:`SemanticSegmentor.forward` for its format.\n                * \"panoptic_seg\": See the return value of\n                  :func:`combine_semantic_and_instance_outputs` for its format.\n        \"\"\"", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "return", "self", ".", "inference", "(", "batched_inputs", ")", "\n", "", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "\n", "assert", "\"sem_seg\"", "in", "batched_inputs", "[", "0", "]", "\n", "gt_sem_seg", "=", "[", "x", "[", "\"sem_seg\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "gt_sem_seg", "=", "ImageList", ".", "from_tensors", "(", "\n", "gt_sem_seg", ",", "self", ".", "backbone", ".", "size_divisibility", ",", "self", ".", "sem_seg_head", ".", "ignore_value", "\n", ")", ".", "tensor", "\n", "sem_seg_results", ",", "sem_seg_losses", "=", "self", ".", "sem_seg_head", "(", "features", ",", "gt_sem_seg", ")", "\n", "\n", "gt_instances", "=", "[", "x", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "proposals", ",", "proposal_losses", "=", "self", ".", "proposal_generator", "(", "images", ",", "features", ",", "gt_instances", ")", "\n", "detector_results", ",", "detector_losses", "=", "self", ".", "roi_heads", "(", "\n", "images", ",", "features", ",", "proposals", ",", "gt_instances", "\n", ")", "\n", "\n", "losses", "=", "sem_seg_losses", "\n", "losses", ".", "update", "(", "proposal_losses", ")", "\n", "losses", ".", "update", "(", "detector_losses", ")", "\n", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.panoptic_fpn.PanopticFPN.inference": [[137, 179], ["panoptic_fpn.PanopticFPN.preprocess_image", "panoptic_fpn.PanopticFPN.backbone", "panoptic_fpn.PanopticFPN.sem_seg_head", "panoptic_fpn.PanopticFPN.proposal_generator", "panoptic_fpn.PanopticFPN.roi_heads", "zip", "input_per_image.get", "input_per_image.get", "postprocessing.sem_seg_postprocess", "postprocessing.detector_postprocess", "processed_results.append", "panoptic_fpn.combine_semantic_and_instance_outputs", "postprocessing.sem_seg_postprocess.argmax"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN.preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.sem_seg_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.panoptic_fpn.combine_semantic_and_instance_outputs"], ["", "def", "inference", "(", "self", ",", "batched_inputs", ":", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ",", "do_postprocess", ":", "bool", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Run inference on the given inputs.\n\n        Args:\n            batched_inputs (list[dict]): same as in :meth:`forward`\n            do_postprocess (bool): whether to apply post-processing on the outputs.\n\n        Returns:\n            When do_postprocess=True, see docs in :meth:`forward`.\n            Otherwise, returns a (list[Instances], list[Tensor]) that contains\n            the raw detector outputs, and raw semantic segmentation outputs.\n        \"\"\"", "\n", "images", "=", "self", ".", "preprocess_image", "(", "batched_inputs", ")", "\n", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "sem_seg_results", ",", "sem_seg_losses", "=", "self", ".", "sem_seg_head", "(", "features", ",", "None", ")", "\n", "proposals", ",", "_", "=", "self", ".", "proposal_generator", "(", "images", ",", "features", ",", "None", ")", "\n", "detector_results", ",", "_", "=", "self", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ",", "None", ")", "\n", "\n", "if", "do_postprocess", ":", "\n", "            ", "processed_results", "=", "[", "]", "\n", "for", "sem_seg_result", ",", "detector_result", ",", "input_per_image", ",", "image_size", "in", "zip", "(", "\n", "sem_seg_results", ",", "detector_results", ",", "batched_inputs", ",", "images", ".", "image_sizes", "\n", ")", ":", "\n", "                ", "height", "=", "input_per_image", ".", "get", "(", "\"height\"", ",", "image_size", "[", "0", "]", ")", "\n", "width", "=", "input_per_image", ".", "get", "(", "\"width\"", ",", "image_size", "[", "1", "]", ")", "\n", "sem_seg_r", "=", "sem_seg_postprocess", "(", "sem_seg_result", ",", "image_size", ",", "height", ",", "width", ")", "\n", "detector_r", "=", "detector_postprocess", "(", "detector_result", ",", "height", ",", "width", ")", "\n", "\n", "processed_results", ".", "append", "(", "{", "\"sem_seg\"", ":", "sem_seg_r", ",", "\"instances\"", ":", "detector_r", "}", ")", "\n", "\n", "panoptic_r", "=", "combine_semantic_and_instance_outputs", "(", "\n", "detector_r", ",", "\n", "sem_seg_r", ".", "argmax", "(", "dim", "=", "0", ")", ",", "\n", "self", ".", "combine_overlap_thresh", ",", "\n", "self", ".", "combine_stuff_area_thresh", ",", "\n", "self", ".", "combine_instances_score_thresh", ",", "\n", ")", "\n", "processed_results", "[", "-", "1", "]", "[", "\"panoptic_seg\"", "]", "=", "panoptic_r", "\n", "", "return", "processed_results", "\n", "", "else", ":", "\n", "            ", "return", "detector_results", ",", "sem_seg_results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.panoptic_fpn.combine_semantic_and_instance_outputs": [[181, 267], ["torch.zeros_like", "torch.argsort", "instance_results.pred_masks.to", "torch.unique().cpu().tolist", "instance_results.scores[].item", "mask.sum().item", "intersect.sum().item", "segments_info.append", "mask.sum().item", "segments_info.append", "torch.unique().cpu", "mask.sum", "intersect.sum", "instance_results.pred_classes[].item", "inst_id.item", "mask.sum", "torch.unique"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "", "", "def", "combine_semantic_and_instance_outputs", "(", "\n", "instance_results", ",", "\n", "semantic_results", ",", "\n", "overlap_threshold", ",", "\n", "stuff_area_thresh", ",", "\n", "instances_score_thresh", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Implement a simple combining logic following\n    \"combine_semantic_and_instance_predictions.py\" in panopticapi\n    to produce panoptic segmentation outputs.\n\n    Args:\n        instance_results: output of :func:`detector_postprocess`.\n        semantic_results: an (H, W) tensor, each element is the contiguous semantic\n            category id\n\n    Returns:\n        panoptic_seg (Tensor): of shape (height, width) where the values are ids for each segment.\n        segments_info (list[dict]): Describe each segment in `panoptic_seg`.\n            Each dict contains keys \"id\", \"category_id\", \"isthing\".\n    \"\"\"", "\n", "panoptic_seg", "=", "torch", ".", "zeros_like", "(", "semantic_results", ",", "dtype", "=", "torch", ".", "int32", ")", "\n", "\n", "# sort instance outputs by scores", "\n", "sorted_inds", "=", "torch", ".", "argsort", "(", "-", "instance_results", ".", "scores", ")", "\n", "\n", "current_segment_id", "=", "0", "\n", "segments_info", "=", "[", "]", "\n", "\n", "instance_masks", "=", "instance_results", ".", "pred_masks", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ",", "device", "=", "panoptic_seg", ".", "device", ")", "\n", "\n", "# Add instances one-by-one, check for overlaps with existing ones", "\n", "for", "inst_id", "in", "sorted_inds", ":", "\n", "        ", "score", "=", "instance_results", ".", "scores", "[", "inst_id", "]", ".", "item", "(", ")", "\n", "if", "score", "<", "instances_score_thresh", ":", "\n", "            ", "break", "\n", "", "mask", "=", "instance_masks", "[", "inst_id", "]", "# H,W", "\n", "mask_area", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "if", "mask_area", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "", "intersect", "=", "(", "mask", ">", "0", ")", "&", "(", "panoptic_seg", ">", "0", ")", "\n", "intersect_area", "=", "intersect", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "if", "intersect_area", "*", "1.0", "/", "mask_area", ">", "overlap_threshold", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "intersect_area", ">", "0", ":", "\n", "            ", "mask", "=", "mask", "&", "(", "panoptic_seg", "==", "0", ")", "\n", "\n", "", "current_segment_id", "+=", "1", "\n", "panoptic_seg", "[", "mask", "]", "=", "current_segment_id", "\n", "segments_info", ".", "append", "(", "\n", "{", "\n", "\"id\"", ":", "current_segment_id", ",", "\n", "\"isthing\"", ":", "True", ",", "\n", "\"score\"", ":", "score", ",", "\n", "\"category_id\"", ":", "instance_results", ".", "pred_classes", "[", "inst_id", "]", ".", "item", "(", ")", ",", "\n", "\"instance_id\"", ":", "inst_id", ".", "item", "(", ")", ",", "\n", "}", "\n", ")", "\n", "\n", "# Add semantic results to remaining empty areas", "\n", "", "semantic_labels", "=", "torch", ".", "unique", "(", "semantic_results", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "for", "semantic_label", "in", "semantic_labels", ":", "\n", "        ", "if", "semantic_label", "==", "0", ":", "# 0 is a special \"thing\" class", "\n", "            ", "continue", "\n", "", "mask", "=", "(", "semantic_results", "==", "semantic_label", ")", "&", "(", "panoptic_seg", "==", "0", ")", "\n", "mask_area", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "if", "mask_area", "<", "stuff_area_thresh", ":", "\n", "            ", "continue", "\n", "\n", "", "current_segment_id", "+=", "1", "\n", "panoptic_seg", "[", "mask", "]", "=", "current_segment_id", "\n", "segments_info", ".", "append", "(", "\n", "{", "\n", "\"id\"", ":", "current_segment_id", ",", "\n", "\"isthing\"", ":", "False", ",", "\n", "\"category_id\"", ":", "semantic_label", ",", "\n", "\"area\"", ":", "mask_area", ",", "\n", "}", "\n", ")", "\n", "\n", "", "return", "panoptic_seg", ",", "segments_info", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.SemanticSegmentor.__init__": [[34, 55], ["torch.nn.Module.__init__", "semantic_seg.SemanticSegmentor.register_buffer", "semantic_seg.SemanticSegmentor.register_buffer", "torch.tensor().view", "torch.tensor().view", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "backbone", ":", "Backbone", ",", "\n", "sem_seg_head", ":", "nn", ".", "Module", ",", "\n", "pixel_mean", ":", "Tuple", "[", "float", "]", ",", "\n", "pixel_std", ":", "Tuple", "[", "float", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            backbone: a backbone module, must follow detectron2's backbone interface\n            sem_seg_head: a module that predicts semantic segmentation from backbone features\n            pixel_mean, pixel_std: list or tuple with #channels element, representing\n                the per-channel mean and std to be used to normalize the input image\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "backbone", "=", "backbone", "\n", "self", ".", "sem_seg_head", "=", "sem_seg_head", "\n", "self", ".", "register_buffer", "(", "\"pixel_mean\"", ",", "torch", ".", "tensor", "(", "pixel_mean", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "self", ".", "register_buffer", "(", "\"pixel_std\"", ",", "torch", ".", "tensor", "(", "pixel_std", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.SemanticSegmentor.from_config": [[56, 65], ["backbone.build_backbone.build_backbone", "semantic_seg.build_sem_seg_head", "backbone.build_backbone.build_backbone.output_shape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.build_sem_seg_head", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "backbone", "=", "build_backbone", "(", "cfg", ")", "\n", "sem_seg_head", "=", "build_sem_seg_head", "(", "cfg", ",", "backbone", ".", "output_shape", "(", ")", ")", "\n", "return", "{", "\n", "\"backbone\"", ":", "backbone", ",", "\n", "\"sem_seg_head\"", ":", "sem_seg_head", ",", "\n", "\"pixel_mean\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_MEAN", ",", "\n", "\"pixel_std\"", ":", "cfg", ".", "MODEL", ".", "PIXEL_STD", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.SemanticSegmentor.device": [[67, 70], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pixel_mean", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.SemanticSegmentor.forward": [[71, 120], ["detectron2.structures.ImageList.from_tensors", "semantic_seg.SemanticSegmentor.backbone", "semantic_seg.SemanticSegmentor.sem_seg_head", "zip", "x[].to", "input_per_image.get", "input_per_image.get", "postprocessing.sem_seg_postprocess", "processed_results.append", "x[].to", "detectron2.structures.ImageList.from_tensors"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.sem_seg_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors"], ["", "def", "forward", "(", "self", ",", "batched_inputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            batched_inputs: a list, batched outputs of :class:`DatasetMapper`.\n                Each item in the list contains the inputs for one image.\n\n                For now, each item in the list is a dict that contains:\n\n                   * \"image\": Tensor, image in (C, H, W) format.\n                   * \"sem_seg\": semantic segmentation ground truth\n                   * Other information that's included in the original dicts, such as:\n                     \"height\", \"width\" (int): the output resolution of the model (may be different\n                     from input resolution), used in inference.\n\n\n        Returns:\n            list[dict]:\n              Each dict is the output for one input image.\n              The dict contains one key \"sem_seg\" whose value is a\n              Tensor that represents the\n              per-pixel segmentation prediced by the head.\n              The prediction has shape KxHxW that represents the logits of\n              each class for each pixel.\n        \"\"\"", "\n", "images", "=", "[", "x", "[", "\"image\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "images", "=", "[", "(", "x", "-", "self", ".", "pixel_mean", ")", "/", "self", ".", "pixel_std", "for", "x", "in", "images", "]", "\n", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "self", ".", "backbone", ".", "size_divisibility", ")", "\n", "\n", "features", "=", "self", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "\n", "if", "\"sem_seg\"", "in", "batched_inputs", "[", "0", "]", ":", "\n", "            ", "targets", "=", "[", "x", "[", "\"sem_seg\"", "]", ".", "to", "(", "self", ".", "device", ")", "for", "x", "in", "batched_inputs", "]", "\n", "targets", "=", "ImageList", ".", "from_tensors", "(", "\n", "targets", ",", "self", ".", "backbone", ".", "size_divisibility", ",", "self", ".", "sem_seg_head", ".", "ignore_value", "\n", ")", ".", "tensor", "\n", "", "else", ":", "\n", "            ", "targets", "=", "None", "\n", "", "results", ",", "losses", "=", "self", ".", "sem_seg_head", "(", "features", ",", "targets", ")", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "return", "losses", "\n", "\n", "", "processed_results", "=", "[", "]", "\n", "for", "result", ",", "input_per_image", ",", "image_size", "in", "zip", "(", "results", ",", "batched_inputs", ",", "images", ".", "image_sizes", ")", ":", "\n", "            ", "height", "=", "input_per_image", ".", "get", "(", "\"height\"", ")", "\n", "width", "=", "input_per_image", ".", "get", "(", "\"width\"", ")", "\n", "r", "=", "sem_seg_postprocess", "(", "result", ",", "image_size", ",", "height", ",", "width", ")", "\n", "processed_results", ".", "append", "(", "{", "\"sem_seg\"", ":", "r", "}", ")", "\n", "", "return", "processed_results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.SemSegFPNHead.__init__": [[140, 202], ["torch.nn.Module.__init__", "sorted", "zip", "detectron2.layers.Conv2d", "fvcore.c2_msra_fill", "sorted.items", "max", "range", "semantic_seg.SemSegFPNHead.scale_heads.append", "semantic_seg.SemSegFPNHead.add_module", "int", "detectron2.layers.get_norm", "detectron2.layers.Conv2d", "fvcore.c2_msra_fill", "head_ops.append", "torch.nn.Sequential", "head_ops.append", "numpy.log2", "numpy.log2", "torch.nn.Upsample"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "input_shape", ":", "Dict", "[", "str", ",", "ShapeSpec", "]", ",", "\n", "*", ",", "\n", "num_classes", ":", "int", ",", "\n", "conv_dims", ":", "int", ",", "\n", "common_stride", ":", "int", ",", "\n", "loss_weight", ":", "float", "=", "1.0", ",", "\n", "norm", ":", "Optional", "[", "Union", "[", "str", ",", "Callable", "]", "]", "=", "None", ",", "\n", "ignore_value", ":", "int", "=", "-", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            input_shape: shapes (channels and stride) of the input features\n            num_classes: number of classes to predict\n            conv_dims: number of output channels for the intermediate conv layers.\n            common_stride: the common stride that all features will be upscaled to\n            loss_weight: loss weight\n            norm (str or callable): normalization for all conv layers\n            ignore_value: category id to be ignored during training.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "input_shape", "=", "sorted", "(", "input_shape", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ".", "stride", ")", "\n", "self", ".", "in_features", "=", "[", "k", "for", "k", ",", "v", "in", "input_shape", "]", "\n", "feature_strides", "=", "[", "v", ".", "stride", "for", "k", ",", "v", "in", "input_shape", "]", "\n", "feature_channels", "=", "[", "v", ".", "channels", "for", "k", ",", "v", "in", "input_shape", "]", "\n", "\n", "self", ".", "ignore_value", "=", "ignore_value", "\n", "self", ".", "common_stride", "=", "common_stride", "\n", "self", ".", "loss_weight", "=", "loss_weight", "\n", "\n", "self", ".", "scale_heads", "=", "[", "]", "\n", "for", "in_feature", ",", "stride", ",", "channels", "in", "zip", "(", "\n", "self", ".", "in_features", ",", "feature_strides", ",", "feature_channels", "\n", ")", ":", "\n", "            ", "head_ops", "=", "[", "]", "\n", "head_length", "=", "max", "(", "1", ",", "int", "(", "np", ".", "log2", "(", "stride", ")", "-", "np", ".", "log2", "(", "self", ".", "common_stride", ")", ")", ")", "\n", "for", "k", "in", "range", "(", "head_length", ")", ":", "\n", "                ", "norm_module", "=", "get_norm", "(", "norm", ",", "conv_dims", ")", "\n", "conv", "=", "Conv2d", "(", "\n", "channels", "if", "k", "==", "0", "else", "conv_dims", ",", "\n", "conv_dims", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "not", "norm", ",", "\n", "norm", "=", "norm_module", ",", "\n", "activation", "=", "F", ".", "relu", ",", "\n", ")", "\n", "weight_init", ".", "c2_msra_fill", "(", "conv", ")", "\n", "head_ops", ".", "append", "(", "conv", ")", "\n", "if", "stride", "!=", "self", ".", "common_stride", ":", "\n", "                    ", "head_ops", ".", "append", "(", "\n", "nn", ".", "Upsample", "(", "scale_factor", "=", "2", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", "\n", ")", "\n", "", "", "self", ".", "scale_heads", ".", "append", "(", "nn", ".", "Sequential", "(", "*", "head_ops", ")", ")", "\n", "self", ".", "add_module", "(", "in_feature", ",", "self", ".", "scale_heads", "[", "-", "1", "]", ")", "\n", "", "self", ".", "predictor", "=", "Conv2d", "(", "conv_dims", ",", "num_classes", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "weight_init", ".", "c2_msra_fill", "(", "self", ".", "predictor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.SemSegFPNHead.from_config": [[203, 215], ["input_shape.items"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ":", "Dict", "[", "str", ",", "ShapeSpec", "]", ")", ":", "\n", "        ", "return", "{", "\n", "\"input_shape\"", ":", "{", "\n", "k", ":", "v", "for", "k", ",", "v", "in", "input_shape", ".", "items", "(", ")", "if", "k", "in", "cfg", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "IN_FEATURES", "\n", "}", ",", "\n", "\"ignore_value\"", ":", "cfg", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "IGNORE_VALUE", ",", "\n", "\"num_classes\"", ":", "cfg", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "NUM_CLASSES", ",", "\n", "\"conv_dims\"", ":", "cfg", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "CONVS_DIM", ",", "\n", "\"common_stride\"", ":", "cfg", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "COMMON_STRIDE", ",", "\n", "\"norm\"", ":", "cfg", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "NORM", ",", "\n", "\"loss_weight\"", ":", "cfg", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "LOSS_WEIGHT", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.SemSegFPNHead.forward": [[217, 231], ["semantic_seg.SemSegFPNHead.layers", "torch.nn.functional.interpolate", "semantic_seg.SemSegFPNHead.losses"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.KRCNNConvDeconvUpsampleHead.layers", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses"], ["", "def", "forward", "(", "self", ",", "features", ",", "targets", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            In training, returns (None, dict of losses)\n            In inference, returns (CxHxW logits, {})\n        \"\"\"", "\n", "x", "=", "self", ".", "layers", "(", "features", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "return", "None", ",", "self", ".", "losses", "(", "x", ",", "targets", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "F", ".", "interpolate", "(", "\n", "x", ",", "scale_factor", "=", "self", ".", "common_stride", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", "\n", ")", "\n", "return", "x", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.SemSegFPNHead.layers": [[232, 240], ["enumerate", "semantic_seg.SemSegFPNHead.predictor"], "methods", ["None"], ["", "", "def", "layers", "(", "self", ",", "features", ")", ":", "\n", "        ", "for", "i", ",", "f", "in", "enumerate", "(", "self", ".", "in_features", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "x", "=", "self", ".", "scale_heads", "[", "i", "]", "(", "features", "[", "f", "]", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "x", "+", "self", ".", "scale_heads", "[", "i", "]", "(", "features", "[", "f", "]", ")", "\n", "", "", "x", "=", "self", ".", "predictor", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.SemSegFPNHead.losses": [[241, 251], ["torch.nn.functional.interpolate.float", "torch.nn.functional.interpolate", "torch.nn.functional.cross_entropy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy"], ["", "def", "losses", "(", "self", ",", "predictions", ",", "targets", ")", ":", "\n", "        ", "predictions", "=", "predictions", ".", "float", "(", ")", "# https://github.com/pytorch/pytorch/issues/48163", "\n", "predictions", "=", "F", ".", "interpolate", "(", "\n", "predictions", ",", "scale_factor", "=", "self", ".", "common_stride", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", "\n", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "\n", "predictions", ",", "targets", ",", "reduction", "=", "\"mean\"", ",", "ignore_index", "=", "self", ".", "ignore_value", "\n", ")", "\n", "losses", "=", "{", "\"loss_sem_seg\"", ":", "loss", "*", "self", ".", "loss_weight", "}", "\n", "return", "losses", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.semantic_seg.build_sem_seg_head": [[122, 128], ["SEM_SEG_HEADS_REGISTRY.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "build_sem_seg_head", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Build a semantic segmentation head from `cfg.MODEL.SEM_SEG_HEAD.NAME`.\n    \"\"\"", "\n", "name", "=", "cfg", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "NAME", "\n", "return", "SEM_SEG_HEADS_REGISTRY", ".", "get", "(", "name", ")", "(", "cfg", ",", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils._is_tracing": [[14, 20], ["torch.jit.is_scripting", "torch.jit.is_tracing"], "function", ["None"], ["def", "_is_tracing", "(", ")", ":", "\n", "    ", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "# https://github.com/pytorch/pytorch/issues/47379", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "return", "TORCH_VERSION", ">=", "(", "1", ",", "7", ")", "and", "torch", ".", "jit", ".", "is_tracing", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.find_top_rpn_proposals": [[22, 131], ["len", "torch.arange", "enumerate", "detectron2.layers.cat", "detectron2.layers.cat", "detectron2.layers.cat", "enumerate", "zip", "isinstance", "logits_i.sort", "logits_i.narrow", "idx.narrow", "detectron2.layers.cat.append", "detectron2.layers.cat.append", "detectron2.layers.cat.append", "detectron2.structures.Boxes", "detectron2.structures.Boxes.clip", "detectron2.structures.Boxes.nonempty", "detectron2.layers.batched_nms", "detectron2.structures.Instances", "results.append", "torch.clamp", "min", "torch.full", "torch.isfinite().all", "torch.isfinite", "valid_mask.all", "proposal_utils._is_tracing", "FloatingPointError", "detectron2.layers.batched_nms.sum().item", "len", "torch.isfinite", "detectron2.layers.batched_nms.sum"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.nonempty", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils._is_tracing"], ["", "", "def", "find_top_rpn_proposals", "(", "\n", "proposals", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "pred_objectness_logits", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "image_sizes", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "nms_thresh", ":", "float", ",", "\n", "pre_nms_topk", ":", "int", ",", "\n", "post_nms_topk", ":", "int", ",", "\n", "min_box_size", ":", "float", ",", "\n", "training", ":", "bool", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    For each feature map, select the `pre_nms_topk` highest scoring proposals,\n    apply NMS, clip proposals, and remove small boxes. Return the `post_nms_topk`\n    highest scoring proposals among all the feature maps for each image.\n\n    Args:\n        proposals (list[Tensor]): A list of L tensors. Tensor i has shape (N, Hi*Wi*A, 4).\n            All proposal predictions on the feature maps.\n        pred_objectness_logits (list[Tensor]): A list of L tensors. Tensor i has shape (N, Hi*Wi*A).\n        image_sizes (list[tuple]): sizes (h, w) for each image\n        nms_thresh (float): IoU threshold to use for NMS\n        pre_nms_topk (int): number of top k scoring proposals to keep before applying NMS.\n            When RPN is run on multiple feature maps (as in FPN) this number is per\n            feature map.\n        post_nms_topk (int): number of top k scoring proposals to keep after applying NMS.\n            When RPN is run on multiple feature maps (as in FPN) this number is total,\n            over all feature maps.\n        min_box_size (float): minimum proposal box side length in pixels (absolute units\n            wrt input images).\n        training (bool): True if proposals are to be used in training, otherwise False.\n            This arg exists only to support a legacy bug; look for the \"NB: Legacy bug ...\"\n            comment.\n\n    Returns:\n        list[Instances]: list of N Instances. The i-th Instances\n            stores post_nms_topk object proposals for image i, sorted by their\n            objectness score in descending order.\n    \"\"\"", "\n", "num_images", "=", "len", "(", "image_sizes", ")", "\n", "device", "=", "proposals", "[", "0", "]", ".", "device", "\n", "\n", "# 1. Select top-k anchor for every level and every image", "\n", "topk_scores", "=", "[", "]", "# #lvl Tensor, each of shape N x topk", "\n", "topk_proposals", "=", "[", "]", "\n", "level_ids", "=", "[", "]", "# #lvl Tensor, each of shape (topk,)", "\n", "batch_idx", "=", "torch", ".", "arange", "(", "num_images", ",", "device", "=", "device", ")", "\n", "for", "level_id", ",", "(", "proposals_i", ",", "logits_i", ")", "in", "enumerate", "(", "zip", "(", "proposals", ",", "pred_objectness_logits", ")", ")", ":", "\n", "        ", "Hi_Wi_A", "=", "logits_i", ".", "shape", "[", "1", "]", "\n", "if", "isinstance", "(", "Hi_Wi_A", ",", "torch", ".", "Tensor", ")", ":", "# it's a tensor in tracing", "\n", "            ", "num_proposals_i", "=", "torch", ".", "clamp", "(", "Hi_Wi_A", ",", "max", "=", "pre_nms_topk", ")", "\n", "", "else", ":", "\n", "            ", "num_proposals_i", "=", "min", "(", "Hi_Wi_A", ",", "pre_nms_topk", ")", "\n", "\n", "# sort is faster than topk: https://github.com/pytorch/pytorch/issues/22812", "\n", "# topk_scores_i, topk_idx = logits_i.topk(num_proposals_i, dim=1)", "\n", "", "logits_i", ",", "idx", "=", "logits_i", ".", "sort", "(", "descending", "=", "True", ",", "dim", "=", "1", ")", "\n", "topk_scores_i", "=", "logits_i", ".", "narrow", "(", "1", ",", "0", ",", "num_proposals_i", ")", "\n", "topk_idx", "=", "idx", ".", "narrow", "(", "1", ",", "0", ",", "num_proposals_i", ")", "\n", "\n", "# each is N x topk", "\n", "topk_proposals_i", "=", "proposals_i", "[", "batch_idx", "[", ":", ",", "None", "]", ",", "topk_idx", "]", "# N x topk x 4", "\n", "\n", "topk_proposals", ".", "append", "(", "topk_proposals_i", ")", "\n", "topk_scores", ".", "append", "(", "topk_scores_i", ")", "\n", "level_ids", ".", "append", "(", "torch", ".", "full", "(", "(", "num_proposals_i", ",", ")", ",", "level_id", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "device", ")", ")", "\n", "\n", "# 2. Concat all levels together", "\n", "", "topk_scores", "=", "cat", "(", "topk_scores", ",", "dim", "=", "1", ")", "\n", "topk_proposals", "=", "cat", "(", "topk_proposals", ",", "dim", "=", "1", ")", "\n", "level_ids", "=", "cat", "(", "level_ids", ",", "dim", "=", "0", ")", "\n", "\n", "# 3. For each image, run a per-level NMS, and choose topk results.", "\n", "results", ":", "List", "[", "Instances", "]", "=", "[", "]", "\n", "for", "n", ",", "image_size", "in", "enumerate", "(", "image_sizes", ")", ":", "\n", "        ", "boxes", "=", "Boxes", "(", "topk_proposals", "[", "n", "]", ")", "\n", "scores_per_img", "=", "topk_scores", "[", "n", "]", "\n", "lvl", "=", "level_ids", "\n", "\n", "valid_mask", "=", "torch", ".", "isfinite", "(", "boxes", ".", "tensor", ")", ".", "all", "(", "dim", "=", "1", ")", "&", "torch", ".", "isfinite", "(", "scores_per_img", ")", "\n", "if", "not", "valid_mask", ".", "all", "(", ")", ":", "\n", "            ", "if", "training", ":", "\n", "                ", "raise", "FloatingPointError", "(", "\n", "\"Predicted boxes or scores contain Inf/NaN. Training has diverged.\"", "\n", ")", "\n", "", "boxes", "=", "boxes", "[", "valid_mask", "]", "\n", "scores_per_img", "=", "scores_per_img", "[", "valid_mask", "]", "\n", "lvl", "=", "lvl", "[", "valid_mask", "]", "\n", "", "boxes", ".", "clip", "(", "image_size", ")", "\n", "\n", "# filter empty boxes", "\n", "keep", "=", "boxes", ".", "nonempty", "(", "threshold", "=", "min_box_size", ")", "\n", "if", "_is_tracing", "(", ")", "or", "keep", ".", "sum", "(", ")", ".", "item", "(", ")", "!=", "len", "(", "boxes", ")", ":", "\n", "            ", "boxes", ",", "scores_per_img", ",", "lvl", "=", "boxes", "[", "keep", "]", ",", "scores_per_img", "[", "keep", "]", ",", "lvl", "[", "keep", "]", "\n", "\n", "", "keep", "=", "batched_nms", "(", "boxes", ".", "tensor", ",", "scores_per_img", ",", "lvl", ",", "nms_thresh", ")", "\n", "# In Detectron1, there was different behavior during training vs. testing.", "\n", "# (https://github.com/facebookresearch/Detectron/issues/459)", "\n", "# During training, topk is over the proposals from *all* images in the training batch.", "\n", "# During testing, it is over the proposals for each image separately.", "\n", "# As a result, the training behavior becomes batch-dependent,", "\n", "# and the configuration \"POST_NMS_TOPK_TRAIN\" end up relying on the batch size.", "\n", "# This bug is addressed in Detectron2 to make the behavior independent of batch size.", "\n", "keep", "=", "keep", "[", ":", "post_nms_topk", "]", "# keep is already sorted", "\n", "\n", "res", "=", "Instances", "(", "image_size", ")", "\n", "res", ".", "proposal_boxes", "=", "boxes", "[", "keep", "]", "\n", "res", ".", "objectness_logits", "=", "scores_per_img", "[", "keep", "]", "\n", "results", ".", "append", "(", "res", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.add_ground_truth_to_proposals": [[133, 159], ["len", "len", "ValueError", "len", "proposal_utils.add_ground_truth_to_proposals_single_image", "zip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.add_ground_truth_to_proposals_single_image"], ["", "def", "add_ground_truth_to_proposals", "(", "\n", "gt", ":", "Union", "[", "List", "[", "Instances", "]", ",", "List", "[", "Boxes", "]", "]", ",", "proposals", ":", "List", "[", "Instances", "]", "\n", ")", "->", "List", "[", "Instances", "]", ":", "\n", "    ", "\"\"\"\n    Call `add_ground_truth_to_proposals_single_image` for all images.\n\n    Args:\n        gt(Union[List[Instances], List[Boxes]): list of N elements. Element i is a Instances\n            representing the ground-truth for image i.\n        proposals (list[Instances]): list of N elements. Element i is a Instances\n            representing the proposals for image i.\n\n    Returns:\n        list[Instances]: list of N Instances. Each is the proposals for the image,\n            with field \"proposal_boxes\" and \"objectness_logits\".\n    \"\"\"", "\n", "assert", "gt", "is", "not", "None", "\n", "\n", "if", "len", "(", "proposals", ")", "!=", "len", "(", "gt", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"proposals and gt should have the same length as the number of images!\"", ")", "\n", "", "if", "len", "(", "proposals", ")", "==", "0", ":", "\n", "        ", "return", "proposals", "\n", "\n", "", "return", "[", "\n", "add_ground_truth_to_proposals_single_image", "(", "gt_i", ",", "proposals_i", ")", "\n", "for", "gt_i", ",", "proposals_i", "in", "zip", "(", "gt", ",", "proposals", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.add_ground_truth_to_proposals_single_image": [[162, 201], ["isinstance", "math.log", "detectron2.structures.Instances", "proposals.get_fields().keys", "detectron2.structures.Instances.cat", "detectron2.structures.Instances", "torch.ones", "detectron2.structures.Instances.has", "len", "detectron2.structures.Instances.get_fields", "proposals.get_fields"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields"], ["", "def", "add_ground_truth_to_proposals_single_image", "(", "\n", "gt", ":", "Union", "[", "Instances", ",", "Boxes", "]", ",", "proposals", ":", "Instances", "\n", ")", "->", "Instances", ":", "\n", "    ", "\"\"\"\n    Augment `proposals` with `gt`.\n\n    Args:\n        Same as `add_ground_truth_to_proposals`, but with gt and proposals\n        per image.\n\n    Returns:\n        Same as `add_ground_truth_to_proposals`, but for only one image.\n    \"\"\"", "\n", "if", "isinstance", "(", "gt", ",", "Boxes", ")", ":", "\n", "# convert Boxes to Instances", "\n", "        ", "gt", "=", "Instances", "(", "proposals", ".", "image_size", ",", "gt_boxes", "=", "gt", ")", "\n", "\n", "", "gt_boxes", "=", "gt", ".", "gt_boxes", "\n", "device", "=", "proposals", ".", "objectness_logits", ".", "device", "\n", "# Assign all ground-truth boxes an objectness logit corresponding to", "\n", "# P(object) = sigmoid(logit) =~ 1.", "\n", "gt_logit_value", "=", "math", ".", "log", "(", "(", "1.0", "-", "1e-10", ")", "/", "(", "1", "-", "(", "1.0", "-", "1e-10", ")", ")", ")", "\n", "gt_logits", "=", "gt_logit_value", "*", "torch", ".", "ones", "(", "len", "(", "gt_boxes", ")", ",", "device", "=", "device", ")", "\n", "\n", "# Concatenating gt_boxes with proposals requires them to have the same fields", "\n", "gt_proposal", "=", "Instances", "(", "proposals", ".", "image_size", ",", "**", "gt", ".", "get_fields", "(", ")", ")", "\n", "gt_proposal", ".", "proposal_boxes", "=", "gt_boxes", "\n", "gt_proposal", ".", "objectness_logits", "=", "gt_logits", "\n", "\n", "for", "key", "in", "proposals", ".", "get_fields", "(", ")", ".", "keys", "(", ")", ":", "\n", "        ", "assert", "gt_proposal", ".", "has", "(", "\n", "key", "\n", ")", ",", "\"The attribute '{}' in `proposals` does not exist in `gt`\"", ".", "format", "(", "key", ")", "\n", "\n", "# NOTE: Instances.cat only use fields from the first item. Extra fields in latter items", "\n", "# will be thrown away.", "\n", "", "new_proposals", "=", "Instances", ".", "cat", "(", "[", "proposals", ",", "gt_proposal", "]", ")", "\n", "\n", "return", "new_proposals", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.StandardRPNHead.__init__": [[75, 125], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "rpn.StandardRPNHead.modules", "len", "rpn.StandardRPNHead._get_rpn_conv", "torch.nn.Sequential", "torch.nn.Sequential", "enumerate", "isinstance", "rpn.StandardRPNHead._get_rpn_conv", "rpn.StandardRPNHead.conv.add_module", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.constant_", "torch.nn.init.constant_", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.StandardRPNHead._get_rpn_conv", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.StandardRPNHead._get_rpn_conv"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "*", ",", "in_channels", ":", "int", ",", "num_anchors", ":", "int", ",", "box_dim", ":", "int", "=", "4", ",", "conv_dims", ":", "List", "[", "int", "]", "=", "(", "-", "1", ",", ")", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            in_channels (int): number of input feature channels. When using multiple\n                input features, they must have the same number of channels.\n            num_anchors (int): number of anchors to predict for *each spatial position*\n                on the feature map. The total number of anchors for each\n                feature map will be `num_anchors * H * W`.\n            box_dim (int): dimension of a box, which is also the number of box regression\n                predictions to make for each anchor. An axis aligned box has\n                box_dim=4, while a rotated box has box_dim=5.\n            conv_dims (list[int]): a list of integers representing the output channels\n                of N conv layers. Set it to -1 to use the same number of output channels\n                as input channels.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "cur_channels", "=", "in_channels", "\n", "# Keeping the old variable names and structure for backwards compatiblity.", "\n", "# Otherwise the old checkpoints will fail to load.", "\n", "if", "len", "(", "conv_dims", ")", "==", "1", ":", "\n", "            ", "out_channels", "=", "cur_channels", "if", "conv_dims", "[", "0", "]", "==", "-", "1", "else", "conv_dims", "[", "0", "]", "\n", "# 3x3 conv for the hidden representation", "\n", "self", ".", "conv", "=", "self", ".", "_get_rpn_conv", "(", "cur_channels", ",", "out_channels", ")", "\n", "cur_channels", "=", "out_channels", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv", "=", "nn", ".", "Sequential", "(", ")", "\n", "for", "k", ",", "conv_dim", "in", "enumerate", "(", "conv_dims", ")", ":", "\n", "                ", "out_channels", "=", "cur_channels", "if", "conv_dim", "==", "-", "1", "else", "conv_dim", "\n", "if", "out_channels", "<=", "0", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "f\"Conv output channels should be greater than 0. Got {out_channels}\"", "\n", ")", "\n", "", "conv", "=", "self", ".", "_get_rpn_conv", "(", "cur_channels", ",", "out_channels", ")", "\n", "self", ".", "conv", ".", "add_module", "(", "f\"conv{k}\"", ",", "conv", ")", "\n", "cur_channels", "=", "out_channels", "\n", "# 1x1 conv for predicting objectness logits", "\n", "", "", "self", ".", "objectness_logits", "=", "nn", ".", "Conv2d", "(", "cur_channels", ",", "num_anchors", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "# 1x1 conv for predicting box2box transform deltas", "\n", "self", ".", "anchor_deltas", "=", "nn", ".", "Conv2d", "(", "cur_channels", ",", "num_anchors", "*", "box_dim", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ")", "\n", "\n", "# Keeping the order of weights initialization same for backwards compatiblility.", "\n", "for", "layer", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "layer", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "normal_", "(", "layer", ".", "weight", ",", "std", "=", "0.01", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "layer", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.StandardRPNHead._get_rpn_conv": [[126, 134], ["detectron2.layers.Conv2d", "torch.nn.ReLU", "torch.nn.ReLU"], "methods", ["None"], ["", "", "", "def", "_get_rpn_conv", "(", "self", ",", "in_channels", ",", "out_channels", ")", ":", "\n", "        ", "return", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "\n", "activation", "=", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.StandardRPNHead.from_config": [[136, 156], ["anchor_generator.build_anchor_generator.build_anchor_generator", "len", "len", "set", "set"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.build_anchor_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "# Standard RPN is shared across levels:", "\n", "        ", "in_channels", "=", "[", "s", ".", "channels", "for", "s", "in", "input_shape", "]", "\n", "assert", "len", "(", "set", "(", "in_channels", ")", ")", "==", "1", ",", "\"Each level must have the same channel!\"", "\n", "in_channels", "=", "in_channels", "[", "0", "]", "\n", "\n", "# RPNHead should take the same input as anchor generator", "\n", "# NOTE: it assumes that creating an anchor generator does not have unwanted side effect.", "\n", "anchor_generator", "=", "build_anchor_generator", "(", "cfg", ",", "input_shape", ")", "\n", "num_anchors", "=", "anchor_generator", ".", "num_anchors", "\n", "box_dim", "=", "anchor_generator", ".", "box_dim", "\n", "assert", "(", "\n", "len", "(", "set", "(", "num_anchors", ")", ")", "==", "1", "\n", ")", ",", "\"Each level must have the same number of anchors per spatial position\"", "\n", "return", "{", "\n", "\"in_channels\"", ":", "in_channels", ",", "\n", "\"num_anchors\"", ":", "num_anchors", "[", "0", "]", ",", "\n", "\"box_dim\"", ":", "box_dim", ",", "\n", "\"conv_dims\"", ":", "cfg", ".", "MODEL", ".", "RPN", ".", "CONV_DIMS", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.StandardRPNHead.forward": [[158, 178], ["rpn.StandardRPNHead.conv", "pred_objectness_logits.append", "pred_anchor_deltas.append", "rpn.StandardRPNHead.objectness_logits", "rpn.StandardRPNHead.anchor_deltas"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            features (list[Tensor]): list of feature maps\n\n        Returns:\n            list[Tensor]: A list of L elements.\n                Element i is a tensor of shape (N, A, Hi, Wi) representing\n                the predicted objectness logits for all anchors. A is the number of cell anchors.\n            list[Tensor]: A list of L elements. Element i is a tensor of shape\n                (N, A*box_dim, Hi, Wi) representing the predicted \"deltas\" used to transform anchors\n                to proposals.\n        \"\"\"", "\n", "pred_objectness_logits", "=", "[", "]", "\n", "pred_anchor_deltas", "=", "[", "]", "\n", "for", "x", "in", "features", ":", "\n", "            ", "t", "=", "self", ".", "conv", "(", "x", ")", "\n", "pred_objectness_logits", ".", "append", "(", "self", ".", "objectness_logits", "(", "t", ")", ")", "\n", "pred_anchor_deltas", ".", "append", "(", "self", ".", "anchor_deltas", "(", "t", ")", ")", "\n", "", "return", "pred_objectness_logits", ",", "pred_anchor_deltas", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN.__init__": [[186, 257], ["torch.nn.Module.__init__", "float", "isinstance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "in_features", ":", "List", "[", "str", "]", ",", "\n", "head", ":", "nn", ".", "Module", ",", "\n", "anchor_generator", ":", "nn", ".", "Module", ",", "\n", "anchor_matcher", ":", "Matcher", ",", "\n", "box2box_transform", ":", "Box2BoxTransform", ",", "\n", "batch_size_per_image", ":", "int", ",", "\n", "positive_fraction", ":", "float", ",", "\n", "pre_nms_topk", ":", "Tuple", "[", "float", ",", "float", "]", ",", "\n", "post_nms_topk", ":", "Tuple", "[", "float", ",", "float", "]", ",", "\n", "nms_thresh", ":", "float", "=", "0.7", ",", "\n", "min_box_size", ":", "float", "=", "0.0", ",", "\n", "anchor_boundary_thresh", ":", "float", "=", "-", "1.0", ",", "\n", "loss_weight", ":", "Union", "[", "float", ",", "Dict", "[", "str", ",", "float", "]", "]", "=", "1.0", ",", "\n", "box_reg_loss_type", ":", "str", "=", "\"smooth_l1\"", ",", "\n", "smooth_l1_beta", ":", "float", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            in_features (list[str]): list of names of input features to use\n            head (nn.Module): a module that predicts logits and regression deltas\n                for each level from a list of per-level features\n            anchor_generator (nn.Module): a module that creates anchors from a\n                list of features. Usually an instance of :class:`AnchorGenerator`\n            anchor_matcher (Matcher): label the anchors by matching them with ground truth.\n            box2box_transform (Box2BoxTransform): defines the transform from anchors boxes to\n                instance boxes\n            batch_size_per_image (int): number of anchors per image to sample for training\n            positive_fraction (float): fraction of foreground anchors to sample for training\n            pre_nms_topk (tuple[float]): (train, test) that represents the\n                number of top k proposals to select before NMS, in\n                training and testing.\n            post_nms_topk (tuple[float]): (train, test) that represents the\n                number of top k proposals to select after NMS, in\n                training and testing.\n            nms_thresh (float): NMS threshold used to de-duplicate the predicted proposals\n            min_box_size (float): remove proposal boxes with any side smaller than this threshold,\n                in the unit of input image pixels\n            anchor_boundary_thresh (float): legacy option\n            loss_weight (float|dict): weights to use for losses. Can be single float for weighting\n                all rpn losses together, or a dict of individual weightings. Valid dict keys are:\n                    \"loss_rpn_cls\" - applied to classification loss\n                    \"loss_rpn_loc\" - applied to box regression loss\n            box_reg_loss_type (str): Loss type to use. Supported losses: \"smooth_l1\", \"giou\".\n            smooth_l1_beta (float): beta parameter for the smooth L1 regression loss. Default to\n                use L1 loss. Only used when `box_reg_loss_type` is \"smooth_l1\"\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "rpn_head", "=", "head", "\n", "self", ".", "anchor_generator", "=", "anchor_generator", "\n", "self", ".", "anchor_matcher", "=", "anchor_matcher", "\n", "self", ".", "box2box_transform", "=", "box2box_transform", "\n", "self", ".", "batch_size_per_image", "=", "batch_size_per_image", "\n", "self", ".", "positive_fraction", "=", "positive_fraction", "\n", "# Map from self.training state to train/test settings", "\n", "self", ".", "pre_nms_topk", "=", "{", "True", ":", "pre_nms_topk", "[", "0", "]", ",", "False", ":", "pre_nms_topk", "[", "1", "]", "}", "\n", "self", ".", "post_nms_topk", "=", "{", "True", ":", "post_nms_topk", "[", "0", "]", ",", "False", ":", "post_nms_topk", "[", "1", "]", "}", "\n", "self", ".", "nms_thresh", "=", "nms_thresh", "\n", "self", ".", "min_box_size", "=", "float", "(", "min_box_size", ")", "\n", "self", ".", "anchor_boundary_thresh", "=", "anchor_boundary_thresh", "\n", "if", "isinstance", "(", "loss_weight", ",", "float", ")", ":", "\n", "            ", "loss_weight", "=", "{", "\"loss_rpn_cls\"", ":", "loss_weight", ",", "\"loss_rpn_loc\"", ":", "loss_weight", "}", "\n", "", "self", ".", "loss_weight", "=", "loss_weight", "\n", "self", ".", "box_reg_loss_type", "=", "box_reg_loss_type", "\n", "self", ".", "smooth_l1_beta", "=", "smooth_l1_beta", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN.from_config": [[258, 286], ["anchor_generator.build_anchor_generator", "matcher.Matcher", "rpn.build_rpn_head", "box_regression.Box2BoxTransform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.anchor_generator.build_anchor_generator", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.build_rpn_head"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ":", "Dict", "[", "str", ",", "ShapeSpec", "]", ")", ":", "\n", "        ", "in_features", "=", "cfg", ".", "MODEL", ".", "RPN", ".", "IN_FEATURES", "\n", "ret", "=", "{", "\n", "\"in_features\"", ":", "in_features", ",", "\n", "\"min_box_size\"", ":", "cfg", ".", "MODEL", ".", "PROPOSAL_GENERATOR", ".", "MIN_SIZE", ",", "\n", "\"nms_thresh\"", ":", "cfg", ".", "MODEL", ".", "RPN", ".", "NMS_THRESH", ",", "\n", "\"batch_size_per_image\"", ":", "cfg", ".", "MODEL", ".", "RPN", ".", "BATCH_SIZE_PER_IMAGE", ",", "\n", "\"positive_fraction\"", ":", "cfg", ".", "MODEL", ".", "RPN", ".", "POSITIVE_FRACTION", ",", "\n", "\"loss_weight\"", ":", "{", "\n", "\"loss_rpn_cls\"", ":", "cfg", ".", "MODEL", ".", "RPN", ".", "LOSS_WEIGHT", ",", "\n", "\"loss_rpn_loc\"", ":", "cfg", ".", "MODEL", ".", "RPN", ".", "BBOX_REG_LOSS_WEIGHT", "*", "cfg", ".", "MODEL", ".", "RPN", ".", "LOSS_WEIGHT", ",", "\n", "}", ",", "\n", "\"anchor_boundary_thresh\"", ":", "cfg", ".", "MODEL", ".", "RPN", ".", "BOUNDARY_THRESH", ",", "\n", "\"box2box_transform\"", ":", "Box2BoxTransform", "(", "weights", "=", "cfg", ".", "MODEL", ".", "RPN", ".", "BBOX_REG_WEIGHTS", ")", ",", "\n", "\"box_reg_loss_type\"", ":", "cfg", ".", "MODEL", ".", "RPN", ".", "BBOX_REG_LOSS_TYPE", ",", "\n", "\"smooth_l1_beta\"", ":", "cfg", ".", "MODEL", ".", "RPN", ".", "SMOOTH_L1_BETA", ",", "\n", "}", "\n", "\n", "ret", "[", "\"pre_nms_topk\"", "]", "=", "(", "cfg", ".", "MODEL", ".", "RPN", ".", "PRE_NMS_TOPK_TRAIN", ",", "cfg", ".", "MODEL", ".", "RPN", ".", "PRE_NMS_TOPK_TEST", ")", "\n", "ret", "[", "\"post_nms_topk\"", "]", "=", "(", "cfg", ".", "MODEL", ".", "RPN", ".", "POST_NMS_TOPK_TRAIN", ",", "cfg", ".", "MODEL", ".", "RPN", ".", "POST_NMS_TOPK_TEST", ")", "\n", "\n", "ret", "[", "\"anchor_generator\"", "]", "=", "build_anchor_generator", "(", "cfg", ",", "[", "input_shape", "[", "f", "]", "for", "f", "in", "in_features", "]", ")", "\n", "ret", "[", "\"anchor_matcher\"", "]", "=", "Matcher", "(", "\n", "cfg", ".", "MODEL", ".", "RPN", ".", "IOU_THRESHOLDS", ",", "cfg", ".", "MODEL", ".", "RPN", ".", "IOU_LABELS", ",", "allow_low_quality_matches", "=", "True", "\n", ")", "\n", "ret", "[", "\"head\"", "]", "=", "build_rpn_head", "(", "cfg", ",", "[", "input_shape", "[", "f", "]", "for", "f", "in", "in_features", "]", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN._subsample_labels": [[287, 304], ["sampling.subsample_labels", "label.fill_", "label.scatter_", "label.scatter_"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.sampling.subsample_labels"], ["", "def", "_subsample_labels", "(", "self", ",", "label", ")", ":", "\n", "        ", "\"\"\"\n        Randomly sample a subset of positive and negative examples, and overwrite\n        the label vector to the ignore value (-1) for all elements that are not\n        included in the sample.\n\n        Args:\n            labels (Tensor): a vector of -1, 0, 1. Will be modified in-place and returned.\n        \"\"\"", "\n", "pos_idx", ",", "neg_idx", "=", "subsample_labels", "(", "\n", "label", ",", "self", ".", "batch_size_per_image", ",", "self", ".", "positive_fraction", ",", "0", "\n", ")", "\n", "# Fill with the ignore label (-1), then set positive and negative labels", "\n", "label", ".", "fill_", "(", "-", "1", ")", "\n", "label", ".", "scatter_", "(", "0", ",", "pos_idx", ",", "1", ")", "\n", "label", ".", "scatter_", "(", "0", ",", "neg_idx", ",", "0", ")", "\n", "return", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN.label_and_sample_anchors": [[305, 364], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "detectron2.structures.Boxes.cat", "zip", "rpn.RPN.to", "rpn.RPN._subsample_labels", "gt_labels.append", "matched_gt_boxes.append", "detectron2.utils.memory.retry_if_cuda_oom", "detectron2.utils.memory.retry_if_cuda_oom", "detectron2.structures.Boxes.cat.inside_box", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN._subsample_labels", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.memory.retry_if_cuda_oom", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.memory.retry_if_cuda_oom", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.inside_box"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "label_and_sample_anchors", "(", "\n", "self", ",", "anchors", ":", "List", "[", "Boxes", "]", ",", "gt_instances", ":", "List", "[", "Instances", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "torch", ".", "Tensor", "]", ",", "List", "[", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        Args:\n            anchors (list[Boxes]): anchors for each feature map.\n            gt_instances: the ground-truth instances for each image.\n\n        Returns:\n            list[Tensor]:\n                List of #img tensors. i-th element is a vector of labels whose length is\n                the total number of anchors across all feature maps R = sum(Hi * Wi * A).\n                Label values are in {-1, 0, 1}, with meanings: -1 = ignore; 0 = negative\n                class; 1 = positive class.\n            list[Tensor]:\n                i-th element is a Rx4 tensor. The values are the matched gt boxes for each\n                anchor. Values are undefined for those anchors not labeled as 1.\n        \"\"\"", "\n", "anchors", "=", "Boxes", ".", "cat", "(", "anchors", ")", "\n", "\n", "gt_boxes", "=", "[", "x", ".", "gt_boxes", "for", "x", "in", "gt_instances", "]", "\n", "image_sizes", "=", "[", "x", ".", "image_size", "for", "x", "in", "gt_instances", "]", "\n", "del", "gt_instances", "\n", "\n", "gt_labels", "=", "[", "]", "\n", "matched_gt_boxes", "=", "[", "]", "\n", "for", "image_size_i", ",", "gt_boxes_i", "in", "zip", "(", "image_sizes", ",", "gt_boxes", ")", ":", "\n", "            ", "\"\"\"\n            image_size_i: (h, w) for the i-th image\n            gt_boxes_i: ground-truth boxes for i-th image\n            \"\"\"", "\n", "\n", "match_quality_matrix", "=", "retry_if_cuda_oom", "(", "pairwise_iou", ")", "(", "gt_boxes_i", ",", "anchors", ")", "\n", "matched_idxs", ",", "gt_labels_i", "=", "retry_if_cuda_oom", "(", "self", ".", "anchor_matcher", ")", "(", "match_quality_matrix", ")", "\n", "# Matching is memory-expensive and may result in CPU tensors. But the result is small", "\n", "gt_labels_i", "=", "gt_labels_i", ".", "to", "(", "device", "=", "gt_boxes_i", ".", "device", ")", "\n", "del", "match_quality_matrix", "\n", "\n", "if", "self", ".", "anchor_boundary_thresh", ">=", "0", ":", "\n", "# Discard anchors that go out of the boundaries of the image", "\n", "# NOTE: This is legacy functionality that is turned off by default in Detectron2", "\n", "                ", "anchors_inside_image", "=", "anchors", ".", "inside_box", "(", "image_size_i", ",", "self", ".", "anchor_boundary_thresh", ")", "\n", "gt_labels_i", "[", "~", "anchors_inside_image", "]", "=", "-", "1", "\n", "\n", "# A vector of labels (-1, 0, 1) for each anchor", "\n", "", "gt_labels_i", "=", "self", ".", "_subsample_labels", "(", "gt_labels_i", ")", "\n", "\n", "if", "len", "(", "gt_boxes_i", ")", "==", "0", ":", "\n", "# These values won't be used anyway since the anchor is labeled as background", "\n", "                ", "matched_gt_boxes_i", "=", "torch", ".", "zeros_like", "(", "anchors", ".", "tensor", ")", "\n", "", "else", ":", "\n", "# TODO wasted indexing computation for ignored boxes", "\n", "                ", "matched_gt_boxes_i", "=", "gt_boxes_i", "[", "matched_idxs", "]", ".", "tensor", "\n", "\n", "", "gt_labels", ".", "append", "(", "gt_labels_i", ")", "# N,AHW", "\n", "matched_gt_boxes", ".", "append", "(", "matched_gt_boxes_i", ")", "\n", "", "return", "gt_labels", ",", "matched_gt_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN.losses": [[365, 430], ["len", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "pos_mask.sum().item", "detectron2.utils.events.get_event_storage", "detectron2.utils.events.get_event_storage.put_scalar", "detectron2.utils.events.get_event_storage.put_scalar", "box_regression._dense_box_regression_loss", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "gt_labels[].to", "pos_mask.sum", "detectron2.layers.cat", "rpn.RPN.loss_weight.get", "losses.items"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression._dense_box_regression_loss", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "losses", "(", "\n", "self", ",", "\n", "anchors", ":", "List", "[", "Boxes", "]", ",", "\n", "pred_objectness_logits", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "gt_labels", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "pred_anchor_deltas", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "gt_boxes", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Return the losses from a set of RPN predictions and their associated ground-truth.\n\n        Args:\n            anchors (list[Boxes or RotatedBoxes]): anchors for each feature map, each\n                has shape (Hi*Wi*A, B), where B is box dimension (4 or 5).\n            pred_objectness_logits (list[Tensor]): A list of L elements.\n                Element i is a tensor of shape (N, Hi*Wi*A) representing\n                the predicted objectness logits for all anchors.\n            gt_labels (list[Tensor]): Output of :meth:`label_and_sample_anchors`.\n            pred_anchor_deltas (list[Tensor]): A list of L elements. Element i is a tensor of shape\n                (N, Hi*Wi*A, 4 or 5) representing the predicted \"deltas\" used to transform anchors\n                to proposals.\n            gt_boxes (list[Tensor]): Output of :meth:`label_and_sample_anchors`.\n\n        Returns:\n            dict[loss name -> loss value]: A dict mapping from loss name to loss value.\n                Loss names are: `loss_rpn_cls` for objectness classification and\n                `loss_rpn_loc` for proposal localization.\n        \"\"\"", "\n", "num_images", "=", "len", "(", "gt_labels", ")", "\n", "gt_labels", "=", "torch", ".", "stack", "(", "gt_labels", ")", "# (N, sum(Hi*Wi*Ai))", "\n", "\n", "# Log the number of positive/negative anchors per-image that's used in training", "\n", "pos_mask", "=", "gt_labels", "==", "1", "\n", "num_pos_anchors", "=", "pos_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "num_neg_anchors", "=", "(", "gt_labels", "==", "0", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "storage", "=", "get_event_storage", "(", ")", "\n", "storage", ".", "put_scalar", "(", "\"rpn/num_pos_anchors\"", ",", "num_pos_anchors", "/", "num_images", ")", "\n", "storage", ".", "put_scalar", "(", "\"rpn/num_neg_anchors\"", ",", "num_neg_anchors", "/", "num_images", ")", "\n", "\n", "localization_loss", "=", "_dense_box_regression_loss", "(", "\n", "anchors", ",", "\n", "self", ".", "box2box_transform", ",", "\n", "pred_anchor_deltas", ",", "\n", "gt_boxes", ",", "\n", "pos_mask", ",", "\n", "box_reg_loss_type", "=", "self", ".", "box_reg_loss_type", ",", "\n", "smooth_l1_beta", "=", "self", ".", "smooth_l1_beta", ",", "\n", ")", "\n", "\n", "valid_mask", "=", "gt_labels", ">=", "0", "\n", "objectness_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "cat", "(", "pred_objectness_logits", ",", "dim", "=", "1", ")", "[", "valid_mask", "]", ",", "\n", "gt_labels", "[", "valid_mask", "]", ".", "to", "(", "torch", ".", "float32", ")", ",", "\n", "reduction", "=", "\"sum\"", ",", "\n", ")", "\n", "normalizer", "=", "self", ".", "batch_size_per_image", "*", "num_images", "\n", "losses", "=", "{", "\n", "\"loss_rpn_cls\"", ":", "objectness_loss", "/", "normalizer", ",", "\n", "# The original Faster R-CNN paper uses a slightly different normalizer", "\n", "# for loc loss. But it doesn't matter in practice", "\n", "\"loss_rpn_loc\"", ":", "localization_loss", "/", "normalizer", ",", "\n", "}", "\n", "losses", "=", "{", "k", ":", "v", "*", "self", ".", "loss_weight", ".", "get", "(", "k", ",", "1.0", ")", "for", "k", ",", "v", "in", "losses", ".", "items", "(", ")", "}", "\n", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN.forward": [[431, 481], ["rpn.RPN.anchor_generator", "rpn.RPN.rpn_head", "rpn.RPN.predict_proposals", "score.permute().flatten", "x.view().permute().flatten", "rpn.RPN.label_and_sample_anchors", "rpn.RPN.losses", "score.permute", "x.view().permute", "x.view"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rrpn.RRPN.predict_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rrpn.RRPN.label_and_sample_anchors", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "images", ":", "ImageList", ",", "\n", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "gt_instances", ":", "Optional", "[", "List", "[", "Instances", "]", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            images (ImageList): input images of length `N`\n            features (dict[str, Tensor]): input data as a mapping from feature\n                map name to tensor. Axis 0 represents the number of images `N` in\n                the input data; axes 1-3 are channels, height, and width, which may\n                vary between feature maps (e.g., if a feature pyramid is used).\n            gt_instances (list[Instances], optional): a length `N` list of `Instances`s.\n                Each `Instances` stores ground-truth instances for the corresponding image.\n\n        Returns:\n            proposals: list[Instances]: contains fields \"proposal_boxes\", \"objectness_logits\"\n            loss: dict[Tensor] or None\n        \"\"\"", "\n", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "in_features", "]", "\n", "anchors", "=", "self", ".", "anchor_generator", "(", "features", ")", "\n", "\n", "pred_objectness_logits", ",", "pred_anchor_deltas", "=", "self", ".", "rpn_head", "(", "features", ")", "\n", "# Transpose the Hi*Wi*A dimension to the middle:", "\n", "pred_objectness_logits", "=", "[", "\n", "# (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)", "\n", "score", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "flatten", "(", "1", ")", "\n", "for", "score", "in", "pred_objectness_logits", "\n", "]", "\n", "pred_anchor_deltas", "=", "[", "\n", "# (N, A*B, Hi, Wi) -> (N, A, B, Hi, Wi) -> (N, Hi, Wi, A, B) -> (N, Hi*Wi*A, B)", "\n", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "self", ".", "anchor_generator", ".", "box_dim", ",", "x", ".", "shape", "[", "-", "2", "]", ",", "x", ".", "shape", "[", "-", "1", "]", ")", "\n", ".", "permute", "(", "0", ",", "3", ",", "4", ",", "1", ",", "2", ")", "\n", ".", "flatten", "(", "1", ",", "-", "2", ")", "\n", "for", "x", "in", "pred_anchor_deltas", "\n", "]", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "assert", "gt_instances", "is", "not", "None", ",", "\"RPN requires gt_instances in training!\"", "\n", "gt_labels", ",", "gt_boxes", "=", "self", ".", "label_and_sample_anchors", "(", "anchors", ",", "gt_instances", ")", "\n", "losses", "=", "self", ".", "losses", "(", "\n", "anchors", ",", "pred_objectness_logits", ",", "gt_labels", ",", "pred_anchor_deltas", ",", "gt_boxes", "\n", ")", "\n", "", "else", ":", "\n", "            ", "losses", "=", "{", "}", "\n", "", "proposals", "=", "self", ".", "predict_proposals", "(", "\n", "anchors", ",", "pred_objectness_logits", ",", "pred_anchor_deltas", ",", "images", ".", "image_sizes", "\n", ")", "\n", "return", "proposals", ",", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN.predict_proposals": [[482, 512], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "rpn.RPN._decode_proposals", "proposal_utils.find_top_rpn_proposals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN._decode_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.find_top_rpn_proposals"], ["", "def", "predict_proposals", "(", "\n", "self", ",", "\n", "anchors", ":", "List", "[", "Boxes", "]", ",", "\n", "pred_objectness_logits", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "pred_anchor_deltas", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "image_sizes", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Decode all the predicted box regression deltas to proposals. Find the top proposals\n        by applying NMS and removing boxes that are too small.\n\n        Returns:\n            proposals (list[Instances]): list of N Instances. The i-th Instances\n                stores post_nms_topk object proposals for image i, sorted by their\n                objectness score in descending order.\n        \"\"\"", "\n", "# The proposals are treated as fixed for joint training with roi heads.", "\n", "# This approach ignores the derivative w.r.t. the proposal boxes\u2019 coordinates that", "\n", "# are also network responses.", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "pred_proposals", "=", "self", ".", "_decode_proposals", "(", "anchors", ",", "pred_anchor_deltas", ")", "\n", "return", "find_top_rpn_proposals", "(", "\n", "pred_proposals", ",", "\n", "pred_objectness_logits", ",", "\n", "image_sizes", ",", "\n", "self", ".", "nms_thresh", ",", "\n", "self", ".", "pre_nms_topk", "[", "self", ".", "training", "]", ",", "\n", "self", ".", "post_nms_topk", "[", "self", ".", "training", "]", ",", "\n", "self", ".", "min_box_size", ",", "\n", "self", ".", "training", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN._decode_proposals": [[514, 534], ["zip", "anchors_i.tensor.unsqueeze().expand().reshape.tensor.unsqueeze().expand().reshape.tensor.size", "pred_anchor_deltas_i.reshape.reshape.reshape", "anchors_i.tensor.unsqueeze().expand().reshape.tensor.unsqueeze().expand().reshape.tensor.unsqueeze().expand().reshape", "rpn.RPN.box2box_transform.apply_deltas", "proposals.append", "rpn.RPN.view", "anchors_i.tensor.unsqueeze().expand().reshape.tensor.unsqueeze().expand().reshape.tensor.unsqueeze().expand", "anchors_i.tensor.unsqueeze().expand().reshape.tensor.unsqueeze().expand().reshape.tensor.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas"], ["", "", "def", "_decode_proposals", "(", "self", ",", "anchors", ":", "List", "[", "Boxes", "]", ",", "pred_anchor_deltas", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "\"\"\"\n        Transform anchors into proposals by applying the predicted anchor deltas.\n\n        Returns:\n            proposals (list[Tensor]): A list of L tensors. Tensor i has shape\n                (N, Hi*Wi*A, B)\n        \"\"\"", "\n", "N", "=", "pred_anchor_deltas", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "proposals", "=", "[", "]", "\n", "# For each feature map", "\n", "for", "anchors_i", ",", "pred_anchor_deltas_i", "in", "zip", "(", "anchors", ",", "pred_anchor_deltas", ")", ":", "\n", "            ", "B", "=", "anchors_i", ".", "tensor", ".", "size", "(", "1", ")", "\n", "pred_anchor_deltas_i", "=", "pred_anchor_deltas_i", ".", "reshape", "(", "-", "1", ",", "B", ")", "\n", "# Expand anchors to shape (N*Hi*Wi*A, B)", "\n", "anchors_i", "=", "anchors_i", ".", "tensor", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "N", ",", "-", "1", ",", "-", "1", ")", ".", "reshape", "(", "-", "1", ",", "B", ")", "\n", "proposals_i", "=", "self", ".", "box2box_transform", ".", "apply_deltas", "(", "pred_anchor_deltas_i", ",", "anchors_i", ")", "\n", "# Append feature map proposals with shape (N, Hi*Wi*A, B)", "\n", "proposals", ".", "append", "(", "proposals_i", ".", "view", "(", "N", ",", "-", "1", ",", "B", ")", ")", "\n", "", "return", "proposals", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.build_rpn_head": [[58, 64], ["RPN_HEAD_REGISTRY.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "build_rpn_head", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Build an RPN head defined by `cfg.MODEL.RPN.HEAD_NAME`.\n    \"\"\"", "\n", "name", "=", "cfg", ".", "MODEL", ".", "RPN", ".", "HEAD_NAME", "\n", "return", "RPN_HEAD_REGISTRY", ".", "get", "(", "name", ")", "(", "cfg", ",", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rrpn.RRPN.__init__": [[130, 136], ["rpn.RPN.__init__", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "if", "self", ".", "anchor_boundary_thresh", ">=", "0", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"anchor_boundary_thresh is a legacy option not implemented for RRPN.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rrpn.RRPN.from_config": [[138, 143], ["super().from_config", "box_regression.Box2BoxTransformRotated"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config"], ["", "", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ":", "Dict", "[", "str", ",", "ShapeSpec", "]", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ",", "input_shape", ")", "\n", "ret", "[", "\"box2box_transform\"", "]", "=", "Box2BoxTransformRotated", "(", "weights", "=", "cfg", ".", "MODEL", ".", "RPN", ".", "BBOX_REG_WEIGHTS", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rrpn.RRPN.label_and_sample_anchors": [[144, 190], ["torch.no_grad", "detectron2.structures.RotatedBoxes.cat", "rrpn.RRPN.to", "rrpn.RRPN._subsample_labels", "gt_labels.append", "matched_gt_boxes.append", "detectron2.utils.memory.retry_if_cuda_oom", "detectron2.utils.memory.retry_if_cuda_oom", "len", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN._subsample_labels", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.memory.retry_if_cuda_oom", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.memory.retry_if_cuda_oom"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "label_and_sample_anchors", "(", "self", ",", "anchors", ":", "List", "[", "RotatedBoxes", "]", ",", "gt_instances", ":", "List", "[", "Instances", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            anchors (list[RotatedBoxes]): anchors for each feature map.\n            gt_instances: the ground-truth instances for each image.\n\n        Returns:\n            list[Tensor]:\n                List of #img tensors. i-th element is a vector of labels whose length is\n                the total number of anchors across feature maps. Label values are in {-1, 0, 1},\n                with meanings: -1 = ignore; 0 = negative class; 1 = positive class.\n            list[Tensor]:\n                i-th element is a Nx5 tensor, where N is the total number of anchors across\n                feature maps.  The values are the matched gt boxes for each anchor.\n                Values are undefined for those anchors not labeled as 1.\n        \"\"\"", "\n", "anchors", "=", "RotatedBoxes", ".", "cat", "(", "anchors", ")", "\n", "\n", "gt_boxes", "=", "[", "x", ".", "gt_boxes", "for", "x", "in", "gt_instances", "]", "\n", "del", "gt_instances", "\n", "\n", "gt_labels", "=", "[", "]", "\n", "matched_gt_boxes", "=", "[", "]", "\n", "for", "gt_boxes_i", "in", "gt_boxes", ":", "\n", "            ", "\"\"\"\n            gt_boxes_i: ground-truth boxes for i-th image\n            \"\"\"", "\n", "match_quality_matrix", "=", "retry_if_cuda_oom", "(", "pairwise_iou_rotated", ")", "(", "gt_boxes_i", ",", "anchors", ")", "\n", "matched_idxs", ",", "gt_labels_i", "=", "retry_if_cuda_oom", "(", "self", ".", "anchor_matcher", ")", "(", "match_quality_matrix", ")", "\n", "# Matching is memory-expensive and may result in CPU tensors. But the result is small", "\n", "gt_labels_i", "=", "gt_labels_i", ".", "to", "(", "device", "=", "gt_boxes_i", ".", "device", ")", "\n", "\n", "# A vector of labels (-1, 0, 1) for each anchor", "\n", "gt_labels_i", "=", "self", ".", "_subsample_labels", "(", "gt_labels_i", ")", "\n", "\n", "if", "len", "(", "gt_boxes_i", ")", "==", "0", ":", "\n", "# These values won't be used anyway since the anchor is labeled as background", "\n", "                ", "matched_gt_boxes_i", "=", "torch", ".", "zeros_like", "(", "anchors", ".", "tensor", ")", "\n", "", "else", ":", "\n", "# TODO wasted indexing computation for ignored boxes", "\n", "                ", "matched_gt_boxes_i", "=", "gt_boxes_i", "[", "matched_idxs", "]", ".", "tensor", "\n", "\n", "", "gt_labels", ".", "append", "(", "gt_labels_i", ")", "# N,AHW", "\n", "matched_gt_boxes", ".", "append", "(", "matched_gt_boxes_i", ")", "\n", "", "return", "gt_labels", ",", "matched_gt_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rrpn.RRPN.predict_proposals": [[191, 203], ["torch.no_grad", "rrpn.RRPN._decode_proposals", "rrpn.find_top_rrpn_proposals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rpn.RPN._decode_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rrpn.find_top_rrpn_proposals"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "predict_proposals", "(", "self", ",", "anchors", ",", "pred_objectness_logits", ",", "pred_anchor_deltas", ",", "image_sizes", ")", ":", "\n", "        ", "pred_proposals", "=", "self", ".", "_decode_proposals", "(", "anchors", ",", "pred_anchor_deltas", ")", "\n", "return", "find_top_rrpn_proposals", "(", "\n", "pred_proposals", ",", "\n", "pred_objectness_logits", ",", "\n", "image_sizes", ",", "\n", "self", ".", "nms_thresh", ",", "\n", "self", ".", "pre_nms_topk", "[", "self", ".", "training", "]", ",", "\n", "self", ".", "post_nms_topk", "[", "self", ".", "training", "]", ",", "\n", "self", ".", "min_box_size", ",", "\n", "self", ".", "training", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.rrpn.find_top_rrpn_proposals": [[19, 122], ["len", "torch.arange", "zip", "detectron2.layers.cat", "detectron2.layers.cat", "detectron2.layers.cat", "enumerate", "itertools.count", "min", "logits_i.sort", "detectron2.layers.cat.append", "detectron2.layers.cat.append", "detectron2.layers.cat.append", "detectron2.structures.RotatedBoxes", "detectron2.structures.RotatedBoxes.clip", "detectron2.structures.RotatedBoxes.nonempty", "detectron2.layers.batched_nms_rotated", "detectron2.structures.Instances", "results.append", "torch.full", "torch.isfinite().all", "torch.isfinite", "valid_mask.all", "detectron2.layers.batched_nms_rotated.sum().item", "len", "torch.isfinite", "detectron2.layers.batched_nms_rotated.sum"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.nonempty", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms_rotated"], ["def", "find_top_rrpn_proposals", "(", "\n", "proposals", ",", "\n", "pred_objectness_logits", ",", "\n", "image_sizes", ",", "\n", "nms_thresh", ",", "\n", "pre_nms_topk", ",", "\n", "post_nms_topk", ",", "\n", "min_box_size", ",", "\n", "training", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    For each feature map, select the `pre_nms_topk` highest scoring proposals,\n    apply NMS, clip proposals, and remove small boxes. Return the `post_nms_topk`\n    highest scoring proposals among all the feature maps if `training` is True,\n    otherwise, returns the highest `post_nms_topk` scoring proposals for each\n    feature map.\n\n    Args:\n        proposals (list[Tensor]): A list of L tensors. Tensor i has shape (N, Hi*Wi*A, 5).\n            All proposal predictions on the feature maps.\n        pred_objectness_logits (list[Tensor]): A list of L tensors. Tensor i has shape (N, Hi*Wi*A).\n        image_sizes (list[tuple]): sizes (h, w) for each image\n        nms_thresh (float): IoU threshold to use for NMS\n        pre_nms_topk (int): number of top k scoring proposals to keep before applying NMS.\n            When RRPN is run on multiple feature maps (as in FPN) this number is per\n            feature map.\n        post_nms_topk (int): number of top k scoring proposals to keep after applying NMS.\n            When RRPN is run on multiple feature maps (as in FPN) this number is total,\n            over all feature maps.\n        min_box_size(float): minimum proposal box side length in pixels (absolute units wrt\n            input images).\n        training (bool): True if proposals are to be used in training, otherwise False.\n            This arg exists only to support a legacy bug; look for the \"NB: Legacy bug ...\"\n            comment.\n\n    Returns:\n        proposals (list[Instances]): list of N Instances. The i-th Instances\n            stores post_nms_topk object proposals for image i.\n    \"\"\"", "\n", "num_images", "=", "len", "(", "image_sizes", ")", "\n", "device", "=", "proposals", "[", "0", "]", ".", "device", "\n", "\n", "# 1. Select top-k anchor for every level and every image", "\n", "topk_scores", "=", "[", "]", "# #lvl Tensor, each of shape N x topk", "\n", "topk_proposals", "=", "[", "]", "\n", "level_ids", "=", "[", "]", "# #lvl Tensor, each of shape (topk,)", "\n", "batch_idx", "=", "torch", ".", "arange", "(", "num_images", ",", "device", "=", "device", ")", "\n", "for", "level_id", ",", "proposals_i", ",", "logits_i", "in", "zip", "(", "\n", "itertools", ".", "count", "(", ")", ",", "proposals", ",", "pred_objectness_logits", "\n", ")", ":", "\n", "        ", "Hi_Wi_A", "=", "logits_i", ".", "shape", "[", "1", "]", "\n", "num_proposals_i", "=", "min", "(", "pre_nms_topk", ",", "Hi_Wi_A", ")", "\n", "\n", "# sort is faster than topk (https://github.com/pytorch/pytorch/issues/22812)", "\n", "# topk_scores_i, topk_idx = logits_i.topk(num_proposals_i, dim=1)", "\n", "logits_i", ",", "idx", "=", "logits_i", ".", "sort", "(", "descending", "=", "True", ",", "dim", "=", "1", ")", "\n", "topk_scores_i", "=", "logits_i", "[", "batch_idx", ",", ":", "num_proposals_i", "]", "\n", "topk_idx", "=", "idx", "[", "batch_idx", ",", ":", "num_proposals_i", "]", "\n", "\n", "# each is N x topk", "\n", "topk_proposals_i", "=", "proposals_i", "[", "batch_idx", "[", ":", ",", "None", "]", ",", "topk_idx", "]", "# N x topk x 5", "\n", "\n", "topk_proposals", ".", "append", "(", "topk_proposals_i", ")", "\n", "topk_scores", ".", "append", "(", "topk_scores_i", ")", "\n", "level_ids", ".", "append", "(", "torch", ".", "full", "(", "(", "num_proposals_i", ",", ")", ",", "level_id", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "device", ")", ")", "\n", "\n", "# 2. Concat all levels together", "\n", "", "topk_scores", "=", "cat", "(", "topk_scores", ",", "dim", "=", "1", ")", "\n", "topk_proposals", "=", "cat", "(", "topk_proposals", ",", "dim", "=", "1", ")", "\n", "level_ids", "=", "cat", "(", "level_ids", ",", "dim", "=", "0", ")", "\n", "\n", "# 3. For each image, run a per-level NMS, and choose topk results.", "\n", "results", "=", "[", "]", "\n", "for", "n", ",", "image_size", "in", "enumerate", "(", "image_sizes", ")", ":", "\n", "        ", "boxes", "=", "RotatedBoxes", "(", "topk_proposals", "[", "n", "]", ")", "\n", "scores_per_img", "=", "topk_scores", "[", "n", "]", "\n", "valid_mask", "=", "torch", ".", "isfinite", "(", "boxes", ".", "tensor", ")", ".", "all", "(", "dim", "=", "1", ")", "&", "torch", ".", "isfinite", "(", "scores_per_img", ")", "\n", "if", "not", "valid_mask", ".", "all", "(", ")", ":", "\n", "            ", "boxes", "=", "boxes", "[", "valid_mask", "]", "\n", "scores_per_img", "=", "scores_per_img", "[", "valid_mask", "]", "\n", "", "boxes", ".", "clip", "(", "image_size", ")", "\n", "\n", "# filter empty boxes", "\n", "keep", "=", "boxes", ".", "nonempty", "(", "threshold", "=", "min_box_size", ")", "\n", "lvl", "=", "level_ids", "\n", "if", "keep", ".", "sum", "(", ")", ".", "item", "(", ")", "!=", "len", "(", "boxes", ")", ":", "\n", "            ", "boxes", ",", "scores_per_img", ",", "lvl", "=", "(", "boxes", "[", "keep", "]", ",", "scores_per_img", "[", "keep", "]", ",", "level_ids", "[", "keep", "]", ")", "\n", "\n", "", "keep", "=", "batched_nms_rotated", "(", "boxes", ".", "tensor", ",", "scores_per_img", ",", "lvl", ",", "nms_thresh", ")", "\n", "# In Detectron1, there was different behavior during training vs. testing.", "\n", "# (https://github.com/facebookresearch/Detectron/issues/459)", "\n", "# During training, topk is over the proposals from *all* images in the training batch.", "\n", "# During testing, it is over the proposals for each image separately.", "\n", "# As a result, the training behavior becomes batch-dependent,", "\n", "# and the configuration \"POST_NMS_TOPK_TRAIN\" end up relying on the batch size.", "\n", "# This bug is addressed in Detectron2 to make the behavior independent of batch size.", "\n", "keep", "=", "keep", "[", ":", "post_nms_topk", "]", "\n", "\n", "res", "=", "Instances", "(", "image_size", ")", "\n", "res", ".", "proposal_boxes", "=", "boxes", "[", "keep", "]", "\n", "res", ".", "objectness_logits", "=", "scores_per_img", "[", "keep", "]", "\n", "results", ".", "append", "(", "res", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.build.build_proposal_generator": [[15, 25], ["PROPOSAL_GENERATOR_REGISTRY.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["\n", "def", "build_model", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Build the whole model architecture, defined by ``cfg.MODEL.META_ARCHITECTURE``.\n    Note that it does not load any weights from ``cfg``.\n    \"\"\"", "\n", "meta_arch", "=", "cfg", ".", "MODEL", ".", "META_ARCHITECTURE", "\n", "model", "=", "META_ARCH_REGISTRY", ".", "get", "(", "meta_arch", ")", "(", "cfg", ")", "\n", "model", ".", "to", "(", "torch", ".", "device", "(", "cfg", ".", "MODEL", ".", "DEVICE", ")", ")", "\n", "_log_api_usage", "(", "\"modeling.meta_arch.\"", "+", "meta_arch", ")", "\n", "return", "model", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.BaseMaskRCNNHead.__init__": [[160, 172], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "self", ",", "*", ",", "loss_weight", ":", "float", "=", "1.0", ",", "vis_period", ":", "int", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            loss_weight (float): multiplier of the loss\n            vis_period (int): visualization period\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vis_period", "=", "vis_period", "\n", "self", ".", "loss_weight", "=", "loss_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.BaseMaskRCNNHead.from_config": [[173, 176], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "return", "{", "\"vis_period\"", ":", "cfg", ".", "VIS_PERIOD", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.BaseMaskRCNNHead.forward": [[177, 197], ["mask_head.BaseMaskRCNNHead.layers", "mask_head.mask_rcnn_inference", "mask_head.mask_rcnn_loss"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.KRCNNConvDeconvUpsampleHead.layers", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.mask_rcnn_inference", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.mask_rcnn_loss"], ["", "def", "forward", "(", "self", ",", "x", ",", "instances", ":", "List", "[", "Instances", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: input region feature(s) provided by :class:`ROIHeads`.\n            instances (list[Instances]): contains the boxes & labels corresponding\n                to the input features.\n                Exact format is up to its caller to decide.\n                Typically, this is the foreground instances in training, with\n                \"proposal_boxes\" field and other gt annotations.\n                In inference, it contains boxes that are already predicted.\n\n        Returns:\n            A dict of losses in training. The predicted \"instances\" in inference.\n        \"\"\"", "\n", "x", "=", "self", ".", "layers", "(", "x", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "return", "{", "\"loss_mask\"", ":", "mask_rcnn_loss", "(", "x", ",", "instances", ",", "self", ".", "vis_period", ")", "*", "self", ".", "loss_weight", "}", "\n", "", "else", ":", "\n", "            ", "mask_rcnn_inference", "(", "x", ",", "instances", ")", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.BaseMaskRCNNHead.layers": [[198, 203], ["None"], "methods", ["None"], ["", "", "def", "layers", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Neural network layers that makes predictions from input features.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.MaskRCNNConvUpsampleHead.__init__": [[215, 264], ["super().__init__", "enumerate", "detectron2.layers.ConvTranspose2d", "mask_head.MaskRCNNConvUpsampleHead.add_module", "detectron2.layers.Conv2d", "torch.nn.init.normal_", "len", "detectron2.layers.Conv2d", "mask_head.MaskRCNNConvUpsampleHead.add_module", "mask_head.MaskRCNNConvUpsampleHead.conv_norm_relus.append", "torch.nn.ReLU", "fvcore.c2_msra_fill", "torch.nn.init.constant_", "detectron2.layers.get_norm", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["@", "configurable", "\n", "def", "__init__", "(", "self", ",", "input_shape", ":", "ShapeSpec", ",", "*", ",", "num_classes", ",", "conv_dims", ",", "conv_norm", "=", "\"\"", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            input_shape (ShapeSpec): shape of the input feature\n            num_classes (int): the number of foreground classes (i.e. background is not\n                included). 1 if using class agnostic prediction.\n            conv_dims (list[int]): a list of N>0 integers representing the output dimensions\n                of N-1 conv layers and the last upsample layer.\n            conv_norm (str or callable): normalization for the conv layers.\n                See :func:`detectron2.layers.get_norm` for supported types.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "assert", "len", "(", "conv_dims", ")", ">=", "1", ",", "\"conv_dims have to be non-empty!\"", "\n", "\n", "self", ".", "conv_norm_relus", "=", "[", "]", "\n", "\n", "cur_channels", "=", "input_shape", ".", "channels", "\n", "for", "k", ",", "conv_dim", "in", "enumerate", "(", "conv_dims", "[", ":", "-", "1", "]", ")", ":", "\n", "            ", "conv", "=", "Conv2d", "(", "\n", "cur_channels", ",", "\n", "conv_dim", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "not", "conv_norm", ",", "\n", "norm", "=", "get_norm", "(", "conv_norm", ",", "conv_dim", ")", ",", "\n", "activation", "=", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"mask_fcn{}\"", ".", "format", "(", "k", "+", "1", ")", ",", "conv", ")", "\n", "self", ".", "conv_norm_relus", ".", "append", "(", "conv", ")", "\n", "cur_channels", "=", "conv_dim", "\n", "\n", "", "self", ".", "deconv", "=", "ConvTranspose2d", "(", "\n", "cur_channels", ",", "conv_dims", "[", "-", "1", "]", ",", "kernel_size", "=", "2", ",", "stride", "=", "2", ",", "padding", "=", "0", "\n", ")", "\n", "self", ".", "add_module", "(", "\"deconv_relu\"", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "cur_channels", "=", "conv_dims", "[", "-", "1", "]", "\n", "\n", "self", ".", "predictor", "=", "Conv2d", "(", "cur_channels", ",", "num_classes", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "\n", "for", "layer", "in", "self", ".", "conv_norm_relus", "+", "[", "self", ".", "deconv", "]", ":", "\n", "            ", "weight_init", ".", "c2_msra_fill", "(", "layer", ")", "\n", "# use normal distribution initialization for mask prediction layer", "\n", "", "nn", ".", "init", ".", "normal_", "(", "self", ".", "predictor", ".", "weight", ",", "std", "=", "0.001", ")", "\n", "if", "self", ".", "predictor", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "predictor", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.MaskRCNNConvUpsampleHead.from_config": [[265, 280], ["super().from_config", "super().from_config.update"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config"], ["", "", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ",", "input_shape", ")", "\n", "conv_dim", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "CONV_DIM", "\n", "num_conv", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "NUM_CONV", "\n", "ret", ".", "update", "(", "\n", "conv_dims", "=", "[", "conv_dim", "]", "*", "(", "num_conv", "+", "1", ")", ",", "# +1 for ConvTranspose", "\n", "conv_norm", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "NORM", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", ")", "\n", "if", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "CLS_AGNOSTIC_MASK", ":", "\n", "            ", "ret", "[", "\"num_classes\"", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "ret", "[", "\"num_classes\"", "]", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NUM_CLASSES", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.MaskRCNNConvUpsampleHead.layers": [[281, 285], ["layer"], "methods", ["None"], ["", "def", "layers", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "layer", "in", "self", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.mask_rcnn_loss": [[31, 112], ["pred_mask_logits.size", "pred_mask_logits.size", "detectron2.layers.cat", "gt_masks.to.to", "gt_masks_bool.sum().item", "detectron2.utils.events.get_event_storage", "detectron2.utils.events.get_event_storage.put_scalar", "detectron2.utils.events.get_event_storage.put_scalar", "detectron2.utils.events.get_event_storage.put_scalar", "torch.nn.functional.binary_cross_entropy_with_logits", "pred_mask_logits.size", "pred_mask_logits.size", "pred_mask_logits.size", "instances_per_image.gt_masks.crop_and_resize().to", "gt_masks.to.append", "len", "torch.arange", "detectron2.layers.cat", "max", "max", "pred_mask_logits.sigmoid", "torch.cat", "enumerate", "len", "instances_per_image.gt_classes.to", "detectron2.layers.cat.append", "pred_mask_logits.sum", "mask_incorrect.sum().item", "max", "gt_masks_bool.sum", "torch.stack", "detectron2.utils.events.get_event_storage.put_image", "instances_per_image.gt_masks.crop_and_resize", "mask_incorrect.numel", "gt_masks_bool.numel", "mask_incorrect.sum"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_image", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.crop_and_resize"], ["@", "torch", ".", "jit", ".", "unused", "\n", "def", "mask_rcnn_loss", "(", "pred_mask_logits", ":", "torch", ".", "Tensor", ",", "instances", ":", "List", "[", "Instances", "]", ",", "vis_period", ":", "int", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Compute the mask prediction loss defined in the Mask R-CNN paper.\n\n    Args:\n        pred_mask_logits (Tensor): A tensor of shape (B, C, Hmask, Wmask) or (B, 1, Hmask, Wmask)\n            for class-specific or class-agnostic, where B is the total number of predicted masks\n            in all images, C is the number of foreground classes, and Hmask, Wmask are the height\n            and width of the mask predictions. The values are logits.\n        instances (list[Instances]): A list of N Instances, where N is the number of images\n            in the batch. These instances are in 1:1\n            correspondence with the pred_mask_logits. The ground-truth labels (class, box, mask,\n            ...) associated with each instance are stored in fields.\n        vis_period (int): the period (in steps) to dump visualization.\n\n    Returns:\n        mask_loss (Tensor): A scalar tensor containing the loss.\n    \"\"\"", "\n", "cls_agnostic_mask", "=", "pred_mask_logits", ".", "size", "(", "1", ")", "==", "1", "\n", "total_num_masks", "=", "pred_mask_logits", ".", "size", "(", "0", ")", "\n", "mask_side_len", "=", "pred_mask_logits", ".", "size", "(", "2", ")", "\n", "assert", "pred_mask_logits", ".", "size", "(", "2", ")", "==", "pred_mask_logits", ".", "size", "(", "3", ")", ",", "\"Mask prediction must be square!\"", "\n", "\n", "gt_classes", "=", "[", "]", "\n", "gt_masks", "=", "[", "]", "\n", "for", "instances_per_image", "in", "instances", ":", "\n", "        ", "if", "len", "(", "instances_per_image", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "if", "not", "cls_agnostic_mask", ":", "\n", "            ", "gt_classes_per_image", "=", "instances_per_image", ".", "gt_classes", ".", "to", "(", "dtype", "=", "torch", ".", "int64", ")", "\n", "gt_classes", ".", "append", "(", "gt_classes_per_image", ")", "\n", "\n", "", "gt_masks_per_image", "=", "instances_per_image", ".", "gt_masks", ".", "crop_and_resize", "(", "\n", "instances_per_image", ".", "proposal_boxes", ".", "tensor", ",", "mask_side_len", "\n", ")", ".", "to", "(", "device", "=", "pred_mask_logits", ".", "device", ")", "\n", "# A tensor of shape (N, M, M), N=#instances in the image; M=mask_side_len", "\n", "gt_masks", ".", "append", "(", "gt_masks_per_image", ")", "\n", "\n", "", "if", "len", "(", "gt_masks", ")", "==", "0", ":", "\n", "        ", "return", "pred_mask_logits", ".", "sum", "(", ")", "*", "0", "\n", "\n", "", "gt_masks", "=", "cat", "(", "gt_masks", ",", "dim", "=", "0", ")", "\n", "\n", "if", "cls_agnostic_mask", ":", "\n", "        ", "pred_mask_logits", "=", "pred_mask_logits", "[", ":", ",", "0", "]", "\n", "", "else", ":", "\n", "        ", "indices", "=", "torch", ".", "arange", "(", "total_num_masks", ")", "\n", "gt_classes", "=", "cat", "(", "gt_classes", ",", "dim", "=", "0", ")", "\n", "pred_mask_logits", "=", "pred_mask_logits", "[", "indices", ",", "gt_classes", "]", "\n", "\n", "", "if", "gt_masks", ".", "dtype", "==", "torch", ".", "bool", ":", "\n", "        ", "gt_masks_bool", "=", "gt_masks", "\n", "", "else", ":", "\n", "# Here we allow gt_masks to be float as well (depend on the implementation of rasterize())", "\n", "        ", "gt_masks_bool", "=", "gt_masks", ">", "0.5", "\n", "", "gt_masks", "=", "gt_masks", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "# Log the training accuracy (using gt classes and 0.5 threshold)", "\n", "mask_incorrect", "=", "(", "pred_mask_logits", ">", "0.0", ")", "!=", "gt_masks_bool", "\n", "mask_accuracy", "=", "1", "-", "(", "mask_incorrect", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "max", "(", "mask_incorrect", ".", "numel", "(", ")", ",", "1.0", ")", ")", "\n", "num_positive", "=", "gt_masks_bool", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "false_positive", "=", "(", "mask_incorrect", "&", "~", "gt_masks_bool", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "max", "(", "\n", "gt_masks_bool", ".", "numel", "(", ")", "-", "num_positive", ",", "1.0", "\n", ")", "\n", "false_negative", "=", "(", "mask_incorrect", "&", "gt_masks_bool", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "max", "(", "num_positive", ",", "1.0", ")", "\n", "\n", "storage", "=", "get_event_storage", "(", ")", "\n", "storage", ".", "put_scalar", "(", "\"mask_rcnn/accuracy\"", ",", "mask_accuracy", ")", "\n", "storage", ".", "put_scalar", "(", "\"mask_rcnn/false_positive\"", ",", "false_positive", ")", "\n", "storage", ".", "put_scalar", "(", "\"mask_rcnn/false_negative\"", ",", "false_negative", ")", "\n", "if", "vis_period", ">", "0", "and", "storage", ".", "iter", "%", "vis_period", "==", "0", ":", "\n", "        ", "pred_masks", "=", "pred_mask_logits", ".", "sigmoid", "(", ")", "\n", "vis_masks", "=", "torch", ".", "cat", "(", "[", "pred_masks", ",", "gt_masks", "]", ",", "axis", "=", "2", ")", "\n", "name", "=", "\"Left: mask prediction;   Right: mask GT\"", "\n", "for", "idx", ",", "vis_mask", "in", "enumerate", "(", "vis_masks", ")", ":", "\n", "            ", "vis_mask", "=", "torch", ".", "stack", "(", "[", "vis_mask", "]", "*", "3", ",", "axis", "=", "0", ")", "\n", "storage", ".", "put_image", "(", "name", "+", "f\" ({idx})\"", ",", "vis_mask", ")", "\n", "\n", "", "", "mask_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "pred_mask_logits", ",", "gt_masks", ",", "reduction", "=", "\"mean\"", ")", "\n", "return", "mask_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.mask_rcnn_inference": [[114, 153], ["[].sigmoid.split", "zip", "pred_mask_logits.size", "pred_mask_logits.sigmoid", "detectron2.layers.cat", "torch.arange", "[].sigmoid", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "mask_rcnn_inference", "(", "pred_mask_logits", ":", "torch", ".", "Tensor", ",", "pred_instances", ":", "List", "[", "Instances", "]", ")", ":", "\n", "    ", "\"\"\"\n    Convert pred_mask_logits to estimated foreground probability masks while also\n    extracting only the masks for the predicted classes in pred_instances. For each\n    predicted box, the mask of the same class is attached to the instance by adding a\n    new \"pred_masks\" field to pred_instances.\n\n    Args:\n        pred_mask_logits (Tensor): A tensor of shape (B, C, Hmask, Wmask) or (B, 1, Hmask, Wmask)\n            for class-specific or class-agnostic, where B is the total number of predicted masks\n            in all images, C is the number of foreground classes, and Hmask, Wmask are the height\n            and width of the mask predictions. The values are logits.\n        pred_instances (list[Instances]): A list of N Instances, where N is the number of images\n            in the batch. Each Instances must have field \"pred_classes\".\n\n    Returns:\n        None. pred_instances will contain an extra \"pred_masks\" field storing a mask of size (Hmask,\n            Wmask) for predicted class. Note that the masks are returned as a soft (non-quantized)\n            masks the resolution predicted by the network; post-processing steps, such as resizing\n            the predicted masks to the original image resolution and/or binarizing them, is left\n            to the caller.\n    \"\"\"", "\n", "cls_agnostic_mask", "=", "pred_mask_logits", ".", "size", "(", "1", ")", "==", "1", "\n", "\n", "if", "cls_agnostic_mask", ":", "\n", "        ", "mask_probs_pred", "=", "pred_mask_logits", ".", "sigmoid", "(", ")", "\n", "", "else", ":", "\n", "# Select masks corresponding to the predicted classes", "\n", "        ", "num_masks", "=", "pred_mask_logits", ".", "shape", "[", "0", "]", "\n", "class_pred", "=", "cat", "(", "[", "i", ".", "pred_classes", "for", "i", "in", "pred_instances", "]", ")", "\n", "indices", "=", "torch", ".", "arange", "(", "num_masks", ",", "device", "=", "class_pred", ".", "device", ")", "\n", "mask_probs_pred", "=", "pred_mask_logits", "[", "indices", ",", "class_pred", "]", "[", ":", ",", "None", "]", ".", "sigmoid", "(", ")", "\n", "# mask_probs_pred.shape: (B, 1, Hmask, Wmask)", "\n", "\n", "", "num_boxes_per_image", "=", "[", "len", "(", "i", ")", "for", "i", "in", "pred_instances", "]", "\n", "mask_probs_pred", "=", "mask_probs_pred", ".", "split", "(", "num_boxes_per_image", ",", "dim", "=", "0", ")", "\n", "\n", "for", "prob", ",", "instances", "in", "zip", "(", "mask_probs_pred", ",", "pred_instances", ")", ":", "\n", "        ", "instances", ".", "pred_masks", "=", "prob", "# (1, Hmask, Wmask)", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.build_mask_head": [[287, 293], ["ROI_MASK_HEAD_REGISTRY.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "build_mask_head", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Build a mask head defined by `cfg.MODEL.ROI_MASK_HEAD.NAME`.\n    \"\"\"", "\n", "name", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "NAME", "\n", "return", "ROI_MASK_HEAD_REGISTRY", ".", "get", "(", "name", ")", "(", "cfg", ",", "input_shape", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputs.__init__": [[219, 284], ["len", "len", "type", "type.cat", "proposals[].has", "detectron2.structures.Boxes", "len", "detectron2.layers.cat", "type.cat", "torch.zeros", "p.has"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has"], ["def", "__init__", "(", "\n", "self", ",", "\n", "box2box_transform", ",", "\n", "pred_class_logits", ",", "\n", "pred_proposal_deltas", ",", "\n", "proposals", ",", "\n", "smooth_l1_beta", "=", "0.0", ",", "\n", "box_reg_loss_type", "=", "\"smooth_l1\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            box2box_transform (Box2BoxTransform/Box2BoxTransformRotated):\n                box2box transform instance for proposal-to-detection transformations.\n            pred_class_logits (Tensor): A tensor of shape (R, K + 1) storing the predicted class\n                logits for all R predicted object instances.\n                Each row corresponds to a predicted object instance.\n            pred_proposal_deltas (Tensor): A tensor of shape (R, K * B) or (R, B) for\n                class-specific or class-agnostic regression. It stores the predicted deltas that\n                transform proposals into final box detections.\n                B is the box dimension (4 or 5).\n                When B is 4, each row is [dx, dy, dw, dh (, ....)].\n                When B is 5, each row is [dx, dy, dw, dh, da (, ....)].\n            proposals (list[Instances]): A list of N Instances, where Instances i stores the\n                proposals for image i, in the field \"proposal_boxes\".\n                When training, each Instances must have ground-truth labels\n                stored in the field \"gt_classes\" and \"gt_boxes\".\n                The total number of all instances must be equal to R.\n            smooth_l1_beta (float): The transition point between L1 and L2 loss in\n                the smooth L1 loss function. When set to 0, the loss becomes L1. When\n                set to +inf, the loss becomes constant 0.\n            box_reg_loss_type (str): Box regression loss type. One of: \"smooth_l1\", \"giou\"\n        \"\"\"", "\n", "self", ".", "box2box_transform", "=", "box2box_transform", "\n", "self", ".", "num_preds_per_image", "=", "[", "len", "(", "p", ")", "for", "p", "in", "proposals", "]", "\n", "self", ".", "pred_class_logits", "=", "pred_class_logits", "\n", "self", ".", "pred_proposal_deltas", "=", "pred_proposal_deltas", "\n", "self", ".", "smooth_l1_beta", "=", "smooth_l1_beta", "\n", "self", ".", "box_reg_loss_type", "=", "box_reg_loss_type", "\n", "\n", "self", ".", "image_shapes", "=", "[", "x", ".", "image_size", "for", "x", "in", "proposals", "]", "\n", "\n", "if", "len", "(", "proposals", ")", ":", "\n", "            ", "box_type", "=", "type", "(", "proposals", "[", "0", "]", ".", "proposal_boxes", ")", "\n", "# cat(..., dim=0) concatenates over all images in the batch", "\n", "self", ".", "proposals", "=", "box_type", ".", "cat", "(", "[", "p", ".", "proposal_boxes", "for", "p", "in", "proposals", "]", ")", "\n", "assert", "(", "\n", "not", "self", ".", "proposals", ".", "tensor", ".", "requires_grad", "\n", ")", ",", "\"Proposals should not require gradients!\"", "\n", "\n", "# \"gt_classes\" exists if and only if training. But other gt fields may", "\n", "# not necessarily exist in training for images that have no groundtruth.", "\n", "if", "proposals", "[", "0", "]", ".", "has", "(", "\"gt_classes\"", ")", ":", "\n", "                ", "self", ".", "gt_classes", "=", "cat", "(", "[", "p", ".", "gt_classes", "for", "p", "in", "proposals", "]", ",", "dim", "=", "0", ")", "\n", "\n", "# If \"gt_boxes\" does not exist, the proposals must be all negative and", "\n", "# should not be included in regression loss computation.", "\n", "# Here we just use proposal_boxes as an arbitrary placeholder because its", "\n", "# value won't be used in self.box_reg_loss().", "\n", "gt_boxes", "=", "[", "\n", "p", ".", "gt_boxes", "if", "p", ".", "has", "(", "\"gt_boxes\"", ")", "else", "p", ".", "proposal_boxes", "for", "p", "in", "proposals", "\n", "]", "\n", "self", ".", "gt_boxes", "=", "box_type", ".", "cat", "(", "gt_boxes", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "proposals", "=", "Boxes", "(", "torch", ".", "zeros", "(", "0", ",", "4", ",", "device", "=", "self", ".", "pred_proposal_deltas", ".", "device", ")", ")", "\n", "", "self", ".", "_no_instances", "=", "len", "(", "self", ".", "proposals", ")", "==", "0", "# no instances found", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputs.softmax_cross_entropy_loss": [[285, 291], ["fast_rcnn._log_classification_stats", "detectron2.layers.cross_entropy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn._log_classification_stats", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy"], ["", "def", "softmax_cross_entropy_loss", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Deprecated\n        \"\"\"", "\n", "_log_classification_stats", "(", "self", ".", "pred_class_logits", ",", "self", ".", "gt_classes", ")", "\n", "return", "cross_entropy", "(", "self", ".", "pred_class_logits", ",", "self", ".", "gt_classes", ",", "reduction", "=", "\"mean\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputs.box_reg_loss": [[292, 347], ["fast_rcnn.FastRCNNOutputs.proposals.tensor.size", "fast_rcnn.FastRCNNOutputs.pred_proposal_deltas.size", "detectron2.layers.nonzero_tuple", "torch.arange", "fast_rcnn.FastRCNNOutputs.box2box_transform.get_deltas", "fvcore.nn.smooth_l1_loss", "fast_rcnn.FastRCNNOutputs.gt_classes.numel", "fast_rcnn.FastRCNNOutputs.pred_proposal_deltas.sum", "torch.arange", "fast_rcnn.FastRCNNOutputs.box2box_transform.apply_deltas", "fvcore.nn.giou_loss", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.get_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas"], ["", "def", "box_reg_loss", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Deprecated\n        \"\"\"", "\n", "if", "self", ".", "_no_instances", ":", "\n", "            ", "return", "0.0", "*", "self", ".", "pred_proposal_deltas", ".", "sum", "(", ")", "\n", "\n", "", "box_dim", "=", "self", ".", "proposals", ".", "tensor", ".", "size", "(", "1", ")", "# 4 or 5", "\n", "cls_agnostic_bbox_reg", "=", "self", ".", "pred_proposal_deltas", ".", "size", "(", "1", ")", "==", "box_dim", "\n", "device", "=", "self", ".", "pred_proposal_deltas", ".", "device", "\n", "\n", "bg_class_ind", "=", "self", ".", "pred_class_logits", ".", "shape", "[", "1", "]", "-", "1", "\n", "# Box delta loss is only computed between the prediction for the gt class k", "\n", "# (if 0 <= k < bg_class_ind) and the target; there is no loss defined on predictions", "\n", "# for non-gt classes and background.", "\n", "# Empty fg_inds should produce a valid loss of zero because reduction=sum.", "\n", "fg_inds", "=", "nonzero_tuple", "(", "(", "self", ".", "gt_classes", ">=", "0", ")", "&", "(", "self", ".", "gt_classes", "<", "bg_class_ind", ")", ")", "[", "0", "]", "\n", "\n", "if", "cls_agnostic_bbox_reg", ":", "\n", "# pred_proposal_deltas only corresponds to foreground class for agnostic", "\n", "            ", "gt_class_cols", "=", "torch", ".", "arange", "(", "box_dim", ",", "device", "=", "device", ")", "\n", "", "else", ":", "\n", "# pred_proposal_deltas for class k are located in columns [b * k : b * k + b],", "\n", "# where b is the dimension of box representation (4 or 5)", "\n", "# Note that compared to Detectron1,", "\n", "# we do not perform bounding box regression for background classes.", "\n", "            ", "gt_class_cols", "=", "box_dim", "*", "self", ".", "gt_classes", "[", "fg_inds", ",", "None", "]", "+", "torch", ".", "arange", "(", "\n", "box_dim", ",", "device", "=", "device", "\n", ")", "\n", "\n", "", "if", "self", ".", "box_reg_loss_type", "==", "\"smooth_l1\"", ":", "\n", "            ", "gt_proposal_deltas", "=", "self", ".", "box2box_transform", ".", "get_deltas", "(", "\n", "self", ".", "proposals", ".", "tensor", ",", "self", ".", "gt_boxes", ".", "tensor", "\n", ")", "\n", "loss_box_reg", "=", "smooth_l1_loss", "(", "\n", "self", ".", "pred_proposal_deltas", "[", "fg_inds", "[", ":", ",", "None", "]", ",", "gt_class_cols", "]", ",", "\n", "gt_proposal_deltas", "[", "fg_inds", "]", ",", "\n", "self", ".", "smooth_l1_beta", ",", "\n", "reduction", "=", "\"sum\"", ",", "\n", ")", "\n", "", "elif", "self", ".", "box_reg_loss_type", "==", "\"giou\"", ":", "\n", "            ", "fg_pred_boxes", "=", "self", ".", "box2box_transform", ".", "apply_deltas", "(", "\n", "self", ".", "pred_proposal_deltas", "[", "fg_inds", "[", ":", ",", "None", "]", ",", "gt_class_cols", "]", ",", "\n", "self", ".", "proposals", ".", "tensor", "[", "fg_inds", "]", ",", "\n", ")", "\n", "loss_box_reg", "=", "giou_loss", "(", "\n", "fg_pred_boxes", ",", "\n", "self", ".", "gt_boxes", ".", "tensor", "[", "fg_inds", "]", ",", "\n", "reduction", "=", "\"sum\"", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid bbox reg loss type '{self.box_reg_loss_type}'\"", ")", "\n", "\n", "", "loss_box_reg", "=", "loss_box_reg", "/", "self", ".", "gt_classes", ".", "numel", "(", ")", "\n", "return", "loss_box_reg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputs.losses": [[348, 353], ["fast_rcnn.FastRCNNOutputs.softmax_cross_entropy_loss", "fast_rcnn.FastRCNNOutputs.box_reg_loss"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputs.softmax_cross_entropy_loss", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.box_reg_loss"], ["", "def", "losses", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Deprecated\n        \"\"\"", "\n", "return", "{", "\"loss_cls\"", ":", "self", ".", "softmax_cross_entropy_loss", "(", ")", ",", "\"loss_box_reg\"", ":", "self", ".", "box_reg_loss", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputs.predict_boxes": [[354, 360], ["fast_rcnn.FastRCNNOutputs.box2box_transform.apply_deltas", "fast_rcnn.FastRCNNOutputs.split"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas"], ["", "def", "predict_boxes", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Deprecated\n        \"\"\"", "\n", "pred", "=", "self", ".", "box2box_transform", ".", "apply_deltas", "(", "self", ".", "pred_proposal_deltas", ",", "self", ".", "proposals", ".", "tensor", ")", "\n", "return", "pred", ".", "split", "(", "self", ".", "num_preds_per_image", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputs.predict_probs": [[361, 367], ["torch.nn.functional.softmax", "torch.nn.functional.softmax.split"], "methods", ["None"], ["", "def", "predict_probs", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Deprecated\n        \"\"\"", "\n", "probs", "=", "F", ".", "softmax", "(", "self", ".", "pred_class_logits", ",", "dim", "=", "-", "1", ")", "\n", "return", "probs", ".", "split", "(", "self", ".", "num_preds_per_image", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.__init__": [[377, 498], ["torch.nn.Module.__init__", "isinstance", "isinstance", "len", "torch.nn.Linear", "torch.nn.init.normal_", "torch.nn.init.constant_", "detectron2.layers.ShapeSpec", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.init.normal_", "torch.nn.init.constant_", "torch.ones", "torch.no_grad", "torch.load", "fast_rcnn.FastRCNNOutputLayers.cls_score.weight.copy_", "torch.no_grad", "torch.nn.init.constant_", "torch.load", "torch.load.size", "torch.nn.Linear", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.no_grad", "fast_rcnn.FastRCNNOutputLayers.test_cls_score.weight.copy_", "torch.nn.init.constant_"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "input_shape", ":", "ShapeSpec", ",", "\n", "*", ",", "\n", "box2box_transform", ",", "\n", "num_classes", ":", "int", ",", "\n", "test_score_thresh", ":", "float", "=", "0.0", ",", "\n", "test_nms_thresh", ":", "float", "=", "0.5", ",", "\n", "soft_nms_enabled", "=", "False", ",", "\n", "soft_nms_method", "=", "\"gaussian\"", ",", "\n", "soft_nms_sigma", "=", "0.5", ",", "\n", "soft_nms_prune", "=", "0.001", ",", "\n", "test_topk_per_image", ":", "int", "=", "100", ",", "\n", "cls_agnostic_bbox_reg", ":", "bool", "=", "False", ",", "\n", "smooth_l1_beta", ":", "float", "=", "0.0", ",", "\n", "box_reg_loss_type", ":", "str", "=", "\"smooth_l1\"", ",", "\n", "loss_weight", ":", "Union", "[", "float", ",", "Dict", "[", "str", ",", "float", "]", "]", "=", "1.0", ",", "\n", "clip_cls_emb", ":", "tuple", "=", "(", "False", ",", "None", ")", ",", "\n", "no_box_delta", ":", "bool", "=", "False", ",", "\n", "bg_cls_loss_weight", ":", "None", ",", "\n", "multiply_rpn_score", ":", "tuple", "=", "(", "False", ",", "False", ")", ",", "\n", "openset_test", ":", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            input_shape (ShapeSpec): shape of the input feature to this module\n            box2box_transform (Box2BoxTransform or Box2BoxTransformRotated):\n            num_classes (int): number of foreground classes\n            test_score_thresh (float): threshold to filter predictions results.\n            test_nms_thresh (float): NMS threshold for prediction results.\n            test_topk_per_image (int): number of top predictions to produce per image.\n            cls_agnostic_bbox_reg (bool): whether to use class agnostic for bbox regression\n            smooth_l1_beta (float): transition point from L1 to L2 loss. Only used if\n                `box_reg_loss_type` is \"smooth_l1\"\n            box_reg_loss_type (str): Box regression loss type. One of: \"smooth_l1\", \"giou\"\n            loss_weight (float|dict): weights to use for losses. Can be single float for weighting\n                all losses, or a dict of individual weightings. Valid dict keys are:\n                    * \"loss_cls\": applied to classification loss\n                    * \"loss_box_reg\": applied to box regression loss\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "box2box_transform", "=", "box2box_transform", "\n", "self", ".", "smooth_l1_beta", "=", "smooth_l1_beta", "\n", "self", ".", "test_score_thresh", "=", "test_score_thresh", "\n", "self", ".", "test_nms_thresh", "=", "test_nms_thresh", "\n", "self", ".", "soft_nms_enabled", "=", "soft_nms_enabled", "\n", "self", ".", "soft_nms_method", "=", "soft_nms_method", "\n", "self", ".", "soft_nms_sigma", "=", "soft_nms_sigma", "\n", "self", ".", "soft_nms_prune", "=", "soft_nms_prune", "\n", "self", ".", "test_topk_per_image", "=", "test_topk_per_image", "\n", "self", ".", "box_reg_loss_type", "=", "box_reg_loss_type", "\n", "if", "isinstance", "(", "loss_weight", ",", "float", ")", ":", "\n", "            ", "loss_weight", "=", "{", "\"loss_cls\"", ":", "loss_weight", ",", "\"loss_box_reg\"", ":", "loss_weight", "}", "\n", "", "self", ".", "loss_weight", "=", "loss_weight", "\n", "\n", "# RegionCLIP", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "if", "isinstance", "(", "input_shape", ",", "int", ")", ":", "# some backward compatibility", "\n", "            ", "input_shape", "=", "ShapeSpec", "(", "channels", "=", "input_shape", ")", "\n", "", "input_size", "=", "input_shape", ".", "channels", "*", "(", "input_shape", ".", "width", "or", "1", ")", "*", "(", "input_shape", ".", "height", "or", "1", ")", "\n", "\n", "self", ".", "use_clip_cls_emb", "=", "clip_cls_emb", "[", "0", "]", "\n", "if", "self", ".", "use_clip_cls_emb", ":", "# use CLIP text embeddings as classifier's weights", "\n", "            ", "input_size", "=", "clip_cls_emb", "[", "3", "]", "if", "clip_cls_emb", "[", "2", "]", "in", "[", "'CLIPRes5ROIHeads'", ",", "'CLIPStandardROIHeads'", "]", "else", "input_size", "\n", "text_emb_require_grad", "=", "False", "\n", "self", ".", "use_bias", "=", "False", "\n", "self", ".", "temperature", "=", "openset_test", "[", "2", "]", "# 0.01 is default for CLIP", "\n", "\n", "# class embedding", "\n", "self", ".", "cls_score", "=", "nn", ".", "Linear", "(", "input_size", ",", "num_classes", ",", "bias", "=", "self", ".", "use_bias", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "pre_computed_w", "=", "torch", ".", "load", "(", "clip_cls_emb", "[", "1", "]", ")", "# [num_classes, 1024] for RN50", "\n", "self", ".", "cls_score", ".", "weight", ".", "copy_", "(", "pre_computed_w", ")", "\n", "self", ".", "cls_score", ".", "weight", ".", "requires_grad", "=", "text_emb_require_grad", "# freeze embeddings", "\n", "if", "self", ".", "use_bias", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "cls_score", ".", "bias", ",", "0", ")", "\n", "\n", "# background embedding", "\n", "", "", "self", ".", "cls_bg_score", "=", "nn", ".", "Linear", "(", "input_size", ",", "1", ",", "bias", "=", "self", ".", "use_bias", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "cls_bg_score", ".", "weight", ",", "0", ")", "# zero embeddings", "\n", "self", ".", "cls_bg_score", ".", "weight", ".", "requires_grad", "=", "text_emb_require_grad", "\n", "if", "self", ".", "use_bias", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "cls_bg_score", ".", "bias", ",", "0", ")", "\n", "\n", "# class embedding during test ", "\n", "", "", "self", ".", "test_cls_score", "=", "None", "\n", "if", "openset_test", "[", "1", "]", "is", "not", "None", ":", "# openset test enabled", "\n", "                ", "pre_computed_w", "=", "torch", ".", "load", "(", "openset_test", "[", "1", "]", ")", "# [#openset_test_num_cls, 1024] for RN50", "\n", "self", ".", "openset_test_num_cls", "=", "pre_computed_w", ".", "size", "(", "0", ")", "\n", "self", ".", "test_cls_score", "=", "nn", ".", "Linear", "(", "input_size", ",", "self", ".", "openset_test_num_cls", ",", "bias", "=", "self", ".", "use_bias", ")", "\n", "self", ".", "test_cls_score", ".", "weight", ".", "requires_grad", "=", "False", "# freeze embeddings", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "self", ".", "test_cls_score", ".", "weight", ".", "copy_", "(", "pre_computed_w", ")", "\n", "if", "self", ".", "use_bias", ":", "\n", "                        ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "test_cls_score", ".", "bias", ",", "0", ")", "\n", "", "", "", "", "else", ":", "# regular classification layer  ", "\n", "            ", "self", ".", "cls_score", "=", "nn", ".", "Linear", "(", "input_size", ",", "num_classes", "+", "1", ")", "# one background class (hence + 1)", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "cls_score", ".", "weight", ",", "std", "=", "0.01", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "cls_score", ".", "bias", ",", "0", ")", "\n", "\n", "# box regression layer", "\n", "", "num_bbox_reg_classes", "=", "1", "if", "cls_agnostic_bbox_reg", "else", "num_classes", "\n", "box_dim", "=", "len", "(", "box2box_transform", ".", "weights", ")", "\n", "self", ".", "bbox_pred", "=", "nn", ".", "Linear", "(", "input_size", ",", "num_bbox_reg_classes", "*", "box_dim", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "bbox_pred", ".", "weight", ",", "std", "=", "0.001", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bbox_pred", ".", "bias", ",", "0", ")", "\n", "\n", "# training options", "\n", "self", ".", "cls_loss_weight", "=", "None", "\n", "if", "bg_cls_loss_weight", "is", "not", "None", ":", "# loss weigh for bg class", "\n", "            ", "self", ".", "cls_loss_weight", "=", "torch", ".", "ones", "(", "num_classes", "+", "1", ")", "\n", "self", ".", "cls_loss_weight", "[", "-", "1", "]", "=", "bg_cls_loss_weight", "\n", "", "self", ".", "focal_scaled_loss", "=", "openset_test", "[", "3", "]", "# focal scaling", "\n", "# inference options", "\n", "self", ".", "no_box_delta", "=", "no_box_delta", "# box delta after regression", "\n", "self", ".", "multiply_rpn_score", "=", "multiply_rpn_score", "[", "0", "]", "\n", "self", ".", "vis", "=", "multiply_rpn_score", "[", "1", "]", "# if enabled, visualize scores before multiplying RPN scores", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.from_config": [[499, 526], ["detectron2.modeling.box_regression.Box2BoxTransform"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "# if cfg.MODEL.CLIP.CROP_REGION_TYPE == \"RPN\":", "\n", "#     assert cfg.MODEL.CLIP.NO_BOX_DELTA is False", "\n", "        ", "return", "{", "\n", "\"input_shape\"", ":", "input_shape", ",", "\n", "\"box2box_transform\"", ":", "Box2BoxTransform", "(", "weights", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_WEIGHTS", ")", ",", "\n", "# fmt: off", "\n", "\"num_classes\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NUM_CLASSES", ",", "\n", "\"cls_agnostic_bbox_reg\"", ":", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "CLS_AGNOSTIC_BBOX_REG", ",", "\n", "\"smooth_l1_beta\"", ":", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "SMOOTH_L1_BETA", ",", "\n", "\"test_score_thresh\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "SCORE_THRESH_TEST", ",", "\n", "\"test_nms_thresh\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NMS_THRESH_TEST", ",", "\n", "\"soft_nms_enabled\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "SOFT_NMS_ENABLED", ",", "\n", "\"soft_nms_method\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "SOFT_NMS_METHOD", ",", "\n", "\"soft_nms_sigma\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "SOFT_NMS_SIGMA", ",", "\n", "\"soft_nms_prune\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "SOFT_NMS_PRUNE", ",", "\n", "\"test_topk_per_image\"", ":", "cfg", ".", "TEST", ".", "DETECTIONS_PER_IMAGE", ",", "\n", "\"box_reg_loss_type\"", ":", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_LOSS_TYPE", ",", "\n", "\"loss_weight\"", ":", "{", "\"loss_box_reg\"", ":", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_LOSS_WEIGHT", "}", ",", "\n", "# RegionCLIP", "\n", "\"clip_cls_emb\"", ":", "(", "cfg", ".", "MODEL", ".", "CLIP", ".", "USE_TEXT_EMB_CLASSIFIER", ",", "cfg", ".", "MODEL", ".", "CLIP", ".", "TEXT_EMB_PATH", ",", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NAME", ",", "cfg", ".", "MODEL", ".", "CLIP", ".", "TEXT_EMB_DIM", ")", ",", "\n", "\"no_box_delta\"", ":", "cfg", ".", "MODEL", ".", "CLIP", ".", "NO_BOX_DELTA", "or", "cfg", ".", "MODEL", ".", "CLIP", ".", "CROP_REGION_TYPE", "==", "'GT'", ",", "\n", "\"bg_cls_loss_weight\"", ":", "cfg", ".", "MODEL", ".", "CLIP", ".", "BG_CLS_LOSS_WEIGHT", ",", "\n", "\"multiply_rpn_score\"", ":", "(", "cfg", ".", "MODEL", ".", "CLIP", ".", "MULTIPLY_RPN_SCORE", ",", "cfg", ".", "MODEL", ".", "CLIP", ".", "VIS", ")", ",", "\n", "\"openset_test\"", ":", "(", "cfg", ".", "MODEL", ".", "CLIP", ".", "OPENSET_TEST_NUM_CLASSES", ",", "cfg", ".", "MODEL", ".", "CLIP", ".", "OPENSET_TEST_TEXT_EMB_PATH", ",", "cfg", ".", "MODEL", ".", "CLIP", ".", "CLSS_TEMP", ",", "cfg", ".", "MODEL", ".", "CLIP", ".", "FOCAL_SCALED_LOSS", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.forward": [[529, 573], ["fast_rcnn.FastRCNNOutputLayers.bbox_pred", "torch.flatten.dim", "torch.flatten", "torch.nn.functional.normalize", "fast_rcnn.FastRCNNOutputLayers.cls_bg_score", "torch.cat", "fast_rcnn.FastRCNNOutputLayers.cls_score", "torch.nn.functional.normalize().t", "torch.nn.functional.normalize().t", "torch.nn.functional.normalize", "torch.nn.functional.normalize"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.normalize", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.normalize", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.normalize"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: per-region features of shape (N, ...) for N bounding boxes to predict.\n\n        Returns:\n            (Tensor, Tensor):\n            First tensor: shape (N,K+1), scores for each of the N box. Each row contains the\n            scores for K object categories and 1 background class.\n\n            Second tensor: bounding box regression deltas for each box. Shape is shape (N,Kx4),\n            or (N,4) for class-agnostic regression.\n        \"\"\"", "\n", "if", "x", ".", "dim", "(", ")", ">", "2", ":", "\n", "            ", "x", "=", "torch", ".", "flatten", "(", "x", ",", "start_dim", "=", "1", ")", "\n", "\n", "# use clip text embeddings as classifier's weights", "\n", "", "if", "self", ".", "use_clip_cls_emb", ":", "\n", "            ", "normalized_x", "=", "F", ".", "normalize", "(", "x", ",", "p", "=", "2.0", ",", "dim", "=", "1", ")", "\n", "# open-set inference enabled", "\n", "if", "not", "self", ".", "training", "and", "self", ".", "test_cls_score", "is", "not", "None", ":", "\n", "                ", "cls_scores", "=", "normalized_x", "@", "F", ".", "normalize", "(", "self", ".", "test_cls_score", ".", "weight", ",", "p", "=", "2.0", ",", "dim", "=", "1", ")", ".", "t", "(", ")", "\n", "if", "self", ".", "use_bias", ":", "\n", "                    ", "cls_scores", "+=", "self", ".", "test_cls_score", ".", "bias", "\n", "# training or closed-set model inference", "\n", "", "", "else", ":", "\n", "                ", "cls_scores", "=", "normalized_x", "@", "F", ".", "normalize", "(", "self", ".", "cls_score", ".", "weight", ",", "p", "=", "2.0", ",", "dim", "=", "1", ")", ".", "t", "(", ")", "\n", "if", "self", ".", "use_bias", ":", "\n", "                    ", "cls_scores", "+=", "self", ".", "cls_score", ".", "bias", "\n", "\n", "# background class (zero embeddings)", "\n", "", "", "bg_score", "=", "self", ".", "cls_bg_score", "(", "normalized_x", ")", "\n", "if", "self", ".", "use_bias", ":", "\n", "                ", "bg_score", "+=", "self", ".", "cls_bg_score", ".", "bias", "\n", "\n", "", "scores", "=", "torch", ".", "cat", "(", "(", "cls_scores", ",", "bg_score", ")", ",", "dim", "=", "1", ")", "\n", "scores", "=", "scores", "/", "self", ".", "temperature", "\n", "# regular classifier", "\n", "", "else", ":", "\n", "            ", "scores", "=", "self", ".", "cls_score", "(", "x", ")", "\n", "\n", "# box regression", "\n", "", "proposal_deltas", "=", "self", ".", "bbox_pred", "(", "x", ")", "\n", "return", "scores", ",", "proposal_deltas", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses": [[574, 623], ["fast_rcnn._log_classification_stats", "len", "len", "detectron2.layers.cat", "torch.empty", "detectron2.layers.cat", "detectron2.layers.cat", "torch.empty", "fast_rcnn.FastRCNNOutputLayers.cls_loss_weight.to", "fast_rcnn.FastRCNNOutputLayers.focal_loss", "fast_rcnn.FastRCNNOutputLayers.box_reg_loss", "detectron2.layers.cross_entropy", "detectron2.layers.cross_entropy", "fast_rcnn.FastRCNNOutputLayers.loss_weight.get", "losses.items", "p.has"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn._log_classification_stats", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.focal_loss", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.box_reg_loss", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has"], ["", "def", "losses", "(", "self", ",", "predictions", ",", "proposals", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            predictions: return values of :meth:`forward()`.\n            proposals (list[Instances]): proposals that match the features that were used\n                to compute predictions. The fields ``proposal_boxes``, ``gt_boxes``,\n                ``gt_classes`` are expected.\n\n        Returns:\n            Dict[str, Tensor]: dict of losses\n        \"\"\"", "\n", "scores", ",", "proposal_deltas", "=", "predictions", "\n", "\n", "# parse classification outputs", "\n", "gt_classes", "=", "(", "\n", "cat", "(", "[", "p", ".", "gt_classes", "for", "p", "in", "proposals", "]", ",", "dim", "=", "0", ")", "if", "len", "(", "proposals", ")", "else", "torch", ".", "empty", "(", "0", ")", "\n", ")", "\n", "_log_classification_stats", "(", "scores", ",", "gt_classes", ")", "\n", "\n", "# parse box regression outputs", "\n", "if", "len", "(", "proposals", ")", ":", "\n", "            ", "proposal_boxes", "=", "cat", "(", "[", "p", ".", "proposal_boxes", ".", "tensor", "for", "p", "in", "proposals", "]", ",", "dim", "=", "0", ")", "# Nx4", "\n", "assert", "not", "proposal_boxes", ".", "requires_grad", ",", "\"Proposals should not require gradients!\"", "\n", "# If \"gt_boxes\" does not exist, the proposals must be all negative and", "\n", "# should not be included in regression loss computation.", "\n", "# Here we just use proposal_boxes as an arbitrary placeholder because its", "\n", "# value won't be used in self.box_reg_loss().", "\n", "gt_boxes", "=", "cat", "(", "\n", "[", "(", "p", ".", "gt_boxes", "if", "p", ".", "has", "(", "\"gt_boxes\"", ")", "else", "p", ".", "proposal_boxes", ")", ".", "tensor", "for", "p", "in", "proposals", "]", ",", "\n", "dim", "=", "0", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "proposal_boxes", "=", "gt_boxes", "=", "torch", ".", "empty", "(", "(", "0", ",", "4", ")", ",", "device", "=", "proposal_deltas", ".", "device", ")", "\n", "\n", "# loss weights", "\n", "", "if", "self", ".", "cls_loss_weight", "is", "not", "None", "and", "self", ".", "cls_loss_weight", ".", "device", "!=", "scores", ".", "device", ":", "\n", "            ", "self", ".", "cls_loss_weight", "=", "self", ".", "cls_loss_weight", ".", "to", "(", "scores", ".", "device", ")", "\n", "", "if", "self", ".", "focal_scaled_loss", "is", "not", "None", ":", "\n", "            ", "loss_cls", "=", "self", ".", "focal_loss", "(", "scores", ",", "gt_classes", ",", "gamma", "=", "self", ".", "focal_scaled_loss", ")", "\n", "", "else", ":", "\n", "            ", "loss_cls", "=", "cross_entropy", "(", "scores", ",", "gt_classes", ",", "reduction", "=", "\"mean\"", ")", "if", "self", ".", "cls_loss_weight", "is", "None", "else", "cross_entropy", "(", "scores", ",", "gt_classes", ",", "reduction", "=", "\"mean\"", ",", "weight", "=", "self", ".", "cls_loss_weight", ")", "\n", "", "losses", "=", "{", "\n", "\"loss_cls\"", ":", "loss_cls", ",", "\n", "\"loss_box_reg\"", ":", "self", ".", "box_reg_loss", "(", "\n", "proposal_boxes", ",", "gt_boxes", ",", "proposal_deltas", ",", "gt_classes", "\n", ")", ",", "\n", "}", "\n", "return", "{", "k", ":", "v", "*", "self", ".", "loss_weight", ".", "get", "(", "k", ",", "1.0", ")", "for", "k", ",", "v", "in", "losses", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.focal_loss": [[624, 645], ["torch.nn.functional.cross_entropy", "torch.nn.functional.softmax", "torch.ones().to", "fast_rcnn.FastRCNNOutputLayers.cls_loss_weight[].item", "loss.mean.mean.mean", "targets.numel", "input.sum", "torch.arange().to", "torch.ones", "loss.mean.mean.size", "torch.arange", "torch.nn.functional.softmax.size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "focal_loss", "(", "self", ",", "inputs", ",", "targets", ",", "gamma", "=", "0.5", ",", "reduction", "=", "\"mean\"", ")", ":", "\n", "        ", "\"\"\"Inspired by RetinaNet implementation\"\"\"", "\n", "if", "targets", ".", "numel", "(", ")", "==", "0", "and", "reduction", "==", "\"mean\"", ":", "\n", "            ", "return", "input", ".", "sum", "(", ")", "*", "0.0", "# connect the gradient", "\n", "\n", "# focal scaling", "\n", "", "ce_loss", "=", "F", ".", "cross_entropy", "(", "inputs", ",", "targets", ",", "reduction", "=", "\"none\"", ")", "\n", "p", "=", "F", ".", "softmax", "(", "inputs", ",", "dim", "=", "-", "1", ")", "\n", "p_t", "=", "p", "[", "torch", ".", "arange", "(", "p", ".", "size", "(", "0", ")", ")", ".", "to", "(", "p", ".", "device", ")", ",", "targets", "]", "# get prob of target class", "\n", "loss", "=", "ce_loss", "*", "(", "(", "1", "-", "p_t", ")", "**", "gamma", ")", "\n", "\n", "# bg loss weight", "\n", "if", "self", ".", "cls_loss_weight", "is", "not", "None", ":", "\n", "            ", "loss_weight", "=", "torch", ".", "ones", "(", "loss", ".", "size", "(", "0", ")", ")", ".", "to", "(", "p", ".", "device", ")", "\n", "loss_weight", "[", "targets", "==", "self", ".", "num_classes", "]", "=", "self", ".", "cls_loss_weight", "[", "-", "1", "]", ".", "item", "(", ")", "\n", "loss", "=", "loss", "*", "loss_weight", "\n", "\n", "", "if", "reduction", "==", "\"mean\"", ":", "\n", "            ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.box_reg_loss": [[646, 690], ["detectron2.layers.nonzero_tuple", "fast_rcnn.FastRCNNOutputLayers.box2box_transform.get_deltas", "fvcore.nn.smooth_l1_loss", "max", "pred_deltas.view", "fast_rcnn.FastRCNNOutputLayers.box2box_transform.apply_deltas", "fvcore.nn.giou_loss", "ValueError", "gt_classes.numel"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.get_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas"], ["", "def", "box_reg_loss", "(", "self", ",", "proposal_boxes", ",", "gt_boxes", ",", "pred_deltas", ",", "gt_classes", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            All boxes are tensors with the same shape Rx(4 or 5).\n            gt_classes is a long tensor of shape R, the gt class label of each proposal.\n            R shall be the number of proposals.\n        \"\"\"", "\n", "box_dim", "=", "proposal_boxes", ".", "shape", "[", "1", "]", "# 4 or 5", "\n", "# Regression loss is only computed for foreground proposals (those matched to a GT)", "\n", "fg_inds", "=", "nonzero_tuple", "(", "(", "gt_classes", ">=", "0", ")", "&", "(", "gt_classes", "<", "self", ".", "num_classes", ")", ")", "[", "0", "]", "\n", "if", "pred_deltas", ".", "shape", "[", "1", "]", "==", "box_dim", ":", "# cls-agnostic regression", "\n", "            ", "fg_pred_deltas", "=", "pred_deltas", "[", "fg_inds", "]", "\n", "", "else", ":", "\n", "            ", "fg_pred_deltas", "=", "pred_deltas", ".", "view", "(", "-", "1", ",", "self", ".", "num_classes", ",", "box_dim", ")", "[", "\n", "fg_inds", ",", "gt_classes", "[", "fg_inds", "]", "\n", "]", "\n", "\n", "", "if", "self", ".", "box_reg_loss_type", "==", "\"smooth_l1\"", ":", "\n", "            ", "gt_pred_deltas", "=", "self", ".", "box2box_transform", ".", "get_deltas", "(", "\n", "proposal_boxes", "[", "fg_inds", "]", ",", "\n", "gt_boxes", "[", "fg_inds", "]", ",", "\n", ")", "\n", "loss_box_reg", "=", "smooth_l1_loss", "(", "\n", "fg_pred_deltas", ",", "gt_pred_deltas", ",", "self", ".", "smooth_l1_beta", ",", "reduction", "=", "\"sum\"", "\n", ")", "\n", "", "elif", "self", ".", "box_reg_loss_type", "==", "\"giou\"", ":", "\n", "            ", "fg_pred_boxes", "=", "self", ".", "box2box_transform", ".", "apply_deltas", "(", "\n", "fg_pred_deltas", ",", "proposal_boxes", "[", "fg_inds", "]", "\n", ")", "\n", "loss_box_reg", "=", "giou_loss", "(", "fg_pred_boxes", ",", "gt_boxes", "[", "fg_inds", "]", ",", "reduction", "=", "\"sum\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid bbox reg loss type '{self.box_reg_loss_type}'\"", ")", "\n", "# The reg loss is normalized using the total number of regions (R), not the number", "\n", "# of foreground regions even though the box regression loss is only defined on", "\n", "# foreground regions. Why? Because doing so gives equal training influence to", "\n", "# each foreground example. To see how, consider two different minibatches:", "\n", "#  (1) Contains a single foreground region", "\n", "#  (2) Contains 100 foreground regions", "\n", "# If we normalize by the number of foreground regions, the single example in", "\n", "# minibatch (1) will be given 100 times as much influence as each foreground", "\n", "# example in minibatch (2). Normalizing by the total number of regions, R,", "\n", "# means that the single example in minibatch (1) and each of the 100 examples", "\n", "# in minibatch (2) are given equal influence.", "\n", "", "return", "loss_box_reg", "/", "max", "(", "gt_classes", ".", "numel", "(", ")", ",", "1.0", ")", "# return 0 if empty", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.inference": [[691, 724], ["fast_rcnn.FastRCNNOutputLayers.predict_boxes", "fast_rcnn.FastRCNNOutputLayers.predict_probs", "fast_rcnn.fast_rcnn_inference", "p.get", "zip"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_probs", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.fast_rcnn_inference", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "inference", "(", "self", ",", "predictions", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ",", "proposals", ":", "List", "[", "Instances", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            predictions: return values of :meth:`forward()`.\n            proposals (list[Instances]): proposals that match the features that were\n                used to compute predictions. The ``proposal_boxes`` field is expected.\n\n        Returns:\n            list[Instances]: same as `fast_rcnn_inference`.\n            list[Tensor]: same as `fast_rcnn_inference`.\n        \"\"\"", "\n", "boxes", "=", "self", ".", "predict_boxes", "(", "predictions", ",", "proposals", ")", "\n", "scores", "=", "self", ".", "predict_probs", "(", "predictions", ",", "proposals", ")", "\n", "image_shapes", "=", "[", "x", ".", "image_size", "for", "x", "in", "proposals", "]", "\n", "\n", "# optional: multiply class scores with RPN scores ", "\n", "scores_bf_multiply", "=", "scores", "# as a backup for visualization purpose", "\n", "if", "self", ".", "multiply_rpn_score", "and", "not", "self", ".", "training", ":", "\n", "            ", "rpn_scores", "=", "[", "p", ".", "get", "(", "'objectness_logits'", ")", "for", "p", "in", "proposals", "]", "\n", "scores", "=", "[", "(", "s", "*", "rpn_s", "[", ":", ",", "None", "]", ")", "**", "0.5", "for", "s", ",", "rpn_s", "in", "zip", "(", "scores", ",", "rpn_scores", ")", "]", "\n", "", "return", "fast_rcnn_inference", "(", "\n", "boxes", ",", "\n", "scores", ",", "\n", "image_shapes", ",", "\n", "self", ".", "test_score_thresh", ",", "\n", "self", ".", "test_nms_thresh", ",", "\n", "self", ".", "soft_nms_enabled", ",", "\n", "self", ".", "soft_nms_method", ",", "\n", "self", ".", "soft_nms_sigma", ",", "\n", "self", ".", "soft_nms_prune", ",", "\n", "self", ".", "test_topk_per_image", ",", "\n", "scores_bf_multiply", "=", "scores_bf_multiply", ",", "\n", "vis", "=", "True", "if", "self", ".", "vis", "else", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_boxes_for_gt_classes": [[726, 760], ["detectron2.layers.cat", "fast_rcnn.FastRCNNOutputLayers.box2box_transform.apply_deltas", "fast_rcnn.FastRCNNOutputLayers.split", "len", "torch.cat", "gt_classes.clamp_.clamp_.clamp_", "len", "fast_rcnn.FastRCNNOutputLayers.view", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "predict_boxes_for_gt_classes", "(", "self", ",", "predictions", ",", "proposals", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            predictions: return values of :meth:`forward()`.\n            proposals (list[Instances]): proposals that match the features that were used\n                to compute predictions. The fields ``proposal_boxes``, ``gt_classes`` are expected.\n\n        Returns:\n            list[Tensor]:\n                A list of Tensors of predicted boxes for GT classes in case of\n                class-specific box head. Element i of the list has shape (Ri, B), where Ri is\n                the number of proposals for image i and B is the box dimension (4 or 5)\n        \"\"\"", "\n", "if", "not", "len", "(", "proposals", ")", ":", "\n", "            ", "return", "[", "]", "\n", "", "scores", ",", "proposal_deltas", "=", "predictions", "\n", "proposal_boxes", "=", "cat", "(", "[", "p", ".", "proposal_boxes", ".", "tensor", "for", "p", "in", "proposals", "]", ",", "dim", "=", "0", ")", "\n", "N", ",", "B", "=", "proposal_boxes", ".", "shape", "\n", "predict_boxes", "=", "self", ".", "box2box_transform", ".", "apply_deltas", "(", "\n", "proposal_deltas", ",", "proposal_boxes", "\n", ")", "# Nx(KxB)", "\n", "\n", "K", "=", "predict_boxes", ".", "shape", "[", "1", "]", "//", "B", "\n", "if", "K", ">", "1", ":", "\n", "            ", "gt_classes", "=", "torch", ".", "cat", "(", "[", "p", ".", "gt_classes", "for", "p", "in", "proposals", "]", ",", "dim", "=", "0", ")", "\n", "# Some proposals are ignored or have a background class. Their gt_classes", "\n", "# cannot be used as index.", "\n", "gt_classes", "=", "gt_classes", ".", "clamp_", "(", "0", ",", "K", "-", "1", ")", "\n", "\n", "predict_boxes", "=", "predict_boxes", ".", "view", "(", "N", ",", "K", ",", "B", ")", "[", "\n", "torch", ".", "arange", "(", "N", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "predict_boxes", ".", "device", ")", ",", "gt_classes", "\n", "]", "\n", "", "num_prop_per_image", "=", "[", "len", "(", "p", ")", "for", "p", "in", "proposals", "]", "\n", "return", "predict_boxes", ".", "split", "(", "num_prop_per_image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_boxes": [[761, 792], ["detectron2.layers.cat", "fast_rcnn.FastRCNNOutputLayers.split", "len", "len", "fast_rcnn.FastRCNNOutputLayers.box2box_transform.apply_deltas"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.box_regression.Box2BoxTransformRotated.apply_deltas"], ["", "def", "predict_boxes", "(", "\n", "self", ",", "predictions", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ",", "proposals", ":", "List", "[", "Instances", "]", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            predictions: return values of :meth:`forward()`.\n            proposals (list[Instances]): proposals that match the features that were\n                used to compute predictions. The ``proposal_boxes`` field is expected.\n\n        Returns:\n            list[Tensor]:\n                A list of Tensors of predicted class-specific or class-agnostic boxes\n                for each image. Element i has shape (Ri, K * B) or (Ri, B), where Ri is\n                the number of proposals for image i and B is the box dimension (4 or 5)\n        \"\"\"", "\n", "if", "not", "len", "(", "proposals", ")", ":", "\n", "            ", "return", "[", "]", "\n", "", "_", ",", "proposal_deltas", "=", "predictions", "\n", "num_prop_per_image", "=", "[", "len", "(", "p", ")", "for", "p", "in", "proposals", "]", "\n", "proposal_boxes", "=", "cat", "(", "[", "p", ".", "proposal_boxes", ".", "tensor", "for", "p", "in", "proposals", "]", ",", "dim", "=", "0", ")", "\n", "\n", "# don't apply box delta, such as GT boxes", "\n", "if", "self", ".", "no_box_delta", ":", "\n", "            ", "predict_boxes", "=", "proposal_boxes", "\n", "# apply box delta", "\n", "", "else", ":", "\n", "            ", "predict_boxes", "=", "self", ".", "box2box_transform", ".", "apply_deltas", "(", "\n", "proposal_deltas", ",", "\n", "proposal_boxes", ",", "\n", ")", "# Nx(KxB)", "\n", "", "return", "predict_boxes", ".", "split", "(", "num_prop_per_image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_probs": [[793, 811], ["torch.nn.functional.softmax", "torch.nn.functional.softmax.split", "len"], "methods", ["None"], ["", "def", "predict_probs", "(", "\n", "self", ",", "predictions", ":", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ",", "proposals", ":", "List", "[", "Instances", "]", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            predictions: return values of :meth:`forward()`.\n            proposals (list[Instances]): proposals that match the features that were\n                used to compute predictions.\n\n        Returns:\n            list[Tensor]:\n                A list of Tensors of predicted class probabilities for each image.\n                Element i has shape (Ri, K + 1), where Ri is the number of proposals for image i.\n        \"\"\"", "\n", "scores", ",", "_", "=", "predictions", "\n", "num_inst_per_image", "=", "[", "len", "(", "p", ")", "for", "p", "in", "proposals", "]", "\n", "probs", "=", "F", ".", "softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "\n", "return", "probs", ".", "split", "(", "num_inst_per_image", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.fast_rcnn_inference": [[47, 98], ["fast_rcnn.fast_rcnn_inference_single_image", "zip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.fast_rcnn_inference_single_image"], ["def", "fast_rcnn_inference", "(", "\n", "boxes", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "scores", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "image_shapes", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "score_thresh", ":", "float", ",", "\n", "nms_thresh", ":", "float", ",", "\n", "soft_nms_enabled", ":", "bool", ",", "\n", "soft_nms_method", ":", "str", ",", "\n", "soft_nms_sigma", ":", "float", ",", "\n", "soft_nms_prune", ":", "float", ",", "\n", "topk_per_image", ":", "int", ",", "\n", "scores_bf_multiply", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "vis", "=", "False", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Call `fast_rcnn_inference_single_image` for all images.\n\n    Args:\n        boxes (list[Tensor]): A list of Tensors of predicted class-specific or class-agnostic\n            boxes for each image. Element i has shape (Ri, K * 4) if doing\n            class-specific regression, or (Ri, 4) if doing class-agnostic\n            regression, where Ri is the number of predicted objects for image i.\n            This is compatible with the output of :meth:`FastRCNNOutputLayers.predict_boxes`.\n        scores (list[Tensor]): A list of Tensors of predicted class scores for each image.\n            Element i has shape (Ri, K + 1), where Ri is the number of predicted objects\n            for image i. Compatible with the output of :meth:`FastRCNNOutputLayers.predict_probs`.\n        image_shapes (list[tuple]): A list of (width, height) tuples for each image in the batch.\n        score_thresh (float): Only return detections with a confidence score exceeding this\n            threshold.\n        nms_thresh (float):  The threshold to use for box non-maximum suppression. Value in [0, 1].\n        soft_nms_enabled (bool): Indicate to use soft non-maximum suppression.\n        soft_nms_method: (str): One of ['gaussian', 'linear', 'hard']\n        soft_nms_sigma: (float): Sigma for gaussian soft nms. Value in (0, inf)\n        soft_nms_prune: (float): Threshold for pruning during soft nms. Value in [0, 1]\n        topk_per_image (int): The number of top scoring detections to return. Set < 0 to return\n            all detections.\n\n    Returns:\n        instances: (list[Instances]): A list of N instances, one for each image in the batch,\n            that stores the topk most confidence detections.\n        kept_indices: (list[Tensor]): A list of 1D tensor of length of N, each element indicates\n            the corresponding boxes/scores index in [0, Ri) from the input, for image i.\n    \"\"\"", "\n", "result_per_image", "=", "[", "\n", "fast_rcnn_inference_single_image", "(", "\n", "boxes_per_image", ",", "scores_per_image", ",", "image_shape", ",", "score_thresh", ",", "nms_thresh", ",", "\n", "soft_nms_enabled", ",", "soft_nms_method", ",", "soft_nms_sigma", ",", "soft_nms_prune", ",", "topk_per_image", ",", "s_bf_per_img", ",", "vis", "\n", ")", "\n", "for", "scores_per_image", ",", "boxes_per_image", ",", "image_shape", ",", "s_bf_per_img", "in", "zip", "(", "scores", ",", "boxes", ",", "image_shapes", ",", "scores_bf_multiply", ")", "\n", "]", "\n", "return", "[", "x", "[", "0", "]", "for", "x", "in", "result_per_image", "]", ",", "[", "x", "[", "1", "]", "for", "x", "in", "result_per_image", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn._log_classification_stats": [[100, 128], ["gt_classes.numel", "pred_logits.argmax", "fg_inds.nonzero().numel", "detectron2.utils.events.get_event_storage", "detectron2.utils.events.get_event_storage.put_scalar", "detectron2.utils.events.get_event_storage.put_scalar", "detectron2.utils.events.get_event_storage.put_scalar", "fg_inds.nonzero"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar"], ["", "def", "_log_classification_stats", "(", "pred_logits", ",", "gt_classes", ",", "prefix", "=", "\"fast_rcnn\"", ")", ":", "\n", "    ", "\"\"\"\n    Log the classification metrics to EventStorage.\n\n    Args:\n        pred_logits: Rx(K+1) logits. The last column is for background class.\n        gt_classes: R labels\n    \"\"\"", "\n", "num_instances", "=", "gt_classes", ".", "numel", "(", ")", "\n", "if", "num_instances", "==", "0", ":", "\n", "        ", "return", "\n", "", "pred_classes", "=", "pred_logits", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "bg_class_ind", "=", "pred_logits", ".", "shape", "[", "1", "]", "-", "1", "\n", "\n", "fg_inds", "=", "(", "gt_classes", ">=", "0", ")", "&", "(", "gt_classes", "<", "bg_class_ind", ")", "\n", "num_fg", "=", "fg_inds", ".", "nonzero", "(", ")", ".", "numel", "(", ")", "\n", "fg_gt_classes", "=", "gt_classes", "[", "fg_inds", "]", "\n", "fg_pred_classes", "=", "pred_classes", "[", "fg_inds", "]", "\n", "\n", "num_false_negative", "=", "(", "fg_pred_classes", "==", "bg_class_ind", ")", ".", "nonzero", "(", ")", ".", "numel", "(", ")", "\n", "num_accurate", "=", "(", "pred_classes", "==", "gt_classes", ")", ".", "nonzero", "(", ")", ".", "numel", "(", ")", "\n", "fg_num_accurate", "=", "(", "fg_pred_classes", "==", "fg_gt_classes", ")", ".", "nonzero", "(", ")", ".", "numel", "(", ")", "\n", "\n", "storage", "=", "get_event_storage", "(", ")", "\n", "storage", ".", "put_scalar", "(", "f\"{prefix}/cls_accuracy\"", ",", "num_accurate", "/", "num_instances", ")", "\n", "if", "num_fg", ">", "0", ":", "\n", "        ", "storage", ".", "put_scalar", "(", "f\"{prefix}/fg_cls_accuracy\"", ",", "fg_num_accurate", "/", "num_fg", ")", "\n", "storage", ".", "put_scalar", "(", "f\"{prefix}/false_negative\"", ",", "num_false_negative", "/", "num_fg", ")", "\n", "#print(\"cls_accuracy {:.2f}; fg_cls_accuracy {:.2f}; false_negative {:.2f}\".format(num_accurate / num_instances, fg_num_accurate / num_fg, num_false_negative / num_fg))", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.fast_rcnn_inference_single_image": [[131, 211], ["detectron2.structures.Boxes", "boxes.tensor.view.clip", "boxes.tensor.view.tensor.view", "filter_mask.nonzero", "detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.isfinite().all", "torch.isfinite().all", "valid_mask.all", "boxes.tensor.view.reshape", "detectron2.layers.batched_nms", "detectron2.layers.soft_nms.batched_soft_nms", "torch.isfinite", "torch.isfinite"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms.batched_soft_nms"], ["", "", "def", "fast_rcnn_inference_single_image", "(", "\n", "boxes", ",", "\n", "scores", ",", "\n", "image_shape", ":", "Tuple", "[", "int", ",", "int", "]", ",", "\n", "score_thresh", ":", "float", ",", "\n", "nms_thresh", ":", "float", ",", "\n", "soft_nms_enabled", ":", "bool", ",", "\n", "soft_nms_method", ":", "str", ",", "\n", "soft_nms_sigma", ":", "float", ",", "\n", "soft_nms_prune", ":", "float", ",", "\n", "topk_per_image", ":", "int", ",", "\n", "scores_bf_multiply", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "\n", "vis", "=", "False", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Single-image inference. Return bounding-box detection results by thresholding\n    on scores and applying non-maximum suppression (NMS).\n\n    Args:\n        Same as `fast_rcnn_inference`, but with boxes, scores, and image shapes\n        per image.\n\n    Returns:\n        Same as `fast_rcnn_inference`, but for only one image.\n    \"\"\"", "\n", "valid_mask", "=", "torch", ".", "isfinite", "(", "boxes", ")", ".", "all", "(", "dim", "=", "1", ")", "&", "torch", ".", "isfinite", "(", "scores", ")", ".", "all", "(", "dim", "=", "1", ")", "\n", "if", "not", "valid_mask", ".", "all", "(", ")", ":", "\n", "        ", "boxes", "=", "boxes", "[", "valid_mask", "]", "\n", "scores", "=", "scores", "[", "valid_mask", "]", "\n", "scores_bf_multiply", "=", "scores_bf_multiply", "[", "valid_mask", "]", "\n", "\n", "", "scores", "=", "scores", "[", ":", ",", ":", "-", "1", "]", "\n", "scores_bf_multiply", "=", "scores_bf_multiply", "[", ":", ",", ":", "-", "1", "]", "\n", "num_bbox_reg_classes", "=", "boxes", ".", "shape", "[", "1", "]", "//", "4", "\n", "# Convert to Boxes to use the `clip` function ...", "\n", "boxes", "=", "Boxes", "(", "boxes", ".", "reshape", "(", "-", "1", ",", "4", ")", ")", "\n", "boxes", ".", "clip", "(", "image_shape", ")", "\n", "boxes", "=", "boxes", ".", "tensor", ".", "view", "(", "-", "1", ",", "num_bbox_reg_classes", ",", "4", ")", "# R x C x 4", "\n", "\n", "# 1. Filter results based on detection scores. It can make NMS more efficient", "\n", "#    by filtering out low-confidence detections.", "\n", "filter_mask", "=", "scores", ">", "score_thresh", "# R x K", "\n", "# R' x 2. First column contains indices of the R predictions;", "\n", "# Second column contains indices of classes.", "\n", "filter_inds", "=", "filter_mask", ".", "nonzero", "(", ")", "\n", "if", "num_bbox_reg_classes", "==", "1", ":", "\n", "        ", "boxes", "=", "boxes", "[", "filter_inds", "[", ":", ",", "0", "]", ",", "0", "]", "\n", "", "else", ":", "\n", "        ", "boxes", "=", "boxes", "[", "filter_mask", "]", "\n", "", "scores", "=", "scores", "[", "filter_mask", "]", "\n", "scores_bf_multiply", "=", "scores_bf_multiply", "[", "filter_mask", "]", "\n", "\n", "# 2. Apply NMS for each class independently.", "\n", "if", "not", "soft_nms_enabled", ":", "\n", "        ", "keep", "=", "batched_nms", "(", "boxes", ",", "scores", ",", "filter_inds", "[", ":", ",", "1", "]", ",", "nms_thresh", ")", "\n", "", "else", ":", "\n", "        ", "keep", ",", "soft_nms_scores", "=", "batched_soft_nms", "(", "\n", "boxes", ",", "\n", "scores", ",", "\n", "filter_inds", "[", ":", ",", "1", "]", ",", "\n", "soft_nms_method", ",", "\n", "soft_nms_sigma", ",", "\n", "nms_thresh", ",", "\n", "soft_nms_prune", ",", "\n", ")", "\n", "scores", "[", "keep", "]", "=", "soft_nms_scores", "\n", "# scores_bf_multiply? (TBD)", "\n", "scores_bf_multiply", "=", "scores", "\n", "", "if", "topk_per_image", ">=", "0", ":", "\n", "        ", "keep", "=", "keep", "[", ":", "topk_per_image", "]", "\n", "", "boxes", ",", "scores", ",", "filter_inds", "=", "boxes", "[", "keep", "]", ",", "scores", "[", "keep", "]", ",", "filter_inds", "[", "keep", "]", "\n", "scores_bf_multiply", "=", "scores_bf_multiply", "[", "keep", "]", "\n", "\n", "result", "=", "Instances", "(", "image_shape", ")", "\n", "result", ".", "pred_boxes", "=", "Boxes", "(", "boxes", ")", "\n", "result", ".", "scores", "=", "scores", "\n", "if", "vis", ":", "# visualization: convert to the original scores before multiplying RPN scores", "\n", "        ", "result", ".", "scores", "=", "scores_bf_multiply", "\n", "", "result", ".", "pred_classes", "=", "filter_inds", "[", ":", ",", "1", "]", "\n", "return", "result", ",", "filter_inds", "[", ":", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.from_config": [[139, 146], ["super().from_config", "box_regression.Box2BoxTransformRotated"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config"], ["@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "args", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ",", "input_shape", ")", "\n", "args", "[", "\"box2box_transform\"", "]", "=", "Box2BoxTransformRotated", "(", "\n", "weights", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_WEIGHTS", "\n", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference": [[147, 164], ["rotated_fast_rcnn.RotatedFastRCNNOutputLayers.predict_boxes", "rotated_fast_rcnn.RotatedFastRCNNOutputLayers.predict_probs", "rotated_fast_rcnn.fast_rcnn_inference_rotated"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_probs", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.fast_rcnn_inference_rotated"], ["", "def", "inference", "(", "self", ",", "predictions", ",", "proposals", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            list[Instances]: same as `fast_rcnn_inference_rotated`.\n            list[Tensor]: same as `fast_rcnn_inference_rotated`.\n        \"\"\"", "\n", "boxes", "=", "self", ".", "predict_boxes", "(", "predictions", ",", "proposals", ")", "\n", "scores", "=", "self", ".", "predict_probs", "(", "predictions", ",", "proposals", ")", "\n", "image_shapes", "=", "[", "x", ".", "image_size", "for", "x", "in", "proposals", "]", "\n", "\n", "return", "fast_rcnn_inference_rotated", "(", "\n", "boxes", ",", "\n", "scores", ",", "\n", "image_shapes", ",", "\n", "self", ".", "test_score_thresh", ",", "\n", "self", ".", "test_nms_thresh", ",", "\n", "self", ".", "test_topk_per_image", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RROIHeads.__init__": [[174, 184], ["roi_heads.StandardROIHeads.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "assert", "(", "\n", "not", "self", ".", "mask_on", "and", "not", "self", ".", "keypoint_on", "\n", ")", ",", "\"Mask/Keypoints not supported in Rotated ROIHeads.\"", "\n", "assert", "not", "self", ".", "train_on_pred_boxes", ",", "\"train_on_pred_boxes not implemented for RROIHeads!\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RROIHeads._init_box_head": [[185, 214], ["tuple", "poolers.ROIPooler", "box_head.build_box_head.build_box_head", "rotated_fast_rcnn.RotatedFastRCNNOutputLayers", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.box_head.build_box_head"], ["", "@", "classmethod", "\n", "def", "_init_box_head", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "# fmt: off", "\n", "        ", "in_features", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_scales", "=", "tuple", "(", "1.0", "/", "input_shape", "[", "k", "]", ".", "stride", "for", "k", "in", "in_features", ")", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "\n", "# fmt: on", "\n", "assert", "pooler_type", "in", "[", "\"ROIAlignRotated\"", "]", ",", "pooler_type", "\n", "# assume all channel counts are equal", "\n", "in_channels", "=", "[", "input_shape", "[", "f", "]", ".", "channels", "for", "f", "in", "in_features", "]", "[", "0", "]", "\n", "\n", "box_pooler", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "box_head", "=", "build_box_head", "(", "\n", "cfg", ",", "ShapeSpec", "(", "channels", "=", "in_channels", ",", "height", "=", "pooler_resolution", ",", "width", "=", "pooler_resolution", ")", "\n", ")", "\n", "# This line is the only difference v.s. StandardROIHeads", "\n", "box_predictor", "=", "RotatedFastRCNNOutputLayers", "(", "cfg", ",", "box_head", ".", "output_shape", ")", "\n", "return", "{", "\n", "\"box_in_features\"", ":", "in_features", ",", "\n", "\"box_pooler\"", ":", "box_pooler", ",", "\n", "\"box_head\"", ":", "box_head", ",", "\n", "\"box_predictor\"", ":", "box_predictor", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RROIHeads.label_and_sample_proposals": [[216, 271], ["torch.no_grad", "zip", "detectron2.utils.events.get_event_storage", "detectron2.utils.events.get_event_storage.put_scalar", "detectron2.utils.events.get_event_storage.put_scalar", "proposal_generator.proposal_utils.add_ground_truth_to_proposals", "detectron2.structures.pairwise_iou_rotated", "rotated_fast_rcnn.RROIHeads.proposal_matcher", "rotated_fast_rcnn.RROIHeads._sample_proposals", "num_bg_samples.append", "num_fg_samples.append", "proposals_with_gt.append", "numpy.mean", "numpy.mean", "len", "gt_classes.numel"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.add_ground_truth_to_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads._sample_proposals"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "label_and_sample_proposals", "(", "self", ",", "proposals", ",", "targets", ")", ":", "\n", "        ", "\"\"\"\n        Prepare some proposals to be used to train the RROI heads.\n        It performs box matching between `proposals` and `targets`, and assigns\n        training labels to the proposals.\n        It returns `self.batch_size_per_image` random samples from proposals and groundtruth boxes,\n        with a fraction of positives that is no larger than `self.positive_sample_fraction.\n\n        Args:\n            See :meth:`StandardROIHeads.forward`\n\n        Returns:\n            list[Instances]: length `N` list of `Instances`s containing the proposals\n                sampled for training. Each `Instances` has the following fields:\n                - proposal_boxes: the rotated proposal boxes\n                - gt_boxes: the ground-truth rotated boxes that the proposal is assigned to\n                  (this is only meaningful if the proposal has a label > 0; if label = 0\n                   then the ground-truth box is random)\n                - gt_classes: the ground-truth classification lable for each proposal\n        \"\"\"", "\n", "if", "self", ".", "proposal_append_gt", ":", "\n", "            ", "proposals", "=", "add_ground_truth_to_proposals", "(", "targets", ",", "proposals", ")", "\n", "\n", "", "proposals_with_gt", "=", "[", "]", "\n", "\n", "num_fg_samples", "=", "[", "]", "\n", "num_bg_samples", "=", "[", "]", "\n", "for", "proposals_per_image", ",", "targets_per_image", "in", "zip", "(", "proposals", ",", "targets", ")", ":", "\n", "            ", "has_gt", "=", "len", "(", "targets_per_image", ")", ">", "0", "\n", "match_quality_matrix", "=", "pairwise_iou_rotated", "(", "\n", "targets_per_image", ".", "gt_boxes", ",", "proposals_per_image", ".", "proposal_boxes", "\n", ")", "\n", "matched_idxs", ",", "matched_labels", "=", "self", ".", "proposal_matcher", "(", "match_quality_matrix", ")", "\n", "sampled_idxs", ",", "gt_classes", "=", "self", ".", "_sample_proposals", "(", "\n", "matched_idxs", ",", "matched_labels", ",", "targets_per_image", ".", "gt_classes", "\n", ")", "\n", "\n", "proposals_per_image", "=", "proposals_per_image", "[", "sampled_idxs", "]", "\n", "proposals_per_image", ".", "gt_classes", "=", "gt_classes", "\n", "\n", "if", "has_gt", ":", "\n", "                ", "sampled_targets", "=", "matched_idxs", "[", "sampled_idxs", "]", "\n", "proposals_per_image", ".", "gt_boxes", "=", "targets_per_image", ".", "gt_boxes", "[", "sampled_targets", "]", "\n", "\n", "", "num_bg_samples", ".", "append", "(", "(", "gt_classes", "==", "self", ".", "num_classes", ")", ".", "sum", "(", ")", ".", "item", "(", ")", ")", "\n", "num_fg_samples", ".", "append", "(", "gt_classes", ".", "numel", "(", ")", "-", "num_bg_samples", "[", "-", "1", "]", ")", "\n", "proposals_with_gt", ".", "append", "(", "proposals_per_image", ")", "\n", "\n", "# Log the number of fg/bg samples that are selected for training ROI heads", "\n", "", "storage", "=", "get_event_storage", "(", ")", "\n", "storage", ".", "put_scalar", "(", "\"roi_head/num_fg_samples\"", ",", "np", ".", "mean", "(", "num_fg_samples", ")", ")", "\n", "storage", ".", "put_scalar", "(", "\"roi_head/num_bg_samples\"", ",", "np", ".", "mean", "(", "num_bg_samples", ")", ")", "\n", "\n", "return", "proposals_with_gt", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.fast_rcnn_inference_rotated": [[46, 81], ["rotated_fast_rcnn.fast_rcnn_inference_single_image_rotated", "zip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.fast_rcnn_inference_single_image_rotated"], ["def", "fast_rcnn_inference_rotated", "(", "\n", "boxes", ",", "scores", ",", "image_shapes", ",", "score_thresh", ",", "nms_thresh", ",", "topk_per_image", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Call `fast_rcnn_inference_single_image_rotated` for all images.\n\n    Args:\n        boxes (list[Tensor]): A list of Tensors of predicted class-specific or class-agnostic\n            boxes for each image. Element i has shape (Ri, K * 5) if doing\n            class-specific regression, or (Ri, 5) if doing class-agnostic\n            regression, where Ri is the number of predicted objects for image i.\n            This is compatible with the output of :meth:`FastRCNNOutputs.predict_boxes`.\n        scores (list[Tensor]): A list of Tensors of predicted class scores for each image.\n            Element i has shape (Ri, K + 1), where Ri is the number of predicted objects\n            for image i. Compatible with the output of :meth:`FastRCNNOutputs.predict_probs`.\n        image_shapes (list[tuple]): A list of (width, height) tuples for each image in the batch.\n        score_thresh (float): Only return detections with a confidence score exceeding this\n            threshold.\n        nms_thresh (float):  The threshold to use for box non-maximum suppression. Value in [0, 1].\n        topk_per_image (int): The number of top scoring detections to return. Set < 0 to return\n            all detections.\n\n    Returns:\n        instances: (list[Instances]): A list of N instances, one for each image in the batch,\n            that stores the topk most confidence detections.\n        kept_indices: (list[Tensor]): A list of 1D tensor of length of N, each element indicates\n            the corresponding boxes/scores index in [0, Ri) from the input, for image i.\n    \"\"\"", "\n", "result_per_image", "=", "[", "\n", "fast_rcnn_inference_single_image_rotated", "(", "\n", "boxes_per_image", ",", "scores_per_image", ",", "image_shape", ",", "score_thresh", ",", "nms_thresh", ",", "topk_per_image", "\n", ")", "\n", "for", "scores_per_image", ",", "boxes_per_image", ",", "image_shape", "in", "zip", "(", "scores", ",", "boxes", ",", "image_shapes", ")", "\n", "]", "\n", "return", "[", "x", "[", "0", "]", "for", "x", "in", "result_per_image", "]", ",", "[", "x", "[", "1", "]", "for", "x", "in", "result_per_image", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.fast_rcnn_inference_single_image_rotated": [[83, 132], ["detectron2.structures.RotatedBoxes", "boxes.tensor.view.clip", "boxes.tensor.view.tensor.view", "filter_mask.nonzero", "detectron2.layers.batched_nms_rotated", "detectron2.structures.Instances", "detectron2.structures.RotatedBoxes", "torch.isfinite().all", "torch.isfinite().all", "valid_mask.all", "boxes.tensor.view.reshape", "torch.isfinite", "torch.isfinite"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms_rotated"], ["", "def", "fast_rcnn_inference_single_image_rotated", "(", "\n", "boxes", ",", "scores", ",", "image_shape", ",", "score_thresh", ",", "nms_thresh", ",", "topk_per_image", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Single-image inference. Return rotated bounding-box detection results by thresholding\n    on scores and applying rotated non-maximum suppression (Rotated NMS).\n\n    Args:\n        Same as `fast_rcnn_inference_rotated`, but with rotated boxes, scores, and image shapes\n        per image.\n\n    Returns:\n        Same as `fast_rcnn_inference_rotated`, but for only one image.\n    \"\"\"", "\n", "valid_mask", "=", "torch", ".", "isfinite", "(", "boxes", ")", ".", "all", "(", "dim", "=", "1", ")", "&", "torch", ".", "isfinite", "(", "scores", ")", ".", "all", "(", "dim", "=", "1", ")", "\n", "if", "not", "valid_mask", ".", "all", "(", ")", ":", "\n", "        ", "boxes", "=", "boxes", "[", "valid_mask", "]", "\n", "scores", "=", "scores", "[", "valid_mask", "]", "\n", "\n", "", "B", "=", "5", "# box dimension", "\n", "scores", "=", "scores", "[", ":", ",", ":", "-", "1", "]", "\n", "num_bbox_reg_classes", "=", "boxes", ".", "shape", "[", "1", "]", "//", "B", "\n", "# Convert to Boxes to use the `clip` function ...", "\n", "boxes", "=", "RotatedBoxes", "(", "boxes", ".", "reshape", "(", "-", "1", ",", "B", ")", ")", "\n", "boxes", ".", "clip", "(", "image_shape", ")", "\n", "boxes", "=", "boxes", ".", "tensor", ".", "view", "(", "-", "1", ",", "num_bbox_reg_classes", ",", "B", ")", "# R x C x B", "\n", "# Filter results based on detection scores", "\n", "filter_mask", "=", "scores", ">", "score_thresh", "# R x K", "\n", "# R' x 2. First column contains indices of the R predictions;", "\n", "# Second column contains indices of classes.", "\n", "filter_inds", "=", "filter_mask", ".", "nonzero", "(", ")", "\n", "if", "num_bbox_reg_classes", "==", "1", ":", "\n", "        ", "boxes", "=", "boxes", "[", "filter_inds", "[", ":", ",", "0", "]", ",", "0", "]", "\n", "", "else", ":", "\n", "        ", "boxes", "=", "boxes", "[", "filter_mask", "]", "\n", "", "scores", "=", "scores", "[", "filter_mask", "]", "\n", "\n", "# Apply per-class Rotated NMS", "\n", "keep", "=", "batched_nms_rotated", "(", "boxes", ",", "scores", ",", "filter_inds", "[", ":", ",", "1", "]", ",", "nms_thresh", ")", "\n", "if", "topk_per_image", ">=", "0", ":", "\n", "        ", "keep", "=", "keep", "[", ":", "topk_per_image", "]", "\n", "", "boxes", ",", "scores", ",", "filter_inds", "=", "boxes", "[", "keep", "]", ",", "scores", "[", "keep", "]", ",", "filter_inds", "[", "keep", "]", "\n", "\n", "result", "=", "Instances", "(", "image_shape", ")", "\n", "result", ".", "pred_boxes", "=", "RotatedBoxes", "(", "boxes", ")", "\n", "result", ".", "scores", "=", "scores", "\n", "result", ".", "pred_classes", "=", "filter_inds", "[", ":", ",", "1", "]", "\n", "\n", "return", "result", ",", "filter_inds", "[", ":", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPRes5ROIHeads.__init__": [[34, 69], ["roi_heads.ROIHeads.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "in_features", ":", "List", "[", "str", "]", ",", "\n", "pooler", ":", "ROIPooler", ",", "\n", "res5", ":", "None", ",", "\n", "box_predictor", ":", "nn", ".", "Module", ",", "\n", "mask_head", ":", "Optional", "[", "nn", ".", "Module", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            in_features (list[str]): list of backbone feature map names to use for\n                feature extraction\n            pooler (ROIPooler): pooler to extra region features from backbone\n            res5 (nn.Sequential): a CNN to compute per-region features, to be used by\n                ``box_predictor`` and ``mask_head``. Typically this is a \"res5\"\n                block from a ResNet.\n            box_predictor (nn.Module): make box predictions from the feature.\n                Should have the same interface as :class:`FastRCNNOutputLayers`.\n            mask_head (nn.Module): transform features to make mask predictions\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "pooler", "=", "pooler", "\n", "# if isinstance(res5, (list, tuple)):", "\n", "#     res5 = nn.Sequential(*res5)", "\n", "self", ".", "res5", "=", "res5", "#  None, this head uses the res5 from backbone", "\n", "self", ".", "box_predictor", "=", "box_predictor", "\n", "self", ".", "mask_on", "=", "mask_head", "is", "not", "None", "\n", "if", "self", ".", "mask_on", ":", "\n", "            ", "self", ".", "mask_head", "=", "mask_head", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPRes5ROIHeads.from_config": [[70, 111], ["super().from_config", "poolers.ROIPooler", "fast_rcnn.FastRCNNOutputLayers", "len", "detectron2.layers.ShapeSpec", "mask_head.build_mask_head", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.build_mask_head"], ["", "", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "# fmt: off", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ")", "\n", "in_features", "=", "ret", "[", "\"in_features\"", "]", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "\n", "pooler_scales", "=", "(", "1.0", "/", "input_shape", "[", "in_features", "[", "0", "]", "]", ".", "stride", ",", ")", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "mask_on", "=", "cfg", ".", "MODEL", ".", "MASK_ON", "\n", "# fmt: on", "\n", "assert", "not", "cfg", ".", "MODEL", ".", "KEYPOINT_ON", "\n", "assert", "len", "(", "in_features", ")", "==", "1", "\n", "\n", "ret", "[", "\"pooler\"", "]", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "\n", "# Compatbility with old moco code. Might be useful.", "\n", "# See notes in StandardROIHeads.from_config", "\n", "# if not inspect.ismethod(cls._build_res5_block):", "\n", "#     logger.warning(", "\n", "#         \"The behavior of _build_res5_block may change. \"", "\n", "#         \"Please do not depend on private methods.\"", "\n", "#     )", "\n", "#     cls._build_res5_block = classmethod(cls._build_res5_block)", "\n", "\n", "ret", "[", "\"res5\"", "]", ",", "out_channels", "=", "None", ",", "cfg", ".", "MODEL", ".", "RESNETS", ".", "RES2_OUT_CHANNELS", "*", "8", "# cls._build_res5_block(cfg)", "\n", "ret", "[", "\"box_predictor\"", "]", "=", "FastRCNNOutputLayers", "(", "\n", "cfg", ",", "ShapeSpec", "(", "channels", "=", "out_channels", ",", "height", "=", "1", ",", "width", "=", "1", ")", "\n", ")", "\n", "\n", "if", "mask_on", ":", "\n", "            ", "ret", "[", "\"mask_head\"", "]", "=", "build_mask_head", "(", "\n", "cfg", ",", "\n", "ShapeSpec", "(", "channels", "=", "out_channels", ",", "width", "=", "pooler_resolution", ",", "height", "=", "pooler_resolution", ")", ",", "\n", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPRes5ROIHeads._shared_roi_transform": [[112, 115], ["clip_roi_heads.CLIPRes5ROIHeads.pooler", "backbone_res5"], "methods", ["None"], ["", "def", "_shared_roi_transform", "(", "self", ",", "features", ",", "boxes", ",", "backbone_res5", ")", ":", "\n", "        ", "x", "=", "self", ".", "pooler", "(", "features", ",", "boxes", ")", "\n", "return", "backbone_res5", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPRes5ROIHeads.forward": [[116, 156], ["clip_roi_heads.CLIPRes5ROIHeads._shared_roi_transform", "clip_roi_heads.CLIPRes5ROIHeads.label_and_sample_proposals", "attnpool", "clip_roi_heads.CLIPRes5ROIHeads.box_predictor", "clip_roi_heads.CLIPRes5ROIHeads.box_predictor", "clip_roi_heads.CLIPRes5ROIHeads.box_predictor.losses", "clip_roi_heads.CLIPRes5ROIHeads.box_predictor.inference", "clip_roi_heads.CLIPRes5ROIHeads.forward_with_given_boxes", "clip_roi_heads.CLIPRes5ROIHeads.mean", "roi_heads.select_foreground_proposals", "clip_roi_heads.CLIPRes5ROIHeads.update", "clip_roi_heads.CLIPRes5ROIHeads.mask_head", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads._shared_roi_transform", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads.label_and_sample_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward_with_given_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.select_foreground_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "forward", "(", "self", ",", "images", ",", "features", ",", "proposals", ",", "targets", "=", "None", ",", "res5", "=", "None", ",", "attnpool", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        See :meth:`ROIHeads.forward`.\n        \"\"\"", "\n", "del", "images", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "assert", "targets", "\n", "proposals", "=", "self", ".", "label_and_sample_proposals", "(", "proposals", ",", "targets", ")", "\n", "", "del", "targets", "\n", "\n", "proposal_boxes", "=", "[", "x", ".", "proposal_boxes", "for", "x", "in", "proposals", "]", "\n", "box_features", "=", "self", ".", "_shared_roi_transform", "(", "\n", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "in_features", "]", ",", "proposal_boxes", ",", "res5", "\n", ")", "\n", "if", "attnpool", ":", "# att pooling", "\n", "            ", "att_feats", "=", "attnpool", "(", "box_features", ")", "\n", "predictions", "=", "self", ".", "box_predictor", "(", "att_feats", ")", "\n", "", "else", ":", "# mean pooling", "\n", "            ", "predictions", "=", "self", ".", "box_predictor", "(", "box_features", ".", "mean", "(", "dim", "=", "[", "2", ",", "3", "]", ")", ")", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "            ", "del", "features", "\n", "losses", "=", "self", ".", "box_predictor", ".", "losses", "(", "predictions", ",", "proposals", ")", "\n", "if", "self", ".", "mask_on", ":", "\n", "                ", "proposals", ",", "fg_selection_masks", "=", "select_foreground_proposals", "(", "\n", "proposals", ",", "self", ".", "num_classes", "\n", ")", "\n", "# Since the ROI feature transform is shared between boxes and masks,", "\n", "# we don't need to recompute features. The mask loss is only defined", "\n", "# on foreground proposals, so we need to select out the foreground", "\n", "# features.", "\n", "mask_features", "=", "box_features", "[", "torch", ".", "cat", "(", "fg_selection_masks", ",", "dim", "=", "0", ")", "]", "\n", "del", "box_features", "\n", "losses", ".", "update", "(", "self", ".", "mask_head", "(", "mask_features", ",", "proposals", ")", ")", "\n", "", "return", "[", "]", ",", "losses", "\n", "", "else", ":", "\n", "            ", "pred_instances", ",", "_", "=", "self", ".", "box_predictor", ".", "inference", "(", "predictions", ",", "proposals", ")", "\n", "pred_instances", "=", "self", ".", "forward_with_given_boxes", "(", "features", ",", "pred_instances", ",", "res5", ")", "\n", "return", "pred_instances", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPRes5ROIHeads.forward_with_given_boxes": [[157, 180], ["instances[].has", "instances[].has", "clip_roi_heads.CLIPRes5ROIHeads._shared_roi_transform", "clip_roi_heads.CLIPRes5ROIHeads.mask_head"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads._shared_roi_transform"], ["", "", "def", "forward_with_given_boxes", "(", "self", ",", "features", ",", "instances", ",", "res5", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.\n\n        Args:\n            features: same as in `forward()`\n            instances (list[Instances]): instances to predict other outputs. Expect the keys\n                \"pred_boxes\" and \"pred_classes\" to exist.\n\n        Returns:\n            instances (Instances):\n                the same `Instances` object, with extra\n                fields such as `pred_masks` or `pred_keypoints`.\n        \"\"\"", "\n", "assert", "not", "self", ".", "training", "\n", "assert", "instances", "[", "0", "]", ".", "has", "(", "\"pred_boxes\"", ")", "and", "instances", "[", "0", "]", ".", "has", "(", "\"pred_classes\"", ")", "\n", "\n", "if", "self", ".", "mask_on", ":", "\n", "            ", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "in_features", "]", "\n", "x", "=", "self", ".", "_shared_roi_transform", "(", "features", ",", "[", "x", ".", "pred_boxes", "for", "x", "in", "instances", "]", ",", "res5", ")", "\n", "return", "self", ".", "mask_head", "(", "x", ",", "instances", ")", "\n", "", "else", ":", "\n", "            ", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.PretrainRes5ROIHeads.__init__": [[188, 221], ["roi_heads.ROIHeads.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "in_features", ":", "List", "[", "str", "]", ",", "\n", "pooler", ":", "ROIPooler", ",", "\n", "res5", ":", "None", ",", "\n", "box_predictor", ":", "Optional", "[", "nn", ".", "Module", "]", "=", "None", ",", "\n", "mask_head", ":", "Optional", "[", "nn", ".", "Module", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            in_features (list[str]): list of backbone feature map names to use for\n                feature extraction\n            pooler (ROIPooler): pooler to extra region features from backbone\n            res5 (nn.Sequential): a CNN to compute per-region features, to be used by\n                ``box_predictor`` and ``mask_head``. Typically this is a \"res5\"\n                block from a ResNet.\n            box_predictor (nn.Module): make box predictions from the feature.\n                Should have the same interface as :class:`FastRCNNOutputLayers`.\n            mask_head (nn.Module): transform features to make mask predictions\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "pooler", "=", "pooler", "\n", "# if isinstance(res5, (list, tuple)):", "\n", "#     res5 = nn.Sequential(*res5)", "\n", "self", ".", "res5", "=", "res5", "#  None, this head uses the res5 from backbone", "\n", "self", ".", "box_predictor", "=", "None", "\n", "self", ".", "mask_on", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.PretrainRes5ROIHeads.from_config": [[222, 247], ["super().from_config", "poolers.ROIPooler", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "# fmt: off", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ")", "\n", "in_features", "=", "ret", "[", "\"in_features\"", "]", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "\n", "pooler_scales", "=", "(", "1.0", "/", "input_shape", "[", "in_features", "[", "0", "]", "]", ".", "stride", ",", ")", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "mask_on", "=", "cfg", ".", "MODEL", ".", "MASK_ON", "\n", "# fmt: on", "\n", "assert", "not", "cfg", ".", "MODEL", ".", "KEYPOINT_ON", "\n", "assert", "len", "(", "in_features", ")", "==", "1", "\n", "\n", "ret", "[", "\"pooler\"", "]", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "\n", "ret", "[", "\"res5\"", "]", ",", "out_channels", "=", "None", ",", "cfg", ".", "MODEL", ".", "RESNETS", ".", "RES2_OUT_CHANNELS", "*", "8", "# cls._build_res5_block(cfg)", "\n", "ret", "[", "\"box_predictor\"", "]", "=", "None", "\n", "ret", "[", "\"mask_head\"", "]", "=", "None", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.PretrainRes5ROIHeads._shared_roi_transform": [[248, 251], ["clip_roi_heads.PretrainRes5ROIHeads.pooler", "backbone_res5"], "methods", ["None"], ["", "def", "_shared_roi_transform", "(", "self", ",", "features", ",", "boxes", ",", "backbone_res5", ")", ":", "\n", "        ", "x", "=", "self", ".", "pooler", "(", "features", ",", "boxes", ")", "\n", "return", "backbone_res5", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.PretrainRes5ROIHeads.forward": [[252, 267], ["clip_roi_heads.PretrainRes5ROIHeads._shared_roi_transform", "attnpool", "clip_roi_heads.PretrainRes5ROIHeads.mean"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads._shared_roi_transform"], ["", "def", "forward", "(", "self", ",", "images", ",", "features", ",", "proposals", ",", "targets", "=", "None", ",", "res5", "=", "None", ",", "attnpool", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        See :meth:`ROIHeads.forward`.\n        \"\"\"", "\n", "proposal_boxes", "=", "[", "x", ".", "proposal_boxes", "for", "x", "in", "proposals", "]", "# object proposals", "\n", "box_features", "=", "self", ".", "_shared_roi_transform", "(", "\n", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "in_features", "]", ",", "proposal_boxes", ",", "res5", "\n", ")", "\n", "if", "attnpool", ":", "# att pooling", "\n", "            ", "att_feats", "=", "attnpool", "(", "box_features", ")", "\n", "region_feats", "=", "att_feats", "\n", "", "else", ":", "# mean pooling", "\n", "            ", "region_feats", "=", "box_features", ".", "mean", "(", "dim", "=", "[", "2", ",", "3", "]", ")", "\n", "\n", "", "return", "region_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.PretrainRes5ROIHeads.forward_with_given_boxes": [[268, 286], ["instances[].has", "instances[].has"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has"], ["", "def", "forward_with_given_boxes", "(", "self", ",", "features", ",", "instances", ",", "res5", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.\n\n        Args:\n            features: same as in `forward()`\n            instances (list[Instances]): instances to predict other outputs. Expect the keys\n                \"pred_boxes\" and \"pred_classes\" to exist.\n\n        Returns:\n            instances (Instances):\n                the same `Instances` object, with extra\n                fields such as `pred_masks` or `pred_keypoints`.\n        \"\"\"", "\n", "assert", "not", "self", ".", "training", "\n", "assert", "instances", "[", "0", "]", ".", "has", "(", "\"pred_boxes\"", ")", "and", "instances", "[", "0", "]", ".", "has", "(", "\"pred_classes\"", ")", "\n", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPStandardROIHeads.__init__": [[294, 342], ["roi_heads.ROIHeads.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "box_in_features", ":", "List", "[", "str", "]", ",", "\n", "box_pooler", ":", "ROIPooler", ",", "\n", "box_head", ":", "nn", ".", "Module", ",", "\n", "box_predictor", ":", "nn", ".", "Module", ",", "\n", "mask_in_features", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "mask_pooler", ":", "Optional", "[", "ROIPooler", "]", "=", "None", ",", "\n", "mask_head", ":", "Optional", "[", "nn", ".", "Module", "]", "=", "None", ",", "\n", "train_on_pred_boxes", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            box_in_features (list[str]): list of feature names to use for the box head.\n            box_pooler (ROIPooler): pooler to extra region features for box head\n            box_head (nn.Module): transform features to make box predictions\n            box_predictor (nn.Module): make box predictions from the feature.\n                Should have the same interface as :class:`FastRCNNOutputLayers`.\n            mask_in_features (list[str]): list of feature names to use for the mask\n                pooler or mask head. None if not using mask head.\n            mask_pooler (ROIPooler): pooler to extract region features from image features.\n                The mask head will then take region features to make predictions.\n                If None, the mask head will directly take the dict of image features\n                defined by `mask_in_features`\n            mask_head (nn.Module): transform features to make mask predictions\n            keypoint_in_features, keypoint_pooler, keypoint_head: similar to ``mask_*``.\n            train_on_pred_boxes (bool): whether to use proposal boxes or\n                predicted boxes from the box head to train other heads.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "# keep self.in_features for backward compatibility", "\n", "self", ".", "in_features", "=", "self", ".", "box_in_features", "=", "box_in_features", "\n", "self", ".", "box_pooler", "=", "box_pooler", "\n", "self", ".", "box_head", "=", "box_head", "\n", "self", ".", "box_predictor", "=", "box_predictor", "\n", "\n", "self", ".", "mask_on", "=", "mask_in_features", "is", "not", "None", "\n", "if", "self", ".", "mask_on", ":", "\n", "            ", "self", ".", "mask_in_features", "=", "mask_in_features", "\n", "self", ".", "mask_pooler", "=", "mask_pooler", "\n", "self", ".", "mask_head", "=", "mask_head", "\n", "\n", "", "self", ".", "train_on_pred_boxes", "=", "train_on_pred_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPStandardROIHeads.from_config": [[343, 357], ["super().from_config", "inspect.ismethod", "inspect.ismethod", "super().from_config.update", "super().from_config.update", "cls._init_box_head", "cls._init_mask_head"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._init_box_head", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._init_mask_head"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ")", "\n", "ret", "[", "\"train_on_pred_boxes\"", "]", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "TRAIN_ON_PRED_BOXES", "\n", "# Subclasses that have not been updated to use from_config style construction", "\n", "# may have overridden _init_*_head methods. In this case, those overridden methods", "\n", "# will not be classmethods and we need to avoid trying to call them here.", "\n", "# We test for this with ismethod which only returns True for bound methods of cls.", "\n", "# Such subclasses will need to handle calling their overridden _init_*_head methods.", "\n", "if", "inspect", ".", "ismethod", "(", "cls", ".", "_init_box_head", ")", ":", "\n", "            ", "ret", ".", "update", "(", "cls", ".", "_init_box_head", "(", "cfg", ",", "input_shape", ")", ")", "\n", "", "if", "inspect", ".", "ismethod", "(", "cls", ".", "_init_mask_head", ")", ":", "\n", "            ", "ret", ".", "update", "(", "cls", ".", "_init_mask_head", "(", "cfg", ",", "input_shape", ")", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPStandardROIHeads._init_box_head": [[358, 394], ["tuple", "poolers.ROIPooler", "fast_rcnn.FastRCNNOutputLayers", "len", "box_head.build_box_head", "set", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.box_head.build_box_head", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "@", "classmethod", "\n", "def", "_init_box_head", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "# fmt: off", "\n", "        ", "in_features", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_scales", "=", "tuple", "(", "1.0", "/", "input_shape", "[", "k", "]", ".", "stride", "for", "k", "in", "in_features", ")", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "\n", "# fmt: on", "\n", "\n", "# If StandardROIHeads is applied on multiple feature maps (as in FPN),", "\n", "# then we share the same predictors and therefore the channel counts must be the same", "\n", "in_channels", "=", "[", "input_shape", "[", "f", "]", ".", "channels", "for", "f", "in", "in_features", "]", "\n", "# Check all channel counts are equal", "\n", "assert", "len", "(", "set", "(", "in_channels", ")", ")", "==", "1", ",", "in_channels", "\n", "in_channels", "=", "in_channels", "[", "0", "]", "\n", "\n", "box_pooler", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "# Here we split \"box head\" and \"box predictor\", which is mainly due to historical reasons.", "\n", "# They are used together so the \"box predictor\" layers should be part of the \"box head\".", "\n", "# New subclasses of ROIHeads do not need \"box predictor\"s.", "\n", "box_head", "=", "None", "if", "cfg", ".", "MODEL", ".", "CLIP", ".", "USE_TEXT_EMB_CLASSIFIER", "else", "build_box_head", "(", "\n", "cfg", ",", "ShapeSpec", "(", "channels", "=", "in_channels", ",", "height", "=", "pooler_resolution", ",", "width", "=", "pooler_resolution", ")", "\n", ")", "\n", "box_head_output_shape", "=", "1024", "\n", "box_predictor", "=", "FastRCNNOutputLayers", "(", "cfg", ",", "box_head_output_shape", ")", "\n", "return", "{", "\n", "\"box_in_features\"", ":", "in_features", ",", "\n", "\"box_pooler\"", ":", "box_pooler", ",", "\n", "\"box_head\"", ":", "box_head", ",", "\n", "\"box_predictor\"", ":", "box_predictor", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPStandardROIHeads._init_mask_head": [[396, 429], ["tuple", "mask_head.build_mask_head", "poolers.ROIPooler", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.build_mask_head"], ["", "@", "classmethod", "\n", "def", "_init_mask_head", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "if", "not", "cfg", ".", "MODEL", ".", "MASK_ON", ":", "\n", "            ", "return", "{", "}", "\n", "# fmt: off", "\n", "", "in_features", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_scales", "=", "tuple", "(", "1.0", "/", "input_shape", "[", "k", "]", ".", "stride", "for", "k", "in", "in_features", ")", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_TYPE", "\n", "# fmt: on", "\n", "\n", "in_channels", "=", "[", "input_shape", "[", "f", "]", ".", "channels", "for", "f", "in", "in_features", "]", "[", "0", "]", "\n", "\n", "ret", "=", "{", "\"mask_in_features\"", ":", "in_features", "}", "\n", "ret", "[", "\"mask_pooler\"", "]", "=", "(", "\n", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "if", "pooler_type", "\n", "else", "None", "\n", ")", "\n", "if", "pooler_type", ":", "\n", "            ", "shape", "=", "ShapeSpec", "(", "\n", "channels", "=", "in_channels", ",", "width", "=", "pooler_resolution", ",", "height", "=", "pooler_resolution", "\n", ")", "\n", "", "else", ":", "\n", "            ", "shape", "=", "{", "f", ":", "input_shape", "[", "f", "]", "for", "f", "in", "in_features", "}", "\n", "", "ret", "[", "\"mask_head\"", "]", "=", "build_mask_head", "(", "cfg", ",", "shape", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPStandardROIHeads.forward": [[430, 460], ["clip_roi_heads.CLIPStandardROIHeads.label_and_sample_proposals", "clip_roi_heads.CLIPStandardROIHeads._forward_box", "clip_roi_heads.CLIPStandardROIHeads.update", "clip_roi_heads.CLIPStandardROIHeads._forward_box", "clip_roi_heads.CLIPStandardROIHeads.forward_with_given_boxes", "clip_roi_heads.CLIPStandardROIHeads._forward_mask"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads.label_and_sample_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._forward_box", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._forward_box", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward_with_given_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_mask"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "images", ":", "ImageList", ",", "\n", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "proposals", ":", "List", "[", "Instances", "]", ",", "\n", "targets", ":", "Optional", "[", "List", "[", "Instances", "]", "]", "=", "None", ",", "\n", "attnpool", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "Instances", "]", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        See :class:`ROIHeads.forward`.\n        \"\"\"", "\n", "del", "images", "\n", "if", "self", ".", "training", ":", "\n", "            ", "assert", "targets", ",", "\"'targets' argument is required during training\"", "\n", "proposals", "=", "self", ".", "label_and_sample_proposals", "(", "proposals", ",", "targets", ")", "\n", "", "del", "targets", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "losses", "=", "self", ".", "_forward_box", "(", "features", ",", "proposals", ",", "attnpool", "=", "attnpool", ")", "\n", "# Usually the original proposals used by the box head are used by the mask, keypoint", "\n", "# heads. But when `self.train_on_pred_boxes is True`, proposals will contain boxes", "\n", "# predicted by the box head.", "\n", "losses", ".", "update", "(", "self", ".", "_forward_mask", "(", "features", ",", "proposals", ")", ")", "\n", "return", "proposals", ",", "losses", "\n", "", "else", ":", "\n", "            ", "pred_instances", "=", "self", ".", "_forward_box", "(", "features", ",", "proposals", ",", "attnpool", "=", "attnpool", ")", "\n", "# During inference cascaded prediction is used: the mask and keypoints heads are only", "\n", "# applied to the top scoring box detections.", "\n", "pred_instances", "=", "self", ".", "forward_with_given_boxes", "(", "features", ",", "pred_instances", ")", "\n", "return", "pred_instances", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPStandardROIHeads.forward_with_given_boxes": [[461, 486], ["clip_roi_heads.CLIPStandardROIHeads._forward_mask", "instances[].has", "instances[].has"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has"], ["", "", "def", "forward_with_given_boxes", "(", "\n", "self", ",", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "instances", ":", "List", "[", "Instances", "]", "\n", ")", "->", "List", "[", "Instances", "]", ":", "\n", "        ", "\"\"\"\n        Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.\n\n        This is useful for downstream tasks where a box is known, but need to obtain\n        other attributes (outputs of other heads).\n        Test-time augmentation also uses this.\n\n        Args:\n            features: same as in `forward()`\n            instances (list[Instances]): instances to predict other outputs. Expect the keys\n                \"pred_boxes\" and \"pred_classes\" to exist.\n\n        Returns:\n            list[Instances]:\n                the same `Instances` objects, with extra\n                fields such as `pred_masks` or `pred_keypoints`.\n        \"\"\"", "\n", "assert", "not", "self", ".", "training", "\n", "assert", "instances", "[", "0", "]", ".", "has", "(", "\"pred_boxes\"", ")", "and", "instances", "[", "0", "]", ".", "has", "(", "\"pred_classes\"", ")", "\n", "\n", "instances", "=", "self", ".", "_forward_mask", "(", "features", ",", "instances", ")", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPStandardROIHeads._forward_box": [[487, 527], ["clip_roi_heads.CLIPStandardROIHeads.box_pooler", "clip_roi_heads.CLIPStandardROIHeads.box_predictor", "attnpool", "clip_roi_heads.CLIPStandardROIHeads.box_head", "clip_roi_heads.CLIPStandardROIHeads.box_predictor.losses", "clip_roi_heads.CLIPStandardROIHeads.box_predictor.inference", "torch.no_grad", "clip_roi_heads.CLIPStandardROIHeads.box_predictor.predict_boxes_for_gt_classes", "zip", "detectron2.structures.Boxes"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_boxes_for_gt_classes"], ["", "def", "_forward_box", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "proposals", ":", "List", "[", "Instances", "]", ",", "attnpool", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Forward logic of the box prediction branch. If `self.train_on_pred_boxes is True`,\n            the function puts predicted boxes in the `proposal_boxes` field of `proposals` argument.\n\n        Args:\n            features (dict[str, Tensor]): mapping from feature map names to tensor.\n                Same as in :meth:`ROIHeads.forward`.\n            proposals (list[Instances]): the per-image object proposals with\n                their matching ground truth.\n                Each has fields \"proposal_boxes\", and \"objectness_logits\",\n                \"gt_classes\", \"gt_boxes\".\n\n        Returns:\n            In training, a dict of losses.\n            In inference, a list of `Instances`, the predicted instances.\n        \"\"\"", "\n", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "box_in_features", "]", "\n", "box_features", "=", "self", ".", "box_pooler", "(", "features", ",", "[", "x", ".", "proposal_boxes", "for", "x", "in", "proposals", "]", ")", "\n", "if", "attnpool", ":", "# att pooling", "\n", "            ", "box_features", "=", "attnpool", "(", "box_features", ")", "\n", "", "else", ":", "# default FPN pooling (FastRCNNConvFCHead)", "\n", "            ", "box_features", "=", "self", ".", "box_head", "(", "box_features", ")", "\n", "", "predictions", "=", "self", ".", "box_predictor", "(", "box_features", ")", "\n", "del", "box_features", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "losses", "=", "self", ".", "box_predictor", ".", "losses", "(", "predictions", ",", "proposals", ")", "\n", "# proposals is modified in-place below, so losses must be computed first.", "\n", "if", "self", ".", "train_on_pred_boxes", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "pred_boxes", "=", "self", ".", "box_predictor", ".", "predict_boxes_for_gt_classes", "(", "\n", "predictions", ",", "proposals", "\n", ")", "\n", "for", "proposals_per_image", ",", "pred_boxes_per_image", "in", "zip", "(", "proposals", ",", "pred_boxes", ")", ":", "\n", "                        ", "proposals_per_image", ".", "proposal_boxes", "=", "Boxes", "(", "pred_boxes_per_image", ")", "\n", "", "", "", "return", "losses", "\n", "", "else", ":", "\n", "            ", "pred_instances", ",", "_", "=", "self", ".", "box_predictor", ".", "inference", "(", "predictions", ",", "proposals", ")", "\n", "return", "pred_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.clip_roi_heads.CLIPStandardROIHeads._forward_mask": [[528, 557], ["clip_roi_heads.CLIPStandardROIHeads.mask_head", "roi_heads.select_foreground_proposals", "clip_roi_heads.CLIPStandardROIHeads.mask_pooler"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.select_foreground_proposals"], ["", "", "def", "_forward_mask", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "instances", ":", "List", "[", "Instances", "]", ")", ":", "\n", "        ", "\"\"\"\n        Forward logic of the mask prediction branch.\n\n        Args:\n            features (dict[str, Tensor]): mapping from feature map names to tensor.\n                Same as in :meth:`ROIHeads.forward`.\n            instances (list[Instances]): the per-image instances to train/predict masks.\n                In training, they can be the proposals.\n                In inference, they can be the boxes predicted by R-CNN box head.\n\n        Returns:\n            In training, a dict of losses.\n            In inference, update `instances` with new fields \"pred_masks\" and return it.\n        \"\"\"", "\n", "if", "not", "self", ".", "mask_on", ":", "\n", "            ", "return", "{", "}", "if", "self", ".", "training", "else", "instances", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "# head is only trained on positive proposals.", "\n", "            ", "instances", ",", "_", "=", "select_foreground_proposals", "(", "instances", ",", "self", ".", "num_classes", ")", "\n", "\n", "", "if", "self", ".", "mask_pooler", "is", "not", "None", ":", "\n", "            ", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "mask_in_features", "]", "\n", "boxes", "=", "[", "x", ".", "proposal_boxes", "if", "self", ".", "training", "else", "x", ".", "pred_boxes", "for", "x", "in", "instances", "]", "\n", "features", "=", "self", ".", "mask_pooler", "(", "features", ",", "boxes", ")", "\n", "", "else", ":", "\n", "            ", "features", "=", "{", "f", ":", "features", "[", "f", "]", "for", "f", "in", "self", ".", "mask_in_features", "}", "\n", "", "return", "self", ".", "mask_head", "(", "features", ",", "instances", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads.__init__": [[138, 167], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "num_classes", ",", "\n", "batch_size_per_image", ",", "\n", "positive_fraction", ",", "\n", "proposal_matcher", ",", "\n", "proposal_append_gt", "=", "True", ",", "\n", "only_sample_fg_proposals", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            num_classes (int): number of foreground classes (i.e. background is not included)\n            batch_size_per_image (int): number of proposals to sample for training\n            positive_fraction (float): fraction of positive (foreground) proposals\n                to sample for training.\n            proposal_matcher (Matcher): matcher that matches proposals and ground truth\n            proposal_append_gt (bool): whether to include ground truth as proposals as well\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "batch_size_per_image", "=", "batch_size_per_image", "\n", "self", ".", "positive_fraction", "=", "positive_fraction", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "proposal_matcher", "=", "proposal_matcher", "\n", "self", ".", "proposal_append_gt", "=", "proposal_append_gt", "\n", "self", ".", "only_sample_fg_proposals", "=", "only_sample_fg_proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads.from_config": [[168, 182], ["matcher.Matcher"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "return", "{", "\n", "\"batch_size_per_image\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "BATCH_SIZE_PER_IMAGE", ",", "\n", "\"positive_fraction\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "POSITIVE_FRACTION", ",", "\n", "\"num_classes\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NUM_CLASSES", ",", "\n", "\"proposal_append_gt\"", ":", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "PROPOSAL_APPEND_GT", ",", "\n", "# Matcher to assign box proposals to gt boxes", "\n", "\"proposal_matcher\"", ":", "Matcher", "(", "\n", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IOU_THRESHOLDS", ",", "\n", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IOU_LABELS", ",", "\n", "allow_low_quality_matches", "=", "False", ",", "\n", ")", ",", "\n", "\"only_sample_fg_proposals\"", ":", "cfg", ".", "MODEL", ".", "CLIP", ".", "ONLY_SAMPLE_FG_PROPOSALS", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads._sample_proposals": [[184, 235], ["sampling.subsample_labels", "torch.cat", "gt_classes.numel", "torch.zeros_like", "int", "min", "torch.zeros_like", "detectron2.layers.nonzero_tuple", "positive.numel", "torch.randperm", "positive.numel"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.sampling.subsample_labels", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple"], ["", "def", "_sample_proposals", "(", "\n", "self", ",", "matched_idxs", ":", "torch", ".", "Tensor", ",", "matched_labels", ":", "torch", ".", "Tensor", ",", "gt_classes", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Based on the matching between N proposals and M groundtruth,\n        sample the proposals and set their classification labels.\n\n        Args:\n            matched_idxs (Tensor): a vector of length N, each is the best-matched\n                gt index in [0, M) for each proposal.\n            matched_labels (Tensor): a vector of length N, the matcher's label\n                (one of cfg.MODEL.ROI_HEADS.IOU_LABELS) for each proposal.\n            gt_classes (Tensor): a vector of length M.\n\n        Returns:\n            Tensor: a vector of indices of sampled proposals. Each is in [0, N).\n            Tensor: a vector of the same length, the classification label for\n                each sampled proposal. Each sample is labeled as either a category in\n                [0, num_classes) or the background (num_classes).\n        \"\"\"", "\n", "has_gt", "=", "gt_classes", ".", "numel", "(", ")", ">", "0", "\n", "# Get the corresponding GT for each proposal", "\n", "if", "has_gt", ":", "\n", "            ", "gt_classes", "=", "gt_classes", "[", "matched_idxs", "]", "\n", "# Label unmatched proposals (0 label from matcher) as background (label=num_classes)", "\n", "gt_classes", "[", "matched_labels", "==", "0", "]", "=", "self", ".", "num_classes", "\n", "# Label ignore proposals (-1 label)", "\n", "gt_classes", "[", "matched_labels", "==", "-", "1", "]", "=", "-", "1", "\n", "", "else", ":", "\n", "            ", "gt_classes", "=", "torch", ".", "zeros_like", "(", "matched_idxs", ")", "+", "self", ".", "num_classes", "\n", "\n", "# only sample fg proposals to train recognition branch (ref to subsample_labels)", "\n", "", "if", "self", ".", "only_sample_fg_proposals", ":", "\n", "            ", "if", "has_gt", ":", "\n", "                ", "positive", "=", "nonzero_tuple", "(", "(", "gt_classes", "!=", "-", "1", ")", "&", "(", "gt_classes", "!=", "self", ".", "num_classes", ")", ")", "[", "0", "]", "\n", "num_pos", "=", "int", "(", "self", ".", "batch_size_per_image", "*", "self", ".", "positive_fraction", ")", "\n", "# protect against not enough positive examples", "\n", "num_pos", "=", "min", "(", "positive", ".", "numel", "(", ")", ",", "num_pos", ")", "\n", "# randomly select positive and negative examples", "\n", "perm1", "=", "torch", ".", "randperm", "(", "positive", ".", "numel", "(", ")", ",", "device", "=", "positive", ".", "device", ")", "[", ":", "num_pos", "]", "\n", "sampled_idxs", "=", "positive", "[", "perm1", "]", "\n", "", "else", ":", "# no gt, only keep 1 bg proposal to fill the slot", "\n", "                ", "sampled_idxs", "=", "torch", ".", "zeros_like", "(", "matched_idxs", "[", "0", ":", "1", "]", ")", "\n", "", "return", "sampled_idxs", ",", "gt_classes", "[", "sampled_idxs", "]", "\n", "\n", "", "sampled_fg_idxs", ",", "sampled_bg_idxs", "=", "subsample_labels", "(", "\n", "gt_classes", ",", "self", ".", "batch_size_per_image", ",", "self", ".", "positive_fraction", ",", "self", ".", "num_classes", "\n", ")", "\n", "\n", "sampled_idxs", "=", "torch", ".", "cat", "(", "[", "sampled_fg_idxs", ",", "sampled_bg_idxs", "]", ",", "dim", "=", "0", ")", "\n", "return", "sampled_idxs", ",", "gt_classes", "[", "sampled_idxs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads.label_and_sample_proposals": [[236, 321], ["torch.no_grad", "zip", "detectron2.utils.events.get_event_storage", "detectron2.utils.events.get_event_storage.put_scalar", "detectron2.utils.events.get_event_storage.put_scalar", "proposal_generator.proposal_utils.add_ground_truth_to_proposals", "detectron2.structures.pairwise_iou", "roi_heads.ROIHeads.proposal_matcher", "roi_heads.ROIHeads._sample_proposals", "num_bg_samples.append", "num_fg_samples.append", "proposals_with_gt.append", "numpy.mean", "numpy.mean", "len", "targets_per_image.get_fields().items", "gt_classes.numel", "targets_per_image.get_fields", "trg_name.startswith", "proposals_per_image.set", "proposals_per_image.has"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.proposal_generator.proposal_utils.add_ground_truth_to_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads._sample_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "label_and_sample_proposals", "(", "\n", "self", ",", "proposals", ":", "List", "[", "Instances", "]", ",", "targets", ":", "List", "[", "Instances", "]", "\n", ")", "->", "List", "[", "Instances", "]", ":", "\n", "        ", "\"\"\"\n        Prepare some proposals to be used to train the ROI heads.\n        It performs box matching between `proposals` and `targets`, and assigns\n        training labels to the proposals.\n        It returns ``self.batch_size_per_image`` random samples from proposals and groundtruth\n        boxes, with a fraction of positives that is no larger than\n        ``self.positive_fraction``.\n\n        Args:\n            See :meth:`ROIHeads.forward`\n\n        Returns:\n            list[Instances]:\n                length `N` list of `Instances`s containing the proposals\n                sampled for training. Each `Instances` has the following fields:\n\n                - proposal_boxes: the proposal boxes\n                - gt_boxes: the ground-truth box that the proposal is assigned to\n                  (this is only meaningful if the proposal has a label > 0; if label = 0\n                  then the ground-truth box is random)\n\n                Other fields such as \"gt_classes\", \"gt_masks\", that's included in `targets`.\n        \"\"\"", "\n", "# Augment proposals with ground-truth boxes.", "\n", "# In the case of learned proposals (e.g., RPN), when training starts", "\n", "# the proposals will be low quality due to random initialization.", "\n", "# It's possible that none of these initial", "\n", "# proposals have high enough overlap with the gt objects to be used", "\n", "# as positive examples for the second stage components (box head,", "\n", "# cls head, mask head). Adding the gt boxes to the set of proposals", "\n", "# ensures that the second stage components will have some positive", "\n", "# examples from the start of training. For RPN, this augmentation improves", "\n", "# convergence and empirically improves box AP on COCO by about 0.5", "\n", "# points (under one tested configuration).", "\n", "if", "self", ".", "proposal_append_gt", ":", "\n", "            ", "proposals", "=", "add_ground_truth_to_proposals", "(", "targets", ",", "proposals", ")", "\n", "\n", "", "proposals_with_gt", "=", "[", "]", "\n", "\n", "num_fg_samples", "=", "[", "]", "\n", "num_bg_samples", "=", "[", "]", "\n", "for", "proposals_per_image", ",", "targets_per_image", "in", "zip", "(", "proposals", ",", "targets", ")", ":", "\n", "            ", "has_gt", "=", "len", "(", "targets_per_image", ")", ">", "0", "\n", "match_quality_matrix", "=", "pairwise_iou", "(", "\n", "targets_per_image", ".", "gt_boxes", ",", "proposals_per_image", ".", "proposal_boxes", "\n", ")", "\n", "matched_idxs", ",", "matched_labels", "=", "self", ".", "proposal_matcher", "(", "match_quality_matrix", ")", "\n", "sampled_idxs", ",", "gt_classes", "=", "self", ".", "_sample_proposals", "(", "\n", "matched_idxs", ",", "matched_labels", ",", "targets_per_image", ".", "gt_classes", "\n", ")", "\n", "\n", "# Set target attributes of the sampled proposals:", "\n", "proposals_per_image", "=", "proposals_per_image", "[", "sampled_idxs", "]", "\n", "proposals_per_image", ".", "gt_classes", "=", "gt_classes", "\n", "\n", "if", "has_gt", ":", "\n", "                ", "sampled_targets", "=", "matched_idxs", "[", "sampled_idxs", "]", "\n", "# We index all the attributes of targets that start with \"gt_\"", "\n", "# and have not been added to proposals yet (=\"gt_classes\").", "\n", "# NOTE: here the indexing waste some compute, because heads", "\n", "# like masks, keypoints, etc, will filter the proposals again,", "\n", "# (by foreground/background, or number of keypoints in the image, etc)", "\n", "# so we essentially index the data twice.", "\n", "for", "(", "trg_name", ",", "trg_value", ")", "in", "targets_per_image", ".", "get_fields", "(", ")", ".", "items", "(", ")", ":", "\n", "                    ", "if", "trg_name", ".", "startswith", "(", "\"gt_\"", ")", "and", "not", "proposals_per_image", ".", "has", "(", "trg_name", ")", ":", "\n", "                        ", "proposals_per_image", ".", "set", "(", "trg_name", ",", "trg_value", "[", "sampled_targets", "]", ")", "\n", "# If no GT is given in the image, we don't know what a dummy gt value can be.", "\n", "# Therefore the returned proposals won't have any gt_* fields, except for a", "\n", "# gt_classes full of background label.", "\n", "\n", "", "", "", "num_bg_samples", ".", "append", "(", "(", "gt_classes", "==", "self", ".", "num_classes", ")", ".", "sum", "(", ")", ".", "item", "(", ")", ")", "\n", "num_fg_samples", ".", "append", "(", "gt_classes", ".", "numel", "(", ")", "-", "num_bg_samples", "[", "-", "1", "]", ")", "\n", "proposals_with_gt", ".", "append", "(", "proposals_per_image", ")", "\n", "\n", "# Log the number of fg/bg samples that are selected for training ROI heads", "\n", "", "storage", "=", "get_event_storage", "(", ")", "\n", "storage", ".", "put_scalar", "(", "\"roi_head/num_fg_samples\"", ",", "np", ".", "mean", "(", "num_fg_samples", ")", ")", "\n", "storage", ".", "put_scalar", "(", "\"roi_head/num_bg_samples\"", ",", "np", ".", "mean", "(", "num_bg_samples", ")", ")", "\n", "#print(\"num_fg: {}; num_bg: {}\".format(num_fg_samples, num_bg_samples))", "\n", "\n", "return", "proposals_with_gt", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads.forward": [[322, 357], ["NotImplementedError"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "images", ":", "ImageList", ",", "\n", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "proposals", ":", "List", "[", "Instances", "]", ",", "\n", "targets", ":", "Optional", "[", "List", "[", "Instances", "]", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "Instances", "]", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        Args:\n            images (ImageList):\n            features (dict[str,Tensor]): input data as a mapping from feature\n                map name to tensor. Axis 0 represents the number of images `N` in\n                the input data; axes 1-3 are channels, height, and width, which may\n                vary between feature maps (e.g., if a feature pyramid is used).\n            proposals (list[Instances]): length `N` list of `Instances`. The i-th\n                `Instances` contains object proposals for the i-th input image,\n                with fields \"proposal_boxes\" and \"objectness_logits\".\n            targets (list[Instances], optional): length `N` list of `Instances`. The i-th\n                `Instances` contains the ground-truth per-instance annotations\n                for the i-th input image.  Specify `targets` during training only.\n                It may have the following fields:\n\n                - gt_boxes: the bounding box of each instance.\n                - gt_classes: the label for each instance with a category ranging in [0, #class].\n                - gt_masks: PolygonMasks or BitMasks, the ground-truth masks of each instance.\n                - gt_keypoints: NxKx3, the groud-truth keypoints for each instance.\n\n        Returns:\n            list[Instances]: length `N` list of `Instances` containing the\n            detected instances. Returned during inference only; may be [] during training.\n\n            dict[str->Tensor]:\n            mapping from a named loss to a tensor storing the loss. Used during training only.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads.__init__": [[368, 403], ["roi_heads.ROIHeads.__init__", "isinstance", "torch.nn.Sequential"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "in_features", ":", "List", "[", "str", "]", ",", "\n", "pooler", ":", "ROIPooler", ",", "\n", "res5", ":", "nn", ".", "Module", ",", "\n", "box_predictor", ":", "nn", ".", "Module", ",", "\n", "mask_head", ":", "Optional", "[", "nn", ".", "Module", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            in_features (list[str]): list of backbone feature map names to use for\n                feature extraction\n            pooler (ROIPooler): pooler to extra region features from backbone\n            res5 (nn.Sequential): a CNN to compute per-region features, to be used by\n                ``box_predictor`` and ``mask_head``. Typically this is a \"res5\"\n                block from a ResNet.\n            box_predictor (nn.Module): make box predictions from the feature.\n                Should have the same interface as :class:`FastRCNNOutputLayers`.\n            mask_head (nn.Module): transform features to make mask predictions\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "pooler", "=", "pooler", "\n", "if", "isinstance", "(", "res5", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "res5", "=", "nn", ".", "Sequential", "(", "*", "res5", ")", "\n", "", "self", ".", "res5", "=", "res5", "\n", "self", ".", "box_predictor", "=", "box_predictor", "\n", "self", ".", "mask_on", "=", "mask_head", "is", "not", "None", "\n", "if", "self", ".", "mask_on", ":", "\n", "            ", "self", ".", "mask_head", "=", "mask_head", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads.from_config": [[404, 445], ["roi_heads.ROIHeads.from_config", "poolers.ROIPooler", "cls._build_res5_block", "fast_rcnn.FastRCNNOutputLayers", "len", "inspect.ismethod", "logger.warning", "classmethod", "detectron2.layers.ShapeSpec", "mask_head.build_mask_head", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads._build_res5_block", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.build_mask_head"], ["", "", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "# fmt: off", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ")", "\n", "in_features", "=", "ret", "[", "\"in_features\"", "]", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "\n", "pooler_scales", "=", "(", "1.0", "/", "input_shape", "[", "in_features", "[", "0", "]", "]", ".", "stride", ",", ")", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "mask_on", "=", "cfg", ".", "MODEL", ".", "MASK_ON", "\n", "# fmt: on", "\n", "assert", "not", "cfg", ".", "MODEL", ".", "KEYPOINT_ON", "\n", "assert", "len", "(", "in_features", ")", "==", "1", "\n", "\n", "ret", "[", "\"pooler\"", "]", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "\n", "# Compatbility with old moco code. Might be useful.", "\n", "# See notes in StandardROIHeads.from_config", "\n", "if", "not", "inspect", ".", "ismethod", "(", "cls", ".", "_build_res5_block", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"The behavior of _build_res5_block may change. \"", "\n", "\"Please do not depend on private methods.\"", "\n", ")", "\n", "cls", ".", "_build_res5_block", "=", "classmethod", "(", "cls", ".", "_build_res5_block", ")", "\n", "\n", "", "ret", "[", "\"res5\"", "]", ",", "out_channels", "=", "cls", ".", "_build_res5_block", "(", "cfg", ")", "\n", "ret", "[", "\"box_predictor\"", "]", "=", "FastRCNNOutputLayers", "(", "\n", "cfg", ",", "ShapeSpec", "(", "channels", "=", "out_channels", ",", "height", "=", "1", ",", "width", "=", "1", ")", "\n", ")", "\n", "\n", "if", "mask_on", ":", "\n", "            ", "ret", "[", "\"mask_head\"", "]", "=", "build_mask_head", "(", "\n", "cfg", ",", "\n", "ShapeSpec", "(", "channels", "=", "out_channels", ",", "width", "=", "pooler_resolution", ",", "height", "=", "pooler_resolution", ")", ",", "\n", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads._build_res5_block": [[446, 472], ["backbone.resnet.ResNet.make_stage", "torch.nn.Sequential"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.make_stage"], ["", "@", "classmethod", "\n", "def", "_build_res5_block", "(", "cls", ",", "cfg", ")", ":", "\n", "# fmt: off", "\n", "        ", "stage_channel_factor", "=", "2", "**", "3", "# res5 is 8x res2", "\n", "num_groups", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "NUM_GROUPS", "\n", "width_per_group", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "WIDTH_PER_GROUP", "\n", "bottleneck_channels", "=", "num_groups", "*", "width_per_group", "*", "stage_channel_factor", "\n", "out_channels", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "RES2_OUT_CHANNELS", "*", "stage_channel_factor", "\n", "stride_in_1x1", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "STRIDE_IN_1X1", "\n", "norm", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "NORM", "\n", "assert", "not", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEFORM_ON_PER_STAGE", "[", "-", "1", "]", ",", "\"Deformable conv is not yet supported in res5 head.\"", "\n", "# fmt: on", "\n", "\n", "blocks", "=", "ResNet", ".", "make_stage", "(", "\n", "BottleneckBlock", ",", "\n", "3", ",", "\n", "stride_per_block", "=", "[", "2", ",", "1", ",", "1", "]", ",", "\n", "in_channels", "=", "out_channels", "//", "2", ",", "\n", "bottleneck_channels", "=", "bottleneck_channels", ",", "\n", "out_channels", "=", "out_channels", ",", "\n", "num_groups", "=", "num_groups", ",", "\n", "norm", "=", "norm", ",", "\n", "stride_in_1x1", "=", "stride_in_1x1", ",", "\n", ")", "\n", "return", "nn", ".", "Sequential", "(", "*", "blocks", ")", ",", "out_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads._shared_roi_transform": [[473, 476], ["roi_heads.Res5ROIHeads.pooler", "roi_heads.Res5ROIHeads.res5"], "methods", ["None"], ["", "def", "_shared_roi_transform", "(", "self", ",", "features", ",", "boxes", ")", ":", "\n", "        ", "x", "=", "self", ".", "pooler", "(", "features", ",", "boxes", ")", "\n", "return", "self", ".", "res5", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads.forward": [[477, 513], ["roi_heads.Res5ROIHeads._shared_roi_transform", "roi_heads.Res5ROIHeads.box_predictor", "roi_heads.Res5ROIHeads.label_and_sample_proposals", "roi_heads.Res5ROIHeads.mean", "roi_heads.Res5ROIHeads.box_predictor.losses", "roi_heads.Res5ROIHeads.box_predictor.inference", "roi_heads.Res5ROIHeads.forward_with_given_boxes", "roi_heads.select_foreground_proposals", "roi_heads.Res5ROIHeads.update", "roi_heads.Res5ROIHeads.mask_head", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads._shared_roi_transform", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads.label_and_sample_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward_with_given_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.select_foreground_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "forward", "(", "self", ",", "images", ",", "features", ",", "proposals", ",", "targets", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        See :meth:`ROIHeads.forward`.\n        \"\"\"", "\n", "del", "images", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "assert", "targets", "\n", "proposals", "=", "self", ".", "label_and_sample_proposals", "(", "proposals", ",", "targets", ")", "\n", "", "del", "targets", "\n", "\n", "proposal_boxes", "=", "[", "x", ".", "proposal_boxes", "for", "x", "in", "proposals", "]", "\n", "box_features", "=", "self", ".", "_shared_roi_transform", "(", "\n", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "in_features", "]", ",", "proposal_boxes", "\n", ")", "\n", "predictions", "=", "self", ".", "box_predictor", "(", "box_features", ".", "mean", "(", "dim", "=", "[", "2", ",", "3", "]", ")", ")", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "del", "features", "\n", "losses", "=", "self", ".", "box_predictor", ".", "losses", "(", "predictions", ",", "proposals", ")", "\n", "if", "self", ".", "mask_on", ":", "\n", "                ", "proposals", ",", "fg_selection_masks", "=", "select_foreground_proposals", "(", "\n", "proposals", ",", "self", ".", "num_classes", "\n", ")", "\n", "# Since the ROI feature transform is shared between boxes and masks,", "\n", "# we don't need to recompute features. The mask loss is only defined", "\n", "# on foreground proposals, so we need to select out the foreground", "\n", "# features.", "\n", "mask_features", "=", "box_features", "[", "torch", ".", "cat", "(", "fg_selection_masks", ",", "dim", "=", "0", ")", "]", "\n", "del", "box_features", "\n", "losses", ".", "update", "(", "self", ".", "mask_head", "(", "mask_features", ",", "proposals", ")", ")", "\n", "", "return", "[", "]", ",", "losses", "\n", "", "else", ":", "\n", "            ", "pred_instances", ",", "_", "=", "self", ".", "box_predictor", ".", "inference", "(", "predictions", ",", "proposals", ")", "\n", "pred_instances", "=", "self", ".", "forward_with_given_boxes", "(", "features", ",", "pred_instances", ")", "\n", "return", "pred_instances", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads.forward_with_given_boxes": [[514, 537], ["instances[].has", "instances[].has", "roi_heads.Res5ROIHeads._shared_roi_transform", "roi_heads.Res5ROIHeads.mask_head"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.Res5ROIHeads._shared_roi_transform"], ["", "", "def", "forward_with_given_boxes", "(", "self", ",", "features", ",", "instances", ")", ":", "\n", "        ", "\"\"\"\n        Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.\n\n        Args:\n            features: same as in `forward()`\n            instances (list[Instances]): instances to predict other outputs. Expect the keys\n                \"pred_boxes\" and \"pred_classes\" to exist.\n\n        Returns:\n            instances (Instances):\n                the same `Instances` object, with extra\n                fields such as `pred_masks` or `pred_keypoints`.\n        \"\"\"", "\n", "assert", "not", "self", ".", "training", "\n", "assert", "instances", "[", "0", "]", ".", "has", "(", "\"pred_boxes\"", ")", "and", "instances", "[", "0", "]", ".", "has", "(", "\"pred_classes\"", ")", "\n", "\n", "if", "self", ".", "mask_on", ":", "\n", "            ", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "in_features", "]", "\n", "x", "=", "self", ".", "_shared_roi_transform", "(", "features", ",", "[", "x", ".", "pred_boxes", "for", "x", "in", "instances", "]", ")", "\n", "return", "self", ".", "mask_head", "(", "x", ",", "instances", ")", "\n", "", "else", ":", "\n", "            ", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.__init__": [[552, 609], ["roi_heads.ROIHeads.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "box_in_features", ":", "List", "[", "str", "]", ",", "\n", "box_pooler", ":", "ROIPooler", ",", "\n", "box_head", ":", "nn", ".", "Module", ",", "\n", "box_predictor", ":", "nn", ".", "Module", ",", "\n", "mask_in_features", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "mask_pooler", ":", "Optional", "[", "ROIPooler", "]", "=", "None", ",", "\n", "mask_head", ":", "Optional", "[", "nn", ".", "Module", "]", "=", "None", ",", "\n", "keypoint_in_features", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "keypoint_pooler", ":", "Optional", "[", "ROIPooler", "]", "=", "None", ",", "\n", "keypoint_head", ":", "Optional", "[", "nn", ".", "Module", "]", "=", "None", ",", "\n", "train_on_pred_boxes", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            box_in_features (list[str]): list of feature names to use for the box head.\n            box_pooler (ROIPooler): pooler to extra region features for box head\n            box_head (nn.Module): transform features to make box predictions\n            box_predictor (nn.Module): make box predictions from the feature.\n                Should have the same interface as :class:`FastRCNNOutputLayers`.\n            mask_in_features (list[str]): list of feature names to use for the mask\n                pooler or mask head. None if not using mask head.\n            mask_pooler (ROIPooler): pooler to extract region features from image features.\n                The mask head will then take region features to make predictions.\n                If None, the mask head will directly take the dict of image features\n                defined by `mask_in_features`\n            mask_head (nn.Module): transform features to make mask predictions\n            keypoint_in_features, keypoint_pooler, keypoint_head: similar to ``mask_*``.\n            train_on_pred_boxes (bool): whether to use proposal boxes or\n                predicted boxes from the box head to train other heads.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "# keep self.in_features for backward compatibility", "\n", "self", ".", "in_features", "=", "self", ".", "box_in_features", "=", "box_in_features", "\n", "self", ".", "box_pooler", "=", "box_pooler", "\n", "self", ".", "box_head", "=", "box_head", "\n", "self", ".", "box_predictor", "=", "box_predictor", "\n", "\n", "self", ".", "mask_on", "=", "mask_in_features", "is", "not", "None", "\n", "if", "self", ".", "mask_on", ":", "\n", "            ", "self", ".", "mask_in_features", "=", "mask_in_features", "\n", "self", ".", "mask_pooler", "=", "mask_pooler", "\n", "self", ".", "mask_head", "=", "mask_head", "\n", "\n", "", "self", ".", "keypoint_on", "=", "keypoint_in_features", "is", "not", "None", "\n", "if", "self", ".", "keypoint_on", ":", "\n", "            ", "self", ".", "keypoint_in_features", "=", "keypoint_in_features", "\n", "self", ".", "keypoint_pooler", "=", "keypoint_pooler", "\n", "self", ".", "keypoint_head", "=", "keypoint_head", "\n", "\n", "", "self", ".", "train_on_pred_boxes", "=", "train_on_pred_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.from_config": [[610, 626], ["roi_heads.ROIHeads.from_config", "inspect.ismethod", "inspect.ismethod", "inspect.ismethod", "super().from_config.update", "super().from_config.update", "super().from_config.update", "cls._init_box_head", "cls._init_mask_head", "cls._init_keypoint_head"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._init_box_head", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._init_mask_head", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._init_keypoint_head"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ")", "\n", "ret", "[", "\"train_on_pred_boxes\"", "]", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "TRAIN_ON_PRED_BOXES", "\n", "# Subclasses that have not been updated to use from_config style construction", "\n", "# may have overridden _init_*_head methods. In this case, those overridden methods", "\n", "# will not be classmethods and we need to avoid trying to call them here.", "\n", "# We test for this with ismethod which only returns True for bound methods of cls.", "\n", "# Such subclasses will need to handle calling their overridden _init_*_head methods.", "\n", "if", "inspect", ".", "ismethod", "(", "cls", ".", "_init_box_head", ")", ":", "\n", "            ", "ret", ".", "update", "(", "cls", ".", "_init_box_head", "(", "cfg", ",", "input_shape", ")", ")", "\n", "", "if", "inspect", ".", "ismethod", "(", "cls", ".", "_init_mask_head", ")", ":", "\n", "            ", "ret", ".", "update", "(", "cls", ".", "_init_mask_head", "(", "cfg", ",", "input_shape", ")", ")", "\n", "", "if", "inspect", ".", "ismethod", "(", "cls", ".", "_init_keypoint_head", ")", ":", "\n", "            ", "ret", ".", "update", "(", "cls", ".", "_init_keypoint_head", "(", "cfg", ",", "input_shape", ")", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._init_box_head": [[627, 662], ["tuple", "poolers.ROIPooler", "box_head.build_box_head.build_box_head", "fast_rcnn.FastRCNNOutputLayers", "len", "detectron2.layers.ShapeSpec", "set"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.box_head.build_box_head", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "@", "classmethod", "\n", "def", "_init_box_head", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "# fmt: off", "\n", "        ", "in_features", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_scales", "=", "tuple", "(", "1.0", "/", "input_shape", "[", "k", "]", ".", "stride", "for", "k", "in", "in_features", ")", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "\n", "# fmt: on", "\n", "\n", "# If StandardROIHeads is applied on multiple feature maps (as in FPN),", "\n", "# then we share the same predictors and therefore the channel counts must be the same", "\n", "in_channels", "=", "[", "input_shape", "[", "f", "]", ".", "channels", "for", "f", "in", "in_features", "]", "\n", "# Check all channel counts are equal", "\n", "assert", "len", "(", "set", "(", "in_channels", ")", ")", "==", "1", ",", "in_channels", "\n", "in_channels", "=", "in_channels", "[", "0", "]", "\n", "\n", "box_pooler", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "# Here we split \"box head\" and \"box predictor\", which is mainly due to historical reasons.", "\n", "# They are used together so the \"box predictor\" layers should be part of the \"box head\".", "\n", "# New subclasses of ROIHeads do not need \"box predictor\"s.", "\n", "box_head", "=", "build_box_head", "(", "\n", "cfg", ",", "ShapeSpec", "(", "channels", "=", "in_channels", ",", "height", "=", "pooler_resolution", ",", "width", "=", "pooler_resolution", ")", "\n", ")", "\n", "box_predictor", "=", "FastRCNNOutputLayers", "(", "cfg", ",", "box_head", ".", "output_shape", ")", "\n", "return", "{", "\n", "\"box_in_features\"", ":", "in_features", ",", "\n", "\"box_pooler\"", ":", "box_pooler", ",", "\n", "\"box_head\"", ":", "box_head", ",", "\n", "\"box_predictor\"", ":", "box_predictor", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._init_mask_head": [[664, 697], ["tuple", "mask_head.build_mask_head", "poolers.ROIPooler", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.build_mask_head"], ["", "@", "classmethod", "\n", "def", "_init_mask_head", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "if", "not", "cfg", ".", "MODEL", ".", "MASK_ON", ":", "\n", "            ", "return", "{", "}", "\n", "# fmt: off", "\n", "", "in_features", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_scales", "=", "tuple", "(", "1.0", "/", "input_shape", "[", "k", "]", ".", "stride", "for", "k", "in", "in_features", ")", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_TYPE", "\n", "# fmt: on", "\n", "\n", "in_channels", "=", "[", "input_shape", "[", "f", "]", ".", "channels", "for", "f", "in", "in_features", "]", "[", "0", "]", "\n", "\n", "ret", "=", "{", "\"mask_in_features\"", ":", "in_features", "}", "\n", "ret", "[", "\"mask_pooler\"", "]", "=", "(", "\n", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "if", "pooler_type", "\n", "else", "None", "\n", ")", "\n", "if", "pooler_type", ":", "\n", "            ", "shape", "=", "ShapeSpec", "(", "\n", "channels", "=", "in_channels", ",", "width", "=", "pooler_resolution", ",", "height", "=", "pooler_resolution", "\n", ")", "\n", "", "else", ":", "\n", "            ", "shape", "=", "{", "f", ":", "input_shape", "[", "f", "]", "for", "f", "in", "in_features", "}", "\n", "", "ret", "[", "\"mask_head\"", "]", "=", "build_mask_head", "(", "cfg", ",", "shape", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._init_keypoint_head": [[698, 731], ["tuple", "keypoint_head.build_keypoint_head", "poolers.ROIPooler", "detectron2.layers.ShapeSpec"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.build_keypoint_head"], ["", "@", "classmethod", "\n", "def", "_init_keypoint_head", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "if", "not", "cfg", ".", "MODEL", ".", "KEYPOINT_ON", ":", "\n", "            ", "return", "{", "}", "\n", "# fmt: off", "\n", "", "in_features", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_scales", "=", "tuple", "(", "1.0", "/", "input_shape", "[", "k", "]", ".", "stride", "for", "k", "in", "in_features", ")", "# noqa", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "POOLER_TYPE", "\n", "# fmt: on", "\n", "\n", "in_channels", "=", "[", "input_shape", "[", "f", "]", ".", "channels", "for", "f", "in", "in_features", "]", "[", "0", "]", "\n", "\n", "ret", "=", "{", "\"keypoint_in_features\"", ":", "in_features", "}", "\n", "ret", "[", "\"keypoint_pooler\"", "]", "=", "(", "\n", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "if", "pooler_type", "\n", "else", "None", "\n", ")", "\n", "if", "pooler_type", ":", "\n", "            ", "shape", "=", "ShapeSpec", "(", "\n", "channels", "=", "in_channels", ",", "width", "=", "pooler_resolution", ",", "height", "=", "pooler_resolution", "\n", ")", "\n", "", "else", ":", "\n", "            ", "shape", "=", "{", "f", ":", "input_shape", "[", "f", "]", "for", "f", "in", "in_features", "}", "\n", "", "ret", "[", "\"keypoint_head\"", "]", "=", "build_keypoint_head", "(", "cfg", ",", "shape", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward": [[732, 762], ["roi_heads.StandardROIHeads.label_and_sample_proposals", "roi_heads.StandardROIHeads._forward_box", "roi_heads.StandardROIHeads.update", "roi_heads.StandardROIHeads.update", "roi_heads.StandardROIHeads._forward_box", "roi_heads.StandardROIHeads.forward_with_given_boxes", "roi_heads.StandardROIHeads._forward_mask", "roi_heads.StandardROIHeads._forward_keypoint"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads.label_and_sample_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._forward_box", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._forward_box", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward_with_given_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_keypoint"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "images", ":", "ImageList", ",", "\n", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "proposals", ":", "List", "[", "Instances", "]", ",", "\n", "targets", ":", "Optional", "[", "List", "[", "Instances", "]", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "Instances", "]", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        See :class:`ROIHeads.forward`.\n        \"\"\"", "\n", "del", "images", "\n", "if", "self", ".", "training", ":", "\n", "            ", "assert", "targets", ",", "\"'targets' argument is required during training\"", "\n", "proposals", "=", "self", ".", "label_and_sample_proposals", "(", "proposals", ",", "targets", ")", "\n", "", "del", "targets", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "losses", "=", "self", ".", "_forward_box", "(", "features", ",", "proposals", ")", "\n", "# Usually the original proposals used by the box head are used by the mask, keypoint", "\n", "# heads. But when `self.train_on_pred_boxes is True`, proposals will contain boxes", "\n", "# predicted by the box head.", "\n", "losses", ".", "update", "(", "self", ".", "_forward_mask", "(", "features", ",", "proposals", ")", ")", "\n", "losses", ".", "update", "(", "self", ".", "_forward_keypoint", "(", "features", ",", "proposals", ")", ")", "\n", "return", "proposals", ",", "losses", "\n", "", "else", ":", "\n", "            ", "pred_instances", "=", "self", ".", "_forward_box", "(", "features", ",", "proposals", ")", "\n", "# During inference cascaded prediction is used: the mask and keypoints heads are only", "\n", "# applied to the top scoring box detections.", "\n", "pred_instances", "=", "self", ".", "forward_with_given_boxes", "(", "features", ",", "pred_instances", ")", "\n", "return", "pred_instances", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward_with_given_boxes": [[763, 789], ["roi_heads.StandardROIHeads._forward_mask", "roi_heads.StandardROIHeads._forward_keypoint", "instances[].has", "instances[].has"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_keypoint", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has"], ["", "", "def", "forward_with_given_boxes", "(", "\n", "self", ",", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "instances", ":", "List", "[", "Instances", "]", "\n", ")", "->", "List", "[", "Instances", "]", ":", "\n", "        ", "\"\"\"\n        Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.\n\n        This is useful for downstream tasks where a box is known, but need to obtain\n        other attributes (outputs of other heads).\n        Test-time augmentation also uses this.\n\n        Args:\n            features: same as in `forward()`\n            instances (list[Instances]): instances to predict other outputs. Expect the keys\n                \"pred_boxes\" and \"pred_classes\" to exist.\n\n        Returns:\n            list[Instances]:\n                the same `Instances` objects, with extra\n                fields such as `pred_masks` or `pred_keypoints`.\n        \"\"\"", "\n", "assert", "not", "self", ".", "training", "\n", "assert", "instances", "[", "0", "]", ".", "has", "(", "\"pred_boxes\"", ")", "and", "instances", "[", "0", "]", ".", "has", "(", "\"pred_classes\"", ")", "\n", "\n", "instances", "=", "self", ".", "_forward_mask", "(", "features", ",", "instances", ")", "\n", "instances", "=", "self", ".", "_forward_keypoint", "(", "features", ",", "instances", ")", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_box": [[790, 827], ["roi_heads.StandardROIHeads.box_pooler", "roi_heads.StandardROIHeads.box_head", "roi_heads.StandardROIHeads.box_predictor", "roi_heads.StandardROIHeads.box_predictor.losses", "roi_heads.StandardROIHeads.box_predictor.inference", "torch.no_grad", "roi_heads.StandardROIHeads.box_predictor.predict_boxes_for_gt_classes", "zip", "detectron2.structures.Boxes"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_boxes_for_gt_classes"], ["", "def", "_forward_box", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "proposals", ":", "List", "[", "Instances", "]", ")", ":", "\n", "        ", "\"\"\"\n        Forward logic of the box prediction branch. If `self.train_on_pred_boxes is True`,\n            the function puts predicted boxes in the `proposal_boxes` field of `proposals` argument.\n\n        Args:\n            features (dict[str, Tensor]): mapping from feature map names to tensor.\n                Same as in :meth:`ROIHeads.forward`.\n            proposals (list[Instances]): the per-image object proposals with\n                their matching ground truth.\n                Each has fields \"proposal_boxes\", and \"objectness_logits\",\n                \"gt_classes\", \"gt_boxes\".\n\n        Returns:\n            In training, a dict of losses.\n            In inference, a list of `Instances`, the predicted instances.\n        \"\"\"", "\n", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "box_in_features", "]", "\n", "box_features", "=", "self", ".", "box_pooler", "(", "features", ",", "[", "x", ".", "proposal_boxes", "for", "x", "in", "proposals", "]", ")", "\n", "box_features", "=", "self", ".", "box_head", "(", "box_features", ")", "\n", "predictions", "=", "self", ".", "box_predictor", "(", "box_features", ")", "\n", "del", "box_features", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "losses", "=", "self", ".", "box_predictor", ".", "losses", "(", "predictions", ",", "proposals", ")", "\n", "# proposals is modified in-place below, so losses must be computed first.", "\n", "if", "self", ".", "train_on_pred_boxes", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "pred_boxes", "=", "self", ".", "box_predictor", ".", "predict_boxes_for_gt_classes", "(", "\n", "predictions", ",", "proposals", "\n", ")", "\n", "for", "proposals_per_image", ",", "pred_boxes_per_image", "in", "zip", "(", "proposals", ",", "pred_boxes", ")", ":", "\n", "                        ", "proposals_per_image", ".", "proposal_boxes", "=", "Boxes", "(", "pred_boxes_per_image", ")", "\n", "", "", "", "return", "losses", "\n", "", "else", ":", "\n", "            ", "pred_instances", ",", "_", "=", "self", ".", "box_predictor", ".", "inference", "(", "predictions", ",", "proposals", ")", "\n", "return", "pred_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_mask": [[828, 857], ["roi_heads.StandardROIHeads.mask_head", "roi_heads.select_foreground_proposals", "roi_heads.StandardROIHeads.mask_pooler"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.select_foreground_proposals"], ["", "", "def", "_forward_mask", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "instances", ":", "List", "[", "Instances", "]", ")", ":", "\n", "        ", "\"\"\"\n        Forward logic of the mask prediction branch.\n\n        Args:\n            features (dict[str, Tensor]): mapping from feature map names to tensor.\n                Same as in :meth:`ROIHeads.forward`.\n            instances (list[Instances]): the per-image instances to train/predict masks.\n                In training, they can be the proposals.\n                In inference, they can be the boxes predicted by R-CNN box head.\n\n        Returns:\n            In training, a dict of losses.\n            In inference, update `instances` with new fields \"pred_masks\" and return it.\n        \"\"\"", "\n", "if", "not", "self", ".", "mask_on", ":", "\n", "            ", "return", "{", "}", "if", "self", ".", "training", "else", "instances", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "# head is only trained on positive proposals.", "\n", "            ", "instances", ",", "_", "=", "select_foreground_proposals", "(", "instances", ",", "self", ".", "num_classes", ")", "\n", "\n", "", "if", "self", ".", "mask_pooler", "is", "not", "None", ":", "\n", "            ", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "mask_in_features", "]", "\n", "boxes", "=", "[", "x", ".", "proposal_boxes", "if", "self", ".", "training", "else", "x", ".", "pred_boxes", "for", "x", "in", "instances", "]", "\n", "features", "=", "self", ".", "mask_pooler", "(", "features", ",", "boxes", ")", "\n", "", "else", ":", "\n", "            ", "features", "=", "{", "f", ":", "features", "[", "f", "]", "for", "f", "in", "self", ".", "mask_in_features", "}", "\n", "", "return", "self", ".", "mask_head", "(", "features", ",", "instances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_keypoint": [[858, 888], ["roi_heads.StandardROIHeads.keypoint_head", "roi_heads.select_foreground_proposals", "roi_heads.select_proposals_with_visible_keypoints", "roi_heads.StandardROIHeads.keypoint_pooler"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.select_foreground_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.select_proposals_with_visible_keypoints"], ["", "def", "_forward_keypoint", "(", "self", ",", "features", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "instances", ":", "List", "[", "Instances", "]", ")", ":", "\n", "        ", "\"\"\"\n        Forward logic of the keypoint prediction branch.\n\n        Args:\n            features (dict[str, Tensor]): mapping from feature map names to tensor.\n                Same as in :meth:`ROIHeads.forward`.\n            instances (list[Instances]): the per-image instances to train/predict keypoints.\n                In training, they can be the proposals.\n                In inference, they can be the boxes predicted by R-CNN box head.\n\n        Returns:\n            In training, a dict of losses.\n            In inference, update `instances` with new fields \"pred_keypoints\" and return it.\n        \"\"\"", "\n", "if", "not", "self", ".", "keypoint_on", ":", "\n", "            ", "return", "{", "}", "if", "self", ".", "training", "else", "instances", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "# head is only trained on positive proposals with >=1 visible keypoints.", "\n", "            ", "instances", ",", "_", "=", "select_foreground_proposals", "(", "instances", ",", "self", ".", "num_classes", ")", "\n", "instances", "=", "select_proposals_with_visible_keypoints", "(", "instances", ")", "\n", "\n", "", "if", "self", ".", "keypoint_pooler", "is", "not", "None", ":", "\n", "            ", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "keypoint_in_features", "]", "\n", "boxes", "=", "[", "x", ".", "proposal_boxes", "if", "self", ".", "training", "else", "x", ".", "pred_boxes", "for", "x", "in", "instances", "]", "\n", "features", "=", "self", ".", "keypoint_pooler", "(", "features", ",", "boxes", ")", "\n", "", "else", ":", "\n", "            ", "features", "=", "{", "f", ":", "features", "[", "f", "]", "for", "f", "in", "self", ".", "keypoint_in_features", "}", "\n", "", "return", "self", ".", "keypoint_head", "(", "features", ",", "instances", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.build_roi_heads": [[38, 44], ["ROI_HEADS_REGISTRY.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "build_roi_heads", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Build ROIHeads defined by `cfg.MODEL.ROI_HEADS.NAME`.\n    \"\"\"", "\n", "name", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NAME", "\n", "return", "ROI_HEADS_REGISTRY", ".", "get", "(", "name", ")", "(", "cfg", ",", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.select_foreground_proposals": [[46, 76], ["isinstance", "isinstance", "proposals[].has", "fg_selection_mask.nonzero().squeeze", "fg_proposals.append", "fg_selection_masks.append", "fg_selection_mask.nonzero"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has"], ["", "def", "select_foreground_proposals", "(", "\n", "proposals", ":", "List", "[", "Instances", "]", ",", "bg_label", ":", "int", "\n", ")", "->", "Tuple", "[", "List", "[", "Instances", "]", ",", "List", "[", "torch", ".", "Tensor", "]", "]", ":", "\n", "    ", "\"\"\"\n    Given a list of N Instances (for N images), each containing a `gt_classes` field,\n    return a list of Instances that contain only instances with `gt_classes != -1 &&\n    gt_classes != bg_label`.\n\n    Args:\n        proposals (list[Instances]): A list of N Instances, where N is the number of\n            images in the batch.\n        bg_label: label index of background class.\n\n    Returns:\n        list[Instances]: N Instances, each contains only the selected foreground instances.\n        list[Tensor]: N boolean vector, correspond to the selection mask of\n            each Instances object. True for selected instances.\n    \"\"\"", "\n", "assert", "isinstance", "(", "proposals", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "isinstance", "(", "proposals", "[", "0", "]", ",", "Instances", ")", "\n", "assert", "proposals", "[", "0", "]", ".", "has", "(", "\"gt_classes\"", ")", "\n", "fg_proposals", "=", "[", "]", "\n", "fg_selection_masks", "=", "[", "]", "\n", "for", "proposals_per_image", "in", "proposals", ":", "\n", "        ", "gt_classes", "=", "proposals_per_image", ".", "gt_classes", "\n", "fg_selection_mask", "=", "(", "gt_classes", "!=", "-", "1", ")", "&", "(", "gt_classes", "!=", "bg_label", ")", "\n", "fg_idxs", "=", "fg_selection_mask", ".", "nonzero", "(", ")", ".", "squeeze", "(", "1", ")", "\n", "fg_proposals", ".", "append", "(", "proposals_per_image", "[", "fg_idxs", "]", ")", "\n", "fg_selection_masks", ".", "append", "(", "fg_selection_mask", ")", "\n", "", "return", "fg_proposals", ",", "fg_selection_masks", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.select_proposals_with_visible_keypoints": [[78, 121], ["detectron2.utils.events.get_event_storage", "detectron2.utils.events.get_event_storage.put_scalar", "proposals_per_image.proposal_boxes.tensor.unsqueeze", "all_num_fg.append", "ret.append", "numpy.mean", "len", "ret.append", "detectron2.layers.nonzero_tuple", "selection_idxs.numel"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple"], ["", "def", "select_proposals_with_visible_keypoints", "(", "proposals", ":", "List", "[", "Instances", "]", ")", "->", "List", "[", "Instances", "]", ":", "\n", "    ", "\"\"\"\n    Args:\n        proposals (list[Instances]): a list of N Instances, where N is the\n            number of images.\n\n    Returns:\n        proposals: only contains proposals with at least one visible keypoint.\n\n    Note that this is still slightly different from Detectron.\n    In Detectron, proposals for training keypoint head are re-sampled from\n    all the proposals with IOU>threshold & >=1 visible keypoint.\n\n    Here, the proposals are first sampled from all proposals with\n    IOU>threshold, then proposals with no visible keypoint are filtered out.\n    This strategy seems to make no difference on Detectron and is easier to implement.\n    \"\"\"", "\n", "ret", "=", "[", "]", "\n", "all_num_fg", "=", "[", "]", "\n", "for", "proposals_per_image", "in", "proposals", ":", "\n", "# If empty/unannotated image (hard negatives), skip filtering for train", "\n", "        ", "if", "len", "(", "proposals_per_image", ")", "==", "0", ":", "\n", "            ", "ret", ".", "append", "(", "proposals_per_image", ")", "\n", "continue", "\n", "", "gt_keypoints", "=", "proposals_per_image", ".", "gt_keypoints", ".", "tensor", "\n", "# #fg x K x 3", "\n", "vis_mask", "=", "gt_keypoints", "[", ":", ",", ":", ",", "2", "]", ">=", "1", "\n", "xs", ",", "ys", "=", "gt_keypoints", "[", ":", ",", ":", ",", "0", "]", ",", "gt_keypoints", "[", ":", ",", ":", ",", "1", "]", "\n", "proposal_boxes", "=", "proposals_per_image", ".", "proposal_boxes", ".", "tensor", ".", "unsqueeze", "(", "dim", "=", "1", ")", "# #fg x 1 x 4", "\n", "kp_in_box", "=", "(", "\n", "(", "xs", ">=", "proposal_boxes", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "&", "(", "xs", "<=", "proposal_boxes", "[", ":", ",", ":", ",", "2", "]", ")", "\n", "&", "(", "ys", ">=", "proposal_boxes", "[", ":", ",", ":", ",", "1", "]", ")", "\n", "&", "(", "ys", "<=", "proposal_boxes", "[", ":", ",", ":", ",", "3", "]", ")", "\n", ")", "\n", "selection", "=", "(", "kp_in_box", "&", "vis_mask", ")", ".", "any", "(", "dim", "=", "1", ")", "\n", "selection_idxs", "=", "nonzero_tuple", "(", "selection", ")", "[", "0", "]", "\n", "all_num_fg", ".", "append", "(", "selection_idxs", ".", "numel", "(", ")", ")", "\n", "ret", ".", "append", "(", "proposals_per_image", "[", "selection_idxs", "]", ")", "\n", "\n", "", "storage", "=", "get_event_storage", "(", ")", "\n", "storage", ".", "put_scalar", "(", "\"keypoint_head/num_fg_samples\"", ",", "np", ".", "mean", "(", "all_num_fg", ")", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.BaseKeypointRCNNHead.__init__": [[141, 159], ["torch.nn.Module.__init__", "isinstance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "self", ",", "*", ",", "num_keypoints", ",", "loss_weight", "=", "1.0", ",", "loss_normalizer", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            num_keypoints (int): number of keypoints to predict\n            loss_weight (float): weight to multiple on the keypoint loss\n            loss_normalizer (float or str):\n                If float, divide the loss by `loss_normalizer * #images`.\n                If 'visible', the loss is normalized by the total number of\n                visible keypoints across images.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_keypoints", "=", "num_keypoints", "\n", "self", ".", "loss_weight", "=", "loss_weight", "\n", "assert", "loss_normalizer", "==", "\"visible\"", "or", "isinstance", "(", "loss_normalizer", ",", "float", ")", ",", "loss_normalizer", "\n", "self", ".", "loss_normalizer", "=", "loss_normalizer", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.BaseKeypointRCNNHead.from_config": [[160, 178], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "ret", "=", "{", "\n", "\"loss_weight\"", ":", "cfg", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "LOSS_WEIGHT", ",", "\n", "\"num_keypoints\"", ":", "cfg", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "NUM_KEYPOINTS", ",", "\n", "}", "\n", "normalize_by_visible", "=", "(", "\n", "cfg", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS", "\n", ")", "# noqa", "\n", "if", "not", "normalize_by_visible", ":", "\n", "            ", "batch_size_per_image", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "BATCH_SIZE_PER_IMAGE", "\n", "positive_sample_fraction", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "POSITIVE_FRACTION", "\n", "ret", "[", "\"loss_normalizer\"", "]", "=", "(", "\n", "ret", "[", "\"num_keypoints\"", "]", "*", "batch_size_per_image", "*", "positive_sample_fraction", "\n", ")", "\n", "", "else", ":", "\n", "            ", "ret", "[", "\"loss_normalizer\"", "]", "=", "\"visible\"", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.BaseKeypointRCNNHead.forward": [[179, 206], ["keypoint_head.BaseKeypointRCNNHead.layers", "len", "keypoint_head.keypoint_rcnn_inference", "keypoint_head.keypoint_rcnn_loss"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.KRCNNConvDeconvUpsampleHead.layers", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.keypoint_rcnn_inference", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.keypoint_rcnn_loss"], ["", "def", "forward", "(", "self", ",", "x", ",", "instances", ":", "List", "[", "Instances", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: input 4D region feature(s) provided by :class:`ROIHeads`.\n            instances (list[Instances]): contains the boxes & labels corresponding\n                to the input features.\n                Exact format is up to its caller to decide.\n                Typically, this is the foreground instances in training, with\n                \"proposal_boxes\" field and other gt annotations.\n                In inference, it contains boxes that are already predicted.\n\n        Returns:\n            A dict of losses if in training. The predicted \"instances\" if in inference.\n        \"\"\"", "\n", "x", "=", "self", ".", "layers", "(", "x", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "num_images", "=", "len", "(", "instances", ")", "\n", "normalizer", "=", "(", "\n", "None", "if", "self", ".", "loss_normalizer", "==", "\"visible\"", "else", "num_images", "*", "self", ".", "loss_normalizer", "\n", ")", "\n", "return", "{", "\n", "\"loss_keypoint\"", ":", "keypoint_rcnn_loss", "(", "x", ",", "instances", ",", "normalizer", "=", "normalizer", ")", "\n", "*", "self", ".", "loss_weight", "\n", "}", "\n", "", "else", ":", "\n", "            ", "keypoint_rcnn_inference", "(", "x", ",", "instances", ")", "\n", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.BaseKeypointRCNNHead.layers": [[207, 212], ["None"], "methods", ["None"], ["", "", "def", "layers", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Neural network layers that makes predictions from regional input features.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.KRCNNConvDeconvUpsampleHead.__init__": [[225, 260], ["super().__init__", "enumerate", "detectron2.layers.ConvTranspose2d", "keypoint_head.KRCNNConvDeconvUpsampleHead.named_parameters", "detectron2.layers.Conv2d", "keypoint_head.KRCNNConvDeconvUpsampleHead.add_module", "keypoint_head.KRCNNConvDeconvUpsampleHead.add_module", "torch.nn.ReLU", "torch.nn.init.constant_", "torch.nn.init.kaiming_normal_"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "self", ",", "input_shape", ",", "*", ",", "num_keypoints", ",", "conv_dims", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            input_shape (ShapeSpec): shape of the input feature\n            conv_dims: an iterable of output channel counts for each conv in the head\n                         e.g. (512, 512, 512) for three convs outputting 512 channels.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "num_keypoints", "=", "num_keypoints", ",", "**", "kwargs", ")", "\n", "\n", "# default up_scale to 2.0 (this can be made an option)", "\n", "up_scale", "=", "2.0", "\n", "in_channels", "=", "input_shape", ".", "channels", "\n", "\n", "for", "idx", ",", "layer_channels", "in", "enumerate", "(", "conv_dims", ",", "1", ")", ":", "\n", "            ", "module", "=", "Conv2d", "(", "in_channels", ",", "layer_channels", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", "\n", "self", ".", "add_module", "(", "\"conv_fcn{}\"", ".", "format", "(", "idx", ")", ",", "module", ")", "\n", "self", ".", "add_module", "(", "\"conv_fcn_relu{}\"", ".", "format", "(", "idx", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "in_channels", "=", "layer_channels", "\n", "\n", "", "deconv_kernel", "=", "4", "\n", "self", ".", "score_lowres", "=", "ConvTranspose2d", "(", "\n", "in_channels", ",", "num_keypoints", ",", "deconv_kernel", ",", "stride", "=", "2", ",", "padding", "=", "deconv_kernel", "//", "2", "-", "1", "\n", ")", "\n", "self", ".", "up_scale", "=", "up_scale", "\n", "\n", "for", "name", ",", "param", "in", "self", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "\"bias\"", "in", "name", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "param", ",", "0", ")", "\n", "", "elif", "\"weight\"", "in", "name", ":", "\n", "# Caffe2 implementation uses MSRAFill, which in fact", "\n", "# corresponds to kaiming_normal_ in PyTorch", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "param", ",", "mode", "=", "\"fan_out\"", ",", "nonlinearity", "=", "\"relu\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.KRCNNConvDeconvUpsampleHead.from_config": [[261, 267], ["super().from_config"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config"], ["", "", "", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ",", "input_shape", ")", "\n", "ret", "[", "\"input_shape\"", "]", "=", "input_shape", "\n", "ret", "[", "\"conv_dims\"", "]", "=", "cfg", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "CONV_DIMS", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.KRCNNConvDeconvUpsampleHead.layers": [[268, 273], ["detectron2.layers.interpolate", "layer"], "methods", ["None"], ["", "def", "layers", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "layer", "in", "self", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "", "x", "=", "interpolate", "(", "x", ",", "scale_factor", "=", "self", ".", "up_scale", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.build_keypoint_head": [[32, 38], ["ROI_KEYPOINT_HEAD_REGISTRY.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "build_keypoint_head", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Build a keypoint head from `cfg.MODEL.ROI_KEYPOINT_HEAD.NAME`.\n    \"\"\"", "\n", "name", "=", "cfg", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "NAME", "\n", "return", "ROI_KEYPOINT_HEAD_REGISTRY", ".", "get", "(", "name", ")", "(", "cfg", ",", "input_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.keypoint_rcnn_loss": [[40, 97], ["len", "pred_keypoint_logits.view.view", "torch.nn.functional.cross_entropy", "keypoints.to_heatmap", "heatmaps.append", "torch.nonzero().squeeze.append", "detectron2.layers.cat", "detectron2.layers.cat().to", "torch.nonzero().squeeze", "detectron2.utils.events.get_event_storage", "detectron2.utils.events.get_event_storage.put_scalar", "torch.nonzero().squeeze.numel", "len", "heatmaps_per_image.view", "valid_per_image.view", "len", "torch.nonzero().squeeze.numel", "pred_keypoint_logits.view.sum", "detectron2.layers.cat", "torch.nonzero"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.Keypoints.to_heatmap", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "keypoint_rcnn_loss", "(", "pred_keypoint_logits", ",", "instances", ",", "normalizer", ")", ":", "\n", "    ", "\"\"\"\n    Arguments:\n        pred_keypoint_logits (Tensor): A tensor of shape (N, K, S, S) where N is the total number\n            of instances in the batch, K is the number of keypoints, and S is the side length\n            of the keypoint heatmap. The values are spatial logits.\n        instances (list[Instances]): A list of M Instances, where M is the batch size.\n            These instances are predictions from the model\n            that are in 1:1 correspondence with pred_keypoint_logits.\n            Each Instances should contain a `gt_keypoints` field containing a `structures.Keypoint`\n            instance.\n        normalizer (float): Normalize the loss by this amount.\n            If not specified, we normalize by the number of visible keypoints in the minibatch.\n\n    Returns a scalar tensor containing the loss.\n    \"\"\"", "\n", "heatmaps", "=", "[", "]", "\n", "valid", "=", "[", "]", "\n", "\n", "keypoint_side_len", "=", "pred_keypoint_logits", ".", "shape", "[", "2", "]", "\n", "for", "instances_per_image", "in", "instances", ":", "\n", "        ", "if", "len", "(", "instances_per_image", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "keypoints", "=", "instances_per_image", ".", "gt_keypoints", "\n", "heatmaps_per_image", ",", "valid_per_image", "=", "keypoints", ".", "to_heatmap", "(", "\n", "instances_per_image", ".", "proposal_boxes", ".", "tensor", ",", "keypoint_side_len", "\n", ")", "\n", "heatmaps", ".", "append", "(", "heatmaps_per_image", ".", "view", "(", "-", "1", ")", ")", "\n", "valid", ".", "append", "(", "valid_per_image", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "len", "(", "heatmaps", ")", ":", "\n", "        ", "keypoint_targets", "=", "cat", "(", "heatmaps", ",", "dim", "=", "0", ")", "\n", "valid", "=", "cat", "(", "valid", ",", "dim", "=", "0", ")", ".", "to", "(", "dtype", "=", "torch", ".", "uint8", ")", "\n", "valid", "=", "torch", ".", "nonzero", "(", "valid", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# torch.mean (in binary_cross_entropy_with_logits) doesn't", "\n", "# accept empty tensors, so handle it separately", "\n", "", "if", "len", "(", "heatmaps", ")", "==", "0", "or", "valid", ".", "numel", "(", ")", "==", "0", ":", "\n", "        ", "global", "_TOTAL_SKIPPED", "\n", "_TOTAL_SKIPPED", "+=", "1", "\n", "storage", "=", "get_event_storage", "(", ")", "\n", "storage", ".", "put_scalar", "(", "\"kpts_num_skipped_batches\"", ",", "_TOTAL_SKIPPED", ",", "smoothing_hint", "=", "False", ")", "\n", "return", "pred_keypoint_logits", ".", "sum", "(", ")", "*", "0", "\n", "\n", "", "N", ",", "K", ",", "H", ",", "W", "=", "pred_keypoint_logits", ".", "shape", "\n", "pred_keypoint_logits", "=", "pred_keypoint_logits", ".", "view", "(", "N", "*", "K", ",", "H", "*", "W", ")", "\n", "\n", "keypoint_loss", "=", "F", ".", "cross_entropy", "(", "\n", "pred_keypoint_logits", "[", "valid", "]", ",", "keypoint_targets", "[", "valid", "]", ",", "reduction", "=", "\"sum\"", "\n", ")", "\n", "\n", "# If a normalizer isn't specified, normalize by the number of visible keypoints in the minibatch", "\n", "if", "normalizer", "is", "None", ":", "\n", "        ", "normalizer", "=", "valid", ".", "numel", "(", ")", "\n", "", "keypoint_loss", "/=", "normalizer", "\n", "\n", "return", "keypoint_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.keypoint_rcnn_inference": [[99, 133], ["detectron2.layers.cat", "pred_keypoint_logits.detach.detach", "detectron2.structures.heatmaps_to_keypoints", "keypoint_results[].split", "pred_keypoint_logits.detach.split", "zip", "detectron2.layers.cat.detach", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.heatmaps_to_keypoints"], ["", "def", "keypoint_rcnn_inference", "(", "pred_keypoint_logits", ":", "torch", ".", "Tensor", ",", "pred_instances", ":", "List", "[", "Instances", "]", ")", ":", "\n", "    ", "\"\"\"\n    Post process each predicted keypoint heatmap in `pred_keypoint_logits` into (x, y, score)\n        and add it to the `pred_instances` as a `pred_keypoints` field.\n\n    Args:\n        pred_keypoint_logits (Tensor): A tensor of shape (R, K, S, S) where R is the total number\n           of instances in the batch, K is the number of keypoints, and S is the side length of\n           the keypoint heatmap. The values are spatial logits.\n        pred_instances (list[Instances]): A list of N Instances, where N is the number of images.\n\n    Returns:\n        None. Each element in pred_instances will contain extra \"pred_keypoints\" and\n            \"pred_keypoint_heatmaps\" fields. \"pred_keypoints\" is a tensor of shape\n            (#instance, K, 3) where the last dimension corresponds to (x, y, score).\n            The scores are larger than 0. \"pred_keypoint_heatmaps\" contains the raw\n            keypoint logits as passed to this function.\n    \"\"\"", "\n", "# flatten all bboxes from all images together (list[Boxes] -> Rx4 tensor)", "\n", "bboxes_flat", "=", "cat", "(", "[", "b", ".", "pred_boxes", ".", "tensor", "for", "b", "in", "pred_instances", "]", ",", "dim", "=", "0", ")", "\n", "\n", "pred_keypoint_logits", "=", "pred_keypoint_logits", ".", "detach", "(", ")", "\n", "keypoint_results", "=", "heatmaps_to_keypoints", "(", "pred_keypoint_logits", ",", "bboxes_flat", ".", "detach", "(", ")", ")", "\n", "num_instances_per_image", "=", "[", "len", "(", "i", ")", "for", "i", "in", "pred_instances", "]", "\n", "keypoint_results", "=", "keypoint_results", "[", ":", ",", ":", ",", "[", "0", ",", "1", ",", "3", "]", "]", ".", "split", "(", "num_instances_per_image", ",", "dim", "=", "0", ")", "\n", "heatmap_results", "=", "pred_keypoint_logits", ".", "split", "(", "num_instances_per_image", ",", "dim", "=", "0", ")", "\n", "\n", "for", "keypoint_results_per_image", ",", "heatmap_results_per_image", ",", "instances_per_image", "in", "zip", "(", "\n", "keypoint_results", ",", "heatmap_results", ",", "pred_instances", "\n", ")", ":", "\n", "# keypoint_results_per_image is (num instances)x(num keypoints)x(x, y, score)", "\n", "# heatmap_results_per_image is (num instances)x(num keypoints)x(side)x(side)", "\n", "        ", "instances_per_image", ".", "pred_keypoints", "=", "keypoint_results_per_image", "\n", "instances_per_image", ".", "pred_keypoint_heatmaps", "=", "heatmap_results_per_image", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.box_head.FastRCNNConvFCHead.__init__": [[32, 80], ["torch.nn.Sequential.__init__", "enumerate", "enumerate", "detectron2.layers.Conv2d", "box_head.FastRCNNConvFCHead.add_module", "box_head.FastRCNNConvFCHead.conv_norm_relus.append", "torch.nn.Linear", "box_head.FastRCNNConvFCHead.add_module", "box_head.FastRCNNConvFCHead.add_module", "box_head.FastRCNNConvFCHead.fcs.append", "fvcore.c2_msra_fill", "fvcore.c2_xavier_fill", "len", "len", "box_head.FastRCNNConvFCHead.add_module", "int", "torch.nn.ReLU", "detectron2.layers.get_norm", "torch.nn.ReLU", "torch.nn.Flatten", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "input_shape", ":", "ShapeSpec", ",", "*", ",", "conv_dims", ":", "List", "[", "int", "]", ",", "fc_dims", ":", "List", "[", "int", "]", ",", "conv_norm", "=", "\"\"", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            input_shape (ShapeSpec): shape of the input feature.\n            conv_dims (list[int]): the output dimensions of the conv layers\n            fc_dims (list[int]): the output dimensions of the fc layers\n            conv_norm (str or callable): normalization for the conv layers.\n                See :func:`detectron2.layers.get_norm` for supported types.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "len", "(", "conv_dims", ")", "+", "len", "(", "fc_dims", ")", ">", "0", "\n", "\n", "self", ".", "_output_size", "=", "(", "input_shape", ".", "channels", ",", "input_shape", ".", "height", ",", "input_shape", ".", "width", ")", "\n", "\n", "self", ".", "conv_norm_relus", "=", "[", "]", "\n", "for", "k", ",", "conv_dim", "in", "enumerate", "(", "conv_dims", ")", ":", "\n", "            ", "conv", "=", "Conv2d", "(", "\n", "self", ".", "_output_size", "[", "0", "]", ",", "\n", "conv_dim", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "not", "conv_norm", ",", "\n", "norm", "=", "get_norm", "(", "conv_norm", ",", "conv_dim", ")", ",", "\n", "activation", "=", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"conv{}\"", ".", "format", "(", "k", "+", "1", ")", ",", "conv", ")", "\n", "self", ".", "conv_norm_relus", ".", "append", "(", "conv", ")", "\n", "self", ".", "_output_size", "=", "(", "conv_dim", ",", "self", ".", "_output_size", "[", "1", "]", ",", "self", ".", "_output_size", "[", "2", "]", ")", "\n", "\n", "", "self", ".", "fcs", "=", "[", "]", "\n", "for", "k", ",", "fc_dim", "in", "enumerate", "(", "fc_dims", ")", ":", "\n", "            ", "if", "k", "==", "0", ":", "\n", "                ", "self", ".", "add_module", "(", "\"flatten\"", ",", "nn", ".", "Flatten", "(", ")", ")", "\n", "", "fc", "=", "nn", ".", "Linear", "(", "int", "(", "np", ".", "prod", "(", "self", ".", "_output_size", ")", ")", ",", "fc_dim", ")", "\n", "self", ".", "add_module", "(", "\"fc{}\"", ".", "format", "(", "k", "+", "1", ")", ",", "fc", ")", "\n", "self", ".", "add_module", "(", "\"fc_relu{}\"", ".", "format", "(", "k", "+", "1", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "fcs", ".", "append", "(", "fc", ")", "\n", "self", ".", "_output_size", "=", "fc_dim", "\n", "\n", "", "for", "layer", "in", "self", ".", "conv_norm_relus", ":", "\n", "            ", "weight_init", ".", "c2_msra_fill", "(", "layer", ")", "\n", "", "for", "layer", "in", "self", ".", "fcs", ":", "\n", "            ", "weight_init", ".", "c2_xavier_fill", "(", "layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.box_head.FastRCNNConvFCHead.from_config": [[81, 92], ["None"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "num_conv", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NUM_CONV", "\n", "conv_dim", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "CONV_DIM", "\n", "num_fc", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NUM_FC", "\n", "fc_dim", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "FC_DIM", "\n", "return", "{", "\n", "\"input_shape\"", ":", "input_shape", ",", "\n", "\"conv_dims\"", ":", "[", "conv_dim", "]", "*", "num_conv", ",", "\n", "\"fc_dims\"", ":", "[", "fc_dim", "]", "*", "num_fc", ",", "\n", "\"conv_norm\"", ":", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NORM", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.box_head.FastRCNNConvFCHead.forward": [[94, 98], ["layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "layer", "in", "self", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.box_head.FastRCNNConvFCHead.output_shape": [[99, 111], ["isinstance", "detectron2.layers.ShapeSpec", "detectron2.layers.ShapeSpec"], "methods", ["None"], ["", "@", "property", "\n", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "output_shape", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            ShapeSpec: the output feature shape\n        \"\"\"", "\n", "o", "=", "self", ".", "_output_size", "\n", "if", "isinstance", "(", "o", ",", "int", ")", ":", "\n", "            ", "return", "ShapeSpec", "(", "channels", "=", "o", ")", "\n", "", "else", ":", "\n", "            ", "return", "ShapeSpec", "(", "channels", "=", "o", "[", "0", "]", ",", "height", "=", "o", "[", "1", "]", ",", "width", "=", "o", "[", "2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.box_head.build_box_head": [[113, 119], ["ROI_BOX_HEAD_REGISTRY.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "", "def", "build_box_head", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Build a box head defined by `cfg.MODEL.ROI_BOX_HEAD.NAME`.\n    \"\"\"", "\n", "name", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NAME", "\n", "return", "ROI_BOX_HEAD_REGISTRY", ".", "get", "(", "name", ")", "(", "cfg", ",", "input_shape", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn._ScaleGradient.forward": [[21, 25], ["None"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ",", "scale", ")", ":", "\n", "        ", "ctx", ".", "scale", "=", "scale", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn._ScaleGradient.backward": [[26, 29], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "return", "grad_output", "*", "ctx", ".", "scale", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads.__init__": [[37, 79], ["len", "torch.nn.ModuleList", "torch.nn.ModuleList", "roi_heads.StandardROIHeads.__init__", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "box_in_features", ":", "List", "[", "str", "]", ",", "\n", "box_pooler", ":", "ROIPooler", ",", "\n", "box_heads", ":", "List", "[", "nn", ".", "Module", "]", ",", "\n", "box_predictors", ":", "List", "[", "nn", ".", "Module", "]", ",", "\n", "proposal_matchers", ":", "List", "[", "Matcher", "]", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            box_pooler (ROIPooler): pooler that extracts region features from given boxes\n            box_heads (list[nn.Module]): box head for each cascade stage\n            box_predictors (list[nn.Module]): box predictor for each cascade stage\n            proposal_matchers (list[Matcher]): matcher with different IoU thresholds to\n                match boxes with ground truth for each stage. The first matcher matches\n                RPN proposals with ground truth, the other matchers use boxes predicted\n                by the previous stage as proposals and match them with ground truth.\n        \"\"\"", "\n", "assert", "\"proposal_matcher\"", "not", "in", "kwargs", ",", "(", "\n", "\"CascadeROIHeads takes 'proposal_matchers=' for each stage instead \"", "\n", "\"of one 'proposal_matcher='.\"", "\n", ")", "\n", "# The first matcher matches RPN proposals with ground truth, done in the base class", "\n", "kwargs", "[", "\"proposal_matcher\"", "]", "=", "proposal_matchers", "[", "0", "]", "\n", "num_stages", "=", "self", ".", "num_cascade_stages", "=", "len", "(", "box_heads", ")", "\n", "box_heads", "=", "nn", ".", "ModuleList", "(", "box_heads", ")", "\n", "box_predictors", "=", "nn", ".", "ModuleList", "(", "box_predictors", ")", "\n", "assert", "len", "(", "box_predictors", ")", "==", "num_stages", ",", "f\"{len(box_predictors)} != {num_stages}!\"", "\n", "assert", "len", "(", "proposal_matchers", ")", "==", "num_stages", ",", "f\"{len(proposal_matchers)} != {num_stages}!\"", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "box_in_features", "=", "box_in_features", ",", "\n", "box_pooler", "=", "box_pooler", ",", "\n", "box_head", "=", "box_heads", ",", "\n", "box_predictor", "=", "box_predictors", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "self", ".", "proposal_matchers", "=", "proposal_matchers", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads.from_config": [[80, 85], ["super().from_config", "super().from_config.pop"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "from_config", "(", "cfg", ",", "input_shape", ")", "\n", "ret", ".", "pop", "(", "\"proposal_matcher\"", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._init_box_head": [[86, 135], ["tuple", "poolers.ROIPooler", "detectron2.layers.ShapeSpec", "zip", "len", "len", "len", "box_head.build_box_head.build_box_head", "box_heads.append", "box_predictors.append", "proposal_matchers.append", "set", "fast_rcnn.FastRCNNOutputLayers", "matcher.Matcher", "box_regression.Box2BoxTransform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.box_head.build_box_head", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "@", "classmethod", "\n", "def", "_init_box_head", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "\n", "# fmt: off", "\n", "        ", "in_features", "=", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IN_FEATURES", "\n", "pooler_resolution", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_RESOLUTION", "\n", "pooler_scales", "=", "tuple", "(", "1.0", "/", "input_shape", "[", "k", "]", ".", "stride", "for", "k", "in", "in_features", ")", "\n", "sampling_ratio", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_SAMPLING_RATIO", "\n", "pooler_type", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "\n", "cascade_bbox_reg_weights", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_CASCADE_HEAD", ".", "BBOX_REG_WEIGHTS", "\n", "cascade_ious", "=", "cfg", ".", "MODEL", ".", "ROI_BOX_CASCADE_HEAD", ".", "IOUS", "\n", "assert", "len", "(", "cascade_bbox_reg_weights", ")", "==", "len", "(", "cascade_ious", ")", "\n", "assert", "cfg", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "CLS_AGNOSTIC_BBOX_REG", ",", "\"CascadeROIHeads only support class-agnostic regression now!\"", "\n", "assert", "cascade_ious", "[", "0", "]", "==", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "IOU_THRESHOLDS", "[", "0", "]", "\n", "# fmt: on", "\n", "\n", "in_channels", "=", "[", "input_shape", "[", "f", "]", ".", "channels", "for", "f", "in", "in_features", "]", "\n", "# Check all channel counts are equal", "\n", "assert", "len", "(", "set", "(", "in_channels", ")", ")", "==", "1", ",", "in_channels", "\n", "in_channels", "=", "in_channels", "[", "0", "]", "\n", "\n", "box_pooler", "=", "ROIPooler", "(", "\n", "output_size", "=", "pooler_resolution", ",", "\n", "scales", "=", "pooler_scales", ",", "\n", "sampling_ratio", "=", "sampling_ratio", ",", "\n", "pooler_type", "=", "pooler_type", ",", "\n", ")", "\n", "pooled_shape", "=", "ShapeSpec", "(", "\n", "channels", "=", "in_channels", ",", "width", "=", "pooler_resolution", ",", "height", "=", "pooler_resolution", "\n", ")", "\n", "\n", "box_heads", ",", "box_predictors", ",", "proposal_matchers", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "match_iou", ",", "bbox_reg_weights", "in", "zip", "(", "cascade_ious", ",", "cascade_bbox_reg_weights", ")", ":", "\n", "            ", "box_head", "=", "build_box_head", "(", "cfg", ",", "pooled_shape", ")", "\n", "box_heads", ".", "append", "(", "box_head", ")", "\n", "box_predictors", ".", "append", "(", "\n", "FastRCNNOutputLayers", "(", "\n", "cfg", ",", "\n", "box_head", ".", "output_shape", ",", "\n", "box2box_transform", "=", "Box2BoxTransform", "(", "weights", "=", "bbox_reg_weights", ")", ",", "\n", ")", "\n", ")", "\n", "proposal_matchers", ".", "append", "(", "Matcher", "(", "[", "match_iou", "]", ",", "[", "0", ",", "1", "]", ",", "allow_low_quality_matches", "=", "False", ")", ")", "\n", "", "return", "{", "\n", "\"box_in_features\"", ":", "in_features", ",", "\n", "\"box_pooler\"", ":", "box_pooler", ",", "\n", "\"box_heads\"", ":", "box_heads", ",", "\n", "\"box_predictors\"", ":", "box_predictors", ",", "\n", "\"proposal_matchers\"", ":", "proposal_matchers", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads.forward": [[137, 152], ["cascade_rcnn.CascadeROIHeads.label_and_sample_proposals", "cascade_rcnn.CascadeROIHeads._forward_box", "cascade_rcnn.CascadeROIHeads.update", "cascade_rcnn.CascadeROIHeads.update", "cascade_rcnn.CascadeROIHeads._forward_box", "cascade_rcnn.CascadeROIHeads.forward_with_given_boxes", "cascade_rcnn.CascadeROIHeads._forward_mask", "cascade_rcnn.CascadeROIHeads._forward_keypoint"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.ROIHeads.label_and_sample_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._forward_box", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._forward_box", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads.forward_with_given_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.roi_heads.StandardROIHeads._forward_keypoint"], ["", "def", "forward", "(", "self", ",", "images", ",", "features", ",", "proposals", ",", "targets", "=", "None", ")", ":", "\n", "        ", "del", "images", "\n", "if", "self", ".", "training", ":", "\n", "            ", "proposals", "=", "self", ".", "label_and_sample_proposals", "(", "proposals", ",", "targets", ")", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "# Need targets to box head", "\n", "            ", "losses", "=", "self", ".", "_forward_box", "(", "features", ",", "proposals", ",", "targets", ")", "\n", "losses", ".", "update", "(", "self", ".", "_forward_mask", "(", "features", ",", "proposals", ")", ")", "\n", "losses", ".", "update", "(", "self", ".", "_forward_keypoint", "(", "features", ",", "proposals", ")", ")", "\n", "return", "proposals", ",", "losses", "\n", "", "else", ":", "\n", "            ", "pred_instances", "=", "self", ".", "_forward_box", "(", "features", ",", "proposals", ")", "\n", "pred_instances", "=", "self", ".", "forward_with_given_boxes", "(", "features", ",", "pred_instances", ")", "\n", "return", "pred_instances", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._forward_box": [[153, 207], ["range", "cascade_rcnn.CascadeROIHeads._run_stage", "cascade_rcnn.CascadeROIHeads.box_predictor[].predict_boxes", "head_outputs.append", "detectron2.utils.events.get_event_storage", "enumerate", "predictor.predict_boxes", "fast_rcnn.fast_rcnn_inference", "cascade_rcnn.CascadeROIHeads._create_proposals_from_boxes", "losses.update", "h[].predict_probs", "cascade_rcnn.CascadeROIHeads._match_and_label_boxes", "detectron2.utils.events.get_event_storage.name_scope", "predictor.losses", "sum", "zip", "list", "predictor.losses.items"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._run_stage", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.fast_rcnn_inference", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._create_proposals_from_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.predict_probs", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._match_and_label_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.name_scope", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "", "def", "_forward_box", "(", "self", ",", "features", ",", "proposals", ",", "targets", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            features, targets: the same as in\n                Same as in :meth:`ROIHeads.forward`.\n            proposals (list[Instances]): the per-image object proposals with\n                their matching ground truth.\n                Each has fields \"proposal_boxes\", and \"objectness_logits\",\n                \"gt_classes\", \"gt_boxes\".\n        \"\"\"", "\n", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "box_in_features", "]", "\n", "head_outputs", "=", "[", "]", "# (predictor, predictions, proposals)", "\n", "prev_pred_boxes", "=", "None", "\n", "image_sizes", "=", "[", "x", ".", "image_size", "for", "x", "in", "proposals", "]", "\n", "for", "k", "in", "range", "(", "self", ".", "num_cascade_stages", ")", ":", "\n", "            ", "if", "k", ">", "0", ":", "\n", "# The output boxes of the previous stage are used to create the input", "\n", "# proposals of the next stage.", "\n", "                ", "proposals", "=", "self", ".", "_create_proposals_from_boxes", "(", "prev_pred_boxes", ",", "image_sizes", ")", "\n", "if", "self", ".", "training", ":", "\n", "                    ", "proposals", "=", "self", ".", "_match_and_label_boxes", "(", "proposals", ",", "k", ",", "targets", ")", "\n", "", "", "predictions", "=", "self", ".", "_run_stage", "(", "features", ",", "proposals", ",", "k", ")", "\n", "prev_pred_boxes", "=", "self", ".", "box_predictor", "[", "k", "]", ".", "predict_boxes", "(", "predictions", ",", "proposals", ")", "\n", "head_outputs", ".", "append", "(", "(", "self", ".", "box_predictor", "[", "k", "]", ",", "predictions", ",", "proposals", ")", ")", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "            ", "losses", "=", "{", "}", "\n", "storage", "=", "get_event_storage", "(", ")", "\n", "for", "stage", ",", "(", "predictor", ",", "predictions", ",", "proposals", ")", "in", "enumerate", "(", "head_outputs", ")", ":", "\n", "                ", "with", "storage", ".", "name_scope", "(", "\"stage{}\"", ".", "format", "(", "stage", ")", ")", ":", "\n", "                    ", "stage_losses", "=", "predictor", ".", "losses", "(", "predictions", ",", "proposals", ")", "\n", "", "losses", ".", "update", "(", "{", "k", "+", "\"_stage{}\"", ".", "format", "(", "stage", ")", ":", "v", "for", "k", ",", "v", "in", "stage_losses", ".", "items", "(", ")", "}", ")", "\n", "", "return", "losses", "\n", "", "else", ":", "\n", "# Each is a list[Tensor] of length #image. Each tensor is Ri x (K+1)", "\n", "            ", "scores_per_stage", "=", "[", "h", "[", "0", "]", ".", "predict_probs", "(", "h", "[", "1", "]", ",", "h", "[", "2", "]", ")", "for", "h", "in", "head_outputs", "]", "\n", "\n", "# Average the scores across heads", "\n", "scores", "=", "[", "\n", "sum", "(", "list", "(", "scores_per_image", ")", ")", "*", "(", "1.0", "/", "self", ".", "num_cascade_stages", ")", "\n", "for", "scores_per_image", "in", "zip", "(", "*", "scores_per_stage", ")", "\n", "]", "\n", "# Use the boxes of the last head", "\n", "predictor", ",", "predictions", ",", "proposals", "=", "head_outputs", "[", "-", "1", "]", "\n", "boxes", "=", "predictor", ".", "predict_boxes", "(", "predictions", ",", "proposals", ")", "\n", "pred_instances", ",", "_", "=", "fast_rcnn_inference", "(", "\n", "boxes", ",", "\n", "scores", ",", "\n", "image_sizes", ",", "\n", "predictor", ".", "test_score_thresh", ",", "\n", "predictor", ".", "test_nms_thresh", ",", "\n", "predictor", ".", "test_topk_per_image", ",", "\n", ")", "\n", "return", "pred_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._match_and_label_boxes": [[208, 257], ["torch.no_grad", "zip", "detectron2.utils.events.get_event_storage", "detectron2.utils.events.get_event_storage.put_scalar", "detectron2.utils.events.get_event_storage.put_scalar", "detectron2.structures.pairwise_iou", "num_fg_samples.append", "num_bg_samples.append", "len", "detectron2.structures.Boxes", "sum", "len", "sum", "len", "torch.zeros_like", "targets_per_image.gt_boxes.tensor.new_zeros", "proposal_labels.numel", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "_match_and_label_boxes", "(", "self", ",", "proposals", ",", "stage", ",", "targets", ")", ":", "\n", "        ", "\"\"\"\n        Match proposals with groundtruth using the matcher at the given stage.\n        Label the proposals as foreground or background based on the match.\n\n        Args:\n            proposals (list[Instances]): One Instances for each image, with\n                the field \"proposal_boxes\".\n            stage (int): the current stage\n            targets (list[Instances]): the ground truth instances\n\n        Returns:\n            list[Instances]: the same proposals, but with fields \"gt_classes\" and \"gt_boxes\"\n        \"\"\"", "\n", "num_fg_samples", ",", "num_bg_samples", "=", "[", "]", ",", "[", "]", "\n", "for", "proposals_per_image", ",", "targets_per_image", "in", "zip", "(", "proposals", ",", "targets", ")", ":", "\n", "            ", "match_quality_matrix", "=", "pairwise_iou", "(", "\n", "targets_per_image", ".", "gt_boxes", ",", "proposals_per_image", ".", "proposal_boxes", "\n", ")", "\n", "# proposal_labels are 0 or 1", "\n", "matched_idxs", ",", "proposal_labels", "=", "self", ".", "proposal_matchers", "[", "stage", "]", "(", "match_quality_matrix", ")", "\n", "if", "len", "(", "targets_per_image", ")", ">", "0", ":", "\n", "                ", "gt_classes", "=", "targets_per_image", ".", "gt_classes", "[", "matched_idxs", "]", "\n", "# Label unmatched proposals (0 label from matcher) as background (label=num_classes)", "\n", "gt_classes", "[", "proposal_labels", "==", "0", "]", "=", "self", ".", "num_classes", "\n", "gt_boxes", "=", "targets_per_image", ".", "gt_boxes", "[", "matched_idxs", "]", "\n", "", "else", ":", "\n", "                ", "gt_classes", "=", "torch", ".", "zeros_like", "(", "matched_idxs", ")", "+", "self", ".", "num_classes", "\n", "gt_boxes", "=", "Boxes", "(", "\n", "targets_per_image", ".", "gt_boxes", ".", "tensor", ".", "new_zeros", "(", "(", "len", "(", "proposals_per_image", ")", ",", "4", ")", ")", "\n", ")", "\n", "", "proposals_per_image", ".", "gt_classes", "=", "gt_classes", "\n", "proposals_per_image", ".", "gt_boxes", "=", "gt_boxes", "\n", "\n", "num_fg_samples", ".", "append", "(", "(", "proposal_labels", "==", "1", ")", ".", "sum", "(", ")", ".", "item", "(", ")", ")", "\n", "num_bg_samples", ".", "append", "(", "proposal_labels", ".", "numel", "(", ")", "-", "num_fg_samples", "[", "-", "1", "]", ")", "\n", "\n", "# Log the number of fg/bg samples in each stage", "\n", "", "storage", "=", "get_event_storage", "(", ")", "\n", "storage", ".", "put_scalar", "(", "\n", "\"stage{}/roi_head/num_fg_samples\"", ".", "format", "(", "stage", ")", ",", "\n", "sum", "(", "num_fg_samples", ")", "/", "len", "(", "num_fg_samples", ")", ",", "\n", ")", "\n", "storage", ".", "put_scalar", "(", "\n", "\"stage{}/roi_head/num_bg_samples\"", ".", "format", "(", "stage", ")", ",", "\n", "sum", "(", "num_bg_samples", ")", "/", "len", "(", "num_bg_samples", ")", ",", "\n", ")", "\n", "return", "proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._run_stage": [[258, 276], ["cascade_rcnn.CascadeROIHeads.box_pooler", "_ScaleGradient.apply"], "methods", ["None"], ["", "def", "_run_stage", "(", "self", ",", "features", ",", "proposals", ",", "stage", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            features (list[Tensor]): #lvl input features to ROIHeads\n            proposals (list[Instances]): #image Instances, with the field \"proposal_boxes\"\n            stage (int): the current stage\n\n        Returns:\n            Same output as `FastRCNNOutputLayers.forward()`.\n        \"\"\"", "\n", "box_features", "=", "self", ".", "box_pooler", "(", "features", ",", "[", "x", ".", "proposal_boxes", "for", "x", "in", "proposals", "]", ")", "\n", "# The original implementation averages the losses among heads,", "\n", "# but scale up the parameter gradients of the heads.", "\n", "# This is equivalent to adding the losses among heads,", "\n", "# but scale down the gradients on features.", "\n", "box_features", "=", "_ScaleGradient", ".", "apply", "(", "box_features", ",", "1.0", "/", "self", ".", "num_cascade_stages", ")", "\n", "box_features", "=", "self", ".", "box_head", "[", "stage", "]", "(", "box_features", ")", "\n", "return", "self", ".", "box_predictor", "[", "stage", "]", "(", "box_features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.cascade_rcnn.CascadeROIHeads._create_proposals_from_boxes": [[277, 299], ["zip", "detectron2.structures.Boxes", "boxes_per_image.clip", "detectron2.structures.Instances", "proposals.append", "b.detach", "boxes_per_image.nonempty"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.nonempty"], ["", "def", "_create_proposals_from_boxes", "(", "self", ",", "boxes", ",", "image_sizes", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            boxes (list[Tensor]): per-image predicted boxes, each of shape Ri x 4\n            image_sizes (list[tuple]): list of image shapes in (h, w)\n\n        Returns:\n            list[Instances]: per-image proposals with the given boxes.\n        \"\"\"", "\n", "# Just like RPN, the proposals should not have gradients", "\n", "boxes", "=", "[", "Boxes", "(", "b", ".", "detach", "(", ")", ")", "for", "b", "in", "boxes", "]", "\n", "proposals", "=", "[", "]", "\n", "for", "boxes_per_image", ",", "image_size", "in", "zip", "(", "boxes", ",", "image_sizes", ")", ":", "\n", "            ", "boxes_per_image", ".", "clip", "(", "image_size", ")", "\n", "if", "self", ".", "training", ":", "\n", "# do not filter empty boxes at inference time,", "\n", "# because the scores from each stage need to be aligned and added later", "\n", "                ", "boxes_per_image", "=", "boxes_per_image", "[", "boxes_per_image", ".", "nonempty", "(", ")", "]", "\n", "", "prop", "=", "Instances", "(", "image_size", ")", "\n", "prop", ".", "proposal_boxes", "=", "boxes_per_image", "\n", "proposals", ".", "append", "(", "prop", ")", "\n", "", "return", "proposals", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone": [[20, 34], ["isinstance", "detectron2.layers.ShapeSpec", "BACKBONE_REGISTRY.get", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["\n", "meta_arch", "=", "cfg", ".", "MODEL", ".", "META_ARCHITECTURE", "\n", "model", "=", "META_ARCH_REGISTRY", ".", "get", "(", "meta_arch", ")", "(", "cfg", ")", "\n", "model", ".", "to", "(", "torch", ".", "device", "(", "cfg", ".", "MODEL", ".", "DEVICE", ")", ")", "\n", "_log_api_usage", "(", "\"modeling.meta_arch.\"", "+", "meta_arch", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.FPN.__init__": [[26, 109], ["backbone.Backbone.__init__", "isinstance", "bottom_up.output_shape", "fpn._assert_strides_are_log2_contiguous", "enumerate", "tuple", "list", "detectron2.layers.get_norm", "detectron2.layers.get_norm", "detectron2.layers.Conv2d", "detectron2.layers.Conv2d", "fvcore.c2_xavier_fill", "fvcore.c2_xavier_fill", "int", "fpn.FPN.add_module", "fpn.FPN.add_module", "lateral_convs.append", "output_convs.append", "range", "fpn.FPN._out_feature_strides.keys", "math.log2", "int", "math.log2"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn._assert_strides_are_log2_contiguous", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "\n", "self", ",", "bottom_up", ",", "in_features", ",", "out_channels", ",", "norm", "=", "\"\"", ",", "top_block", "=", "None", ",", "fuse_type", "=", "\"sum\"", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            bottom_up (Backbone): module representing the bottom up subnetwork.\n                Must be a subclass of :class:`Backbone`. The multi-scale feature\n                maps generated by the bottom up network, and listed in `in_features`,\n                are used to generate FPN levels.\n            in_features (list[str]): names of the input feature maps coming\n                from the backbone to which FPN is attached. For example, if the\n                backbone produces [\"res2\", \"res3\", \"res4\"], any *contiguous* sublist\n                of these may be used; order must be from high to low resolution.\n            out_channels (int): number of channels in the output feature maps.\n            norm (str): the normalization to use.\n            top_block (nn.Module or None): if provided, an extra operation will\n                be performed on the output of the last (smallest resolution)\n                FPN output, and the result will extend the result list. The top_block\n                further downsamples the feature map. It must have an attribute\n                \"num_levels\", meaning the number of extra FPN levels added by\n                this block, and \"in_feature\", which is a string representing\n                its input feature (e.g., p5).\n            fuse_type (str): types for fusing the top down features and the lateral\n                ones. It can be \"sum\" (default), which sums up element-wise; or \"avg\",\n                which takes the element-wise mean of the two.\n        \"\"\"", "\n", "super", "(", "FPN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "isinstance", "(", "bottom_up", ",", "Backbone", ")", "\n", "assert", "in_features", ",", "in_features", "\n", "\n", "# Feature map strides and channels from the bottom up network (e.g. ResNet)", "\n", "input_shapes", "=", "bottom_up", ".", "output_shape", "(", ")", "\n", "strides", "=", "[", "input_shapes", "[", "f", "]", ".", "stride", "for", "f", "in", "in_features", "]", "\n", "in_channels_per_feature", "=", "[", "input_shapes", "[", "f", "]", ".", "channels", "for", "f", "in", "in_features", "]", "\n", "\n", "_assert_strides_are_log2_contiguous", "(", "strides", ")", "\n", "lateral_convs", "=", "[", "]", "\n", "output_convs", "=", "[", "]", "\n", "\n", "use_bias", "=", "norm", "==", "\"\"", "\n", "for", "idx", ",", "in_channels", "in", "enumerate", "(", "in_channels_per_feature", ")", ":", "\n", "            ", "lateral_norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", "\n", "output_norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", "\n", "\n", "lateral_conv", "=", "Conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "use_bias", ",", "norm", "=", "lateral_norm", "\n", ")", "\n", "output_conv", "=", "Conv2d", "(", "\n", "out_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "use_bias", ",", "\n", "norm", "=", "output_norm", ",", "\n", ")", "\n", "weight_init", ".", "c2_xavier_fill", "(", "lateral_conv", ")", "\n", "weight_init", ".", "c2_xavier_fill", "(", "output_conv", ")", "\n", "stage", "=", "int", "(", "math", ".", "log2", "(", "strides", "[", "idx", "]", ")", ")", "\n", "self", ".", "add_module", "(", "\"fpn_lateral{}\"", ".", "format", "(", "stage", ")", ",", "lateral_conv", ")", "\n", "self", ".", "add_module", "(", "\"fpn_output{}\"", ".", "format", "(", "stage", ")", ",", "output_conv", ")", "\n", "\n", "lateral_convs", ".", "append", "(", "lateral_conv", ")", "\n", "output_convs", ".", "append", "(", "output_conv", ")", "\n", "# Place convs into top-down order (from low to high resolution)", "\n", "# to make the top-down computation in forward clearer.", "\n", "", "self", ".", "lateral_convs", "=", "lateral_convs", "[", ":", ":", "-", "1", "]", "\n", "self", ".", "output_convs", "=", "output_convs", "[", ":", ":", "-", "1", "]", "\n", "self", ".", "top_block", "=", "top_block", "\n", "self", ".", "in_features", "=", "tuple", "(", "in_features", ")", "\n", "self", ".", "bottom_up", "=", "bottom_up", "\n", "# Return feature names are \"p<stage>\", like [\"p2\", \"p3\", ..., \"p6\"]", "\n", "self", ".", "_out_feature_strides", "=", "{", "\"p{}\"", ".", "format", "(", "int", "(", "math", ".", "log2", "(", "s", ")", ")", ")", ":", "s", "for", "s", "in", "strides", "}", "\n", "# top block output feature maps.", "\n", "if", "self", ".", "top_block", "is", "not", "None", ":", "\n", "            ", "for", "s", "in", "range", "(", "stage", ",", "stage", "+", "self", ".", "top_block", ".", "num_levels", ")", ":", "\n", "                ", "self", ".", "_out_feature_strides", "[", "\"p{}\"", ".", "format", "(", "s", "+", "1", ")", "]", "=", "2", "**", "(", "s", "+", "1", ")", "\n", "\n", "", "", "self", ".", "_out_features", "=", "list", "(", "self", ".", "_out_feature_strides", ".", "keys", "(", ")", ")", "\n", "self", ".", "_out_feature_channels", "=", "{", "k", ":", "out_channels", "for", "k", "in", "self", ".", "_out_features", "}", "\n", "self", ".", "_size_divisibility", "=", "strides", "[", "-", "1", "]", "\n", "assert", "fuse_type", "in", "{", "\"avg\"", ",", "\"sum\"", "}", "\n", "self", ".", "_fuse_type", "=", "fuse_type", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.FPN.size_divisibility": [[110, 113], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "size_divisibility", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_size_divisibility", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.FPN.forward": [[114, 156], ["fpn.FPN.bottom_up", "results.append", "enumerate", "zip", "results.extend", "len", "len", "torch.interpolate", "torch.interpolate", "lateral_conv", "results.insert", "fpn.FPN.top_block", "zip", "output_conv", "fpn.FPN._out_features.index"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\n                feature map tensor for each feature level in high to low resolution order.\n\n        Returns:\n            dict[str->Tensor]:\n                mapping from feature map name to FPN feature map tensor\n                in high to low resolution order. Returned feature names follow the FPN\n                paper convention: \"p<stage>\", where stage has stride = 2 ** stage e.g.,\n                [\"p2\", \"p3\", ..., \"p6\"].\n        \"\"\"", "\n", "bottom_up_features", "=", "self", ".", "bottom_up", "(", "x", ")", "\n", "results", "=", "[", "]", "\n", "prev_features", "=", "self", ".", "lateral_convs", "[", "0", "]", "(", "bottom_up_features", "[", "self", ".", "in_features", "[", "-", "1", "]", "]", ")", "\n", "results", ".", "append", "(", "self", ".", "output_convs", "[", "0", "]", "(", "prev_features", ")", ")", "\n", "\n", "# Reverse feature maps into top-down order (from low to high resolution)", "\n", "for", "idx", ",", "(", "lateral_conv", ",", "output_conv", ")", "in", "enumerate", "(", "\n", "zip", "(", "self", ".", "lateral_convs", ",", "self", ".", "output_convs", ")", "\n", ")", ":", "\n", "# Slicing of ModuleList is not supported https://github.com/pytorch/pytorch/issues/47336", "\n", "# Therefore we loop over all modules but skip the first one", "\n", "            ", "if", "idx", ">", "0", ":", "\n", "                ", "features", "=", "self", ".", "in_features", "[", "-", "idx", "-", "1", "]", "\n", "features", "=", "bottom_up_features", "[", "features", "]", "\n", "top_down_features", "=", "F", ".", "interpolate", "(", "prev_features", ",", "scale_factor", "=", "2.0", ",", "mode", "=", "\"nearest\"", ")", "\n", "lateral_features", "=", "lateral_conv", "(", "features", ")", "\n", "prev_features", "=", "lateral_features", "+", "top_down_features", "\n", "if", "self", ".", "_fuse_type", "==", "\"avg\"", ":", "\n", "                    ", "prev_features", "/=", "2", "\n", "", "results", ".", "insert", "(", "0", ",", "output_conv", "(", "prev_features", ")", ")", "\n", "\n", "", "", "if", "self", ".", "top_block", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "top_block", ".", "in_feature", "in", "bottom_up_features", ":", "\n", "                ", "top_block_in_feature", "=", "bottom_up_features", "[", "self", ".", "top_block", ".", "in_feature", "]", "\n", "", "else", ":", "\n", "                ", "top_block_in_feature", "=", "results", "[", "self", ".", "_out_features", ".", "index", "(", "self", ".", "top_block", ".", "in_feature", ")", "]", "\n", "", "results", ".", "extend", "(", "self", ".", "top_block", "(", "top_block_in_feature", ")", ")", "\n", "", "assert", "len", "(", "self", ".", "_out_features", ")", "==", "len", "(", "results", ")", "\n", "return", "{", "f", ":", "res", "for", "f", ",", "res", "in", "zip", "(", "self", ".", "_out_features", ",", "results", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.FPN.output_shape": [[157, 163], ["detectron2.layers.ShapeSpec"], "methods", ["None"], ["", "def", "output_shape", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "name", ":", "ShapeSpec", "(", "\n", "channels", "=", "self", ".", "_out_feature_channels", "[", "name", "]", ",", "stride", "=", "self", ".", "_out_feature_strides", "[", "name", "]", "\n", ")", "\n", "for", "name", "in", "self", ".", "_out_features", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.LastLevelMaxPool.__init__": [[182, 186], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_levels", "=", "1", "\n", "self", ".", "in_feature", "=", "\"p5\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.LastLevelMaxPool.forward": [[187, 189], ["torch.max_pool2d", "torch.max_pool2d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "[", "F", ".", "max_pool2d", "(", "x", ",", "kernel_size", "=", "1", ",", "stride", "=", "2", ",", "padding", "=", "0", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.LastLevelP6P7.__init__": [[197, 205], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "fvcore.c2_xavier_fill"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "in_feature", "=", "\"res5\"", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_levels", "=", "2", "\n", "self", ".", "in_feature", "=", "in_feature", "\n", "self", ".", "p6", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "3", ",", "2", ",", "1", ")", "\n", "self", ".", "p7", "=", "nn", ".", "Conv2d", "(", "out_channels", ",", "out_channels", ",", "3", ",", "2", ",", "1", ")", "\n", "for", "module", "in", "[", "self", ".", "p6", ",", "self", ".", "p7", "]", ":", "\n", "            ", "weight_init", ".", "c2_xavier_fill", "(", "module", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.LastLevelP6P7.forward": [[206, 210], ["fpn.LastLevelP6P7.p6", "fpn.LastLevelP6P7.p7", "torch.relu", "torch.relu"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "c5", ")", ":", "\n", "        ", "p6", "=", "self", ".", "p6", "(", "c5", ")", "\n", "p7", "=", "self", ".", "p7", "(", "F", ".", "relu", "(", "p6", ")", ")", "\n", "return", "[", "p6", ",", "p7", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn._assert_strides_are_log2_contiguous": [[166, 173], ["enumerate"], "function", ["None"], ["", "", "def", "_assert_strides_are_log2_contiguous", "(", "strides", ")", ":", "\n", "    ", "\"\"\"\n    Assert that each stride is 2x times its preceding stride, i.e. \"contiguous in log2\".\n    \"\"\"", "\n", "for", "i", ",", "stride", "in", "enumerate", "(", "strides", "[", "1", ":", "]", ",", "1", ")", ":", "\n", "        ", "assert", "stride", "==", "2", "*", "strides", "[", "i", "-", "1", "]", ",", "\"Strides {} {} are not log2 contiguous\"", ".", "format", "(", "\n", "stride", ",", "strides", "[", "i", "-", "1", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.build_resnet_fpn_backbone": [[212, 233], ["build.BACKBONE_REGISTRY.register", "resnet.build_resnet_backbone", "fpn.FPN", "fpn.LastLevelMaxPool"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.build_resnet_backbone"], ["", "", "@", "BACKBONE_REGISTRY", ".", "register", "(", ")", "\n", "def", "build_resnet_fpn_backbone", "(", "cfg", ",", "input_shape", ":", "ShapeSpec", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    \"\"\"", "\n", "bottom_up", "=", "build_resnet_backbone", "(", "cfg", ",", "input_shape", ")", "\n", "in_features", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "IN_FEATURES", "\n", "out_channels", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "OUT_CHANNELS", "\n", "backbone", "=", "FPN", "(", "\n", "bottom_up", "=", "bottom_up", ",", "\n", "in_features", "=", "in_features", ",", "\n", "out_channels", "=", "out_channels", ",", "\n", "norm", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "NORM", ",", "\n", "top_block", "=", "LastLevelMaxPool", "(", ")", ",", "\n", "fuse_type", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "FUSE_TYPE", ",", "\n", ")", "\n", "return", "backbone", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.build_clip_resnet_fpn_backbone": [[234, 255], ["build.BACKBONE_REGISTRY.register", "clip_backbone.build_clip_resnet_backbone", "fpn.FPN", "fpn.LastLevelMaxPool"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.build_clip_resnet_backbone"], ["", "@", "BACKBONE_REGISTRY", ".", "register", "(", ")", "\n", "def", "build_clip_resnet_fpn_backbone", "(", "cfg", ",", "input_shape", ":", "ShapeSpec", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    \"\"\"", "\n", "bottom_up", "=", "build_clip_resnet_backbone", "(", "cfg", ",", "input_shape", ")", "\n", "in_features", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "IN_FEATURES", "\n", "out_channels", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "OUT_CHANNELS", "\n", "backbone", "=", "FPN", "(", "\n", "bottom_up", "=", "bottom_up", ",", "\n", "in_features", "=", "in_features", ",", "\n", "out_channels", "=", "out_channels", ",", "\n", "norm", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "NORM", ",", "\n", "top_block", "=", "LastLevelMaxPool", "(", ")", ",", "\n", "fuse_type", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "FUSE_TYPE", ",", "\n", ")", "\n", "return", "backbone", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.fpn.build_retinanet_resnet_fpn_backbone": [[256, 278], ["build.BACKBONE_REGISTRY.register", "resnet.build_resnet_backbone", "fpn.FPN", "resnet.build_resnet_backbone.output_shape", "fpn.LastLevelP6P7"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.build_resnet_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape"], ["", "@", "BACKBONE_REGISTRY", ".", "register", "(", ")", "\n", "def", "build_retinanet_resnet_fpn_backbone", "(", "cfg", ",", "input_shape", ":", "ShapeSpec", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    \"\"\"", "\n", "bottom_up", "=", "build_resnet_backbone", "(", "cfg", ",", "input_shape", ")", "\n", "in_features", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "IN_FEATURES", "\n", "out_channels", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "OUT_CHANNELS", "\n", "in_channels_p6p7", "=", "bottom_up", ".", "output_shape", "(", ")", "[", "\"res5\"", "]", ".", "channels", "\n", "backbone", "=", "FPN", "(", "\n", "bottom_up", "=", "bottom_up", ",", "\n", "in_features", "=", "in_features", ",", "\n", "out_channels", "=", "out_channels", ",", "\n", "norm", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "NORM", ",", "\n", "top_block", "=", "LastLevelP6P7", "(", "in_channels_p6p7", ",", "out_channels", ")", ",", "\n", "fuse_type", "=", "cfg", ".", "MODEL", ".", "FPN", ".", "FUSE_TYPE", ",", "\n", ")", "\n", "return", "backbone", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.BasicBlock.__init__": [[38, 84], ["detectron2.layers.CNNBlockBase.__init__", "detectron2.layers.Conv2d", "detectron2.layers.Conv2d", "detectron2.layers.Conv2d", "detectron2.layers.get_norm", "detectron2.layers.get_norm", "fvcore.c2_msra_fill", "detectron2.layers.get_norm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "*", ",", "stride", "=", "1", ",", "norm", "=", "\"BN\"", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            stride (int): Stride for the first conv.\n            norm (str or callable): normalization for all conv layers.\n                See :func:`layers.get_norm` for supported format.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "in_channels", ",", "out_channels", ",", "stride", ")", "\n", "\n", "if", "in_channels", "!=", "out_channels", ":", "\n", "            ", "self", ".", "shortcut", "=", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "stride", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "shortcut", "=", "None", "\n", "\n", "", "self", ".", "conv1", "=", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", ")", "\n", "\n", "self", ".", "conv2", "=", "Conv2d", "(", "\n", "out_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", ")", "\n", "\n", "for", "layer", "in", "[", "self", ".", "conv1", ",", "self", ".", "conv2", ",", "self", ".", "shortcut", "]", ":", "\n", "            ", "if", "layer", "is", "not", "None", ":", "# shortcut can be None", "\n", "                ", "weight_init", ".", "c2_msra_fill", "(", "layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.BasicBlock.forward": [[85, 98], ["resnet.BasicBlock.conv1", "torch.relu_", "torch.relu_", "resnet.BasicBlock.conv2", "torch.relu_", "torch.relu_", "resnet.BasicBlock.shortcut"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "F", ".", "relu_", "(", "out", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "\n", "if", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "shortcut", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "shortcut", "=", "x", "\n", "\n", "", "out", "+=", "shortcut", "\n", "out", "=", "F", ".", "relu_", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.BottleneckBlock.__init__": [[107, 181], ["detectron2.layers.CNNBlockBase.__init__", "detectron2.layers.Conv2d", "detectron2.layers.Conv2d", "detectron2.layers.Conv2d", "detectron2.layers.Conv2d", "detectron2.layers.get_norm", "detectron2.layers.get_norm", "detectron2.layers.get_norm", "fvcore.c2_msra_fill", "detectron2.layers.get_norm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "*", ",", "\n", "bottleneck_channels", ",", "\n", "stride", "=", "1", ",", "\n", "num_groups", "=", "1", ",", "\n", "norm", "=", "\"BN\"", ",", "\n", "stride_in_1x1", "=", "False", ",", "\n", "dilation", "=", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            bottleneck_channels (int): number of output channels for the 3x3\n                \"bottleneck\" conv layers.\n            num_groups (int): number of groups for the 3x3 conv layer.\n            norm (str or callable): normalization for all conv layers.\n                See :func:`layers.get_norm` for supported format.\n            stride_in_1x1 (bool): when stride>1, whether to put stride in the\n                first 1x1 convolution or the bottleneck 3x3 convolution.\n            dilation (int): the dilation rate of the 3x3 conv layer.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "in_channels", ",", "out_channels", ",", "stride", ")", "\n", "\n", "if", "in_channels", "!=", "out_channels", ":", "\n", "            ", "self", ".", "shortcut", "=", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "stride", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "shortcut", "=", "None", "\n", "\n", "# The original MSRA ResNet models have stride in the first 1x1 conv", "\n", "# The subsequent fb.torch.resnet and Caffe2 ResNe[X]t implementations have", "\n", "# stride in the 3x3 conv", "\n", "", "stride_1x1", ",", "stride_3x3", "=", "(", "stride", ",", "1", ")", "if", "stride_in_1x1", "else", "(", "1", ",", "stride", ")", "\n", "\n", "self", ".", "conv1", "=", "Conv2d", "(", "\n", "in_channels", ",", "\n", "bottleneck_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "stride_1x1", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "bottleneck_channels", ")", ",", "\n", ")", "\n", "\n", "self", ".", "conv2", "=", "Conv2d", "(", "\n", "bottleneck_channels", ",", "\n", "bottleneck_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "stride_3x3", ",", "\n", "padding", "=", "1", "*", "dilation", ",", "\n", "bias", "=", "False", ",", "\n", "groups", "=", "num_groups", ",", "\n", "dilation", "=", "dilation", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "bottleneck_channels", ")", ",", "\n", ")", "\n", "\n", "self", ".", "conv3", "=", "Conv2d", "(", "\n", "bottleneck_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", ")", "\n", "\n", "for", "layer", "in", "[", "self", ".", "conv1", ",", "self", ".", "conv2", ",", "self", ".", "conv3", ",", "self", ".", "shortcut", "]", ":", "\n", "            ", "if", "layer", "is", "not", "None", ":", "# shortcut can be None", "\n", "                ", "weight_init", ".", "c2_msra_fill", "(", "layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.BottleneckBlock.forward": [[194, 211], ["resnet.BottleneckBlock.conv1", "torch.relu_", "torch.relu_", "resnet.BottleneckBlock.conv2", "torch.relu_", "torch.relu_", "resnet.BottleneckBlock.conv3", "torch.relu_", "torch.relu_", "resnet.BottleneckBlock.shortcut"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "F", ".", "relu_", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "F", ".", "relu_", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "\n", "if", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "shortcut", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "shortcut", "=", "x", "\n", "\n", "", "out", "+=", "shortcut", "\n", "out", "=", "F", ".", "relu_", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.DeformBottleneckBlock.__init__": [[219, 302], ["detectron2.layers.CNNBlockBase.__init__", "detectron2.layers.Conv2d", "detectron2.layers.Conv2d", "deform_conv_op", "detectron2.layers.Conv2d", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "detectron2.layers.Conv2d", "detectron2.layers.get_norm", "detectron2.layers.get_norm", "detectron2.layers.get_norm", "fvcore.c2_msra_fill", "detectron2.layers.get_norm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "*", ",", "\n", "bottleneck_channels", ",", "\n", "stride", "=", "1", ",", "\n", "num_groups", "=", "1", ",", "\n", "norm", "=", "\"BN\"", ",", "\n", "stride_in_1x1", "=", "False", ",", "\n", "dilation", "=", "1", ",", "\n", "deform_modulated", "=", "False", ",", "\n", "deform_num_groups", "=", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "in_channels", ",", "out_channels", ",", "stride", ")", "\n", "self", ".", "deform_modulated", "=", "deform_modulated", "\n", "\n", "if", "in_channels", "!=", "out_channels", ":", "\n", "            ", "self", ".", "shortcut", "=", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "stride", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "shortcut", "=", "None", "\n", "\n", "", "stride_1x1", ",", "stride_3x3", "=", "(", "stride", ",", "1", ")", "if", "stride_in_1x1", "else", "(", "1", ",", "stride", ")", "\n", "\n", "self", ".", "conv1", "=", "Conv2d", "(", "\n", "in_channels", ",", "\n", "bottleneck_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "stride_1x1", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "bottleneck_channels", ")", ",", "\n", ")", "\n", "\n", "if", "deform_modulated", ":", "\n", "            ", "deform_conv_op", "=", "ModulatedDeformConv", "\n", "# offset channels are 2 or 3 (if with modulated) * kernel_size * kernel_size", "\n", "offset_channels", "=", "27", "\n", "", "else", ":", "\n", "            ", "deform_conv_op", "=", "DeformConv", "\n", "offset_channels", "=", "18", "\n", "\n", "", "self", ".", "conv2_offset", "=", "Conv2d", "(", "\n", "bottleneck_channels", ",", "\n", "offset_channels", "*", "deform_num_groups", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "stride_3x3", ",", "\n", "padding", "=", "1", "*", "dilation", ",", "\n", "dilation", "=", "dilation", ",", "\n", ")", "\n", "self", ".", "conv2", "=", "deform_conv_op", "(", "\n", "bottleneck_channels", ",", "\n", "bottleneck_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "stride_3x3", ",", "\n", "padding", "=", "1", "*", "dilation", ",", "\n", "bias", "=", "False", ",", "\n", "groups", "=", "num_groups", ",", "\n", "dilation", "=", "dilation", ",", "\n", "deformable_groups", "=", "deform_num_groups", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "bottleneck_channels", ")", ",", "\n", ")", "\n", "\n", "self", ".", "conv3", "=", "Conv2d", "(", "\n", "bottleneck_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", ")", "\n", "\n", "for", "layer", "in", "[", "self", ".", "conv1", ",", "self", ".", "conv2", ",", "self", ".", "conv3", ",", "self", ".", "shortcut", "]", ":", "\n", "            ", "if", "layer", "is", "not", "None", ":", "# shortcut can be None", "\n", "                ", "weight_init", ".", "c2_msra_fill", "(", "layer", ")", "\n", "\n", "", "", "nn", ".", "init", ".", "constant_", "(", "self", ".", "conv2_offset", ".", "weight", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "conv2_offset", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.DeformBottleneckBlock.forward": [[303, 328], ["resnet.DeformBottleneckBlock.conv1", "torch.relu_", "torch.relu_", "torch.relu_", "torch.relu_", "resnet.DeformBottleneckBlock.conv3", "torch.relu_", "torch.relu_", "resnet.DeformBottleneckBlock.conv2_offset", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "mask.sigmoid.sigmoid.sigmoid", "resnet.DeformBottleneckBlock.conv2", "resnet.DeformBottleneckBlock.conv2_offset", "resnet.DeformBottleneckBlock.conv2", "resnet.DeformBottleneckBlock.shortcut"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "F", ".", "relu_", "(", "out", ")", "\n", "\n", "if", "self", ".", "deform_modulated", ":", "\n", "            ", "offset_mask", "=", "self", ".", "conv2_offset", "(", "out", ")", "\n", "offset_x", ",", "offset_y", ",", "mask", "=", "torch", ".", "chunk", "(", "offset_mask", ",", "3", ",", "dim", "=", "1", ")", "\n", "offset", "=", "torch", ".", "cat", "(", "(", "offset_x", ",", "offset_y", ")", ",", "dim", "=", "1", ")", "\n", "mask", "=", "mask", ".", "sigmoid", "(", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ",", "offset", ",", "mask", ")", "\n", "", "else", ":", "\n", "            ", "offset", "=", "self", ".", "conv2_offset", "(", "out", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ",", "offset", ")", "\n", "", "out", "=", "F", ".", "relu_", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "\n", "if", "self", ".", "shortcut", "is", "not", "None", ":", "\n", "            ", "shortcut", "=", "self", ".", "shortcut", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "shortcut", "=", "x", "\n", "\n", "", "out", "+=", "shortcut", "\n", "out", "=", "F", ".", "relu_", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.BasicStem.__init__": [[336, 354], ["detectron2.layers.CNNBlockBase.__init__", "detectron2.layers.Conv2d", "fvcore.c2_msra_fill", "detectron2.layers.get_norm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "self", ",", "in_channels", "=", "3", ",", "out_channels", "=", "64", ",", "norm", "=", "\"BN\"", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            norm (str or callable): norm after the first conv layer.\n                See :func:`layers.get_norm` for supported format.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "in_channels", ",", "out_channels", ",", "4", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "conv1", "=", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "7", ",", "\n", "stride", "=", "2", ",", "\n", "padding", "=", "3", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", ")", "\n", "weight_init", ".", "c2_msra_fill", "(", "self", ".", "conv1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.BasicStem.forward": [[355, 360], ["resnet.BasicStem.conv1", "torch.relu_", "torch.relu_", "torch.max_pool2d", "torch.max_pool2d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "F", ".", "relu_", "(", "x", ")", "\n", "x", "=", "F", ".", "max_pool2d", "(", "x", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.ResNet.__init__": [[367, 434], ["backbone.Backbone.__init__", "enumerate", "tuple", "len", "resnet.ResNet.freeze", "max", "len", "torch.nn.Sequential", "torch.nn.Sequential", "resnet.ResNet.add_module", "resnet.ResNet.stage_names.append", "resnet.ResNet.stages.append", "int", "torch.nn.AdaptiveAvgPool2d", "torch.nn.AdaptiveAvgPool2d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.init.normal_", "torch.nn.init.normal_", "len", "isinstance", "str", "resnet.ResNet.named_children", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze"], ["def", "__init__", "(", "self", ",", "stem", ",", "stages", ",", "num_classes", "=", "None", ",", "out_features", "=", "None", ",", "freeze_at", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            stem (nn.Module): a stem module\n            stages (list[list[CNNBlockBase]]): several (typically 4) stages,\n                each contains multiple :class:`CNNBlockBase`.\n            num_classes (None or int): if None, will not perform classification.\n                Otherwise, will create a linear layer.\n            out_features (list[str]): name of the layers whose outputs should\n                be returned in forward. Can be anything in \"stem\", \"linear\", or \"res2\" ...\n                If None, will return the output of the last layer.\n            freeze_at (int): The number of stages at the beginning to freeze.\n                see :meth:`freeze` for detailed explanation.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stem", "=", "stem", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "current_stride", "=", "self", ".", "stem", ".", "stride", "\n", "self", ".", "_out_feature_strides", "=", "{", "\"stem\"", ":", "current_stride", "}", "\n", "self", ".", "_out_feature_channels", "=", "{", "\"stem\"", ":", "self", ".", "stem", ".", "out_channels", "}", "\n", "\n", "self", ".", "stage_names", ",", "self", ".", "stages", "=", "[", "]", ",", "[", "]", "\n", "\n", "if", "out_features", "is", "not", "None", ":", "\n", "# Avoid keeping unused layers in this module. They consume extra memory", "\n", "# and may cause allreduce to fail", "\n", "            ", "num_stages", "=", "max", "(", "\n", "[", "{", "\"res2\"", ":", "1", ",", "\"res3\"", ":", "2", ",", "\"res4\"", ":", "3", ",", "\"res5\"", ":", "4", "}", ".", "get", "(", "f", ",", "0", ")", "for", "f", "in", "out_features", "]", "\n", ")", "\n", "stages", "=", "stages", "[", ":", "num_stages", "]", "\n", "", "for", "i", ",", "blocks", "in", "enumerate", "(", "stages", ")", ":", "\n", "            ", "assert", "len", "(", "blocks", ")", ">", "0", ",", "len", "(", "blocks", ")", "\n", "for", "block", "in", "blocks", ":", "\n", "                ", "assert", "isinstance", "(", "block", ",", "CNNBlockBase", ")", ",", "block", "\n", "\n", "", "name", "=", "\"res\"", "+", "str", "(", "i", "+", "2", ")", "\n", "stage", "=", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "\n", "self", ".", "add_module", "(", "name", ",", "stage", ")", "\n", "self", ".", "stage_names", ".", "append", "(", "name", ")", "\n", "self", ".", "stages", ".", "append", "(", "stage", ")", "\n", "\n", "self", ".", "_out_feature_strides", "[", "name", "]", "=", "current_stride", "=", "int", "(", "\n", "current_stride", "*", "np", ".", "prod", "(", "[", "k", ".", "stride", "for", "k", "in", "blocks", "]", ")", "\n", ")", "\n", "self", ".", "_out_feature_channels", "[", "name", "]", "=", "curr_channels", "=", "blocks", "[", "-", "1", "]", ".", "out_channels", "\n", "", "self", ".", "stage_names", "=", "tuple", "(", "self", ".", "stage_names", ")", "# Make it static for scripting", "\n", "\n", "if", "num_classes", "is", "not", "None", ":", "\n", "            ", "self", ".", "avgpool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "1", ",", "1", ")", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "curr_channels", ",", "num_classes", ")", "\n", "\n", "# Sec 5.1 in \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\":", "\n", "# \"The 1000-way fully-connected layer is initialized by", "\n", "# drawing weights from a zero-mean Gaussian with standard deviation of 0.01.\"", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "linear", ".", "weight", ",", "std", "=", "0.01", ")", "\n", "name", "=", "\"linear\"", "\n", "\n", "", "if", "out_features", "is", "None", ":", "\n", "            ", "out_features", "=", "[", "name", "]", "\n", "", "self", ".", "_out_features", "=", "out_features", "\n", "assert", "len", "(", "self", ".", "_out_features", ")", "\n", "children", "=", "[", "x", "[", "0", "]", "for", "x", "in", "self", ".", "named_children", "(", ")", "]", "\n", "for", "out_feature", "in", "self", ".", "_out_features", ":", "\n", "            ", "assert", "out_feature", "in", "children", ",", "\"Available children: {}\"", ".", "format", "(", "\", \"", ".", "join", "(", "children", ")", ")", "\n", "", "self", ".", "freeze", "(", "freeze_at", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.ResNet.forward": [[435, 459], ["resnet.ResNet.stem", "zip", "resnet.ResNet.dim", "stage", "resnet.ResNet.avgpool", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "resnet.ResNet.linear"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\n\n        Returns:\n            dict[str->Tensor]: names and the corresponding features\n        \"\"\"", "\n", "assert", "x", ".", "dim", "(", ")", "==", "4", ",", "f\"ResNet takes an input of shape (N, C, H, W). Got {x.shape} instead!\"", "\n", "outputs", "=", "{", "}", "\n", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "if", "\"stem\"", "in", "self", ".", "_out_features", ":", "\n", "            ", "outputs", "[", "\"stem\"", "]", "=", "x", "\n", "", "for", "name", ",", "stage", "in", "zip", "(", "self", ".", "stage_names", ",", "self", ".", "stages", ")", ":", "\n", "            ", "x", "=", "stage", "(", "x", ")", "\n", "if", "name", "in", "self", ".", "_out_features", ":", "\n", "                ", "outputs", "[", "name", "]", "=", "x", "\n", "", "", "if", "self", ".", "num_classes", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "avgpool", "(", "x", ")", "\n", "x", "=", "torch", ".", "flatten", "(", "x", ",", "1", ")", "\n", "x", "=", "self", ".", "linear", "(", "x", ")", "\n", "if", "\"linear\"", "in", "self", ".", "_out_features", ":", "\n", "                ", "outputs", "[", "\"linear\"", "]", "=", "x", "\n", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.ResNet.output_shape": [[460, 466], ["detectron2.layers.ShapeSpec"], "methods", ["None"], ["", "def", "output_shape", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "name", ":", "ShapeSpec", "(", "\n", "channels", "=", "self", ".", "_out_feature_channels", "[", "name", "]", ",", "stride", "=", "self", ".", "_out_feature_strides", "[", "name", "]", "\n", ")", "\n", "for", "name", "in", "self", ".", "_out_features", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.ResNet.freeze": [[468, 491], ["enumerate", "resnet.ResNet.stem.freeze", "stage.children", "block.freeze"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze"], ["", "def", "freeze", "(", "self", ",", "freeze_at", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Freeze the first several stages of the ResNet. Commonly used in\n        fine-tuning.\n\n        Layers that produce the same feature map spatial size are defined as one\n        \"stage\" by :paper:`FPN`.\n\n        Args:\n            freeze_at (int): number of stages to freeze.\n                `1` means freezing the stem. `2` means freezing the stem and\n                one residual stage, etc.\n\n        Returns:\n            nn.Module: this ResNet itself\n        \"\"\"", "\n", "if", "freeze_at", ">=", "1", ":", "\n", "            ", "self", ".", "stem", ".", "freeze", "(", ")", "\n", "", "for", "idx", ",", "stage", "in", "enumerate", "(", "self", ".", "stages", ",", "start", "=", "2", ")", ":", "\n", "            ", "if", "freeze_at", ">=", "idx", ":", "\n", "                ", "for", "block", "in", "stage", ".", "children", "(", ")", ":", "\n", "                    ", "block", ".", "freeze", "(", ")", "\n", "", "", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.ResNet.make_stage": [[492, 546], ["range", "kwargs.items", "blocks.append", "k.endswith", "block_class", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "make_stage", "(", "block_class", ",", "num_blocks", ",", "*", ",", "in_channels", ",", "out_channels", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Create a list of blocks of the same type that forms one ResNet stage.\n\n        Args:\n            block_class (type): a subclass of CNNBlockBase that's used to create all blocks in this\n                stage. A module of this type must not change spatial resolution of inputs unless its\n                stride != 1.\n            num_blocks (int): number of blocks in this stage\n            in_channels (int): input channels of the entire stage.\n            out_channels (int): output channels of **every block** in the stage.\n            kwargs: other arguments passed to the constructor of\n                `block_class`. If the argument name is \"xx_per_block\", the\n                argument is a list of values to be passed to each block in the\n                stage. Otherwise, the same argument is passed to every block\n                in the stage.\n\n        Returns:\n            list[CNNBlockBase]: a list of block module.\n\n        Examples:\n        ::\n            stage = ResNet.make_stage(\n                BottleneckBlock, 3, in_channels=16, out_channels=64,\n                bottleneck_channels=16, num_groups=1,\n                stride_per_block=[2, 1, 1],\n                dilations_per_block=[1, 1, 2]\n            )\n\n        Usually, layers that produce the same feature map spatial size are defined as one\n        \"stage\" (in :paper:`FPN`). Under such definition, ``stride_per_block[1:]`` should\n        all be 1.\n        \"\"\"", "\n", "blocks", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_blocks", ")", ":", "\n", "            ", "curr_kwargs", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "                ", "if", "k", ".", "endswith", "(", "\"_per_block\"", ")", ":", "\n", "                    ", "assert", "len", "(", "v", ")", "==", "num_blocks", ",", "(", "\n", "f\"Argument '{k}' of make_stage should have the \"", "\n", "f\"same length as num_blocks={num_blocks}.\"", "\n", ")", "\n", "newk", "=", "k", "[", ":", "-", "len", "(", "\"_per_block\"", ")", "]", "\n", "assert", "newk", "not", "in", "kwargs", ",", "f\"Cannot call make_stage with both {k} and {newk}!\"", "\n", "curr_kwargs", "[", "newk", "]", "=", "v", "[", "i", "]", "\n", "", "else", ":", "\n", "                    ", "curr_kwargs", "[", "k", "]", "=", "v", "\n", "\n", "", "", "blocks", ".", "append", "(", "\n", "block_class", "(", "in_channels", "=", "in_channels", ",", "out_channels", "=", "out_channels", ",", "**", "curr_kwargs", ")", "\n", ")", "\n", "in_channels", "=", "out_channels", "\n", "", "return", "blocks", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.ResNet.make_default_stages": [[547, 598], ["zip", "ret.append", "resnet.ResNet.make_stage"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.make_stage"], ["", "@", "staticmethod", "\n", "def", "make_default_stages", "(", "depth", ",", "block_class", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Created list of ResNet stages from pre-defined depth (one of 18, 34, 50, 101, 152).\n        If it doesn't create the ResNet variant you need, please use :meth:`make_stage`\n        instead for fine-grained customization.\n\n        Args:\n            depth (int): depth of ResNet\n            block_class (type): the CNN block class. Has to accept\n                `bottleneck_channels` argument for depth > 50.\n                By default it is BasicBlock or BottleneckBlock, based on the\n                depth.\n            kwargs:\n                other arguments to pass to `make_stage`. Should not contain\n                stride and channels, as they are predefined for each depth.\n\n        Returns:\n            list[list[CNNBlockBase]]: modules in all stages; see arguments of\n                :class:`ResNet.__init__`.\n        \"\"\"", "\n", "num_blocks_per_stage", "=", "{", "\n", "18", ":", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "34", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "50", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "101", ":", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "152", ":", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "\n", "}", "[", "depth", "]", "\n", "if", "block_class", "is", "None", ":", "\n", "            ", "block_class", "=", "BasicBlock", "if", "depth", "<", "50", "else", "BottleneckBlock", "\n", "", "if", "depth", "<", "50", ":", "\n", "            ", "in_channels", "=", "[", "64", ",", "64", ",", "128", ",", "256", "]", "\n", "out_channels", "=", "[", "64", ",", "128", ",", "256", ",", "512", "]", "\n", "", "else", ":", "\n", "            ", "in_channels", "=", "[", "64", ",", "256", ",", "512", ",", "1024", "]", "\n", "out_channels", "=", "[", "256", ",", "512", ",", "1024", ",", "2048", "]", "\n", "", "ret", "=", "[", "]", "\n", "for", "(", "n", ",", "s", ",", "i", ",", "o", ")", "in", "zip", "(", "num_blocks_per_stage", ",", "[", "1", ",", "2", ",", "2", ",", "2", "]", ",", "in_channels", ",", "out_channels", ")", ":", "\n", "            ", "if", "depth", ">=", "50", ":", "\n", "                ", "kwargs", "[", "\"bottleneck_channels\"", "]", "=", "o", "//", "4", "\n", "", "ret", ".", "append", "(", "\n", "ResNet", ".", "make_stage", "(", "\n", "block_class", "=", "block_class", ",", "\n", "num_blocks", "=", "n", ",", "\n", "stride_per_block", "=", "[", "s", "]", "+", "[", "1", "]", "*", "(", "n", "-", "1", ")", ",", "\n", "in_channels", "=", "i", ",", "\n", "out_channels", "=", "o", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.make_stage": [[606, 611], ["resnet.ResNet.make_stage"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.make_stage"], ["def", "make_stage", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Deprecated alias for backward compatibiltiy.\n    \"\"\"", "\n", "return", "ResNet", ".", "make_stage", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.build_resnet_backbone": [[613, 695], ["build.BACKBONE_REGISTRY.register", "resnet.BasicStem", "enumerate", "resnet.ResNet", "range", "resnet.ResNet.make_stage", "stages.append", "any"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.make_stage"], ["", "@", "BACKBONE_REGISTRY", ".", "register", "(", ")", "\n", "def", "build_resnet_backbone", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Create a ResNet instance from config.\n\n    Returns:\n        ResNet: a :class:`ResNet` instance.\n    \"\"\"", "\n", "# need registration of new blocks/stems?", "\n", "norm", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "NORM", "\n", "stem", "=", "BasicStem", "(", "\n", "in_channels", "=", "input_shape", ".", "channels", ",", "\n", "out_channels", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "STEM_OUT_CHANNELS", ",", "\n", "norm", "=", "norm", ",", "\n", ")", "\n", "\n", "# fmt: off", "\n", "freeze_at", "=", "cfg", ".", "MODEL", ".", "BACKBONE", ".", "FREEZE_AT", "\n", "out_features", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "OUT_FEATURES", "\n", "depth", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEPTH", "\n", "num_groups", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "NUM_GROUPS", "\n", "width_per_group", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "WIDTH_PER_GROUP", "\n", "bottleneck_channels", "=", "num_groups", "*", "width_per_group", "\n", "in_channels", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "STEM_OUT_CHANNELS", "\n", "out_channels", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "RES2_OUT_CHANNELS", "\n", "stride_in_1x1", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "STRIDE_IN_1X1", "\n", "res5_dilation", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "RES5_DILATION", "\n", "deform_on_per_stage", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEFORM_ON_PER_STAGE", "\n", "deform_modulated", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEFORM_MODULATED", "\n", "deform_num_groups", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEFORM_NUM_GROUPS", "\n", "# fmt: on", "\n", "assert", "res5_dilation", "in", "{", "1", ",", "2", "}", ",", "\"res5_dilation cannot be {}.\"", ".", "format", "(", "res5_dilation", ")", "\n", "\n", "num_blocks_per_stage", "=", "{", "\n", "18", ":", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "34", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "50", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "101", ":", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "152", ":", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "\n", "}", "[", "depth", "]", "\n", "\n", "if", "depth", "in", "[", "18", ",", "34", "]", ":", "\n", "        ", "assert", "out_channels", "==", "64", ",", "\"Must set MODEL.RESNETS.RES2_OUT_CHANNELS = 64 for R18/R34\"", "\n", "assert", "not", "any", "(", "\n", "deform_on_per_stage", "\n", ")", ",", "\"MODEL.RESNETS.DEFORM_ON_PER_STAGE unsupported for R18/R34\"", "\n", "assert", "res5_dilation", "==", "1", ",", "\"Must set MODEL.RESNETS.RES5_DILATION = 1 for R18/R34\"", "\n", "assert", "num_groups", "==", "1", ",", "\"Must set MODEL.RESNETS.NUM_GROUPS = 1 for R18/R34\"", "\n", "\n", "", "stages", "=", "[", "]", "\n", "\n", "for", "idx", ",", "stage_idx", "in", "enumerate", "(", "range", "(", "2", ",", "6", ")", ")", ":", "\n", "# res5_dilation is used this way as a convention in R-FCN & Deformable Conv paper", "\n", "        ", "dilation", "=", "res5_dilation", "if", "stage_idx", "==", "5", "else", "1", "\n", "first_stride", "=", "1", "if", "idx", "==", "0", "or", "(", "stage_idx", "==", "5", "and", "dilation", "==", "2", ")", "else", "2", "\n", "stage_kargs", "=", "{", "\n", "\"num_blocks\"", ":", "num_blocks_per_stage", "[", "idx", "]", ",", "\n", "\"stride_per_block\"", ":", "[", "first_stride", "]", "+", "[", "1", "]", "*", "(", "num_blocks_per_stage", "[", "idx", "]", "-", "1", ")", ",", "\n", "\"in_channels\"", ":", "in_channels", ",", "\n", "\"out_channels\"", ":", "out_channels", ",", "\n", "\"norm\"", ":", "norm", ",", "\n", "}", "\n", "# Use BasicBlock for R18 and R34.", "\n", "if", "depth", "in", "[", "18", ",", "34", "]", ":", "\n", "            ", "stage_kargs", "[", "\"block_class\"", "]", "=", "BasicBlock", "\n", "", "else", ":", "\n", "            ", "stage_kargs", "[", "\"bottleneck_channels\"", "]", "=", "bottleneck_channels", "\n", "stage_kargs", "[", "\"stride_in_1x1\"", "]", "=", "stride_in_1x1", "\n", "stage_kargs", "[", "\"dilation\"", "]", "=", "dilation", "\n", "stage_kargs", "[", "\"num_groups\"", "]", "=", "num_groups", "\n", "if", "deform_on_per_stage", "[", "idx", "]", ":", "\n", "                ", "stage_kargs", "[", "\"block_class\"", "]", "=", "DeformBottleneckBlock", "\n", "stage_kargs", "[", "\"deform_modulated\"", "]", "=", "deform_modulated", "\n", "stage_kargs", "[", "\"deform_num_groups\"", "]", "=", "deform_num_groups", "\n", "", "else", ":", "\n", "                ", "stage_kargs", "[", "\"block_class\"", "]", "=", "BottleneckBlock", "\n", "", "", "blocks", "=", "ResNet", ".", "make_stage", "(", "**", "stage_kargs", ")", "\n", "in_channels", "=", "out_channels", "\n", "out_channels", "*=", "2", "\n", "bottleneck_channels", "*=", "2", "\n", "stages", ".", "append", "(", "blocks", ")", "\n", "", "return", "ResNet", "(", "stem", ",", "stages", ",", "out_features", "=", "out_features", ",", "freeze_at", "=", "freeze_at", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.Bottleneck.__init__": [[17, 55], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.ReLU", "torch.nn.ReLU", "detectron2.layers.blocks.FrozenBatchNorm2d", "detectron2.layers.blocks.FrozenBatchNorm2d", "torch.nn.AvgPool2d", "torch.nn.AvgPool2d", "torch.nn.Identity", "torch.nn.Identity", "detectron2.layers.blocks.FrozenBatchNorm2d", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "detectron2.layers.blocks.FrozenBatchNorm2d", "collections.OrderedDict", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "torch.nn.AvgPool2d", "torch.nn.AvgPool2d", "torch.nn.Conv2d", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "norm_type", "=", "'FronzenBN'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "inplanes", ",", "planes", ",", "1", ",", "bias", "=", "False", ")", "\n", "if", "norm_type", "==", "'FronzenBN'", ":", "\n", "            ", "self", ".", "bn1", "=", "FrozenBatchNorm2d", "(", "planes", ")", "# nn.BatchNorm2d(planes)", "\n", "", "elif", "norm_type", "==", "'SyncBN'", ":", "\n", "            ", "self", ".", "bn1", "=", "nn", ".", "SyncBatchNorm", "(", "planes", ")", "\n", "\n", "", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "planes", ",", "planes", ",", "3", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "if", "norm_type", "==", "'FronzenBN'", ":", "\n", "            ", "self", ".", "bn2", "=", "FrozenBatchNorm2d", "(", "planes", ")", "# nn.BatchNorm2d(planes)", "\n", "", "elif", "norm_type", "==", "'SyncBN'", ":", "\n", "            ", "self", ".", "bn2", "=", "nn", ".", "SyncBatchNorm", "(", "planes", ")", "\n", "\n", "", "self", ".", "avgpool", "=", "nn", ".", "AvgPool2d", "(", "stride", ")", "if", "stride", ">", "1", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "planes", ",", "planes", "*", "self", ".", "expansion", ",", "1", ",", "bias", "=", "False", ")", "\n", "if", "norm_type", "==", "'FronzenBN'", ":", "\n", "            ", "self", ".", "bn3", "=", "FrozenBatchNorm2d", "(", "planes", "*", "self", ".", "expansion", ")", "# nn.BatchNorm2d(planes * self.expansion)", "\n", "", "elif", "norm_type", "==", "'SyncBN'", ":", "\n", "            ", "self", ".", "bn3", "=", "nn", ".", "SyncBatchNorm", "(", "planes", "*", "self", ".", "expansion", ")", "\n", "\n", "", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "None", "\n", "self", ".", "stride", "=", "stride", "\n", "\n", "if", "stride", ">", "1", "or", "inplanes", "!=", "planes", "*", "Bottleneck", ".", "expansion", ":", "\n", "# downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1", "\n", "            ", "if", "norm_type", "==", "'FronzenBN'", ":", "\n", "                ", "this_norm", "=", "FrozenBatchNorm2d", "(", "planes", "*", "self", ".", "expansion", ")", "#(\"1\", nn.BatchNorm2d(planes * self.expansion))", "\n", "", "elif", "norm_type", "==", "'SyncBN'", ":", "\n", "                ", "this_norm", "=", "nn", ".", "SyncBatchNorm", "(", "planes", "*", "self", ".", "expansion", ")", "\n", "", "self", ".", "downsample", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "\"-1\"", ",", "nn", ".", "AvgPool2d", "(", "stride", ")", ")", ",", "\n", "(", "\"0\"", ",", "nn", ".", "Conv2d", "(", "inplanes", ",", "planes", "*", "self", ".", "expansion", ",", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ")", ")", ",", "\n", "(", "\"1\"", ",", "this_norm", ")", ",", "#(\"1\", nn.BatchNorm2d(planes * self.expansion))", "\n", "]", ")", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.Bottleneck.forward": [[57, 71], ["clip_backbone.Bottleneck.relu", "clip_backbone.Bottleneck.relu", "clip_backbone.Bottleneck.avgpool", "clip_backbone.Bottleneck.bn3", "clip_backbone.Bottleneck.relu", "clip_backbone.Bottleneck.bn1", "clip_backbone.Bottleneck.bn2", "clip_backbone.Bottleneck.conv3", "clip_backbone.Bottleneck.downsample", "clip_backbone.Bottleneck.conv1", "clip_backbone.Bottleneck.conv2"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "relu", "(", "self", ".", "bn1", "(", "self", ".", "conv1", "(", "x", ")", ")", ")", "\n", "out", "=", "self", ".", "relu", "(", "self", ".", "bn2", "(", "self", ".", "conv2", "(", "out", ")", ")", ")", "\n", "out", "=", "self", ".", "avgpool", "(", "out", ")", "\n", "out", "=", "self", ".", "bn3", "(", "self", ".", "conv3", "(", "out", ")", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.AttentionPool2d.__init__": [[74, 82], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "spacial_dim", ":", "int", ",", "embed_dim", ":", "int", ",", "num_heads", ":", "int", ",", "output_dim", ":", "int", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "positional_embedding", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "spacial_dim", "**", "2", "+", "1", ",", "embed_dim", ")", "/", "embed_dim", "**", "0.5", ")", "\n", "self", ".", "k_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "q_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "v_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "c_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "output_dim", "or", "embed_dim", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.AttentionPool2d.forward": [[83, 108], ["torch.cat.reshape().permute", "torch.cat.reshape().permute", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.multi_head_attention_forward", "torch.multi_head_attention_forward", "clip_backbone.AttentionPool2d.positional_embedding[].to", "torch.cat.reshape", "torch.cat.reshape", "torch.cat.mean", "torch.cat.mean", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", ".", "reshape", "(", "x", ".", "shape", "[", "0", "]", ",", "x", ".", "shape", "[", "1", "]", ",", "x", ".", "shape", "[", "2", "]", "*", "x", ".", "shape", "[", "3", "]", ")", ".", "permute", "(", "2", ",", "0", ",", "1", ")", "# NCHW -> (HW)NC", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ".", "mean", "(", "dim", "=", "0", ",", "keepdim", "=", "True", ")", ",", "x", "]", ",", "dim", "=", "0", ")", "# (HW+1)NC", "\n", "x", "=", "x", "+", "self", ".", "positional_embedding", "[", ":", ",", "None", ",", ":", "]", ".", "to", "(", "x", ".", "dtype", ")", "# (HW+1)NC", "\n", "x", ",", "_", "=", "F", ".", "multi_head_attention_forward", "(", "\n", "query", "=", "x", ",", "key", "=", "x", ",", "value", "=", "x", ",", "\n", "embed_dim_to_check", "=", "x", ".", "shape", "[", "-", "1", "]", ",", "\n", "num_heads", "=", "self", ".", "num_heads", ",", "\n", "q_proj_weight", "=", "self", ".", "q_proj", ".", "weight", ",", "\n", "k_proj_weight", "=", "self", ".", "k_proj", ".", "weight", ",", "\n", "v_proj_weight", "=", "self", ".", "v_proj", ".", "weight", ",", "\n", "in_proj_weight", "=", "None", ",", "\n", "in_proj_bias", "=", "torch", ".", "cat", "(", "[", "self", ".", "q_proj", ".", "bias", ",", "self", ".", "k_proj", ".", "bias", ",", "self", ".", "v_proj", ".", "bias", "]", ")", ",", "\n", "bias_k", "=", "None", ",", "\n", "bias_v", "=", "None", ",", "\n", "add_zero_attn", "=", "False", ",", "\n", "dropout_p", "=", "0", ",", "\n", "out_proj_weight", "=", "self", ".", "c_proj", ".", "weight", ",", "\n", "out_proj_bias", "=", "self", ".", "c_proj", ".", "bias", ",", "\n", "use_separate_proj_weight", "=", "True", ",", "\n", "training", "=", "self", ".", "training", ",", "\n", "need_weights", "=", "False", "\n", ")", "\n", "\n", "return", "x", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet.__init__": [[125, 182], ["backbone.Backbone.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.AvgPool2d", "torch.nn.AvgPool2d", "torch.nn.ReLU", "torch.nn.ReLU", "clip_backbone.ModifiedResNet._make_layer", "clip_backbone.ModifiedResNet._make_layer", "clip_backbone.ModifiedResNet._make_layer", "clip_backbone.ModifiedResNet.freeze", "detectron2.layers.blocks.FrozenBatchNorm2d", "detectron2.layers.blocks.FrozenBatchNorm2d", "detectron2.layers.blocks.FrozenBatchNorm2d", "clip_backbone.ModifiedResNet._make_layer", "clip_backbone.ModifiedResNet._make_layer", "clip_backbone.AttentionPool2d", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm", "torch.nn.SyncBatchNorm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet._make_layer", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet._make_layer", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet._make_layer", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet._make_layer", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet._make_layer"], ["def", "__init__", "(", "self", ",", "layers", ",", "output_dim", ",", "heads", ",", "input_resolution", "=", "224", ",", "width", "=", "64", ",", "\n", "out_features", "=", "None", ",", "freeze_at", "=", "0", ",", "depth", "=", "None", ",", "pool_vec", "=", "True", ",", "create_att_pool", "=", "False", ",", "norm_type", "=", "'FronzenBN'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "self", ".", "input_resolution", "=", "input_resolution", "\n", "self", ".", "norm_type", "=", "norm_type", "\n", "\n", "# the 3-layer stem", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "3", ",", "width", "//", "2", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "if", "norm_type", "==", "'FronzenBN'", ":", "\n", "            ", "self", ".", "bn1", "=", "FrozenBatchNorm2d", "(", "width", "//", "2", ")", "# nn.BatchNorm2d(width // 2)", "\n", "", "elif", "norm_type", "==", "'SyncBN'", ":", "\n", "            ", "self", ".", "bn1", "=", "nn", ".", "SyncBatchNorm", "(", "width", "//", "2", ")", "\n", "", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "width", "//", "2", ",", "width", "//", "2", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "if", "norm_type", "==", "'FronzenBN'", ":", "\n", "            ", "self", ".", "bn2", "=", "FrozenBatchNorm2d", "(", "width", "//", "2", ")", "# nn.BatchNorm2d(width // 2)", "\n", "", "elif", "norm_type", "==", "'SyncBN'", ":", "\n", "            ", "self", ".", "bn2", "=", "nn", ".", "SyncBatchNorm", "(", "width", "//", "2", ")", "\n", "", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "width", "//", "2", ",", "width", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "if", "norm_type", "==", "'FronzenBN'", ":", "\n", "            ", "self", ".", "bn3", "=", "FrozenBatchNorm2d", "(", "width", ")", "# nn.BatchNorm2d(width)", "\n", "", "elif", "norm_type", "==", "'SyncBN'", ":", "\n", "            ", "self", ".", "bn3", "=", "nn", ".", "SyncBatchNorm", "(", "width", ")", "\n", "", "self", ".", "avgpool", "=", "nn", ".", "AvgPool2d", "(", "2", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "# residual layers", "\n", "self", ".", "_inplanes", "=", "width", "# this is a *mutable* variable used during construction", "\n", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "width", ",", "layers", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "self", ".", "_make_layer", "(", "width", "*", "2", ",", "layers", "[", "1", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "layer3", "=", "self", ".", "_make_layer", "(", "width", "*", "4", ",", "layers", "[", "2", "]", ",", "stride", "=", "2", ")", "\n", "if", "'res5'", "in", "out_features", ":", "# FPN", "\n", "            ", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "width", "*", "8", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ")", "\n", "", "else", ":", "# C4, layer4 created here won't be used in backbone, but used in roi_head", "\n", "            ", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "width", "*", "8", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ")", "# None", "\n", "\n", "", "self", ".", "pool_vec", "=", "pool_vec", "\n", "if", "self", ".", "pool_vec", "or", "create_att_pool", ":", "# pool a vector representation for an image", "\n", "            ", "embed_dim", "=", "width", "*", "32", "# the ResNet feature dimension", "\n", "self", ".", "attnpool", "=", "AttentionPool2d", "(", "input_resolution", "//", "32", ",", "embed_dim", ",", "heads", ",", "output_dim", ")", "\n", "# if create_att_pool:  # freeze attnpool layer", "\n", "#     for p in self.attnpool.parameters(): p.requires_grad = False", "\n", "\n", "", "self", ".", "_out_features", "=", "out_features", "if", "out_features", "else", "[", "]", "\n", "if", "depth", "in", "[", "50", ",", "101", "]", ":", "# resnet50 or resnet 101", "\n", "# FPN: [\"res2\", \"res3\", \"res4\", \"res5\"]; C4: [\"res4\"]", "\n", "            ", "self", ".", "_out_feature_channels", "=", "{", "'stem'", ":", "64", ",", "'res2'", ":", "256", ",", "'res3'", ":", "512", ",", "'res4'", ":", "1024", ",", "'res5'", ":", "2048", "}", "if", "'res5'", "in", "self", ".", "_out_features", "else", "{", "'stem'", ":", "64", ",", "'res2'", ":", "256", ",", "'res3'", ":", "512", ",", "'res4'", ":", "1024", "}", "\n", "self", ".", "_out_feature_strides", "=", "{", "'stem'", ":", "4", ",", "'res2'", ":", "4", ",", "'res3'", ":", "8", ",", "'res4'", ":", "16", ",", "'res5'", ":", "32", "}", "if", "'res5'", "in", "self", ".", "_out_features", "else", "{", "'stem'", ":", "4", ",", "'res2'", ":", "4", ",", "'res3'", ":", "8", ",", "'res4'", ":", "16", "}", "# anti-aliasing strided conv???        ", "\n", "", "elif", "depth", "in", "[", "200", "]", ":", "# resnet50x4", "\n", "# FPN: [\"res2\", \"res3\", \"res4\", \"res5\"]; C4: [\"res4\"]", "\n", "            ", "self", ".", "_out_feature_channels", "=", "{", "'stem'", ":", "80", ",", "'res2'", ":", "320", ",", "'res3'", ":", "640", ",", "'res4'", ":", "1280", ",", "'res5'", ":", "2560", "}", "if", "'res5'", "in", "self", ".", "_out_features", "else", "{", "'stem'", ":", "80", ",", "'res2'", ":", "320", ",", "'res3'", ":", "640", ",", "'res4'", ":", "1280", "}", "\n", "self", ".", "_out_feature_strides", "=", "{", "'stem'", ":", "4", ",", "'res2'", ":", "4", ",", "'res3'", ":", "8", ",", "'res4'", ":", "16", ",", "'res5'", ":", "32", "}", "if", "'res5'", "in", "self", ".", "_out_features", "else", "{", "'stem'", ":", "4", ",", "'res2'", ":", "4", ",", "'res3'", ":", "8", ",", "'res4'", ":", "16", "}", "# anti-aliasing strided conv???        ", "\n", "", "self", ".", "freeze", "(", "freeze_at", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet._make_layer": [[184, 192], ["range", "torch.nn.Sequential", "torch.nn.Sequential", "clip_backbone.Bottleneck", "layers.append", "clip_backbone.Bottleneck"], "methods", ["None"], ["", "def", "_make_layer", "(", "self", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ")", ":", "\n", "        ", "layers", "=", "[", "Bottleneck", "(", "self", ".", "_inplanes", ",", "planes", ",", "stride", ",", "norm_type", "=", "self", ".", "norm_type", ")", "]", "\n", "\n", "self", ".", "_inplanes", "=", "planes", "*", "Bottleneck", ".", "expansion", "\n", "for", "_", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "Bottleneck", "(", "self", ".", "_inplanes", ",", "planes", ",", "norm_type", "=", "self", ".", "norm_type", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet.forward": [[193, 220], ["clip_backbone.ModifiedResNet.type", "clip_backbone.ModifiedResNet.forward.stem"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "def", "stem", "(", "x", ")", ":", "\n", "            ", "for", "conv", ",", "bn", "in", "[", "(", "self", ".", "conv1", ",", "self", ".", "bn1", ")", ",", "(", "self", ".", "conv2", ",", "self", ".", "bn2", ")", ",", "(", "self", ".", "conv3", ",", "self", ".", "bn3", ")", "]", ":", "\n", "                ", "x", "=", "self", ".", "relu", "(", "bn", "(", "conv", "(", "x", ")", ")", ")", "\n", "", "x", "=", "self", ".", "avgpool", "(", "x", ")", "\n", "return", "x", "\n", "\n", "", "assert", "x", ".", "dim", "(", ")", "==", "4", ",", "f\"ResNet takes an input of shape (N, C, H, W). Got {x.shape} instead!\"", "\n", "outputs", "=", "{", "}", "\n", "x", "=", "x", ".", "type", "(", "self", ".", "conv1", ".", "weight", ".", "dtype", ")", "# det2 resnet50: [3, 800, 1216]; CLIP resnet50: [3, 224, 224]", "\n", "x", "=", "stem", "(", "x", ")", "# det2 resnet50: [64, 200, 304]; CLIP resnet50: [64, 56, 56]", "\n", "if", "\"stem\"", "in", "self", ".", "_out_features", ":", "\n", "            ", "outputs", "[", "\"stem\"", "]", "=", "x", "\n", "", "x", "=", "self", ".", "layer1", "(", "x", ")", "# det2 resnet50: [256, 200, 304]; CLIP resnet50: [256, 56, 56]", "\n", "outputs", "[", "'res2'", "]", "=", "x", "if", "\"res2\"", "in", "self", ".", "_out_features", "else", "None", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "# det2 resnet50: [512, 100, 152]; CLIP resnet50: [512, 28, 28]", "\n", "outputs", "[", "'res3'", "]", "=", "x", "if", "\"res3\"", "in", "self", ".", "_out_features", "else", "None", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "# det2 resnet50: [1024, 50, 76]; CLIP resnet50: [1024, 14, 14]", "\n", "outputs", "[", "'res4'", "]", "=", "x", "if", "\"res4\"", "in", "self", ".", "_out_features", "else", "None", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "if", "\"res5\"", "in", "self", ".", "_out_features", "else", "x", "# det2 resnet50: [2048, 25, 38]; CLIP resnet50: [2048, 7, 7]", "\n", "outputs", "[", "'res5'", "]", "=", "x", "if", "\"res5\"", "in", "self", ".", "_out_features", "else", "None", "\n", "\n", "if", "self", ".", "pool_vec", ":", "# pool a vector representation for an image, for global image classification", "\n", "            ", "x", "=", "self", ".", "attnpool", "(", "x", ")", "# CLIP resnet50: [1024]", "\n", "return", "x", "\n", "", "else", ":", "# for FPN", "\n", "            ", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet.freeze": [[221, 263], ["enumerate", "nn_module.parameters", "detectron2.layers.blocks.FrozenBatchNorm2d.convert_frozen_batchnorm", "clip_backbone.ModifiedResNet.freeze.cnnblockbase_freeze"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.FrozenBatchNorm2d.convert_frozen_batchnorm"], ["", "", "def", "freeze", "(", "self", ",", "freeze_at", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Freeze the first several stages of the ResNet. Commonly used in\n        fine-tuning.\n\n        Layers that produce the same feature map spatial size are defined as one\n        \"stage\" by :paper:`FPN`.\n\n        Args:\n            freeze_at (int): number of stages to freeze.\n                `1` means freezing the stem. `2` means freezing the stem and\n                one residual stage, etc.\n\n        Returns:\n            nn.Module: this ResNet itself\n        \"\"\"", "\n", "def", "cnnblockbase_freeze", "(", "nn_module", ")", ":", "\n", "            ", "\"\"\"\n            Make this block not trainable.\n            This method sets all parameters to `requires_grad=False`,\n            and convert all BatchNorm layers to FrozenBatchNorm\n\n            Returns:\n                the block itself\n            \"\"\"", "\n", "for", "p", "in", "nn_module", ".", "parameters", "(", ")", ":", "\n", "                ", "p", ".", "requires_grad", "=", "False", "\n", "", "FrozenBatchNorm2d", ".", "convert_frozen_batchnorm", "(", "nn_module", ")", "\n", "\n", "", "if", "freeze_at", ">=", "1", ":", "# stem", "\n", "            ", "cnnblockbase_freeze", "(", "self", ".", "conv1", ")", "\n", "cnnblockbase_freeze", "(", "self", ".", "bn1", ")", "\n", "cnnblockbase_freeze", "(", "self", ".", "conv2", ")", "\n", "cnnblockbase_freeze", "(", "self", ".", "bn2", ")", "\n", "cnnblockbase_freeze", "(", "self", ".", "conv3", ")", "\n", "cnnblockbase_freeze", "(", "self", ".", "bn3", ")", "\n", "# each stage is a torch.nn.modules.container.Sequential", "\n", "", "for", "idx", ",", "stage", "in", "enumerate", "(", "[", "self", ".", "layer1", ",", "self", ".", "layer2", ",", "self", ".", "layer3", ",", "self", ".", "layer4", "]", ",", "start", "=", "2", ")", ":", "\n", "            ", "if", "freeze_at", ">=", "idx", ":", "\n", "                ", "for", "block", "in", "stage", ".", "children", "(", ")", ":", "# each block is a Bottleneck", "\n", "                    ", "cnnblockbase_freeze", "(", "block", ")", "\n", "", "", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ModifiedResNet.output_shape": [[264, 270], ["detectron2.layers.ShapeSpec"], "methods", ["None"], ["", "def", "output_shape", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "name", ":", "ShapeSpec", "(", "\n", "channels", "=", "self", ".", "_out_feature_channels", "[", "name", "]", ",", "stride", "=", "self", ".", "_out_feature_strides", "[", "name", "]", "\n", ")", "\n", "for", "name", "in", "self", ".", "_out_features", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.LayerNorm.forward": [[276, 280], ["super().forward", "super().forward.type", "x.type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward"], ["def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "orig_type", "=", "x", ".", "dtype", "\n", "ret", "=", "super", "(", ")", ".", "forward", "(", "x", ".", "type", "(", "torch", ".", "float32", ")", ")", "\n", "return", "ret", ".", "type", "(", "orig_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.QuickGELU.forward": [[283, 285], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "x", "*", "torch", ".", "sigmoid", "(", "1.702", "*", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ResidualAttentionBlock.__init__": [[288, 300], ["torch.nn.Module.__init__", "torch.nn.MultiheadAttention", "torch.nn.MultiheadAttention", "clip_backbone.LayerNorm", "torch.nn.Sequential", "torch.nn.Sequential", "clip_backbone.LayerNorm", "collections.OrderedDict", "torch.nn.Linear", "torch.nn.Linear", "clip_backbone.QuickGELU", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ":", "int", ",", "n_head", ":", "int", ",", "attn_mask", ":", "torch", ".", "Tensor", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "attn", "=", "nn", ".", "MultiheadAttention", "(", "d_model", ",", "n_head", ")", "\n", "self", ".", "ln_1", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "mlp", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "\"c_fc\"", ",", "nn", ".", "Linear", "(", "d_model", ",", "d_model", "*", "4", ")", ")", ",", "\n", "(", "\"gelu\"", ",", "QuickGELU", "(", ")", ")", ",", "\n", "(", "\"c_proj\"", ",", "nn", ".", "Linear", "(", "d_model", "*", "4", ",", "d_model", ")", ")", "\n", "]", ")", ")", "\n", "self", ".", "ln_2", "=", "LayerNorm", "(", "d_model", ")", "\n", "self", ".", "attn_mask", "=", "attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ResidualAttentionBlock.attention": [[301, 304], ["clip_backbone.ResidualAttentionBlock.attn_mask.to", "clip_backbone.ResidualAttentionBlock.attn"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "attention", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "self", ".", "attn_mask", "=", "self", ".", "attn_mask", ".", "to", "(", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "if", "self", ".", "attn_mask", "is", "not", "None", "else", "None", "\n", "return", "self", ".", "attn", "(", "x", ",", "x", ",", "x", ",", "need_weights", "=", "False", ",", "attn_mask", "=", "self", ".", "attn_mask", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ResidualAttentionBlock.forward": [[305, 309], ["clip_backbone.ResidualAttentionBlock.attention", "clip_backbone.ResidualAttentionBlock.mlp", "clip_backbone.ResidualAttentionBlock.ln_1", "clip_backbone.ResidualAttentionBlock.ln_2"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.ResidualAttentionBlock.attention"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "attention", "(", "self", ".", "ln_1", "(", "x", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "mlp", "(", "self", ".", "ln_2", "(", "x", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.Transformer.__init__": [[312, 317], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "clip_backbone.ResidualAttentionBlock", "range"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "width", ":", "int", ",", "layers", ":", "int", ",", "heads", ":", "int", ",", "attn_mask", ":", "torch", ".", "Tensor", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "width", "=", "width", "\n", "self", ".", "layers", "=", "layers", "\n", "self", ".", "resblocks", "=", "nn", ".", "Sequential", "(", "*", "[", "ResidualAttentionBlock", "(", "width", ",", "heads", ",", "attn_mask", ")", "for", "_", "in", "range", "(", "layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.Transformer.forward": [[318, 320], ["clip_backbone.Transformer.resblocks"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "self", ".", "resblocks", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.VisualTransformer.__init__": [[323, 338], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "clip_backbone.LayerNorm", "clip_backbone.Transformer", "clip_backbone.LayerNorm", "torch.nn.Parameter", "torch.nn.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_resolution", ":", "int", ",", "patch_size", ":", "int", ",", "width", ":", "int", ",", "layers", ":", "int", ",", "heads", ":", "int", ",", "output_dim", ":", "int", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_resolution", "=", "input_resolution", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "3", ",", "out_channels", "=", "width", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ",", "bias", "=", "False", ")", "\n", "\n", "scale", "=", "width", "**", "-", "0.5", "\n", "self", ".", "class_embedding", "=", "nn", ".", "Parameter", "(", "scale", "*", "torch", ".", "randn", "(", "width", ")", ")", "\n", "self", ".", "positional_embedding", "=", "nn", ".", "Parameter", "(", "scale", "*", "torch", ".", "randn", "(", "(", "input_resolution", "//", "patch_size", ")", "**", "2", "+", "1", ",", "width", ")", ")", "\n", "self", ".", "ln_pre", "=", "LayerNorm", "(", "width", ")", "\n", "\n", "self", ".", "transformer", "=", "Transformer", "(", "width", ",", "layers", ",", "heads", ")", "\n", "\n", "self", ".", "ln_post", "=", "LayerNorm", "(", "width", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Parameter", "(", "scale", "*", "torch", ".", "randn", "(", "width", ",", "output_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.VisualTransformer.forward": [[339, 357], ["clip_backbone.VisualTransformer.conv1", "clip_backbone.VisualTransformer.reshape", "clip_backbone.VisualTransformer.permute", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "clip_backbone.VisualTransformer.ln_pre", "clip_backbone.VisualTransformer.permute", "clip_backbone.VisualTransformer.transformer", "clip_backbone.VisualTransformer.permute", "clip_backbone.VisualTransformer.ln_post", "clip_backbone.VisualTransformer.positional_embedding.to", "clip_backbone.VisualTransformer.class_embedding.to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "# shape = [*, width, grid, grid]", "\n", "x", "=", "x", ".", "reshape", "(", "x", ".", "shape", "[", "0", "]", ",", "x", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "# shape = [*, width, grid ** 2]", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# shape = [*, grid ** 2, width]", "\n", "x", "=", "torch", ".", "cat", "(", "[", "self", ".", "class_embedding", ".", "to", "(", "x", ".", "dtype", ")", "+", "torch", ".", "zeros", "(", "x", ".", "shape", "[", "0", "]", ",", "1", ",", "x", ".", "shape", "[", "-", "1", "]", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", ",", "x", "]", ",", "dim", "=", "1", ")", "# shape = [*, grid ** 2 + 1, width]", "\n", "x", "=", "x", "+", "self", ".", "positional_embedding", ".", "to", "(", "x", ".", "dtype", ")", "\n", "x", "=", "self", ".", "ln_pre", "(", "x", ")", "\n", "\n", "x", "=", "x", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# NLD -> LND", "\n", "x", "=", "self", ".", "transformer", "(", "x", ")", "\n", "x", "=", "x", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# LND -> NLD", "\n", "\n", "x", "=", "self", ".", "ln_post", "(", "x", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "\n", "if", "self", ".", "proj", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "@", "self", ".", "proj", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIP.__init__": [[360, 418], ["backbone.Backbone.__init__", "isinstance", "clip_backbone.Transformer", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Parameter", "torch.nn.Parameter", "clip_backbone.LayerNorm", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "clip_backbone.CLIP.initialize_parameters", "clip_backbone.ModifiedResNet", "clip_backbone.VisualTransformer", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "clip_backbone.CLIP.build_attention_mask", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "numpy.log"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.initialize_parameters", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.build_attention_mask"], ["    ", "def", "__init__", "(", "self", ",", "\n", "embed_dim", ":", "int", ",", "\n", "# vision", "\n", "image_resolution", ":", "int", ",", "\n", "vision_layers", ":", "Union", "[", "Tuple", "[", "int", ",", "int", ",", "int", ",", "int", "]", ",", "int", "]", ",", "\n", "vision_width", ":", "int", ",", "\n", "vision_patch_size", ":", "int", ",", "\n", "# text", "\n", "context_length", ":", "int", ",", "\n", "vocab_size", ":", "int", ",", "\n", "transformer_width", ":", "int", ",", "\n", "transformer_heads", ":", "int", ",", "\n", "transformer_layers", ":", "int", ",", "\n", "out_features", ",", "\n", "freeze_at", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "context_length", "=", "context_length", "\n", "\n", "if", "isinstance", "(", "vision_layers", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "vision_heads", "=", "vision_width", "*", "32", "//", "64", "\n", "self", ".", "visual", "=", "ModifiedResNet", "(", "\n", "layers", "=", "vision_layers", ",", "\n", "output_dim", "=", "embed_dim", ",", "\n", "heads", "=", "vision_heads", ",", "\n", "input_resolution", "=", "image_resolution", ",", "\n", "width", "=", "vision_width", ",", "\n", "out_features", "=", "out_features", ",", "\n", "freeze_at", "=", "freeze_at", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "vision_heads", "=", "vision_width", "//", "64", "\n", "self", ".", "visual", "=", "VisualTransformer", "(", "\n", "input_resolution", "=", "image_resolution", ",", "\n", "patch_size", "=", "vision_patch_size", ",", "\n", "width", "=", "vision_width", ",", "\n", "layers", "=", "vision_layers", ",", "\n", "heads", "=", "vision_heads", ",", "\n", "output_dim", "=", "embed_dim", "\n", ")", "\n", "\n", "", "self", ".", "transformer", "=", "Transformer", "(", "\n", "width", "=", "transformer_width", ",", "\n", "layers", "=", "transformer_layers", ",", "\n", "heads", "=", "transformer_heads", ",", "\n", "attn_mask", "=", "self", ".", "build_attention_mask", "(", ")", "\n", ")", "\n", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "token_embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "transformer_width", ")", "\n", "self", ".", "positional_embedding", "=", "nn", ".", "Parameter", "(", "torch", ".", "empty", "(", "self", ".", "context_length", ",", "transformer_width", ")", ")", "\n", "self", ".", "ln_final", "=", "LayerNorm", "(", "transformer_width", ")", "\n", "\n", "self", ".", "text_projection", "=", "nn", ".", "Parameter", "(", "torch", ".", "empty", "(", "transformer_width", ",", "embed_dim", ")", ")", "\n", "self", ".", "logit_scale", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "[", "]", ")", "*", "np", ".", "log", "(", "1", "/", "0.07", ")", ")", "\n", "\n", "self", ".", "initialize_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIP.initialize_parameters": [[419, 447], ["torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "isinstance", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "resnet_block.named_parameters", "name.endswith", "torch.nn.init.zeros_", "torch.nn.init.zeros_"], "methods", ["None"], ["", "def", "initialize_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "token_embedding", ".", "weight", ",", "std", "=", "0.02", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "positional_embedding", ",", "std", "=", "0.01", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "visual", ",", "ModifiedResNet", ")", ":", "\n", "            ", "if", "self", ".", "visual", ".", "attnpool", "is", "not", "None", ":", "\n", "                ", "std", "=", "self", ".", "visual", ".", "attnpool", ".", "c_proj", ".", "in_features", "**", "-", "0.5", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "visual", ".", "attnpool", ".", "q_proj", ".", "weight", ",", "std", "=", "std", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "visual", ".", "attnpool", ".", "k_proj", ".", "weight", ",", "std", "=", "std", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "visual", ".", "attnpool", ".", "v_proj", ".", "weight", ",", "std", "=", "std", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "visual", ".", "attnpool", ".", "c_proj", ".", "weight", ",", "std", "=", "std", ")", "\n", "\n", "", "for", "resnet_block", "in", "[", "self", ".", "visual", ".", "layer1", ",", "self", ".", "visual", ".", "layer2", ",", "self", ".", "visual", ".", "layer3", ",", "self", ".", "visual", ".", "layer4", "]", ":", "\n", "                ", "for", "name", ",", "param", "in", "resnet_block", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "if", "name", ".", "endswith", "(", "\"bn3.weight\"", ")", ":", "\n", "                        ", "nn", ".", "init", ".", "zeros_", "(", "param", ")", "\n", "\n", "", "", "", "", "proj_std", "=", "(", "self", ".", "transformer", ".", "width", "**", "-", "0.5", ")", "*", "(", "(", "2", "*", "self", ".", "transformer", ".", "layers", ")", "**", "-", "0.5", ")", "\n", "attn_std", "=", "self", ".", "transformer", ".", "width", "**", "-", "0.5", "\n", "fc_std", "=", "(", "2", "*", "self", ".", "transformer", ".", "width", ")", "**", "-", "0.5", "\n", "for", "block", "in", "self", ".", "transformer", ".", "resblocks", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "block", ".", "attn", ".", "in_proj_weight", ",", "std", "=", "attn_std", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "block", ".", "attn", ".", "out_proj", ".", "weight", ",", "std", "=", "proj_std", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "block", ".", "mlp", ".", "c_fc", ".", "weight", ",", "std", "=", "fc_std", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "block", ".", "mlp", ".", "c_proj", ".", "weight", ",", "std", "=", "proj_std", ")", "\n", "\n", "", "if", "self", ".", "text_projection", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "text_projection", ",", "std", "=", "self", ".", "transformer", ".", "width", "**", "-", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIP.build_attention_mask": [[448, 455], ["torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty.fill_", "torch.empty.fill_", "torch.empty.triu_", "torch.empty.triu_", "float"], "methods", ["None"], ["", "", "def", "build_attention_mask", "(", "self", ")", ":", "\n", "# lazily create causal attention mask, with full attention between the vision tokens", "\n", "# pytorch uses additive attention mask; fill with -inf", "\n", "        ", "mask", "=", "torch", ".", "empty", "(", "self", ".", "context_length", ",", "self", ".", "context_length", ")", "\n", "mask", ".", "fill_", "(", "float", "(", "\"-inf\"", ")", ")", "\n", "mask", ".", "triu_", "(", "1", ")", "# zero out the lower diagonal", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIP.dtype": [[456, 459], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dtype", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "visual", ".", "conv1", ".", "weight", ".", "dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIP.encode_image": [[460, 462], ["clip_backbone.CLIP.visual", "image.type"], "methods", ["None"], ["", "def", "encode_image", "(", "self", ",", "image", ")", ":", "\n", "        ", "return", "self", ".", "visual", "(", "image", ".", "type", "(", "self", ".", "dtype", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIP.encode_text": [[463, 477], ["clip_backbone.CLIP.token_embedding().type", "clip_backbone.CLIP.permute", "clip_backbone.CLIP.transformer", "clip_backbone.CLIP.permute", "clip_backbone.CLIP.ln_final().type", "clip_backbone.CLIP.positional_embedding.type", "clip_backbone.CLIP.token_embedding", "clip_backbone.CLIP.ln_final", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "text.argmax"], "methods", ["None"], ["", "def", "encode_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "x", "=", "self", ".", "token_embedding", "(", "text", ")", ".", "type", "(", "self", ".", "dtype", ")", "# [batch_size, n_ctx, d_model]", "\n", "\n", "x", "=", "x", "+", "self", ".", "positional_embedding", ".", "type", "(", "self", ".", "dtype", ")", "\n", "x", "=", "x", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# NLD -> LND", "\n", "x", "=", "self", ".", "transformer", "(", "x", ")", "\n", "x", "=", "x", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# LND -> NLD", "\n", "x", "=", "self", ".", "ln_final", "(", "x", ")", ".", "type", "(", "self", ".", "dtype", ")", "\n", "\n", "# x.shape = [batch_size, n_ctx, transformer.width]", "\n", "# take features from the eot embedding (eot_token is the highest number in each sequence)", "\n", "x", "=", "x", "[", "torch", ".", "arange", "(", "x", ".", "shape", "[", "0", "]", ")", ",", "text", ".", "argmax", "(", "dim", "=", "-", "1", ")", "]", "@", "self", ".", "text_projection", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIP.forward": [[478, 493], ["clip_backbone.CLIP.encode_image", "clip_backbone.CLIP.encode_text", "clip_backbone.CLIP.logit_scale.exp", "clip_backbone.CLIP.norm", "clip_backbone.CLIP.norm", "clip_backbone.CLIP.t", "clip_backbone.CLIP.t"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIP.encode_image", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.encode_text"], ["", "def", "forward", "(", "self", ",", "image", ",", "text", ")", ":", "\n", "        ", "image_features", "=", "self", ".", "encode_image", "(", "image", ")", "\n", "text_features", "=", "self", ".", "encode_text", "(", "text", ")", "\n", "\n", "# normalized features", "\n", "image_features", "=", "image_features", "/", "image_features", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "text_features", "=", "text_features", "/", "text_features", ".", "norm", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "# cosine similarity as logits", "\n", "logit_scale", "=", "self", ".", "logit_scale", ".", "exp", "(", ")", "\n", "logits_per_image", "=", "logit_scale", "*", "image_features", "@", "text_features", ".", "t", "(", ")", "\n", "logits_per_text", "=", "logit_scale", "*", "text_features", "@", "image_features", ".", "t", "(", ")", "\n", "\n", "# shape = [global_batch_size, global_batch_size]", "\n", "return", "logits_per_image", ",", "logits_per_text", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.__init__": [[733, 769], ["torch.nn.Module.__init__", "clip_backbone.Transformer", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Parameter", "torch.nn.Parameter", "clip_backbone.LayerNorm", "torch.nn.Parameter", "torch.nn.Parameter", "clip_backbone.CLIPLangEncoder.initialize_parameters", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "clip_backbone.CLIPLangEncoder.build_attention_mask"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.initialize_parameters", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.build_attention_mask"], ["    ", "def", "__init__", "(", "self", ",", "\n", "embed_dim", ":", "int", ",", "\n", "# vision", "\n", "image_resolution", ":", "int", ",", "\n", "vision_layers", ":", "Union", "[", "Tuple", "[", "int", ",", "int", ",", "int", ",", "int", "]", ",", "int", "]", ",", "\n", "vision_width", ":", "int", ",", "\n", "vision_patch_size", ":", "int", ",", "\n", "# text", "\n", "context_length", ":", "int", ",", "\n", "vocab_size", ":", "int", ",", "\n", "transformer_width", ":", "int", ",", "\n", "transformer_heads", ":", "int", ",", "\n", "transformer_layers", ":", "int", ",", "\n", "out_features", ",", "\n", "freeze_at", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "context_length", "=", "context_length", "\n", "\n", "self", ".", "transformer", "=", "Transformer", "(", "\n", "width", "=", "transformer_width", ",", "\n", "layers", "=", "transformer_layers", ",", "\n", "heads", "=", "transformer_heads", ",", "\n", "attn_mask", "=", "self", ".", "build_attention_mask", "(", ")", "\n", ")", "\n", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "token_embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "transformer_width", ")", "\n", "self", ".", "positional_embedding", "=", "nn", ".", "Parameter", "(", "torch", ".", "empty", "(", "self", ".", "context_length", ",", "transformer_width", ")", ")", "\n", "self", ".", "ln_final", "=", "LayerNorm", "(", "transformer_width", ")", "\n", "\n", "self", ".", "text_projection", "=", "nn", ".", "Parameter", "(", "torch", ".", "empty", "(", "transformer_width", ",", "embed_dim", ")", ")", "\n", "#self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))", "\n", "\n", "self", ".", "initialize_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.initialize_parameters": [[770, 785], ["torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_"], "methods", ["None"], ["", "def", "initialize_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "token_embedding", ".", "weight", ",", "std", "=", "0.02", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "positional_embedding", ",", "std", "=", "0.01", ")", "\n", "\n", "proj_std", "=", "(", "self", ".", "transformer", ".", "width", "**", "-", "0.5", ")", "*", "(", "(", "2", "*", "self", ".", "transformer", ".", "layers", ")", "**", "-", "0.5", ")", "\n", "attn_std", "=", "self", ".", "transformer", ".", "width", "**", "-", "0.5", "\n", "fc_std", "=", "(", "2", "*", "self", ".", "transformer", ".", "width", ")", "**", "-", "0.5", "\n", "for", "block", "in", "self", ".", "transformer", ".", "resblocks", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "block", ".", "attn", ".", "in_proj_weight", ",", "std", "=", "attn_std", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "block", ".", "attn", ".", "out_proj", ".", "weight", ",", "std", "=", "proj_std", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "block", ".", "mlp", ".", "c_fc", ".", "weight", ",", "std", "=", "fc_std", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "block", ".", "mlp", ".", "c_proj", ".", "weight", ",", "std", "=", "proj_std", ")", "\n", "\n", "", "if", "self", ".", "text_projection", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "text_projection", ",", "std", "=", "self", ".", "transformer", ".", "width", "**", "-", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.build_attention_mask": [[786, 793], ["torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty.fill_", "torch.empty.fill_", "torch.empty.triu_", "torch.empty.triu_", "float"], "methods", ["None"], ["", "", "def", "build_attention_mask", "(", "self", ")", ":", "\n", "# lazily create causal attention mask, with full attention between the vision tokens", "\n", "# pytorch uses additive attention mask; fill with -inf", "\n", "        ", "mask", "=", "torch", ".", "empty", "(", "self", ".", "context_length", ",", "self", ".", "context_length", ")", "\n", "mask", ".", "fill_", "(", "float", "(", "\"-inf\"", ")", ")", "\n", "mask", ".", "triu_", "(", "1", ")", "# zero out the lower diagonal", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype": [[794, 797], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dtype", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "transformer", ".", "resblocks", "[", "0", "]", ".", "mlp", "[", "0", "]", ".", "weight", ".", "dtype", "# torch.float32, not sure whether need to be fp16 in pretraining", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.encode_text": [[798, 815], ["clip_backbone.CLIPLangEncoder.token_embedding().type", "clip_backbone.CLIPLangEncoder.permute", "clip_backbone.CLIPLangEncoder.transformer", "clip_backbone.CLIPLangEncoder.permute", "clip_backbone.CLIPLangEncoder.ln_final().type", "clip_backbone.CLIPLangEncoder.positional_embedding.type", "clip_backbone.CLIPLangEncoder.token_embedding", "clip_backbone.CLIPLangEncoder.ln_final", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "text.argmax"], "methods", ["None"], ["", "def", "encode_text", "(", "self", ",", "text", ",", "only_eot", "=", "True", ")", ":", "\n", "        ", "x", "=", "self", ".", "token_embedding", "(", "text", ")", ".", "type", "(", "self", ".", "dtype", ")", "# [batch_size, n_ctx, d_model]", "\n", "\n", "x", "=", "x", "+", "self", ".", "positional_embedding", ".", "type", "(", "self", ".", "dtype", ")", "\n", "x", "=", "x", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# NLD -> LND", "\n", "x", "=", "self", ".", "transformer", "(", "x", ")", "\n", "x", "=", "x", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# LND -> NLD", "\n", "x", "=", "self", ".", "ln_final", "(", "x", ")", ".", "type", "(", "self", ".", "dtype", ")", "\n", "\n", "if", "only_eot", ":", "\n", "# x.shape = [batch_size, n_ctx, transformer.width]", "\n", "# take features from the eot embedding (eot_token is the highest number in each sequence)", "\n", "            ", "x", "=", "x", "[", "torch", ".", "arange", "(", "x", ".", "shape", "[", "0", "]", ")", ",", "text", ".", "argmax", "(", "dim", "=", "-", "1", ")", "]", "@", "self", ".", "text_projection", "\n", "return", "x", "\n", "", "else", ":", "\n", "# return embeddings for all tokens, instead of the eot embedding as CLIP implementation below", "\n", "            ", "return", "x", "@", "self", ".", "text_projection", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.convert_weights": [[495, 517], ["model.apply", "isinstance", "isinstance", "l.weight.data.half", "hasattr", "l.bias.data.half", "getattr", "getattr", "getattr.data.half", "getattr.data.half"], "function", ["None"], ["", "", "def", "convert_weights", "(", "model", ":", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\"Convert applicable model parameters to fp16\"\"\"", "\n", "\n", "def", "_convert_weights_to_fp16", "(", "l", ")", ":", "\n", "        ", "if", "isinstance", "(", "l", ",", "(", "nn", ".", "Conv1d", ",", "nn", ".", "Conv2d", ",", "nn", ".", "Linear", ")", ")", ":", "\n", "            ", "l", ".", "weight", ".", "data", "=", "l", ".", "weight", ".", "data", ".", "half", "(", ")", "\n", "if", "l", ".", "bias", "is", "not", "None", ":", "\n", "                ", "l", ".", "bias", ".", "data", "=", "l", ".", "bias", ".", "data", ".", "half", "(", ")", "\n", "\n", "", "", "if", "isinstance", "(", "l", ",", "nn", ".", "MultiheadAttention", ")", ":", "\n", "            ", "for", "attr", "in", "[", "*", "[", "f\"{s}_proj_weight\"", "for", "s", "in", "[", "\"in\"", ",", "\"q\"", ",", "\"k\"", ",", "\"v\"", "]", "]", ",", "\"in_proj_bias\"", ",", "\"bias_k\"", ",", "\"bias_v\"", "]", ":", "\n", "                ", "tensor", "=", "getattr", "(", "l", ",", "attr", ")", "\n", "if", "tensor", "is", "not", "None", ":", "\n", "                    ", "tensor", ".", "data", "=", "tensor", ".", "data", ".", "half", "(", ")", "\n", "\n", "", "", "", "for", "name", "in", "[", "\"text_projection\"", ",", "\"proj\"", "]", ":", "\n", "            ", "if", "hasattr", "(", "l", ",", "name", ")", ":", "\n", "                ", "attr", "=", "getattr", "(", "l", ",", "name", ")", "\n", "if", "attr", "is", "not", "None", ":", "\n", "                    ", "attr", ".", "data", "=", "attr", ".", "data", ".", "half", "(", ")", "\n", "\n", "", "", "", "", "model", ".", "apply", "(", "_convert_weights_to_fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.build_model": [[519, 557], ["len", "clip_backbone.CLIP", "clip_backbone.convert_weights", "CLIP.load_state_dict", "CLIP.eval", "len", "round", "tuple", "round", "set", "len", "set", "state_dict.keys", "k.split", "k.startswith", "k.startswith", "k.endswith", "k.split", "k.startswith"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.convert_weights", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.load_state_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "build_model", "(", "state_dict", ":", "dict", ")", ":", "\n", "    ", "vit", "=", "\"visual.proj\"", "in", "state_dict", "\n", "\n", "if", "vit", ":", "\n", "        ", "vision_width", "=", "state_dict", "[", "\"visual.conv1.weight\"", "]", ".", "shape", "[", "0", "]", "\n", "vision_layers", "=", "len", "(", "[", "k", "for", "k", "in", "state_dict", ".", "keys", "(", ")", "if", "k", ".", "startswith", "(", "\"visual.\"", ")", "and", "k", ".", "endswith", "(", "\".attn.in_proj_weight\"", ")", "]", ")", "\n", "vision_patch_size", "=", "state_dict", "[", "\"visual.conv1.weight\"", "]", ".", "shape", "[", "-", "1", "]", "\n", "grid_size", "=", "round", "(", "(", "state_dict", "[", "\"visual.positional_embedding\"", "]", ".", "shape", "[", "0", "]", "-", "1", ")", "**", "0.5", ")", "\n", "image_resolution", "=", "vision_patch_size", "*", "grid_size", "\n", "", "else", ":", "\n", "        ", "counts", ":", "list", "=", "[", "len", "(", "set", "(", "k", ".", "split", "(", "\".\"", ")", "[", "2", "]", "for", "k", "in", "state_dict", "if", "k", ".", "startswith", "(", "f\"visual.layer{b}\"", ")", ")", ")", "for", "b", "in", "[", "1", ",", "2", ",", "3", ",", "4", "]", "]", "\n", "vision_layers", "=", "tuple", "(", "counts", ")", "\n", "vision_width", "=", "state_dict", "[", "\"visual.layer1.0.conv1.weight\"", "]", ".", "shape", "[", "0", "]", "\n", "output_width", "=", "round", "(", "(", "state_dict", "[", "\"visual.attnpool.positional_embedding\"", "]", ".", "shape", "[", "0", "]", "-", "1", ")", "**", "0.5", ")", "\n", "vision_patch_size", "=", "None", "\n", "assert", "output_width", "**", "2", "+", "1", "==", "state_dict", "[", "\"visual.attnpool.positional_embedding\"", "]", ".", "shape", "[", "0", "]", "\n", "image_resolution", "=", "output_width", "*", "32", "\n", "\n", "", "embed_dim", "=", "state_dict", "[", "\"text_projection\"", "]", ".", "shape", "[", "1", "]", "\n", "context_length", "=", "state_dict", "[", "\"positional_embedding\"", "]", ".", "shape", "[", "0", "]", "\n", "vocab_size", "=", "state_dict", "[", "\"token_embedding.weight\"", "]", ".", "shape", "[", "0", "]", "\n", "transformer_width", "=", "state_dict", "[", "\"ln_final.weight\"", "]", ".", "shape", "[", "0", "]", "\n", "transformer_heads", "=", "transformer_width", "//", "64", "\n", "transformer_layers", "=", "len", "(", "set", "(", "k", ".", "split", "(", "\".\"", ")", "[", "2", "]", "for", "k", "in", "state_dict", "if", "k", ".", "startswith", "(", "f\"transformer.resblocks\"", ")", ")", ")", "\n", "\n", "model", "=", "CLIP", "(", "\n", "embed_dim", ",", "\n", "image_resolution", ",", "vision_layers", ",", "vision_width", ",", "vision_patch_size", ",", "\n", "context_length", ",", "vocab_size", ",", "transformer_width", ",", "transformer_heads", ",", "transformer_layers", "\n", ")", "\n", "\n", "for", "key", "in", "[", "\"input_resolution\"", ",", "\"context_length\"", ",", "\"vocab_size\"", "]", ":", "\n", "        ", "if", "key", "in", "state_dict", ":", "\n", "            ", "del", "state_dict", "[", "key", "]", "\n", "\n", "", "", "convert_weights", "(", "model", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "return", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.build_vit_clip": [[559, 599], ["build.BACKBONE_REGISTRY.register", "clip_backbone.CLIP"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register"], ["", "@", "BACKBONE_REGISTRY", ".", "register", "(", ")", "\n", "def", "build_vit_clip", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Create the whole CLIP instance from config.\n\n    Returns:\n        CLIP: a :class:`CLIP` instance.\n    \"\"\"", "\n", "# port standard ResNet config to CLIP ModifiedResNet", "\n", "freeze_at", "=", "cfg", ".", "MODEL", ".", "BACKBONE", ".", "FREEZE_AT", "\n", "out_features", "=", "[", "'res5'", "]", "# includes the whole ResNet # cfg.MODEL.RESNETS.OUT_FEATURES", "\n", "depth", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEPTH", "\n", "\n", "# num_blocks_per_stage = {", "\n", "#     18: [2, 2, 2, 2],", "\n", "#     34: [3, 4, 6, 3],", "\n", "#     50: [3, 4, 6, 3],", "\n", "#     101: [3, 4, 23, 3],", "\n", "#     152: [3, 8, 36, 3],", "\n", "# }[depth]", "\n", "vision_layers", "=", "12", "# num_blocks_per_stage", "\n", "vision_width", "=", "768", "# cfg.MODEL.RESNETS.STEM_OUT_CHANNELS", "\n", "\n", "# default configs of CLIP", "\n", "embed_dim", "=", "512", "# 1024", "\n", "image_resolution", "=", "224", "\n", "vision_patch_size", "=", "32", "# None", "\n", "context_length", "=", "77", "\n", "vocab_size", "=", "49408", "\n", "transformer_width", "=", "512", "\n", "transformer_heads", "=", "8", "\n", "transformer_layers", "=", "12", "\n", "\n", "model", "=", "CLIP", "(", "\n", "embed_dim", ",", "\n", "image_resolution", ",", "vision_layers", ",", "vision_width", ",", "vision_patch_size", ",", "\n", "context_length", ",", "vocab_size", ",", "transformer_width", ",", "transformer_heads", ",", "transformer_layers", ",", "\n", "out_features", ",", "freeze_at", "\n", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.build_resnet_clip": [[600, 662], ["build.BACKBONE_REGISTRY.register", "clip_backbone.CLIP"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register"], ["", "@", "BACKBONE_REGISTRY", ".", "register", "(", ")", "\n", "def", "build_resnet_clip", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Create the whole CLIP instance from config.\n\n    Returns:\n        CLIP: a :class:`CLIP` instance.\n    \"\"\"", "\n", "# port standard ResNet config to CLIP ModifiedResNet", "\n", "freeze_at", "=", "cfg", ".", "MODEL", ".", "BACKBONE", ".", "FREEZE_AT", "\n", "out_features", "=", "[", "'res5'", "]", "# includes the whole ResNet # cfg.MODEL.RESNETS.OUT_FEATURES", "\n", "depth", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEPTH", "\n", "\n", "num_blocks_per_stage", "=", "{", "\n", "18", ":", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "34", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "50", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "101", ":", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "152", ":", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "\n", "200", ":", "[", "4", ",", "6", ",", "10", ",", "6", "]", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "vision_layers", "=", "num_blocks_per_stage", "\n", "vision_width", "=", "{", "\n", "50", ":", "64", ",", "\n", "101", ":", "64", ",", "\n", "200", ":", "80", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "# cfg.MODEL.RESNETS.STEM_OUT_CHANNELS", "\n", "\n", "# default configs of CLIP", "\n", "embed_dim", "=", "{", "\n", "50", ":", "1024", ",", "\n", "101", ":", "512", ",", "\n", "200", ":", "640", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "vision_heads", "=", "vision_width", "*", "32", "//", "64", "\n", "image_resolution", "=", "{", "\n", "50", ":", "224", ",", "\n", "101", ":", "224", ",", "\n", "200", ":", "288", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "vision_patch_size", "=", "None", "\n", "context_length", "=", "77", "\n", "vocab_size", "=", "49408", "\n", "transformer_width", "=", "{", "\n", "50", ":", "512", ",", "\n", "101", ":", "512", ",", "\n", "200", ":", "640", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "transformer_heads", "=", "{", "\n", "50", ":", "8", ",", "\n", "101", ":", "8", ",", "\n", "200", ":", "10", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "transformer_layers", "=", "12", "\n", "\n", "model", "=", "CLIP", "(", "\n", "embed_dim", ",", "\n", "image_resolution", ",", "vision_layers", ",", "vision_width", ",", "vision_patch_size", ",", "\n", "context_length", ",", "vocab_size", ",", "transformer_width", ",", "transformer_heads", ",", "transformer_layers", ",", "\n", "out_features", ",", "freeze_at", "\n", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.build_clip_resnet_backbone": [[664, 729], ["build.BACKBONE_REGISTRY.register", "clip_backbone.ModifiedResNet"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register"], ["", "@", "BACKBONE_REGISTRY", ".", "register", "(", ")", "\n", "def", "build_clip_resnet_backbone", "(", "cfg", ",", "input_shape", ")", ":", "\n", "    ", "\"\"\"\n    Create a CLIP-version ResNet instance from config.\n\n    Returns:\n        ModifiedResNet: a :class:`ModifiedResNet` instance.\n    \"\"\"", "\n", "# port standard ResNet config to CLIP ModifiedResNet", "\n", "freeze_at", "=", "cfg", ".", "MODEL", ".", "BACKBONE", ".", "FREEZE_AT", "\n", "out_features", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "OUT_FEATURES", "\n", "depth", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEPTH", "\n", "# num_groups          = cfg.MODEL.RESNETS.NUM_GROUPS", "\n", "# width_per_group     = cfg.MODEL.RESNETS.WIDTH_PER_GROUP", "\n", "# bottleneck_channels = num_groups * width_per_group", "\n", "# in_channels         = cfg.MODEL.RESNETS.STEM_OUT_CHANNELS", "\n", "# out_channels        = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS", "\n", "# stride_in_1x1       = cfg.MODEL.RESNETS.STRIDE_IN_1X1", "\n", "# res5_dilation       = cfg.MODEL.RESNETS.RES5_DILATION", "\n", "# deform_on_per_stage = cfg.MODEL.RESNETS.DEFORM_ON_PER_STAGE", "\n", "# deform_modulated    = cfg.MODEL.RESNETS.DEFORM_MODULATED", "\n", "# deform_num_groups   = cfg.MODEL.RESNETS.DEFORM_NUM_GROUPS", "\n", "\n", "num_blocks_per_stage", "=", "{", "\n", "18", ":", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "34", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "50", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "101", ":", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "152", ":", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "\n", "200", ":", "[", "4", ",", "6", ",", "10", ",", "6", "]", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "vision_layers", "=", "num_blocks_per_stage", "\n", "vision_width", "=", "{", "\n", "50", ":", "64", ",", "\n", "101", ":", "64", ",", "\n", "200", ":", "80", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "# cfg.MODEL.RESNETS.STEM_OUT_CHANNELS", "\n", "\n", "# default configs of CLIP ModifiedResNet, but not used if only building ModifiedResNet as backbone", "\n", "embed_dim", "=", "{", "\n", "50", ":", "1024", ",", "\n", "101", ":", "512", ",", "\n", "200", ":", "640", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "vision_heads", "=", "vision_width", "*", "32", "//", "64", "\n", "image_resolution", "=", "{", "\n", "50", ":", "224", ",", "\n", "101", ":", "224", ",", "\n", "200", ":", "288", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "\n", "# if combine {ModifiedResNet of CLIP, C4, text emb as classifier}, then has to use att_pool to match dimension", "\n", "create_att_pool", "=", "True", "if", "(", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NAME", "in", "[", "'CLIPRes5ROIHeads'", ",", "'CLIPStandardROIHeads'", "]", "and", "cfg", ".", "MODEL", ".", "CLIP", ".", "USE_TEXT_EMB_CLASSIFIER", ")", "or", "cfg", ".", "MODEL", ".", "ROI_HEADS", ".", "NAME", "==", "'PretrainRes5ROIHeads'", "else", "False", "\n", "\n", "return", "ModifiedResNet", "(", "layers", "=", "vision_layers", ",", "\n", "output_dim", "=", "embed_dim", ",", "\n", "heads", "=", "vision_heads", ",", "\n", "input_resolution", "=", "image_resolution", ",", "\n", "width", "=", "vision_width", ",", "\n", "out_features", "=", "out_features", ",", "\n", "freeze_at", "=", "freeze_at", ",", "\n", "depth", "=", "depth", ",", "\n", "pool_vec", "=", "False", ",", "\n", "create_att_pool", "=", "create_att_pool", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.build_clip_language_encoder": [[817, 878], ["clip_backbone.CLIPLangEncoder"], "function", ["None"], ["", "", "", "def", "build_clip_language_encoder", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Create the CLIP language encoder instance from config.\n\n    Returns:\n        CLIP: a :class:`CLIP` instance.\n    \"\"\"", "\n", "# port standard ResNet config to CLIP ModifiedResNet", "\n", "freeze_at", "=", "cfg", ".", "MODEL", ".", "BACKBONE", ".", "FREEZE_AT", "\n", "out_features", "=", "[", "'res5'", "]", "# includes the whole ResNet # cfg.MODEL.RESNETS.OUT_FEATURES", "\n", "depth", "=", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEPTH", "\n", "\n", "num_blocks_per_stage", "=", "{", "\n", "18", ":", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "34", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "50", ":", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "101", ":", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "152", ":", "[", "3", ",", "8", ",", "36", ",", "3", "]", ",", "\n", "200", ":", "[", "4", ",", "6", ",", "10", ",", "6", "]", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "vision_layers", "=", "num_blocks_per_stage", "\n", "vision_width", "=", "{", "\n", "50", ":", "64", ",", "\n", "101", ":", "64", ",", "\n", "200", ":", "80", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "# cfg.MODEL.RESNETS.STEM_OUT_CHANNELS", "\n", "\n", "# default configs of CLIP", "\n", "embed_dim", "=", "{", "\n", "50", ":", "1024", ",", "\n", "101", ":", "512", ",", "\n", "200", ":", "640", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "vision_heads", "=", "vision_width", "*", "32", "//", "64", "\n", "image_resolution", "=", "{", "\n", "50", ":", "224", ",", "\n", "101", ":", "224", ",", "\n", "200", ":", "288", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "vision_patch_size", "=", "None", "\n", "context_length", "=", "77", "\n", "vocab_size", "=", "49408", "\n", "transformer_width", "=", "{", "\n", "50", ":", "512", ",", "\n", "101", ":", "512", ",", "\n", "200", ":", "640", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "transformer_heads", "=", "{", "\n", "50", ":", "8", ",", "\n", "101", ":", "8", ",", "\n", "200", ":", "10", ",", "# flag for ResNet50x4", "\n", "}", "[", "depth", "]", "\n", "transformer_layers", "=", "12", "\n", "\n", "model", "=", "CLIPLangEncoder", "(", "\n", "embed_dim", ",", "\n", "image_resolution", ",", "vision_layers", ",", "vision_width", ",", "vision_patch_size", ",", "\n", "context_length", ",", "vocab_size", ",", "transformer_width", ",", "transformer_heads", ",", "transformer_layers", ",", "\n", "out_features", ",", "freeze_at", "\n", ")", "\n", "return", "model", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.ResStem.__init__": [[63, 69], ["detectron2.layers.CNNBlockBase.__init__", "regnet.conv2d", "detectron2.layers.get_norm", "activation_class", "regnet.pool2d"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.pool2d"], ["def", "__init__", "(", "self", ",", "w_in", ",", "w_out", ",", "norm", ",", "activation_class", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "w_in", ",", "w_out", ",", "4", ")", "\n", "self", ".", "conv", "=", "conv2d", "(", "w_in", ",", "w_out", ",", "7", ",", "stride", "=", "2", ")", "\n", "self", ".", "bn", "=", "get_norm", "(", "norm", ",", "w_out", ")", "\n", "self", ".", "af", "=", "activation_class", "(", ")", "\n", "self", ".", "pool", "=", "pool2d", "(", "3", ",", "stride", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.ResStem.forward": [[70, 74], ["regnet.ResStem.children", "layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "children", "(", ")", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.SimpleStem.__init__": [[79, 84], ["detectron2.layers.CNNBlockBase.__init__", "regnet.conv2d", "detectron2.layers.get_norm", "activation_class"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "self", ",", "w_in", ",", "w_out", ",", "norm", ",", "activation_class", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "w_in", ",", "w_out", ",", "2", ")", "\n", "self", ".", "conv", "=", "conv2d", "(", "w_in", ",", "w_out", ",", "3", ",", "stride", "=", "2", ")", "\n", "self", ".", "bn", "=", "get_norm", "(", "norm", ",", "w_out", ")", "\n", "self", ".", "af", "=", "activation_class", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.SimpleStem.forward": [[85, 89], ["regnet.SimpleStem.children", "layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "children", "(", ")", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.SE.__init__": [[94, 102], ["torch.nn.Module.__init__", "regnet.gap2d", "torch.nn.Sequential", "regnet.conv2d", "activation_class", "regnet.conv2d", "torch.nn.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.gap2d", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d"], ["def", "__init__", "(", "self", ",", "w_in", ",", "w_se", ",", "activation_class", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "avg_pool", "=", "gap2d", "(", ")", "\n", "self", ".", "f_ex", "=", "nn", ".", "Sequential", "(", "\n", "conv2d", "(", "w_in", ",", "w_se", ",", "1", ",", "bias", "=", "True", ")", ",", "\n", "activation_class", "(", ")", ",", "\n", "conv2d", "(", "w_se", ",", "w_in", ",", "1", ",", "bias", "=", "True", ")", ",", "\n", "nn", ".", "Sigmoid", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.SE.forward": [[104, 106], ["regnet.SE.f_ex", "regnet.SE.avg_pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "*", "self", ".", "f_ex", "(", "self", ".", "avg_pool", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.VanillaBlock.__init__": [[111, 119], ["detectron2.layers.CNNBlockBase.__init__", "regnet.conv2d", "detectron2.layers.get_norm", "activation_class", "regnet.conv2d", "detectron2.layers.get_norm", "activation_class"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "self", ",", "w_in", ",", "w_out", ",", "stride", ",", "norm", ",", "activation_class", ",", "_params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "w_in", ",", "w_out", ",", "stride", ")", "\n", "self", ".", "a", "=", "conv2d", "(", "w_in", ",", "w_out", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "a_bn", "=", "get_norm", "(", "norm", ",", "w_out", ")", "\n", "self", ".", "a_af", "=", "activation_class", "(", ")", "\n", "self", ".", "b", "=", "conv2d", "(", "w_out", ",", "w_out", ",", "3", ")", "\n", "self", ".", "b_bn", "=", "get_norm", "(", "norm", ",", "w_out", ")", "\n", "self", ".", "b_af", "=", "activation_class", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.VanillaBlock.forward": [[120, 124], ["regnet.VanillaBlock.children", "layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "children", "(", ")", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.BasicTransform.__init__": [[129, 137], ["torch.nn.Module.__init__", "regnet.conv2d", "detectron2.layers.get_norm", "activation_class", "regnet.conv2d", "detectron2.layers.get_norm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "self", ",", "w_in", ",", "w_out", ",", "stride", ",", "norm", ",", "activation_class", ",", "_params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "a", "=", "conv2d", "(", "w_in", ",", "w_out", ",", "3", ",", "stride", "=", "stride", ")", "\n", "self", ".", "a_bn", "=", "get_norm", "(", "norm", ",", "w_out", ")", "\n", "self", ".", "a_af", "=", "activation_class", "(", ")", "\n", "self", ".", "b", "=", "conv2d", "(", "w_out", ",", "w_out", ",", "3", ")", "\n", "self", ".", "b_bn", "=", "get_norm", "(", "norm", ",", "w_out", ")", "\n", "self", ".", "b_bn", ".", "final_bn", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.BasicTransform.forward": [[138, 142], ["regnet.BasicTransform.children", "layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "children", "(", ")", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.ResBasicBlock.__init__": [[147, 155], ["detectron2.layers.CNNBlockBase.__init__", "regnet.BasicTransform", "activation_class", "regnet.conv2d", "detectron2.layers.get_norm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "self", ",", "w_in", ",", "w_out", ",", "stride", ",", "norm", ",", "activation_class", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "w_in", ",", "w_out", ",", "stride", ")", "\n", "self", ".", "proj", ",", "self", ".", "bn", "=", "None", ",", "None", "\n", "if", "(", "w_in", "!=", "w_out", ")", "or", "(", "stride", "!=", "1", ")", ":", "\n", "            ", "self", ".", "proj", "=", "conv2d", "(", "w_in", ",", "w_out", ",", "1", ",", "stride", "=", "stride", ")", "\n", "self", ".", "bn", "=", "get_norm", "(", "norm", ",", "w_out", ")", "\n", "", "self", ".", "f", "=", "BasicTransform", "(", "w_in", ",", "w_out", ",", "stride", ",", "norm", ",", "activation_class", ",", "params", ")", "\n", "self", ".", "af", "=", "activation_class", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.ResBasicBlock.forward": [[156, 159], ["regnet.ResBasicBlock.af", "regnet.ResBasicBlock.bn", "regnet.ResBasicBlock.proj", "regnet.ResBasicBlock.f"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_p", "=", "self", ".", "bn", "(", "self", ".", "proj", "(", "x", ")", ")", "if", "self", ".", "proj", "else", "x", "\n", "return", "self", ".", "af", "(", "x_p", "+", "self", ".", "f", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.BottleneckTransform.__init__": [[164, 179], ["torch.nn.Module.__init__", "int", "int", "regnet.conv2d", "detectron2.layers.get_norm", "activation_class", "regnet.conv2d", "detectron2.layers.get_norm", "activation_class", "regnet.conv2d", "detectron2.layers.get_norm", "round", "round", "regnet.SE"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "self", ",", "w_in", ",", "w_out", ",", "stride", ",", "norm", ",", "activation_class", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "w_b", "=", "int", "(", "round", "(", "w_out", "*", "params", "[", "\"bot_mul\"", "]", ")", ")", "\n", "w_se", "=", "int", "(", "round", "(", "w_in", "*", "params", "[", "\"se_r\"", "]", ")", ")", "\n", "groups", "=", "w_b", "//", "params", "[", "\"group_w\"", "]", "\n", "self", ".", "a", "=", "conv2d", "(", "w_in", ",", "w_b", ",", "1", ")", "\n", "self", ".", "a_bn", "=", "get_norm", "(", "norm", ",", "w_b", ")", "\n", "self", ".", "a_af", "=", "activation_class", "(", ")", "\n", "self", ".", "b", "=", "conv2d", "(", "w_b", ",", "w_b", ",", "3", ",", "stride", "=", "stride", ",", "groups", "=", "groups", ")", "\n", "self", ".", "b_bn", "=", "get_norm", "(", "norm", ",", "w_b", ")", "\n", "self", ".", "b_af", "=", "activation_class", "(", ")", "\n", "self", ".", "se", "=", "SE", "(", "w_b", ",", "w_se", ",", "activation_class", ")", "if", "w_se", "else", "None", "\n", "self", ".", "c", "=", "conv2d", "(", "w_b", ",", "w_out", ",", "1", ")", "\n", "self", ".", "c_bn", "=", "get_norm", "(", "norm", ",", "w_out", ")", "\n", "self", ".", "c_bn", ".", "final_bn", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.BottleneckTransform.forward": [[180, 184], ["regnet.BottleneckTransform.children", "layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "children", "(", ")", ":", "\n", "            ", "x", "=", "layer", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.ResBottleneckBlock.__init__": [[189, 197], ["detectron2.layers.CNNBlockBase.__init__", "regnet.BottleneckTransform", "activation_class", "regnet.conv2d", "detectron2.layers.get_norm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "self", ",", "w_in", ",", "w_out", ",", "stride", ",", "norm", ",", "activation_class", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "w_in", ",", "w_out", ",", "stride", ")", "\n", "self", ".", "proj", ",", "self", ".", "bn", "=", "None", ",", "None", "\n", "if", "(", "w_in", "!=", "w_out", ")", "or", "(", "stride", "!=", "1", ")", ":", "\n", "            ", "self", ".", "proj", "=", "conv2d", "(", "w_in", ",", "w_out", ",", "1", ",", "stride", "=", "stride", ")", "\n", "self", ".", "bn", "=", "get_norm", "(", "norm", ",", "w_out", ")", "\n", "", "self", ".", "f", "=", "BottleneckTransform", "(", "w_in", ",", "w_out", ",", "stride", ",", "norm", ",", "activation_class", ",", "params", ")", "\n", "self", ".", "af", "=", "activation_class", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.ResBottleneckBlock.forward": [[198, 201], ["regnet.ResBottleneckBlock.af", "regnet.ResBottleneckBlock.bn", "regnet.ResBottleneckBlock.proj", "regnet.ResBottleneckBlock.f"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_p", "=", "self", ".", "bn", "(", "self", ".", "proj", "(", "x", ")", ")", "if", "self", ".", "proj", "else", "x", "\n", "return", "self", ".", "af", "(", "x_p", "+", "self", ".", "f", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.AnyStage.__init__": [[206, 212], ["torch.nn.Module.__init__", "range", "block_class", "regnet.AnyStage.add_module"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "w_in", ",", "w_out", ",", "stride", ",", "d", ",", "block_class", ",", "norm", ",", "activation_class", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "for", "i", "in", "range", "(", "d", ")", ":", "\n", "            ", "block", "=", "block_class", "(", "w_in", ",", "w_out", ",", "stride", ",", "norm", ",", "activation_class", ",", "params", ")", "\n", "self", ".", "add_module", "(", "\"b{}\"", ".", "format", "(", "i", "+", "1", ")", ",", "block", ")", "\n", "stride", ",", "w_in", "=", "1", ",", "w_out", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.AnyStage.forward": [[213, 217], ["regnet.AnyStage.children", "block"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "block", "in", "self", ".", "children", "(", ")", ":", "\n", "            ", "x", "=", "block", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.AnyNet.__init__": [[222, 304], ["backbone.Backbone.__init__", "stem_class", "enumerate", "regnet.AnyNet.apply", "len", "regnet.AnyNet.freeze", "zip", "regnet.AnyStage", "regnet.AnyNet.add_module", "regnet.AnyNet.stages_and_names.append", "int", "regnet.AnyNet.named_children", "numpy.prod", "list", "AnyStage.children", "AnyStage.children"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "stem_class", ",", "\n", "stem_width", ",", "\n", "block_class", ",", "\n", "depths", ",", "\n", "widths", ",", "\n", "group_widths", ",", "\n", "strides", ",", "\n", "bottleneck_ratios", ",", "\n", "se_ratio", ",", "\n", "activation_class", ",", "\n", "freeze_at", "=", "0", ",", "\n", "norm", "=", "\"BN\"", ",", "\n", "out_features", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            stem_class (callable): A callable taking 4 arguments (channels in, channels out,\n                normalization, callable returning an activation function) that returns another\n                callable implementing the stem module.\n            stem_width (int): The number of output channels that the stem produces.\n            block_class (callable): A callable taking 6 arguments (channels in, channels out,\n                stride, normalization, callable returning an activation function, a dict of\n                block-specific parameters) that returns another callable implementing the repeated\n                block module.\n            depths (list[int]): Number of blocks in each stage.\n            widths (list[int]): For each stage, the number of output channels of each block.\n            group_widths (list[int]): For each stage, the number of channels per group in group\n                convolution, if the block uses group convolution.\n            strides (list[int]): The stride that each network stage applies to its input.\n            bottleneck_ratios (list[float]): For each stage, the ratio of the number of bottleneck\n                channels to the number of block input channels (or, equivalently, output channels),\n                if the block uses a bottleneck.\n            se_ratio (float): The ratio of the number of channels used inside the squeeze-excitation\n                (SE) module to it number of input channels, if SE the block uses SE.\n            activation_class (callable): A callable taking no arguments that returns another\n                callable implementing an activation function.\n            freeze_at (int): The number of stages at the beginning to freeze.\n                see :meth:`freeze` for detailed explanation.\n            norm (str or callable): normalization for all conv layers.\n                See :func:`layers.get_norm` for supported format.\n            out_features (list[str]): name of the layers whose outputs should\n                be returned in forward. RegNet's use \"stem\" and \"s1\", \"s2\", etc for the stages after\n                the stem. If None, will return the output of the last layer.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stem", "=", "stem_class", "(", "3", ",", "stem_width", ",", "norm", ",", "activation_class", ")", "\n", "\n", "current_stride", "=", "self", ".", "stem", ".", "stride", "\n", "self", ".", "_out_feature_strides", "=", "{", "\"stem\"", ":", "current_stride", "}", "\n", "self", ".", "_out_feature_channels", "=", "{", "\"stem\"", ":", "self", ".", "stem", ".", "out_channels", "}", "\n", "self", ".", "stages_and_names", "=", "[", "]", "\n", "prev_w", "=", "stem_width", "\n", "\n", "for", "i", ",", "(", "d", ",", "w", ",", "s", ",", "b", ",", "g", ")", "in", "enumerate", "(", "\n", "zip", "(", "depths", ",", "widths", ",", "strides", ",", "bottleneck_ratios", ",", "group_widths", ")", "\n", ")", ":", "\n", "            ", "params", "=", "{", "\"bot_mul\"", ":", "b", ",", "\"group_w\"", ":", "g", ",", "\"se_r\"", ":", "se_ratio", "}", "\n", "stage", "=", "AnyStage", "(", "prev_w", ",", "w", ",", "s", ",", "d", ",", "block_class", ",", "norm", ",", "activation_class", ",", "params", ")", "\n", "name", "=", "\"s{}\"", ".", "format", "(", "i", "+", "1", ")", "\n", "self", ".", "add_module", "(", "name", ",", "stage", ")", "\n", "self", ".", "stages_and_names", ".", "append", "(", "(", "stage", ",", "name", ")", ")", "\n", "self", ".", "_out_feature_strides", "[", "name", "]", "=", "current_stride", "=", "int", "(", "\n", "current_stride", "*", "np", ".", "prod", "(", "[", "k", ".", "stride", "for", "k", "in", "stage", ".", "children", "(", ")", "]", ")", "\n", ")", "\n", "self", ".", "_out_feature_channels", "[", "name", "]", "=", "list", "(", "stage", ".", "children", "(", ")", ")", "[", "-", "1", "]", ".", "out_channels", "\n", "prev_w", "=", "w", "\n", "\n", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "\n", "if", "out_features", "is", "None", ":", "\n", "            ", "out_features", "=", "[", "name", "]", "\n", "", "self", ".", "_out_features", "=", "out_features", "\n", "assert", "len", "(", "self", ".", "_out_features", ")", "\n", "children", "=", "[", "x", "[", "0", "]", "for", "x", "in", "self", ".", "named_children", "(", ")", "]", "\n", "for", "out_feature", "in", "self", ".", "_out_features", ":", "\n", "            ", "assert", "out_feature", "in", "children", ",", "\"Available children: {} does not include {}\"", ".", "format", "(", "\n", "\", \"", ".", "join", "(", "children", ")", ",", "out_feature", "\n", ")", "\n", "", "self", ".", "freeze", "(", "freeze_at", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.AnyNet.forward": [[305, 323], ["regnet.AnyNet.stem", "stage.dim", "stage"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\n\n        Returns:\n            dict[str->Tensor]: names and the corresponding features\n        \"\"\"", "\n", "assert", "x", ".", "dim", "(", ")", "==", "4", ",", "f\"Model takes an input of shape (N, C, H, W). Got {x.shape} instead!\"", "\n", "outputs", "=", "{", "}", "\n", "x", "=", "self", ".", "stem", "(", "x", ")", "\n", "if", "\"stem\"", "in", "self", ".", "_out_features", ":", "\n", "            ", "outputs", "[", "\"stem\"", "]", "=", "x", "\n", "", "for", "stage", ",", "name", "in", "self", ".", "stages_and_names", ":", "\n", "            ", "x", "=", "stage", "(", "x", ")", "\n", "if", "name", "in", "self", ".", "_out_features", ":", "\n", "                ", "outputs", "[", "name", "]", "=", "x", "\n", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.AnyNet.output_shape": [[324, 330], ["detectron2.layers.ShapeSpec"], "methods", ["None"], ["", "def", "output_shape", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "name", ":", "ShapeSpec", "(", "\n", "channels", "=", "self", ".", "_out_feature_channels", "[", "name", "]", ",", "stride", "=", "self", ".", "_out_feature_strides", "[", "name", "]", "\n", ")", "\n", "for", "name", "in", "self", ".", "_out_features", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.AnyNet.freeze": [[332, 354], ["enumerate", "regnet.AnyNet.stem.freeze", "stage.children", "block.freeze"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze"], ["", "def", "freeze", "(", "self", ",", "freeze_at", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Freeze the first several stages of the model. Commonly used in fine-tuning.\n\n        Layers that produce the same feature map spatial size are defined as one\n        \"stage\" by :paper:`FPN`.\n\n        Args:\n            freeze_at (int): number of stages to freeze.\n                `1` means freezing the stem. `2` means freezing the stem and\n                one residual stage, etc.\n\n        Returns:\n            nn.Module: this model itself\n        \"\"\"", "\n", "if", "freeze_at", ">=", "1", ":", "\n", "            ", "self", ".", "stem", ".", "freeze", "(", ")", "\n", "", "for", "idx", ",", "(", "stage", ",", "_", ")", "in", "enumerate", "(", "self", ".", "stages_and_names", ",", "start", "=", "2", ")", ":", "\n", "            ", "if", "freeze_at", ">=", "idx", ":", "\n", "                ", "for", "block", "in", "stage", ".", "children", "(", ")", ":", "\n", "                    ", "block", ".", "freeze", "(", ")", "\n", "", "", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.RegNet.__init__": [[390, 452], ["regnet.adjust_block_compatibility", "regnet.AnyNet.__init__", "regnet.generate_regnet_parameters", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.adjust_block_compatibility", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.generate_regnet_parameters"], ["def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "stem_class", ",", "\n", "stem_width", ",", "\n", "block_class", ",", "\n", "depth", ",", "\n", "w_a", ",", "\n", "w_0", ",", "\n", "w_m", ",", "\n", "group_width", ",", "\n", "stride", "=", "2", ",", "\n", "bottleneck_ratio", "=", "1.0", ",", "\n", "se_ratio", "=", "0.0", ",", "\n", "activation_class", "=", "None", ",", "\n", "freeze_at", "=", "0", ",", "\n", "norm", "=", "\"BN\"", ",", "\n", "out_features", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Build a RegNet from the parameterization described in :paper:`dds` Section 3.3.\n\n        Args:\n            See :class:`AnyNet` for arguments that are not listed here.\n            depth (int): Total number of blocks in the RegNet.\n            w_a (float): Factor by which block width would increase prior to quantizing block widths\n                by stage. See :paper:`dds` Section 3.3.\n            w_0 (int): Initial block width. See :paper:`dds` Section 3.3.\n            w_m (float): Parameter controlling block width quantization.\n                See :paper:`dds` Section 3.3.\n            group_width (int): Number of channels per group in group convolution, if the block uses\n                group convolution.\n            bottleneck_ratio (float): The ratio of the number of bottleneck channels to the number\n                of block input channels (or, equivalently, output channels), if the block uses a\n                bottleneck.\n            stride (int): The stride that each network stage applies to its input.\n        \"\"\"", "\n", "ws", ",", "ds", "=", "generate_regnet_parameters", "(", "w_a", ",", "w_0", ",", "w_m", ",", "depth", ")", "[", "0", ":", "2", "]", "\n", "ss", "=", "[", "stride", "for", "_", "in", "ws", "]", "\n", "bs", "=", "[", "bottleneck_ratio", "for", "_", "in", "ws", "]", "\n", "gs", "=", "[", "group_width", "for", "_", "in", "ws", "]", "\n", "ws", ",", "bs", ",", "gs", "=", "adjust_block_compatibility", "(", "ws", ",", "bs", ",", "gs", ")", "\n", "\n", "def", "default_activation_class", "(", ")", ":", "\n", "            ", "return", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "\n", "stem_class", "=", "stem_class", ",", "\n", "stem_width", "=", "stem_width", ",", "\n", "block_class", "=", "block_class", ",", "\n", "depths", "=", "ds", ",", "\n", "widths", "=", "ws", ",", "\n", "strides", "=", "ss", ",", "\n", "group_widths", "=", "gs", ",", "\n", "bottleneck_ratios", "=", "bs", ",", "\n", "se_ratio", "=", "se_ratio", ",", "\n", "activation_class", "=", "default_activation_class", "\n", "if", "activation_class", "is", "None", "\n", "else", "activation_class", ",", "\n", "freeze_at", "=", "freeze_at", ",", "\n", "norm", "=", "norm", ",", "\n", "out_features", "=", "out_features", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d": [[28, 33], ["torch.nn.Conv2d"], "function", ["None"], ["def", "conv2d", "(", "w_in", ",", "w_out", ",", "k", ",", "*", ",", "stride", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "False", ")", ":", "\n", "    ", "\"\"\"Helper for building a conv2d layer.\"\"\"", "\n", "assert", "k", "%", "2", "==", "1", ",", "\"Only odd size kernels supported to avoid padding issues.\"", "\n", "s", ",", "p", ",", "g", ",", "b", "=", "stride", ",", "(", "k", "-", "1", ")", "//", "2", ",", "groups", ",", "bias", "\n", "return", "nn", ".", "Conv2d", "(", "w_in", ",", "w_out", ",", "k", ",", "stride", "=", "s", ",", "padding", "=", "p", ",", "groups", "=", "g", ",", "bias", "=", "b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.gap2d": [[35, 38], ["torch.nn.AdaptiveAvgPool2d"], "function", ["None"], ["", "def", "gap2d", "(", ")", ":", "\n", "    ", "\"\"\"Helper for building a global average pooling layer.\"\"\"", "\n", "return", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "1", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.pool2d": [[40, 44], ["torch.nn.MaxPool2d"], "function", ["None"], ["", "def", "pool2d", "(", "k", ",", "*", ",", "stride", "=", "1", ")", ":", "\n", "    ", "\"\"\"Helper for building a pool2d layer.\"\"\"", "\n", "assert", "k", "%", "2", "==", "1", ",", "\"Only odd size kernels supported to avoid padding issues.\"", "\n", "return", "nn", ".", "MaxPool2d", "(", "k", ",", "stride", "=", "stride", ",", "padding", "=", "(", "k", "-", "1", ")", "//", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.init_weights": [[46, 58], ["isinstance", "m.weight.data.normal_", "isinstance", "m.weight.data.fill_", "m.bias.data.zero_", "isinstance", "numpy.sqrt", "m.weight.data.normal_", "m.bias.data.zero_"], "function", ["None"], ["", "def", "init_weights", "(", "m", ")", ":", "\n", "    ", "\"\"\"Performs ResNet-style weight initialization.\"\"\"", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "# Note that there is no bias due to BN", "\n", "        ", "fan_out", "=", "m", ".", "kernel_size", "[", "0", "]", "*", "m", ".", "kernel_size", "[", "1", "]", "*", "m", ".", "out_channels", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "np", ".", "sqrt", "(", "2.0", "/", "fan_out", ")", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "0.01", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.adjust_block_compatibility": [[356, 367], ["all", "all", "len", "len", "len", "int", "int", "max", "int", "max", "zip", "min", "zip", "numpy.lcm", "zip", "int", "zip", "zip", "zip", "zip", "round"], "function", ["None"], ["", "", "def", "adjust_block_compatibility", "(", "ws", ",", "bs", ",", "gs", ")", ":", "\n", "    ", "\"\"\"Adjusts the compatibility of widths, bottlenecks, and groups.\"\"\"", "\n", "assert", "len", "(", "ws", ")", "==", "len", "(", "bs", ")", "==", "len", "(", "gs", ")", "\n", "assert", "all", "(", "w", ">", "0", "and", "b", ">", "0", "and", "g", ">", "0", "for", "w", ",", "b", ",", "g", "in", "zip", "(", "ws", ",", "bs", ",", "gs", ")", ")", "\n", "vs", "=", "[", "int", "(", "max", "(", "1", ",", "w", "*", "b", ")", ")", "for", "w", ",", "b", "in", "zip", "(", "ws", ",", "bs", ")", "]", "\n", "gs", "=", "[", "int", "(", "min", "(", "g", ",", "v", ")", ")", "for", "g", ",", "v", "in", "zip", "(", "gs", ",", "vs", ")", "]", "\n", "ms", "=", "[", "np", ".", "lcm", "(", "g", ",", "b", ")", "if", "b", ">", "1", "else", "g", "for", "g", ",", "b", "in", "zip", "(", "gs", ",", "bs", ")", "]", "\n", "vs", "=", "[", "max", "(", "m", ",", "int", "(", "round", "(", "v", "/", "m", ")", "*", "m", ")", ")", "for", "v", ",", "m", "in", "zip", "(", "vs", ",", "ms", ")", "]", "\n", "ws", "=", "[", "int", "(", "v", "/", "b", ")", "for", "v", ",", "b", "in", "zip", "(", "vs", ",", "bs", ")", "]", "\n", "assert", "all", "(", "w", "*", "b", "%", "g", "==", "0", "for", "w", ",", "b", ",", "g", "in", "zip", "(", "ws", ",", "bs", ",", "gs", ")", ")", "\n", "return", "ws", ",", "bs", ",", "gs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.generate_regnet_parameters": [[369, 385], ["numpy.round", "numpy.unique", "numpy.power", "numpy.round().astype", "len", "x.tolist", "numpy.arange", "numpy.log", "numpy.log", "np.round.max", "numpy.round", "numpy.divide"], "function", ["None"], ["", "def", "generate_regnet_parameters", "(", "w_a", ",", "w_0", ",", "w_m", ",", "d", ",", "q", "=", "8", ")", ":", "\n", "    ", "\"\"\"Generates per stage widths and depths from RegNet parameters.\"\"\"", "\n", "assert", "w_a", ">=", "0", "and", "w_0", ">", "0", "and", "w_m", ">", "1", "and", "w_0", "%", "q", "==", "0", "\n", "# Generate continuous per-block ws", "\n", "ws_cont", "=", "np", ".", "arange", "(", "d", ")", "*", "w_a", "+", "w_0", "\n", "# Generate quantized per-block ws", "\n", "ks", "=", "np", ".", "round", "(", "np", ".", "log", "(", "ws_cont", "/", "w_0", ")", "/", "np", ".", "log", "(", "w_m", ")", ")", "\n", "ws_all", "=", "w_0", "*", "np", ".", "power", "(", "w_m", ",", "ks", ")", "\n", "ws_all", "=", "np", ".", "round", "(", "np", ".", "divide", "(", "ws_all", ",", "q", ")", ")", ".", "astype", "(", "int", ")", "*", "q", "\n", "# Generate per stage ws and ds (assumes ws_all are sorted)", "\n", "ws", ",", "ds", "=", "np", ".", "unique", "(", "ws_all", ",", "return_counts", "=", "True", ")", "\n", "# Compute number of actual stages and total possible stages", "\n", "num_stages", ",", "total_stages", "=", "len", "(", "ws", ")", ",", "ks", ".", "max", "(", ")", "+", "1", "\n", "# Convert numpy arrays to lists and return", "\n", "ws", ",", "ds", ",", "ws_all", ",", "ws_cont", "=", "(", "x", ".", "tolist", "(", ")", "for", "x", "in", "(", "ws", ",", "ds", ",", "ws_all", ",", "ws_cont", ")", ")", "\n", "return", "ws", ",", "ds", ",", "num_stages", ",", "total_stages", ",", "ws_all", ",", "ws_cont", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.__init__": [[15, 20], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass can specify its own set of arguments.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.forward": [[21, 30], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "forward", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Subclasses must override this method, but adhere to the same return type.\n\n        Returns:\n            dict[str->Tensor]: mapping from feature name (e.g., \"res2\") to tensor\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.size_divisibility": [[31, 41], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "size_divisibility", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Some backbones require the input height and width to be divisible by a\n        specific integer. This is typically true for encoder / decoder type networks\n        with lateral connection (e.g., FPN) for which feature maps need to match\n        dimension in the \"bottom up\" and \"top down\" paths. Set to 0 if no specific\n        input size divisibility is required.\n        \"\"\"", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.backbone.Backbone.output_shape": [[42, 53], ["detectron2.layers.ShapeSpec"], "methods", ["None"], ["", "def", "output_shape", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            dict[str->ShapeSpec]\n        \"\"\"", "\n", "# this is a backward-compatible default", "\n", "return", "{", "\n", "name", ":", "ShapeSpec", "(", "\n", "channels", "=", "self", ".", "_out_feature_channels", "[", "name", "]", ",", "stride", "=", "self", ".", "_out_feature_strides", "[", "name", "]", "\n", ")", "\n", "for", "name", "in", "self", ".", "_out_features", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.__init__": [[20, 222], ["torch.as_tensor", "tensor.reshape().to.reshape().to.size", "isinstance", "torch.device", "tensor.reshape().to.reshape().to.numel", "tensor.reshape().to.reshape().to.reshape().to", "tensor.reshape().to.reshape().to.dim", "tensor.reshape().to.reshape().to.size", "tensor.reshape().to.reshape().to.reshape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["def", "__init__", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tensor (Tensor[float]): a Nx5 matrix.  Each row is\n                (x_center, y_center, width, height, angle),\n                in which angle is represented in degrees.\n                While there's no strict range restriction for it,\n                the recommended principal range is between [-180, 180) degrees.\n\n        Assume we have a horizontal box B = (x_center, y_center, width, height),\n        where width is along the x-axis and height is along the y-axis.\n        The rotated box B_rot (x_center, y_center, width, height, angle)\n        can be seen as:\n\n        1. When angle == 0:\n           B_rot == B\n        2. When angle > 0:\n           B_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CCW;\n        3. When angle < 0:\n           B_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CW.\n\n        Mathematically, since the right-handed coordinate system for image space\n        is (y, x), where y is top->down and x is left->right, the 4 vertices of the\n        rotated rectangle :math:`(yr_i, xr_i)` (i = 1, 2, 3, 4) can be obtained from\n        the vertices of the horizontal rectangle :math:`(y_i, x_i)` (i = 1, 2, 3, 4)\n        in the following way (:math:`\\\\theta = angle*\\\\pi/180` is the angle in radians,\n        :math:`(y_c, x_c)` is the center of the rectangle):\n\n        .. math::\n\n            yr_i = \\\\cos(\\\\theta) (y_i - y_c) - \\\\sin(\\\\theta) (x_i - x_c) + y_c,\n\n            xr_i = \\\\sin(\\\\theta) (y_i - y_c) + \\\\cos(\\\\theta) (x_i - x_c) + x_c,\n\n        which is the standard rigid-body rotation transformation.\n\n        Intuitively, the angle is\n        (1) the rotation angle from y-axis in image space\n        to the height vector (top->down in the box's local coordinate system)\n        of the box in CCW, and\n        (2) the rotation angle from x-axis in image space\n        to the width vector (left->right in the box's local coordinate system)\n        of the box in CCW.\n\n        More intuitively, consider the following horizontal box ABCD represented\n        in (x1, y1, x2, y2): (3, 2, 7, 4),\n        covering the [3, 7] x [2, 4] region of the continuous coordinate system\n        which looks like this:\n\n        .. code:: none\n\n            O--------> x\n            |\n            |  A---B\n            |  |   |\n            |  D---C\n            |\n            v y\n\n        Note that each capital letter represents one 0-dimensional geometric point\n        instead of a 'square pixel' here.\n\n        In the example above, using (x, y) to represent a point we have:\n\n        .. math::\n\n            O = (0, 0), A = (3, 2), B = (7, 2), C = (7, 4), D = (3, 4)\n\n        We name vector AB = vector DC as the width vector in box's local coordinate system, and\n        vector AD = vector BC as the height vector in box's local coordinate system. Initially,\n        when angle = 0 degree, they're aligned with the positive directions of x-axis and y-axis\n        in the image space, respectively.\n\n        For better illustration, we denote the center of the box as E,\n\n        .. code:: none\n\n            O--------> x\n            |\n            |  A---B\n            |  | E |\n            |  D---C\n            |\n            v y\n\n        where the center E = ((3+7)/2, (2+4)/2) = (5, 3).\n\n        Also,\n\n        .. math::\n\n            width = |AB| = |CD| = 7 - 3 = 4,\n            height = |AD| = |BC| = 4 - 2 = 2.\n\n        Therefore, the corresponding representation for the same shape in rotated box in\n        (x_center, y_center, width, height, angle) format is:\n\n        (5, 3, 4, 2, 0),\n\n        Now, let's consider (5, 3, 4, 2, 90), which is rotated by 90 degrees\n        CCW (counter-clockwise) by definition. It looks like this:\n\n        .. code:: none\n\n            O--------> x\n            |   B-C\n            |   | |\n            |   |E|\n            |   | |\n            |   A-D\n            v y\n\n        The center E is still located at the same point (5, 3), while the vertices\n        ABCD are rotated by 90 degrees CCW with regard to E:\n        A = (4, 5), B = (4, 1), C = (6, 1), D = (6, 5)\n\n        Here, 90 degrees can be seen as the CCW angle to rotate from y-axis to\n        vector AD or vector BC (the top->down height vector in box's local coordinate system),\n        or the CCW angle to rotate from x-axis to vector AB or vector DC (the left->right\n        width vector in box's local coordinate system).\n\n        .. math::\n\n            width = |AB| = |CD| = 5 - 1 = 4,\n            height = |AD| = |BC| = 6 - 4 = 2.\n\n        Next, how about (5, 3, 4, 2, -90), which is rotated by 90 degrees CW (clockwise)\n        by definition? It looks like this:\n\n        .. code:: none\n\n            O--------> x\n            |   D-A\n            |   | |\n            |   |E|\n            |   | |\n            |   C-B\n            v y\n\n        The center E is still located at the same point (5, 3), while the vertices\n        ABCD are rotated by 90 degrees CW with regard to E:\n        A = (6, 1), B = (6, 5), C = (4, 5), D = (4, 1)\n\n        .. math::\n\n            width = |AB| = |CD| = 5 - 1 = 4,\n            height = |AD| = |BC| = 6 - 4 = 2.\n\n        This covers exactly the same region as (5, 3, 4, 2, 90) does, and their IoU\n        will be 1. However, these two will generate different RoI Pooling results and\n        should not be treated as an identical box.\n\n        On the other hand, it's easy to see that (X, Y, W, H, A) is identical to\n        (X, Y, W, H, A+360N), for any integer N. For example (5, 3, 4, 2, 270) would be\n        identical to (5, 3, 4, 2, -90), because rotating the shape 270 degrees CCW is\n        equivalent to rotating the same shape 90 degrees CW.\n\n        We could rotate further to get (5, 3, 4, 2, 180), or (5, 3, 4, 2, -180):\n\n        .. code:: none\n\n            O--------> x\n            |\n            |  C---D\n            |  | E |\n            |  B---A\n            |\n            v y\n\n        .. math::\n\n            A = (7, 4), B = (3, 4), C = (3, 2), D = (7, 2),\n\n            width = |AB| = |CD| = 7 - 3 = 4,\n            height = |AD| = |BC| = 4 - 2 = 2.\n\n        Finally, this is a very inaccurate (heavily quantized) illustration of\n        how (5, 3, 4, 2, 60) looks like in case anyone wonders:\n\n        .. code:: none\n\n            O--------> x\n            |     B\\\n            |    /  C\n            |   /E /\n            |  A  /\n            |   `D\n            v y\n\n        It's still a rectangle with center of (5, 3), width of 4 and height of 2,\n        but its angle (and thus orientation) is somewhere between\n        (5, 3, 4, 2, 0) and (5, 3, 4, 2, 90).\n        \"\"\"", "\n", "device", "=", "tensor", ".", "device", "if", "isinstance", "(", "tensor", ",", "torch", ".", "Tensor", ")", "else", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "tensor", "=", "torch", ".", "as_tensor", "(", "tensor", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "if", "tensor", ".", "numel", "(", ")", "==", "0", ":", "\n", "# Use reshape, so we don't end up creating a new tensor that does not depend on", "\n", "# the inputs (and consequently confuses jit)", "\n", "            ", "tensor", "=", "tensor", ".", "reshape", "(", "(", "0", ",", "5", ")", ")", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "", "assert", "tensor", ".", "dim", "(", ")", "==", "2", "and", "tensor", ".", "size", "(", "-", "1", ")", "==", "5", ",", "tensor", ".", "size", "(", ")", "\n", "\n", "self", ".", "tensor", "=", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.clone": [[223, 231], ["rotated_boxes.RotatedBoxes", "rotated_boxes.RotatedBoxes.tensor.clone"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["", "def", "clone", "(", "self", ")", "->", "\"RotatedBoxes\"", ":", "\n", "        ", "\"\"\"\n        Clone the RotatedBoxes.\n\n        Returns:\n            RotatedBoxes\n        \"\"\"", "\n", "return", "RotatedBoxes", "(", "self", ".", "tensor", ".", "clone", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.to": [[232, 236], ["rotated_boxes.RotatedBoxes", "rotated_boxes.RotatedBoxes.tensor.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "@", "_maybe_jit_unused", "\n", "def", "to", "(", "self", ",", "device", ":", "torch", ".", "device", ")", ":", "\n", "# Boxes are assumed float32 and does not support to(dtype)", "\n", "        ", "return", "RotatedBoxes", "(", "self", ".", "tensor", ".", "to", "(", "device", "=", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.area": [[237, 247], ["None"], "methods", ["None"], ["", "def", "area", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes the area of all the boxes.\n\n        Returns:\n            torch.Tensor: a vector with areas of each box.\n        \"\"\"", "\n", "box", "=", "self", ".", "tensor", "\n", "area", "=", "box", "[", ":", ",", "2", "]", "*", "box", "[", ":", ",", "3", "]", "\n", "return", "area", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.normalize_angles": [[248, 253], ["None"], "methods", ["None"], ["", "def", "normalize_angles", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Restrict angles to the range of [-180, 180) degrees\n        \"\"\"", "\n", "self", ".", "tensor", "[", ":", ",", "4", "]", "=", "(", "self", ".", "tensor", "[", ":", ",", "4", "]", "+", "180.0", ")", "%", "360.0", "-", "180.0", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.clip": [[254, 303], ["rotated_boxes.RotatedBoxes.normalize_angles", "x1.clamp_", "y1.clamp_", "x2.clamp_", "y2.clamp_", "torch.min", "torch.min", "torch.where", "torch.abs"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.normalize_angles"], ["", "def", "clip", "(", "self", ",", "box_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "clip_angle_threshold", ":", "float", "=", "1.0", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Clip (in place) the boxes by limiting x coordinates to the range [0, width]\n        and y coordinates to the range [0, height].\n\n        For RRPN:\n        Only clip boxes that are almost horizontal with a tolerance of\n        clip_angle_threshold to maintain backward compatibility.\n\n        Rotated boxes beyond this threshold are not clipped for two reasons:\n\n        1. There are potentially multiple ways to clip a rotated box to make it\n           fit within the image.\n        2. It's tricky to make the entire rectangular box fit within the image\n           and still be able to not leave out pixels of interest.\n\n        Therefore we rely on ops like RoIAlignRotated to safely handle this.\n\n        Args:\n            box_size (height, width): The clipping box's size.\n            clip_angle_threshold:\n                Iff. abs(normalized(angle)) <= clip_angle_threshold (in degrees),\n                we do the clipping as horizontal boxes.\n        \"\"\"", "\n", "h", ",", "w", "=", "box_size", "\n", "\n", "# normalize angles to be within (-180, 180] degrees", "\n", "self", ".", "normalize_angles", "(", ")", "\n", "\n", "idx", "=", "torch", ".", "where", "(", "torch", ".", "abs", "(", "self", ".", "tensor", "[", ":", ",", "4", "]", ")", "<=", "clip_angle_threshold", ")", "[", "0", "]", "\n", "\n", "# convert to (x1, y1, x2, y2)", "\n", "x1", "=", "self", ".", "tensor", "[", "idx", ",", "0", "]", "-", "self", ".", "tensor", "[", "idx", ",", "2", "]", "/", "2.0", "\n", "y1", "=", "self", ".", "tensor", "[", "idx", ",", "1", "]", "-", "self", ".", "tensor", "[", "idx", ",", "3", "]", "/", "2.0", "\n", "x2", "=", "self", ".", "tensor", "[", "idx", ",", "0", "]", "+", "self", ".", "tensor", "[", "idx", ",", "2", "]", "/", "2.0", "\n", "y2", "=", "self", ".", "tensor", "[", "idx", ",", "1", "]", "+", "self", ".", "tensor", "[", "idx", ",", "3", "]", "/", "2.0", "\n", "\n", "# clip", "\n", "x1", ".", "clamp_", "(", "min", "=", "0", ",", "max", "=", "w", ")", "\n", "y1", ".", "clamp_", "(", "min", "=", "0", ",", "max", "=", "h", ")", "\n", "x2", ".", "clamp_", "(", "min", "=", "0", ",", "max", "=", "w", ")", "\n", "y2", ".", "clamp_", "(", "min", "=", "0", ",", "max", "=", "h", ")", "\n", "\n", "# convert back to (xc, yc, w, h)", "\n", "self", ".", "tensor", "[", "idx", ",", "0", "]", "=", "(", "x1", "+", "x2", ")", "/", "2.0", "\n", "self", ".", "tensor", "[", "idx", ",", "1", "]", "=", "(", "y1", "+", "y2", ")", "/", "2.0", "\n", "# make sure widths and heights do not increase due to numerical errors", "\n", "self", ".", "tensor", "[", "idx", ",", "2", "]", "=", "torch", ".", "min", "(", "self", ".", "tensor", "[", "idx", ",", "2", "]", ",", "x2", "-", "x1", ")", "\n", "self", ".", "tensor", "[", "idx", ",", "3", "]", "=", "torch", ".", "min", "(", "self", ".", "tensor", "[", "idx", ",", "3", "]", ",", "y2", "-", "y1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.nonempty": [[304, 318], ["None"], "methods", ["None"], ["", "def", "nonempty", "(", "self", ",", "threshold", ":", "float", "=", "0.0", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Find boxes that are non-empty.\n        A box is considered empty, if either of its side is no larger than threshold.\n\n        Returns:\n            Tensor: a binary vector which represents\n            whether each box is empty (False) or non-empty (True).\n        \"\"\"", "\n", "box", "=", "self", ".", "tensor", "\n", "widths", "=", "box", "[", ":", ",", "2", "]", "\n", "heights", "=", "box", "[", ":", ",", "3", "]", "\n", "keep", "=", "(", "widths", ">", "threshold", ")", "&", "(", "heights", ">", "threshold", ")", "\n", "return", "keep", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.__getitem__": [[319, 341], ["isinstance", "rotated_boxes.RotatedBoxes", "rotated_boxes.RotatedBoxes", "b.dim", "rotated_boxes.RotatedBoxes.tensor[].view"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", "->", "\"RotatedBoxes\"", ":", "\n", "        ", "\"\"\"\n        Returns:\n            RotatedBoxes: Create a new :class:`RotatedBoxes` by indexing.\n\n        The following usage are allowed:\n\n        1. `new_boxes = boxes[3]`: return a `RotatedBoxes` which contains only one box.\n        2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n        3. `new_boxes = boxes[vector]`, where vector is a torch.ByteTensor\n           with `length = len(boxes)`. Nonzero elements in the vector will be selected.\n\n        Note that the returned RotatedBoxes might share storage with this RotatedBoxes,\n        subject to Pytorch's indexing semantics.\n        \"\"\"", "\n", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "            ", "return", "RotatedBoxes", "(", "self", ".", "tensor", "[", "item", "]", ".", "view", "(", "1", ",", "-", "1", ")", ")", "\n", "", "b", "=", "self", ".", "tensor", "[", "item", "]", "\n", "assert", "b", ".", "dim", "(", ")", "==", "2", ",", "\"Indexing on RotatedBoxes with {} failed to return a matrix!\"", ".", "format", "(", "\n", "item", "\n", ")", "\n", "return", "RotatedBoxes", "(", "b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.__len__": [[342, 344], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.__repr__": [[345, 347], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "\"RotatedBoxes(\"", "+", "str", "(", "self", ".", "tensor", ")", "+", "\")\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.inside_box": [[348, 384], ["torch.abs", "torch.abs", "torch.cos", "torch.sin"], "methods", ["None"], ["", "def", "inside_box", "(", "self", ",", "box_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "boundary_threshold", ":", "int", "=", "0", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Args:\n            box_size (height, width): Size of the reference box covering\n                [0, width] x [0, height]\n            boundary_threshold (int): Boxes that extend beyond the reference box\n                boundary by more than boundary_threshold are considered \"outside\".\n\n        For RRPN, it might not be necessary to call this function since it's common\n        for rotated box to extend to outside of the image boundaries\n        (the clip function only clips the near-horizontal boxes)\n\n        Returns:\n            a binary vector, indicating whether each box is inside the reference box.\n        \"\"\"", "\n", "height", ",", "width", "=", "box_size", "\n", "\n", "cnt_x", "=", "self", ".", "tensor", "[", "...", ",", "0", "]", "\n", "cnt_y", "=", "self", ".", "tensor", "[", "...", ",", "1", "]", "\n", "half_w", "=", "self", ".", "tensor", "[", "...", ",", "2", "]", "/", "2.0", "\n", "half_h", "=", "self", ".", "tensor", "[", "...", ",", "3", "]", "/", "2.0", "\n", "a", "=", "self", ".", "tensor", "[", "...", ",", "4", "]", "\n", "c", "=", "torch", ".", "abs", "(", "torch", ".", "cos", "(", "a", "*", "math", ".", "pi", "/", "180.0", ")", ")", "\n", "s", "=", "torch", ".", "abs", "(", "torch", ".", "sin", "(", "a", "*", "math", ".", "pi", "/", "180.0", ")", ")", "\n", "# This basically computes the horizontal bounding rectangle of the rotated box", "\n", "max_rect_dx", "=", "c", "*", "half_w", "+", "s", "*", "half_h", "\n", "max_rect_dy", "=", "c", "*", "half_h", "+", "s", "*", "half_w", "\n", "\n", "inds_inside", "=", "(", "\n", "(", "cnt_x", "-", "max_rect_dx", ">=", "-", "boundary_threshold", ")", "\n", "&", "(", "cnt_y", "-", "max_rect_dy", ">=", "-", "boundary_threshold", ")", "\n", "&", "(", "cnt_x", "+", "max_rect_dx", "<", "width", "+", "boundary_threshold", ")", "\n", "&", "(", "cnt_y", "+", "max_rect_dy", "<", "height", "+", "boundary_threshold", ")", "\n", ")", "\n", "\n", "return", "inds_inside", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.get_centers": [[385, 391], ["None"], "methods", ["None"], ["", "def", "get_centers", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Returns:\n            The box centers in a Nx2 array of (x, y).\n        \"\"\"", "\n", "return", "self", ".", "tensor", "[", ":", ",", ":", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.scale": [[392, 456], ["torch.cos", "torch.sin", "torch.sqrt", "torch.sqrt", "torch.atan2"], "methods", ["None"], ["", "def", "scale", "(", "self", ",", "scale_x", ":", "float", ",", "scale_y", ":", "float", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Scale the rotated box with horizontal and vertical scaling factors\n        Note: when scale_factor_x != scale_factor_y,\n        the rotated box does not preserve the rectangular shape when the angle\n        is not a multiple of 90 degrees under resize transformation.\n        Instead, the shape is a parallelogram (that has skew)\n        Here we make an approximation by fitting a rotated rectangle to the parallelogram.\n        \"\"\"", "\n", "self", ".", "tensor", "[", ":", ",", "0", "]", "*=", "scale_x", "\n", "self", ".", "tensor", "[", ":", ",", "1", "]", "*=", "scale_y", "\n", "theta", "=", "self", ".", "tensor", "[", ":", ",", "4", "]", "*", "math", ".", "pi", "/", "180.0", "\n", "c", "=", "torch", ".", "cos", "(", "theta", ")", "\n", "s", "=", "torch", ".", "sin", "(", "theta", ")", "\n", "\n", "# In image space, y is top->down and x is left->right", "\n", "# Consider the local coordintate system for the rotated box,", "\n", "# where the box center is located at (0, 0), and the four vertices ABCD are", "\n", "# A(-w / 2, -h / 2), B(w / 2, -h / 2), C(w / 2, h / 2), D(-w / 2, h / 2)", "\n", "# the midpoint of the left edge AD of the rotated box E is:", "\n", "# E = (A+D)/2 = (-w / 2, 0)", "\n", "# the midpoint of the top edge AB of the rotated box F is:", "\n", "# F(0, -h / 2)", "\n", "# To get the old coordinates in the global system, apply the rotation transformation", "\n", "# (Note: the right-handed coordinate system for image space is yOx):", "\n", "# (old_x, old_y) = (s * y + c * x, c * y - s * x)", "\n", "# E(old) = (s * 0 + c * (-w/2), c * 0 - s * (-w/2)) = (-c * w / 2, s * w / 2)", "\n", "# F(old) = (s * (-h / 2) + c * 0, c * (-h / 2) - s * 0) = (-s * h / 2, -c * h / 2)", "\n", "# After applying the scaling factor (sfx, sfy):", "\n", "# E(new) = (-sfx * c * w / 2, sfy * s * w / 2)", "\n", "# F(new) = (-sfx * s * h / 2, -sfy * c * h / 2)", "\n", "# The new width after scaling tranformation becomes:", "\n", "\n", "# w(new) = |E(new) - O| * 2", "\n", "#        = sqrt[(sfx * c * w / 2)^2 + (sfy * s * w / 2)^2] * 2", "\n", "#        = sqrt[(sfx * c)^2 + (sfy * s)^2] * w", "\n", "# i.e., scale_factor_w = sqrt[(sfx * c)^2 + (sfy * s)^2]", "\n", "#", "\n", "# For example,", "\n", "# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_w == scale_factor_x;", "\n", "# when |angle| = 90, c = 0, |s| = 1, scale_factor_w == scale_factor_y", "\n", "self", ".", "tensor", "[", ":", ",", "2", "]", "*=", "torch", ".", "sqrt", "(", "(", "scale_x", "*", "c", ")", "**", "2", "+", "(", "scale_y", "*", "s", ")", "**", "2", ")", "\n", "\n", "# h(new) = |F(new) - O| * 2", "\n", "#        = sqrt[(sfx * s * h / 2)^2 + (sfy * c * h / 2)^2] * 2", "\n", "#        = sqrt[(sfx * s)^2 + (sfy * c)^2] * h", "\n", "# i.e., scale_factor_h = sqrt[(sfx * s)^2 + (sfy * c)^2]", "\n", "#", "\n", "# For example,", "\n", "# when angle = 0 or 180, |c| = 1, s = 0, scale_factor_h == scale_factor_y;", "\n", "# when |angle| = 90, c = 0, |s| = 1, scale_factor_h == scale_factor_x", "\n", "self", ".", "tensor", "[", ":", ",", "3", "]", "*=", "torch", ".", "sqrt", "(", "(", "scale_x", "*", "s", ")", "**", "2", "+", "(", "scale_y", "*", "c", ")", "**", "2", ")", "\n", "\n", "# The angle is the rotation angle from y-axis in image space to the height", "\n", "# vector (top->down in the box's local coordinate system) of the box in CCW.", "\n", "#", "\n", "# angle(new) = angle_yOx(O - F(new))", "\n", "#            = angle_yOx( (sfx * s * h / 2, sfy * c * h / 2) )", "\n", "#            = atan2(sfx * s * h / 2, sfy * c * h / 2)", "\n", "#            = atan2(sfx * s, sfy * c)", "\n", "#", "\n", "# For example,", "\n", "# when sfx == sfy, angle(new) == atan2(s, c) == angle(old)", "\n", "self", ".", "tensor", "[", ":", ",", "4", "]", "=", "torch", ".", "atan2", "(", "scale_x", "*", "s", ",", "scale_y", "*", "c", ")", "*", "180", "/", "math", ".", "pi", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.cat": [[457, 477], ["isinstance", "all", "cls", "len", "cls", "torch.cat", "torch.empty", "isinstance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "@", "classmethod", "\n", "@", "_maybe_jit_unused", "\n", "def", "cat", "(", "cls", ",", "boxes_list", ":", "List", "[", "\"RotatedBoxes\"", "]", ")", "->", "\"RotatedBoxes\"", ":", "\n", "        ", "\"\"\"\n        Concatenates a list of RotatedBoxes into a single RotatedBoxes\n\n        Arguments:\n            boxes_list (list[RotatedBoxes])\n\n        Returns:\n            RotatedBoxes: the concatenated RotatedBoxes\n        \"\"\"", "\n", "assert", "isinstance", "(", "boxes_list", ",", "(", "list", ",", "tuple", ")", ")", "\n", "if", "len", "(", "boxes_list", ")", "==", "0", ":", "\n", "            ", "return", "cls", "(", "torch", ".", "empty", "(", "0", ")", ")", "\n", "", "assert", "all", "(", "[", "isinstance", "(", "box", ",", "RotatedBoxes", ")", "for", "box", "in", "boxes_list", "]", ")", "\n", "\n", "# use torch.cat (v.s. layers.cat) so the returned boxes never share storage with input", "\n", "cat_boxes", "=", "cls", "(", "torch", ".", "cat", "(", "[", "b", ".", "tensor", "for", "b", "in", "boxes_list", "]", ",", "dim", "=", "0", ")", ")", "\n", "return", "cat_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.device": [[478, 481], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", "->", "torch", ".", "device", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.__iter__": [[482, 488], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Yield a box as a Tensor of shape (5,) at a time.\n        \"\"\"", "\n", "yield", "from", "self", ".", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.pairwise_iou": [[490, 506], ["detectron2.layers.rotated_boxes.pairwise_iou_rotated"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "", "def", "pairwise_iou", "(", "boxes1", ":", "RotatedBoxes", ",", "boxes2", ":", "RotatedBoxes", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Given two lists of rotated boxes of size N and M,\n    compute the IoU (intersection over union)\n    between **all** N x M pairs of boxes.\n    The box order must be (x_center, y_center, width, height, angle).\n\n    Args:\n        boxes1, boxes2 (RotatedBoxes):\n            two `RotatedBoxes`. Contains N & M rotated boxes, respectively.\n\n    Returns:\n        Tensor: IoU, sized [N,M].\n    \"\"\"", "\n", "\n", "return", "pairwise_iou_rotated", "(", "boxes1", ".", "tensor", ",", "boxes2", ".", "tensor", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.__init__": [[36, 45], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ",", "image_sizes", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Arguments:\n            tensor (Tensor): of shape (N, H, W) or (N, C_1, ..., C_K, H, W) where K >= 1\n            image_sizes (list[tuple[int, int]]): Each tuple is (h, w). It can\n                be smaller than (H, W) due to padding.\n        \"\"\"", "\n", "self", ".", "tensor", "=", "tensor", "\n", "self", ".", "image_sizes", "=", "image_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.__len__": [[46, 48], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "image_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.__getitem__": [[49, 61], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Access the individual image in its original size.\n\n        Args:\n            idx: int or slice\n\n        Returns:\n            Tensor: an image of shape (H, W) or (C_1, ..., C_K, H, W) where K >= 1\n        \"\"\"", "\n", "size", "=", "self", ".", "image_sizes", "[", "idx", "]", "\n", "return", "self", ".", "tensor", "[", "idx", ",", "...", ",", ":", "size", "[", "0", "]", ",", ":", "size", "[", "1", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.to": [[62, 66], ["image_list.ImageList.tensor.to", "image_list.ImageList"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "to", "(", "self", ",", "*", "args", ":", "Any", ",", "**", "kwargs", ":", "Any", ")", "->", "\"ImageList\"", ":", "\n", "        ", "cast_tensor", "=", "self", ".", "tensor", ".", "to", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "return", "ImageList", "(", "cast_tensor", ",", "self", ".", "image_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.device": [[67, 70], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", "->", "device", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors": [[71, 125], ["isinstance", "torch.jit.is_scripting", "image_list.ImageList", "len", "isinstance", "type", "image_list._as_tensor", "torch.stack().max", "max_size.to().tolist", "len", "torch.nn.functional.pad().unsqueeze_", "tensors[].new_full", "zip", "tensors[].new_full.contiguous", "torch.jit.is_tracing", "list", "pad_img[].copy_", "torch.stack", "max_size.to", "torch.nn.functional.pad", "list", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list._as_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "@", "staticmethod", "\n", "def", "from_tensors", "(", "\n", "tensors", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "size_divisibility", ":", "int", "=", "0", ",", "pad_value", ":", "float", "=", "0.0", "\n", ")", "->", "\"ImageList\"", ":", "\n", "        ", "\"\"\"\n        Args:\n            tensors: a tuple or list of `torch.Tensor`, each of shape (Hi, Wi) or\n                (C_1, ..., C_K, Hi, Wi) where K >= 1. The Tensors will be padded\n                to the same shape with `pad_value`.\n            size_divisibility (int): If `size_divisibility > 0`, add padding to ensure\n                the common height and width is divisible by `size_divisibility`.\n                This depends on the model and many models need a divisibility of 32.\n            pad_value (float): value to pad\n\n        Returns:\n            an `ImageList`.\n        \"\"\"", "\n", "assert", "len", "(", "tensors", ")", ">", "0", "\n", "assert", "isinstance", "(", "tensors", ",", "(", "tuple", ",", "list", ")", ")", "\n", "for", "t", "in", "tensors", ":", "\n", "            ", "assert", "isinstance", "(", "t", ",", "torch", ".", "Tensor", ")", ",", "type", "(", "t", ")", "\n", "assert", "t", ".", "shape", "[", ":", "-", "2", "]", "==", "tensors", "[", "0", "]", ".", "shape", "[", ":", "-", "2", "]", ",", "t", ".", "shape", "\n", "\n", "", "image_sizes", "=", "[", "(", "im", ".", "shape", "[", "-", "2", "]", ",", "im", ".", "shape", "[", "-", "1", "]", ")", "for", "im", "in", "tensors", "]", "\n", "image_sizes_tensor", "=", "[", "_as_tensor", "(", "x", ")", "for", "x", "in", "image_sizes", "]", "\n", "max_size", "=", "torch", ".", "stack", "(", "image_sizes_tensor", ")", ".", "max", "(", "0", ")", ".", "values", "\n", "\n", "if", "size_divisibility", ">", "1", ":", "\n", "            ", "stride", "=", "size_divisibility", "\n", "# the last two dims are H,W, both subject to divisibility requirement", "\n", "max_size", "=", "(", "max_size", "+", "(", "stride", "-", "1", ")", ")", "//", "stride", "*", "stride", "\n", "\n", "# handle weirdness of scripting and tracing ...", "\n", "", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "max_size", ":", "List", "[", "int", "]", "=", "max_size", ".", "to", "(", "dtype", "=", "torch", ".", "long", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "# https://github.com/pytorch/pytorch/issues/42448", "\n", "            ", "if", "TORCH_VERSION", ">=", "(", "1", ",", "7", ")", "and", "torch", ".", "jit", ".", "is_tracing", "(", ")", ":", "\n", "                ", "image_sizes", "=", "image_sizes_tensor", "\n", "\n", "", "", "if", "len", "(", "tensors", ")", "==", "1", ":", "\n", "# This seems slightly (2%) faster.", "\n", "# TODO: check whether it's faster for multiple images as well", "\n", "            ", "image_size", "=", "image_sizes", "[", "0", "]", "\n", "padding_size", "=", "[", "0", ",", "max_size", "[", "-", "1", "]", "-", "image_size", "[", "1", "]", ",", "0", ",", "max_size", "[", "-", "2", "]", "-", "image_size", "[", "0", "]", "]", "\n", "batched_imgs", "=", "F", ".", "pad", "(", "tensors", "[", "0", "]", ",", "padding_size", ",", "value", "=", "pad_value", ")", ".", "unsqueeze_", "(", "0", ")", "\n", "", "else", ":", "\n", "# max_size can be a tensor in tracing mode, therefore convert to list", "\n", "            ", "batch_shape", "=", "[", "len", "(", "tensors", ")", "]", "+", "list", "(", "tensors", "[", "0", "]", ".", "shape", "[", ":", "-", "2", "]", ")", "+", "list", "(", "max_size", ")", "\n", "batched_imgs", "=", "tensors", "[", "0", "]", ".", "new_full", "(", "batch_shape", ",", "pad_value", ")", "\n", "for", "img", ",", "pad_img", "in", "zip", "(", "tensors", ",", "batched_imgs", ")", ":", "\n", "                ", "pad_img", "[", "...", ",", ":", "img", ".", "shape", "[", "-", "2", "]", ",", ":", "img", ".", "shape", "[", "-", "1", "]", "]", ".", "copy_", "(", "img", ")", "\n", "\n", "", "", "return", "ImageList", "(", "batched_imgs", ".", "contiguous", "(", ")", ",", "image_sizes", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list._as_tensor": [[11, 22], ["torch.jit.is_scripting", "torch.as_tensor", "torch.as_tensor", "isinstance", "all", "torch.stack", "isinstance"], "function", ["None"], ["def", "_as_tensor", "(", "x", ":", "Tuple", "[", "int", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    An equivalent of `torch.as_tensor`, but works under tracing if input\n    is a list of tensor. `torch.as_tensor` will record a constant in tracing,\n    but this function will use `torch.stack` instead.\n    \"\"\"", "\n", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "        ", "return", "torch", ".", "as_tensor", "(", "x", ")", "\n", "", "if", "isinstance", "(", "x", ",", "(", "list", ",", "tuple", ")", ")", "and", "all", "(", "[", "isinstance", "(", "t", ",", "torch", ".", "Tensor", ")", "for", "t", "in", "x", "]", ")", ":", "\n", "        ", "return", "torch", ".", "stack", "(", "x", ")", "\n", "", "return", "torch", ".", "as_tensor", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.__init__": [[35, 46], ["tsv_file.generate_lineidx", "os.splitext", "os.splitext", "os.isfile", "os.isfile"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.generate_lineidx"], ["    ", "def", "__init__", "(", "self", ",", "tsv_file", ",", "generate_lineidx", "=", "False", ")", ":", "\n", "        ", "self", ".", "tsv_file", "=", "tsv_file", "\n", "self", ".", "lineidx", "=", "op", ".", "splitext", "(", "tsv_file", ")", "[", "0", "]", "+", "'.lineidx'", "\n", "self", ".", "_fp", "=", "None", "\n", "self", ".", "_lineidx", "=", "None", "\n", "# the process always keeps the process which opens the file. ", "\n", "# If the pid is not equal to the currrent pid, we will re-open the file.", "\n", "self", ".", "pid", "=", "None", "\n", "# generate lineidx if not exist", "\n", "if", "not", "op", ".", "isfile", "(", "self", ".", "lineidx", ")", "and", "generate_lineidx", ":", "\n", "            ", "generate_lineidx", "(", "self", ".", "tsv_file", ",", "self", ".", "lineidx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.__del__": [[47, 50], ["tsv_file.TSVFile._fp.close"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close"], ["", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_fp", ":", "\n", "            ", "self", ".", "_fp", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.__str__": [[51, 53], ["None"], "methods", ["None"], ["", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\"TSVFile(tsv_file='{}')\"", ".", "format", "(", "self", ".", "tsv_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.__repr__": [[54, 56], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.num_rows": [[57, 60], ["tsv_file.TSVFile._ensure_lineidx_loaded", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_lineidx_loaded"], ["", "def", "num_rows", "(", "self", ")", ":", "\n", "        ", "self", ".", "_ensure_lineidx_loaded", "(", ")", "\n", "return", "len", "(", "self", ".", "_lineidx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.seek": [[61, 71], ["tsv_file.TSVFile._ensure_tsv_opened", "tsv_file.TSVFile._ensure_lineidx_loaded", "tsv_file.TSVFile._fp.seek", "s.strip", "logging.info", "tsv_file.TSVFile._fp.readline().split", "tsv_file.TSVFile._fp.readline"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_tsv_opened", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_lineidx_loaded", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek"], ["", "def", "seek", "(", "self", ",", "idx", ")", ":", "\n", "        ", "self", ".", "_ensure_tsv_opened", "(", ")", "\n", "self", ".", "_ensure_lineidx_loaded", "(", ")", "\n", "try", ":", "\n", "            ", "pos", "=", "self", ".", "_lineidx", "[", "idx", "]", "\n", "", "except", ":", "\n", "            ", "logging", ".", "info", "(", "'{}-{}'", ".", "format", "(", "self", ".", "tsv_file", ",", "idx", ")", ")", "\n", "raise", "\n", "", "self", ".", "_fp", ".", "seek", "(", "pos", ")", "\n", "return", "[", "s", ".", "strip", "(", ")", "for", "s", "in", "self", ".", "_fp", ".", "readline", "(", ")", ".", "split", "(", "'\\t'", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.seek_first_column": [[72, 78], ["tsv_file.TSVFile._ensure_tsv_opened", "tsv_file.TSVFile._ensure_lineidx_loaded", "tsv_file.TSVFile._fp.seek", "tsv_file.read_to_character"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_tsv_opened", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_lineidx_loaded", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.read_to_character"], ["", "def", "seek_first_column", "(", "self", ",", "idx", ")", ":", "\n", "        ", "self", ".", "_ensure_tsv_opened", "(", ")", "\n", "self", ".", "_ensure_lineidx_loaded", "(", ")", "\n", "pos", "=", "self", ".", "_lineidx", "[", "idx", "]", "\n", "self", ".", "_fp", ".", "seek", "(", "pos", ")", "\n", "return", "read_to_character", "(", "self", ".", "_fp", ",", "'\\t'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.get_key": [[79, 81], ["tsv_file.TSVFile.seek_first_column"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek_first_column"], ["", "def", "get_key", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "seek_first_column", "(", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.__getitem__": [[82, 84], ["tsv_file.TSVFile.seek"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "seek", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile.__len__": [[85, 87], ["tsv_file.TSVFile.num_rows"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.num_rows"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_rows", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile._ensure_lineidx_loaded": [[88, 94], ["open", "int", "i.strip", "fp.readlines"], "methods", ["None"], ["", "def", "_ensure_lineidx_loaded", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_lineidx", "is", "None", ":", "\n", "# print('loading lineidx: {}'.format(self.lineidx))", "\n", "\n", "            ", "with", "open", "(", "self", ".", "lineidx", ",", "'r'", ")", "as", "fp", ":", "\n", "                ", "self", ".", "_lineidx", "=", "[", "int", "(", "i", ".", "strip", "(", ")", ")", "for", "i", "in", "fp", ".", "readlines", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFile._ensure_tsv_opened": [[95, 104], ["open", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "open", "os.getpid", "os.getpid", "os.getpid", "os.getpid"], "methods", ["None"], ["", "", "", "def", "_ensure_tsv_opened", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_fp", "is", "None", ":", "\n", "            ", "self", ".", "_fp", "=", "open", "(", "self", ".", "tsv_file", ",", "'r'", ")", "\n", "self", ".", "pid", "=", "os", ".", "getpid", "(", ")", "\n", "\n", "", "if", "self", ".", "pid", "!=", "os", ".", "getpid", "(", ")", ":", "\n", "# print('re-open {} because the process id changed'.format(self.tsv_file))", "\n", "            ", "self", ".", "_fp", "=", "open", "(", "self", ".", "tsv_file", ",", "'r'", ")", "\n", "self", ".", "pid", "=", "os", ".", "getpid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.__init__": [[106, 127], ["tsv_file.generate_lineidx", "os.splitext", "os.splitext", "os.splitext", "os.splitext", "os.isfile", "os.isfile", "os.splitext", "os.splitext"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.generate_lineidx"], ["    ", "def", "__init__", "(", "self", ",", "\n", "tsv_file", ":", "str", ",", "\n", "if_generate_lineidx", ":", "bool", "=", "False", ",", "\n", "lineidx", ":", "str", "=", "None", ",", "\n", "class_selector", ":", "List", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "tsv_file", "=", "tsv_file", "\n", "self", ".", "lineidx", "=", "op", ".", "splitext", "(", "tsv_file", ")", "[", "0", "]", "+", "'.lineidx'", "if", "not", "lineidx", "else", "lineidx", "\n", "self", ".", "linelist", "=", "op", ".", "splitext", "(", "tsv_file", ")", "[", "0", "]", "+", "'.linelist'", "\n", "self", ".", "chunks", "=", "op", ".", "splitext", "(", "tsv_file", ")", "[", "0", "]", "+", "'.chunks'", "\n", "self", ".", "_fp", "=", "None", "\n", "self", ".", "_lineidx", "=", "None", "\n", "self", ".", "_sample_indices", "=", "None", "\n", "self", ".", "_class_boundaries", "=", "None", "\n", "self", ".", "_class_selector", "=", "class_selector", "\n", "# the process always keeps the process which opens the file.", "\n", "# If the pid is not equal to the currrent pid, we will re-open the file.", "\n", "self", ".", "pid", "=", "None", "\n", "# generate lineidx if not exist", "\n", "if", "not", "op", ".", "isfile", "(", "self", ".", "lineidx", ")", "and", "if_generate_lineidx", ":", "\n", "            ", "generate_lineidx", "(", "self", ".", "tsv_file", ",", "self", ".", "lineidx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.__del__": [[128, 131], ["tsv_file.TSVFileNew._fp.close"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close"], ["", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_fp", ":", "\n", "            ", "self", ".", "_fp", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.__str__": [[132, 134], ["None"], "methods", ["None"], ["", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\"TSVFile(tsv_file='{}')\"", ".", "format", "(", "self", ".", "tsv_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.__repr__": [[135, 137], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.get_class_boundaries": [[138, 140], ["None"], "methods", ["None"], ["", "def", "get_class_boundaries", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_class_boundaries", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.num_rows": [[141, 144], ["tsv_file.TSVFileNew._ensure_lineidx_loaded", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_lineidx_loaded"], ["", "def", "num_rows", "(", "self", ")", ":", "\n", "        ", "self", ".", "_ensure_lineidx_loaded", "(", ")", "\n", "return", "len", "(", "self", ".", "_sample_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek": [[145, 155], ["tsv_file.TSVFileNew._ensure_tsv_opened", "tsv_file.TSVFileNew._ensure_lineidx_loaded", "tsv_file.TSVFileNew._fp.seek", "s.strip", "logging.info", "tsv_file.TSVFileNew._fp.readline().split", "tsv_file.TSVFileNew._fp.readline"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_tsv_opened", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_lineidx_loaded", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek"], ["", "def", "seek", "(", "self", ",", "idx", ":", "int", ")", ":", "\n", "        ", "self", ".", "_ensure_tsv_opened", "(", ")", "\n", "self", ".", "_ensure_lineidx_loaded", "(", ")", "\n", "try", ":", "\n", "            ", "pos", "=", "self", ".", "_lineidx", "[", "self", ".", "_sample_indices", "[", "idx", "]", "]", "\n", "", "except", ":", "\n", "            ", "logging", ".", "info", "(", "'=> {}-{}'", ".", "format", "(", "self", ".", "tsv_file", ",", "idx", ")", ")", "\n", "raise", "\n", "", "self", ".", "_fp", ".", "seek", "(", "pos", ")", "\n", "return", "[", "s", ".", "strip", "(", ")", "for", "s", "in", "self", ".", "_fp", ".", "readline", "(", ")", ".", "split", "(", "'\\t'", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek_first_column": [[156, 162], ["tsv_file.TSVFileNew._ensure_tsv_opened", "tsv_file.TSVFileNew._ensure_lineidx_loaded", "tsv_file.TSVFileNew._fp.seek", "tsv_file.read_to_character"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_tsv_opened", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_lineidx_loaded", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.read_to_character"], ["", "def", "seek_first_column", "(", "self", ",", "idx", ":", "int", ")", ":", "\n", "        ", "self", ".", "_ensure_tsv_opened", "(", ")", "\n", "self", ".", "_ensure_lineidx_loaded", "(", ")", "\n", "pos", "=", "self", ".", "_lineidx", "[", "idx", "]", "\n", "self", ".", "_fp", ".", "seek", "(", "pos", ")", "\n", "return", "read_to_character", "(", "self", ".", "_fp", ",", "'\\t'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.get_key": [[163, 165], ["tsv_file.TSVFileNew.seek_first_column"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek_first_column"], ["", "def", "get_key", "(", "self", ",", "idx", ":", "int", ")", ":", "\n", "        ", "return", "self", ".", "seek_first_column", "(", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.__getitem__": [[166, 168], ["tsv_file.TSVFileNew.seek"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "return", "self", ".", "seek", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.__len__": [[169, 171], ["tsv_file.TSVFileNew.num_rows"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.num_rows"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_rows", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_lineidx_loaded": [[172, 215], ["os.isfile", "os.isfile", "open", "fp.readlines", "os.isfile", "os.isfile", "json.load", "json.load.items", "line.strip", "int", "open", "sorted", "open", "len", "len", "tsv_file.TSVFileNew._class_boundaries.append", "list", "range", "range", "int", "tsv_file.TSVFileNew._sample_indices.append", "len", "line.strip", "fp.readlines"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "_ensure_lineidx_loaded", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_lineidx", "is", "None", ":", "\n", "# print('=> loading lineidx: {}'.format(self.lineidx))", "\n", "            ", "with", "open", "(", "self", ".", "lineidx", ",", "'r'", ")", "as", "fp", ":", "\n", "                ", "lines", "=", "fp", ".", "readlines", "(", ")", "\n", "lines", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "lines", "]", "\n", "self", ".", "_lineidx", "=", "[", "int", "(", "line", ")", "for", "line", "in", "lines", "]", "\n", "# except:", "\n", "#     print(\"error in loading lineidx file {}, regenerate it\".format(self.lineidx))", "\n", "#     generate_lineidx(self.tsv_file, self.lineidx)", "\n", "#     with open(self.lineidx, 'r') as fp:", "\n", "#         lines = fp.readlines()", "\n", "#         lines = [line.strip() for line in lines]", "\n", "#         self._lineidx = [int(line) for line in lines]                ", "\n", "# read the line list if exists", "\n", "", "linelist", "=", "None", "\n", "if", "op", ".", "isfile", "(", "self", ".", "linelist", ")", ":", "\n", "                ", "with", "open", "(", "self", ".", "linelist", ",", "'r'", ")", "as", "fp", ":", "\n", "                    ", "linelist", "=", "sorted", "(", "\n", "[", "\n", "int", "(", "line", ".", "strip", "(", ")", ")", "\n", "for", "line", "in", "fp", ".", "readlines", "(", ")", "\n", "]", "\n", ")", "\n", "", "", "if", "op", ".", "isfile", "(", "self", ".", "chunks", ")", "and", "self", ".", "_class_selector", ":", "\n", "                ", "self", ".", "_sample_indices", "=", "[", "]", "\n", "self", ".", "_class_boundaries", "=", "[", "]", "\n", "class_boundaries", "=", "json", ".", "load", "(", "open", "(", "self", ".", "chunks", ",", "'r'", ")", ")", "\n", "for", "class_name", ",", "boundary", "in", "class_boundaries", ".", "items", "(", ")", ":", "\n", "                    ", "start", "=", "len", "(", "self", ".", "_sample_indices", ")", "\n", "if", "class_name", "in", "self", ".", "_class_selector", ":", "\n", "                        ", "for", "idx", "in", "range", "(", "boundary", "[", "0", "]", ",", "boundary", "[", "1", "]", "+", "1", ")", ":", "\n", "# NOTE: potentially slow when linelist is long, try to speed it up", "\n", "                            ", "if", "linelist", "and", "idx", "not", "in", "linelist", ":", "\n", "                                ", "continue", "\n", "", "self", ".", "_sample_indices", ".", "append", "(", "idx", ")", "\n", "", "", "end", "=", "len", "(", "self", ".", "_sample_indices", ")", "\n", "self", ".", "_class_boundaries", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "linelist", ":", "\n", "                    ", "self", ".", "_sample_indices", "=", "linelist", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_sample_indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "_lineidx", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew._ensure_tsv_opened": [[216, 225], ["open", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "os.getpid", "logging.debug", "open", "os.getpid", "os.getpid", "os.getpid", "os.getpid"], "methods", ["None"], ["", "", "", "", "def", "_ensure_tsv_opened", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_fp", "is", "None", ":", "\n", "            ", "self", ".", "_fp", "=", "open", "(", "self", ".", "tsv_file", ",", "'r'", ")", "\n", "self", ".", "pid", "=", "os", ".", "getpid", "(", ")", "\n", "\n", "", "if", "self", ".", "pid", "!=", "os", ".", "getpid", "(", ")", ":", "\n", "            ", "logging", ".", "debug", "(", "'=> re-open {} because the process id changed'", ".", "format", "(", "self", ".", "tsv_file", ")", ")", "\n", "self", ".", "_fp", "=", "open", "(", "self", ".", "tsv_file", ",", "'r'", ")", "\n", "self", ".", "pid", "=", "os", ".", "getpid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.LRU.__init__": [[231, 234], ["collections.OrderedDict.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "maxsize", "=", "4", ",", "*", "args", ",", "**", "kwds", ")", ":", "\n", "        ", "self", ".", "maxsize", "=", "maxsize", "\n", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.LRU.__getitem__": [[235, 239], ["collections.OrderedDict.__getitem__", "tsv_file.LRU.move_to_end"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.__getitem__"], ["", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "value", "=", "super", "(", ")", ".", "__getitem__", "(", "key", ")", "\n", "self", ".", "move_to_end", "(", "key", ")", "\n", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.LRU.__setitem__": [[240, 247], ["super().__setitem__", "tsv_file.LRU.move_to_end", "len", "next", "iter"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.LRU.__setitem__", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter"], ["", "def", "__setitem__", "(", "self", ",", "key", ",", "value", ")", ":", "\n", "        ", "if", "key", "in", "self", ":", "\n", "            ", "self", ".", "move_to_end", "(", "key", ")", "\n", "", "super", "(", ")", ".", "__setitem__", "(", "key", ",", "value", ")", "\n", "if", "len", "(", "self", ")", ">", "self", ".", "maxsize", ":", "\n", "            ", "oldest", "=", "next", "(", "iter", "(", "self", ")", ")", "\n", "del", "self", "[", "oldest", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.__init__": [[250, 269], ["isinstance", "tsv_file.LRU", "tsv_file.CompositeTSVFile.initialize", "tsv_file.load_list_file", "isinstance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.initialize", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.load_list_file"], ["    ", "def", "__init__", "(", "self", ",", "\n", "file_list", ":", "Union", "[", "str", ",", "list", "]", ",", "\n", "root", ":", "str", "=", "'.'", ",", "\n", "class_selector", ":", "List", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "if", "isinstance", "(", "file_list", ",", "str", ")", ":", "\n", "            ", "self", ".", "file_list", "=", "load_list_file", "(", "file_list", ")", "\n", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "file_list", ",", "list", ")", "\n", "self", ".", "file_list", "=", "file_list", "\n", "\n", "", "self", ".", "root", "=", "root", "\n", "self", ".", "cache", "=", "LRU", "(", ")", "\n", "self", ".", "tsvs", "=", "None", "\n", "self", ".", "chunk_sizes", "=", "None", "\n", "self", ".", "accum_chunk_sizes", "=", "None", "\n", "self", ".", "_class_selector", "=", "class_selector", "\n", "self", ".", "_class_boundaries", "=", "None", "\n", "self", ".", "initialized", "=", "False", "\n", "self", ".", "initialize", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.get_key": [[270, 274], ["tsv_file.CompositeTSVFile._calc_chunk_idx_row", "tsv_file.CompositeTSVFile.tsvs[].get_key"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile._calc_chunk_idx_row", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.get_key"], ["", "def", "get_key", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "idx_source", ",", "idx_row", "=", "self", ".", "_calc_chunk_idx_row", "(", "index", ")", "\n", "k", "=", "self", ".", "tsvs", "[", "idx_source", "]", ".", "get_key", "(", "idx_row", ")", "\n", "return", "'_'", ".", "join", "(", "[", "self", ".", "file_list", "[", "idx_source", "]", ",", "k", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.get_class_boundaries": [[275, 277], ["None"], "methods", ["None"], ["", "def", "get_class_boundaries", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_class_boundaries", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.get_chunk_size": [[278, 280], ["None"], "methods", ["None"], ["", "def", "get_chunk_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "chunk_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.num_rows": [[281, 283], ["sum"], "methods", ["None"], ["", "def", "num_rows", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "self", ".", "chunk_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile._calc_chunk_idx_row": [[284, 291], ["None"], "methods", ["None"], ["", "def", "_calc_chunk_idx_row", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "idx_chunk", "=", "0", "\n", "idx_row", "=", "index", "\n", "while", "index", ">=", "self", ".", "accum_chunk_sizes", "[", "idx_chunk", "]", ":", "\n", "            ", "idx_chunk", "+=", "1", "\n", "idx_row", "=", "index", "-", "self", ".", "accum_chunk_sizes", "[", "idx_chunk", "-", "1", "]", "\n", "", "return", "idx_chunk", ",", "idx_row", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.__getitem__": [[293, 301], ["tsv_file.CompositeTSVFile._calc_chunk_idx_row", "tsv_file.CompositeTSVFile.cache[].seek", "tsv_file.TSVFileNew", "os.join", "os.join"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile._calc_chunk_idx_row", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.TSVFileNew.seek"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "idx_source", ",", "idx_row", "=", "self", ".", "_calc_chunk_idx_row", "(", "index", ")", "\n", "if", "idx_source", "not", "in", "self", ".", "cache", ":", "\n", "            ", "self", ".", "cache", "[", "idx_source", "]", "=", "TSVFileNew", "(", "\n", "op", ".", "join", "(", "self", ".", "root", ",", "self", ".", "file_list", "[", "idx_source", "]", ")", ",", "\n", "class_selector", "=", "self", ".", "_class_selector", "\n", ")", "\n", "", "return", "self", ".", "cache", "[", "idx_source", "]", ".", "seek", "(", "idx_row", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.__len__": [[302, 304], ["sum"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "self", ".", "chunk_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.initialize": [[305, 344], ["logging.info", "tsv_file.TSVFileNew", "len", "all", "os.join", "os.join", "tsv.get_class_boundaries", "len", "tsv.get_class_boundaries", "tsv_file.CompositeTSVFile._class_boundaries.append"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_class_boundaries", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_class_boundaries"], ["", "def", "initialize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        this function has to be called in init function if cache_policy is\n        enabled. Thus, let's always call it in init funciton to make it simple.\n        \"\"\"", "\n", "if", "self", ".", "initialized", ":", "\n", "            ", "return", "\n", "", "tsvs", "=", "[", "\n", "TSVFileNew", "(", "\n", "op", ".", "join", "(", "self", ".", "root", ",", "f", ")", ",", "\n", "class_selector", "=", "self", ".", "_class_selector", "\n", ")", "for", "f", "in", "self", ".", "file_list", "\n", "]", "\n", "logging", ".", "info", "(", "\"Calculating chunk sizes ...\"", ")", "\n", "self", ".", "chunk_sizes", "=", "[", "len", "(", "tsv", ")", "for", "tsv", "in", "tsvs", "]", "\n", "\n", "self", ".", "accum_chunk_sizes", "=", "[", "0", "]", "\n", "for", "size", "in", "self", ".", "chunk_sizes", ":", "\n", "            ", "self", ".", "accum_chunk_sizes", "+=", "[", "self", ".", "accum_chunk_sizes", "[", "-", "1", "]", "+", "size", "]", "\n", "", "self", ".", "accum_chunk_sizes", "=", "self", ".", "accum_chunk_sizes", "[", "1", ":", "]", "\n", "\n", "if", "(", "\n", "self", ".", "_class_selector", "\n", "and", "all", "(", "[", "tsv", ".", "get_class_boundaries", "(", ")", "for", "tsv", "in", "tsvs", "]", ")", "\n", ")", ":", "\n", "            ", "\"\"\"\n            Note: When using CompositeTSVFile, make sure that the classes contained in each\n            tsv file do not overlap. Otherwise, the class boundaries won't be correct.\n            \"\"\"", "\n", "self", ".", "_class_boundaries", "=", "[", "]", "\n", "offset", "=", "0", "\n", "for", "tsv", "in", "tsvs", ":", "\n", "                ", "boundaries", "=", "tsv", ".", "get_class_boundaries", "(", ")", "\n", "for", "bound", "in", "boundaries", ":", "\n", "                    ", "self", ".", "_class_boundaries", ".", "append", "(", "(", "bound", "[", "0", "]", "+", "offset", ",", "bound", "[", "1", "]", "+", "offset", ")", ")", "\n", "", "offset", "+=", "len", "(", "tsv", ")", "\n", "# NOTE: in current setting, get_key is not used during training, so we remove tsvs for saving memory cost", "\n", "", "", "del", "tsvs", "\n", "self", ".", "initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.generate_lineidx": [[9, 19], ["os.rename", "os.rename", "open", "open", "os.fstat", "os.fstat", "tsvout.write", "tsvin.readline", "tsvin.tell", "tsvin.fileno", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["def", "generate_lineidx", "(", "filein", ",", "idxout", ")", ":", "\n", "    ", "idxout_tmp", "=", "idxout", "+", "'.tmp'", "\n", "with", "open", "(", "filein", ",", "'r'", ")", "as", "tsvin", ",", "open", "(", "idxout_tmp", ",", "'w'", ")", "as", "tsvout", ":", "\n", "        ", "fsize", "=", "os", ".", "fstat", "(", "tsvin", ".", "fileno", "(", ")", ")", ".", "st_size", "\n", "fpos", "=", "0", "\n", "while", "fpos", "!=", "fsize", ":", "\n", "            ", "tsvout", ".", "write", "(", "str", "(", "fpos", ")", "+", "\"\\n\"", ")", "\n", "tsvin", ".", "readline", "(", ")", "\n", "fpos", "=", "tsvin", ".", "tell", "(", ")", "\n", "", "", "os", ".", "rename", "(", "idxout_tmp", ",", "idxout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.read_to_character": [[21, 32], ["fp.read", "result.append", "result.append", "fp.read.index"], "function", ["None"], ["", "def", "read_to_character", "(", "fp", ",", "c", ")", ":", "\n", "    ", "result", "=", "[", "]", "\n", "while", "True", ":", "\n", "        ", "s", "=", "fp", ".", "read", "(", "32", ")", "\n", "assert", "s", "!=", "''", "\n", "if", "c", "in", "s", ":", "\n", "            ", "result", ".", "append", "(", "s", "[", ":", "s", ".", "index", "(", "c", ")", "]", ")", "\n", "break", "\n", "", "else", ":", "\n", "            ", "result", ".", "append", "(", "s", ")", "\n", "", "", "return", "''", ".", "join", "(", "result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.load_list_file": [[346, 353], ["open", "fp.readlines", "line.strip", "len"], "function", ["None"], ["", "", "def", "load_list_file", "(", "fname", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "with", "open", "(", "fname", ",", "'r'", ")", "as", "fp", ":", "\n", "        ", "lines", "=", "fp", ".", "readlines", "(", ")", "\n", "", "result", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "lines", "]", "\n", "if", "len", "(", "result", ")", ">", "0", "and", "result", "[", "-", "1", "]", "==", "''", ":", "\n", "        ", "result", "=", "result", "[", ":", "-", "1", "]", "\n", "", "return", "result", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.__init__": [[38, 48], ["kwargs.items", "instances.Instances.set"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["def", "__init__", "(", "self", ",", "image_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "**", "kwargs", ":", "Any", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            image_size (height, width): the spatial size of the image.\n            kwargs: fields to add to this `Instances`.\n        \"\"\"", "\n", "self", ".", "_image_size", "=", "image_size", "\n", "self", ".", "_fields", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "set", "(", "k", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.image_size": [[49, 56], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "image_size", "(", "self", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        Returns:\n            tuple: height, width\n        \"\"\"", "\n", "return", "self", ".", "_image_size", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.__setattr__": [[57, 62], ["name.startswith", "super().__setattr__", "instances.Instances.set"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.__setattr__", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "__setattr__", "(", "self", ",", "name", ":", "str", ",", "val", ":", "Any", ")", "->", "None", ":", "\n", "        ", "if", "name", ".", "startswith", "(", "\"_\"", ")", ":", "\n", "            ", "super", "(", ")", ".", "__setattr__", "(", "name", ",", "val", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "set", "(", "name", ",", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.__getattr__": [[63, 67], ["AttributeError"], "methods", ["None"], ["", "", "def", "__getattr__", "(", "self", ",", "name", ":", "str", ")", "->", "Any", ":", "\n", "        ", "if", "name", "==", "\"_fields\"", "or", "name", "not", "in", "self", ".", "_fields", ":", "\n", "            ", "raise", "AttributeError", "(", "\"Cannot find field '{}' in the given Instances!\"", ".", "format", "(", "name", ")", ")", "\n", "", "return", "self", ".", "_fields", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.set": [[68, 80], ["len", "len", "len", "len"], "methods", ["None"], ["", "def", "set", "(", "self", ",", "name", ":", "str", ",", "value", ":", "Any", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Set the field named `name` to `value`.\n        The length of `value` must be the number of instances,\n        and must agree with other existing fields in this object.\n        \"\"\"", "\n", "data_len", "=", "len", "(", "value", ")", "\n", "if", "len", "(", "self", ".", "_fields", ")", ":", "\n", "            ", "assert", "(", "\n", "len", "(", "self", ")", "==", "data_len", "\n", ")", ",", "\"Adding a field of length {} to a Instances of length {}\"", ".", "format", "(", "data_len", ",", "len", "(", "self", ")", ")", "\n", "", "self", ".", "_fields", "[", "name", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.has": [[81, 87], ["None"], "methods", ["None"], ["", "def", "has", "(", "self", ",", "name", ":", "str", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Returns:\n            bool: whether the field called `name` exists.\n        \"\"\"", "\n", "return", "name", "in", "self", ".", "_fields", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.remove": [[88, 93], ["None"], "methods", ["None"], ["", "def", "remove", "(", "self", ",", "name", ":", "str", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Remove the field called `name`.\n        \"\"\"", "\n", "del", "self", ".", "_fields", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.get": [[94, 99], ["None"], "methods", ["None"], ["", "def", "get", "(", "self", ",", "name", ":", "str", ")", "->", "Any", ":", "\n", "        ", "\"\"\"\n        Returns the field called `name`.\n        \"\"\"", "\n", "return", "self", ".", "_fields", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.get_fields": [[100, 108], ["None"], "methods", ["None"], ["", "def", "get_fields", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        Returns:\n            dict: a dict which maps names (str) to data of the fields\n\n        Modifying the returned dict will modify this instance.\n        \"\"\"", "\n", "return", "self", ".", "_fields", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.to": [[110, 121], ["instances.Instances", "instances.Instances._fields.items", "hasattr", "instances.Instances.set", "v.to.to.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "to", "(", "self", ",", "*", "args", ":", "Any", ",", "**", "kwargs", ":", "Any", ")", "->", "\"Instances\"", ":", "\n", "        ", "\"\"\"\n        Returns:\n            Instances: all fields are called with a `to(device)`, if the field has this method.\n        \"\"\"", "\n", "ret", "=", "Instances", "(", "self", ".", "_image_size", ")", "\n", "for", "k", ",", "v", "in", "self", ".", "_fields", ".", "items", "(", ")", ":", "\n", "            ", "if", "hasattr", "(", "v", ",", "\"to\"", ")", ":", "\n", "                ", "v", "=", "v", ".", "to", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "ret", ".", "set", "(", "k", ",", "v", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.__getitem__": [[122, 141], ["instances.Instances", "instances.Instances._fields.items", "type", "instances.Instances.set", "IndexError", "slice", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "__getitem__", "(", "self", ",", "item", ":", "Union", "[", "int", ",", "slice", ",", "torch", ".", "BoolTensor", "]", ")", "->", "\"Instances\"", ":", "\n", "        ", "\"\"\"\n        Args:\n            item: an index-like object and will be used to index all the fields.\n\n        Returns:\n            If `item` is a string, return the data in the corresponding field.\n            Otherwise, returns an `Instances` where all fields are indexed by `item`.\n        \"\"\"", "\n", "if", "type", "(", "item", ")", "==", "int", ":", "\n", "            ", "if", "item", ">=", "len", "(", "self", ")", "or", "item", "<", "-", "len", "(", "self", ")", ":", "\n", "                ", "raise", "IndexError", "(", "\"Instances index out of range!\"", ")", "\n", "", "else", ":", "\n", "                ", "item", "=", "slice", "(", "item", ",", "None", ",", "len", "(", "self", ")", ")", "\n", "\n", "", "", "ret", "=", "Instances", "(", "self", ".", "_image_size", ")", "\n", "for", "k", ",", "v", "in", "self", ".", "_fields", ".", "items", "(", ")", ":", "\n", "            ", "ret", ".", "set", "(", "k", ",", "v", "[", "item", "]", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.__len__": [[142, 147], ["instances.Instances._fields.values", "NotImplementedError", "v.__len__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.__len__"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "for", "v", "in", "self", ".", "_fields", ".", "values", "(", ")", ":", "\n", "# use __len__ because len() has to be int and is not friendly to tracing", "\n", "            ", "return", "v", ".", "__len__", "(", ")", "\n", "", "raise", "NotImplementedError", "(", "\"Empty Instances does not support __len__!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.__iter__": [[148, 150], ["NotImplementedError"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"`Instances` object is not iterable!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.cat": [[151, 182], ["all", "instances.Instances", "instance_lists[]._fields.keys", "len", "len", "isinstance", "instances.Instances.set", "isinstance", "i.get", "torch.cat", "isinstance", "list", "hasattr", "itertools.chain", "type", "type().cat", "ValueError", "type", "type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "@", "staticmethod", "\n", "def", "cat", "(", "instance_lists", ":", "List", "[", "\"Instances\"", "]", ")", "->", "\"Instances\"", ":", "\n", "        ", "\"\"\"\n        Args:\n            instance_lists (list[Instances])\n\n        Returns:\n            Instances\n        \"\"\"", "\n", "assert", "all", "(", "isinstance", "(", "i", ",", "Instances", ")", "for", "i", "in", "instance_lists", ")", "\n", "assert", "len", "(", "instance_lists", ")", ">", "0", "\n", "if", "len", "(", "instance_lists", ")", "==", "1", ":", "\n", "            ", "return", "instance_lists", "[", "0", "]", "\n", "\n", "", "image_size", "=", "instance_lists", "[", "0", "]", ".", "image_size", "\n", "for", "i", "in", "instance_lists", "[", "1", ":", "]", ":", "\n", "            ", "assert", "i", ".", "image_size", "==", "image_size", "\n", "", "ret", "=", "Instances", "(", "image_size", ")", "\n", "for", "k", "in", "instance_lists", "[", "0", "]", ".", "_fields", ".", "keys", "(", ")", ":", "\n", "            ", "values", "=", "[", "i", ".", "get", "(", "k", ")", "for", "i", "in", "instance_lists", "]", "\n", "v0", "=", "values", "[", "0", "]", "\n", "if", "isinstance", "(", "v0", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "values", "=", "torch", ".", "cat", "(", "values", ",", "dim", "=", "0", ")", "\n", "", "elif", "isinstance", "(", "v0", ",", "list", ")", ":", "\n", "                ", "values", "=", "list", "(", "itertools", ".", "chain", "(", "*", "values", ")", ")", "\n", "", "elif", "hasattr", "(", "type", "(", "v0", ")", ",", "\"cat\"", ")", ":", "\n", "                ", "values", "=", "type", "(", "v0", ")", ".", "cat", "(", "values", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unsupported type {} for concatenation\"", ".", "format", "(", "type", "(", "v0", ")", ")", ")", "\n", "", "ret", ".", "set", "(", "k", ",", "values", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.instances.Instances.__str__": [[183, 190], ["len", "instances.Instances._fields.items"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "s", "=", "self", ".", "__class__", ".", "__name__", "+", "\"(\"", "\n", "s", "+=", "\"num_instances={}, \"", ".", "format", "(", "len", "(", "self", ")", ")", "\n", "s", "+=", "\"image_height={}, \"", ".", "format", "(", "self", ".", "_image_size", "[", "0", "]", ")", "\n", "s", "+=", "\"image_width={}, \"", ".", "format", "(", "self", ".", "_image_size", "[", "1", "]", ")", "\n", "s", "+=", "\"fields=[{}])\"", ".", "format", "(", "\", \"", ".", "join", "(", "(", "f\"{k}: {v}\"", "for", "k", ",", "v", "in", "self", ".", "_fields", ".", "items", "(", ")", ")", ")", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.Keypoints.__init__": [[32, 43], ["torch.as_tensor", "isinstance", "torch.device", "torch.as_tensor.dim"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["def", "__init__", "(", "self", ",", "keypoints", ":", "Union", "[", "torch", ".", "Tensor", ",", "np", ".", "ndarray", ",", "List", "[", "List", "[", "float", "]", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Arguments:\n            keypoints: A Tensor, numpy array, or list of the x, y, and visibility of each keypoint.\n                The shape should be (N, K, 3) where N is the number of\n                instances, and K is the number of keypoints per instance.\n        \"\"\"", "\n", "device", "=", "keypoints", ".", "device", "if", "isinstance", "(", "keypoints", ",", "torch", ".", "Tensor", ")", "else", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "keypoints", "=", "torch", ".", "as_tensor", "(", "keypoints", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "assert", "keypoints", ".", "dim", "(", ")", "==", "3", "and", "keypoints", ".", "shape", "[", "2", "]", "==", "3", ",", "keypoints", ".", "shape", "\n", "self", ".", "tensor", "=", "keypoints", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.Keypoints.__len__": [[44, 46], ["keypoints.Keypoints.tensor.size"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "size", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.Keypoints.to": [[47, 49], ["type", "keypoints.Keypoints.tensor.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "to", "(", "self", ",", "*", "args", ":", "Any", ",", "**", "kwargs", ":", "Any", ")", "->", "\"Keypoints\"", ":", "\n", "        ", "return", "type", "(", "self", ")", "(", "self", ".", "tensor", ".", "to", "(", "*", "args", ",", "**", "kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.Keypoints.device": [[50, 53], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", "->", "torch", ".", "device", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.Keypoints.to_heatmap": [[54, 70], ["keypoints._keypoints_to_heatmap"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints._keypoints_to_heatmap"], ["", "def", "to_heatmap", "(", "self", ",", "boxes", ":", "torch", ".", "Tensor", ",", "heatmap_size", ":", "int", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Convert keypoint annotations to a heatmap of one-hot labels for training,\n        as described in :paper:`Mask R-CNN`.\n\n        Arguments:\n            boxes: Nx4 tensor, the boxes to draw the keypoints to\n\n        Returns:\n            heatmaps:\n                A tensor of shape (N, K), each element is integer spatial label\n                in the range [0, heatmap_size**2 - 1] for each keypoint in the input.\n            valid:\n                A tensor of shape (N, K) containing whether each keypoint is in the roi or not.\n        \"\"\"", "\n", "return", "_keypoints_to_heatmap", "(", "self", ".", "tensor", ",", "boxes", ",", "heatmap_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.Keypoints.__getitem__": [[71, 88], ["isinstance", "keypoints.Keypoints", "keypoints.Keypoints"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ":", "Union", "[", "int", ",", "slice", ",", "torch", ".", "BoolTensor", "]", ")", "->", "\"Keypoints\"", ":", "\n", "        ", "\"\"\"\n        Create a new `Keypoints` by indexing on this `Keypoints`.\n\n        The following usage are allowed:\n\n        1. `new_kpts = kpts[3]`: return a `Keypoints` which contains only one instance.\n        2. `new_kpts = kpts[2:10]`: return a slice of key points.\n        3. `new_kpts = kpts[vector]`, where vector is a torch.ByteTensor\n           with `length = len(kpts)`. Nonzero elements in the vector will be selected.\n\n        Note that the returned Keypoints might share storage with this Keypoints,\n        subject to Pytorch's indexing semantics.\n        \"\"\"", "\n", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "            ", "return", "Keypoints", "(", "[", "self", ".", "tensor", "[", "item", "]", "]", ")", "\n", "", "return", "Keypoints", "(", "self", ".", "tensor", "[", "item", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.Keypoints.__repr__": [[89, 93], ["len"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "s", "=", "self", ".", "__class__", ".", "__name__", "+", "\"(\"", "\n", "s", "+=", "\"num_instances={})\"", ".", "format", "(", "len", "(", "self", ".", "tensor", ")", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints._keypoints_to_heatmap": [[96, 153], ["x.floor().long.floor().long", "y.floor().long.floor().long", "rois.numel", "rois.new().long", "rois.new().long", "x.floor().long.floor", "y.floor().long.floor", "rois.new", "rois.new"], "function", ["None"], ["", "", "def", "_keypoints_to_heatmap", "(", "\n", "keypoints", ":", "torch", ".", "Tensor", ",", "rois", ":", "torch", ".", "Tensor", ",", "heatmap_size", ":", "int", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    Encode keypoint locations into a target heatmap for use in SoftmaxWithLoss across space.\n\n    Maps keypoints from the half-open interval [x1, x2) on continuous image coordinates to the\n    closed interval [0, heatmap_size - 1] on discrete image coordinates. We use the\n    continuous-discrete conversion from Heckbert 1990 (\"What is the coordinate of a pixel?\"):\n    d = floor(c) and c = d + 0.5, where d is a discrete coordinate and c is a continuous coordinate.\n\n    Arguments:\n        keypoints: tensor of keypoint locations in of shape (N, K, 3).\n        rois: Nx4 tensor of rois in xyxy format\n        heatmap_size: integer side length of square heatmap.\n\n    Returns:\n        heatmaps: A tensor of shape (N, K) containing an integer spatial label\n            in the range [0, heatmap_size**2 - 1] for each keypoint in the input.\n        valid: A tensor of shape (N, K) containing whether each keypoint is in\n            the roi or not.\n    \"\"\"", "\n", "\n", "if", "rois", ".", "numel", "(", ")", "==", "0", ":", "\n", "        ", "return", "rois", ".", "new", "(", ")", ".", "long", "(", ")", ",", "rois", ".", "new", "(", ")", ".", "long", "(", ")", "\n", "", "offset_x", "=", "rois", "[", ":", ",", "0", "]", "\n", "offset_y", "=", "rois", "[", ":", ",", "1", "]", "\n", "scale_x", "=", "heatmap_size", "/", "(", "rois", "[", ":", ",", "2", "]", "-", "rois", "[", ":", ",", "0", "]", ")", "\n", "scale_y", "=", "heatmap_size", "/", "(", "rois", "[", ":", ",", "3", "]", "-", "rois", "[", ":", ",", "1", "]", ")", "\n", "\n", "offset_x", "=", "offset_x", "[", ":", ",", "None", "]", "\n", "offset_y", "=", "offset_y", "[", ":", ",", "None", "]", "\n", "scale_x", "=", "scale_x", "[", ":", ",", "None", "]", "\n", "scale_y", "=", "scale_y", "[", ":", ",", "None", "]", "\n", "\n", "x", "=", "keypoints", "[", "...", ",", "0", "]", "\n", "y", "=", "keypoints", "[", "...", ",", "1", "]", "\n", "\n", "x_boundary_inds", "=", "x", "==", "rois", "[", ":", ",", "2", "]", "[", ":", ",", "None", "]", "\n", "y_boundary_inds", "=", "y", "==", "rois", "[", ":", ",", "3", "]", "[", ":", ",", "None", "]", "\n", "\n", "x", "=", "(", "x", "-", "offset_x", ")", "*", "scale_x", "\n", "x", "=", "x", ".", "floor", "(", ")", ".", "long", "(", ")", "\n", "y", "=", "(", "y", "-", "offset_y", ")", "*", "scale_y", "\n", "y", "=", "y", ".", "floor", "(", ")", ".", "long", "(", ")", "\n", "\n", "x", "[", "x_boundary_inds", "]", "=", "heatmap_size", "-", "1", "\n", "y", "[", "y_boundary_inds", "]", "=", "heatmap_size", "-", "1", "\n", "\n", "valid_loc", "=", "(", "x", ">=", "0", ")", "&", "(", "y", ">=", "0", ")", "&", "(", "x", "<", "heatmap_size", ")", "&", "(", "y", "<", "heatmap_size", ")", "\n", "vis", "=", "keypoints", "[", "...", ",", "2", "]", ">", "0", "\n", "valid", "=", "(", "valid_loc", "&", "vis", ")", ".", "long", "(", ")", "\n", "\n", "lin_ind", "=", "y", "*", "heatmap_size", "+", "x", "\n", "heatmaps", "=", "lin_ind", "*", "valid", "\n", "\n", "return", "heatmaps", ",", "valid", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.keypoints.heatmaps_to_keypoints": [[155, 231], ["maps.detach.detach", "rois.detach.detach", "widths.ceil", "heights.ceil", "maps.detach.new_zeros", "torch.arange", "range", "torch.nn.functional.interpolate().squeeze", "F.interpolate().squeeze.view().max", "max_score.view.view", "F.interpolate().squeeze.view().argmax", "int", "int", "tmp_pool_resolution.sum", "torch.nn.functional.interpolate", "F.interpolate().squeeze.view", "F.interpolate().squeeze.view", "x_int.float", "y_int.float", "roi_map_scores.view().max", "roi_map_scores.view"], "function", ["None"], ["", "@", "script_if_tracing", "\n", "def", "heatmaps_to_keypoints", "(", "maps", ":", "torch", ".", "Tensor", ",", "rois", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Extract predicted keypoint locations from heatmaps.\n\n    Args:\n        maps (Tensor): (#ROIs, #keypoints, POOL_H, POOL_W). The predicted heatmap of logits for\n            each ROI and each keypoint.\n        rois (Tensor): (#ROIs, 4). The box of each ROI.\n\n    Returns:\n        Tensor of shape (#ROIs, #keypoints, 4) with the last dimension corresponding to\n        (x, y, logit, score) for each keypoint.\n\n    When converting discrete pixel indices in an NxN image to a continuous keypoint coordinate,\n    we maintain consistency with :meth:`Keypoints.to_heatmap` by using the conversion from\n    Heckbert 1990: c = d + 0.5, where d is a discrete coordinate and c is a continuous coordinate.\n    \"\"\"", "\n", "# The decorator use of torch.no_grad() was not supported by torchscript.", "\n", "# https://github.com/pytorch/pytorch/issues/44768", "\n", "maps", "=", "maps", ".", "detach", "(", ")", "\n", "rois", "=", "rois", ".", "detach", "(", ")", "\n", "\n", "offset_x", "=", "rois", "[", ":", ",", "0", "]", "\n", "offset_y", "=", "rois", "[", ":", ",", "1", "]", "\n", "\n", "widths", "=", "(", "rois", "[", ":", ",", "2", "]", "-", "rois", "[", ":", ",", "0", "]", ")", ".", "clamp", "(", "min", "=", "1", ")", "\n", "heights", "=", "(", "rois", "[", ":", ",", "3", "]", "-", "rois", "[", ":", ",", "1", "]", ")", ".", "clamp", "(", "min", "=", "1", ")", "\n", "widths_ceil", "=", "widths", ".", "ceil", "(", ")", "\n", "heights_ceil", "=", "heights", ".", "ceil", "(", ")", "\n", "\n", "num_rois", ",", "num_keypoints", "=", "maps", ".", "shape", "[", ":", "2", "]", "\n", "xy_preds", "=", "maps", ".", "new_zeros", "(", "rois", ".", "shape", "[", "0", "]", ",", "num_keypoints", ",", "4", ")", "\n", "\n", "width_corrections", "=", "widths", "/", "widths_ceil", "\n", "height_corrections", "=", "heights", "/", "heights_ceil", "\n", "\n", "keypoints_idx", "=", "torch", ".", "arange", "(", "num_keypoints", ",", "device", "=", "maps", ".", "device", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_rois", ")", ":", "\n", "        ", "outsize", "=", "(", "int", "(", "heights_ceil", "[", "i", "]", ")", ",", "int", "(", "widths_ceil", "[", "i", "]", ")", ")", "\n", "roi_map", "=", "F", ".", "interpolate", "(", "\n", "maps", "[", "[", "i", "]", "]", ",", "size", "=", "outsize", ",", "mode", "=", "\"bicubic\"", ",", "align_corners", "=", "False", "\n", ")", ".", "squeeze", "(", "\n", "0", "\n", ")", "# #keypoints x H x W", "\n", "\n", "# softmax over the spatial region", "\n", "max_score", ",", "_", "=", "roi_map", ".", "view", "(", "num_keypoints", ",", "-", "1", ")", ".", "max", "(", "1", ")", "\n", "max_score", "=", "max_score", ".", "view", "(", "num_keypoints", ",", "1", ",", "1", ")", "\n", "tmp_full_resolution", "=", "(", "roi_map", "-", "max_score", ")", ".", "exp_", "(", ")", "\n", "tmp_pool_resolution", "=", "(", "maps", "[", "i", "]", "-", "max_score", ")", ".", "exp_", "(", ")", "\n", "# Produce scores over the region H x W, but normalize with POOL_H x POOL_W,", "\n", "# so that the scores of objects of different absolute sizes will be more comparable", "\n", "roi_map_scores", "=", "tmp_full_resolution", "/", "tmp_pool_resolution", ".", "sum", "(", "(", "1", ",", "2", ")", ",", "keepdim", "=", "True", ")", "\n", "\n", "w", "=", "roi_map", ".", "shape", "[", "2", "]", "\n", "pos", "=", "roi_map", ".", "view", "(", "num_keypoints", ",", "-", "1", ")", ".", "argmax", "(", "1", ")", "\n", "\n", "x_int", "=", "pos", "%", "w", "\n", "y_int", "=", "(", "pos", "-", "x_int", ")", "//", "w", "\n", "\n", "assert", "(", "\n", "roi_map_scores", "[", "keypoints_idx", ",", "y_int", ",", "x_int", "]", "\n", "==", "roi_map_scores", ".", "view", "(", "num_keypoints", ",", "-", "1", ")", ".", "max", "(", "1", ")", "[", "0", "]", "\n", ")", ".", "all", "(", ")", "\n", "\n", "x", "=", "(", "x_int", ".", "float", "(", ")", "+", "0.5", ")", "*", "width_corrections", "[", "i", "]", "\n", "y", "=", "(", "y_int", ".", "float", "(", ")", "+", "0.5", ")", "*", "height_corrections", "[", "i", "]", "\n", "\n", "xy_preds", "[", "i", ",", ":", ",", "0", "]", "=", "x", "+", "offset_x", "[", "i", "]", "\n", "xy_preds", "[", "i", ",", ":", ",", "1", "]", "=", "y", "+", "offset_y", "[", "i", "]", "\n", "xy_preds", "[", "i", ",", ":", ",", "2", "]", "=", "roi_map", "[", "keypoints_idx", ",", "y_int", ",", "x_int", "]", "\n", "xy_preds", "[", "i", ",", ":", ",", "3", "]", "=", "roi_map_scores", "[", "keypoints_idx", ",", "y_int", ",", "x_int", "]", "\n", "\n", "", "return", "xy_preds", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.__init__": [[95, 105], ["torch.as_tensor", "torch.as_tensor.size", "isinstance", "torch.device", "torch.as_tensor.dim"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["def", "__init__", "(", "self", ",", "tensor", ":", "Union", "[", "torch", ".", "Tensor", ",", "np", ".", "ndarray", "]", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tensor: bool Tensor of N,H,W, representing N instances in the image.\n        \"\"\"", "\n", "device", "=", "tensor", ".", "device", "if", "isinstance", "(", "tensor", ",", "torch", ".", "Tensor", ")", "else", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "tensor", "=", "torch", ".", "as_tensor", "(", "tensor", ",", "dtype", "=", "torch", ".", "bool", ",", "device", "=", "device", ")", "\n", "assert", "tensor", ".", "dim", "(", ")", "==", "3", ",", "tensor", ".", "size", "(", ")", "\n", "self", ".", "image_size", "=", "tensor", ".", "shape", "[", "1", ":", "]", "\n", "self", ".", "tensor", "=", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.to": [[106, 109], ["masks.BitMasks", "masks.BitMasks.tensor.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "to", "(", "self", ",", "*", "args", ":", "Any", ",", "**", "kwargs", ":", "Any", ")", "->", "\"BitMasks\"", ":", "\n", "        ", "return", "BitMasks", "(", "self", ".", "tensor", ".", "to", "(", "*", "args", ",", "**", "kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.device": [[110, 113], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", "->", "torch", ".", "device", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.__getitem__": [[114, 137], ["isinstance", "masks.BitMasks", "masks.BitMasks", "m.dim", "masks.BitMasks.tensor[].view"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "__getitem__", "(", "self", ",", "item", ":", "Union", "[", "int", ",", "slice", ",", "torch", ".", "BoolTensor", "]", ")", "->", "\"BitMasks\"", ":", "\n", "        ", "\"\"\"\n        Returns:\n            BitMasks: Create a new :class:`BitMasks` by indexing.\n\n        The following usage are allowed:\n\n        1. `new_masks = masks[3]`: return a `BitMasks` which contains only one mask.\n        2. `new_masks = masks[2:10]`: return a slice of masks.\n        3. `new_masks = masks[vector]`, where vector is a torch.BoolTensor\n           with `length = len(masks)`. Nonzero elements in the vector will be selected.\n\n        Note that the returned object might share storage with this object,\n        subject to Pytorch's indexing semantics.\n        \"\"\"", "\n", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "            ", "return", "BitMasks", "(", "self", ".", "tensor", "[", "item", "]", ".", "view", "(", "1", ",", "-", "1", ")", ")", "\n", "", "m", "=", "self", ".", "tensor", "[", "item", "]", "\n", "assert", "m", ".", "dim", "(", ")", "==", "3", ",", "\"Indexing on BitMasks with {} returns a tensor with shape {}!\"", ".", "format", "(", "\n", "item", ",", "m", ".", "shape", "\n", ")", "\n", "return", "BitMasks", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.__iter__": [[138, 141], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "__iter__", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "yield", "from", "self", ".", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.__repr__": [[142, 147], ["len"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "s", "=", "self", ".", "__class__", ".", "__name__", "+", "\"(\"", "\n", "s", "+=", "\"num_instances={})\"", ".", "format", "(", "len", "(", "self", ".", "tensor", ")", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.__len__": [[148, 150], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.nonempty": [[151, 160], ["masks.BitMasks.tensor.flatten().any", "masks.BitMasks.tensor.flatten"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "def", "nonempty", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Find masks that are non-empty.\n\n        Returns:\n            Tensor: a BoolTensor which represents\n                whether each mask is empty (False) or non-empty (True).\n        \"\"\"", "\n", "return", "self", ".", "tensor", ".", "flatten", "(", "1", ")", ".", "any", "(", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.from_polygon_masks": [[161, 174], ["isinstance", "masks.BitMasks", "masks.polygons_to_bitmask", "torch.stack", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.polygons_to_bitmask"], ["", "@", "staticmethod", "\n", "def", "from_polygon_masks", "(", "\n", "polygon_masks", ":", "Union", "[", "\"PolygonMasks\"", ",", "List", "[", "List", "[", "np", ".", "ndarray", "]", "]", "]", ",", "height", ":", "int", ",", "width", ":", "int", "\n", ")", "->", "\"BitMasks\"", ":", "\n", "        ", "\"\"\"\n        Args:\n            polygon_masks (list[list[ndarray]] or PolygonMasks)\n            height, width (int)\n        \"\"\"", "\n", "if", "isinstance", "(", "polygon_masks", ",", "PolygonMasks", ")", ":", "\n", "            ", "polygon_masks", "=", "polygon_masks", ".", "polygons", "\n", "", "masks", "=", "[", "polygons_to_bitmask", "(", "p", ",", "height", ",", "width", ")", "for", "p", "in", "polygon_masks", "]", "\n", "return", "BitMasks", "(", "torch", ".", "stack", "(", "[", "torch", ".", "from_numpy", "(", "x", ")", "for", "x", "in", "masks", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.from_roi_masks": [[175, 183], ["roi_masks.to_bitmasks"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.ROIMasks.to_bitmasks"], ["", "@", "staticmethod", "\n", "def", "from_roi_masks", "(", "roi_masks", ":", "\"ROIMasks\"", ",", "height", ":", "int", ",", "width", ":", "int", ")", "->", "\"BitMasks\"", ":", "\n", "        ", "\"\"\"\n        Args:\n            roi_masks:\n            height, width (int):\n        \"\"\"", "\n", "return", "roi_masks", ".", "to_bitmasks", "(", "height", ",", "width", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.crop_and_resize": [[184, 216], ["torch.cat", "masks.BitMasks.tensor.to", "rois.to.to.to", "detectron2.layers.roi_align.ROIAlign().forward().squeeze", "len", "len", "len", "len", "torch.arange().to", "detectron2.layers.roi_align.ROIAlign().forward", "torch.arange", "len", "detectron2.layers.roi_align.ROIAlign"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward"], ["", "def", "crop_and_resize", "(", "self", ",", "boxes", ":", "torch", ".", "Tensor", ",", "mask_size", ":", "int", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Crop each bitmask by the given box, and resize results to (mask_size, mask_size).\n        This can be used to prepare training targets for Mask R-CNN.\n        It has less reconstruction error compared to rasterization with polygons.\n        However we observe no difference in accuracy,\n        but BitMasks requires more memory to store all the masks.\n\n        Args:\n            boxes (Tensor): Nx4 tensor storing the boxes for each mask\n            mask_size (int): the size of the rasterized mask.\n\n        Returns:\n            Tensor:\n                A bool tensor of shape (N, mask_size, mask_size), where\n                N is the number of predicted boxes for this image.\n        \"\"\"", "\n", "assert", "len", "(", "boxes", ")", "==", "len", "(", "self", ")", ",", "\"{} != {}\"", ".", "format", "(", "len", "(", "boxes", ")", ",", "len", "(", "self", ")", ")", "\n", "device", "=", "self", ".", "tensor", ".", "device", "\n", "\n", "batch_inds", "=", "torch", ".", "arange", "(", "len", "(", "boxes", ")", ",", "device", "=", "device", ")", ".", "to", "(", "dtype", "=", "boxes", ".", "dtype", ")", "[", ":", ",", "None", "]", "\n", "rois", "=", "torch", ".", "cat", "(", "[", "batch_inds", ",", "boxes", "]", ",", "dim", "=", "1", ")", "# Nx5", "\n", "\n", "bit_masks", "=", "self", ".", "tensor", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", "\n", "rois", "=", "rois", ".", "to", "(", "device", "=", "device", ")", "\n", "output", "=", "(", "\n", "ROIAlign", "(", "(", "mask_size", ",", "mask_size", ")", ",", "1.0", ",", "0", ",", "aligned", "=", "True", ")", "\n", ".", "forward", "(", "bit_masks", "[", ":", ",", "None", ",", ":", ",", ":", "]", ",", "rois", ")", "\n", ".", "squeeze", "(", "1", ")", "\n", ")", "\n", "output", "=", "output", ">=", "0.5", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.get_bounding_boxes": [[217, 234], ["torch.zeros", "torch.any", "torch.any", "range", "torch.zeros.Boxes", "torch.where", "torch.where", "torch.as_tensor", "len", "len"], "methods", ["None"], ["", "def", "get_bounding_boxes", "(", "self", ")", "->", "Boxes", ":", "\n", "        ", "\"\"\"\n        Returns:\n            Boxes: tight bounding boxes around bitmasks.\n            If a mask is empty, it's bounding box will be all zero.\n        \"\"\"", "\n", "boxes", "=", "torch", ".", "zeros", "(", "self", ".", "tensor", ".", "shape", "[", "0", "]", ",", "4", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "x_any", "=", "torch", ".", "any", "(", "self", ".", "tensor", ",", "dim", "=", "1", ")", "\n", "y_any", "=", "torch", ".", "any", "(", "self", ".", "tensor", ",", "dim", "=", "2", ")", "\n", "for", "idx", "in", "range", "(", "self", ".", "tensor", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "x", "=", "torch", ".", "where", "(", "x_any", "[", "idx", ",", ":", "]", ")", "[", "0", "]", "\n", "y", "=", "torch", ".", "where", "(", "y_any", "[", "idx", ",", ":", "]", ")", "[", "0", "]", "\n", "if", "len", "(", "x", ")", ">", "0", "and", "len", "(", "y", ")", ">", "0", ":", "\n", "                ", "boxes", "[", "idx", ",", ":", "]", "=", "torch", ".", "as_tensor", "(", "\n", "[", "x", "[", "0", "]", ",", "y", "[", "0", "]", ",", "x", "[", "-", "1", "]", "+", "1", ",", "y", "[", "-", "1", "]", "+", "1", "]", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "", "", "return", "Boxes", "(", "boxes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.BitMasks.cat": [[235, 252], ["isinstance", "all", "len", "type", "torch.cat", "isinstance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "@", "staticmethod", "\n", "def", "cat", "(", "bitmasks_list", ":", "List", "[", "\"BitMasks\"", "]", ")", "->", "\"BitMasks\"", ":", "\n", "        ", "\"\"\"\n        Concatenates a list of BitMasks into a single BitMasks\n\n        Arguments:\n            bitmasks_list (list[BitMasks])\n\n        Returns:\n            BitMasks: the concatenated BitMasks\n        \"\"\"", "\n", "assert", "isinstance", "(", "bitmasks_list", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "len", "(", "bitmasks_list", ")", ">", "0", "\n", "assert", "all", "(", "isinstance", "(", "bitmask", ",", "BitMasks", ")", "for", "bitmask", "in", "bitmasks_list", ")", "\n", "\n", "cat_bitmasks", "=", "type", "(", "bitmasks_list", "[", "0", "]", ")", "(", "torch", ".", "cat", "(", "[", "bm", ".", "tensor", "for", "bm", "in", "bitmasks_list", "]", ",", "dim", "=", "0", ")", ")", "\n", "return", "cat_bitmasks", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.__init__": [[262, 304], ["isinstance", "ValueError", "isinstance", "numpy.asarray().astype", "masks.PolygonMasks.__init__.process_polygons"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "polygons", ":", "List", "[", "List", "[", "Union", "[", "torch", ".", "Tensor", ",", "np", ".", "ndarray", "]", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Arguments:\n            polygons (list[list[np.ndarray]]): The first\n                level of the list correspond to individual instances,\n                the second level to all the polygons that compose the\n                instance, and the third level to the polygon coordinates.\n                The third level array should have the format of\n                [x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "polygons", ",", "list", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Cannot create PolygonMasks: Expect a list of list of polygons per image. \"", "\n", "\"Got '{}' instead.\"", ".", "format", "(", "type", "(", "polygons", ")", ")", "\n", ")", "\n", "\n", "", "def", "_make_array", "(", "t", ":", "Union", "[", "torch", ".", "Tensor", ",", "np", ".", "ndarray", "]", ")", "->", "np", ".", "ndarray", ":", "\n", "# Use float64 for higher precision, because why not?", "\n", "# Always put polygons on CPU (self.to is a no-op) since they", "\n", "# are supposed to be small tensors.", "\n", "# May need to change this assumption if GPU placement becomes useful", "\n", "            ", "if", "isinstance", "(", "t", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "t", "=", "t", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "return", "np", ".", "asarray", "(", "t", ")", ".", "astype", "(", "\"float64\"", ")", "\n", "\n", "", "def", "process_polygons", "(", "\n", "polygons_per_instance", ":", "List", "[", "Union", "[", "torch", ".", "Tensor", ",", "np", ".", "ndarray", "]", "]", "\n", ")", "->", "List", "[", "np", ".", "ndarray", "]", ":", "\n", "            ", "if", "not", "isinstance", "(", "polygons_per_instance", ",", "list", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Cannot create polygons: Expect a list of polygons per instance. \"", "\n", "\"Got '{}' instead.\"", ".", "format", "(", "type", "(", "polygons_per_instance", ")", ")", "\n", ")", "\n", "# transform each polygon to a numpy array", "\n", "", "polygons_per_instance", "=", "[", "_make_array", "(", "p", ")", "for", "p", "in", "polygons_per_instance", "]", "\n", "for", "polygon", "in", "polygons_per_instance", ":", "\n", "                ", "if", "len", "(", "polygon", ")", "%", "2", "!=", "0", "or", "len", "(", "polygon", ")", "<", "6", ":", "\n", "                    ", "raise", "ValueError", "(", "f\"Cannot create a polygon from {len(polygon)} coordinates.\"", ")", "\n", "", "", "return", "polygons_per_instance", "\n", "\n", "", "self", ".", "polygons", ":", "List", "[", "List", "[", "np", ".", "ndarray", "]", "]", "=", "[", "\n", "process_polygons", "(", "polygons_per_instance", ")", "for", "polygons_per_instance", "in", "polygons", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.to": [[306, 308], ["None"], "methods", ["None"], ["", "def", "to", "(", "self", ",", "*", "args", ":", "Any", ",", "**", "kwargs", ":", "Any", ")", "->", "\"PolygonMasks\"", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.device": [[309, 312], ["torch.device"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", "->", "torch", ".", "device", ":", "\n", "        ", "return", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.get_bounding_boxes": [[313, 329], ["torch.zeros", "enumerate", "torch.zeros.Boxes", "len", "torch.as_tensor", "torch.zeros", "torch.from_numpy().view().to", "torch.min", "torch.max", "float", "float", "torch.from_numpy().view", "torch.min", "torch.max", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "get_bounding_boxes", "(", "self", ")", "->", "Boxes", ":", "\n", "        ", "\"\"\"\n        Returns:\n            Boxes: tight bounding boxes around polygon masks.\n        \"\"\"", "\n", "boxes", "=", "torch", ".", "zeros", "(", "len", "(", "self", ".", "polygons", ")", ",", "4", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "for", "idx", ",", "polygons_per_instance", "in", "enumerate", "(", "self", ".", "polygons", ")", ":", "\n", "            ", "minxy", "=", "torch", ".", "as_tensor", "(", "[", "float", "(", "\"inf\"", ")", ",", "float", "(", "\"inf\"", ")", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "maxxy", "=", "torch", ".", "zeros", "(", "2", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "for", "polygon", "in", "polygons_per_instance", ":", "\n", "                ", "coords", "=", "torch", ".", "from_numpy", "(", "polygon", ")", ".", "view", "(", "-", "1", ",", "2", ")", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", "\n", "minxy", "=", "torch", ".", "min", "(", "minxy", ",", "torch", ".", "min", "(", "coords", ",", "dim", "=", "0", ")", ".", "values", ")", "\n", "maxxy", "=", "torch", ".", "max", "(", "maxxy", ",", "torch", ".", "max", "(", "coords", ",", "dim", "=", "0", ")", ".", "values", ")", "\n", "", "boxes", "[", "idx", ",", ":", "2", "]", "=", "minxy", "\n", "boxes", "[", "idx", ",", "2", ":", "]", "=", "maxxy", "\n", "", "return", "Boxes", "(", "boxes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.nonempty": [[330, 340], ["torch.from_numpy", "numpy.asarray", "len"], "methods", ["None"], ["", "def", "nonempty", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Find masks that are non-empty.\n\n        Returns:\n            Tensor:\n                a BoolTensor which represents whether each mask is empty (False) or not (True).\n        \"\"\"", "\n", "keep", "=", "[", "1", "if", "len", "(", "polygon", ")", ">", "0", "else", "0", "for", "polygon", "in", "self", ".", "polygons", "]", "\n", "return", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "keep", ",", "dtype", "=", "np", ".", "bool", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.__getitem__": [[341, 370], ["isinstance", "masks.PolygonMasks", "isinstance", "isinstance", "isinstance", "item.cpu().numpy().tolist.cpu().numpy().tolist.nonzero().squeeze().cpu().numpy().tolist", "item.cpu().numpy().tolist.cpu().numpy().tolist.dim", "item.cpu().numpy().tolist.cpu().numpy().tolist.cpu().numpy().tolist", "ValueError", "item.cpu().numpy().tolist.cpu().numpy().tolist.nonzero().squeeze().cpu().numpy", "item.cpu().numpy().tolist.cpu().numpy().tolist.cpu().numpy", "item.cpu().numpy().tolist.cpu().numpy().tolist.nonzero().squeeze().cpu", "item.cpu().numpy().tolist.cpu().numpy().tolist.cpu", "item.cpu().numpy().tolist.cpu().numpy().tolist.nonzero().squeeze", "item.cpu().numpy().tolist.cpu().numpy().tolist.nonzero"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ":", "Union", "[", "int", ",", "slice", ",", "List", "[", "int", "]", ",", "torch", ".", "BoolTensor", "]", ")", "->", "\"PolygonMasks\"", ":", "\n", "        ", "\"\"\"\n        Support indexing over the instances and return a `PolygonMasks` object.\n        `item` can be:\n\n        1. An integer. It will return an object with only one instance.\n        2. A slice. It will return an object with the selected instances.\n        3. A list[int]. It will return an object with the selected instances,\n           correpsonding to the indices in the list.\n        4. A vector mask of type BoolTensor, whose length is num_instances.\n           It will return an object with the instances whose mask is nonzero.\n        \"\"\"", "\n", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "            ", "selected_polygons", "=", "[", "self", ".", "polygons", "[", "item", "]", "]", "\n", "", "elif", "isinstance", "(", "item", ",", "slice", ")", ":", "\n", "            ", "selected_polygons", "=", "self", ".", "polygons", "[", "item", "]", "\n", "", "elif", "isinstance", "(", "item", ",", "list", ")", ":", "\n", "            ", "selected_polygons", "=", "[", "self", ".", "polygons", "[", "i", "]", "for", "i", "in", "item", "]", "\n", "", "elif", "isinstance", "(", "item", ",", "torch", ".", "Tensor", ")", ":", "\n", "# Polygons is a list, so we have to move the indices back to CPU.", "\n", "            ", "if", "item", ".", "dtype", "==", "torch", ".", "bool", ":", "\n", "                ", "assert", "item", ".", "dim", "(", ")", "==", "1", ",", "item", ".", "shape", "\n", "item", "=", "item", ".", "nonzero", "(", ")", ".", "squeeze", "(", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "", "elif", "item", ".", "dtype", "in", "[", "torch", ".", "int32", ",", "torch", ".", "int64", "]", ":", "\n", "                ", "item", "=", "item", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unsupported tensor dtype={} for indexing!\"", ".", "format", "(", "item", ".", "dtype", ")", ")", "\n", "", "selected_polygons", "=", "[", "self", ".", "polygons", "[", "i", "]", "for", "i", "in", "item", "]", "\n", "", "return", "PolygonMasks", "(", "selected_polygons", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.__iter__": [[371, 378], ["iter"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterator", "[", "List", "[", "np", ".", "ndarray", "]", "]", ":", "\n", "        ", "\"\"\"\n        Yields:\n            list[ndarray]: the polygons for one instance.\n            Each Tensor is a float64 vector representing a polygon.\n        \"\"\"", "\n", "return", "iter", "(", "self", ".", "polygons", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.__repr__": [[379, 383], ["len"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "s", "=", "self", ".", "__class__", ".", "__name__", "+", "\"(\"", "\n", "s", "+=", "\"num_instances={})\"", ".", "format", "(", "len", "(", "self", ".", "polygons", ")", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.__len__": [[384, 386], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "polygons", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.crop_and_resize": [[387, 418], ["boxes.to.to.to", "torch.stack().to", "len", "len", "len", "len", "torch.device", "masks.rasterize_polygons_within_box", "len", "torch.empty", "box.numpy", "zip", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.rasterize_polygons_within_box"], ["", "def", "crop_and_resize", "(", "self", ",", "boxes", ":", "torch", ".", "Tensor", ",", "mask_size", ":", "int", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Crop each mask by the given box, and resize results to (mask_size, mask_size).\n        This can be used to prepare training targets for Mask R-CNN.\n\n        Args:\n            boxes (Tensor): Nx4 tensor storing the boxes for each mask\n            mask_size (int): the size of the rasterized mask.\n\n        Returns:\n            Tensor: A bool tensor of shape (N, mask_size, mask_size), where\n            N is the number of predicted boxes for this image.\n        \"\"\"", "\n", "assert", "len", "(", "boxes", ")", "==", "len", "(", "self", ")", ",", "\"{} != {}\"", ".", "format", "(", "len", "(", "boxes", ")", ",", "len", "(", "self", ")", ")", "\n", "\n", "device", "=", "boxes", ".", "device", "\n", "# Put boxes on the CPU, as the polygon representation is not efficient GPU-wise", "\n", "# (several small tensors for representing a single instance mask)", "\n", "boxes", "=", "boxes", ".", "to", "(", "torch", ".", "device", "(", "\"cpu\"", ")", ")", "\n", "\n", "results", "=", "[", "\n", "rasterize_polygons_within_box", "(", "poly", ",", "box", ".", "numpy", "(", ")", ",", "mask_size", ")", "\n", "for", "poly", ",", "box", "in", "zip", "(", "self", ".", "polygons", ",", "boxes", ")", "\n", "]", "\n", "\"\"\"\n        poly: list[list[float]], the polygons for one instance\n        box: a tensor of shape (4,)\n        \"\"\"", "\n", "if", "len", "(", "results", ")", "==", "0", ":", "\n", "            ", "return", "torch", ".", "empty", "(", "0", ",", "mask_size", ",", "mask_size", ",", "dtype", "=", "torch", ".", "bool", ",", "device", "=", "device", ")", "\n", "", "return", "torch", ".", "stack", "(", "results", ",", "dim", "=", "0", ")", ".", "to", "(", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.area": [[419, 437], ["torch.tensor", "area.append", "masks.polygon_area"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.polygon_area"], ["", "def", "area", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Computes area of the mask.\n        Only works with Polygons, using the shoelace formula:\n        https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates\n\n        Returns:\n            Tensor: a vector, area for each instance\n        \"\"\"", "\n", "\n", "area", "=", "[", "]", "\n", "for", "polygons_per_instance", "in", "self", ".", "polygons", ":", "\n", "            ", "area_per_instance", "=", "0", "\n", "for", "p", "in", "polygons_per_instance", ":", "\n", "                ", "area_per_instance", "+=", "polygon_area", "(", "p", "[", "0", ":", ":", "2", "]", ",", "p", "[", "1", ":", ":", "2", "]", ")", "\n", "", "area", ".", "append", "(", "area_per_instance", ")", "\n", "\n", "", "return", "torch", ".", "tensor", "(", "area", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.cat": [[438, 457], ["isinstance", "all", "len", "type", "list", "isinstance", "itertools.chain.from_iterable"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "@", "staticmethod", "\n", "def", "cat", "(", "polymasks_list", ":", "List", "[", "\"PolygonMasks\"", "]", ")", "->", "\"PolygonMasks\"", ":", "\n", "        ", "\"\"\"\n        Concatenates a list of PolygonMasks into a single PolygonMasks\n\n        Arguments:\n            polymasks_list (list[PolygonMasks])\n\n        Returns:\n            PolygonMasks: the concatenated PolygonMasks\n        \"\"\"", "\n", "assert", "isinstance", "(", "polymasks_list", ",", "(", "list", ",", "tuple", ")", ")", "\n", "assert", "len", "(", "polymasks_list", ")", ">", "0", "\n", "assert", "all", "(", "isinstance", "(", "polymask", ",", "PolygonMasks", ")", "for", "polymask", "in", "polymasks_list", ")", "\n", "\n", "cat_polymasks", "=", "type", "(", "polymasks_list", "[", "0", "]", ")", "(", "\n", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "pm", ".", "polygons", "for", "pm", "in", "polymasks_list", ")", ")", "\n", ")", "\n", "return", "cat_polymasks", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.ROIMasks.__init__": [[466, 474], ["tensor.dim", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tensor: (N, M, M) mask tensor that defines the mask within each ROI.\n        \"\"\"", "\n", "if", "tensor", ".", "dim", "(", ")", "!=", "3", ":", "\n", "            ", "raise", "ValueError", "(", "\"ROIMasks must take a masks of 3 dimension.\"", ")", "\n", "", "self", ".", "tensor", "=", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.ROIMasks.to": [[475, 477], ["masks.ROIMasks", "masks.ROIMasks.tensor.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "to", "(", "self", ",", "device", ":", "torch", ".", "device", ")", "->", "\"ROIMasks\"", ":", "\n", "        ", "return", "ROIMasks", "(", "self", ".", "tensor", ".", "to", "(", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.ROIMasks.device": [[478, 481], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", "->", "device", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.ROIMasks.__len__": [[482, 484], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.ROIMasks.__getitem__": [[485, 505], ["masks.ROIMasks", "t.dim", "ValueError"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", "->", "\"ROIMasks\"", ":", "\n", "        ", "\"\"\"\n        Returns:\n            ROIMasks: Create a new :class:`ROIMasks` by indexing.\n\n        The following usage are allowed:\n\n        1. `new_masks = masks[2:10]`: return a slice of masks.\n        2. `new_masks = masks[vector]`, where vector is a torch.BoolTensor\n           with `length = len(masks)`. Nonzero elements in the vector will be selected.\n\n        Note that the returned object might share storage with this object,\n        subject to Pytorch's indexing semantics.\n        \"\"\"", "\n", "t", "=", "self", ".", "tensor", "[", "item", "]", "\n", "if", "t", ".", "dim", "(", ")", "!=", "3", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Indexing on ROIMasks with {item} returns a tensor with shape {t.shape}!\"", "\n", ")", "\n", "", "return", "ROIMasks", "(", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.ROIMasks.__repr__": [[506, 511], ["len"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "s", "=", "self", ".", "__class__", ".", "__name__", "+", "\"(\"", "\n", "s", "+=", "\"num_instances={})\"", ".", "format", "(", "len", "(", "self", ".", "tensor", ")", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.ROIMasks.to_bitmasks": [[512, 528], ["detectron2.utils.memory.retry_if_cuda_oom", "detectron2.utils.memory.retry_if_cuda_oom.", "masks.BitMasks"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.memory.retry_if_cuda_oom"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "to_bitmasks", "(", "self", ",", "boxes", ":", "torch", ".", "Tensor", ",", "height", ",", "width", ",", "threshold", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n\n        \"\"\"", "\n", "from", "detectron2", ".", "layers", "import", "paste_masks_in_image", "\n", "\n", "paste", "=", "retry_if_cuda_oom", "(", "paste_masks_in_image", ")", "\n", "bitmasks", "=", "paste", "(", "\n", "self", ".", "tensor", ",", "\n", "boxes", ",", "\n", "(", "height", ",", "width", ")", ",", "\n", "threshold", "=", "threshold", ",", "\n", ")", "\n", "return", "BitMasks", "(", "bitmasks", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.polygon_area": [[16, 20], ["numpy.abs", "numpy.dot", "numpy.dot", "numpy.roll", "numpy.roll"], "function", ["None"], ["def", "polygon_area", "(", "x", ",", "y", ")", ":", "\n", "# Using the shoelace formula", "\n", "# https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates", "\n", "    ", "return", "0.5", "*", "np", ".", "abs", "(", "np", ".", "dot", "(", "x", ",", "np", ".", "roll", "(", "y", ",", "1", ")", ")", "-", "np", ".", "dot", "(", "y", ",", "np", ".", "roll", "(", "x", ",", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.polygons_to_bitmask": [[22, 35], ["pycocotools.frPyObjects", "pycocotools.merge", "pycocotools.decode().astype", "len", "pycocotools.decode"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode"], ["", "def", "polygons_to_bitmask", "(", "polygons", ":", "List", "[", "np", ".", "ndarray", "]", ",", "height", ":", "int", ",", "width", ":", "int", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"\n    Args:\n        polygons (list[ndarray]): each array has shape (Nx2,)\n        height, width (int)\n\n    Returns:\n        ndarray: a bool mask of shape (height, width)\n    \"\"\"", "\n", "assert", "len", "(", "polygons", ")", ">", "0", ",", "\"COCOAPI does not support empty polygons\"", "\n", "rles", "=", "mask_util", ".", "frPyObjects", "(", "polygons", ",", "height", ",", "width", ")", "\n", "rle", "=", "mask_util", ".", "merge", "(", "rles", ")", "\n", "return", "mask_util", ".", "decode", "(", "rle", ")", ".", "astype", "(", "np", ".", "bool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.rasterize_polygons_within_box": [[37, 84], ["copy.deepcopy", "masks.polygons_to_bitmask", "torch.from_numpy", "max", "max"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.polygons_to_bitmask"], ["", "def", "rasterize_polygons_within_box", "(", "\n", "polygons", ":", "List", "[", "np", ".", "ndarray", "]", ",", "box", ":", "np", ".", "ndarray", ",", "mask_size", ":", "int", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Rasterize the polygons into a mask image and\n    crop the mask content in the given box.\n    The cropped mask is resized to (mask_size, mask_size).\n\n    This function is used when generating training targets for mask head in Mask R-CNN.\n    Given original ground-truth masks for an image, new ground-truth mask\n    training targets in the size of `mask_size x mask_size`\n    must be provided for each predicted box. This function will be called to\n    produce such targets.\n\n    Args:\n        polygons (list[ndarray[float]]): a list of polygons, which represents an instance.\n        box: 4-element numpy array\n        mask_size (int):\n\n    Returns:\n        Tensor: BoolTensor of shape (mask_size, mask_size)\n    \"\"\"", "\n", "# 1. Shift the polygons w.r.t the boxes", "\n", "w", ",", "h", "=", "box", "[", "2", "]", "-", "box", "[", "0", "]", ",", "box", "[", "3", "]", "-", "box", "[", "1", "]", "\n", "\n", "polygons", "=", "copy", ".", "deepcopy", "(", "polygons", ")", "\n", "for", "p", "in", "polygons", ":", "\n", "        ", "p", "[", "0", ":", ":", "2", "]", "=", "p", "[", "0", ":", ":", "2", "]", "-", "box", "[", "0", "]", "\n", "p", "[", "1", ":", ":", "2", "]", "=", "p", "[", "1", ":", ":", "2", "]", "-", "box", "[", "1", "]", "\n", "\n", "# 2. Rescale the polygons to the new box size", "\n", "# max() to avoid division by small number", "\n", "", "ratio_h", "=", "mask_size", "/", "max", "(", "h", ",", "0.1", ")", "\n", "ratio_w", "=", "mask_size", "/", "max", "(", "w", ",", "0.1", ")", "\n", "\n", "if", "ratio_h", "==", "ratio_w", ":", "\n", "        ", "for", "p", "in", "polygons", ":", "\n", "            ", "p", "*=", "ratio_h", "\n", "", "", "else", ":", "\n", "        ", "for", "p", "in", "polygons", ":", "\n", "            ", "p", "[", "0", ":", ":", "2", "]", "*=", "ratio_w", "\n", "p", "[", "1", ":", ":", "2", "]", "*=", "ratio_h", "\n", "\n", "# 3. Rasterize the polygons with coco api", "\n", "", "", "mask", "=", "polygons_to_bitmask", "(", "polygons", ",", "mask_size", ",", "mask_size", ")", "\n", "mask", "=", "torch", ".", "from_numpy", "(", "mask", ")", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert": [[53, 138], ["type", "isinstance", "isinstance", "torch.cat().to.double", "torch.abs", "torch.abs", "arr[].to", "type.", "torch.cat().to.numpy", "torch.tensor", "torch.from_numpy().clone", "box.clone", "torch.cos", "torch.sin", "torch.cat().to.double", "torch.zeros", "torch.cat().to", "torch.cat().to.flatten().tolist", "len", "len", "torch.from_numpy", "torch.cat", "NotImplementedError", "torch.cat().to.flatten", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["@", "staticmethod", "\n", "def", "convert", "(", "box", ":", "_RawBoxType", ",", "from_mode", ":", "\"BoxMode\"", ",", "to_mode", ":", "\"BoxMode\"", ")", "->", "_RawBoxType", ":", "\n", "        ", "\"\"\"\n        Args:\n            box: can be a k-tuple, k-list or an Nxk array/tensor, where k = 4 or 5\n            from_mode, to_mode (BoxMode)\n\n        Returns:\n            The converted box of the same type.\n        \"\"\"", "\n", "if", "from_mode", "==", "to_mode", ":", "\n", "            ", "return", "box", "\n", "\n", "", "original_type", "=", "type", "(", "box", ")", "\n", "is_numpy", "=", "isinstance", "(", "box", ",", "np", ".", "ndarray", ")", "\n", "single_box", "=", "isinstance", "(", "box", ",", "(", "list", ",", "tuple", ")", ")", "\n", "if", "single_box", ":", "\n", "            ", "assert", "len", "(", "box", ")", "==", "4", "or", "len", "(", "box", ")", "==", "5", ",", "(", "\n", "\"BoxMode.convert takes either a k-tuple/list or an Nxk array/tensor,\"", "\n", "\" where k == 4 or 5\"", "\n", ")", "\n", "arr", "=", "torch", ".", "tensor", "(", "box", ")", "[", "None", ",", ":", "]", "\n", "", "else", ":", "\n", "# avoid modifying the input box", "\n", "            ", "if", "is_numpy", ":", "\n", "                ", "arr", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "box", ")", ")", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "                ", "arr", "=", "box", ".", "clone", "(", ")", "\n", "\n", "", "", "assert", "to_mode", "not", "in", "[", "BoxMode", ".", "XYXY_REL", ",", "BoxMode", ".", "XYWH_REL", "]", "and", "from_mode", "not", "in", "[", "\n", "BoxMode", ".", "XYXY_REL", ",", "\n", "BoxMode", ".", "XYWH_REL", ",", "\n", "]", ",", "\"Relative mode not yet supported!\"", "\n", "\n", "if", "from_mode", "==", "BoxMode", ".", "XYWHA_ABS", "and", "to_mode", "==", "BoxMode", ".", "XYXY_ABS", ":", "\n", "            ", "assert", "(", "\n", "arr", ".", "shape", "[", "-", "1", "]", "==", "5", "\n", ")", ",", "\"The last dimension of input shape must be 5 for XYWHA format\"", "\n", "original_dtype", "=", "arr", ".", "dtype", "\n", "arr", "=", "arr", ".", "double", "(", ")", "\n", "\n", "w", "=", "arr", "[", ":", ",", "2", "]", "\n", "h", "=", "arr", "[", ":", ",", "3", "]", "\n", "a", "=", "arr", "[", ":", ",", "4", "]", "\n", "c", "=", "torch", ".", "abs", "(", "torch", ".", "cos", "(", "a", "*", "math", ".", "pi", "/", "180.0", ")", ")", "\n", "s", "=", "torch", ".", "abs", "(", "torch", ".", "sin", "(", "a", "*", "math", ".", "pi", "/", "180.0", ")", ")", "\n", "# This basically computes the horizontal bounding rectangle of the rotated box", "\n", "new_w", "=", "c", "*", "w", "+", "s", "*", "h", "\n", "new_h", "=", "c", "*", "h", "+", "s", "*", "w", "\n", "\n", "# convert center to top-left corner", "\n", "arr", "[", ":", ",", "0", "]", "-=", "new_w", "/", "2.0", "\n", "arr", "[", ":", ",", "1", "]", "-=", "new_h", "/", "2.0", "\n", "# bottom-right corner", "\n", "arr", "[", ":", ",", "2", "]", "=", "arr", "[", ":", ",", "0", "]", "+", "new_w", "\n", "arr", "[", ":", ",", "3", "]", "=", "arr", "[", ":", ",", "1", "]", "+", "new_h", "\n", "\n", "arr", "=", "arr", "[", ":", ",", ":", "4", "]", ".", "to", "(", "dtype", "=", "original_dtype", ")", "\n", "", "elif", "from_mode", "==", "BoxMode", ".", "XYWH_ABS", "and", "to_mode", "==", "BoxMode", ".", "XYWHA_ABS", ":", "\n", "            ", "original_dtype", "=", "arr", ".", "dtype", "\n", "arr", "=", "arr", ".", "double", "(", ")", "\n", "arr", "[", ":", ",", "0", "]", "+=", "arr", "[", ":", ",", "2", "]", "/", "2.0", "\n", "arr", "[", ":", ",", "1", "]", "+=", "arr", "[", ":", ",", "3", "]", "/", "2.0", "\n", "angles", "=", "torch", ".", "zeros", "(", "(", "arr", ".", "shape", "[", "0", "]", ",", "1", ")", ",", "dtype", "=", "arr", ".", "dtype", ")", "\n", "arr", "=", "torch", ".", "cat", "(", "(", "arr", ",", "angles", ")", ",", "axis", "=", "1", ")", ".", "to", "(", "dtype", "=", "original_dtype", ")", "\n", "", "else", ":", "\n", "            ", "if", "to_mode", "==", "BoxMode", ".", "XYXY_ABS", "and", "from_mode", "==", "BoxMode", ".", "XYWH_ABS", ":", "\n", "                ", "arr", "[", ":", ",", "2", "]", "+=", "arr", "[", ":", ",", "0", "]", "\n", "arr", "[", ":", ",", "3", "]", "+=", "arr", "[", ":", ",", "1", "]", "\n", "", "elif", "from_mode", "==", "BoxMode", ".", "XYXY_ABS", "and", "to_mode", "==", "BoxMode", ".", "XYWH_ABS", ":", "\n", "                ", "arr", "[", ":", ",", "2", "]", "-=", "arr", "[", ":", ",", "0", "]", "\n", "arr", "[", ":", ",", "3", "]", "-=", "arr", "[", ":", ",", "1", "]", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\n", "\"Conversion from BoxMode {} to {} is not supported yet\"", ".", "format", "(", "\n", "from_mode", ",", "to_mode", "\n", ")", "\n", ")", "\n", "\n", "", "", "if", "single_box", ":", "\n", "            ", "return", "original_type", "(", "arr", ".", "flatten", "(", ")", ".", "tolist", "(", ")", ")", "\n", "", "if", "is_numpy", ":", "\n", "            ", "return", "arr", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "arr", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.__init__": [[152, 166], ["torch.as_tensor", "tensor.reshape().to.reshape().to.size", "isinstance", "torch.device", "tensor.reshape().to.reshape().to.numel", "tensor.reshape().to.reshape().to.reshape().to", "tensor.reshape().to.reshape().to.dim", "tensor.reshape().to.reshape().to.size", "tensor.reshape().to.reshape().to.reshape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["def", "__init__", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tensor (Tensor[float]): a Nx4 matrix.  Each row is (x1, y1, x2, y2).\n        \"\"\"", "\n", "device", "=", "tensor", ".", "device", "if", "isinstance", "(", "tensor", ",", "torch", ".", "Tensor", ")", "else", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "tensor", "=", "torch", ".", "as_tensor", "(", "tensor", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "if", "tensor", ".", "numel", "(", ")", "==", "0", ":", "\n", "# Use reshape, so we don't end up creating a new tensor that does not depend on", "\n", "# the inputs (and consequently confuses jit)", "\n", "            ", "tensor", "=", "tensor", ".", "reshape", "(", "(", "-", "1", ",", "4", ")", ")", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "", "assert", "tensor", ".", "dim", "(", ")", "==", "2", "and", "tensor", ".", "size", "(", "-", "1", ")", "==", "4", ",", "tensor", ".", "size", "(", ")", "\n", "\n", "self", ".", "tensor", "=", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone": [[167, 175], ["boxes.Boxes", "boxes.Boxes.tensor.clone"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["", "def", "clone", "(", "self", ")", "->", "\"Boxes\"", ":", "\n", "        ", "\"\"\"\n        Clone the Boxes.\n\n        Returns:\n            Boxes\n        \"\"\"", "\n", "return", "Boxes", "(", "self", ".", "tensor", ".", "clone", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to": [[176, 180], ["boxes.Boxes", "boxes.Boxes.tensor.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "@", "_maybe_jit_unused", "\n", "def", "to", "(", "self", ",", "device", ":", "torch", ".", "device", ")", ":", "\n", "# Boxes are assumed float32 and does not support to(dtype)", "\n", "        ", "return", "Boxes", "(", "self", ".", "tensor", ".", "to", "(", "device", "=", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area": [[181, 191], ["None"], "methods", ["None"], ["", "def", "area", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes the area of all the boxes.\n\n        Returns:\n            torch.Tensor: a vector with areas of each box.\n        \"\"\"", "\n", "box", "=", "self", ".", "tensor", "\n", "area", "=", "(", "box", "[", ":", ",", "2", "]", "-", "box", "[", ":", ",", "0", "]", ")", "*", "(", "box", "[", ":", ",", "3", "]", "-", "box", "[", ":", ",", "1", "]", ")", "\n", "return", "area", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip": [[192, 207], ["torch.isfinite().all", "boxes.Boxes.tensor[].clamp", "boxes.Boxes.tensor[].clamp", "boxes.Boxes.tensor[].clamp", "boxes.Boxes.tensor[].clamp", "torch.stack", "torch.isfinite"], "methods", ["None"], ["", "def", "clip", "(", "self", ",", "box_size", ":", "Tuple", "[", "int", ",", "int", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Clip (in place) the boxes by limiting x coordinates to the range [0, width]\n        and y coordinates to the range [0, height].\n\n        Args:\n            box_size (height, width): The clipping box's size.\n        \"\"\"", "\n", "assert", "torch", ".", "isfinite", "(", "self", ".", "tensor", ")", ".", "all", "(", ")", ",", "\"Box tensor contains infinite or NaN!\"", "\n", "h", ",", "w", "=", "box_size", "\n", "x1", "=", "self", ".", "tensor", "[", ":", ",", "0", "]", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "w", ")", "\n", "y1", "=", "self", ".", "tensor", "[", ":", ",", "1", "]", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "h", ")", "\n", "x2", "=", "self", ".", "tensor", "[", ":", ",", "2", "]", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "w", ")", "\n", "y2", "=", "self", ".", "tensor", "[", ":", ",", "3", "]", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "h", ")", "\n", "self", ".", "tensor", "=", "torch", ".", "stack", "(", "(", "x1", ",", "y1", ",", "x2", ",", "y2", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.nonempty": [[208, 223], ["None"], "methods", ["None"], ["", "def", "nonempty", "(", "self", ",", "threshold", ":", "float", "=", "0.0", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Find boxes that are non-empty.\n        A box is considered empty, if either of its side is no larger than threshold.\n\n        Returns:\n            Tensor:\n                a binary vector which represents whether each box is empty\n                (False) or non-empty (True).\n        \"\"\"", "\n", "box", "=", "self", ".", "tensor", "\n", "widths", "=", "box", "[", ":", ",", "2", "]", "-", "box", "[", ":", ",", "0", "]", "\n", "heights", "=", "box", "[", ":", ",", "3", "]", "-", "box", "[", ":", ",", "1", "]", "\n", "keep", "=", "(", "widths", ">", "threshold", ")", "&", "(", "heights", ">", "threshold", ")", "\n", "return", "keep", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.__getitem__": [[224, 247], ["isinstance", "boxes.Boxes", "boxes.Boxes", "b.dim", "boxes.Boxes.tensor[].view"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", "->", "\"Boxes\"", ":", "\n", "        ", "\"\"\"\n        Args:\n            item: int, slice, or a BoolTensor\n\n        Returns:\n            Boxes: Create a new :class:`Boxes` by indexing.\n\n        The following usage are allowed:\n\n        1. `new_boxes = boxes[3]`: return a `Boxes` which contains only one box.\n        2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n        3. `new_boxes = boxes[vector]`, where vector is a torch.BoolTensor\n           with `length = len(boxes)`. Nonzero elements in the vector will be selected.\n\n        Note that the returned Boxes might share storage with this Boxes,\n        subject to Pytorch's indexing semantics.\n        \"\"\"", "\n", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "            ", "return", "Boxes", "(", "self", ".", "tensor", "[", "item", "]", ".", "view", "(", "1", ",", "-", "1", ")", ")", "\n", "", "b", "=", "self", ".", "tensor", "[", "item", "]", "\n", "assert", "b", ".", "dim", "(", ")", "==", "2", ",", "\"Indexing on Boxes with {} failed to return a matrix!\"", ".", "format", "(", "item", ")", "\n", "return", "Boxes", "(", "b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.__len__": [[248, 250], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.__repr__": [[251, 253], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "\"Boxes(\"", "+", "str", "(", "self", ".", "tensor", ")", "+", "\")\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.inside_box": [[254, 272], ["None"], "methods", ["None"], ["", "def", "inside_box", "(", "self", ",", "box_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "boundary_threshold", ":", "int", "=", "0", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Args:\n            box_size (height, width): Size of the reference box.\n            boundary_threshold (int): Boxes that extend beyond the reference box\n                boundary by more than boundary_threshold are considered \"outside\".\n\n        Returns:\n            a binary vector, indicating whether each box is inside the reference box.\n        \"\"\"", "\n", "height", ",", "width", "=", "box_size", "\n", "inds_inside", "=", "(", "\n", "(", "self", ".", "tensor", "[", "...", ",", "0", "]", ">=", "-", "boundary_threshold", ")", "\n", "&", "(", "self", ".", "tensor", "[", "...", ",", "1", "]", ">=", "-", "boundary_threshold", ")", "\n", "&", "(", "self", ".", "tensor", "[", "...", ",", "2", "]", "<", "width", "+", "boundary_threshold", ")", "\n", "&", "(", "self", ".", "tensor", "[", "...", ",", "3", "]", "<", "height", "+", "boundary_threshold", ")", "\n", ")", "\n", "return", "inds_inside", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.get_centers": [[273, 279], ["None"], "methods", ["None"], ["", "def", "get_centers", "(", "self", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Returns:\n            The box centers in a Nx2 array of (x, y).\n        \"\"\"", "\n", "return", "(", "self", ".", "tensor", "[", ":", ",", ":", "2", "]", "+", "self", ".", "tensor", "[", ":", ",", "2", ":", "]", ")", "/", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.scale": [[280, 286], ["None"], "methods", ["None"], ["", "def", "scale", "(", "self", ",", "scale_x", ":", "float", ",", "scale_y", ":", "float", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Scale the box with horizontal and vertical scaling factors\n        \"\"\"", "\n", "self", ".", "tensor", "[", ":", ",", "0", ":", ":", "2", "]", "*=", "scale_x", "\n", "self", ".", "tensor", "[", ":", ",", "1", ":", ":", "2", "]", "*=", "scale_y", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.cat": [[287, 307], ["isinstance", "all", "cls", "len", "cls", "torch.cat", "torch.empty", "isinstance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "@", "classmethod", "\n", "@", "_maybe_jit_unused", "\n", "def", "cat", "(", "cls", ",", "boxes_list", ":", "List", "[", "\"Boxes\"", "]", ")", "->", "\"Boxes\"", ":", "\n", "        ", "\"\"\"\n        Concatenates a list of Boxes into a single Boxes\n\n        Arguments:\n            boxes_list (list[Boxes])\n\n        Returns:\n            Boxes: the concatenated Boxes\n        \"\"\"", "\n", "assert", "isinstance", "(", "boxes_list", ",", "(", "list", ",", "tuple", ")", ")", "\n", "if", "len", "(", "boxes_list", ")", "==", "0", ":", "\n", "            ", "return", "cls", "(", "torch", ".", "empty", "(", "0", ")", ")", "\n", "", "assert", "all", "(", "[", "isinstance", "(", "box", ",", "Boxes", ")", "for", "box", "in", "boxes_list", "]", ")", "\n", "\n", "# use torch.cat (v.s. layers.cat) so the returned boxes never share storage with input", "\n", "cat_boxes", "=", "cls", "(", "torch", ".", "cat", "(", "[", "b", ".", "tensor", "for", "b", "in", "boxes_list", "]", ",", "dim", "=", "0", ")", ")", "\n", "return", "cat_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.device": [[308, 311], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", "->", "device", ":", "\n", "        ", "return", "self", ".", "tensor", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.__iter__": [[314, 320], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Yield a box as a Tensor of shape (4,) at a time.\n        \"\"\"", "\n", "yield", "from", "self", ".", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_intersection": [[322, 342], ["width_height.clamp_", "width_height.prod", "torch.min", "torch.max"], "function", ["None"], ["", "", "def", "pairwise_intersection", "(", "boxes1", ":", "Boxes", ",", "boxes2", ":", "Boxes", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Given two lists of boxes of size N and M,\n    compute the intersection area between __all__ N x M pairs of boxes.\n    The box order must be (xmin, ymin, xmax, ymax)\n\n    Args:\n        boxes1,boxes2 (Boxes): two `Boxes`. Contains N & M boxes, respectively.\n\n    Returns:\n        Tensor: intersection, sized [N,M].\n    \"\"\"", "\n", "boxes1", ",", "boxes2", "=", "boxes1", ".", "tensor", ",", "boxes2", ".", "tensor", "\n", "width_height", "=", "torch", ".", "min", "(", "boxes1", "[", ":", ",", "None", ",", "2", ":", "]", ",", "boxes2", "[", ":", ",", "2", ":", "]", ")", "-", "torch", ".", "max", "(", "\n", "boxes1", "[", ":", ",", "None", ",", ":", "2", "]", ",", "boxes2", "[", ":", ",", ":", "2", "]", "\n", ")", "# [N,M,2]", "\n", "\n", "width_height", ".", "clamp_", "(", "min", "=", "0", ")", "# [N,M,2]", "\n", "intersection", "=", "width_height", ".", "prod", "(", "dim", "=", "2", ")", "# [N,M]", "\n", "return", "intersection", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou": [[346, 369], ["boxes1.area", "boxes2.area", "boxes.pairwise_intersection", "torch.where", "torch.zeros"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_intersection"], ["", "def", "pairwise_iou", "(", "boxes1", ":", "Boxes", ",", "boxes2", ":", "Boxes", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Given two lists of boxes of size N and M, compute the IoU\n    (intersection over union) between **all** N x M pairs of boxes.\n    The box order must be (xmin, ymin, xmax, ymax).\n\n    Args:\n        boxes1,boxes2 (Boxes): two `Boxes`. Contains N & M boxes, respectively.\n\n    Returns:\n        Tensor: IoU, sized [N,M].\n    \"\"\"", "\n", "area1", "=", "boxes1", ".", "area", "(", ")", "# [N]", "\n", "area2", "=", "boxes2", ".", "area", "(", ")", "# [M]", "\n", "inter", "=", "pairwise_intersection", "(", "boxes1", ",", "boxes2", ")", "\n", "\n", "# handle empty boxes", "\n", "iou", "=", "torch", ".", "where", "(", "\n", "inter", ">", "0", ",", "\n", "inter", "/", "(", "area1", "[", ":", ",", "None", "]", "+", "area2", "-", "inter", ")", ",", "\n", "torch", ".", "zeros", "(", "1", ",", "dtype", "=", "inter", ".", "dtype", ",", "device", "=", "inter", ".", "device", ")", ",", "\n", ")", "\n", "return", "iou", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_ioa": [[371, 389], ["boxes2.area", "boxes.pairwise_intersection", "torch.where", "torch.zeros"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_intersection"], ["", "def", "pairwise_ioa", "(", "boxes1", ":", "Boxes", ",", "boxes2", ":", "Boxes", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Similar to :func:`pariwise_iou` but compute the IoA (intersection over boxes2 area).\n\n    Args:\n        boxes1,boxes2 (Boxes): two `Boxes`. Contains N & M boxes, respectively.\n\n    Returns:\n        Tensor: IoA, sized [N,M].\n    \"\"\"", "\n", "area2", "=", "boxes2", ".", "area", "(", ")", "# [M]", "\n", "inter", "=", "pairwise_intersection", "(", "boxes1", ",", "boxes2", ")", "\n", "\n", "# handle empty boxes", "\n", "ioa", "=", "torch", ".", "where", "(", "\n", "inter", ">", "0", ",", "inter", "/", "area2", ",", "torch", ".", "zeros", "(", "1", ",", "dtype", "=", "inter", ".", "dtype", ",", "device", "=", "inter", ".", "device", ")", "\n", ")", "\n", "return", "ioa", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.matched_boxlist_iou": [[391, 417], ["boxes1.area", "boxes2.area", "torch.max", "torch.min", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area"], ["", "def", "matched_boxlist_iou", "(", "boxes1", ":", "Boxes", ",", "boxes2", ":", "Boxes", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Compute pairwise intersection over union (IOU) of two sets of matched\n    boxes. The box order must be (xmin, ymin, xmax, ymax).\n    Similar to boxlist_iou, but computes only diagonal elements of the matrix\n\n    Args:\n        boxes1: (Boxes) bounding boxes, sized [N,4].\n        boxes2: (Boxes) bounding boxes, sized [N,4].\n    Returns:\n        Tensor: iou, sized [N].\n    \"\"\"", "\n", "assert", "len", "(", "boxes1", ")", "==", "len", "(", "\n", "boxes2", "\n", ")", ",", "\"boxlists should have the same\"", "\"number of entries, got {}, {}\"", ".", "format", "(", "\n", "len", "(", "boxes1", ")", ",", "len", "(", "boxes2", ")", "\n", ")", "\n", "area1", "=", "boxes1", ".", "area", "(", ")", "# [N]", "\n", "area2", "=", "boxes2", ".", "area", "(", ")", "# [N]", "\n", "box1", ",", "box2", "=", "boxes1", ".", "tensor", ",", "boxes2", ".", "tensor", "\n", "lt", "=", "torch", ".", "max", "(", "box1", "[", ":", ",", ":", "2", "]", ",", "box2", "[", ":", ",", ":", "2", "]", ")", "# [N,2]", "\n", "rb", "=", "torch", ".", "min", "(", "box1", "[", ":", ",", "2", ":", "]", ",", "box2", "[", ":", ",", "2", ":", "]", ")", "# [N,2]", "\n", "wh", "=", "(", "rb", "-", "lt", ")", ".", "clamp", "(", "min", "=", "0", ")", "# [N,2]", "\n", "inter", "=", "wh", "[", ":", ",", "0", "]", "*", "wh", "[", ":", ",", "1", "]", "# [N]", "\n", "iou", "=", "inter", "/", "(", "area1", "+", "area2", "-", "inter", ")", "# [N]", "\n", "return", "iou", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xy_to_wh": [[14, 16], ["detectron2.structures.BoxMode.convert"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["    ", "def", "_convert_xy_to_wh", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "BoxMode", ".", "convert", "(", "x", ",", "BoxMode", ".", "XYXY_ABS", ",", "BoxMode", ".", "XYWH_ABS", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywha_to_xyxy": [[17, 19], ["detectron2.structures.BoxMode.convert"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "def", "_convert_xywha_to_xyxy", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "BoxMode", ".", "convert", "(", "x", ",", "BoxMode", ".", "XYWHA_ABS", ",", "BoxMode", ".", "XYXY_ABS", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywh_to_xywha": [[20, 22], ["detectron2.structures.BoxMode.convert"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "def", "_convert_xywh_to_xywha", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "BoxMode", ".", "convert", "(", "x", ",", "BoxMode", ".", "XYWH_ABS", ",", "BoxMode", ".", "XYWHA_ABS", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_convert_int_mode": [[23, 25], ["detectron2.structures.BoxMode.convert"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "def", "test_convert_int_mode", "(", "self", ")", ":", "\n", "        ", "BoxMode", ".", "convert", "(", "[", "1", ",", "2", ",", "3", ",", "4", "]", ",", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_list": [[26, 36], ["tp", "test_boxes.TestBoxMode._convert_xy_to_wh", "test_boxes.TestBoxMode.assertIsInstance", "test_boxes.TestBoxMode.assertIsInstance", "test_boxes.TestBoxMode.assertEqual", "tp", "test_boxes.TestBoxMode.assertRaises", "test_boxes.TestBoxMode._convert_xy_to_wh"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xy_to_wh", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xy_to_wh"], ["", "def", "test_box_convert_list", "(", "self", ")", ":", "\n", "        ", "for", "tp", "in", "[", "list", ",", "tuple", "]", ":", "\n", "            ", "box", "=", "tp", "(", "[", "5.0", ",", "5.0", ",", "10.0", ",", "10.0", "]", ")", "\n", "output", "=", "self", ".", "_convert_xy_to_wh", "(", "box", ")", "\n", "self", ".", "assertIsInstance", "(", "output", ",", "tp", ")", "\n", "self", ".", "assertIsInstance", "(", "output", "[", "0", "]", ",", "float", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "tp", "(", "[", "5.0", ",", "5.0", ",", "5.0", ",", "5.0", "]", ")", ")", "\n", "\n", "with", "self", ".", "assertRaises", "(", "Exception", ")", ":", "\n", "                ", "self", ".", "_convert_xy_to_wh", "(", "[", "box", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_array": [[37, 44], ["numpy.asarray", "test_boxes.TestBoxMode._convert_xy_to_wh", "test_boxes.TestBoxMode.assertEqual", "test_boxes.TestBoxMode.assertEqual", "test_boxes.TestBoxMode.assertTrue", "test_boxes.TestBoxMode.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xy_to_wh"], ["", "", "", "def", "test_box_convert_array", "(", "self", ")", ":", "\n", "        ", "box", "=", "np", ".", "asarray", "(", "[", "[", "5", ",", "5", ",", "10", ",", "10", "]", ",", "[", "1", ",", "1", ",", "2", ",", "3", "]", "]", ")", "\n", "output", "=", "self", ".", "_convert_xy_to_wh", "(", "box", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "dtype", ",", "box", ".", "dtype", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "shape", ",", "box", ".", "shape", ")", "\n", "self", ".", "assertTrue", "(", "(", "output", "[", "0", "]", "==", "[", "5", ",", "5", ",", "5", ",", "5", "]", ")", ".", "all", "(", ")", ")", "\n", "self", ".", "assertTrue", "(", "(", "output", "[", "1", "]", "==", "[", "1", ",", "1", ",", "1", ",", "2", "]", ")", ".", "all", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_cpu_tensor": [[45, 53], ["torch.tensor", "test_boxes.TestBoxMode._convert_xy_to_wh", "test_boxes.TestBoxMode.assertEqual", "test_boxes.TestBoxMode.assertEqual", "output.numpy.numpy.numpy", "test_boxes.TestBoxMode.assertTrue", "test_boxes.TestBoxMode.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xy_to_wh"], ["", "def", "test_box_convert_cpu_tensor", "(", "self", ")", ":", "\n", "        ", "box", "=", "torch", ".", "tensor", "(", "[", "[", "5", ",", "5", ",", "10", ",", "10", "]", ",", "[", "1", ",", "1", ",", "2", ",", "3", "]", "]", ")", "\n", "output", "=", "self", ".", "_convert_xy_to_wh", "(", "box", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "dtype", ",", "box", ".", "dtype", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "shape", ",", "box", ".", "shape", ")", "\n", "output", "=", "output", ".", "numpy", "(", ")", "\n", "self", ".", "assertTrue", "(", "(", "output", "[", "0", "]", "==", "[", "5", ",", "5", ",", "5", ",", "5", "]", ")", ".", "all", "(", ")", ")", "\n", "self", ".", "assertTrue", "(", "(", "output", "[", "1", "]", "==", "[", "1", ",", "1", ",", "1", ",", "2", "]", ")", ".", "all", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_cuda_tensor": [[54, 64], ["unittest.skipIf", "torch.tensor().cuda", "test_boxes.TestBoxMode._convert_xy_to_wh", "test_boxes.TestBoxMode.assertEqual", "test_boxes.TestBoxMode.assertEqual", "test_boxes.TestBoxMode.assertEqual", "output.cpu().numpy.cpu().numpy.cpu().numpy", "test_boxes.TestBoxMode.assertTrue", "test_boxes.TestBoxMode.assertTrue", "torch.cuda.is_available", "torch.tensor", "output.cpu().numpy.cpu().numpy.cpu"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xy_to_wh"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_box_convert_cuda_tensor", "(", "self", ")", ":", "\n", "        ", "box", "=", "torch", ".", "tensor", "(", "[", "[", "5", ",", "5", ",", "10", ",", "10", "]", ",", "[", "1", ",", "1", ",", "2", ",", "3", "]", "]", ")", ".", "cuda", "(", ")", "\n", "output", "=", "self", ".", "_convert_xy_to_wh", "(", "box", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "dtype", ",", "box", ".", "dtype", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "shape", ",", "box", ".", "shape", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "device", ",", "box", ".", "device", ")", "\n", "output", "=", "output", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "self", ".", "assertTrue", "(", "(", "output", "[", "0", "]", "==", "[", "5", ",", "5", ",", "5", ",", "5", "]", ")", ".", "all", "(", ")", ")", "\n", "self", ".", "assertTrue", "(", "(", "output", "[", "1", "]", "==", "[", "1", ",", "1", ",", "1", ",", "2", "]", ")", ".", "all", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_xywha_to_xyxy_list": [[65, 74], ["tp", "test_boxes.TestBoxMode._convert_xywha_to_xyxy", "test_boxes.TestBoxMode.assertIsInstance", "test_boxes.TestBoxMode.assertEqual", "tp", "test_boxes.TestBoxMode.assertRaises", "test_boxes.TestBoxMode._convert_xywha_to_xyxy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywha_to_xyxy", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywha_to_xyxy"], ["", "def", "test_box_convert_xywha_to_xyxy_list", "(", "self", ")", ":", "\n", "        ", "for", "tp", "in", "[", "list", ",", "tuple", "]", ":", "\n", "            ", "box", "=", "tp", "(", "[", "50", ",", "50", ",", "30", ",", "20", ",", "0", "]", ")", "\n", "output", "=", "self", ".", "_convert_xywha_to_xyxy", "(", "box", ")", "\n", "self", ".", "assertIsInstance", "(", "output", ",", "tp", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "tp", "(", "[", "35", ",", "40", ",", "65", ",", "60", "]", ")", ")", "\n", "\n", "with", "self", ".", "assertRaises", "(", "Exception", ")", ":", "\n", "                ", "self", ".", "_convert_xywha_to_xyxy", "(", "[", "box", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_xywha_to_xyxy_array": [[75, 89], ["numpy.asarray", "test_boxes.TestBoxMode._convert_xywha_to_xyxy", "test_boxes.TestBoxMode.assertEqual", "numpy.asarray", "test_boxes.TestBoxMode.assertTrue", "numpy.allclose", "math.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywha_to_xyxy"], ["", "", "", "def", "test_box_convert_xywha_to_xyxy_array", "(", "self", ")", ":", "\n", "        ", "for", "dtype", "in", "[", "np", ".", "float64", ",", "np", ".", "float32", "]", ":", "\n", "            ", "box", "=", "np", ".", "asarray", "(", "\n", "[", "\n", "[", "50", ",", "50", ",", "30", ",", "20", ",", "0", "]", ",", "\n", "[", "50", ",", "50", ",", "30", ",", "20", ",", "90", "]", ",", "\n", "[", "1", ",", "1", ",", "math", ".", "sqrt", "(", "2", ")", ",", "math", ".", "sqrt", "(", "2", ")", ",", "-", "45", "]", ",", "\n", "]", ",", "\n", "dtype", "=", "dtype", ",", "\n", ")", "\n", "output", "=", "self", ".", "_convert_xywha_to_xyxy", "(", "box", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "dtype", ",", "box", ".", "dtype", ")", "\n", "expected", "=", "np", ".", "asarray", "(", "[", "[", "35", ",", "40", ",", "65", ",", "60", "]", ",", "[", "40", ",", "35", ",", "60", ",", "65", "]", ",", "[", "0", ",", "0", ",", "2", ",", "2", "]", "]", ",", "dtype", "=", "dtype", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output", ",", "expected", ",", "atol", "=", "1e-6", ")", ",", "\"output={}\"", ".", "format", "(", "output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_xywha_to_xyxy_tensor": [[90, 105], ["torch.tensor", "test_boxes.TestBoxMode._convert_xywha_to_xyxy", "test_boxes.TestBoxMode.assertEqual", "torch.tensor", "test_boxes.TestBoxMode.assertTrue", "torch.allclose", "math.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywha_to_xyxy"], ["", "", "def", "test_box_convert_xywha_to_xyxy_tensor", "(", "self", ")", ":", "\n", "        ", "for", "dtype", "in", "[", "torch", ".", "float32", ",", "torch", ".", "float64", "]", ":", "\n", "            ", "box", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "50", ",", "50", ",", "30", ",", "20", ",", "0", "]", ",", "\n", "[", "50", ",", "50", ",", "30", ",", "20", ",", "90", "]", ",", "\n", "[", "1", ",", "1", ",", "math", ".", "sqrt", "(", "2", ")", ",", "math", ".", "sqrt", "(", "2", ")", ",", "-", "45", "]", ",", "\n", "]", ",", "\n", "dtype", "=", "dtype", ",", "\n", ")", "\n", "output", "=", "self", ".", "_convert_xywha_to_xyxy", "(", "box", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "dtype", ",", "box", ".", "dtype", ")", "\n", "expected", "=", "torch", ".", "tensor", "(", "[", "[", "35", ",", "40", ",", "65", ",", "60", "]", ",", "[", "40", ",", "35", ",", "60", ",", "65", "]", ",", "[", "0", ",", "0", ",", "2", ",", "2", "]", "]", ",", "dtype", "=", "dtype", ")", "\n", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "output", ",", "expected", ",", "atol", "=", "1e-6", ")", ",", "\"output={}\"", ".", "format", "(", "output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_xywh_to_xywha_list": [[106, 115], ["tp", "test_boxes.TestBoxMode._convert_xywh_to_xywha", "test_boxes.TestBoxMode.assertIsInstance", "test_boxes.TestBoxMode.assertEqual", "tp", "test_boxes.TestBoxMode.assertRaises", "test_boxes.TestBoxMode._convert_xywh_to_xywha"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywh_to_xywha", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywh_to_xywha"], ["", "", "def", "test_box_convert_xywh_to_xywha_list", "(", "self", ")", ":", "\n", "        ", "for", "tp", "in", "[", "list", ",", "tuple", "]", ":", "\n", "            ", "box", "=", "tp", "(", "[", "50", ",", "50", ",", "30", ",", "20", "]", ")", "\n", "output", "=", "self", ".", "_convert_xywh_to_xywha", "(", "box", ")", "\n", "self", ".", "assertIsInstance", "(", "output", ",", "tp", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "tp", "(", "[", "65", ",", "60", ",", "30", ",", "20", ",", "0", "]", ")", ")", "\n", "\n", "with", "self", ".", "assertRaises", "(", "Exception", ")", ":", "\n", "                ", "self", ".", "_convert_xywh_to_xywha", "(", "[", "box", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_xywh_to_xywha_array": [[116, 125], ["numpy.asarray", "test_boxes.TestBoxMode._convert_xywh_to_xywha", "test_boxes.TestBoxMode.assertEqual", "numpy.asarray", "test_boxes.TestBoxMode.assertTrue", "numpy.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywh_to_xywha"], ["", "", "", "def", "test_box_convert_xywh_to_xywha_array", "(", "self", ")", ":", "\n", "        ", "for", "dtype", "in", "[", "np", ".", "float64", ",", "np", ".", "float32", "]", ":", "\n", "            ", "box", "=", "np", ".", "asarray", "(", "[", "[", "30", ",", "40", ",", "70", ",", "60", "]", ",", "[", "30", ",", "40", ",", "60", ",", "70", "]", ",", "[", "-", "1", ",", "-", "1", ",", "2", ",", "2", "]", "]", ",", "dtype", "=", "dtype", ")", "\n", "output", "=", "self", ".", "_convert_xywh_to_xywha", "(", "box", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "dtype", ",", "box", ".", "dtype", ")", "\n", "expected", "=", "np", ".", "asarray", "(", "\n", "[", "[", "65", ",", "70", ",", "70", ",", "60", ",", "0", "]", ",", "[", "60", ",", "75", ",", "60", ",", "70", ",", "0", "]", ",", "[", "0", ",", "0", ",", "2", ",", "2", ",", "0", "]", "]", ",", "dtype", "=", "dtype", "\n", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output", ",", "expected", ",", "atol", "=", "1e-6", ")", ",", "\"output={}\"", ".", "format", "(", "output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_box_convert_xywh_to_xywha_tensor": [[126, 136], ["torch.tensor", "test_boxes.TestBoxMode._convert_xywh_to_xywha", "test_boxes.TestBoxMode.assertEqual", "torch.tensor", "test_boxes.TestBoxMode.assertTrue", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode._convert_xywh_to_xywha"], ["", "", "def", "test_box_convert_xywh_to_xywha_tensor", "(", "self", ")", ":", "\n", "        ", "for", "dtype", "in", "[", "torch", ".", "float32", ",", "torch", ".", "float64", "]", ":", "\n", "            ", "box", "=", "torch", ".", "tensor", "(", "[", "[", "30", ",", "40", ",", "70", ",", "60", "]", ",", "[", "30", ",", "40", ",", "60", ",", "70", "]", ",", "[", "-", "1", ",", "-", "1", ",", "2", ",", "2", "]", "]", ",", "dtype", "=", "dtype", ")", "\n", "output", "=", "self", ".", "_convert_xywh_to_xywha", "(", "box", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "dtype", ",", "box", ".", "dtype", ")", "\n", "expected", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "65", ",", "70", ",", "70", ",", "60", ",", "0", "]", ",", "[", "60", ",", "75", ",", "60", ",", "70", ",", "0", "]", ",", "[", "0", ",", "0", ",", "2", ",", "2", ",", "0", "]", "]", ",", "dtype", "=", "dtype", "\n", ")", "\n", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "output", ",", "expected", ",", "atol", "=", "1e-6", ")", ",", "\"output={}\"", ".", "format", "(", "output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_json_serializable": [[137, 143], ["json.dumps", "test_boxes.TestBoxMode.fail"], "methods", ["None"], ["", "", "def", "test_json_serializable", "(", "self", ")", ":", "\n", "        ", "payload", "=", "{", "\"box_mode\"", ":", "BoxMode", ".", "XYWH_REL", "}", "\n", "try", ":", "\n", "            ", "json", ".", "dumps", "(", "payload", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "self", ".", "fail", "(", "\"JSON serialization failed\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxMode.test_json_deserializable": [[144, 151], ["json.loads", "detectron2.structures.BoxMode", "test_boxes.TestBoxMode.fail"], "methods", ["None"], ["", "", "def", "test_json_deserializable", "(", "self", ")", ":", "\n", "        ", "payload", "=", "'{\"box_mode\": 2}'", "\n", "obj", "=", "json", ".", "loads", "(", "payload", ")", "\n", "try", ":", "\n", "            ", "obj", "[", "\"box_mode\"", "]", "=", "BoxMode", "(", "obj", "[", "\"box_mode\"", "]", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "self", ".", "fail", "(", "\"JSON deserialization failed\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxIOU.create_boxes": [[154, 168], ["torch.tensor", "torch.tensor"], "methods", ["None"], ["    ", "def", "create_boxes", "(", "self", ")", ":", "\n", "        ", "boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "0.0", ",", "0.0", ",", "1.0", ",", "1.0", "]", ",", "[", "0.0", ",", "0.0", ",", "1.0", ",", "1.0", "]", "]", ")", "\n", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "0.0", ",", "0.0", ",", "1.0", ",", "1.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "0.5", ",", "1.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "1.0", ",", "0.5", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "0.5", ",", "0.5", "]", ",", "\n", "[", "0.5", ",", "0.5", ",", "1.0", ",", "1.0", "]", ",", "\n", "[", "0.5", ",", "0.5", ",", "1.5", ",", "1.5", "]", ",", "\n", "]", "\n", ")", "\n", "return", "boxes1", ",", "boxes2", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxIOU.test_pairwise_iou": [[169, 180], ["test_boxes.TestBoxIOU.create_boxes", "torch.tensor", "detectron2.structures.pairwise_iou", "test_boxes.TestBoxIOU.assertTrue", "detectron2.structures.Boxes", "detectron2.structures.Boxes", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxIOU.create_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "def", "test_pairwise_iou", "(", "self", ")", ":", "\n", "        ", "boxes1", ",", "boxes2", "=", "self", ".", "create_boxes", "(", ")", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "1.0", ",", "0.5", ",", "0.5", ",", "0.25", ",", "0.25", ",", "0.25", "/", "(", "2", "-", "0.25", ")", "]", ",", "\n", "[", "1.0", ",", "0.5", ",", "0.5", ",", "0.25", ",", "0.25", ",", "0.25", "/", "(", "2", "-", "0.25", ")", "]", ",", "\n", "]", "\n", ")", "\n", "\n", "ious", "=", "pairwise_iou", "(", "Boxes", "(", "boxes1", ")", ",", "Boxes", "(", "boxes2", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxIOU.test_pairwise_ioa": [[181, 188], ["test_boxes.TestBoxIOU.create_boxes", "torch.tensor", "detectron2.structures.pairwise_ioa", "test_boxes.TestBoxIOU.assertTrue", "detectron2.structures.Boxes", "detectron2.structures.Boxes", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxIOU.create_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_ioa"], ["", "def", "test_pairwise_ioa", "(", "self", ")", ":", "\n", "        ", "boxes1", ",", "boxes2", "=", "self", ".", "create_boxes", "(", ")", "\n", "expected_ioas", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", ",", "0.25", "]", ",", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", ",", "0.25", "]", "]", "\n", ")", "\n", "ioas", "=", "pairwise_ioa", "(", "Boxes", "(", "boxes1", ")", ",", "Boxes", "(", "boxes2", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ioas", ",", "expected_ioas", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxes.test_empty_cat": [[191, 194], ["detectron2.structures.Boxes.cat", "test_boxes.TestBoxes.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["    ", "def", "test_empty_cat", "(", "self", ")", ":", "\n", "        ", "x", "=", "Boxes", ".", "cat", "(", "[", "]", ")", "\n", "self", ".", "assertTrue", "(", "x", ".", "tensor", ".", "shape", ",", "(", "0", ",", "4", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxes.test_to": [[195, 198], ["detectron2.structures.Boxes", "test_boxes.TestBoxes.assertEqual", "torch.rand", "detectron2.structures.Boxes.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "test_to", "(", "self", ")", ":", "\n", "        ", "x", "=", "Boxes", "(", "torch", ".", "rand", "(", "3", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "x", ".", "to", "(", "device", "=", "\"cpu\"", ")", ".", "tensor", ".", "device", ".", "type", ",", "\"cpu\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_boxes.TestBoxes.test_scriptability": [[199, 222], ["unittest.skipIf", "torch.jit.script", "detectron2.utils.testing.reload_script_model", "torch.jit.script.", "torch.rand", "torch.jit.script", "torch.jit.script.", "test_boxes.TestBoxes.assertTrue", "detectron2.structures.Boxes", "torch.rand", "detectron2.structures.Boxes", "detectron2.structures.Boxes", "detectron2.structures.Boxes.cat", "torch.equal", "detectron2.structures.Boxes.to", "detectron2.structures.Boxes.area", "torch.cat", "torch.device"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.reload_script_model", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_scriptability", "(", "self", ")", ":", "\n", "        ", "def", "func", "(", "x", ")", ":", "\n", "            ", "boxes", "=", "Boxes", "(", "x", ")", "\n", "test", "=", "boxes", ".", "to", "(", "torch", ".", "device", "(", "\"cpu\"", ")", ")", ".", "tensor", "\n", "return", "boxes", ".", "area", "(", ")", ",", "test", "\n", "\n", "", "f", "=", "torch", ".", "jit", ".", "script", "(", "func", ")", "\n", "f", "=", "reload_script_model", "(", "f", ")", "\n", "f", "(", "torch", ".", "rand", "(", "(", "3", ",", "4", ")", ")", ")", "\n", "\n", "data", "=", "torch", ".", "rand", "(", "(", "3", ",", "4", ")", ")", "\n", "\n", "def", "func_cat", "(", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "            ", "boxes1", "=", "Boxes", "(", "x", ")", "\n", "boxes2", "=", "Boxes", "(", "x", ")", "\n", "# boxes3 = Boxes.cat([boxes1, boxes2])  # this is not supported by torchsript for now.", "\n", "boxes3", "=", "boxes1", ".", "cat", "(", "[", "boxes1", ",", "boxes2", "]", ")", "\n", "return", "boxes3", "\n", "\n", "", "f", "=", "torch", ".", "jit", ".", "script", "(", "func_cat", ")", "\n", "script_box", "=", "f", "(", "data", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "torch", ".", "cat", "(", "[", "data", ",", "data", "]", ")", ",", "script_box", ".", "tensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_imagelist.TestImageList.test_imagelist_padding_tracing": [[12, 44], ["unittest.skipIf", "torch.jit.trace", "torch.jit.trace.", "test_imagelist.TestImageList.assertEqual", "test_imagelist.TestImageList.assertEqual", "torch.jit.trace", "torch.jit.trace.", "test_imagelist.TestImageList.assertEqual", "test_imagelist.TestImageList.assertEqual", "test_imagelist.TestImageList.assertEqual", "detectron2.structures.ImageList.from_tensors", "torch.ones", "torch.jit.trace", "torch.jit.trace.", "test_imagelist.TestImageList.assertEqual", "test_imagelist.TestImageList.assertEqual", "image_sizes[].tolist", "image_sizes[].tolist", "image_sizes[].tolist", "image_sizes[].tolist", "test_imagelist.TestImageList.test_imagelist_padding_tracing._tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors"], ["    ", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_imagelist_padding_tracing", "(", "self", ")", ":", "\n", "# test that the trace does not contain hard-coded constant sizes", "\n", "        ", "def", "to_imagelist", "(", "tensors", ":", "Sequence", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "            ", "image_list", "=", "ImageList", ".", "from_tensors", "(", "tensors", ",", "4", ")", "\n", "return", "image_list", ".", "tensor", ",", "image_list", ".", "image_sizes", "\n", "\n", "", "def", "_tensor", "(", "*", "shape", ")", ":", "\n", "            ", "return", "torch", ".", "ones", "(", "shape", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "# test CHW (inputs needs padding vs. no padding)", "\n", "", "for", "shape", "in", "[", "(", "3", ",", "10", ",", "10", ")", ",", "(", "3", ",", "12", ",", "12", ")", "]", ":", "\n", "            ", "func", "=", "torch", ".", "jit", ".", "trace", "(", "to_imagelist", ",", "(", "[", "_tensor", "(", "*", "shape", ")", "]", ",", ")", ")", "\n", "tensor", ",", "image_sizes", "=", "func", "(", "[", "_tensor", "(", "3", ",", "15", ",", "20", ")", "]", ")", "\n", "self", ".", "assertEqual", "(", "tensor", ".", "shape", ",", "(", "1", ",", "3", ",", "16", ",", "20", ")", ",", "tensor", ".", "shape", ")", "\n", "self", ".", "assertEqual", "(", "image_sizes", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "15", ",", "20", "]", ",", "image_sizes", "[", "0", "]", ")", "\n", "\n", "# test HW", "\n", "", "func", "=", "torch", ".", "jit", ".", "trace", "(", "to_imagelist", ",", "(", "[", "_tensor", "(", "10", ",", "10", ")", "]", ",", ")", ")", "\n", "tensor", ",", "image_sizes", "=", "func", "(", "[", "_tensor", "(", "15", ",", "20", ")", "]", ")", "\n", "self", ".", "assertEqual", "(", "tensor", ".", "shape", ",", "(", "1", ",", "16", ",", "20", ")", ",", "tensor", ".", "shape", ")", "\n", "self", ".", "assertEqual", "(", "image_sizes", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "15", ",", "20", "]", ",", "image_sizes", "[", "0", "]", ")", "\n", "\n", "# test 2x CHW", "\n", "func", "=", "torch", ".", "jit", ".", "trace", "(", "\n", "to_imagelist", ",", "\n", "(", "[", "_tensor", "(", "3", ",", "16", ",", "10", ")", ",", "_tensor", "(", "3", ",", "13", ",", "11", ")", "]", ",", ")", ",", "\n", ")", "\n", "tensor", ",", "image_sizes", "=", "func", "(", "[", "_tensor", "(", "3", ",", "25", ",", "20", ")", ",", "_tensor", "(", "3", ",", "10", ",", "10", ")", "]", ")", "\n", "self", ".", "assertEqual", "(", "tensor", ".", "shape", ",", "(", "2", ",", "3", ",", "28", ",", "20", ")", ",", "tensor", ".", "shape", ")", "\n", "self", ".", "assertEqual", "(", "image_sizes", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "25", ",", "20", "]", ",", "image_sizes", "[", "0", "]", ")", "\n", "self", ".", "assertEqual", "(", "image_sizes", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "10", ",", "10", "]", ",", "image_sizes", "[", "1", "]", ")", "\n", "# support calling with different spatial sizes, but not with different #images", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_imagelist.TestImageList.test_imagelist_scriptability": [[46, 61], ["unittest.skipIf", "torch.randn", "test_imagelist.TestImageList.test_imagelist_scriptability.f"], "methods", ["None"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_imagelist_scriptability", "(", "self", ")", ":", "\n", "        ", "image_nums", "=", "2", "\n", "image_tensor", "=", "torch", ".", "randn", "(", "(", "image_nums", ",", "10", ",", "20", ")", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "image_shape", "=", "[", "(", "10", ",", "20", ")", "]", "*", "image_nums", "\n", "\n", "def", "f", "(", "image_tensor", ",", "image_shape", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ")", ":", "\n", "            ", "return", "ImageList", "(", "image_tensor", ",", "image_shape", ")", "\n", "\n", "", "ret", "=", "f", "(", "image_tensor", ",", "image_shape", ")", "\n", "ret_script", "=", "torch", ".", "jit", ".", "script", "(", "f", ")", "(", "image_tensor", ",", "image_shape", ")", "\n", "\n", "self", ".", "assertEqual", "(", "len", "(", "ret", ")", ",", "len", "(", "ret_script", ")", ")", "\n", "for", "i", "in", "range", "(", "image_nums", ")", ":", "\n", "            ", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "ret", "[", "i", "]", ",", "ret_script", "[", "i", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_imagelist.TestImageList.test_imagelist_from_tensors_scriptability": [[62, 76], ["unittest.skipIf", "torch.randn", "torch.randn", "test_imagelist.TestImageList.test_imagelist_scriptability.f"], "methods", ["None"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_imagelist_from_tensors_scriptability", "(", "self", ")", ":", "\n", "        ", "image_tensor_0", "=", "torch", ".", "randn", "(", "10", ",", "20", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "image_tensor_1", "=", "torch", ".", "randn", "(", "12", ",", "22", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "inputs", "=", "[", "image_tensor_0", ",", "image_tensor_1", "]", "\n", "\n", "def", "f", "(", "image_tensor", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "            ", "return", "ImageList", ".", "from_tensors", "(", "image_tensor", ",", "10", ")", "\n", "\n", "", "ret", "=", "f", "(", "inputs", ")", "\n", "ret_script", "=", "torch", ".", "jit", ".", "script", "(", "f", ")", "(", "inputs", ")", "\n", "\n", "self", ".", "assertEqual", "(", "len", "(", "ret", ")", ",", "len", "(", "ret_script", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "ret", ".", "tensor", ",", "ret_script", ".", "tensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_masks.TestBitMask.test_get_bounding_box": [[9, 40], ["torch.tensor", "detectron2.structures.masks.BitMasks", "torch.tensor", "detectron2.structures.masks.BitMasks.get_bounding_boxes", "test_masks.TestBitMask.assertTrue", "torch.all().item", "box[].numpy", "detectron2.structures.masks.polygons_to_bitmask", "test_masks.TestBitMask.assertTrue", "test_masks.TestBitMask.assertTrue", "torch.zeros", "torch.all().item", "torch.all().item", "torch.all", "detectron2.structures.masks.BitMasks().get_bounding_boxes", "detectron2.structures.masks.PolygonMasks().get_bounding_boxes", "torch.all", "torch.all", "detectron2.structures.masks.BitMasks", "detectron2.structures.masks.PolygonMasks"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.get_bounding_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.polygons_to_bitmask", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.get_bounding_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.get_bounding_boxes"], ["    ", "def", "test_get_bounding_box", "(", "self", ")", ":", "\n", "        ", "masks", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "\n", "[", "False", ",", "False", ",", "False", ",", "True", "]", ",", "\n", "[", "False", ",", "False", ",", "True", ",", "True", "]", ",", "\n", "[", "False", ",", "True", ",", "True", ",", "False", "]", ",", "\n", "[", "False", ",", "True", ",", "True", ",", "False", "]", ",", "\n", "]", ",", "\n", "[", "\n", "[", "False", ",", "False", ",", "False", ",", "False", "]", ",", "\n", "[", "False", ",", "False", ",", "True", ",", "False", "]", ",", "\n", "[", "False", ",", "True", ",", "True", ",", "False", "]", ",", "\n", "[", "False", ",", "True", ",", "True", ",", "False", "]", ",", "\n", "]", ",", "\n", "torch", ".", "zeros", "(", "4", ",", "4", ")", ",", "\n", "]", "\n", ")", "\n", "bitmask", "=", "BitMasks", "(", "masks", ")", "\n", "box_true", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "0", ",", "4", ",", "4", "]", ",", "[", "1", ",", "1", ",", "3", ",", "4", "]", ",", "[", "0", ",", "0", ",", "0", ",", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "box", "=", "bitmask", ".", "get_bounding_boxes", "(", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "all", "(", "box", ".", "tensor", "==", "box_true", ")", ".", "item", "(", ")", ")", "\n", "\n", "for", "box", "in", "box_true", ":", "\n", "            ", "poly", "=", "box", "[", "[", "0", ",", "1", ",", "2", ",", "1", ",", "2", ",", "3", ",", "0", ",", "3", "]", "]", ".", "numpy", "(", ")", "\n", "mask", "=", "polygons_to_bitmask", "(", "[", "poly", "]", ",", "4", ",", "4", ")", "\n", "reconstruct_box", "=", "BitMasks", "(", "mask", "[", "None", ",", ":", ",", ":", "]", ")", ".", "get_bounding_boxes", "(", ")", "[", "0", "]", ".", "tensor", "\n", "self", ".", "assertTrue", "(", "torch", ".", "all", "(", "box", "==", "reconstruct_box", ")", ".", "item", "(", ")", ")", "\n", "\n", "reconstruct_box", "=", "PolygonMasks", "(", "[", "[", "poly", "]", "]", ")", ".", "get_bounding_boxes", "(", ")", "[", "0", "]", ".", "tensor", "\n", "self", ".", "assertTrue", "(", "torch", ".", "all", "(", "box", "==", "reconstruct_box", ")", ".", "item", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_instances.TestInstances.test_int_indexing": [[13, 26], ["torch.tensor", "torch.tensor", "detectron2.structures.Instances", "range", "test_instances.TestInstances.assertRaises", "test_instances.TestInstances.assertRaises", "len", "test_instances.TestInstances.assertEqual", "test_instances.TestInstances.assertEqual", "len", "len", "len"], "methods", ["None"], ["    ", "def", "test_int_indexing", "(", "self", ")", ":", "\n", "        ", "attr1", "=", "torch", ".", "tensor", "(", "[", "[", "0.0", ",", "0.0", ",", "1.0", "]", ",", "[", "0.0", ",", "0.0", ",", "0.5", "]", ",", "[", "0.0", ",", "0.0", ",", "1.0", "]", ",", "[", "0.0", ",", "0.5", ",", "0.5", "]", "]", ")", "\n", "attr2", "=", "torch", ".", "tensor", "(", "[", "0.1", ",", "0.2", ",", "0.3", ",", "0.4", "]", ")", "\n", "instances", "=", "Instances", "(", "(", "100", ",", "100", ")", ")", "\n", "instances", ".", "attr1", "=", "attr1", "\n", "instances", ".", "attr2", "=", "attr2", "\n", "for", "i", "in", "range", "(", "-", "len", "(", "instances", ")", ",", "len", "(", "instances", ")", ")", ":", "\n", "            ", "inst", "=", "instances", "[", "i", "]", "\n", "self", ".", "assertEqual", "(", "(", "inst", ".", "attr1", "==", "attr1", "[", "i", "]", ")", ".", "all", "(", ")", ",", "True", ")", "\n", "self", ".", "assertEqual", "(", "(", "inst", ".", "attr2", "==", "attr2", "[", "i", "]", ")", ".", "all", "(", ")", ",", "True", ")", "\n", "\n", "", "self", ".", "assertRaises", "(", "IndexError", ",", "lambda", ":", "instances", "[", "len", "(", "instances", ")", "]", ")", "\n", "self", ".", "assertRaises", "(", "IndexError", ",", "lambda", ":", "instances", "[", "-", "len", "(", "instances", ")", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_instances.TestInstances.test_script_new_fields": [[27, 75], ["unittest.skipIf", "detectron2.export.torchscript.patch_instances", "torch.jit.script", "test_instances.TestInstances.assertRaises", "torch.jit.script", "detectron2.export.torchscript.patch_instances", "torch.jit.script", "detectron2.export.torchscript.patch_instances", "torch.jit.script", "NewInstances", "torch.rand", "detectron2.structures.Boxes", "torch.jit.script.", "test_instances.TestInstances.test_script_new_fields.get_mask"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_script_new_fields", "(", "self", ")", ":", "\n", "        ", "def", "get_mask", "(", "x", ":", "Instances", ")", "->", "torch", ".", "Tensor", ":", "\n", "            ", "return", "x", ".", "mask", "\n", "\n", "", "class", "f", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "x", ":", "Instances", ")", ":", "\n", "                ", "proposal_boxes", "=", "x", ".", "proposal_boxes", "# noqa F841", "\n", "objectness_logits", "=", "x", ".", "objectness_logits", "# noqa F841", "\n", "return", "x", "\n", "\n", "", "", "class", "g", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "x", ":", "Instances", ")", ":", "\n", "                ", "return", "get_mask", "(", "x", ")", "\n", "\n", "", "", "class", "g2", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "__init__", "(", "self", ")", ":", "\n", "                ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "g", "=", "g", "(", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ":", "Instances", ")", ":", "\n", "                ", "proposal_boxes", "=", "x", ".", "proposal_boxes", "# noqa F841", "\n", "return", "x", ",", "self", ".", "g", "(", "x", ")", "\n", "\n", "", "", "fields", "=", "{", "\"proposal_boxes\"", ":", "Boxes", ",", "\"objectness_logits\"", ":", "Tensor", "}", "\n", "with", "patch_instances", "(", "fields", ")", ":", "\n", "            ", "torch", ".", "jit", ".", "script", "(", "f", "(", ")", ")", "\n", "\n", "# can't script anymore after exiting the context", "\n", "", "with", "self", ".", "assertRaises", "(", "Exception", ")", ":", "\n", "# will create a ConcreteType for g", "\n", "            ", "torch", ".", "jit", ".", "script", "(", "g2", "(", ")", ")", "\n", "\n", "", "new_fields", "=", "{", "\"mask\"", ":", "Tensor", "}", "\n", "with", "patch_instances", "(", "new_fields", ")", ":", "\n", "# will compile g with a different Instances; this should pass", "\n", "            ", "torch", ".", "jit", ".", "script", "(", "g", "(", ")", ")", "\n", "with", "self", ".", "assertRaises", "(", "Exception", ")", ":", "\n", "                ", "torch", ".", "jit", ".", "script", "(", "g2", "(", ")", ")", "\n", "\n", "", "", "new_fields", "=", "{", "\"mask\"", ":", "Tensor", ",", "\"proposal_boxes\"", ":", "Boxes", "}", "\n", "with", "patch_instances", "(", "new_fields", ")", "as", "NewInstances", ":", "\n", "# get_mask will be compiled with a different Instances; this should pass", "\n", "            ", "scripted_g2", "=", "torch", ".", "jit", ".", "script", "(", "g2", "(", ")", ")", "\n", "x", "=", "NewInstances", "(", "(", "3", ",", "4", ")", ")", "\n", "x", ".", "mask", "=", "torch", ".", "rand", "(", "3", ")", "\n", "x", ".", "proposal_boxes", "=", "Boxes", "(", "torch", ".", "rand", "(", "3", ",", "4", ")", ")", "\n", "scripted_g2", "(", "x", ")", "# it should accept the new Instances object and run successfully", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_instances.TestInstances.test_script_access_fields": [[76, 87], ["unittest.skipIf", "detectron2.export.torchscript.patch_instances", "torch.jit.script", "f"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_script_access_fields", "(", "self", ")", ":", "\n", "        ", "class", "f", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "x", ":", "Instances", ")", ":", "\n", "                ", "proposal_boxes", "=", "x", ".", "proposal_boxes", "\n", "objectness_logits", "=", "x", ".", "objectness_logits", "\n", "return", "proposal_boxes", ".", "tensor", "+", "objectness_logits", "\n", "\n", "", "", "fields", "=", "{", "\"proposal_boxes\"", ":", "Boxes", ",", "\"objectness_logits\"", ":", "Tensor", "}", "\n", "with", "patch_instances", "(", "fields", ")", ":", "\n", "            ", "torch", ".", "jit", ".", "script", "(", "f", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_instances.TestInstances.test_script_len": [[88, 119], ["unittest.skipIf", "detectron2.export.torchscript.patch_instances", "torch.jit.script", "new_instance", "torch.tensor", "detectron2.structures.Boxes", "torch.jit.script.", "test_instances.TestInstances.assertEqual", "detectron2.export.torchscript.patch_instances", "torch.jit.script", "new_instance", "torch.tensor().reshape", "torch.jit.script.", "test_instances.TestInstances.assertEqual", "len", "len", "f", "test_instances.TestInstances.assertRaises", "torch.jit.script.", "g", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_script_len", "(", "self", ")", ":", "\n", "        ", "class", "f", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "x", ":", "Instances", ")", ":", "\n", "                ", "return", "len", "(", "x", ")", "\n", "\n", "", "", "class", "g", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "x", ":", "Instances", ")", ":", "\n", "                ", "return", "len", "(", "x", ")", "\n", "\n", "", "", "image_shape", "=", "(", "15", ",", "15", ")", "\n", "\n", "fields", "=", "{", "\"proposal_boxes\"", ":", "Boxes", "}", "\n", "with", "patch_instances", "(", "fields", ")", "as", "new_instance", ":", "\n", "            ", "script_module", "=", "torch", ".", "jit", ".", "script", "(", "f", "(", ")", ")", "\n", "x", "=", "new_instance", "(", "image_shape", ")", "\n", "with", "self", ".", "assertRaises", "(", "Exception", ")", ":", "\n", "                ", "script_module", "(", "x", ")", "\n", "", "box_tensors", "=", "torch", ".", "tensor", "(", "[", "[", "5", ",", "5", ",", "10", ",", "10", "]", ",", "[", "1", ",", "1", ",", "2", ",", "3", "]", "]", ")", "\n", "x", ".", "proposal_boxes", "=", "Boxes", "(", "box_tensors", ")", "\n", "length", "=", "script_module", "(", "x", ")", "\n", "self", ".", "assertEqual", "(", "length", ",", "2", ")", "\n", "\n", "", "fields", "=", "{", "\"objectness_logits\"", ":", "Tensor", "}", "\n", "with", "patch_instances", "(", "fields", ")", "as", "new_instance", ":", "\n", "            ", "script_module", "=", "torch", ".", "jit", ".", "script", "(", "g", "(", ")", ")", "\n", "x", "=", "new_instance", "(", "image_shape", ")", "\n", "objectness_logits", "=", "torch", ".", "tensor", "(", "[", "1.0", "]", ")", ".", "reshape", "(", "1", ",", "1", ")", "\n", "x", ".", "objectness_logits", "=", "objectness_logits", "\n", "length", "=", "script_module", "(", "x", ")", "\n", "self", ".", "assertEqual", "(", "length", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_instances.TestInstances.test_script_has": [[120, 136], ["unittest.skipIf", "detectron2.export.torchscript.patch_instances", "torch.jit.script", "new_instance", "test_instances.TestInstances.assertFalse", "torch.tensor", "detectron2.structures.Boxes", "test_instances.TestInstances.assertTrue", "new_instance.has", "f", "torch.jit.script.", "torch.jit.script."], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_script_has", "(", "self", ")", ":", "\n", "        ", "class", "f", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "x", ":", "Instances", ")", ":", "\n", "                ", "return", "x", ".", "has", "(", "\"proposal_boxes\"", ")", "\n", "\n", "", "", "image_shape", "=", "(", "15", ",", "15", ")", "\n", "fields", "=", "{", "\"proposal_boxes\"", ":", "Boxes", "}", "\n", "with", "patch_instances", "(", "fields", ")", "as", "new_instance", ":", "\n", "            ", "script_module", "=", "torch", ".", "jit", ".", "script", "(", "f", "(", ")", ")", "\n", "x", "=", "new_instance", "(", "image_shape", ")", "\n", "self", ".", "assertFalse", "(", "script_module", "(", "x", ")", ")", "\n", "\n", "box_tensors", "=", "torch", ".", "tensor", "(", "[", "[", "5", ",", "5", ",", "10", ",", "10", "]", ",", "[", "1", ",", "1", ",", "2", ",", "3", "]", "]", ")", "\n", "x", ".", "proposal_boxes", "=", "Boxes", "(", "box_tensors", ")", "\n", "self", ".", "assertTrue", "(", "script_module", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_instances.TestInstances.test_script_to": [[137, 154], ["unittest.skipIf", "detectron2.export.torchscript.patch_instances", "torch.jit.script", "new_instance", "torch.jit.script.", "torch.tensor", "detectron2.structures.Boxes", "torch.jit.script.", "new_instance.to", "f", "torch.device"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_script_to", "(", "self", ")", ":", "\n", "        ", "class", "f", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "x", ":", "Instances", ")", ":", "\n", "                ", "return", "x", ".", "to", "(", "torch", ".", "device", "(", "\"cpu\"", ")", ")", "\n", "\n", "", "", "image_shape", "=", "(", "15", ",", "15", ")", "\n", "fields", "=", "{", "\"proposal_boxes\"", ":", "Boxes", ",", "\"a\"", ":", "Tensor", "}", "\n", "with", "patch_instances", "(", "fields", ")", "as", "new_instance", ":", "\n", "            ", "script_module", "=", "torch", ".", "jit", ".", "script", "(", "f", "(", ")", ")", "\n", "x", "=", "new_instance", "(", "image_shape", ")", "\n", "script_module", "(", "x", ")", "\n", "\n", "box_tensors", "=", "torch", ".", "tensor", "(", "[", "[", "5", ",", "5", ",", "10", ",", "10", "]", ",", "[", "1", ",", "1", ",", "2", ",", "3", "]", "]", ")", "\n", "x", ".", "proposal_boxes", "=", "Boxes", "(", "box_tensors", ")", "\n", "x", ".", "a", "=", "box_tensors", "\n", "script_module", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_instances.TestInstances.test_script_getitem": [[155, 176], ["unittest.skipIf", "detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.rand", "torch.tensor", "torch.rand", "detectron2.export.torchscript.patch_instances", "torch.jit.script", "torch.jit.script.", "test_instances.TestInstances.assertTrue", "test_instances.TestInstances.assertTrue", "f", "f", "new_instance.from_instances", "torch.equal", "torch.equal"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_script_getitem", "(", "self", ")", ":", "\n", "        ", "class", "f", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "x", ":", "Instances", ",", "idx", ")", ":", "\n", "                ", "return", "x", "[", "idx", "]", "\n", "\n", "", "", "image_shape", "=", "(", "15", ",", "15", ")", "\n", "fields", "=", "{", "\"proposal_boxes\"", ":", "Boxes", ",", "\"a\"", ":", "Tensor", "}", "\n", "inst", "=", "Instances", "(", "image_shape", ")", "\n", "inst", ".", "proposal_boxes", "=", "Boxes", "(", "torch", ".", "rand", "(", "4", ",", "4", ")", ")", "\n", "inst", ".", "a", "=", "torch", ".", "rand", "(", "4", ",", "10", ")", "\n", "idx", "=", "torch", ".", "tensor", "(", "[", "True", ",", "False", ",", "True", ",", "False", "]", ")", "\n", "with", "patch_instances", "(", "fields", ")", "as", "new_instance", ":", "\n", "            ", "script_module", "=", "torch", ".", "jit", ".", "script", "(", "f", "(", ")", ")", "\n", "\n", "out", "=", "f", "(", ")", "(", "inst", ",", "idx", ")", "\n", "out_scripted", "=", "script_module", "(", "new_instance", ".", "from_instances", "(", "inst", ")", ",", "idx", ")", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "equal", "(", "out", ".", "proposal_boxes", ".", "tensor", ",", "out_scripted", ".", "proposal_boxes", ".", "tensor", ")", "\n", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "out", ".", "a", ",", "out_scripted", ".", "a", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_instances.TestInstances.test_from_to_instances": [[177, 189], ["unittest.skipIf", "detectron2.structures.Instances", "detectron2.structures.Boxes", "test_instances.TestInstances.assertTrue", "test_instances.TestInstances.assertTrue", "torch.rand", "detectron2.export.torchscript.patch_instances", "NewInstances.from_instances", "detectron2.utils.testing.convert_scripted_instances", "torch.equal", "torch.equal"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.convert_scripted_instances"], ["", "", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_from_to_instances", "(", "self", ")", ":", "\n", "        ", "orig", "=", "Instances", "(", "(", "30", ",", "30", ")", ")", "\n", "orig", ".", "proposal_boxes", "=", "Boxes", "(", "torch", ".", "rand", "(", "3", ",", "4", ")", ")", "\n", "\n", "fields", "=", "{", "\"proposal_boxes\"", ":", "Boxes", ",", "\"a\"", ":", "Tensor", "}", "\n", "with", "patch_instances", "(", "fields", ")", "as", "NewInstances", ":", "\n", "# convert to NewInstances and back", "\n", "            ", "new1", "=", "NewInstances", ".", "from_instances", "(", "orig", ")", "\n", "new2", "=", "convert_scripted_instances", "(", "new1", ")", "\n", "", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "orig", ".", "proposal_boxes", ".", "tensor", ",", "new1", ".", "proposal_boxes", ".", "tensor", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "orig", ".", "proposal_boxes", ".", "tensor", ",", "new2", ".", "proposal_boxes", ".", "tensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesLayer.test_iou_0_dim_cpu": [[20, 32], ["torch.rand", "torch.rand", "torch.zeros", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.rand", "torch.rand", "torch.zeros", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.allclose", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["    ", "def", "test_iou_0_dim_cpu", "(", "self", ")", ":", "\n", "        ", "boxes1", "=", "torch", ".", "rand", "(", "0", ",", "5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "boxes2", "=", "torch", ".", "rand", "(", "10", ",", "5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "expected_ious", "=", "torch", ".", "zeros", "(", "0", ",", "10", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "ious", "=", "pairwise_iou_rotated", "(", "boxes1", ",", "boxes2", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n", "boxes1", "=", "torch", ".", "rand", "(", "10", ",", "5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "boxes2", "=", "torch", ".", "rand", "(", "0", ",", "5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "expected_ious", "=", "torch", ".", "zeros", "(", "10", ",", "0", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "ious", "=", "pairwise_iou_rotated", "(", "boxes1", ",", "boxes2", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesLayer.test_iou_0_dim_cuda": [[33, 46], ["unittest.skipIf", "torch.rand", "torch.rand", "torch.zeros", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.rand", "torch.rand", "torch.zeros", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.rand.cuda", "torch.rand.cuda", "torch.allclose", "torch.rand.cuda", "torch.rand.cuda", "torch.allclose", "torch.cuda.is_available", "detectron2.layers.rotated_boxes.pairwise_iou_rotated.cpu", "detectron2.layers.rotated_boxes.pairwise_iou_rotated.cpu"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_iou_0_dim_cuda", "(", "self", ")", ":", "\n", "        ", "boxes1", "=", "torch", ".", "rand", "(", "0", ",", "5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "boxes2", "=", "torch", ".", "rand", "(", "10", ",", "5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "expected_ious", "=", "torch", ".", "zeros", "(", "0", ",", "10", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "ious_cuda", "=", "pairwise_iou_rotated", "(", "boxes1", ".", "cuda", "(", ")", ",", "boxes2", ".", "cuda", "(", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious_cuda", ".", "cpu", "(", ")", ",", "expected_ious", ")", ")", "\n", "\n", "boxes1", "=", "torch", ".", "rand", "(", "10", ",", "5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "boxes2", "=", "torch", ".", "rand", "(", "0", ",", "5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "expected_ious", "=", "torch", ".", "zeros", "(", "10", ",", "0", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "ious_cuda", "=", "pairwise_iou_rotated", "(", "boxes1", ".", "cuda", "(", ")", ",", "boxes2", ".", "cuda", "(", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious_cuda", ".", "cpu", "(", ")", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesLayer.test_iou_half_overlap_cpu": [[47, 53], ["torch.tensor", "torch.tensor", "torch.tensor", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "def", "test_iou_half_overlap_cpu", "(", "self", ")", ":", "\n", "        ", "boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "0.5", ",", "0.5", ",", "1.0", ",", "1.0", ",", "0.0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "[", "[", "0.25", ",", "0.5", ",", "0.5", ",", "1.0", ",", "0.0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "0.5", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "ious", "=", "pairwise_iou_rotated", "(", "boxes1", ",", "boxes2", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesLayer.test_iou_half_overlap_cuda": [[54, 61], ["unittest.skipIf", "torch.tensor", "torch.tensor", "torch.tensor", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.tensor.cuda", "torch.tensor.cuda", "torch.allclose", "torch.cuda.is_available", "detectron2.layers.rotated_boxes.pairwise_iou_rotated.cpu"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_iou_half_overlap_cuda", "(", "self", ")", ":", "\n", "        ", "boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "0.5", ",", "0.5", ",", "1.0", ",", "1.0", ",", "0.0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "[", "[", "0.25", ",", "0.5", ",", "0.5", ",", "1.0", ",", "0.0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "0.5", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "ious_cuda", "=", "pairwise_iou_rotated", "(", "boxes1", ".", "cuda", "(", ")", ",", "boxes2", ".", "cuda", "(", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious_cuda", ".", "cpu", "(", ")", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesLayer.test_iou_precision": [[62, 70], ["torch.tensor", "torch.tensor", "torch.tensor", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.cuda.is_available", "torch.allclose", "detectron2.layers.rotated_boxes.pairwise_iou_rotated.cpu"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "def", "test_iou_precision", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "565", ",", "565", ",", "10", ",", "10.0", ",", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "[", "[", "565", ",", "565", ",", "10", ",", "8.3", ",", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "iou", "=", "8.3", "/", "10.0", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "iou", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "ious", "=", "pairwise_iou_rotated", "(", "boxes1", ",", "boxes2", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ".", "cpu", "(", ")", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesLayer.test_iou_too_many_boxes_cuda": [[71, 78], ["unittest.skipIf", "torch.zeros", "torch.zeros", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "test_rotated_boxes.TestRotatedBoxesLayer.assertTupleEqual", "torch.zeros.cuda", "torch.zeros.cuda", "tuple", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_iou_too_many_boxes_cuda", "(", "self", ")", ":", "\n", "        ", "s1", ",", "s2", "=", "5", ",", "1289035", "\n", "boxes1", "=", "torch", ".", "zeros", "(", "s1", ",", "5", ")", "\n", "boxes2", "=", "torch", ".", "zeros", "(", "s2", ",", "5", ")", "\n", "ious_cuda", "=", "pairwise_iou_rotated", "(", "boxes1", ".", "cuda", "(", ")", ",", "boxes2", ".", "cuda", "(", ")", ")", "\n", "self", ".", "assertTupleEqual", "(", "tuple", "(", "ious_cuda", ".", "shape", ")", ",", "(", "s1", ",", "s2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesLayer.test_iou_extreme": [[79, 97], ["torch.tensor", "torch.tensor", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.cuda.is_available", "detectron2.layers.rotated_boxes.pairwise_iou_rotated.min"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "def", "test_iou_extreme", "(", "self", ")", ":", "\n", "# Cause floating point issues in cuda kernels (#1266)", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "160.0", ",", "153.0", ",", "230.0", ",", "23.0", ",", "-", "37.0", "]", "]", ",", "device", "=", "device", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "\n", "-", "1.117407639806935e17", ",", "\n", "1.3858420478349148e18", ",", "\n", "1000.0000610351562", ",", "\n", "1000.0000610351562", ",", "\n", "1612.0", ",", "\n", "]", "\n", "]", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "ious", "=", "pairwise_iou_rotated", "(", "boxes1", ",", "boxes2", ")", "\n", "self", ".", "assertTrue", "(", "ious", ".", "min", "(", ")", ">=", "0", ",", "ious", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesLayer.test_iou_issue_2154": [[98, 119], ["torch.tensor", "torch.tensor", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "torch.tensor", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.cuda.is_available", "torch.allclose", "detectron2.layers.rotated_boxes.pairwise_iou_rotated.cpu"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "", "def", "test_iou_issue_2154", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "\n", "296.6620178222656", ",", "\n", "458.73883056640625", ",", "\n", "23.515729904174805", ",", "\n", "47.677001953125", ",", "\n", "0.08795166015625", ",", "\n", "]", "\n", "]", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "296.66201", ",", "458.73882000000003", ",", "23.51573", ",", "47.67702", ",", "0.087951", "]", "]", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "ious", "=", "pairwise_iou_rotated", "(", "boxes1", ",", "boxes2", ")", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "1.0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ".", "cpu", "(", ")", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesLayer.test_iou_issue_2167": [[120, 149], ["torch.tensor", "torch.tensor", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "torch.tensor", "test_rotated_boxes.TestRotatedBoxesLayer.assertTrue", "torch.cuda.is_available", "torch.allclose", "detectron2.layers.rotated_boxes.pairwise_iou_rotated.cpu"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "", "def", "test_iou_issue_2167", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "\n", "2563.74462890625000000000", ",", "\n", "1436.79016113281250000000", ",", "\n", "2174.70336914062500000000", ",", "\n", "214.09500122070312500000", ",", "\n", "115.11834716796875000000", ",", "\n", "]", "\n", "]", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "\n", "2563.74462890625000000000", ",", "\n", "1436.79028320312500000000", ",", "\n", "2174.70288085937500000000", ",", "\n", "214.09495544433593750000", ",", "\n", "115.11835479736328125000", ",", "\n", "]", "\n", "]", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "ious", "=", "pairwise_iou_rotated", "(", "boxes1", ",", "boxes2", ")", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "1.0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ".", "cpu", "(", ")", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_clip_area_0_degree": [[152, 180], ["range", "torch.zeros", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.zeros", "detectron2.structures.boxes.Boxes", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.boxes.Boxes.area", "detectron2.structures.rotated_boxes.RotatedBoxes.area", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "detectron2.structures.boxes.Boxes.clip", "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "detectron2.structures.boxes.Boxes.area", "detectron2.structures.rotated_boxes.RotatedBoxes.area", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "torch.allclose", "torch.allclose", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area"], ["    ", "def", "test_clip_area_0_degree", "(", "self", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "50", ")", ":", "\n", "            ", "num_boxes", "=", "100", "\n", "boxes_5d", "=", "torch", ".", "zeros", "(", "num_boxes", ",", "5", ")", "\n", "boxes_5d", "[", ":", ",", "0", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "100", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "1", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "100", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "2", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "0", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "3", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "0", ",", "500", ")", "\n", "# Convert from (x_ctr, y_ctr, w, h, 0) to  (x1, y1, x2, y2)", "\n", "boxes_4d", "=", "torch", ".", "zeros", "(", "num_boxes", ",", "4", ")", "\n", "boxes_4d", "[", ":", ",", "0", "]", "=", "boxes_5d", "[", ":", ",", "0", "]", "-", "boxes_5d", "[", ":", ",", "2", "]", "/", "2.0", "\n", "boxes_4d", "[", ":", ",", "1", "]", "=", "boxes_5d", "[", ":", ",", "1", "]", "-", "boxes_5d", "[", ":", ",", "3", "]", "/", "2.0", "\n", "boxes_4d", "[", ":", ",", "2", "]", "=", "boxes_5d", "[", ":", ",", "0", "]", "+", "boxes_5d", "[", ":", ",", "2", "]", "/", "2.0", "\n", "boxes_4d", "[", ":", ",", "3", "]", "=", "boxes_5d", "[", ":", ",", "1", "]", "+", "boxes_5d", "[", ":", ",", "3", "]", "/", "2.0", "\n", "\n", "image_size", "=", "(", "500", ",", "600", ")", "\n", "test_boxes_4d", "=", "Boxes", "(", "boxes_4d", ")", "\n", "test_boxes_5d", "=", "RotatedBoxes", "(", "boxes_5d", ")", "\n", "# Before clip", "\n", "areas_4d", "=", "test_boxes_4d", ".", "area", "(", ")", "\n", "areas_5d", "=", "test_boxes_5d", ".", "area", "(", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "areas_4d", ",", "areas_5d", ",", "atol", "=", "1e-1", ",", "rtol", "=", "1e-5", ")", ")", "\n", "# After clip", "\n", "test_boxes_4d", ".", "clip", "(", "image_size", ")", "\n", "test_boxes_5d", ".", "clip", "(", "image_size", ")", "\n", "areas_4d", "=", "test_boxes_4d", ".", "area", "(", ")", "\n", "areas_5d", "=", "test_boxes_5d", ".", "area", "(", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "areas_4d", ",", "areas_5d", ",", "atol", "=", "1e-1", ",", "rtol", "=", "1e-5", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_clip_area_arbitrary_angle": [[181, 207], ["torch.zeros", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "random.uniform", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes.area", "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "detectron2.structures.rotated_boxes.RotatedBoxes.area", "torch.all", "torch.all", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.abs", "torch.where"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area"], ["", "", "def", "test_clip_area_arbitrary_angle", "(", "self", ")", ":", "\n", "        ", "num_boxes", "=", "100", "\n", "boxes_5d", "=", "torch", ".", "zeros", "(", "num_boxes", ",", "5", ")", "\n", "boxes_5d", "[", ":", ",", "0", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "100", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "1", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "100", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "2", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "0", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "3", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "0", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "4", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "1800", ",", "1800", ")", "\n", "clip_angle_threshold", "=", "random", ".", "uniform", "(", "0", ",", "180", ")", "\n", "\n", "image_size", "=", "(", "500", ",", "600", ")", "\n", "test_boxes_5d", "=", "RotatedBoxes", "(", "boxes_5d", ")", "\n", "# Before clip", "\n", "areas_before", "=", "test_boxes_5d", ".", "area", "(", ")", "\n", "# After clip", "\n", "test_boxes_5d", ".", "clip", "(", "image_size", ",", "clip_angle_threshold", ")", "\n", "areas_diff", "=", "test_boxes_5d", ".", "area", "(", ")", "-", "areas_before", "\n", "\n", "# the areas should only decrease after clipping", "\n", "self", ".", "assertTrue", "(", "torch", ".", "all", "(", "areas_diff", "<=", "0", ")", ")", "\n", "# whenever the box is clipped (thus the area shrinks),", "\n", "# the angle for the box must be within the clip_angle_threshold", "\n", "# Note that the clip function will normalize the angle range", "\n", "# to be within (-180, 180]", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "all", "(", "torch", ".", "abs", "(", "boxes_5d", "[", ":", ",", "4", "]", "[", "torch", ".", "where", "(", "areas_diff", "<", "0", ")", "]", ")", "<", "clip_angle_threshold", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_normalize_angles": [[209, 241], ["range", "torch.zeros", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes.clone", "detectron2.structures.rotated_boxes.RotatedBoxes.clone.normalize_angles", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "torch.all", "torch.all", "torch.allclose", "torch.allclose", "torch.allclose", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.cos", "torch.cos", "torch.sin", "torch.sin"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.rotated_boxes.RotatedBoxes.normalize_angles"], ["", "def", "test_normalize_angles", "(", "self", ")", ":", "\n", "# torch.manual_seed(0)", "\n", "        ", "for", "_", "in", "range", "(", "50", ")", ":", "\n", "            ", "num_boxes", "=", "100", "\n", "boxes_5d", "=", "torch", ".", "zeros", "(", "num_boxes", ",", "5", ")", "\n", "boxes_5d", "[", ":", ",", "0", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "100", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "1", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "100", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "2", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "0", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "3", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "0", ",", "500", ")", "\n", "boxes_5d", "[", ":", ",", "4", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "1800", ",", "1800", ")", "\n", "rotated_boxes", "=", "RotatedBoxes", "(", "boxes_5d", ")", "\n", "normalized_boxes", "=", "rotated_boxes", ".", "clone", "(", ")", "\n", "normalized_boxes", ".", "normalize_angles", "(", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "all", "(", "normalized_boxes", ".", "tensor", "[", ":", ",", "4", "]", ">=", "-", "180", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "all", "(", "normalized_boxes", ".", "tensor", "[", ":", ",", "4", "]", "<", "180", ")", ")", "\n", "# x, y, w, h should not change", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "boxes_5d", "[", ":", ",", ":", "4", "]", ",", "normalized_boxes", ".", "tensor", "[", ":", ",", ":", "4", "]", ")", ")", "\n", "# the cos/sin values of the angles should stay the same", "\n", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "allclose", "(", "\n", "torch", ".", "cos", "(", "boxes_5d", "[", ":", ",", "4", "]", "*", "math", ".", "pi", "/", "180", ")", ",", "\n", "torch", ".", "cos", "(", "normalized_boxes", ".", "tensor", "[", ":", ",", "4", "]", "*", "math", ".", "pi", "/", "180", ")", ",", "\n", "atol", "=", "1e-5", ",", "\n", ")", "\n", ")", "\n", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "allclose", "(", "\n", "torch", ".", "sin", "(", "boxes_5d", "[", ":", ",", "4", "]", "*", "math", ".", "pi", "/", "180", ")", ",", "\n", "torch", ".", "sin", "(", "normalized_boxes", ".", "tensor", "[", ":", ",", "4", "]", "*", "math", ".", "pi", "/", "180", ")", ",", "\n", "atol", "=", "1e-5", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_pairwise_iou_0_degree": [[244, 273], ["torch.tensor", "torch.tensor", "torch.tensor", "detectron2.structures.rotated_boxes.pairwise_iou", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "torch.cuda.is_available", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "", "def", "test_pairwise_iou_0_degree", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "0.5", ",", "0.5", ",", "1.0", ",", "1.0", ",", "0.0", "]", ",", "[", "0.5", ",", "0.5", ",", "1.0", ",", "1.0", ",", "0.0", "]", "]", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "0.5", ",", "0.5", ",", "1.0", ",", "1.0", ",", "0.0", "]", ",", "\n", "[", "0.25", ",", "0.5", ",", "0.5", ",", "1.0", ",", "0.0", "]", ",", "\n", "[", "0.5", ",", "0.25", ",", "1.0", ",", "0.5", ",", "0.0", "]", ",", "\n", "[", "0.25", ",", "0.25", ",", "0.5", ",", "0.5", ",", "0.0", "]", ",", "\n", "[", "0.75", ",", "0.75", ",", "0.5", ",", "0.5", ",", "0.0", "]", ",", "\n", "[", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", ",", "0.0", "]", ",", "\n", "]", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "1.0", ",", "0.5", ",", "0.5", ",", "0.25", ",", "0.25", ",", "0.25", "/", "(", "2", "-", "0.25", ")", "]", ",", "\n", "[", "1.0", ",", "0.5", ",", "0.5", ",", "0.25", ",", "0.25", ",", "0.25", "/", "(", "2", "-", "0.25", ")", "]", ",", "\n", "]", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "ious", "=", "pairwise_iou", "(", "RotatedBoxes", "(", "boxes1", ")", ",", "RotatedBoxes", "(", "boxes2", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_pairwise_iou_45_degrees": [[274, 288], ["torch.tensor", "torch.tensor", "torch.tensor", "detectron2.structures.rotated_boxes.pairwise_iou", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "torch.cuda.is_available", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes", "torch.allclose", "math.sqrt", "math.sqrt", "math.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "", "def", "test_pairwise_iou_45_degrees", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "1", ",", "1", ",", "math", ".", "sqrt", "(", "2", ")", ",", "math", ".", "sqrt", "(", "2", ")", ",", "45", "]", ",", "\n", "[", "1", ",", "1", ",", "2", "*", "math", ".", "sqrt", "(", "2", ")", ",", "2", "*", "math", ".", "sqrt", "(", "2", ")", ",", "-", "45", "]", ",", "\n", "]", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "[", "[", "1", ",", "1", ",", "2", ",", "2", ",", "0", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "0.5", "]", ",", "[", "0.5", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "ious", "=", "pairwise_iou", "(", "RotatedBoxes", "(", "boxes1", ")", ",", "RotatedBoxes", "(", "boxes2", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_pairwise_iou_orthogonal": [[289, 297], ["torch.tensor", "torch.tensor", "torch.tensor", "detectron2.structures.rotated_boxes.pairwise_iou", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "torch.cuda.is_available", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "", "def", "test_pairwise_iou_orthogonal", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "5", ",", "5", ",", "10", ",", "6", ",", "55", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "[", "[", "5", ",", "5", ",", "10", ",", "6", ",", "-", "35", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "iou", "=", "(", "6.0", "*", "6.0", ")", "/", "(", "6.0", "*", "6.0", "+", "4.0", "*", "6.0", "+", "4.0", "*", "6.0", ")", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "iou", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "ious", "=", "pairwise_iou", "(", "RotatedBoxes", "(", "boxes1", ")", ",", "RotatedBoxes", "(", "boxes2", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_pairwise_iou_large_close_boxes": [[298, 314], ["torch.tensor", "torch.tensor", "torch.tensor", "detectron2.structures.rotated_boxes.pairwise_iou", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "torch.cuda.is_available", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "", "def", "test_pairwise_iou_large_close_boxes", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "299.500000", ",", "417.370422", ",", "600.000000", ",", "364.259186", ",", "27.1828", "]", "]", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "299.500000", ",", "417.370422", ",", "600.000000", ",", "364.259155", ",", "27.1828", "]", "]", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "iou", "=", "364.259155", "/", "364.259186", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "iou", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "ious", "=", "pairwise_iou", "(", "RotatedBoxes", "(", "boxes1", ")", ",", "RotatedBoxes", "(", "boxes2", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_pairwise_iou_many_boxes": [[315, 344], ["torch.stack", "torch.stack", "torch.zeros", "range", "detectron2.structures.rotated_boxes.pairwise_iou", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "torch.cuda.is_available", "min", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes", "torch.allclose", "torch.tensor", "torch.tensor", "range", "range"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "", "def", "test_pairwise_iou_many_boxes", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "            ", "num_boxes1", "=", "100", "\n", "num_boxes2", "=", "200", "\n", "boxes1", "=", "torch", ".", "stack", "(", "\n", "[", "\n", "torch", ".", "tensor", "(", "\n", "[", "5", "+", "20", "*", "i", ",", "5", "+", "20", "*", "i", ",", "10", ",", "10", ",", "0", "]", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "num_boxes1", ")", "\n", "]", "\n", ")", "\n", "boxes2", "=", "torch", ".", "stack", "(", "\n", "[", "\n", "torch", ".", "tensor", "(", "\n", "[", "5", "+", "20", "*", "i", ",", "5", "+", "20", "*", "i", ",", "10", ",", "1", "+", "9", "*", "i", "/", "num_boxes2", ",", "0", "]", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "num_boxes2", ")", "\n", "]", "\n", ")", "\n", "expected_ious", "=", "torch", ".", "zeros", "(", "num_boxes1", ",", "num_boxes2", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "for", "i", "in", "range", "(", "min", "(", "num_boxes1", ",", "num_boxes2", ")", ")", ":", "\n", "                ", "expected_ious", "[", "i", "]", "[", "i", "]", "=", "(", "1", "+", "9", "*", "i", "/", "num_boxes2", ")", "/", "10.0", "\n", "", "ious", "=", "pairwise_iou", "(", "RotatedBoxes", "(", "boxes1", ")", ",", "RotatedBoxes", "(", "boxes2", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_pairwise_iou_issue1207_simplified": [[345, 355], ["torch.tensor", "torch.tensor", "torch.tensor", "detectron2.structures.rotated_boxes.pairwise_iou", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "torch.cuda.is_available", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "", "def", "test_pairwise_iou_issue1207_simplified", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "# Simplified test case of D2-issue-1207", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "3", ",", "3", ",", "8", ",", "2", ",", "-", "45.0", "]", "]", ",", "device", "=", "device", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "[", "[", "6", ",", "0", ",", "8", ",", "2", ",", "-", "45.0", "]", "]", ",", "device", "=", "device", ")", "\n", "iou", "=", "0.0", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "iou", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "\n", "ious", "=", "pairwise_iou", "(", "RotatedBoxes", "(", "boxes1", ")", ",", "RotatedBoxes", "(", "boxes2", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_pairwise_iou_issue1207": [[356, 367], ["torch.tensor", "torch.tensor", "torch.tensor", "detectron2.structures.rotated_boxes.pairwise_iou", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "torch.cuda.is_available", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes", "torch.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou"], ["", "", "def", "test_pairwise_iou_issue1207", "(", "self", ")", ":", "\n", "        ", "for", "device", "in", "[", "\"cpu\"", "]", "+", "(", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ")", ":", "\n", "# The original test case in D2-issue-1207", "\n", "            ", "boxes1", "=", "torch", ".", "tensor", "(", "[", "[", "160.0", ",", "153.0", ",", "230.0", ",", "23.0", ",", "-", "37.0", "]", "]", ",", "device", "=", "device", ")", "\n", "boxes2", "=", "torch", ".", "tensor", "(", "[", "[", "190.0", ",", "127.0", ",", "80.0", ",", "21.0", ",", "-", "46.0", "]", "]", ",", "device", "=", "device", ")", "\n", "\n", "iou", "=", "0.0", "\n", "expected_ious", "=", "torch", ".", "tensor", "(", "[", "[", "iou", "]", "]", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "device", ")", "\n", "\n", "ious", "=", "pairwise_iou", "(", "RotatedBoxes", "(", "boxes1", ")", ",", "RotatedBoxes", "(", "boxes2", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "ious", ",", "expected_ious", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_empty_cat": [[368, 371], ["detectron2.structures.rotated_boxes.RotatedBoxes.cat", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "", "def", "test_empty_cat", "(", "self", ")", ":", "\n", "        ", "x", "=", "RotatedBoxes", ".", "cat", "(", "[", "]", ")", "\n", "self", ".", "assertTrue", "(", "x", ".", "tensor", ".", "shape", ",", "(", "0", ",", "5", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.TestRotatedBoxesStructure.test_scriptability": [[372, 396], ["unittest.skipIf", "torch.jit.script", "detectron2.utils.testing.reload_script_model", "torch.jit.script.", "torch.rand", "torch.jit.script", "torch.jit.script.", "test_rotated_boxes.TestRotatedBoxesStructure.assertTrue", "detectron2.structures.rotated_boxes.RotatedBoxes", "torch.rand", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes", "detectron2.structures.rotated_boxes.RotatedBoxes.cat", "torch.equal", "detectron2.structures.rotated_boxes.RotatedBoxes.to", "detectron2.structures.rotated_boxes.RotatedBoxes.area", "torch.cat", "torch.device"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.reload_script_model", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "8", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_scriptability", "(", "self", ")", ":", "\n", "        ", "def", "func", "(", "x", ")", ":", "\n", "            ", "boxes", "=", "RotatedBoxes", "(", "x", ")", "\n", "test", "=", "boxes", ".", "to", "(", "torch", ".", "device", "(", "\"cpu\"", ")", ")", ".", "tensor", "\n", "return", "boxes", ".", "area", "(", ")", ",", "test", "\n", "\n", "", "f", "=", "torch", ".", "jit", ".", "script", "(", "func", ")", "\n", "f", "=", "reload_script_model", "(", "f", ")", "\n", "f", "(", "torch", ".", "rand", "(", "(", "3", ",", "5", ")", ")", ")", "\n", "\n", "data", "=", "torch", ".", "rand", "(", "(", "3", ",", "5", ")", ")", "\n", "\n", "def", "func_cat", "(", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "            ", "boxes1", "=", "RotatedBoxes", "(", "x", ")", "\n", "boxes2", "=", "RotatedBoxes", "(", "x", ")", "\n", "# this is not supported by torchscript for now.", "\n", "# boxes3 = RotatedBoxes.cat([boxes1, boxes2])", "\n", "boxes3", "=", "boxes1", ".", "cat", "(", "[", "boxes1", ",", "boxes2", "]", ")", "\n", "return", "boxes3", "\n", "\n", "", "f", "=", "torch", ".", "jit", ".", "script", "(", "func_cat", ")", "\n", "script_box", "=", "f", "(", "data", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "torch", ".", "cat", "(", "[", "data", ",", "data", "]", ")", ",", "script_box", ".", "tensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.structures.test_rotated_boxes.benchmark_rotated_iou": [[398, 435], ["torch.stack", "torch.stack", "torch.cuda.is_available", "fvcore.common.benchmark.benchmark", "torch.stack.to", "torch.stack.to", "args.append", "torch.tensor", "torch.tensor", "range", "torch.device", "range", "range", "detectron2.layers.rotated_boxes.pairwise_iou_rotated", "torch.cuda.synchronize", "torch.device"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["", "", "def", "benchmark_rotated_iou", "(", ")", ":", "\n", "    ", "num_boxes1", "=", "200", "\n", "num_boxes2", "=", "500", "\n", "boxes1", "=", "torch", ".", "stack", "(", "\n", "[", "\n", "torch", ".", "tensor", "(", "[", "5", "+", "20", "*", "i", ",", "5", "+", "20", "*", "i", ",", "10", ",", "10", ",", "0", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "for", "i", "in", "range", "(", "num_boxes1", ")", "\n", "]", "\n", ")", "\n", "boxes2", "=", "torch", ".", "stack", "(", "\n", "[", "\n", "torch", ".", "tensor", "(", "\n", "[", "5", "+", "20", "*", "i", ",", "5", "+", "20", "*", "i", ",", "10", ",", "1", "+", "9", "*", "i", "/", "num_boxes2", ",", "0", "]", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "num_boxes2", ")", "\n", "]", "\n", ")", "\n", "\n", "def", "func", "(", "dev", ",", "n", "=", "1", ")", ":", "\n", "        ", "b1", "=", "boxes1", ".", "to", "(", "device", "=", "dev", ")", "\n", "b2", "=", "boxes2", ".", "to", "(", "device", "=", "dev", ")", "\n", "\n", "def", "bench", "(", ")", ":", "\n", "            ", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "                ", "pairwise_iou_rotated", "(", "b1", ",", "b2", ")", "\n", "", "if", "dev", ".", "type", "==", "\"cuda\"", ":", "\n", "                ", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "\n", "", "", "return", "bench", "\n", "\n", "# only run it once per timed loop, since it's slow", "\n", "", "args", "=", "[", "{", "\"dev\"", ":", "torch", ".", "device", "(", "\"cpu\"", ")", ",", "\"n\"", ":", "1", "}", "]", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "args", ".", "append", "(", "{", "\"dev\"", ":", "torch", ".", "device", "(", "\"cuda\"", ")", ",", "\"n\"", ":", "10", "}", ")", "\n", "\n", "", "benchmark", "(", "func", ",", "\"rotated_iou\"", ",", "args", ",", "warmup_iters", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode._open_cfg": [[24, 27], ["detectron2.utils.file_io.PathManager.open"], "methods", ["None"], ["@", "classmethod", "\n", "def", "_open_cfg", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "return", "PathManager", ".", "open", "(", "filename", ",", "\"r\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file": [[29, 71], ["detectron2.utils.file_io.PathManager.isfile", "config.CfgNode.load_yaml_with_base", "logging.getLogger", "config.CfgNode.get", "type", "guess_version", "config.CfgNode.merge_from_other_cfg", "logging.getLogger.warning", "downgrade_config", "downgrade_config.merge_from_other_cfg", "upgrade_config", "config.CfgNode.clear", "config.CfgNode.update"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.guess_version", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.downgrade_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.upgrade_config"], ["", "def", "merge_from_file", "(", "self", ",", "cfg_filename", ":", "str", ",", "allow_unsafe", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "assert", "PathManager", ".", "isfile", "(", "cfg_filename", ")", ",", "f\"Config file '{cfg_filename}' does not exist!\"", "\n", "loaded_cfg", "=", "self", ".", "load_yaml_with_base", "(", "cfg_filename", ",", "allow_unsafe", "=", "allow_unsafe", ")", "\n", "loaded_cfg", "=", "type", "(", "self", ")", "(", "loaded_cfg", ")", "\n", "\n", "# defaults.py needs to import CfgNode", "\n", "from", ".", "defaults", "import", "_C", "\n", "\n", "latest_ver", "=", "_C", ".", "VERSION", "\n", "assert", "(", "\n", "latest_ver", "==", "self", ".", "VERSION", "\n", ")", ",", "\"CfgNode.merge_from_file is only allowed on a config object of latest version!\"", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n", "loaded_ver", "=", "loaded_cfg", ".", "get", "(", "\"VERSION\"", ",", "None", ")", "\n", "if", "loaded_ver", "is", "None", ":", "\n", "            ", "from", ".", "compat", "import", "guess_version", "\n", "\n", "loaded_ver", "=", "guess_version", "(", "loaded_cfg", ",", "cfg_filename", ")", "\n", "", "assert", "loaded_ver", "<=", "self", ".", "VERSION", ",", "\"Cannot merge a v{} config into a v{} config.\"", ".", "format", "(", "\n", "loaded_ver", ",", "self", ".", "VERSION", "\n", ")", "\n", "\n", "if", "loaded_ver", "==", "self", ".", "VERSION", ":", "\n", "            ", "self", ".", "merge_from_other_cfg", "(", "loaded_cfg", ")", "\n", "", "else", ":", "\n", "# compat.py needs to import CfgNode", "\n", "            ", "from", ".", "compat", "import", "upgrade_config", ",", "downgrade_config", "\n", "\n", "logger", ".", "warning", "(", "\n", "\"Loading an old v{} config file '{}' by automatically upgrading to v{}. \"", "\n", "\"See docs/CHANGELOG.md for instructions to update your files.\"", ".", "format", "(", "\n", "loaded_ver", ",", "cfg_filename", ",", "self", ".", "VERSION", "\n", ")", "\n", ")", "\n", "# To convert, first obtain a full config at an old version", "\n", "old_self", "=", "downgrade_config", "(", "self", ",", "to_version", "=", "loaded_ver", ")", "\n", "old_self", ".", "merge_from_other_cfg", "(", "loaded_cfg", ")", "\n", "new_config", "=", "upgrade_config", "(", "old_self", ")", "\n", "self", ".", "clear", "(", ")", "\n", "self", ".", "update", "(", "new_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump": [[72, 79], ["super().dump"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump"], ["", "", "def", "dump", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            str: a yaml string representation of the config\n        \"\"\"", "\n", "# to make it show up in docs", "\n", "return", "super", "(", ")", ".", "dump", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg": [[84, 94], ["_C.clone"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["def", "get_cfg", "(", ")", "->", "CfgNode", ":", "\n", "    ", "\"\"\"\n    Get a copy of the default config.\n\n    Returns:\n        a detectron2 CfgNode instance.\n    \"\"\"", "\n", "from", ".", "defaults", "import", "_C", "\n", "\n", "return", "_C", ".", "clone", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.set_global_cfg": [[96, 113], ["global_cfg.clear", "global_cfg.update"], "function", ["None"], ["", "def", "set_global_cfg", "(", "cfg", ":", "CfgNode", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Let the global config point to the given cfg.\n\n    Assume that the given \"cfg\" has the key \"KEY\", after calling\n    `set_global_cfg(cfg)`, the key can be accessed by:\n    ::\n        from detectron2.config import global_cfg\n        print(global_cfg.KEY)\n\n    By using a hacky global config, you can access these configs anywhere,\n    without having to pass the config object or the values deep into the code.\n    This is a hacky feature introduced for quick prototyping / research exploration.\n    \"\"\"", "\n", "global", "global_cfg", "\n", "global_cfg", ".", "clear", "(", ")", "\n", "global_cfg", ".", "update", "(", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.configurable": [[115, 200], ["functools.wraps", "inspect.isfunction", "inspect.isfunction", "config._called_with_cfg", "functools.wraps", "inspect.ismethod", "TypeError", "config._get_args_from_config", "init_func", "init_func", "config._called_with_cfg", "type", "AttributeError", "config._get_args_from_config", "orig_func", "orig_func"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config._called_with_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config._get_args_from_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config._called_with_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config._get_args_from_config"], ["", "def", "configurable", "(", "init_func", "=", "None", ",", "*", ",", "from_config", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Decorate a function or a class's __init__ method so that it can be called\n    with a :class:`CfgNode` object using a :func:`from_config` function that translates\n    :class:`CfgNode` to arguments.\n\n    Examples:\n    ::\n        # Usage 1: Decorator on __init__:\n        class A:\n            @configurable\n            def __init__(self, a, b=2, c=3):\n                pass\n\n            @classmethod\n            def from_config(cls, cfg):   # 'cfg' must be the first argument\n                # Returns kwargs to be passed to __init__\n                return {\"a\": cfg.A, \"b\": cfg.B}\n\n        a1 = A(a=1, b=2)  # regular construction\n        a2 = A(cfg)       # construct with a cfg\n        a3 = A(cfg, b=3, c=4)  # construct with extra overwrite\n\n        # Usage 2: Decorator on any function. Needs an extra from_config argument:\n        @configurable(from_config=lambda cfg: {\"a: cfg.A, \"b\": cfg.B})\n        def a_func(a, b=2, c=3):\n            pass\n\n        a1 = a_func(a=1, b=2)  # regular call\n        a2 = a_func(cfg)       # call with a cfg\n        a3 = a_func(cfg, b=3, c=4)  # call with extra overwrite\n\n    Args:\n        init_func (callable): a class's ``__init__`` method in usage 1. The\n            class must have a ``from_config`` classmethod which takes `cfg` as\n            the first argument.\n        from_config (callable): the from_config function in usage 2. It must take `cfg`\n            as its first argument.\n    \"\"\"", "\n", "\n", "if", "init_func", "is", "not", "None", ":", "\n", "        ", "assert", "(", "\n", "inspect", ".", "isfunction", "(", "init_func", ")", "\n", "and", "from_config", "is", "None", "\n", "and", "init_func", ".", "__name__", "==", "\"__init__\"", "\n", ")", ",", "\"Incorrect use of @configurable. Check API documentation for examples.\"", "\n", "\n", "@", "functools", ".", "wraps", "(", "init_func", ")", "\n", "def", "wrapped", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "from_config_func", "=", "type", "(", "self", ")", ".", "from_config", "\n", "", "except", "AttributeError", "as", "e", ":", "\n", "                ", "raise", "AttributeError", "(", "\n", "\"Class with @configurable must have a 'from_config' classmethod.\"", "\n", ")", "from", "e", "\n", "", "if", "not", "inspect", ".", "ismethod", "(", "from_config_func", ")", ":", "\n", "                ", "raise", "TypeError", "(", "\"Class with @configurable must have a 'from_config' classmethod.\"", ")", "\n", "\n", "", "if", "_called_with_cfg", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "                ", "explicit_args", "=", "_get_args_from_config", "(", "from_config_func", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "init_func", "(", "self", ",", "**", "explicit_args", ")", "\n", "", "else", ":", "\n", "                ", "init_func", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "return", "wrapped", "\n", "\n", "", "else", ":", "\n", "        ", "if", "from_config", "is", "None", ":", "\n", "            ", "return", "configurable", "# @configurable() is made equivalent to @configurable", "\n", "", "assert", "inspect", ".", "isfunction", "(", "\n", "from_config", "\n", ")", ",", "\"from_config argument of configurable must be a function!\"", "\n", "\n", "def", "wrapper", "(", "orig_func", ")", ":", "\n", "            ", "@", "functools", ".", "wraps", "(", "orig_func", ")", "\n", "def", "wrapped", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "                ", "if", "_called_with_cfg", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "                    ", "explicit_args", "=", "_get_args_from_config", "(", "from_config", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "return", "orig_func", "(", "**", "explicit_args", ")", "\n", "", "else", ":", "\n", "                    ", "return", "orig_func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "return", "wrapped", "\n", "\n", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.config._get_args_from_config": [[202, 233], ["inspect.signature", "any", "inspect.isfunction", "TypeError", "from_config_func", "set", "list", "from_config_func", "from_config_func.update", "list", "inspect.signature.parameters.keys", "kwargs.keys", "inspect.signature.parameters.keys", "inspect.signature.parameters.values", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "", "def", "_get_args_from_config", "(", "from_config_func", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Use `from_config` to obtain explicit arguments.\n\n    Returns:\n        dict: arguments to be used for cls.__init__\n    \"\"\"", "\n", "signature", "=", "inspect", ".", "signature", "(", "from_config_func", ")", "\n", "if", "list", "(", "signature", ".", "parameters", ".", "keys", "(", ")", ")", "[", "0", "]", "!=", "\"cfg\"", ":", "\n", "        ", "if", "inspect", ".", "isfunction", "(", "from_config_func", ")", ":", "\n", "            ", "name", "=", "from_config_func", ".", "__name__", "\n", "", "else", ":", "\n", "            ", "name", "=", "f\"{from_config_func.__self__}.from_config\"", "\n", "", "raise", "TypeError", "(", "f\"{name} must take 'cfg' as the first argument!\"", ")", "\n", "", "support_var_arg", "=", "any", "(", "\n", "param", ".", "kind", "in", "[", "param", ".", "VAR_POSITIONAL", ",", "param", ".", "VAR_KEYWORD", "]", "\n", "for", "param", "in", "signature", ".", "parameters", ".", "values", "(", ")", "\n", ")", "\n", "if", "support_var_arg", ":", "# forward all arguments to from_config, if from_config accepts them", "\n", "        ", "ret", "=", "from_config_func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "# forward supported arguments to from_config", "\n", "        ", "supported_arg_names", "=", "set", "(", "signature", ".", "parameters", ".", "keys", "(", ")", ")", "\n", "extra_kwargs", "=", "{", "}", "\n", "for", "name", "in", "list", "(", "kwargs", ".", "keys", "(", ")", ")", ":", "\n", "            ", "if", "name", "not", "in", "supported_arg_names", ":", "\n", "                ", "extra_kwargs", "[", "name", "]", "=", "kwargs", ".", "pop", "(", "name", ")", "\n", "", "", "ret", "=", "from_config_func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "# forward the other arguments to __init__", "\n", "ret", ".", "update", "(", "extra_kwargs", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.config._called_with_cfg": [[235, 250], ["isinstance", "len", "isinstance", "kwargs.pop"], "function", ["None"], ["", "def", "_called_with_cfg", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Returns:\n        bool: whether the arguments contain CfgNode and should be considered\n            forwarded to from_config.\n    \"\"\"", "\n", "from", "omegaconf", "import", "DictConfig", "\n", "\n", "if", "len", "(", "args", ")", "and", "isinstance", "(", "args", "[", "0", "]", ",", "(", "_CfgNode", ",", "DictConfig", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "if", "isinstance", "(", "kwargs", ".", "pop", "(", "\"cfg\"", ",", "None", ")", ",", "(", "_CfgNode", ",", "DictConfig", ")", ")", ":", "\n", "        ", "return", "True", "\n", "# `from_config`'s first argument is forced to be \"cfg\".", "\n", "# So the above check covers all cases.", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.dump_dataclass": [[12, 34], ["dataclasses.fields", "dataclasses.is_dataclass", "detectron2.utils.registry._convert_target_to_string", "getattr", "dataclasses.is_dataclass", "isinstance", "isinstance", "type", "instantiate.dump_dataclass", "dataclasses.is_dataclass", "instantiate.dump_dataclass"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry._convert_target_to_string", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.dump_dataclass", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.dump_dataclass"], ["def", "dump_dataclass", "(", "obj", ":", "Any", ")", ":", "\n", "    ", "\"\"\"\n    Dump a dataclass recursively into a dict that can be later instantiated.\n\n    Args:\n        obj: a dataclass object\n\n    Returns:\n        dict\n    \"\"\"", "\n", "assert", "dataclasses", ".", "is_dataclass", "(", "obj", ")", "and", "not", "isinstance", "(", "\n", "obj", ",", "type", "\n", ")", ",", "\"dump_dataclass() requires an instance of a dataclass.\"", "\n", "ret", "=", "{", "\"_target_\"", ":", "_convert_target_to_string", "(", "type", "(", "obj", ")", ")", "}", "\n", "for", "f", "in", "dataclasses", ".", "fields", "(", "obj", ")", ":", "\n", "        ", "v", "=", "getattr", "(", "obj", ",", "f", ".", "name", ")", "\n", "if", "dataclasses", ".", "is_dataclass", "(", "v", ")", ":", "\n", "            ", "v", "=", "dump_dataclass", "(", "v", ")", "\n", "", "if", "isinstance", "(", "v", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "v", "=", "[", "dump_dataclass", "(", "x", ")", "if", "dataclasses", ".", "is_dataclass", "(", "x", ")", "else", "x", "for", "x", "in", "v", "]", "\n", "", "ret", "[", "f", ".", "name", "]", "=", "v", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate": [[36, 83], ["isinstance", "isinstance", "ListConfig", "isinstance", "cfg.pop", "instantiate.instantiate", "isinstance", "callable", "instantiate.instantiate", "instantiate.instantiate", "instantiate.instantiate", "detectron2.utils.registry.locate", "detectron2.utils.registry.locate.", "cfg.items", "logging.getLogger", "logging.getLogger.error", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry.locate"], ["", "def", "instantiate", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Recursively instantiate objects defined in dictionaries by\n    \"_target_\" and arguments.\n\n    Args:\n        cfg: a dict-like object with \"_target_\" that defines the caller, and\n            other keys that define the arguments\n\n    Returns:\n        object instantiated by cfg\n    \"\"\"", "\n", "from", "omegaconf", "import", "ListConfig", "\n", "\n", "if", "isinstance", "(", "cfg", ",", "ListConfig", ")", ":", "\n", "        ", "lst", "=", "[", "instantiate", "(", "x", ")", "for", "x", "in", "cfg", "]", "\n", "return", "ListConfig", "(", "lst", ",", "flags", "=", "{", "\"allow_objects\"", ":", "True", "}", ")", "\n", "", "if", "isinstance", "(", "cfg", ",", "list", ")", ":", "\n", "# Specialize for list, because many classes take", "\n", "# list[objects] as arguments, such as ResNet, DatasetMapper", "\n", "        ", "return", "[", "instantiate", "(", "x", ")", "for", "x", "in", "cfg", "]", "\n", "\n", "", "if", "isinstance", "(", "cfg", ",", "abc", ".", "Mapping", ")", "and", "\"_target_\"", "in", "cfg", ":", "\n", "# conceptually equivalent to hydra.utils.instantiate(cfg) with _convert_=all,", "\n", "# but faster: https://github.com/facebookresearch/hydra/issues/1200", "\n", "        ", "cfg", "=", "{", "k", ":", "instantiate", "(", "v", ")", "for", "k", ",", "v", "in", "cfg", ".", "items", "(", ")", "}", "\n", "cls", "=", "cfg", ".", "pop", "(", "\"_target_\"", ")", "\n", "cls", "=", "instantiate", "(", "cls", ")", "\n", "\n", "if", "isinstance", "(", "cls", ",", "str", ")", ":", "\n", "            ", "cls_name", "=", "cls", "\n", "cls", "=", "locate", "(", "cls_name", ")", "\n", "assert", "cls", "is", "not", "None", ",", "cls_name", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "cls_name", "=", "cls", ".", "__module__", "+", "\".\"", "+", "cls", ".", "__qualname__", "\n", "", "except", "Exception", ":", "\n", "# target could be anything, so the above could fail", "\n", "                ", "cls_name", "=", "str", "(", "cls", ")", "\n", "", "", "assert", "callable", "(", "cls", ")", ",", "f\"_target_ {cls} does not define a callable object\"", "\n", "try", ":", "\n", "            ", "return", "cls", "(", "**", "cfg", ")", "\n", "", "except", "TypeError", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "error", "(", "f\"Error when instantiating {cls_name}!\"", ")", "\n", "raise", "\n", "", "", "return", "cfg", "# return as-is if don't know what to do", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyCall.__init__": [[40, 46], ["TypeError", "callable", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "target", ")", ":", "\n", "        ", "if", "not", "(", "callable", "(", "target", ")", "or", "isinstance", "(", "target", ",", "(", "str", ",", "abc", ".", "Mapping", ")", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "\"target of LazyCall must be a callable or defines a callable! Got {target}\"", "\n", ")", "\n", "", "self", ".", "_target", "=", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyCall.__call__": [[47, 50], ["omegaconf.DictConfig"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", "[", "\"_target_\"", "]", "=", "self", ".", "_target", "\n", "return", "DictConfig", "(", "content", "=", "kwargs", ",", "flags", "=", "{", "\"allow_objects\"", ":", "True", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.load_rel": [[158, 173], ["os.path.dirname", "os.path.join", "lazy.LazyConfig.load", "inspect.stack"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], ["@", "staticmethod", "\n", "def", "load_rel", "(", "filename", ":", "str", ",", "keys", ":", "Union", "[", "None", ",", "str", ",", "Tuple", "[", "str", ",", "...", "]", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Similar to :meth:`load()`, but load path relative to the caller's\n        source file.\n\n        This has the same functionality as a relative import, except that this method\n        accepts filename as a string, so more characters are allowed in the filename.\n        \"\"\"", "\n", "caller_frame", "=", "inspect", ".", "stack", "(", ")", "[", "1", "]", "\n", "caller_fname", "=", "caller_frame", "[", "0", "]", ".", "f_code", ".", "co_filename", "\n", "assert", "caller_fname", "!=", "\"<string>\"", ",", "\"load_rel Unable to find caller\"", "\n", "caller_dir", "=", "os", ".", "path", ".", "dirname", "(", "caller_fname", ")", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "caller_dir", ",", "filename", ")", "\n", "return", "LazyConfig", ".", "load", "(", "filename", ",", "keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.load": [[174, 228], ["filename.replace.replace.replace", "filename.replace.replace.endswith", "ValueError", "lazy._validate_py_syntax", "omegaconf.OmegaConf.create", "isinstance", "filename.replace.replace.endswith", "os.path.splitext", "lazy._patch_import", "exec", "detectron2.utils.file_io.PathManager.open", "yaml.unsafe_load", "lazy._cast_to_config", "tuple", "omegaconf.DictConfig", "lazy._random_package_name", "detectron2.utils.file_io.PathManager.open", "f.read", "compile", "lazy._cast_to_config", "lazy._cast_to_config", "omegaconf.DictConfig.items", "isinstance", "name.startswith"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._validate_py_syntax", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._patch_import", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._cast_to_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._random_package_name", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._cast_to_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._cast_to_config"], ["", "@", "staticmethod", "\n", "def", "load", "(", "filename", ":", "str", ",", "keys", ":", "Union", "[", "None", ",", "str", ",", "Tuple", "[", "str", ",", "...", "]", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Load a config file.\n\n        Args:\n            filename: absolute path or relative path w.r.t. the current working directory\n            keys: keys to load and return. If not given, return all keys\n                (whose values are config objects) in a dict.\n        \"\"\"", "\n", "has_keys", "=", "keys", "is", "not", "None", "\n", "filename", "=", "filename", ".", "replace", "(", "\"/./\"", ",", "\"/\"", ")", "# redundant", "\n", "if", "os", ".", "path", ".", "splitext", "(", "filename", ")", "[", "1", "]", "not", "in", "[", "\".py\"", ",", "\".yaml\"", ",", "\".yml\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Config file {filename} has to be a python or yaml file.\"", ")", "\n", "", "if", "filename", ".", "endswith", "(", "\".py\"", ")", ":", "\n", "            ", "_validate_py_syntax", "(", "filename", ")", "\n", "\n", "with", "_patch_import", "(", ")", ":", "\n", "# Record the filename", "\n", "                ", "module_namespace", "=", "{", "\n", "\"__file__\"", ":", "filename", ",", "\n", "\"__package__\"", ":", "_random_package_name", "(", "filename", ")", ",", "\n", "}", "\n", "with", "PathManager", ".", "open", "(", "filename", ")", "as", "f", ":", "\n", "                    ", "content", "=", "f", ".", "read", "(", ")", "\n", "# Compile first with filename to:", "\n", "# 1. make filename appears in stacktrace", "\n", "# 2. make load_rel able to find its parent's (possibly remote) location", "\n", "", "exec", "(", "compile", "(", "content", ",", "filename", ",", "\"exec\"", ")", ",", "module_namespace", ")", "\n", "\n", "", "ret", "=", "module_namespace", "\n", "", "else", ":", "\n", "            ", "with", "PathManager", ".", "open", "(", "filename", ")", "as", "f", ":", "\n", "                ", "obj", "=", "yaml", ".", "unsafe_load", "(", "f", ")", "\n", "", "ret", "=", "OmegaConf", ".", "create", "(", "obj", ",", "flags", "=", "{", "\"allow_objects\"", ":", "True", "}", ")", "\n", "\n", "", "if", "has_keys", ":", "\n", "            ", "if", "isinstance", "(", "keys", ",", "str", ")", ":", "\n", "                ", "return", "_cast_to_config", "(", "ret", "[", "keys", "]", ")", "\n", "", "else", ":", "\n", "                ", "return", "tuple", "(", "_cast_to_config", "(", "ret", "[", "a", "]", ")", "for", "a", "in", "keys", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "filename", ".", "endswith", "(", "\".py\"", ")", ":", "\n", "# when not specified, only load those that are config objects", "\n", "                ", "ret", "=", "DictConfig", "(", "\n", "{", "\n", "name", ":", "_cast_to_config", "(", "value", ")", "\n", "for", "name", ",", "value", "in", "ret", ".", "items", "(", ")", "\n", "if", "isinstance", "(", "value", ",", "(", "DictConfig", ",", "ListConfig", ",", "dict", ")", ")", "\n", "and", "not", "name", ".", "startswith", "(", "\"_\"", ")", "\n", "}", ",", "\n", "flags", "=", "{", "\"allow_objects\"", ":", "True", "}", ",", "\n", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save": [[229, 268], ["logging.getLogger", "copy.deepcopy", "lazy._visit_dict_config", "detectron2.utils.file_io.PathManager.open", "omegaconf.OmegaConf.to_container", "yaml.dump", "f.write", "logging.getLogger.exception", "callable", "logging.getLogger.warning", "detectron2.utils.registry._convert_target_to_string", "detectron2.utils.file_io.PathManager.open", "cloudpickle.dump"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._visit_dict_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry._convert_target_to_string", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump"], ["", "", "@", "staticmethod", "\n", "def", "save", "(", "cfg", ",", "filename", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            cfg: an omegaconf config object\n            filename: yaml file name to save the config file\n        \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "try", ":", "\n", "            ", "cfg", "=", "deepcopy", "(", "cfg", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "# if it's deep-copyable, then...", "\n", "            ", "def", "_replace_type_by_name", "(", "x", ")", ":", "\n", "                ", "if", "\"_target_\"", "in", "x", "and", "callable", "(", "x", ".", "_target_", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "x", ".", "_target_", "=", "_convert_target_to_string", "(", "x", ".", "_target_", ")", "\n", "", "except", "AttributeError", ":", "\n", "                        ", "pass", "\n", "\n", "# not necessary, but makes yaml looks nicer", "\n", "", "", "", "_visit_dict_config", "(", "cfg", ",", "_replace_type_by_name", ")", "\n", "\n", "", "try", ":", "\n", "            ", "with", "PathManager", ".", "open", "(", "filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "dict", "=", "OmegaConf", ".", "to_container", "(", "cfg", ",", "resolve", "=", "False", ")", "\n", "dumped", "=", "yaml", ".", "dump", "(", "dict", ",", "default_flow_style", "=", "None", ",", "allow_unicode", "=", "True", ",", "width", "=", "9999", ")", "\n", "f", ".", "write", "(", "dumped", ")", "\n", "", "", "except", "Exception", ":", "\n", "            ", "logger", ".", "exception", "(", "\"Unable to serialize the config to yaml. Error:\"", ")", "\n", "new_filename", "=", "filename", "+", "\".pkl\"", "\n", "try", ":", "\n", "# retry by pickle", "\n", "                ", "with", "PathManager", ".", "open", "(", "new_filename", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                    ", "cloudpickle", ".", "dump", "(", "cfg", ",", "f", ")", "\n", "", "logger", ".", "warning", "(", "f\"Config saved using cloudpickle at {new_filename} ...\"", ")", "\n", "", "except", "Exception", ":", "\n", "                ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.apply_overrides": [[269, 310], ["OverridesParser.create", "OverridesParser.create.parse_overrides", "key.split", "range", "omegaconf.OmegaConf.update", "o.value", "o.is_delete", "lazy.LazyConfig.apply_overrides.safe_update"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "apply_overrides", "(", "cfg", ",", "overrides", ":", "List", "[", "str", "]", ")", ":", "\n", "        ", "\"\"\"\n        In-place override contents of cfg.\n\n        Args:\n            cfg: an omegaconf config object\n            overrides: list of strings in the format of \"a=b\" to override configs.\n                See https://hydra.cc/docs/next/advanced/override_grammar/basic/\n                for syntax.\n\n        Returns:\n            the cfg object\n        \"\"\"", "\n", "\n", "def", "safe_update", "(", "cfg", ",", "key", ",", "value", ")", ":", "\n", "            ", "parts", "=", "key", ".", "split", "(", "\".\"", ")", "\n", "for", "idx", "in", "range", "(", "1", ",", "len", "(", "parts", ")", ")", ":", "\n", "                ", "prefix", "=", "\".\"", ".", "join", "(", "parts", "[", ":", "idx", "]", ")", "\n", "v", "=", "OmegaConf", ".", "select", "(", "cfg", ",", "prefix", ",", "default", "=", "None", ")", "\n", "if", "v", "is", "None", ":", "\n", "                    ", "break", "\n", "", "if", "not", "OmegaConf", ".", "is_config", "(", "v", ")", ":", "\n", "                    ", "raise", "KeyError", "(", "\n", "f\"Trying to update key {key}, but {prefix} \"", "\n", "f\"is not a config, but has type {type(v)}.\"", "\n", ")", "\n", "", "", "OmegaConf", ".", "update", "(", "cfg", ",", "key", ",", "value", ",", "merge", "=", "True", ")", "\n", "\n", "", "from", "hydra", ".", "core", ".", "override_parser", ".", "overrides_parser", "import", "OverridesParser", "\n", "\n", "parser", "=", "OverridesParser", ".", "create", "(", ")", "\n", "overrides", "=", "parser", ".", "parse_overrides", "(", "overrides", ")", "\n", "for", "o", "in", "overrides", ":", "\n", "            ", "key", "=", "o", ".", "key_or_group", "\n", "value", "=", "o", ".", "value", "(", ")", "\n", "if", "o", ".", "is_delete", "(", ")", ":", "\n", "# TODO support this", "\n", "                ", "raise", "NotImplementedError", "(", "\"deletion is not yet a supported override\"", ")", "\n", "", "safe_update", "(", "cfg", ",", "key", ",", "value", ")", "\n", "", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.to_py": [[311, 371], ["omegaconf.OmegaConf.to_container", "lazy.LazyConfig.to_py._to_str"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "to_py", "(", "cfg", ",", "prefix", ":", "str", "=", "\"cfg.\"", ")", ":", "\n", "        ", "\"\"\"\n        Convert a config object into its equivalent Python code.\n\n        Args:\n            cfg: an omegaconf config object\n            prefix: root name for the resulting code (default: \"cfg.\")\n\n\n        Returns:\n            str of formatted Python code\n        \"\"\"", "\n", "import", "black", "\n", "\n", "cfg", "=", "OmegaConf", ".", "to_container", "(", "cfg", ",", "resolve", "=", "True", ")", "\n", "\n", "def", "_to_str", "(", "obj", ",", "prefix", "=", "None", ",", "inside_call", "=", "False", ")", ":", "\n", "            ", "if", "prefix", "is", "None", ":", "\n", "                ", "prefix", "=", "[", "]", "\n", "", "if", "isinstance", "(", "obj", ",", "abc", ".", "Mapping", ")", "and", "\"_target_\"", "in", "obj", ":", "\n", "# Dict representing a function call", "\n", "                ", "target", "=", "_convert_target_to_string", "(", "obj", ".", "pop", "(", "\"_target_\"", ")", ")", "\n", "args", "=", "[", "]", "\n", "for", "k", ",", "v", "in", "sorted", "(", "obj", ".", "items", "(", ")", ")", ":", "\n", "                    ", "args", ".", "append", "(", "f\"{k}={_to_str(v, inside_call=True)}\"", ")", "\n", "", "args", "=", "\", \"", ".", "join", "(", "args", ")", "\n", "call", "=", "f\"{target}({args})\"", "\n", "return", "\"\"", ".", "join", "(", "prefix", ")", "+", "call", "\n", "", "elif", "isinstance", "(", "obj", ",", "abc", ".", "Mapping", ")", "and", "not", "inside_call", ":", "\n", "# Dict that is not inside a call is a list of top-level config objects that we", "\n", "# render as one object per line with dot separated prefixes", "\n", "                ", "key_list", "=", "[", "]", "\n", "for", "k", ",", "v", "in", "sorted", "(", "obj", ".", "items", "(", ")", ")", ":", "\n", "                    ", "if", "isinstance", "(", "v", ",", "abc", ".", "Mapping", ")", "and", "\"_target_\"", "not", "in", "v", ":", "\n", "                        ", "key_list", ".", "append", "(", "_to_str", "(", "v", ",", "prefix", "=", "prefix", "+", "[", "k", "+", "\".\"", "]", ")", ")", "\n", "", "else", ":", "\n", "                        ", "key", "=", "\"\"", ".", "join", "(", "prefix", ")", "+", "k", "\n", "key_list", ".", "append", "(", "f\"{key}={_to_str(v)}\"", ")", "\n", "", "", "return", "\"\\n\"", ".", "join", "(", "key_list", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "abc", ".", "Mapping", ")", ":", "\n", "# Dict that is inside a call is rendered as a regular dict", "\n", "                ", "return", "(", "\n", "\"{\"", "\n", "+", "\",\"", ".", "join", "(", "\n", "f\"{repr(k)}: {_to_str(v, inside_call=inside_call)}\"", "\n", "for", "k", ",", "v", "in", "sorted", "(", "obj", ".", "items", "(", ")", ")", "\n", ")", "\n", "+", "\"}\"", "\n", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "list", ")", ":", "\n", "                ", "return", "\"[\"", "+", "\",\"", ".", "join", "(", "_to_str", "(", "x", ",", "inside_call", "=", "inside_call", ")", "for", "x", "in", "obj", ")", "+", "\"]\"", "\n", "", "else", ":", "\n", "                ", "return", "repr", "(", "obj", ")", "\n", "\n", "", "", "py_str", "=", "_to_str", "(", "cfg", ",", "prefix", "=", "[", "prefix", "]", ")", "\n", "try", ":", "\n", "            ", "return", "black", ".", "format_str", "(", "py_str", ",", "mode", "=", "black", ".", "Mode", "(", ")", ")", "\n", "", "except", "black", ".", "InvalidInput", ":", "\n", "            ", "return", "py_str", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._visit_dict_config": [[52, 63], ["isinstance", "func", "cfg.values", "isinstance", "lazy._visit_dict_config", "lazy._visit_dict_config"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._visit_dict_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._visit_dict_config"], ["", "", "def", "_visit_dict_config", "(", "cfg", ",", "func", ")", ":", "\n", "    ", "\"\"\"\n    Apply func recursively to all DictConfig in cfg.\n    \"\"\"", "\n", "if", "isinstance", "(", "cfg", ",", "DictConfig", ")", ":", "\n", "        ", "func", "(", "cfg", ")", "\n", "for", "v", "in", "cfg", ".", "values", "(", ")", ":", "\n", "            ", "_visit_dict_config", "(", "v", ",", "func", ")", "\n", "", "", "elif", "isinstance", "(", "cfg", ",", "ListConfig", ")", ":", "\n", "        ", "for", "v", "in", "cfg", ":", "\n", "            ", "_visit_dict_config", "(", "v", ",", "func", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._validate_py_syntax": [[65, 73], ["detectron2.utils.file_io.PathManager.open", "f.read", "ast.parse", "SyntaxError"], "function", ["None"], ["", "", "", "def", "_validate_py_syntax", "(", "filename", ")", ":", "\n", "# see also https://github.com/open-mmlab/mmcv/blob/master/mmcv/utils/config.py", "\n", "    ", "with", "PathManager", ".", "open", "(", "filename", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "content", "=", "f", ".", "read", "(", ")", "\n", "", "try", ":", "\n", "        ", "ast", ".", "parse", "(", "content", ")", "\n", "", "except", "SyntaxError", "as", "e", ":", "\n", "        ", "raise", "SyntaxError", "(", "f\"Config file {filename} has syntax error!\"", ")", "from", "e", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._cast_to_config": [[75, 80], ["isinstance", "omegaconf.DictConfig"], "function", ["None"], ["", "", "def", "_cast_to_config", "(", "obj", ")", ":", "\n", "# if given a dict, return DictConfig instead", "\n", "    ", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "\n", "        ", "return", "DictConfig", "(", "obj", ",", "flags", "=", "{", "\"allow_objects\"", ":", "True", "}", ")", "\n", "", "return", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._random_package_name": [[88, 91], ["os.path.basename", "str", "uuid.uuid4"], "function", ["None"], ["def", "_random_package_name", "(", "filename", ")", ":", "\n", "# generate a random package name when loading config files", "\n", "    ", "return", "_CFG_PACKAGE_NAME", "+", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "[", ":", "4", "]", "+", "\".\"", "+", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy._patch_import": [[93, 150], ["os.path.dirname", "range", "relative_import_path.lstrip", "relative_import_path.lstrip.split", "old_import", "os.path.dirname", "os.path.join", "find_relative_file.endswith", "detectron2.utils.file_io.PathManager.isfile", "ImportError", "lazy._patch_import.find_relative_file"], "function", ["None"], ["", "@", "contextmanager", "\n", "def", "_patch_import", "(", ")", ":", "\n", "    ", "\"\"\"\n    Enhance relative import statements in config files, so that they:\n    1. locate files purely based on relative location, regardless of packages.\n       e.g. you can import file without having __init__\n    2. do not cache modules globally; modifications of module states has no side effect\n    3. support other storage system through PathManager\n    4. imported dict are turned into omegaconf.DictConfig automatically\n    \"\"\"", "\n", "old_import", "=", "builtins", ".", "__import__", "\n", "\n", "def", "find_relative_file", "(", "original_file", ",", "relative_import_path", ",", "level", ")", ":", "\n", "        ", "cur_file", "=", "os", ".", "path", ".", "dirname", "(", "original_file", ")", "\n", "for", "_", "in", "range", "(", "level", "-", "1", ")", ":", "\n", "            ", "cur_file", "=", "os", ".", "path", ".", "dirname", "(", "cur_file", ")", "\n", "", "cur_name", "=", "relative_import_path", ".", "lstrip", "(", "\".\"", ")", "\n", "for", "part", "in", "cur_name", ".", "split", "(", "\".\"", ")", ":", "\n", "            ", "cur_file", "=", "os", ".", "path", ".", "join", "(", "cur_file", ",", "part", ")", "\n", "# NOTE: directory import is not handled. Because then it's unclear", "\n", "# if such import should produce python module or DictConfig. This can", "\n", "# be discussed further if needed.", "\n", "", "if", "not", "cur_file", ".", "endswith", "(", "\".py\"", ")", ":", "\n", "            ", "cur_file", "+=", "\".py\"", "\n", "", "if", "not", "PathManager", ".", "isfile", "(", "cur_file", ")", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "f\"Cannot import name {relative_import_path} from \"", "\n", "f\"{original_file}: {cur_file} has to exist.\"", "\n", ")", "\n", "", "return", "cur_file", "\n", "\n", "", "def", "new_import", "(", "name", ",", "globals", "=", "None", ",", "locals", "=", "None", ",", "fromlist", "=", "(", ")", ",", "level", "=", "0", ")", ":", "\n", "        ", "if", "(", "\n", "# Only deal with relative imports inside config files", "\n", "level", "!=", "0", "\n", "and", "globals", "is", "not", "None", "\n", "and", "(", "globals", ".", "get", "(", "\"__package__\"", ",", "\"\"", ")", "or", "\"\"", ")", ".", "startswith", "(", "_CFG_PACKAGE_NAME", ")", "\n", ")", ":", "\n", "            ", "cur_file", "=", "find_relative_file", "(", "globals", "[", "\"__file__\"", "]", ",", "name", ",", "level", ")", "\n", "_validate_py_syntax", "(", "cur_file", ")", "\n", "spec", "=", "importlib", ".", "machinery", ".", "ModuleSpec", "(", "\n", "_random_package_name", "(", "cur_file", ")", ",", "None", ",", "origin", "=", "cur_file", "\n", ")", "\n", "module", "=", "importlib", ".", "util", ".", "module_from_spec", "(", "spec", ")", "\n", "module", ".", "__file__", "=", "cur_file", "\n", "with", "PathManager", ".", "open", "(", "cur_file", ")", "as", "f", ":", "\n", "                ", "content", "=", "f", ".", "read", "(", ")", "\n", "", "exec", "(", "compile", "(", "content", ",", "cur_file", ",", "\"exec\"", ")", ",", "module", ".", "__dict__", ")", "\n", "for", "name", "in", "fromlist", ":", "# turn imported dict into DictConfig automatically", "\n", "                ", "val", "=", "_cast_to_config", "(", "module", ".", "__dict__", "[", "name", "]", ")", "\n", "module", ".", "__dict__", "[", "name", "]", "=", "val", "\n", "", "return", "module", "\n", "", "return", "old_import", "(", "name", ",", "globals", ",", "locals", ",", "fromlist", "=", "fromlist", ",", "level", "=", "level", ")", "\n", "\n", "", "builtins", ".", "__import__", "=", "new_import", "\n", "yield", "new_import", "\n", "builtins", ".", "__import__", "=", "old_import", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._RenameConverter.upgrade": [[153, 157], ["compat._rename"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._rename"], ["@", "classmethod", "\n", "def", "upgrade", "(", "cls", ",", "cfg", ":", "CN", ")", "->", "None", ":", "\n", "        ", "for", "old", ",", "new", "in", "cls", ".", "RENAME", ":", "\n", "            ", "_rename", "(", "cfg", ",", "old", ",", "new", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._RenameConverter.downgrade": [[158, 162], ["compat._rename"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._rename"], ["", "", "@", "classmethod", "\n", "def", "downgrade", "(", "cls", ",", "cfg", ":", "CN", ")", "->", "None", ":", "\n", "        ", "for", "old", ",", "new", "in", "cls", ".", "RENAME", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "_rename", "(", "cfg", ",", "new", ",", "old", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.ConverterV2.upgrade": [[203, 220], ["compat._RenameConverter.upgrade", "compat._rename", "compat._rename", "compat._rename", "compat._rename"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.ConverterV2.upgrade", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._rename", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._rename", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._rename", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._rename"], ["@", "classmethod", "\n", "def", "upgrade", "(", "cls", ",", "cfg", ":", "CN", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "upgrade", "(", "cfg", ")", "\n", "\n", "if", "cfg", ".", "MODEL", ".", "META_ARCHITECTURE", "==", "\"RetinaNet\"", ":", "\n", "            ", "_rename", "(", "\n", "cfg", ",", "\"MODEL.RETINANET.ANCHOR_ASPECT_RATIOS\"", ",", "\"MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS\"", "\n", ")", "\n", "_rename", "(", "cfg", ",", "\"MODEL.RETINANET.ANCHOR_SIZES\"", ",", "\"MODEL.ANCHOR_GENERATOR.SIZES\"", ")", "\n", "del", "cfg", "[", "\"MODEL\"", "]", "[", "\"RPN\"", "]", "[", "\"ANCHOR_SIZES\"", "]", "\n", "del", "cfg", "[", "\"MODEL\"", "]", "[", "\"RPN\"", "]", "[", "\"ANCHOR_ASPECT_RATIOS\"", "]", "\n", "", "else", ":", "\n", "            ", "_rename", "(", "cfg", ",", "\"MODEL.RPN.ANCHOR_ASPECT_RATIOS\"", ",", "\"MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS\"", ")", "\n", "_rename", "(", "cfg", ",", "\"MODEL.RPN.ANCHOR_SIZES\"", ",", "\"MODEL.ANCHOR_GENERATOR.SIZES\"", ")", "\n", "del", "cfg", "[", "\"MODEL\"", "]", "[", "\"RETINANET\"", "]", "[", "\"ANCHOR_SIZES\"", "]", "\n", "del", "cfg", "[", "\"MODEL\"", "]", "[", "\"RETINANET\"", "]", "[", "\"ANCHOR_ASPECT_RATIOS\"", "]", "\n", "", "del", "cfg", "[", "\"MODEL\"", "]", "[", "\"RETINANET\"", "]", "[", "\"ANCHOR_STRIDES\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.ConverterV2.downgrade": [[221, 230], ["compat._RenameConverter.downgrade", "compat._rename", "compat._rename"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.ConverterV2.downgrade", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._rename", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._rename"], ["", "@", "classmethod", "\n", "def", "downgrade", "(", "cls", ",", "cfg", ":", "CN", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "downgrade", "(", "cfg", ")", "\n", "\n", "_rename", "(", "cfg", ",", "\"MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS\"", ",", "\"MODEL.RPN.ANCHOR_ASPECT_RATIOS\"", ")", "\n", "_rename", "(", "cfg", ",", "\"MODEL.ANCHOR_GENERATOR.SIZES\"", ",", "\"MODEL.RPN.ANCHOR_SIZES\"", ")", "\n", "cfg", ".", "MODEL", ".", "RETINANET", ".", "ANCHOR_ASPECT_RATIOS", "=", "cfg", ".", "MODEL", ".", "RPN", ".", "ANCHOR_ASPECT_RATIOS", "\n", "cfg", ".", "MODEL", ".", "RETINANET", ".", "ANCHOR_SIZES", "=", "cfg", ".", "MODEL", ".", "RPN", ".", "ANCHOR_SIZES", "\n", "cfg", ".", "MODEL", ".", "RETINANET", ".", "ANCHOR_STRIDES", "=", "[", "]", "# this is not used anywhere in any version", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.upgrade_config": [[33, 53], ["cfg.clone.clone", "range", "converter.upgrade", "globals", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.ConverterV2.upgrade"], ["def", "upgrade_config", "(", "cfg", ":", "CN", ",", "to_version", ":", "Optional", "[", "int", "]", "=", "None", ")", "->", "CN", ":", "\n", "    ", "\"\"\"\n    Upgrade a config from its current version to a newer version.\n\n    Args:\n        cfg (CfgNode):\n        to_version (int): defaults to the latest version.\n    \"\"\"", "\n", "cfg", "=", "cfg", ".", "clone", "(", ")", "\n", "if", "to_version", "is", "None", ":", "\n", "        ", "to_version", "=", "_C", ".", "VERSION", "\n", "\n", "", "assert", "cfg", ".", "VERSION", "<=", "to_version", ",", "\"Cannot upgrade from v{} to v{}!\"", ".", "format", "(", "\n", "cfg", ".", "VERSION", ",", "to_version", "\n", ")", "\n", "for", "k", "in", "range", "(", "cfg", ".", "VERSION", ",", "to_version", ")", ":", "\n", "        ", "converter", "=", "globals", "(", ")", "[", "\"ConverterV\"", "+", "str", "(", "k", "+", "1", ")", "]", "\n", "converter", ".", "upgrade", "(", "cfg", ")", "\n", "cfg", ".", "VERSION", "=", "k", "+", "1", "\n", "", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.downgrade_config": [[55, 80], ["cfg.clone.clone", "range", "converter.downgrade", "globals", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.ConverterV2.downgrade"], ["", "def", "downgrade_config", "(", "cfg", ":", "CN", ",", "to_version", ":", "int", ")", "->", "CN", ":", "\n", "    ", "\"\"\"\n    Downgrade a config from its current version to an older version.\n\n    Args:\n        cfg (CfgNode):\n        to_version (int):\n\n    Note:\n        A general downgrade of arbitrary configs is not always possible due to the\n        different functionalities in different versions.\n        The purpose of downgrade is only to recover the defaults in old versions,\n        allowing it to load an old partial yaml config.\n        Therefore, the implementation only needs to fill in the default values\n        in the old version when a general downgrade is not possible.\n    \"\"\"", "\n", "cfg", "=", "cfg", ".", "clone", "(", ")", "\n", "assert", "cfg", ".", "VERSION", ">=", "to_version", ",", "\"Cannot downgrade from v{} to v{}!\"", ".", "format", "(", "\n", "cfg", ".", "VERSION", ",", "to_version", "\n", ")", "\n", "for", "k", "in", "range", "(", "cfg", ".", "VERSION", ",", "to_version", ",", "-", "1", ")", ":", "\n", "        ", "converter", "=", "globals", "(", ")", "[", "\"ConverterV\"", "+", "str", "(", "k", ")", "]", "\n", "converter", ".", "downgrade", "(", "cfg", ")", "\n", "cfg", ".", "VERSION", "=", "k", "-", "1", "\n", "", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.guess_version": [[82, 114], ["logging.getLogger", "name.split", "compat.guess_version._has"], "function", ["None"], ["", "def", "guess_version", "(", "cfg", ":", "CN", ",", "filename", ":", "str", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Guess the version of a partial config where the VERSION field is not specified.\n    Returns the version, or the latest if cannot make a guess.\n\n    This makes it easier for users to migrate.\n    \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n", "def", "_has", "(", "name", ":", "str", ")", "->", "bool", ":", "\n", "        ", "cur", "=", "cfg", "\n", "for", "n", "in", "name", ".", "split", "(", "\".\"", ")", ":", "\n", "            ", "if", "n", "not", "in", "cur", ":", "\n", "                ", "return", "False", "\n", "", "cur", "=", "cur", "[", "n", "]", "\n", "", "return", "True", "\n", "\n", "# Most users' partial configs have \"MODEL.WEIGHT\", so guess on it", "\n", "", "ret", "=", "None", "\n", "if", "_has", "(", "\"MODEL.WEIGHT\"", ")", "or", "_has", "(", "\"TEST.AUG_ON\"", ")", ":", "\n", "        ", "ret", "=", "1", "\n", "\n", "", "if", "ret", "is", "not", "None", ":", "\n", "        ", "logger", ".", "warning", "(", "\"Config '{}' has no VERSION. Assuming it to be v{}.\"", ".", "format", "(", "filename", ",", "ret", ")", ")", "\n", "", "else", ":", "\n", "        ", "ret", "=", "_C", ".", "VERSION", "\n", "logger", ".", "warning", "(", "\n", "\"Config '{}' has no VERSION. Assuming it to be compatible with latest v{}.\"", ".", "format", "(", "\n", "filename", ",", "ret", "\n", ")", "\n", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat._rename": [[116, 144], ["old.split", "new.split", "compat._rename._set"], "function", ["None"], ["", "def", "_rename", "(", "cfg", ":", "CN", ",", "old", ":", "str", ",", "new", ":", "str", ")", "->", "None", ":", "\n", "    ", "old_keys", "=", "old", ".", "split", "(", "\".\"", ")", "\n", "new_keys", "=", "new", ".", "split", "(", "\".\"", ")", "\n", "\n", "def", "_set", "(", "key_seq", ":", "List", "[", "str", "]", ",", "val", ":", "str", ")", "->", "None", ":", "\n", "        ", "cur", "=", "cfg", "\n", "for", "k", "in", "key_seq", "[", ":", "-", "1", "]", ":", "\n", "            ", "if", "k", "not", "in", "cur", ":", "\n", "                ", "cur", "[", "k", "]", "=", "CN", "(", ")", "\n", "", "cur", "=", "cur", "[", "k", "]", "\n", "", "cur", "[", "key_seq", "[", "-", "1", "]", "]", "=", "val", "\n", "\n", "", "def", "_get", "(", "key_seq", ":", "List", "[", "str", "]", ")", "->", "CN", ":", "\n", "        ", "cur", "=", "cfg", "\n", "for", "k", "in", "key_seq", ":", "\n", "            ", "cur", "=", "cur", "[", "k", "]", "\n", "", "return", "cur", "\n", "\n", "", "def", "_del", "(", "key_seq", ":", "List", "[", "str", "]", ")", "->", "None", ":", "\n", "        ", "cur", "=", "cfg", "\n", "for", "k", "in", "key_seq", "[", ":", "-", "1", "]", ":", "\n", "            ", "cur", "=", "cur", "[", "k", "]", "\n", "", "del", "cur", "[", "key_seq", "[", "-", "1", "]", "]", "\n", "if", "len", "(", "cur", ")", "==", "0", "and", "len", "(", "key_seq", ")", ">", "1", ":", "\n", "            ", "_del", "(", "key_seq", "[", ":", "-", "1", "]", ")", "\n", "\n", "", "", "_set", "(", "new_keys", ",", "_get", "(", "old_keys", ")", ")", "\n", "_del", "(", "old_keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_instantiate_config.TestClass.__init__": [[17, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "int_arg", ",", "list_arg", "=", "None", ",", "dict_arg", "=", "None", ",", "extra_arg", "=", "None", ")", ":", "\n", "        ", "self", ".", "int_arg", "=", "int_arg", "\n", "self", ".", "list_arg", "=", "list_arg", "\n", "self", ".", "dict_arg", "=", "dict_arg", "\n", "self", ".", "extra_arg", "=", "extra_arg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_instantiate_config.TestClass.__call__": [[23, 25], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "call_arg", ")", ":", "\n", "        ", "return", "call_arg", "+", "self", ".", "int_arg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_instantiate_config.TestConstruction.test_basic_construct": [[29, 47], ["detectron2.config.instantiate", "test_instantiate_config.TestConstruction.assertIsInstance", "test_instantiate_config.TestConstruction.assertEqual", "test_instantiate_config.TestConstruction.assertEqual", "test_instantiate_config.TestConstruction.assertEqual", "detectron2.config.instantiate", "test_instantiate_config.TestConstruction.assertIsInstance", "test_instantiate_config.TestConstruction.assertEqual", "detectron2.config.LazyCall", "detectron2.config.LazyCall"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate"], ["    ", "def", "test_basic_construct", "(", "self", ")", ":", "\n", "        ", "objconf", "=", "L", "(", "TestClass", ")", "(", "\n", "int_arg", "=", "3", ",", "\n", "list_arg", "=", "[", "10", "]", ",", "\n", "dict_arg", "=", "{", "}", ",", "\n", "extra_arg", "=", "L", "(", "TestClass", ")", "(", "int_arg", "=", "4", ",", "list_arg", "=", "\"${..list_arg}\"", ")", ",", "\n", ")", "\n", "\n", "obj", "=", "instantiate", "(", "objconf", ")", "\n", "self", ".", "assertIsInstance", "(", "obj", ",", "TestClass", ")", "\n", "self", ".", "assertEqual", "(", "obj", ".", "int_arg", ",", "3", ")", "\n", "self", ".", "assertEqual", "(", "obj", ".", "extra_arg", ".", "int_arg", ",", "4", ")", "\n", "self", ".", "assertEqual", "(", "obj", ".", "extra_arg", ".", "list_arg", ",", "obj", ".", "list_arg", ")", "\n", "\n", "objconf", ".", "extra_arg", ".", "list_arg", "=", "[", "5", "]", "\n", "obj", "=", "instantiate", "(", "objconf", ")", "\n", "self", ".", "assertIsInstance", "(", "obj", ",", "TestClass", ")", "\n", "self", ".", "assertEqual", "(", "obj", ".", "extra_arg", ".", "list_arg", ",", "[", "5", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_instantiate_config.TestConstruction.test_instantiate_other_obj": [[48, 57], ["test_instantiate_config.TestConstruction.assertEqual", "test_instantiate_config.TestConstruction.assertEqual", "test_instantiate_config.TestClass", "test_instantiate_config.TestConstruction.assertIs", "test_instantiate_config.TestConstruction.assertIs", "detectron2.config.instantiate", "detectron2.config.instantiate", "detectron2.config.instantiate", "detectron2.config.instantiate"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate"], ["", "def", "test_instantiate_other_obj", "(", "self", ")", ":", "\n", "# do nothing for other obj", "\n", "        ", "self", ".", "assertEqual", "(", "instantiate", "(", "5", ")", ",", "5", ")", "\n", "x", "=", "[", "3", ",", "4", ",", "5", "]", "\n", "self", ".", "assertEqual", "(", "instantiate", "(", "x", ")", ",", "x", ")", "\n", "x", "=", "TestClass", "(", "1", ")", "\n", "self", ".", "assertIs", "(", "instantiate", "(", "x", ")", ",", "x", ")", "\n", "x", "=", "{", "\"xx\"", ":", "\"yy\"", "}", "\n", "self", ".", "assertIs", "(", "instantiate", "(", "x", ")", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_instantiate_config.TestConstruction.test_instantiate_lazy_target": [[58, 63], ["test_instantiate_config.TestConstruction.assertEqual", "detectron2.config.LazyCall", "detectron2.config.instantiate", "detectron2.config.LazyCall"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate"], ["", "def", "test_instantiate_lazy_target", "(", "self", ")", ":", "\n", "# _target_ is result of instantiate", "\n", "        ", "objconf", "=", "L", "(", "L", "(", "len", ")", "(", "int_arg", "=", "3", ")", ")", "(", "call_arg", "=", "4", ")", "\n", "objconf", ".", "_target_", ".", "_target_", "=", "TestClass", "\n", "self", ".", "assertEqual", "(", "instantiate", "(", "objconf", ")", ",", "7", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_instantiate_config.TestConstruction.test_instantiate_lst": [[64, 71], ["test_instantiate_config.TestConstruction.assertEqual", "test_instantiate_config.TestConstruction.assertIsInstance", "test_instantiate_config.TestConstruction.assertEqual", "detectron2.config.LazyCall", "detectron2.config.instantiate", "detectron2.config.LazyCall"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate"], ["", "def", "test_instantiate_lst", "(", "self", ")", ":", "\n", "        ", "lst", "=", "[", "1", ",", "2", ",", "L", "(", "TestClass", ")", "(", "int_arg", "=", "1", ")", "]", "\n", "x", "=", "L", "(", "TestClass", ")", "(", "int_arg", "=", "lst", ")", "# list as an argument should be recursively instantiated", "\n", "x", "=", "instantiate", "(", "x", ")", ".", "int_arg", "\n", "self", ".", "assertEqual", "(", "x", "[", ":", "2", "]", ",", "[", "1", ",", "2", "]", ")", "\n", "self", ".", "assertIsInstance", "(", "x", "[", "2", "]", ",", "TestClass", ")", "\n", "self", ".", "assertEqual", "(", "x", "[", "2", "]", ".", "int_arg", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_instantiate_config.TestConstruction.test_instantiate_namedtuple": [[72, 84], ["detectron2.config.instantiate", "test_instantiate_config.TestConstruction.assertIsInstance", "test_instantiate_config.TestConstruction.assertEqual", "detectron2.config.LazyCall", "tempfile.TemporaryDirectory", "os.path.join", "omegaconf.OmegaConf.save", "detectron2.layers.ShapeSpec", "open", "yaml.unsafe_load"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save"], ["", "def", "test_instantiate_namedtuple", "(", "self", ")", ":", "\n", "        ", "x", "=", "L", "(", "TestClass", ")", "(", "int_arg", "=", "ShapeSpec", "(", "channels", "=", "1", ",", "width", "=", "3", ")", ")", "\n", "# test serialization", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "d", ":", "\n", "            ", "fname", "=", "os", ".", "path", ".", "join", "(", "d", ",", "\"d2_test.yaml\"", ")", "\n", "OmegaConf", ".", "save", "(", "x", ",", "fname", ")", "\n", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "                ", "x", "=", "yaml", ".", "unsafe_load", "(", "f", ")", "\n", "\n", "", "", "x", "=", "instantiate", "(", "x", ")", "\n", "self", ".", "assertIsInstance", "(", "x", ".", "int_arg", ",", "ShapeSpec", ")", "\n", "self", ".", "assertEqual", "(", "x", ".", "int_arg", ".", "channels", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_instantiate_config.TestConstruction.test_bad_lazycall": [[85, 88], ["test_instantiate_config.TestConstruction.assertRaises", "detectron2.config.LazyCall"], "methods", ["None"], ["", "def", "test_bad_lazycall", "(", "self", ")", ":", "\n", "        ", "with", "self", ".", "assertRaises", "(", "Exception", ")", ":", "\n", "            ", "L", "(", "3", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_lazy_config.TestLazyPythonConfig.setUp": [[11, 13], ["os.path.join", "os.path.dirname"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "root_filename", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "\"root_cfg.py\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_lazy_config.TestLazyPythonConfig.test_load": [[14, 25], ["detectron2.config.LazyConfig.load", "test_lazy_config.TestLazyPythonConfig.assertEqual", "test_lazy_config.TestLazyPythonConfig.assertEqual", "test_lazy_config.TestLazyPythonConfig.assertEqual", "detectron2.config.LazyConfig.load", "test_lazy_config.TestLazyPythonConfig.assertEqual"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], ["", "def", "test_load", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "LazyConfig", ".", "load", "(", "self", ".", "root_filename", ")", "\n", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "dir1a_dict", ".", "a", ",", "\"modified\"", ")", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "dir1b_dict", ".", "a", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "lazyobj", ".", "x", ",", "\"base_a_1\"", ")", "\n", "\n", "cfg", ".", "lazyobj", ".", "x", "=", "\"new_x\"", "\n", "# reload", "\n", "cfg", "=", "LazyConfig", ".", "load", "(", "self", ".", "root_filename", ")", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "lazyobj", ".", "x", ",", "\"base_a_1\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_lazy_config.TestLazyPythonConfig.test_save_load": [[26, 39], ["detectron2.config.LazyConfig.load", "test_lazy_config.TestLazyPythonConfig.assertEqual", "test_lazy_config.TestLazyPythonConfig.assertEqual", "detectron2.config.LazyConfig.load.lazyobj.pop", "detectron2.config.LazyConfig.load.lazyobj.pop", "test_lazy_config.TestLazyPythonConfig.assertEqual", "tempfile.TemporaryDirectory", "os.path.join", "detectron2.config.LazyConfig.save", "detectron2.config.LazyConfig.load"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], ["", "def", "test_save_load", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "LazyConfig", ".", "load", "(", "self", ".", "root_filename", ")", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2\"", ")", "as", "d", ":", "\n", "            ", "fname", "=", "os", ".", "path", ".", "join", "(", "d", ",", "\"test_config.yaml\"", ")", "\n", "LazyConfig", ".", "save", "(", "cfg", ",", "fname", ")", "\n", "cfg2", "=", "LazyConfig", ".", "load", "(", "fname", ")", "\n", "\n", "", "self", ".", "assertEqual", "(", "cfg2", ".", "lazyobj", ".", "_target_", ",", "\"itertools.count\"", ")", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "lazyobj", ".", "_target_", ",", "count", ")", "\n", "cfg2", ".", "lazyobj", ".", "pop", "(", "\"_target_\"", ")", "\n", "cfg", ".", "lazyobj", ".", "pop", "(", "\"_target_\"", ")", "\n", "# the rest are equal", "\n", "self", ".", "assertEqual", "(", "cfg", ",", "cfg2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_lazy_config.TestLazyPythonConfig.test_overrides": [[40, 45], ["detectron2.config.LazyConfig.load", "detectron2.config.LazyConfig.apply_overrides", "test_lazy_config.TestLazyPythonConfig.assertEqual", "test_lazy_config.TestLazyPythonConfig.assertEqual"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.apply_overrides"], ["", "def", "test_overrides", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "LazyConfig", ".", "load", "(", "self", ".", "root_filename", ")", "\n", "LazyConfig", ".", "apply_overrides", "(", "cfg", ",", "[", "\"lazyobj.x=123\"", ",", "'dir1b_dict.a=\"123\"'", "]", ")", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "dir1b_dict", ".", "a", ",", "\"123\"", ")", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "lazyobj", ".", "x", ",", "123", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_lazy_config.TestLazyPythonConfig.test_invalid_overrides": [[46, 50], ["detectron2.config.LazyConfig.load", "test_lazy_config.TestLazyPythonConfig.assertRaises", "detectron2.config.LazyConfig.apply_overrides"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.apply_overrides"], ["", "def", "test_invalid_overrides", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "LazyConfig", ".", "load", "(", "self", ".", "root_filename", ")", "\n", "with", "self", ".", "assertRaises", "(", "KeyError", ")", ":", "\n", "            ", "LazyConfig", ".", "apply_overrides", "(", "cfg", ",", "[", "\"lazyobj.x.xxx=123\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_lazy_config.TestLazyPythonConfig.test_to_py": [[51, 71], ["detectron2.config.LazyConfig.load", "detectron2.config.LazyConfig.to_py", "test_lazy_config.TestLazyPythonConfig.assertEqual", "detectron2.config.LazyCall"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.to_py"], ["", "", "def", "test_to_py", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "LazyConfig", ".", "load", "(", "self", ".", "root_filename", ")", "\n", "cfg", ".", "lazyobj", ".", "x", "=", "{", "\"a\"", ":", "1", ",", "\"b\"", ":", "2", ",", "\"c\"", ":", "L", "(", "count", ")", "(", "x", "=", "{", "\"r\"", ":", "\"a\"", ",", "\"s\"", ":", "2.4", ",", "\"t\"", ":", "[", "1", ",", "2", ",", "3", ",", "\"z\"", "]", "}", ")", "}", "\n", "cfg", ".", "list", "=", "[", "\"a\"", ",", "1", ",", "\"b\"", ",", "3.2", "]", "\n", "py_str", "=", "LazyConfig", ".", "to_py", "(", "cfg", ")", "\n", "expected", "=", "\"\"\"cfg.dir1a_dict.a = \"modified\"\ncfg.dir1a_dict.b = 2\ncfg.dir1b_dict.a = 1\ncfg.dir1b_dict.b = 2\ncfg.lazyobj = itertools.count(\n    x={\n        \"a\": 1,\n        \"b\": 2,\n        \"c\": itertools.count(x={\"r\": \"a\", \"s\": 2.4, \"t\": [1, 2, 3, \"z\"]}),\n    },\n    y=\"base_a_1_from_b\",\n)\ncfg.list = [\"a\", 1, \"b\", 3.2]\n\"\"\"", "\n", "self", ".", "assertEqual", "(", "py_str", ",", "expected", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigVersioning.test_upgrade_downgrade_consistency": [[30, 38], ["detectron2.config.get_cfg", "detectron2.config.downgrade_config", "detectron2.config.upgrade_config", "test_yacs_config.TestConfigVersioning.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.downgrade_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.compat.upgrade_config"], ["    ", "def", "test_upgrade_downgrade_consistency", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "# check that custom is preserved", "\n", "cfg", ".", "USER_CUSTOM", "=", "1", "\n", "\n", "down", "=", "downgrade_config", "(", "cfg", ",", "to_version", "=", "0", ")", "\n", "up", "=", "upgrade_config", "(", "down", ")", "\n", "self", ".", "assertTrue", "(", "up", "==", "cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigVersioning._merge_cfg_str": [[39, 48], ["tempfile.NamedTemporaryFile", "tempfile.NamedTemporaryFile.write", "tempfile.NamedTemporaryFile.close", "cfg.merge_from_file", "os.remove"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.remove"], ["", "def", "_merge_cfg_str", "(", "self", ",", "cfg", ",", "merge_str", ")", ":", "\n", "        ", "f", "=", "tempfile", ".", "NamedTemporaryFile", "(", "mode", "=", "\"w\"", ",", "suffix", "=", "\".yaml\"", ",", "delete", "=", "False", ")", "\n", "try", ":", "\n", "            ", "f", ".", "write", "(", "merge_str", ")", "\n", "f", ".", "close", "(", ")", "\n", "cfg", ".", "merge_from_file", "(", "f", ".", "name", ")", "\n", "", "finally", ":", "\n", "            ", "os", ".", "remove", "(", "f", ".", "name", ")", "\n", "", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigVersioning.test_auto_upgrade": [[49, 58], ["detectron2.config.get_cfg", "test_yacs_config.TestConfigVersioning._merge_cfg_str", "test_yacs_config.TestConfigVersioning.assertEqual", "test_yacs_config.TestConfigVersioning.assertEqual"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigVersioning._merge_cfg_str"], ["", "def", "test_auto_upgrade", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "latest_ver", "=", "cfg", ".", "VERSION", "\n", "cfg", ".", "USER_CUSTOM", "=", "1", "\n", "\n", "self", ".", "_merge_cfg_str", "(", "cfg", ",", "_V0_CFG", ")", "\n", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "MODEL", ".", "RPN", ".", "HEAD_NAME", ",", "\"TEST\"", ")", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "VERSION", ",", "latest_ver", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigVersioning.test_guess_v1": [[59, 64], ["detectron2.config.get_cfg", "test_yacs_config.TestConfigVersioning._merge_cfg_str", "test_yacs_config.TestConfigVersioning.assertEqual"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigVersioning._merge_cfg_str"], ["", "def", "test_guess_v1", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "latest_ver", "=", "cfg", ".", "VERSION", "\n", "self", ".", "_merge_cfg_str", "(", "cfg", ",", "_V1_CFG", ")", "\n", "self", ".", "assertEqual", "(", "cfg", ".", "VERSION", ",", "latest_ver", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._TestClassA.__init__": [[67, 76], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "@", "configurable", "\n", "def", "__init__", "(", "self", ",", "arg1", ",", "arg2", ",", "arg3", "=", "3", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "arg1", "=", "arg1", "\n", "self", ".", "arg2", "=", "arg2", "\n", "self", ".", "arg3", "=", "arg3", "\n", "assert", "arg1", "==", "1", "\n", "assert", "arg2", "==", "2", "\n", "assert", "arg3", "==", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._TestClassA.from_config": [[77, 81], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "args", "=", "{", "\"arg1\"", ":", "cfg", ".", "ARG1", ",", "\"arg2\"", ":", "cfg", ".", "ARG2", "}", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._TestClassB.__init__": [[84, 91], ["test_yacs_config._TestClassA.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "@", "configurable", "\n", "def", "__init__", "(", "self", ",", "input_shape", ",", "arg1", ",", "arg2", ",", "arg3", "=", "3", ")", ":", "\n", "        ", "\"\"\"\n        Doc of _TestClassB\n        \"\"\"", "\n", "assert", "input_shape", "==", "\"shape\"", "\n", "super", "(", ")", ".", "__init__", "(", "arg1", ",", "arg2", ",", "arg3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._TestClassB.from_config": [[92, 97], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ")", ":", "# test extra positional arg in from_config", "\n", "        ", "args", "=", "{", "\"arg1\"", ":", "cfg", ".", "ARG1", ",", "\"arg2\"", ":", "cfg", ".", "ARG2", "}", "\n", "args", "[", "\"input_shape\"", "]", "=", "input_shape", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._LegacySubClass.__init__": [[101, 106], ["test_yacs_config._TestClassB.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ",", "input_shape", ",", "arg4", "=", "4", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "cfg", ",", "input_shape", ")", "\n", "assert", "self", ".", "arg1", "==", "1", "\n", "assert", "self", ".", "arg2", "==", "2", "\n", "assert", "self", ".", "arg3", "==", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._NewSubClassNewInit.__init__": [[110, 116], ["test_yacs_config._TestClassB.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "@", "configurable", "\n", "def", "__init__", "(", "self", ",", "input_shape", ",", "arg4", "=", "4", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "input_shape", ",", "**", "kwargs", ")", "\n", "assert", "self", ".", "arg1", "==", "1", "\n", "assert", "self", ".", "arg2", "==", "2", "\n", "assert", "self", ".", "arg3", "==", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._LegacySubClassNotCfg.__init__": [[120, 125], ["test_yacs_config._TestClassB.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "input_shape", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ",", "input_shape", ")", "\n", "assert", "self", ".", "arg1", "==", "1", "\n", "assert", "self", ".", "arg2", "==", "2", "\n", "assert", "self", ".", "arg3", "==", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._TestClassC.from_config": [[128, 134], ["args.update"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "input_shape", ",", "**", "kwargs", ")", ":", "# test extra kwarg overwrite", "\n", "        ", "args", "=", "{", "\"arg1\"", ":", "cfg", ".", "ARG1", ",", "\"arg2\"", ":", "cfg", ".", "ARG2", "}", "\n", "args", "[", "\"input_shape\"", "]", "=", "input_shape", "\n", "args", ".", "update", "(", "kwargs", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._TestClassD.__init__": [[137, 141], ["test_yacs_config._TestClassA.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "@", "configurable", "\n", "def", "__init__", "(", "self", ",", "input_shape", ":", "ShapeSpec", ",", "arg1", ":", "int", ",", "arg2", ",", "arg3", "=", "3", ")", ":", "\n", "        ", "assert", "input_shape", "==", "\"shape\"", "\n", "super", "(", ")", ".", "__init__", "(", "arg1", ",", "arg2", ",", "arg3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigurable.testInitWithArgs": [[152, 157], ["test_yacs_config._TestClassA", "test_yacs_config._TestClassB", "test_yacs_config._TestClassC", "test_yacs_config._TestClassD"], "methods", ["None"], ["    ", "def", "testInitWithArgs", "(", "self", ")", ":", "\n", "        ", "_", "=", "_TestClassA", "(", "arg1", "=", "1", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "_", "=", "_TestClassB", "(", "\"shape\"", ",", "arg1", "=", "1", ",", "arg2", "=", "2", ")", "\n", "_", "=", "_TestClassC", "(", "\"shape\"", ",", "arg1", "=", "1", ",", "arg2", "=", "2", ")", "\n", "_", "=", "_TestClassD", "(", "\"shape\"", ",", "arg1", "=", "1", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigurable.testPatchedAttr": [[158, 161], ["test_yacs_config.TestConfigurable.assertTrue", "test_yacs_config.TestConfigurable.assertEqual"], "methods", ["None"], ["", "def", "testPatchedAttr", "(", "self", ")", ":", "\n", "        ", "self", ".", "assertTrue", "(", "\"Doc\"", "in", "_TestClassB", ".", "__init__", ".", "__doc__", ")", "\n", "self", ".", "assertEqual", "(", "_TestClassD", ".", "__init__", ".", "__annotations__", "[", "\"arg1\"", "]", ",", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigurable.testInitWithCfg": [[162, 186], ["detectron2.config.get_cfg", "test_yacs_config._TestClassA", "test_yacs_config._TestClassB", "test_yacs_config._TestClassC", "test_yacs_config._TestClassD", "test_yacs_config._LegacySubClass", "test_yacs_config._NewSubClassNewInit", "test_yacs_config._LegacySubClassNotCfg", "test_yacs_config._TestClassA", "test_yacs_config._TestClassB", "test_yacs_config._TestClassC", "test_yacs_config._TestClassD", "test_yacs_config._LegacySubClass", "test_yacs_config._NewSubClassNewInit", "test_yacs_config._LegacySubClassNotCfg", "test_yacs_config.TestConfigurable.assertRaises", "test_yacs_config._TestClassD"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg"], ["", "def", "testInitWithCfg", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "ARG1", "=", "1", "\n", "cfg", ".", "ARG2", "=", "2", "\n", "cfg", ".", "ARG3", "=", "3", "\n", "_", "=", "_TestClassA", "(", "cfg", ")", "\n", "_", "=", "_TestClassB", "(", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_TestClassC", "(", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_TestClassD", "(", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_LegacySubClass", "(", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_NewSubClassNewInit", "(", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_LegacySubClassNotCfg", "(", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "with", "self", ".", "assertRaises", "(", "TypeError", ")", ":", "\n", "# disallow forwarding positional args to __init__ since it's prone to errors", "\n", "            ", "_", "=", "_TestClassD", "(", "cfg", ",", "\"shape\"", ")", "\n", "\n", "# call with kwargs instead", "\n", "", "_", "=", "_TestClassA", "(", "cfg", "=", "cfg", ")", "\n", "_", "=", "_TestClassB", "(", "cfg", "=", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_TestClassC", "(", "cfg", "=", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_TestClassD", "(", "cfg", "=", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_LegacySubClass", "(", "cfg", "=", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_NewSubClassNewInit", "(", "cfg", "=", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "_", "=", "_LegacySubClassNotCfg", "(", "config", "=", "cfg", ",", "input_shape", "=", "\"shape\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigurable.testInitWithCfgOverwrite": [[187, 205], ["detectron2.config.get_cfg", "test_yacs_config._TestClassA", "test_yacs_config._TestClassB", "test_yacs_config._TestClassC", "test_yacs_config._TestClassD", "test_yacs_config._TestClassA", "test_yacs_config._TestClassB", "test_yacs_config._TestClassC", "test_yacs_config._TestClassD", "test_yacs_config.TestConfigurable.assertRaises", "test_yacs_config._TestClassA"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg"], ["", "def", "testInitWithCfgOverwrite", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "ARG1", "=", "1", "\n", "cfg", ".", "ARG2", "=", "999", "# wrong config", "\n", "with", "self", ".", "assertRaises", "(", "AssertionError", ")", ":", "\n", "            ", "_", "=", "_TestClassA", "(", "cfg", ",", "arg3", "=", "3", ")", "\n", "\n", "# overwrite arg2 with correct config later:", "\n", "", "_", "=", "_TestClassA", "(", "cfg", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "_", "=", "_TestClassB", "(", "cfg", ",", "input_shape", "=", "\"shape\"", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "_", "=", "_TestClassC", "(", "cfg", ",", "input_shape", "=", "\"shape\"", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "_", "=", "_TestClassD", "(", "cfg", ",", "input_shape", "=", "\"shape\"", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "\n", "# call with kwargs cfg=cfg instead", "\n", "_", "=", "_TestClassA", "(", "cfg", "=", "cfg", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "_", "=", "_TestClassB", "(", "cfg", "=", "cfg", ",", "input_shape", "=", "\"shape\"", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "_", "=", "_TestClassC", "(", "cfg", "=", "cfg", ",", "input_shape", "=", "\"shape\"", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "_", "=", "_TestClassD", "(", "cfg", "=", "cfg", ",", "input_shape", "=", "\"shape\"", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigurable.testInitWithCfgWrongArgs": [[206, 216], ["detectron2.config.get_cfg", "test_yacs_config.TestConfigurable.assertRaises", "test_yacs_config._TestClassB", "test_yacs_config.TestConfigurable.assertRaises", "test_yacs_config._TestClassC", "test_yacs_config.TestConfigurable.assertRaises", "test_yacs_config._TestClassD"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg"], ["", "def", "testInitWithCfgWrongArgs", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "ARG1", "=", "1", "\n", "cfg", ".", "ARG2", "=", "2", "\n", "with", "self", ".", "assertRaises", "(", "TypeError", ")", ":", "\n", "            ", "_", "=", "_TestClassB", "(", "cfg", ",", "\"shape\"", ",", "not_exist", "=", "1", ")", "\n", "", "with", "self", ".", "assertRaises", "(", "TypeError", ")", ":", "\n", "            ", "_", "=", "_TestClassC", "(", "cfg", ",", "\"shape\"", ",", "not_exist", "=", "1", ")", "\n", "", "with", "self", ".", "assertRaises", "(", "TypeError", ")", ":", "\n", "            ", "_", "=", "_TestClassD", "(", "cfg", ",", "\"shape\"", ",", "not_exist", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigurable.testBadClass": [[217, 249], ["test_yacs_config.TestConfigurable.assertRaises", "_BadClass1", "test_yacs_config.TestConfigurable.assertRaises", "_BadClass2", "test_yacs_config.TestConfigurable.assertRaises", "_BadClass3", "detectron2.config.get_cfg"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg"], ["", "", "def", "testBadClass", "(", "self", ")", ":", "\n", "        ", "class", "_BadClass1", ":", "\n", "            ", "@", "configurable", "\n", "def", "__init__", "(", "self", ",", "a", "=", "1", ",", "b", "=", "2", ")", ":", "\n", "                ", "pass", "\n", "\n", "", "", "class", "_BadClass2", ":", "\n", "            ", "@", "configurable", "\n", "def", "__init__", "(", "self", ",", "a", "=", "1", ",", "b", "=", "2", ")", ":", "\n", "                ", "pass", "\n", "\n", "", "def", "from_config", "(", "self", ",", "cfg", ")", ":", "# noqa", "\n", "                ", "pass", "\n", "\n", "", "", "class", "_BadClass3", ":", "\n", "            ", "@", "configurable", "\n", "def", "__init__", "(", "self", ",", "a", "=", "1", ",", "b", "=", "2", ")", ":", "\n", "                ", "pass", "\n", "\n", "# bad name: must be cfg", "\n", "", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "config", ")", ":", "# noqa", "\n", "                ", "pass", "\n", "\n", "", "", "with", "self", ".", "assertRaises", "(", "AttributeError", ")", ":", "\n", "            ", "_", "=", "_BadClass1", "(", "a", "=", "1", ")", "\n", "\n", "", "with", "self", ".", "assertRaises", "(", "TypeError", ")", ":", "\n", "            ", "_", "=", "_BadClass2", "(", "a", "=", "1", ")", "\n", "\n", "", "with", "self", ".", "assertRaises", "(", "TypeError", ")", ":", "\n", "            ", "_", "=", "_BadClass3", "(", "get_cfg", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigurable.testFuncWithCfg": [[250, 261], ["detectron2.config.get_cfg", "test_yacs_config.TestConfigurable.assertEqual", "test_yacs_config.TestConfigurable.assertEqual", "test_yacs_config.TestConfigurable.assertEqual", "test_yacs_config.TestConfigurable.assertEqual", "test_yacs_config._test_func", "test_yacs_config.TestConfigurable.assertRaises", "test_yacs_config._test_func", "test_yacs_config._test_func", "test_yacs_config._test_func", "test_yacs_config._test_func"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._test_func", "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._test_func", "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._test_func", "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._test_func", "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._test_func"], ["", "", "def", "testFuncWithCfg", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "ARG1", "=", "10", "\n", "cfg", ".", "ARG3", "=", "30", "\n", "\n", "self", ".", "assertEqual", "(", "_test_func", "(", "1", ")", ",", "(", "1", ",", "2", ",", "3", ",", "4", ")", ")", "\n", "with", "self", ".", "assertRaises", "(", "TypeError", ")", ":", "\n", "            ", "_test_func", "(", "cfg", ")", "\n", "", "self", ".", "assertEqual", "(", "_test_func", "(", "cfg", ",", "arg2", "=", "2", ")", ",", "(", "10", ",", "2", ",", "30", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "_test_func", "(", "cfg", ",", "arg1", "=", "100", ",", "arg2", "=", "20", ")", ",", "(", "100", ",", "20", ",", "30", ",", "4", ")", ")", "\n", "self", ".", "assertEqual", "(", "_test_func", "(", "cfg", ",", "arg1", "=", "100", ",", "arg2", "=", "20", ",", "arg4", "=", "40", ")", ",", "(", "100", ",", "20", ",", "30", ",", "40", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config.TestConfigurable.testOmegaConf": [[262, 269], ["detectron2.model_zoo.get_config", "omegaconf.OmegaConf.create", "detectron2.modeling.build_model", "omegaconf.OmegaConf.create.dump", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump"], ["", "def", "testOmegaConf", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "model_zoo", ".", "get_config", "(", "\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"", ")", "\n", "cfg", "=", "OmegaConf", ".", "create", "(", "cfg", ".", "dump", "(", ")", ")", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "cfg", ".", "MODEL", ".", "DEVICE", "=", "\"cpu\"", "\n", "# test that a model can be built with omegaconf config as well", "\n", "", "build_model", "(", "cfg", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.config.test_yacs_config._test_func": [[146, 149], ["detectron2.config.configurable"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.configurable"], ["", "", "@", "configurable", "(", "from_config", "=", "lambda", "cfg", ",", "arg2", ":", "{", "\"arg1\"", ":", "cfg", ".", "ARG1", ",", "\"arg2\"", ":", "arg2", ",", "\"arg3\"", ":", "cfg", ".", "ARG3", "}", ")", "\n", "def", "_test_func", "(", "arg1", ",", "arg2", "=", "2", ",", "arg3", "=", "3", ",", "arg4", "=", "4", ")", ":", "\n", "    ", "return", "arg1", ",", "arg2", ",", "arg3", ",", "arg4", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers._NewEmptyTensorOp.forward": [[37, 41], ["x.new_empty"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ",", "new_shape", ")", ":", "\n", "        ", "ctx", ".", "shape", "=", "x", ".", "shape", "\n", "return", "x", ".", "new_empty", "(", "new_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers._NewEmptyTensorOp.backward": [[42, 46], ["_NewEmptyTensorOp.apply"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "shape", "=", "ctx", ".", "shape", "\n", "return", "_NewEmptyTensorOp", ".", "apply", "(", "grad", ",", "shape", ")", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.Conv2d.__init__": [[53, 69], ["kwargs.pop", "kwargs.pop", "super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Extra keyword arguments supported in addition to those in `torch.nn.Conv2d`:\n\n        Args:\n            norm (nn.Module, optional): a normalization layer\n            activation (callable(Tensor) -> Tensor): a callable activation function\n\n        It assumes that norm layer is used before activation.\n        \"\"\"", "\n", "norm", "=", "kwargs", ".", "pop", "(", "\"norm\"", ",", "None", ")", "\n", "activation", "=", "kwargs", ".", "pop", "(", "\"activation\"", ",", "None", ")", "\n", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "norm", "=", "norm", "\n", "self", ".", "activation", "=", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.Conv2d.forward": [[70, 92], ["torch.nn.functional.conv2d", "torch.jit.is_scripting", "wrappers.Conv2d.norm", "wrappers.Conv2d.activation", "wrappers.Conv2d.numel", "isinstance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# torchscript does not support SyncBatchNorm yet", "\n", "# https://github.com/pytorch/pytorch/issues/40507", "\n", "# and we skip these codes in torchscript since:", "\n", "# 1. currently we only support torchscript in evaluation mode", "\n", "# 2. features needed by exporting module to torchscript are added in PyTorch 1.6 or", "\n", "# later version, `Conv2d` in these PyTorch versions has already supported empty inputs.", "\n", "        ", "if", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "if", "x", ".", "numel", "(", ")", "==", "0", "and", "self", ".", "training", ":", "\n", "# https://github.com/pytorch/pytorch/issues/12013", "\n", "                ", "assert", "not", "isinstance", "(", "\n", "self", ".", "norm", ",", "torch", ".", "nn", ".", "SyncBatchNorm", "\n", ")", ",", "\"SyncBatchNorm does not support empty inputs!\"", "\n", "\n", "", "", "x", "=", "F", ".", "conv2d", "(", "\n", "x", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", "\n", ")", "\n", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "", "if", "self", ".", "activation", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "activation", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat": [[16, 24], ["isinstance", "torch.cat", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["def", "cat", "(", "tensors", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "dim", ":", "int", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Efficient version of torch.cat that avoids a copy if there is only a single element in a list\n    \"\"\"", "\n", "assert", "isinstance", "(", "tensors", ",", "(", "list", ",", "tuple", ")", ")", "\n", "if", "len", "(", "tensors", ")", "==", "1", ":", "\n", "        ", "return", "tensors", "[", "0", "]", "\n", "", "return", "torch", ".", "cat", "(", "tensors", ",", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy": [[26, 34], ["torch.nn.functional.cross_entropy", "target.numel", "input.sum"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy"], ["", "def", "cross_entropy", "(", "input", ",", "target", ",", "*", ",", "reduction", "=", "\"mean\"", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Same as `torch.nn.functional.cross_entropy`, but returns 0 (instead of nan)\n    for empty inputs.\n    \"\"\"", "\n", "if", "target", ".", "numel", "(", ")", "==", "0", "and", "reduction", "==", "\"mean\"", ":", "\n", "        ", "return", "input", ".", "sum", "(", ")", "*", "0.0", "# connect the gradient", "\n", "", "return", "F", ".", "cross_entropy", "(", "input", ",", "target", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.nonzero_tuple": [[100, 111], ["torch.jit.is_scripting", "x.nonzero().unbind", "x.nonzero", "x.dim", "x.unsqueeze().nonzero().unbind", "x.nonzero", "x.unsqueeze().nonzero", "x.unsqueeze"], "function", ["None"], ["def", "nonzero_tuple", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    A 'as_tuple=True' version of torch.nonzero to support torchscript.\n    because of https://github.com/pytorch/pytorch/issues/38718\n    \"\"\"", "\n", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "        ", "if", "x", ".", "dim", "(", ")", "==", "0", ":", "\n", "            ", "return", "x", ".", "unsqueeze", "(", "0", ")", ".", "nonzero", "(", ")", ".", "unbind", "(", "1", ")", "\n", "", "return", "x", ".", "nonzero", "(", ")", ".", "unbind", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "return", "x", ".", "nonzero", "(", "as_tuple", "=", "True", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms": [[19, 40], ["scores.new_zeros", "torch.jit.annotate", "scores.new_zeros.nonzero().view", "len", "torchvision.ops.boxes.batched_nms", "scores.size", "torch.unique().cpu().tolist", "torchvision.ops.nms", "torchvision.ops.boxes.float", "scores.new_zeros.nonzero", "scores[].argsort", "torch.unique().cpu", "torch.unique"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms"], ["", "def", "batched_nms", "(", "\n", "boxes", ":", "torch", ".", "Tensor", ",", "scores", ":", "torch", ".", "Tensor", ",", "idxs", ":", "torch", ".", "Tensor", ",", "iou_threshold", ":", "float", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Same as torchvision.ops.boxes.batched_nms, but safer.\n    \"\"\"", "\n", "assert", "boxes", ".", "shape", "[", "-", "1", "]", "==", "4", "\n", "# TODO may need better strategy.", "\n", "# Investigate after having a fully-cuda NMS op.", "\n", "if", "len", "(", "boxes", ")", "<", "40000", ":", "\n", "# fp16 does not have enough range for batched NMS", "\n", "        ", "return", "box_ops", ".", "batched_nms", "(", "boxes", ".", "float", "(", ")", ",", "scores", ",", "idxs", ",", "iou_threshold", ")", "\n", "\n", "", "result_mask", "=", "scores", ".", "new_zeros", "(", "scores", ".", "size", "(", ")", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "for", "id", "in", "torch", ".", "jit", ".", "annotate", "(", "List", "[", "int", "]", ",", "torch", ".", "unique", "(", "idxs", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", ")", ":", "\n", "        ", "mask", "=", "(", "idxs", "==", "id", ")", ".", "nonzero", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "keep", "=", "nms", "(", "boxes", "[", "mask", "]", ",", "scores", "[", "mask", "]", ",", "iou_threshold", ")", "\n", "result_mask", "[", "mask", "[", "keep", "]", "]", "=", "True", "\n", "", "keep", "=", "result_mask", ".", "nonzero", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "keep", "=", "keep", "[", "scores", "[", "keep", "]", ".", "argsort", "(", "descending", "=", "True", ")", "]", "\n", "return", "keep", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.nms_rotated": [[44, 106], ["nms_rotated_func"], "function", ["None"], ["", "def", "nms_rotated", "(", "boxes", ",", "scores", ",", "iou_threshold", ")", ":", "\n", "    ", "\"\"\"\n    Performs non-maximum suppression (NMS) on the rotated boxes according\n    to their intersection-over-union (IoU).\n\n    Rotated NMS iteratively removes lower scoring rotated boxes which have an\n    IoU greater than iou_threshold with another (higher scoring) rotated box.\n\n    Note that RotatedBox (5, 3, 4, 2, -90) covers exactly the same region as\n    RotatedBox (5, 3, 4, 2, 90) does, and their IoU will be 1. However, they\n    can be representing completely different objects in certain tasks, e.g., OCR.\n\n    As for the question of whether rotated-NMS should treat them as faraway boxes\n    even though their IOU is 1, it depends on the application and/or ground truth annotation.\n\n    As an extreme example, consider a single character v and the square box around it.\n\n    If the angle is 0 degree, the object (text) would be read as 'v';\n\n    If the angle is 90 degrees, the object (text) would become '>';\n\n    If the angle is 180 degrees, the object (text) would become '^';\n\n    If the angle is 270/-90 degrees, the object (text) would become '<'\n\n    All of these cases have IoU of 1 to each other, and rotated NMS that only\n    uses IoU as criterion would only keep one of them with the highest score -\n    which, practically, still makes sense in most cases because typically\n    only one of theses orientations is the correct one. Also, it does not matter\n    as much if the box is only used to classify the object (instead of transcribing\n    them with a sequential OCR recognition model) later.\n\n    On the other hand, when we use IoU to filter proposals that are close to the\n    ground truth during training, we should definitely take the angle into account if\n    we know the ground truth is labeled with the strictly correct orientation (as in,\n    upside-down words are annotated with -180 degrees even though they can be covered\n    with a 0/90/-90 degree box, etc.)\n\n    The way the original dataset is annotated also matters. For example, if the dataset\n    is a 4-point polygon dataset that does not enforce ordering of vertices/orientation,\n    we can estimate a minimum rotated bounding box to this polygon, but there's no way\n    we can tell the correct angle with 100% confidence (as shown above, there could be 4 different\n    rotated boxes, with angles differed by 90 degrees to each other, covering the exactly\n    same region). In that case we have to just use IoU to determine the box\n    proximity (as many detection benchmarks (even for text) do) unless there're other\n    assumptions we can make (like width is always larger than height, or the object is not\n    rotated by more than 90 degrees CCW/CW, etc.)\n\n    In summary, not considering angles in rotated NMS seems to be a good option for now,\n    but we should be aware of its implications.\n\n    Args:\n        boxes (Tensor[N, 5]): Rotated boxes to perform NMS on. They are expected to be in\n           (x_center, y_center, width, height, angle_degrees) format.\n        scores (Tensor[N]): Scores for each one of the rotated boxes\n        iou_threshold (float): Discards all overlapping rotated boxes with IoU < iou_threshold\n\n    Returns:\n        keep (Tensor): int64 tensor with the indices of the elements that have been kept\n        by Rotated NMS, sorted in decreasing order of scores\n    \"\"\"", "\n", "return", "nms_rotated_func", "(", "boxes", ",", "scores", ",", "iou_threshold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms_rotated": [[110, 159], ["boxes.float.float", "boxes.float.clone", "nms.nms_rotated", "boxes.float.numel", "torch.empty", "idxs.to", "torch.max", "torch.min", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.nms_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "batched_nms_rotated", "(", "boxes", ",", "scores", ",", "idxs", ",", "iou_threshold", ")", ":", "\n", "    ", "\"\"\"\n    Performs non-maximum suppression in a batched fashion.\n\n    Each index value correspond to a category, and NMS\n    will not be applied between elements of different categories.\n\n    Args:\n        boxes (Tensor[N, 5]):\n           boxes where NMS will be performed. They\n           are expected to be in (x_ctr, y_ctr, width, height, angle_degrees) format\n        scores (Tensor[N]):\n           scores for each one of the boxes\n        idxs (Tensor[N]):\n           indices of the categories for each one of the boxes.\n        iou_threshold (float):\n           discards all overlapping boxes\n           with IoU < iou_threshold\n\n    Returns:\n        Tensor:\n            int64 tensor with the indices of the elements that have been kept\n            by NMS, sorted in decreasing order of scores\n    \"\"\"", "\n", "assert", "boxes", ".", "shape", "[", "-", "1", "]", "==", "5", "\n", "\n", "if", "boxes", ".", "numel", "(", ")", "==", "0", ":", "\n", "        ", "return", "torch", ".", "empty", "(", "(", "0", ",", ")", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "boxes", ".", "device", ")", "\n", "", "boxes", "=", "boxes", ".", "float", "(", ")", "# fp16 does not have enough range for batched NMS", "\n", "# Strategy: in order to perform NMS independently per class,", "\n", "# we add an offset to all the boxes. The offset is dependent", "\n", "# only on the class idx, and is large enough so that boxes", "\n", "# from different classes do not overlap", "\n", "\n", "# Note that batched_nms in torchvision/ops/boxes.py only uses max_coordinate,", "\n", "# which won't handle negative coordinates correctly.", "\n", "# Here by using min_coordinate we can make sure the negative coordinates are", "\n", "# correctly handled.", "\n", "max_coordinate", "=", "(", "\n", "torch", ".", "max", "(", "boxes", "[", ":", ",", "0", "]", ",", "boxes", "[", ":", ",", "1", "]", ")", "+", "torch", ".", "max", "(", "boxes", "[", ":", ",", "2", "]", ",", "boxes", "[", ":", ",", "3", "]", ")", "/", "2", "\n", ")", ".", "max", "(", ")", "\n", "min_coordinate", "=", "(", "\n", "torch", ".", "min", "(", "boxes", "[", ":", ",", "0", "]", ",", "boxes", "[", ":", ",", "1", "]", ")", "-", "torch", ".", "max", "(", "boxes", "[", ":", ",", "2", "]", ",", "boxes", "[", ":", ",", "3", "]", ")", "/", "2", "\n", ")", ".", "min", "(", ")", "\n", "offsets", "=", "idxs", ".", "to", "(", "boxes", ")", "*", "(", "max_coordinate", "-", "min_coordinate", "+", "1", ")", "\n", "boxes_for_nms", "=", "boxes", ".", "clone", "(", ")", "# avoid modifying the original values in boxes", "\n", "boxes_for_nms", "[", ":", ",", ":", "2", "]", "+=", "offsets", "[", ":", ",", "None", "]", "\n", "keep", "=", "nms_rotated", "(", "boxes_for_nms", ",", "scores", ",", "iou_threshold", ")", "\n", "return", "keep", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.aspp.ASPP.__init__": [[19, 128], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "aspp.ASPP.convs.append", "fvcore.c2_xavier_fill", "fvcore.c2_xavier_fill", "aspp.ASPP.convs.append", "wrappers.Conv2d", "fvcore.c2_xavier_fill", "len", "len", "wrappers.Conv2d", "torch.nn.Sequential", "torch.nn.Sequential", "aspp.ASPP.convs.append", "aspp.ASPP.convs.append", "fvcore.c2_xavier_fill", "torch.nn.AdaptiveAvgPool2d", "wrappers.Conv2d", "torch.nn.AvgPool2d", "wrappers.Conv2d", "batch_norm.get_norm", "copy.deepcopy", "batch_norm.get_norm", "copy.deepcopy", "blocks.DepthwiseSeparableConv2d", "wrappers.Conv2d", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "batch_norm.get_norm", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "dilations", ",", "\n", "*", ",", "\n", "norm", ",", "\n", "activation", ",", "\n", "pool_kernel_size", "=", "None", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "use_depthwise_separable_conv", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            in_channels (int): number of input channels for ASPP.\n            out_channels (int): number of output channels.\n            dilations (list): a list of 3 dilations in ASPP.\n            norm (str or callable): normalization for all conv layers.\n                See :func:`layers.get_norm` for supported format. norm is\n                applied to all conv layers except the conv following\n                global average pooling.\n            activation (callable): activation function.\n            pool_kernel_size (tuple, list): the average pooling size (kh, kw)\n                for image pooling layer in ASPP. If set to None, it always\n                performs global average pooling. If not None, it must be\n                divisible by the shape of inputs in forward(). It is recommended\n                to use a fixed input feature size in training, and set this\n                option to match this size, so that it performs global average\n                pooling in training, and the size of the pooling window stays\n                consistent in inference.\n            dropout (float): apply dropout on the output of ASPP. It is used in\n                the official DeepLab implementation with a rate of 0.1:\n                https://github.com/tensorflow/models/blob/21b73d22f3ed05b650e85ac50849408dd36de32e/research/deeplab/model.py#L532  # noqa\n            use_depthwise_separable_conv (bool): use DepthwiseSeparableConv2d\n                for 3x3 convs in ASPP, proposed in :paper:`DeepLabV3+`.\n        \"\"\"", "\n", "super", "(", "ASPP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "len", "(", "dilations", ")", "==", "3", ",", "\"ASPP expects 3 dilations, got {}\"", ".", "format", "(", "len", "(", "dilations", ")", ")", "\n", "self", ".", "pool_kernel_size", "=", "pool_kernel_size", "\n", "self", ".", "dropout", "=", "dropout", "\n", "use_bias", "=", "norm", "==", "\"\"", "\n", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", ")", "\n", "# conv 1x1", "\n", "self", ".", "convs", ".", "append", "(", "\n", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "use_bias", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", "activation", "=", "deepcopy", "(", "activation", ")", ",", "\n", ")", "\n", ")", "\n", "weight_init", ".", "c2_xavier_fill", "(", "self", ".", "convs", "[", "-", "1", "]", ")", "\n", "# atrous convs", "\n", "for", "dilation", "in", "dilations", ":", "\n", "            ", "if", "use_depthwise_separable_conv", ":", "\n", "                ", "self", ".", "convs", ".", "append", "(", "\n", "DepthwiseSeparableConv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "dilation", ",", "\n", "dilation", "=", "dilation", ",", "\n", "norm1", "=", "norm", ",", "\n", "activation1", "=", "deepcopy", "(", "activation", ")", ",", "\n", "norm2", "=", "norm", ",", "\n", "activation2", "=", "deepcopy", "(", "activation", ")", ",", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "convs", ".", "append", "(", "\n", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "dilation", ",", "\n", "dilation", "=", "dilation", ",", "\n", "bias", "=", "use_bias", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", "activation", "=", "deepcopy", "(", "activation", ")", ",", "\n", ")", "\n", ")", "\n", "weight_init", ".", "c2_xavier_fill", "(", "self", ".", "convs", "[", "-", "1", "]", ")", "\n", "# image pooling", "\n", "# We do not add BatchNorm because the spatial resolution is 1x1,", "\n", "# the original TF implementation has BatchNorm.", "\n", "", "", "if", "pool_kernel_size", "is", "None", ":", "\n", "            ", "image_pooling", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", ",", "\n", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "1", ",", "bias", "=", "True", ",", "activation", "=", "deepcopy", "(", "activation", ")", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "image_pooling", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "pool_kernel_size", ",", "stride", "=", "1", ")", ",", "\n", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "1", ",", "bias", "=", "True", ",", "activation", "=", "deepcopy", "(", "activation", ")", ")", ",", "\n", ")", "\n", "", "weight_init", ".", "c2_xavier_fill", "(", "image_pooling", "[", "1", "]", ")", "\n", "self", ".", "convs", ".", "append", "(", "image_pooling", ")", "\n", "\n", "self", ".", "project", "=", "Conv2d", "(", "\n", "5", "*", "out_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "use_bias", ",", "\n", "norm", "=", "get_norm", "(", "norm", ",", "out_channels", ")", ",", "\n", "activation", "=", "deepcopy", "(", "activation", ")", ",", "\n", ")", "\n", "weight_init", ".", "c2_xavier_fill", "(", "self", ".", "project", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.aspp.ASPP.forward": [[129, 145], ["torch.nn.functional.interpolate", "torch.cat", "aspp.ASPP.project", "aspp.ASPP.append", "torch.nn.functional.dropout", "ValueError", "conv"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "size", "=", "x", ".", "shape", "[", "-", "2", ":", "]", "\n", "if", "self", ".", "pool_kernel_size", "is", "not", "None", ":", "\n", "            ", "if", "size", "[", "0", "]", "%", "self", ".", "pool_kernel_size", "[", "0", "]", "or", "size", "[", "1", "]", "%", "self", ".", "pool_kernel_size", "[", "1", "]", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"`pool_kernel_size` must be divisible by the shape of inputs. \"", "\n", "\"Input size: {} `pool_kernel_size`: {}\"", ".", "format", "(", "size", ",", "self", ".", "pool_kernel_size", ")", "\n", ")", "\n", "", "", "res", "=", "[", "]", "\n", "for", "conv", "in", "self", ".", "convs", ":", "\n", "            ", "res", ".", "append", "(", "conv", "(", "x", ")", ")", "\n", "", "res", "[", "-", "1", "]", "=", "F", ".", "interpolate", "(", "res", "[", "-", "1", "]", ",", "size", "=", "size", ",", "mode", "=", "\"bilinear\"", ",", "align_corners", "=", "False", ")", "\n", "res", "=", "torch", ".", "cat", "(", "res", ",", "dim", "=", "1", ")", "\n", "res", "=", "self", ".", "project", "(", "res", ")", "\n", "res", "=", "F", ".", "dropout", "(", "res", ",", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "if", "self", ".", "dropout", ">", "0", "else", "res", "\n", "return", "res", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated": [[7, 23], ["detectron2._C.box_iou_rotated"], "function", ["None"], ["\n", "from", ".", "boxes", "import", "Boxes", ",", "_maybe_jit_unused", "\n", "\n", "\n", "class", "RotatedBoxes", "(", "Boxes", ")", ":", "\n", "    ", "\"\"\"\n    This structure stores a list of rotated boxes as a Nx5 torch.Tensor.\n    It supports some common methods about boxes\n    (`area`, `clip`, `nonempty`, etc),\n    and also behaves like a Tensor\n    (support indexing, `to(device)`, `.device`, and iteration over all boxes)\n    \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._DeformConv.forward": [[17, 81], ["torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "ctx.save_for_backward", "input.new_empty", "ValueError", "deform_conv._DeformConv._output_size", "input.new_empty", "input.new_empty", "torchvision.ops.deform_conv2d", "deform_conv._DeformConv._cal_im2col_step", "detectron2._C.deform_conv_forward", "input.dim", "NotImplementedError", "weight.size", "weight.size", "input.dim"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._DeformConv._output_size", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._DeformConv._cal_im2col_step"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "\n", "ctx", ",", "\n", "input", ",", "\n", "offset", ",", "\n", "weight", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "deformable_groups", "=", "1", ",", "\n", "im2col_step", "=", "64", ",", "\n", ")", ":", "\n", "        ", "if", "input", "is", "not", "None", "and", "input", ".", "dim", "(", ")", "!=", "4", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Expected 4D tensor as input, got {}D tensor instead.\"", ".", "format", "(", "input", ".", "dim", "(", ")", ")", "\n", ")", "\n", "", "ctx", ".", "stride", "=", "_pair", "(", "stride", ")", "\n", "ctx", ".", "padding", "=", "_pair", "(", "padding", ")", "\n", "ctx", ".", "dilation", "=", "_pair", "(", "dilation", ")", "\n", "ctx", ".", "groups", "=", "groups", "\n", "ctx", ".", "deformable_groups", "=", "deformable_groups", "\n", "ctx", ".", "im2col_step", "=", "im2col_step", "\n", "\n", "ctx", ".", "save_for_backward", "(", "input", ",", "offset", ",", "weight", ")", "\n", "\n", "output", "=", "input", ".", "new_empty", "(", "\n", "_DeformConv", ".", "_output_size", "(", "input", ",", "weight", ",", "ctx", ".", "padding", ",", "ctx", ".", "dilation", ",", "ctx", ".", "stride", ")", "\n", ")", "\n", "\n", "ctx", ".", "bufs_", "=", "[", "input", ".", "new_empty", "(", "0", ")", ",", "input", ".", "new_empty", "(", "0", ")", "]", "# columns, ones", "\n", "\n", "if", "not", "input", ".", "is_cuda", ":", "\n", "            ", "if", "deformable_groups", "!=", "1", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\n", "\"Deformable Conv with deformable_groups != 1 is not supported on CPUs!\"", "\n", ")", "\n", "", "return", "deform_conv2d", "(", "\n", "input", ",", "offset", ",", "weight", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", "\n", ")", "\n", "", "else", ":", "\n", "            ", "cur_im2col_step", "=", "_DeformConv", ".", "_cal_im2col_step", "(", "input", ".", "shape", "[", "0", "]", ",", "ctx", ".", "im2col_step", ")", "\n", "assert", "(", "input", ".", "shape", "[", "0", "]", "%", "cur_im2col_step", ")", "==", "0", ",", "\"im2col step must divide batchsize\"", "\n", "\n", "_C", ".", "deform_conv_forward", "(", "\n", "input", ",", "\n", "weight", ",", "\n", "offset", ",", "\n", "output", ",", "\n", "ctx", ".", "bufs_", "[", "0", "]", ",", "\n", "ctx", ".", "bufs_", "[", "1", "]", ",", "\n", "weight", ".", "size", "(", "3", ")", ",", "\n", "weight", ".", "size", "(", "2", ")", ",", "\n", "ctx", ".", "stride", "[", "1", "]", ",", "\n", "ctx", ".", "stride", "[", "0", "]", ",", "\n", "ctx", ".", "padding", "[", "1", "]", ",", "\n", "ctx", ".", "padding", "[", "0", "]", ",", "\n", "ctx", ".", "dilation", "[", "1", "]", ",", "\n", "ctx", ".", "dilation", "[", "0", "]", ",", "\n", "ctx", ".", "groups", ",", "\n", "ctx", ".", "deformable_groups", ",", "\n", "cur_im2col_step", ",", "\n", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._DeformConv.backward": [[82, 143], ["NotImplementedError", "deform_conv._DeformConv._cal_im2col_step", "torch.zeros_like", "torch.zeros_like", "detectron2._C.deform_conv_backward_input", "torch.zeros_like", "detectron2._C.deform_conv_backward_filter", "weight.size", "weight.size", "weight.size", "weight.size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._DeformConv._cal_im2col_step"], ["", "@", "staticmethod", "\n", "@", "once_differentiable", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "input", ",", "offset", ",", "weight", "=", "ctx", ".", "saved_tensors", "\n", "\n", "grad_input", "=", "grad_offset", "=", "grad_weight", "=", "None", "\n", "\n", "if", "not", "grad_output", ".", "is_cuda", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Deformable Conv is not supported on CPUs!\"", ")", "\n", "", "else", ":", "\n", "            ", "cur_im2col_step", "=", "_DeformConv", ".", "_cal_im2col_step", "(", "input", ".", "shape", "[", "0", "]", ",", "ctx", ".", "im2col_step", ")", "\n", "assert", "(", "input", ".", "shape", "[", "0", "]", "%", "cur_im2col_step", ")", "==", "0", ",", "\"im2col step must divide batchsize\"", "\n", "\n", "if", "ctx", ".", "needs_input_grad", "[", "0", "]", "or", "ctx", ".", "needs_input_grad", "[", "1", "]", ":", "\n", "                ", "grad_input", "=", "torch", ".", "zeros_like", "(", "input", ")", "\n", "grad_offset", "=", "torch", ".", "zeros_like", "(", "offset", ")", "\n", "_C", ".", "deform_conv_backward_input", "(", "\n", "input", ",", "\n", "offset", ",", "\n", "grad_output", ",", "\n", "grad_input", ",", "\n", "grad_offset", ",", "\n", "weight", ",", "\n", "ctx", ".", "bufs_", "[", "0", "]", ",", "\n", "weight", ".", "size", "(", "3", ")", ",", "\n", "weight", ".", "size", "(", "2", ")", ",", "\n", "ctx", ".", "stride", "[", "1", "]", ",", "\n", "ctx", ".", "stride", "[", "0", "]", ",", "\n", "ctx", ".", "padding", "[", "1", "]", ",", "\n", "ctx", ".", "padding", "[", "0", "]", ",", "\n", "ctx", ".", "dilation", "[", "1", "]", ",", "\n", "ctx", ".", "dilation", "[", "0", "]", ",", "\n", "ctx", ".", "groups", ",", "\n", "ctx", ".", "deformable_groups", ",", "\n", "cur_im2col_step", ",", "\n", ")", "\n", "\n", "", "if", "ctx", ".", "needs_input_grad", "[", "2", "]", ":", "\n", "                ", "grad_weight", "=", "torch", ".", "zeros_like", "(", "weight", ")", "\n", "_C", ".", "deform_conv_backward_filter", "(", "\n", "input", ",", "\n", "offset", ",", "\n", "grad_output", ",", "\n", "grad_weight", ",", "\n", "ctx", ".", "bufs_", "[", "0", "]", ",", "\n", "ctx", ".", "bufs_", "[", "1", "]", ",", "\n", "weight", ".", "size", "(", "3", ")", ",", "\n", "weight", ".", "size", "(", "2", ")", ",", "\n", "ctx", ".", "stride", "[", "1", "]", ",", "\n", "ctx", ".", "stride", "[", "0", "]", ",", "\n", "ctx", ".", "padding", "[", "1", "]", ",", "\n", "ctx", ".", "padding", "[", "0", "]", ",", "\n", "ctx", ".", "dilation", "[", "1", "]", ",", "\n", "ctx", ".", "dilation", "[", "0", "]", ",", "\n", "ctx", ".", "groups", ",", "\n", "ctx", ".", "deformable_groups", ",", "\n", "1", ",", "\n", "cur_im2col_step", ",", "\n", ")", "\n", "\n", "", "", "return", "grad_input", ",", "grad_offset", ",", "grad_weight", ",", "None", ",", "None", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._DeformConv._output_size": [[144, 161], ["weight.size", "range", "input.size", "input.size", "all", "ValueError", "input.dim", "map", "weight.size", "map"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_output_size", "(", "input", ",", "weight", ",", "padding", ",", "dilation", ",", "stride", ")", ":", "\n", "        ", "channels", "=", "weight", ".", "size", "(", "0", ")", "\n", "output_size", "=", "(", "input", ".", "size", "(", "0", ")", ",", "channels", ")", "\n", "for", "d", "in", "range", "(", "input", ".", "dim", "(", ")", "-", "2", ")", ":", "\n", "            ", "in_size", "=", "input", ".", "size", "(", "d", "+", "2", ")", "\n", "pad", "=", "padding", "[", "d", "]", "\n", "kernel", "=", "dilation", "[", "d", "]", "*", "(", "weight", ".", "size", "(", "d", "+", "2", ")", "-", "1", ")", "+", "1", "\n", "stride_", "=", "stride", "[", "d", "]", "\n", "output_size", "+=", "(", "(", "in_size", "+", "(", "2", "*", "pad", ")", "-", "kernel", ")", "//", "stride_", "+", "1", ",", ")", "\n", "", "if", "not", "all", "(", "map", "(", "lambda", "s", ":", "s", ">", "0", ",", "output_size", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"convolution input is too small (output would be {})\"", ".", "format", "(", "\n", "\"x\"", ".", "join", "(", "map", "(", "str", ",", "output_size", ")", ")", "\n", ")", "\n", ")", "\n", "", "return", "output_size", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._DeformConv._cal_im2col_step": [[162, 184], ["functools.lru_cache", "range", "min", "int", "math.sqrt"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "lru_cache", "(", "maxsize", "=", "128", ")", "\n", "def", "_cal_im2col_step", "(", "input_size", ",", "default_size", ")", ":", "\n", "        ", "\"\"\"\n        Calculate proper im2col step size, which should be divisible by input_size and not larger\n        than prefer_size. Meanwhile the step size should be as large as possible to be more\n        efficient. So we choose the largest one among all divisors of input_size which are smaller\n        than prefer_size.\n        :param input_size: input batch size .\n        :param default_size: default preferred im2col step size.\n        :return: the largest proper step size.\n        \"\"\"", "\n", "if", "input_size", "<=", "default_size", ":", "\n", "            ", "return", "input_size", "\n", "", "best_step", "=", "1", "\n", "for", "step", "in", "range", "(", "2", ",", "min", "(", "int", "(", "math", ".", "sqrt", "(", "input_size", ")", ")", "+", "1", ",", "default_size", ")", ")", ":", "\n", "            ", "if", "input_size", "%", "step", "==", "0", ":", "\n", "                ", "if", "input_size", "//", "step", "<=", "default_size", ":", "\n", "                    ", "return", "input_size", "//", "step", "\n", "", "best_step", "=", "step", "\n", "\n", "", "", "return", "best_step", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._ModulatedDeformConv.forward": [[187, 242], ["input.new_empty", "detectron2._C.modulated_deform_conv_forward", "input.new_empty", "NotImplementedError", "ctx.save_for_backward", "deform_conv._ModulatedDeformConv._infer_shape", "input.new_empty", "input.new_empty"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._ModulatedDeformConv._infer_shape"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "\n", "ctx", ",", "\n", "input", ",", "\n", "offset", ",", "\n", "mask", ",", "\n", "weight", ",", "\n", "bias", "=", "None", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "deformable_groups", "=", "1", ",", "\n", ")", ":", "\n", "        ", "ctx", ".", "stride", "=", "stride", "\n", "ctx", ".", "padding", "=", "padding", "\n", "ctx", ".", "dilation", "=", "dilation", "\n", "ctx", ".", "groups", "=", "groups", "\n", "ctx", ".", "deformable_groups", "=", "deformable_groups", "\n", "ctx", ".", "with_bias", "=", "bias", "is", "not", "None", "\n", "if", "not", "ctx", ".", "with_bias", ":", "\n", "            ", "bias", "=", "input", ".", "new_empty", "(", "1", ")", "# fake tensor", "\n", "", "if", "not", "input", ".", "is_cuda", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Deformable Conv is not supported on CPUs!\"", ")", "\n", "", "if", "(", "\n", "weight", ".", "requires_grad", "\n", "or", "mask", ".", "requires_grad", "\n", "or", "offset", ".", "requires_grad", "\n", "or", "input", ".", "requires_grad", "\n", ")", ":", "\n", "            ", "ctx", ".", "save_for_backward", "(", "input", ",", "offset", ",", "mask", ",", "weight", ",", "bias", ")", "\n", "", "output", "=", "input", ".", "new_empty", "(", "_ModulatedDeformConv", ".", "_infer_shape", "(", "ctx", ",", "input", ",", "weight", ")", ")", "\n", "ctx", ".", "_bufs", "=", "[", "input", ".", "new_empty", "(", "0", ")", ",", "input", ".", "new_empty", "(", "0", ")", "]", "\n", "_C", ".", "modulated_deform_conv_forward", "(", "\n", "input", ",", "\n", "weight", ",", "\n", "bias", ",", "\n", "ctx", ".", "_bufs", "[", "0", "]", ",", "\n", "offset", ",", "\n", "mask", ",", "\n", "output", ",", "\n", "ctx", ".", "_bufs", "[", "1", "]", ",", "\n", "weight", ".", "shape", "[", "2", "]", ",", "\n", "weight", ".", "shape", "[", "3", "]", ",", "\n", "ctx", ".", "stride", ",", "\n", "ctx", ".", "stride", ",", "\n", "ctx", ".", "padding", ",", "\n", "ctx", ".", "padding", ",", "\n", "ctx", ".", "dilation", ",", "\n", "ctx", ".", "dilation", ",", "\n", "ctx", ".", "groups", ",", "\n", "ctx", ".", "deformable_groups", ",", "\n", "ctx", ".", "with_bias", ",", "\n", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._ModulatedDeformConv.backward": [[243, 294], ["torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "detectron2._C.modulated_deform_conv_backward", "NotImplementedError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "once_differentiable", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "if", "not", "grad_output", ".", "is_cuda", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Deformable Conv is not supported on CPUs!\"", ")", "\n", "", "input", ",", "offset", ",", "mask", ",", "weight", ",", "bias", "=", "ctx", ".", "saved_tensors", "\n", "grad_input", "=", "torch", ".", "zeros_like", "(", "input", ")", "\n", "grad_offset", "=", "torch", ".", "zeros_like", "(", "offset", ")", "\n", "grad_mask", "=", "torch", ".", "zeros_like", "(", "mask", ")", "\n", "grad_weight", "=", "torch", ".", "zeros_like", "(", "weight", ")", "\n", "grad_bias", "=", "torch", ".", "zeros_like", "(", "bias", ")", "\n", "_C", ".", "modulated_deform_conv_backward", "(", "\n", "input", ",", "\n", "weight", ",", "\n", "bias", ",", "\n", "ctx", ".", "_bufs", "[", "0", "]", ",", "\n", "offset", ",", "\n", "mask", ",", "\n", "ctx", ".", "_bufs", "[", "1", "]", ",", "\n", "grad_input", ",", "\n", "grad_weight", ",", "\n", "grad_bias", ",", "\n", "grad_offset", ",", "\n", "grad_mask", ",", "\n", "grad_output", ",", "\n", "weight", ".", "shape", "[", "2", "]", ",", "\n", "weight", ".", "shape", "[", "3", "]", ",", "\n", "ctx", ".", "stride", ",", "\n", "ctx", ".", "stride", ",", "\n", "ctx", ".", "padding", ",", "\n", "ctx", ".", "padding", ",", "\n", "ctx", ".", "dilation", ",", "\n", "ctx", ".", "dilation", ",", "\n", "ctx", ".", "groups", ",", "\n", "ctx", ".", "deformable_groups", ",", "\n", "ctx", ".", "with_bias", ",", "\n", ")", "\n", "if", "not", "ctx", ".", "with_bias", ":", "\n", "            ", "grad_bias", "=", "None", "\n", "\n", "", "return", "(", "\n", "grad_input", ",", "\n", "grad_offset", ",", "\n", "grad_mask", ",", "\n", "grad_weight", ",", "\n", "grad_bias", ",", "\n", "None", ",", "\n", "None", ",", "\n", "None", ",", "\n", "None", ",", "\n", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv._ModulatedDeformConv._infer_shape": [[296, 309], ["input.size", "weight.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_infer_shape", "(", "ctx", ",", "input", ",", "weight", ")", ":", "\n", "        ", "n", "=", "input", ".", "size", "(", "0", ")", "\n", "channels_out", "=", "weight", ".", "size", "(", "0", ")", "\n", "height", ",", "width", "=", "input", ".", "shape", "[", "2", ":", "4", "]", "\n", "kernel_h", ",", "kernel_w", "=", "weight", ".", "shape", "[", "2", ":", "4", "]", "\n", "height_out", "=", "(", "\n", "height", "+", "2", "*", "ctx", ".", "padding", "-", "(", "ctx", ".", "dilation", "*", "(", "kernel_h", "-", "1", ")", "+", "1", ")", "\n", ")", "//", "ctx", ".", "stride", "+", "1", "\n", "width_out", "=", "(", "\n", "width", "+", "2", "*", "ctx", ".", "padding", "-", "(", "ctx", ".", "dilation", "*", "(", "kernel_w", "-", "1", ")", "+", "1", ")", "\n", ")", "//", "ctx", ".", "stride", "+", "1", "\n", "return", "n", ",", "channels_out", ",", "height_out", ",", "width_out", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv.DeformConv.__init__": [[316, 367], ["torch.nn.Module.__init__", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.Parameter", "torch.nn.init.kaiming_uniform_", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "deformable_groups", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "norm", "=", "None", ",", "\n", "activation", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Deformable convolution from :paper:`deformconv`.\n\n        Arguments are similar to :class:`Conv2D`. Extra arguments:\n\n        Args:\n            deformable_groups (int): number of groups used in deformable convolution.\n            norm (nn.Module, optional): a normalization layer\n            activation (callable(Tensor) -> Tensor): a callable activation function\n        \"\"\"", "\n", "super", "(", "DeformConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "not", "bias", "\n", "assert", "in_channels", "%", "groups", "==", "0", ",", "\"in_channels {} cannot be divisible by groups {}\"", ".", "format", "(", "\n", "in_channels", ",", "groups", "\n", ")", "\n", "assert", "(", "\n", "out_channels", "%", "groups", "==", "0", "\n", ")", ",", "\"out_channels {} cannot be divisible by groups {}\"", ".", "format", "(", "out_channels", ",", "groups", ")", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "_pair", "(", "kernel_size", ")", "\n", "self", ".", "stride", "=", "_pair", "(", "stride", ")", "\n", "self", ".", "padding", "=", "_pair", "(", "padding", ")", "\n", "self", ".", "dilation", "=", "_pair", "(", "dilation", ")", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "deformable_groups", "=", "deformable_groups", "\n", "self", ".", "norm", "=", "norm", "\n", "self", ".", "activation", "=", "activation", "\n", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "out_channels", ",", "in_channels", "//", "self", ".", "groups", ",", "*", "self", ".", "kernel_size", ")", "\n", ")", "\n", "self", ".", "bias", "=", "None", "\n", "\n", "nn", ".", "init", ".", "kaiming_uniform_", "(", "self", ".", "weight", ",", "nonlinearity", "=", "\"relu\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv.DeformConv.forward": [[368, 398], ["deform_conv", "deform_conv.DeformConv.numel", "wrappers._NewEmptyTensorOp.apply", "deform_conv.DeformConv.norm", "deform_conv.DeformConv.activation", "zip"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "offset", ")", ":", "\n", "        ", "if", "x", ".", "numel", "(", ")", "==", "0", ":", "\n", "# When input is empty, we want to return a empty tensor with \"correct\" shape,", "\n", "# So that the following operations will not panic", "\n", "# if they check for the shape of the tensor.", "\n", "# This computes the height and width of the output tensor", "\n", "            ", "output_shape", "=", "[", "\n", "(", "i", "+", "2", "*", "p", "-", "(", "di", "*", "(", "k", "-", "1", ")", "+", "1", ")", ")", "//", "s", "+", "1", "\n", "for", "i", ",", "p", ",", "di", ",", "k", ",", "s", "in", "zip", "(", "\n", "x", ".", "shape", "[", "-", "2", ":", "]", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", "\n", ")", "\n", "]", "\n", "output_shape", "=", "[", "x", ".", "shape", "[", "0", "]", ",", "self", ".", "weight", ".", "shape", "[", "0", "]", "]", "+", "output_shape", "\n", "return", "_NewEmptyTensorOp", ".", "apply", "(", "x", ",", "output_shape", ")", "\n", "\n", "", "x", "=", "deform_conv", "(", "\n", "x", ",", "\n", "offset", ",", "\n", "self", ".", "weight", ",", "\n", "self", ".", "stride", ",", "\n", "self", ".", "padding", ",", "\n", "self", ".", "dilation", ",", "\n", "self", ".", "groups", ",", "\n", "self", ".", "deformable_groups", ",", "\n", ")", "\n", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "", "if", "self", ".", "activation", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "activation", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv.DeformConv.extra_repr": [[399, 410], ["str", "str", "str", "str", "str", "str", "str", "str"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "tmpstr", "=", "\"in_channels=\"", "+", "str", "(", "self", ".", "in_channels", ")", "\n", "tmpstr", "+=", "\", out_channels=\"", "+", "str", "(", "self", ".", "out_channels", ")", "\n", "tmpstr", "+=", "\", kernel_size=\"", "+", "str", "(", "self", ".", "kernel_size", ")", "\n", "tmpstr", "+=", "\", stride=\"", "+", "str", "(", "self", ".", "stride", ")", "\n", "tmpstr", "+=", "\", padding=\"", "+", "str", "(", "self", ".", "padding", ")", "\n", "tmpstr", "+=", "\", dilation=\"", "+", "str", "(", "self", ".", "dilation", ")", "\n", "tmpstr", "+=", "\", groups=\"", "+", "str", "(", "self", ".", "groups", ")", "\n", "tmpstr", "+=", "\", deformable_groups=\"", "+", "str", "(", "self", ".", "deformable_groups", ")", "\n", "tmpstr", "+=", "\", bias=False\"", "\n", "return", "tmpstr", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv.ModulatedDeformConv.__init__": [[413, 461], ["torch.nn.Module.__init__", "torch.nn.modules.utils._pair", "torch.nn.Parameter", "torch.nn.init.kaiming_uniform_", "torch.Tensor", "torch.nn.Parameter", "torch.nn.init.constant_", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "deformable_groups", "=", "1", ",", "\n", "bias", "=", "True", ",", "\n", "norm", "=", "None", ",", "\n", "activation", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Modulated deformable convolution from :paper:`deformconv2`.\n\n        Arguments are similar to :class:`Conv2D`. Extra arguments:\n\n        Args:\n            deformable_groups (int): number of groups used in deformable convolution.\n            norm (nn.Module, optional): a normalization layer\n            activation (callable(Tensor) -> Tensor): a callable activation function\n        \"\"\"", "\n", "super", "(", "ModulatedDeformConv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "_pair", "(", "kernel_size", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "deformable_groups", "=", "deformable_groups", "\n", "self", ".", "with_bias", "=", "bias", "\n", "self", ".", "norm", "=", "norm", "\n", "self", ".", "activation", "=", "activation", "\n", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "out_channels", ",", "in_channels", "//", "groups", ",", "*", "self", ".", "kernel_size", ")", "\n", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "out_channels", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias", "=", "None", "\n", "\n", "", "nn", ".", "init", ".", "kaiming_uniform_", "(", "self", ".", "weight", ",", "nonlinearity", "=", "\"relu\"", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv.ModulatedDeformConv.forward": [[462, 490], ["modulated_deform_conv", "deform_conv.ModulatedDeformConv.numel", "wrappers._NewEmptyTensorOp.apply", "deform_conv.ModulatedDeformConv.norm", "deform_conv.ModulatedDeformConv.activation", "zip"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "offset", ",", "mask", ")", ":", "\n", "        ", "if", "x", ".", "numel", "(", ")", "==", "0", ":", "\n", "            ", "output_shape", "=", "[", "\n", "(", "i", "+", "2", "*", "p", "-", "(", "di", "*", "(", "k", "-", "1", ")", "+", "1", ")", ")", "//", "s", "+", "1", "\n", "for", "i", ",", "p", ",", "di", ",", "k", ",", "s", "in", "zip", "(", "\n", "x", ".", "shape", "[", "-", "2", ":", "]", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", "\n", ")", "\n", "]", "\n", "output_shape", "=", "[", "x", ".", "shape", "[", "0", "]", ",", "self", ".", "weight", ".", "shape", "[", "0", "]", "]", "+", "output_shape", "\n", "return", "_NewEmptyTensorOp", ".", "apply", "(", "x", ",", "output_shape", ")", "\n", "\n", "", "x", "=", "modulated_deform_conv", "(", "\n", "x", ",", "\n", "offset", ",", "\n", "mask", ",", "\n", "self", ".", "weight", ",", "\n", "self", ".", "bias", ",", "\n", "self", ".", "stride", ",", "\n", "self", ".", "padding", ",", "\n", "self", ".", "dilation", ",", "\n", "self", ".", "groups", ",", "\n", "self", ".", "deformable_groups", ",", "\n", ")", "\n", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "", "if", "self", ".", "activation", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "activation", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.deform_conv.ModulatedDeformConv.extra_repr": [[491, 502], ["str", "str", "str", "str", "str", "str", "str", "str", "str"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "tmpstr", "=", "\"in_channels=\"", "+", "str", "(", "self", ".", "in_channels", ")", "\n", "tmpstr", "+=", "\", out_channels=\"", "+", "str", "(", "self", ".", "out_channels", ")", "\n", "tmpstr", "+=", "\", kernel_size=\"", "+", "str", "(", "self", ".", "kernel_size", ")", "\n", "tmpstr", "+=", "\", stride=\"", "+", "str", "(", "self", ".", "stride", ")", "\n", "tmpstr", "+=", "\", padding=\"", "+", "str", "(", "self", ".", "padding", ")", "\n", "tmpstr", "+=", "\", dilation=\"", "+", "str", "(", "self", ".", "dilation", ")", "\n", "tmpstr", "+=", "\", groups=\"", "+", "str", "(", "self", ".", "groups", ")", "\n", "tmpstr", "+=", "\", deformable_groups=\"", "+", "str", "(", "self", ".", "deformable_groups", ")", "\n", "tmpstr", "+=", "\", bias=\"", "+", "str", "(", "self", ".", "with_bias", ")", "\n", "return", "tmpstr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms.soft_nms": [[8, 44], ["soft_nms._soft_nms"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms._soft_nms"], ["def", "soft_nms", "(", "boxes", ",", "scores", ",", "method", ",", "gaussian_sigma", ",", "linear_threshold", ",", "prune_threshold", ")", ":", "\n", "    ", "\"\"\"\n    Performs soft non-maximum suppression algorithm on axis aligned boxes\n    Args:\n        boxes (Tensor[N, 5]):\n           boxes where NMS will be performed. They\n           are expected to be in (x_ctr, y_ctr, width, height, angle_degrees) format\n        scores (Tensor[N]):\n           scores for each one of the boxes\n        method (str):\n           one of ['gaussian', 'linear', 'hard']\n           see paper for details. users encouraged not to use \"hard\", as this is the\n           same nms available elsewhere in detectron2\n        gaussian_sigma (float):\n           parameter for Gaussian penalty function\n        linear_threshold (float):\n           iou threshold for applying linear decay. Nt from the paper\n           re-used as threshold for standard \"hard\" nms\n        prune_threshold (float):\n           boxes with scores below this threshold are pruned at each iteration.\n           Dramatically reduces computation time. Authors use values in [10e-4, 10e-2]\n    Returns:\n        tuple(Tensor, Tensor):\n            [0]: int64 tensor with the indices of the elements that have been kept\n            by Soft NMS, sorted in decreasing order of scores\n            [1]: float tensor with the re-scored scores of the elements that were kept\n\"\"\"", "\n", "return", "_soft_nms", "(", "\n", "Boxes", ",", "\n", "pairwise_iou", ",", "\n", "boxes", ",", "\n", "scores", ",", "\n", "method", ",", "\n", "gaussian_sigma", ",", "\n", "linear_threshold", ",", "\n", "prune_threshold", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms.soft_nms_rotated": [[47, 82], ["soft_nms._soft_nms"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms._soft_nms"], ["", "def", "soft_nms_rotated", "(", "boxes", ",", "scores", ",", "method", ",", "gaussian_sigma", ",", "linear_threshold", ",", "prune_threshold", ")", ":", "\n", "    ", "\"\"\"\n    Performs soft non-maximum suppression algorithm on rotated boxes\n    Args:\n        boxes (Tensor[N, 5]):\n           boxes where NMS will be performed. They\n           are expected to be in (x_ctr, y_ctr, width, height, angle_degrees) format\n        scores (Tensor[N]):\n           scores for each one of the boxes\n        method (str):\n           one of ['gaussian', 'linear', 'hard']\n           see paper for details. users encouraged not to use \"hard\", as this is the\n           same nms available elsewhere in detectron2\n        gaussian_sigma (float):\n           parameter for Gaussian penalty function\n        linear_threshold (float):\n           iou threshold for applying linear decay. Nt from the paper\n           re-used as threshold for standard \"hard\" nms\n        prune_threshold (float):\n           boxes with scores below this threshold are pruned at each iteration.\n           Dramatically reduces computation time. Authors use values in [10e-4, 10e-2]\n    Returns:\n        tuple(Tensor, Tensor):\n            [0]: int64 tensor with the indices of the elements that have been kept\n            by Soft NMS, sorted in decreasing order of scores\n            [1]: float tensor with the re-scored scores of the elements that were kept    \"\"\"", "\n", "return", "_soft_nms", "(", "\n", "RotatedBoxes", ",", "\n", "pairwise_iou_rotated", ",", "\n", "boxes", ",", "\n", "scores", ",", "\n", "method", ",", "\n", "gaussian_sigma", ",", "\n", "linear_threshold", ",", "\n", "prune_threshold", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms.batched_soft_nms": [[85, 132], ["boxes.max", "soft_nms.soft_nms", "boxes.numel", "idxs.to", "torch.empty", "torch.empty"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms.soft_nms", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "batched_soft_nms", "(", "\n", "boxes", ",", "scores", ",", "idxs", ",", "method", ",", "gaussian_sigma", ",", "linear_threshold", ",", "prune_threshold", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Performs soft non-maximum suppression in a batched fashion.\n    Each index value correspond to a category, and NMS\n    will not be applied between elements of different categories.\n    Args:\n        boxes (Tensor[N, 4]):\n           boxes where NMS will be performed. They\n           are expected to be in (x1, y1, x2, y2) format\n        scores (Tensor[N]):\n           scores for each one of the boxes\n        idxs (Tensor[N]):\n           indices of the categories for each one of the boxes.\n        method (str):\n           one of ['gaussian', 'linear', 'hard']\n           see paper for details. users encouraged not to use \"hard\", as this is the\n           same nms available elsewhere in detectron2\n        gaussian_sigma (float):\n           parameter for Gaussian penalty function\n        linear_threshold (float):\n           iou threshold for applying linear decay. Nt from the paper\n           re-used as threshold for standard \"hard\" nms\n        prune_threshold (float):\n           boxes with scores below this threshold are pruned at each iteration.\n           Dramatically reduces computation time. Authors use values in [10e-4, 10e-2]\n    Returns:\n        tuple(Tensor, Tensor):\n            [0]: int64 tensor with the indices of the elements that have been kept\n            by Soft NMS, sorted in decreasing order of scores\n            [1]: float tensor with the re-scored scores of the elements that were kept\n    \"\"\"", "\n", "if", "boxes", ".", "numel", "(", ")", "==", "0", ":", "\n", "        ", "return", "(", "\n", "torch", ".", "empty", "(", "(", "0", ",", ")", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "boxes", ".", "device", ")", ",", "\n", "torch", ".", "empty", "(", "(", "0", ",", ")", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "scores", ".", "device", ")", ",", "\n", ")", "\n", "# strategy: in order to perform NMS independently per class.", "\n", "# we add an offset to all the boxes. The offset is dependent", "\n", "# only on the class idx, and is large enough so that boxes", "\n", "# from different classes do not overlap", "\n", "", "max_coordinate", "=", "boxes", ".", "max", "(", ")", "\n", "offsets", "=", "idxs", ".", "to", "(", "boxes", ")", "*", "(", "max_coordinate", "+", "1", ")", "\n", "boxes_for_nms", "=", "boxes", "+", "offsets", "[", ":", ",", "None", "]", "\n", "return", "soft_nms", "(", "\n", "boxes_for_nms", ",", "scores", ",", "method", ",", "gaussian_sigma", ",", "linear_threshold", ",", "prune_threshold", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms.batched_soft_nms_rotated": [[135, 183], ["boxes.clone", "soft_nms.soft_nms_rotated", "boxes.numel", "boxes[].max", "torch.norm().max", "idxs.to", "torch.empty", "torch.empty", "torch.norm"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms.soft_nms_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "batched_soft_nms_rotated", "(", "\n", "boxes", ",", "scores", ",", "idxs", ",", "method", ",", "gaussian_sigma", ",", "linear_threshold", ",", "prune_threshold", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Performs soft non-maximum suppression in a batched fashion on rotated bounding boxes.\n    Each index value correspond to a category, and NMS\n    will not be applied between elements of different categories.\n    Args:\n        boxes (Tensor[N, 5]):\n           boxes where NMS will be performed. They\n           are expected to be in (x_ctr, y_ctr, width, height, angle_degrees) format\n        scores (Tensor[N]):\n           scores for each one of the boxes\n        idxs (Tensor[N]):\n           indices of the categories for each one of the boxes.\n        method (str):\n           one of ['gaussian', 'linear', 'hard']\n           see paper for details. users encouraged not to use \"hard\", as this is the\n           same nms available elsewhere in detectron2\n        gaussian_sigma (float):\n           parameter for Gaussian penalty function\n        linear_threshold (float):\n           iou threshold for applying linear decay. Nt from the paper\n           re-used as threshold for standard \"hard\" nms\n        prune_threshold (float):\n           boxes with scores below this threshold are pruned at each iteration.\n           Dramatically reduces computation time. Authors use values in [10e-4, 10e-2]\n    Returns:\n        tuple(Tensor, Tensor):\n            [0]: int64 tensor with the indices of the elements that have been kept\n            by Soft NMS, sorted in decreasing order of scores\n            [1]: float tensor with the re-scored scores of the elements that were kept\n    \"\"\"", "\n", "if", "boxes", ".", "numel", "(", ")", "==", "0", ":", "\n", "        ", "return", "(", "\n", "torch", ".", "empty", "(", "(", "0", ",", ")", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "boxes", ".", "device", ")", ",", "\n", "torch", ".", "empty", "(", "(", "0", ",", ")", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "scores", ".", "device", ")", ",", "\n", ")", "\n", "# strategy: in order to perform NMS independently per class.", "\n", "# we add an offset to all the boxes. The offset is dependent", "\n", "# only on the class idx, and is large enough so that boxes", "\n", "# from different classes do not overlap", "\n", "", "max_coordinate", "=", "boxes", "[", ":", ",", ":", "2", "]", ".", "max", "(", ")", "+", "torch", ".", "norm", "(", "boxes", "[", ":", ",", "2", ":", "4", "]", ",", "2", ",", "dim", "=", "1", ")", ".", "max", "(", ")", "\n", "offsets", "=", "idxs", ".", "to", "(", "boxes", ")", "*", "(", "max_coordinate", "+", "1", ")", "\n", "boxes_for_nms", "=", "boxes", ".", "clone", "(", ")", "\n", "boxes_for_nms", "[", ":", ",", ":", "2", "]", "+=", "offsets", "[", ":", ",", "None", "]", "\n", "return", "soft_nms_rotated", "(", "\n", "boxes_for_nms", ",", "scores", ",", "method", ",", "gaussian_sigma", ",", "linear_threshold", ",", "prune_threshold", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.soft_nms._soft_nms": [[186, 262], ["boxes.clone.clone", "scores.clone.clone", "torch.arange", "scores.clone.numel", "torch.argmax", "idxs_out.append", "scores_out.append", "torch.tensor().to", "torch.tensor().to", "scores.clone.size", "idxs[].item", "scores[].item", "torch.ones_like", "torch.exp", "torch.tensor", "torch.tensor", "top_box.unsqueeze", "NotImplementedError", "torch.pow", "detectron2.structures.Boxes", "detectron2.structures.pairwise_iou", "detectron2.structures.RotatedBoxes", "detectron2.structures.pairwise_iou_rotated"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated"], ["", "def", "_soft_nms", "(", "\n", "box_class", ",", "\n", "pairwise_iou_func", ",", "\n", "boxes", ",", "\n", "scores", ",", "\n", "method", ",", "\n", "gaussian_sigma", ",", "\n", "linear_threshold", ",", "\n", "prune_threshold", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Soft non-max suppression algorithm.\n    Implementation of [Soft-NMS -- Improving Object Detection With One Line of Codec]\n    (https://arxiv.org/abs/1704.04503)\n    Args:\n        box_class (cls): one of Box, RotatedBoxes\n        pairwise_iou_func (func): one of pairwise_iou, pairwise_iou_rotated\n        boxes (Tensor[N, ?]):\n           boxes where NMS will be performed\n           if Boxes, in (x1, y1, x2, y2) format\n           if RotatedBoxes, in (x_ctr, y_ctr, width, height, angle_degrees) format\n        scores (Tensor[N]):\n           scores for each one of the boxes\n        method (str):\n           one of ['gaussian', 'linear', 'hard']\n           see paper for details. users encouraged not to use \"hard\", as this is the\n           same nms available elsewhere in detectron2\n        gaussian_sigma (float):\n           parameter for Gaussian penalty function\n        linear_threshold (float):\n           iou threshold for applying linear decay. Nt from the paper\n           re-used as threshold for standard \"hard\" nms\n        prune_threshold (float):\n           boxes with scores below this threshold are pruned at each iteration.\n           Dramatically reduces computation time. Authors use values in [10e-4, 10e-2]\n    Returns:\n        tuple(Tensor, Tensor):\n            [0]: int64 tensor with the indices of the elements that have been kept\n            by Soft NMS, sorted in decreasing order of scores\n            [1]: float tensor with the re-scored scores of the elements that were kept\n    \"\"\"", "\n", "boxes", "=", "boxes", ".", "clone", "(", ")", "\n", "scores", "=", "scores", ".", "clone", "(", ")", "\n", "idxs", "=", "torch", ".", "arange", "(", "scores", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "\n", "idxs_out", "=", "[", "]", "\n", "scores_out", "=", "[", "]", "\n", "\n", "while", "scores", ".", "numel", "(", ")", ">", "0", ":", "\n", "        ", "top_idx", "=", "torch", ".", "argmax", "(", "scores", ")", "\n", "idxs_out", ".", "append", "(", "idxs", "[", "top_idx", "]", ".", "item", "(", ")", ")", "\n", "scores_out", ".", "append", "(", "scores", "[", "top_idx", "]", ".", "item", "(", ")", ")", "\n", "\n", "top_box", "=", "boxes", "[", "top_idx", "]", "\n", "ious", "=", "pairwise_iou_func", "(", "box_class", "(", "top_box", ".", "unsqueeze", "(", "0", ")", ")", ",", "box_class", "(", "boxes", ")", ")", "[", "0", "]", "\n", "\n", "if", "method", "==", "\"linear\"", ":", "\n", "            ", "decay", "=", "torch", ".", "ones_like", "(", "ious", ")", "\n", "decay_mask", "=", "ious", ">", "linear_threshold", "\n", "decay", "[", "decay_mask", "]", "=", "1", "-", "ious", "[", "decay_mask", "]", "\n", "", "elif", "method", "==", "\"gaussian\"", ":", "\n", "            ", "decay", "=", "torch", ".", "exp", "(", "-", "torch", ".", "pow", "(", "ious", ",", "2", ")", "/", "gaussian_sigma", ")", "\n", "", "elif", "method", "==", "\"hard\"", ":", "# standard NMS", "\n", "            ", "decay", "=", "(", "ious", "<", "linear_threshold", ")", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"{} soft nms method not implemented.\"", ".", "format", "(", "method", ")", ")", "\n", "\n", "", "scores", "*=", "decay", "\n", "keep", "=", "scores", ">", "prune_threshold", "\n", "keep", "[", "top_idx", "]", "=", "False", "\n", "\n", "boxes", "=", "boxes", "[", "keep", "]", "\n", "scores", "=", "scores", "[", "keep", "]", "\n", "idxs", "=", "idxs", "[", "keep", "]", "\n", "\n", "", "return", "torch", ".", "tensor", "(", "idxs_out", ")", ".", "to", "(", "boxes", ".", "device", ")", ",", "torch", ".", "tensor", "(", "scores_out", ")", ".", "to", "(", "scores", ".", "device", ")", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.__init__": [[29, 42], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "stride", ")", ":", "\n", "        ", "\"\"\"\n        The `__init__` method of any subclass should also contain these arguments.\n\n        Args:\n            in_channels (int):\n            out_channels (int):\n            stride (int):\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze": [[43, 56], ["blocks.CNNBlockBase.parameters", "batch_norm.FrozenBatchNorm2d.convert_frozen_batchnorm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.FrozenBatchNorm2d.convert_frozen_batchnorm"], ["", "def", "freeze", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Make this block not trainable.\n        This method sets all parameters to `requires_grad=False`,\n        and convert all BatchNorm layers to FrozenBatchNorm\n\n        Returns:\n            the block itself\n        \"\"\"", "\n", "for", "p", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "", "FrozenBatchNorm2d", ".", "convert_frozen_batchnorm", "(", "self", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.DepthwiseSeparableConv2d.__init__": [[66, 109], ["torch.nn.Module.__init__", "wrappers.Conv2d", "wrappers.Conv2d", "fvcore.c2_msra_fill", "fvcore.c2_msra_fill", "batch_norm.get_norm", "batch_norm.get_norm"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "*", ",", "\n", "norm1", "=", "None", ",", "\n", "activation1", "=", "None", ",", "\n", "norm2", "=", "None", ",", "\n", "activation2", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            norm1, norm2 (str or callable): normalization for the two conv layers.\n            activation1, activation2 (callable(Tensor) -> Tensor): activation\n                function for the two conv layers.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "depthwise", "=", "Conv2d", "(", "\n", "in_channels", ",", "\n", "in_channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "padding", ",", "\n", "dilation", "=", "dilation", ",", "\n", "groups", "=", "in_channels", ",", "\n", "bias", "=", "not", "norm1", ",", "\n", "norm", "=", "get_norm", "(", "norm1", ",", "in_channels", ")", ",", "\n", "activation", "=", "activation1", ",", "\n", ")", "\n", "self", ".", "pointwise", "=", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "not", "norm2", ",", "\n", "norm", "=", "get_norm", "(", "norm2", ",", "out_channels", ")", ",", "\n", "activation", "=", "activation2", ",", "\n", ")", "\n", "\n", "# default initialization", "\n", "weight_init", ".", "c2_msra_fill", "(", "self", ".", "depthwise", ")", "\n", "weight_init", ".", "c2_msra_fill", "(", "self", ".", "pointwise", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.DepthwiseSeparableConv2d.forward": [[110, 112], ["blocks.DepthwiseSeparableConv2d.pointwise", "blocks.DepthwiseSeparableConv2d.depthwise"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "pointwise", "(", "self", ".", "depthwise", "(", "x", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops._do_paste_mask": [[19, 72], ["torch.split", "img_x[].expand", "img_y[].expand", "torch.stack", "torch.nn.functional.grid_sample", "torch.clamp().to", "torch.clamp().to", "torch.clamp().to", "torch.arange", "torch.arange", "img_y.size", "img_x.size", "img_y.size", "img_x.size", "torch.jit.is_scripting", "torch.stack.to", "torch.jit.is_scripting", "masks.float.float", "torch.jit.is_scripting", "torch.clamp", "torch.clamp", "torch.clamp", "slice", "slice", "boxes[].max().ceil", "boxes[].max().ceil", "boxes.min().values.floor", "boxes[].max", "boxes[].max", "boxes.min"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["def", "_do_paste_mask", "(", "masks", ",", "boxes", ",", "img_h", ":", "int", ",", "img_w", ":", "int", ",", "skip_empty", ":", "bool", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        masks: N, 1, H, W\n        boxes: N, 4\n        img_h, img_w (int):\n        skip_empty (bool): only paste masks within the region that\n            tightly bound all boxes, and returns the results this region only.\n            An important optimization for CPU.\n\n    Returns:\n        if skip_empty == False, a mask of shape (N, img_h, img_w)\n        if skip_empty == True, a mask of shape (N, h', w'), and the slice\n            object for the corresponding region.\n    \"\"\"", "\n", "# On GPU, paste all masks together (up to chunk size)", "\n", "# by using the entire image to sample the masks", "\n", "# Compared to pasting them one by one,", "\n", "# this has more operations but is faster on COCO-scale dataset.", "\n", "device", "=", "masks", ".", "device", "\n", "\n", "if", "skip_empty", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "        ", "x0_int", ",", "y0_int", "=", "torch", ".", "clamp", "(", "boxes", ".", "min", "(", "dim", "=", "0", ")", ".", "values", ".", "floor", "(", ")", "[", ":", "2", "]", "-", "1", ",", "min", "=", "0", ")", ".", "to", "(", "\n", "dtype", "=", "torch", ".", "int32", "\n", ")", "\n", "x1_int", "=", "torch", ".", "clamp", "(", "boxes", "[", ":", ",", "2", "]", ".", "max", "(", ")", ".", "ceil", "(", ")", "+", "1", ",", "max", "=", "img_w", ")", ".", "to", "(", "dtype", "=", "torch", ".", "int32", ")", "\n", "y1_int", "=", "torch", ".", "clamp", "(", "boxes", "[", ":", ",", "3", "]", ".", "max", "(", ")", ".", "ceil", "(", ")", "+", "1", ",", "max", "=", "img_h", ")", ".", "to", "(", "dtype", "=", "torch", ".", "int32", ")", "\n", "", "else", ":", "\n", "        ", "x0_int", ",", "y0_int", "=", "0", ",", "0", "\n", "x1_int", ",", "y1_int", "=", "img_w", ",", "img_h", "\n", "", "x0", ",", "y0", ",", "x1", ",", "y1", "=", "torch", ".", "split", "(", "boxes", ",", "1", ",", "dim", "=", "1", ")", "# each is Nx1", "\n", "\n", "N", "=", "masks", ".", "shape", "[", "0", "]", "\n", "\n", "img_y", "=", "torch", ".", "arange", "(", "y0_int", ",", "y1_int", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "+", "0.5", "\n", "img_x", "=", "torch", ".", "arange", "(", "x0_int", ",", "x1_int", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "+", "0.5", "\n", "img_y", "=", "(", "img_y", "-", "y0", ")", "/", "(", "y1", "-", "y0", ")", "*", "2", "-", "1", "\n", "img_x", "=", "(", "img_x", "-", "x0", ")", "/", "(", "x1", "-", "x0", ")", "*", "2", "-", "1", "\n", "# img_x, img_y have shapes (N, w), (N, h)", "\n", "\n", "gx", "=", "img_x", "[", ":", ",", "None", ",", ":", "]", ".", "expand", "(", "N", ",", "img_y", ".", "size", "(", "1", ")", ",", "img_x", ".", "size", "(", "1", ")", ")", "\n", "gy", "=", "img_y", "[", ":", ",", ":", ",", "None", "]", ".", "expand", "(", "N", ",", "img_y", ".", "size", "(", "1", ")", ",", "img_x", ".", "size", "(", "1", ")", ")", "\n", "grid", "=", "torch", ".", "stack", "(", "[", "gx", ",", "gy", "]", ",", "dim", "=", "3", ")", "\n", "\n", "if", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "        ", "if", "not", "masks", ".", "dtype", ".", "is_floating_point", ":", "\n", "            ", "masks", "=", "masks", ".", "float", "(", ")", "\n", "", "", "img_masks", "=", "F", ".", "grid_sample", "(", "masks", ",", "grid", ".", "to", "(", "masks", ".", "dtype", ")", ",", "align_corners", "=", "False", ")", "\n", "\n", "if", "skip_empty", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "        ", "return", "img_masks", "[", ":", ",", "0", "]", ",", "(", "slice", "(", "y0_int", ",", "y1_int", ")", ",", "slice", "(", "x0_int", ",", "x1_int", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "img_masks", "[", ":", ",", "0", "]", ",", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.paste_masks_in_image": [[74, 148], ["len", "torch.chunk", "torch.zeros", "masks.new_empty", "isinstance", "len", "torch.jit.is_scripting", "int", "torch.arange", "mask_ops._do_paste_mask", "torch.jit.is_scripting", "numpy.ceil", "int", "int"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops._do_paste_mask"], ["", "", "def", "paste_masks_in_image", "(", "\n", "masks", ":", "torch", ".", "Tensor", ",", "boxes", ":", "Boxes", ",", "image_shape", ":", "Tuple", "[", "int", ",", "int", "]", ",", "threshold", ":", "float", "=", "0.5", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Paste a set of masks that are of a fixed resolution (e.g., 28 x 28) into an image.\n    The location, height, and width for pasting each mask is determined by their\n    corresponding bounding boxes in boxes.\n\n    Note:\n        This is a complicated but more accurate implementation. In actual deployment, it is\n        often enough to use a faster but less accurate implementation.\n        See :func:`paste_mask_in_image_old` in this file for an alternative implementation.\n\n    Args:\n        masks (tensor): Tensor of shape (Bimg, Hmask, Wmask), where Bimg is the number of\n            detected object instances in the image and Hmask, Wmask are the mask width and mask\n            height of the predicted mask (e.g., Hmask = Wmask = 28). Values are in [0, 1].\n        boxes (Boxes or Tensor): A Boxes of length Bimg or Tensor of shape (Bimg, 4).\n            boxes[i] and masks[i] correspond to the same object instance.\n        image_shape (tuple): height, width\n        threshold (float): A threshold in [0, 1] for converting the (soft) masks to\n            binary masks.\n\n    Returns:\n        img_masks (Tensor): A tensor of shape (Bimg, Himage, Wimage), where Bimg is the\n        number of detected object instances and Himage, Wimage are the image width\n        and height. img_masks[i] is a binary mask for object instance i.\n    \"\"\"", "\n", "\n", "assert", "masks", ".", "shape", "[", "-", "1", "]", "==", "masks", ".", "shape", "[", "-", "2", "]", ",", "\"Only square mask predictions are supported\"", "\n", "N", "=", "len", "(", "masks", ")", "\n", "if", "N", "==", "0", ":", "\n", "        ", "return", "masks", ".", "new_empty", "(", "(", "0", ",", ")", "+", "image_shape", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "", "if", "not", "isinstance", "(", "boxes", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "boxes", "=", "boxes", ".", "tensor", "\n", "", "device", "=", "boxes", ".", "device", "\n", "assert", "len", "(", "boxes", ")", "==", "N", ",", "boxes", ".", "shape", "\n", "\n", "img_h", ",", "img_w", "=", "image_shape", "\n", "\n", "# The actual implementation split the input into chunks,", "\n", "# and paste them chunk by chunk.", "\n", "if", "device", ".", "type", "==", "\"cpu\"", "or", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "# CPU is most efficient when they are pasted one by one with skip_empty=True", "\n", "# so that it performs minimal number of operations.", "\n", "        ", "num_chunks", "=", "N", "\n", "", "else", ":", "\n", "# GPU benefits from parallelism for larger chunks, but may have memory issue", "\n", "# int(img_h) because shape may be tensors in tracing", "\n", "        ", "num_chunks", "=", "int", "(", "np", ".", "ceil", "(", "N", "*", "int", "(", "img_h", ")", "*", "int", "(", "img_w", ")", "*", "BYTES_PER_FLOAT", "/", "GPU_MEM_LIMIT", ")", ")", "\n", "assert", "(", "\n", "num_chunks", "<=", "N", "\n", ")", ",", "\"Default GPU_MEM_LIMIT in mask_ops.py is too small; try increasing it\"", "\n", "", "chunks", "=", "torch", ".", "chunk", "(", "torch", ".", "arange", "(", "N", ",", "device", "=", "device", ")", ",", "num_chunks", ")", "\n", "\n", "img_masks", "=", "torch", ".", "zeros", "(", "\n", "N", ",", "img_h", ",", "img_w", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "bool", "if", "threshold", ">=", "0", "else", "torch", ".", "uint8", "\n", ")", "\n", "for", "inds", "in", "chunks", ":", "\n", "        ", "masks_chunk", ",", "spatial_inds", "=", "_do_paste_mask", "(", "\n", "masks", "[", "inds", ",", "None", ",", ":", ",", ":", "]", ",", "boxes", "[", "inds", "]", ",", "img_h", ",", "img_w", ",", "skip_empty", "=", "device", ".", "type", "==", "\"cpu\"", "\n", ")", "\n", "\n", "if", "threshold", ">=", "0", ":", "\n", "            ", "masks_chunk", "=", "(", "masks_chunk", ">=", "threshold", ")", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", "\n", "", "else", ":", "\n", "# for visualization and debugging", "\n", "            ", "masks_chunk", "=", "(", "masks_chunk", "*", "255", ")", ".", "to", "(", "dtype", "=", "torch", ".", "uint8", ")", "\n", "\n", "", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "# Scripting does not use the optimized codepath", "\n", "            ", "img_masks", "[", "inds", "]", "=", "masks_chunk", "\n", "", "else", ":", "\n", "            ", "img_masks", "[", "(", "inds", ",", ")", "+", "spatial_inds", "]", "=", "masks_chunk", "\n", "", "", "return", "img_masks", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.paste_mask_in_image_old": [[155, 208], ["box.to.to", "PIL.Image.fromarray", "torch.from_numpy().to.resize", "numpy.array", "torch.zeros", "max", "min", "max", "min", "torch.from_numpy().to.cpu().numpy", "numpy.array", "torch.from_numpy", "torch.from_numpy().to", "torch.from_numpy().to.cpu", "torch.from_numpy"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "paste_mask_in_image_old", "(", "mask", ",", "box", ",", "img_h", ",", "img_w", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"\n    Paste a single mask in an image.\n    This is a per-box implementation of :func:`paste_masks_in_image`.\n    This function has larger quantization error due to incorrect pixel\n    modeling and is not used any more.\n\n    Args:\n        mask (Tensor): A tensor of shape (Hmask, Wmask) storing the mask of a single\n            object instance. Values are in [0, 1].\n        box (Tensor): A tensor of shape (4, ) storing the x0, y0, x1, y1 box corners\n            of the object instance.\n        img_h, img_w (int): Image height and width.\n        threshold (float): Mask binarization threshold in [0, 1].\n\n    Returns:\n        im_mask (Tensor):\n            The resized and binarized object mask pasted into the original\n            image plane (a tensor of shape (img_h, img_w)).\n    \"\"\"", "\n", "# Conversion from continuous box coordinates to discrete pixel coordinates", "\n", "# via truncation (cast to int32). This determines which pixels to paste the", "\n", "# mask onto.", "\n", "box", "=", "box", ".", "to", "(", "dtype", "=", "torch", ".", "int32", ")", "# Continuous to discrete coordinate conversion", "\n", "# An example (1D) box with continuous coordinates (x0=0.7, x1=4.3) will map to", "\n", "# a discrete coordinates (x0=0, x1=4). Note that box is mapped to 5 = x1 - x0 + 1", "\n", "# pixels (not x1 - x0 pixels).", "\n", "samples_w", "=", "box", "[", "2", "]", "-", "box", "[", "0", "]", "+", "1", "# Number of pixel samples, *not* geometric width", "\n", "samples_h", "=", "box", "[", "3", "]", "-", "box", "[", "1", "]", "+", "1", "# Number of pixel samples, *not* geometric height", "\n", "\n", "# Resample the mask from it's original grid to the new samples_w x samples_h grid", "\n", "mask", "=", "Image", ".", "fromarray", "(", "mask", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "mask", "=", "mask", ".", "resize", "(", "(", "samples_w", ",", "samples_h", ")", ",", "resample", "=", "Image", ".", "BILINEAR", ")", "\n", "mask", "=", "np", ".", "array", "(", "mask", ",", "copy", "=", "False", ")", "\n", "\n", "if", "threshold", ">=", "0", ":", "\n", "        ", "mask", "=", "np", ".", "array", "(", "mask", ">", "threshold", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "mask", "=", "torch", ".", "from_numpy", "(", "mask", ")", "\n", "", "else", ":", "\n", "# for visualization and debugging, we also", "\n", "# allow it to return an unmodified mask", "\n", "        ", "mask", "=", "torch", ".", "from_numpy", "(", "mask", "*", "255", ")", ".", "to", "(", "torch", ".", "uint8", ")", "\n", "\n", "", "im_mask", "=", "torch", ".", "zeros", "(", "(", "img_h", ",", "img_w", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "x_0", "=", "max", "(", "box", "[", "0", "]", ",", "0", ")", "\n", "x_1", "=", "min", "(", "box", "[", "2", "]", "+", "1", ",", "img_w", ")", "\n", "y_0", "=", "max", "(", "box", "[", "1", "]", ",", "0", ")", "\n", "y_1", "=", "min", "(", "box", "[", "3", "]", "+", "1", ",", "img_h", ")", "\n", "\n", "im_mask", "[", "y_0", ":", "y_1", ",", "x_0", ":", "x_1", "]", "=", "mask", "[", "\n", "(", "y_0", "-", "box", "[", "1", "]", ")", ":", "(", "y_1", "-", "box", "[", "1", "]", ")", ",", "(", "x_0", "-", "box", "[", "0", "]", ")", ":", "(", "x_1", "-", "box", "[", "0", "]", ")", "\n", "]", "\n", "return", "im_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.pad_masks": [[219, 235], ["masks.new_zeros", "float"], "function", ["None"], ["", "def", "pad_masks", "(", "masks", ",", "padding", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        masks (tensor): A tensor of shape (B, M, M) representing B masks.\n        padding (int): Number of cells to pad on all sides.\n\n    Returns:\n        The padded masks and the scale factor of the padding size / original size.\n    \"\"\"", "\n", "B", "=", "masks", ".", "shape", "[", "0", "]", "\n", "M", "=", "masks", ".", "shape", "[", "-", "1", "]", "\n", "pad2", "=", "2", "*", "padding", "\n", "scale", "=", "float", "(", "M", "+", "pad2", ")", "/", "M", "\n", "padded_masks", "=", "masks", ".", "new_zeros", "(", "(", "B", ",", "M", "+", "pad2", ",", "M", "+", "pad2", ")", ")", "\n", "padded_masks", "[", ":", ",", "padding", ":", "-", "padding", ",", "padding", ":", "-", "padding", "]", "=", "masks", "\n", "return", "padded_masks", ",", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.scale_boxes": [[237, 261], ["torch.zeros_like"], "function", ["None"], ["", "def", "scale_boxes", "(", "boxes", ",", "scale", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        boxes (tensor): A tensor of shape (B, 4) representing B boxes with 4\n            coords representing the corners x0, y0, x1, y1,\n        scale (float): The box scaling factor.\n\n    Returns:\n        Scaled boxes.\n    \"\"\"", "\n", "w_half", "=", "(", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", ")", "*", "0.5", "\n", "h_half", "=", "(", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", ")", "*", "0.5", "\n", "x_c", "=", "(", "boxes", "[", ":", ",", "2", "]", "+", "boxes", "[", ":", ",", "0", "]", ")", "*", "0.5", "\n", "y_c", "=", "(", "boxes", "[", ":", ",", "3", "]", "+", "boxes", "[", ":", ",", "1", "]", ")", "*", "0.5", "\n", "\n", "w_half", "*=", "scale", "\n", "h_half", "*=", "scale", "\n", "\n", "scaled_boxes", "=", "torch", ".", "zeros_like", "(", "boxes", ")", "\n", "scaled_boxes", "[", ":", ",", "0", "]", "=", "x_c", "-", "w_half", "\n", "scaled_boxes", "[", ":", ",", "2", "]", "=", "x_c", "+", "w_half", "\n", "scaled_boxes", "[", ":", ",", "1", "]", "=", "y_c", "-", "h_half", "\n", "scaled_boxes", "[", ":", ",", "3", "]", "=", "y_c", "+", "h_half", "\n", "return", "scaled_boxes", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.FrozenBatchNorm2d.__init__": [[36, 44], ["torch.nn.Module.__init__", "batch_norm.FrozenBatchNorm2d.register_buffer", "batch_norm.FrozenBatchNorm2d.register_buffer", "batch_norm.FrozenBatchNorm2d.register_buffer", "batch_norm.FrozenBatchNorm2d.register_buffer", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "num_features", ",", "eps", "=", "1e-5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "register_buffer", "(", "\"weight\"", ",", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "register_buffer", "(", "\"bias\"", ",", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "register_buffer", "(", "\"running_mean\"", ",", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "register_buffer", "(", "\"running_var\"", ",", "torch", ".", "ones", "(", "num_features", ")", "-", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.FrozenBatchNorm2d.forward": [[45, 66], ["scale.reshape.reshape.reshape", "bias.reshape.reshape.reshape", "torch.nn.functional.batch_norm", "torch.nn.functional.batch_norm", "bias.reshape.reshape.to", "scale.reshape.reshape.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "x", ".", "requires_grad", ":", "\n", "# When gradients are needed, F.batch_norm will use extra memory", "\n", "# because its backward op computes gradients for weight/bias as well.", "\n", "            ", "scale", "=", "self", ".", "weight", "*", "(", "self", ".", "running_var", "+", "self", ".", "eps", ")", ".", "rsqrt", "(", ")", "\n", "bias", "=", "self", ".", "bias", "-", "self", ".", "running_mean", "*", "scale", "\n", "scale", "=", "scale", ".", "reshape", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "bias", "=", "bias", ".", "reshape", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "out_dtype", "=", "x", ".", "dtype", "# may be half", "\n", "return", "x", "*", "scale", ".", "to", "(", "out_dtype", ")", "+", "bias", ".", "to", "(", "out_dtype", ")", "\n", "", "else", ":", "\n", "# When gradients are not needed, F.batch_norm is a single fused op", "\n", "# and provide more optimization opportunities.", "\n", "            ", "return", "F", ".", "batch_norm", "(", "\n", "x", ",", "\n", "self", ".", "running_mean", ",", "\n", "self", ".", "running_var", ",", "\n", "self", ".", "weight", ",", "\n", "self", ".", "bias", ",", "\n", "training", "=", "False", ",", "\n", "eps", "=", "self", ".", "eps", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.FrozenBatchNorm2d._load_from_state_dict": [[68, 98], ["local_metadata.get", "super()._load_from_state_dict", "logging.getLogger", "logging.getLogger.info", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "prefix.rstrip"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.FrozenBatchNorm2d._load_from_state_dict"], ["", "", "def", "_load_from_state_dict", "(", "\n", "self", ",", "state_dict", ",", "prefix", ",", "local_metadata", ",", "strict", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", "\n", ")", ":", "\n", "        ", "version", "=", "local_metadata", ".", "get", "(", "\"version\"", ",", "None", ")", "\n", "\n", "if", "version", "is", "None", "or", "version", "<", "2", ":", "\n", "# when use offline modules, avoid overwriting running mean and var for loaded weights", "\n", "            ", "skip_reset", "=", "False", "\n", "for", "k_n", "in", "state_dict", ":", "# checkpoint weights", "\n", "                ", "if", "'ignore_others'", "in", "k_n", ":", "#if 'offline' in k_n:", "\n", "                    ", "skip_reset", "=", "True", "\n", "", "", "if", "not", "skip_reset", ":", "\n", "# No running_mean/var in early versions", "\n", "# This will silent the warnings", "\n", "                ", "if", "prefix", "+", "\"running_mean\"", "not", "in", "state_dict", ":", "\n", "                    ", "state_dict", "[", "prefix", "+", "\"running_mean\"", "]", "=", "torch", ".", "zeros_like", "(", "self", ".", "running_mean", ")", "\n", "", "if", "prefix", "+", "\"running_var\"", "not", "in", "state_dict", ":", "\n", "                    ", "state_dict", "[", "prefix", "+", "\"running_var\"", "]", "=", "torch", ".", "ones_like", "(", "self", ".", "running_var", ")", "\n", "\n", "# NOTE: if a checkpoint is trained with BatchNorm and loaded (together with", "\n", "# version number) to FrozenBatchNorm, running_var will be wrong. One solution", "\n", "# is to remove the version number from the checkpoint.", "\n", "", "", "", "if", "version", "is", "not", "None", "and", "version", "<", "3", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"FrozenBatchNorm {} is upgraded to version 3.\"", ".", "format", "(", "prefix", ".", "rstrip", "(", "\".\"", ")", ")", ")", "\n", "# In version < 3, running_var are used without +eps.", "\n", "state_dict", "[", "prefix", "+", "\"running_var\"", "]", "-=", "self", ".", "eps", "\n", "\n", "", "super", "(", ")", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "strict", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.FrozenBatchNorm2d.__repr__": [[100, 102], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"FrozenBatchNorm2d(num_features={}, eps={})\"", ".", "format", "(", "self", ".", "num_features", ",", "self", ".", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.FrozenBatchNorm2d.convert_frozen_batchnorm": [[103, 135], ["isinstance", "cls", "module.named_children", "module.weight.data.clone().detach", "module.bias.data.clone().detach", "cls.convert_frozen_batchnorm", "cls.add_module", "module.weight.data.clone", "module.bias.data.clone"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.FrozenBatchNorm2d.convert_frozen_batchnorm", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["", "@", "classmethod", "\n", "def", "convert_frozen_batchnorm", "(", "cls", ",", "module", ")", ":", "\n", "        ", "\"\"\"\n        Convert all BatchNorm/SyncBatchNorm in module into FrozenBatchNorm.\n\n        Args:\n            module (torch.nn.Module):\n\n        Returns:\n            If module is BatchNorm/SyncBatchNorm, returns a new module.\n            Otherwise, in-place convert module and return it.\n\n        Similar to convert_sync_batchnorm in\n        https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py\n        \"\"\"", "\n", "bn_module", "=", "nn", ".", "modules", ".", "batchnorm", "\n", "bn_module", "=", "(", "bn_module", ".", "BatchNorm2d", ",", "bn_module", ".", "SyncBatchNorm", ")", "\n", "res", "=", "module", "\n", "if", "isinstance", "(", "module", ",", "bn_module", ")", ":", "\n", "            ", "res", "=", "cls", "(", "module", ".", "num_features", ")", "\n", "if", "module", ".", "affine", ":", "\n", "                ", "res", ".", "weight", ".", "data", "=", "module", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "res", ".", "bias", ".", "data", "=", "module", ".", "bias", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "res", ".", "running_mean", ".", "data", "=", "module", ".", "running_mean", ".", "data", "\n", "res", ".", "running_var", ".", "data", "=", "module", ".", "running_var", ".", "data", "\n", "res", ".", "eps", "=", "module", ".", "eps", "\n", "", "else", ":", "\n", "            ", "for", "name", ",", "child", "in", "module", ".", "named_children", "(", ")", ":", "\n", "                ", "new_child", "=", "cls", ".", "convert_frozen_batchnorm", "(", "child", ")", "\n", "if", "new_child", "is", "not", "child", ":", "\n", "                    ", "res", ".", "add_module", "(", "name", ",", "new_child", ")", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.NaiveSyncBatchNorm.__init__": [[193, 197], ["wrappers.BatchNorm2d.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "stats_mode", "=", "\"\"", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "assert", "stats_mode", "in", "[", "\"\"", ",", "\"N\"", "]", "\n", "self", ".", "_stats_mode", "=", "stats_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.NaiveSyncBatchNorm.forward": [[198, 244], ["torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "scale.reshape.reshape.reshape", "bias.reshape.reshape.reshape", "super().forward", "input.float.float.float", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.split", "torch.split", "torch.split", "torch.split", "fvcore.nn.distributed.differentiable_all_reduce", "vec[].detach", "torch.split", "torch.split", "torch.split", "torch.split", "ret.half.half.half", "detectron2.utils.comm.get_world_size", "fvcore.nn.distributed.differentiable_all_reduce", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "vec[].detach.clamp", "torch.mean.detach", "torch.mean.detach", "var.detach", "torch.get_world_size", "torch.get_world_size", "input.float.float.sum", "vec[].detach.clamp", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "if", "comm", ".", "get_world_size", "(", ")", "==", "1", "or", "not", "self", ".", "training", ":", "\n", "            ", "return", "super", "(", ")", ".", "forward", "(", "input", ")", "\n", "\n", "", "B", ",", "C", "=", "input", ".", "shape", "[", "0", "]", ",", "input", ".", "shape", "[", "1", "]", "\n", "\n", "half_input", "=", "input", ".", "dtype", "==", "torch", ".", "float16", "\n", "if", "half_input", ":", "\n", "# fp16 does not have good enough numerics for the reduction here", "\n", "            ", "input", "=", "input", ".", "float", "(", ")", "\n", "", "mean", "=", "torch", ".", "mean", "(", "input", ",", "dim", "=", "[", "0", ",", "2", ",", "3", "]", ")", "\n", "meansqr", "=", "torch", ".", "mean", "(", "input", "*", "input", ",", "dim", "=", "[", "0", ",", "2", ",", "3", "]", ")", "\n", "\n", "if", "self", ".", "_stats_mode", "==", "\"\"", ":", "\n", "            ", "assert", "B", ">", "0", ",", "'SyncBatchNorm(stats_mode=\"\") does not support zero batch size.'", "\n", "vec", "=", "torch", ".", "cat", "(", "[", "mean", ",", "meansqr", "]", ",", "dim", "=", "0", ")", "\n", "vec", "=", "differentiable_all_reduce", "(", "vec", ")", "*", "(", "1.0", "/", "dist", ".", "get_world_size", "(", ")", ")", "\n", "mean", ",", "meansqr", "=", "torch", ".", "split", "(", "vec", ",", "C", ")", "\n", "momentum", "=", "self", ".", "momentum", "\n", "", "else", ":", "\n", "            ", "if", "B", "==", "0", ":", "\n", "                ", "vec", "=", "torch", ".", "zeros", "(", "[", "2", "*", "C", "+", "1", "]", ",", "device", "=", "mean", ".", "device", ",", "dtype", "=", "mean", ".", "dtype", ")", "\n", "vec", "=", "vec", "+", "input", ".", "sum", "(", ")", "# make sure there is gradient w.r.t input", "\n", "", "else", ":", "\n", "                ", "vec", "=", "torch", ".", "cat", "(", "\n", "[", "mean", ",", "meansqr", ",", "torch", ".", "ones", "(", "[", "1", "]", ",", "device", "=", "mean", ".", "device", ",", "dtype", "=", "mean", ".", "dtype", ")", "]", ",", "dim", "=", "0", "\n", ")", "\n", "", "vec", "=", "differentiable_all_reduce", "(", "vec", "*", "B", ")", "\n", "\n", "total_batch", "=", "vec", "[", "-", "1", "]", ".", "detach", "(", ")", "\n", "momentum", "=", "total_batch", ".", "clamp", "(", "max", "=", "1", ")", "*", "self", ".", "momentum", "# no update if total_batch is 0", "\n", "mean", ",", "meansqr", ",", "_", "=", "torch", ".", "split", "(", "vec", "/", "total_batch", ".", "clamp", "(", "min", "=", "1", ")", ",", "C", ")", "# avoid div-by-zero", "\n", "\n", "", "var", "=", "meansqr", "-", "mean", "*", "mean", "\n", "invstd", "=", "torch", ".", "rsqrt", "(", "var", "+", "self", ".", "eps", ")", "\n", "scale", "=", "self", ".", "weight", "*", "invstd", "\n", "bias", "=", "self", ".", "bias", "-", "mean", "*", "scale", "\n", "scale", "=", "scale", ".", "reshape", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "bias", "=", "bias", ".", "reshape", "(", "1", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "\n", "self", ".", "running_mean", "+=", "momentum", "*", "(", "mean", ".", "detach", "(", ")", "-", "self", ".", "running_mean", ")", "\n", "self", ".", "running_var", "+=", "momentum", "*", "(", "var", ".", "detach", "(", ")", "-", "self", ".", "running_var", ")", "\n", "ret", "=", "input", "*", "scale", "+", "bias", "\n", "if", "half_input", ":", "\n", "            ", "ret", "=", "ret", ".", "half", "(", ")", "\n", "", "return", "ret", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.batch_norm.get_norm": [[137, 163], ["isinstance", "norm", "len", "torch.nn.GroupNorm"], "function", ["None"], ["", "", "def", "get_norm", "(", "norm", ",", "out_channels", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        norm (str or callable): either one of BN, SyncBN, FrozenBN, GN;\n            or a callable that takes a channel number and returns\n            the normalization layer as a nn.Module.\n\n    Returns:\n        nn.Module or None: the normalization layer\n    \"\"\"", "\n", "if", "norm", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "if", "isinstance", "(", "norm", ",", "str", ")", ":", "\n", "        ", "if", "len", "(", "norm", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "", "norm", "=", "{", "\n", "\"BN\"", ":", "BatchNorm2d", ",", "\n", "# Fixed in https://github.com/pytorch/pytorch/pull/36382", "\n", "\"SyncBN\"", ":", "NaiveSyncBatchNorm", "if", "env", ".", "TORCH_VERSION", "<=", "(", "1", ",", "5", ")", "else", "nn", ".", "SyncBatchNorm", ",", "\n", "\"FrozenBN\"", ":", "FrozenBatchNorm2d", ",", "\n", "\"GN\"", ":", "lambda", "channels", ":", "nn", ".", "GroupNorm", "(", "32", ",", "channels", ")", ",", "\n", "# for debugging:", "\n", "\"nnSyncBN\"", ":", "nn", ".", "SyncBatchNorm", ",", "\n", "\"naiveSyncBN\"", ":", "NaiveSyncBatchNorm", ",", "\n", "}", "[", "norm", "]", "\n", "", "return", "norm", "(", "out_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align.ROIAlign.__init__": [[8, 48], ["torch.nn.Module.__init__", "tuple", "int", "__version__.split"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "output_size", ",", "spatial_scale", ",", "sampling_ratio", ",", "aligned", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            output_size (tuple): h, w\n            spatial_scale (float): scale the input boxes by this number\n            sampling_ratio (int): number of inputs samples to take for each output\n                sample. 0 to take samples densely.\n            aligned (bool): if False, use the legacy implementation in\n                Detectron. If True, align the results more perfectly.\n\n        Note:\n            The meaning of aligned=True:\n\n            Given a continuous coordinate c, its two neighboring pixel indices (in our\n            pixel model) are computed by floor(c - 0.5) and ceil(c - 0.5). For example,\n            c=1.3 has pixel neighbors with discrete indices [0] and [1] (which are sampled\n            from the underlying signal at continuous coordinates 0.5 and 1.5). But the original\n            roi_align (aligned=False) does not subtract the 0.5 when computing neighboring\n            pixel indices and therefore it uses pixels with a slightly incorrect alignment\n            (relative to our pixel model) when performing bilinear interpolation.\n\n            With `aligned=True`,\n            we first appropriately scale the ROI and then shift it by -0.5\n            prior to calling roi_align. This produces the correct neighbors; see\n            detectron2/tests/test_roi_align.py for verification.\n\n            The difference does not make a difference to the model's performance if\n            ROIAlign is used together with conv layers.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "spatial_scale", "=", "spatial_scale", "\n", "self", ".", "sampling_ratio", "=", "sampling_ratio", "\n", "self", ".", "aligned", "=", "aligned", "\n", "\n", "from", "torchvision", "import", "__version__", "\n", "\n", "version", "=", "tuple", "(", "int", "(", "x", ")", "for", "x", "in", "__version__", ".", "split", "(", "\".\"", ")", "[", ":", "2", "]", ")", "\n", "# https://github.com/pytorch/vision/pull/2438", "\n", "assert", "version", ">=", "(", "0", ",", "7", ")", ",", "\"Require torchvision >= 0.7\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align.ROIAlign.forward": [[49, 65], ["torchvision.ops.roi_align", "input.dequantize.dequantize.dequantize", "rois.to", "rois.dim", "rois.size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "forward", "(", "self", ",", "input", ",", "rois", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input: NCHW images\n            rois: Bx5 boxes. First column is the index into N. The other 4 columns are xyxy.\n        \"\"\"", "\n", "assert", "rois", ".", "dim", "(", ")", "==", "2", "and", "rois", ".", "size", "(", "1", ")", "==", "5", "\n", "if", "input", ".", "is_quantized", ":", "\n", "            ", "input", "=", "input", ".", "dequantize", "(", ")", "\n", "", "return", "roi_align", "(", "\n", "input", ",", "\n", "rois", ".", "to", "(", "dtype", "=", "input", ".", "dtype", ")", ",", "\n", "self", ".", "output_size", ",", "\n", "self", ".", "spatial_scale", ",", "\n", "self", ".", "sampling_ratio", ",", "\n", "self", ".", "aligned", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align.ROIAlign.__repr__": [[67, 75], ["str", "str", "str", "str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "tmpstr", "=", "self", ".", "__class__", ".", "__name__", "+", "\"(\"", "\n", "tmpstr", "+=", "\"output_size=\"", "+", "str", "(", "self", ".", "output_size", ")", "\n", "tmpstr", "+=", "\", spatial_scale=\"", "+", "str", "(", "self", ".", "spatial_scale", ")", "\n", "tmpstr", "+=", "\", sampling_ratio=\"", "+", "str", "(", "self", ".", "sampling_ratio", ")", "\n", "tmpstr", "+=", "\", aligned=\"", "+", "str", "(", "self", ".", "aligned", ")", "\n", "tmpstr", "+=", "\")\"", "\n", "return", "tmpstr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.shape_spec.ShapeSpec.__new__": [[19, 21], ["super().__new__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.shape_spec.ShapeSpec.__new__"], ["def", "__new__", "(", "cls", ",", "channels", "=", "None", ",", "height", "=", "None", ",", "width", "=", "None", ",", "stride", "=", "None", ")", ":", "\n", "        ", "return", "super", "(", ")", ".", "__new__", "(", "cls", ",", "channels", ",", "height", ",", "width", ",", "stride", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.forward": [[12, 23], ["ctx.save_for_backward", "torch.nn.modules.utils._pair", "input.size", "detectron2._C.roi_align_rotated_forward"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ",", "roi", ",", "output_size", ",", "spatial_scale", ",", "sampling_ratio", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "roi", ")", "\n", "ctx", ".", "output_size", "=", "_pair", "(", "output_size", ")", "\n", "ctx", ".", "spatial_scale", "=", "spatial_scale", "\n", "ctx", ".", "sampling_ratio", "=", "sampling_ratio", "\n", "ctx", ".", "input_shape", "=", "input", ".", "size", "(", ")", "\n", "output", "=", "_C", ".", "roi_align_rotated_forward", "(", "\n", "input", ",", "roi", ",", "spatial_scale", ",", "output_size", "[", "0", "]", ",", "output_size", "[", "1", "]", ",", "sampling_ratio", "\n", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward": [[24, 45], ["detectron2._C.roi_align_rotated_backward"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "once_differentiable", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "(", "rois", ",", ")", "=", "ctx", ".", "saved_tensors", "\n", "output_size", "=", "ctx", ".", "output_size", "\n", "spatial_scale", "=", "ctx", ".", "spatial_scale", "\n", "sampling_ratio", "=", "ctx", ".", "sampling_ratio", "\n", "bs", ",", "ch", ",", "h", ",", "w", "=", "ctx", ".", "input_shape", "\n", "grad_input", "=", "_C", ".", "roi_align_rotated_backward", "(", "\n", "grad_output", ",", "\n", "rois", ",", "\n", "spatial_scale", ",", "\n", "output_size", "[", "0", "]", ",", "\n", "output_size", "[", "1", "]", ",", "\n", "bs", ",", "\n", "ch", ",", "\n", "h", ",", "\n", "w", ",", "\n", "sampling_ratio", ",", "\n", ")", "\n", "return", "grad_input", ",", "None", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated.ROIAlignRotated.__init__": [[51, 70], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "output_size", ",", "spatial_scale", ",", "sampling_ratio", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            output_size (tuple): h, w\n            spatial_scale (float): scale the input boxes by this number\n            sampling_ratio (int): number of inputs samples to take for each output\n                sample. 0 to take samples densely.\n\n        Note:\n            ROIAlignRotated supports continuous coordinate by default:\n            Given a continuous coordinate c, its two neighboring pixel indices (in our\n            pixel model) are computed by floor(c - 0.5) and ceil(c - 0.5). For example,\n            c=1.3 has pixel neighbors with discrete indices [0] and [1] (which are sampled\n            from the underlying signal at continuous coordinates 0.5 and 1.5).\n        \"\"\"", "\n", "super", "(", "ROIAlignRotated", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "spatial_scale", "=", "spatial_scale", "\n", "self", ".", "sampling_ratio", "=", "sampling_ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated.ROIAlignRotated.forward": [[71, 86], ["roi_align_rotated().to", "input.float.float.float", "rois.float.float.float", "rois.float.float.dim", "rois.float.float.size", "roi_align_rotated"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "forward", "(", "self", ",", "input", ",", "rois", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input: NCHW images\n            rois: Bx6 boxes. First column is the index into N.\n                The other 5 columns are (x_ctr, y_ctr, width, height, angle_degrees).\n        \"\"\"", "\n", "assert", "rois", ".", "dim", "(", ")", "==", "2", "and", "rois", ".", "size", "(", "1", ")", "==", "6", "\n", "orig_dtype", "=", "input", ".", "dtype", "\n", "if", "orig_dtype", "==", "torch", ".", "float16", ":", "\n", "            ", "input", "=", "input", ".", "float", "(", ")", "\n", "rois", "=", "rois", ".", "float", "(", ")", "\n", "", "return", "roi_align_rotated", "(", "\n", "input", ",", "rois", ",", "self", ".", "output_size", ",", "self", ".", "spatial_scale", ",", "self", ".", "sampling_ratio", "\n", ")", ".", "to", "(", "dtype", "=", "orig_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated.ROIAlignRotated.__repr__": [[87, 94], ["str", "str", "str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "tmpstr", "=", "self", ".", "__class__", ".", "__name__", "+", "\"(\"", "\n", "tmpstr", "+=", "\"output_size=\"", "+", "str", "(", "self", ".", "output_size", ")", "\n", "tmpstr", "+=", "\", spatial_scale=\"", "+", "str", "(", "self", ".", "spatial_scale", ")", "\n", "tmpstr", "+=", "\", sampling_ratio=\"", "+", "str", "(", "self", ".", "sampling_ratio", ")", "\n", "tmpstr", "+=", "\")\"", "\n", "return", "tmpstr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest.test_forward_output": [[14, 47], ["numpy.arange().reshape().astype", "test_roi_align.ROIAlignTest._simple_roialign", "test_roi_align.ROIAlignTest._simple_roialign", "test_roi_align.ROIAlignTest.assertTrue", "test_roi_align.ROIAlignTest.assertTrue", "numpy.allclose", "numpy.allclose", "numpy.arange().reshape", "test_roi_align.ROIAlignTest.flatten", "numpy.asarray().flatten", "test_roi_align.ROIAlignTest.flatten", "numpy.asarray().flatten", "numpy.arange", "numpy.asarray", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest._simple_roialign", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest._simple_roialign", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["    ", "def", "test_forward_output", "(", "self", ")", ":", "\n", "        ", "input", "=", "np", ".", "arange", "(", "25", ")", ".", "reshape", "(", "5", ",", "5", ")", ".", "astype", "(", "\"float32\"", ")", "\n", "\"\"\"\n        0  1  2   3 4\n        5  6  7   8 9\n        10 11 12 13 14\n        15 16 17 18 19\n        20 21 22 23 24\n        \"\"\"", "\n", "\n", "output", "=", "self", ".", "_simple_roialign", "(", "input", ",", "[", "1", ",", "1", ",", "3", ",", "3", "]", ",", "(", "4", ",", "4", ")", ",", "aligned", "=", "False", ")", "\n", "output_correct", "=", "self", ".", "_simple_roialign", "(", "input", ",", "[", "1", ",", "1", ",", "3", ",", "3", "]", ",", "(", "4", ",", "4", ")", ",", "aligned", "=", "True", ")", "\n", "\n", "# without correction:", "\n", "old_results", "=", "[", "\n", "[", "7.5", ",", "8", ",", "8.5", ",", "9", "]", ",", "\n", "[", "10", ",", "10.5", ",", "11", ",", "11.5", "]", ",", "\n", "[", "12.5", ",", "13", ",", "13.5", ",", "14", "]", ",", "\n", "[", "15", ",", "15.5", ",", "16", ",", "16.5", "]", ",", "\n", "]", "\n", "\n", "# with 0.5 correction:", "\n", "correct_results", "=", "[", "\n", "[", "4.5", ",", "5.0", ",", "5.5", ",", "6.0", "]", ",", "\n", "[", "7.0", ",", "7.5", ",", "8.0", ",", "8.5", "]", ",", "\n", "[", "9.5", ",", "10.0", ",", "10.5", ",", "11.0", "]", ",", "\n", "[", "12.0", ",", "12.5", ",", "13.0", ",", "13.5", "]", ",", "\n", "]", "\n", "# This is an upsampled version of [[6, 7], [11, 12]]", "\n", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output", ".", "flatten", "(", ")", ",", "np", ".", "asarray", "(", "old_results", ")", ".", "flatten", "(", ")", ")", ")", "\n", "self", ".", "assertTrue", "(", "\n", "np", ".", "allclose", "(", "output_correct", ".", "flatten", "(", ")", ",", "np", ".", "asarray", "(", "correct_results", ")", ".", "flatten", "(", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest.test_resize": [[52, 63], ["test_roi_align.ROIAlignTest._simple_roialign", "cv2.resize", "test_roi_align.ROIAlignTest._simple_roialign", "numpy.abs", "test_roi_align.ROIAlignTest.assertTrue", "numpy.random.rand().astype", "numpy.abs.max", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest._simple_roialign", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest._simple_roialign"], ["", "def", "test_resize", "(", "self", ")", ":", "\n", "        ", "H", ",", "W", "=", "30", ",", "30", "\n", "input", "=", "np", ".", "random", ".", "rand", "(", "H", ",", "W", ")", ".", "astype", "(", "\"float32\"", ")", "*", "100", "\n", "box", "=", "[", "10", ",", "10", ",", "20", ",", "20", "]", "\n", "output", "=", "self", ".", "_simple_roialign", "(", "input", ",", "box", ",", "(", "5", ",", "5", ")", ",", "aligned", "=", "True", ")", "\n", "\n", "input2x", "=", "cv2", ".", "resize", "(", "input", ",", "(", "W", "//", "2", ",", "H", "//", "2", ")", ",", "interpolation", "=", "cv2", ".", "INTER_LINEAR", ")", "\n", "box2x", "=", "[", "x", "/", "2", "for", "x", "in", "box", "]", "\n", "output2x", "=", "self", ".", "_simple_roialign", "(", "input2x", ",", "box2x", ",", "(", "5", ",", "5", ")", ",", "aligned", "=", "True", ")", "\n", "diff", "=", "np", ".", "abs", "(", "output2x", "-", "output", ")", "\n", "self", ".", "assertTrue", "(", "diff", ".", "max", "(", ")", "<", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest.test_grid_sample_equivalence": [[64, 78], ["numpy.random.rand().astype", "test_roi_align.ROIAlignTest._simple_roialign", "test_roi_align.grid_sample_roi_align", "test_roi_align.ROIAlignTest.assertTrue", "torch.from_numpy().float", "torch.allclose", "numpy.random.rand", "torch.as_tensor().float", "torch.from_numpy", "torch.as_tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest._simple_roialign", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.grid_sample_roi_align"], ["", "def", "test_grid_sample_equivalence", "(", "self", ")", ":", "\n", "        ", "H", ",", "W", "=", "30", ",", "30", "\n", "input", "=", "np", ".", "random", ".", "rand", "(", "H", ",", "W", ")", ".", "astype", "(", "\"float32\"", ")", "*", "100", "\n", "box", "=", "[", "10", ",", "10", ",", "20", ",", "20", "]", "\n", "for", "ratio", "in", "[", "1", ",", "2", ",", "3", "]", ":", "\n", "            ", "output", "=", "self", ".", "_simple_roialign", "(", "input", ",", "box", ",", "(", "5", ",", "5", ")", ",", "sampling_ratio", "=", "ratio", ")", "\n", "output_grid_sample", "=", "grid_sample_roi_align", "(", "\n", "torch", ".", "from_numpy", "(", "input", "[", "None", ",", "None", ",", ":", ",", ":", "]", ")", ".", "float", "(", ")", ",", "\n", "torch", ".", "as_tensor", "(", "box", ")", ".", "float", "(", ")", "[", "None", ",", ":", "]", ",", "\n", "5", ",", "\n", "1.0", ",", "\n", "ratio", ",", "\n", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "output", ",", "output_grid_sample", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest._simple_roialign": [[79, 95], ["isinstance", "detectron2.layers.roi_align.ROIAlign", "torch.from_numpy", "torch.from_numpy", "detectron2.layers.roi_align.ROIAlign.forward", "torch.cuda.is_available", "img[].astype", "list", "[].astype", "detectron2.layers.roi_align.ROIAlign.forward().cpu", "test_roi_align.ROIAlignTest.assertTrue", "torch.allclose", "detectron2.layers.roi_align.ROIAlign.forward", "numpy.asarray", "torch.from_numpy.cuda", "torch.from_numpy.cuda"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward"], ["", "", "def", "_simple_roialign", "(", "self", ",", "img", ",", "box", ",", "resolution", ",", "sampling_ratio", "=", "0", ",", "aligned", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        RoiAlign with scale 1.0.\n        \"\"\"", "\n", "if", "isinstance", "(", "resolution", ",", "int", ")", ":", "\n", "            ", "resolution", "=", "(", "resolution", ",", "resolution", ")", "\n", "", "op", "=", "ROIAlign", "(", "resolution", ",", "1.0", ",", "sampling_ratio", ",", "aligned", "=", "aligned", ")", "\n", "input", "=", "torch", ".", "from_numpy", "(", "img", "[", "None", ",", "None", ",", ":", ",", ":", "]", ".", "astype", "(", "\"float32\"", ")", ")", "\n", "\n", "rois", "=", "[", "0", "]", "+", "list", "(", "box", ")", "\n", "rois", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "rois", ")", "[", "None", ",", ":", "]", ".", "astype", "(", "\"float32\"", ")", ")", "\n", "output", "=", "op", ".", "forward", "(", "input", ",", "rois", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "output_cuda", "=", "op", ".", "forward", "(", "input", ".", "cuda", "(", ")", ",", "rois", ".", "cuda", "(", ")", ")", ".", "cpu", "(", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "output", ",", "output_cuda", ")", ")", "\n", "", "return", "output", "[", "0", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest._simple_roialign_with_grad": [[96, 110], ["isinstance", "detectron2.layers.roi_align.ROIAlign", "torch.from_numpy", "torch.from_numpy", "input.to.to.to", "rois.to.to.to", "detectron2.layers.roi_align.ROIAlign.forward", "img[].astype", "list", "[].astype", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "_simple_roialign_with_grad", "(", "self", ",", "img", ",", "box", ",", "resolution", ",", "device", ")", ":", "\n", "        ", "if", "isinstance", "(", "resolution", ",", "int", ")", ":", "\n", "            ", "resolution", "=", "(", "resolution", ",", "resolution", ")", "\n", "\n", "", "op", "=", "ROIAlign", "(", "resolution", ",", "1.0", ",", "0", ",", "aligned", "=", "True", ")", "\n", "input", "=", "torch", ".", "from_numpy", "(", "img", "[", "None", ",", "None", ",", ":", ",", ":", "]", ".", "astype", "(", "\"float32\"", ")", ")", "\n", "\n", "rois", "=", "[", "0", "]", "+", "list", "(", "box", ")", "\n", "rois", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "rois", ")", "[", "None", ",", ":", "]", ".", "astype", "(", "\"float32\"", ")", ")", "\n", "input", "=", "input", ".", "to", "(", "device", "=", "device", ")", "\n", "rois", "=", "rois", ".", "to", "(", "device", "=", "device", ")", "\n", "input", ".", "requires_grad", "=", "True", "\n", "output", "=", "op", ".", "forward", "(", "input", ",", "rois", ")", "\n", "return", "input", ",", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest.test_empty_box": [[111, 122], ["numpy.random.rand", "test_roi_align.ROIAlignTest._simple_roialign", "test_roi_align.ROIAlignTest.assertTrue", "test_roi_align.ROIAlignTest.assertTrue", "torch.cuda.is_available", "test_roi_align.ROIAlignTest._simple_roialign_with_grad", "output.sum().backward", "test_roi_align.ROIAlignTest.assertTrue", "torch.device", "torch.allclose", "output.sum", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest._simple_roialign", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest._simple_roialign_with_grad", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["", "def", "test_empty_box", "(", "self", ")", ":", "\n", "        ", "img", "=", "np", ".", "random", ".", "rand", "(", "5", ",", "5", ")", "\n", "box", "=", "[", "3", ",", "4", ",", "5", ",", "4", "]", "\n", "o", "=", "self", ".", "_simple_roialign", "(", "img", ",", "box", ",", "7", ")", "\n", "self", ".", "assertTrue", "(", "o", ".", "shape", "==", "(", "7", ",", "7", ")", ")", "\n", "self", ".", "assertTrue", "(", "(", "o", "==", "0", ")", ".", "all", "(", ")", ")", "\n", "\n", "for", "dev", "in", "[", "\"cpu\"", "]", "+", "[", "\"cuda\"", "]", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "[", "]", ":", "\n", "            ", "input", ",", "output", "=", "self", ".", "_simple_roialign_with_grad", "(", "img", ",", "box", ",", "7", ",", "torch", ".", "device", "(", "dev", ")", ")", "\n", "output", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "input", ".", "grad", ",", "torch", ".", "zeros_like", "(", "input", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.ROIAlignTest.test_empty_batch": [[123, 129], ["torch.zeros", "torch.zeros", "detectron2.layers.roi_align.ROIAlign", "detectron2.layers.roi_align.ROIAlign.forward", "test_roi_align.ROIAlignTest.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward"], ["", "", "def", "test_empty_batch", "(", "self", ")", ":", "\n", "        ", "input", "=", "torch", ".", "zeros", "(", "0", ",", "3", ",", "10", ",", "10", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "rois", "=", "torch", ".", "zeros", "(", "0", ",", "5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "op", "=", "ROIAlign", "(", "(", "7", ",", "7", ")", ",", "1.0", ",", "0", ",", "aligned", "=", "True", ")", "\n", "output", "=", "op", ".", "forward", "(", "input", ",", "rois", ")", "\n", "self", ".", "assertTrue", "(", "output", ".", "shape", "==", "(", "0", ",", "3", ",", "7", ",", "7", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.grid_sample_roi_align": [[131, 154], ["len", "generate_regular_grid_point_coords", "get_point_coords_wrt_image", "point_sample", "F.avg_pool2d.squeeze().permute().reshape", "torch.nn.functional.avg_pool2d", "torch.as_tensor", "get_point_coords_wrt_image.unsqueeze", "F.avg_pool2d.squeeze().permute", "F.avg_pool2d.squeeze"], "function", ["None"], ["", "", "def", "grid_sample_roi_align", "(", "input", ",", "boxes", ",", "output_size", ",", "scale", ",", "sampling_ratio", ")", ":", "\n", "# unlike true roi_align, this does not support different batch_idx", "\n", "    ", "from", "detectron2", ".", "projects", ".", "point_rend", ".", "point_features", "import", "(", "\n", "generate_regular_grid_point_coords", ",", "\n", "get_point_coords_wrt_image", ",", "\n", "point_sample", ",", "\n", ")", "\n", "\n", "N", ",", "_", ",", "H", ",", "W", "=", "input", ".", "shape", "\n", "R", "=", "len", "(", "boxes", ")", "\n", "assert", "N", "==", "1", "\n", "boxes", "=", "boxes", "*", "scale", "\n", "grid", "=", "generate_regular_grid_point_coords", "(", "R", ",", "output_size", "*", "sampling_ratio", ",", "device", "=", "boxes", ".", "device", ")", "\n", "coords", "=", "get_point_coords_wrt_image", "(", "boxes", ",", "grid", ")", "\n", "coords", "=", "coords", "/", "torch", ".", "as_tensor", "(", "[", "W", ",", "H", "]", ",", "device", "=", "coords", ".", "device", ")", "# R, s^2, 2", "\n", "res", "=", "point_sample", "(", "input", ",", "coords", ".", "unsqueeze", "(", "0", ")", ",", "align_corners", "=", "False", ")", "# 1,C, R,s^2", "\n", "res", "=", "(", "\n", "res", ".", "squeeze", "(", "0", ")", "\n", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", ".", "reshape", "(", "R", ",", "-", "1", ",", "output_size", "*", "sampling_ratio", ",", "output_size", "*", "sampling_ratio", ")", "\n", ")", "\n", "res", "=", "F", ".", "avg_pool2d", "(", "res", ",", "sampling_ratio", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align.benchmark_roi_align": [[156, 205], ["dict", "fvcore.common.benchmark.benchmark", "dict.update", "fvcore.common.benchmark.benchmark", "ret.clamp_", "torch.rand", "range", "torch.cat", "torch.cat", "torch.cat", "input.to.to", "boxes.to.to", "test_roi_align.benchmark_roi_align.gen_args"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "benchmark_roi_align", "(", ")", ":", "\n", "    ", "def", "random_boxes", "(", "mean_box", ",", "stdev", ",", "N", ",", "maxsize", ")", ":", "\n", "        ", "ret", "=", "torch", ".", "rand", "(", "N", ",", "4", ")", "*", "stdev", "+", "torch", ".", "tensor", "(", "mean_box", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "ret", ".", "clamp_", "(", "min", "=", "0", ",", "max", "=", "maxsize", ")", "\n", "return", "ret", "\n", "\n", "", "def", "func", "(", "shape", ",", "nboxes_per_img", ",", "sampling_ratio", ",", "device", ",", "box_size", "=", "\"large\"", ")", ":", "\n", "        ", "N", ",", "_", ",", "H", ",", "_", "=", "shape", "\n", "input", "=", "torch", ".", "rand", "(", "*", "shape", ")", "\n", "boxes", "=", "[", "]", "\n", "batch_idx", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "N", ")", ":", "\n", "            ", "if", "box_size", "==", "\"large\"", ":", "\n", "                ", "b", "=", "random_boxes", "(", "[", "80", ",", "80", ",", "130", ",", "130", "]", ",", "24", ",", "nboxes_per_img", ",", "H", ")", "\n", "", "else", ":", "\n", "                ", "b", "=", "random_boxes", "(", "[", "100", ",", "100", ",", "110", ",", "110", "]", ",", "4", ",", "nboxes_per_img", ",", "H", ")", "\n", "", "boxes", ".", "append", "(", "b", ")", "\n", "batch_idx", ".", "append", "(", "torch", ".", "zeros", "(", "nboxes_per_img", ",", "1", ",", "dtype", "=", "torch", ".", "float32", ")", "+", "k", ")", "\n", "", "boxes", "=", "torch", ".", "cat", "(", "boxes", ",", "axis", "=", "0", ")", "\n", "batch_idx", "=", "torch", ".", "cat", "(", "batch_idx", ",", "axis", "=", "0", ")", "\n", "boxes", "=", "torch", ".", "cat", "(", "[", "batch_idx", ",", "boxes", "]", ",", "axis", "=", "1", ")", "\n", "\n", "input", "=", "input", ".", "to", "(", "device", "=", "device", ")", "\n", "boxes", "=", "boxes", ".", "to", "(", "device", "=", "device", ")", "\n", "\n", "def", "bench", "(", ")", ":", "\n", "            ", "if", "False", "and", "sampling_ratio", ">", "0", "and", "N", "==", "1", ":", "\n", "# enable to benchmark grid_sample (slower)", "\n", "                ", "grid_sample_roi_align", "(", "input", ",", "boxes", "[", ":", ",", "1", ":", "]", ",", "7", ",", "1.0", ",", "sampling_ratio", ")", "\n", "", "else", ":", "\n", "                ", "roi_align", "(", "input", ",", "boxes", ",", "7", ",", "1.0", ",", "sampling_ratio", ",", "True", ")", "\n", "", "if", "device", "==", "\"cuda\"", ":", "\n", "                ", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "\n", "", "", "return", "bench", "\n", "\n", "", "def", "gen_args", "(", "arg", ")", ":", "\n", "        ", "args", "=", "[", "]", "\n", "for", "size", "in", "[", "\"small\"", ",", "\"large\"", "]", ":", "\n", "            ", "for", "ratio", "in", "[", "0", ",", "2", "]", ":", "\n", "                ", "args", ".", "append", "(", "copy", "(", "arg", ")", ")", "\n", "args", "[", "-", "1", "]", "[", "\"sampling_ratio\"", "]", "=", "ratio", "\n", "args", "[", "-", "1", "]", "[", "\"box_size\"", "]", "=", "size", "\n", "", "", "return", "args", "\n", "\n", "", "arg", "=", "dict", "(", "shape", "=", "(", "1", ",", "512", ",", "256", ",", "256", ")", ",", "nboxes_per_img", "=", "512", ",", "device", "=", "\"cuda\"", ")", "\n", "benchmark", "(", "func", ",", "\"cuda_roialign\"", ",", "gen_args", "(", "arg", ")", ",", "num_iters", "=", "20", ",", "warmup_iters", "=", "1", ")", "\n", "arg", ".", "update", "(", "{", "\"device\"", ":", "\"cpu\"", ",", "\"shape\"", ":", "(", "1", ",", "256", ",", "128", ",", "128", ")", "}", ")", "\n", "benchmark", "(", "func", ",", "\"cpu_roialign\"", ",", "gen_args", "(", "arg", ")", ",", "num_iters", "=", "5", ",", "warmup_iters", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._box_to_rotated_box": [[15, 22], ["None"], "methods", ["None"], ["    ", "def", "_box_to_rotated_box", "(", "self", ",", "box", ",", "angle", ")", ":", "\n", "        ", "return", "[", "\n", "(", "box", "[", "0", "]", "+", "box", "[", "2", "]", ")", "/", "2.0", ",", "\n", "(", "box", "[", "1", "]", "+", "box", "[", "3", "]", ")", "/", "2.0", ",", "\n", "box", "[", "2", "]", "-", "box", "[", "0", "]", ",", "\n", "box", "[", "3", "]", "-", "box", "[", "1", "]", ",", "\n", "angle", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._rot90": [[24, 29], ["range", "img.transpose().flip.transpose().flip.transpose().flip", "img.transpose().flip.transpose().flip.transpose"], "methods", ["None"], ["", "def", "_rot90", "(", "self", ",", "img", ",", "num", ")", ":", "\n", "        ", "num", "=", "num", "%", "4", "# note: -1 % 4 == 3", "\n", "for", "_", "in", "range", "(", "num", ")", ":", "\n", "            ", "img", "=", "img", ".", "transpose", "(", "0", ",", "1", ")", ".", "flip", "(", "0", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest.test_forward_output_0_90_180_270": [[30, 72], ["range", "torch.arange().reshape", "test_roi_align_rotated.ROIAlignRotatedTest._box_to_rotated_box", "test_roi_align_rotated.ROIAlignRotatedTest._simple_roi_align_rotated", "torch.tensor", "test_roi_align_rotated.ROIAlignRotatedTest._rot90", "torch.allclose", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._box_to_rotated_box", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._simple_roi_align_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._rot90"], ["", "def", "test_forward_output_0_90_180_270", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "4", ")", ":", "\n", "# i = 0, 1, 2, 3 corresponding to 0, 90, 180, 270 degrees", "\n", "            ", "img", "=", "torch", ".", "arange", "(", "25", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "reshape", "(", "5", ",", "5", ")", "\n", "\"\"\"\n            0  1  2   3 4\n            5  6  7   8 9\n            10 11 12 13 14\n            15 16 17 18 19\n            20 21 22 23 24\n            \"\"\"", "\n", "box", "=", "[", "1", ",", "1", ",", "3", ",", "3", "]", "\n", "rotated_box", "=", "self", ".", "_box_to_rotated_box", "(", "box", "=", "box", ",", "angle", "=", "90", "*", "i", ")", "\n", "\n", "result", "=", "self", ".", "_simple_roi_align_rotated", "(", "img", "=", "img", ",", "box", "=", "rotated_box", ",", "resolution", "=", "(", "4", ",", "4", ")", ")", "\n", "\n", "# Here's an explanation for 0 degree case:", "\n", "# point 0 in the original input lies at [0.5, 0.5]", "\n", "# (the center of bin [0, 1] x [0, 1])", "\n", "# point 1 in the original input lies at [1.5, 0.5], etc.", "\n", "# since the resolution is (4, 4) that divides [1, 3] x [1, 3]", "\n", "# into 4 x 4 equal bins,", "\n", "# the top-left bin is [1, 1.5] x [1, 1.5], and its center", "\n", "# (1.25, 1.25) lies at the 3/4 position", "\n", "# between point 0 and point 1, point 5 and point 6,", "\n", "# point 0 and point 5, point 1 and point 6, so it can be calculated as", "\n", "# 0.25*(0*0.25+1*0.75)+(5*0.25+6*0.75)*0.75 = 4.5", "\n", "result_expected", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "4.5", ",", "5.0", ",", "5.5", ",", "6.0", "]", ",", "\n", "[", "7.0", ",", "7.5", ",", "8.0", ",", "8.5", "]", ",", "\n", "[", "9.5", ",", "10.0", ",", "10.5", ",", "11.0", "]", ",", "\n", "[", "12.0", ",", "12.5", ",", "13.0", ",", "13.5", "]", ",", "\n", "]", "\n", ")", "\n", "# This is also an upsampled version of [[6, 7], [11, 12]]", "\n", "\n", "# When the box is rotated by 90 degrees CCW,", "\n", "# the result would be rotated by 90 degrees CW, thus it's -i here", "\n", "result_expected", "=", "self", ".", "_rot90", "(", "result_expected", ",", "-", "i", ")", "\n", "\n", "assert", "torch", ".", "allclose", "(", "result", ",", "result_expected", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest.test_resize": [[73, 86], ["test_roi_align_rotated.ROIAlignRotatedTest._box_to_rotated_box", "test_roi_align_rotated.ROIAlignRotatedTest._simple_roi_align_rotated", "cv2.resize", "torch.from_numpy", "test_roi_align_rotated.ROIAlignRotatedTest._box_to_rotated_box", "test_roi_align_rotated.ROIAlignRotatedTest._simple_roi_align_rotated", "torch.allclose", "torch.rand", "input.numpy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._box_to_rotated_box", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._simple_roi_align_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._box_to_rotated_box", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._simple_roi_align_rotated"], ["", "", "def", "test_resize", "(", "self", ")", ":", "\n", "        ", "H", ",", "W", "=", "30", ",", "30", "\n", "input", "=", "torch", ".", "rand", "(", "H", ",", "W", ")", "*", "100", "\n", "box", "=", "[", "10", ",", "10", ",", "20", ",", "20", "]", "\n", "rotated_box", "=", "self", ".", "_box_to_rotated_box", "(", "box", ",", "angle", "=", "0", ")", "\n", "output", "=", "self", ".", "_simple_roi_align_rotated", "(", "img", "=", "input", ",", "box", "=", "rotated_box", ",", "resolution", "=", "(", "5", ",", "5", ")", ")", "\n", "\n", "input2x", "=", "cv2", ".", "resize", "(", "input", ".", "numpy", "(", ")", ",", "(", "W", "//", "2", ",", "H", "//", "2", ")", ",", "interpolation", "=", "cv2", ".", "INTER_LINEAR", ")", "\n", "input2x", "=", "torch", ".", "from_numpy", "(", "input2x", ")", "\n", "box2x", "=", "[", "x", "/", "2", "for", "x", "in", "box", "]", "\n", "rotated_box2x", "=", "self", ".", "_box_to_rotated_box", "(", "box2x", ",", "angle", "=", "0", ")", "\n", "output2x", "=", "self", ".", "_simple_roi_align_rotated", "(", "img", "=", "input2x", ",", "box", "=", "rotated_box2x", ",", "resolution", "=", "(", "5", ",", "5", ")", ")", "\n", "assert", "torch", ".", "allclose", "(", "output2x", ",", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._simple_roi_align_rotated": [[87, 101], ["detectron2.layers.roi_align_rotated.ROIAlignRotated", "detectron2.layers.roi_align_rotated.ROIAlignRotated.forward", "torch.cuda.is_available", "list", "torch.tensor", "detectron2.layers.roi_align_rotated.ROIAlignRotated.forward", "torch.allclose", "input.cuda", "rois.cuda", "detectron2.layers.roi_align_rotated.ROIAlignRotated.forward.cpu"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward"], ["", "def", "_simple_roi_align_rotated", "(", "self", ",", "img", ",", "box", ",", "resolution", ")", ":", "\n", "        ", "\"\"\"\n        RoiAlignRotated with scale 1.0 and 0 sample ratio.\n        \"\"\"", "\n", "op", "=", "ROIAlignRotated", "(", "output_size", "=", "resolution", ",", "spatial_scale", "=", "1.0", ",", "sampling_ratio", "=", "0", ")", "\n", "input", "=", "img", "[", "None", ",", "None", ",", ":", ",", ":", "]", "\n", "\n", "rois", "=", "[", "0", "]", "+", "list", "(", "box", ")", "\n", "rois", "=", "torch", ".", "tensor", "(", "rois", ",", "dtype", "=", "torch", ".", "float32", ")", "[", "None", ",", ":", "]", "\n", "result_cpu", "=", "op", ".", "forward", "(", "input", ",", "rois", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "result_cuda", "=", "op", ".", "forward", "(", "input", ".", "cuda", "(", ")", ",", "rois", ".", "cuda", "(", ")", ")", "\n", "assert", "torch", ".", "allclose", "(", "result_cpu", ",", "result_cuda", ".", "cpu", "(", ")", ")", "\n", "", "return", "result_cpu", "[", "0", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest.test_empty_box": [[102, 106], ["torch.rand", "test_roi_align_rotated.ROIAlignRotatedTest._simple_roi_align_rotated", "test_roi_align_rotated.ROIAlignRotatedTest.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest._simple_roi_align_rotated"], ["", "def", "test_empty_box", "(", "self", ")", ":", "\n", "        ", "img", "=", "torch", ".", "rand", "(", "5", ",", "5", ")", "\n", "out", "=", "self", ".", "_simple_roi_align_rotated", "(", "img", ",", "[", "2", ",", "3", ",", "0", ",", "0", ",", "0", "]", ",", "(", "7", ",", "7", ")", ")", "\n", "self", ".", "assertTrue", "(", "(", "out", "==", "0", ")", ".", "all", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest.test_roi_align_rotated_gradcheck_cpu": [[107, 126], ["torch.device", "detectron2.layers.roi_align_rotated.ROIAlignRotated().to", "torch.rand", "torch.tensor", "torch.autograd.gradcheck", "torch.autograd.gradcheck", "detectron2.layers.roi_align_rotated.ROIAlignRotated().to.", "detectron2.layers.roi_align_rotated.ROIAlignRotated", "torch.rand.transpose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "test_roi_align_rotated_gradcheck_cpu", "(", "self", ")", ":", "\n", "        ", "dtype", "=", "torch", ".", "float64", "\n", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "roi_align_rotated_op", "=", "ROIAlignRotated", "(", "\n", "output_size", "=", "(", "5", ",", "5", ")", ",", "spatial_scale", "=", "0.5", ",", "sampling_ratio", "=", "1", "\n", ")", ".", "to", "(", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "x", "=", "torch", ".", "rand", "(", "1", ",", "1", ",", "10", ",", "10", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", "\n", "# roi format is (batch index, x_center, y_center, width, height, angle)", "\n", "rois", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "0", ",", "4.5", ",", "4.5", ",", "9", ",", "9", ",", "0", "]", ",", "[", "0", ",", "2", ",", "7", ",", "4", ",", "4", ",", "0", "]", ",", "[", "0", ",", "7", ",", "7", ",", "4", ",", "4", ",", "0", "]", "]", ",", "\n", "dtype", "=", "dtype", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "\n", "def", "func", "(", "input", ")", ":", "\n", "            ", "return", "roi_align_rotated_op", "(", "input", ",", "rois", ")", "\n", "\n", "", "assert", "gradcheck", "(", "func", ",", "(", "x", ",", ")", ")", ",", "\"gradcheck failed for RoIAlignRotated CPU\"", "\n", "assert", "gradcheck", "(", "func", ",", "(", "x", ".", "transpose", "(", "2", ",", "3", ")", ",", ")", ")", ",", "\"gradcheck failed for RoIAlignRotated CPU\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_roi_align_rotated.ROIAlignRotatedTest.test_roi_align_rotated_gradient_cuda": [[127, 173], ["unittest.skipIf", "torch.device", "detectron2.layers.roi_align.ROIAlign().to", "detectron2.layers.roi_align_rotated.ROIAlignRotated().to", "torch.rand", "torch.autograd.Variable", "torch.tensor", "detectron2.layers.roi_align_rotated.ROIAlignRotated().to.", "ROIAlignRotated().to.sum", "roi_align_rotated.sum.backward", "torch.tensor", "detectron2.layers.roi_align.ROIAlign().to.", "ROIAlign().to.sum", "roi_align.sum.backward", "torch.allclose", "torch.rand.data.clone", "torch.cuda.is_available", "detectron2.layers.roi_align.ROIAlign", "detectron2.layers.roi_align_rotated.ROIAlignRotated"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_roi_align_rotated_gradient_cuda", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Compute gradients for ROIAlignRotated with multiple bounding boxes on the GPU,\n        and compare the result with ROIAlign\n        \"\"\"", "\n", "# torch.manual_seed(123)", "\n", "dtype", "=", "torch", ".", "float64", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ")", "\n", "pool_h", ",", "pool_w", "=", "(", "5", ",", "5", ")", "\n", "\n", "roi_align", "=", "ROIAlign", "(", "output_size", "=", "(", "pool_h", ",", "pool_w", ")", ",", "spatial_scale", "=", "1", ",", "sampling_ratio", "=", "2", ")", ".", "to", "(", "\n", "device", "=", "device", "\n", ")", "\n", "\n", "roi_align_rotated", "=", "ROIAlignRotated", "(", "\n", "output_size", "=", "(", "pool_h", ",", "pool_w", ")", ",", "spatial_scale", "=", "1", ",", "sampling_ratio", "=", "2", "\n", ")", ".", "to", "(", "device", "=", "device", ")", "\n", "\n", "x", "=", "torch", ".", "rand", "(", "1", ",", "1", ",", "10", ",", "10", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", "\n", "# x_rotated = x.clone() won't work (will lead to grad_fun=CloneBackward)!", "\n", "x_rotated", "=", "Variable", "(", "x", ".", "data", ".", "clone", "(", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n", "# roi_rotated format is (batch index, x_center, y_center, width, height, angle)", "\n", "rois_rotated", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "0", ",", "4.5", ",", "4.5", ",", "9", ",", "9", ",", "0", "]", ",", "[", "0", ",", "2", ",", "7", ",", "4", ",", "4", ",", "0", "]", ",", "[", "0", ",", "7", ",", "7", ",", "4", ",", "4", ",", "0", "]", "]", ",", "\n", "dtype", "=", "dtype", ",", "\n", "device", "=", "device", ",", "\n", ")", "\n", "\n", "y_rotated", "=", "roi_align_rotated", "(", "x_rotated", ",", "rois_rotated", ")", "\n", "s_rotated", "=", "y_rotated", ".", "sum", "(", ")", "\n", "s_rotated", ".", "backward", "(", ")", "\n", "\n", "# roi format is (batch index, x1, y1, x2, y2)", "\n", "rois", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "0", ",", "0", ",", "0", ",", "9", ",", "9", "]", ",", "[", "0", ",", "0", ",", "5", ",", "4", ",", "9", "]", ",", "[", "0", ",", "5", ",", "5", ",", "9", ",", "9", "]", "]", ",", "dtype", "=", "dtype", ",", "device", "=", "device", "\n", ")", "\n", "\n", "y", "=", "roi_align", "(", "x", ",", "rois", ")", "\n", "s", "=", "y", ".", "sum", "(", ")", "\n", "s", ".", "backward", "(", ")", "\n", "\n", "assert", "torch", ".", "allclose", "(", "\n", "x", ".", "grad", ",", "x_rotated", ".", "grad", "\n", ")", ",", "\"gradients for ROIAlign and ROIAlignRotated mismatch on CUDA\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.TestMaskCropPaste.setUp": [[61, 68], ["detectron2.data.MetadataCatalog.get", "detectron2.utils.file_io.PathManager.isfile", "unittest.SkipTest", "contextlib.redirect_stdout", "detectron2.utils.file_io.PathManager.get_local_path", "pycocotools.coco.COCO", "io.StringIO"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "json_file", "=", "MetadataCatalog", ".", "get", "(", "\"coco_2017_val_100\"", ")", ".", "json_file", "\n", "if", "not", "PathManager", ".", "isfile", "(", "json_file", ")", ":", "\n", "            ", "raise", "unittest", ".", "SkipTest", "(", "\"{} not found\"", ".", "format", "(", "json_file", ")", ")", "\n", "", "with", "contextlib", ".", "redirect_stdout", "(", "io", ".", "StringIO", "(", ")", ")", ":", "\n", "            ", "json_file", "=", "PathManager", ".", "get_local_path", "(", "json_file", ")", "\n", "self", ".", "coco", "=", "COCO", "(", "json_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.TestMaskCropPaste.test_crop_paste_consistency": [[69, 100], ["test_mask_ops.TestMaskCropPaste.coco.loadAnns", "tqdm.tqdm", "numpy.array", "numpy.array.mean", "collections.defaultdict", "zip", "print", "test_mask_ops.TestMaskCropPaste.assertTrue", "test_mask_ops.TestMaskCropPaste.assertTrue", "test_mask_ops.TestMaskCropPaste.coco.getAnnIds", "test_mask_ops.TestMaskCropPaste.process_annotation", "numpy.array.append", "table.append", "tabulate.tabulate.tabulate"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.TestMaskCropPaste.process_annotation"], ["", "", "def", "test_crop_paste_consistency", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        rasterize_polygons_within_box (used in training)\n        and\n        paste_masks_in_image (used in inference)\n        should be inverse operations to each other.\n\n        This function runs several implementation of the above two operations and prints\n        the reconstruction error.\n        \"\"\"", "\n", "\n", "anns", "=", "self", ".", "coco", ".", "loadAnns", "(", "self", ".", "coco", ".", "getAnnIds", "(", "iscrowd", "=", "False", ")", ")", "# avoid crowd annotations", "\n", "\n", "selected_anns", "=", "anns", "[", ":", "100", "]", "\n", "\n", "ious", "=", "[", "]", "\n", "for", "ann", "in", "tqdm", ".", "tqdm", "(", "selected_anns", ")", ":", "\n", "            ", "results", "=", "self", ".", "process_annotation", "(", "ann", ")", "\n", "ious", ".", "append", "(", "[", "k", "[", "2", "]", "for", "k", "in", "results", "]", ")", "\n", "\n", "", "ious", "=", "np", ".", "array", "(", "ious", ")", "\n", "mean_ious", "=", "ious", ".", "mean", "(", "axis", "=", "0", ")", "\n", "table", "=", "[", "]", "\n", "res_dic", "=", "defaultdict", "(", "dict", ")", "\n", "for", "row", ",", "iou", "in", "zip", "(", "results", ",", "mean_ious", ")", ":", "\n", "            ", "table", ".", "append", "(", "(", "row", "[", "0", "]", ",", "row", "[", "1", "]", ",", "iou", ")", ")", "\n", "res_dic", "[", "row", "[", "0", "]", "]", "[", "row", "[", "1", "]", "]", "=", "iou", "\n", "", "print", "(", "tabulate", "(", "table", ",", "headers", "=", "[", "\"rasterize\"", ",", "\"paste\"", ",", "\"iou\"", "]", ",", "tablefmt", "=", "\"simple\"", ")", ")", "\n", "# assert that the reconstruction is good:", "\n", "self", ".", "assertTrue", "(", "res_dic", "[", "\"polygon\"", "]", "[", "\"aligned\"", "]", ">", "0.94", ")", "\n", "self", ".", "assertTrue", "(", "res_dic", "[", "\"roialign\"", "]", "[", "\"aligned\"", "]", ">", "0.95", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.TestMaskCropPaste.process_annotation": [[101, 140], ["detectron2.structures.BoxMode.convert", "detectron2.structures.masks.polygons_to_bitmask", "torch.tensor().to().reshape", "collections.defaultdict", "box_bitmasks.items", "collections.defaultdict.items", "test_mask_ops.TestMaskCropPaste.coco.loadImgs", "numpy.array", "test_mask_ops.rasterize_polygons_with_grid_sample", "detectron2.layers.mask_ops.pad_masks", "detectron2.layers.mask_ops.scale_boxes", "detectron2.layers.mask_ops.paste_mask_in_image_old", "r.items", "torch.tensor().to", "detectron2.structures.PolygonMasks().crop_and_resize", "detectron2.structures.BitMasks().crop_and_resize", "detectron2.layers.mask_ops.paste_masks_in_image", "numpy.asarray", "test_mask_ops.iou_between_full_image_bit_masks", "table.append", "detectron2.structures.Boxes", "detectron2.structures.masks.polygons_to_bitmask.astype", "torch.tensor", "detectron2.structures.PolygonMasks", "detectron2.structures.BitMasks", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.polygons_to_bitmask", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.rasterize_polygons_with_grid_sample", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.pad_masks", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.scale_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.paste_mask_in_image_old", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.crop_and_resize", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.crop_and_resize", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.paste_masks_in_image", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.iou_between_full_image_bit_masks"], ["", "def", "process_annotation", "(", "self", ",", "ann", ",", "mask_side_len", "=", "28", ")", ":", "\n", "# Parse annotation data", "\n", "        ", "img_info", "=", "self", ".", "coco", ".", "loadImgs", "(", "ids", "=", "[", "ann", "[", "\"image_id\"", "]", "]", ")", "[", "0", "]", "\n", "height", ",", "width", "=", "img_info", "[", "\"height\"", "]", ",", "img_info", "[", "\"width\"", "]", "\n", "gt_polygons", "=", "[", "np", ".", "array", "(", "p", ",", "dtype", "=", "np", ".", "float64", ")", "for", "p", "in", "ann", "[", "\"segmentation\"", "]", "]", "\n", "gt_bbox", "=", "BoxMode", ".", "convert", "(", "ann", "[", "\"bbox\"", "]", ",", "BoxMode", ".", "XYWH_ABS", ",", "BoxMode", ".", "XYXY_ABS", ")", "\n", "gt_bit_mask", "=", "polygons_to_bitmask", "(", "gt_polygons", ",", "height", ",", "width", ")", "\n", "\n", "# Run rasterize ..", "\n", "torch_gt_bbox", "=", "torch", ".", "tensor", "(", "gt_bbox", ")", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", ".", "reshape", "(", "-", "1", ",", "4", ")", "\n", "box_bitmasks", "=", "{", "\n", "\"polygon\"", ":", "PolygonMasks", "(", "[", "gt_polygons", "]", ")", ".", "crop_and_resize", "(", "torch_gt_bbox", ",", "mask_side_len", ")", "[", "0", "]", ",", "\n", "\"gridsample\"", ":", "rasterize_polygons_with_grid_sample", "(", "gt_bit_mask", ",", "gt_bbox", ",", "mask_side_len", ")", ",", "\n", "\"roialign\"", ":", "BitMasks", "(", "torch", ".", "from_numpy", "(", "gt_bit_mask", "[", "None", ",", ":", ",", ":", "]", ")", ")", ".", "crop_and_resize", "(", "\n", "torch_gt_bbox", ",", "mask_side_len", "\n", ")", "[", "0", "]", ",", "\n", "}", "\n", "\n", "# Run paste ..", "\n", "results", "=", "defaultdict", "(", "dict", ")", "\n", "for", "k", ",", "box_bitmask", "in", "box_bitmasks", ".", "items", "(", ")", ":", "\n", "            ", "padded_bitmask", ",", "scale", "=", "pad_masks", "(", "box_bitmask", "[", "None", ",", ":", ",", ":", "]", ",", "1", ")", "\n", "scaled_boxes", "=", "scale_boxes", "(", "torch_gt_bbox", ",", "scale", ")", "\n", "\n", "r", "=", "results", "[", "k", "]", "\n", "r", "[", "\"old\"", "]", "=", "paste_mask_in_image_old", "(", "\n", "padded_bitmask", "[", "0", "]", ",", "scaled_boxes", "[", "0", "]", ",", "height", ",", "width", ",", "threshold", "=", "0.5", "\n", ")", "\n", "r", "[", "\"aligned\"", "]", "=", "paste_masks_in_image", "(", "\n", "box_bitmask", "[", "None", ",", ":", ",", ":", "]", ",", "Boxes", "(", "torch_gt_bbox", ")", ",", "(", "height", ",", "width", ")", "\n", ")", "[", "0", "]", "\n", "\n", "", "table", "=", "[", "]", "\n", "for", "rasterize_method", ",", "r", "in", "results", ".", "items", "(", ")", ":", "\n", "            ", "for", "paste_method", ",", "mask", "in", "r", ".", "items", "(", ")", ":", "\n", "                ", "mask", "=", "np", ".", "asarray", "(", "mask", ")", "\n", "iou", "=", "iou_between_full_image_bit_masks", "(", "gt_bit_mask", ".", "astype", "(", "\"uint8\"", ")", ",", "mask", ")", "\n", "table", ".", "append", "(", "(", "rasterize_method", ",", "paste_method", ",", "iou", ")", ")", "\n", "", "", "return", "table", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.TestMaskCropPaste.test_polygon_area": [[141, 155], ["detectron2.structures.PolygonMasks", "test_mask_ops.TestMaskCropPaste.assertEqual", "detectron2.structures.PolygonMasks", "test_mask_ops.TestMaskCropPaste.assertEqual", "detectron2.structures.PolygonMasks.area", "detectron2.structures.PolygonMasks.area"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area"], ["", "def", "test_polygon_area", "(", "self", ")", ":", "\n", "# Draw polygon boxes", "\n", "        ", "for", "d", "in", "[", "5.0", ",", "10.0", ",", "1000.0", "]", ":", "\n", "            ", "polygon", "=", "PolygonMasks", "(", "[", "[", "[", "0", ",", "0", ",", "0", ",", "d", ",", "d", ",", "d", ",", "d", ",", "0", "]", "]", "]", ")", "\n", "area", "=", "polygon", ".", "area", "(", ")", "[", "0", "]", "\n", "target", "=", "d", "**", "2", "\n", "self", ".", "assertEqual", "(", "area", ",", "target", ")", "\n", "\n", "# Draw polygon triangles", "\n", "", "for", "d", "in", "[", "5.0", ",", "10.0", ",", "1000.0", "]", ":", "\n", "            ", "polygon", "=", "PolygonMasks", "(", "[", "[", "[", "0", ",", "0", ",", "0", ",", "d", ",", "d", ",", "d", "]", "]", "]", ")", "\n", "area", "=", "polygon", ".", "area", "(", ")", "[", "0", "]", "\n", "target", "=", "d", "**", "2", "/", "2", "\n", "self", ".", "assertEqual", "(", "area", ",", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.TestMaskCropPaste.test_paste_mask_scriptable": [[156, 166], ["torch.jit.script", "torch.rand", "detectron2.structures.Boxes", "detectron2.layers.mask_ops.paste_masks_in_image", "torch.jit.script.", "test_mask_ops.TestMaskCropPaste.assertTrue", "detectron2.utils.testing.random_boxes", "torch.equal"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.paste_masks_in_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes"], ["", "", "def", "test_paste_mask_scriptable", "(", "self", ")", ":", "\n", "        ", "scripted_f", "=", "torch", ".", "jit", ".", "script", "(", "paste_masks_in_image", ")", "\n", "N", "=", "10", "\n", "masks", "=", "torch", ".", "rand", "(", "N", ",", "28", ",", "28", ")", "\n", "boxes", "=", "Boxes", "(", "random_boxes", "(", "N", ",", "100", ")", ")", "\n", "image_shape", "=", "(", "150", ",", "150", ")", "\n", "\n", "out", "=", "paste_masks_in_image", "(", "masks", ",", "boxes", ",", "image_shape", ")", "\n", "scripted_out", "=", "scripted_f", "(", "masks", ",", "boxes", ",", "image_shape", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "out", ",", "scripted_out", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.iou_between_full_image_bit_masks": [[29, 33], ["None"], "function", ["None"], ["def", "iou_between_full_image_bit_masks", "(", "a", ",", "b", ")", ":", "\n", "    ", "intersect", "=", "(", "a", "&", "b", ")", ".", "sum", "(", ")", "\n", "union", "=", "(", "a", "|", "b", ")", ".", "sum", "(", ")", "\n", "return", "intersect", "/", "union", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.rasterize_polygons_with_grid_sample": [[35, 58], ["torch.meshgrid", "torch.stack().to", "torch.from_numpy", "torch.nn.functional.grid_sample", "numpy.arange", "numpy.arange", "torch.from_numpy", "torch.from_numpy", "full_image_bit_mask[].to", "torch.stack"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "rasterize_polygons_with_grid_sample", "(", "full_image_bit_mask", ",", "box", ",", "mask_size", ",", "threshold", "=", "0.5", ")", ":", "\n", "    ", "x0", ",", "y0", ",", "x1", ",", "y1", "=", "box", "[", "0", "]", ",", "box", "[", "1", "]", ",", "box", "[", "2", "]", ",", "box", "[", "3", "]", "\n", "\n", "img_h", ",", "img_w", "=", "full_image_bit_mask", ".", "shape", "\n", "\n", "mask_y", "=", "np", ".", "arange", "(", "0.0", ",", "mask_size", ")", "+", "0.5", "# mask y sample coords in [0.5, mask_size - 0.5]", "\n", "mask_x", "=", "np", ".", "arange", "(", "0.0", ",", "mask_size", ")", "+", "0.5", "# mask x sample coords in [0.5, mask_size - 0.5]", "\n", "mask_y", "=", "mask_y", "/", "mask_size", "*", "(", "y1", "-", "y0", ")", "+", "y0", "\n", "mask_x", "=", "mask_x", "/", "mask_size", "*", "(", "x1", "-", "x0", ")", "+", "x0", "\n", "\n", "mask_x", "=", "(", "mask_x", "-", "0.5", ")", "/", "(", "img_w", "-", "1", ")", "*", "2", "+", "-", "1", "\n", "mask_y", "=", "(", "mask_y", "-", "0.5", ")", "/", "(", "img_h", "-", "1", ")", "*", "2", "+", "-", "1", "\n", "gy", ",", "gx", "=", "torch", ".", "meshgrid", "(", "torch", ".", "from_numpy", "(", "mask_y", ")", ",", "torch", ".", "from_numpy", "(", "mask_x", ")", ")", "\n", "ind", "=", "torch", ".", "stack", "(", "[", "gx", ",", "gy", "]", ",", "dim", "=", "-", "1", ")", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "full_image_bit_mask", "=", "torch", ".", "from_numpy", "(", "full_image_bit_mask", ")", "\n", "mask", "=", "F", ".", "grid_sample", "(", "\n", "full_image_bit_mask", "[", "None", ",", "None", ",", ":", ",", ":", "]", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", ",", "\n", "ind", "[", "None", ",", ":", ",", ":", ",", ":", "]", ",", "\n", "align_corners", "=", "True", ",", "\n", ")", "\n", "\n", "return", "mask", "[", "0", ",", "0", "]", ">=", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_mask_ops.benchmark_paste": [[168, 198], ["torch.manual_seed", "torch.rand", "torch.clamp", "torch.clamp", "torch.clamp", "detectron2.structures.Boxes", "torch.cuda.is_available", "fvcore.common.benchmark.benchmark", "torch.cat", "torch.rand.to", "detectron2.structures.Boxes.to", "specs.append", "torch.rand", "range", "torch.device", "torch.randn", "detectron2.layers.mask_ops.paste_masks_in_image", "torch.cuda.synchronize", "torch.device"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.mask_ops.paste_masks_in_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["", "", "def", "benchmark_paste", "(", ")", ":", "\n", "    ", "S", "=", "800", "\n", "H", ",", "W", "=", "image_shape", "=", "(", "S", ",", "S", ")", "\n", "N", "=", "64", "\n", "torch", ".", "manual_seed", "(", "42", ")", "\n", "masks", "=", "torch", ".", "rand", "(", "N", ",", "28", ",", "28", ")", "\n", "\n", "center", "=", "torch", ".", "rand", "(", "N", ",", "2", ")", "*", "600", "+", "100", "\n", "wh", "=", "torch", ".", "clamp", "(", "torch", ".", "randn", "(", "N", ",", "2", ")", "*", "40", "+", "200", ",", "min", "=", "50", ")", "\n", "x0y0", "=", "torch", ".", "clamp", "(", "center", "-", "wh", "*", "0.5", ",", "min", "=", "0.0", ")", "\n", "x1y1", "=", "torch", ".", "clamp", "(", "center", "+", "wh", "*", "0.5", ",", "max", "=", "S", ")", "\n", "boxes", "=", "Boxes", "(", "torch", ".", "cat", "(", "[", "x0y0", ",", "x1y1", "]", ",", "axis", "=", "1", ")", ")", "\n", "\n", "def", "func", "(", "device", ",", "n", "=", "3", ")", ":", "\n", "        ", "m", "=", "masks", ".", "to", "(", "device", "=", "device", ")", "\n", "b", "=", "boxes", ".", "to", "(", "device", "=", "device", ")", "\n", "\n", "def", "bench", "(", ")", ":", "\n", "            ", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "                ", "paste_masks_in_image", "(", "m", ",", "b", ",", "image_shape", ")", "\n", "", "if", "device", ".", "type", "==", "\"cuda\"", ":", "\n", "                ", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "\n", "", "", "return", "bench", "\n", "\n", "", "specs", "=", "[", "{", "\"device\"", ":", "torch", ".", "device", "(", "\"cpu\"", ")", ",", "\"n\"", ":", "3", "}", "]", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "specs", ".", "append", "(", "{", "\"device\"", ":", "torch", ".", "device", "(", "\"cuda\"", ")", ",", "\"n\"", ":", "3", "}", ")", "\n", "\n", "", "benchmark", "(", "func", ",", "\"paste_masks\"", ",", "specs", ",", "num_iters", "=", "10", ",", "warmup_iters", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms.TestNMS._create_tensors": [[11, 15], ["detectron2.utils.testing.random_boxes", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes"], ["    ", "def", "_create_tensors", "(", "self", ",", "N", ")", ":", "\n", "        ", "boxes", "=", "random_boxes", "(", "N", ",", "200", ")", "\n", "scores", "=", "torch", ".", "rand", "(", "N", ")", "\n", "return", "boxes", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms.TestNMS.test_nms_scriptability": [[16, 30], ["test_nms.TestNMS._create_tensors", "torch.randint", "torch.jit.script", "detectron2.layers.batched_nms", "boxes.clone", "torch.jit.script.", "torch.allclose", "test_nms.TestNMS.assertTrue", "torch.equal", "err_msg.format"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated._create_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["", "def", "test_nms_scriptability", "(", "self", ")", ":", "\n", "        ", "N", "=", "2000", "\n", "num_classes", "=", "50", "\n", "boxes", ",", "scores", "=", "self", ".", "_create_tensors", "(", "N", ")", "\n", "idxs", "=", "torch", ".", "randint", "(", "0", ",", "num_classes", ",", "(", "N", ",", ")", ")", "\n", "scripted_batched_nms", "=", "torch", ".", "jit", ".", "script", "(", "batched_nms", ")", "\n", "err_msg", "=", "\"NMS is incompatible with jit-scripted NMS for IoU={}\"", "\n", "\n", "for", "iou", "in", "[", "0.2", ",", "0.5", ",", "0.8", "]", ":", "\n", "            ", "keep_ref", "=", "batched_nms", "(", "boxes", ",", "scores", ",", "idxs", ",", "iou", ")", "\n", "backup", "=", "boxes", ".", "clone", "(", ")", "\n", "scripted_keep", "=", "scripted_batched_nms", "(", "boxes", ",", "scores", ",", "idxs", ",", "iou", ")", "\n", "assert", "torch", ".", "allclose", "(", "boxes", ",", "backup", ")", ",", "\"boxes modified by jit-scripted batched_nms\"", "\n", "self", ".", "assertTrue", "(", "torch", ".", "equal", "(", "keep_ref", ",", "scripted_keep", ")", ",", "err_msg", ".", "format", "(", "iou", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_blocks.TestBlocks.test_separable_conv": [[17, 19], ["detectron2.layers.DepthwiseSeparableConv2d", "torch.nn.PReLU"], "methods", ["None"], ["    ", "def", "test_separable_conv", "(", "self", ")", ":", "\n", "        ", "DepthwiseSeparableConv2d", "(", "3", ",", "10", ",", "norm1", "=", "\"BN\"", ",", "activation1", "=", "nn", ".", "PReLU", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_blocks.TestBlocks.test_aspp": [[20, 24], ["detectron2.layers.ASPP", "test_blocks.TestBlocks.assertIsNot", "test_blocks.TestBlocks.assertIsNot", "torch.nn.PReLU"], "methods", ["None"], ["", "def", "test_aspp", "(", "self", ")", ":", "\n", "        ", "m", "=", "ASPP", "(", "3", ",", "10", ",", "[", "2", ",", "3", ",", "4", "]", ",", "norm", "=", "\"\"", ",", "activation", "=", "nn", ".", "PReLU", "(", ")", ")", "\n", "self", ".", "assertIsNot", "(", "m", ".", "convs", "[", "0", "]", ".", "activation", ".", "weight", ",", "m", ".", "convs", "[", "1", "]", ".", "activation", ".", "weight", ")", "\n", "self", ".", "assertIsNot", "(", "m", ".", "convs", "[", "0", "]", ".", "activation", ".", "weight", ",", "m", ".", "project", ".", "activation", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_blocks.TestBlocks.test_frozen_batchnorm_fp16": [[25, 41], ["unittest.skipIf", "torch.rand().cuda", "detectron2.layers.FrozenBatchNorm2d().cuda", "test_blocks.TestBlocks.assertEqual", "torch.rand().cuda.requires_grad_", "test_blocks.TestBlocks.assertEqual", "autocast", "detectron2.layers.FrozenBatchNorm2d().cuda.", "autocast", "detectron2.layers.FrozenBatchNorm2d().cuda.", "torch.cuda.is_available", "torch.rand", "detectron2.layers.FrozenBatchNorm2d", "torch.rand().cuda.half", "torch.rand().cuda.half"], "methods", ["None"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_frozen_batchnorm_fp16", "(", "self", ")", ":", "\n", "        ", "from", "torch", ".", "cuda", ".", "amp", "import", "autocast", "\n", "\n", "C", "=", "10", "\n", "input", "=", "torch", ".", "rand", "(", "1", ",", "C", ",", "10", ",", "10", ")", ".", "cuda", "(", ")", "\n", "m", "=", "FrozenBatchNorm2d", "(", "C", ")", ".", "cuda", "(", ")", "\n", "with", "autocast", "(", ")", ":", "\n", "            ", "output", "=", "m", "(", "input", ".", "half", "(", ")", ")", "\n", "", "self", ".", "assertEqual", "(", "output", ".", "dtype", ",", "torch", ".", "float16", ")", "\n", "\n", "# requires_grad triggers a different codepath", "\n", "input", ".", "requires_grad_", "(", ")", "\n", "with", "autocast", "(", ")", ":", "\n", "            ", "output", "=", "m", "(", "input", ".", "half", "(", ")", ")", "\n", "", "self", ".", "assertEqual", "(", "output", ".", "dtype", ",", "torch", ".", "float16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_blocks.TestBlocks.test_resnet_unused_stages": [[42, 52], ["detectron2.modeling.backbone.resnet.ResNet", "test_blocks.TestBlocks.assertTrue", "test_blocks.TestBlocks.assertFalse", "test_blocks.TestBlocks.assertFalse", "detectron2.modeling.backbone.resnet.ResNet", "test_blocks.TestBlocks.assertTrue", "test_blocks.TestBlocks.assertTrue", "test_blocks.TestBlocks.assertTrue", "detectron2.modeling.backbone.resnet.BasicStem", "detectron2.modeling.backbone.resnet.ResNet.make_default_stages", "hasattr", "hasattr", "hasattr", "detectron2.modeling.backbone.resnet.BasicStem", "detectron2.modeling.backbone.resnet.ResNet.make_default_stages", "hasattr", "hasattr", "hasattr"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.ResNet.make_default_stages", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.resnet.ResNet.make_default_stages"], ["", "def", "test_resnet_unused_stages", "(", "self", ")", ":", "\n", "        ", "resnet", "=", "ResNet", "(", "BasicStem", "(", ")", ",", "ResNet", ".", "make_default_stages", "(", "18", ")", ",", "out_features", "=", "[", "\"res2\"", "]", ")", "\n", "self", ".", "assertTrue", "(", "hasattr", "(", "resnet", ",", "\"res2\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "hasattr", "(", "resnet", ",", "\"res3\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "hasattr", "(", "resnet", ",", "\"res5\"", ")", ")", "\n", "\n", "resnet", "=", "ResNet", "(", "BasicStem", "(", ")", ",", "ResNet", ".", "make_default_stages", "(", "18", ")", ",", "out_features", "=", "[", "\"res2\"", ",", "\"res5\"", "]", ")", "\n", "self", ".", "assertTrue", "(", "hasattr", "(", "resnet", ",", "\"res2\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "hasattr", "(", "resnet", ",", "\"res4\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "hasattr", "(", "resnet", ",", "\"res5\"", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.reference_horizontal_nms": [[45, 68], ["scores.sort", "torch.as_tensor", "len", "picked.append", "torchvision.ops.box_iou().squeeze", "current.item", "len", "torchvision.ops.box_iou", "current_box.unsqueeze"], "methods", ["None"], ["    ", "def", "reference_horizontal_nms", "(", "self", ",", "boxes", ",", "scores", ",", "iou_threshold", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            box_scores (N, 5): boxes in corner-form and probabilities.\n                (Note here 5 == 4 + 1, i.e., 4-dim horizontal box + 1-dim prob)\n            iou_threshold: intersection over union threshold.\n        Returns:\n             picked: a list of indexes of the kept boxes\n        \"\"\"", "\n", "picked", "=", "[", "]", "\n", "_", ",", "indexes", "=", "scores", ".", "sort", "(", "descending", "=", "True", ")", "\n", "while", "len", "(", "indexes", ")", ">", "0", ":", "\n", "            ", "current", "=", "indexes", "[", "0", "]", "\n", "picked", ".", "append", "(", "current", ".", "item", "(", ")", ")", "\n", "if", "len", "(", "indexes", ")", "==", "1", ":", "\n", "                ", "break", "\n", "", "current_box", "=", "boxes", "[", "current", ",", ":", "]", "\n", "indexes", "=", "indexes", "[", "1", ":", "]", "\n", "rest_boxes", "=", "boxes", "[", "indexes", ",", ":", "]", "\n", "iou", "=", "ops", ".", "box_iou", "(", "rest_boxes", ",", "current_box", ".", "unsqueeze", "(", "0", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "indexes", "=", "indexes", "[", "iou", "<=", "iou_threshold", "]", "\n", "\n", "", "return", "torch", ".", "as_tensor", "(", "picked", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated._create_tensors": [[69, 73], ["detectron2.utils.testing.random_boxes", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes"], ["", "def", "_create_tensors", "(", "self", ",", "N", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "boxes", "=", "random_boxes", "(", "N", ",", "200", ",", "device", "=", "device", ")", "\n", "scores", "=", "torch", ".", "rand", "(", "N", ",", "device", "=", "device", ")", "\n", "return", "boxes", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.test_batched_nms_rotated_0_degree_cpu": [[74, 96], ["test_nms_rotated.TestNMSRotated._create_tensors", "torch.randint", "torch.zeros", "boxes.clone", "detectron2.layers.batched_nms", "torch.allclose", "torch.zeros.clone", "detectron2.layers.batched_nms_rotated", "torch.allclose", "test_nms_rotated.TestNMSRotated.assertLessEqual", "test_nms_rotated.nms_edit_distance", "err_msg.format"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated._create_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.batched_nms_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.nms_edit_distance"], ["", "def", "test_batched_nms_rotated_0_degree_cpu", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "N", "=", "2000", "\n", "num_classes", "=", "50", "\n", "boxes", ",", "scores", "=", "self", ".", "_create_tensors", "(", "N", ",", "device", "=", "device", ")", "\n", "idxs", "=", "torch", ".", "randint", "(", "0", ",", "num_classes", ",", "(", "N", ",", ")", ")", "\n", "rotated_boxes", "=", "torch", ".", "zeros", "(", "N", ",", "5", ",", "device", "=", "device", ")", "\n", "rotated_boxes", "[", ":", ",", "0", "]", "=", "(", "boxes", "[", ":", ",", "0", "]", "+", "boxes", "[", ":", ",", "2", "]", ")", "/", "2.0", "\n", "rotated_boxes", "[", ":", ",", "1", "]", "=", "(", "boxes", "[", ":", ",", "1", "]", "+", "boxes", "[", ":", ",", "3", "]", ")", "/", "2.0", "\n", "rotated_boxes", "[", ":", ",", "2", "]", "=", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", "\n", "rotated_boxes", "[", ":", ",", "3", "]", "=", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", "\n", "err_msg", "=", "\"Rotated NMS with 0 degree is incompatible with horizontal NMS for IoU={}\"", "\n", "for", "iou", "in", "[", "0.2", ",", "0.5", ",", "0.8", "]", ":", "\n", "            ", "backup", "=", "boxes", ".", "clone", "(", ")", "\n", "keep_ref", "=", "batched_nms", "(", "boxes", ",", "scores", ",", "idxs", ",", "iou", ")", "\n", "assert", "torch", ".", "allclose", "(", "boxes", ",", "backup", ")", ",", "\"boxes modified by batched_nms\"", "\n", "backup", "=", "rotated_boxes", ".", "clone", "(", ")", "\n", "keep", "=", "batched_nms_rotated", "(", "rotated_boxes", ",", "scores", ",", "idxs", ",", "iou", ")", "\n", "assert", "torch", ".", "allclose", "(", "\n", "rotated_boxes", ",", "backup", "\n", ")", ",", "\"rotated_boxes modified by batched_nms_rotated\"", "\n", "# Occasionally the gap can be large if there are many IOU on the threshold boundary", "\n", "self", ".", "assertLessEqual", "(", "nms_edit_distance", "(", "keep", ",", "keep_ref", ")", ",", "5", ",", "err_msg", ".", "format", "(", "iou", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.test_batched_nms_rotated_0_degree_cuda": [[97, 100], ["unittest.skipIf", "test_nms_rotated.TestNMSRotated.test_batched_nms_rotated_0_degree_cpu", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.test_batched_nms_rotated_0_degree_cpu"], ["", "", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_batched_nms_rotated_0_degree_cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "test_batched_nms_rotated_0_degree_cpu", "(", "device", "=", "\"cuda\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.test_nms_rotated_0_degree_cpu": [[101, 114], ["test_nms_rotated.TestNMSRotated._create_tensors", "torch.zeros", "test_nms_rotated.TestNMSRotated.reference_horizontal_nms", "detectron2.layers.nms_rotated", "test_nms_rotated.TestNMSRotated.assertLessEqual", "test_nms_rotated.nms_edit_distance", "err_msg.format"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated._create_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.reference_horizontal_nms", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.nms_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.nms_edit_distance"], ["", "def", "test_nms_rotated_0_degree_cpu", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "N", "=", "1000", "\n", "boxes", ",", "scores", "=", "self", ".", "_create_tensors", "(", "N", ",", "device", "=", "device", ")", "\n", "rotated_boxes", "=", "torch", ".", "zeros", "(", "N", ",", "5", ",", "device", "=", "device", ")", "\n", "rotated_boxes", "[", ":", ",", "0", "]", "=", "(", "boxes", "[", ":", ",", "0", "]", "+", "boxes", "[", ":", ",", "2", "]", ")", "/", "2.0", "\n", "rotated_boxes", "[", ":", ",", "1", "]", "=", "(", "boxes", "[", ":", ",", "1", "]", "+", "boxes", "[", ":", ",", "3", "]", ")", "/", "2.0", "\n", "rotated_boxes", "[", ":", ",", "2", "]", "=", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", "\n", "rotated_boxes", "[", ":", ",", "3", "]", "=", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", "\n", "err_msg", "=", "\"Rotated NMS incompatible between CPU and reference implementation for IoU={}\"", "\n", "for", "iou", "in", "[", "0.2", ",", "0.5", ",", "0.8", "]", ":", "\n", "            ", "keep_ref", "=", "self", ".", "reference_horizontal_nms", "(", "boxes", ",", "scores", ",", "iou", ")", "\n", "keep", "=", "nms_rotated", "(", "rotated_boxes", ",", "scores", ",", "iou", ")", "\n", "self", ".", "assertLessEqual", "(", "nms_edit_distance", "(", "keep", ",", "keep_ref", ")", ",", "1", ",", "err_msg", ".", "format", "(", "iou", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.test_nms_rotated_0_degree_cuda": [[115, 118], ["unittest.skipIf", "test_nms_rotated.TestNMSRotated.test_nms_rotated_0_degree_cpu", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.test_nms_rotated_0_degree_cpu"], ["", "", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_nms_rotated_0_degree_cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "test_nms_rotated_0_degree_cpu", "(", "device", "=", "\"cuda\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.test_nms_rotated_90_degrees_cpu": [[119, 137], ["test_nms_rotated.TestNMSRotated._create_tensors", "torch.zeros", "torch.ones", "test_nms_rotated.TestNMSRotated.reference_horizontal_nms", "detectron2.layers.nms_rotated", "test_nms_rotated.TestNMSRotated.assertLessEqual", "test_nms_rotated.nms_edit_distance", "err_msg.format"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated._create_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.reference_horizontal_nms", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.nms_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.nms_edit_distance"], ["", "def", "test_nms_rotated_90_degrees_cpu", "(", "self", ")", ":", "\n", "        ", "N", "=", "1000", "\n", "boxes", ",", "scores", "=", "self", ".", "_create_tensors", "(", "N", ")", "\n", "rotated_boxes", "=", "torch", ".", "zeros", "(", "N", ",", "5", ")", "\n", "rotated_boxes", "[", ":", ",", "0", "]", "=", "(", "boxes", "[", ":", ",", "0", "]", "+", "boxes", "[", ":", ",", "2", "]", ")", "/", "2.0", "\n", "rotated_boxes", "[", ":", ",", "1", "]", "=", "(", "boxes", "[", ":", ",", "1", "]", "+", "boxes", "[", ":", ",", "3", "]", ")", "/", "2.0", "\n", "# Note for rotated_boxes[:, 2] and rotated_boxes[:, 3]:", "\n", "# widths and heights are intentionally swapped here for 90 degrees case", "\n", "# so that the reference horizontal nms could be used", "\n", "rotated_boxes", "[", ":", ",", "2", "]", "=", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", "\n", "rotated_boxes", "[", ":", ",", "3", "]", "=", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", "\n", "\n", "rotated_boxes", "[", ":", ",", "4", "]", "=", "torch", ".", "ones", "(", "N", ")", "*", "90", "\n", "err_msg", "=", "\"Rotated NMS incompatible between CPU and reference implementation for IoU={}\"", "\n", "for", "iou", "in", "[", "0.2", ",", "0.5", ",", "0.8", "]", ":", "\n", "            ", "keep_ref", "=", "self", ".", "reference_horizontal_nms", "(", "boxes", ",", "scores", ",", "iou", ")", "\n", "keep", "=", "nms_rotated", "(", "rotated_boxes", ",", "scores", ",", "iou", ")", "\n", "self", ".", "assertLessEqual", "(", "nms_edit_distance", "(", "keep", ",", "keep_ref", ")", ",", "1", ",", "err_msg", ".", "format", "(", "iou", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.test_nms_rotated_180_degrees_cpu": [[138, 152], ["test_nms_rotated.TestNMSRotated._create_tensors", "torch.zeros", "torch.ones", "test_nms_rotated.TestNMSRotated.reference_horizontal_nms", "detectron2.layers.nms_rotated", "test_nms_rotated.TestNMSRotated.assertLessEqual", "test_nms_rotated.nms_edit_distance", "err_msg.format"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated._create_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestNMSRotated.reference_horizontal_nms", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.nms_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.nms_edit_distance"], ["", "", "def", "test_nms_rotated_180_degrees_cpu", "(", "self", ")", ":", "\n", "        ", "N", "=", "1000", "\n", "boxes", ",", "scores", "=", "self", ".", "_create_tensors", "(", "N", ")", "\n", "rotated_boxes", "=", "torch", ".", "zeros", "(", "N", ",", "5", ")", "\n", "rotated_boxes", "[", ":", ",", "0", "]", "=", "(", "boxes", "[", ":", ",", "0", "]", "+", "boxes", "[", ":", ",", "2", "]", ")", "/", "2.0", "\n", "rotated_boxes", "[", ":", ",", "1", "]", "=", "(", "boxes", "[", ":", ",", "1", "]", "+", "boxes", "[", ":", ",", "3", "]", ")", "/", "2.0", "\n", "rotated_boxes", "[", ":", ",", "2", "]", "=", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", "\n", "rotated_boxes", "[", ":", ",", "3", "]", "=", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", "\n", "rotated_boxes", "[", ":", ",", "4", "]", "=", "torch", ".", "ones", "(", "N", ")", "*", "180", "\n", "err_msg", "=", "\"Rotated NMS incompatible between CPU and reference implementation for IoU={}\"", "\n", "for", "iou", "in", "[", "0.2", ",", "0.5", ",", "0.8", "]", ":", "\n", "            ", "keep_ref", "=", "self", ".", "reference_horizontal_nms", "(", "boxes", ",", "scores", ",", "iou", ")", "\n", "keep", "=", "nms_rotated", "(", "rotated_boxes", ",", "scores", ",", "iou", ")", "\n", "self", ".", "assertLessEqual", "(", "nms_edit_distance", "(", "keep", ",", "keep_ref", ")", ",", "1", ",", "err_msg", ".", "format", "(", "iou", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestScriptable.setUp": [[155, 161], ["TestingModule", "detectron2.layers.nms_rotated"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.nms.nms_rotated"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "class", "TestingModule", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "boxes", ",", "scores", ",", "threshold", ")", ":", "\n", "                ", "return", "nms_rotated", "(", "boxes", ",", "scores", ",", "threshold", ")", "\n", "\n", "", "", "self", ".", "module", "=", "TestingModule", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestScriptable.test_scriptable_cpu": [[162, 166], ["unittest.skipIf", "copy.deepcopy().cpu", "torch.jit.script", "copy.deepcopy"], "methods", ["None"], ["", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_scriptable_cpu", "(", "self", ")", ":", "\n", "        ", "m", "=", "deepcopy", "(", "self", ".", "module", ")", ".", "cpu", "(", ")", "\n", "_", "=", "torch", ".", "jit", ".", "script", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.TestScriptable.test_scriptable_cuda": [[167, 172], ["unittest.skipIf", "unittest.skipIf", "copy.deepcopy().cuda", "torch.jit.script", "torch.cuda.is_available", "copy.deepcopy"], "methods", ["None"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "@", "unittest", ".", "skipIf", "(", "TORCH_VERSION", "<", "(", "1", ",", "7", ")", ",", "\"Insufficient pytorch version\"", ")", "\n", "def", "test_scriptable_cuda", "(", "self", ")", ":", "\n", "        ", "m", "=", "deepcopy", "(", "self", ".", "module", ")", ".", "cuda", "(", ")", "\n", "_", "=", "torch", ".", "jit", ".", "script", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_nms_rotated.nms_edit_distance": [[14, 42], ["torch.equal", "range", "keep1.cpu", "keep2.cpu", "tuple", "tuple", "len", "len", "numpy.arange", "numpy.arange", "range", "min", "min"], "function", ["None"], ["def", "nms_edit_distance", "(", "keep1", ",", "keep2", ")", ":", "\n", "    ", "\"\"\"\n    Compare the \"keep\" result of two nms call.\n    They are allowed to be different in terms of edit distance\n    due to floating point precision issues, e.g.,\n    if a box happen to have an IoU of 0.5 with another box,\n    one implentation may choose to keep it while another may discard it.\n    \"\"\"", "\n", "keep1", ",", "keep2", "=", "keep1", ".", "cpu", "(", ")", ",", "keep2", ".", "cpu", "(", ")", "\n", "if", "torch", ".", "equal", "(", "keep1", ",", "keep2", ")", ":", "\n", "# they should be equal most of the time", "\n", "        ", "return", "0", "\n", "", "keep1", ",", "keep2", "=", "tuple", "(", "keep1", ")", ",", "tuple", "(", "keep2", ")", "\n", "m", ",", "n", "=", "len", "(", "keep1", ")", ",", "len", "(", "keep2", ")", "\n", "\n", "# edit distance with DP", "\n", "f", "=", "[", "np", ".", "arange", "(", "n", "+", "1", ")", ",", "np", ".", "arange", "(", "n", "+", "1", ")", "]", "\n", "for", "i", "in", "range", "(", "m", ")", ":", "\n", "        ", "cur_row", "=", "i", "%", "2", "\n", "other_row", "=", "(", "i", "+", "1", ")", "%", "2", "\n", "f", "[", "other_row", "]", "[", "0", "]", "=", "i", "+", "1", "\n", "for", "j", "in", "range", "(", "n", ")", ":", "\n", "            ", "f", "[", "other_row", "]", "[", "j", "+", "1", "]", "=", "(", "\n", "f", "[", "cur_row", "]", "[", "j", "]", "\n", "if", "keep1", "[", "i", "]", "==", "keep2", "[", "j", "]", "\n", "else", "min", "(", "min", "(", "f", "[", "cur_row", "]", "[", "j", "]", ",", "f", "[", "cur_row", "]", "[", "j", "+", "1", "]", ")", ",", "f", "[", "other_row", "]", "[", "j", "]", ")", "+", "1", "\n", ")", "\n", "", "", "return", "f", "[", "m", "%", "2", "]", "[", "n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_deformable.DeformableTest.test_forward_output": [[10, 54], ["unittest.skipIf", "torch.device", "torch.arange().reshape().to", "torch.full().to", "detectron2.layers.DeformConv().to", "torch.nn.Parameter", "detectron2.layers.DeformConv().to.", "output.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.array", "test_deformable.DeformableTest.assertTrue", "torch.full().to", "detectron2.layers.ModulatedDeformConv().to", "detectron2.layers.ModulatedDeformConv().to.", "output.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "test_deformable.DeformableTest.assertTrue", "torch.ones_like", "numpy.allclose", "numpy.allclose", "torch.cuda.is_available", "torch.arange().reshape", "torch.full", "detectron2.layers.DeformConv", "output.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "output.detach().cpu().numpy.detach().cpu().numpy.flatten", "numpy.array.flatten", "torch.full", "detectron2.layers.ModulatedDeformConv", "output.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "output.detach().cpu().numpy.detach().cpu().numpy.flatten", "numpy.array.flatten", "torch.arange", "output.detach().cpu().numpy.detach().cpu().numpy.detach", "output.detach().cpu().numpy.detach().cpu().numpy.detach", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["    ", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"Deformable not supported for cpu\"", ")", "\n", "def", "test_forward_output", "(", "self", ")", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ")", "\n", "N", ",", "C", ",", "H", ",", "W", "=", "shape", "=", "1", ",", "1", ",", "5", ",", "5", "\n", "kernel_size", "=", "3", "\n", "padding", "=", "1", "\n", "\n", "inputs", "=", "torch", ".", "arange", "(", "np", ".", "prod", "(", "shape", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "reshape", "(", "*", "shape", ")", ".", "to", "(", "device", ")", "\n", "\"\"\"\n        0  1  2   3 4\n        5  6  7   8 9\n        10 11 12 13 14\n        15 16 17 18 19\n        20 21 22 23 24\n        \"\"\"", "\n", "offset_channels", "=", "kernel_size", "*", "kernel_size", "*", "2", "\n", "offset", "=", "torch", ".", "full", "(", "(", "N", ",", "offset_channels", ",", "H", ",", "W", ")", ",", "0.5", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Test DCN v1", "\n", "deform", "=", "DeformConv", "(", "C", ",", "C", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ")", ".", "to", "(", "device", ")", "\n", "deform", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones_like", "(", "deform", ".", "weight", ")", ")", "\n", "output", "=", "deform", "(", "inputs", ",", "offset", ")", "\n", "output", "=", "output", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "deform_results", "=", "np", ".", "array", "(", "\n", "[", "\n", "[", "30", ",", "41.25", ",", "48.75", ",", "45", ",", "28.75", "]", ",", "\n", "[", "62.25", ",", "81", ",", "90", ",", "80.25", ",", "50.25", "]", ",", "\n", "[", "99.75", ",", "126", ",", "135", ",", "117.75", ",", "72.75", "]", ",", "\n", "[", "105", ",", "131.25", ",", "138.75", ",", "120", ",", "73.75", "]", ",", "\n", "[", "71.75", ",", "89.25", ",", "93.75", ",", "80.75", ",", "49.5", "]", ",", "\n", "]", "\n", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output", ".", "flatten", "(", ")", ",", "deform_results", ".", "flatten", "(", ")", ")", ")", "\n", "\n", "# Test DCN v2", "\n", "mask_channels", "=", "kernel_size", "*", "kernel_size", "\n", "mask", "=", "torch", ".", "full", "(", "(", "N", ",", "mask_channels", ",", "H", ",", "W", ")", ",", "0.5", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "to", "(", "device", ")", "\n", "modulate_deform", "=", "ModulatedDeformConv", "(", "C", ",", "C", ",", "kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "False", ")", ".", "to", "(", "\n", "device", "\n", ")", "\n", "modulate_deform", ".", "weight", "=", "deform", ".", "weight", "\n", "output", "=", "modulate_deform", "(", "inputs", ",", "offset", ",", "mask", ")", "\n", "output", "=", "output", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output", ".", "flatten", "(", ")", ",", "deform_results", ".", "flatten", "(", ")", "*", "0.5", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_deformable.DeformableTest.test_forward_output_on_cpu": [[55, 81], ["torch.device", "torch.arange().reshape().to", "torch.full().to", "detectron2.layers.DeformConv().to", "torch.nn.Parameter", "detectron2.layers.DeformConv().to.", "output.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.array", "test_deformable.DeformableTest.assertTrue", "torch.ones_like", "numpy.allclose", "torch.arange().reshape", "torch.full", "detectron2.layers.DeformConv", "output.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "output.detach().cpu().numpy.detach().cpu().numpy.flatten", "numpy.array.flatten", "torch.arange", "output.detach().cpu().numpy.detach().cpu().numpy.detach", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "def", "test_forward_output_on_cpu", "(", "self", ")", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "N", ",", "C", ",", "H", ",", "W", "=", "shape", "=", "1", ",", "1", ",", "5", ",", "5", "\n", "kernel_size", "=", "3", "\n", "padding", "=", "1", "\n", "\n", "inputs", "=", "torch", ".", "arange", "(", "np", ".", "prod", "(", "shape", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "reshape", "(", "*", "shape", ")", ".", "to", "(", "device", ")", "\n", "\n", "offset_channels", "=", "kernel_size", "*", "kernel_size", "*", "2", "\n", "offset", "=", "torch", ".", "full", "(", "(", "N", ",", "offset_channels", ",", "H", ",", "W", ")", ",", "0.5", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Test DCN v1 on cpu", "\n", "deform", "=", "DeformConv", "(", "C", ",", "C", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ")", ".", "to", "(", "device", ")", "\n", "deform", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones_like", "(", "deform", ".", "weight", ")", ")", "\n", "output", "=", "deform", "(", "inputs", ",", "offset", ")", "\n", "output", "=", "output", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "deform_results", "=", "np", ".", "array", "(", "\n", "[", "\n", "[", "30", ",", "41.25", ",", "48.75", ",", "45", ",", "28.75", "]", ",", "\n", "[", "62.25", ",", "81", ",", "90", ",", "80.25", ",", "50.25", "]", ",", "\n", "[", "99.75", ",", "126", ",", "135", ",", "117.75", ",", "72.75", "]", ",", "\n", "[", "105", ",", "131.25", ",", "138.75", ",", "120", ",", "73.75", "]", ",", "\n", "[", "71.75", ",", "89.25", ",", "93.75", ",", "80.75", ",", "49.5", "]", ",", "\n", "]", "\n", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output", ".", "flatten", "(", ")", ",", "deform_results", ".", "flatten", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_deformable.DeformableTest.test_forward_output_on_cpu_equals_output_on_gpu": [[82, 106], ["unittest.skipIf", "test_deformable.DeformableTest.assertTrue", "torch.arange().reshape", "torch.full", "detectron2.layers.DeformConv().to", "torch.nn.Parameter", "detectron2.layers.DeformConv().to.detach().cpu().numpy", "detectron2.layers.DeformConv().to", "torch.nn.Parameter", "detectron2.layers.DeformConv().to.detach().numpy", "numpy.allclose", "torch.cuda.is_available", "torch.ones_like", "torch.ones_like", "DeformConv().to.detach().cpu().numpy.flatten", "DeformConv().to.detach().numpy.flatten", "torch.arange", "detectron2.layers.DeformConv", "detectron2.layers.DeformConv().to.detach().cpu", "detectron2.layers.DeformConv", "detectron2.layers.DeformConv().to.detach", "numpy.prod", "detectron2.layers.DeformConv().to.detach", "detectron2.layers.DeformConv().to.", "torch.arange().reshape.to", "torch.full.to", "detectron2.layers.DeformConv().to.", "torch.arange().reshape.to", "torch.full.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"This test requires gpu access\"", ")", "\n", "def", "test_forward_output_on_cpu_equals_output_on_gpu", "(", "self", ")", ":", "\n", "        ", "N", ",", "C", ",", "H", ",", "W", "=", "shape", "=", "2", ",", "4", ",", "10", ",", "10", "\n", "kernel_size", "=", "3", "\n", "padding", "=", "1", "\n", "\n", "for", "groups", "in", "[", "1", ",", "2", "]", ":", "\n", "            ", "inputs", "=", "torch", ".", "arange", "(", "np", ".", "prod", "(", "shape", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "reshape", "(", "*", "shape", ")", "\n", "offset_channels", "=", "kernel_size", "*", "kernel_size", "*", "2", "\n", "offset", "=", "torch", ".", "full", "(", "(", "N", ",", "offset_channels", ",", "H", ",", "W", ")", ",", "0.5", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "deform_gpu", "=", "DeformConv", "(", "\n", "C", ",", "C", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "groups", "=", "groups", "\n", ")", ".", "to", "(", "\"cuda\"", ")", "\n", "deform_gpu", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones_like", "(", "deform_gpu", ".", "weight", ")", ")", "\n", "output_gpu", "=", "deform_gpu", "(", "inputs", ".", "to", "(", "\"cuda\"", ")", ",", "offset", ".", "to", "(", "\"cuda\"", ")", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "deform_cpu", "=", "DeformConv", "(", "\n", "C", ",", "C", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "groups", "=", "groups", "\n", ")", ".", "to", "(", "\"cpu\"", ")", "\n", "deform_cpu", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones_like", "(", "deform_cpu", ".", "weight", ")", ")", "\n", "output_cpu", "=", "deform_cpu", "(", "inputs", ".", "to", "(", "\"cpu\"", ")", ",", "offset", ".", "to", "(", "\"cpu\"", ")", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output_gpu", ".", "flatten", "(", ")", ",", "output_cpu", ".", "flatten", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_deformable.DeformableTest.test_small_input": [[107, 129], ["unittest.skipIf", "torch.device", "torch.rand().to", "torch.randn().to", "detectron2.layers.DeformConv().to", "detectron2.layers.DeformConv().to.", "test_deformable.DeformableTest.assertTrue", "torch.ones().to", "detectron2.layers.ModulatedDeformConv().to", "detectron2.layers.ModulatedDeformConv().to.", "test_deformable.DeformableTest.assertTrue", "torch.cuda.is_available", "torch.rand", "torch.randn", "detectron2.layers.DeformConv", "torch.ones", "detectron2.layers.ModulatedDeformConv"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"Deformable not supported for cpu\"", ")", "\n", "def", "test_small_input", "(", "self", ")", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ")", "\n", "for", "kernel_size", "in", "[", "3", ",", "5", "]", ":", "\n", "            ", "padding", "=", "kernel_size", "//", "2", "\n", "N", ",", "C", ",", "H", ",", "W", "=", "shape", "=", "(", "1", ",", "1", ",", "kernel_size", "-", "1", ",", "kernel_size", "-", "1", ")", "\n", "\n", "inputs", "=", "torch", ".", "rand", "(", "shape", ")", ".", "to", "(", "device", ")", "# input size is smaller than kernel size", "\n", "\n", "offset_channels", "=", "kernel_size", "*", "kernel_size", "*", "2", "\n", "offset", "=", "torch", ".", "randn", "(", "(", "N", ",", "offset_channels", ",", "H", ",", "W", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "to", "(", "device", ")", "\n", "deform", "=", "DeformConv", "(", "C", ",", "C", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ")", ".", "to", "(", "device", ")", "\n", "output", "=", "deform", "(", "inputs", ",", "offset", ")", "\n", "self", ".", "assertTrue", "(", "output", ".", "shape", "==", "inputs", ".", "shape", ")", "\n", "\n", "mask_channels", "=", "kernel_size", "*", "kernel_size", "\n", "mask", "=", "torch", ".", "ones", "(", "(", "N", ",", "mask_channels", ",", "H", ",", "W", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "to", "(", "device", ")", "\n", "modulate_deform", "=", "ModulatedDeformConv", "(", "\n", "C", ",", "C", ",", "kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "False", "\n", ")", ".", "to", "(", "device", ")", "\n", "output", "=", "modulate_deform", "(", "inputs", ",", "offset", ",", "mask", ")", "\n", "self", ".", "assertTrue", "(", "output", ".", "shape", "==", "inputs", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_deformable.DeformableTest.test_raise_exception": [[130, 151], ["unittest.skipIf", "torch.device", "torch.rand().to", "torch.randn().to", "detectron2.layers.DeformConv().to", "test_deformable.DeformableTest.assertRaises", "torch.randn().to", "torch.ones().to", "detectron2.layers.ModulatedDeformConv().to", "test_deformable.DeformableTest.assertRaises", "torch.cuda.is_available", "torch.rand", "torch.randn", "detectron2.layers.DeformConv", "torch.randn", "torch.ones", "detectron2.layers.ModulatedDeformConv"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"Deformable not supported for cpu\"", ")", "\n", "def", "test_raise_exception", "(", "self", ")", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ")", "\n", "N", ",", "C", ",", "H", ",", "W", "=", "shape", "=", "1", ",", "1", ",", "3", ",", "3", "\n", "kernel_size", "=", "3", "\n", "padding", "=", "1", "\n", "\n", "inputs", "=", "torch", ".", "rand", "(", "shape", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "to", "(", "device", ")", "\n", "offset_channels", "=", "kernel_size", "*", "kernel_size", "# This is wrong channels for offset", "\n", "offset", "=", "torch", ".", "randn", "(", "(", "N", ",", "offset_channels", ",", "H", ",", "W", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "to", "(", "device", ")", "\n", "deform", "=", "DeformConv", "(", "C", ",", "C", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "assertRaises", "(", "RuntimeError", ",", "deform", ",", "inputs", ",", "offset", ")", "\n", "\n", "offset_channels", "=", "kernel_size", "*", "kernel_size", "*", "2", "\n", "offset", "=", "torch", ".", "randn", "(", "(", "N", ",", "offset_channels", ",", "H", ",", "W", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "to", "(", "device", ")", "\n", "mask_channels", "=", "kernel_size", "*", "kernel_size", "*", "2", "# This is wrong channels for mask", "\n", "mask", "=", "torch", ".", "ones", "(", "(", "N", ",", "mask_channels", ",", "H", ",", "W", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "to", "(", "device", ")", "\n", "modulate_deform", "=", "ModulatedDeformConv", "(", "C", ",", "C", ",", "kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "False", ")", ".", "to", "(", "\n", "device", "\n", ")", "\n", "self", ".", "assertRaises", "(", "RuntimeError", ",", "modulate_deform", ",", "inputs", ",", "offset", ",", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.layers.test_deformable.DeformableTest.test_repr": [[152, 167], ["detectron2.layers.DeformConv", "test_deformable.DeformableTest.assertEqual", "detectron2.layers.ModulatedDeformConv", "test_deformable.DeformableTest.assertEqual", "repr", "repr"], "methods", ["None"], ["", "def", "test_repr", "(", "self", ")", ":", "\n", "        ", "module", "=", "DeformConv", "(", "3", ",", "10", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ",", "deformable_groups", "=", "2", ")", "\n", "correct_string", "=", "(", "\n", "\"DeformConv(in_channels=3, out_channels=10, kernel_size=(3, 3), \"", "\n", "\"stride=(1, 1), padding=(1, 1), dilation=(1, 1), \"", "\n", "\"groups=1, deformable_groups=2, bias=False)\"", "\n", ")", "\n", "self", ".", "assertEqual", "(", "repr", "(", "module", ")", ",", "correct_string", ")", "\n", "\n", "module", "=", "ModulatedDeformConv", "(", "3", ",", "10", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ",", "deformable_groups", "=", "2", ")", "\n", "correct_string", "=", "(", "\n", "\"ModulatedDeformConv(in_channels=3, out_channels=10, kernel_size=(3, 3), \"", "\n", "\"stride=1, padding=1, dilation=1, groups=1, deformable_groups=2, bias=True)\"", "\n", ")", "\n", "self", ".", "assertEqual", "(", "repr", "(", "module", ")", ",", "correct_string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalog.get": [[57, 64], ["name.startswith", "name.startswith", "RuntimeError", "catalog.ModelCatalog._get_c2_detectron_baseline", "catalog.ModelCatalog._get_c2_imagenet_pretrained"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalog._get_c2_detectron_baseline", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalog._get_c2_imagenet_pretrained"], ["@", "staticmethod", "\n", "def", "get", "(", "name", ")", ":", "\n", "        ", "if", "name", ".", "startswith", "(", "\"Caffe2Detectron/COCO\"", ")", ":", "\n", "            ", "return", "ModelCatalog", ".", "_get_c2_detectron_baseline", "(", "name", ")", "\n", "", "if", "name", ".", "startswith", "(", "\"ImageNetPretrained/\"", ")", ":", "\n", "            ", "return", "ModelCatalog", ".", "_get_c2_imagenet_pretrained", "(", "name", ")", "\n", "", "raise", "RuntimeError", "(", "\"model not present in the catalog: {}\"", ".", "format", "(", "name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalog._get_c2_imagenet_pretrained": [[65, 72], ["len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_c2_imagenet_pretrained", "(", "name", ")", ":", "\n", "        ", "prefix", "=", "ModelCatalog", ".", "S3_C2_DETECTRON_PREFIX", "\n", "name", "=", "name", "[", "len", "(", "\"ImageNetPretrained/\"", ")", ":", "]", "\n", "name", "=", "ModelCatalog", ".", "C2_IMAGENET_MODELS", "[", "name", "]", "\n", "url", "=", "\"/\"", ".", "join", "(", "[", "prefix", ",", "name", "]", ")", "\n", "return", "url", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalog._get_c2_detectron_baseline": [[73, 93], ["ModelCatalog.C2_DETECTRON_PATH_FORMAT.format", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_c2_detectron_baseline", "(", "name", ")", ":", "\n", "        ", "name", "=", "name", "[", "len", "(", "\"Caffe2Detectron/COCO/\"", ")", ":", "]", "\n", "url", "=", "ModelCatalog", ".", "C2_DETECTRON_MODELS", "[", "name", "]", "\n", "if", "\"keypoint_rcnn\"", "in", "name", ":", "\n", "            ", "dataset", "=", "ModelCatalog", ".", "C2_DATASET_COCO_KEYPOINTS", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "ModelCatalog", ".", "C2_DATASET_COCO", "\n", "\n", "", "if", "\"35998355/rpn_R-50-C4_1x\"", "in", "name", ":", "\n", "# this one model is somehow different from others ..", "\n", "            ", "type", "=", "\"rpn\"", "\n", "", "else", ":", "\n", "            ", "type", "=", "\"generalized_rcnn\"", "\n", "\n", "# Detectron C2 models are stored in the structure defined in `C2_DETECTRON_PATH_FORMAT`.", "\n", "", "url", "=", "ModelCatalog", ".", "C2_DETECTRON_PATH_FORMAT", ".", "format", "(", "\n", "prefix", "=", "ModelCatalog", ".", "S3_C2_DETECTRON_PREFIX", ",", "url", "=", "url", ",", "type", "=", "type", ",", "dataset", "=", "dataset", "\n", ")", "\n", "return", "url", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalogHandler._get_supported_prefixes": [[102, 104], ["None"], "methods", ["None"], ["def", "_get_supported_prefixes", "(", "self", ")", ":", "\n", "        ", "return", "[", "self", ".", "PREFIX", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalogHandler._get_local_path": [[105, 110], ["logging.getLogger", "catalog.ModelCatalog.get", "logging.getLogger.info", "detectron2.utils.file_io.PathManager.get_local_path", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "_get_local_path", "(", "self", ",", "path", ",", "**", "kwargs", ")", ":", "\n", "        ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "catalog_path", "=", "ModelCatalog", ".", "get", "(", "path", "[", "len", "(", "self", ".", "PREFIX", ")", ":", "]", ")", "\n", "logger", ".", "info", "(", "\"Catalog entry {} points to {}\"", ".", "format", "(", "path", ",", "catalog_path", ")", ")", "\n", "return", "PathManager", ".", "get_local_path", "(", "catalog_path", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalogHandler._open": [[111, 113], ["detectron2.utils.file_io.PathManager.open", "catalog.ModelCatalogHandler._get_local_path"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.catalog.ModelCatalogHandler._get_local_path"], ["", "def", "_open", "(", "self", ",", "path", ",", "mode", "=", "\"r\"", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "PathManager", ".", "open", "(", "self", ".", "_get_local_path", "(", "path", ")", ",", "mode", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading.convert_basic_clip_names": [[10, 45], ["copy.deepcopy", "k.replace", "k.replace", "k.replace", "k.replace"], "function", ["None"], ["def", "convert_basic_clip_names", "(", "original_keys", ",", "add_backbone_prefix", "=", "False", ",", "use_whole_clip", "=", "False", ",", "use_fpn_arch", "=", "False", ",", "regionclip", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Apply some basic name conversion to names in CLIP weights.\n    It only deals with typical backbone models.\n\n    Args:\n        original_keys (list[str]):\n    Returns:\n        list[str]: The same number of strings matching those in original_keys.\n    \"\"\"", "\n", "layer_keys", "=", "copy", ".", "deepcopy", "(", "original_keys", ")", "\n", "\n", "vit", "=", "False", "\n", "for", "l_k", "in", "layer_keys", ":", "\n", "        ", "if", "'visual.transformer'", "in", "l_k", ":", "\n", "            ", "vit", "=", "True", "\n", "\n", "# load pretrained oai clip", "\n", "", "", "if", "not", "vit", ":", "# resnet", "\n", "        ", "if", "add_backbone_prefix", ":", "# CLIPRCNN or CLIPFastRCNN", "\n", "            ", "if", "use_whole_clip", ":", "# CLIPRCNN", "\n", "                ", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"visual.\"", ",", "\"clip_backbone.visual.\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "", "else", ":", "# CLIPFastRCNN", "\n", "                ", "if", "use_fpn_arch", ":", "# FPN", "\n", "                    ", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"visual.\"", ",", "\"backbone.bottom_up.\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "", "else", ":", "# C4", "\n", "                    ", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"visual.\"", ",", "\"backbone.\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "", "", "", "else", ":", "# GeneralizedRCNN or ProposalNetwork", "\n", "#layer_keys = [k.replace(\"visual.\", \"backbone.bottom_up.\") for k in layer_keys] #", "\n", "            ", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"visual.\"", ",", "\"\"", ")", "for", "k", "in", "layer_keys", "]", "# ", "\n", "#layer_keys = [k.replace(\"visual.\", \"backbone.visual.\") for k in layer_keys] #", "\n", "", "", "else", ":", "# vit", "\n", "        ", "pass", "\n", "\n", "", "return", "layer_keys", ",", "vit", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading.convert_clip_names": [[47, 186], ["logging.getLogger", "logging.getLogger.info", "sorted", "copy.deepcopy", "clip_model_loading.convert_basic_clip_names", "zip", "weights.keys", "k.replace", "k.replace", "k.replace", "k.replace", "k.replace", "k.replace", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "name.split", "name.startswith", "clip_model_loading.convert_clip_names.fpn_map"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading.convert_basic_clip_names"], ["", "def", "convert_clip_names", "(", "weights", ",", "add_backbone_prefix", "=", "False", ",", "use_whole_clip", "=", "False", ",", "use_fpn_arch", "=", "False", ",", "regionclip", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Map CLIP Detectron weight names to Detectron2 names.\n\n    Args:\n        weights (dict): name -> tensor\n\n    Returns:\n        dict: detectron2 names -> tensor\n        dict: detectron2 names -> C2 names\n    \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"Renaming CLIP weights ......\"", ")", "\n", "original_keys", "=", "sorted", "(", "weights", ".", "keys", "(", ")", ")", "\n", "layer_keys", "=", "copy", ".", "deepcopy", "(", "original_keys", ")", "\n", "\n", "layer_keys", ",", "use_vit", "=", "convert_basic_clip_names", "(", "layer_keys", ",", "add_backbone_prefix", ",", "use_whole_clip", ",", "use_fpn_arch", ",", "regionclip", ")", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# RPN hidden representation conv", "\n", "# --------------------------------------------------------------------------", "\n", "# FPN case", "\n", "# In the C2 model, the RPN hidden layer conv is defined for FPN level 2 and then", "\n", "# shared for all other levels, hence the appearance of \"fpn2\"", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"conv.rpn.fpn2\"", ",", "\"proposal_generator.rpn_head.conv\"", ")", "for", "k", "in", "layer_keys", "\n", "]", "\n", "# Non-FPN case", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"conv.rpn\"", ",", "\"proposal_generator.rpn_head.conv\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# RPN box transformation conv", "\n", "# --------------------------------------------------------------------------", "\n", "# FPN case (see note above about \"fpn2\")", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"rpn.bbox.pred.fpn2\"", ",", "\"proposal_generator.rpn_head.anchor_deltas\"", ")", "\n", "for", "k", "in", "layer_keys", "\n", "]", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"rpn.cls.logits.fpn2\"", ",", "\"proposal_generator.rpn_head.objectness_logits\"", ")", "\n", "for", "k", "in", "layer_keys", "\n", "]", "\n", "# Non-FPN case", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"rpn.bbox.pred\"", ",", "\"proposal_generator.rpn_head.anchor_deltas\"", ")", "for", "k", "in", "layer_keys", "\n", "]", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"rpn.cls.logits\"", ",", "\"proposal_generator.rpn_head.objectness_logits\"", ")", "\n", "for", "k", "in", "layer_keys", "\n", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# Fast R-CNN box head", "\n", "# --------------------------------------------------------------------------", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^bbox\\\\.pred\"", ",", "\"bbox_pred\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^cls\\\\.score\"", ",", "\"cls_score\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^fc6\\\\.\"", ",", "\"box_head.fc1.\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^fc7\\\\.\"", ",", "\"box_head.fc2.\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "# 4conv1fc head tensor names: head_conv1_w, head_conv1_gn_s", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^head\\\\.conv\"", ",", "\"box_head.conv\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# FPN lateral and output convolutions", "\n", "# --------------------------------------------------------------------------", "\n", "def", "fpn_map", "(", "name", ")", ":", "\n", "        ", "\"\"\"\n        Look for keys with the following patterns:\n        1) Starts with \"fpn.inner.\"\n           Example: \"fpn.inner.res2.2.sum.lateral.weight\"\n           Meaning: These are lateral pathway convolutions\n        2) Starts with \"fpn.res\"\n           Example: \"fpn.res2.2.sum.weight\"\n           Meaning: These are FPN output convolutions\n        \"\"\"", "\n", "splits", "=", "name", ".", "split", "(", "\".\"", ")", "\n", "norm", "=", "\".norm\"", "if", "\"norm\"", "in", "splits", "else", "\"\"", "\n", "if", "name", ".", "startswith", "(", "\"fpn.inner.\"", ")", ":", "\n", "# splits example: ['fpn', 'inner', 'res2', '2', 'sum', 'lateral', 'weight']", "\n", "            ", "stage", "=", "int", "(", "splits", "[", "2", "]", "[", "len", "(", "\"res\"", ")", ":", "]", ")", "\n", "return", "\"fpn_lateral{}{}.{}\"", ".", "format", "(", "stage", ",", "norm", ",", "splits", "[", "-", "1", "]", ")", "\n", "", "elif", "name", ".", "startswith", "(", "\"fpn.res\"", ")", ":", "\n", "# splits example: ['fpn', 'res2', '2', 'sum', 'weight']", "\n", "            ", "stage", "=", "int", "(", "splits", "[", "1", "]", "[", "len", "(", "\"res\"", ")", ":", "]", ")", "\n", "return", "\"fpn_output{}{}.{}\"", ".", "format", "(", "stage", ",", "norm", ",", "splits", "[", "-", "1", "]", ")", "\n", "", "return", "name", "\n", "\n", "", "layer_keys", "=", "[", "fpn_map", "(", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# Mask R-CNN mask head", "\n", "# --------------------------------------------------------------------------", "\n", "# roi_heads.StandardROIHeads case", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\".[mask].fcn\"", ",", "\"mask_head.mask_fcn\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^\\\\.mask\\\\.fcn\"", ",", "\"mask_head.mask_fcn\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"mask.fcn.logits\"", ",", "\"mask_head.predictor\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "# roi_heads.Res5ROIHeads case", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"conv5.mask\"", ",", "\"mask_head.deconv\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# Keypoint R-CNN head", "\n", "# --------------------------------------------------------------------------", "\n", "# interestingly, the keypoint head convs have blob names that are simply \"conv_fcnX\"", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"conv.fcn\"", ",", "\"roi_heads.keypoint_head.conv_fcn\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"kps.score.lowres\"", ",", "\"roi_heads.keypoint_head.score_lowres\"", ")", "for", "k", "in", "layer_keys", "\n", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"kps.score.\"", ",", "\"roi_heads.keypoint_head.score.\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# Done with replacements", "\n", "# --------------------------------------------------------------------------", "\n", "assert", "len", "(", "set", "(", "layer_keys", ")", ")", "==", "len", "(", "layer_keys", ")", "\n", "assert", "len", "(", "original_keys", ")", "==", "len", "(", "layer_keys", ")", "\n", "\n", "new_weights", "=", "{", "}", "\n", "new_keys_to_original_keys", "=", "{", "}", "\n", "for", "orig", ",", "renamed", "in", "zip", "(", "original_keys", ",", "layer_keys", ")", ":", "\n", "        ", "new_keys_to_original_keys", "[", "renamed", "]", "=", "orig", "\n", "if", "renamed", ".", "startswith", "(", "\"bbox_pred.\"", ")", "or", "renamed", ".", "startswith", "(", "\"mask_head.predictor.\"", ")", ":", "\n", "# remove the meaningless prediction weight for background class", "\n", "            ", "new_start_idx", "=", "4", "if", "renamed", ".", "startswith", "(", "\"bbox_pred.\"", ")", "else", "1", "\n", "new_weights", "[", "renamed", "]", "=", "weights", "[", "orig", "]", "[", "new_start_idx", ":", "]", "\n", "logger", ".", "info", "(", "\n", "\"Remove prediction weight for background class in {}. The shape changes from \"", "\n", "\"{} to {}.\"", ".", "format", "(", "\n", "renamed", ",", "tuple", "(", "weights", "[", "orig", "]", ".", "shape", ")", ",", "tuple", "(", "new_weights", "[", "renamed", "]", ".", "shape", ")", "\n", ")", "\n", ")", "\n", "", "elif", "renamed", ".", "startswith", "(", "\"cls_score.\"", ")", ":", "\n", "# move weights of bg class from original index 0 to last index", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Move classification weights for background class in {} from index 0 to \"", "\n", "\"index {}.\"", ".", "format", "(", "renamed", ",", "weights", "[", "orig", "]", ".", "shape", "[", "0", "]", "-", "1", ")", "\n", ")", "\n", "new_weights", "[", "renamed", "]", "=", "torch", ".", "cat", "(", "[", "weights", "[", "orig", "]", "[", "1", ":", "]", ",", "weights", "[", "orig", "]", "[", ":", "1", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "new_weights", "[", "renamed", "]", "=", "weights", "[", "orig", "]", "\n", "\n", "", "", "return", "new_weights", ",", "new_keys_to_original_keys", ",", "use_vit", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading.align_and_update_state_dicts_for_CLIP": [[190, 343], ["sorted", "clip_model_loading.convert_clip_names", "sorted", "torch.as_tensor().view", "torch.as_tensor().view.max", "logging.getLogger", "enumerate", "sorted", "clip_model_loading._longest_common_prefix", "clip_model_loading._group_keys_by_module", "set", "tabulate.tabulate", "model_state_dict.keys", "torch.tensor", "ckpt_state_dict.keys", "len", "len", "idxs.tolist", "matched_keys.values", "len", "logging.getLogger.warning", "logging.getLogger.info", "logging.getLogger.info", "a.endswith", "clip_model_loading.align_and_update_state_dicts_for_CLIP.match"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading.convert_clip_names", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._longest_common_prefix", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._group_keys_by_module", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "align_and_update_state_dicts_for_CLIP", "(", "model_state_dict", ",", "ckpt_state_dict", ",", "c2_conversion", "=", "True", ",", "bb_rpn_weights", "=", "False", ",", "regionclip", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Extended from ./c2_model_loading.py\n    Match names between the two state-dict, and returns a new chkpt_state_dict with names\n    converted to match model_state_dict with heuristics. The returned dict can be later\n    loaded with fvcore checkpointer.\n    If `c2_conversion==True`, `ckpt_state_dict` is assumed to be a Caffe2\n    model and will be renamed at first.\n\n    Strategy: suppose that the models that we will create will have prefixes appended\n    to each of its keys, for example due to an extra level of nesting that the original\n    pre-trained weights from ImageNet won't contain. For example, model.state_dict()\n    might return backbone[0].body.res2.conv1.weight, while the pre-trained model contains\n    res2.conv1.weight. We thus want to match both parameters together.\n    For that, we look for each model weight, look among all loaded keys if there is one\n    that is a suffix of the current weight name, and use it if that's the case.\n    If multiple matches exist, take the one with longest size\n    of the corresponding name. For example, for the same model as before, the pretrained\n    weight file can contain both res2.conv1.weight, as well as conv1.weight. In this case,\n    we want to match backbone[0].body.conv1.weight to conv1.weight, and\n    backbone[0].body.res2.conv1.weight to res2.conv1.weight.\n    \"\"\"", "\n", "model_keys", "=", "sorted", "(", "model_state_dict", ".", "keys", "(", ")", ")", "\n", "use_whole_clip", "=", "False", "# whether use the whole clip (text & visual encoders), typically in CLIPRCNN meta arch", "\n", "add_backbone_prefix", "=", "False", "# convert to 'backbone.' prefix, typically in CLIPFastRCNN meta arch", "\n", "use_fpn_arch", "=", "False", "# if use FPN arch then convert to `bottom_up`, typically in CLIPFastRCNN meta arch with FPN backbone", "\n", "if", "bb_rpn_weights", ":", "# a 2nd pretrained weights to load, for offline backbone & RPN, then convert the ckpt key names and only keep the ones we need", "\n", "        ", "new_ckpt_state_dict", "=", "{", "}", "\n", "for", "original_k", "in", "ckpt_state_dict", ":", "\n", "            ", "if", "'backbone'", "in", "original_k", ":", "\n", "                ", "new_key", "=", "original_k", ".", "replace", "(", "'backbone'", ",", "'offline_backbone'", ")", "\n", "new_ckpt_state_dict", "[", "new_key", "]", "=", "ckpt_state_dict", "[", "original_k", "]", "\n", "", "if", "'proposal_generator'", "in", "original_k", ":", "\n", "                ", "new_key", "=", "original_k", ".", "replace", "(", "'proposal_generator'", ",", "'offline_proposal_generator'", ")", "\n", "new_ckpt_state_dict", "[", "new_key", "]", "=", "ckpt_state_dict", "[", "original_k", "]", "\n", "", "", "new_ckpt_state_dict", "[", "'ignore_others'", "]", "=", "torch", ".", "tensor", "(", "[", "1", "]", ")", "# ignore other model weights (not 'offline_*') in batch_norm.py", "\n", "ckpt_state_dict", "=", "new_ckpt_state_dict", "\n", "", "else", ":", "# the 1st pretrained weigths to load", "\n", "        ", "for", "model_key", "in", "model_keys", ":", "# if use the whole clip, then convert ckpt 'visual.' names to 'clip_backbone.visual.'", "\n", "            ", "if", "'clip_backbone'", "in", "model_key", ":", "\n", "                ", "use_whole_clip", "=", "True", "\n", "", "", "for", "model_key", "in", "model_keys", ":", "# if there are backbone & offline_backbone, then convert the ckpt 'visual.' names to 'backbone.' to avoid ambiguity", "\n", "            ", "if", "'offline_backbone'", "in", "model_key", ":", "\n", "                ", "add_backbone_prefix", "=", "True", "\n", "", "if", "'fpn'", "in", "model_key", ":", "\n", "                ", "use_fpn_arch", "=", "True", "\n", "# original_keys: the name in the original dict (before renaming)", "\n", "", "", "", "ckpt_state_dict", ",", "original_keys", ",", "use_vit", "=", "convert_clip_names", "(", "ckpt_state_dict", ",", "add_backbone_prefix", ",", "use_whole_clip", ",", "use_fpn_arch", ",", "regionclip", ")", "\n", "ckpt_keys", "=", "sorted", "(", "ckpt_state_dict", ".", "keys", "(", ")", ")", "\n", "\n", "def", "match", "(", "a", ",", "b", ")", ":", "\n", "# Matched ckpt_key should be a complete (starts with '.') suffix.", "\n", "# For example, roi_heads.mesh_head.whatever_conv1 does not match conv1,", "\n", "# but matches whatever_conv1 or mesh_head.whatever_conv1.", "\n", "        ", "return", "a", "==", "b", "or", "a", ".", "endswith", "(", "\".\"", "+", "b", ")", "\n", "\n", "# get a matrix of string matches, where each (i, j) entry correspond to the size of the", "\n", "# ckpt_key string, if it matches", "\n", "", "match_matrix", "=", "[", "len", "(", "j", ")", "if", "match", "(", "i", ",", "j", ")", "else", "0", "for", "i", "in", "model_keys", "for", "j", "in", "ckpt_keys", "]", "\n", "match_matrix", "=", "torch", ".", "as_tensor", "(", "match_matrix", ")", ".", "view", "(", "len", "(", "model_keys", ")", ",", "len", "(", "ckpt_keys", ")", ")", "\n", "# use the matched one with longest size in case of multiple matches", "\n", "max_match_size", ",", "idxs", "=", "match_matrix", ".", "max", "(", "1", ")", "\n", "# remove indices that correspond to no-match", "\n", "idxs", "[", "max_match_size", "==", "0", "]", "=", "-", "1", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "# matched_pairs (matched checkpoint key --> matched model key)", "\n", "matched_keys", "=", "{", "}", "\n", "result_state_dict", "=", "{", "}", "\n", "for", "idx_model", ",", "idx_ckpt", "in", "enumerate", "(", "idxs", ".", "tolist", "(", ")", ")", ":", "\n", "        ", "if", "idx_ckpt", "==", "-", "1", ":", "\n", "            ", "continue", "\n", "", "key_model", "=", "model_keys", "[", "idx_model", "]", "\n", "key_ckpt", "=", "ckpt_keys", "[", "idx_ckpt", "]", "\n", "value_ckpt", "=", "ckpt_state_dict", "[", "key_ckpt", "]", "\n", "shape_in_model", "=", "model_state_dict", "[", "key_model", "]", ".", "shape", "\n", "\n", "if", "shape_in_model", "!=", "value_ckpt", ".", "shape", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Shape of {} in checkpoint is {}, while shape of {} in model is {}.\"", ".", "format", "(", "\n", "key_ckpt", ",", "value_ckpt", ".", "shape", ",", "key_model", ",", "shape_in_model", "\n", ")", "\n", ")", "\n", "logger", ".", "warning", "(", "\n", "\"{} will not be loaded. Please double check and see if this is desired.\"", ".", "format", "(", "\n", "key_ckpt", "\n", ")", "\n", ")", "\n", "continue", "\n", "\n", "", "assert", "key_model", "not", "in", "result_state_dict", "\n", "result_state_dict", "[", "key_model", "]", "=", "value_ckpt", "\n", "if", "key_ckpt", "in", "matched_keys", ":", "# already added to matched_keys", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Ambiguity found for {} in checkpoint!\"", "\n", "\"It matches at least two keys in the model ({} and {}).\"", ".", "format", "(", "\n", "key_ckpt", ",", "key_model", ",", "matched_keys", "[", "key_ckpt", "]", "\n", ")", "\n", ")", "\n", "raise", "ValueError", "(", "\"Cannot match one checkpoint key to multiple keys in the model.\"", ")", "\n", "\n", "", "matched_keys", "[", "key_ckpt", "]", "=", "key_model", "\n", "\n", "# logging:", "\n", "", "matched_model_keys", "=", "sorted", "(", "matched_keys", ".", "values", "(", ")", ")", "\n", "mmk_list", "=", "\"The following model parameters are loaded from checkpoints:\\n\"", "\n", "for", "mmk", "in", "matched_model_keys", ":", "\n", "        ", "mmk_list", "+=", "mmk", "+", "\"\\n\"", "\n", "", "if", "len", "(", "matched_model_keys", ")", "==", "0", ":", "\n", "        ", "logger", ".", "warning", "(", "\"No weights in checkpoint matched with model.\"", ")", "\n", "return", "ckpt_state_dict", "\n", "", "common_prefix", "=", "_longest_common_prefix", "(", "matched_model_keys", ")", "\n", "rev_matched_keys", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "matched_keys", ".", "items", "(", ")", "}", "\n", "original_keys", "=", "{", "k", ":", "original_keys", "[", "rev_matched_keys", "[", "k", "]", "]", "for", "k", "in", "matched_model_keys", "}", "\n", "\n", "model_key_groups", "=", "_group_keys_by_module", "(", "matched_model_keys", ",", "original_keys", ")", "\n", "table", "=", "[", "]", "\n", "memo", "=", "set", "(", ")", "\n", "for", "key_model", "in", "matched_model_keys", ":", "\n", "        ", "if", "key_model", "in", "memo", ":", "\n", "            ", "continue", "\n", "", "if", "key_model", "in", "model_key_groups", ":", "\n", "            ", "group", "=", "model_key_groups", "[", "key_model", "]", "\n", "memo", "|=", "set", "(", "group", ")", "\n", "shapes", "=", "[", "tuple", "(", "model_state_dict", "[", "k", "]", ".", "shape", ")", "for", "k", "in", "group", "]", "\n", "table", ".", "append", "(", "\n", "(", "\n", "_longest_common_prefix", "(", "[", "k", "[", "len", "(", "common_prefix", ")", ":", "]", "for", "k", "in", "group", "]", ")", "+", "\"*\"", ",", "\n", "_group_str", "(", "[", "original_keys", "[", "k", "]", "for", "k", "in", "group", "]", ")", ",", "\n", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "for", "x", "in", "shapes", "]", ")", ",", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "key_checkpoint", "=", "original_keys", "[", "key_model", "]", "\n", "shape", "=", "str", "(", "tuple", "(", "model_state_dict", "[", "key_model", "]", ".", "shape", ")", ")", "\n", "table", ".", "append", "(", "(", "key_model", "[", "len", "(", "common_prefix", ")", ":", "]", ",", "key_checkpoint", ",", "shape", ")", ")", "\n", "", "", "table_str", "=", "tabulate", "(", "\n", "table", ",", "tablefmt", "=", "\"pipe\"", ",", "headers", "=", "[", "\"Names in Model\"", ",", "\"Names in Checkpoint\"", ",", "\"Shapes\"", "]", "\n", ")", "\n", "if", "len", "(", "table", ")", "!=", "1", "and", "not", "use_vit", ":", "# do this for now; the table function has some bugs when the whole CLIP is loaded", "\n", "        ", "logger", ".", "info", "(", "\n", "\"Following weights matched with \"", "\n", "+", "(", "f\"submodule {common_prefix[:-1]}\"", "if", "common_prefix", "else", "\"model\"", ")", "\n", "+", "\":\\n\"", "\n", "+", "table_str", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "mmk_list", ")", "\n", "\n", "", "unmatched_ckpt_keys", "=", "[", "k", "for", "k", "in", "ckpt_keys", "if", "k", "not", "in", "set", "(", "matched_keys", ".", "keys", "(", ")", ")", "]", "\n", "for", "k", "in", "unmatched_ckpt_keys", ":", "\n", "        ", "result_state_dict", "[", "k", "]", "=", "ckpt_state_dict", "[", "k", "]", "\n", "", "return", "result_state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading._group_keys_by_module": [[345, 383], ["sorted", "key.rfind", "clip_model_loading._group_keys_by_module._submodule_name"], "function", ["None"], ["", "def", "_group_keys_by_module", "(", "keys", ":", "List", "[", "str", "]", ",", "original_names", ":", "Dict", "[", "str", ",", "str", "]", ")", ":", "\n", "    ", "\"\"\"\n    Params in the same submodule are grouped together.\n\n    Args:\n        keys: names of all parameters\n        original_names: mapping from parameter name to their name in the checkpoint\n\n    Returns:\n        dict[name -> all other names in the same group]\n    \"\"\"", "\n", "\n", "def", "_submodule_name", "(", "key", ")", ":", "\n", "        ", "pos", "=", "key", ".", "rfind", "(", "\".\"", ")", "\n", "if", "pos", "<", "0", ":", "\n", "            ", "return", "None", "\n", "", "prefix", "=", "key", "[", ":", "pos", "+", "1", "]", "\n", "return", "prefix", "\n", "\n", "", "all_submodules", "=", "[", "_submodule_name", "(", "k", ")", "for", "k", "in", "keys", "]", "\n", "all_submodules", "=", "[", "x", "for", "x", "in", "all_submodules", "if", "x", "]", "\n", "all_submodules", "=", "sorted", "(", "all_submodules", ",", "key", "=", "len", ")", "\n", "\n", "ret", "=", "{", "}", "\n", "for", "prefix", "in", "all_submodules", ":", "\n", "        ", "group", "=", "[", "k", "for", "k", "in", "keys", "if", "k", ".", "startswith", "(", "prefix", ")", "]", "\n", "if", "len", "(", "group", ")", "<=", "1", ":", "\n", "            ", "continue", "\n", "", "original_name_lcp", "=", "_longest_common_prefix_str", "(", "[", "original_names", "[", "k", "]", "for", "k", "in", "group", "]", ")", "\n", "if", "len", "(", "original_name_lcp", ")", "==", "0", ":", "\n", "# don't group weights if original names don't share prefix", "\n", "            ", "continue", "\n", "\n", "", "for", "k", "in", "group", ":", "\n", "            ", "if", "k", "in", "ret", ":", "\n", "                ", "continue", "\n", "", "ret", "[", "k", "]", "=", "group", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading._longest_common_prefix": [[385, 394], ["n.split", "min", "max", "len", "zip"], "function", ["None"], ["", "def", "_longest_common_prefix", "(", "names", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    [\"abc.zfg\", \"abc.zef\"] -> \"abc.\"\n    \"\"\"", "\n", "names", "=", "[", "n", ".", "split", "(", "\".\"", ")", "for", "n", "in", "names", "]", "\n", "m1", ",", "m2", "=", "min", "(", "names", ")", ",", "max", "(", "names", ")", "\n", "ret", "=", "[", "a", "for", "a", ",", "b", "in", "zip", "(", "m1", ",", "m2", ")", "if", "a", "==", "b", "]", "\n", "ret", "=", "\".\"", ".", "join", "(", "ret", ")", "+", "\".\"", "if", "len", "(", "ret", ")", "else", "\"\"", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading._longest_common_prefix_str": [[396, 401], ["min", "max", "zip"], "function", ["None"], ["", "def", "_longest_common_prefix_str", "(", "names", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "m1", ",", "m2", "=", "min", "(", "names", ")", ",", "max", "(", "names", ")", "\n", "lcp", "=", "[", "a", "for", "a", ",", "b", "in", "zip", "(", "m1", ",", "m2", ")", "if", "a", "==", "b", "]", "\n", "lcp", "=", "\"\"", ".", "join", "(", "lcp", ")", "\n", "return", "lcp", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading._group_str": [[403, 416], ["clip_model_loading._longest_common_prefix_str", "ret.replace.replace", "ret.replace.replace", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._longest_common_prefix_str"], ["", "def", "_group_str", "(", "names", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Turn \"common1\", \"common2\", \"common3\" into \"common{1,2,3}\"\n    \"\"\"", "\n", "lcp", "=", "_longest_common_prefix_str", "(", "names", ")", "\n", "rest", "=", "[", "x", "[", "len", "(", "lcp", ")", ":", "]", "for", "x", "in", "names", "]", "\n", "rest", "=", "\"{\"", "+", "\",\"", ".", "join", "(", "rest", ")", "+", "\"}\"", "\n", "ret", "=", "lcp", "+", "rest", "\n", "\n", "# add some simplification for BN specifically", "\n", "ret", "=", "ret", ".", "replace", "(", "\"bn_{beta,running_mean,running_var,gamma}\"", ",", "\"bn_*\"", ")", "\n", "ret", "=", "ret", ".", "replace", "(", "\"bn_beta,bn_running_mean,bn_running_var,bn_gamma\"", ",", "\"bn_*\"", ")", "\n", "return", "ret", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.__init__": [[23, 33], ["detectron2.is_main_process", "fvcore.common.checkpoint.Checkpointer.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "save_dir", "=", "\"\"", ",", "*", ",", "save_to_disk", "=", "None", ",", "bb_rpn_weights", "=", "False", ",", "**", "checkpointables", ")", ":", "\n", "        ", "is_main_process", "=", "comm", ".", "is_main_process", "(", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "model", ",", "\n", "save_dir", ",", "\n", "save_to_disk", "=", "is_main_process", "if", "save_to_disk", "is", "None", "else", "save_to_disk", ",", "\n", "**", "checkpointables", ",", "\n", ")", "\n", "self", ".", "path_manager", "=", "PathManager", "\n", "self", ".", "bb_rpn_weights", "=", "bb_rpn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load": [[34, 61], ["super().load", "isinstance", "logging.getLogger", "detection_checkpoint.DetectionCheckpointer.path_manager.get_local_path", "os.path.isfile", "detectron2.all_gather", "logging.getLogger.info", "OSError", "all", "logging.getLogger.warning", "detection_checkpoint.DetectionCheckpointer.model._sync_params_and_buffers"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather"], ["", "def", "load", "(", "self", ",", "path", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "need_sync", "=", "False", "\n", "\n", "if", "path", "and", "isinstance", "(", "self", ".", "model", ",", "DistributedDataParallel", ")", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "path", "=", "self", ".", "path_manager", ".", "get_local_path", "(", "path", ")", "\n", "has_file", "=", "os", ".", "path", ".", "isfile", "(", "path", ")", "\n", "all_has_file", "=", "comm", ".", "all_gather", "(", "has_file", ")", "\n", "if", "not", "all_has_file", "[", "0", "]", ":", "\n", "                ", "raise", "OSError", "(", "f\"File {path} not found on main worker.\"", ")", "\n", "", "if", "not", "all", "(", "all_has_file", ")", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "f\"Not all workers can read checkpoint {path}. \"", "\n", "\"Training may fail to fully resume.\"", "\n", ")", "\n", "# TODO: broadcast the checkpoint file contents from main", "\n", "# worker, and load from it instead.", "\n", "need_sync", "=", "True", "\n", "", "if", "not", "has_file", ":", "\n", "                ", "path", "=", "None", "# don't load if not readable", "\n", "", "", "ret", "=", "super", "(", ")", ".", "load", "(", "path", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "if", "need_sync", ":", "\n", "            ", "logger", ".", "info", "(", "\"Broadcasting model states from main worker ...\"", ")", "\n", "if", "TORCH_VERSION", ">=", "(", "1", ",", "7", ")", ":", "\n", "                ", "self", ".", "model", ".", "_sync_params_and_buffers", "(", ")", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer._load_file": [[62, 101], ["filename.endswith", "super()._load_file", "filename.endswith", "detectron2.utils.file_io.PathManager.open", "pickle.load", "detection_checkpoint.DetectionCheckpointer.logger.info", "detectron2.utils.file_io.PathManager.open", "torch.load", "super()._load_file", "torch.load.items", "data[].items", "k.endswith", "k.endswith"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer._load_file", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer._load_file"], ["", "def", "_load_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "if", "filename", ".", "endswith", "(", "\".pkl\"", ")", ":", "\n", "            ", "with", "PathManager", ".", "open", "(", "filename", ",", "\"rb\"", ")", "as", "f", ":", "\n", "                ", "data", "=", "pickle", ".", "load", "(", "f", ",", "encoding", "=", "\"latin1\"", ")", "\n", "", "if", "\"model\"", "in", "data", "and", "\"__author__\"", "in", "data", ":", "\n", "# file is in Detectron2 model zoo format", "\n", "                ", "self", ".", "logger", ".", "info", "(", "\"Reading a file from '{}'\"", ".", "format", "(", "data", "[", "\"__author__\"", "]", ")", ")", "\n", "return", "data", "\n", "", "else", ":", "\n", "# assume file is from Caffe2 / Detectron1 model zoo", "\n", "                ", "if", "\"blobs\"", "in", "data", ":", "\n", "# Detection models have \"blobs\", but ImageNet models don't", "\n", "                    ", "data", "=", "data", "[", "\"blobs\"", "]", "\n", "", "data", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "data", ".", "items", "(", ")", "if", "not", "k", ".", "endswith", "(", "\"_momentum\"", ")", "}", "\n", "return", "{", "\"model\"", ":", "data", ",", "\"__author__\"", ":", "\"Caffe2\"", ",", "\"matching_heuristics\"", ":", "True", "}", "\n", "", "", "elif", "filename", ".", "endswith", "(", "\".pyth\"", ")", ":", "\n", "# assume file is from pycls; no one else seems to use the \".pyth\" extension", "\n", "            ", "with", "PathManager", ".", "open", "(", "filename", ",", "\"rb\"", ")", "as", "f", ":", "\n", "                ", "data", "=", "torch", ".", "load", "(", "f", ")", "\n", "", "assert", "(", "\n", "\"model_state\"", "in", "data", "\n", ")", ",", "f\"Cannot load .pyth file {filename}; pycls checkpoints must contain 'model_state'.\"", "\n", "model_state", "=", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "data", "[", "\"model_state\"", "]", ".", "items", "(", ")", "\n", "if", "not", "k", ".", "endswith", "(", "\"num_batches_tracked\"", ")", "\n", "}", "\n", "return", "{", "\"model\"", ":", "model_state", ",", "\"__author__\"", ":", "\"pycls\"", ",", "\"matching_heuristics\"", ":", "True", "}", "\n", "", "elif", "\"OAI_CLIP\"", "in", "filename", ":", "\n", "# assume file is from OpenAI CLIP pre-trained model", "\n", "            ", "loaded", "=", "super", "(", ")", ".", "_load_file", "(", "filename", ")", "# load native pth checkpoint", "\n", "if", "\"model\"", "not", "in", "loaded", ":", "\n", "                ", "loaded", "=", "{", "\"model\"", ":", "loaded", "}", "\n", "", "return", "{", "\"model\"", ":", "loaded", "[", "\"model\"", "]", ",", "\"__author__\"", ":", "\"OAI_CLIP\"", ",", "\"matching_heuristics\"", ":", "True", "}", "\n", "\n", "", "loaded", "=", "super", "(", ")", ".", "_load_file", "(", "filename", ")", "# load native pth checkpoint", "\n", "if", "\"model\"", "not", "in", "loaded", ":", "\n", "            ", "loaded", "=", "{", "\"model\"", ":", "loaded", "}", "\n", "", "return", "loaded", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer._load_model": [[102, 133], ["super()._load_model", "dict", "checkpoint.get", "detection_checkpoint.DetectionCheckpointer._convert_ndarray_to_tensor", "detection_checkpoint.DetectionCheckpointer.model.named_buffers", "clip_model_loading.align_and_update_state_dicts_for_CLIP", "c2_model_loading.align_and_update_state_dicts", "checkpoint.get", "detection_checkpoint.DetectionCheckpointer.model.state_dict", "detection_checkpoint.DetectionCheckpointer.model.state_dict", "super()._load_model.missing_keys.remove", "checkpoint.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer._load_model", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.clip_model_loading.align_and_update_state_dicts_for_CLIP", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading.align_and_update_state_dicts", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.remove", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "_load_model", "(", "self", ",", "checkpoint", ")", ":", "\n", "        ", "if", "checkpoint", ".", "get", "(", "\"matching_heuristics\"", ",", "False", ")", "or", "self", ".", "bb_rpn_weights", ":", "\n", "            ", "self", ".", "_convert_ndarray_to_tensor", "(", "checkpoint", "[", "\"model\"", "]", ")", "\n", "# convert weights by name-matching heuristics", "\n", "if", "checkpoint", ".", "get", "(", "\"__author__\"", ",", "\"NA\"", ")", "==", "\"OAI_CLIP\"", "or", "self", ".", "bb_rpn_weights", ":", "# for OAI_CLIP or 2nd ckpt (offline modules)", "\n", "                ", "checkpoint", "[", "\"model\"", "]", "=", "align_and_update_state_dicts_for_CLIP", "(", "\n", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "checkpoint", "[", "\"model\"", "]", ",", "\n", "bb_rpn_weights", "=", "self", ".", "bb_rpn_weights", ",", "\n", ")", "\n", "", "else", ":", "# default loading", "\n", "                ", "checkpoint", "[", "\"model\"", "]", "=", "align_and_update_state_dicts", "(", "\n", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "checkpoint", "[", "\"model\"", "]", ",", "\n", "c2_conversion", "=", "checkpoint", ".", "get", "(", "\"__author__\"", ",", "None", ")", "==", "\"Caffe2\"", ",", "\n", ")", "\n", "# for non-caffe2 models, use standard ways to load it", "\n", "", "", "incompatible", "=", "super", "(", ")", ".", "_load_model", "(", "checkpoint", ")", "\n", "del", "checkpoint", "# try saving memory", "\n", "\n", "model_buffers", "=", "dict", "(", "self", ".", "model", ".", "named_buffers", "(", "recurse", "=", "False", ")", ")", "\n", "for", "k", "in", "[", "\"pixel_mean\"", ",", "\"pixel_std\"", "]", ":", "\n", "# Ignore missing key message about pixel_mean/std.", "\n", "# Though they may be missing in old checkpoints, they will be correctly", "\n", "# initialized from config anyway.", "\n", "            ", "if", "k", "in", "model_buffers", ":", "\n", "                ", "try", ":", "\n", "                    ", "incompatible", ".", "missing_keys", ".", "remove", "(", "k", ")", "\n", "", "except", "ValueError", ":", "\n", "                    ", "pass", "\n", "", "", "", "return", "incompatible", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading.convert_basic_c2_names": [[10, 64], ["copy.deepcopy", "k.replace", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "k.replace", "k.replace", "k.replace", "k.replace", "re.sub", "k.replace", "k.replace", "k.replace", "k.replace"], "function", ["None"], ["def", "convert_basic_c2_names", "(", "original_keys", ")", ":", "\n", "    ", "\"\"\"\n    Apply some basic name conversion to names in C2 weights.\n    It only deals with typical backbone models.\n\n    Args:\n        original_keys (list[str]):\n    Returns:\n        list[str]: The same number of strings matching those in original_keys.\n    \"\"\"", "\n", "layer_keys", "=", "copy", ".", "deepcopy", "(", "original_keys", ")", "\n", "layer_keys", "=", "[", "\n", "{", "\"pred_b\"", ":", "\"linear_b\"", ",", "\"pred_w\"", ":", "\"linear_w\"", "}", ".", "get", "(", "k", ",", "k", ")", "for", "k", "in", "layer_keys", "\n", "]", "# some hard-coded mappings", "\n", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"_\"", ",", "\".\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"\\\\.b$\"", ",", "\".bias\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"\\\\.w$\"", ",", "\".weight\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "# Uniform both bn and gn names to \"norm\"", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"bn\\\\.s$\"", ",", "\"norm.weight\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"bn\\\\.bias$\"", ",", "\"norm.bias\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"bn\\\\.rm\"", ",", "\"norm.running_mean\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"bn\\\\.running.mean$\"", ",", "\"norm.running_mean\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"bn\\\\.riv$\"", ",", "\"norm.running_var\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"bn\\\\.running.var$\"", ",", "\"norm.running_var\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"bn\\\\.gamma$\"", ",", "\"norm.weight\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"bn\\\\.beta$\"", ",", "\"norm.bias\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"gn\\\\.s$\"", ",", "\"norm.weight\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"gn\\\\.bias$\"", ",", "\"norm.bias\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# stem", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^res\\\\.conv1\\\\.norm\\\\.\"", ",", "\"conv1.norm.\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "# to avoid mis-matching with \"conv1\" in other components (e.g. detection head)", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^conv1\\\\.\"", ",", "\"stem.conv1.\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# layer1-4 is used by torchvision, however we follow the C2 naming strategy (res2-5)", "\n", "# layer_keys = [re.sub(\"^res2.\", \"layer1.\", k) for k in layer_keys]", "\n", "# layer_keys = [re.sub(\"^res3.\", \"layer2.\", k) for k in layer_keys]", "\n", "# layer_keys = [re.sub(\"^res4.\", \"layer3.\", k) for k in layer_keys]", "\n", "# layer_keys = [re.sub(\"^res5.\", \"layer4.\", k) for k in layer_keys]", "\n", "\n", "# blocks", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\".branch1.\"", ",", "\".shortcut.\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\".branch2a.\"", ",", "\".conv1.\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\".branch2b.\"", ",", "\".conv2.\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\".branch2c.\"", ",", "\".conv3.\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# DensePose substitutions", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^body.conv.fcn\"", ",", "\"body_conv_fcn\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"AnnIndex.lowres\"", ",", "\"ann_index_lowres\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"Index.UV.lowres\"", ",", "\"index_uv_lowres\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"U.lowres\"", ",", "\"u_lowres\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"V.lowres\"", ",", "\"v_lowres\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "return", "layer_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading.convert_c2_detectron_names": [[66, 205], ["logging.getLogger", "logging.getLogger.info", "sorted", "copy.deepcopy", "c2_model_loading.convert_basic_c2_names", "zip", "weights.keys", "k.replace", "k.replace", "k.replace", "k.replace", "k.replace", "k.replace", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "name.split", "name.startswith", "c2_model_loading.convert_c2_detectron_names.fpn_map"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading.convert_basic_c2_names"], ["", "def", "convert_c2_detectron_names", "(", "weights", ")", ":", "\n", "    ", "\"\"\"\n    Map Caffe2 Detectron weight names to Detectron2 names.\n\n    Args:\n        weights (dict): name -> tensor\n\n    Returns:\n        dict: detectron2 names -> tensor\n        dict: detectron2 names -> C2 names\n    \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"Renaming Caffe2 weights ......\"", ")", "\n", "original_keys", "=", "sorted", "(", "weights", ".", "keys", "(", ")", ")", "\n", "layer_keys", "=", "copy", ".", "deepcopy", "(", "original_keys", ")", "\n", "\n", "layer_keys", "=", "convert_basic_c2_names", "(", "layer_keys", ")", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# RPN hidden representation conv", "\n", "# --------------------------------------------------------------------------", "\n", "# FPN case", "\n", "# In the C2 model, the RPN hidden layer conv is defined for FPN level 2 and then", "\n", "# shared for all other levels, hence the appearance of \"fpn2\"", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"conv.rpn.fpn2\"", ",", "\"proposal_generator.rpn_head.conv\"", ")", "for", "k", "in", "layer_keys", "\n", "]", "\n", "# Non-FPN case", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"conv.rpn\"", ",", "\"proposal_generator.rpn_head.conv\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# RPN box transformation conv", "\n", "# --------------------------------------------------------------------------", "\n", "# FPN case (see note above about \"fpn2\")", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"rpn.bbox.pred.fpn2\"", ",", "\"proposal_generator.rpn_head.anchor_deltas\"", ")", "\n", "for", "k", "in", "layer_keys", "\n", "]", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"rpn.cls.logits.fpn2\"", ",", "\"proposal_generator.rpn_head.objectness_logits\"", ")", "\n", "for", "k", "in", "layer_keys", "\n", "]", "\n", "# Non-FPN case", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"rpn.bbox.pred\"", ",", "\"proposal_generator.rpn_head.anchor_deltas\"", ")", "for", "k", "in", "layer_keys", "\n", "]", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"rpn.cls.logits\"", ",", "\"proposal_generator.rpn_head.objectness_logits\"", ")", "\n", "for", "k", "in", "layer_keys", "\n", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# Fast R-CNN box head", "\n", "# --------------------------------------------------------------------------", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^bbox\\\\.pred\"", ",", "\"bbox_pred\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^cls\\\\.score\"", ",", "\"cls_score\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^fc6\\\\.\"", ",", "\"box_head.fc1.\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^fc7\\\\.\"", ",", "\"box_head.fc2.\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "# 4conv1fc head tensor names: head_conv1_w, head_conv1_gn_s", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^head\\\\.conv\"", ",", "\"box_head.conv\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# FPN lateral and output convolutions", "\n", "# --------------------------------------------------------------------------", "\n", "def", "fpn_map", "(", "name", ")", ":", "\n", "        ", "\"\"\"\n        Look for keys with the following patterns:\n        1) Starts with \"fpn.inner.\"\n           Example: \"fpn.inner.res2.2.sum.lateral.weight\"\n           Meaning: These are lateral pathway convolutions\n        2) Starts with \"fpn.res\"\n           Example: \"fpn.res2.2.sum.weight\"\n           Meaning: These are FPN output convolutions\n        \"\"\"", "\n", "splits", "=", "name", ".", "split", "(", "\".\"", ")", "\n", "norm", "=", "\".norm\"", "if", "\"norm\"", "in", "splits", "else", "\"\"", "\n", "if", "name", ".", "startswith", "(", "\"fpn.inner.\"", ")", ":", "\n", "# splits example: ['fpn', 'inner', 'res2', '2', 'sum', 'lateral', 'weight']", "\n", "            ", "stage", "=", "int", "(", "splits", "[", "2", "]", "[", "len", "(", "\"res\"", ")", ":", "]", ")", "\n", "return", "\"fpn_lateral{}{}.{}\"", ".", "format", "(", "stage", ",", "norm", ",", "splits", "[", "-", "1", "]", ")", "\n", "", "elif", "name", ".", "startswith", "(", "\"fpn.res\"", ")", ":", "\n", "# splits example: ['fpn', 'res2', '2', 'sum', 'weight']", "\n", "            ", "stage", "=", "int", "(", "splits", "[", "1", "]", "[", "len", "(", "\"res\"", ")", ":", "]", ")", "\n", "return", "\"fpn_output{}{}.{}\"", ".", "format", "(", "stage", ",", "norm", ",", "splits", "[", "-", "1", "]", ")", "\n", "", "return", "name", "\n", "\n", "", "layer_keys", "=", "[", "fpn_map", "(", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# Mask R-CNN mask head", "\n", "# --------------------------------------------------------------------------", "\n", "# roi_heads.StandardROIHeads case", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\".[mask].fcn\"", ",", "\"mask_head.mask_fcn\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "re", ".", "sub", "(", "\"^\\\\.mask\\\\.fcn\"", ",", "\"mask_head.mask_fcn\"", ",", "k", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"mask.fcn.logits\"", ",", "\"mask_head.predictor\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "# roi_heads.Res5ROIHeads case", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"conv5.mask\"", ",", "\"mask_head.deconv\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# Keypoint R-CNN head", "\n", "# --------------------------------------------------------------------------", "\n", "# interestingly, the keypoint head convs have blob names that are simply \"conv_fcnX\"", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"conv.fcn\"", ",", "\"roi_heads.keypoint_head.conv_fcn\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "layer_keys", "=", "[", "\n", "k", ".", "replace", "(", "\"kps.score.lowres\"", ",", "\"roi_heads.keypoint_head.score_lowres\"", ")", "for", "k", "in", "layer_keys", "\n", "]", "\n", "layer_keys", "=", "[", "k", ".", "replace", "(", "\"kps.score.\"", ",", "\"roi_heads.keypoint_head.score.\"", ")", "for", "k", "in", "layer_keys", "]", "\n", "\n", "# --------------------------------------------------------------------------", "\n", "# Done with replacements", "\n", "# --------------------------------------------------------------------------", "\n", "assert", "len", "(", "set", "(", "layer_keys", ")", ")", "==", "len", "(", "layer_keys", ")", "\n", "assert", "len", "(", "original_keys", ")", "==", "len", "(", "layer_keys", ")", "\n", "\n", "new_weights", "=", "{", "}", "\n", "new_keys_to_original_keys", "=", "{", "}", "\n", "for", "orig", ",", "renamed", "in", "zip", "(", "original_keys", ",", "layer_keys", ")", ":", "\n", "        ", "new_keys_to_original_keys", "[", "renamed", "]", "=", "orig", "\n", "if", "renamed", ".", "startswith", "(", "\"bbox_pred.\"", ")", "or", "renamed", ".", "startswith", "(", "\"mask_head.predictor.\"", ")", ":", "\n", "# remove the meaningless prediction weight for background class", "\n", "            ", "new_start_idx", "=", "4", "if", "renamed", ".", "startswith", "(", "\"bbox_pred.\"", ")", "else", "1", "\n", "new_weights", "[", "renamed", "]", "=", "weights", "[", "orig", "]", "[", "new_start_idx", ":", "]", "\n", "logger", ".", "info", "(", "\n", "\"Remove prediction weight for background class in {}. The shape changes from \"", "\n", "\"{} to {}.\"", ".", "format", "(", "\n", "renamed", ",", "tuple", "(", "weights", "[", "orig", "]", ".", "shape", ")", ",", "tuple", "(", "new_weights", "[", "renamed", "]", ".", "shape", ")", "\n", ")", "\n", ")", "\n", "", "elif", "renamed", ".", "startswith", "(", "\"cls_score.\"", ")", ":", "\n", "# move weights of bg class from original index 0 to last index", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Move classification weights for background class in {} from index 0 to \"", "\n", "\"index {}.\"", ".", "format", "(", "renamed", ",", "weights", "[", "orig", "]", ".", "shape", "[", "0", "]", "-", "1", ")", "\n", ")", "\n", "new_weights", "[", "renamed", "]", "=", "torch", ".", "cat", "(", "[", "weights", "[", "orig", "]", "[", "1", ":", "]", ",", "weights", "[", "orig", "]", "[", ":", "1", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "new_weights", "[", "renamed", "]", "=", "weights", "[", "orig", "]", "\n", "\n", "", "", "return", "new_weights", ",", "new_keys_to_original_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading.align_and_update_state_dicts": [[209, 335], ["sorted", "sorted", "torch.as_tensor().view", "torch.as_tensor().view.max", "logging.getLogger", "enumerate", "sorted", "c2_model_loading._longest_common_prefix", "c2_model_loading._group_keys_by_module", "set", "tabulate.tabulate", "logging.getLogger.info", "model_state_dict.keys", "c2_model_loading.convert_c2_detectron_names", "ckpt_state_dict.keys", "len", "len", "idxs.tolist", "matched_keys.values", "len", "logging.getLogger.warning", "a.endswith", "c2_model_loading.align_and_update_state_dicts.match"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._longest_common_prefix", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._group_keys_by_module", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading.convert_c2_detectron_names"], ["", "def", "align_and_update_state_dicts", "(", "model_state_dict", ",", "ckpt_state_dict", ",", "c2_conversion", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Match names between the two state-dict, and returns a new chkpt_state_dict with names\n    converted to match model_state_dict with heuristics. The returned dict can be later\n    loaded with fvcore checkpointer.\n    If `c2_conversion==True`, `ckpt_state_dict` is assumed to be a Caffe2\n    model and will be renamed at first.\n\n    Strategy: suppose that the models that we will create will have prefixes appended\n    to each of its keys, for example due to an extra level of nesting that the original\n    pre-trained weights from ImageNet won't contain. For example, model.state_dict()\n    might return backbone[0].body.res2.conv1.weight, while the pre-trained model contains\n    res2.conv1.weight. We thus want to match both parameters together.\n    For that, we look for each model weight, look among all loaded keys if there is one\n    that is a suffix of the current weight name, and use it if that's the case.\n    If multiple matches exist, take the one with longest size\n    of the corresponding name. For example, for the same model as before, the pretrained\n    weight file can contain both res2.conv1.weight, as well as conv1.weight. In this case,\n    we want to match backbone[0].body.conv1.weight to conv1.weight, and\n    backbone[0].body.res2.conv1.weight to res2.conv1.weight.\n    \"\"\"", "\n", "model_keys", "=", "sorted", "(", "model_state_dict", ".", "keys", "(", ")", ")", "\n", "if", "c2_conversion", ":", "\n", "        ", "ckpt_state_dict", ",", "original_keys", "=", "convert_c2_detectron_names", "(", "ckpt_state_dict", ")", "\n", "# original_keys: the name in the original dict (before renaming)", "\n", "", "else", ":", "\n", "        ", "original_keys", "=", "{", "x", ":", "x", "for", "x", "in", "ckpt_state_dict", ".", "keys", "(", ")", "}", "\n", "", "ckpt_keys", "=", "sorted", "(", "ckpt_state_dict", ".", "keys", "(", ")", ")", "\n", "\n", "def", "match", "(", "a", ",", "b", ")", ":", "\n", "# Matched ckpt_key should be a complete (starts with '.') suffix.", "\n", "# For example, roi_heads.mesh_head.whatever_conv1 does not match conv1,", "\n", "# but matches whatever_conv1 or mesh_head.whatever_conv1.", "\n", "        ", "return", "a", "==", "b", "or", "a", ".", "endswith", "(", "\".\"", "+", "b", ")", "\n", "\n", "# get a matrix of string matches, where each (i, j) entry correspond to the size of the", "\n", "# ckpt_key string, if it matches", "\n", "", "match_matrix", "=", "[", "len", "(", "j", ")", "if", "match", "(", "i", ",", "j", ")", "else", "0", "for", "i", "in", "model_keys", "for", "j", "in", "ckpt_keys", "]", "\n", "match_matrix", "=", "torch", ".", "as_tensor", "(", "match_matrix", ")", ".", "view", "(", "len", "(", "model_keys", ")", ",", "len", "(", "ckpt_keys", ")", ")", "\n", "# use the matched one with longest size in case of multiple matches", "\n", "max_match_size", ",", "idxs", "=", "match_matrix", ".", "max", "(", "1", ")", "\n", "# remove indices that correspond to no-match", "\n", "idxs", "[", "max_match_size", "==", "0", "]", "=", "-", "1", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "# matched_pairs (matched checkpoint key --> matched model key)", "\n", "matched_keys", "=", "{", "}", "\n", "result_state_dict", "=", "{", "}", "\n", "for", "idx_model", ",", "idx_ckpt", "in", "enumerate", "(", "idxs", ".", "tolist", "(", ")", ")", ":", "\n", "        ", "if", "idx_ckpt", "==", "-", "1", ":", "\n", "            ", "continue", "\n", "", "key_model", "=", "model_keys", "[", "idx_model", "]", "\n", "key_ckpt", "=", "ckpt_keys", "[", "idx_ckpt", "]", "\n", "value_ckpt", "=", "ckpt_state_dict", "[", "key_ckpt", "]", "\n", "shape_in_model", "=", "model_state_dict", "[", "key_model", "]", ".", "shape", "\n", "\n", "if", "shape_in_model", "!=", "value_ckpt", ".", "shape", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Shape of {} in checkpoint is {}, while shape of {} in model is {}.\"", ".", "format", "(", "\n", "key_ckpt", ",", "value_ckpt", ".", "shape", ",", "key_model", ",", "shape_in_model", "\n", ")", "\n", ")", "\n", "logger", ".", "warning", "(", "\n", "\"{} will not be loaded. Please double check and see if this is desired.\"", ".", "format", "(", "\n", "key_ckpt", "\n", ")", "\n", ")", "\n", "continue", "\n", "\n", "", "assert", "key_model", "not", "in", "result_state_dict", "\n", "result_state_dict", "[", "key_model", "]", "=", "value_ckpt", "\n", "if", "key_ckpt", "in", "matched_keys", ":", "# already added to matched_keys", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Ambiguity found for {} in checkpoint!\"", "\n", "\"It matches at least two keys in the model ({} and {}).\"", ".", "format", "(", "\n", "key_ckpt", ",", "key_model", ",", "matched_keys", "[", "key_ckpt", "]", "\n", ")", "\n", ")", "\n", "raise", "ValueError", "(", "\"Cannot match one checkpoint key to multiple keys in the model.\"", ")", "\n", "\n", "", "matched_keys", "[", "key_ckpt", "]", "=", "key_model", "\n", "\n", "# logging:", "\n", "", "matched_model_keys", "=", "sorted", "(", "matched_keys", ".", "values", "(", ")", ")", "\n", "if", "len", "(", "matched_model_keys", ")", "==", "0", ":", "\n", "        ", "logger", ".", "warning", "(", "\"No weights in checkpoint matched with model.\"", ")", "\n", "return", "ckpt_state_dict", "\n", "", "common_prefix", "=", "_longest_common_prefix", "(", "matched_model_keys", ")", "\n", "rev_matched_keys", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "matched_keys", ".", "items", "(", ")", "}", "\n", "original_keys", "=", "{", "k", ":", "original_keys", "[", "rev_matched_keys", "[", "k", "]", "]", "for", "k", "in", "matched_model_keys", "}", "\n", "\n", "model_key_groups", "=", "_group_keys_by_module", "(", "matched_model_keys", ",", "original_keys", ")", "\n", "table", "=", "[", "]", "\n", "memo", "=", "set", "(", ")", "\n", "for", "key_model", "in", "matched_model_keys", ":", "\n", "        ", "if", "key_model", "in", "memo", ":", "\n", "            ", "continue", "\n", "", "if", "key_model", "in", "model_key_groups", ":", "\n", "            ", "group", "=", "model_key_groups", "[", "key_model", "]", "\n", "memo", "|=", "set", "(", "group", ")", "\n", "shapes", "=", "[", "tuple", "(", "model_state_dict", "[", "k", "]", ".", "shape", ")", "for", "k", "in", "group", "]", "\n", "table", ".", "append", "(", "\n", "(", "\n", "_longest_common_prefix", "(", "[", "k", "[", "len", "(", "common_prefix", ")", ":", "]", "for", "k", "in", "group", "]", ")", "+", "\"*\"", ",", "\n", "_group_str", "(", "[", "original_keys", "[", "k", "]", "for", "k", "in", "group", "]", ")", ",", "\n", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "for", "x", "in", "shapes", "]", ")", ",", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "key_checkpoint", "=", "original_keys", "[", "key_model", "]", "\n", "shape", "=", "str", "(", "tuple", "(", "model_state_dict", "[", "key_model", "]", ".", "shape", ")", ")", "\n", "table", ".", "append", "(", "(", "key_model", "[", "len", "(", "common_prefix", ")", ":", "]", ",", "key_checkpoint", ",", "shape", ")", ")", "\n", "", "", "table_str", "=", "tabulate", "(", "\n", "table", ",", "tablefmt", "=", "\"pipe\"", ",", "headers", "=", "[", "\"Names in Model\"", ",", "\"Names in Checkpoint\"", ",", "\"Shapes\"", "]", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"Following weights matched with \"", "\n", "+", "(", "f\"submodule {common_prefix[:-1]}\"", "if", "common_prefix", "else", "\"model\"", ")", "\n", "+", "\":\\n\"", "\n", "+", "table_str", "\n", ")", "\n", "\n", "unmatched_ckpt_keys", "=", "[", "k", "for", "k", "in", "ckpt_keys", "if", "k", "not", "in", "set", "(", "matched_keys", ".", "keys", "(", ")", ")", "]", "\n", "for", "k", "in", "unmatched_ckpt_keys", ":", "\n", "        ", "result_state_dict", "[", "k", "]", "=", "ckpt_state_dict", "[", "k", "]", "\n", "", "return", "result_state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._group_keys_by_module": [[337, 375], ["sorted", "key.rfind", "c2_model_loading._group_keys_by_module._submodule_name"], "function", ["None"], ["", "def", "_group_keys_by_module", "(", "keys", ":", "List", "[", "str", "]", ",", "original_names", ":", "Dict", "[", "str", ",", "str", "]", ")", ":", "\n", "    ", "\"\"\"\n    Params in the same submodule are grouped together.\n\n    Args:\n        keys: names of all parameters\n        original_names: mapping from parameter name to their name in the checkpoint\n\n    Returns:\n        dict[name -> all other names in the same group]\n    \"\"\"", "\n", "\n", "def", "_submodule_name", "(", "key", ")", ":", "\n", "        ", "pos", "=", "key", ".", "rfind", "(", "\".\"", ")", "\n", "if", "pos", "<", "0", ":", "\n", "            ", "return", "None", "\n", "", "prefix", "=", "key", "[", ":", "pos", "+", "1", "]", "\n", "return", "prefix", "\n", "\n", "", "all_submodules", "=", "[", "_submodule_name", "(", "k", ")", "for", "k", "in", "keys", "]", "\n", "all_submodules", "=", "[", "x", "for", "x", "in", "all_submodules", "if", "x", "]", "\n", "all_submodules", "=", "sorted", "(", "all_submodules", ",", "key", "=", "len", ")", "\n", "\n", "ret", "=", "{", "}", "\n", "for", "prefix", "in", "all_submodules", ":", "\n", "        ", "group", "=", "[", "k", "for", "k", "in", "keys", "if", "k", ".", "startswith", "(", "prefix", ")", "]", "\n", "if", "len", "(", "group", ")", "<=", "1", ":", "\n", "            ", "continue", "\n", "", "original_name_lcp", "=", "_longest_common_prefix_str", "(", "[", "original_names", "[", "k", "]", "for", "k", "in", "group", "]", ")", "\n", "if", "len", "(", "original_name_lcp", ")", "==", "0", ":", "\n", "# don't group weights if original names don't share prefix", "\n", "            ", "continue", "\n", "\n", "", "for", "k", "in", "group", ":", "\n", "            ", "if", "k", "in", "ret", ":", "\n", "                ", "continue", "\n", "", "ret", "[", "k", "]", "=", "group", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._longest_common_prefix": [[377, 386], ["n.split", "min", "max", "len", "zip"], "function", ["None"], ["", "def", "_longest_common_prefix", "(", "names", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    [\"abc.zfg\", \"abc.zef\"] -> \"abc.\"\n    \"\"\"", "\n", "names", "=", "[", "n", ".", "split", "(", "\".\"", ")", "for", "n", "in", "names", "]", "\n", "m1", ",", "m2", "=", "min", "(", "names", ")", ",", "max", "(", "names", ")", "\n", "ret", "=", "[", "a", "for", "a", ",", "b", "in", "zip", "(", "m1", ",", "m2", ")", "if", "a", "==", "b", "]", "\n", "ret", "=", "\".\"", ".", "join", "(", "ret", ")", "+", "\".\"", "if", "len", "(", "ret", ")", "else", "\"\"", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._longest_common_prefix_str": [[388, 393], ["min", "max", "zip"], "function", ["None"], ["", "def", "_longest_common_prefix_str", "(", "names", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "m1", ",", "m2", "=", "min", "(", "names", ")", ",", "max", "(", "names", ")", "\n", "lcp", "=", "[", "a", "for", "a", ",", "b", "in", "zip", "(", "m1", ",", "m2", ")", "if", "a", "==", "b", "]", "\n", "lcp", "=", "\"\"", ".", "join", "(", "lcp", ")", "\n", "return", "lcp", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._group_str": [[395, 408], ["c2_model_loading._longest_common_prefix_str", "ret.replace.replace", "ret.replace.replace", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading._longest_common_prefix_str"], ["", "def", "_group_str", "(", "names", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Turn \"common1\", \"common2\", \"common3\" into \"common{1,2,3}\"\n    \"\"\"", "\n", "lcp", "=", "_longest_common_prefix_str", "(", "names", ")", "\n", "rest", "=", "[", "x", "[", "len", "(", "lcp", ")", ":", "]", "for", "x", "in", "names", "]", "\n", "rest", "=", "\"{\"", "+", "\",\"", ".", "join", "(", "rest", ")", "+", "\"}\"", "\n", "ret", "=", "lcp", "+", "rest", "\n", "\n", "# add some simplification for BN specifically", "\n", "ret", "=", "ret", ".", "replace", "(", "\"bn_{beta,running_mean,running_var,gamma}\"", ",", "\"bn_*\"", ")", "\n", "ret", "=", "ret", ".", "replace", "(", "\"bn_beta,bn_running_mean,bn_running_var,bn_gamma\"", ",", "\"bn_*\"", ")", "\n", "return", "ret", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo._ModelZooUrls.query": [[85, 96], ["config_path.replace().replace", "config_path.replace"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "query", "(", "config_path", ":", "str", ")", "->", "Optional", "[", "str", "]", ":", "\n", "        ", "\"\"\"\n        Args:\n            config_path: relative config filename\n        \"\"\"", "\n", "name", "=", "config_path", ".", "replace", "(", "\".yaml\"", ",", "\"\"", ")", ".", "replace", "(", "\".py\"", ",", "\"\"", ")", "\n", "if", "name", "in", "_ModelZooUrls", ".", "CONFIG_PATH_TO_URL_SUFFIX", ":", "\n", "            ", "suffix", "=", "_ModelZooUrls", ".", "CONFIG_PATH_TO_URL_SUFFIX", "[", "name", "]", "\n", "return", "_ModelZooUrls", ".", "S3_PREFIX", "+", "name", "+", "\"/\"", "+", "suffix", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_checkpoint_url": [[98, 113], ["model_zoo._ModelZooUrls.query", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo._ModelZooUrls.query"], ["", "", "def", "get_checkpoint_url", "(", "config_path", ")", ":", "\n", "    ", "\"\"\"\n    Returns the URL to the model trained using the given config\n\n    Args:\n        config_path (str): config file name relative to detectron2's \"configs/\"\n            directory, e.g., \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"\n\n    Returns:\n        str: a URL to the model\n    \"\"\"", "\n", "url", "=", "_ModelZooUrls", ".", "query", "(", "config_path", ")", "\n", "if", "url", "is", "None", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Pretrained model for {} is not available!\"", ".", "format", "(", "config_path", ")", ")", "\n", "", "return", "url", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config_file": [[115, 132], ["pkg_resources.resource_filename", "os.path.join", "os.path.exists", "RuntimeError"], "function", ["None"], ["", "def", "get_config_file", "(", "config_path", ")", ":", "\n", "    ", "\"\"\"\n    Returns path to a builtin config file.\n\n    Args:\n        config_path (str): config file name relative to detectron2's \"configs/\"\n            directory, e.g., \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"\n\n    Returns:\n        str: the real path to the config file.\n    \"\"\"", "\n", "cfg_file", "=", "pkg_resources", ".", "resource_filename", "(", "\n", "\"detectron2.model_zoo\"", ",", "os", ".", "path", ".", "join", "(", "\"configs\"", ",", "config_path", ")", "\n", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cfg_file", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"{} not available in Model Zoo!\"", ".", "format", "(", "config_path", ")", ")", "\n", "", "return", "cfg_file", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config": [[134, 165], ["model_zoo.get_config_file", "get_config_file.endswith", "detectron2.config.get_cfg", "LazyConfig.load.merge_from_file", "get_config_file.endswith", "model_zoo.get_checkpoint_url", "detectron2.config.LazyConfig.load", "model_zoo.get_checkpoint_url"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config_file", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.merge_from_file", "home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_checkpoint_url", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_checkpoint_url"], ["", "def", "get_config", "(", "config_path", ",", "trained", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Returns a config object for a model in model zoo.\n\n    Args:\n        config_path (str): config file name relative to detectron2's \"configs/\"\n            directory, e.g., \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"\n        trained (bool): If True, will set ``MODEL.WEIGHTS`` to trained model zoo weights.\n            If False, the checkpoint specified in the config file's ``MODEL.WEIGHTS`` is used\n            instead; this will typically (though not always) initialize a subset of weights using\n            an ImageNet pre-trained model, while randomly initializing the other weights.\n\n    Returns:\n        CfgNode or omegaconf.DictConfig: a config object\n    \"\"\"", "\n", "cfg_file", "=", "get_config_file", "(", "config_path", ")", "\n", "if", "cfg_file", ".", "endswith", "(", "\".yaml\"", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "merge_from_file", "(", "cfg_file", ")", "\n", "if", "trained", ":", "\n", "            ", "cfg", ".", "MODEL", ".", "WEIGHTS", "=", "get_checkpoint_url", "(", "config_path", ")", "\n", "", "return", "cfg", "\n", "", "elif", "cfg_file", ".", "endswith", "(", "\".py\"", ")", ":", "\n", "        ", "cfg", "=", "LazyConfig", ".", "load", "(", "cfg_file", ")", "\n", "if", "trained", ":", "\n", "            ", "url", "=", "get_checkpoint_url", "(", "config_path", ")", "\n", "if", "\"train\"", "in", "cfg", "and", "\"init_checkpoint\"", "in", "cfg", ".", "train", ":", "\n", "                ", "cfg", ".", "train", ".", "init_checkpoint", "=", "url", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "", "", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get": [[167, 201], ["model_zoo.get_config", "isinstance", "isinstance", "detectron2.modeling.build_model", "detectron2.checkpoint.DetectionCheckpointer().load", "detectron2.config.instantiate", "torch.cuda.is_available", "model.to.to", "detectron2.checkpoint.DetectionCheckpointer().load", "detectron2.checkpoint.DetectionCheckpointer", "detectron2.checkpoint.DetectionCheckpointer"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], ["", "", "def", "get", "(", "config_path", ",", "trained", ":", "bool", "=", "False", ",", "device", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Get a model specified by relative path under Detectron2's official ``configs/`` directory.\n\n    Args:\n        config_path (str): config file name relative to detectron2's \"configs/\"\n            directory, e.g., \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\"\n        trained (bool): see :func:`get_config`.\n        device (str or None): overwrite the device in config, if given.\n\n    Returns:\n        nn.Module: a detectron2 model. Will be in training mode.\n\n    Example:\n    ::\n        from detectron2 import model_zoo\n        model = model_zoo.get(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\", trained=True)\n    \"\"\"", "\n", "cfg", "=", "get_config", "(", "config_path", ",", "trained", ")", "\n", "if", "device", "is", "None", "and", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "device", "=", "\"cpu\"", "\n", "", "if", "device", "is", "not", "None", "and", "isinstance", "(", "cfg", ",", "CfgNode", ")", ":", "\n", "        ", "cfg", ".", "MODEL", ".", "DEVICE", "=", "device", "\n", "\n", "", "if", "isinstance", "(", "cfg", ",", "CfgNode", ")", ":", "\n", "        ", "model", "=", "build_model", "(", "cfg", ")", "\n", "DetectionCheckpointer", "(", "model", ")", ".", "load", "(", "cfg", ".", "MODEL", ".", "WEIGHTS", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "instantiate", "(", "cfg", ".", "model", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "", "if", "\"train\"", "in", "cfg", "and", "\"init_checkpoint\"", "in", "cfg", ".", "train", ":", "\n", "            ", "DetectionCheckpointer", "(", "model", ")", ".", "load", "(", "cfg", ".", "train", ".", "init_checkpoint", ")", "\n", "", "", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.scripting_with_instances": [[14, 59], ["torchscript_patch.freeze_training_mode", "torchscript_patch.patch_instances", "torch.jit.script"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.freeze_training_mode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances"], ["def", "scripting_with_instances", "(", "model", ",", "fields", ")", ":", "\n", "    ", "\"\"\"\n    Run :func:`torch.jit.script` on a model that uses the :class:`Instances` class. Since\n    attributes of :class:`Instances` are \"dynamically\" added in eager mode\uff0cit is difficult\n    for scripting to support it out of the box. This function is made to support scripting\n    a model that uses :class:`Instances`. It does the following:\n\n    1. Create a scriptable ``new_Instances`` class which behaves similarly to ``Instances``,\n       but with all attributes been \"static\".\n       The attributes need to be statically declared in the ``fields`` argument.\n    2. Register ``new_Instances``, and force scripting compiler to\n       use it when trying to compile ``Instances``.\n\n    After this function, the process will be reverted. User should be able to script another model\n    using different fields.\n\n    Example:\n        Assume that ``Instances`` in the model consist of two attributes named\n        ``proposal_boxes`` and ``objectness_logits`` with type :class:`Boxes` and\n        :class:`Tensor` respectively during inference. You can call this function like:\n        ::\n            fields = {\"proposal_boxes\": Boxes, \"objectness_logits\": torch.Tensor}\n            torchscipt_model =  scripting_with_instances(model, fields)\n\n    Note:\n        It only support models in evaluation mode.\n\n    Args:\n        model (nn.Module): The input model to be exported by scripting.\n        fields (Dict[str, type]): Attribute names and corresponding type that\n            ``Instances`` will use in the model. Note that all attributes used in ``Instances``\n            need to be added, regardless of whether they are inputs/outputs of the model.\n            Data type not defined in detectron2 is not supported for now.\n\n    Returns:\n        torch.jit.ScriptModule: the model in torchscript format\n    \"\"\"", "\n", "assert", "TORCH_VERSION", ">=", "(", "1", ",", "8", ")", ",", "\"This feature is not available in PyTorch < 1.8\"", "\n", "assert", "(", "\n", "not", "model", ".", "training", "\n", ")", ",", "\"Currently we only support exporting models in evaluation mode to torchscript\"", "\n", "\n", "with", "freeze_training_mode", "(", "model", ")", ",", "patch_instances", "(", "fields", ")", ":", "\n", "        ", "scripted_model", "=", "torch", ".", "jit", ".", "script", "(", "model", ")", "\n", "return", "scripted_model", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.dump_torchscript_IR": [[65, 134], ["detectron2.utils.file_io.PathManager.mkdirs", "isinstance", "detectron2.utils.file_io.PathManager.open", "isinstance", "detectron2.utils.file_io.PathManager.open", "f.write", "detectron2.utils.file_io.PathManager.open", "f.write", "isinstance", "os.path.join", "torchscript.dump_torchscript_IR.get_code"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["def", "dump_torchscript_IR", "(", "model", ",", "dir", ")", ":", "\n", "    ", "\"\"\"\n    Dump IR of a TracedModule/ScriptModule/Function in various format (code, graph,\n    inlined graph). Useful for debugging.\n\n    Args:\n        model (TracedModule/ScriptModule/ScriptFUnction): traced or scripted module\n        dir (str): output directory to dump files.\n    \"\"\"", "\n", "PathManager", ".", "mkdirs", "(", "dir", ")", "\n", "\n", "def", "_get_script_mod", "(", "mod", ")", ":", "\n", "        ", "if", "isinstance", "(", "mod", ",", "torch", ".", "jit", ".", "TracedModule", ")", ":", "\n", "            ", "return", "mod", ".", "_actual_script_module", "\n", "", "return", "mod", "\n", "\n", "# Dump pretty-printed code: https://pytorch.org/docs/stable/jit.html#inspecting-code", "\n", "", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "\"model_ts_code.txt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "\n", "        ", "def", "get_code", "(", "mod", ")", ":", "\n", "# Try a few ways to get code using private attributes.", "\n", "            ", "try", ":", "\n", "# This contains more information than just `mod.code`", "\n", "                ", "return", "_get_script_mod", "(", "mod", ")", ".", "_c", ".", "code", "\n", "", "except", "AttributeError", ":", "\n", "                ", "pass", "\n", "", "try", ":", "\n", "                ", "return", "mod", ".", "code", "\n", "", "except", "AttributeError", ":", "\n", "                ", "return", "None", "\n", "\n", "", "", "def", "dump_code", "(", "prefix", ",", "mod", ")", ":", "\n", "            ", "code", "=", "get_code", "(", "mod", ")", "\n", "name", "=", "prefix", "or", "\"root model\"", "\n", "if", "code", "is", "None", ":", "\n", "                ", "f", ".", "write", "(", "f\"Could not found code for {name} (type={mod.original_name})\\n\"", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "", "else", ":", "\n", "                ", "f", ".", "write", "(", "f\"\\nCode for {name}, type={mod.original_name}:\\n\"", ")", "\n", "f", ".", "write", "(", "code", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "f", ".", "write", "(", "\"-\"", "*", "80", ")", "\n", "\n", "", "for", "name", ",", "m", "in", "mod", ".", "named_children", "(", ")", ":", "\n", "                ", "dump_code", "(", "prefix", "+", "\".\"", "+", "name", ",", "m", ")", "\n", "\n", "", "", "if", "isinstance", "(", "model", ",", "torch", ".", "jit", ".", "ScriptFunction", ")", ":", "\n", "            ", "f", ".", "write", "(", "get_code", "(", "model", ")", ")", "\n", "", "else", ":", "\n", "            ", "dump_code", "(", "\"\"", ",", "model", ")", "\n", "\n", "", "", "def", "_get_graph", "(", "model", ")", ":", "\n", "        ", "try", ":", "\n", "# Recursively dump IR of all modules", "\n", "            ", "return", "_get_script_mod", "(", "model", ")", ".", "_c", ".", "dump_to_str", "(", "True", ",", "False", ",", "False", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "return", "model", ".", "graph", ".", "str", "(", ")", "\n", "\n", "", "", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "\"model_ts_IR.txt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "_get_graph", "(", "model", ")", ")", "\n", "\n", "# Dump IR of the entire graph (all submodules inlined)", "\n", "", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "\"model_ts_IR_inlined.txt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "str", "(", "model", ".", "inlined_graph", ")", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "model", ",", "torch", ".", "jit", ".", "ScriptFunction", ")", ":", "\n", "# Dump the model structure in pytorch style", "\n", "        ", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "\"model.txt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "model", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.ScopedWS.__init__": [[130, 135], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "ws_name", ",", "is_reset", ",", "is_cleanup", "=", "False", ")", ":", "\n", "        ", "self", ".", "ws_name", "=", "ws_name", "\n", "self", ".", "is_reset", "=", "is_reset", "\n", "self", ".", "is_cleanup", "=", "is_cleanup", "\n", "self", ".", "org_ws", "=", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.ScopedWS.__enter__": [[136, 144], ["caffe2.python.workspace.CurrentWorkspace", "caffe2.python.workspace.SwitchWorkspace", "caffe2.python.workspace.ResetWorkspace"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "self", ".", "org_ws", "=", "workspace", ".", "CurrentWorkspace", "(", ")", "\n", "if", "self", ".", "ws_name", "is", "not", "None", ":", "\n", "            ", "workspace", ".", "SwitchWorkspace", "(", "self", ".", "ws_name", ",", "True", ")", "\n", "", "if", "self", ".", "is_reset", ":", "\n", "            ", "workspace", ".", "ResetWorkspace", "(", ")", "\n", "\n", "", "return", "workspace", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.ScopedWS.__exit__": [[145, 150], ["caffe2.python.workspace.ResetWorkspace", "caffe2.python.workspace.SwitchWorkspace"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "if", "self", ".", "is_cleanup", ":", "\n", "            ", "workspace", ".", "ResetWorkspace", "(", ")", "\n", "", "if", "self", ".", "ws_name", "is", "not", "None", ":", "\n", "            ", "workspace", ".", "SwitchWorkspace", "(", "self", ".", "org_ws", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.DiGraph.__init__": [[785, 788], ["set", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "vertices", "=", "set", "(", ")", "\n", "self", ".", "graph", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.DiGraph.add_edge": [[789, 793], ["shared.DiGraph.graph[].append", "shared.DiGraph.vertices.add", "shared.DiGraph.vertices.add"], "methods", ["None"], ["", "def", "add_edge", "(", "self", ",", "u", ",", "v", ")", ":", "\n", "        ", "self", ".", "graph", "[", "u", "]", ".", "append", "(", "v", ")", "\n", "self", ".", "vertices", ".", "add", "(", "u", ")", "\n", "self", ".", "vertices", ".", "add", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.DiGraph.get_all_paths": [[795, 814], ["shared.DiGraph.get_all_paths._get_all_paths_util"], "methods", ["None"], ["", "def", "get_all_paths", "(", "self", ",", "s", ",", "d", ")", ":", "\n", "        ", "visited", "=", "{", "k", ":", "False", "for", "k", "in", "self", ".", "vertices", "}", "\n", "path", "=", "[", "]", "\n", "all_paths", "=", "[", "]", "\n", "\n", "def", "_get_all_paths_util", "(", "graph", ",", "u", ",", "d", ",", "visited", ",", "path", ")", ":", "\n", "            ", "visited", "[", "u", "]", "=", "True", "\n", "path", ".", "append", "(", "u", ")", "\n", "if", "u", "==", "d", ":", "\n", "                ", "all_paths", ".", "append", "(", "copy", ".", "deepcopy", "(", "path", ")", ")", "\n", "", "else", ":", "\n", "                ", "for", "i", "in", "graph", "[", "u", "]", ":", "\n", "                    ", "if", "not", "visited", "[", "i", "]", ":", "\n", "                        ", "_get_all_paths_util", "(", "graph", ",", "i", ",", "d", ",", "visited", ",", "path", ")", "\n", "", "", "", "path", ".", "pop", "(", ")", "\n", "visited", "[", "u", "]", "=", "False", "\n", "\n", "", "_get_all_paths_util", "(", "self", ".", "graph", ",", "s", ",", "d", ",", "visited", ",", "path", ")", "\n", "return", "all_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.DiGraph.from_ssa": [[815, 823], ["shared.DiGraph", "range", "len", "shared.DiGraph.add_edge"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.DiGraph.add_edge"], ["", "@", "staticmethod", "\n", "def", "from_ssa", "(", "ssa", ")", ":", "\n", "        ", "graph", "=", "DiGraph", "(", ")", "\n", "for", "op_id", "in", "range", "(", "len", "(", "ssa", ")", ")", ":", "\n", "            ", "for", "inp", "in", "ssa", "[", "op_id", "]", "[", "0", "]", ":", "\n", "                ", "for", "outp", "in", "ssa", "[", "op_id", "]", "[", "1", "]", ":", "\n", "                    ", "graph", ".", "add_edge", "(", "inp", ",", "outp", ")", "\n", "", "", "", "return", "graph", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device": [[25, 42], ["torch.device", "torch.device", "torch.ops._caffe2.CopyGPUToCPU", "torch.ops._caffe2.CopyGPUToCPU", "torch.ops._caffe2.CopyCPUToGPU", "torch.ops._caffe2.CopyCPUToGPU", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["def", "to_device", "(", "t", ",", "device_str", ")", ":", "\n", "    ", "\"\"\"\n    This function is a replacement of .to(another_device) such that it allows the\n    casting to be traced properly by explicitly calling the underlying copy ops.\n    It also avoids introducing unncessary op when casting to the same device.\n    \"\"\"", "\n", "src", "=", "t", ".", "device", "\n", "dst", "=", "torch", ".", "device", "(", "device_str", ")", "\n", "\n", "if", "src", "==", "dst", ":", "\n", "        ", "return", "t", "\n", "", "elif", "src", ".", "type", "==", "\"cuda\"", "and", "dst", ".", "type", "==", "\"cpu\"", ":", "\n", "        ", "return", "torch", ".", "ops", ".", "_caffe2", ".", "CopyGPUToCPU", "(", "t", ")", "\n", "", "elif", "src", ".", "type", "==", "\"cpu\"", "and", "dst", ".", "type", "==", "\"cuda\"", ":", "\n", "        ", "return", "torch", ".", "ops", ".", "_caffe2", ".", "CopyCPUToGPU", "(", "t", ")", "\n", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Can't cast tensor from device {} to device {}\"", ".", "format", "(", "src", ",", "dst", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.BilinearInterpolation": [[48, 77], ["shared.BilinearInterpolation.upsample_filt"], "function", ["None"], ["", "", "def", "BilinearInterpolation", "(", "tensor_in", ",", "up_scale", ")", ":", "\n", "    ", "assert", "up_scale", "%", "2", "==", "0", ",", "\"Scale should be even\"", "\n", "\n", "def", "upsample_filt", "(", "size", ")", ":", "\n", "        ", "factor", "=", "(", "size", "+", "1", ")", "//", "2", "\n", "if", "size", "%", "2", "==", "1", ":", "\n", "            ", "center", "=", "factor", "-", "1", "\n", "", "else", ":", "\n", "            ", "center", "=", "factor", "-", "0.5", "\n", "\n", "", "og", "=", "np", ".", "ogrid", "[", ":", "size", ",", ":", "size", "]", "\n", "return", "(", "1", "-", "abs", "(", "og", "[", "0", "]", "-", "center", ")", "/", "factor", ")", "*", "(", "1", "-", "abs", "(", "og", "[", "1", "]", "-", "center", ")", "/", "factor", ")", "\n", "\n", "", "kernel_size", "=", "int", "(", "up_scale", ")", "*", "2", "\n", "bil_filt", "=", "upsample_filt", "(", "kernel_size", ")", "\n", "\n", "dim", "=", "int", "(", "tensor_in", ".", "shape", "[", "1", "]", ")", "\n", "kernel", "=", "np", ".", "zeros", "(", "(", "dim", ",", "dim", ",", "kernel_size", ",", "kernel_size", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "kernel", "[", "range", "(", "dim", ")", ",", "range", "(", "dim", ")", ",", ":", ",", ":", "]", "=", "bil_filt", "\n", "\n", "tensor_out", "=", "F", ".", "conv_transpose2d", "(", "\n", "tensor_in", ",", "\n", "weight", "=", "to_device", "(", "torch", ".", "Tensor", "(", "kernel", ")", ",", "tensor_in", ".", "device", ")", ",", "\n", "bias", "=", "None", ",", "\n", "stride", "=", "int", "(", "up_scale", ")", ",", "\n", "padding", "=", "int", "(", "up_scale", "/", "2", ")", ",", "\n", ")", "\n", "\n", "return", "tensor_out", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.onnx_compatibale_interpolate": [[82, 113], ["torch.nn.functional.interpolate", "logger.warning", "input.dim", "isinstance", "isinstance", "torch.ops._caffe2.ResizeNearest", "torch.ops._caffe2.ResizeNearest", "len", "logger.warning", "shared.BilinearInterpolation"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.BilinearInterpolation"], ["", "def", "onnx_compatibale_interpolate", "(", "\n", "input", ",", "size", "=", "None", ",", "scale_factor", "=", "None", ",", "mode", "=", "\"nearest\"", ",", "align_corners", "=", "None", "\n", ")", ":", "\n", "# NOTE: The input dimensions are interpreted in the form:", "\n", "# `mini-batch x channels x [optional depth] x [optional height] x width`.", "\n", "    ", "if", "size", "is", "None", "and", "scale_factor", "is", "not", "None", ":", "\n", "        ", "if", "input", ".", "dim", "(", ")", "==", "4", ":", "\n", "            ", "if", "isinstance", "(", "scale_factor", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "height_scale", ",", "width_scale", "=", "(", "scale_factor", ",", "scale_factor", ")", "\n", "", "else", ":", "\n", "                ", "assert", "isinstance", "(", "scale_factor", ",", "(", "tuple", ",", "list", ")", ")", "\n", "assert", "len", "(", "scale_factor", ")", "==", "2", "\n", "height_scale", ",", "width_scale", "=", "scale_factor", "\n", "\n", "", "assert", "not", "align_corners", ",", "\"No matching C2 op for align_corners == True\"", "\n", "if", "mode", "==", "\"nearest\"", ":", "\n", "                ", "return", "torch", ".", "ops", ".", "_caffe2", ".", "ResizeNearest", "(", "\n", "input", ",", "order", "=", "\"NCHW\"", ",", "width_scale", "=", "width_scale", ",", "height_scale", "=", "height_scale", "\n", ")", "\n", "", "elif", "mode", "==", "\"bilinear\"", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"Use F.conv_transpose2d for bilinear interpolate\"", "\n", "\" because there's no such C2 op, this may cause significant\"", "\n", "\" slowdown and the boundary pixels won't be as same as\"", "\n", "\" using F.interpolate due to padding.\"", "\n", ")", "\n", "assert", "height_scale", "==", "width_scale", "\n", "return", "BilinearInterpolation", "(", "input", ",", "up_scale", "=", "height_scale", ")", "\n", "", "", "logger", ".", "warning", "(", "\"Output size is not static, it might cause ONNX conversion issue\"", ")", "\n", "\n", "", "return", "interp", "(", "input", ",", "size", ",", "scale_factor", ",", "mode", ",", "align_corners", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.mock_torch_nn_functional_interpolate": [[115, 124], ["torch.onnx.is_in_onnx_export", "torch.onnx.is_in_onnx_export", "unittest.mock.patch"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch"], ["", "@", "contextlib", ".", "contextmanager", "\n", "def", "mock_torch_nn_functional_interpolate", "(", ")", ":", "\n", "    ", "if", "torch", ".", "onnx", ".", "is_in_onnx_export", "(", ")", ":", "\n", "        ", "with", "mock", ".", "patch", "(", "\n", "\"torch.nn.functional.interpolate\"", ",", "side_effect", "=", "onnx_compatibale_interpolate", "\n", ")", ":", "\n", "            ", "yield", "\n", "", "", "else", ":", "\n", "        ", "yield", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.fetch_any_blob": [[152, 162], ["caffe2.python.workspace.FetchBlob", "caffe2.python.workspace.FetchInt8Blob", "logger.error"], "function", ["None"], ["", "", "", "def", "fetch_any_blob", "(", "name", ")", ":", "\n", "    ", "bb", "=", "None", "\n", "try", ":", "\n", "        ", "bb", "=", "workspace", ".", "FetchBlob", "(", "name", ")", "\n", "", "except", "TypeError", ":", "\n", "        ", "bb", "=", "workspace", ".", "FetchInt8Blob", "(", "name", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "logger", ".", "error", "(", "\"Get blob {} error: {}\"", ".", "format", "(", "name", ",", "e", ")", ")", "\n", "\n", "", "return", "bb", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg": [[167, 172], ["None"], "function", ["None"], ["", "def", "get_pb_arg", "(", "pb", ",", "arg_name", ")", ":", "\n", "    ", "for", "x", "in", "pb", ".", "arg", ":", "\n", "        ", "if", "x", ".", "name", "==", "arg_name", ":", "\n", "            ", "return", "x", "\n", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_valf": [[174, 177], ["shared.get_pb_arg"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg"], ["", "def", "get_pb_arg_valf", "(", "pb", ",", "arg_name", ",", "default_val", ")", ":", "\n", "    ", "arg", "=", "get_pb_arg", "(", "pb", ",", "arg_name", ")", "\n", "return", "arg", ".", "f", "if", "arg", "is", "not", "None", "else", "default_val", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_floats": [[179, 182], ["shared.get_pb_arg", "list", "map"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "get_pb_arg_floats", "(", "pb", ",", "arg_name", ",", "default_val", ")", ":", "\n", "    ", "arg", "=", "get_pb_arg", "(", "pb", ",", "arg_name", ")", "\n", "return", "list", "(", "map", "(", "float", ",", "arg", ".", "floats", ")", ")", "if", "arg", "is", "not", "None", "else", "default_val", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_ints": [[184, 187], ["shared.get_pb_arg", "list", "map"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "get_pb_arg_ints", "(", "pb", ",", "arg_name", ",", "default_val", ")", ":", "\n", "    ", "arg", "=", "get_pb_arg", "(", "pb", ",", "arg_name", ")", "\n", "return", "list", "(", "map", "(", "int", ",", "arg", ".", "ints", ")", ")", "if", "arg", "is", "not", "None", "else", "default_val", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vali": [[189, 192], ["shared.get_pb_arg"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg"], ["", "def", "get_pb_arg_vali", "(", "pb", ",", "arg_name", ",", "default_val", ")", ":", "\n", "    ", "arg", "=", "get_pb_arg", "(", "pb", ",", "arg_name", ")", "\n", "return", "arg", ".", "i", "if", "arg", "is", "not", "None", "else", "default_val", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vals": [[194, 197], ["shared.get_pb_arg"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg"], ["", "def", "get_pb_arg_vals", "(", "pb", ",", "arg_name", ",", "default_val", ")", ":", "\n", "    ", "arg", "=", "get_pb_arg", "(", "pb", ",", "arg_name", ")", "\n", "return", "arg", ".", "s", "if", "arg", "is", "not", "None", "else", "default_val", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_valstrings": [[199, 202], ["shared.get_pb_arg", "list"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "get_pb_arg_valstrings", "(", "pb", ",", "arg_name", ",", "default_val", ")", ":", "\n", "    ", "arg", "=", "get_pb_arg", "(", "pb", ",", "arg_name", ")", "\n", "return", "list", "(", "arg", ".", "strings", ")", "if", "arg", "is", "not", "None", "else", "default_val", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg": [[204, 219], ["shared.get_pb_arg", "caffe2.MakeArgument", "hasattr", "pb.arg.extend", "logger.warning", "setattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg"], ["", "def", "check_set_pb_arg", "(", "pb", ",", "arg_name", ",", "arg_attr", ",", "arg_value", ",", "allow_override", "=", "False", ")", ":", "\n", "    ", "arg", "=", "get_pb_arg", "(", "pb", ",", "arg_name", ")", "\n", "if", "arg", "is", "None", ":", "\n", "        ", "arg", "=", "putils", ".", "MakeArgument", "(", "arg_name", ",", "arg_value", ")", "\n", "assert", "hasattr", "(", "arg", ",", "arg_attr", ")", "\n", "pb", ".", "arg", ".", "extend", "(", "[", "arg", "]", ")", "\n", "", "if", "allow_override", "and", "getattr", "(", "arg", ",", "arg_attr", ")", "!=", "arg_value", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"Override argument {}: {} -> {}\"", ".", "format", "(", "arg_name", ",", "getattr", "(", "arg", ",", "arg_attr", ")", ",", "arg_value", ")", "\n", ")", "\n", "setattr", "(", "arg", ",", "arg_attr", ",", "arg_value", ")", "\n", "", "else", ":", "\n", "        ", "assert", "arg", "is", "not", "None", "\n", "assert", "getattr", "(", "arg", ",", "arg_attr", ")", "==", "arg_value", ",", "\"Existing value {}, new value {}\"", ".", "format", "(", "\n", "getattr", "(", "arg", ",", "arg_attr", ")", ",", "arg_value", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._create_const_fill_op_from_numpy": [[222, 241], ["caffe2.python.core.CreateOperator", "type", "numpy.dtype", "numpy.dtype", "numpy.dtype", "numpy.dtype", "numpy.dtype", "args_dict.update", "args_dict.update", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype"], ["", "", "def", "_create_const_fill_op_from_numpy", "(", "name", ",", "tensor", ",", "device_option", "=", "None", ")", ":", "\n", "    ", "assert", "type", "(", "tensor", ")", "==", "np", ".", "ndarray", "\n", "kTypeNameMapper", "=", "{", "\n", "np", ".", "dtype", "(", "\"float32\"", ")", ":", "\"GivenTensorFill\"", ",", "\n", "np", ".", "dtype", "(", "\"int32\"", ")", ":", "\"GivenTensorIntFill\"", ",", "\n", "np", ".", "dtype", "(", "\"int64\"", ")", ":", "\"GivenTensorInt64Fill\"", ",", "\n", "np", ".", "dtype", "(", "\"uint8\"", ")", ":", "\"GivenTensorStringFill\"", ",", "\n", "}", "\n", "\n", "args_dict", "=", "{", "}", "\n", "if", "tensor", ".", "dtype", "==", "np", ".", "dtype", "(", "\"uint8\"", ")", ":", "\n", "        ", "args_dict", ".", "update", "(", "{", "\"values\"", ":", "[", "str", "(", "tensor", ".", "data", ")", "]", ",", "\"shape\"", ":", "[", "1", "]", "}", ")", "\n", "", "else", ":", "\n", "        ", "args_dict", ".", "update", "(", "{", "\"values\"", ":", "tensor", ",", "\"shape\"", ":", "tensor", ".", "shape", "}", ")", "\n", "\n", "", "if", "device_option", "is", "not", "None", ":", "\n", "        ", "args_dict", "[", "\"device_option\"", "]", "=", "device_option", "\n", "\n", "", "return", "core", ".", "CreateOperator", "(", "kTypeNameMapper", "[", "tensor", ".", "dtype", "]", ",", "[", "]", ",", "[", "name", "]", ",", "**", "args_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._create_const_fill_op_from_c2_int8_tensor": [[243, 262], ["caffe2.python.core.CreateOperator", "type", "numpy.dtype", "numpy.dtype", "tensor.tobytes", "numpy.dtype", "numpy.dtype", "numpy.dtype"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.clip_backbone.CLIPLangEncoder.dtype"], ["", "def", "_create_const_fill_op_from_c2_int8_tensor", "(", "name", ",", "int8_tensor", ")", ":", "\n", "    ", "assert", "type", "(", "int8_tensor", ")", "==", "workspace", ".", "Int8Tensor", "\n", "kTypeNameMapper", "=", "{", "\n", "np", ".", "dtype", "(", "\"int32\"", ")", ":", "\"Int8GivenIntTensorFill\"", ",", "\n", "np", ".", "dtype", "(", "\"uint8\"", ")", ":", "\"Int8GivenTensorFill\"", ",", "\n", "}", "\n", "\n", "tensor", "=", "int8_tensor", ".", "data", "\n", "assert", "tensor", ".", "dtype", "in", "[", "np", ".", "dtype", "(", "\"uint8\"", ")", ",", "np", ".", "dtype", "(", "\"int32\"", ")", "]", "\n", "values", "=", "tensor", ".", "tobytes", "(", ")", "if", "tensor", ".", "dtype", "==", "np", ".", "dtype", "(", "\"uint8\"", ")", "else", "tensor", "\n", "\n", "return", "core", ".", "CreateOperator", "(", "\n", "kTypeNameMapper", "[", "tensor", ".", "dtype", "]", ",", "\n", "[", "]", ",", "\n", "[", "name", "]", ",", "\n", "values", "=", "values", ",", "\n", "shape", "=", "tensor", ".", "shape", ",", "\n", "Y_scale", "=", "int8_tensor", ".", "scale", ",", "\n", "Y_zero_point", "=", "int8_tensor", ".", "zero_point", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.create_const_fill_op": [[265, 288], ["type", "type", "shared._create_const_fill_op_from_numpy", "shared._create_const_fill_op_from_c2_int8_tensor"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._create_const_fill_op_from_numpy", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._create_const_fill_op_from_c2_int8_tensor"], ["", "def", "create_const_fill_op", "(", "\n", "name", ":", "str", ",", "\n", "blob", ":", "Union", "[", "np", ".", "ndarray", ",", "workspace", ".", "Int8Tensor", "]", ",", "\n", "device_option", ":", "Optional", "[", "caffe2_pb2", ".", "DeviceOption", "]", "=", "None", ",", "\n", ")", "->", "caffe2_pb2", ".", "OperatorDef", ":", "\n", "    ", "\"\"\"\n    Given a blob object, return the Caffe2 operator that creates this blob\n    as constant. Currently support NumPy tensor and Caffe2 Int8Tensor.\n    \"\"\"", "\n", "\n", "tensor_type", "=", "type", "(", "blob", ")", "\n", "assert", "tensor_type", "in", "[", "\n", "np", ".", "ndarray", ",", "\n", "workspace", ".", "Int8Tensor", ",", "\n", "]", ",", "'Error when creating const fill op for \"{}\", unsupported blob type: {}'", ".", "format", "(", "\n", "name", ",", "type", "(", "blob", ")", "\n", ")", "\n", "\n", "if", "tensor_type", "==", "np", ".", "ndarray", ":", "\n", "        ", "return", "_create_const_fill_op_from_numpy", "(", "name", ",", "blob", ",", "device_option", ")", "\n", "", "elif", "tensor_type", "==", "workspace", ".", "Int8Tensor", ":", "\n", "        ", "assert", "device_option", "is", "None", "\n", "return", "_create_const_fill_op_from_c2_int8_tensor", "(", "name", ",", "blob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.construct_init_net_from_params": [[290, 312], ["caffe2.proto.caffe2_pb2.NetDef", "params.items", "isinstance", "caffe2_pb2.NetDef.op.extend", "caffe2_pb2.NetDef.external_output.append", "logger.warning", "shared.create_const_fill_op", "type", "device_options.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.create_const_fill_op", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "construct_init_net_from_params", "(", "\n", "params", ":", "Dict", "[", "str", ",", "Any", "]", ",", "device_options", ":", "Optional", "[", "Dict", "[", "str", ",", "caffe2_pb2", ".", "DeviceOption", "]", "]", "=", "None", "\n", ")", "->", "caffe2_pb2", ".", "NetDef", ":", "\n", "    ", "\"\"\"\n    Construct the init_net from params dictionary\n    \"\"\"", "\n", "init_net", "=", "caffe2_pb2", ".", "NetDef", "(", ")", "\n", "device_options", "=", "device_options", "or", "{", "}", "\n", "for", "name", ",", "blob", "in", "params", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "blob", ",", "str", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "(", "\n", "\"Blob {} with type {} is not supported in generating init net,\"", "\n", "\" skipped.\"", ".", "format", "(", "name", ",", "type", "(", "blob", ")", ")", "\n", ")", "\n", ")", "\n", "continue", "\n", "", "init_net", ".", "op", ".", "extend", "(", "\n", "[", "create_const_fill_op", "(", "name", ",", "blob", ",", "device_option", "=", "device_options", ".", "get", "(", "name", ",", "None", ")", ")", "]", "\n", ")", "\n", "init_net", ".", "external_output", ".", "append", "(", "name", ")", "\n", "", "return", "init_net", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_producer_map": [[314, 325], ["range", "len", "enumerate"], "function", ["None"], ["", "def", "get_producer_map", "(", "ssa", ")", ":", "\n", "    ", "\"\"\"\n    Return dict from versioned blob to (i, j),\n        where i is index of producer op, j is the index of output of that op.\n    \"\"\"", "\n", "producer_map", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "ssa", ")", ")", ":", "\n", "        ", "outputs", "=", "ssa", "[", "i", "]", "[", "1", "]", "\n", "for", "j", ",", "outp", "in", "enumerate", "(", "outputs", ")", ":", "\n", "            ", "producer_map", "[", "outp", "]", "=", "(", "i", ",", "j", ")", "\n", "", "", "return", "producer_map", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_consumer_map": [[327, 338], ["collections.defaultdict", "range", "len", "enumerate", "consumer_map[].append"], "function", ["None"], ["", "def", "get_consumer_map", "(", "ssa", ")", ":", "\n", "    ", "\"\"\"\n    Return dict from versioned blob to list of (i, j),\n        where i is index of consumer op, j is the index of input of that op.\n    \"\"\"", "\n", "consumer_map", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "ssa", ")", ")", ":", "\n", "        ", "inputs", "=", "ssa", "[", "i", "]", "[", "0", "]", "\n", "for", "j", ",", "inp", "in", "enumerate", "(", "inputs", ")", ":", "\n", "            ", "consumer_map", "[", "inp", "]", ".", "append", "(", "(", "i", ",", "j", ")", ")", "\n", "", "", "return", "consumer_map", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_params_from_init_net": [[340, 367], ["caffe2.python.core.get_ssa", "shared.get_producer_map", "shared.ScopedWS", "ws.RunNetOnce", "shared.get_params_from_init_net._get_device_option"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_producer_map"], ["", "def", "get_params_from_init_net", "(", "\n", "init_net", ":", "caffe2_pb2", ".", "NetDef", ",", "\n", ")", "->", "[", "Dict", "[", "str", ",", "Any", "]", ",", "Dict", "[", "str", ",", "caffe2_pb2", ".", "DeviceOption", "]", "]", ":", "\n", "    ", "\"\"\"\n    Take the output blobs from init_net by running it.\n    Outputs:\n        params: dict from blob name to numpy array\n        device_options: dict from blob name to the device option of its creating op\n    \"\"\"", "\n", "# NOTE: this assumes that the params is determined by producer op with the", "\n", "# only exception be CopyGPUToCPU which is CUDA op but returns CPU tensor.", "\n", "def", "_get_device_option", "(", "producer_op", ")", ":", "\n", "        ", "if", "producer_op", ".", "type", "==", "\"CopyGPUToCPU\"", ":", "\n", "            ", "return", "caffe2_pb2", ".", "DeviceOption", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "producer_op", ".", "device_option", "\n", "\n", "", "", "with", "ScopedWS", "(", "\"__get_params_from_init_net__\"", ",", "is_reset", "=", "True", ",", "is_cleanup", "=", "True", ")", "as", "ws", ":", "\n", "        ", "ws", ".", "RunNetOnce", "(", "init_net", ")", "\n", "params", "=", "{", "b", ":", "fetch_any_blob", "(", "b", ")", "for", "b", "in", "init_net", ".", "external_output", "}", "\n", "", "ssa", ",", "versions", "=", "core", ".", "get_ssa", "(", "init_net", ")", "\n", "producer_map", "=", "get_producer_map", "(", "ssa", ")", "\n", "device_options", "=", "{", "\n", "b", ":", "_get_device_option", "(", "init_net", ".", "op", "[", "producer_map", "[", "(", "b", ",", "versions", "[", "b", "]", ")", "]", "[", "0", "]", "]", ")", "\n", "for", "b", "in", "init_net", ".", "external_output", "\n", "}", "\n", "return", "params", ",", "device_options", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._updater_raise": [[369, 373], ["RuntimeError"], "function", ["None"], ["", "def", "_updater_raise", "(", "op", ",", "input_types", ",", "output_types", ")", ":", "\n", "    ", "raise", "RuntimeError", "(", "\n", "\"Failed to apply updater for op {} given input_types {} and\"", "\n", "\" output_types {}\"", ".", "format", "(", "op", ",", "input_types", ",", "output_types", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._generic_status_identifier": [[376, 446], ["caffe2.python.core.get_ssa", "set().union", "set().union.union().union", "all", "all", "copy.deepcopy", "zip", "zip", "status_updater", "zip", "shared._generic_status_identifier._update_i"], "function", ["None"], ["", "def", "_generic_status_identifier", "(", "\n", "predict_net", ":", "caffe2_pb2", ".", "NetDef", ",", "\n", "status_updater", ":", "Callable", ",", "\n", "known_status", ":", "Dict", "[", "Tuple", "[", "str", ",", "int", "]", ",", "Any", "]", ",", "\n", ")", "->", "Dict", "[", "Tuple", "[", "str", ",", "int", "]", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    Statically infer the status of each blob, the status can be such as device type\n        (CPU/GPU), layout (NCHW/NHWC), data type (float32/int8), etc. \"Blob\" here\n        is versioned blob (Tuple[str, int]) in the format compatible with ssa.\n    Inputs:\n        predict_net: the caffe2 network\n        status_updater: a callable, given an op and the status of its input/output,\n            it returns the updated status of input/output. `None` is used for\n            representing unknown status.\n        known_status: a dict containing known status, used as initialization.\n    Outputs:\n        A dict mapping from versioned blob to its status\n    \"\"\"", "\n", "ssa", ",", "versions", "=", "core", ".", "get_ssa", "(", "predict_net", ")", "\n", "versioned_ext_input", "=", "[", "(", "b", ",", "0", ")", "for", "b", "in", "predict_net", ".", "external_input", "]", "\n", "versioned_ext_output", "=", "[", "(", "b", ",", "versions", "[", "b", "]", ")", "for", "b", "in", "predict_net", ".", "external_output", "]", "\n", "all_versioned_blobs", "=", "set", "(", ")", ".", "union", "(", "*", "[", "set", "(", "x", "[", "0", "]", "+", "x", "[", "1", "]", ")", "for", "x", "in", "ssa", "]", ")", "\n", "\n", "allowed_vbs", "=", "all_versioned_blobs", ".", "union", "(", "versioned_ext_input", ")", ".", "union", "(", "versioned_ext_output", ")", "\n", "assert", "all", "(", "k", "in", "allowed_vbs", "for", "k", "in", "known_status", ")", "\n", "assert", "all", "(", "v", "is", "not", "None", "for", "v", "in", "known_status", ".", "values", "(", ")", ")", "\n", "_known_status", "=", "copy", ".", "deepcopy", "(", "known_status", ")", "\n", "\n", "def", "_check_and_update", "(", "key", ",", "value", ")", ":", "\n", "        ", "assert", "value", "is", "not", "None", "\n", "if", "key", "in", "_known_status", ":", "\n", "            ", "if", "not", "_known_status", "[", "key", "]", "==", "value", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Confilict status for {}, existing status {}, new status {}\"", ".", "format", "(", "\n", "key", ",", "_known_status", "[", "key", "]", ",", "value", "\n", ")", "\n", ")", "\n", "", "", "_known_status", "[", "key", "]", "=", "value", "\n", "\n", "", "def", "_update_i", "(", "op", ",", "ssa_i", ")", ":", "\n", "        ", "versioned_inputs", "=", "ssa_i", "[", "0", "]", "\n", "versioned_outputs", "=", "ssa_i", "[", "1", "]", "\n", "\n", "inputs_status", "=", "[", "_known_status", ".", "get", "(", "b", ",", "None", ")", "for", "b", "in", "versioned_inputs", "]", "\n", "outputs_status", "=", "[", "_known_status", ".", "get", "(", "b", ",", "None", ")", "for", "b", "in", "versioned_outputs", "]", "\n", "\n", "new_inputs_status", ",", "new_outputs_status", "=", "status_updater", "(", "op", ",", "inputs_status", ",", "outputs_status", ")", "\n", "\n", "for", "versioned_blob", ",", "status", "in", "zip", "(", "\n", "versioned_inputs", "+", "versioned_outputs", ",", "new_inputs_status", "+", "new_outputs_status", "\n", ")", ":", "\n", "            ", "if", "status", "is", "not", "None", ":", "\n", "                ", "_check_and_update", "(", "versioned_blob", ",", "status", ")", "\n", "\n", "", "", "", "for", "op", ",", "ssa_i", "in", "zip", "(", "predict_net", ".", "op", ",", "ssa", ")", ":", "\n", "        ", "_update_i", "(", "op", ",", "ssa_i", ")", "\n", "", "for", "op", ",", "ssa_i", "in", "zip", "(", "reversed", "(", "predict_net", ".", "op", ")", ",", "reversed", "(", "ssa", ")", ")", ":", "\n", "        ", "_update_i", "(", "op", ",", "ssa_i", ")", "\n", "\n", "# NOTE: This strictly checks all the blob from predict_net must be assgined", "\n", "# a known status. However sometimes it's impossible (eg. having deadend op),", "\n", "# we may relax this constraint if", "\n", "", "for", "k", "in", "all_versioned_blobs", ":", "\n", "        ", "if", "k", "not", "in", "_known_status", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"Can not infer the status for {}. Currently only support the case where\"", "\n", "\" a single forward and backward pass can identify status for all blobs.\"", ".", "format", "(", "k", ")", "\n", ")", "\n", "\n", "", "", "return", "_known_status", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.infer_device_type": [[448, 486], ["shared._generic_status_identifier", "shared._updater_raise", "shared._updater_raise", "len", "all", "shared._updater_raise"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._generic_status_identifier", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._updater_raise", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._updater_raise", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._updater_raise"], ["", "def", "infer_device_type", "(", "\n", "predict_net", ":", "caffe2_pb2", ".", "NetDef", ",", "\n", "known_status", ":", "Dict", "[", "Tuple", "[", "str", ",", "int", "]", ",", "Any", "]", ",", "\n", "device_name_style", ":", "str", "=", "\"caffe2\"", ",", "\n", ")", "->", "Dict", "[", "Tuple", "[", "str", ",", "int", "]", ",", "str", "]", ":", "\n", "    ", "\"\"\"Return the device type (\"cpu\" or \"gpu\"/\"cuda\") of each (versioned) blob\"\"\"", "\n", "\n", "assert", "device_name_style", "in", "[", "\"caffe2\"", ",", "\"pytorch\"", "]", "\n", "_CPU_STR", "=", "\"cpu\"", "\n", "_GPU_STR", "=", "\"gpu\"", "if", "device_name_style", "==", "\"caffe2\"", "else", "\"cuda\"", "\n", "\n", "def", "_copy_cpu_to_gpu_updater", "(", "op", ",", "input_types", ",", "output_types", ")", ":", "\n", "        ", "if", "input_types", "[", "0", "]", "==", "_GPU_STR", "or", "output_types", "[", "0", "]", "==", "_CPU_STR", ":", "\n", "            ", "_updater_raise", "(", "op", ",", "input_types", ",", "output_types", ")", "\n", "", "return", "(", "[", "_CPU_STR", "]", ",", "[", "_GPU_STR", "]", ")", "\n", "\n", "", "def", "_copy_gpu_to_cpu_updater", "(", "op", ",", "input_types", ",", "output_types", ")", ":", "\n", "        ", "if", "input_types", "[", "0", "]", "==", "_CPU_STR", "or", "output_types", "[", "0", "]", "==", "_GPU_STR", ":", "\n", "            ", "_updater_raise", "(", "op", ",", "input_types", ",", "output_types", ")", "\n", "", "return", "(", "[", "_GPU_STR", "]", ",", "[", "_CPU_STR", "]", ")", "\n", "\n", "", "def", "_other_ops_updater", "(", "op", ",", "input_types", ",", "output_types", ")", ":", "\n", "        ", "non_none_types", "=", "[", "x", "for", "x", "in", "input_types", "+", "output_types", "if", "x", "is", "not", "None", "]", "\n", "if", "len", "(", "non_none_types", ")", ">", "0", ":", "\n", "            ", "the_type", "=", "non_none_types", "[", "0", "]", "\n", "if", "not", "all", "(", "x", "==", "the_type", "for", "x", "in", "non_none_types", ")", ":", "\n", "                ", "_updater_raise", "(", "op", ",", "input_types", ",", "output_types", ")", "\n", "", "", "else", ":", "\n", "            ", "the_type", "=", "None", "\n", "", "return", "(", "[", "the_type", "for", "_", "in", "op", ".", "input", "]", ",", "[", "the_type", "for", "_", "in", "op", ".", "output", "]", ")", "\n", "\n", "", "def", "_device_updater", "(", "op", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "{", "\n", "\"CopyCPUToGPU\"", ":", "_copy_cpu_to_gpu_updater", ",", "\n", "\"CopyGPUToCPU\"", ":", "_copy_gpu_to_cpu_updater", ",", "\n", "}", ".", "get", "(", "op", ".", "type", ",", "_other_ops_updater", ")", "(", "op", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "return", "_generic_status_identifier", "(", "predict_net", ",", "_device_updater", ",", "known_status", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._modify_blob_names": [[491, 505], ["blob_list.extend", "copy.deepcopy", "shared._modify_blob_names._replace_list"], "function", ["None"], ["", "def", "_modify_blob_names", "(", "ops", ",", "blob_rename_f", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "\n", "def", "_replace_list", "(", "blob_list", ",", "replaced_list", ")", ":", "\n", "        ", "del", "blob_list", "[", ":", "]", "\n", "blob_list", ".", "extend", "(", "replaced_list", ")", "\n", "\n", "", "for", "x", "in", "ops", ":", "\n", "        ", "cur", "=", "copy", ".", "deepcopy", "(", "x", ")", "\n", "_replace_list", "(", "cur", ".", "input", ",", "list", "(", "map", "(", "blob_rename_f", ",", "cur", ".", "input", ")", ")", ")", "\n", "_replace_list", "(", "cur", ".", "output", ",", "list", "(", "map", "(", "blob_rename_f", ",", "cur", ".", "output", ")", ")", ")", "\n", "ret", ".", "append", "(", "cur", ")", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._rename_blob": [[507, 520], ["shared._rename_blob._list_to_str"], "function", ["None"], ["", "def", "_rename_blob", "(", "name", ",", "blob_sizes", ",", "blob_ranges", ")", ":", "\n", "    ", "def", "_list_to_str", "(", "bsize", ")", ":", "\n", "        ", "ret", "=", "\", \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "bsize", "]", ")", "\n", "ret", "=", "\"[\"", "+", "ret", "+", "\"]\"", "\n", "return", "ret", "\n", "\n", "", "ret", "=", "name", "\n", "if", "blob_sizes", "is", "not", "None", "and", "name", "in", "blob_sizes", ":", "\n", "        ", "ret", "+=", "\"\\n\"", "+", "_list_to_str", "(", "blob_sizes", "[", "name", "]", ")", "\n", "", "if", "blob_ranges", "is", "not", "None", "and", "name", "in", "blob_ranges", ":", "\n", "        ", "ret", "+=", "\"\\n\"", "+", "_list_to_str", "(", "blob_ranges", "[", "name", "]", ")", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.save_graph": [[523, 526], ["functools.partial", "shared.save_graph_base"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.save_graph_base"], ["", "def", "save_graph", "(", "net", ",", "file_name", ",", "graph_name", "=", "\"net\"", ",", "op_only", "=", "True", ",", "blob_sizes", "=", "None", ",", "blob_ranges", "=", "None", ")", ":", "\n", "    ", "blob_rename_f", "=", "functools", ".", "partial", "(", "_rename_blob", ",", "blob_sizes", "=", "blob_sizes", ",", "blob_ranges", "=", "blob_ranges", ")", "\n", "return", "save_graph_base", "(", "net", ",", "file_name", ",", "graph_name", ",", "op_only", ",", "blob_rename_f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.save_graph_base": [[528, 558], ["shared._modify_blob_names", "caffe2.python.net_drawer.GetPydotGraph", "caffe2.python.net_drawer.GetPydotGraphMinimal", "os.path.dirname", "os.path.exists", "os.makedirs", "os.path.splitext", "net_drawer.GetPydotGraphMinimal.write_png", "print", "os.path.basename", "net_drawer.GetPydotGraphMinimal.write_pdf", "net_drawer.GetPydotGraphMinimal.write_svg", "print"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._modify_blob_names"], ["", "def", "save_graph_base", "(", "net", ",", "file_name", ",", "graph_name", "=", "\"net\"", ",", "op_only", "=", "True", ",", "blob_rename_func", "=", "None", ")", ":", "\n", "    ", "graph", "=", "None", "\n", "ops", "=", "net", ".", "op", "\n", "if", "blob_rename_func", "is", "not", "None", ":", "\n", "        ", "ops", "=", "_modify_blob_names", "(", "ops", ",", "blob_rename_func", ")", "\n", "", "if", "not", "op_only", ":", "\n", "        ", "graph", "=", "net_drawer", ".", "GetPydotGraph", "(", "ops", ",", "graph_name", ",", "rankdir", "=", "\"TB\"", ")", "\n", "", "else", ":", "\n", "        ", "graph", "=", "net_drawer", ".", "GetPydotGraphMinimal", "(", "\n", "ops", ",", "graph_name", ",", "rankdir", "=", "\"TB\"", ",", "minimal_dependency", "=", "True", "\n", ")", "\n", "\n", "", "try", ":", "\n", "        ", "par_dir", "=", "os", ".", "path", ".", "dirname", "(", "file_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "par_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "par_dir", ")", "\n", "\n", "", "format", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "file_name", ")", ")", "[", "-", "1", "]", "\n", "if", "format", "==", "\".png\"", ":", "\n", "            ", "graph", ".", "write_png", "(", "file_name", ")", "\n", "", "elif", "format", "==", "\".pdf\"", ":", "\n", "            ", "graph", ".", "write_pdf", "(", "file_name", ")", "\n", "", "elif", "format", "==", "\".svg\"", ":", "\n", "            ", "graph", ".", "write_svg", "(", "file_name", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Incorrect format {}\"", ".", "format", "(", "format", ")", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "        ", "print", "(", "\"Error when writing graph to image {}\"", ".", "format", "(", "e", ")", ")", "\n", "\n", "", "return", "graph", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.group_norm_replace_aten_with_caffe2": [[563, 587], ["logger.info", "shared.get_pb_arg_vals", "op.arg.remove", "shared.get_pb_arg_vali", "shared.get_pb_arg_vali", "get_pb_arg_vals.decode", "shared.get_pb_arg", "op.arg.remove", "op.arg.remove", "shared.check_set_pb_arg", "shared.get_pb_arg", "shared.get_pb_arg"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vals", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.remove", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vali", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vali", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.remove", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.remove", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg"], ["", "def", "group_norm_replace_aten_with_caffe2", "(", "predict_net", ":", "caffe2_pb2", ".", "NetDef", ")", ":", "\n", "    ", "\"\"\"\n    For ONNX exported model, GroupNorm will be represented as ATen op,\n        this can be a drop in replacement from ATen to GroupNorm\n    \"\"\"", "\n", "count", "=", "0", "\n", "for", "op", "in", "predict_net", ".", "op", ":", "\n", "        ", "if", "op", ".", "type", "==", "\"ATen\"", ":", "\n", "            ", "op_name", "=", "get_pb_arg_vals", "(", "op", ",", "\"operator\"", ",", "None", ")", "# return byte in py3", "\n", "if", "op_name", "and", "op_name", ".", "decode", "(", ")", "==", "\"group_norm\"", ":", "\n", "                ", "op", ".", "arg", ".", "remove", "(", "get_pb_arg", "(", "op", ",", "\"operator\"", ")", ")", "\n", "\n", "if", "get_pb_arg_vali", "(", "op", ",", "\"cudnn_enabled\"", ",", "None", ")", ":", "\n", "                    ", "op", ".", "arg", ".", "remove", "(", "get_pb_arg", "(", "op", ",", "\"cudnn_enabled\"", ")", ")", "\n", "\n", "", "num_groups", "=", "get_pb_arg_vali", "(", "op", ",", "\"num_groups\"", ",", "None", ")", "\n", "if", "num_groups", "is", "not", "None", ":", "\n", "                    ", "op", ".", "arg", ".", "remove", "(", "get_pb_arg", "(", "op", ",", "\"num_groups\"", ")", ")", "\n", "check_set_pb_arg", "(", "op", ",", "\"group\"", ",", "\"i\"", ",", "num_groups", ")", "\n", "\n", "", "op", ".", "type", "=", "\"GroupNorm\"", "\n", "count", "+=", "1", "\n", "", "", "", "if", "count", ">", "1", ":", "\n", "        ", "logger", ".", "info", "(", "\"Replaced {} ATen operator to GroupNormOp\"", ".", "format", "(", "count", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias": [[592, 597], ["isinstance", "torch.ops._caffe2.AliasWithName", "torch.ops._caffe2.AliasWithName", "torch.onnx.is_in_onnx_export", "torch.onnx.is_in_onnx_export"], "function", ["None"], ["", "", "def", "alias", "(", "x", ",", "name", ",", "is_backward", "=", "False", ")", ":", "\n", "    ", "if", "not", "torch", ".", "onnx", ".", "is_in_onnx_export", "(", ")", ":", "\n", "        ", "return", "x", "\n", "", "assert", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", "\n", "return", "torch", ".", "ops", ".", "_caffe2", ".", "AliasWithName", "(", "x", ",", "name", ",", "is_backward", "=", "is_backward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.fuse_alias_placeholder": [[599, 622], ["enumerate", "predict_net.op.extend", "get_pb_arg_vals().decode", "bool", "shared.rename_op_input", "shared.rename_op_output", "new_ops.append", "len", "len", "shared.get_pb_arg_vali", "op.arg[].s.decode", "shared.get_pb_arg_vals"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.rename_op_input", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.rename_op_output", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vali", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vals"], ["", "def", "fuse_alias_placeholder", "(", "predict_net", ",", "init_net", ")", ":", "\n", "    ", "\"\"\"Remove AliasWithName placeholder and rename the input/output of it\"\"\"", "\n", "# First we finish all the re-naming", "\n", "for", "i", ",", "op", "in", "enumerate", "(", "predict_net", ".", "op", ")", ":", "\n", "        ", "if", "op", ".", "type", "==", "\"AliasWithName\"", ":", "\n", "            ", "assert", "len", "(", "op", ".", "input", ")", "==", "1", "\n", "assert", "len", "(", "op", ".", "output", ")", "==", "1", "\n", "name", "=", "get_pb_arg_vals", "(", "op", ",", "\"name\"", ",", "None", ")", ".", "decode", "(", ")", "\n", "is_backward", "=", "bool", "(", "get_pb_arg_vali", "(", "op", ",", "\"is_backward\"", ",", "0", ")", ")", "\n", "rename_op_input", "(", "predict_net", ",", "init_net", ",", "i", ",", "0", ",", "name", ",", "from_producer", "=", "is_backward", ")", "\n", "rename_op_output", "(", "predict_net", ",", "i", ",", "0", ",", "name", ")", "\n", "\n", "# Remove AliasWithName, should be very safe since it's a non-op", "\n", "", "", "new_ops", "=", "[", "]", "\n", "for", "op", "in", "predict_net", ".", "op", ":", "\n", "        ", "if", "op", ".", "type", "!=", "\"AliasWithName\"", ":", "\n", "            ", "new_ops", ".", "append", "(", "op", ")", "\n", "", "else", ":", "\n", "# safety check", "\n", "            ", "assert", "op", ".", "input", "==", "op", ".", "output", "\n", "assert", "op", ".", "input", "[", "0", "]", "==", "op", ".", "arg", "[", "0", "]", ".", "s", ".", "decode", "(", ")", "\n", "", "", "del", "predict_net", ".", "op", "[", ":", "]", "\n", "predict_net", ".", "op", ".", "extend", "(", "new_ops", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._rename_versioned_blob_in_proto": [[631, 660], ["zip", "range", "range", "start_versions.get", "range", "end_versions.get", "range", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "_rename_versioned_blob_in_proto", "(", "\n", "proto", ":", "caffe2_pb2", ".", "NetDef", ",", "\n", "old_name", ":", "str", ",", "\n", "new_name", ":", "str", ",", "\n", "version", ":", "int", ",", "\n", "ssa", ":", "List", "[", "Tuple", "[", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", ",", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", "]", "]", ",", "\n", "start_versions", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "end_versions", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", ")", ":", "\n", "    ", "\"\"\"In given proto, rename all blobs with matched version\"\"\"", "\n", "# Operater list", "\n", "for", "op", ",", "i_th_ssa", "in", "zip", "(", "proto", ".", "op", ",", "ssa", ")", ":", "\n", "        ", "versioned_inputs", ",", "versioned_outputs", "=", "i_th_ssa", "\n", "for", "i", "in", "range", "(", "len", "(", "op", ".", "input", ")", ")", ":", "\n", "            ", "if", "versioned_inputs", "[", "i", "]", "==", "(", "old_name", ",", "version", ")", ":", "\n", "                ", "op", ".", "input", "[", "i", "]", "=", "new_name", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "op", ".", "output", ")", ")", ":", "\n", "            ", "if", "versioned_outputs", "[", "i", "]", "==", "(", "old_name", ",", "version", ")", ":", "\n", "                ", "op", ".", "output", "[", "i", "]", "=", "new_name", "\n", "# external_input", "\n", "", "", "", "if", "start_versions", ".", "get", "(", "old_name", ",", "0", ")", "==", "version", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "proto", ".", "external_input", ")", ")", ":", "\n", "            ", "if", "proto", ".", "external_input", "[", "i", "]", "==", "old_name", ":", "\n", "                ", "proto", ".", "external_input", "[", "i", "]", "=", "new_name", "\n", "# external_output", "\n", "", "", "", "if", "end_versions", ".", "get", "(", "old_name", ",", "0", ")", "==", "version", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "proto", ".", "external_output", ")", ")", ":", "\n", "            ", "if", "proto", ".", "external_output", "[", "i", "]", "==", "old_name", ":", "\n", "                ", "proto", ".", "external_output", "[", "i", "]", "=", "new_name", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.rename_op_input": [[662, 726], ["isinstance", "isinstance", "caffe2.python.core.get_ssa", "caffe2.python.core.get_ssa", "shared._rename_versioned_blob_in_proto", "shared._rename_versioned_blob_in_proto", "copy.deepcopy", "shared.get_producer_map", "shared.rename_op_output", "shared.rename_op_input.contain_targets"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._rename_versioned_blob_in_proto", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._rename_versioned_blob_in_proto", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_producer_map", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.rename_op_output"], ["", "", "", "", "def", "rename_op_input", "(", "\n", "predict_net", ":", "caffe2_pb2", ".", "NetDef", ",", "\n", "init_net", ":", "caffe2_pb2", ".", "NetDef", ",", "\n", "op_id", ":", "int", ",", "\n", "input_id", ":", "int", ",", "\n", "new_name", ":", "str", ",", "\n", "from_producer", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Rename the op_id-th operator in predict_net, change it's input_id-th input's\n        name to the new_name. It also does automatic re-route and change\n        external_input and init_net if necessary.\n    - It requires the input is only consumed by this op.\n    - This function modifies predict_net and init_net in-place.\n    - When from_producer is enable, this also updates other operators that consumes\n        the same input. Be cautious because may trigger unintended behavior.\n    \"\"\"", "\n", "assert", "isinstance", "(", "predict_net", ",", "caffe2_pb2", ".", "NetDef", ")", "\n", "assert", "isinstance", "(", "init_net", ",", "caffe2_pb2", ".", "NetDef", ")", "\n", "\n", "init_net_ssa", ",", "init_net_versions", "=", "core", ".", "get_ssa", "(", "init_net", ")", "\n", "predict_net_ssa", ",", "predict_net_versions", "=", "core", ".", "get_ssa", "(", "\n", "predict_net", ",", "copy", ".", "deepcopy", "(", "init_net_versions", ")", "\n", ")", "\n", "\n", "versioned_inputs", ",", "versioned_outputs", "=", "predict_net_ssa", "[", "op_id", "]", "\n", "old_name", ",", "version", "=", "versioned_inputs", "[", "input_id", "]", "\n", "\n", "if", "from_producer", ":", "\n", "        ", "producer_map", "=", "get_producer_map", "(", "predict_net_ssa", ")", "\n", "if", "not", "(", "old_name", ",", "version", ")", "in", "producer_map", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"Can't find producer, the input {} is probably from\"", "\n", "\" init_net, this is not supported yet.\"", ".", "format", "(", "old_name", ")", "\n", ")", "\n", "", "producer", "=", "producer_map", "[", "(", "old_name", ",", "version", ")", "]", "\n", "rename_op_output", "(", "predict_net", ",", "producer", "[", "0", "]", ",", "producer", "[", "1", "]", ",", "new_name", ")", "\n", "return", "\n", "\n", "", "def", "contain_targets", "(", "op_ssa", ")", ":", "\n", "        ", "return", "(", "old_name", ",", "version", ")", "in", "op_ssa", "[", "0", "]", "\n", "\n", "", "is_consumer", "=", "[", "contain_targets", "(", "op_ssa", ")", "for", "op_ssa", "in", "predict_net_ssa", "]", "\n", "if", "sum", "(", "is_consumer", ")", ">", "1", ":", "\n", "        ", "raise", "IllegalGraphTransformError", "(", "\n", "(", "\n", "\"Input '{}' of operator(#{}) are consumed by other ops, please use\"", "\n", "+", "\" rename_op_output on the producer instead. Offending op: \\n{}\"", "\n", ")", ".", "format", "(", "old_name", ",", "op_id", ",", "predict_net", ".", "op", "[", "op_id", "]", ")", "\n", ")", "\n", "\n", "# update init_net", "\n", "", "_rename_versioned_blob_in_proto", "(", "\n", "init_net", ",", "old_name", ",", "new_name", ",", "version", ",", "init_net_ssa", ",", "{", "}", ",", "init_net_versions", "\n", ")", "\n", "# update predict_net", "\n", "_rename_versioned_blob_in_proto", "(", "\n", "predict_net", ",", "\n", "old_name", ",", "\n", "new_name", ",", "\n", "version", ",", "\n", "predict_net_ssa", ",", "\n", "init_net_versions", ",", "\n", "predict_net_versions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.rename_op_output": [[729, 747], ["isinstance", "caffe2.python.core.get_ssa", "shared._rename_versioned_blob_in_proto"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._rename_versioned_blob_in_proto"], ["", "def", "rename_op_output", "(", "predict_net", ":", "caffe2_pb2", ".", "NetDef", ",", "op_id", ":", "int", ",", "output_id", ":", "int", ",", "new_name", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    Rename the op_id-th operator in predict_net, change it's output_id-th input's\n        name to the new_name. It also does automatic re-route and change\n        external_output and if necessary.\n    - It allows multiple consumers of its output.\n    - This function modifies predict_net in-place, doesn't need init_net.\n    \"\"\"", "\n", "assert", "isinstance", "(", "predict_net", ",", "caffe2_pb2", ".", "NetDef", ")", "\n", "\n", "ssa", ",", "blob_versions", "=", "core", ".", "get_ssa", "(", "predict_net", ")", "\n", "\n", "versioned_inputs", ",", "versioned_outputs", "=", "ssa", "[", "op_id", "]", "\n", "old_name", ",", "version", "=", "versioned_outputs", "[", "output_id", "]", "\n", "\n", "# update predict_net", "\n", "_rename_versioned_blob_in_proto", "(", "\n", "predict_net", ",", "old_name", ",", "new_name", ",", "version", ",", "ssa", ",", "{", "}", ",", "blob_versions", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_sub_graph_external_input_output": [[750, 780], ["caffe2.python.core.get_ssa", "sum", "list", "range", "set", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "get_sub_graph_external_input_output", "(", "\n", "predict_net", ":", "caffe2_pb2", ".", "NetDef", ",", "sub_graph_op_indices", ":", "List", "[", "int", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", ",", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", "]", ":", "\n", "    ", "\"\"\"\n    Return the list of external input/output of sub-graph,\n    each element is tuple of the name and corresponding version in predict_net.\n\n    external input/output is defined the same way as caffe2 NetDef.\n    \"\"\"", "\n", "ssa", ",", "versions", "=", "core", ".", "get_ssa", "(", "predict_net", ")", "\n", "\n", "all_inputs", "=", "[", "]", "\n", "all_outputs", "=", "[", "]", "\n", "for", "op_id", "in", "sub_graph_op_indices", ":", "\n", "        ", "all_inputs", "+=", "[", "inp", "for", "inp", "in", "ssa", "[", "op_id", "]", "[", "0", "]", "if", "inp", "not", "in", "all_inputs", "]", "\n", "all_outputs", "+=", "list", "(", "ssa", "[", "op_id", "]", "[", "1", "]", ")", "# ssa output won't repeat", "\n", "\n", "# for versioned blobs, external inputs are just those blob in all_inputs", "\n", "# but not in all_outputs", "\n", "", "ext_inputs", "=", "[", "inp", "for", "inp", "in", "all_inputs", "if", "inp", "not", "in", "all_outputs", "]", "\n", "\n", "# external outputs are essentially outputs of this subgraph that are used", "\n", "# outside of this sub-graph (including predict_net.external_output)", "\n", "all_other_inputs", "=", "sum", "(", "\n", "(", "ssa", "[", "i", "]", "[", "0", "]", "for", "i", "in", "range", "(", "len", "(", "ssa", ")", ")", "if", "i", "not", "in", "sub_graph_op_indices", ")", ",", "\n", "[", "(", "outp", ",", "versions", "[", "outp", "]", ")", "for", "outp", "in", "predict_net", ".", "external_output", "]", ",", "\n", ")", "\n", "ext_outputs", "=", "[", "outp", "for", "outp", "in", "all_outputs", "if", "outp", "in", "set", "(", "all_other_inputs", ")", "]", "\n", "\n", "return", "ext_inputs", ",", "ext_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._get_dependency_chain": [[825, 853], ["shared.get_consumer_map", "shared.get_producer_map", "shared.DiGraph.from_ssa", "DiGraph.from_ssa.get_all_paths", "sorted", "min", "len", "logger.warning", "set().union", "set", "set"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_consumer_map", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_producer_map", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.DiGraph.from_ssa", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.DiGraph.get_all_paths", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "", "def", "_get_dependency_chain", "(", "ssa", ",", "versioned_target", ",", "versioned_source", ")", ":", "\n", "    ", "\"\"\"\n    Return the index list of relevant operator to produce target blob from source blob,\n        if there's no dependency, return empty list.\n    \"\"\"", "\n", "\n", "# finding all paths between nodes can be O(N!), thus we can only search", "\n", "# in the subgraph using the op starting from the first consumer of source blob", "\n", "# to the producer of the target blob.", "\n", "consumer_map", "=", "get_consumer_map", "(", "ssa", ")", "\n", "producer_map", "=", "get_producer_map", "(", "ssa", ")", "\n", "start_op", "=", "min", "(", "x", "[", "0", "]", "for", "x", "in", "consumer_map", "[", "versioned_source", "]", ")", "-", "15", "\n", "end_op", "=", "(", "\n", "producer_map", "[", "versioned_target", "]", "[", "0", "]", "+", "15", "if", "versioned_target", "in", "producer_map", "else", "start_op", "\n", ")", "\n", "sub_graph_ssa", "=", "ssa", "[", "start_op", ":", "end_op", "+", "1", "]", "\n", "if", "len", "(", "sub_graph_ssa", ")", ">", "30", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"Subgraph bebetween {} and {} is large (from op#{} to op#{}), it\"", "\n", "\" might take non-trival time to find all paths between them.\"", ".", "format", "(", "\n", "versioned_source", ",", "versioned_target", ",", "start_op", ",", "end_op", "\n", ")", "\n", ")", "\n", "\n", "", "dag", "=", "DiGraph", ".", "from_ssa", "(", "sub_graph_ssa", ")", "\n", "paths", "=", "dag", ".", "get_all_paths", "(", "versioned_source", ",", "versioned_target", ")", "# include two ends", "\n", "ops_in_paths", "=", "[", "[", "producer_map", "[", "blob", "]", "[", "0", "]", "for", "blob", "in", "path", "[", "1", ":", "]", "]", "for", "path", "in", "paths", "]", "\n", "return", "sorted", "(", "set", "(", ")", ".", "union", "(", "*", "[", "set", "(", "ops", ")", "for", "ops", "in", "ops_in_paths", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.identify_reshape_sub_graph": [[855, 880], ["caffe2.python.core.get_ssa", "enumerate", "shared._get_dependency_chain", "ret.append", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared._get_dependency_chain"], ["", "def", "identify_reshape_sub_graph", "(", "predict_net", ":", "caffe2_pb2", ".", "NetDef", ")", "->", "List", "[", "List", "[", "int", "]", "]", ":", "\n", "    ", "\"\"\"\n    Idenfity the reshape sub-graph in a protobuf.\n    The reshape sub-graph is defined as matching the following pattern:\n\n    (input_blob) -> Op_1 -> ... -> Op_N -> (new_shape) -\u2500\u2510\n        \u2514-------------------------------------------> Reshape -> (output_blob)\n\n    Return:\n        List of sub-graphs, each sub-graph is represented as a list of indices\n        of the relavent ops, [Op_1, Op_2, ..., Op_N, Reshape]\n    \"\"\"", "\n", "\n", "ssa", ",", "_", "=", "core", ".", "get_ssa", "(", "predict_net", ")", "\n", "\n", "ret", "=", "[", "]", "\n", "for", "i", ",", "op", "in", "enumerate", "(", "predict_net", ".", "op", ")", ":", "\n", "        ", "if", "op", ".", "type", "==", "\"Reshape\"", ":", "\n", "            ", "assert", "len", "(", "op", ".", "input", ")", "==", "2", "\n", "input_ssa", "=", "ssa", "[", "i", "]", "[", "0", "]", "\n", "data_source", "=", "input_ssa", "[", "0", "]", "\n", "shape_source", "=", "input_ssa", "[", "1", "]", "\n", "op_indices", "=", "_get_dependency_chain", "(", "ssa", ",", "shape_source", ",", "data_source", ")", "\n", "ret", ".", "append", "(", "op_indices", "+", "[", "i", "]", ")", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.remove_reshape_for_fc": [[882, 950], ["shared.identify_reshape_sub_graph", "copy.deepcopy", "copy.deepcopy.op.extend", "caffe2.python.core.get_ssa", "all", "logger.info", "shared.rename_op_output", "shared.get_sub_graph_external_input_output", "remove_op_ids.extend", "params_to_remove.extend", "logger.info", "copy.deepcopy.external_input.remove", "shared.get_sub_graph_external_input_output", "enumerate", "range", "sub_graphs_to_remove.append", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.identify_reshape_sub_graph", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.rename_op_output", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_sub_graph_external_input_output", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.remove", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_sub_graph_external_input_output"], ["", "def", "remove_reshape_for_fc", "(", "predict_net", ",", "params", ")", ":", "\n", "    ", "\"\"\"\n    In PyTorch nn.Linear has to take 2D tensor, this often leads to reshape\n        a 4D tensor to 2D by calling .view(). However this (dynamic) reshaping\n        doesn't work well with ONNX and Int8 tools, and cause using extra\n        ops (eg. ExpandDims) that might not be available on mobile.\n    Luckily Caffe2 supports 4D tensor for FC, so we can remove those reshape\n        after exporting ONNX model.\n    \"\"\"", "\n", "from", "caffe2", ".", "python", "import", "core", "\n", "\n", "# find all reshape sub-graph that can be removed, which is now all Reshape", "\n", "# sub-graph whose output is only consumed by FC.", "\n", "# TODO: to make it safer, we may need the actually value to better determine", "\n", "# if a Reshape before FC is removable.", "\n", "reshape_sub_graphs", "=", "identify_reshape_sub_graph", "(", "predict_net", ")", "\n", "sub_graphs_to_remove", "=", "[", "]", "\n", "for", "reshape_sub_graph", "in", "reshape_sub_graphs", ":", "\n", "        ", "reshape_op_id", "=", "reshape_sub_graph", "[", "-", "1", "]", "\n", "assert", "predict_net", ".", "op", "[", "reshape_op_id", "]", ".", "type", "==", "\"Reshape\"", "\n", "ssa", ",", "_", "=", "core", ".", "get_ssa", "(", "predict_net", ")", "\n", "reshape_output", "=", "ssa", "[", "reshape_op_id", "]", "[", "1", "]", "[", "0", "]", "\n", "consumers", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "ssa", ")", ")", "if", "reshape_output", "in", "ssa", "[", "i", "]", "[", "0", "]", "]", "\n", "if", "all", "(", "predict_net", ".", "op", "[", "consumer", "]", ".", "type", "==", "\"FC\"", "for", "consumer", "in", "consumers", ")", ":", "\n", "# safety check if the sub-graph is isolated, for this reshape sub-graph,", "\n", "# it means it has one non-param external input and one external output.", "\n", "            ", "ext_inputs", ",", "ext_outputs", "=", "get_sub_graph_external_input_output", "(", "\n", "predict_net", ",", "reshape_sub_graph", "\n", ")", "\n", "non_params_ext_inputs", "=", "[", "inp", "for", "inp", "in", "ext_inputs", "if", "inp", "[", "1", "]", "!=", "0", "]", "\n", "if", "len", "(", "non_params_ext_inputs", ")", "==", "1", "and", "len", "(", "ext_outputs", ")", "==", "1", ":", "\n", "                ", "sub_graphs_to_remove", ".", "append", "(", "reshape_sub_graph", ")", "\n", "\n", "# perform removing subgraph by:", "\n", "# 1: rename the Reshape's output to its input, then the graph can be", "\n", "#   seen as in-place itentify, meaning whose external input/output are the same.", "\n", "# 2: simply remove those ops.", "\n", "", "", "", "remove_op_ids", "=", "[", "]", "\n", "params_to_remove", "=", "[", "]", "\n", "for", "sub_graph", "in", "sub_graphs_to_remove", ":", "\n", "        ", "logger", ".", "info", "(", "\n", "\"Remove Reshape sub-graph:\\n{}\"", ".", "format", "(", "\n", "\"\"", ".", "join", "(", "[", "\"(#{:>4})\\n{}\"", ".", "format", "(", "i", ",", "predict_net", ".", "op", "[", "i", "]", ")", "for", "i", "in", "sub_graph", "]", ")", "\n", ")", "\n", ")", "\n", "reshape_op_id", "=", "sub_graph", "[", "-", "1", "]", "\n", "new_reshap_output", "=", "predict_net", ".", "op", "[", "reshape_op_id", "]", ".", "input", "[", "0", "]", "\n", "rename_op_output", "(", "predict_net", ",", "reshape_op_id", ",", "0", ",", "new_reshap_output", ")", "\n", "ext_inputs", ",", "ext_outputs", "=", "get_sub_graph_external_input_output", "(", "predict_net", ",", "sub_graph", ")", "\n", "non_params_ext_inputs", "=", "[", "inp", "for", "inp", "in", "ext_inputs", "if", "inp", "[", "1", "]", "!=", "0", "]", "\n", "params_ext_inputs", "=", "[", "inp", "for", "inp", "in", "ext_inputs", "if", "inp", "[", "1", "]", "==", "0", "]", "\n", "assert", "len", "(", "non_params_ext_inputs", ")", "==", "1", "and", "len", "(", "ext_outputs", ")", "==", "1", "\n", "assert", "ext_outputs", "[", "0", "]", "[", "0", "]", "==", "non_params_ext_inputs", "[", "0", "]", "[", "0", "]", "\n", "assert", "ext_outputs", "[", "0", "]", "[", "1", "]", "==", "non_params_ext_inputs", "[", "0", "]", "[", "1", "]", "+", "1", "\n", "remove_op_ids", ".", "extend", "(", "sub_graph", ")", "\n", "params_to_remove", ".", "extend", "(", "params_ext_inputs", ")", "\n", "\n", "", "predict_net", "=", "copy", ".", "deepcopy", "(", "predict_net", ")", "\n", "new_ops", "=", "[", "op", "for", "i", ",", "op", "in", "enumerate", "(", "predict_net", ".", "op", ")", "if", "i", "not", "in", "remove_op_ids", "]", "\n", "del", "predict_net", ".", "op", "[", ":", "]", "\n", "predict_net", ".", "op", ".", "extend", "(", "new_ops", ")", "\n", "for", "versioned_params", "in", "params_to_remove", ":", "\n", "        ", "name", "=", "versioned_params", "[", "0", "]", "\n", "logger", ".", "info", "(", "\"Remove params: {} from init_net and predict_net.external_input\"", ".", "format", "(", "name", ")", ")", "\n", "del", "params", "[", "name", "]", "\n", "predict_net", ".", "external_input", ".", "remove", "(", "name", ")", "\n", "\n", "", "return", "predict_net", ",", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.fuse_copy_between_cpu_and_gpu": [[952, 1008], ["shared.fuse_copy_between_cpu_and_gpu._fuse_once"], "function", ["None"], ["", "def", "fuse_copy_between_cpu_and_gpu", "(", "predict_net", ":", "caffe2_pb2", ".", "NetDef", ")", ":", "\n", "    ", "\"\"\"\n    In-place fuse extra copy ops between cpu/gpu for the following case:\n        a -CopyAToB-> b -CopyBToA> c1 -NextOp1-> d1\n                        -CopyBToA> c2 -NextOp2-> d2\n    The fused network will look like:\n        a -NextOp1-> d1\n          -NextOp2-> d2\n    \"\"\"", "\n", "\n", "_COPY_OPS", "=", "[", "\"CopyCPUToGPU\"", ",", "\"CopyGPUToCPU\"", "]", "\n", "\n", "def", "_fuse_once", "(", "predict_net", ")", ":", "\n", "        ", "ssa", ",", "blob_versions", "=", "core", ".", "get_ssa", "(", "predict_net", ")", "\n", "consumer_map", "=", "get_consumer_map", "(", "ssa", ")", "\n", "versioned_external_output", "=", "[", "\n", "(", "name", ",", "blob_versions", "[", "name", "]", ")", "for", "name", "in", "predict_net", ".", "external_output", "\n", "]", "\n", "\n", "for", "op_id", ",", "op", "in", "enumerate", "(", "predict_net", ".", "op", ")", ":", "\n", "            ", "if", "op", ".", "type", "in", "_COPY_OPS", ":", "\n", "                ", "fw_copy_versioned_output", "=", "ssa", "[", "op_id", "]", "[", "1", "]", "[", "0", "]", "\n", "consumer_ids", "=", "[", "x", "[", "0", "]", "for", "x", "in", "consumer_map", "[", "fw_copy_versioned_output", "]", "]", "\n", "reverse_op_type", "=", "_COPY_OPS", "[", "1", "-", "_COPY_OPS", ".", "index", "(", "op", ".", "type", ")", "]", "\n", "\n", "is_fusable", "=", "(", "\n", "len", "(", "consumer_ids", ")", ">", "0", "\n", "and", "fw_copy_versioned_output", "not", "in", "versioned_external_output", "\n", "and", "all", "(", "\n", "predict_net", ".", "op", "[", "_op_id", "]", ".", "type", "==", "reverse_op_type", "\n", "and", "ssa", "[", "_op_id", "]", "[", "1", "]", "[", "0", "]", "not", "in", "versioned_external_output", "\n", "for", "_op_id", "in", "consumer_ids", "\n", ")", "\n", ")", "\n", "\n", "if", "is_fusable", ":", "\n", "                    ", "for", "rv_copy_op_id", "in", "consumer_ids", ":", "\n", "# making each NextOp uses \"a\" directly and removing Copy ops", "\n", "                        ", "rs_copy_versioned_output", "=", "ssa", "[", "rv_copy_op_id", "]", "[", "1", "]", "[", "0", "]", "\n", "next_op_id", ",", "inp_id", "=", "consumer_map", "[", "rs_copy_versioned_output", "]", "[", "0", "]", "\n", "predict_net", ".", "op", "[", "next_op_id", "]", ".", "input", "[", "inp_id", "]", "=", "op", ".", "input", "[", "0", "]", "\n", "# remove CopyOps", "\n", "", "new_ops", "=", "[", "\n", "op", "\n", "for", "i", ",", "op", "in", "enumerate", "(", "predict_net", ".", "op", ")", "\n", "if", "i", "!=", "op_id", "and", "i", "not", "in", "consumer_ids", "\n", "]", "\n", "del", "predict_net", ".", "op", "[", ":", "]", "\n", "predict_net", ".", "op", ".", "extend", "(", "new_ops", ")", "\n", "return", "True", "\n", "\n", "", "", "", "return", "False", "\n", "\n", "# _fuse_once returns False is nothing can be fused", "\n", "", "while", "_fuse_once", "(", "predict_net", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.remove_dead_end_ops": [[1010, 1035], ["caffe2.python.core.get_ssa", "shared.get_consumer_map", "set", "reversed", "net_def.op.extend", "list", "all", "enumerate", "set.add", "enumerate", "shared.remove_dead_end_ops._is_dead_end"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_consumer_map", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "", "def", "remove_dead_end_ops", "(", "net_def", ":", "caffe2_pb2", ".", "NetDef", ")", ":", "\n", "    ", "\"\"\"remove ops if its output is not used or not in external_output\"\"\"", "\n", "ssa", ",", "versions", "=", "core", ".", "get_ssa", "(", "net_def", ")", "\n", "versioned_external_output", "=", "[", "(", "name", ",", "versions", "[", "name", "]", ")", "for", "name", "in", "net_def", ".", "external_output", "]", "\n", "consumer_map", "=", "get_consumer_map", "(", "ssa", ")", "\n", "removed_op_ids", "=", "set", "(", ")", "\n", "\n", "def", "_is_dead_end", "(", "versioned_blob", ")", ":", "\n", "        ", "return", "not", "(", "\n", "versioned_blob", "in", "versioned_external_output", "\n", "or", "(", "\n", "len", "(", "consumer_map", "[", "versioned_blob", "]", ")", ">", "0", "\n", "and", "all", "(", "x", "[", "0", "]", "not", "in", "removed_op_ids", "for", "x", "in", "consumer_map", "[", "versioned_blob", "]", ")", "\n", ")", "\n", ")", "\n", "\n", "", "for", "i", ",", "ssa_i", "in", "reversed", "(", "list", "(", "enumerate", "(", "ssa", ")", ")", ")", ":", "\n", "        ", "versioned_outputs", "=", "ssa_i", "[", "1", "]", "\n", "if", "all", "(", "_is_dead_end", "(", "outp", ")", "for", "outp", "in", "versioned_outputs", ")", ":", "\n", "            ", "removed_op_ids", ".", "add", "(", "i", ")", "\n", "\n", "# simply removing those deadend ops should have no effect to external_output", "\n", "", "", "new_ops", "=", "[", "op", "for", "i", ",", "op", "in", "enumerate", "(", "net_def", ".", "op", ")", "if", "i", "not", "in", "removed_op_ids", "]", "\n", "del", "net_def", ".", "op", "[", ":", "]", "\n", "net_def", ".", "op", ".", "extend", "(", "new_ops", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.Caffe2CompatibleConverter.__init__": [[32, 34], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "replaceCls", ")", ":", "\n", "        ", "self", ".", "replaceCls", "=", "replaceCls", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.Caffe2CompatibleConverter.create_from": [[35, 55], ["isinstance", "issubclass", "isinstance", "type"], "methods", ["None"], ["", "def", "create_from", "(", "self", ",", "module", ")", ":", "\n", "# update module's class to the new class", "\n", "        ", "assert", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "Module", ")", "\n", "if", "issubclass", "(", "self", ".", "replaceCls", ",", "GenericMixin", ")", ":", "\n", "# replaceCls should act as mixin, create a new class on-the-fly", "\n", "            ", "new_class", "=", "type", "(", "\n", "\"{}MixedWith{}\"", ".", "format", "(", "self", ".", "replaceCls", ".", "__name__", ",", "module", ".", "__class__", ".", "__name__", ")", ",", "\n", "(", "self", ".", "replaceCls", ",", "module", ".", "__class__", ")", ",", "\n", "{", "}", ",", "# {\"new_method\": lambda self: ...},", "\n", ")", "\n", "module", ".", "__class__", "=", "new_class", "\n", "", "else", ":", "\n", "# replaceCls is complete class, this allow arbitrary class swap", "\n", "            ", "module", ".", "__class__", "=", "self", ".", "replaceCls", "\n", "\n", "# initialize Caffe2Compatible", "\n", "", "if", "isinstance", "(", "module", ",", "Caffe2Compatible", ")", ":", "\n", "            ", "module", ".", "tensor_mode", "=", "False", "\n", "\n", "", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.ROIHeadsPatcher.__init__": [[115, 118], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "heads", ",", "use_heatmap_max_keypoint", ")", ":", "\n", "        ", "self", ".", "heads", "=", "heads", "\n", "self", ".", "use_heatmap_max_keypoint", "=", "use_heatmap_max_keypoint", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.ROIHeadsPatcher.mock_roi_heads": [[119, 153], ["getattr", "getattr", "caffe2_patch.mock_fastrcnn_outputs_inference", "contextlib.ExitStack", "caffe2_patch.mock_keypoint_rcnn_inference", "caffe2_patch.mock_mask_rcnn_inference", "stack.enter_context", "type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.mock_fastrcnn_outputs_inference", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.mock_keypoint_rcnn_inference", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.mock_mask_rcnn_inference"], ["", "@", "contextlib", ".", "contextmanager", "\n", "def", "mock_roi_heads", "(", "self", ",", "tensor_mode", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Patching several inference functions inside ROIHeads and its subclasses\n\n        Args:\n            tensor_mode (bool): whether the inputs/outputs are caffe2's tensor\n                format or not. Default to True.\n        \"\"\"", "\n", "# NOTE: this requries the `keypoint_rcnn_inference` and `mask_rcnn_inference`", "\n", "# are called inside the same file as BaseXxxHead due to using mock.patch.", "\n", "kpt_heads_mod", "=", "keypoint_head", ".", "BaseKeypointRCNNHead", ".", "__module__", "\n", "mask_head_mod", "=", "mask_head", ".", "BaseMaskRCNNHead", ".", "__module__", "\n", "\n", "mock_ctx_managers", "=", "[", "\n", "mock_fastrcnn_outputs_inference", "(", "\n", "tensor_mode", "=", "tensor_mode", ",", "\n", "check", "=", "True", ",", "\n", "box_predictor_type", "=", "type", "(", "self", ".", "heads", ".", "box_predictor", ")", ",", "\n", ")", "\n", "]", "\n", "if", "getattr", "(", "self", ".", "heads", ",", "\"keypoint_on\"", ",", "False", ")", ":", "\n", "            ", "mock_ctx_managers", "+=", "[", "\n", "mock_keypoint_rcnn_inference", "(", "\n", "tensor_mode", ",", "kpt_heads_mod", ",", "self", ".", "use_heatmap_max_keypoint", "\n", ")", "\n", "]", "\n", "", "if", "getattr", "(", "self", ".", "heads", ",", "\"mask_on\"", ",", "False", ")", ":", "\n", "            ", "mock_ctx_managers", "+=", "[", "mock_mask_rcnn_inference", "(", "tensor_mode", ",", "mask_head_mod", ")", "]", "\n", "\n", "", "with", "contextlib", ".", "ExitStack", "(", ")", "as", "stack", ":", "# python 3.3+", "\n", "            ", "for", "mgr", "in", "mock_ctx_managers", ":", "\n", "                ", "stack", ".", "enter_context", "(", "mgr", ")", "\n", "", "yield", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch": [[57, 68], ["model.named_children", "isinstance", "caffe2_patch.patch", "updater.create_from", "detectron2.modeling.proposal_generator.rpn.RPN", "detectron2.modeling.poolers.ROIPooler"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.Caffe2CompatibleConverter.create_from"], ["", "", "def", "patch", "(", "model", ",", "target", ",", "updater", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    recursively (post-order) update all modules with the target type and its\n    subclasses, make a initialization/composition/inheritance/... via the\n    updater.create_from.\n    \"\"\"", "\n", "for", "name", ",", "module", "in", "model", ".", "named_children", "(", ")", ":", "\n", "        ", "model", ".", "_modules", "[", "name", "]", "=", "patch", "(", "module", ",", "target", ",", "updater", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "if", "isinstance", "(", "model", ",", "target", ")", ":", "\n", "        ", "return", "updater", ".", "create_from", "(", "model", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch_generalized_rcnn": [[70, 76], ["caffe2_patch.patch", "caffe2_patch.patch", "ccc", "ccc"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch"], ["", "def", "patch_generalized_rcnn", "(", "model", ")", ":", "\n", "    ", "ccc", "=", "Caffe2CompatibleConverter", "\n", "model", "=", "patch", "(", "model", ",", "rpn", ".", "RPN", ",", "ccc", "(", "Caffe2RPN", ")", ")", "\n", "model", "=", "patch", "(", "model", ",", "poolers", ".", "ROIPooler", ",", "ccc", "(", "Caffe2ROIPooler", ")", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.mock_fastrcnn_outputs_inference": [[78, 91], ["unittest.mock.patch.object", "c10.Caffe2FastRCNNOutputsInference"], "function", ["None"], ["", "@", "contextlib", ".", "contextmanager", "\n", "def", "mock_fastrcnn_outputs_inference", "(", "\n", "tensor_mode", ",", "check", "=", "True", ",", "box_predictor_type", "=", "FastRCNNOutputLayers", "\n", ")", ":", "\n", "    ", "with", "mock", ".", "patch", ".", "object", "(", "\n", "box_predictor_type", ",", "\n", "\"inference\"", ",", "\n", "autospec", "=", "True", ",", "\n", "side_effect", "=", "Caffe2FastRCNNOutputsInference", "(", "tensor_mode", ")", ",", "\n", ")", "as", "mocked_func", ":", "\n", "        ", "yield", "\n", "", "if", "check", ":", "\n", "        ", "assert", "mocked_func", ".", "call_count", ">", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.mock_mask_rcnn_inference": [[93, 101], ["unittest.mock.patch", "c10.Caffe2MaskRCNNInference"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch"], ["", "", "@", "contextlib", ".", "contextmanager", "\n", "def", "mock_mask_rcnn_inference", "(", "tensor_mode", ",", "patched_module", ",", "check", "=", "True", ")", ":", "\n", "    ", "with", "mock", ".", "patch", "(", "\n", "\"{}.mask_rcnn_inference\"", ".", "format", "(", "patched_module", ")", ",", "side_effect", "=", "Caffe2MaskRCNNInference", "(", ")", "\n", ")", "as", "mocked_func", ":", "\n", "        ", "yield", "\n", "", "if", "check", ":", "\n", "        ", "assert", "mocked_func", ".", "call_count", ">", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.mock_keypoint_rcnn_inference": [[103, 112], ["unittest.mock.patch", "c10.Caffe2KeypointRCNNInference"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch"], ["", "", "@", "contextlib", ".", "contextmanager", "\n", "def", "mock_keypoint_rcnn_inference", "(", "tensor_mode", ",", "patched_module", ",", "use_heatmap_max_keypoint", ",", "check", "=", "True", ")", ":", "\n", "    ", "with", "mock", ".", "patch", "(", "\n", "\"{}.keypoint_rcnn_inference\"", ".", "format", "(", "patched_module", ")", ",", "\n", "side_effect", "=", "Caffe2KeypointRCNNInference", "(", "use_heatmap_max_keypoint", ")", ",", "\n", ")", "as", "mocked_func", ":", "\n", "        ", "yield", "\n", "", "if", "check", ":", "\n", "        ", "assert", "mocked_func", ".", "call_count", ">", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch.__init__": [[145, 156], ["super().__init__", "caffe2_modeling.Caffe2MetaArch.eval", "caffe2_modeling.set_caffe2_compatible_tensor_mode"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.set_caffe2_compatible_tensor_mode"], ["def", "__init__", "(", "self", ",", "cfg", ",", "torch_model", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            cfg (CfgNode):\n            torch_model (nn.Module): the detectron2 model (meta_arch) to be\n                converted.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_wrapped_model", "=", "torch_model", "\n", "self", ".", "eval", "(", ")", "\n", "set_caffe2_compatible_tensor_mode", "(", "self", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch.get_caffe2_inputs": [[157, 179], ["caffe2_modeling.convert_batched_inputs_to_c2_format"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.convert_batched_inputs_to_c2_format"], ["", "def", "get_caffe2_inputs", "(", "self", ",", "batched_inputs", ")", ":", "\n", "        ", "\"\"\"\n        Convert pytorch-style structured inputs to caffe2-style inputs that\n        are tuples of tensors.\n\n        Args:\n            batched_inputs (list[dict]): inputs to a detectron2 model\n                in its standard format. Each dict has \"image\" (CHW tensor), and optionally\n                \"height\" and \"width\".\n\n        Returns:\n            tuple[Tensor]:\n                tuple of tensors that will be the inputs to the\n                :meth:`forward` method. For existing models, the first\n                is an NCHW tensor (padded and batched); the second is\n                a im_info Nx3 tensor, where the rows are\n                (height, width, unused legacy parameter)\n        \"\"\"", "\n", "return", "convert_batched_inputs_to_c2_format", "(", "\n", "batched_inputs", ",", "\n", "self", ".", "_wrapped_model", ".", "backbone", ".", "size_divisibility", ",", "\n", "self", ".", "_wrapped_model", ".", "device", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch.encode_additional_info": [[181, 186], ["None"], "methods", ["None"], ["", "def", "encode_additional_info", "(", "self", ",", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "\"\"\"\n        Save extra metadata that will be used by inference in the output protobuf.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch.forward": [[187, 201], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        Run the forward in caffe2-style. It has to use caffe2-compatible ops\n        and the method will be used for tracing.\n\n        Args:\n            inputs (tuple[Tensor]): inputs defined by :meth:`get_caffe2_input`.\n                They will be the inputs of the converted caffe2 graph.\n\n        Returns:\n            tuple[Tensor]: output tensors. They will be the outputs of the\n                converted caffe2 graph.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch._caffe2_preprocess_image": [[202, 218], ["shared.alias", "shared.alias", "shared.alias", "detectron2.structures.ImageList"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias"], ["", "def", "_caffe2_preprocess_image", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        Caffe2 implementation of preprocess_image, which is called inside each MetaArch's forward.\n        It normalizes the input images, and the final caffe2 graph assumes the\n        inputs have been batched already.\n        \"\"\"", "\n", "data", ",", "im_info", "=", "inputs", "\n", "data", "=", "alias", "(", "data", ",", "\"data\"", ")", "\n", "im_info", "=", "alias", "(", "im_info", ",", "\"im_info\"", ")", "\n", "mean", ",", "std", "=", "self", ".", "_wrapped_model", ".", "pixel_mean", ",", "self", ".", "_wrapped_model", ".", "pixel_std", "\n", "normalized_data", "=", "(", "data", "-", "mean", ")", "/", "std", "\n", "normalized_data", "=", "alias", "(", "normalized_data", ",", "\"normalized_data\"", ")", "\n", "\n", "# Pack (data, im_info) into ImageList which is recognized by self.inference.", "\n", "images", "=", "ImageList", "(", "tensor", "=", "normalized_data", ",", "image_sizes", "=", "im_info", ")", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch.get_outputs_converter": [[219, 246], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_outputs_converter", "(", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "\"\"\"\n        Creates a function that converts outputs of the caffe2 model to\n        detectron2's standard format.\n        The function uses information in `predict_net` and `init_net` that are\n        available at inferene time. Therefore the function logic can be used in inference.\n\n        The returned function has the following signature:\n\n            def convert(batched_inputs, c2_inputs, c2_results) -> detectron2_outputs\n\n        Where\n\n            * batched_inputs (list[dict]): the original input format of the meta arch\n            * c2_inputs (tuple[Tensor]): the caffe2 inputs.\n            * c2_results (dict[str, Tensor]): the caffe2 output format,\n                corresponding to the outputs of the :meth:`forward` function.\n            * detectron2_outputs: the original output format of the meta arch.\n\n        This function can be used to compare the outputs of the original meta arch and\n        the converted caffe2 graph.\n\n        Returns:\n            callable: a callable of the above signature.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2GeneralizedRCNN.__init__": [[249, 256], ["isinstance", "caffe2_patch.patch_generalized_rcnn", "caffe2_modeling.Caffe2MetaArch.__init__", "caffe2_patch.ROIHeadsPatcher"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch_generalized_rcnn", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ",", "torch_model", ")", ":", "\n", "        ", "assert", "isinstance", "(", "torch_model", ",", "meta_arch", ".", "GeneralizedRCNN", ")", "\n", "torch_model", "=", "patch_generalized_rcnn", "(", "torch_model", ")", "\n", "super", "(", ")", ".", "__init__", "(", "cfg", ",", "torch_model", ")", "\n", "\n", "self", ".", "roi_heads_patcher", "=", "ROIHeadsPatcher", "(", "\n", "self", ".", "_wrapped_model", ".", "roi_heads", ",", "cfg", ".", "EXPORT_CAFFE2", ".", "USE_HEATMAP_MAX_KEYPOINT", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2GeneralizedRCNN.encode_additional_info": [[258, 265], ["shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "str.encode", "str"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "def", "encode_additional_info", "(", "self", ",", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "size_divisibility", "=", "self", ".", "_wrapped_model", ".", "backbone", ".", "size_divisibility", "\n", "check_set_pb_arg", "(", "predict_net", ",", "\"size_divisibility\"", ",", "\"i\"", ",", "size_divisibility", ")", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\"device\"", ",", "\"s\"", ",", "str", ".", "encode", "(", "str", "(", "self", ".", "_wrapped_model", ".", "device", ")", ",", "\"ascii\"", ")", "\n", ")", "\n", "check_set_pb_arg", "(", "predict_net", ",", "\"meta_architecture\"", ",", "\"s\"", ",", "b\"GeneralizedRCNN\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2GeneralizedRCNN.forward": [[266, 276], ["shared.mock_torch_nn_functional_interpolate", "caffe2_modeling.Caffe2GeneralizedRCNN._caffe2_preprocess_image", "caffe2_modeling.Caffe2GeneralizedRCNN._wrapped_model.backbone", "caffe2_modeling.Caffe2GeneralizedRCNN._wrapped_model.proposal_generator", "tuple", "caffe2_modeling.Caffe2GeneralizedRCNN._wrapped_model.inference", "caffe2_modeling.Caffe2GeneralizedRCNN.roi_heads_patcher.mock_roi_heads", "caffe2_modeling.Caffe2GeneralizedRCNN._wrapped_model.roi_heads", "detector_results[].flatten"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.mock_torch_nn_functional_interpolate", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch._caffe2_preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.ROIHeadsPatcher.mock_roi_heads", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "@", "mock_torch_nn_functional_interpolate", "(", ")", "\n", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "if", "not", "self", ".", "tensor_mode", ":", "\n", "            ", "return", "self", ".", "_wrapped_model", ".", "inference", "(", "inputs", ")", "\n", "", "images", "=", "self", ".", "_caffe2_preprocess_image", "(", "inputs", ")", "\n", "features", "=", "self", ".", "_wrapped_model", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "proposals", ",", "_", "=", "self", ".", "_wrapped_model", ".", "proposal_generator", "(", "images", ",", "features", ")", "\n", "with", "self", ".", "roi_heads_patcher", ".", "mock_roi_heads", "(", ")", ":", "\n", "            ", "detector_results", ",", "_", "=", "self", ".", "_wrapped_model", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ")", "\n", "", "return", "tuple", "(", "detector_results", "[", "0", "]", ".", "flatten", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2GeneralizedRCNN.get_outputs_converter": [[277, 286], ["caffe2_modeling.assemble_rcnn_outputs_by_name", "detectron2.modeling.meta_arch.GeneralizedRCNN._postprocess", "int", "int"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.assemble_rcnn_outputs_by_name", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN._postprocess"], ["", "@", "staticmethod", "\n", "def", "get_outputs_converter", "(", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "def", "f", "(", "batched_inputs", ",", "c2_inputs", ",", "c2_results", ")", ":", "\n", "            ", "_", ",", "im_info", "=", "c2_inputs", "\n", "image_sizes", "=", "[", "[", "int", "(", "im", "[", "0", "]", ")", ",", "int", "(", "im", "[", "1", "]", ")", "]", "for", "im", "in", "im_info", "]", "\n", "results", "=", "assemble_rcnn_outputs_by_name", "(", "image_sizes", ",", "c2_results", ")", "\n", "return", "meta_arch", ".", "GeneralizedRCNN", ".", "_postprocess", "(", "results", ",", "batched_inputs", ",", "image_sizes", ")", "\n", "\n", "", "return", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2PanopticFPN.__init__": [[289, 296], ["isinstance", "caffe2_patch.patch_generalized_rcnn", "caffe2_modeling.Caffe2MetaArch.__init__", "caffe2_patch.ROIHeadsPatcher"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch_generalized_rcnn", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ",", "torch_model", ")", ":", "\n", "        ", "assert", "isinstance", "(", "torch_model", ",", "meta_arch", ".", "PanopticFPN", ")", "\n", "torch_model", "=", "patch_generalized_rcnn", "(", "torch_model", ")", "\n", "super", "(", ")", ".", "__init__", "(", "cfg", ",", "torch_model", ")", "\n", "\n", "self", ".", "roi_heads_patcher", "=", "ROIHeadsPatcher", "(", "\n", "self", ".", "_wrapped_model", ".", "roi_heads", ",", "cfg", ".", "EXPORT_CAFFE2", ".", "USE_HEATMAP_MAX_KEYPOINT", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2PanopticFPN.forward": [[298, 313], ["shared.mock_torch_nn_functional_interpolate", "caffe2_modeling.Caffe2PanopticFPN._caffe2_preprocess_image", "caffe2_modeling.Caffe2PanopticFPN._wrapped_model.backbone", "caffe2_modeling.Caffe2PanopticFPN._wrapped_model.sem_seg_head", "shared.alias", "caffe2_modeling.Caffe2PanopticFPN._wrapped_model.proposal_generator", "caffe2_modeling.Caffe2PanopticFPN.roi_heads_patcher.mock_roi_heads", "caffe2_modeling.Caffe2PanopticFPN._wrapped_model.roi_heads", "tuple", "detector_results[].flatten"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.mock_torch_nn_functional_interpolate", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch._caffe2_preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.ROIHeadsPatcher.mock_roi_heads", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "@", "mock_torch_nn_functional_interpolate", "(", ")", "\n", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "assert", "self", ".", "tensor_mode", "\n", "images", "=", "self", ".", "_caffe2_preprocess_image", "(", "inputs", ")", "\n", "features", "=", "self", ".", "_wrapped_model", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "\n", "sem_seg_results", ",", "_", "=", "self", ".", "_wrapped_model", ".", "sem_seg_head", "(", "features", ")", "\n", "sem_seg_results", "=", "alias", "(", "sem_seg_results", ",", "\"sem_seg\"", ")", "\n", "\n", "proposals", ",", "_", "=", "self", ".", "_wrapped_model", ".", "proposal_generator", "(", "images", ",", "features", ")", "\n", "\n", "with", "self", ".", "roi_heads_patcher", ".", "mock_roi_heads", "(", "self", ".", "tensor_mode", ")", ":", "\n", "            ", "detector_results", ",", "_", "=", "self", ".", "_wrapped_model", ".", "roi_heads", "(", "images", ",", "features", ",", "proposals", ")", "\n", "\n", "", "return", "tuple", "(", "detector_results", "[", "0", "]", ".", "flatten", "(", ")", ")", "+", "(", "sem_seg_results", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2PanopticFPN.encode_additional_info": [[314, 340], ["shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "str.encode", "caffe2_modeling._cast_to_f32", "caffe2_modeling._cast_to_f32", "str"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling._cast_to_f32", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling._cast_to_f32"], ["", "def", "encode_additional_info", "(", "self", ",", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "size_divisibility", "=", "self", ".", "_wrapped_model", ".", "backbone", ".", "size_divisibility", "\n", "check_set_pb_arg", "(", "predict_net", ",", "\"size_divisibility\"", ",", "\"i\"", ",", "size_divisibility", ")", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\"device\"", ",", "\"s\"", ",", "str", ".", "encode", "(", "str", "(", "self", ".", "_wrapped_model", ".", "device", ")", ",", "\"ascii\"", ")", "\n", ")", "\n", "check_set_pb_arg", "(", "predict_net", ",", "\"meta_architecture\"", ",", "\"s\"", ",", "b\"PanopticFPN\"", ")", "\n", "\n", "# Inference parameters:", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\n", "\"combine_overlap_threshold\"", ",", "\n", "\"f\"", ",", "\n", "_cast_to_f32", "(", "self", ".", "_wrapped_model", ".", "combine_overlap_thresh", ")", ",", "\n", ")", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\n", "\"combine_stuff_area_limit\"", ",", "\n", "\"i\"", ",", "\n", "self", ".", "_wrapped_model", ".", "combine_stuff_area_thresh", ",", "\n", ")", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\n", "\"combine_instances_confidence_threshold\"", ",", "\n", "\"f\"", ",", "\n", "_cast_to_f32", "(", "self", ".", "_wrapped_model", ".", "combine_instances_score_thresh", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2PanopticFPN.get_outputs_converter": [[342, 381], ["shared.get_pb_arg_valf", "shared.get_pb_arg_vali", "shared.get_pb_arg_valf", "caffe2_modeling.assemble_rcnn_outputs_by_name", "zip", "input_per_image.get", "input_per_image.get", "detectron2.modeling.postprocessing.sem_seg_postprocess", "detectron2.modeling.postprocessing.detector_postprocess", "processed_results.append", "detectron2.modeling.meta_arch.panoptic_fpn.combine_semantic_and_instance_outputs", "int", "int", "detectron2.modeling.postprocessing.sem_seg_postprocess.argmax"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_valf", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vali", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_valf", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.assemble_rcnn_outputs_by_name", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.sem_seg_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.panoptic_fpn.combine_semantic_and_instance_outputs"], ["", "@", "staticmethod", "\n", "def", "get_outputs_converter", "(", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "combine_overlap_threshold", "=", "get_pb_arg_valf", "(", "predict_net", ",", "\"combine_overlap_threshold\"", ",", "None", ")", "\n", "combine_stuff_area_limit", "=", "get_pb_arg_vali", "(", "predict_net", ",", "\"combine_stuff_area_limit\"", ",", "None", ")", "\n", "combine_instances_confidence_threshold", "=", "get_pb_arg_valf", "(", "\n", "predict_net", ",", "\"combine_instances_confidence_threshold\"", ",", "None", "\n", ")", "\n", "\n", "def", "f", "(", "batched_inputs", ",", "c2_inputs", ",", "c2_results", ")", ":", "\n", "            ", "_", ",", "im_info", "=", "c2_inputs", "\n", "image_sizes", "=", "[", "[", "int", "(", "im", "[", "0", "]", ")", ",", "int", "(", "im", "[", "1", "]", ")", "]", "for", "im", "in", "im_info", "]", "\n", "detector_results", "=", "assemble_rcnn_outputs_by_name", "(", "\n", "image_sizes", ",", "c2_results", ",", "force_mask_on", "=", "True", "\n", ")", "\n", "sem_seg_results", "=", "c2_results", "[", "\"sem_seg\"", "]", "\n", "\n", "# copied from meta_arch/panoptic_fpn.py ...", "\n", "processed_results", "=", "[", "]", "\n", "for", "sem_seg_result", ",", "detector_result", ",", "input_per_image", ",", "image_size", "in", "zip", "(", "\n", "sem_seg_results", ",", "detector_results", ",", "batched_inputs", ",", "image_sizes", "\n", ")", ":", "\n", "                ", "height", "=", "input_per_image", ".", "get", "(", "\"height\"", ",", "image_size", "[", "0", "]", ")", "\n", "width", "=", "input_per_image", ".", "get", "(", "\"width\"", ",", "image_size", "[", "1", "]", ")", "\n", "sem_seg_r", "=", "sem_seg_postprocess", "(", "sem_seg_result", ",", "image_size", ",", "height", ",", "width", ")", "\n", "detector_r", "=", "detector_postprocess", "(", "detector_result", ",", "height", ",", "width", ")", "\n", "\n", "processed_results", ".", "append", "(", "{", "\"sem_seg\"", ":", "sem_seg_r", ",", "\"instances\"", ":", "detector_r", "}", ")", "\n", "\n", "panoptic_r", "=", "combine_semantic_and_instance_outputs", "(", "\n", "detector_r", ",", "\n", "sem_seg_r", ".", "argmax", "(", "dim", "=", "0", ")", ",", "\n", "combine_overlap_threshold", ",", "\n", "combine_stuff_area_limit", ",", "\n", "combine_instances_confidence_threshold", ",", "\n", ")", "\n", "processed_results", "[", "-", "1", "]", "[", "\"panoptic_seg\"", "]", "=", "panoptic_r", "\n", "", "return", "processed_results", "\n", "\n", "", "return", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2RetinaNet.__init__": [[384, 387], ["isinstance", "caffe2_modeling.Caffe2MetaArch.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "cfg", ",", "torch_model", ")", ":", "\n", "        ", "assert", "isinstance", "(", "torch_model", ",", "meta_arch", ".", "RetinaNet", ")", "\n", "super", "(", ")", ".", "__init__", "(", "cfg", ",", "torch_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2RetinaNet.forward": [[388, 409], ["shared.mock_torch_nn_functional_interpolate", "caffe2_modeling.Caffe2RetinaNet._caffe2_preprocess_image", "caffe2_modeling.Caffe2RetinaNet._wrapped_model.backbone", "enumerate", "caffe2_modeling.Caffe2RetinaNet._wrapped_model.head", "enumerate", "tuple", "shared.alias", "return_tensors.append", "zip", "return_tensors.append", "return_tensors.append", "shared.alias", "shared.alias"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.mock_torch_nn_functional_interpolate", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch._caffe2_preprocess_image", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias"], ["", "@", "mock_torch_nn_functional_interpolate", "(", ")", "\n", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "assert", "self", ".", "tensor_mode", "\n", "images", "=", "self", ".", "_caffe2_preprocess_image", "(", "inputs", ")", "\n", "\n", "# explicitly return the images sizes to avoid removing \"im_info\" by ONNX", "\n", "# since it's not used in the forward path", "\n", "return_tensors", "=", "[", "images", ".", "image_sizes", "]", "\n", "\n", "features", "=", "self", ".", "_wrapped_model", ".", "backbone", "(", "images", ".", "tensor", ")", "\n", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "_wrapped_model", ".", "head_in_features", "]", "\n", "for", "i", ",", "feature_i", "in", "enumerate", "(", "features", ")", ":", "\n", "            ", "features", "[", "i", "]", "=", "alias", "(", "feature_i", ",", "\"feature_{}\"", ".", "format", "(", "i", ")", ",", "is_backward", "=", "True", ")", "\n", "return_tensors", ".", "append", "(", "features", "[", "i", "]", ")", "\n", "\n", "", "pred_logits", ",", "pred_anchor_deltas", "=", "self", ".", "_wrapped_model", ".", "head", "(", "features", ")", "\n", "for", "i", ",", "(", "box_cls_i", ",", "box_delta_i", ")", "in", "enumerate", "(", "zip", "(", "pred_logits", ",", "pred_anchor_deltas", ")", ")", ":", "\n", "            ", "return_tensors", ".", "append", "(", "alias", "(", "box_cls_i", ",", "\"box_cls_{}\"", ".", "format", "(", "i", ")", ")", ")", "\n", "return_tensors", ".", "append", "(", "alias", "(", "box_delta_i", ",", "\"box_delta_{}\"", ".", "format", "(", "i", ")", ")", ")", "\n", "\n", "", "return", "tuple", "(", "return_tensors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2RetinaNet.encode_additional_info": [[410, 442], ["shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "shared.check_set_pb_arg", "caffe2_modeling.Caffe2RetinaNet._encode_anchor_generator_cfg", "str.encode", "caffe2_modeling._cast_to_f32", "caffe2_modeling._cast_to_f32", "str", "caffe2_modeling._cast_to_f32"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2RetinaNet._encode_anchor_generator_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling._cast_to_f32", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling._cast_to_f32", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling._cast_to_f32"], ["", "def", "encode_additional_info", "(", "self", ",", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "size_divisibility", "=", "self", ".", "_wrapped_model", ".", "backbone", ".", "size_divisibility", "\n", "check_set_pb_arg", "(", "predict_net", ",", "\"size_divisibility\"", ",", "\"i\"", ",", "size_divisibility", ")", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\"device\"", ",", "\"s\"", ",", "str", ".", "encode", "(", "str", "(", "self", ".", "_wrapped_model", ".", "device", ")", ",", "\"ascii\"", ")", "\n", ")", "\n", "check_set_pb_arg", "(", "predict_net", ",", "\"meta_architecture\"", ",", "\"s\"", ",", "b\"RetinaNet\"", ")", "\n", "\n", "# Inference parameters:", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\"score_threshold\"", ",", "\"f\"", ",", "_cast_to_f32", "(", "self", ".", "_wrapped_model", ".", "test_score_thresh", ")", "\n", ")", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\"topk_candidates\"", ",", "\"i\"", ",", "self", ".", "_wrapped_model", ".", "test_topk_candidates", "\n", ")", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\"nms_threshold\"", ",", "\"f\"", ",", "_cast_to_f32", "(", "self", ".", "_wrapped_model", ".", "test_nms_thresh", ")", "\n", ")", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\n", "\"max_detections_per_image\"", ",", "\n", "\"i\"", ",", "\n", "self", ".", "_wrapped_model", ".", "max_detections_per_image", ",", "\n", ")", "\n", "\n", "check_set_pb_arg", "(", "\n", "predict_net", ",", "\n", "\"bbox_reg_weights\"", ",", "\n", "\"floats\"", ",", "\n", "[", "_cast_to_f32", "(", "w", ")", "for", "w", "in", "self", ".", "_wrapped_model", ".", "box2box_transform", ".", "weights", "]", ",", "\n", ")", "\n", "self", ".", "_encode_anchor_generator_cfg", "(", "predict_net", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2RetinaNet._encode_anchor_generator_cfg": [[443, 451], ["io.BytesIO", "torch.save", "io.BytesIO.getvalue", "shared.check_set_pb_arg"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.check_set_pb_arg"], ["", "def", "_encode_anchor_generator_cfg", "(", "self", ",", "predict_net", ")", ":", "\n", "# serialize anchor_generator for future use", "\n", "        ", "serialized_anchor_generator", "=", "io", ".", "BytesIO", "(", ")", "\n", "torch", ".", "save", "(", "self", ".", "_wrapped_model", ".", "anchor_generator", ",", "serialized_anchor_generator", ")", "\n", "# Ideally we can put anchor generating inside the model, then we don't", "\n", "# need to store this information.", "\n", "bytes", "=", "serialized_anchor_generator", ".", "getvalue", "(", ")", "\n", "check_set_pb_arg", "(", "predict_net", ",", "\"serialized_anchor_generator\"", ",", "\"s\"", ",", "bytes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2RetinaNet.get_outputs_converter": [[452, 497], ["types.SimpleNamespace", "io.BytesIO", "torch.load", "shared.get_pb_arg_floats", "detectron2.modeling.box_regression.Box2BoxTransform", "shared.get_pb_arg_valf", "shared.get_pb_arg_vali", "shared.get_pb_arg_valf", "shared.get_pb_arg_vali", "functools.partial", "functools.partial", "shared.get_pb_arg_vals", "len", "types.SimpleNamespace.anchor_generator", "types.SimpleNamespace.inference", "detectron2.modeling.meta_arch.GeneralizedRCNN._postprocess", "tuple", "detectron2.modeling.meta_arch.retinanet.permute_to_N_HWA_K", "detectron2.modeling.meta_arch.retinanet.permute_to_N_HWA_K", "int", "int", "range", "range", "x.clone", "c2_results.keys", "x.startswith"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_floats", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_valf", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vali", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_valf", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vali", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vals", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.clip_rcnn.PretrainFastRCNN._postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.permute_to_N_HWA_K", "home.repos.pwc.inspect_result.microsoft_regionclip.meta_arch.retinanet.permute_to_N_HWA_K", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["", "@", "staticmethod", "\n", "def", "get_outputs_converter", "(", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "self", "=", "types", ".", "SimpleNamespace", "(", ")", "\n", "serialized_anchor_generator", "=", "io", ".", "BytesIO", "(", "\n", "get_pb_arg_vals", "(", "predict_net", ",", "\"serialized_anchor_generator\"", ",", "None", ")", "\n", ")", "\n", "self", ".", "anchor_generator", "=", "torch", ".", "load", "(", "serialized_anchor_generator", ")", "\n", "bbox_reg_weights", "=", "get_pb_arg_floats", "(", "predict_net", ",", "\"bbox_reg_weights\"", ",", "None", ")", "\n", "self", ".", "box2box_transform", "=", "Box2BoxTransform", "(", "weights", "=", "tuple", "(", "bbox_reg_weights", ")", ")", "\n", "self", ".", "test_score_thresh", "=", "get_pb_arg_valf", "(", "predict_net", ",", "\"score_threshold\"", ",", "None", ")", "\n", "self", ".", "test_topk_candidates", "=", "get_pb_arg_vali", "(", "predict_net", ",", "\"topk_candidates\"", ",", "None", ")", "\n", "self", ".", "test_nms_thresh", "=", "get_pb_arg_valf", "(", "predict_net", ",", "\"nms_threshold\"", ",", "None", ")", "\n", "self", ".", "max_detections_per_image", "=", "get_pb_arg_vali", "(", "\n", "predict_net", ",", "\"max_detections_per_image\"", ",", "None", "\n", ")", "\n", "\n", "# hack to reuse inference code from RetinaNet", "\n", "self", ".", "inference", "=", "functools", ".", "partial", "(", "meta_arch", ".", "RetinaNet", ".", "inference", ",", "self", ")", "\n", "self", ".", "inference_single_image", "=", "functools", ".", "partial", "(", "\n", "meta_arch", ".", "RetinaNet", ".", "inference_single_image", ",", "self", "\n", ")", "\n", "\n", "def", "f", "(", "batched_inputs", ",", "c2_inputs", ",", "c2_results", ")", ":", "\n", "            ", "_", ",", "im_info", "=", "c2_inputs", "\n", "image_sizes", "=", "[", "[", "int", "(", "im", "[", "0", "]", ")", ",", "int", "(", "im", "[", "1", "]", ")", "]", "for", "im", "in", "im_info", "]", "\n", "\n", "num_features", "=", "len", "(", "[", "x", "for", "x", "in", "c2_results", ".", "keys", "(", ")", "if", "x", ".", "startswith", "(", "\"box_cls_\"", ")", "]", ")", "\n", "pred_logits", "=", "[", "c2_results", "[", "\"box_cls_{}\"", ".", "format", "(", "i", ")", "]", "for", "i", "in", "range", "(", "num_features", ")", "]", "\n", "pred_anchor_deltas", "=", "[", "c2_results", "[", "\"box_delta_{}\"", ".", "format", "(", "i", ")", "]", "for", "i", "in", "range", "(", "num_features", ")", "]", "\n", "\n", "# For each feature level, feature should have the same batch size and", "\n", "# spatial dimension as the box_cls and box_delta.", "\n", "dummy_features", "=", "[", "x", ".", "clone", "(", ")", "[", ":", ",", "0", ":", "0", ",", ":", ",", ":", "]", "for", "x", "in", "pred_logits", "]", "\n", "anchors", "=", "self", ".", "anchor_generator", "(", "dummy_features", ")", "\n", "\n", "# self.num_classess can be inferred", "\n", "self", ".", "num_classes", "=", "pred_logits", "[", "0", "]", ".", "shape", "[", "1", "]", "//", "(", "pred_anchor_deltas", "[", "0", "]", ".", "shape", "[", "1", "]", "//", "4", ")", "\n", "\n", "pred_logits", "=", "[", "permute_to_N_HWA_K", "(", "x", ",", "self", ".", "num_classes", ")", "for", "x", "in", "pred_logits", "]", "\n", "pred_anchor_deltas", "=", "[", "permute_to_N_HWA_K", "(", "x", ",", "4", ")", "for", "x", "in", "pred_anchor_deltas", "]", "\n", "\n", "results", "=", "self", ".", "inference", "(", "anchors", ",", "pred_logits", ",", "pred_anchor_deltas", ",", "image_sizes", ")", "\n", "return", "meta_arch", ".", "GeneralizedRCNN", ".", "_postprocess", "(", "results", ",", "batched_inputs", ",", "image_sizes", ")", "\n", "\n", "", "return", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.assemble_rcnn_outputs_by_name": [[30, 96], ["tensor_outputs.get", "class_nms.to", "tensor_outputs.get", "tensor_outputs.get", "tensor_outputs.get", "detectron2.structures.Instances", "NotImplementedError", "len", "detectron2.structures.RotatedBoxes", "detectron2.structures.Boxes", "torch.arange", "torch.zeros", "keypoints_tensor.transpose", "detectron2.modeling.roi_heads.keypoint_head.keypoint_rcnn_inference"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.keypoint_head.keypoint_rcnn_inference"], ["def", "assemble_rcnn_outputs_by_name", "(", "image_sizes", ",", "tensor_outputs", ",", "force_mask_on", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    A function to assemble caffe2 model's outputs (i.e. Dict[str, Tensor])\n    to detectron2's format (i.e. list of Instances instance).\n    This only works when the model follows the Caffe2 detectron's naming convention.\n\n    Args:\n        image_sizes (List[List[int, int]]): [H, W] of every image.\n        tensor_outputs (Dict[str, Tensor]): external_output to its tensor.\n\n        force_mask_on (Bool): if true, the it make sure there'll be pred_masks even\n            if the mask is not found from tensor_outputs (usually due to model crash)\n    \"\"\"", "\n", "\n", "results", "=", "[", "Instances", "(", "image_size", ")", "for", "image_size", "in", "image_sizes", "]", "\n", "\n", "batch_splits", "=", "tensor_outputs", ".", "get", "(", "\"batch_splits\"", ",", "None", ")", "\n", "if", "batch_splits", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "", "assert", "len", "(", "image_sizes", ")", "==", "1", "\n", "result", "=", "results", "[", "0", "]", "\n", "\n", "bbox_nms", "=", "tensor_outputs", "[", "\"bbox_nms\"", "]", "\n", "score_nms", "=", "tensor_outputs", "[", "\"score_nms\"", "]", "\n", "class_nms", "=", "tensor_outputs", "[", "\"class_nms\"", "]", "\n", "# Detection will always success because Conv support 0-batch", "\n", "assert", "bbox_nms", "is", "not", "None", "\n", "assert", "score_nms", "is", "not", "None", "\n", "assert", "class_nms", "is", "not", "None", "\n", "if", "bbox_nms", ".", "shape", "[", "1", "]", "==", "5", ":", "\n", "        ", "result", ".", "pred_boxes", "=", "RotatedBoxes", "(", "bbox_nms", ")", "\n", "", "else", ":", "\n", "        ", "result", ".", "pred_boxes", "=", "Boxes", "(", "bbox_nms", ")", "\n", "", "result", ".", "scores", "=", "score_nms", "\n", "result", ".", "pred_classes", "=", "class_nms", ".", "to", "(", "torch", ".", "int64", ")", "\n", "\n", "mask_fcn_probs", "=", "tensor_outputs", ".", "get", "(", "\"mask_fcn_probs\"", ",", "None", ")", "\n", "if", "mask_fcn_probs", "is", "not", "None", ":", "\n", "# finish the mask pred", "\n", "        ", "mask_probs_pred", "=", "mask_fcn_probs", "\n", "num_masks", "=", "mask_probs_pred", ".", "shape", "[", "0", "]", "\n", "class_pred", "=", "result", ".", "pred_classes", "\n", "indices", "=", "torch", ".", "arange", "(", "num_masks", ",", "device", "=", "class_pred", ".", "device", ")", "\n", "mask_probs_pred", "=", "mask_probs_pred", "[", "indices", ",", "class_pred", "]", "[", ":", ",", "None", "]", "\n", "result", ".", "pred_masks", "=", "mask_probs_pred", "\n", "", "elif", "force_mask_on", ":", "\n", "# NOTE: there's no way to know the height/width of mask here, it won't be", "\n", "# used anyway when batch size is 0, so just set them to 0.", "\n", "        ", "result", ".", "pred_masks", "=", "torch", ".", "zeros", "(", "[", "0", ",", "1", ",", "0", ",", "0", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "\n", "", "keypoints_out", "=", "tensor_outputs", ".", "get", "(", "\"keypoints_out\"", ",", "None", ")", "\n", "kps_score", "=", "tensor_outputs", ".", "get", "(", "\"kps_score\"", ",", "None", ")", "\n", "if", "keypoints_out", "is", "not", "None", ":", "\n", "# keypoints_out: [N, 4, #kypoints], where 4 is in order of (x, y, score, prob)", "\n", "        ", "keypoints_tensor", "=", "keypoints_out", "\n", "# NOTE: it's possible that prob is not calculated if \"should_output_softmax\"", "\n", "# is set to False in HeatmapMaxKeypoint, so just using raw score, seems", "\n", "# it doesn't affect mAP. TODO: check more carefully.", "\n", "keypoint_xyp", "=", "keypoints_tensor", ".", "transpose", "(", "1", ",", "2", ")", "[", ":", ",", ":", ",", "[", "0", ",", "1", ",", "2", "]", "]", "\n", "result", ".", "pred_keypoints", "=", "keypoint_xyp", "\n", "", "elif", "kps_score", "is", "not", "None", ":", "\n", "# keypoint heatmap to sparse data structure", "\n", "        ", "pred_keypoint_logits", "=", "kps_score", "\n", "keypoint_head", ".", "keypoint_rcnn_inference", "(", "pred_keypoint_logits", ",", "[", "result", "]", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling._cast_to_f32": [[98, 100], ["struct.unpack", "struct.pack"], "function", ["None"], ["", "def", "_cast_to_f32", "(", "f64", ")", ":", "\n", "    ", "return", "struct", ".", "unpack", "(", "\"f\"", ",", "struct", ".", "pack", "(", "\"f\"", ",", "f64", ")", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.set_caffe2_compatible_tensor_mode": [[102, 108], ["model.apply", "isinstance"], "function", ["None"], ["", "def", "set_caffe2_compatible_tensor_mode", "(", "model", ",", "enable", "=", "True", ")", ":", "\n", "    ", "def", "_fn", "(", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "Caffe2Compatible", ")", ":", "\n", "            ", "m", ".", "tensor_mode", "=", "enable", "\n", "\n", "", "", "model", ".", "apply", "(", "_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.convert_batched_inputs_to_c2_format": [[110, 136], ["all", "all", "detectron2.structures.ImageList.from_tensors", "zip", "torch.Tensor", "input_per_image.get", "input_per_image.get", "torch.Tensor.append", "ImageList.from_tensors.tensor.to", "torch.Tensor.to", "isinstance", "x[].dim"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.image_list.ImageList.from_tensors", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "convert_batched_inputs_to_c2_format", "(", "batched_inputs", ",", "size_divisibility", ",", "device", ")", ":", "\n", "    ", "\"\"\"\n    See get_caffe2_inputs() below.\n    \"\"\"", "\n", "assert", "all", "(", "isinstance", "(", "x", ",", "dict", ")", "for", "x", "in", "batched_inputs", ")", "\n", "assert", "all", "(", "x", "[", "\"image\"", "]", ".", "dim", "(", ")", "==", "3", "for", "x", "in", "batched_inputs", ")", "\n", "\n", "images", "=", "[", "x", "[", "\"image\"", "]", "for", "x", "in", "batched_inputs", "]", "\n", "images", "=", "ImageList", ".", "from_tensors", "(", "images", ",", "size_divisibility", ")", "\n", "\n", "im_info", "=", "[", "]", "\n", "for", "input_per_image", ",", "image_size", "in", "zip", "(", "batched_inputs", ",", "images", ".", "image_sizes", ")", ":", "\n", "        ", "target_height", "=", "input_per_image", ".", "get", "(", "\"height\"", ",", "image_size", "[", "0", "]", ")", "\n", "target_width", "=", "input_per_image", ".", "get", "(", "\"width\"", ",", "image_size", "[", "1", "]", ")", "# noqa", "\n", "# NOTE: The scale inside im_info is kept as convention and for providing", "\n", "# post-processing information if further processing is needed. For", "\n", "# current Caffe2 model definitions that don't include post-processing inside", "\n", "# the model, this number is not used.", "\n", "# NOTE: There can be a slight difference between width and height", "\n", "# scales, using a single number can results in numerical difference", "\n", "# compared with D2's post-processing.", "\n", "scale", "=", "target_height", "/", "image_size", "[", "0", "]", "\n", "im_info", ".", "append", "(", "[", "image_size", "[", "0", "]", ",", "image_size", "[", "1", "]", ",", "scale", "]", ")", "\n", "", "im_info", "=", "torch", ".", "Tensor", "(", "im_info", ")", "\n", "\n", "return", "images", ".", "tensor", ".", "to", "(", "device", ")", ",", "im_info", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._clear_jit_cache": [[20, 26], ["concrete_type_store.type_store.clear", "_jit_caching_layer.clear"], "function", ["None"], ["def", "_clear_jit_cache", "(", ")", ":", "\n", "    ", "from", "torch", ".", "jit", ".", "_recursive", "import", "concrete_type_store", "\n", "from", "torch", ".", "jit", ".", "_state", "import", "_jit_caching_layer", "\n", "\n", "concrete_type_store", ".", "type_store", ".", "clear", "(", ")", "# for modules", "\n", "_jit_caching_layer", ".", "clear", "(", ")", "# for free functions", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._add_instances_conversion_methods": [[28, 48], ["instances.get_fields", "newInstances", "instances.get_fields.items", "hasattr", "setattr", "copy.deepcopy"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields"], ["", "def", "_add_instances_conversion_methods", "(", "newInstances", ")", ":", "\n", "    ", "\"\"\"\n    Add from_instances methods to the scripted Instances class.\n    \"\"\"", "\n", "cls_name", "=", "newInstances", ".", "__name__", "\n", "\n", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "from_instances", "(", "instances", ":", "Instances", ")", ":", "\n", "        ", "\"\"\"\n        Create scripted Instances from original Instances\n        \"\"\"", "\n", "fields", "=", "instances", ".", "get_fields", "(", ")", "\n", "image_size", "=", "instances", ".", "image_size", "\n", "ret", "=", "newInstances", "(", "image_size", ")", "\n", "for", "name", ",", "val", "in", "fields", ".", "items", "(", ")", ":", "\n", "            ", "assert", "hasattr", "(", "ret", ",", "f\"_{name}\"", ")", ",", "f\"No attribute named {name} in {cls_name}\"", "\n", "setattr", "(", "ret", ",", "name", ",", "deepcopy", "(", "val", ")", ")", "\n", "", "return", "ret", "\n", "\n", "", "newInstances", ".", "from_instances", "=", "from_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_instances": [[50, 88], ["tempfile.TemporaryDirectory", "tempfile.NamedTemporaryFile", "torchscript_patch._clear_jit_cache", "torchscript_patch._gen_instance_module", "f.write", "f.flush", "f.close", "torchscript_patch._import", "getattr", "torch.jit.script", "torch._jit_internal._qualified_name", "torchscript_patch._add_instances_conversion_methods", "sys.modules.pop"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._clear_jit_cache", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._gen_instance_module", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._import", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._add_instances_conversion_methods"], ["", "@", "contextmanager", "\n", "def", "patch_instances", "(", "fields", ")", ":", "\n", "    ", "\"\"\"\n    A contextmanager, under which the Instances class in detectron2 is replaced\n    by a statically-typed scriptable class, defined by `fields`.\n    See more in `scripting_with_instances`.\n    \"\"\"", "\n", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2\"", ")", "as", "dir", ",", "tempfile", ".", "NamedTemporaryFile", "(", "\n", "mode", "=", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ",", "suffix", "=", "\".py\"", ",", "dir", "=", "dir", ",", "delete", "=", "False", "\n", ")", "as", "f", ":", "\n", "        ", "try", ":", "\n", "# Objects that use Instances should not reuse previously-compiled", "\n", "# results in cache, because `Instances` could be a new class each time.", "\n", "            ", "_clear_jit_cache", "(", ")", "\n", "\n", "cls_name", ",", "s", "=", "_gen_instance_module", "(", "fields", ")", "\n", "f", ".", "write", "(", "s", ")", "\n", "f", ".", "flush", "(", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "module", "=", "_import", "(", "f", ".", "name", ")", "\n", "new_instances", "=", "getattr", "(", "module", ",", "cls_name", ")", "\n", "_", "=", "torch", ".", "jit", ".", "script", "(", "new_instances", ")", "\n", "# let torchscript think Instances was scripted already", "\n", "Instances", ".", "__torch_script_class__", "=", "True", "\n", "# let torchscript find new_instances when looking for the jit type of Instances", "\n", "Instances", ".", "_jit_override_qualname", "=", "torch", ".", "_jit_internal", ".", "_qualified_name", "(", "new_instances", ")", "\n", "\n", "_add_instances_conversion_methods", "(", "new_instances", ")", "\n", "yield", "new_instances", "\n", "", "finally", ":", "\n", "            ", "try", ":", "\n", "                ", "del", "Instances", ".", "__torch_script_class__", "\n", "del", "Instances", ".", "_jit_override_qualname", "\n", "", "except", "AttributeError", ":", "\n", "                ", "pass", "\n", "", "sys", ".", "modules", ".", "pop", "(", "module", ".", "__name__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._gen_instance_class": [[90, 259], ["tuple", "lines.append", "lines.append", "lines.append", "lines.append", "lines.append", "lines.append", "lines.append", "lines.append", "lines.append", "lines.append", "lines.append", "_FieldType", "lines.append", "lines.append", "lines.append", "lines.append", "hasattr", "lines.append", "lines.append", "os.linesep.join", "isinstance", "fields.items", "torchscript_patch._gen_instance_class.indent"], "function", ["None"], ["", "", "", "def", "_gen_instance_class", "(", "fields", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        fields (dict[name: type])\n    \"\"\"", "\n", "\n", "class", "_FieldType", ":", "\n", "        ", "def", "__init__", "(", "self", ",", "name", ",", "type_", ")", ":", "\n", "            ", "assert", "isinstance", "(", "name", ",", "str", ")", ",", "f\"Field name must be str, got {name}\"", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "type_", "=", "type_", "\n", "self", ".", "annotation", "=", "f\"{type_.__module__}.{type_.__name__}\"", "\n", "\n", "", "", "fields", "=", "[", "_FieldType", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "fields", ".", "items", "(", ")", "]", "\n", "\n", "def", "indent", "(", "level", ",", "s", ")", ":", "\n", "        ", "return", "\" \"", "*", "4", "*", "level", "+", "s", "\n", "\n", "", "lines", "=", "[", "]", "\n", "\n", "global", "_counter", "\n", "_counter", "+=", "1", "\n", "\n", "cls_name", "=", "\"ScriptedInstances{}\"", ".", "format", "(", "_counter", ")", "\n", "\n", "field_names", "=", "tuple", "(", "x", ".", "name", "for", "x", "in", "fields", ")", "\n", "lines", ".", "append", "(", "\n", "f\"\"\"\nclass {cls_name}:\n    def __init__(self, image_size: Tuple[int, int]):\n        self.image_size = image_size\n        self._field_names = {field_names}\n\"\"\"", "\n", ")", "\n", "\n", "for", "f", "in", "fields", ":", "\n", "        ", "lines", ".", "append", "(", "\n", "indent", "(", "2", ",", "f\"self._{f.name} = torch.jit.annotate(Optional[{f.annotation}], None)\"", ")", "\n", ")", "\n", "\n", "", "for", "f", "in", "fields", ":", "\n", "        ", "lines", ".", "append", "(", "\n", "f\"\"\"\n    @property\n    def {f.name}(self) -> {f.annotation}:\n        # has to use a local for type refinement\n        # https://pytorch.org/docs/stable/jit_language_reference.html#optional-type-refinement\n        t = self._{f.name}\n        assert t is not None\n        return t\n\n    @{f.name}.setter\n    def {f.name}(self, value: {f.annotation}) -> None:\n        self._{f.name} = value\n\"\"\"", "\n", ")", "\n", "\n", "# support method `__len__`", "\n", "", "lines", ".", "append", "(", "\n", "\"\"\"\n    def __len__(self) -> int:\n\"\"\"", "\n", ")", "\n", "for", "f", "in", "fields", ":", "\n", "        ", "lines", ".", "append", "(", "\n", "f\"\"\"\n        t = self._{f.name}\n        if t is not None:\n            return len(t)\n\"\"\"", "\n", ")", "\n", "", "lines", ".", "append", "(", "\n", "\"\"\"\n        raise NotImplementedError(\"Empty Instances does not support __len__!\")\n\"\"\"", "\n", ")", "\n", "\n", "# support method `has`", "\n", "lines", ".", "append", "(", "\n", "\"\"\"\n    def has(self, name: str) -> bool:\n\"\"\"", "\n", ")", "\n", "for", "f", "in", "fields", ":", "\n", "        ", "lines", ".", "append", "(", "\n", "f\"\"\"\n        if name == \"{f.name}\":\n            return self._{f.name} is not None\n\"\"\"", "\n", ")", "\n", "", "lines", ".", "append", "(", "\n", "\"\"\"\n        return False\n\"\"\"", "\n", ")", "\n", "\n", "# support method `to`", "\n", "lines", ".", "append", "(", "\n", "f\"\"\"\n    def to(self, device: torch.device) -> \"{cls_name}\":\n        ret = {cls_name}(self.image_size)\n\"\"\"", "\n", ")", "\n", "for", "f", "in", "fields", ":", "\n", "        ", "if", "hasattr", "(", "f", ".", "type_", ",", "\"to\"", ")", ":", "\n", "            ", "lines", ".", "append", "(", "\n", "f\"\"\"\n        t = self._{f.name}\n        if t is not None:\n            ret._{f.name} = t.to(device)\n\"\"\"", "\n", ")", "\n", "", "else", ":", "\n", "# For now, ignore fields that cannot be moved to devices.", "\n", "# Maybe can support other tensor-like classes (e.g. __torch_function__)", "\n", "            ", "pass", "\n", "", "", "lines", ".", "append", "(", "\n", "\"\"\"\n        return ret\n\"\"\"", "\n", ")", "\n", "\n", "# support method `getitem`", "\n", "lines", ".", "append", "(", "\n", "f\"\"\"\n    def __getitem__(self, item) -> \"{cls_name}\":\n        ret = {cls_name}(self.image_size)\n\"\"\"", "\n", ")", "\n", "for", "f", "in", "fields", ":", "\n", "        ", "lines", ".", "append", "(", "\n", "f\"\"\"\n        t = self._{f.name}\n        if t is not None:\n            ret._{f.name} = t[item]\n\"\"\"", "\n", ")", "\n", "", "lines", ".", "append", "(", "\n", "\"\"\"\n        return ret\n\"\"\"", "\n", ")", "\n", "\n", "# support method `get_fields()`", "\n", "lines", ".", "append", "(", "\n", "\"\"\"\n    def get_fields(self) -> Dict[str, Tensor]:\n        ret = {}\n    \"\"\"", "\n", ")", "\n", "for", "f", "in", "fields", ":", "\n", "        ", "if", "f", ".", "type_", "==", "Boxes", ":", "\n", "            ", "stmt", "=", "\"t.tensor\"", "\n", "", "elif", "f", ".", "type_", "==", "torch", ".", "Tensor", ":", "\n", "            ", "stmt", "=", "\"t\"", "\n", "", "else", ":", "\n", "            ", "stmt", "=", "f'assert False, \"unsupported type {str(f.type_)}\"'", "\n", "", "lines", ".", "append", "(", "\n", "f\"\"\"\n        t = self._{f.name}\n        if t is not None:\n            ret[\"{f.name}\"] = {stmt}\n        \"\"\"", "\n", ")", "\n", "", "lines", ".", "append", "(", "\n", "\"\"\"\n        return ret\"\"\"", "\n", ")", "\n", "return", "cls_name", ",", "os", ".", "linesep", ".", "join", "(", "lines", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._gen_instance_module": [[261, 278], ["torchscript_patch._gen_instance_class"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._gen_instance_class"], ["", "def", "_gen_instance_module", "(", "fields", ")", ":", "\n", "# TODO: find a more automatic way to enable import of other classes", "\n", "    ", "s", "=", "\"\"\"\nfrom copy import deepcopy\nimport torch\nfrom torch import Tensor\nimport typing\nfrom typing import *\n\nimport detectron2\nfrom detectron2.structures import Boxes, Instances\n\n\"\"\"", "\n", "\n", "cls_name", ",", "cls_def", "=", "_gen_instance_class", "(", "fields", ")", "\n", "s", "+=", "cls_def", "\n", "return", "cls_name", ",", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch._import": [[280, 283], ["detectron2.utils.env._import_file"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.env._import_file"], ["", "def", "_import", "(", "path", ")", ":", "\n", "    ", "return", "_import_file", "(", "\n", "\"{}{}\"", ".", "format", "(", "sys", ".", "modules", "[", "__name__", "]", ".", "__name__", ",", "_counter", ")", ",", "path", ",", "make_importable", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_builtin_len": [[286, 311], ["obj.__len__", "contextlib.ExitStack", "list", "stack.enter_context", "unittest.mock.patch"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.__len__", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_patch.patch"], ["", "@", "contextmanager", "\n", "def", "patch_builtin_len", "(", "modules", "=", "(", ")", ")", ":", "\n", "    ", "\"\"\"\n    Patch the builtin len() function of a few detectron2 modules\n    to use __len__ instead, because __len__ does not convert values to\n    integers and therefore is friendly to tracing.\n\n    Args:\n        modules (list[stsr]): names of extra modules to patch len(), in\n            addition to those in detectron2.\n    \"\"\"", "\n", "\n", "def", "_new_len", "(", "obj", ")", ":", "\n", "        ", "return", "obj", ".", "__len__", "(", ")", "\n", "\n", "", "with", "ExitStack", "(", ")", "as", "stack", ":", "\n", "        ", "MODULES", "=", "[", "\n", "\"detectron2.modeling.roi_heads.fast_rcnn\"", ",", "\n", "\"detectron2.modeling.roi_heads.mask_head\"", ",", "\n", "\"detectron2.modeling.roi_heads.keypoint_head\"", ",", "\n", "]", "+", "list", "(", "modules", ")", "\n", "ctxs", "=", "[", "stack", ".", "enter_context", "(", "mock", ".", "patch", "(", "mod", "+", "\".len\"", ")", ")", "for", "mod", "in", "MODULES", "]", "\n", "for", "m", "in", "ctxs", ":", "\n", "            ", "m", ".", "side_effect", "=", "_new_len", "\n", "", "yield", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_nonscriptable_classes": [[313, 356], ["hasattr", "copy.deepcopy", "torch.nn.ModuleList", "copy.deepcopy", "torch.nn.ModuleList", "torch.nn.ModuleList", "torchscript_patch..named_children", "copy.deepcopy", "delattr", "name.startswith", "delattr"], "function", ["None"], ["", "", "def", "patch_nonscriptable_classes", "(", ")", ":", "\n", "    ", "\"\"\"\n    Apply patches on a few nonscriptable detectron2 classes.\n    Should not have side-effects on eager usage.\n    \"\"\"", "\n", "# __prepare_scriptable__ can also be added to models for easier maintenance.", "\n", "# But it complicates the clean model code.", "\n", "\n", "from", "detectron2", ".", "modeling", ".", "backbone", "import", "ResNet", ",", "FPN", "\n", "\n", "# Due to https://github.com/pytorch/pytorch/issues/36061,", "\n", "# we change backbone to use ModuleList for scripting.", "\n", "# (note: this changes param names in state_dict)", "\n", "\n", "def", "prepare_resnet", "(", "self", ")", ":", "\n", "        ", "ret", "=", "deepcopy", "(", "self", ")", "\n", "ret", ".", "stages", "=", "nn", ".", "ModuleList", "(", "ret", ".", "stages", ")", "\n", "for", "k", "in", "self", ".", "stage_names", ":", "\n", "            ", "delattr", "(", "ret", ",", "k", ")", "\n", "", "return", "ret", "\n", "\n", "", "ResNet", ".", "__prepare_scriptable__", "=", "prepare_resnet", "\n", "\n", "def", "prepare_fpn", "(", "self", ")", ":", "\n", "        ", "ret", "=", "deepcopy", "(", "self", ")", "\n", "ret", ".", "lateral_convs", "=", "nn", ".", "ModuleList", "(", "ret", ".", "lateral_convs", ")", "\n", "ret", ".", "output_convs", "=", "nn", ".", "ModuleList", "(", "ret", ".", "output_convs", ")", "\n", "for", "name", ",", "_", "in", "self", ".", "named_children", "(", ")", ":", "\n", "            ", "if", "name", ".", "startswith", "(", "\"fpn_\"", ")", ":", "\n", "                ", "delattr", "(", "ret", ",", "name", ")", "\n", "", "", "return", "ret", "\n", "\n", "", "FPN", ".", "__prepare_scriptable__", "=", "prepare_fpn", "\n", "\n", "# Annotate some attributes to be constants for the purpose of scripting,", "\n", "# even though they are not constants in eager mode.", "\n", "from", "detectron2", ".", "modeling", ".", "roi_heads", "import", "StandardROIHeads", "\n", "\n", "if", "hasattr", "(", "StandardROIHeads", ",", "\"__annotations__\"", ")", ":", "\n", "# copy first to avoid editing annotations of base class", "\n", "        ", "StandardROIHeads", ".", "__annotations__", "=", "deepcopy", "(", "StandardROIHeads", ".", "__annotations__", ")", "\n", "StandardROIHeads", ".", "__annotations__", "[", "\"mask_on\"", "]", "=", "torch", ".", "jit", ".", "Final", "[", "bool", "]", "\n", "StandardROIHeads", ".", "__annotations__", "[", "\"keypoint_on\"", "]", "=", "torch", ".", "jit", ".", "Final", "[", "bool", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.freeze_training_mode": [[362, 378], ["type", "model.modules", "hasattr"], "function", ["None"], ["@", "contextmanager", "\n", "def", "freeze_training_mode", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    A context manager that annotates the \"training\" attribute of every submodule\n    to constant, so that the training codepath in these modules can be\n    meta-compiled away. Upon exiting, the annotations are reverted.\n    \"\"\"", "\n", "classes", "=", "{", "type", "(", "x", ")", "for", "x", "in", "model", ".", "modules", "(", ")", "}", "\n", "# __constants__ is the old way to annotate constants and not compatible", "\n", "# with __annotations__ .", "\n", "classes", "=", "{", "x", "for", "x", "in", "classes", "if", "not", "hasattr", "(", "x", ",", "\"__constants__\"", ")", "}", "\n", "for", "cls", "in", "classes", ":", "\n", "        ", "cls", ".", "__annotations__", "[", "\"training\"", "]", "=", "torch", ".", "jit", ".", "Final", "[", "bool", "]", "\n", "", "yield", "\n", "for", "cls", "in", "classes", ":", "\n", "        ", "cls", ".", "__annotations__", "[", "\"training\"", "]", "=", "bool", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export.export_onnx_model": [[33, 72], ["isinstance", "model.apply", "onnx.optimizer.get_available_passes", "all", "onnx.optimizer.optimize", "torch.no_grad", "io.BytesIO", "torch.onnx.export", "onnx.load_from_string", "f.getvalue"], "function", ["None"], ["def", "export_onnx_model", "(", "model", ",", "inputs", ")", ":", "\n", "    ", "\"\"\"\n    Trace and export a model to onnx format.\n\n    Args:\n        model (nn.Module):\n        inputs (tuple[args]): the model will be called by `model(*inputs)`\n\n    Returns:\n        an onnx model\n    \"\"\"", "\n", "assert", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "Module", ")", "\n", "\n", "# make sure all modules are in eval mode, onnx may change the training state", "\n", "# of the module if the states are not consistent", "\n", "def", "_check_eval", "(", "module", ")", ":", "\n", "        ", "assert", "not", "module", ".", "training", "\n", "\n", "", "model", ".", "apply", "(", "_check_eval", ")", "\n", "\n", "# Export the model to ONNX", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "with", "io", ".", "BytesIO", "(", ")", "as", "f", ":", "\n", "            ", "torch", ".", "onnx", ".", "export", "(", "\n", "model", ",", "\n", "inputs", ",", "\n", "f", ",", "\n", "operator_export_type", "=", "OperatorExportTypes", ".", "ONNX_ATEN_FALLBACK", ",", "\n", "# verbose=True,  # NOTE: uncomment this for debugging", "\n", "# export_params=True,", "\n", ")", "\n", "onnx_model", "=", "onnx", ".", "load_from_string", "(", "f", ".", "getvalue", "(", ")", ")", "\n", "\n", "# Apply ONNX's Optimization", "\n", "", "", "all_passes", "=", "onnx", ".", "optimizer", ".", "get_available_passes", "(", ")", "\n", "passes", "=", "[", "\"fuse_bn_into_conv\"", "]", "\n", "assert", "all", "(", "p", "in", "all_passes", "for", "p", "in", "passes", ")", "\n", "onnx_model", "=", "onnx", ".", "optimizer", ".", "optimize", "(", "onnx_model", ",", "passes", ")", "\n", "return", "onnx_model", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export._op_stats": [[74, 81], ["sorted", "sorted", "type_count.items", "type_count.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "_op_stats", "(", "net_def", ")", ":", "\n", "    ", "type_count", "=", "{", "}", "\n", "for", "t", "in", "[", "op", ".", "type", "for", "op", "in", "net_def", ".", "op", "]", ":", "\n", "        ", "type_count", "[", "t", "]", "=", "type_count", ".", "get", "(", "t", ",", "0", ")", "+", "1", "\n", "", "type_count_list", "=", "sorted", "(", "type_count", ".", "items", "(", ")", ",", "key", "=", "lambda", "kv", ":", "kv", "[", "0", "]", ")", "# alphabet", "\n", "type_count_list", "=", "sorted", "(", "type_count_list", ",", "key", "=", "lambda", "kv", ":", "-", "kv", "[", "1", "]", ")", "# count", "\n", "return", "\"\\n\"", ".", "join", "(", "\"{:>4}x {}\"", ".", "format", "(", "count", ",", "name", ")", "for", "name", ",", "count", "in", "type_count_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export._assign_device_option": [[83, 127], ["shared.infer_device_type", "caffe2.python.core.get_ssa", "caffe2_export._assign_device_option._assign_op_device_option"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.infer_device_type"], ["", "def", "_assign_device_option", "(", "\n", "predict_net", ":", "caffe2_pb2", ".", "NetDef", ",", "init_net", ":", "caffe2_pb2", ".", "NetDef", ",", "tensor_inputs", ":", "List", "[", "torch", ".", "Tensor", "]", "\n", ")", ":", "\n", "    ", "\"\"\"\n    ONNX exported network doesn't have concept of device, assign necessary\n    device option for each op in order to make it runable on GPU runtime.\n    \"\"\"", "\n", "\n", "def", "_get_device_type", "(", "torch_tensor", ")", ":", "\n", "        ", "assert", "torch_tensor", ".", "device", ".", "type", "in", "[", "\"cpu\"", ",", "\"cuda\"", "]", "\n", "assert", "torch_tensor", ".", "device", ".", "index", "==", "0", "\n", "return", "torch_tensor", ".", "device", ".", "type", "\n", "\n", "", "def", "_assign_op_device_option", "(", "net_proto", ",", "net_ssa", ",", "blob_device_types", ")", ":", "\n", "        ", "for", "op", ",", "ssa_i", "in", "zip", "(", "net_proto", ".", "op", ",", "net_ssa", ")", ":", "\n", "            ", "if", "op", ".", "type", "in", "[", "\"CopyCPUToGPU\"", ",", "\"CopyGPUToCPU\"", "]", ":", "\n", "                ", "op", ".", "device_option", ".", "CopyFrom", "(", "core", ".", "DeviceOption", "(", "caffe2_pb2", ".", "CUDA", ",", "0", ")", ")", "\n", "", "else", ":", "\n", "                ", "devices", "=", "[", "blob_device_types", "[", "b", "]", "for", "b", "in", "ssa_i", "[", "0", "]", "+", "ssa_i", "[", "1", "]", "]", "\n", "assert", "all", "(", "d", "==", "devices", "[", "0", "]", "for", "d", "in", "devices", ")", "\n", "if", "devices", "[", "0", "]", "==", "\"cuda\"", ":", "\n", "                    ", "op", ".", "device_option", ".", "CopyFrom", "(", "core", ".", "DeviceOption", "(", "caffe2_pb2", ".", "CUDA", ",", "0", ")", ")", "\n", "\n", "# update ops in predict_net", "\n", "", "", "", "", "predict_net_input_device_types", "=", "{", "\n", "(", "name", ",", "0", ")", ":", "_get_device_type", "(", "tensor", ")", "\n", "for", "name", ",", "tensor", "in", "zip", "(", "predict_net", ".", "external_input", ",", "tensor_inputs", ")", "\n", "}", "\n", "predict_net_device_types", "=", "infer_device_type", "(", "\n", "predict_net", ",", "known_status", "=", "predict_net_input_device_types", ",", "device_name_style", "=", "\"pytorch\"", "\n", ")", "\n", "predict_net_ssa", ",", "_", "=", "core", ".", "get_ssa", "(", "predict_net", ")", "\n", "_assign_op_device_option", "(", "predict_net", ",", "predict_net_ssa", ",", "predict_net_device_types", ")", "\n", "\n", "# update ops in init_net", "\n", "init_net_ssa", ",", "versions", "=", "core", ".", "get_ssa", "(", "init_net", ")", "\n", "init_net_output_device_types", "=", "{", "\n", "(", "name", ",", "versions", "[", "name", "]", ")", ":", "predict_net_device_types", "[", "(", "name", ",", "0", ")", "]", "\n", "for", "name", "in", "init_net", ".", "external_output", "\n", "}", "\n", "init_net_device_types", "=", "infer_device_type", "(", "\n", "init_net", ",", "known_status", "=", "init_net_output_device_types", ",", "device_name_style", "=", "\"pytorch\"", "\n", ")", "\n", "_assign_op_device_option", "(", "init_net", ",", "init_net_ssa", ",", "init_net_device_types", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export.export_caffe2_detection_model": [[129, 173], ["copy.deepcopy", "isinstance", "hasattr", "logger.info", "caffe2_export.export_onnx_model", "caffe2.python.onnx.backend.Caffe2Backend.onnx_graph_to_caffe2_net", "tabulate.tabulate", "logger.info", "shared.fuse_alias_placeholder", "any", "shared.get_params_from_init_net", "shared.remove_reshape_for_fc", "shared.construct_init_net_from_params", "shared.group_norm_replace_aten_with_caffe2", "copy.deepcopy.encode_additional_info", "logger.info", "logger.info", "shared.fuse_copy_between_cpu_and_gpu", "shared.remove_dead_end_ops", "caffe2_export._assign_device_option", "termcolor.colored", "caffe2_export._op_stats", "caffe2_export._op_stats", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.api.export_onnx_model", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.fuse_alias_placeholder", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_params_from_init_net", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.remove_reshape_for_fc", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.construct_init_net_from_params", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.group_norm_replace_aten_with_caffe2", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2RetinaNet.encode_additional_info", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.fuse_copy_between_cpu_and_gpu", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.remove_dead_end_ops", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export._assign_device_option", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export._op_stats", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export._op_stats"], ["", "def", "export_caffe2_detection_model", "(", "model", ":", "torch", ".", "nn", ".", "Module", ",", "tensor_inputs", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "    ", "\"\"\"\n    Export a caffe2-compatible Detectron2 model to caffe2 format via ONNX.\n\n    Arg:\n        model: a caffe2-compatible version of detectron2 model, defined in caffe2_modeling.py\n        tensor_inputs: a list of tensors that caffe2 model takes as input.\n    \"\"\"", "\n", "model", "=", "copy", ".", "deepcopy", "(", "model", ")", "\n", "assert", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "Module", ")", "\n", "assert", "hasattr", "(", "model", ",", "\"encode_additional_info\"", ")", "\n", "\n", "# Export via ONNX", "\n", "logger", ".", "info", "(", "\n", "\"Exporting a {} model via ONNX ...\"", ".", "format", "(", "type", "(", "model", ")", ".", "__name__", ")", "\n", "+", "\" Some warnings from ONNX are expected and are usually not to worry about.\"", "\n", ")", "\n", "onnx_model", "=", "export_onnx_model", "(", "model", ",", "(", "tensor_inputs", ",", ")", ")", "\n", "# Convert ONNX model to Caffe2 protobuf", "\n", "init_net", ",", "predict_net", "=", "Caffe2Backend", ".", "onnx_graph_to_caffe2_net", "(", "onnx_model", ")", "\n", "ops_table", "=", "[", "[", "op", ".", "type", ",", "op", ".", "input", ",", "op", ".", "output", "]", "for", "op", "in", "predict_net", ".", "op", "]", "\n", "table", "=", "tabulate", "(", "ops_table", ",", "headers", "=", "[", "\"type\"", ",", "\"input\"", ",", "\"output\"", "]", ",", "tablefmt", "=", "\"pipe\"", ")", "\n", "logger", ".", "info", "(", "\n", "\"ONNX export Done. Exported predict_net (before optimizations):\\n\"", "+", "colored", "(", "table", ",", "\"cyan\"", ")", "\n", ")", "\n", "\n", "# Apply protobuf optimization", "\n", "fuse_alias_placeholder", "(", "predict_net", ",", "init_net", ")", "\n", "if", "any", "(", "t", ".", "device", ".", "type", "!=", "\"cpu\"", "for", "t", "in", "tensor_inputs", ")", ":", "\n", "        ", "fuse_copy_between_cpu_and_gpu", "(", "predict_net", ")", "\n", "remove_dead_end_ops", "(", "init_net", ")", "\n", "_assign_device_option", "(", "predict_net", ",", "init_net", ",", "tensor_inputs", ")", "\n", "", "params", ",", "device_options", "=", "get_params_from_init_net", "(", "init_net", ")", "\n", "predict_net", ",", "params", "=", "remove_reshape_for_fc", "(", "predict_net", ",", "params", ")", "\n", "init_net", "=", "construct_init_net_from_params", "(", "params", ",", "device_options", ")", "\n", "group_norm_replace_aten_with_caffe2", "(", "predict_net", ")", "\n", "\n", "# Record necessary information for running the pb model in Detectron2 system.", "\n", "model", ".", "encode_additional_info", "(", "predict_net", ",", "init_net", ")", "\n", "\n", "logger", ".", "info", "(", "\"Operators used in predict_net: \\n{}\"", ".", "format", "(", "_op_stats", "(", "predict_net", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Operators used in init_net: \\n{}\"", ".", "format", "(", "_op_stats", "(", "init_net", ")", ")", ")", "\n", "\n", "return", "predict_net", ",", "init_net", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export.run_and_save_graph": [[175, 208], ["logger.info", "shared.save_graph", "logger.info", "shared.ScopedWS", "ws.RunNetOnce", "set", "zip", "logger.info", "shared.save_graph", "ws.Blobs", "ws.FeedBlob", "ws.RunNetOnce", "ws.FetchBlob", "logger.warning", "ws.Blobs", "isinstance", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.save_graph", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.save_graph"], ["", "def", "run_and_save_graph", "(", "predict_net", ",", "init_net", ",", "tensor_inputs", ",", "graph_save_path", ")", ":", "\n", "    ", "\"\"\"\n    Run the caffe2 model on given inputs, recording the shape and draw the graph.\n\n    predict_net/init_net: caffe2 model.\n    tensor_inputs: a list of tensors that caffe2 model takes as input.\n    graph_save_path: path for saving graph of exported model.\n    \"\"\"", "\n", "\n", "logger", ".", "info", "(", "\"Saving graph of ONNX exported model to {} ...\"", ".", "format", "(", "graph_save_path", ")", ")", "\n", "save_graph", "(", "predict_net", ",", "graph_save_path", ",", "op_only", "=", "False", ")", "\n", "\n", "# Run the exported Caffe2 net", "\n", "logger", ".", "info", "(", "\"Running ONNX exported model ...\"", ")", "\n", "with", "ScopedWS", "(", "\"__ws_tmp__\"", ",", "True", ")", "as", "ws", ":", "\n", "        ", "ws", ".", "RunNetOnce", "(", "init_net", ")", "\n", "initialized_blobs", "=", "set", "(", "ws", ".", "Blobs", "(", ")", ")", "\n", "uninitialized", "=", "[", "inp", "for", "inp", "in", "predict_net", ".", "external_input", "if", "inp", "not", "in", "initialized_blobs", "]", "\n", "for", "name", ",", "blob", "in", "zip", "(", "uninitialized", ",", "tensor_inputs", ")", ":", "\n", "            ", "ws", ".", "FeedBlob", "(", "name", ",", "blob", ")", "\n", "\n", "", "try", ":", "\n", "            ", "ws", ".", "RunNetOnce", "(", "predict_net", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "            ", "logger", ".", "warning", "(", "\"Encountered RuntimeError: \\n{}\"", ".", "format", "(", "str", "(", "e", ")", ")", ")", "\n", "\n", "", "ws_blobs", "=", "{", "b", ":", "ws", ".", "FetchBlob", "(", "b", ")", "for", "b", "in", "ws", ".", "Blobs", "(", ")", "}", "\n", "blob_sizes", "=", "{", "b", ":", "ws_blobs", "[", "b", "]", ".", "shape", "for", "b", "in", "ws_blobs", "if", "isinstance", "(", "ws_blobs", "[", "b", "]", ",", "np", ".", "ndarray", ")", "}", "\n", "\n", "logger", ".", "info", "(", "\"Saving graph with blob shapes to {} ...\"", ".", "format", "(", "graph_save_path", ")", ")", "\n", "save_graph", "(", "predict_net", ",", "graph_save_path", ",", "op_only", "=", "False", ",", "blob_sizes", "=", "blob_sizes", ")", "\n", "\n", "return", "ws_blobs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2Boxes.__init__": [[29, 35], ["isinstance", "tensor.size", "tensor.dim", "tensor.size"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "assert", "isinstance", "(", "tensor", ",", "torch", ".", "Tensor", ")", "\n", "assert", "tensor", ".", "dim", "(", ")", "==", "2", "and", "tensor", ".", "size", "(", "-", "1", ")", "in", "[", "4", ",", "5", ",", "6", "]", ",", "tensor", ".", "size", "(", ")", "\n", "# TODO: make tensor immutable when dim is Nx5 for Boxes,", "\n", "# and Nx6 for RotatedBoxes?", "\n", "self", ".", "tensor", "=", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.__init__": [[48, 57], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "im_info", ",", "indices", ",", "extra_fields", "=", "None", ")", ":", "\n", "# [N, 3] -> (H, W, Scale)", "\n", "        ", "self", ".", "im_info", "=", "im_info", "\n", "# [N,] -> indice of batch to which the instance belongs", "\n", "self", ".", "indices", "=", "indices", "\n", "# [N, ...]", "\n", "self", ".", "batch_extra_fields", "=", "extra_fields", "or", "{", "}", "\n", "\n", "self", ".", "image_size", "=", "self", ".", "im_info", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields": [[58, 71], ["c10.InstancesList.batch_extra_fields.items"], "methods", ["None"], ["", "def", "get_fields", "(", "self", ")", ":", "\n", "        ", "\"\"\"like `get_fields` in the Instances object,\n        but return each field in tensor representations\"\"\"", "\n", "ret", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "self", ".", "batch_extra_fields", ".", "items", "(", ")", ":", "\n", "# if isinstance(v, torch.Tensor):", "\n", "#     tensor_rep = v", "\n", "# elif isinstance(v, (Boxes, Keypoints)):", "\n", "#     tensor_rep = v.tensor", "\n", "# else:", "\n", "#     raise ValueError(\"Can't find tensor representation for: {}\".format())", "\n", "            ", "ret", "[", "k", "]", "=", "v", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has": [[72, 74], ["None"], "methods", ["None"], ["", "def", "has", "(", "self", ",", "name", ")", ":", "\n", "        ", "return", "name", "in", "self", ".", "batch_extra_fields", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.set": [[75, 82], ["len", "len", "len", "len"], "methods", ["None"], ["", "def", "set", "(", "self", ",", "name", ",", "value", ")", ":", "\n", "        ", "data_len", "=", "len", "(", "value", ")", "\n", "if", "len", "(", "self", ".", "batch_extra_fields", ")", ":", "\n", "            ", "assert", "(", "\n", "len", "(", "self", ")", "==", "data_len", "\n", ")", ",", "\"Adding a field of length {} to a Instances of length {}\"", ".", "format", "(", "data_len", ",", "len", "(", "self", ")", ")", "\n", "", "self", ".", "batch_extra_fields", "[", "name", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.__setattr__": [[83, 88], ["object.__setattr__", "c10.InstancesList.set"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.__setattr__", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "__setattr__", "(", "self", ",", "name", ",", "val", ")", ":", "\n", "        ", "if", "name", "in", "[", "\"im_info\"", ",", "\"indices\"", ",", "\"batch_extra_fields\"", ",", "\"image_size\"", "]", ":", "\n", "            ", "super", "(", ")", ".", "__setattr__", "(", "name", ",", "val", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "set", "(", "name", ",", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.__getattr__": [[89, 93], ["AttributeError"], "methods", ["None"], ["", "", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "if", "name", "not", "in", "self", ".", "batch_extra_fields", ":", "\n", "            ", "raise", "AttributeError", "(", "\"Cannot find field '{}' in the given Instances!\"", ".", "format", "(", "name", ")", ")", "\n", "", "return", "self", ".", "batch_extra_fields", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.__len__": [[94, 96], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.flatten": [[97, 105], ["c10.InstancesList.batch_extra_fields.items", "isinstance", "ret.append", "ret.append"], "methods", ["None"], ["", "def", "flatten", "(", "self", ")", ":", "\n", "        ", "ret", "=", "[", "]", "\n", "for", "_", ",", "v", "in", "self", ".", "batch_extra_fields", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "(", "Boxes", ",", "Keypoints", ")", ")", ":", "\n", "                ", "ret", ".", "append", "(", "v", ".", "tensor", ")", "\n", "", "else", ":", "\n", "                ", "ret", ".", "append", "(", "v", ")", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.to_d2_instances_list": [[106, 145], ["enumerate", "isinstance", "all", "detectron2.structures.Instances", "instances_list.batch_extra_fields.items", "ret.append", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "isinstance", "isinstance", "issubclass", "isinstance", "detectron2.structures.Instances.set", "isinstance", "detectron2.structures.Instances.set", "issubclass", "int", "int", "detectron2.structures.Instances.set", "detectron2.structures.Boxes", "detectron2.structures.Instances.set", "issubclass", "info[].item", "info[].item", "detectron2.structures.Keypoints", "detectron2.structures.Instances.set", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "@", "staticmethod", "\n", "def", "to_d2_instances_list", "(", "instances_list", ")", ":", "\n", "        ", "\"\"\"\n        Convert InstancesList to List[Instances]. The input `instances_list` can\n        also be a List[Instances], in this case this method is a non-op.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "instances_list", ",", "InstancesList", ")", ":", "\n", "            ", "assert", "all", "(", "isinstance", "(", "x", ",", "Instances", ")", "for", "x", "in", "instances_list", ")", "\n", "return", "instances_list", "\n", "\n", "", "ret", "=", "[", "]", "\n", "for", "i", ",", "info", "in", "enumerate", "(", "instances_list", ".", "im_info", ")", ":", "\n", "            ", "instances", "=", "Instances", "(", "torch", ".", "Size", "(", "[", "int", "(", "info", "[", "0", "]", ".", "item", "(", ")", ")", ",", "int", "(", "info", "[", "1", "]", ".", "item", "(", ")", ")", "]", ")", ")", "\n", "\n", "ids", "=", "instances_list", ".", "indices", "==", "i", "\n", "for", "k", ",", "v", "in", "instances_list", ".", "batch_extra_fields", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "instances", ".", "set", "(", "k", ",", "v", "[", "ids", "]", ")", "\n", "continue", "\n", "", "elif", "isinstance", "(", "v", ",", "Boxes", ")", ":", "\n", "                    ", "instances", ".", "set", "(", "k", ",", "v", "[", "ids", ",", "-", "4", ":", "]", ")", "\n", "continue", "\n", "\n", "", "target_type", ",", "tensor_source", "=", "v", "\n", "assert", "isinstance", "(", "tensor_source", ",", "torch", ".", "Tensor", ")", "\n", "assert", "tensor_source", ".", "shape", "[", "0", "]", "==", "instances_list", ".", "indices", ".", "shape", "[", "0", "]", "\n", "tensor_source", "=", "tensor_source", "[", "ids", "]", "\n", "\n", "if", "issubclass", "(", "target_type", ",", "Boxes", ")", ":", "\n", "                    ", "instances", ".", "set", "(", "k", ",", "Boxes", "(", "tensor_source", "[", ":", ",", "-", "4", ":", "]", ")", ")", "\n", "", "elif", "issubclass", "(", "target_type", ",", "Keypoints", ")", ":", "\n", "                    ", "instances", ".", "set", "(", "k", ",", "Keypoints", "(", "tensor_source", ")", ")", "\n", "", "elif", "issubclass", "(", "target_type", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "instances", ".", "set", "(", "k", ",", "tensor_source", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Can't handle targe type: {}\"", ".", "format", "(", "target_type", ")", ")", "\n", "\n", "", "", "ret", ".", "append", "(", "instances", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2Compatible._get_tensor_mode": [[152, 154], ["None"], "methods", ["None"], ["def", "_get_tensor_mode", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tensor_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2Compatible._set_tensor_mode": [[155, 157], ["None"], "methods", ["None"], ["", "def", "_set_tensor_mode", "(", "self", ",", "v", ")", ":", "\n", "        ", "self", ".", "_tensor_mode", "=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2RPN._generate_proposals": [[165, 247], ["isinstance", "isinstance", "zip", "c10.Caffe2RPN.c2_postprocess", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "iter", "scores.detach.detach.detach", "bbox_deltas.detach.detach.detach", "torch.ops._caffe2.GenerateProposals", "torch.ops._caffe2.GenerateProposals", "torch.ops._caffe2.GenerateProposals", "torch.ops._caffe2.GenerateProposals", "rpn_rois_list.append", "rpn_roi_probs_list.append", "len", "list", "int", "int", "torch.ops._caffe2.CollectRpnProposals", "torch.ops._caffe2.CollectRpnProposals", "torch.ops._caffe2.CollectRpnProposals", "torch.ops._caffe2.CollectRpnProposals", "shared.to_device", "len", "len", "shared.to_device", "math.log2", "math.log2", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2RPN.c2_postprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device"], ["    ", "def", "_generate_proposals", "(", "\n", "self", ",", "images", ",", "objectness_logits_pred", ",", "anchor_deltas_pred", ",", "gt_instances", "=", "None", "\n", ")", ":", "\n", "        ", "assert", "isinstance", "(", "images", ",", "ImageList", ")", "\n", "if", "self", ".", "tensor_mode", ":", "\n", "            ", "im_info", "=", "images", ".", "image_sizes", "\n", "", "else", ":", "\n", "            ", "im_info", "=", "torch", ".", "tensor", "(", "[", "[", "im_sz", "[", "0", "]", ",", "im_sz", "[", "1", "]", ",", "1.0", "]", "for", "im_sz", "in", "images", ".", "image_sizes", "]", ")", ".", "to", "(", "\n", "images", ".", "tensor", ".", "device", "\n", ")", "\n", "", "assert", "isinstance", "(", "im_info", ",", "torch", ".", "Tensor", ")", "\n", "\n", "rpn_rois_list", "=", "[", "]", "\n", "rpn_roi_probs_list", "=", "[", "]", "\n", "for", "scores", ",", "bbox_deltas", ",", "cell_anchors_tensor", ",", "feat_stride", "in", "zip", "(", "\n", "objectness_logits_pred", ",", "\n", "anchor_deltas_pred", ",", "\n", "iter", "(", "self", ".", "anchor_generator", ".", "cell_anchors", ")", ",", "\n", "self", ".", "anchor_generator", ".", "strides", ",", "\n", ")", ":", "\n", "            ", "scores", "=", "scores", ".", "detach", "(", ")", "\n", "bbox_deltas", "=", "bbox_deltas", ".", "detach", "(", ")", "\n", "\n", "rpn_rois", ",", "rpn_roi_probs", "=", "torch", ".", "ops", ".", "_caffe2", ".", "GenerateProposals", "(", "\n", "scores", ",", "\n", "bbox_deltas", ",", "\n", "im_info", ",", "\n", "cell_anchors_tensor", ",", "\n", "spatial_scale", "=", "1.0", "/", "feat_stride", ",", "\n", "pre_nms_topN", "=", "self", ".", "pre_nms_topk", "[", "self", ".", "training", "]", ",", "\n", "post_nms_topN", "=", "self", ".", "post_nms_topk", "[", "self", ".", "training", "]", ",", "\n", "nms_thresh", "=", "self", ".", "nms_thresh", ",", "\n", "min_size", "=", "self", ".", "min_box_size", ",", "\n", "# correct_transform_coords=True,  # deprecated argument", "\n", "angle_bound_on", "=", "True", ",", "# Default", "\n", "angle_bound_lo", "=", "-", "180", ",", "\n", "angle_bound_hi", "=", "180", ",", "\n", "clip_angle_thresh", "=", "1.0", ",", "# Default", "\n", "legacy_plus_one", "=", "False", ",", "\n", ")", "\n", "rpn_rois_list", ".", "append", "(", "rpn_rois", ")", "\n", "rpn_roi_probs_list", ".", "append", "(", "rpn_roi_probs", ")", "\n", "\n", "# For FPN in D2, in RPN all proposals from different levels are concated", "\n", "# together, ranked and picked by top post_nms_topk. Then in ROIPooler", "\n", "# it calculates level_assignments and calls the RoIAlign from", "\n", "# the corresponding level.", "\n", "\n", "", "if", "len", "(", "objectness_logits_pred", ")", "==", "1", ":", "\n", "            ", "rpn_rois", "=", "rpn_rois_list", "[", "0", "]", "\n", "rpn_roi_probs", "=", "rpn_roi_probs_list", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "assert", "len", "(", "rpn_rois_list", ")", "==", "len", "(", "rpn_roi_probs_list", ")", "\n", "rpn_post_nms_topN", "=", "self", ".", "post_nms_topk", "[", "self", ".", "training", "]", "\n", "\n", "device", "=", "rpn_rois_list", "[", "0", "]", ".", "device", "\n", "input_list", "=", "[", "to_device", "(", "x", ",", "\"cpu\"", ")", "for", "x", "in", "(", "rpn_rois_list", "+", "rpn_roi_probs_list", ")", "]", "\n", "\n", "# TODO remove this after confirming rpn_max_level/rpn_min_level", "\n", "# is not needed in CollectRpnProposals.", "\n", "feature_strides", "=", "list", "(", "self", ".", "anchor_generator", ".", "strides", ")", "\n", "rpn_min_level", "=", "int", "(", "math", ".", "log2", "(", "feature_strides", "[", "0", "]", ")", ")", "\n", "rpn_max_level", "=", "int", "(", "math", ".", "log2", "(", "feature_strides", "[", "-", "1", "]", ")", ")", "\n", "assert", "(", "rpn_max_level", "-", "rpn_min_level", "+", "1", ")", "==", "len", "(", "\n", "rpn_rois_list", "\n", ")", ",", "\"CollectRpnProposals requires continuous levels\"", "\n", "\n", "rpn_rois", "=", "torch", ".", "ops", ".", "_caffe2", ".", "CollectRpnProposals", "(", "\n", "input_list", ",", "\n", "# NOTE: in current implementation, rpn_max_level and rpn_min_level", "\n", "# are not needed, only the subtraction of two matters and it", "\n", "# can be infer from the number of inputs. Keep them now for", "\n", "# consistency.", "\n", "rpn_max_level", "=", "2", "+", "len", "(", "rpn_rois_list", ")", "-", "1", ",", "\n", "rpn_min_level", "=", "2", ",", "\n", "rpn_post_nms_topN", "=", "rpn_post_nms_topN", ",", "\n", ")", "\n", "rpn_rois", "=", "to_device", "(", "rpn_rois", ",", "device", ")", "\n", "rpn_roi_probs", "=", "[", "]", "\n", "\n", "", "proposals", "=", "self", ".", "c2_postprocess", "(", "im_info", ",", "rpn_rois", ",", "rpn_roi_probs", ",", "self", ".", "tensor_mode", ")", "\n", "return", "proposals", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2RPN.forward": [[248, 257], ["c10.Caffe2RPN.rpn_head", "c10.Caffe2RPN._generate_proposals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2RPN._generate_proposals"], ["", "def", "forward", "(", "self", ",", "images", ",", "features", ",", "gt_instances", "=", "None", ")", ":", "\n", "        ", "assert", "not", "self", ".", "training", "\n", "features", "=", "[", "features", "[", "f", "]", "for", "f", "in", "self", ".", "in_features", "]", "\n", "objectness_logits_pred", ",", "anchor_deltas_pred", "=", "self", ".", "rpn_head", "(", "features", ")", "\n", "return", "self", ".", "_generate_proposals", "(", "\n", "images", ",", "\n", "objectness_logits_pred", ",", "\n", "anchor_deltas_pred", ",", "\n", "gt_instances", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2RPN.c2_postprocess": [[259, 274], ["c10.InstancesList", "c10.InstancesList.to_d2_instances_list", "c10.Caffe2Boxes"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.to_d2_instances_list"], ["", "@", "staticmethod", "\n", "def", "c2_postprocess", "(", "im_info", ",", "rpn_rois", ",", "rpn_roi_probs", ",", "tensor_mode", ")", ":", "\n", "        ", "proposals", "=", "InstancesList", "(", "\n", "im_info", "=", "im_info", ",", "\n", "indices", "=", "rpn_rois", "[", ":", ",", "0", "]", ",", "\n", "extra_fields", "=", "{", "\n", "\"proposal_boxes\"", ":", "Caffe2Boxes", "(", "rpn_rois", ")", ",", "\n", "\"objectness_logits\"", ":", "(", "torch", ".", "Tensor", ",", "rpn_roi_probs", ")", ",", "\n", "}", ",", "\n", ")", "\n", "if", "not", "tensor_mode", ":", "\n", "            ", "proposals", "=", "InstancesList", ".", "to_d2_instances_list", "(", "proposals", ")", "\n", "", "else", ":", "\n", "            ", "proposals", "=", "[", "proposals", "]", "\n", "", "return", "proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2ROIPooler.c2_preprocess": [[277, 287], ["all", "all", "detectron2.modeling.poolers.convert_boxes_to_pooler_format", "isinstance", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.modeling.poolers.convert_boxes_to_pooler_format"], ["    ", "@", "staticmethod", "\n", "def", "c2_preprocess", "(", "box_lists", ")", ":", "\n", "        ", "assert", "all", "(", "isinstance", "(", "x", ",", "Boxes", ")", "for", "x", "in", "box_lists", ")", "\n", "if", "all", "(", "isinstance", "(", "x", ",", "Caffe2Boxes", ")", "for", "x", "in", "box_lists", ")", ":", "\n", "# input is pure-tensor based", "\n", "            ", "assert", "len", "(", "box_lists", ")", "==", "1", "\n", "pooler_fmt_boxes", "=", "box_lists", "[", "0", "]", ".", "tensor", "\n", "", "else", ":", "\n", "            ", "pooler_fmt_boxes", "=", "poolers", ".", "convert_boxes_to_pooler_format", "(", "box_lists", ")", "\n", "", "return", "pooler_fmt_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2ROIPooler.forward": [[288, 359], ["c10.Caffe2ROIPooler.c2_preprocess", "len", "torch.ops._caffe2.DistributeFpnProposals", "torch.ops._caffe2.DistributeFpnProposals", "torch.ops._caffe2.DistributeFpnProposals", "torch.ops._caffe2.DistributeFpnProposals", "zip", "detectron2.layers.cat", "torch.ops._caffe2.BatchPermutation", "torch.ops._caffe2.BatchPermutation", "torch.ops._caffe2.BatchPermutation", "torch.ops._caffe2.BatchPermutation", "isinstance", "c2_roi_align", "shared.to_device", "shared.to_device", "isinstance", "c2_roi_align", "roi_feat_fpn_list.append", "bool", "detectron2.layers.cat.numel", "rois_idx_restore_int32.numel", "float", "int", "int", "int", "float", "int", "int", "int"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2ROIPooler.c2_preprocess", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device"], ["", "def", "forward", "(", "self", ",", "x", ",", "box_lists", ")", ":", "\n", "        ", "assert", "not", "self", ".", "training", "\n", "\n", "pooler_fmt_boxes", "=", "self", ".", "c2_preprocess", "(", "box_lists", ")", "\n", "num_level_assignments", "=", "len", "(", "self", ".", "level_poolers", ")", "\n", "\n", "if", "num_level_assignments", "==", "1", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "level_poolers", "[", "0", "]", ",", "ROIAlignRotated", ")", ":", "\n", "                ", "c2_roi_align", "=", "torch", ".", "ops", ".", "_caffe2", ".", "RoIAlignRotated", "\n", "aligned", "=", "True", "\n", "", "else", ":", "\n", "                ", "c2_roi_align", "=", "torch", ".", "ops", ".", "_caffe2", ".", "RoIAlign", "\n", "aligned", "=", "self", ".", "level_poolers", "[", "0", "]", ".", "aligned", "\n", "\n", "", "out", "=", "c2_roi_align", "(", "\n", "x", "[", "0", "]", ",", "\n", "pooler_fmt_boxes", ",", "\n", "order", "=", "\"NCHW\"", ",", "\n", "spatial_scale", "=", "float", "(", "self", ".", "level_poolers", "[", "0", "]", ".", "spatial_scale", ")", ",", "\n", "pooled_h", "=", "int", "(", "self", ".", "output_size", "[", "0", "]", ")", ",", "\n", "pooled_w", "=", "int", "(", "self", ".", "output_size", "[", "1", "]", ")", ",", "\n", "sampling_ratio", "=", "int", "(", "self", ".", "level_poolers", "[", "0", "]", ".", "sampling_ratio", ")", ",", "\n", "aligned", "=", "aligned", ",", "\n", ")", "\n", "return", "out", "\n", "\n", "", "device", "=", "pooler_fmt_boxes", ".", "device", "\n", "assert", "(", "\n", "self", ".", "max_level", "-", "self", ".", "min_level", "+", "1", "==", "4", "\n", ")", ",", "\"Currently DistributeFpnProposals only support 4 levels\"", "\n", "fpn_outputs", "=", "torch", ".", "ops", ".", "_caffe2", ".", "DistributeFpnProposals", "(", "\n", "to_device", "(", "pooler_fmt_boxes", ",", "\"cpu\"", ")", ",", "\n", "roi_canonical_scale", "=", "self", ".", "canonical_box_size", ",", "\n", "roi_canonical_level", "=", "self", ".", "canonical_level", ",", "\n", "roi_max_level", "=", "self", ".", "max_level", ",", "\n", "roi_min_level", "=", "self", ".", "min_level", ",", "\n", "legacy_plus_one", "=", "False", ",", "\n", ")", "\n", "fpn_outputs", "=", "[", "to_device", "(", "x", ",", "device", ")", "for", "x", "in", "fpn_outputs", "]", "\n", "\n", "rois_fpn_list", "=", "fpn_outputs", "[", ":", "-", "1", "]", "\n", "rois_idx_restore_int32", "=", "fpn_outputs", "[", "-", "1", "]", "\n", "\n", "roi_feat_fpn_list", "=", "[", "]", "\n", "for", "roi_fpn", ",", "x_level", ",", "pooler", "in", "zip", "(", "rois_fpn_list", ",", "x", ",", "self", ".", "level_poolers", ")", ":", "\n", "            ", "if", "isinstance", "(", "pooler", ",", "ROIAlignRotated", ")", ":", "\n", "                ", "c2_roi_align", "=", "torch", ".", "ops", ".", "_caffe2", ".", "RoIAlignRotated", "\n", "aligned", "=", "True", "\n", "", "else", ":", "\n", "                ", "c2_roi_align", "=", "torch", ".", "ops", ".", "_caffe2", ".", "RoIAlign", "\n", "aligned", "=", "bool", "(", "pooler", ".", "aligned", ")", "\n", "\n", "", "roi_feat_fpn", "=", "c2_roi_align", "(", "\n", "x_level", ",", "\n", "roi_fpn", ",", "\n", "order", "=", "\"NCHW\"", ",", "\n", "spatial_scale", "=", "float", "(", "pooler", ".", "spatial_scale", ")", ",", "\n", "pooled_h", "=", "int", "(", "self", ".", "output_size", "[", "0", "]", ")", ",", "\n", "pooled_w", "=", "int", "(", "self", ".", "output_size", "[", "1", "]", ")", ",", "\n", "sampling_ratio", "=", "int", "(", "pooler", ".", "sampling_ratio", ")", ",", "\n", "aligned", "=", "aligned", ",", "\n", ")", "\n", "roi_feat_fpn_list", ".", "append", "(", "roi_feat_fpn", ")", "\n", "\n", "", "roi_feat_shuffled", "=", "cat", "(", "roi_feat_fpn_list", ",", "dim", "=", "0", ")", "\n", "assert", "roi_feat_shuffled", ".", "numel", "(", ")", ">", "0", "and", "rois_idx_restore_int32", ".", "numel", "(", ")", ">", "0", ",", "(", "\n", "\"Caffe2 export requires tracing with a model checkpoint + input that can produce valid\"", "\n", "\" detections. But no detections were obtained with the given checkpoint and input!\"", "\n", ")", "\n", "roi_feat", "=", "torch", ".", "ops", ".", "_caffe2", ".", "BatchPermutation", "(", "roi_feat_shuffled", ",", "rois_idx_restore_int32", ")", "\n", "return", "roi_feat", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2FastRCNNOutputsInference.__init__": [[362, 364], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tensor_mode", ")", ":", "\n", "        ", "self", ".", "tensor_mode", "=", "tensor_mode", "# whether the output is caffe2 tensor mode", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2FastRCNNOutputsInference.__call__": [[365, 493], ["type().cat", "torch.ops._caffe2.BBoxTransform", "torch.ops._caffe2.BBoxTransform", "torch.ops._caffe2.BBoxTransform", "torch.ops._caffe2.BBoxTransform", "shared.to_device", "shared.to_device", "torch.ops._caffe2.BoxWithNMSLimit", "torch.ops._caffe2.BoxWithNMSLimit", "torch.ops._caffe2.BoxWithNMSLimit", "torch.ops._caffe2.BoxWithNMSLimit", "shared.to_device", "shared.to_device", "shared.to_device", "shared.to_device", "shared.to_device", "shared.to_device", "detectron2.layers.cat", "shared.alias", "shared.alias", "shared.alias", "shared.alias", "shared.alias", "shared.alias", "c10.InstancesList", "len", "torch.softmax", "torch.softmax", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "detectron2.layers.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "shared.to_device", "shared.to_device", "shared.to_device", "shared.to_device", "shared.to_device", "shared.to_device", "roi_class_nms.to.to.to", "c10.InstancesList.to_d2_instances_list", "shared.alias.int().tolist", "list", "type", "float", "float", "int", "torch.full", "torch.full", "torch.full", "torch.full", "shared.alias.to().split", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.full", "torch.full", "torch.full", "torch.full", "enumerate", "c10.Caffe2Boxes", "shared.alias.int", "enumerate", "shared.alias.to", "int", "len", "x.item"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.to_d2_instances_list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "__call__", "(", "self", ",", "box_predictor", ",", "predictions", ",", "proposals", ")", ":", "\n", "        ", "\"\"\"equivalent to FastRCNNOutputLayers.inference\"\"\"", "\n", "num_classes", "=", "box_predictor", ".", "num_classes", "\n", "score_thresh", "=", "box_predictor", ".", "test_score_thresh", "\n", "nms_thresh", "=", "box_predictor", ".", "test_nms_thresh", "\n", "topk_per_image", "=", "box_predictor", ".", "test_topk_per_image", "\n", "is_rotated", "=", "len", "(", "box_predictor", ".", "box2box_transform", ".", "weights", ")", "==", "5", "\n", "\n", "if", "is_rotated", ":", "\n", "            ", "box_dim", "=", "5", "\n", "assert", "box_predictor", ".", "box2box_transform", ".", "weights", "[", "4", "]", "==", "1", ",", "(", "\n", "\"The weights for Rotated BBoxTransform in C2 have only 4 dimensions,\"", "\n", "+", "\" thus enforcing the angle weight to be 1 for now\"", "\n", ")", "\n", "box2box_transform_weights", "=", "box_predictor", ".", "box2box_transform", ".", "weights", "[", ":", "4", "]", "\n", "", "else", ":", "\n", "            ", "box_dim", "=", "4", "\n", "box2box_transform_weights", "=", "box_predictor", ".", "box2box_transform", ".", "weights", "\n", "\n", "", "class_logits", ",", "box_regression", "=", "predictions", "\n", "if", "num_classes", "+", "1", "==", "class_logits", ".", "shape", "[", "1", "]", ":", "\n", "            ", "class_prob", "=", "F", ".", "softmax", "(", "class_logits", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "assert", "num_classes", "==", "class_logits", ".", "shape", "[", "1", "]", "\n", "class_prob", "=", "F", ".", "sigmoid", "(", "class_logits", ")", "\n", "# BoxWithNMSLimit will infer num_classes from the shape of the class_prob", "\n", "# So append a zero column as placeholder for the background class", "\n", "class_prob", "=", "torch", ".", "cat", "(", "(", "class_prob", ",", "torch", ".", "zeros", "(", "class_prob", ".", "shape", "[", "0", "]", ",", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "assert", "box_regression", ".", "shape", "[", "1", "]", "%", "box_dim", "==", "0", "\n", "cls_agnostic_bbox_reg", "=", "box_regression", ".", "shape", "[", "1", "]", "//", "box_dim", "==", "1", "\n", "\n", "input_tensor_mode", "=", "proposals", "[", "0", "]", ".", "proposal_boxes", ".", "tensor", ".", "shape", "[", "1", "]", "==", "box_dim", "+", "1", "\n", "\n", "rois", "=", "type", "(", "proposals", "[", "0", "]", ".", "proposal_boxes", ")", ".", "cat", "(", "[", "p", ".", "proposal_boxes", "for", "p", "in", "proposals", "]", ")", "\n", "device", ",", "dtype", "=", "rois", ".", "tensor", ".", "device", ",", "rois", ".", "tensor", ".", "dtype", "\n", "if", "input_tensor_mode", ":", "\n", "            ", "im_info", "=", "proposals", "[", "0", "]", ".", "image_size", "\n", "rois", "=", "rois", ".", "tensor", "\n", "", "else", ":", "\n", "            ", "im_info", "=", "torch", ".", "tensor", "(", "\n", "[", "[", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1.0", "]", "for", "sz", "in", "[", "x", ".", "image_size", "for", "x", "in", "proposals", "]", "]", "\n", ")", "\n", "batch_ids", "=", "cat", "(", "\n", "[", "\n", "torch", ".", "full", "(", "(", "b", ",", "1", ")", ",", "i", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "len", "(", "p", ")", "for", "p", "in", "proposals", ")", "\n", "]", ",", "\n", "dim", "=", "0", ",", "\n", ")", "\n", "rois", "=", "torch", ".", "cat", "(", "[", "batch_ids", ",", "rois", ".", "tensor", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "roi_pred_bbox", ",", "roi_batch_splits", "=", "torch", ".", "ops", ".", "_caffe2", ".", "BBoxTransform", "(", "\n", "to_device", "(", "rois", ",", "\"cpu\"", ")", ",", "\n", "to_device", "(", "box_regression", ",", "\"cpu\"", ")", ",", "\n", "to_device", "(", "im_info", ",", "\"cpu\"", ")", ",", "\n", "weights", "=", "box2box_transform_weights", ",", "\n", "apply_scale", "=", "True", ",", "\n", "rotated", "=", "is_rotated", ",", "\n", "angle_bound_on", "=", "True", ",", "\n", "angle_bound_lo", "=", "-", "180", ",", "\n", "angle_bound_hi", "=", "180", ",", "\n", "clip_angle_thresh", "=", "1.0", ",", "\n", "legacy_plus_one", "=", "False", ",", "\n", ")", "\n", "roi_pred_bbox", "=", "to_device", "(", "roi_pred_bbox", ",", "device", ")", "\n", "roi_batch_splits", "=", "to_device", "(", "roi_batch_splits", ",", "device", ")", "\n", "\n", "nms_outputs", "=", "torch", ".", "ops", ".", "_caffe2", ".", "BoxWithNMSLimit", "(", "\n", "to_device", "(", "class_prob", ",", "\"cpu\"", ")", ",", "\n", "to_device", "(", "roi_pred_bbox", ",", "\"cpu\"", ")", ",", "\n", "to_device", "(", "roi_batch_splits", ",", "\"cpu\"", ")", ",", "\n", "score_thresh", "=", "float", "(", "score_thresh", ")", ",", "\n", "nms", "=", "float", "(", "nms_thresh", ")", ",", "\n", "detections_per_im", "=", "int", "(", "topk_per_image", ")", ",", "\n", "soft_nms_enabled", "=", "False", ",", "\n", "soft_nms_method", "=", "\"linear\"", ",", "\n", "soft_nms_sigma", "=", "0.5", ",", "\n", "soft_nms_min_score_thres", "=", "0.001", ",", "\n", "rotated", "=", "is_rotated", ",", "\n", "cls_agnostic_bbox_reg", "=", "cls_agnostic_bbox_reg", ",", "\n", "input_boxes_include_bg_cls", "=", "False", ",", "\n", "output_classes_include_bg_cls", "=", "False", ",", "\n", "legacy_plus_one", "=", "False", ",", "\n", ")", "\n", "roi_score_nms", "=", "to_device", "(", "nms_outputs", "[", "0", "]", ",", "device", ")", "\n", "roi_bbox_nms", "=", "to_device", "(", "nms_outputs", "[", "1", "]", ",", "device", ")", "\n", "roi_class_nms", "=", "to_device", "(", "nms_outputs", "[", "2", "]", ",", "device", ")", "\n", "roi_batch_splits_nms", "=", "to_device", "(", "nms_outputs", "[", "3", "]", ",", "device", ")", "\n", "roi_keeps_nms", "=", "to_device", "(", "nms_outputs", "[", "4", "]", ",", "device", ")", "\n", "roi_keeps_size_nms", "=", "to_device", "(", "nms_outputs", "[", "5", "]", ",", "device", ")", "\n", "if", "not", "self", ".", "tensor_mode", ":", "\n", "            ", "roi_class_nms", "=", "roi_class_nms", ".", "to", "(", "torch", ".", "int64", ")", "\n", "\n", "", "roi_batch_ids", "=", "cat", "(", "\n", "[", "\n", "torch", ".", "full", "(", "(", "b", ",", "1", ")", ",", "i", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "int", "(", "x", ".", "item", "(", ")", ")", "for", "x", "in", "roi_batch_splits_nms", ")", "\n", "]", ",", "\n", "dim", "=", "0", ",", "\n", ")", "\n", "\n", "roi_class_nms", "=", "alias", "(", "roi_class_nms", ",", "\"class_nms\"", ")", "\n", "roi_score_nms", "=", "alias", "(", "roi_score_nms", ",", "\"score_nms\"", ")", "\n", "roi_bbox_nms", "=", "alias", "(", "roi_bbox_nms", ",", "\"bbox_nms\"", ")", "\n", "roi_batch_splits_nms", "=", "alias", "(", "roi_batch_splits_nms", ",", "\"batch_splits_nms\"", ")", "\n", "roi_keeps_nms", "=", "alias", "(", "roi_keeps_nms", ",", "\"keeps_nms\"", ")", "\n", "roi_keeps_size_nms", "=", "alias", "(", "roi_keeps_size_nms", ",", "\"keeps_size_nms\"", ")", "\n", "\n", "results", "=", "InstancesList", "(", "\n", "im_info", "=", "im_info", ",", "\n", "indices", "=", "roi_batch_ids", "[", ":", ",", "0", "]", ",", "\n", "extra_fields", "=", "{", "\n", "\"pred_boxes\"", ":", "Caffe2Boxes", "(", "roi_bbox_nms", ")", ",", "\n", "\"scores\"", ":", "roi_score_nms", ",", "\n", "\"pred_classes\"", ":", "roi_class_nms", ",", "\n", "}", ",", "\n", ")", "\n", "\n", "if", "not", "self", ".", "tensor_mode", ":", "\n", "            ", "results", "=", "InstancesList", ".", "to_d2_instances_list", "(", "results", ")", "\n", "batch_splits", "=", "roi_batch_splits_nms", ".", "int", "(", ")", ".", "tolist", "(", ")", "\n", "kept_indices", "=", "list", "(", "roi_keeps_nms", ".", "to", "(", "torch", ".", "int64", ")", ".", "split", "(", "batch_splits", ")", ")", "\n", "", "else", ":", "\n", "            ", "results", "=", "[", "results", "]", "\n", "kept_indices", "=", "[", "roi_keeps_nms", "]", "\n", "\n", "", "return", "results", ",", "kept_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2MaskRCNNInference.__call__": [[496, 505], ["all", "pred_mask_logits.sigmoid", "shared.alias", "detectron2.modeling.roi_heads.mask_head.mask_rcnn_inference", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.mask_head.mask_rcnn_inference"], ["    ", "def", "__call__", "(", "self", ",", "pred_mask_logits", ",", "pred_instances", ")", ":", "\n", "        ", "\"\"\"equivalent to mask_head.mask_rcnn_inference\"\"\"", "\n", "if", "all", "(", "isinstance", "(", "x", ",", "InstancesList", ")", "for", "x", "in", "pred_instances", ")", ":", "\n", "            ", "assert", "len", "(", "pred_instances", ")", "==", "1", "\n", "mask_probs_pred", "=", "pred_mask_logits", ".", "sigmoid", "(", ")", "\n", "mask_probs_pred", "=", "alias", "(", "mask_probs_pred", ",", "\"mask_fcn_probs\"", ")", "\n", "pred_instances", "[", "0", "]", ".", "pred_masks", "=", "mask_probs_pred", "\n", "", "else", ":", "\n", "            ", "mask_rcnn_inference", "(", "pred_mask_logits", ",", "pred_instances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2KeypointRCNNInference.__init__": [[508, 510], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "use_heatmap_max_keypoint", ")", ":", "\n", "        ", "self", ".", "use_heatmap_max_keypoint", "=", "use_heatmap_max_keypoint", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.Caffe2KeypointRCNNInference.__call__": [[511, 528], ["shared.alias", "all", "isinstance", "len", "torch.ops._caffe2.HeatmapMaxKeypoint", "torch.ops._caffe2.HeatmapMaxKeypoint", "torch.ops._caffe2.HeatmapMaxKeypoint", "torch.ops._caffe2.HeatmapMaxKeypoint", "shared.to_device", "shared.alias", "shared.to_device"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.alias", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.to_device"], ["", "def", "__call__", "(", "self", ",", "pred_keypoint_logits", ",", "pred_instances", ")", ":", "\n", "# just return the keypoint heatmap for now,", "\n", "# there will be option to call HeatmapMaxKeypointOp", "\n", "        ", "output", "=", "alias", "(", "pred_keypoint_logits", ",", "\"kps_score\"", ")", "\n", "if", "all", "(", "isinstance", "(", "x", ",", "InstancesList", ")", "for", "x", "in", "pred_instances", ")", ":", "\n", "            ", "assert", "len", "(", "pred_instances", ")", "==", "1", "\n", "if", "self", ".", "use_heatmap_max_keypoint", ":", "\n", "                ", "device", "=", "output", ".", "device", "\n", "output", "=", "torch", ".", "ops", ".", "_caffe2", ".", "HeatmapMaxKeypoint", "(", "\n", "to_device", "(", "output", ",", "\"cpu\"", ")", ",", "\n", "pred_instances", "[", "0", "]", ".", "pred_boxes", ".", "tensor", ",", "\n", "should_output_softmax", "=", "True", ",", "# worth make it configerable?", "\n", ")", "\n", "output", "=", "to_device", "(", "output", ",", "device", ")", "\n", "output", "=", "alias", "(", "output", ",", "\"keypoints_out\"", ")", "\n", "", "pred_instances", "[", "0", "]", ".", "pred_keypoints", "=", "output", "\n", "", "return", "pred_keypoint_logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.__init__": [[68, 91], ["isinstance", "isinstance", "type", "C2MetaArch", "api.Caffe2Tracer.traceable_model.get_caffe2_inputs", "api.add_export_config", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2MetaArch.get_caffe2_inputs", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.add_export_config"], ["def", "__init__", "(", "self", ",", "cfg", ":", "CfgNode", ",", "model", ":", "nn", ".", "Module", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            cfg (CfgNode): a detectron2 config, with extra export-related options\n                added by :func:`add_export_config`. It's used to construct\n                caffe2-compatible model.\n            model (nn.Module): An original pytorch model. Must be among a few official models\n                in detectron2 that can be converted to become caffe2-compatible automatically.\n                Weights have to be already loaded to this model.\n            inputs: sample inputs that the given model takes for inference.\n                Will be used to trace the model. For most models, random inputs with\n                no detected objects will not work as they lead to wrong traces.\n        \"\"\"", "\n", "assert", "isinstance", "(", "cfg", ",", "CfgNode", ")", ",", "cfg", "\n", "assert", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "Module", ")", ",", "type", "(", "model", ")", "\n", "\n", "if", "\"EXPORT_CAFFE2\"", "not", "in", "cfg", ":", "\n", "            ", "cfg", "=", "add_export_config", "(", "cfg", ")", "# will just the defaults", "\n", "# TODO make it support custom models, by passing in c2 model directly", "\n", "", "C2MetaArch", "=", "META_ARCH_CAFFE2_EXPORT_TYPE_MAP", "[", "cfg", ".", "MODEL", ".", "META_ARCHITECTURE", "]", "\n", "self", ".", "traceable_model", "=", "C2MetaArch", "(", "cfg", ",", "copy", ".", "deepcopy", "(", "model", ")", ")", "\n", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "traceable_inputs", "=", "self", ".", "traceable_model", ".", "get_caffe2_inputs", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_caffe2": [[92, 107], ["export_caffe2_detection_model", "api.Caffe2Model"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export.export_caffe2_detection_model"], ["", "def", "export_caffe2", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Export the model to Caffe2's protobuf format.\n        The returned object can be saved with its :meth:`.save_protobuf()` method.\n        The result can be loaded and executed using Caffe2 runtime.\n\n        Returns:\n            :class:`Caffe2Model`\n        \"\"\"", "\n", "from", ".", "caffe2_export", "import", "export_caffe2_detection_model", "\n", "\n", "predict_net", ",", "init_net", "=", "export_caffe2_detection_model", "(", "\n", "self", ".", "traceable_model", ",", "self", ".", "traceable_inputs", "\n", ")", "\n", "return", "Caffe2Model", "(", "predict_net", ",", "init_net", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_onnx": [[108, 122], ["export_onnx_model_impl"], "methods", ["None"], ["", "def", "export_onnx", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Export the model to ONNX format.\n        Note that the exported model contains custom ops only available in caffe2, therefore it\n        cannot be directly executed by other runtime (such as onnxruntime or TensorRT).\n        Post-processing or transformation passes may be applied on the model to accommodate\n        different runtimes, but we currently do not provide support for them.\n\n        Returns:\n            onnx.ModelProto: an onnx model.\n        \"\"\"", "\n", "from", ".", "caffe2_export", "import", "export_onnx_model", "as", "export_onnx_model_impl", "\n", "\n", "return", "export_onnx_model_impl", "(", "self", ".", "traceable_model", ",", "(", "self", ".", "traceable_inputs", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_torchscript": [[123, 135], ["logging.getLogger", "logging.getLogger.info", "torch.no_grad", "torch.jit.trace"], "methods", ["None"], ["", "def", "export_torchscript", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Export the model to a ``torch.jit.TracedModule`` by tracing.\n        The returned object can be saved to a file by ``.save()``.\n\n        Returns:\n            torch.jit.TracedModule: a torch TracedModule\n        \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"Tracing the model with torch.jit.trace ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "return", "torch", ".", "jit", ".", "trace", "(", "self", ".", "traceable_model", ",", "(", "self", ".", "traceable_inputs", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.__init__": [[153, 159], ["torch.nn.Module.__init__", "api.Caffe2Model.eval"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "eval", "(", ")", "# always in eval mode", "\n", "self", ".", "_predict_net", "=", "predict_net", "\n", "self", ".", "_init_net", "=", "init_net", "\n", "self", ".", "_predictor", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.predict_net": [[162, 168], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "predict_net", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        caffe2.core.Net: the underlying caffe2 predict net\n        \"\"\"", "\n", "return", "self", ".", "_predict_net", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.init_net": [[169, 175], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "init_net", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        caffe2.core.Net: the underlying caffe2 init net\n        \"\"\"", "\n", "return", "self", ".", "_init_net", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.save_protobuf": [[176, 201], ["logging.getLogger", "logging.getLogger.info", "detectron2.utils.file_io.PathManager.exists", "detectron2.utils.file_io.PathManager.mkdirs", "detectron2.utils.file_io.PathManager.open", "f.write", "detectron2.utils.file_io.PathManager.open", "f.write", "detectron2.utils.file_io.PathManager.open", "f.write", "os.path.join", "api.Caffe2Model._predict_net.SerializeToString", "os.path.join", "str", "os.path.join", "api.Caffe2Model._init_net.SerializeToString"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["", "def", "save_protobuf", "(", "self", ",", "output_dir", ")", ":", "\n", "        ", "\"\"\"\n        Save the model as caffe2's protobuf format.\n        It saves the following files:\n\n            * \"model.pb\": definition of the graph. Can be visualized with\n              tools like `netron <https://github.com/lutzroeder/netron>`_.\n            * \"model_init.pb\": model parameters\n            * \"model.pbtxt\": human-readable definition of the graph. Not\n              needed for deployment.\n\n        Args:\n            output_dir (str): the output directory to save protobuf files.\n        \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"Saving model to {} ...\"", ".", "format", "(", "output_dir", ")", ")", "\n", "if", "not", "PathManager", ".", "exists", "(", "output_dir", ")", ":", "\n", "            ", "PathManager", ".", "mkdirs", "(", "output_dir", ")", "\n", "\n", "", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.pb\"", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "self", ".", "_predict_net", ".", "SerializeToString", "(", ")", ")", "\n", "", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.pbtxt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "self", ".", "_predict_net", ")", ")", "\n", "", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model_init.pb\"", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "self", ".", "_init_net", ".", "SerializeToString", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.save_graph": [[202, 223], ["shared.save_graph", "shared.get_pb_arg_vali", "shared.get_pb_arg_vals().decode", "caffe2_modeling.convert_batched_inputs_to_c2_format", "run_and_save_graph", "x.cpu().numpy", "shared.get_pb_arg_vals", "x.cpu"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.save_graph", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vali", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.convert_batched_inputs_to_c2_format", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_export.run_and_save_graph", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vals"], ["", "", "def", "save_graph", "(", "self", ",", "output_file", ",", "inputs", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Save the graph as SVG format.\n\n        Args:\n            output_file (str): a SVG file\n            inputs: optional inputs given to the model.\n                If given, the inputs will be used to run the graph to record\n                shape of every tensor. The shape information will be\n                saved together with the graph.\n        \"\"\"", "\n", "from", ".", "caffe2_export", "import", "run_and_save_graph", "\n", "\n", "if", "inputs", "is", "None", ":", "\n", "            ", "save_graph", "(", "self", ".", "_predict_net", ",", "output_file", ",", "op_only", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "size_divisibility", "=", "get_pb_arg_vali", "(", "self", ".", "_predict_net", ",", "\"size_divisibility\"", ",", "0", ")", "\n", "device", "=", "get_pb_arg_vals", "(", "self", ".", "_predict_net", ",", "\"device\"", ",", "b\"cpu\"", ")", ".", "decode", "(", "\"ascii\"", ")", "\n", "inputs", "=", "convert_batched_inputs_to_c2_format", "(", "inputs", ",", "size_divisibility", ",", "device", ")", "\n", "inputs", "=", "[", "x", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "for", "x", "in", "inputs", "]", "\n", "run_and_save_graph", "(", "self", ".", "_predict_net", ",", "self", ".", "_init_net", ",", "inputs", ",", "output_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.load_protobuf": [[224, 244], ["caffe2.proto.caffe2_pb2.NetDef", "caffe2.proto.caffe2_pb2.NetDef", "api.Caffe2Model", "detectron2.utils.file_io.PathManager.open", "caffe2.proto.caffe2_pb2.NetDef.ParseFromString", "detectron2.utils.file_io.PathManager.open", "caffe2.proto.caffe2_pb2.NetDef.ParseFromString", "os.path.join", "f.read", "os.path.join", "f.read"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "load_protobuf", "(", "dir", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dir (str): a directory used to save Caffe2Model with\n                :meth:`save_protobuf`.\n                The files \"model.pb\" and \"model_init.pb\" are needed.\n\n        Returns:\n            Caffe2Model: the caffe2 model loaded from this directory.\n        \"\"\"", "\n", "predict_net", "=", "caffe2_pb2", ".", "NetDef", "(", ")", "\n", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "\"model.pb\"", ")", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "predict_net", ".", "ParseFromString", "(", "f", ".", "read", "(", ")", ")", "\n", "\n", "", "init_net", "=", "caffe2_pb2", ".", "NetDef", "(", ")", "\n", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "\"model_init.pb\"", ")", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "init_net", ".", "ParseFromString", "(", "f", ".", "read", "(", ")", ")", "\n", "\n", "", "return", "Caffe2Model", "(", "predict_net", ",", "init_net", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.__call__": [[245, 258], ["api.Caffe2Model._predictor", "caffe2_inference.ProtobufDetectionModel"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        An interface that wraps around a Caffe2 model and mimics detectron2's models'\n        input/output format. See details about the format at :doc:`/tutorials/models`.\n        This is used to compare the outputs of caffe2 model with its original torch model.\n\n        Due to the extra conversion between Pytorch/Caffe2, this method is not meant for\n        benchmark. Because of the conversion, this method also has dependency\n        on detectron2 in order to convert to detectron2's output format.\n        \"\"\"", "\n", "if", "self", ".", "_predictor", "is", "None", ":", "\n", "            ", "self", ".", "_predictor", "=", "ProtobufDetectionModel", "(", "self", ".", "_predict_net", ",", "self", ".", "_init_net", ")", "\n", "", "return", "self", ".", "_predictor", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.add_export_config": [[25, 43], ["cfg.is_frozen", "cfg.defrost", "detectron2.config.CfgNode", "cfg.freeze"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze"], ["def", "add_export_config", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Add options needed by caffe2 export.\n\n    Args:\n        cfg (CfgNode): a detectron2 config\n\n    Returns:\n        CfgNode:\n            an updated config with new options that will be used by :class:`Caffe2Tracer`.\n    \"\"\"", "\n", "is_frozen", "=", "cfg", ".", "is_frozen", "(", ")", "\n", "cfg", ".", "defrost", "(", ")", "\n", "cfg", ".", "EXPORT_CAFFE2", "=", "CfgNode", "(", ")", "\n", "cfg", ".", "EXPORT_CAFFE2", ".", "USE_HEATMAP_MAX_KEYPOINT", "=", "False", "\n", "if", "is_frozen", ":", "\n", "        ", "cfg", ".", "freeze", "(", ")", "\n", "", "return", "cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.export_caffe2_model": [[260, 266], ["logging.getLogger", "logging.getLogger.warning", "api.Caffe2Tracer.export_caffe2", "api.Caffe2Tracer"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_caffe2"], ["", "", "def", "export_caffe2_model", "(", "cfg", ",", "model", ",", "inputs", ")", ":", "\n", "    ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "warning", "(", "\n", "\"export_caffe2_model() is deprecated. Please use `Caffe2Tracer().export_caffe2() instead.\"", "\n", ")", "\n", "return", "Caffe2Tracer", "(", "cfg", ",", "model", ",", "inputs", ")", ".", "export_caffe2", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.export_onnx_model": [[268, 274], ["logging.getLogger", "logging.getLogger.warning", "api.Caffe2Tracer.export_onnx", "api.Caffe2Tracer"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_onnx"], ["", "def", "export_onnx_model", "(", "cfg", ",", "model", ",", "inputs", ")", ":", "\n", "    ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "warning", "(", "\n", "\"export_caffe2_model() is deprecated. Please use `Caffe2Tracer().export_onnx() instead.\"", "\n", ")", "\n", "return", "Caffe2Tracer", "(", "cfg", ",", "model", ",", "inputs", ")", ".", "export_onnx", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.Schema.flatten": [[35, 38], ["None"], "methods", ["None"], ["@", "classmethod", "\n", "def", "flatten", "(", "cls", ",", "obj", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.Schema.__call__": [[39, 41], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "values", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.Schema._concat": [[42, 51], ["isinstance", "sizes.append", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_concat", "(", "values", ")", ":", "\n", "        ", "ret", "=", "(", ")", "\n", "sizes", "=", "[", "]", "\n", "for", "v", "in", "values", ":", "\n", "            ", "assert", "isinstance", "(", "v", ",", "tuple", ")", ",", "\"Flattened results must be a tuple\"", "\n", "ret", "=", "ret", "+", "v", "\n", "sizes", ".", "append", "(", "len", "(", "v", ")", ")", "\n", "", "return", "ret", ",", "sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.Schema._split": [[52, 64], ["len", "range", "sum", "len", "ret.append", "len", "sum", "sum", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_split", "(", "values", ",", "sizes", ")", ":", "\n", "        ", "if", "len", "(", "sizes", ")", ":", "\n", "            ", "expected_len", "=", "sum", "(", "sizes", ")", "\n", "assert", "(", "\n", "len", "(", "values", ")", "==", "expected_len", "\n", ")", ",", "f\"Values has length {len(values)} but expect length {expected_len}.\"", "\n", "", "ret", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "len", "(", "sizes", ")", ")", ":", "\n", "            ", "begin", ",", "end", "=", "sum", "(", "sizes", "[", ":", "k", "]", ")", ",", "sum", "(", "sizes", "[", ":", "k", "+", "1", "]", ")", "\n", "ret", ".", "append", "(", "values", "[", "begin", ":", "end", "]", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.ListSchema.__call__": [[71, 79], ["flatten.ListSchema._split", "list", "len", "len", "ValueError", "m", "zip", "len", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.Schema._split", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["def", "__call__", "(", "self", ",", "values", ")", ":", "\n", "        ", "values", "=", "self", ".", "_split", "(", "values", ",", "self", ".", "sizes", ")", "\n", "if", "len", "(", "values", ")", "!=", "len", "(", "self", ".", "schemas", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Values has length {len(values)} but schemas \"", "f\"has length {len(self.schemas)}!\"", "\n", ")", "\n", "", "values", "=", "[", "m", "(", "v", ")", "for", "m", ",", "v", "in", "zip", "(", "self", ".", "schemas", ",", "values", ")", "]", "\n", "return", "list", "(", "values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.ListSchema.flatten": [[80, 85], ["cls._concat", "flatten.flatten_to_tuple", "cls"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.Schema._concat", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.flatten_to_tuple"], ["", "@", "classmethod", "\n", "def", "flatten", "(", "cls", ",", "obj", ")", ":", "\n", "        ", "res", "=", "[", "flatten_to_tuple", "(", "k", ")", "for", "k", "in", "obj", "]", "\n", "values", ",", "sizes", "=", "cls", ".", "_concat", "(", "[", "k", "[", "0", "]", "for", "k", "in", "res", "]", ")", "\n", "return", "values", ",", "cls", "(", "[", "k", "[", "1", "]", "for", "k", "in", "res", "]", ",", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TupleSchema.__call__": [[89, 91], ["tuple", "flatten.ListSchema.__call__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomChoice.__call__"], ["    ", "def", "__call__", "(", "self", ",", "values", ")", ":", "\n", "        ", "return", "tuple", "(", "super", "(", ")", ".", "__call__", "(", "values", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.IdentitySchema.__call__": [[95, 97], ["None"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "values", ")", ":", "\n", "        ", "return", "values", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.IdentitySchema.flatten": [[98, 101], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "flatten", "(", "cls", ",", "obj", ")", ":", "\n", "        ", "return", "(", "obj", ",", ")", ",", "cls", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.DictSchema.__call__": [[107, 110], ["flatten.ListSchema.__call__", "dict", "zip"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomChoice.__call__"], ["def", "__call__", "(", "self", ",", "values", ")", ":", "\n", "        ", "values", "=", "super", "(", ")", ".", "__call__", "(", "values", ")", "\n", "return", "dict", "(", "zip", "(", "self", ".", "keys", ",", "values", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.DictSchema.flatten": [[111, 120], ["obj.keys", "sorted", "ListSchema.flatten", "obj.keys", "cls", "isinstance", "KeyError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "@", "classmethod", "\n", "def", "flatten", "(", "cls", ",", "obj", ")", ":", "\n", "        ", "for", "k", "in", "obj", ".", "keys", "(", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "k", ",", "str", ")", ":", "\n", "                ", "raise", "KeyError", "(", "\"Only support flattening dictionaries if keys are str.\"", ")", "\n", "", "", "keys", "=", "sorted", "(", "obj", ".", "keys", "(", ")", ")", "\n", "values", "=", "[", "obj", "[", "k", "]", "for", "k", "in", "keys", "]", "\n", "ret", ",", "schema", "=", "ListSchema", ".", "flatten", "(", "values", ")", "\n", "return", "ret", ",", "cls", "(", "schema", ".", "schemas", ",", "schema", ".", "sizes", ",", "keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.InstancesSchema.__call__": [[124, 128], ["flatten.DictSchema.__call__", "detectron2.structures.Instances"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomChoice.__call__"], ["    ", "def", "__call__", "(", "self", ",", "values", ")", ":", "\n", "        ", "image_size", ",", "fields", "=", "values", "[", "-", "1", "]", ",", "values", "[", ":", "-", "1", "]", "\n", "fields", "=", "super", "(", ")", ".", "__call__", "(", "fields", ")", "\n", "return", "Instances", "(", "image_size", ",", "**", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.InstancesSchema.flatten": [[129, 136], ["flatten.DictSchema.flatten", "obj.get_fields", "isinstance", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.get_fields"], ["", "@", "classmethod", "\n", "def", "flatten", "(", "cls", ",", "obj", ")", ":", "\n", "        ", "ret", ",", "schema", "=", "super", "(", ")", ".", "flatten", "(", "obj", ".", "get_fields", "(", ")", ")", "\n", "size", "=", "obj", ".", "image_size", "\n", "if", "not", "isinstance", "(", "size", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "size", "=", "torch", ".", "tensor", "(", "size", ")", "\n", "", "return", "ret", "+", "(", "size", ",", ")", ",", "schema", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.__call__": [[147, 149], ["detectron2.utils.registry.locate"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry.locate"], ["def", "__call__", "(", "self", ",", "values", ")", ":", "\n", "        ", "return", "locate", "(", "self", ".", "class_name", ")", "(", "values", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten": [[150, 153], ["cls", "detectron2.utils.registry._convert_target_to_string", "type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry._convert_target_to_string"], ["", "@", "classmethod", "\n", "def", "flatten", "(", "cls", ",", "obj", ")", ":", "\n", "        ", "return", "(", "obj", ".", "tensor", ",", ")", ",", "cls", "(", "_convert_target_to_string", "(", "type", "(", "obj", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TracingAdapter.__init__": [[224, 275], ["torch.nn.Module.__init__", "isinstance", "flatten.flatten_to_tuple", "all", "isinstance", "tuple", "model", "isinstance", "isinstance", "ValueError", "isinstance", "type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.flatten_to_tuple"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "nn", ".", "Module", ",", "\n", "inputs", ",", "\n", "inference_func", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "\n", "allow_non_tensor", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            model: an nn.Module\n            inputs: An input argument or a tuple of input arguments used to call model.\n                After flattening, it has to only consist of tensors.\n            inference_func: a callable that takes (model, *inputs), calls the\n                model with inputs, and return outputs. By default it\n                is ``lambda model, *inputs: model(*inputs)``. Can be override\n                if you need to call the model differently.\n            allow_non_tensor: allow inputs/outputs to contain non-tensor objects.\n                This option will filter out non-tensor objects to make the\n                model traceable, but ``inputs_schema``/``outputs_schema`` cannot be\n                used anymore because inputs/outputs cannot be rebuilt from pure tensors.\n                This is useful when you're only interested in the single trace of\n                execution (e.g. for flop count), but not interested in\n                generalizing the traced graph to new inputs.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "isinstance", "(", "model", ",", "(", "nn", ".", "parallel", ".", "distributed", ".", "DistributedDataParallel", ",", "nn", ".", "DataParallel", ")", ")", ":", "\n", "            ", "model", "=", "model", ".", "module", "\n", "", "self", ".", "model", "=", "model", "\n", "if", "not", "isinstance", "(", "inputs", ",", "tuple", ")", ":", "\n", "            ", "inputs", "=", "(", "inputs", ",", ")", "\n", "", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "allow_non_tensor", "=", "allow_non_tensor", "\n", "\n", "if", "inference_func", "is", "None", ":", "\n", "            ", "inference_func", "=", "lambda", "model", ",", "*", "inputs", ":", "model", "(", "*", "inputs", ")", "# noqa", "\n", "", "self", ".", "inference_func", "=", "inference_func", "\n", "\n", "self", ".", "flattened_inputs", ",", "self", ".", "inputs_schema", "=", "flatten_to_tuple", "(", "inputs", ")", "\n", "\n", "if", "all", "(", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", "for", "x", "in", "self", ".", "flattened_inputs", ")", ":", "\n", "            ", "return", "\n", "", "if", "self", ".", "allow_non_tensor", ":", "\n", "            ", "self", ".", "flattened_inputs", "=", "tuple", "(", "\n", "[", "x", "for", "x", "in", "self", ".", "flattened_inputs", "if", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", "]", "\n", ")", "\n", "self", ".", "inputs_schema", "=", "None", "\n", "", "else", ":", "\n", "            ", "for", "input", "in", "self", ".", "flattened_inputs", ":", "\n", "                ", "if", "not", "isinstance", "(", "input", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Inputs for tracing must only contain tensors. \"", "\n", "f\"Got a {type(input)} instead.\"", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TracingAdapter.forward": [[278, 315], ["torch.no_grad", "torchscript_patch.patch_builtin_len", "flatten.TracingAdapter.inference_func", "flatten.flatten_to_tuple", "tuple", "flatten.TracingAdapter.inputs_schema", "len", "len", "ValueError", "ValueError", "isinstance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_builtin_len", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.flatten_to_tuple"], ["", "", "", "", "def", "forward", "(", "self", ",", "*", "args", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ",", "patch_builtin_len", "(", ")", ":", "\n", "            ", "if", "self", ".", "inputs_schema", "is", "not", "None", ":", "\n", "                ", "inputs_orig_format", "=", "self", ".", "inputs_schema", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "if", "args", "!=", "self", ".", "flattened_inputs", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"TracingAdapter does not contain valid inputs_schema.\"", "\n", "\" So it cannot generalize to other inputs and must be\"", "\n", "\" traced with `.flattened_inputs`.\"", "\n", ")", "\n", "", "inputs_orig_format", "=", "self", ".", "inputs", "\n", "\n", "", "outputs", "=", "self", ".", "inference_func", "(", "self", ".", "model", ",", "*", "inputs_orig_format", ")", "\n", "flattened_outputs", ",", "schema", "=", "flatten_to_tuple", "(", "outputs", ")", "\n", "\n", "flattened_output_tensors", "=", "tuple", "(", "\n", "[", "x", "for", "x", "in", "flattened_outputs", "if", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", "]", "\n", ")", "\n", "if", "len", "(", "flattened_output_tensors", ")", "<", "len", "(", "flattened_outputs", ")", ":", "\n", "                ", "if", "self", ".", "allow_non_tensor", ":", "\n", "                    ", "flattened_outputs", "=", "flattened_output_tensors", "\n", "self", ".", "outputs_schema", "=", "None", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Model cannot be traced because some model outputs \"", "\n", "\"cannot flatten to tensors.\"", "\n", ")", "\n", "", "", "else", ":", "# schema is valid", "\n", "                ", "if", "self", ".", "outputs_schema", "is", "None", ":", "\n", "                    ", "self", ".", "outputs_schema", "=", "schema", "\n", "", "else", ":", "\n", "                    ", "assert", "self", ".", "outputs_schema", "==", "schema", ",", "(", "\n", "\"Model should always return outputs with the same \"", "\n", "\"structure so it can be traced!\"", "\n", ")", "\n", "", "", "return", "flattened_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TracingAdapter._create_wrapper": [[316, 328], ["flatten.flatten_to_tuple", "traced_model", "flatten.TracingAdapter.outputs_schema"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.flatten_to_tuple"], ["", "", "def", "_create_wrapper", "(", "self", ",", "traced_model", ")", ":", "\n", "        ", "\"\"\"\n        Return a function that has an input/output interface the same as the\n        original model, but it calls the given traced model under the hood.\n        \"\"\"", "\n", "\n", "def", "forward", "(", "*", "args", ")", ":", "\n", "            ", "flattened_inputs", ",", "_", "=", "flatten_to_tuple", "(", "args", ")", "\n", "flattened_outputs", "=", "traced_model", "(", "*", "flattened_inputs", ")", "\n", "return", "self", ".", "outputs_schema", "(", "flattened_outputs", ")", "\n", "\n", "", "return", "forward", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.flatten_to_tuple": [[157, 183], ["F.flatten", "isinstance"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "", "def", "flatten_to_tuple", "(", "obj", ")", ":", "\n", "    ", "\"\"\"\n    Flatten an object so it can be used for PyTorch tracing.\n    Also returns how to rebuild the original object from the flattened outputs.\n\n    Returns:\n        res (tuple): the flattened results that can be used as tracing outputs\n        schema: an object with a ``__call__`` method such that ``schema(res) == obj``.\n             It is a pure dataclass that can be serialized.\n    \"\"\"", "\n", "schemas", "=", "[", "\n", "(", "(", "str", ",", "bytes", ")", ",", "IdentitySchema", ")", ",", "\n", "(", "list", ",", "ListSchema", ")", ",", "\n", "(", "tuple", ",", "TupleSchema", ")", ",", "\n", "(", "collections", ".", "abc", ".", "Mapping", ",", "DictSchema", ")", ",", "\n", "(", "Instances", ",", "InstancesSchema", ")", ",", "\n", "(", "(", "Boxes", ",", "ROIMasks", ")", ",", "TensorWrapSchema", ")", ",", "\n", "]", "\n", "for", "klass", ",", "schema", "in", "schemas", ":", "\n", "        ", "if", "isinstance", "(", "obj", ",", "klass", ")", ":", "\n", "            ", "F", "=", "schema", "\n", "break", "\n", "", "", "else", ":", "\n", "        ", "F", "=", "IdentitySchema", "\n", "\n", "", "return", "F", ".", "flatten", "(", "obj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_inference.ProtobufModel.__init__": [[26, 47], ["logger.info", "super().__init__", "isinstance", "isinstance", "caffe2.python.core.Net", "logger.info", "set", "next", "shared.ScopedWS", "ws.RunNetOnce", "ws.CreateNet", "caffe2_inference.ProtobufModel.net.Proto", "ws.Blobs", "uninitialized_external_input.append", "ws.CreateBlob"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["def", "__init__", "(", "self", ",", "predict_net", ",", "init_net", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f\"Initializing ProtobufModel for: {predict_net.name} ...\"", ")", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "isinstance", "(", "predict_net", ",", "caffe2_pb2", ".", "NetDef", ")", "\n", "assert", "isinstance", "(", "init_net", ",", "caffe2_pb2", ".", "NetDef", ")", "\n", "# create unique temporary workspace for each instance", "\n", "self", ".", "ws_name", "=", "\"__tmp_ProtobufModel_{}__\"", ".", "format", "(", "next", "(", "self", ".", "_ids", ")", ")", "\n", "self", ".", "net", "=", "core", ".", "Net", "(", "predict_net", ")", "\n", "\n", "logger", ".", "info", "(", "\"Running init_net once to fill the parameters ...\"", ")", "\n", "with", "ScopedWS", "(", "self", ".", "ws_name", ",", "is_reset", "=", "True", ",", "is_cleanup", "=", "False", ")", "as", "ws", ":", "\n", "            ", "ws", ".", "RunNetOnce", "(", "init_net", ")", "\n", "uninitialized_external_input", "=", "[", "]", "\n", "for", "blob", "in", "self", ".", "net", ".", "Proto", "(", ")", ".", "external_input", ":", "\n", "                ", "if", "blob", "not", "in", "ws", ".", "Blobs", "(", ")", ":", "\n", "                    ", "uninitialized_external_input", ".", "append", "(", "blob", ")", "\n", "ws", ".", "CreateBlob", "(", "blob", ")", "\n", "", "", "ws", ".", "CreateNet", "(", "self", ".", "net", ")", "\n", "\n", "", "self", ".", "_error_msgs", "=", "set", "(", ")", "\n", "self", ".", "_input_blobs", "=", "uninitialized_external_input", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_inference.ProtobufModel._infer_output_devices": [[48, 70], ["caffe2_inference.ProtobufModel.net.Proto", "shared.infer_device_type", "caffe2.python.core.get_ssa", "caffe2_inference.ProtobufModel._infer_output_devices._get_device_type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.infer_device_type"], ["", "def", "_infer_output_devices", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            list[str]: list of device for each external output\n        \"\"\"", "\n", "\n", "def", "_get_device_type", "(", "torch_tensor", ")", ":", "\n", "            ", "assert", "torch_tensor", ".", "device", ".", "type", "in", "[", "\"cpu\"", ",", "\"cuda\"", "]", "\n", "assert", "torch_tensor", ".", "device", ".", "index", "==", "0", "\n", "return", "torch_tensor", ".", "device", ".", "type", "\n", "\n", "", "predict_net", "=", "self", ".", "net", ".", "Proto", "(", ")", "\n", "input_device_types", "=", "{", "\n", "(", "name", ",", "0", ")", ":", "_get_device_type", "(", "tensor", ")", "for", "name", ",", "tensor", "in", "zip", "(", "self", ".", "_input_blobs", ",", "inputs", ")", "\n", "}", "\n", "device_type_map", "=", "infer_device_type", "(", "\n", "predict_net", ",", "known_status", "=", "input_device_types", ",", "device_name_style", "=", "\"pytorch\"", "\n", ")", "\n", "ssa", ",", "versions", "=", "core", ".", "get_ssa", "(", "predict_net", ")", "\n", "versioned_outputs", "=", "[", "(", "name", ",", "versions", "[", "name", "]", ")", "for", "name", "in", "predict_net", ".", "external_output", "]", "\n", "output_devices", "=", "[", "device_type_map", "[", "outp", "]", "for", "outp", "in", "versioned_outputs", "]", "\n", "return", "output_devices", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_inference.ProtobufModel.forward": [[71, 123], ["zip", "tuple", "len", "len", "shared.ScopedWS", "zip", "any", "caffe2_inference.ProtobufModel._infer_output_devices", "outputs.append", "len", "ws.FeedBlob", "ws.RunNet", "ws.FetchBlob", "caffe2_inference.ProtobufModel.net.Proto", "ws.FeedBlob", "caffe2_inference.ProtobufModel.net.Proto", "isinstance", "RuntimeError", "torch.tensor().to", "logger.warning", "caffe2_inference.ProtobufModel.net.Proto", "caffe2_inference.ProtobufModel._error_msgs.add", "logger.warning", "caffe2_inference.ProtobufModel.net.Proto", "caffe2_inference.ProtobufModel.net.Proto", "torch.tensor", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_inference.ProtobufModel._infer_output_devices", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            inputs (tuple[torch.Tensor])\n\n        Returns:\n            tuple[torch.Tensor]\n        \"\"\"", "\n", "assert", "len", "(", "inputs", ")", "==", "len", "(", "self", ".", "_input_blobs", ")", ",", "(", "\n", "f\"Length of inputs ({len(inputs)}) \"", "\n", "f\"doesn't match the required input blobs: {self._input_blobs}\"", "\n", ")", "\n", "\n", "with", "ScopedWS", "(", "self", ".", "ws_name", ",", "is_reset", "=", "False", ",", "is_cleanup", "=", "False", ")", "as", "ws", ":", "\n", "            ", "for", "b", ",", "tensor", "in", "zip", "(", "self", ".", "_input_blobs", ",", "inputs", ")", ":", "\n", "                ", "ws", ".", "FeedBlob", "(", "b", ",", "tensor", ")", "\n", "\n", "", "try", ":", "\n", "                ", "ws", ".", "RunNet", "(", "self", ".", "net", ".", "Proto", "(", ")", ".", "name", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "if", "not", "str", "(", "e", ")", "in", "self", ".", "_error_msgs", ":", "\n", "                    ", "self", ".", "_error_msgs", ".", "add", "(", "str", "(", "e", ")", ")", "\n", "logger", ".", "warning", "(", "\"Encountered new RuntimeError: \\n{}\"", ".", "format", "(", "str", "(", "e", ")", ")", ")", "\n", "", "logger", ".", "warning", "(", "\"Catch the error and use partial results.\"", ")", "\n", "\n", "", "c2_outputs", "=", "[", "ws", ".", "FetchBlob", "(", "b", ")", "for", "b", "in", "self", ".", "net", ".", "Proto", "(", ")", ".", "external_output", "]", "\n", "# Remove outputs of current run, this is necessary in order to", "\n", "# prevent fetching the result from previous run if the model fails", "\n", "# in the middle.", "\n", "for", "b", "in", "self", ".", "net", ".", "Proto", "(", ")", ".", "external_output", ":", "\n", "# Needs to create uninitialized blob to make the net runable.", "\n", "# This is \"equivalent\" to: ws.RemoveBlob(b) then ws.CreateBlob(b),", "\n", "# but there'no such API.", "\n", "                ", "ws", ".", "FeedBlob", "(", "b", ",", "f\"{b}, a C++ native class of type nullptr (uninitialized).\"", ")", "\n", "\n", "# Cast output to torch.Tensor on the desired device", "\n", "", "", "output_devices", "=", "(", "\n", "self", ".", "_infer_output_devices", "(", "inputs", ")", "\n", "if", "any", "(", "t", ".", "device", ".", "type", "!=", "\"cpu\"", "for", "t", "in", "inputs", ")", "\n", "else", "[", "\"cpu\"", "for", "_", "in", "self", ".", "net", ".", "Proto", "(", ")", ".", "external_output", "]", "\n", ")", "\n", "\n", "outputs", "=", "[", "]", "\n", "for", "name", ",", "c2_output", ",", "device", "in", "zip", "(", "\n", "self", ".", "net", ".", "Proto", "(", ")", ".", "external_output", ",", "c2_outputs", ",", "output_devices", "\n", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "c2_output", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Invalid output for blob {}, received: {}\"", ".", "format", "(", "name", ",", "c2_output", ")", "\n", ")", "\n", "", "outputs", ".", "append", "(", "torch", ".", "tensor", "(", "c2_output", ")", ".", "to", "(", "device", "=", "device", ")", ")", "\n", "", "return", "tuple", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_inference.ProtobufDetectionModel.__init__": [[131, 150], ["super().__init__", "caffe2_inference.ProtobufModel", "shared.get_pb_arg_vali", "shared.get_pb_arg_vals().decode", "shared.get_pb_arg_vals", "shared.get_pb_arg_vals.get_outputs_converter", "shared.get_pb_arg_vals", "shared.get_pb_arg_vals.decode"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vali", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vals", "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.Caffe2RetinaNet.get_outputs_converter", "home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.get_pb_arg_vals", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode"], ["def", "__init__", "(", "self", ",", "predict_net", ",", "init_net", ",", "*", ",", "convert_outputs", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            predict_net, init_net (core.Net): caffe2 nets\n            convert_outptus (callable): a function that converts caffe2\n                outputs to the same format of the original pytorch model.\n                By default, use the one defined in the caffe2 meta_arch.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "protobuf_model", "=", "ProtobufModel", "(", "predict_net", ",", "init_net", ")", "\n", "self", ".", "size_divisibility", "=", "get_pb_arg_vali", "(", "predict_net", ",", "\"size_divisibility\"", ",", "0", ")", "\n", "self", ".", "device", "=", "get_pb_arg_vals", "(", "predict_net", ",", "\"device\"", ",", "b\"cpu\"", ")", ".", "decode", "(", "\"ascii\"", ")", "\n", "\n", "if", "convert_outputs", "is", "None", ":", "\n", "            ", "meta_arch", "=", "get_pb_arg_vals", "(", "predict_net", ",", "\"meta_architecture\"", ",", "b\"GeneralizedRCNN\"", ")", "\n", "meta_arch", "=", "META_ARCH_CAFFE2_EXPORT_TYPE_MAP", "[", "meta_arch", ".", "decode", "(", "\"ascii\"", ")", "]", "\n", "self", ".", "_convert_outputs", "=", "meta_arch", ".", "get_outputs_converter", "(", "predict_net", ",", "init_net", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_convert_outputs", "=", "convert_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_inference.ProtobufDetectionModel._convert_inputs": [[151, 155], ["caffe2_modeling.convert_batched_inputs_to_c2_format"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_modeling.convert_batched_inputs_to_c2_format"], ["", "", "def", "_convert_inputs", "(", "self", ",", "batched_inputs", ")", ":", "\n", "# currently all models convert inputs in the same way", "\n", "        ", "return", "convert_batched_inputs_to_c2_format", "(", "\n", "batched_inputs", ",", "self", ".", "size_divisibility", ",", "self", ".", "device", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_inference.ProtobufDetectionModel.forward": [[157, 162], ["caffe2_inference.ProtobufDetectionModel._convert_inputs", "caffe2_inference.ProtobufDetectionModel.protobuf_model", "dict", "caffe2_inference.ProtobufDetectionModel._convert_outputs", "zip", "caffe2_inference.ProtobufDetectionModel.protobuf_model.net.Proto"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.caffe2_inference.ProtobufDetectionModel._convert_inputs"], ["", "def", "forward", "(", "self", ",", "batched_inputs", ")", ":", "\n", "        ", "c2_inputs", "=", "self", ".", "_convert_inputs", "(", "batched_inputs", ")", "\n", "c2_results", "=", "self", ".", "protobuf_model", "(", "c2_inputs", ")", "\n", "c2_results", "=", "dict", "(", "zip", "(", "self", ".", "protobuf_model", ".", "net", ".", "Proto", "(", ")", ".", "external_output", ",", "c2_results", ")", ")", "\n", "return", "self", ".", "_convert_outputs", "(", "batched_inputs", ",", "c2_inputs", ",", "c2_results", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultPredictor.__init__": [[281, 297], ["cfg.clone", "detectron2.modeling.build_model", "defaults.DefaultPredictor.model.eval", "len", "detectron2.checkpoint.DetectionCheckpointer", "detectron2.checkpoint.DetectionCheckpointer.load", "detectron2.ResizeShortestEdge", "detectron2.data.MetadataCatalog.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["# Format: list[list[float]]. SIZES[i] specifies the list of sizes to use for", "\n", "# IN_FEATURES[i]; len(SIZES) must be equal to len(IN_FEATURES) or 1.", "\n", "# When len(SIZES) == 1, SIZES[0] is used for all IN_FEATURES.", "\n", "_C", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "SIZES", "=", "[", "[", "32", ",", "64", ",", "128", ",", "256", ",", "512", "]", "]", "\n", "# Anchor aspect ratios. For each area given in `SIZES`, anchors with different aspect", "\n", "# ratios are generated by an anchor generator.", "\n", "# Format: list[list[float]]. ASPECT_RATIOS[i] specifies the list of aspect ratios (H/W)", "\n", "# to use for IN_FEATURES[i]; len(ASPECT_RATIOS) == len(IN_FEATURES) must be true,", "\n", "# or len(ASPECT_RATIOS) == 1 is true and aspect ratio list ASPECT_RATIOS[0] is used", "\n", "# for all IN_FEATURES.", "\n", "_C", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ASPECT_RATIOS", "=", "[", "[", "0.5", ",", "1.0", ",", "2.0", "]", "]", "\n", "# Anchor angles.", "\n", "# list[list[float]], the angle in degrees, for each input feature map.", "\n", "# ANGLES[i] specifies the list of angles for IN_FEATURES[i].", "\n", "_C", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "ANGLES", "=", "[", "[", "-", "90", ",", "0", ",", "90", "]", "]", "\n", "# Relative offset between the center of the first anchor and the top-left corner of the image", "\n", "# Value has to be in [0, 1). Recommend to use 0.5, which means half stride.", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultPredictor.__call__": [[298, 320], ["torch.no_grad", "defaults.DefaultPredictor.aug.get_transform().apply_image", "torch.as_tensor", "torch.as_tensor.astype().transpose", "defaults.DefaultPredictor.model", "defaults.DefaultPredictor.aug.get_transform", "torch.as_tensor.astype"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.get_transform"], ["# The value is not expected to affect model accuracy.", "\n", "_C", ".", "MODEL", ".", "ANCHOR_GENERATOR", ".", "OFFSET", "=", "0.0", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# RPN options", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "RPN", "=", "CN", "(", ")", "\n", "_C", ".", "MODEL", ".", "RPN", ".", "HEAD_NAME", "=", "\"StandardRPNHead\"", "# used by RPN_HEAD_REGISTRY", "\n", "\n", "# Names of the input feature maps to be used by RPN", "\n", "# e.g., [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"] for FPN", "\n", "_C", ".", "MODEL", ".", "RPN", ".", "IN_FEATURES", "=", "[", "\"res4\"", "]", "\n", "# Remove RPN anchors that go outside the image by BOUNDARY_THRESH pixels", "\n", "# Set to -1 or a large value, e.g. 100000, to disable pruning anchors", "\n", "_C", ".", "MODEL", ".", "RPN", ".", "BOUNDARY_THRESH", "=", "-", "1", "\n", "# IOU overlap ratios [BG_IOU_THRESHOLD, FG_IOU_THRESHOLD]", "\n", "# Minimum overlap required between an anchor and ground-truth box for the", "\n", "# (anchor, gt box) pair to be a positive example (IoU >= FG_IOU_THRESHOLD", "\n", "# ==> positive RPN example: 1)", "\n", "# Maximum overlap allowed between an anchor and ground-truth box for the", "\n", "# (anchor, gt box) pair to be a negative examples (IoU < BG_IOU_THRESHOLD", "\n", "# ==> negative RPN example: 0)", "\n", "# Anchors with overlap in between (BG_IOU_THRESHOLD <= IoU < FG_IOU_THRESHOLD)", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.__init__": [[365, 405], ["train_loop.TrainerBase.__init__", "logging.getLogger", "defaults.DefaultTrainer.auto_scale_workers", "defaults.DefaultTrainer.build_model", "defaults.DefaultTrainer.build_optimizer", "defaults.DefaultTrainer.build_train_loader", "defaults.create_ddp_model", "defaults.DefaultTrainer.build_lr_scheduler", "detectron2.checkpoint.DetectionCheckpointer", "defaults.DefaultTrainer.register_hooks", "logging.getLogger.isEnabledFor", "detectron2.utils.logger.setup_logger", "detectron2.utils.comm.get_world_size", "detectron2.checkpoint.DetectionCheckpointer", "defaults.DefaultTrainer.build_hooks", "weakref.proxy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.auto_scale_workers", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.build_optimizer", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_train_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.create_ddp_model", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.build_lr_scheduler", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.register_hooks", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_hooks"], ["# Overlap threshold for an RoI to be considered background (if < IOU_THRESHOLD)", "\n", "# Overlap threshold for an RoI to be considered foreground (if >= IOU_THRESHOLD)", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "IOU_THRESHOLDS", "=", "[", "0.5", "]", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "IOU_LABELS", "=", "[", "0", ",", "1", "]", "\n", "# RoI minibatch size *per image* (number of regions of interest [ROIs])", "\n", "# Total number of RoIs per training minibatch =", "\n", "#   ROI_HEADS.BATCH_SIZE_PER_IMAGE * SOLVER.IMS_PER_BATCH", "\n", "# E.g., a common configuration is: 512 * 16 = 8192", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "BATCH_SIZE_PER_IMAGE", "=", "512", "\n", "# Target fraction of RoI minibatch that is labeled foreground (i.e. class > 0)", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "POSITIVE_FRACTION", "=", "0.25", "\n", "\n", "# Only used on test mode", "\n", "\n", "# Minimum score threshold (assuming scores in a [0, 1] range); a value chosen to", "\n", "# balance obtaining high recall with not having too many low precision", "\n", "# detections that will slow down inference post processing steps (like NMS)", "\n", "# A default threshold of 0.0 increases AP by ~0.2-0.3 but significantly slows down", "\n", "# inference.", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "SCORE_THRESH_TEST", "=", "0.05", "\n", "# Overlap threshold used for non-maximum suppression (suppress boxes with", "\n", "# IoU >= this threshold)", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "NMS_THRESH_TEST", "=", "0.5", "\n", "# If True, augment proposals with ground-truth boxes before sampling proposals to", "\n", "# train ROI heads.", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "PROPOSAL_APPEND_GT", "=", "True", "\n", "\n", "# Use soft NMS instead of standard NMS if set to True", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "SOFT_NMS_ENABLED", "=", "False", "\n", "# See soft NMS paper for definition of these options", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "SOFT_NMS_METHOD", "=", "\"gaussian\"", "# \"linear\"", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "SOFT_NMS_SIGMA", "=", "0.5", "\n", "# For the linear_threshold we use NMS_THRESH_TEST", "\n", "_C", ".", "MODEL", ".", "ROI_HEADS", ".", "SOFT_NMS_PRUNE", "=", "0.001", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# Box Head", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", "=", "CN", "(", ")", "\n", "# C4 don't use head name option", "\n", "# Options for non-C4 models: FastRCNNConvFCHead,", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load": [[406, 429], ["defaults.DefaultTrainer.checkpointer.resume_or_load", "defaults.DefaultTrainer.second_checkpointer.resume_or_load", "defaults.DefaultTrainer.checkpointer.has_checkpoint"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load"], ["_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NAME", "=", "\"\"", "\n", "# Options are: \"smooth_l1\", \"giou\"", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_LOSS_TYPE", "=", "\"smooth_l1\"", "\n", "# The final scaling coefficient on the box regression loss, used to balance the magnitude of its", "\n", "# gradients with other losses in the model. See also `MODEL.ROI_KEYPOINT_HEAD.LOSS_WEIGHT`.", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_LOSS_WEIGHT", "=", "1.0", "\n", "# Default weights on (dx, dy, dw, dh) for normalizing bbox regression targets", "\n", "# These are empirically chosen to approximately lead to unit variance targets", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "BBOX_REG_WEIGHTS", "=", "(", "10.0", ",", "10.0", ",", "5.0", ",", "5.0", ")", "\n", "# The transition point from L1 to L2 loss. Set to 0.0 to make the loss simply L1.", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "SMOOTH_L1_BETA", "=", "0.0", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_RESOLUTION", "=", "14", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_SAMPLING_RATIO", "=", "0", "\n", "# Type of pooling operation applied to the incoming feature map for each RoI", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "POOLER_TYPE", "=", "\"ROIAlignV2\"", "\n", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NUM_FC", "=", "0", "\n", "# Hidden layer dimension for FC layers in the RoI box head", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "FC_DIM", "=", "1024", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NUM_CONV", "=", "0", "\n", "# Channel dimension for Conv layers in the RoI box head", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "CONV_DIM", "=", "256", "\n", "# Normalization method for the convolution layers.", "\n", "# Options: \"\" (no norm), \"GN\", \"SyncBN\".", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_hooks": [[430, 477], ["defaults.DefaultTrainer.cfg.clone", "defaults.DefaultTrainer.defrost", "detectron2.utils.comm.is_main_process", "ret.append", "detectron2.utils.comm.is_main_process", "hooks.IterationTimer", "hooks.LRScheduler", "ret.append", "defaults.DefaultTrainer.test", "hooks.EvalHook", "ret.append", "hooks.PreciseBN", "hooks.PeriodicCheckpointer", "hooks.PeriodicWriter", "fvcore.nn.precise_bn.get_bn_modules", "defaults.DefaultTrainer.build_train_loader", "defaults.DefaultTrainer.build_writers"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_packaging.TestCollectEnv.test", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_train_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_writers"], ["_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "NORM", "=", "\"\"", "\n", "# Whether to use class agnostic for bbox regression", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "CLS_AGNOSTIC_BBOX_REG", "=", "False", "\n", "# If true, RoI heads use bounding boxes predicted by the box head rather than proposal boxes.", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_HEAD", ".", "TRAIN_ON_PRED_BOXES", "=", "False", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# Cascaded Box Head", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_CASCADE_HEAD", "=", "CN", "(", ")", "\n", "# The number of cascade stages is implicitly defined by the length of the following two configs.", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_CASCADE_HEAD", ".", "BBOX_REG_WEIGHTS", "=", "(", "\n", "(", "10.0", ",", "10.0", ",", "5.0", ",", "5.0", ")", ",", "\n", "(", "20.0", ",", "20.0", ",", "10.0", ",", "10.0", ")", ",", "\n", "(", "30.0", ",", "30.0", ",", "15.0", ",", "15.0", ")", ",", "\n", ")", "\n", "_C", ".", "MODEL", ".", "ROI_BOX_CASCADE_HEAD", ".", "IOUS", "=", "(", "0.5", ",", "0.6", ",", "0.7", ")", "\n", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# Mask Head", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "ROI_MASK_HEAD", "=", "CN", "(", ")", "\n", "_C", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "NAME", "=", "\"MaskRCNNConvUpsampleHead\"", "\n", "_C", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_RESOLUTION", "=", "14", "\n", "_C", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_SAMPLING_RATIO", "=", "0", "\n", "_C", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "NUM_CONV", "=", "0", "# The number of convs in the mask head", "\n", "_C", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "CONV_DIM", "=", "256", "\n", "# Normalization method for the convolution layers.", "\n", "# Options: \"\" (no norm), \"GN\", \"SyncBN\".", "\n", "_C", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "NORM", "=", "\"\"", "\n", "# Whether to use class agnostic for mask prediction", "\n", "_C", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "CLS_AGNOSTIC_MASK", "=", "False", "\n", "# Type of pooling operation applied to the incoming feature map for each RoI", "\n", "_C", ".", "MODEL", ".", "ROI_MASK_HEAD", ".", "POOLER_TYPE", "=", "\"ROIAlignV2\"", "\n", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# Keypoint Head", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", "=", "CN", "(", ")", "\n", "_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "NAME", "=", "\"KRCNNConvDeconvUpsampleHead\"", "\n", "_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "POOLER_RESOLUTION", "=", "14", "\n", "_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "POOLER_SAMPLING_RATIO", "=", "0", "\n", "_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "CONV_DIMS", "=", "tuple", "(", "512", "for", "_", "in", "range", "(", "8", ")", ")", "\n", "_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "NUM_KEYPOINTS", "=", "17", "# 17 is the number of keypoints in COCO.", "\n", "\n", "# Images with too few (or no) keypoints are excluded from training.", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_writers": [[478, 488], ["defaults.default_writers"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.default_writers"], ["_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "MIN_KEYPOINTS_PER_IMAGE", "=", "1", "\n", "# Normalize by the total number of visible keypoints in the minibatch if True.", "\n", "# Otherwise, normalize by the total number of keypoints that could ever exist", "\n", "# in the minibatch.", "\n", "# The keypoint softmax loss is only calculated on visible keypoints.", "\n", "# Since the number of visible keypoints can vary significantly between", "\n", "# minibatches, this has the effect of up-weighting the importance of", "\n", "# minibatches with few visible keypoints. (Imagine the extreme case of", "\n", "# only one visible keypoint versus N: in the case of N, each one", "\n", "# contributes 1/N to the gradient compared to the single keypoint", "\n", "# determining the gradient direction). Instead, we can normalize the", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.train": [[489, 503], ["super().train", "len", "detectron2.utils.comm.is_main_process", "hasattr", "detectron2.evaluation.verify_results"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.verify_results"], ["# loss by the total number of keypoints, if it were the case that all", "\n", "# keypoints were visible in a full minibatch. (Returning to the example,", "\n", "# this means that the one visible keypoint contributes as much as each", "\n", "# of the N keypoints.)", "\n", "_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS", "=", "True", "\n", "# Multi-task loss weight to use for keypoints", "\n", "# Recommended values:", "\n", "#   - use 1.0 if NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS is True", "\n", "#   - use 4.0 if NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS is False", "\n", "_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "LOSS_WEIGHT", "=", "1.0", "\n", "# Type of pooling operation applied to the incoming feature map for each RoI", "\n", "_C", ".", "MODEL", ".", "ROI_KEYPOINT_HEAD", ".", "POOLER_TYPE", "=", "\"ROIAlignV2\"", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# Semantic Segmentation Head", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.run_step": [[504, 507], ["defaults.DefaultTrainer._trainer.run_step"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.run_step"], ["# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "SEM_SEG_HEAD", "=", "CN", "(", ")", "\n", "_C", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "NAME", "=", "\"SemSegFPNHead\"", "\n", "_C", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "IN_FEATURES", "=", "[", "\"p2\"", ",", "\"p3\"", ",", "\"p4\"", ",", "\"p5\"", "]", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model": [[508, 521], ["detectron2.modeling.build_model", "logging.getLogger", "logging.getLogger.info"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_model"], ["# Label in the semantic segmentation ground truth that is ignored, i.e., no loss is calculated for", "\n", "# the correposnding pixel.", "\n", "_C", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "IGNORE_VALUE", "=", "255", "\n", "# Number of classes in the semantic segmentation head", "\n", "_C", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "NUM_CLASSES", "=", "54", "\n", "# Number of channels in the 3x3 convs inside semantic-FPN heads.", "\n", "_C", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "CONVS_DIM", "=", "128", "\n", "# Outputs from semantic-FPN heads are up-scaled to the COMMON_STRIDE stride.", "\n", "_C", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "COMMON_STRIDE", "=", "4", "\n", "# Normalization method for the convolution layers. Options: \"\" (no norm), \"GN\".", "\n", "_C", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "NORM", "=", "\"GN\"", "\n", "_C", ".", "MODEL", ".", "SEM_SEG_HEAD", ".", "LOSS_WEIGHT", "=", "1.0", "\n", "\n", "_C", ".", "MODEL", ".", "PANOPTIC_FPN", "=", "CN", "(", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_optimizer": [[522, 532], ["detectron2.solver.build_optimizer"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.build_optimizer"], ["# Scaling of all losses from instance detection / segmentation head.", "\n", "_C", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "INSTANCE_LOSS_WEIGHT", "=", "1.0", "\n", "\n", "# options when combining instance & semantic segmentation outputs", "\n", "_C", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "COMBINE", "=", "CN", "(", "{", "\"ENABLED\"", ":", "True", "}", ")", "# \"COMBINE.ENABLED\" is deprecated & not used", "\n", "_C", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "COMBINE", ".", "OVERLAP_THRESH", "=", "0.5", "\n", "_C", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "COMBINE", ".", "STUFF_AREA_LIMIT", "=", "4096", "\n", "_C", ".", "MODEL", ".", "PANOPTIC_FPN", ".", "COMBINE", ".", "INSTANCES_CONFIDENCE_THRESH", "=", "0.5", "\n", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_lr_scheduler": [[533, 540], ["detectron2.solver.build_lr_scheduler"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.build_lr_scheduler"], ["# RetinaNet Head", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "RETINANET", "=", "CN", "(", ")", "\n", "\n", "# This is the number of foreground classes.", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "NUM_CLASSES", "=", "80", "\n", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "IN_FEATURES", "=", "[", "\"p3\"", ",", "\"p4\"", ",", "\"p5\"", ",", "\"p6\"", ",", "\"p7\"", "]", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_train_loader": [[541, 551], ["detectron2.data.build_detection_train_loader"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_train_loader"], ["\n", "# Convolutions to use in the cls and bbox tower", "\n", "# NOTE: this doesn't include the last conv for logits", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "NUM_CONVS", "=", "4", "\n", "\n", "# IoU overlap ratio [bg, fg] for labeling anchors.", "\n", "# Anchors with < bg are labeled negative (0)", "\n", "# Anchors  with >= bg and < fg are ignored (-1)", "\n", "# Anchors with >= fg are labeled positive (1)", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "IOU_THRESHOLDS", "=", "[", "0.4", ",", "0.5", "]", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "IOU_LABELS", "=", "[", "0", ",", "-", "1", ",", "1", "]", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_test_loader": [[552, 562], ["detectron2.data.build_detection_test_loader"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_test_loader"], ["\n", "# Prior prob for rare case (i.e. foreground) at the beginning of training.", "\n", "# This is used to set the bias for the logits layer of the classifier subnet.", "\n", "# This improves training stability in the case of heavy class imbalance.", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "PRIOR_PROB", "=", "0.01", "\n", "\n", "# Inference cls score threshold, only anchors with score > INFERENCE_TH are", "\n", "# considered for inference (to improve speed)", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "SCORE_THRESH_TEST", "=", "0.05", "\n", "# Select topk candidates before NMS", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "TOPK_CANDIDATES_TEST", "=", "1000", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_evaluator": [[563, 577], ["NotImplementedError"], "methods", ["None"], ["_C", ".", "MODEL", ".", "RETINANET", ".", "NMS_THRESH_TEST", "=", "0.5", "\n", "\n", "# Weights on (dx, dy, dw, dh) for normalizing Retinanet anchor regression targets", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "BBOX_REG_WEIGHTS", "=", "(", "1.0", ",", "1.0", ",", "1.0", ",", "1.0", ")", "\n", "\n", "# Loss parameters", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "FOCAL_LOSS_GAMMA", "=", "2.0", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "FOCAL_LOSS_ALPHA", "=", "0.25", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "SMOOTH_L1_LOSS_BETA", "=", "0.1", "\n", "# Options are: \"smooth_l1\", \"giou\"", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "BBOX_REG_LOSS_TYPE", "=", "\"smooth_l1\"", "\n", "\n", "# One of BN, SyncBN, FrozenBN, GN", "\n", "# Only supports GN until unshared norm is implemented", "\n", "_C", ".", "MODEL", ".", "RETINANET", ".", "NORM", "=", "\"\"", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.test": [[579, 631], ["logging.getLogger", "isinstance", "collections.OrderedDict", "enumerate", "cls.build_test_loader", "detectron2.evaluation.inference_on_dataset", "detectron2.utils.comm.is_main_process", "len", "len", "len", "len", "len", "isinstance", "logging.getLogger.info", "detectron2.evaluation.print_csv_format", "list", "cls.build_evaluator", "collections.OrderedDict.values", "logging.getLogger.warn"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_test_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.inference_on_dataset", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.print_csv_format", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.build_evaluator"], ["\n", "# ---------------------------------------------------------------------------- #", "\n", "# ResNe[X]t options (ResNets = {ResNet, ResNeXt}", "\n", "# Note that parts of a resnet may be used for both the backbone and the head", "\n", "# These options apply to both", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "RESNETS", "=", "CN", "(", ")", "\n", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "DEPTH", "=", "50", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "OUT_FEATURES", "=", "[", "\"res4\"", "]", "# res4 for C4 backbone, res2..5 for FPN backbone", "\n", "\n", "# Number of groups to use; 1 ==> ResNet; > 1 ==> ResNeXt", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "NUM_GROUPS", "=", "1", "\n", "\n", "# Options: FrozenBN, GN, \"SyncBN\", \"BN\"", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "NORM", "=", "\"FrozenBN\"", "\n", "\n", "# Baseline width of each group.", "\n", "# Scaling this parameters will scale the width of all bottleneck layers.", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "WIDTH_PER_GROUP", "=", "64", "\n", "\n", "# Place the stride 2 conv on the 1x1 filter", "\n", "# Use True only for the original MSRA ResNet; use False for C2 and Torch models", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "STRIDE_IN_1X1", "=", "True", "\n", "\n", "# Apply dilation in stage \"res5\"", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "RES5_DILATION", "=", "1", "\n", "\n", "# Output width of res2. Scaling this parameters will scale the width of all 1x1 convs in ResNet", "\n", "# For R18 and R34, this needs to be set to 64", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "RES2_OUT_CHANNELS", "=", "256", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "STEM_OUT_CHANNELS", "=", "64", "\n", "\n", "# Apply Deformable Convolution in stages", "\n", "# Specify if apply deform_conv on Res2, Res3, Res4, Res5", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "DEFORM_ON_PER_STAGE", "=", "[", "False", ",", "False", ",", "False", ",", "False", "]", "\n", "# Use True to use modulated deform_conv (DeformableV2, https://arxiv.org/abs/1811.11168);", "\n", "# Use False for DeformableV1.", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "DEFORM_MODULATED", "=", "False", "\n", "# Number of groups in deformable conv.", "\n", "_C", ".", "MODEL", ".", "RESNETS", ".", "DEFORM_NUM_GROUPS", "=", "1", "\n", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# Solver", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "SOLVER", "=", "CN", "(", ")", "\n", "\n", "# See detectron2/solver/build.py for LR scheduler options", "\n", "_C", ".", "SOLVER", ".", "LR_SCHEDULER_NAME", "=", "\"WarmupMultiStepLR\"", "\n", "\n", "_C", ".", "SOLVER", ".", "MAX_ITER", "=", "40000", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.auto_scale_workers": [[632, 702], ["cfg.clone.clone.clone", "cfg.clone.clone.is_frozen", "cfg.clone.clone.defrost", "int", "int", "int", "tuple", "int", "int", "logging.getLogger", "logging.getLogger.info", "round", "round", "round", "round", "round", "cfg.clone.clone.freeze", "int", "round"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.blocks.CNNBlockBase.freeze"], ["_C", ".", "SOLVER", ".", "BASE_LR", "=", "0.001", "\n", "\n", "_C", ".", "SOLVER", ".", "MOMENTUM", "=", "0.9", "\n", "\n", "_C", ".", "SOLVER", ".", "NESTEROV", "=", "False", "\n", "\n", "_C", ".", "SOLVER", ".", "WEIGHT_DECAY", "=", "0.0001", "\n", "# The weight decay that's applied to parameters of normalization layers", "\n", "# (typically the affine transformation)", "\n", "_C", ".", "SOLVER", ".", "WEIGHT_DECAY_NORM", "=", "0.0", "\n", "\n", "_C", ".", "SOLVER", ".", "GAMMA", "=", "0.1", "\n", "# The iteration number to decrease learning rate by GAMMA.", "\n", "_C", ".", "SOLVER", ".", "STEPS", "=", "(", "30000", ",", ")", "\n", "\n", "_C", ".", "SOLVER", ".", "WARMUP_FACTOR", "=", "1.0", "/", "1000", "\n", "_C", ".", "SOLVER", ".", "WARMUP_ITERS", "=", "1000", "\n", "_C", ".", "SOLVER", ".", "WARMUP_METHOD", "=", "\"linear\"", "\n", "\n", "# Save a checkpoint after every this number of iterations", "\n", "_C", ".", "SOLVER", ".", "CHECKPOINT_PERIOD", "=", "5000", "\n", "\n", "# Number of images per batch across all machines. This is also the number", "\n", "# of training images per step (i.e. per iteration). If we use 16 GPUs", "\n", "# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.", "\n", "# May be adjusted automatically if REFERENCE_WORLD_SIZE is set.", "\n", "_C", ".", "SOLVER", ".", "IMS_PER_BATCH", "=", "16", "\n", "\n", "# The reference number of workers (GPUs) this config is meant to train with.", "\n", "# It takes no effect when set to 0.", "\n", "# With a non-zero value, it will be used by DefaultTrainer to compute a desired", "\n", "# per-worker batch size, and then scale the other related configs (total batch size,", "\n", "# learning rate, etc) to match the per-worker batch size.", "\n", "# See documentation of `DefaultTrainer.auto_scale_workers` for details:", "\n", "_C", ".", "SOLVER", ".", "REFERENCE_WORLD_SIZE", "=", "0", "\n", "\n", "# Detectron v1 (and previous detection code) used a 2x higher LR and 0 WD for", "\n", "# biases. This is not useful (at least for recent models). You should avoid", "\n", "# changing these and they exist only to reproduce Detectron v1 training if", "\n", "# desired.", "\n", "_C", ".", "SOLVER", ".", "BIAS_LR_FACTOR", "=", "1.0", "\n", "_C", ".", "SOLVER", ".", "WEIGHT_DECAY_BIAS", "=", "_C", ".", "SOLVER", ".", "WEIGHT_DECAY", "\n", "\n", "# Gradient clipping", "\n", "_C", ".", "SOLVER", ".", "CLIP_GRADIENTS", "=", "CN", "(", "{", "\"ENABLED\"", ":", "False", "}", ")", "\n", "# Type of gradient clipping, currently 2 values are supported:", "\n", "# - \"value\": the absolute values of elements of each gradients are clipped", "\n", "# - \"norm\": the norm of the gradient for each parameter is clipped thus", "\n", "#   affecting all elements in the parameter", "\n", "_C", ".", "SOLVER", ".", "CLIP_GRADIENTS", ".", "CLIP_TYPE", "=", "\"value\"", "\n", "# Maximum absolute value used for clipping gradients", "\n", "_C", ".", "SOLVER", ".", "CLIP_GRADIENTS", ".", "CLIP_VALUE", "=", "1.0", "\n", "# Floating point number p for L-p norm to be used with the \"norm\"", "\n", "# gradient clipping type; for L-inf, please specify .inf", "\n", "_C", ".", "SOLVER", ".", "CLIP_GRADIENTS", ".", "NORM_TYPE", "=", "2.0", "\n", "\n", "# Enable automatic mixed precision for training", "\n", "# Note that this does not change model's inference behavior.", "\n", "# To use AMP in inference, run inference under autocast()", "\n", "_C", ".", "SOLVER", ".", "AMP", "=", "CN", "(", "{", "\"ENABLED\"", ":", "False", "}", ")", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# Specific test options", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "TEST", "=", "CN", "(", ")", "\n", "# For end-to-end tests to verify the expected accuracy.", "\n", "# Each item is [task, metric, value, tolerance]", "\n", "# e.g.: [['bbox', 'AP', 38.5, 0.2]]", "\n", "_C", ".", "TEST", ".", "EXPECTED_RESULTS", "=", "[", "]", "\n", "# The period (in terms of steps) to evaluate the model during training.", "\n", "# Set to 0 to disable.", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.create_ddp_model": [[60, 80], ["torch.nn.parallel.DistributedDataParallel", "detectron2.utils.comm.get_world_size", "torch.nn.parallel.DistributedDataParallel.register_comm_hook", "detectron2.utils.comm.get_local_rank"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_local_rank"], ["# Mode for flipping images used in data augmentation during training", "\n", "# choose one of [\"horizontal, \"vertical\", \"none\"]", "\n", "_C", ".", "INPUT", ".", "RANDOM_FLIP", "=", "\"horizontal\"", "\n", "\n", "# `True` if cropping is used for data augmentation during training", "\n", "_C", ".", "INPUT", ".", "CROP", "=", "CN", "(", "{", "\"ENABLED\"", ":", "False", "}", ")", "\n", "# Cropping type. See documentation of `detectron2.data.transforms.RandomCrop` for explanation.", "\n", "_C", ".", "INPUT", ".", "CROP", ".", "TYPE", "=", "\"relative_range\"", "\n", "# Size of crop in range (0, 1] if CROP.TYPE is \"relative\" or \"relative_range\" and in number of", "\n", "# pixels if CROP.TYPE is \"absolute\"", "\n", "_C", ".", "INPUT", ".", "CROP", ".", "SIZE", "=", "[", "0.9", ",", "0.9", "]", "\n", "\n", "\n", "# Whether the model needs RGB, YUV, HSV etc.", "\n", "# Should be one of the modes defined here, as we use PIL to read the image:", "\n", "# https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes", "\n", "# with BGR being the one exception. One can set image format to BGR, we will", "\n", "# internally use RGB for conversion and flip the channels over", "\n", "_C", ".", "INPUT", ".", "FORMAT", "=", "\"BGR\"", "\n", "# The ground truth mask format that the model will use.", "\n", "# Mask R-CNN supports either \"polygon\" or \"bitmask\" as ground truth.", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.default_argument_parser": [[82, 142], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "hash", "os.getuid"], "function", ["None"], ["\n", "################### Text Tokenizer from MSR-CLIP ##################", "\n", "_C", ".", "INPUT", ".", "TEXT_TOKENIZER", "=", "\"openai_bpe\"", "# \"bert-base-cased\"", "\n", "\n", "################## Data Augmentation from MSR-CLIP ##################", "\n", "_C", ".", "AUG", "=", "CN", "(", ")", "\n", "_C", ".", "AUG", ".", "SCALE", "=", "(", "0.08", ",", "1.0", ")", "\n", "_C", ".", "AUG", ".", "RATIO", "=", "(", "3.0", "/", "4.0", ",", "4.0", "/", "3.0", ")", "\n", "_C", ".", "AUG", ".", "COLOR_JITTER", "=", "[", "0.4", ",", "0.4", ",", "0.4", ",", "0.1", ",", "0.0", "]", "\n", "_C", ".", "AUG", ".", "GRAY_SCALE", "=", "0.0", "\n", "_C", ".", "AUG", ".", "GAUSSIAN_BLUR", "=", "0.0", "\n", "_C", ".", "AUG", ".", "DROPBLOCK_LAYERS", "=", "[", "3", ",", "4", "]", "\n", "_C", ".", "AUG", ".", "DROPBLOCK_KEEP_PROB", "=", "1.0", "\n", "_C", ".", "AUG", ".", "DROPBLOCK_BLOCK_SIZE", "=", "7", "\n", "_C", ".", "AUG", ".", "MIXUP_PROB", "=", "0.0", "\n", "_C", ".", "AUG", ".", "MIXUP", "=", "0.0", "\n", "_C", ".", "AUG", ".", "MIXCUT", "=", "0.0", "\n", "_C", ".", "AUG", ".", "MIXCUT_MINMAX", "=", "[", "]", "\n", "_C", ".", "AUG", ".", "MIXUP_SWITCH_PROB", "=", "0.5", "\n", "_C", ".", "AUG", ".", "MIXUP_MODE", "=", "'batch'", "\n", "_C", ".", "AUG", ".", "MIXCUT_AND_MIXUP", "=", "False", "\n", "_C", ".", "AUG", ".", "INTERPOLATION", "=", "3", "\n", "_C", ".", "AUG", ".", "USE_TIMM", "=", "False", "\n", "_C", ".", "AUG", ".", "TIMM_AUG", "=", "CN", "(", "new_allowed", "=", "True", ")", "\n", "_C", ".", "AUG", ".", "TIMM_AUG", ".", "USE_LOADER", "=", "False", "\n", "_C", ".", "AUG", ".", "TIMM_AUG", ".", "USE_TRANSFORM", "=", "False", "\n", "\n", "_C", ".", "AUG", ".", "TRAIN", "=", "CN", "(", ")", "\n", "_C", ".", "AUG", ".", "TRAIN", ".", "IMAGE_SIZE", "=", "[", "224", ",", "224", "]", "# width * height, ex: 192 * 256", "\n", "_C", ".", "AUG", ".", "TRAIN", ".", "MAX_SIZE", "=", "None", "# the maximum size for longer edge after resizing", "\n", "_C", ".", "AUG", ".", "TEST", "=", "CN", "(", ")", "\n", "_C", ".", "AUG", ".", "TEST", ".", "IMAGE_SIZE", "=", "[", "224", ",", "224", "]", "# width * height, ex: 192 * 256", "\n", "_C", ".", "AUG", ".", "TEST", ".", "MAX_SIZE", "=", "None", "# the maximum size for longer edge after resizing", "\n", "_C", ".", "AUG", ".", "TEST", ".", "CENTER_CROP", "=", "False", "\n", "_C", ".", "AUG", ".", "TEST", ".", "INTERPOLATION", "=", "3", "\n", "\n", "\n", "# -----------------------------------------------------------------------------", "\n", "# Dataset", "\n", "# -----------------------------------------------------------------------------", "\n", "_C", ".", "DATASETS", "=", "CN", "(", ")", "\n", "# List of the dataset names for training. Must be registered in DatasetCatalog", "\n", "# Samples from these datasets will be merged and used as one dataset.", "\n", "_C", ".", "DATASETS", ".", "TRAIN", "=", "(", ")", "\n", "# List of the pre-computed proposal files for training, which must be consistent", "\n", "# with datasets listed in DATASETS.TRAIN.", "\n", "_C", ".", "DATASETS", ".", "PROPOSAL_FILES_TRAIN", "=", "(", ")", "\n", "# Number of top scoring precomputed proposals to keep for training", "\n", "_C", ".", "DATASETS", ".", "PRECOMPUTED_PROPOSAL_TOPK_TRAIN", "=", "2000", "\n", "# List of the dataset names for testing. Must be registered in DatasetCatalog", "\n", "_C", ".", "DATASETS", ".", "TEST", "=", "(", ")", "\n", "# List of the pre-computed proposal files for test, which must be consistent", "\n", "# with datasets listed in DATASETS.TEST.", "\n", "_C", ".", "DATASETS", ".", "PROPOSAL_FILES_TEST", "=", "(", ")", "\n", "# Number of top scoring precomputed proposals to keep for test", "\n", "_C", ".", "DATASETS", ".", "PRECOMPUTED_PROPOSAL_TOPK_TEST", "=", "1000", "\n", "################## Data Loading from MSR-CLIP ##################", "\n", "# List of dataset class names for training", "\n", "_C", ".", "DATASETS", ".", "FACTORY_TRAIN", "=", "(", ")", "\n", "# List of dataset folder for training", "\n", "_C", ".", "DATASETS", ".", "PATH_TRAIN", "=", "(", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults._try_get_key": [[144, 160], ["isinstance", "omegaconf.OmegaConf.create", "k.split", "OmegaConf.select.dump", "omegaconf.OmegaConf.select"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump"], ["_C", ".", "DATASETS", ".", "AUX", "=", "(", ")", "\n", "# List of dataset class names for auxilary training", "\n", "_C", ".", "DATASETS", ".", "FACTORY_AUX", "=", "(", ")", "\n", "# List of dataset folder for auxilary training", "\n", "_C", ".", "DATASETS", ".", "PATH_AUX", "=", "(", ")", "\n", "# List of dataset class names for testing", "\n", "_C", ".", "DATASETS", ".", "FACTORY_TEST", "=", "(", ")", "\n", "# List of dataset folder for testing", "\n", "_C", ".", "DATASETS", ".", "PATH_TEST", "=", "(", ")", "\n", "# Labelmap file to convert to tsv or for demo purpose", "\n", "_C", ".", "DATASETS", ".", "LABELMAP_FILE", "=", "''", "\n", "_C", ".", "DATASETS", ".", "ATTR_LABELMAP_FILE", "=", "''", "\n", "_C", ".", "DATASETS", ".", "FILTERED_CLASSIFICATION_DATASETS", "=", "''", "\n", "# hierarchy file for test time score aggregation (developed on OpenImages)", "\n", "_C", ".", "DATASETS", ".", "HIERARCHY_FILE", "=", "''", "\n", "# List of box extra fields for training/testing", "\n", "# If given, will not infer from the other cfgs.", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults._highlight": [[162, 174], ["pygments.highlight", "filename.endswith", "Python3Lexer", "YamlLexer", "Terminal256Formatter"], "function", ["None"], ["\n", "_C", ".", "DATASETS", ".", "NUM_CLASSES", "=", "0", "\n", "_C", ".", "DATASETS", ".", "ROOT", "=", "''", "\n", "_C", ".", "DATASETS", ".", "TRAIN_SET", "=", "'train'", "\n", "_C", ".", "DATASETS", ".", "VAL_SET", "=", "''", "\n", "_C", ".", "DATASETS", ".", "TEST_SET", "=", "'val'", "\n", "\n", "# The maximum total input sequence length after WordPiece tokenization", "\n", "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.", "\n", "_C", ".", "DATASETS", ".", "MAX_SEQ_LENGTH", "=", "35", "\n", "\n", "# -----------------------------------------------------------------------------", "\n", "# DataLoader", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.default_setup": [[176, 229], ["defaults._try_get_key", "detectron2.utils.comm.get_rank", "detectron2.utils.logger.setup_logger", "detectron2.utils.logger.setup_logger", "detectron2.utils.logger.setup_logger.info", "detectron2.utils.logger.setup_logger.info", "detectron2.utils.logger.setup_logger.info", "defaults._try_get_key", "detectron2.utils.env.seed_all_rng", "detectron2.utils.comm.is_main_process", "detectron2.utils.file_io.PathManager.mkdirs", "hasattr", "detectron2.utils.logger.setup_logger.info", "detectron2.utils.comm.is_main_process", "os.path.join", "isinstance", "detectron2.utils.logger.setup_logger.info", "defaults._try_get_key", "detectron2.utils.comm.get_world_size", "detectron2.utils.collect_env.collect_env_info", "str", "detectron2.utils.logger.setup_logger.info", "detectron2.config.LazyConfig.save", "hasattr", "defaults._highlight", "detectron2.utils.file_io.PathManager.open", "f.write", "detectron2.utils.file_io.PathManager.open().read", "defaults._highlight", "cfg.dump", "cfg.dump", "detectron2.utils.file_io.PathManager.open"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults._try_get_key", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults._try_get_key", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.seed_all_rng", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults._try_get_key", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.collect_env_info", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults._highlight", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults._highlight", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump"], ["_C", ".", "DATALOADER", "=", "CN", "(", ")", "\n", "# Number of data loading threads", "\n", "_C", ".", "DATALOADER", ".", "NUM_WORKERS", "=", "4", "\n", "# If True, each batch should contain only images for which the aspect ratio", "\n", "# is compatible. This groups portrait images together, and landscape images", "\n", "# are not batched with portrait images.", "\n", "_C", ".", "DATALOADER", ".", "ASPECT_RATIO_GROUPING", "=", "True", "\n", "# Options: TrainingSampler, RepeatFactorTrainingSampler", "\n", "_C", ".", "DATALOADER", ".", "SAMPLER_TRAIN", "=", "\"TrainingSampler\"", "\n", "# Repeat threshold for RepeatFactorTrainingSampler", "\n", "_C", ".", "DATALOADER", ".", "REPEAT_THRESHOLD", "=", "0.0", "\n", "# Tf True, when working on datasets that have instance annotations, the", "\n", "# training dataloader will filter out images without associated annotations", "\n", "_C", ".", "DATALOADER", ".", "FILTER_EMPTY_ANNOTATIONS", "=", "True", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# CLIP options", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "CLIP", "=", "CN", "(", ")", "\n", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "CROP_REGION_TYPE", "=", "\"\"", "# options: \"GT\", \"RPN\" ", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "BB_RPN_WEIGHTS", "=", "None", "# the weights of pretrained MaskRCNN", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "IMS_PER_BATCH_TEST", "=", "8", "# the #images during inference per batch", "\n", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "USE_TEXT_EMB_CLASSIFIER", "=", "False", "# if True, use the CLIP text embedding as the classifier's weights", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "TEXT_EMB_PATH", "=", "None", "# \"/mnt/output_storage/trained_models/lvis_cls_emb/lvis_1203_cls_emb.pth\"", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_CONFIG", "=", "None", "# option: all configs of pretrained RPN", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "NO_BOX_DELTA", "=", "False", "# if True, during inference, no box delta will be applied to region proposals", "\n", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "BG_CLS_LOSS_WEIGHT", "=", "None", "# if not None, it is the loss weight for bg regions", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "ONLY_SAMPLE_FG_PROPOSALS", "=", "False", "# if True, during training, ignore all bg proposals and only sample fg proposals", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "MULTIPLY_RPN_SCORE", "=", "False", "# if True, during inference, multiply RPN scores with classification scores", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "VIS", "=", "False", "# if True, when visualizing the object scores, we convert them to the scores before multiplying RPN scores", "\n", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "OPENSET_TEST_NUM_CLASSES", "=", "None", "# if an integer, it is #all_cls in test", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "OPENSET_TEST_TEXT_EMB_PATH", "=", "None", "# if not None, enables the openset/zero-shot training, the category embeddings during test", "\n", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "CLSS_TEMP", "=", "0.01", "# normalization + dot product + temperature", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "RUN_CVPR_OVR", "=", "False", "# if True, train CVPR OVR model with their text embeddings", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "FOCAL_SCALED_LOSS", "=", "None", "# if not None (float value for gamma), apply focal loss scaling idea to standard cross-entropy loss", "\n", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_NMS_THRESH", "=", "None", "# the threshold of NMS in offline RPN", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "PRETRAIN_IMG_TXT_LEVEL", "=", "True", "# if True, pretrain model using image-text level matching", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "PRETRAIN_ONLY_EOT", "=", "False", "# if True, use end-of-token emb to match region features, in image-text level matching", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "PRETRAIN_RPN_REGIONS", "=", "None", "# if not None, the number of RPN regions per image during pretraining", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "PRETRAIN_SAMPLE_REGIONS", "=", "None", "# if not None, the number of regions per image during pretraining after sampling, to avoid overfitting", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "GATHER_GPUS", "=", "False", "# if True, gather tensors across GPUS to increase batch size", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "GRID_REGIONS", "=", "False", "# if True, use grid boxes to extract grid features, instead of object proposals", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "CONCEPT_POOL_EMB", "=", "None", "# if not None, it provides the file path of embs of concept pool and thus enables region-concept matching", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "CONCEPT_THRES", "=", "None", "# if not None, the threshold to filter out the regions with low matching score with concept embs, dependent on temp (default: 0.01)", "\n", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "OFFLINE_RPN_LSJ_PRETRAINED", "=", "False", "# if True, use large-scale jittering (LSJ) pretrained RPN", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "TEACHER_RESNETS_DEPTH", "=", "50", "# the type of visual encoder of teacher model, sucha as ResNet 50, 101, 200 (a flag for 50x4)", "\n", "_C", ".", "MODEL", ".", "CLIP", ".", "TEACHER_CONCEPT_POOL_EMB", "=", "None", "# if not None, it uses the same concept embedding as student model; otherwise, uses a seperate embedding of teacher model", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.default_writers": [[232, 250], ["detectron2.utils.events.CommonMetricPrinter", "detectron2.utils.events.JSONWriter", "detectron2.utils.events.TensorboardXWriter", "os.path.join"], "function", ["None"], ["_C", ".", "MODEL", ".", "CLIP", ".", "TEXT_EMB_DIM", "=", "1024", "# the dimension of precomputed class embeddings", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# Backbone options", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "BACKBONE", "=", "CN", "(", ")", "\n", "\n", "_C", ".", "MODEL", ".", "BACKBONE", ".", "NAME", "=", "\"build_resnet_backbone\"", "\n", "# Freeze the first several stages so they are not trained.", "\n", "# There are 5 stages in ResNet. The first is a convolution, and the following", "\n", "# stages are each group of residual blocks.", "\n", "_C", ".", "MODEL", ".", "BACKBONE", ".", "FREEZE_AT", "=", "2", "\n", "\n", "\n", "# ---------------------------------------------------------------------------- #", "\n", "# FPN options", "\n", "# ---------------------------------------------------------------------------- #", "\n", "_C", ".", "MODEL", ".", "FPN", "=", "CN", "(", ")", "\n", "# Names of the input feature maps to be used by FPN", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.launch._find_free_port": [[15, 25], ["socket.socket", "socket.socket.bind", "socket.socket.close", "socket.socket.getsockname"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close"], ["def", "_find_free_port", "(", ")", ":", "\n", "    ", "import", "socket", "\n", "\n", "sock", "=", "socket", ".", "socket", "(", "socket", ".", "AF_INET", ",", "socket", ".", "SOCK_STREAM", ")", "\n", "# Binding to port 0 will cause the OS to find an available port for us", "\n", "sock", ".", "bind", "(", "(", "\"\"", ",", "0", ")", ")", "\n", "port", "=", "sock", ".", "getsockname", "(", ")", "[", "1", "]", "\n", "sock", ".", "close", "(", ")", "\n", "# NOTE: there is still a chance the port could be taken by other processes.", "\n", "return", "port", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.launch.launch": [[27, 83], ["torch.spawn", "main_func", "launch._find_free_port", "dist_url.startswith", "logging.getLogger", "logging.getLogger.warning"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.launch._find_free_port"], ["", "def", "launch", "(", "\n", "main_func", ",", "\n", "num_gpus_per_machine", ",", "\n", "num_machines", "=", "1", ",", "\n", "machine_rank", "=", "0", ",", "\n", "dist_url", "=", "None", ",", "\n", "args", "=", "(", ")", ",", "\n", "timeout", "=", "DEFAULT_TIMEOUT", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Launch multi-gpu or distributed training.\n    This function must be called on all machines involved in the training.\n    It will spawn child processes (defined by ``num_gpus_per_machine``) on each machine.\n\n    Args:\n        main_func: a function that will be called by `main_func(*args)`\n        num_gpus_per_machine (int): number of GPUs per machine\n        num_machines (int): the total number of machines\n        machine_rank (int): the rank of this machine\n        dist_url (str): url to connect to for distributed jobs, including protocol\n                       e.g. \"tcp://127.0.0.1:8686\".\n                       Can be set to \"auto\" to automatically select a free port on localhost\n        timeout (timedelta): timeout of the distributed workers\n        args (tuple): arguments passed to main_func\n    \"\"\"", "\n", "world_size", "=", "num_machines", "*", "num_gpus_per_machine", "\n", "if", "world_size", ">", "1", ":", "\n", "# https://github.com/pytorch/pytorch/pull/14391", "\n", "# TODO prctl in spawned processes", "\n", "\n", "        ", "if", "dist_url", "==", "\"auto\"", ":", "\n", "            ", "assert", "num_machines", "==", "1", ",", "\"dist_url=auto not supported in multi-machine jobs.\"", "\n", "port", "=", "_find_free_port", "(", ")", "\n", "dist_url", "=", "f\"tcp://127.0.0.1:{port}\"", "\n", "", "if", "num_machines", ">", "1", "and", "dist_url", ".", "startswith", "(", "\"file://\"", ")", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "warning", "(", "\n", "\"file:// is not a reliable init_method in multi-machine jobs. Prefer tcp://\"", "\n", ")", "\n", "\n", "", "mp", ".", "spawn", "(", "\n", "_distributed_worker", ",", "\n", "nprocs", "=", "num_gpus_per_machine", ",", "\n", "args", "=", "(", "\n", "main_func", ",", "\n", "world_size", ",", "\n", "num_gpus_per_machine", ",", "\n", "machine_rank", ",", "\n", "dist_url", ",", "\n", "args", ",", "\n", "timeout", ",", "\n", ")", ",", "\n", "daemon", "=", "False", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "main_func", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.launch._distributed_worker": [[85, 126], ["torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "detectron2.utils.comm.synchronize", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "range", "main_func", "torch.init_process_group", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "list", "torch.new_group", "logging.getLogger", "logging.getLogger.error", "range"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "", "def", "_distributed_worker", "(", "\n", "local_rank", ",", "\n", "main_func", ",", "\n", "world_size", ",", "\n", "num_gpus_per_machine", ",", "\n", "machine_rank", ",", "\n", "dist_url", ",", "\n", "args", ",", "\n", "timeout", "=", "DEFAULT_TIMEOUT", ",", "\n", ")", ":", "\n", "    ", "assert", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"cuda is not available. Please check your installation.\"", "\n", "global_rank", "=", "machine_rank", "*", "num_gpus_per_machine", "+", "local_rank", "\n", "try", ":", "\n", "        ", "dist", ".", "init_process_group", "(", "\n", "backend", "=", "\"NCCL\"", ",", "\n", "init_method", "=", "dist_url", ",", "\n", "world_size", "=", "world_size", ",", "\n", "rank", "=", "global_rank", ",", "\n", "timeout", "=", "timeout", ",", "\n", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "error", "(", "\"Process group URL: {}\"", ".", "format", "(", "dist_url", ")", ")", "\n", "raise", "e", "\n", "# synchronize is needed here to prevent a possible timeout after calling init_process_group", "\n", "# See: https://github.com/facebookresearch/maskrcnn-benchmark/issues/172", "\n", "", "comm", ".", "synchronize", "(", ")", "\n", "\n", "assert", "num_gpus_per_machine", "<=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "local_rank", ")", "\n", "\n", "# Setup the local process group (which contains ranks within the same machine)", "\n", "assert", "comm", ".", "_LOCAL_PROCESS_GROUP", "is", "None", "\n", "num_machines", "=", "world_size", "//", "num_gpus_per_machine", "\n", "for", "i", "in", "range", "(", "num_machines", ")", ":", "\n", "        ", "ranks_on_i", "=", "list", "(", "range", "(", "i", "*", "num_gpus_per_machine", ",", "(", "i", "+", "1", ")", "*", "num_gpus_per_machine", ")", ")", "\n", "pg", "=", "dist", ".", "new_group", "(", "ranks_on_i", ")", "\n", "if", "i", "==", "machine_rank", ":", "\n", "            ", "comm", ".", "_LOCAL_PROCESS_GROUP", "=", "pg", "\n", "\n", "", "", "main_func", "(", "*", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.CallbackHook.__init__": [[47, 55], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "*", ",", "before_train", "=", "None", ",", "after_train", "=", "None", ",", "before_step", "=", "None", ",", "after_step", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Each argument is a function that takes one argument: the trainer.\n        \"\"\"", "\n", "self", ".", "_before_train", "=", "before_train", "\n", "self", ".", "_before_step", "=", "before_step", "\n", "self", ".", "_after_step", "=", "after_step", "\n", "self", ".", "_after_train", "=", "after_train", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.CallbackHook.before_train": [[56, 59], ["hooks.CallbackHook._before_train"], "methods", ["None"], ["", "def", "before_train", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_before_train", ":", "\n", "            ", "self", ".", "_before_train", "(", "self", ".", "trainer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.CallbackHook.after_train": [[60, 67], ["hooks.CallbackHook._after_train"], "methods", ["None"], ["", "", "def", "after_train", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_after_train", ":", "\n", "            ", "self", ".", "_after_train", "(", "self", ".", "trainer", ")", "\n", "# The functions may be closures that hold reference to the trainer", "\n", "# Therefore, delete them to avoid circular reference.", "\n", "", "del", "self", ".", "_before_train", ",", "self", ".", "_after_train", "\n", "del", "self", ".", "_before_step", ",", "self", ".", "_after_step", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.CallbackHook.before_step": [[68, 71], ["hooks.CallbackHook._before_step"], "methods", ["None"], ["", "def", "before_step", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_before_step", ":", "\n", "            ", "self", ".", "_before_step", "(", "self", ".", "trainer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.CallbackHook.after_step": [[72, 75], ["hooks.CallbackHook._after_step"], "methods", ["None"], ["", "", "def", "after_step", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_after_step", ":", "\n", "            ", "self", ".", "_after_step", "(", "self", ".", "trainer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.IterationTimer.__init__": [[89, 99], ["fvcore.common.timer.Timer", "time.perf_counter", "fvcore.common.timer.Timer"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "warmup_iter", "=", "3", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            warmup_iter (int): the number of iterations at the beginning to exclude\n                from timing.\n        \"\"\"", "\n", "self", ".", "_warmup_iter", "=", "warmup_iter", "\n", "self", ".", "_step_timer", "=", "Timer", "(", ")", "\n", "self", ".", "_start_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "self", ".", "_total_timer", "=", "Timer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.IterationTimer.before_train": [[100, 104], ["time.perf_counter", "hooks.IterationTimer._total_timer.reset", "hooks.IterationTimer._total_timer.pause"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.reset"], ["", "def", "before_train", "(", "self", ")", ":", "\n", "        ", "self", ".", "_start_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "self", ".", "_total_timer", ".", "reset", "(", ")", "\n", "self", ".", "_total_timer", ".", "pause", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.IterationTimer.after_train": [[105, 128], ["logging.getLogger", "hooks.IterationTimer._total_timer.seconds", "logging.getLogger.info", "time.perf_counter", "logging.getLogger.info", "str", "str", "str", "datetime.timedelta", "datetime.timedelta", "datetime.timedelta", "int", "int", "int"], "methods", ["None"], ["", "def", "after_train", "(", "self", ")", ":", "\n", "        ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "total_time", "=", "time", ".", "perf_counter", "(", ")", "-", "self", ".", "_start_time", "\n", "total_time_minus_hooks", "=", "self", ".", "_total_timer", ".", "seconds", "(", ")", "\n", "hook_time", "=", "total_time", "-", "total_time_minus_hooks", "\n", "\n", "num_iter", "=", "self", ".", "trainer", ".", "iter", "+", "1", "-", "self", ".", "trainer", ".", "start_iter", "-", "self", ".", "_warmup_iter", "\n", "\n", "if", "num_iter", ">", "0", "and", "total_time_minus_hooks", ">", "0", ":", "\n", "# Speed is meaningful only after warmup", "\n", "# NOTE this format is parsed by grep in some scripts", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Overall training speed: {} iterations in {} ({:.4f} s / it)\"", ".", "format", "(", "\n", "num_iter", ",", "\n", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "total_time_minus_hooks", ")", ")", ")", ",", "\n", "total_time_minus_hooks", "/", "num_iter", ",", "\n", ")", "\n", ")", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"Total training time: {} ({} on hooks)\"", ".", "format", "(", "\n", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "total_time", ")", ")", ")", ",", "\n", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "hook_time", ")", ")", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.IterationTimer.before_step": [[131, 134], ["hooks.IterationTimer._step_timer.reset", "hooks.IterationTimer._total_timer.resume"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.reset"], ["", "def", "before_step", "(", "self", ")", ":", "\n", "        ", "self", ".", "_step_timer", ".", "reset", "(", ")", "\n", "self", ".", "_total_timer", ".", "resume", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.IterationTimer.after_step": [[135, 147], ["hooks.IterationTimer._total_timer.pause", "hooks.IterationTimer._step_timer.seconds", "hooks.IterationTimer.trainer.storage.put_scalars", "time.perf_counter", "hooks.IterationTimer._total_timer.reset"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalars", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.reset"], ["", "def", "after_step", "(", "self", ")", ":", "\n", "# +1 because we're in after_step, the current step is done", "\n", "# but not yet counted", "\n", "        ", "iter_done", "=", "self", ".", "trainer", ".", "iter", "-", "self", ".", "trainer", ".", "start_iter", "+", "1", "\n", "if", "iter_done", ">=", "self", ".", "_warmup_iter", ":", "\n", "            ", "sec", "=", "self", ".", "_step_timer", ".", "seconds", "(", ")", "\n", "self", ".", "trainer", ".", "storage", ".", "put_scalars", "(", "time", "=", "sec", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_start_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "self", ".", "_total_timer", ".", "reset", "(", ")", "\n", "\n", "", "self", ".", "_total_timer", ".", "pause", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.PeriodicWriter.__init__": [[157, 167], ["isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "writers", ",", "period", "=", "20", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            writers (list[EventWriter]): a list of EventWriter objects\n            period (int):\n        \"\"\"", "\n", "self", ".", "_writers", "=", "writers", "\n", "for", "w", "in", "writers", ":", "\n", "            ", "assert", "isinstance", "(", "w", ",", "EventWriter", ")", ",", "w", "\n", "", "self", ".", "_period", "=", "period", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.PeriodicWriter.after_step": [[168, 174], ["writer.write"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["", "def", "after_step", "(", "self", ")", ":", "\n", "        ", "if", "(", "self", ".", "trainer", ".", "iter", "+", "1", ")", "%", "self", ".", "_period", "==", "0", "or", "(", "\n", "self", ".", "trainer", ".", "iter", "==", "self", ".", "trainer", ".", "max_iter", "-", "1", "\n", ")", ":", "\n", "            ", "for", "writer", "in", "self", ".", "_writers", ":", "\n", "                ", "writer", ".", "write", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.PeriodicWriter.after_train": [[175, 181], ["writer.write", "writer.close"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close"], ["", "", "", "def", "after_train", "(", "self", ")", ":", "\n", "        ", "for", "writer", "in", "self", ".", "_writers", ":", "\n", "# If any new data is found (e.g. produced by other after_train),", "\n", "# write them before closing", "\n", "            ", "writer", ".", "write", "(", ")", "\n", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.PeriodicCheckpointer.before_train": [[194, 196], ["None"], "methods", ["None"], ["def", "before_train", "(", "self", ")", ":", "\n", "        ", "self", ".", "max_iter", "=", "self", ".", "trainer", ".", "max_iter", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.PeriodicCheckpointer.after_step": [[197, 200], ["hooks.PeriodicCheckpointer.step"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step"], ["", "def", "after_step", "(", "self", ")", ":", "\n", "# No way to use **kwargs", "\n", "        ", "self", ".", "step", "(", "self", ".", "trainer", ".", "iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.LRScheduler.__init__": [[208, 220], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "optimizer", "=", "None", ",", "scheduler", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            optimizer (torch.optim.Optimizer):\n            scheduler (torch.optim.LRScheduler or fvcore.common.param_scheduler.ParamScheduler):\n                if a :class:`ParamScheduler` object, it defines the multiplier over the base LR\n                in the optimizer.\n\n        If any argument is not given, will try to obtain it from the trainer.\n        \"\"\"", "\n", "self", ".", "_optimizer", "=", "optimizer", "\n", "self", ".", "_scheduler", "=", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.LRScheduler.before_train": [[221, 249], ["isinstance", "max", "detectron2.solver.LRMultiplier", "collections.Counter", "enumerate", "enumerate", "len", "collections.Counter.most_common", "len"], "methods", ["None"], ["", "def", "before_train", "(", "self", ")", ":", "\n", "        ", "self", ".", "_optimizer", "=", "self", ".", "_optimizer", "or", "self", ".", "trainer", ".", "optimizer", "\n", "if", "isinstance", "(", "self", ".", "scheduler", ",", "ParamScheduler", ")", ":", "\n", "            ", "self", ".", "_scheduler", "=", "LRMultiplier", "(", "\n", "self", ".", "_optimizer", ",", "\n", "self", ".", "scheduler", ",", "\n", "self", ".", "trainer", ".", "max_iter", ",", "\n", "last_iter", "=", "self", ".", "trainer", ".", "iter", "-", "1", ",", "\n", ")", "\n", "\n", "# NOTE: some heuristics on what LR to summarize", "\n", "# summarize the param group with most parameters", "\n", "", "largest_group", "=", "max", "(", "len", "(", "g", "[", "\"params\"", "]", ")", "for", "g", "in", "self", ".", "_optimizer", ".", "param_groups", ")", "\n", "\n", "if", "largest_group", "==", "1", ":", "\n", "# If all groups have one parameter,", "\n", "# then find the most common initial LR, and use it for summary", "\n", "            ", "lr_count", "=", "Counter", "(", "[", "g", "[", "\"lr\"", "]", "for", "g", "in", "self", ".", "_optimizer", ".", "param_groups", "]", ")", "\n", "lr", "=", "lr_count", ".", "most_common", "(", ")", "[", "0", "]", "[", "0", "]", "\n", "for", "i", ",", "g", "in", "enumerate", "(", "self", ".", "_optimizer", ".", "param_groups", ")", ":", "\n", "                ", "if", "g", "[", "\"lr\"", "]", "==", "lr", ":", "\n", "                    ", "self", ".", "_best_param_group_id", "=", "i", "\n", "break", "\n", "", "", "", "else", ":", "\n", "            ", "for", "i", ",", "g", "in", "enumerate", "(", "self", ".", "_optimizer", ".", "param_groups", ")", ":", "\n", "                ", "if", "len", "(", "g", "[", "\"params\"", "]", ")", "==", "largest_group", ":", "\n", "                    ", "self", ".", "_best_param_group_id", "=", "i", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.LRScheduler.after_step": [[250, 254], ["hooks.LRScheduler.trainer.storage.put_scalar", "hooks.LRScheduler.scheduler.step"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step"], ["", "", "", "", "def", "after_step", "(", "self", ")", ":", "\n", "        ", "lr", "=", "self", ".", "_optimizer", ".", "param_groups", "[", "self", ".", "_best_param_group_id", "]", "[", "\"lr\"", "]", "\n", "self", ".", "trainer", ".", "storage", ".", "put_scalar", "(", "\"lr\"", ",", "lr", ",", "smoothing_hint", "=", "False", ")", "\n", "self", ".", "scheduler", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.LRScheduler.scheduler": [[255, 258], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "scheduler", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_scheduler", "or", "self", ".", "trainer", ".", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.LRScheduler.state_dict": [[259, 263], ["isinstance", "hooks.LRScheduler.scheduler.state_dict"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "scheduler", ",", "torch", ".", "optim", ".", "lr_scheduler", ".", "_LRScheduler", ")", ":", "\n", "            ", "return", "self", ".", "scheduler", ".", "state_dict", "(", ")", "\n", "", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.LRScheduler.load_state_dict": [[264, 269], ["isinstance", "logging.getLogger", "logging.getLogger.info", "hooks.LRScheduler.scheduler.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "scheduler", ",", "torch", ".", "optim", ".", "lr_scheduler", ".", "_LRScheduler", ")", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"Loading scheduler from state_dict ...\"", ")", "\n", "self", ".", "scheduler", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.AutogradProfiler.__init__": [[294, 306], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "enable_predicate", ",", "output_dir", ",", "*", ",", "use_cuda", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            enable_predicate (callable[trainer -> bool]): a function which takes a trainer,\n                and returns whether to enable the profiler.\n                It will be called once every step, and can be used to select which steps to profile.\n            output_dir (str): the output directory to dump tracing files.\n            use_cuda (bool): same as in `torch.autograd.profiler.profile`.\n        \"\"\"", "\n", "self", ".", "_enable_predicate", "=", "enable_predicate", "\n", "self", ".", "_use_cuda", "=", "use_cuda", "\n", "self", ".", "_output_dir", "=", "output_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.AutogradProfiler.before_step": [[307, 313], ["hooks.AutogradProfiler._enable_predicate", "torch.autograd.profiler.profile", "hooks.AutogradProfiler._profiler.__enter__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.ScopedWS.__enter__"], ["", "def", "before_step", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_enable_predicate", "(", "self", ".", "trainer", ")", ":", "\n", "            ", "self", ".", "_profiler", "=", "torch", ".", "autograd", ".", "profiler", ".", "profile", "(", "use_cuda", "=", "self", ".", "_use_cuda", ")", "\n", "self", ".", "_profiler", ".", "__enter__", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_profiler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.AutogradProfiler.after_step": [[314, 333], ["hooks.AutogradProfiler._profiler.__exit__", "detectron2.utils.file_io.PathManager.mkdirs", "os.path.join", "hooks.AutogradProfiler._profiler.export_chrome_trace", "tempfile.TemporaryDirectory", "os.path.join", "hooks.AutogradProfiler._profiler.export_chrome_trace", "detectron2.utils.file_io.PathManager.open", "f.write", "open", "f.read"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.shared.ScopedWS.__exit__", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["", "", "def", "after_step", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_profiler", "is", "None", ":", "\n", "            ", "return", "\n", "", "self", ".", "_profiler", ".", "__exit__", "(", "None", ",", "None", ",", "None", ")", "\n", "PathManager", ".", "mkdirs", "(", "self", ".", "_output_dir", ")", "\n", "out_file", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_output_dir", ",", "\"profiler-trace-iter{}.json\"", ".", "format", "(", "self", ".", "trainer", ".", "iter", ")", "\n", ")", "\n", "if", "\"://\"", "not", "in", "out_file", ":", "\n", "            ", "self", ".", "_profiler", ".", "export_chrome_trace", "(", "out_file", ")", "\n", "", "else", ":", "\n", "# Support non-posix filesystems", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2_profiler\"", ")", "as", "d", ":", "\n", "                ", "tmp_file", "=", "os", ".", "path", ".", "join", "(", "d", ",", "\"tmp.json\"", ")", "\n", "self", ".", "_profiler", ".", "export_chrome_trace", "(", "tmp_file", ")", "\n", "with", "open", "(", "tmp_file", ")", "as", "f", ":", "\n", "                    ", "content", "=", "f", ".", "read", "(", ")", "\n", "", "", "with", "PathManager", ".", "open", "(", "out_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "content", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.EvalHook.__init__": [[342, 357], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "eval_period", ",", "eval_function", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            eval_period (int): the period to run `eval_function`. Set to 0 to\n                not evaluate periodically (but still after the last iteration).\n            eval_function (callable): a function which takes no arguments, and\n                returns a nested dict of evaluation metrics.\n\n        Note:\n            This hook must be enabled in all or none workers.\n            If you would like only certain workers to perform evaluation,\n            give other workers a no-op function (`eval_function=lambda: None`).\n        \"\"\"", "\n", "self", ".", "_period", "=", "eval_period", "\n", "self", ".", "_func", "=", "eval_function", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.EvalHook._do_eval": [[358, 380], ["hooks.EvalHook._func", "detectron2.synchronize", "isinstance", "detectron2.evaluation.testing.flatten_results_dict", "detectron2.evaluation.testing.flatten_results_dict.items", "hooks.EvalHook.trainer.storage.put_scalars", "float", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.flatten_results_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalars"], ["", "def", "_do_eval", "(", "self", ")", ":", "\n", "        ", "results", "=", "self", ".", "_func", "(", ")", "\n", "\n", "if", "results", ":", "\n", "            ", "assert", "isinstance", "(", "\n", "results", ",", "dict", "\n", ")", ",", "\"Eval function must return a dict. Got {} instead.\"", ".", "format", "(", "results", ")", "\n", "\n", "flattened_results", "=", "flatten_results_dict", "(", "results", ")", "\n", "for", "k", ",", "v", "in", "flattened_results", ".", "items", "(", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "v", "=", "float", "(", "v", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"[EvalHook] eval_function should return a nested dict of float. \"", "\n", "\"Got '{}: {}' instead.\"", ".", "format", "(", "k", ",", "v", ")", "\n", ")", "from", "e", "\n", "", "", "self", ".", "trainer", ".", "storage", ".", "put_scalars", "(", "**", "flattened_results", ",", "smoothing_hint", "=", "False", ")", "\n", "\n", "# Evaluation may take different time among workers.", "\n", "# A barrier make them start the next iteration together.", "\n", "", "comm", ".", "synchronize", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.EvalHook.after_step": [[381, 387], ["hooks.EvalHook._do_eval"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.EvalHook._do_eval"], ["", "def", "after_step", "(", "self", ")", ":", "\n", "        ", "next_iter", "=", "self", ".", "trainer", ".", "iter", "+", "1", "\n", "if", "self", ".", "_period", ">", "0", "and", "next_iter", "%", "self", ".", "_period", "==", "0", ":", "\n", "# do the last eval in after_train", "\n", "            ", "if", "next_iter", "!=", "self", ".", "trainer", ".", "max_iter", ":", "\n", "                ", "self", ".", "_do_eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.EvalHook.after_train": [[388, 395], ["hooks.EvalHook._do_eval"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.EvalHook._do_eval"], ["", "", "", "def", "after_train", "(", "self", ")", ":", "\n", "# This condition is to prevent the eval from running after a failed training", "\n", "        ", "if", "self", ".", "trainer", ".", "iter", "+", "1", ">=", "self", ".", "trainer", ".", "max_iter", ":", "\n", "            ", "self", ".", "_do_eval", "(", ")", "\n", "# func is likely a closure that holds reference to the trainer", "\n", "# therefore we clean it to avoid circular reference in the end", "\n", "", "del", "self", ".", "_func", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.PreciseBN.__init__": [[407, 435], ["logging.getLogger", "len", "hooks.PreciseBN._logger.info", "fvcore.nn.precise_bn.get_bn_modules"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "period", ",", "model", ",", "data_loader", ",", "num_iter", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            period (int): the period this hook is run, or 0 to not run during training.\n                The hook will always run in the end of training.\n            model (nn.Module): a module whose all BN layers in training mode will be\n                updated by precise BN.\n                Note that user is responsible for ensuring the BN layers to be\n                updated are in training mode when this hook is triggered.\n            data_loader (iterable): it will produce data to be run by `model(data)`.\n            num_iter (int): number of iterations used to compute the precise\n                statistics.\n        \"\"\"", "\n", "self", ".", "_logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "if", "len", "(", "get_bn_modules", "(", "model", ")", ")", "==", "0", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\n", "\"PreciseBN is disabled because model does not contain BN layers in training mode.\"", "\n", ")", "\n", "self", ".", "_disabled", "=", "True", "\n", "return", "\n", "\n", "", "self", ".", "_model", "=", "model", "\n", "self", ".", "_data_loader", "=", "data_loader", "\n", "self", ".", "_num_iter", "=", "num_iter", "\n", "self", ".", "_period", "=", "period", "\n", "self", ".", "_disabled", "=", "False", "\n", "\n", "self", ".", "_data_iter", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.PreciseBN.after_step": [[436, 441], ["hooks.PreciseBN.update_stats"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.PreciseBN.update_stats"], ["", "def", "after_step", "(", "self", ")", ":", "\n", "        ", "next_iter", "=", "self", ".", "trainer", ".", "iter", "+", "1", "\n", "is_final", "=", "next_iter", "==", "self", ".", "trainer", ".", "max_iter", "\n", "if", "is_final", "or", "(", "self", ".", "_period", ">", "0", "and", "next_iter", "%", "self", ".", "_period", "==", "0", ")", ":", "\n", "            ", "self", ".", "update_stats", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.PreciseBN.update_stats": [[442, 467], ["iter", "itertools.count", "detectron2.utils.events.EventStorage", "hooks.PreciseBN._logger.info", "fvcore.nn.precise_bn.update_bn_stats", "hooks.PreciseBN.update_stats.data_loader"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter"], ["", "", "def", "update_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Update the model with precise statistics. Users can manually call this method.\n        \"\"\"", "\n", "if", "self", ".", "_disabled", ":", "\n", "            ", "return", "\n", "\n", "", "if", "self", ".", "_data_iter", "is", "None", ":", "\n", "            ", "self", ".", "_data_iter", "=", "iter", "(", "self", ".", "_data_loader", ")", "\n", "\n", "", "def", "data_loader", "(", ")", ":", "\n", "            ", "for", "num_iter", "in", "itertools", ".", "count", "(", "1", ")", ":", "\n", "                ", "if", "num_iter", "%", "100", "==", "0", ":", "\n", "                    ", "self", ".", "_logger", ".", "info", "(", "\n", "\"Running precise-BN ... {}/{} iterations.\"", ".", "format", "(", "num_iter", ",", "self", ".", "_num_iter", ")", "\n", ")", "\n", "# This way we can reuse the same iterator", "\n", "", "yield", "next", "(", "self", ".", "_data_iter", ")", "\n", "\n", "", "", "with", "EventStorage", "(", ")", ":", "# capture events in a new storage to discard them", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\n", "\"Running precise-BN for {} iterations...  \"", ".", "format", "(", "self", ".", "_num_iter", ")", "\n", "+", "\"Note that this could produce different statistics every time.\"", "\n", ")", "\n", "update_bn_stats", "(", "self", ".", "_model", ",", "data_loader", "(", ")", ",", "self", ".", "_num_iter", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.HookBase.before_train": [[56, 61], ["None"], "methods", ["None"], ["def", "before_train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Called before the first iteration.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.HookBase.after_train": [[62, 67], ["None"], "methods", ["None"], ["", "def", "after_train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Called after the last iteration.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.HookBase.before_step": [[68, 73], ["None"], "methods", ["None"], ["", "def", "before_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Called before each iteration.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.HookBase.after_step": [[74, 79], ["None"], "methods", ["None"], ["", "def", "after_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Called after each iteration.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.HookBase.state_dict": [[80, 86], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Hooks are stateless by default, but can be made checkpointable by\n        implementing `state_dict` and `load_state_dict`.\n        \"\"\"", "\n", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.__init__": [[107, 114], ["detectron2.utils.logger._log_api_usage"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._log_api_usage"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_hooks", ":", "List", "[", "HookBase", "]", "=", "[", "]", "\n", "self", ".", "iter", ":", "int", "=", "0", "\n", "self", ".", "start_iter", ":", "int", "=", "0", "\n", "self", ".", "max_iter", ":", "int", "\n", "self", ".", "storage", ":", "EventStorage", "\n", "_log_api_usage", "(", "\"trainer.\"", "+", "self", ".", "__class__", ".", "__name__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.register_hooks": [[115, 132], ["train_loop.TrainerBase._hooks.extend", "isinstance", "weakref.proxy"], "methods", ["None"], ["", "def", "register_hooks", "(", "self", ",", "hooks", ":", "List", "[", "Optional", "[", "HookBase", "]", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Register hooks to the trainer. The hooks are executed in the order\n        they are registered.\n\n        Args:\n            hooks (list[Optional[HookBase]]): list of hooks\n        \"\"\"", "\n", "hooks", "=", "[", "h", "for", "h", "in", "hooks", "if", "h", "is", "not", "None", "]", "\n", "for", "h", "in", "hooks", ":", "\n", "            ", "assert", "isinstance", "(", "h", ",", "HookBase", ")", "\n", "# To avoid circular reference, hooks and trainer cannot own each other.", "\n", "# This normally does not matter, but will cause memory leak if the", "\n", "# involved objects contain __del__:", "\n", "# See http://engineering.hearsaysocial.com/2013/06/16/circular-references-in-python/", "\n", "h", ".", "trainer", "=", "weakref", ".", "proxy", "(", "self", ")", "\n", "", "self", ".", "_hooks", ".", "extend", "(", "hooks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train": [[133, 160], ["logging.getLogger", "logging.getLogger.info", "detectron2.utils.events.EventStorage", "train_loop.TrainerBase.before_train", "range", "train_loop.TrainerBase.after_train", "train_loop.TrainerBase.before_step", "train_loop.TrainerBase.run_step", "train_loop.TrainerBase.after_step", "logging.getLogger.exception"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.before_train", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.after_train", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.before_step", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.run_step", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.after_step"], ["", "def", "train", "(", "self", ",", "start_iter", ":", "int", ",", "max_iter", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            start_iter, max_iter (int): See docs above\n        \"\"\"", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"Starting training from iteration {}\"", ".", "format", "(", "start_iter", ")", ")", "\n", "\n", "self", ".", "iter", "=", "self", ".", "start_iter", "=", "start_iter", "\n", "self", ".", "max_iter", "=", "max_iter", "\n", "\n", "with", "EventStorage", "(", "start_iter", ")", "as", "self", ".", "storage", ":", "\n", "            ", "try", ":", "\n", "                ", "self", ".", "before_train", "(", ")", "\n", "for", "self", ".", "iter", "in", "range", "(", "start_iter", ",", "max_iter", ")", ":", "\n", "                    ", "self", ".", "before_step", "(", ")", "\n", "self", ".", "run_step", "(", ")", "\n", "self", ".", "after_step", "(", ")", "\n", "# self.iter == max_iter can be used by `after_train` to", "\n", "# tell whether the training successfully finished or failed", "\n", "# due to exceptions.", "\n", "", "self", ".", "iter", "+=", "1", "\n", "", "except", "Exception", ":", "\n", "                ", "logger", ".", "exception", "(", "\"Exception during training:\"", ")", "\n", "raise", "\n", "", "finally", ":", "\n", "                ", "self", ".", "after_train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.before_train": [[161, 164], ["h.before_train"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.before_train"], ["", "", "", "def", "before_train", "(", "self", ")", ":", "\n", "        ", "for", "h", "in", "self", ".", "_hooks", ":", "\n", "            ", "h", ".", "before_train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.after_train": [[165, 169], ["h.after_train"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.after_train"], ["", "", "def", "after_train", "(", "self", ")", ":", "\n", "        ", "self", ".", "storage", ".", "iter", "=", "self", ".", "iter", "\n", "for", "h", "in", "self", ".", "_hooks", ":", "\n", "            ", "h", ".", "after_train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.before_step": [[170, 177], ["h.before_step"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.before_step"], ["", "", "def", "before_step", "(", "self", ")", ":", "\n", "# Maintain the invariant that storage.iter == trainer.iter", "\n", "# for the entire execution of each step", "\n", "        ", "self", ".", "storage", ".", "iter", "=", "self", ".", "iter", "\n", "\n", "for", "h", "in", "self", ".", "_hooks", ":", "\n", "            ", "h", ".", "before_step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.after_step": [[178, 181], ["h.after_step"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.after_step"], ["", "", "def", "after_step", "(", "self", ")", ":", "\n", "        ", "for", "h", "in", "self", ".", "_hooks", ":", "\n", "            ", "h", ".", "after_step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.run_step": [[182, 184], ["None"], "methods", ["None"], ["", "", "def", "run_step", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.state_dict": [[185, 199], ["h.state_dict", "type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "ret", "=", "{", "\"iteration\"", ":", "self", ".", "iter", "}", "\n", "hooks_state", "=", "{", "}", "\n", "for", "h", "in", "self", ".", "_hooks", ":", "\n", "            ", "sd", "=", "h", ".", "state_dict", "(", ")", "\n", "if", "sd", ":", "\n", "                ", "name", "=", "type", "(", "h", ")", ".", "__qualname__", "\n", "if", "name", "in", "hooks_state", ":", "\n", "# TODO handle repetitive stateful hooks", "\n", "                    ", "continue", "\n", "", "hooks_state", "[", "name", "]", "=", "sd", "\n", "", "", "if", "hooks_state", ":", "\n", "            ", "ret", "[", "\"hooks\"", "]", "=", "hooks_state", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.load_state_dict": [[200, 214], ["logging.getLogger", "state_dict.get().items", "state_dict.get", "logging.getLogger.warning", "h.load_state_dict", "type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "self", ".", "iter", "=", "state_dict", "[", "\"iteration\"", "]", "\n", "for", "key", ",", "value", "in", "state_dict", ".", "get", "(", "\"hooks\"", ",", "{", "}", ")", ".", "items", "(", ")", ":", "\n", "            ", "for", "h", "in", "self", ".", "_hooks", ":", "\n", "                ", "try", ":", "\n", "                    ", "name", "=", "type", "(", "h", ")", ".", "__qualname__", "\n", "", "except", "AttributeError", ":", "\n", "                    ", "continue", "\n", "", "if", "name", "==", "key", ":", "\n", "                    ", "h", ".", "load_state_dict", "(", "value", ")", "\n", "break", "\n", "", "", "else", ":", "\n", "                ", "logger", ".", "warning", "(", "f\"Cannot find the hook '{key}', its state_dict is ignored.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.SimpleTrainer.__init__": [[235, 257], ["train_loop.TrainerBase.__init__", "model.train", "iter"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter"], ["def", "__init__", "(", "self", ",", "model", ",", "data_loader", ",", "optimizer", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            model: a torch Module. Takes a data from data_loader and returns a\n                dict of losses.\n            data_loader: an iterable. Contains data to be used to call model.\n            optimizer: a torch optimizer.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "\"\"\"\n        We set the model to training mode in the trainer.\n        However it's valid to train a model that's in eval mode.\n        If you want your model (or a submodule of it) to behave\n        like evaluation during training, you can overwrite its train() method.\n        \"\"\"", "\n", "model", ".", "train", "(", ")", "\n", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "data_loader", "=", "data_loader", "\n", "self", ".", "_data_loader_iter", "=", "iter", "(", "data_loader", ")", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.SimpleTrainer.run_step": [[258, 295], ["time.perf_counter", "next", "train_loop.SimpleTrainer.model", "isinstance", "train_loop.SimpleTrainer.optimizer.zero_grad", "sum.backward", "train_loop.SimpleTrainer._write_metrics", "train_loop.SimpleTrainer.optimizer.step", "time.perf_counter", "sum", "train_loop.SimpleTrainer.values"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.SimpleTrainer._write_metrics", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step"], ["", "def", "run_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Implement the standard training logic described above.\n        \"\"\"", "\n", "assert", "self", ".", "model", ".", "training", ",", "\"[SimpleTrainer] model was changed to eval mode!\"", "\n", "start", "=", "time", ".", "perf_counter", "(", ")", "\n", "\"\"\"\n        If you want to do something with the data, you can wrap the dataloader.\n        \"\"\"", "\n", "data", "=", "next", "(", "self", ".", "_data_loader_iter", ")", "\n", "data_time", "=", "time", ".", "perf_counter", "(", ")", "-", "start", "\n", "\n", "\"\"\"\n        If you want to do something with the losses, you can wrap the model.\n        \"\"\"", "\n", "loss_dict", "=", "self", ".", "model", "(", "data", ")", "\n", "if", "isinstance", "(", "loss_dict", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "losses", "=", "loss_dict", "\n", "loss_dict", "=", "{", "\"total_loss\"", ":", "loss_dict", "}", "\n", "", "else", ":", "\n", "            ", "losses", "=", "sum", "(", "loss_dict", ".", "values", "(", ")", ")", "\n", "\n", "", "\"\"\"\n        If you need to accumulate gradients or do something similar, you can\n        wrap the optimizer with your custom `zero_grad()` method.\n        \"\"\"", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "losses", ".", "backward", "(", ")", "\n", "\n", "self", ".", "_write_metrics", "(", "loss_dict", ",", "data_time", ")", "\n", "\n", "\"\"\"\n        If you need gradient clipping/scaling or other processing, you can\n        wrap the optimizer with your custom `step()` method. But it is\n        suboptimal as explained in https://arxiv.org/abs/2006.15704 Sec 3.2.4\n        \"\"\"", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.SimpleTrainer._write_metrics": [[296, 337], ["detectron2.gather", "detectron2.is_main_process", "v.detach().cpu().item", "detectron2.utils.events.get_event_storage", "numpy.max", "detectron2.utils.events.get_event_storage.put_scalar", "sum", "detectron2.utils.events.get_event_storage.put_scalar", "loss_dict.items", "numpy.mean", "metrics_dict.values", "numpy.isfinite", "FloatingPointError", "len", "detectron2.utils.events.get_event_storage.put_scalars", "v.detach().cpu", "x.pop", "all_metrics_dict[].keys", "v.detach"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.get_event_storage", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalars"], ["", "def", "_write_metrics", "(", "\n", "self", ",", "\n", "loss_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "data_time", ":", "float", ",", "\n", "prefix", ":", "str", "=", "\"\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            loss_dict (dict): dict of scalar losses\n            data_time (float): time taken by the dataloader iteration\n        \"\"\"", "\n", "metrics_dict", "=", "{", "k", ":", "v", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "for", "k", ",", "v", "in", "loss_dict", ".", "items", "(", ")", "}", "\n", "metrics_dict", "[", "\"data_time\"", "]", "=", "data_time", "\n", "\n", "# Gather metrics among all workers for logging", "\n", "# This assumes we do DDP-style training, which is currently the only", "\n", "# supported method in detectron2.", "\n", "all_metrics_dict", "=", "comm", ".", "gather", "(", "metrics_dict", ")", "\n", "\n", "if", "comm", ".", "is_main_process", "(", ")", ":", "\n", "            ", "storage", "=", "get_event_storage", "(", ")", "\n", "\n", "# data_time among workers can have high variance. The actual latency", "\n", "# caused by data_time is the maximum among workers.", "\n", "data_time", "=", "np", ".", "max", "(", "[", "x", ".", "pop", "(", "\"data_time\"", ")", "for", "x", "in", "all_metrics_dict", "]", ")", "\n", "storage", ".", "put_scalar", "(", "\"data_time\"", ",", "data_time", ")", "\n", "\n", "# average the rest metrics", "\n", "metrics_dict", "=", "{", "\n", "k", ":", "np", ".", "mean", "(", "[", "x", "[", "k", "]", "for", "x", "in", "all_metrics_dict", "]", ")", "for", "k", "in", "all_metrics_dict", "[", "0", "]", ".", "keys", "(", ")", "\n", "}", "\n", "total_losses_reduced", "=", "sum", "(", "metrics_dict", ".", "values", "(", ")", ")", "\n", "if", "not", "np", ".", "isfinite", "(", "total_losses_reduced", ")", ":", "\n", "                ", "raise", "FloatingPointError", "(", "\n", "f\"Loss became infinite or NaN at iteration={self.iter}!\\n\"", "\n", "f\"loss_dict = {metrics_dict}\"", "\n", ")", "\n", "\n", "", "storage", ".", "put_scalar", "(", "\"{}total_loss\"", ".", "format", "(", "prefix", ")", ",", "total_losses_reduced", ")", "\n", "if", "len", "(", "metrics_dict", ")", ">", "1", ":", "\n", "                ", "storage", ".", "put_scalars", "(", "**", "metrics_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.SimpleTrainer.state_dict": [[338, 342], ["train_loop.TrainerBase.state_dict", "train_loop.SimpleTrainer.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict"], ["", "", "", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "state_dict", "(", ")", "\n", "ret", "[", "\"optimizer\"", "]", "=", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.SimpleTrainer.load_state_dict": [[343, 346], ["train_loop.TrainerBase.load_state_dict", "train_loop.SimpleTrainer.optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.load_state_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_state_dict", "(", "state_dict", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "state_dict", "[", "\"optimizer\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.__init__": [[354, 372], ["isinstance", "train_loop.SimpleTrainer.__init__", "isinstance", "GradScaler", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "data_loader", ",", "optimizer", ",", "grad_scaler", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            model, data_loader, optimizer: same as in :class:`SimpleTrainer`.\n            grad_scaler: torch GradScaler to automatically scale gradients.\n        \"\"\"", "\n", "unsupported", "=", "\"AMPTrainer does not support single-process multi-device training!\"", "\n", "if", "isinstance", "(", "model", ",", "DistributedDataParallel", ")", ":", "\n", "            ", "assert", "not", "(", "model", ".", "device_ids", "and", "len", "(", "model", ".", "device_ids", ")", ">", "1", ")", ",", "unsupported", "\n", "", "assert", "not", "isinstance", "(", "model", ",", "DataParallel", ")", ",", "unsupported", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "model", ",", "data_loader", ",", "optimizer", ")", "\n", "\n", "if", "grad_scaler", "is", "None", ":", "\n", "            ", "from", "torch", ".", "cuda", ".", "amp", "import", "GradScaler", "\n", "\n", "grad_scaler", "=", "GradScaler", "(", ")", "\n", "", "self", ".", "grad_scaler", "=", "grad_scaler", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.run_step": [[373, 400], ["torch.cuda.is_available", "time.perf_counter", "next", "train_loop.AMPTrainer.optimizer.zero_grad", "train_loop.AMPTrainer.grad_scaler.scale().backward", "train_loop.AMPTrainer._write_metrics", "train_loop.AMPTrainer.grad_scaler.step", "train_loop.AMPTrainer.grad_scaler.update", "time.perf_counter", "autocast", "train_loop.AMPTrainer.model", "isinstance", "sum", "train_loop.AMPTrainer.grad_scaler.scale", "train_loop.AMPTrainer.values"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.SimpleTrainer._write_metrics", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.scale"], ["", "def", "run_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Implement the AMP training logic.\n        \"\"\"", "\n", "assert", "self", ".", "model", ".", "training", ",", "\"[AMPTrainer] model was changed to eval mode!\"", "\n", "assert", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"[AMPTrainer] CUDA is required for AMP training!\"", "\n", "from", "torch", ".", "cuda", ".", "amp", "import", "autocast", "\n", "\n", "start", "=", "time", ".", "perf_counter", "(", ")", "\n", "data", "=", "next", "(", "self", ".", "_data_loader_iter", ")", "\n", "data_time", "=", "time", ".", "perf_counter", "(", ")", "-", "start", "\n", "\n", "with", "autocast", "(", ")", ":", "\n", "            ", "loss_dict", "=", "self", ".", "model", "(", "data", ")", "\n", "if", "isinstance", "(", "loss_dict", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "losses", "=", "loss_dict", "\n", "loss_dict", "=", "{", "\"total_loss\"", ":", "loss_dict", "}", "\n", "", "else", ":", "\n", "                ", "losses", "=", "sum", "(", "loss_dict", ".", "values", "(", ")", ")", "\n", "\n", "", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "grad_scaler", ".", "scale", "(", "losses", ")", ".", "backward", "(", ")", "\n", "\n", "self", ".", "_write_metrics", "(", "loss_dict", ",", "data_time", ")", "\n", "\n", "self", ".", "grad_scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "grad_scaler", ".", "update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.state_dict": [[401, 405], ["train_loop.SimpleTrainer.state_dict", "train_loop.AMPTrainer.grad_scaler.state_dict"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "state_dict", "(", ")", "\n", "ret", "[", "\"grad_scaler\"", "]", "=", "self", ".", "grad_scaler", ".", "state_dict", "(", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.load_state_dict": [[406, 409], ["train_loop.SimpleTrainer.load_state_dict", "train_loop.AMPTrainer.grad_scaler.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.load_state_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_state_dict", "(", "state_dict", ")", "\n", "self", ".", "grad_scaler", ".", "load_state_dict", "(", "state_dict", "[", "\"grad_scaler\"", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.MapDataset.__init__": [[28, 34], ["detectron2.utils.serialize.PicklableWrapper", "random.Random", "set", "range", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["def", "__init__", "(", "self", ",", "dataset", ",", "map_func", ")", ":", "\n", "        ", "self", ".", "_dataset", "=", "dataset", "\n", "self", ".", "_map_func", "=", "PicklableWrapper", "(", "map_func", ")", "# wrap so that a lambda will work", "\n", "\n", "self", ".", "_rng", "=", "random", ".", "Random", "(", "42", ")", "\n", "self", ".", "_fallback_candidates", "=", "set", "(", "range", "(", "len", "(", "dataset", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.MapDataset.__len__": [[35, 37], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.MapDataset.__getitem__": [[38, 58], ["int", "common.MapDataset._map_func", "common.MapDataset._fallback_candidates.discard", "common.MapDataset._fallback_candidates.add", "common.MapDataset._rng.sample", "logging.getLogger", "logging.getLogger.warning"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "retry_count", "=", "0", "\n", "cur_idx", "=", "int", "(", "idx", ")", "\n", "\n", "while", "True", ":", "\n", "            ", "data", "=", "self", ".", "_map_func", "(", "self", ".", "_dataset", "[", "cur_idx", "]", ")", "\n", "if", "data", "is", "not", "None", ":", "\n", "                ", "self", ".", "_fallback_candidates", ".", "add", "(", "cur_idx", ")", "\n", "return", "data", "\n", "\n", "# _map_func fails for this idx, use a random new index from the pool", "\n", "", "retry_count", "+=", "1", "\n", "self", ".", "_fallback_candidates", ".", "discard", "(", "cur_idx", ")", "\n", "cur_idx", "=", "self", ".", "_rng", ".", "sample", "(", "self", ".", "_fallback_candidates", ",", "k", "=", "1", ")", "[", "0", "]", "\n", "\n", "if", "retry_count", ">=", "3", ":", "\n", "                ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "warning", "(", "\n", "\"Failed to apply `_map_func` for idx: {}, retry count: {}\"", ".", "format", "(", "\n", "idx", ",", "retry_count", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.DatasetFromList.__init__": [[67, 98], ["pickle.dumps", "numpy.frombuffer", "logging.getLogger", "logging.getLogger.info", "numpy.asarray", "numpy.cumsum", "numpy.concatenate", "logging.getLogger.info", "common.DatasetFromList.__init__._serialize"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lst", ":", "list", ",", "copy", ":", "bool", "=", "True", ",", "serialize", ":", "bool", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            lst (list): a list which contains elements to produce.\n            copy (bool): whether to deepcopy the element when producing it,\n                so that the result can be modified in place without affecting the\n                source in the list.\n            serialize (bool): whether to hold memory using serialized objects, when\n                enabled, data loader workers can use shared RAM from master\n                process instead of making a copy.\n        \"\"\"", "\n", "self", ".", "_lst", "=", "lst", "\n", "self", ".", "_copy", "=", "copy", "\n", "self", ".", "_serialize", "=", "serialize", "\n", "\n", "def", "_serialize", "(", "data", ")", ":", "\n", "            ", "buffer", "=", "pickle", ".", "dumps", "(", "data", ",", "protocol", "=", "-", "1", ")", "\n", "return", "np", ".", "frombuffer", "(", "buffer", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n", "", "if", "self", ".", "_serialize", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\n", "\"Serializing {} elements to byte tensors and concatenating them all ...\"", ".", "format", "(", "\n", "len", "(", "self", ".", "_lst", ")", "\n", ")", "\n", ")", "\n", "self", ".", "_lst", "=", "[", "_serialize", "(", "x", ")", "for", "x", "in", "self", ".", "_lst", "]", "\n", "self", ".", "_addr", "=", "np", ".", "asarray", "(", "[", "len", "(", "x", ")", "for", "x", "in", "self", ".", "_lst", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "_addr", "=", "np", ".", "cumsum", "(", "self", ".", "_addr", ")", "\n", "self", ".", "_lst", "=", "np", ".", "concatenate", "(", "self", ".", "_lst", ")", "\n", "logger", ".", "info", "(", "\"Serialized dataset takes {:.2f} MiB\"", ".", "format", "(", "len", "(", "self", ".", "_lst", ")", "/", "1024", "**", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.DatasetFromList.__len__": [[99, 104], ["len", "len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_serialize", ":", "\n", "            ", "return", "len", "(", "self", ".", "_addr", ")", "\n", "", "else", ":", "\n", "            ", "return", "len", "(", "self", ".", "_lst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.DatasetFromList.__getitem__": [[105, 115], ["common.DatasetFromList._addr[].item", "memoryview", "pickle.loads", "common.DatasetFromList._addr[].item", "copy.deepcopy"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "self", ".", "_serialize", ":", "\n", "            ", "start_addr", "=", "0", "if", "idx", "==", "0", "else", "self", ".", "_addr", "[", "idx", "-", "1", "]", ".", "item", "(", ")", "\n", "end_addr", "=", "self", ".", "_addr", "[", "idx", "]", ".", "item", "(", ")", "\n", "bytes", "=", "memoryview", "(", "self", ".", "_lst", "[", "start_addr", ":", "end_addr", "]", ")", "\n", "return", "pickle", ".", "loads", "(", "bytes", ")", "\n", "", "elif", "self", ".", "_copy", ":", "\n", "            ", "return", "copy", ".", "deepcopy", "(", "self", ".", "_lst", "[", "idx", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_lst", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.ToIterableDataset.__init__": [[123, 134], ["isinstance", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "sampler", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dataset (torch.utils.data.Dataset): an old-style dataset with ``__getitem__``\n            sampler (torch.utils.data.sampler.Sampler): a cheap iterable that produces indices\n                to be applied on ``dataset``.\n        \"\"\"", "\n", "assert", "not", "isinstance", "(", "dataset", ",", "data", ".", "IterableDataset", ")", ",", "dataset", "\n", "assert", "isinstance", "(", "sampler", ",", "Sampler", ")", ",", "sampler", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "sampler", "=", "sampler", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.ToIterableDataset.__iter__": [[135, 150], ["torch.get_worker_info", "itertools.islice"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "worker_info", "=", "data", ".", "get_worker_info", "(", ")", "\n", "if", "worker_info", "is", "None", "or", "worker_info", ".", "num_workers", "==", "1", ":", "\n", "            ", "for", "idx", "in", "self", ".", "sampler", ":", "\n", "                ", "yield", "self", ".", "dataset", "[", "idx", "]", "\n", "", "", "else", ":", "\n", "# With map-style dataset, `DataLoader(dataset, sampler)` runs the", "\n", "# sampler in main process only. But `DataLoader(ToIterableDataset(dataset, sampler))`", "\n", "# will run sampler in every of the N worker and only keep 1/N of the ids on each", "\n", "# worker. The assumption is that sampler is cheap to iterate and it's fine to discard", "\n", "# ids in workers.", "\n", "            ", "for", "idx", "in", "itertools", ".", "islice", "(", "\n", "self", ".", "sampler", ",", "worker_info", ".", "id", ",", "None", ",", "worker_info", ".", "num_workers", "\n", ")", ":", "\n", "                ", "yield", "self", ".", "dataset", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.AspectRatioGroupedDataset.__init__": [[165, 175], ["range"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dataset: an iterable. Each element must be a dict with keys\n                \"width\" and \"height\", which will be used to batch data.\n            batch_size (int):\n        \"\"\"", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "_buckets", "=", "[", "[", "]", "for", "_", "in", "range", "(", "2", ")", "]", "\n", "# Hard-coded two aspect ratio groups: w > h and w < h.", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.common.AspectRatioGroupedDataset.__iter__": [[178, 187], ["bucket.append", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "d", "in", "self", ".", "dataset", ":", "\n", "            ", "w", ",", "h", "=", "d", "[", "\"width\"", "]", ",", "d", "[", "\"height\"", "]", "\n", "bucket_id", "=", "0", "if", "w", ">", "h", "else", "1", "\n", "bucket", "=", "self", ".", "_buckets", "[", "bucket_id", "]", "\n", "bucket", ".", "append", "(", "d", ")", "\n", "if", "len", "(", "bucket", ")", "==", "self", ".", "batch_size", ":", "\n", "                ", "yield", "bucket", "[", ":", "]", "\n", "del", "bucket", "[", ":", "]", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.convert_PIL_to_numpy": [[60, 91], ["numpy.asarray", "np.dot.convert", "numpy.expand_dims", "numpy.dot", "numpy.array"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["def", "convert_PIL_to_numpy", "(", "image", ",", "format", ")", ":", "\n", "    ", "\"\"\"\n    Convert PIL image to numpy array of target format.\n\n    Args:\n        image (PIL.Image): a PIL image\n        format (str): the format of output image\n\n    Returns:\n        (np.ndarray): also see `read_image`\n    \"\"\"", "\n", "if", "format", "is", "not", "None", ":", "\n", "# PIL only supports RGB, so convert to RGB and flip channels over below", "\n", "        ", "conversion_format", "=", "format", "\n", "if", "format", "in", "[", "\"BGR\"", ",", "\"YUV-BT.601\"", "]", ":", "\n", "            ", "conversion_format", "=", "\"RGB\"", "\n", "", "image", "=", "image", ".", "convert", "(", "conversion_format", ")", "\n", "", "image", "=", "np", ".", "asarray", "(", "image", ")", "\n", "# PIL squeezes out the channel dimension for \"L\", so make it HWC", "\n", "if", "format", "==", "\"L\"", ":", "\n", "        ", "image", "=", "np", ".", "expand_dims", "(", "image", ",", "-", "1", ")", "\n", "\n", "# handle formats not supported by PIL", "\n", "", "elif", "format", "==", "\"BGR\"", ":", "\n", "# flip channels if needed", "\n", "        ", "image", "=", "image", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "", "elif", "format", "==", "\"YUV-BT.601\"", ":", "\n", "        ", "image", "=", "image", "/", "255.0", "\n", "image", "=", "np", ".", "dot", "(", "image", ",", "np", ".", "array", "(", "_M_RGB2YUV", ")", ".", "T", ")", "\n", "\n", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.convert_image_to_rgb": [[93, 117], ["isinstance", "np.asarray.cpu().numpy", "numpy.dot", "np.asarray.astype", "numpy.asarray", "np.asarray.cpu", "PIL.Image.fromarray().convert", "numpy.array", "PIL.Image.fromarray"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "def", "convert_image_to_rgb", "(", "image", ",", "format", ")", ":", "\n", "    ", "\"\"\"\n    Convert an image from given format to RGB.\n\n    Args:\n        image (np.ndarray or Tensor): an HWC image\n        format (str): the format of input image, also see `read_image`\n\n    Returns:\n        (np.ndarray): (H,W,3) RGB image in 0-255 range, can be either float or uint8\n    \"\"\"", "\n", "if", "isinstance", "(", "image", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "image", "=", "image", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "if", "format", "==", "\"BGR\"", ":", "\n", "        ", "image", "=", "image", "[", ":", ",", ":", ",", "[", "2", ",", "1", ",", "0", "]", "]", "\n", "", "elif", "format", "==", "\"YUV-BT.601\"", ":", "\n", "        ", "image", "=", "np", ".", "dot", "(", "image", ",", "np", ".", "array", "(", "_M_YUV2RGB", ")", ".", "T", ")", "\n", "image", "=", "image", "*", "255.0", "\n", "", "else", ":", "\n", "        ", "if", "format", "==", "\"L\"", ":", "\n", "            ", "image", "=", "image", "[", ":", ",", ":", ",", "0", "]", "\n", "", "image", "=", "image", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "image", "=", "np", ".", "asarray", "(", "Image", ".", "fromarray", "(", "image", ",", "mode", "=", "format", ")", ".", "convert", "(", "\"RGB\"", ")", ")", "\n", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils._apply_exif_orientation": [[119, 164], ["image.getexif.get", "hasattr", "image.getexif", "image.transpose"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "_apply_exif_orientation", "(", "image", ")", ":", "\n", "    ", "\"\"\"\n    Applies the exif orientation correctly.\n\n    This code exists per the bug:\n      https://github.com/python-pillow/Pillow/issues/3973\n    with the function `ImageOps.exif_transpose`. The Pillow source raises errors with\n    various methods, especially `tobytes`\n\n    Function based on:\n      https://github.com/wkentaro/labelme/blob/v4.5.4/labelme/utils/image.py#L59\n      https://github.com/python-pillow/Pillow/blob/7.1.2/src/PIL/ImageOps.py#L527\n\n    Args:\n        image (PIL.Image): a PIL image\n\n    Returns:\n        (PIL.Image): the PIL image with exif orientation applied, if applicable\n    \"\"\"", "\n", "if", "not", "hasattr", "(", "image", ",", "\"getexif\"", ")", ":", "\n", "        ", "return", "image", "\n", "\n", "", "try", ":", "\n", "        ", "exif", "=", "image", ".", "getexif", "(", ")", "\n", "", "except", "Exception", ":", "# https://github.com/facebookresearch/detectron2/issues/1885", "\n", "        ", "exif", "=", "None", "\n", "\n", "", "if", "exif", "is", "None", ":", "\n", "        ", "return", "image", "\n", "\n", "", "orientation", "=", "exif", ".", "get", "(", "_EXIF_ORIENT", ")", "\n", "\n", "method", "=", "{", "\n", "2", ":", "Image", ".", "FLIP_LEFT_RIGHT", ",", "\n", "3", ":", "Image", ".", "ROTATE_180", ",", "\n", "4", ":", "Image", ".", "FLIP_TOP_BOTTOM", ",", "\n", "5", ":", "Image", ".", "TRANSPOSE", ",", "\n", "6", ":", "Image", ".", "ROTATE_270", ",", "\n", "7", ":", "Image", ".", "TRANSVERSE", ",", "\n", "8", ":", "Image", ".", "ROTATE_90", ",", "\n", "}", ".", "get", "(", "orientation", ")", "\n", "\n", "if", "method", "is", "not", "None", ":", "\n", "        ", "return", "image", ".", "transpose", "(", "method", ")", "\n", "", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.read_image": [[166, 186], ["detectron2.utils.file_io.PathManager.open", "PIL.Image.open", "detection_utils._apply_exif_orientation", "detection_utils.convert_PIL_to_numpy"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils._apply_exif_orientation", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.convert_PIL_to_numpy"], ["", "def", "read_image", "(", "file_name", ",", "format", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Read an image into the given format.\n    Will apply rotation and flipping if the image has such exif information.\n\n    Args:\n        file_name (str): image file path\n        format (str): one of the supported image modes in PIL, or \"BGR\" or \"YUV-BT.601\".\n\n    Returns:\n        image (np.ndarray):\n            an HWC image in the given format, which is 0-255, uint8 for\n            supported image modes in PIL or \"BGR\"; float (0-1 for Y) for YUV-BT.601.\n    \"\"\"", "\n", "with", "PathManager", ".", "open", "(", "file_name", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "image", "=", "Image", ".", "open", "(", "f", ")", "\n", "\n", "# work around this bug: https://github.com/python-pillow/Pillow/issues/3973", "\n", "image", "=", "_apply_exif_orientation", "(", "image", ")", "\n", "return", "convert_PIL_to_numpy", "(", "image", ",", "format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.check_image_size": [[188, 212], ["detection_utils.SizeMismatchError"], "function", ["None"], ["", "", "def", "check_image_size", "(", "dataset_dict", ",", "image", ")", ":", "\n", "    ", "\"\"\"\n    Raise an error if the image does not match the size specified in the dict.\n    \"\"\"", "\n", "if", "\"width\"", "in", "dataset_dict", "or", "\"height\"", "in", "dataset_dict", ":", "\n", "        ", "image_wh", "=", "(", "image", ".", "shape", "[", "1", "]", ",", "image", ".", "shape", "[", "0", "]", ")", "\n", "expected_wh", "=", "(", "dataset_dict", "[", "\"width\"", "]", ",", "dataset_dict", "[", "\"height\"", "]", ")", "\n", "if", "not", "image_wh", "==", "expected_wh", ":", "\n", "            ", "raise", "SizeMismatchError", "(", "\n", "\"Mismatched image shape{}, got {}, expect {}.\"", ".", "format", "(", "\n", "\" for image \"", "+", "dataset_dict", "[", "\"file_name\"", "]", "\n", "if", "\"file_name\"", "in", "dataset_dict", "\n", "else", "\"\"", ",", "\n", "image_wh", ",", "\n", "expected_wh", ",", "\n", ")", "\n", "+", "\" Please check the width/height in your annotation.\"", "\n", ")", "\n", "\n", "# To ensure bbox always remap to original image size", "\n", "", "", "if", "\"width\"", "not", "in", "dataset_dict", ":", "\n", "        ", "dataset_dict", "[", "\"width\"", "]", "=", "image", ".", "shape", "[", "1", "]", "\n", "", "if", "\"height\"", "not", "in", "dataset_dict", ":", "\n", "        ", "dataset_dict", "[", "\"height\"", "]", "=", "image", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_proposals": [[214, 255], ["transforms.apply_box", "detectron2.structures.Boxes", "torch.as_tensor", "detectron2.structures.Boxes.clip", "detectron2.structures.Boxes.nonempty", "detectron2.structures.Instances", "detectron2.structures.BoxMode.convert", "dataset_dict.pop().astype", "dataset_dict.pop", "dataset_dict.pop", "dataset_dict.pop"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.nonempty", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "", "def", "transform_proposals", "(", "dataset_dict", ",", "image_shape", ",", "transforms", ",", "*", ",", "proposal_topk", ",", "min_box_size", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Apply transformations to the proposals in dataset_dict, if any.\n\n    Args:\n        dataset_dict (dict): a dict read from the dataset, possibly\n            contains fields \"proposal_boxes\", \"proposal_objectness_logits\", \"proposal_bbox_mode\"\n        image_shape (tuple): height, width\n        transforms (TransformList):\n        proposal_topk (int): only keep top-K scoring proposals\n        min_box_size (int): proposals with either side smaller than this\n            threshold are removed\n\n    The input dict is modified in-place, with abovementioned keys removed. A new\n    key \"proposals\" will be added. Its value is an `Instances`\n    object which contains the transformed proposals in its field\n    \"proposal_boxes\" and \"objectness_logits\".\n    \"\"\"", "\n", "if", "\"proposal_boxes\"", "in", "dataset_dict", ":", "\n", "# Transform proposal boxes", "\n", "        ", "boxes", "=", "transforms", ".", "apply_box", "(", "\n", "BoxMode", ".", "convert", "(", "\n", "dataset_dict", ".", "pop", "(", "\"proposal_boxes\"", ")", ",", "\n", "dataset_dict", ".", "pop", "(", "\"proposal_bbox_mode\"", ")", ",", "\n", "BoxMode", ".", "XYXY_ABS", ",", "\n", ")", "\n", ")", "\n", "boxes", "=", "Boxes", "(", "boxes", ")", "\n", "objectness_logits", "=", "torch", ".", "as_tensor", "(", "\n", "dataset_dict", ".", "pop", "(", "\"proposal_objectness_logits\"", ")", ".", "astype", "(", "\"float32\"", ")", "\n", ")", "\n", "\n", "boxes", ".", "clip", "(", "image_shape", ")", "\n", "keep", "=", "boxes", ".", "nonempty", "(", "threshold", "=", "min_box_size", ")", "\n", "boxes", "=", "boxes", "[", "keep", "]", "\n", "objectness_logits", "=", "objectness_logits", "[", "keep", "]", "\n", "\n", "proposals", "=", "Instances", "(", "image_shape", ")", "\n", "proposals", ".", "proposal_boxes", "=", "boxes", "[", ":", "proposal_topk", "]", "\n", "proposals", ".", "objectness_logits", "=", "objectness_logits", "[", ":", "proposal_topk", "]", "\n", "dataset_dict", "[", "\"proposals\"", "]", "=", "proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_instance_annotations": [[257, 329], ["isinstance", "detectron2.structures.BoxMode.convert", "[].clip", "numpy.minimum", "transforms.TransformList", "isinstance", "detection_utils.transform_keypoint_annotations", "list", "isinstance", "T.TransformList.apply_box", "numpy.asarray().reshape", "p.reshape", "pycocotools.decode", "T.TransformList.apply_segmentation", "ValueError", "numpy.array", "T.TransformList.apply_polygons", "tuple", "numpy.asarray", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_keypoint_annotations", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_segmentation"], ["", "", "def", "transform_instance_annotations", "(", "\n", "annotation", ",", "transforms", ",", "image_size", ",", "*", ",", "keypoint_hflip_indices", "=", "None", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Apply transforms to box, segmentation and keypoints annotations of a single instance.\n\n    It will use `transforms.apply_box` for the box, and\n    `transforms.apply_coords` for segmentation polygons & keypoints.\n    If you need anything more specially designed for each data structure,\n    you'll need to implement your own version of this function or the transforms.\n\n    Args:\n        annotation (dict): dict of instance annotations for a single instance.\n            It will be modified in-place.\n        transforms (TransformList or list[Transform]):\n        image_size (tuple): the height, width of the transformed image\n        keypoint_hflip_indices (ndarray[int]): see `create_keypoint_hflip_indices`.\n\n    Returns:\n        dict:\n            the same input dict with fields \"bbox\", \"segmentation\", \"keypoints\"\n            transformed according to `transforms`.\n            The \"bbox_mode\" field will be set to XYXY_ABS.\n    \"\"\"", "\n", "# if isinstance(transforms, (tuple, list)):", "\n", "#     transforms = T.TransformList(transforms)", "\n", "# # bbox is 1d (per-instance bounding box)", "\n", "# bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)", "\n", "# # clip transformed bbox to image size", "\n", "# #bbox = transforms.apply_box(np.array([bbox]))[0].clip(min=0)", "\n", "# annotation[\"bbox\"] = np.minimum(bbox, list(image_size + image_size)[::-1])", "\n", "# annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS", "\n", "# return annotation", "\n", "\n", "if", "isinstance", "(", "transforms", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "transforms", "=", "T", ".", "TransformList", "(", "transforms", ")", "\n", "# bbox is 1d (per-instance bounding box)", "\n", "", "bbox", "=", "BoxMode", ".", "convert", "(", "annotation", "[", "\"bbox\"", "]", ",", "annotation", "[", "\"bbox_mode\"", "]", ",", "BoxMode", ".", "XYXY_ABS", ")", "\n", "# clip transformed bbox to image size", "\n", "bbox", "=", "transforms", ".", "apply_box", "(", "np", ".", "array", "(", "[", "bbox", "]", ")", ")", "[", "0", "]", ".", "clip", "(", "min", "=", "0", ")", "\n", "annotation", "[", "\"bbox\"", "]", "=", "np", ".", "minimum", "(", "bbox", ",", "list", "(", "image_size", "+", "image_size", ")", "[", ":", ":", "-", "1", "]", ")", "\n", "annotation", "[", "\"bbox_mode\"", "]", "=", "BoxMode", ".", "XYXY_ABS", "\n", "\n", "if", "\"segmentation\"", "in", "annotation", ":", "\n", "# each instance contains 1 or more polygons", "\n", "        ", "segm", "=", "annotation", "[", "\"segmentation\"", "]", "\n", "if", "isinstance", "(", "segm", ",", "list", ")", ":", "\n", "# polygons", "\n", "            ", "polygons", "=", "[", "np", ".", "asarray", "(", "p", ")", ".", "reshape", "(", "-", "1", ",", "2", ")", "for", "p", "in", "segm", "]", "\n", "annotation", "[", "\"segmentation\"", "]", "=", "[", "\n", "p", ".", "reshape", "(", "-", "1", ")", "for", "p", "in", "transforms", ".", "apply_polygons", "(", "polygons", ")", "\n", "]", "\n", "", "elif", "isinstance", "(", "segm", ",", "dict", ")", ":", "\n", "# RLE", "\n", "            ", "mask", "=", "mask_util", ".", "decode", "(", "segm", ")", "\n", "mask", "=", "transforms", ".", "apply_segmentation", "(", "mask", ")", "\n", "assert", "tuple", "(", "mask", ".", "shape", "[", ":", "2", "]", ")", "==", "image_size", "\n", "annotation", "[", "\"segmentation\"", "]", "=", "mask", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Cannot transform segmentation of type '{}'!\"", "\n", "\"Supported types are: polygons as list[list[float] or ndarray],\"", "\n", "\" COCO-style RLE as a dict.\"", ".", "format", "(", "type", "(", "segm", ")", ")", "\n", ")", "\n", "\n", "", "", "if", "\"keypoints\"", "in", "annotation", ":", "\n", "        ", "keypoints", "=", "transform_keypoint_annotations", "(", "\n", "annotation", "[", "\"keypoints\"", "]", ",", "transforms", ",", "image_size", ",", "keypoint_hflip_indices", "\n", ")", "\n", "annotation", "[", "\"keypoints\"", "]", "=", "keypoints", "\n", "\n", "", "return", "annotation", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_keypoint_annotations": [[331, 371], ["numpy.asarray().reshape", "transforms.apply_coords", "inside.all.all", "numpy.asarray", "numpy.array", "numpy.array", "sum", "isinstance", "numpy.asarray"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords"], ["", "def", "transform_keypoint_annotations", "(", "keypoints", ",", "transforms", ",", "image_size", ",", "keypoint_hflip_indices", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Transform keypoint annotations of an image.\n    If a keypoint is transformed out of image boundary, it will be marked \"unlabeled\" (visibility=0)\n\n    Args:\n        keypoints (list[float]): Nx3 float in Detectron2's Dataset format.\n            Each point is represented by (x, y, visibility).\n        transforms (TransformList):\n        image_size (tuple): the height, width of the transformed image\n        keypoint_hflip_indices (ndarray[int]): see `create_keypoint_hflip_indices`.\n            When `transforms` includes horizontal flip, will use the index\n            mapping to flip keypoints.\n    \"\"\"", "\n", "# (N*3,) -> (N, 3)", "\n", "keypoints", "=", "np", ".", "asarray", "(", "keypoints", ",", "dtype", "=", "\"float64\"", ")", ".", "reshape", "(", "-", "1", ",", "3", ")", "\n", "keypoints_xy", "=", "transforms", ".", "apply_coords", "(", "keypoints", "[", ":", ",", ":", "2", "]", ")", "\n", "\n", "# Set all out-of-boundary points to \"unlabeled\"", "\n", "inside", "=", "(", "keypoints_xy", ">=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", ")", "&", "(", "keypoints_xy", "<=", "np", ".", "array", "(", "image_size", "[", ":", ":", "-", "1", "]", ")", ")", "\n", "inside", "=", "inside", ".", "all", "(", "axis", "=", "1", ")", "\n", "keypoints", "[", ":", ",", ":", "2", "]", "=", "keypoints_xy", "\n", "keypoints", "[", ":", ",", "2", "]", "[", "~", "inside", "]", "=", "0", "\n", "\n", "# This assumes that HorizFlipTransform is the only one that does flip", "\n", "do_hflip", "=", "sum", "(", "isinstance", "(", "t", ",", "T", ".", "HFlipTransform", ")", "for", "t", "in", "transforms", ".", "transforms", ")", "%", "2", "==", "1", "\n", "\n", "# Alternative way: check if probe points was horizontally flipped.", "\n", "# probe = np.asarray([[0.0, 0.0], [image_width, 0.0]])", "\n", "# probe_aug = transforms.apply_coords(probe.copy())", "\n", "# do_hflip = np.sign(probe[1][0] - probe[0][0]) != np.sign(probe_aug[1][0] - probe_aug[0][0])  # noqa", "\n", "\n", "# If flipped, swap each keypoint with its opposite-handed equivalent", "\n", "if", "do_hflip", ":", "\n", "        ", "assert", "keypoint_hflip_indices", "is", "not", "None", "\n", "keypoints", "=", "keypoints", "[", "np", ".", "asarray", "(", "keypoint_hflip_indices", ",", "dtype", "=", "np", ".", "int32", ")", ",", ":", "]", "\n", "\n", "# Maintain COCO convention that if visibility == 0 (unlabeled), then x, y = 0", "\n", "", "keypoints", "[", "keypoints", "[", ":", ",", "2", "]", "==", "0", "]", "=", "0", "\n", "return", "keypoints", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.annotations_to_instances": [[373, 440], ["detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.tensor", "detectron2.structures.BoxMode.convert", "int", "len", "len", "detectron2.structures.Keypoints", "detectron2.structures.BitMasks", "obj.get", "detectron2.structures.PolygonMasks", "isinstance", "torch.stack", "ValueError", "detectron2.structures.PolygonMasks.append", "isinstance", "detectron2.structures.polygons_to_bitmask", "detectron2.structures.PolygonMasks.append", "isinstance", "torch.from_numpy", "pycocotools.decode", "detectron2.structures.PolygonMasks.append", "ValueError", "numpy.ascontiguousarray", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.polygons_to_bitmask", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode"], ["", "def", "annotations_to_instances", "(", "annos", ",", "image_size", ",", "mask_format", "=", "\"polygon\"", ")", ":", "\n", "    ", "\"\"\"\n    Create an :class:`Instances` object used by the models,\n    from instance annotations in the dataset dict.\n\n    Args:\n        annos (list[dict]): a list of instance annotations in one image, each\n            element for one instance.\n        image_size (tuple): height, width\n\n    Returns:\n        Instances:\n            It will contain fields \"gt_boxes\", \"gt_classes\",\n            \"gt_masks\", \"gt_keypoints\", if they can be obtained from `annos`.\n            This is the format that builtin models expect.\n    \"\"\"", "\n", "boxes", "=", "[", "BoxMode", ".", "convert", "(", "obj", "[", "\"bbox\"", "]", ",", "obj", "[", "\"bbox_mode\"", "]", ",", "BoxMode", ".", "XYXY_ABS", ")", "for", "obj", "in", "annos", "]", "\n", "target", "=", "Instances", "(", "image_size", ")", "\n", "target", ".", "gt_boxes", "=", "Boxes", "(", "boxes", ")", "\n", "\n", "classes", "=", "[", "int", "(", "obj", "[", "\"category_id\"", "]", ")", "for", "obj", "in", "annos", "]", "\n", "classes", "=", "torch", ".", "tensor", "(", "classes", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "target", ".", "gt_classes", "=", "classes", "\n", "\n", "if", "len", "(", "annos", ")", "and", "\"segmentation\"", "in", "annos", "[", "0", "]", ":", "\n", "        ", "segms", "=", "[", "obj", "[", "\"segmentation\"", "]", "for", "obj", "in", "annos", "]", "\n", "if", "mask_format", "==", "\"polygon\"", ":", "\n", "            ", "try", ":", "\n", "                ", "masks", "=", "PolygonMasks", "(", "segms", ")", "\n", "", "except", "ValueError", "as", "e", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Failed to use mask_format=='polygon' from the given annotations!\"", "\n", ")", "from", "e", "\n", "", "", "else", ":", "\n", "            ", "assert", "mask_format", "==", "\"bitmask\"", ",", "mask_format", "\n", "masks", "=", "[", "]", "\n", "for", "segm", "in", "segms", ":", "\n", "                ", "if", "isinstance", "(", "segm", ",", "list", ")", ":", "\n", "# polygon", "\n", "                    ", "masks", ".", "append", "(", "polygons_to_bitmask", "(", "segm", ",", "*", "image_size", ")", ")", "\n", "", "elif", "isinstance", "(", "segm", ",", "dict", ")", ":", "\n", "# COCO RLE", "\n", "                    ", "masks", ".", "append", "(", "mask_util", ".", "decode", "(", "segm", ")", ")", "\n", "", "elif", "isinstance", "(", "segm", ",", "np", ".", "ndarray", ")", ":", "\n", "                    ", "assert", "segm", ".", "ndim", "==", "2", ",", "\"Expect segmentation of 2 dimensions, got {}.\"", ".", "format", "(", "\n", "segm", ".", "ndim", "\n", ")", "\n", "# mask array", "\n", "masks", ".", "append", "(", "segm", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Cannot convert segmentation of type '{}' to BitMasks!\"", "\n", "\"Supported types are: polygons as list[list[float] or ndarray],\"", "\n", "\" COCO-style RLE as a dict, or a binary segmentation mask \"", "\n", "\" in a 2D numpy array of shape HxW.\"", ".", "format", "(", "type", "(", "segm", ")", ")", "\n", ")", "\n", "# torch.from_numpy does not support array with negative stride.", "\n", "", "", "masks", "=", "BitMasks", "(", "\n", "torch", ".", "stack", "(", "[", "torch", ".", "from_numpy", "(", "np", ".", "ascontiguousarray", "(", "x", ")", ")", "for", "x", "in", "masks", "]", ")", "\n", ")", "\n", "", "target", ".", "gt_masks", "=", "masks", "\n", "\n", "", "if", "len", "(", "annos", ")", "and", "\"keypoints\"", "in", "annos", "[", "0", "]", ":", "\n", "        ", "kpts", "=", "[", "obj", ".", "get", "(", "\"keypoints\"", ",", "[", "]", ")", "for", "obj", "in", "annos", "]", "\n", "target", ".", "gt_keypoints", "=", "Keypoints", "(", "kpts", ")", "\n", "\n", "", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.annotations_to_instances_rotated": [[442, 469], ["detectron2.structures.Instances", "detectron2.structures.RotatedBoxes", "detectron2.structures.RotatedBoxes.clip", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clip"], ["", "def", "annotations_to_instances_rotated", "(", "annos", ",", "image_size", ")", ":", "\n", "    ", "\"\"\"\n    Create an :class:`Instances` object used by the models,\n    from instance annotations in the dataset dict.\n    Compared to `annotations_to_instances`, this function is for rotated boxes only\n\n    Args:\n        annos (list[dict]): a list of instance annotations in one image, each\n            element for one instance.\n        image_size (tuple): height, width\n\n    Returns:\n        Instances:\n            Containing fields \"gt_boxes\", \"gt_classes\",\n            if they can be obtained from `annos`.\n            This is the format that builtin models expect.\n    \"\"\"", "\n", "boxes", "=", "[", "obj", "[", "\"bbox\"", "]", "for", "obj", "in", "annos", "]", "\n", "target", "=", "Instances", "(", "image_size", ")", "\n", "boxes", "=", "target", ".", "gt_boxes", "=", "RotatedBoxes", "(", "boxes", ")", "\n", "boxes", ".", "clip", "(", "image_size", ")", "\n", "\n", "classes", "=", "[", "obj", "[", "\"category_id\"", "]", "for", "obj", "in", "annos", "]", "\n", "classes", "=", "torch", ".", "tensor", "(", "classes", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "target", ".", "gt_classes", "=", "classes", "\n", "\n", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.filter_empty_instances": [[471, 499], ["r.append", "instances.has", "r.append", "instances.gt_boxes.nonempty", "instances.gt_masks.nonempty"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.nonempty", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.nonempty"], ["", "def", "filter_empty_instances", "(", "instances", ",", "by_box", "=", "True", ",", "by_mask", "=", "True", ",", "box_threshold", "=", "1e-5", ")", ":", "\n", "    ", "\"\"\"\n    Filter out empty instances in an `Instances` object.\n\n    Args:\n        instances (Instances):\n        by_box (bool): whether to filter out instances with empty boxes\n        by_mask (bool): whether to filter out instances with empty masks\n        box_threshold (float): minimum width and height to be considered non-empty\n\n    Returns:\n        Instances: the filtered instances.\n    \"\"\"", "\n", "assert", "by_box", "or", "by_mask", "\n", "r", "=", "[", "]", "\n", "if", "by_box", ":", "\n", "        ", "r", ".", "append", "(", "instances", ".", "gt_boxes", ".", "nonempty", "(", "threshold", "=", "box_threshold", ")", ")", "\n", "", "if", "instances", ".", "has", "(", "\"gt_masks\"", ")", "and", "by_mask", ":", "\n", "        ", "r", ".", "append", "(", "instances", ".", "gt_masks", ".", "nonempty", "(", ")", ")", "\n", "\n", "# TODO: can also filter visible keypoints", "\n", "\n", "", "if", "not", "r", ":", "\n", "        ", "return", "instances", "\n", "", "m", "=", "r", "[", "0", "]", "\n", "for", "x", "in", "r", "[", "1", ":", "]", ":", "\n", "        ", "m", "=", "m", "&", "x", "\n", "", "return", "instances", "[", "m", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.create_keypoint_hflip_indices": [[501, 524], ["isinstance", "detection_utils.check_metadata_consistency", "detection_utils.check_metadata_consistency", "catalog.MetadataCatalog.get", "dict", "dict.update", "names.index", "dict.items"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.check_metadata_consistency", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.check_metadata_consistency", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "create_keypoint_hflip_indices", "(", "dataset_names", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\"\n    Args:\n        dataset_names: list of dataset names\n\n    Returns:\n        list[int]: a list of size=#keypoints, storing the\n        horizontally-flipped keypoint indices.\n    \"\"\"", "\n", "if", "isinstance", "(", "dataset_names", ",", "str", ")", ":", "\n", "        ", "dataset_names", "=", "[", "dataset_names", "]", "\n", "\n", "", "check_metadata_consistency", "(", "\"keypoint_names\"", ",", "dataset_names", ")", "\n", "check_metadata_consistency", "(", "\"keypoint_flip_map\"", ",", "dataset_names", ")", "\n", "\n", "meta", "=", "MetadataCatalog", ".", "get", "(", "dataset_names", "[", "0", "]", ")", "\n", "names", "=", "meta", ".", "keypoint_names", "\n", "# TODO flip -> hflip", "\n", "flip_map", "=", "dict", "(", "meta", ".", "keypoint_flip_map", ")", "\n", "flip_map", ".", "update", "(", "{", "v", ":", "k", "for", "k", ",", "v", "in", "flip_map", ".", "items", "(", ")", "}", ")", "\n", "flipped_names", "=", "[", "i", "if", "i", "not", "in", "flip_map", "else", "flip_map", "[", "i", "]", "for", "i", "in", "names", "]", "\n", "flip_indices", "=", "[", "names", ".", "index", "(", "i", ")", "for", "i", "in", "flipped_names", "]", "\n", "return", "flip_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.gen_crop_transform_with_instance": [[526, 554], ["numpy.asarray", "detectron2.structures.BoxMode.convert", "numpy.maximum", "numpy.maximum", "numpy.minimum", "numpy.random.randint", "numpy.random.randint", "transforms.CropTransform", "numpy.ceil().astype", "numpy.floor().astype", "numpy.asarray", "numpy.ceil", "numpy.floor"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "def", "gen_crop_transform_with_instance", "(", "crop_size", ",", "image_size", ",", "instance", ")", ":", "\n", "    ", "\"\"\"\n    Generate a CropTransform so that the cropping region contains\n    the center of the given instance.\n\n    Args:\n        crop_size (tuple): h, w in pixels\n        image_size (tuple): h, w\n        instance (dict): an annotation dict of one instance, in Detectron2's\n            dataset format.\n    \"\"\"", "\n", "crop_size", "=", "np", ".", "asarray", "(", "crop_size", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "bbox", "=", "BoxMode", ".", "convert", "(", "instance", "[", "\"bbox\"", "]", ",", "instance", "[", "\"bbox_mode\"", "]", ",", "BoxMode", ".", "XYXY_ABS", ")", "\n", "center_yx", "=", "(", "bbox", "[", "1", "]", "+", "bbox", "[", "3", "]", ")", "*", "0.5", ",", "(", "bbox", "[", "0", "]", "+", "bbox", "[", "2", "]", ")", "*", "0.5", "\n", "assert", "(", "\n", "image_size", "[", "0", "]", ">=", "center_yx", "[", "0", "]", "and", "image_size", "[", "1", "]", ">=", "center_yx", "[", "1", "]", "\n", ")", ",", "\"The annotation bounding box is outside of the image!\"", "\n", "assert", "(", "\n", "image_size", "[", "0", "]", ">=", "crop_size", "[", "0", "]", "and", "image_size", "[", "1", "]", ">=", "crop_size", "[", "1", "]", "\n", ")", ",", "\"Crop size is larger than image size!\"", "\n", "\n", "min_yx", "=", "np", ".", "maximum", "(", "np", ".", "floor", "(", "center_yx", ")", ".", "astype", "(", "np", ".", "int32", ")", "-", "crop_size", ",", "0", ")", "\n", "max_yx", "=", "np", ".", "maximum", "(", "np", ".", "asarray", "(", "image_size", ",", "dtype", "=", "np", ".", "int32", ")", "-", "crop_size", ",", "0", ")", "\n", "max_yx", "=", "np", ".", "minimum", "(", "max_yx", ",", "np", ".", "ceil", "(", "center_yx", ")", ".", "astype", "(", "np", ".", "int32", ")", ")", "\n", "\n", "y0", "=", "np", ".", "random", ".", "randint", "(", "min_yx", "[", "0", "]", ",", "max_yx", "[", "0", "]", "+", "1", ")", "\n", "x0", "=", "np", ".", "random", ".", "randint", "(", "min_yx", "[", "1", "]", ",", "max_yx", "[", "1", "]", "+", "1", ")", "\n", "return", "T", ".", "CropTransform", "(", "x0", ",", "y0", ",", "crop_size", "[", "1", "]", ",", "crop_size", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.check_metadata_consistency": [[556, 583], ["logging.getLogger", "enumerate", "len", "getattr", "catalog.MetadataCatalog.get", "logging.getLogger.error", "logging.getLogger.error", "ValueError", "str", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "check_metadata_consistency", "(", "key", ",", "dataset_names", ")", ":", "\n", "    ", "\"\"\"\n    Check that the datasets have consistent metadata.\n\n    Args:\n        key (str): a metadata key\n        dataset_names (list[str]): a list of dataset names\n\n    Raises:\n        AttributeError: if the key does not exist in the metadata\n        ValueError: if the given datasets do not have the same metadata values defined by key\n    \"\"\"", "\n", "if", "len", "(", "dataset_names", ")", "==", "0", ":", "\n", "        ", "return", "\n", "", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "entries_per_dataset", "=", "[", "getattr", "(", "MetadataCatalog", ".", "get", "(", "d", ")", ",", "key", ")", "for", "d", "in", "dataset_names", "]", "\n", "for", "idx", ",", "entry", "in", "enumerate", "(", "entries_per_dataset", ")", ":", "\n", "        ", "if", "entry", "!=", "entries_per_dataset", "[", "0", "]", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Metadata '{}' for dataset '{}' is '{}'\"", ".", "format", "(", "key", ",", "dataset_names", "[", "idx", "]", ",", "str", "(", "entry", ")", ")", "\n", ")", "\n", "logger", ".", "error", "(", "\n", "\"Metadata '{}' for dataset '{}' is '{}'\"", ".", "format", "(", "\n", "key", ",", "dataset_names", "[", "0", "]", ",", "str", "(", "entries_per_dataset", "[", "0", "]", ")", "\n", ")", "\n", ")", "\n", "raise", "ValueError", "(", "\"Datasets have different metadata '{}'!\"", ".", "format", "(", "key", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.build_augmentation": [[585, 610], ["transforms.ResizeShortestEdge", "augmentation.append", "transforms.RandomFlip"], "function", ["None"], ["", "", "", "def", "build_augmentation", "(", "cfg", ",", "is_train", ")", ":", "\n", "    ", "\"\"\"\n    Create a list of default :class:`Augmentation` from config.\n    Now it includes resizing and flipping.\n\n    Returns:\n        list[Augmentation]\n    \"\"\"", "\n", "if", "is_train", ":", "\n", "        ", "min_size", "=", "cfg", ".", "INPUT", ".", "MIN_SIZE_TRAIN", "\n", "max_size", "=", "cfg", ".", "INPUT", ".", "MAX_SIZE_TRAIN", "\n", "sample_style", "=", "cfg", ".", "INPUT", ".", "MIN_SIZE_TRAIN_SAMPLING", "\n", "", "else", ":", "\n", "        ", "min_size", "=", "cfg", ".", "INPUT", ".", "MIN_SIZE_TEST", "\n", "max_size", "=", "cfg", ".", "INPUT", ".", "MAX_SIZE_TEST", "\n", "sample_style", "=", "\"choice\"", "\n", "", "augmentation", "=", "[", "T", ".", "ResizeShortestEdge", "(", "min_size", ",", "max_size", ",", "sample_style", ")", "]", "\n", "if", "is_train", "and", "cfg", ".", "INPUT", ".", "RANDOM_FLIP", "!=", "\"none\"", ":", "\n", "        ", "augmentation", ".", "append", "(", "\n", "T", ".", "RandomFlip", "(", "\n", "horizontal", "=", "cfg", ".", "INPUT", ".", "RANDOM_FLIP", "==", "\"horizontal\"", ",", "\n", "vertical", "=", "cfg", ".", "INPUT", ".", "RANDOM_FLIP", "==", "\"vertical\"", ",", "\n", ")", "\n", ")", "\n", "", "return", "augmentation", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register": [[29, 39], ["callable"], "methods", ["None"], ["\"FAIR/X-101-64x4d\"", ":", "\"ImageNetPretrained/FBResNeXt/X-101-64x4d.pkl\"", ",", "\n", "\"FAIR/X-152-32x8d-IN5k\"", ":", "\"ImageNetPretrained/25093814/X-152-32x8d-IN5k.pkl\"", ",", "\n", "}", "\n", "\n", "C2_DETECTRON_PATH_FORMAT", "=", "(", "\n", "\"{prefix}/{url}/output/train/{dataset}/{type}/model_final.pkl\"", "# noqa B950", "\n", ")", "\n", "\n", "C2_DATASET_COCO", "=", "\"coco_2014_train%3Acoco_2014_valminusminival\"", "\n", "C2_DATASET_COCO_KEYPOINTS", "=", "\"keypoints_coco_2014_train%3Akeypoints_coco_2014_valminusminival\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.get": [[40, 59], ["f", "KeyError", "catalog._DatasetCatalog.list"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["# format: {model_name} -> part of the url", "\n", "C2_DETECTRON_MODELS", "=", "{", "\n", "\"35857197/e2e_faster_rcnn_R-50-C4_1x\"", ":", "\"35857197/12_2017_baselines/e2e_faster_rcnn_R-50-C4_1x.yaml.01_33_49.iAX0mXvW\"", ",", "# noqa B950", "\n", "\"35857345/e2e_faster_rcnn_R-50-FPN_1x\"", ":", "\"35857345/12_2017_baselines/e2e_faster_rcnn_R-50-FPN_1x.yaml.01_36_30.cUF7QR7I\"", ",", "# noqa B950", "\n", "\"35857890/e2e_faster_rcnn_R-101-FPN_1x\"", ":", "\"35857890/12_2017_baselines/e2e_faster_rcnn_R-101-FPN_1x.yaml.01_38_50.sNxI7sX7\"", ",", "# noqa B950", "\n", "\"36761737/e2e_faster_rcnn_X-101-32x8d-FPN_1x\"", ":", "\"36761737/12_2017_baselines/e2e_faster_rcnn_X-101-32x8d-FPN_1x.yaml.06_31_39.5MIHi1fZ\"", ",", "# noqa B950", "\n", "\"35858791/e2e_mask_rcnn_R-50-C4_1x\"", ":", "\"35858791/12_2017_baselines/e2e_mask_rcnn_R-50-C4_1x.yaml.01_45_57.ZgkA7hPB\"", ",", "# noqa B950", "\n", "\"35858933/e2e_mask_rcnn_R-50-FPN_1x\"", ":", "\"35858933/12_2017_baselines/e2e_mask_rcnn_R-50-FPN_1x.yaml.01_48_14.DzEQe4wC\"", ",", "# noqa B950", "\n", "\"35861795/e2e_mask_rcnn_R-101-FPN_1x\"", ":", "\"35861795/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_1x.yaml.02_31_37.KqyEK4tT\"", ",", "# noqa B950", "\n", "\"36761843/e2e_mask_rcnn_X-101-32x8d-FPN_1x\"", ":", "\"36761843/12_2017_baselines/e2e_mask_rcnn_X-101-32x8d-FPN_1x.yaml.06_35_59.RZotkLKI\"", ",", "# noqa B950", "\n", "\"48616381/e2e_mask_rcnn_R-50-FPN_2x_gn\"", ":", "\"GN/48616381/04_2018_gn_baselines/e2e_mask_rcnn_R-50-FPN_2x_gn_0416.13_23_38.bTlTI97Q\"", ",", "# noqa B950", "\n", "\"37697547/e2e_keypoint_rcnn_R-50-FPN_1x\"", ":", "\"37697547/12_2017_baselines/e2e_keypoint_rcnn_R-50-FPN_1x.yaml.08_42_54.kdzV35ao\"", ",", "# noqa B950", "\n", "\"35998355/rpn_R-50-C4_1x\"", ":", "\"35998355/12_2017_baselines/rpn_R-50-C4_1x.yaml.08_00_43.njH5oD9L\"", ",", "# noqa B950", "\n", "\"35998814/rpn_R-50-FPN_1x\"", ":", "\"35998814/12_2017_baselines/rpn_R-50-FPN_1x.yaml.08_06_03.Axg0r179\"", ",", "# noqa B950", "\n", "\"36225147/fast_R-50-FPN_1x\"", ":", "\"36225147/12_2017_baselines/fast_rcnn_R-50-FPN_1x.yaml.08_39_09.L3obSdQ2\"", ",", "# noqa B950", "\n", "}", "\n", "\n", "@", "staticmethod", "\n", "def", "get", "(", "name", ")", ":", "\n", "        ", "if", "name", ".", "startswith", "(", "\"Caffe2Detectron/COCO\"", ")", ":", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.list": [[60, 68], ["catalog._DatasetCatalog.list"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["            ", "return", "ModelCatalog", ".", "_get_c2_detectron_baseline", "(", "name", ")", "\n", "", "if", "name", ".", "startswith", "(", "\"ImageNetPretrained/\"", ")", ":", "\n", "            ", "return", "ModelCatalog", ".", "_get_c2_imagenet_pretrained", "(", "name", ")", "\n", "", "raise", "RuntimeError", "(", "\"model not present in the catalog: {}\"", ".", "format", "(", "name", ")", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "_get_c2_imagenet_pretrained", "(", "name", ")", ":", "\n", "        ", "prefix", "=", "ModelCatalog", ".", "S3_C2_DETECTRON_PREFIX", "\n", "name", "=", "name", "[", "len", "(", "\"ImageNetPretrained/\"", ")", ":", "]", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.remove": [[69, 74], ["catalog._DatasetCatalog.pop"], "methods", ["None"], ["name", "=", "ModelCatalog", ".", "C2_IMAGENET_MODELS", "[", "name", "]", "\n", "url", "=", "\"/\"", ".", "join", "(", "[", "prefix", ",", "name", "]", ")", "\n", "return", "url", "\n", "\n", "", "@", "staticmethod", "\n", "def", "_get_c2_detectron_baseline", "(", "name", ")", ":", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.__str__": [[75, 77], ["catalog._DatasetCatalog.keys"], "methods", ["None"], ["        ", "name", "=", "name", "[", "len", "(", "\"Caffe2Detectron/COCO/\"", ")", ":", "]", "\n", "url", "=", "ModelCatalog", ".", "C2_DETECTRON_MODELS", "[", "name", "]", "\n", "if", "\"keypoint_rcnn\"", "in", "name", ":", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.__getattr__": [[115, 133], ["detectron2.utils.logger.log_first_n", "getattr", "len", "AttributeError", "AttributeError", "str", "catalog.Metadata.__dict__.keys"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.log_first_n"], ["", "", "PathManager", ".", "register_handler", "(", "ModelCatalogHandler", "(", ")", ")", "\n", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.__setattr__": [[136, 154], ["detectron2.utils.logger.log_first_n", "setattr", "getattr", "types.SimpleNamespace.__setattr__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.log_first_n", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.__setattr__"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.as_dict": [[155, 161], ["copy.copy"], "methods", ["None"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set": [[162, 169], ["kwargs.items", "setattr"], "methods", ["None"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.get": [[170, 179], ["getattr"], "methods", ["None"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get": [[194, 208], ["len", "super().get", "catalog.Metadata"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list": [[209, 217], ["catalog._MetadataCatalog.list"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.remove": [[218, 223], ["catalog._MetadataCatalog.pop"], "methods", ["None"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.__str__": [[224, 226], ["catalog._MetadataCatalog.keys"], "methods", ["None"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.filter_images_with_only_crowd_annotations": [[40, 69], ["len", "len", "logging.getLogger", "logging.getLogger.info", "build.filter_images_with_only_crowd_annotations.valid"], "function", ["None"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.filter_images_with_few_keypoints": [[71, 103], ["len", "len", "logging.getLogger", "logging.getLogger.info", "sum", "build.filter_images_with_few_keypoints.visible_keypoints_in_image"], "function", ["None"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.load_proposals_into_dataset": [[105, 157], ["logging.getLogger", "logging.getLogger.info", "set", "detectron2.utils.file_io.PathManager.open", "pickle.load", "str", "detectron2.structures.BoxMode", "pickle.load.pop", "str", "enumerate", "objectness_logits.argsort", "str", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.print_instances_class_histogram": [[159, 208], ["len", "numpy.arange", "numpy.zeros", "min", "list", "sum", "itertools.zip_longest.extend", "itertools.zip_longest", "tabulate.tabulate", "detectron2.utils.logger.log_first_n", "numpy.asarray", "len", "itertools.chain", "itertools.zip_longest.extend", "numpy.histogram", "len", "len", "termcolor.colored", "np.asarray.min", "np.asarray.max", "np.asarray.min", "np.asarray.max", "len", "range", "x.get", "build.print_instances_class_histogram.short_name"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.log_first_n", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.get_detection_dataset_dicts": [[211, 259], ["isinstance", "len", "zip", "list", "len", "catalog.DatasetCatalog.get", "len", "itertools.chain.from_iterable", "build.filter_images_with_only_crowd_annotations", "build.filter_images_with_few_keypoints", "len", "len", "build.load_proposals_into_dataset", "detection_utils.check_metadata_consistency", "build.print_instances_class_histogram", "zip", "catalog.MetadataCatalog.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.filter_images_with_only_crowd_annotations", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.filter_images_with_few_keypoints", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.load_proposals_into_dataset", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.check_metadata_consistency", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.print_instances_class_histogram", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_batch_data_loader": [[261, 307], ["detectron2.utils.comm.get_world_size", "torch.utils.data.DataLoader", "common.AspectRatioGroupedDataset", "torch.utils.data.sampler.BatchSampler", "torch.utils.data.DataLoader", "operator.itemgetter"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build._train_loader_from_config": [[310, 376], ["logging.getLogger", "logging.getLogger.info", "clip_build.make_clip_dataset", "build.get_detection_dataset_dicts", "detectron2.utils.logger._log_api_usage", "dataset_mapper.DatasetMapper", "logging.getLogger", "logging.getLogger.info", "logging.getLogger", "logging.getLogger.info", "samplers.TrainingSampler", "samplers.TrainingSampler", "len", "samplers.RepeatFactorTrainingSampler.repeat_factors_from_category_frequency", "samplers.RepeatFactorTrainingSampler", "ValueError", "len", "samplers.RepeatFactorTrainingSampler.repeat_factors_from_category_frequency", "samplers.RepeatFactorTrainingSampler", "ValueError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.clip_build.make_clip_dataset", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.get_detection_dataset_dicts", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger._log_api_usage", "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler.repeat_factors_from_category_frequency", "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler.repeat_factors_from_category_frequency"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_train_loader": [[380, 424], ["detectron2.config.configurable", "isinstance", "isinstance", "build.build_batch_data_loader", "common.DatasetFromList", "common.MapDataset", "samplers.TrainingSampler", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.configurable", "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_batch_data_loader"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build._test_loader_from_config": [[427, 462], ["build.get_detection_dataset_dicts", "logging.getLogger", "logging.getLogger.info", "clip_build.make_clip_dataset", "dataset_mapper.DatasetMapper", "list().index", "list"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.build.get_detection_dataset_dicts", "home.repos.pwc.inspect_result.microsoft_regionclip.data.clip_build.make_clip_dataset", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.build_detection_test_loader": [[464, 525], ["detectron2.config.configurable", "isinstance", "torch.utils.data.sampler.BatchSampler", "torch.utils.data.DataLoader", "common.DatasetFromList", "common.MapDataset", "samplers.InferenceSampler", "detectron2.utils.comm.get_world_size", "torch.utils.data.sampler.BatchSampler", "torch.utils.data.DataLoader", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.configurable", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.trivial_batch_collator": [[527, 532], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.build.worker_init_reset_seed": [[534, 537], ["detectron2.utils.env.seed_all_rng", "torch.initial_seed"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.seed_all_rng"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.data.dataset_mapper.DatasetMapper.__init__": [[37, 90], ["transforms.AugmentationList", "logging.getLogger", "logging.getLogger.info"], "methods", ["None"], ["@", "configurable", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "is_train", ":", "bool", ",", "\n", "*", ",", "\n", "augmentations", ":", "List", "[", "Union", "[", "T", ".", "Augmentation", ",", "T", ".", "Transform", "]", "]", ",", "\n", "image_format", ":", "str", ",", "\n", "use_instance_mask", ":", "bool", "=", "False", ",", "\n", "use_keypoint", ":", "bool", "=", "False", ",", "\n", "instance_mask_format", ":", "str", "=", "\"polygon\"", ",", "\n", "keypoint_hflip_indices", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "precomputed_proposal_topk", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "recompute_boxes", ":", "bool", "=", "False", ",", "\n", "filter_open_cls", ":", "bool", "=", "False", ",", "\n", "clip_crop", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: this interface is experimental.\n\n        Args:\n            is_train: whether it's used in training or inference\n            augmentations: a list of augmentations or deterministic transforms to apply\n            image_format: an image format supported by :func:`detection_utils.read_image`.\n            use_instance_mask: whether to process instance segmentation annotations, if available\n            use_keypoint: whether to process keypoint annotations if available\n            instance_mask_format: one of \"polygon\" or \"bitmask\". Process instance segmentation\n                masks into this format.\n            keypoint_hflip_indices: see :func:`detection_utils.create_keypoint_hflip_indices`\n            precomputed_proposal_topk: if given, will load pre-computed\n                proposals from dataset_dict and keep the top k proposals for each image.\n            recompute_boxes: whether to overwrite bounding box annotations\n                by computing tight bounding boxes from instance mask annotations.\n            filter_open_cls: open-set setting, filter the open-set categories during training\n            clip_crop: the mode that directly use CLIP on cropped image regions\n        \"\"\"", "\n", "if", "recompute_boxes", ":", "\n", "            ", "assert", "use_instance_mask", ",", "\"recompute_boxes requires instance masks\"", "\n", "# fmt: off", "\n", "", "self", ".", "is_train", "=", "is_train", "\n", "self", ".", "augmentations", "=", "T", ".", "AugmentationList", "(", "augmentations", ")", "\n", "self", ".", "image_format", "=", "image_format", "\n", "self", ".", "use_instance_mask", "=", "use_instance_mask", "\n", "self", ".", "instance_mask_format", "=", "instance_mask_format", "\n", "self", ".", "use_keypoint", "=", "use_keypoint", "\n", "self", ".", "keypoint_hflip_indices", "=", "keypoint_hflip_indices", "\n", "self", ".", "proposal_topk", "=", "precomputed_proposal_topk", "\n", "self", ".", "recompute_boxes", "=", "recompute_boxes", "\n", "self", ".", "filter_open_cls", "=", "filter_open_cls", "\n", "self", ".", "clip_crop", "=", "clip_crop", "\n", "# fmt: on", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "mode", "=", "\"training\"", "if", "is_train", "else", "\"inference\"", "\n", "logger", ".", "info", "(", "f\"[DatasetMapper] Augmentations used in {mode}: {augmentations}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.dataset_mapper.DatasetMapper.from_config": [[91, 123], ["detection_utils.build_augmentation", "detection_utils.build_augmentation.insert", "detection_utils.create_keypoint_hflip_indices", "transforms.RandomCrop"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.build_augmentation", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.create_keypoint_hflip_indices"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ",", "is_train", ":", "bool", "=", "True", ")", ":", "\n", "        ", "augs", "=", "utils", ".", "build_augmentation", "(", "cfg", ",", "is_train", ")", "\n", "if", "cfg", ".", "INPUT", ".", "CROP", ".", "ENABLED", "and", "is_train", ":", "\n", "            ", "augs", ".", "insert", "(", "0", ",", "T", ".", "RandomCrop", "(", "cfg", ".", "INPUT", ".", "CROP", ".", "TYPE", ",", "cfg", ".", "INPUT", ".", "CROP", ".", "SIZE", ")", ")", "\n", "recompute_boxes", "=", "cfg", ".", "MODEL", ".", "MASK_ON", "\n", "", "else", ":", "\n", "            ", "recompute_boxes", "=", "False", "\n", "\n", "", "ret", "=", "{", "\n", "\"is_train\"", ":", "is_train", ",", "\n", "\"augmentations\"", ":", "augs", ",", "\n", "\"image_format\"", ":", "cfg", ".", "INPUT", ".", "FORMAT", ",", "\n", "\"use_instance_mask\"", ":", "cfg", ".", "MODEL", ".", "MASK_ON", ",", "\n", "\"instance_mask_format\"", ":", "cfg", ".", "INPUT", ".", "MASK_FORMAT", ",", "\n", "\"use_keypoint\"", ":", "cfg", ".", "MODEL", ".", "KEYPOINT_ON", ",", "\n", "\"recompute_boxes\"", ":", "recompute_boxes", ",", "\n", "}", "\n", "\n", "if", "cfg", ".", "MODEL", ".", "KEYPOINT_ON", ":", "\n", "            ", "ret", "[", "\"keypoint_hflip_indices\"", "]", "=", "utils", ".", "create_keypoint_hflip_indices", "(", "cfg", ".", "DATASETS", ".", "TRAIN", ")", "\n", "\n", "", "if", "cfg", ".", "MODEL", ".", "LOAD_PROPOSALS", ":", "\n", "            ", "ret", "[", "\"precomputed_proposal_topk\"", "]", "=", "(", "\n", "cfg", ".", "DATASETS", ".", "PRECOMPUTED_PROPOSAL_TOPK_TRAIN", "\n", "if", "is_train", "\n", "else", "cfg", ".", "DATASETS", ".", "PRECOMPUTED_PROPOSAL_TOPK_TEST", "\n", ")", "\n", "# CLIP inference on cropped image regions", "\n", "", "if", "cfg", ".", "MODEL", ".", "META_ARCHITECTURE", "in", "[", "\"CLIPRCNN\"", ",", "\"CLIPFastRCNN\"", "]", ":", "\n", "            ", "ret", "[", "\"clip_crop\"", "]", "=", "True", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.dataset_mapper.DatasetMapper.__call__": [[124, 201], ["copy.deepcopy", "detection_utils.read_image", "detection_utils.check_image_size", "dataset_mapper.DatasetMapper.AugInput", "dataset_mapper.DatasetMapper.augmentations", "torch.as_tensor", "detection_utils.read_image().squeeze", "numpy.ascontiguousarray", "torch.as_tensor", "detection_utils.transform_proposals", "detection_utils.annotations_to_instances", "detection_utils.filter_empty_instances", "detection_utils.read_image.transpose", "detection_utils.read_image().squeeze.astype", "copy.deepcopy.pop", "copy.deepcopy.pop", "detection_utils.transform_instance_annotations", "detection_utils.annotations_to_instances.gt_masks.get_bounding_boxes", "detection_utils.read_image", "anno.pop", "anno.pop", "copy.deepcopy.pop", "copy.deepcopy.pop", "obj.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.read_image", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.check_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.annotations_to_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.filter_empty_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_instance_annotations", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.masks.PolygonMasks.get_bounding_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.read_image", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "__call__", "(", "self", ",", "dataset_dict", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n\n        Returns:\n            dict: a format that builtin models in detectron2 accept\n        \"\"\"", "\n", "dataset_dict", "=", "copy", ".", "deepcopy", "(", "dataset_dict", ")", "# it will be modified by code below", "\n", "# USER: Write your own image loading if it's not from a file", "\n", "image", "=", "utils", ".", "read_image", "(", "dataset_dict", "[", "\"file_name\"", "]", ",", "format", "=", "self", ".", "image_format", ")", "\n", "utils", ".", "check_image_size", "(", "dataset_dict", ",", "image", ")", "\n", "\n", "# USER: Remove if you don't do semantic/panoptic segmentation.", "\n", "if", "\"sem_seg_file_name\"", "in", "dataset_dict", ":", "\n", "            ", "sem_seg_gt", "=", "utils", ".", "read_image", "(", "dataset_dict", ".", "pop", "(", "\"sem_seg_file_name\"", ")", ",", "\"L\"", ")", ".", "squeeze", "(", "2", ")", "\n", "", "else", ":", "\n", "            ", "sem_seg_gt", "=", "None", "\n", "\n", "", "aug_input", "=", "T", ".", "AugInput", "(", "image", ",", "sem_seg", "=", "sem_seg_gt", ")", "\n", "transforms", "=", "self", ".", "augmentations", "(", "aug_input", ")", "\n", "\n", "image", ",", "sem_seg_gt", "=", "aug_input", ".", "image", ",", "aug_input", ".", "sem_seg", "\n", "\n", "image_shape", "=", "image", ".", "shape", "[", ":", "2", "]", "# h, w", "\n", "# Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,", "\n", "# but not efficient on large generic data structures due to the use of pickle & mp.Queue.", "\n", "# Therefore it's important to use torch.Tensor.", "\n", "dataset_dict", "[", "\"image\"", "]", "=", "torch", ".", "as_tensor", "(", "np", ".", "ascontiguousarray", "(", "image", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ")", ")", "\n", "if", "sem_seg_gt", "is", "not", "None", ":", "\n", "            ", "dataset_dict", "[", "\"sem_seg\"", "]", "=", "torch", ".", "as_tensor", "(", "sem_seg_gt", ".", "astype", "(", "\"long\"", ")", ")", "\n", "\n", "# USER: Remove if you don't use pre-computed proposals.", "\n", "# Most users would not need this feature.", "\n", "", "if", "self", ".", "proposal_topk", "is", "not", "None", ":", "\n", "            ", "utils", ".", "transform_proposals", "(", "\n", "dataset_dict", ",", "image_shape", ",", "transforms", ",", "proposal_topk", "=", "self", ".", "proposal_topk", "\n", ")", "\n", "\n", "", "if", "not", "self", ".", "is_train", ":", "\n", "            ", "if", "self", ".", "clip_crop", ":", "# still load the GT annotations", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "# USER: Modify this if you want to keep them for some reason.", "\n", "                ", "dataset_dict", ".", "pop", "(", "\"annotations\"", ",", "None", ")", "\n", "dataset_dict", ".", "pop", "(", "\"sem_seg_file_name\"", ",", "None", ")", "\n", "return", "dataset_dict", "\n", "\n", "", "", "if", "\"annotations\"", "in", "dataset_dict", ":", "\n", "# USER: Modify this if you want to keep them for some reason.", "\n", "            ", "for", "anno", "in", "dataset_dict", "[", "\"annotations\"", "]", ":", "\n", "                ", "if", "not", "self", ".", "use_instance_mask", ":", "\n", "                    ", "anno", ".", "pop", "(", "\"segmentation\"", ",", "None", ")", "\n", "", "if", "not", "self", ".", "use_keypoint", ":", "\n", "                    ", "anno", ".", "pop", "(", "\"keypoints\"", ",", "None", ")", "\n", "\n", "# USER: Implement additional transformations if you have other types of data", "\n", "", "", "annos", "=", "[", "\n", "utils", ".", "transform_instance_annotations", "(", "\n", "obj", ",", "transforms", ",", "image_shape", ",", "keypoint_hflip_indices", "=", "self", ".", "keypoint_hflip_indices", "\n", ")", "\n", "for", "obj", "in", "dataset_dict", ".", "pop", "(", "\"annotations\"", ")", "\n", "if", "obj", ".", "get", "(", "\"iscrowd\"", ",", "0", ")", "==", "0", "\n", "]", "\n", "instances", "=", "utils", ".", "annotations_to_instances", "(", "\n", "annos", ",", "image_shape", ",", "mask_format", "=", "self", ".", "instance_mask_format", "\n", ")", "\n", "\n", "# After transforms such as cropping are applied, the bounding box may no longer", "\n", "# tightly bound the object. As an example, imagine a triangle object", "\n", "# [(0,0), (2,0), (0,2)] cropped by a box [(1,0),(2,2)] (XYXY format). The tight", "\n", "# bounding box of the cropped triangle should be [(1,0),(2,1)], which is not equal to", "\n", "# the intersection of original bounding box and the cropping box.", "\n", "if", "self", ".", "recompute_boxes", ":", "\n", "                ", "instances", ".", "gt_boxes", "=", "instances", ".", "gt_masks", ".", "get_bounding_boxes", "(", ")", "\n", "", "dataset_dict", "[", "\"instances\"", "]", "=", "utils", ".", "filter_empty_instances", "(", "instances", ")", "\n", "", "return", "dataset_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.clip_build.config_tsv_dataset_args": [[16, 28], ["None"], "function", ["None"], ["def", "config_tsv_dataset_args", "(", "cfg", ",", "dataset_file", ",", "factory_name", "=", "None", ",", "is_train", "=", "True", ")", ":", "\n", "############### code removecd as tsv_dataset_name = factory_name = \"CLIPImgTxtPairTSVDataset\" ##############", "\n", "    ", "if", "factory_name", "is", "not", "None", ":", "\n", "        ", "tsv_dataset_name", "=", "factory_name", "\n", "\n", "", "if", "tsv_dataset_name", "in", "[", "\"CLIPImgTxtPairTSVDataset\"", "]", ":", "\n", "# no need for extra arguments", "\n", "        ", "args", "=", "{", "}", "\n", "args", "[", "'args'", "]", "=", "cfg", "\n", "args", "[", "'seq_len'", "]", "=", "cfg", ".", "DATASETS", ".", "MAX_SEQ_LENGTH", "# cfg.max_seq_length", "\n", "\n", "", "return", "args", ",", "tsv_dataset_name", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.clip_build.build_dataset": [[30, 147], ["enumerate", "isinstance", "RuntimeError", "isinstance", "RuntimeError", "factory", "datasets.append", "hasattr", "hasattr", "len", "len", "torch.utils.data.dataset.ConcatDataset", "dataset_name.split", "path_list[].split", "zip", "print", "print", "_Tokenizer", "isinstance", "len", "len", "len", "clip_build.config_tsv_dataset_args", "len", "os.path.isfile", "len", "len", "len", "len", "len", "len", "dataset_classes.update", "os.path.dirname", "sorted", "len", "len", "os.path.basename", "os.listdir", "filename.endswith", "image_tsv_list.append", "len", "os.path.join", "text_tsv_list.append", "os.path.join", "len", "len", "os.path.join", "text_tsv_list.append", "len", "filename.replace", "os.path.join", "text_tsv_list.append", "filename.replace", "os.path.join", "filename.replace", "catalog.DatasetCatalog"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.clip_build.config_tsv_dataset_args"], ["", "def", "build_dataset", "(", "cfg", ",", "transforms", ",", "dataset_catalog", ",", "is_train", "=", "True", ",", "is_aux", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Arguments:\n        cfg: config file.\n        transforms (callable): transforms to apply to each (image, target) sample\n        dataset_catalog (DatasetCatalog): contains the information on how to construct a dataset.\n        is_train (bool): whether to setup the dataset for training or testing\n    \"\"\"", "\n", "\n", "dataset_list", "=", "(", "cfg", ".", "DATASETS", ".", "TRAIN", "if", "not", "is_aux", "else", "cfg", ".", "DATASETS", ".", "AUX", ")", "if", "is_train", "else", "cfg", ".", "DATASETS", ".", "TEST", "\n", "factory_list", "=", "(", "cfg", ".", "DATASETS", ".", "FACTORY_TRAIN", "if", "not", "is_aux", "else", "cfg", ".", "DATASETS", ".", "FACTORY_AUX", ")", "if", "is_train", "else", "cfg", ".", "DATASETS", ".", "FACTORY_TEST", "\n", "path_list", "=", "(", "cfg", ".", "DATASETS", ".", "PATH_TRAIN", "if", "not", "is_aux", "else", "cfg", ".", "DATASETS", ".", "PATH_AUX", ")", "if", "is_train", "else", "cfg", ".", "DATASETS", ".", "PATH_TEST", "\n", "\n", "if", "not", "isinstance", "(", "dataset_list", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "\n", "\"dataset_list should be a list of strings, got {}\"", ".", "format", "(", "dataset_list", ")", ")", "\n", "", "if", "not", "isinstance", "(", "factory_list", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "\n", "\"factory_list should be a list of strings, got {}\"", ".", "format", "(", "factory_list", ")", ")", "\n", "", "datasets", "=", "[", "]", "\n", "target_offset", "=", "0", "\n", "for", "i", ",", "dataset_name", "in", "enumerate", "(", "dataset_list", ")", ":", "\n", "        ", "factory_name", "=", "factory_list", "[", "i", "]", "if", "i", "<", "len", "(", "factory_list", ")", "else", "None", "\n", "\n", "if", "factory_name", "==", "\"CLIPImgTxtPairTSVDataset\"", ":", "\n", "            ", "dataset_names_merged", "=", "dataset_name", ".", "split", "(", "'+'", ")", "\n", "path_lists_merged", "=", "path_list", "[", "i", "]", ".", "split", "(", "'+'", ")", "\n", "\n", "assert", "len", "(", "dataset_names_merged", ")", "==", "len", "(", "path_lists_merged", ")", ",", "\"number of datasets must match that of dataset paths\"", "\n", "\n", "image_tsv_list", "=", "[", "]", "\n", "text_tsv_list", "=", "[", "]", "\n", "dataset_name_list", "=", "[", "]", "\n", "map_files", "=", "[", "]", "\n", "max_num_tsv", "=", "20", "# maximum tsv files to load within a given folder        ", "\n", "\n", "for", "dname", ",", "dpath", "in", "zip", "(", "dataset_names_merged", ",", "path_lists_merged", ")", ":", "\n", "                ", "args", ",", "tsv_dataset_name", "=", "config_tsv_dataset_args", "(", "\n", "cfg", ",", "dataset_name", ",", "factory_name", ",", "is_train", "\n", ")", "\n", "factory", "=", "CLIPImgTxtPairTSVDataset", "if", "tsv_dataset_name", "in", "[", "\"CLIPImgTxtPairTSVDataset\"", "]", "else", "None", "\n", "prev_len", "=", "len", "(", "image_tsv_list", ")", "\n", "\n", "isFile", "=", "os", ".", "path", ".", "isfile", "(", "dpath", ")", "\n", "if", "isFile", ":", "\n", "                    ", "dpath_listed_files", "=", "[", "os", ".", "path", ".", "basename", "(", "dpath", ")", "]", "\n", "dpath", "=", "os", ".", "path", ".", "dirname", "(", "dpath", ")", "\n", "", "else", ":", "\n", "                    ", "dpath_listed_files", "=", "sorted", "(", "os", ".", "listdir", "(", "dpath", ")", ")", "\n", "\n", "", "for", "filename", "in", "dpath_listed_files", ":", "\n", "                    ", "if", "(", "\"images\"", "in", "filename", "or", "\"image\"", "in", "filename", "or", "\"img\"", "in", "filename", ")", "and", "filename", ".", "endswith", "(", "\".tsv\"", ")", ":", "\n", "                        ", "image_tsv_list", ".", "append", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "filename", ")", ")", "\n", "if", "\"images\"", "in", "filename", ":", "# \"images\" - \"text\"", "\n", "                            ", "text_tsv_list", ".", "append", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "filename", ".", "replace", "(", "\"images\"", ",", "\"text\"", ")", ")", ")", "\n", "", "elif", "\"image\"", "in", "filename", ":", "# \"image\"-\"text\"", "\n", "                            ", "text_tsv_list", ".", "append", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "filename", ".", "replace", "(", "\"image\"", ",", "\"text\"", ")", ")", ")", "\n", "", "elif", "\"img\"", "in", "filename", ":", "# \"img\"-\"caption\"", "\n", "                            ", "text_tsv_list", ".", "append", "(", "os", ".", "path", ".", "join", "(", "dpath", ",", "filename", ".", "replace", "(", "\"img\"", ",", "\"caption\"", ")", ")", ")", "\n", "", "if", "len", "(", "image_tsv_list", ")", "-", "prev_len", "==", "max_num_tsv", ":", "\n", "                            ", "break", "\n", "", "", "", "dataset_name_list", "+=", "[", "dname", "]", "*", "(", "len", "(", "image_tsv_list", ")", "-", "prev_len", ")", "\n", "\n", "if", "dname", "==", "\"imagenet22k\"", ":", "\n", "                    ", "map_files", "+=", "[", "os", ".", "path", ".", "join", "(", "dpath", ",", "'darknet_data_imagenet.labels.list'", ")", "]", "*", "(", "len", "(", "image_tsv_list", ")", "-", "prev_len", ")", "\n", "", "else", ":", "\n", "                    ", "map_files", "+=", "[", "None", "]", "*", "(", "len", "(", "image_tsv_list", ")", "-", "prev_len", ")", "\n", "\n", "", "assert", "len", "(", "image_tsv_list", ")", "==", "len", "(", "text_tsv_list", ")", ",", "\"the number image tsv files must be equal to that of text tsv files, otherwise check your data!\"", "\n", "\n", "", "args", "[", "\"image_tsv_file\"", "]", "=", "image_tsv_list", "\n", "args", "[", "\"text_tsv_file\"", "]", "=", "text_tsv_list", "\n", "args", "[", "\"dataset_name\"", "]", "=", "dataset_name_list", "\n", "args", "[", "\"map_file\"", "]", "=", "map_files", "\n", "args", "[", "\"filtered_datasets\"", "]", "=", "cfg", ".", "DATASETS", ".", "FILTERED_CLASSIFICATION_DATASETS", "\n", "assert", "len", "(", "image_tsv_list", ")", "==", "len", "(", "text_tsv_list", ")", "==", "len", "(", "dataset_name_list", ")", "==", "len", "(", "map_files", ")", "\n", "\n", "print", "(", "\"number of image tsv files: \"", ",", "len", "(", "image_tsv_list", ")", ")", "\n", "print", "(", "\"number of text tsv fies: \"", ",", "len", "(", "text_tsv_list", ")", ")", "\n", "\n", "", "args", "[", "\"is_train\"", "]", "=", "is_train", "\n", "args", "[", "\"transforms\"", "]", "=", "transforms", "\n", "args", "[", "\"target_offset\"", "]", "=", "target_offset", "\n", "if", "\"bpe\"", "in", "cfg", ".", "INPUT", ".", "TEXT_TOKENIZER", ":", "\n", "            ", "from", "detectron2", ".", "data", ".", "datasets", ".", "clip_prompt_utils", "import", "SimpleTokenizer", "as", "_Tokenizer", "\n", "tokenizer", "=", "_Tokenizer", "(", ")", "\n", "args", "[", "\"tokenizer_type\"", "]", "=", "\"bpe\"", "\n", "", "args", "[", "\"tokenizer\"", "]", "=", "tokenizer", "\n", "# make dataset from factory", "\n", "dataset", "=", "factory", "(", "**", "args", ")", "\n", "datasets", ".", "append", "(", "dataset", ")", "\n", "\n", "", "precomputed_tokens", "=", "{", "}", "\n", "dataset_classes", "=", "{", "}", "\n", "for", "dataset", "in", "datasets", ":", "\n", "        ", "if", "hasattr", "(", "dataset", ",", "\"input_ids_all_classes\"", ")", ":", "\n", "            ", "precomputed_tokens", "[", "\"imagenet\"", "]", "=", "[", "dataset", ".", "input_ids_all_classes", ",", "dataset", ".", "input_mask_all_classes", ",", "dataset", ".", "segment_ids_all_classes", "]", "\n", "", "if", "hasattr", "(", "dataset", ",", "\"classnames\"", ")", ":", "\n", "            ", "if", "isinstance", "(", "dataset", ".", "classnames", ",", "dict", ")", ":", "\n", "                ", "dataset_classes", ".", "update", "(", "dataset", ".", "classnames", ")", "\n", "", "else", ":", "\n", "                ", "dataset_classes", "[", "dataset", ".", "dataset_name", "]", "=", "dataset", ".", "classnames", "\n", "\n", "# for testing, return a list of datasets", "\n", "", "", "", "if", "not", "is_train", ":", "\n", "        ", "return", "datasets", ",", "precomputed_tokens", ",", "dataset_classes", "\n", "\n", "", "if", "len", "(", "datasets", ")", "==", "0", ":", "\n", "        ", "return", "None", ",", "None", ",", "None", "\n", "\n", "# for training, concatenate all datasets into a single one", "\n", "", "dataset", "=", "datasets", "[", "0", "]", "\n", "if", "len", "(", "datasets", ")", ">", "1", ":", "\n", "        ", "dataset", "=", "ConcatDataset", "(", "datasets", ")", "\n", "", "return", "[", "dataset", "]", ",", "precomputed_tokens", ",", "dataset_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.clip_build.make_clip_dataset": [[149, 159], ["print", "print", "clip_build.build_dataset", "transforms.build.build_clip_transforms"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.clip_build.build_dataset", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.build.build_clip_transforms"], ["", "def", "make_clip_dataset", "(", "cfg", ",", "is_train", "=", "True", ",", "is_aux", "=", "False", ",", "transforms", "=", "None", ")", ":", "\n", "    ", "if", "transforms", "is", "None", ":", "\n", "        ", "transforms", "=", "build_clip_transforms", "(", "cfg", ",", "is_train", ")", "\n", "", "print", "(", "\"data transforms: \"", ")", "\n", "print", "(", "transforms", ")", "\n", "datasets", ",", "precomputed_tokens", ",", "dataset_classes", "=", "build_dataset", "(", "cfg", ",", "transforms", ",", "DatasetCatalog", ",", "is_train", ",", "is_aux", ")", "\n", "\n", "if", "not", "datasets", ":", "\n", "        ", "return", "None", ",", "None", ",", "None", "\n", "", "return", "datasets", ",", "precomputed_tokens", ",", "dataset_classes", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.setUp": [[19, 21], ["detectron2.utils.logger.setup_logger"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "setup_logger", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_apply_rotated_boxes": [[22, 39], ["numpy.random.seed", "detectron2.config.get_cfg", "detectron2.data.detection_utils.build_augmentation", "numpy.random.rand", "detectron2.data.transforms.apply_augmentations", "numpy.array", "numpy.array", "numpy.allclose", "detectron2.data.transforms.apply_rotated_box"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.build_augmentation", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.apply_augmentations"], ["", "def", "test_apply_rotated_boxes", "(", "self", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "125", ")", "\n", "cfg", "=", "get_cfg", "(", ")", "\n", "is_train", "=", "True", "\n", "augs", "=", "detection_utils", ".", "build_augmentation", "(", "cfg", ",", "is_train", ")", "\n", "image", "=", "np", ".", "random", ".", "rand", "(", "200", ",", "300", ")", "\n", "image", ",", "transforms", "=", "T", ".", "apply_augmentations", "(", "augs", ",", "image", ")", "\n", "image_shape", "=", "image", ".", "shape", "[", ":", "2", "]", "# h, w", "\n", "assert", "image_shape", "==", "(", "800", ",", "1200", ")", "\n", "annotation", "=", "{", "\"bbox\"", ":", "[", "179", ",", "97", ",", "62", ",", "40", ",", "-", "56", "]", "}", "\n", "\n", "boxes", "=", "np", ".", "array", "(", "[", "annotation", "[", "\"bbox\"", "]", "]", ",", "dtype", "=", "np", ".", "float64", ")", "# boxes.shape = (1, 5)", "\n", "transformed_bbox", "=", "transforms", ".", "apply_rotated_box", "(", "boxes", ")", "[", "0", "]", "\n", "\n", "expected_bbox", "=", "np", ".", "array", "(", "[", "484", ",", "388", ",", "248", ",", "160", ",", "56", "]", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "err_msg", "=", "\"transformed_bbox = {}, expected {}\"", ".", "format", "(", "transformed_bbox", ",", "expected_bbox", ")", "\n", "assert", "np", ".", "allclose", "(", "transformed_bbox", ",", "expected_bbox", ")", ",", "err_msg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_resize_and_crop": [[40, 78], ["numpy.random.seed", "detectron2.data.transforms.ResizeScale", "detectron2.data.transforms.FixedSizeCrop", "detectron2.data.transforms.RandomFlip", "numpy.random.rand", "detectron2.data.transforms.apply_augmentations", "test_transforms.TestTransforms.assertEqual", "numpy.array", "detectron2.data.transforms.apply_box", "numpy.array", "test_transforms.TestTransforms.assertTrue", "numpy.array", "detectron2.data.transforms.apply_polygons", "numpy.array", "test_transforms.TestTransforms.assertEqual", "test_transforms.TestTransforms.assertTrue", "numpy.allclose", "len", "numpy.allclose"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.apply_augmentations"], ["", "def", "test_resize_and_crop", "(", "self", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "125", ")", "\n", "min_scale", "=", "0.2", "\n", "max_scale", "=", "2.0", "\n", "target_height", "=", "1100", "\n", "target_width", "=", "1000", "\n", "resize_aug", "=", "T", ".", "ResizeScale", "(", "min_scale", ",", "max_scale", ",", "target_height", ",", "target_width", ")", "\n", "fixed_size_crop_aug", "=", "T", ".", "FixedSizeCrop", "(", "(", "target_height", ",", "target_width", ")", ")", "\n", "hflip_aug", "=", "T", ".", "RandomFlip", "(", ")", "\n", "augs", "=", "[", "resize_aug", ",", "fixed_size_crop_aug", ",", "hflip_aug", "]", "\n", "original_image", "=", "np", ".", "random", ".", "rand", "(", "900", ",", "800", ")", "\n", "image", ",", "transforms", "=", "T", ".", "apply_augmentations", "(", "augs", ",", "original_image", ")", "\n", "image_shape", "=", "image", ".", "shape", "[", ":", "2", "]", "# h, w", "\n", "self", ".", "assertEqual", "(", "(", "1100", ",", "1000", ")", ",", "image_shape", ")", "\n", "\n", "boxes", "=", "np", ".", "array", "(", "\n", "[", "[", "91", ",", "46", ",", "144", ",", "111", "]", ",", "[", "523", ",", "251", ",", "614", ",", "295", "]", "]", ",", "\n", "dtype", "=", "np", ".", "float64", ",", "\n", ")", "\n", "transformed_bboxs", "=", "transforms", ".", "apply_box", "(", "boxes", ")", "\n", "expected_bboxs", "=", "np", ".", "array", "(", "\n", "[", "\n", "[", "895.42", ",", "33.42666667", ",", "933.91125", ",", "80.66", "]", ",", "\n", "[", "554.0825", ",", "182.39333333", ",", "620.17125", ",", "214.36666667", "]", ",", "\n", "]", ",", "\n", "dtype", "=", "np", ".", "float64", ",", "\n", ")", "\n", "err_msg", "=", "\"transformed_bbox = {}, expected {}\"", ".", "format", "(", "transformed_bboxs", ",", "expected_bboxs", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "transformed_bboxs", ",", "expected_bboxs", ")", ",", "err_msg", ")", "\n", "\n", "polygon", "=", "np", ".", "array", "(", "[", "[", "91", ",", "46", "]", ",", "[", "144", ",", "46", "]", ",", "[", "144", ",", "111", "]", ",", "[", "91", ",", "111", "]", "]", ")", "\n", "transformed_polygons", "=", "transforms", ".", "apply_polygons", "(", "[", "polygon", "]", ")", "\n", "expected_polygon", "=", "np", ".", "array", "(", "[", "[", "934.0", ",", "33.0", "]", ",", "[", "934.0", ",", "80.0", "]", ",", "[", "896.0", ",", "80.0", "]", ",", "[", "896.0", ",", "33.0", "]", "]", ")", "\n", "self", ".", "assertEqual", "(", "1", ",", "len", "(", "transformed_polygons", ")", ")", "\n", "err_msg", "=", "\"transformed_polygon = {}, expected {}\"", ".", "format", "(", "\n", "transformed_polygons", "[", "0", "]", ",", "expected_polygon", "\n", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "transformed_polygons", "[", "0", "]", ",", "expected_polygon", ")", ",", "err_msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_apply_rotated_boxes_unequal_scaling_factor": [[79, 112], ["numpy.random.seed", "numpy.random.rand", "augs.append", "detectron2.data.transforms.apply_augmentations", "numpy.array", "detectron2.data.transforms.apply_rotated_box", "numpy.array", "numpy.allclose", "detectron2.data.transforms.Resize"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.apply_augmentations"], ["", "def", "test_apply_rotated_boxes_unequal_scaling_factor", "(", "self", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "125", ")", "\n", "h", ",", "w", "=", "400", ",", "200", "\n", "newh", ",", "neww", "=", "800", ",", "800", "\n", "image", "=", "np", ".", "random", ".", "rand", "(", "h", ",", "w", ")", "\n", "augs", "=", "[", "]", "\n", "augs", ".", "append", "(", "T", ".", "Resize", "(", "shape", "=", "(", "newh", ",", "neww", ")", ")", ")", "\n", "image", ",", "transforms", "=", "T", ".", "apply_augmentations", "(", "augs", ",", "image", ")", "\n", "image_shape", "=", "image", ".", "shape", "[", ":", "2", "]", "# h, w", "\n", "assert", "image_shape", "==", "(", "newh", ",", "neww", ")", "\n", "\n", "boxes", "=", "np", ".", "array", "(", "\n", "[", "\n", "[", "150", ",", "100", ",", "40", ",", "20", ",", "0", "]", ",", "\n", "[", "150", ",", "100", ",", "40", ",", "20", ",", "30", "]", ",", "\n", "[", "150", ",", "100", ",", "40", ",", "20", ",", "90", "]", ",", "\n", "[", "150", ",", "100", ",", "40", ",", "20", ",", "-", "90", "]", ",", "\n", "]", ",", "\n", "dtype", "=", "np", ".", "float64", ",", "\n", ")", "\n", "transformed_boxes", "=", "transforms", ".", "apply_rotated_box", "(", "boxes", ")", "\n", "\n", "expected_bboxes", "=", "np", ".", "array", "(", "\n", "[", "\n", "[", "600", ",", "200", ",", "160", ",", "40", ",", "0", "]", ",", "\n", "[", "600", ",", "200", ",", "144.22205102", ",", "52.91502622", ",", "49.10660535", "]", ",", "\n", "[", "600", ",", "200", ",", "80", ",", "80", ",", "90", "]", ",", "\n", "[", "600", ",", "200", ",", "80", ",", "80", ",", "-", "90", "]", ",", "\n", "]", ",", "\n", "dtype", "=", "np", ".", "float64", ",", "\n", ")", "\n", "err_msg", "=", "\"transformed_boxes = {}, expected {}\"", ".", "format", "(", "transformed_boxes", ",", "expected_bboxes", ")", "\n", "assert", "np", ".", "allclose", "(", "transformed_boxes", ",", "expected_bboxes", ")", ",", "err_msg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_print_augmentation": [[113, 125], ["detectron2.data.transforms.RandomCrop", "test_transforms.TestTransforms.assertEqual", "detectron2.data.transforms.RandomFlip", "test_transforms.TestTransforms.assertEqual", "detectron2.data.transforms.RandomFlip", "test_transforms.TestTransforms.assertEqual", "detectron2.data.transforms.AugmentationList", "test_transforms.TestTransforms.assertEqual", "str", "str", "str", "str"], "methods", ["None"], ["", "def", "test_print_augmentation", "(", "self", ")", ":", "\n", "        ", "t", "=", "T", ".", "RandomCrop", "(", "\"relative\"", ",", "(", "100", ",", "100", ")", ")", "\n", "self", ".", "assertEqual", "(", "str", "(", "t", ")", ",", "\"RandomCrop(crop_type='relative', crop_size=(100, 100))\"", ")", "\n", "\n", "t0", "=", "T", ".", "RandomFlip", "(", "prob", "=", "0.5", ")", "\n", "self", ".", "assertEqual", "(", "str", "(", "t0", ")", ",", "\"RandomFlip(prob=0.5)\"", ")", "\n", "\n", "t1", "=", "T", ".", "RandomFlip", "(", ")", "\n", "self", ".", "assertEqual", "(", "str", "(", "t1", ")", ",", "\"RandomFlip()\"", ")", "\n", "\n", "t", "=", "T", ".", "AugmentationList", "(", "[", "t0", ",", "t1", "]", ")", "\n", "self", ".", "assertEqual", "(", "str", "(", "t", ")", ",", "f\"AugmentationList[{t0}, {t1}]\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_random_apply_prob_out_of_range_check": [[126, 134], ["test_probabilities.items", "test_transforms.TestTransforms.assertRaises", "detectron2.data.transforms.RandomApply", "detectron2.data.transforms.NoOpTransform"], "methods", ["None"], ["", "def", "test_random_apply_prob_out_of_range_check", "(", "self", ")", ":", "\n", "        ", "test_probabilities", "=", "{", "0.0", ":", "True", ",", "0.5", ":", "True", ",", "1.0", ":", "True", ",", "-", "0.01", ":", "False", ",", "1.01", ":", "False", "}", "\n", "\n", "for", "given_probability", ",", "is_valid", "in", "test_probabilities", ".", "items", "(", ")", ":", "\n", "            ", "if", "not", "is_valid", ":", "\n", "                ", "self", ".", "assertRaises", "(", "AssertionError", ",", "T", ".", "RandomApply", ",", "None", ",", "prob", "=", "given_probability", ")", "\n", "", "else", ":", "\n", "                ", "T", ".", "RandomApply", "(", "T", ".", "NoOpTransform", "(", ")", ",", "prob", "=", "given_probability", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_random_apply_wrapping_aug_probability_occured_evaluation": [[135, 144], ["unittest.mock.MagicMock", "unittest.mock.MagicMock", "detectron2.data.transforms.RandomApply", "unittest.mock.MagicMock.get_transform.assert_called_once_with", "test_transforms.TestTransforms.assertIsNot", "unittest.mock.patch.object", "detectron2.data.transforms.RandomApply.get_transform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.get_transform"], ["", "", "", "def", "test_random_apply_wrapping_aug_probability_occured_evaluation", "(", "self", ")", ":", "\n", "        ", "transform_mock", "=", "mock", ".", "MagicMock", "(", "name", "=", "\"MockTransform\"", ",", "spec", "=", "T", ".", "Augmentation", ")", "\n", "image_mock", "=", "mock", ".", "MagicMock", "(", "name", "=", "\"MockImage\"", ")", "\n", "random_apply", "=", "T", ".", "RandomApply", "(", "transform_mock", ",", "prob", "=", "0.001", ")", "\n", "\n", "with", "mock", ".", "patch", ".", "object", "(", "random_apply", ",", "\"_rand_range\"", ",", "return_value", "=", "0.0001", ")", ":", "\n", "            ", "transform", "=", "random_apply", ".", "get_transform", "(", "image_mock", ")", "\n", "", "transform_mock", ".", "get_transform", ".", "assert_called_once_with", "(", "image_mock", ")", "\n", "self", ".", "assertIsNot", "(", "transform", ",", "transform_mock", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_random_apply_wrapping_std_transform_probability_occured_evaluation": [[145, 153], ["unittest.mock.MagicMock", "unittest.mock.MagicMock", "detectron2.data.transforms.RandomApply", "test_transforms.TestTransforms.assertIs", "unittest.mock.patch.object", "detectron2.data.transforms.RandomApply.get_transform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.get_transform"], ["", "def", "test_random_apply_wrapping_std_transform_probability_occured_evaluation", "(", "self", ")", ":", "\n", "        ", "transform_mock", "=", "mock", ".", "MagicMock", "(", "name", "=", "\"MockTransform\"", ",", "spec", "=", "T", ".", "Transform", ")", "\n", "image_mock", "=", "mock", ".", "MagicMock", "(", "name", "=", "\"MockImage\"", ")", "\n", "random_apply", "=", "T", ".", "RandomApply", "(", "transform_mock", ",", "prob", "=", "0.001", ")", "\n", "\n", "with", "mock", ".", "patch", ".", "object", "(", "random_apply", ",", "\"_rand_range\"", ",", "return_value", "=", "0.0001", ")", ":", "\n", "            ", "transform", "=", "random_apply", ".", "get_transform", "(", "image_mock", ")", "\n", "", "self", ".", "assertIs", "(", "transform", ",", "transform_mock", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_random_apply_probability_not_occured_evaluation": [[154, 163], ["unittest.mock.MagicMock", "unittest.mock.MagicMock", "detectron2.data.transforms.RandomApply", "unittest.mock.MagicMock.get_transform.assert_not_called", "test_transforms.TestTransforms.assertIsInstance", "unittest.mock.patch.object", "detectron2.data.transforms.RandomApply.get_transform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.get_transform"], ["", "def", "test_random_apply_probability_not_occured_evaluation", "(", "self", ")", ":", "\n", "        ", "transform_mock", "=", "mock", ".", "MagicMock", "(", "name", "=", "\"MockTransform\"", ",", "spec", "=", "T", ".", "Augmentation", ")", "\n", "image_mock", "=", "mock", ".", "MagicMock", "(", "name", "=", "\"MockImage\"", ")", "\n", "random_apply", "=", "T", ".", "RandomApply", "(", "transform_mock", ",", "prob", "=", "0.001", ")", "\n", "\n", "with", "mock", ".", "patch", ".", "object", "(", "random_apply", ",", "\"_rand_range\"", ",", "return_value", "=", "0.9", ")", ":", "\n", "            ", "transform", "=", "random_apply", ".", "get_transform", "(", "image_mock", ")", "\n", "", "transform_mock", ".", "get_transform", ".", "assert_not_called", "(", ")", "\n", "self", ".", "assertIsInstance", "(", "transform", ",", "T", ".", "NoOpTransform", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_augmentation_input_args": [[164, 195], ["numpy.random.rand().astype", "detectron2.data.transforms.AugInput", "detectron2.data.transforms.AugInput.apply_augmentations", "test_transforms.TestTransforms.assertIsInstance", "test_transforms.TestTransforms.assertIsInstance", "test_transforms.TestTransforms.assertTrue", "test_transforms.TestTransforms.assertTrue", "test_transforms.TestTransforms.assertRaises", "detectron2.data.transforms.AugInput.apply_augmentations", "detectron2.data.transforms.ResizeTransform", "detectron2.data.transforms.HFlipTransform", "numpy.random.rand", "TG1", "TG2", "numpy.random.rand", "TG3"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.apply_augmentations", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.apply_augmentations"], ["", "def", "test_augmentation_input_args", "(", "self", ")", ":", "\n", "        ", "input_shape", "=", "(", "100", ",", "100", ")", "\n", "output_shape", "=", "(", "50", ",", "50", ")", "\n", "\n", "# define two augmentations with different args", "\n", "class", "TG1", "(", "T", ".", "Augmentation", ")", ":", "\n", "            ", "def", "get_transform", "(", "self", ",", "image", ",", "sem_seg", ")", ":", "\n", "                ", "return", "T", ".", "ResizeTransform", "(", "\n", "input_shape", "[", "0", "]", ",", "input_shape", "[", "1", "]", ",", "output_shape", "[", "0", "]", ",", "output_shape", "[", "1", "]", "\n", ")", "\n", "\n", "", "", "class", "TG2", "(", "T", ".", "Augmentation", ")", ":", "\n", "            ", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "                ", "assert", "image", ".", "shape", "[", ":", "2", "]", "==", "output_shape", "# check that TG1 is applied", "\n", "return", "T", ".", "HFlipTransform", "(", "output_shape", "[", "1", "]", ")", "\n", "\n", "", "", "image", "=", "np", ".", "random", ".", "rand", "(", "*", "input_shape", ")", ".", "astype", "(", "\"float32\"", ")", "\n", "sem_seg", "=", "(", "np", ".", "random", ".", "rand", "(", "*", "input_shape", ")", "<", "0.5", ")", ".", "astype", "(", "\"uint8\"", ")", "\n", "inputs", "=", "T", ".", "AugInput", "(", "image", ",", "sem_seg", "=", "sem_seg", ")", "# provide two args", "\n", "tfms", "=", "inputs", ".", "apply_augmentations", "(", "[", "TG1", "(", ")", ",", "TG2", "(", ")", "]", ")", "\n", "self", ".", "assertIsInstance", "(", "tfms", "[", "0", "]", ",", "T", ".", "ResizeTransform", ")", "\n", "self", ".", "assertIsInstance", "(", "tfms", "[", "1", "]", ",", "T", ".", "HFlipTransform", ")", "\n", "self", ".", "assertTrue", "(", "inputs", ".", "image", ".", "shape", "[", ":", "2", "]", "==", "output_shape", ")", "\n", "self", ".", "assertTrue", "(", "inputs", ".", "sem_seg", ".", "shape", "[", ":", "2", "]", "==", "output_shape", ")", "\n", "\n", "class", "TG3", "(", "T", ".", "Augmentation", ")", ":", "\n", "            ", "def", "get_transform", "(", "self", ",", "image", ",", "nonexist", ")", ":", "\n", "                ", "pass", "\n", "\n", "", "", "with", "self", ".", "assertRaises", "(", "AttributeError", ")", ":", "\n", "            ", "inputs", ".", "apply_augmentations", "(", "[", "TG3", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_augmentation_list": [[196, 204], ["numpy.random.rand().astype", "detectron2.data.transforms.AugInput", "detectron2.data.transforms.AugmentationList", "detectron2.data.transforms.AugmentationList", "numpy.random.rand", "detectron2.data.transforms.RandomFlip", "detectron2.data.transforms.Resize", "numpy.random.rand", "detectron2.data.transforms.Resize"], "methods", ["None"], ["", "", "def", "test_augmentation_list", "(", "self", ")", ":", "\n", "        ", "input_shape", "=", "(", "100", ",", "100", ")", "\n", "image", "=", "np", ".", "random", ".", "rand", "(", "*", "input_shape", ")", ".", "astype", "(", "\"float32\"", ")", "\n", "sem_seg", "=", "(", "np", ".", "random", ".", "rand", "(", "*", "input_shape", ")", "<", "0.5", ")", ".", "astype", "(", "\"uint8\"", ")", "\n", "inputs", "=", "T", ".", "AugInput", "(", "image", ",", "sem_seg", "=", "sem_seg", ")", "# provide two args", "\n", "\n", "augs", "=", "T", ".", "AugmentationList", "(", "[", "T", ".", "RandomFlip", "(", ")", ",", "T", ".", "Resize", "(", "20", ")", "]", ")", "\n", "_", "=", "T", ".", "AugmentationList", "(", "[", "augs", ",", "T", ".", "Resize", "(", "30", ")", "]", ")", "(", "inputs", ")", "\n", "# 3 in latest fvcore (flattened transformlist), 2 in older", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_color_transforms": [[207, 220], ["rand_img.astype.astype.astype", "detectron2.data.transforms.ColorTransform", "test_transforms.TestTransforms.assertTrue", "numpy.random.randint", "detectron2.data.transforms.PILColorTransform", "PIL.ImageOps.solarize", "test_transforms.TestTransforms.assertTrue", "numpy.random.random", "numpy.array_equal", "PIL.Image.fromarray", "numpy.array_equal", "detectron2.data.transforms.ColorTransform.apply_image", "PIL.ImageOps.solarize", "detectron2.data.transforms.PILColorTransform.apply_image"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.solarize", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.solarize", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image"], ["", "def", "test_color_transforms", "(", "self", ")", ":", "\n", "        ", "rand_img", "=", "np", ".", "random", ".", "random", "(", "(", "100", ",", "100", ",", "3", ")", ")", "*", "255", "\n", "rand_img", "=", "rand_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "\n", "# Test no-op", "\n", "noop_transform", "=", "T", ".", "ColorTransform", "(", "lambda", "img", ":", "img", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "array_equal", "(", "rand_img", ",", "noop_transform", ".", "apply_image", "(", "rand_img", ")", ")", ")", "\n", "\n", "# Test a ImageOps operation", "\n", "magnitude", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "256", ")", "\n", "solarize_transform", "=", "T", ".", "PILColorTransform", "(", "lambda", "img", ":", "ImageOps", ".", "solarize", "(", "img", ",", "magnitude", ")", ")", "\n", "expected_img", "=", "ImageOps", ".", "solarize", "(", "Image", ".", "fromarray", "(", "rand_img", ")", ",", "magnitude", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "array_equal", "(", "expected_img", ",", "solarize_transform", ".", "apply_image", "(", "rand_img", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_resize_transform": [[221, 229], ["zip", "numpy.random.randint", "detectron2.data.transforms.ResizeTransform", "detectron2.data.transforms.ResizeTransform.apply_image", "test_transforms.TestTransforms.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image"], ["", "def", "test_resize_transform", "(", "self", ")", ":", "\n", "        ", "input_shapes", "=", "[", "(", "100", ",", "100", ")", ",", "(", "100", ",", "100", ",", "1", ")", ",", "(", "100", ",", "100", ",", "3", ")", "]", "\n", "output_shapes", "=", "[", "(", "200", ",", "200", ")", ",", "(", "200", ",", "200", ",", "1", ")", ",", "(", "200", ",", "200", ",", "3", ")", "]", "\n", "for", "in_shape", ",", "out_shape", "in", "zip", "(", "input_shapes", ",", "output_shapes", ")", ":", "\n", "            ", "in_img", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "255", ",", "size", "=", "in_shape", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "tfm", "=", "T", ".", "ResizeTransform", "(", "in_shape", "[", "0", "]", ",", "in_shape", "[", "1", "]", ",", "out_shape", "[", "0", "]", ",", "out_shape", "[", "1", "]", ")", "\n", "out_img", "=", "tfm", ".", "apply_image", "(", "in_img", ")", "\n", "self", ".", "assertTrue", "(", "out_img", ".", "shape", "==", "out_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_transforms.TestTransforms.test_extent_transform": [[230, 239], ["zip", "numpy.random.randint", "detectron2.data.transforms.ExtentTransform", "detectron2.data.transforms.ExtentTransform.apply_image", "test_transforms.TestTransforms.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image"], ["", "", "def", "test_extent_transform", "(", "self", ")", ":", "\n", "        ", "input_shapes", "=", "[", "(", "100", ",", "100", ")", ",", "(", "100", ",", "100", ",", "1", ")", ",", "(", "100", ",", "100", ",", "3", ")", "]", "\n", "src_rect", "=", "(", "20", ",", "20", ",", "80", ",", "80", ")", "\n", "output_shapes", "=", "[", "(", "200", ",", "200", ")", ",", "(", "200", ",", "200", ",", "1", ")", ",", "(", "200", ",", "200", ",", "3", ")", "]", "\n", "for", "in_shape", ",", "out_shape", "in", "zip", "(", "input_shapes", ",", "output_shapes", ")", ":", "\n", "            ", "in_img", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "255", ",", "size", "=", "in_shape", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "tfm", "=", "T", ".", "ExtentTransform", "(", "src_rect", ",", "out_shape", "[", ":", "2", "]", ")", "\n", "out_img", "=", "tfm", ".", "apply_image", "(", "in_img", ")", "\n", "self", ".", "assertTrue", "(", "out_img", ".", "shape", "==", "out_shape", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays": [[9, 11], ["test_rotation_transform.TestRotationTransform.assertTrue", "numpy.allclose"], "methods", ["None"], ["    ", "def", "assertEqualsArrays", "(", "self", ",", "a1", ",", "a2", ")", ":", "\n", "        ", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "a1", ",", "a2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.randomData": [[12, 16], ["numpy.random.rand", "numpy.array", "range", "range"], "methods", ["None"], ["", "def", "randomData", "(", "self", ",", "h", "=", "5", ",", "w", "=", "5", ")", ":", "\n", "        ", "image", "=", "np", ".", "random", ".", "rand", "(", "h", ",", "w", ")", "\n", "coords", "=", "np", ".", "array", "(", "[", "[", "i", ",", "j", "]", "for", "j", "in", "range", "(", "h", "+", "1", ")", "for", "i", "in", "range", "(", "w", "+", "1", ")", "]", ",", "dtype", "=", "float", ")", "\n", "return", "image", ",", "coords", ",", "h", ",", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.test180": [[17, 23], ["test_rotation_transform.TestRotationTransform.randomData", "detectron2.data.transforms.transform.RotationTransform", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "detectron2.data.transforms.transform.RotationTransform.apply_image", "detectron2.data.transforms.transform.RotationTransform.apply_coords"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.randomData", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords"], ["", "def", "test180", "(", "self", ")", ":", "\n", "        ", "image", ",", "coords", ",", "h", ",", "w", "=", "self", ".", "randomData", "(", "6", ",", "6", ")", "\n", "rot", "=", "RotationTransform", "(", "h", ",", "w", ",", "180", ",", "expand", "=", "False", ",", "center", "=", "None", ")", "\n", "self", ".", "assertEqualsArrays", "(", "rot", ".", "apply_image", "(", "image", ")", ",", "image", "[", ":", ":", "-", "1", ",", ":", ":", "-", "1", "]", ")", "\n", "rotated_coords", "=", "[", "[", "w", "-", "c", "[", "0", "]", ",", "h", "-", "c", "[", "1", "]", "]", "for", "c", "in", "coords", "]", "\n", "self", ".", "assertEqualsArrays", "(", "rot", ".", "apply_coords", "(", "coords", ")", ",", "rotated_coords", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.test45_coords": [[24, 32], ["test_rotation_transform.TestRotationTransform.randomData", "detectron2.data.transforms.transform.RotationTransform", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "detectron2.data.transforms.transform.RotationTransform.apply_coords", "numpy.sqrt", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.randomData", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords"], ["", "def", "test45_coords", "(", "self", ")", ":", "\n", "        ", "_", ",", "coords", ",", "h", ",", "w", "=", "self", ".", "randomData", "(", "4", ",", "6", ")", "\n", "rot", "=", "RotationTransform", "(", "h", ",", "w", ",", "45", ",", "expand", "=", "False", ",", "center", "=", "None", ")", "\n", "rotated_coords", "=", "[", "\n", "[", "(", "x", "+", "y", "-", "(", "h", "+", "w", ")", "/", "2", ")", "/", "np", ".", "sqrt", "(", "2", ")", "+", "w", "/", "2", ",", "h", "/", "2", "+", "(", "y", "+", "(", "w", "-", "h", ")", "/", "2", "-", "x", ")", "/", "np", ".", "sqrt", "(", "2", ")", "]", "\n", "for", "(", "x", ",", "y", ")", "in", "coords", "\n", "]", "\n", "self", ".", "assertEqualsArrays", "(", "rot", ".", "apply_coords", "(", "coords", ")", ",", "rotated_coords", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.test90": [[33, 39], ["test_rotation_transform.TestRotationTransform.randomData", "detectron2.data.transforms.transform.RotationTransform", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "detectron2.data.transforms.transform.RotationTransform.apply_image", "detectron2.data.transforms.transform.RotationTransform.apply_coords"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.randomData", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords"], ["", "def", "test90", "(", "self", ")", ":", "\n", "        ", "image", ",", "coords", ",", "h", ",", "w", "=", "self", ".", "randomData", "(", ")", "\n", "rot", "=", "RotationTransform", "(", "h", ",", "w", ",", "90", ",", "expand", "=", "False", ",", "center", "=", "None", ")", "\n", "self", ".", "assertEqualsArrays", "(", "rot", ".", "apply_image", "(", "image", ")", ",", "image", ".", "T", "[", ":", ":", "-", "1", "]", ")", "\n", "rotated_coords", "=", "[", "[", "c", "[", "1", "]", ",", "w", "-", "c", "[", "0", "]", "]", "for", "c", "in", "coords", "]", "\n", "self", ".", "assertEqualsArrays", "(", "rot", ".", "apply_coords", "(", "coords", ")", ",", "rotated_coords", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.test90_expand": [[40, 46], ["test_rotation_transform.TestRotationTransform.randomData", "detectron2.data.transforms.transform.RotationTransform", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "detectron2.data.transforms.transform.RotationTransform.apply_image", "detectron2.data.transforms.transform.RotationTransform.apply_coords"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.randomData", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords"], ["", "def", "test90_expand", "(", "self", ")", ":", "# non-square image", "\n", "        ", "image", ",", "coords", ",", "h", ",", "w", "=", "self", ".", "randomData", "(", "h", "=", "5", ",", "w", "=", "8", ")", "\n", "rot", "=", "RotationTransform", "(", "h", ",", "w", ",", "90", ",", "expand", "=", "True", ",", "center", "=", "None", ")", "\n", "self", ".", "assertEqualsArrays", "(", "rot", ".", "apply_image", "(", "image", ")", ",", "image", ".", "T", "[", ":", ":", "-", "1", "]", ")", "\n", "rotated_coords", "=", "[", "[", "c", "[", "1", "]", ",", "w", "-", "c", "[", "0", "]", "]", "for", "c", "in", "coords", "]", "\n", "self", ".", "assertEqualsArrays", "(", "rot", ".", "apply_coords", "(", "coords", ")", ",", "rotated_coords", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.test_center_expand": [[47, 59], ["test_rotation_transform.TestRotationTransform.randomData", "numpy.random.randint", "detectron2.data.transforms.transform.RotationTransform", "detectron2.data.transforms.transform.RotationTransform", "detectron2.data.transforms.transform.RotationTransform", "detectron2.data.transforms.transform.RotationTransform", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "r1.apply_image", "r2.apply_image", "r1.apply_coords", "r2.apply_coords"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.randomData", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords"], ["", "def", "test_center_expand", "(", "self", ")", ":", "\n", "# center has no effect if expand=True because it only affects shifting", "\n", "        ", "image", ",", "coords", ",", "h", ",", "w", "=", "self", ".", "randomData", "(", "h", "=", "5", ",", "w", "=", "8", ")", "\n", "angle", "=", "np", ".", "random", ".", "randint", "(", "360", ")", "\n", "rot1", "=", "RotationTransform", "(", "h", ",", "w", ",", "angle", ",", "expand", "=", "True", ",", "center", "=", "None", ")", "\n", "rot2", "=", "RotationTransform", "(", "h", ",", "w", ",", "angle", ",", "expand", "=", "True", ",", "center", "=", "(", "0", ",", "0", ")", ")", "\n", "rot3", "=", "RotationTransform", "(", "h", ",", "w", ",", "angle", ",", "expand", "=", "True", ",", "center", "=", "(", "h", ",", "w", ")", ")", "\n", "rot4", "=", "RotationTransform", "(", "h", ",", "w", ",", "angle", ",", "expand", "=", "True", ",", "center", "=", "(", "2", ",", "5", ")", ")", "\n", "for", "r1", "in", "[", "rot1", ",", "rot2", ",", "rot3", ",", "rot4", "]", ":", "\n", "            ", "for", "r2", "in", "[", "rot1", ",", "rot2", ",", "rot3", ",", "rot4", "]", ":", "\n", "                ", "self", ".", "assertEqualsArrays", "(", "r1", ".", "apply_image", "(", "image", ")", ",", "r2", ".", "apply_image", "(", "image", ")", ")", "\n", "self", ".", "assertEqualsArrays", "(", "r1", ".", "apply_coords", "(", "coords", ")", ",", "r2", ".", "apply_coords", "(", "coords", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.test_inverse_transform": [[60, 68], ["test_rotation_transform.TestRotationTransform.randomData", "detectron2.data.transforms.transform.RotationTransform", "detectron2.data.transforms.transform.RotationTransform.apply_image", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "detectron2.data.transforms.transform.RotationTransform", "detectron2.data.transforms.transform.RotationTransform.apply_coords", "test_rotation_transform.TestRotationTransform.assertEqualsArrays", "detectron2.data.transforms.transform.RotationTransform.inverse().apply_image", "detectron2.data.transforms.transform.RotationTransform.inverse().apply_coords", "detectron2.data.transforms.transform.RotationTransform.inverse", "detectron2.data.transforms.transform.RotationTransform.inverse"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.randomData", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_rotation_transform.TestRotationTransform.assertEqualsArrays", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.inverse", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.inverse"], ["", "", "", "def", "test_inverse_transform", "(", "self", ")", ":", "\n", "        ", "image", ",", "coords", ",", "h", ",", "w", "=", "self", ".", "randomData", "(", "h", "=", "5", ",", "w", "=", "8", ")", "\n", "rot", "=", "RotationTransform", "(", "h", ",", "w", ",", "90", ",", "expand", "=", "True", ",", "center", "=", "None", ")", "\n", "rot_image", "=", "rot", ".", "apply_image", "(", "image", ")", "\n", "self", ".", "assertEqualsArrays", "(", "rot", ".", "inverse", "(", ")", ".", "apply_image", "(", "rot_image", ")", ",", "image", ")", "\n", "rot", "=", "RotationTransform", "(", "h", ",", "w", ",", "65", ",", "expand", "=", "True", ",", "center", "=", "None", ")", "\n", "rotated_coords", "=", "rot", ".", "apply_coords", "(", "coords", ")", "\n", "self", ".", "assertEqualsArrays", "(", "rot", ".", "inverse", "(", ")", ".", "apply_coords", "(", "rotated_coords", ")", ",", "coords", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_detection_utils.TestTransformAnnotations.test_transform_simple_annotation": [[16, 31], ["detectron2.data.transforms.TransformList", "detectron2.data.detection_utils.transform_instance_annotations", "test_detection_utils.TestTransformAnnotations.assertTrue", "test_detection_utils.TestTransformAnnotations.assertEqual", "test_detection_utils.TestTransformAnnotations.assertTrue", "detectron2.data.detection_utils.annotations_to_instances", "numpy.asarray", "numpy.allclose", "len", "len", "numpy.allclose", "detectron2.data.transforms.HFlipTransform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_instance_annotations", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.annotations_to_instances"], ["    ", "def", "test_transform_simple_annotation", "(", "self", ")", ":", "\n", "        ", "transforms", "=", "T", ".", "TransformList", "(", "[", "T", ".", "HFlipTransform", "(", "400", ")", "]", ")", "\n", "anno", "=", "{", "\n", "\"bbox\"", ":", "np", ".", "asarray", "(", "[", "10", ",", "10", ",", "200", ",", "300", "]", ")", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", ",", "\n", "\"category_id\"", ":", "3", ",", "\n", "\"segmentation\"", ":", "[", "[", "10", ",", "10", ",", "100", ",", "100", ",", "100", ",", "10", "]", ",", "[", "150", ",", "150", ",", "200", ",", "150", ",", "200", ",", "200", "]", "]", ",", "\n", "}", "\n", "\n", "output", "=", "detection_utils", ".", "transform_instance_annotations", "(", "anno", ",", "transforms", ",", "(", "400", ",", "400", ")", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output", "[", "\"bbox\"", "]", ",", "[", "200", ",", "10", ",", "390", ",", "300", "]", ")", ")", "\n", "self", ".", "assertEqual", "(", "len", "(", "output", "[", "\"segmentation\"", "]", ")", ",", "len", "(", "anno", "[", "\"segmentation\"", "]", ")", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output", "[", "\"segmentation\"", "]", "[", "0", "]", ",", "[", "390", ",", "10", ",", "300", ",", "100", ",", "300", ",", "10", "]", ")", ")", "\n", "\n", "detection_utils", ".", "annotations_to_instances", "(", "[", "output", ",", "output", "]", ",", "(", "400", ",", "400", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_detection_utils.TestTransformAnnotations.test_flip_keypoints": [[32, 61], ["detectron2.data.transforms.TransformList", "detectron2.data.detection_utils.transform_instance_annotations", "test_detection_utils.TestTransformAnnotations.assertTrue", "test_detection_utils.TestTransformAnnotations.assertTrue", "test_detection_utils.TestTransformAnnotations.assertTrue", "numpy.asarray", "copy.deepcopy", "numpy.allclose", "numpy.allclose", "numpy.allclose", "detectron2.data.transforms.HFlipTransform", "detectron2.data.detection_utils.create_keypoint_hflip_indices", "[].reshape", "numpy.random.rand", "[].reshape", "[].reshape", "[].reshape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_instance_annotations", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.create_keypoint_hflip_indices"], ["", "def", "test_flip_keypoints", "(", "self", ")", ":", "\n", "        ", "transforms", "=", "T", ".", "TransformList", "(", "[", "T", ".", "HFlipTransform", "(", "400", ")", "]", ")", "\n", "anno", "=", "{", "\n", "\"bbox\"", ":", "np", ".", "asarray", "(", "[", "10", ",", "10", ",", "200", ",", "300", "]", ")", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", ",", "\n", "\"keypoints\"", ":", "np", ".", "random", ".", "rand", "(", "17", ",", "3", ")", "*", "50", "+", "15", ",", "\n", "}", "\n", "\n", "output", "=", "detection_utils", ".", "transform_instance_annotations", "(", "\n", "copy", ".", "deepcopy", "(", "anno", ")", ",", "\n", "transforms", ",", "\n", "(", "400", ",", "400", ")", ",", "\n", "keypoint_hflip_indices", "=", "detection_utils", ".", "create_keypoint_hflip_indices", "(", "\n", "[", "\"keypoints_coco_2017_train\"", "]", "\n", ")", ",", "\n", ")", "\n", "# The first keypoint is nose", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "output", "[", "\"keypoints\"", "]", "[", "0", ",", "0", "]", ",", "400", "-", "anno", "[", "\"keypoints\"", "]", "[", "0", ",", "0", "]", ")", ")", "\n", "# The last 16 keypoints are 8 left-right pairs", "\n", "self", ".", "assertTrue", "(", "\n", "np", ".", "allclose", "(", "\n", "output", "[", "\"keypoints\"", "]", "[", "1", ":", ",", "0", "]", ".", "reshape", "(", "-", "1", ",", "2", ")", "[", ":", ",", ":", ":", "-", "1", "]", ",", "\n", "400", "-", "anno", "[", "\"keypoints\"", "]", "[", "1", ":", ",", "0", "]", ".", "reshape", "(", "-", "1", ",", "2", ")", ",", "\n", ")", "\n", ")", "\n", "self", ".", "assertTrue", "(", "\n", "np", ".", "allclose", "(", "\n", "output", "[", "\"keypoints\"", "]", "[", "1", ":", ",", "1", ":", "]", ".", "reshape", "(", "-", "1", ",", "2", ",", "2", ")", "[", ":", ",", ":", ":", "-", "1", ",", ":", "]", ",", "\n", "anno", "[", "\"keypoints\"", "]", "[", "1", ":", ",", "1", ":", "]", ".", "reshape", "(", "-", "1", ",", "2", ",", "2", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_detection_utils.TestTransformAnnotations.test_crop": [[64, 81], ["detectron2.data.transforms.TransformList", "detectron2.data.detection_utils.transform_instance_annotations", "test_detection_utils.TestTransformAnnotations.assertTrue", "test_detection_utils.TestTransformAnnotations.assertTrue", "numpy.asarray", "copy.deepcopy", "detectron2.data.transforms.CropTransform", "numpy.random.rand", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_instance_annotations"], ["", "def", "test_crop", "(", "self", ")", ":", "\n", "        ", "transforms", "=", "T", ".", "TransformList", "(", "[", "T", ".", "CropTransform", "(", "300", ",", "300", ",", "10", ",", "10", ")", "]", ")", "\n", "keypoints", "=", "np", ".", "random", ".", "rand", "(", "17", ",", "3", ")", "*", "50", "+", "15", "\n", "keypoints", "[", ":", ",", "2", "]", "=", "2", "\n", "anno", "=", "{", "\n", "\"bbox\"", ":", "np", ".", "asarray", "(", "[", "10", ",", "10", ",", "200", ",", "400", "]", ")", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", ",", "\n", "\"keypoints\"", ":", "keypoints", ",", "\n", "}", "\n", "\n", "output", "=", "detection_utils", ".", "transform_instance_annotations", "(", "\n", "copy", ".", "deepcopy", "(", "anno", ")", ",", "transforms", ",", "(", "10", ",", "10", ")", "\n", ")", "\n", "# box is shifted and cropped", "\n", "self", ".", "assertTrue", "(", "(", "output", "[", "\"bbox\"", "]", "==", "np", ".", "asarray", "(", "[", "0", ",", "0", ",", "0", ",", "10", "]", ")", ")", ".", "all", "(", ")", ")", "\n", "# keypoints are no longer visible", "\n", "self", ".", "assertTrue", "(", "(", "output", "[", "\"keypoints\"", "]", "[", ":", ",", "2", "]", "==", "0", ")", ".", "all", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_detection_utils.TestTransformAnnotations.test_transform_RLE": [[82, 104], ["detectron2.data.transforms.TransformList", "numpy.zeros().astype", "detectron2.data.detection_utils.transform_instance_annotations", "test_detection_utils.TestTransformAnnotations.assertTrue", "test_detection_utils.TestTransformAnnotations.assertTrue", "detectron2.data.detection_utils.annotations_to_instances", "test_detection_utils.TestTransformAnnotations.assertTrue", "numpy.asarray", "copy.deepcopy", "isinstance", "detectron2.data.transforms.HFlipTransform", "numpy.zeros", "pycocotools.encode"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_instance_annotations", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.annotations_to_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "def", "test_transform_RLE", "(", "self", ")", ":", "\n", "        ", "transforms", "=", "T", ".", "TransformList", "(", "[", "T", ".", "HFlipTransform", "(", "400", ")", "]", ")", "\n", "mask", "=", "np", ".", "zeros", "(", "(", "300", ",", "400", ")", ",", "order", "=", "\"F\"", ")", ".", "astype", "(", "\"uint8\"", ")", "\n", "mask", "[", ":", ",", ":", "200", "]", "=", "1", "\n", "\n", "anno", "=", "{", "\n", "\"bbox\"", ":", "np", ".", "asarray", "(", "[", "10", ",", "10", ",", "200", ",", "300", "]", ")", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", ",", "\n", "\"segmentation\"", ":", "mask_util", ".", "encode", "(", "mask", "[", ":", ",", ":", ",", "None", "]", ")", "[", "0", "]", ",", "\n", "\"category_id\"", ":", "3", ",", "\n", "}", "\n", "output", "=", "detection_utils", ".", "transform_instance_annotations", "(", "\n", "copy", ".", "deepcopy", "(", "anno", ")", ",", "transforms", ",", "(", "300", ",", "400", ")", "\n", ")", "\n", "mask", "=", "output", "[", "\"segmentation\"", "]", "\n", "self", ".", "assertTrue", "(", "(", "mask", "[", ":", ",", "200", ":", "]", "==", "1", ")", ".", "all", "(", ")", ")", "\n", "self", ".", "assertTrue", "(", "(", "mask", "[", ":", ",", ":", "200", "]", "==", "0", ")", ".", "all", "(", ")", ")", "\n", "\n", "inst", "=", "detection_utils", ".", "annotations_to_instances", "(", "\n", "[", "output", ",", "output", "]", ",", "(", "400", ",", "400", ")", ",", "mask_format", "=", "\"bitmask\"", "\n", ")", "\n", "self", ".", "assertTrue", "(", "isinstance", "(", "inst", ".", "gt_masks", ",", "BitMasks", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_detection_utils.TestTransformAnnotations.test_transform_RLE_resize": [[105, 126], ["detectron2.data.transforms.TransformList", "numpy.zeros().astype", "detectron2.data.detection_utils.transform_instance_annotations", "detectron2.data.detection_utils.annotations_to_instances", "test_detection_utils.TestTransformAnnotations.assertTrue", "numpy.asarray", "copy.deepcopy", "isinstance", "detectron2.data.transforms.HFlipTransform", "detectron2.data.transforms.ScaleTransform", "numpy.zeros", "pycocotools.encode"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.transform_instance_annotations", "home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.annotations_to_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "def", "test_transform_RLE_resize", "(", "self", ")", ":", "\n", "        ", "transforms", "=", "T", ".", "TransformList", "(", "\n", "[", "T", ".", "HFlipTransform", "(", "400", ")", ",", "T", ".", "ScaleTransform", "(", "300", ",", "400", ",", "400", ",", "400", ",", "\"bilinear\"", ")", "]", "\n", ")", "\n", "mask", "=", "np", ".", "zeros", "(", "(", "300", ",", "400", ")", ",", "order", "=", "\"F\"", ")", ".", "astype", "(", "\"uint8\"", ")", "\n", "mask", "[", ":", ",", ":", "200", "]", "=", "1", "\n", "\n", "anno", "=", "{", "\n", "\"bbox\"", ":", "np", ".", "asarray", "(", "[", "10", ",", "10", ",", "200", ",", "300", "]", ")", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", ",", "\n", "\"segmentation\"", ":", "mask_util", ".", "encode", "(", "mask", "[", ":", ",", ":", ",", "None", "]", ")", "[", "0", "]", ",", "\n", "\"category_id\"", ":", "3", ",", "\n", "}", "\n", "output", "=", "detection_utils", ".", "transform_instance_annotations", "(", "\n", "copy", ".", "deepcopy", "(", "anno", ")", ",", "transforms", ",", "(", "400", ",", "400", ")", "\n", ")", "\n", "\n", "inst", "=", "detection_utils", ".", "annotations_to_instances", "(", "\n", "[", "output", ",", "output", "]", ",", "(", "400", ",", "400", ")", ",", "mask_format", "=", "\"bitmask\"", "\n", ")", "\n", "self", ".", "assertTrue", "(", "isinstance", "(", "inst", ".", "gt_masks", ",", "BitMasks", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_detection_utils.TestTransformAnnotations.test_gen_crop": [[127, 132], ["detectron2.data.detection_utils.gen_crop_transform_with_instance", "test_detection_utils.TestTransformAnnotations.assertTrue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.gen_crop_transform_with_instance"], ["", "def", "test_gen_crop", "(", "self", ")", ":", "\n", "        ", "instance", "=", "{", "\"bbox\"", ":", "[", "10", ",", "10", ",", "100", ",", "100", "]", ",", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", "}", "\n", "t", "=", "detection_utils", ".", "gen_crop_transform_with_instance", "(", "(", "10", ",", "10", ")", ",", "(", "150", ",", "150", ")", ",", "instance", ")", "\n", "# the box center must fall into the cropped region", "\n", "self", ".", "assertTrue", "(", "t", ".", "x0", "<=", "55", "<=", "t", ".", "x0", "+", "t", ".", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_detection_utils.TestTransformAnnotations.test_gen_crop_outside_boxes": [[133, 137], ["test_detection_utils.TestTransformAnnotations.assertRaises", "detectron2.data.detection_utils.gen_crop_transform_with_instance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.gen_crop_transform_with_instance"], ["", "def", "test_gen_crop_outside_boxes", "(", "self", ")", ":", "\n", "        ", "instance", "=", "{", "\"bbox\"", ":", "[", "10", ",", "10", ",", "100", ",", "100", "]", ",", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", "}", "\n", "with", "self", ".", "assertRaises", "(", "AssertionError", ")", ":", "\n", "            ", "detection_utils", ".", "gen_crop_transform_with_instance", "(", "(", "10", ",", "10", ")", ",", "(", "15", ",", "15", ")", ",", "instance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_detection_utils.TestTransformAnnotations.test_read_sem_seg": [[138, 153], ["os.path.join", "detectron2.data.detection_utils.read_image", "test_detection_utils.TestTransformAnnotations.assertEqual", "test_detection_utils.TestTransformAnnotations.assertEqual", "test_detection_utils.TestTransformAnnotations.assertEqual", "test_detection_utils.TestTransformAnnotations.assertEqual", "test_detection_utils.TestTransformAnnotations.assertEqual", "detectron2.data.MetadataCatalog.get", "detectron2.utils.file_io.PathManager.exists", "unittest.SkipTest", "detectron2.data.detection_utils.read_image.max", "detectron2.data.detection_utils.read_image.min"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.read_image", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "test_read_sem_seg", "(", "self", ")", ":", "\n", "        ", "cityscapes_dir", "=", "MetadataCatalog", ".", "get", "(", "\"cityscapes_fine_sem_seg_val\"", ")", ".", "gt_dir", "\n", "sem_seg_gt_path", "=", "os", ".", "path", ".", "join", "(", "\n", "cityscapes_dir", ",", "\"frankfurt\"", ",", "\"frankfurt_000001_083852_gtFine_labelIds.png\"", "\n", ")", "\n", "if", "not", "PathManager", ".", "exists", "(", "sem_seg_gt_path", ")", ":", "\n", "            ", "raise", "unittest", ".", "SkipTest", "(", "\n", "\"Semantic segmentation ground truth {} not found.\"", ".", "format", "(", "sem_seg_gt_path", ")", "\n", ")", "\n", "", "sem_seg", "=", "detection_utils", ".", "read_image", "(", "sem_seg_gt_path", ",", "\"L\"", ")", "\n", "self", ".", "assertEqual", "(", "sem_seg", ".", "ndim", ",", "3", ")", "\n", "self", ".", "assertEqual", "(", "sem_seg", ".", "shape", "[", "2", "]", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "sem_seg", ".", "dtype", ",", "np", ".", "uint8", ")", "\n", "self", ".", "assertEqual", "(", "sem_seg", ".", "max", "(", ")", ",", "32", ")", "\n", "self", ".", "assertEqual", "(", "sem_seg", ".", "min", "(", ")", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_detection_utils.TestTransformAnnotations.test_read_exif_orientation": [[154, 161], ["detectron2.data.detection_utils.read_image", "test_detection_utils.TestTransformAnnotations.assertEqual", "test_detection_utils.TestTransformAnnotations.assertEqual", "test_detection_utils.TestTransformAnnotations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.detection_utils.read_image"], ["", "def", "test_read_exif_orientation", "(", "self", ")", ":", "\n", "# https://github.com/recurser/exif-orientation-examples/raw/master/Landscape_5.jpg", "\n", "        ", "URL", "=", "\"detectron2://assets/Landscape_5.jpg\"", "\n", "img", "=", "detection_utils", ".", "read_image", "(", "URL", ",", "\"RGB\"", ")", "\n", "self", ".", "assertEqual", "(", "img", ".", "ndim", ",", "3", ")", "\n", "self", ".", "assertEqual", "(", "img", ".", "dtype", ",", "np", ".", "uint8", ")", "\n", "self", ".", "assertEqual", "(", "img", ".", "shape", ",", "(", "1200", ",", "1800", ",", "3", ")", ")", "# check that shape is not transposed", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_sampler.TestGroupedBatchSampler.test_missing_group_id": [[21, 28], ["torch.utils.data.sampler.SequentialSampler", "detectron2.data.samplers.GroupedBatchSampler", "list", "test_sampler.TestGroupedBatchSampler.assertEqual", "range", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["    ", "def", "test_missing_group_id", "(", "self", ")", ":", "\n", "        ", "sampler", "=", "SequentialSampler", "(", "list", "(", "range", "(", "100", ")", ")", ")", "\n", "group_ids", "=", "[", "1", "]", "*", "100", "\n", "samples", "=", "GroupedBatchSampler", "(", "sampler", ",", "group_ids", ",", "2", ")", "\n", "\n", "for", "mini_batch", "in", "samples", ":", "\n", "            ", "self", ".", "assertEqual", "(", "len", "(", "mini_batch", ")", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_sampler.TestGroupedBatchSampler.test_groups": [[29, 36], ["torch.utils.data.sampler.SequentialSampler", "detectron2.data.samplers.GroupedBatchSampler", "list", "test_sampler.TestGroupedBatchSampler.assertEqual", "range"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "", "def", "test_groups", "(", "self", ")", ":", "\n", "        ", "sampler", "=", "SequentialSampler", "(", "list", "(", "range", "(", "100", ")", ")", ")", "\n", "group_ids", "=", "[", "1", ",", "0", "]", "*", "50", "\n", "samples", "=", "GroupedBatchSampler", "(", "sampler", ",", "group_ids", ",", "2", ")", "\n", "\n", "for", "mini_batch", "in", "samples", ":", "\n", "            ", "self", ".", "assertEqual", "(", "(", "mini_batch", "[", "0", "]", "+", "mini_batch", "[", "1", "]", ")", "%", "2", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_sampler.TestSamplerDeterministic.test_to_iterable": [[39, 58], ["detectron2.data.samplers.TrainingSampler", "detectron2.data.common.DatasetFromList", "detectron2.data.common.ToIterableDataset", "torch.utils.data.DataLoader", "list", "test_sampler.TestSamplerDeterministic.assertEqual", "torch.utils.data.DataLoader", "list", "test_sampler.TestSamplerDeterministic.assertEqual", "list", "itertools.islice", "set", "set", "itertools.islice", "set", "set", "range", "operator.itemgetter", "range", "operator.itemgetter", "range"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["    ", "def", "test_to_iterable", "(", "self", ")", ":", "\n", "        ", "sampler", "=", "TrainingSampler", "(", "100", ",", "seed", "=", "10", ")", "\n", "dataset", "=", "DatasetFromList", "(", "list", "(", "range", "(", "100", ")", ")", ")", "\n", "dataset", "=", "ToIterableDataset", "(", "dataset", ",", "sampler", ")", "\n", "data_loader", "=", "data", ".", "DataLoader", "(", "dataset", ",", "num_workers", "=", "0", ",", "collate_fn", "=", "operator", ".", "itemgetter", "(", "0", ")", ")", "\n", "\n", "output", "=", "list", "(", "itertools", ".", "islice", "(", "data_loader", ",", "100", ")", ")", "\n", "self", ".", "assertEqual", "(", "set", "(", "output", ")", ",", "set", "(", "range", "(", "100", ")", ")", ")", "\n", "\n", "data_loader", "=", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "num_workers", "=", "2", ",", "\n", "collate_fn", "=", "operator", ".", "itemgetter", "(", "0", ")", ",", "\n", "worker_init_fn", "=", "worker_init_reset_seed", ",", "\n", "# reset seed should not affect behavior of TrainingSampler", "\n", ")", "\n", "output", "=", "list", "(", "itertools", ".", "islice", "(", "data_loader", ",", "100", ")", ")", "\n", "# multiple workers should not lead to duplicate or different data", "\n", "self", ".", "assertEqual", "(", "set", "(", "output", ")", ",", "set", "(", "range", "(", "100", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_sampler.TestSamplerDeterministic.test_training_sampler_seed": [[59, 69], ["detectron2.utils.env.seed_all_rng", "detectron2.data.samplers.TrainingSampler", "list", "detectron2.utils.env.seed_all_rng", "detectron2.data.samplers.TrainingSampler", "detectron2.utils.env.seed_all_rng", "list", "test_sampler.TestSamplerDeterministic.assertEqual", "itertools.islice", "itertools.islice"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.seed_all_rng", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.seed_all_rng", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.env.seed_all_rng", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "test_training_sampler_seed", "(", "self", ")", ":", "\n", "        ", "seed_all_rng", "(", "42", ")", "\n", "sampler", "=", "TrainingSampler", "(", "30", ")", "\n", "data", "=", "list", "(", "itertools", ".", "islice", "(", "sampler", ",", "65", ")", ")", "\n", "\n", "seed_all_rng", "(", "42", ")", "\n", "sampler", "=", "TrainingSampler", "(", "30", ")", "\n", "seed_all_rng", "(", "999", ")", "# should be ineffective", "\n", "data2", "=", "list", "(", "itertools", ".", "islice", "(", "sampler", ",", "65", ")", ")", "\n", "self", ".", "assertEqual", "(", "data", ",", "data2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_sampler.TestRepeatFactorTrainingSampler.test_repeat_factors_from_category_frequency": [[72, 87], ["detectron2.data.samplers.RepeatFactorTrainingSampler.repeat_factors_from_category_frequency", "torch.tensor", "test_sampler.TestRepeatFactorTrainingSampler.assertTrue", "torch.allclose", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler.repeat_factors_from_category_frequency"], ["    ", "def", "test_repeat_factors_from_category_frequency", "(", "self", ")", ":", "\n", "        ", "repeat_thresh", "=", "0.5", "\n", "\n", "dataset_dicts", "=", "[", "\n", "{", "\"annotations\"", ":", "[", "{", "\"category_id\"", ":", "0", "}", ",", "{", "\"category_id\"", ":", "1", "}", "]", "}", ",", "\n", "{", "\"annotations\"", ":", "[", "{", "\"category_id\"", ":", "0", "}", "]", "}", ",", "\n", "{", "\"annotations\"", ":", "[", "]", "}", ",", "\n", "]", "\n", "\n", "rep_factors", "=", "RepeatFactorTrainingSampler", ".", "repeat_factors_from_category_frequency", "(", "\n", "dataset_dicts", ",", "repeat_thresh", "\n", ")", "\n", "\n", "expected_rep_factors", "=", "torch", ".", "tensor", "(", "[", "math", ".", "sqrt", "(", "3", "/", "2", ")", ",", "1.0", ",", "1.0", "]", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "rep_factors", ",", "expected_rep_factors", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco_evaluation.TestCOCOeval.test_fast_eval": [[21, 125], ["copy.deepcopy", "experiments.items", "copy.deepcopy", "copy.deepcopy", "tempfile.TemporaryDirectory", "os.path.join", "test_coco_evaluation.TestCOCOeval.assertTrue", "open", "json.dump", "contextlib.redirect_stdout", "pycocotools.coco.COCO", "contextlib.redirect_stdout", "pycocotools.coco.COCO.loadRes", "pycocotools.cocoeval.COCOeval", "params.items", "pycocotools.cocoeval.COCOeval.evaluate", "pycocotools.cocoeval.COCOeval.accumulate", "pycocotools.cocoeval.COCOeval.summarize", "contextlib.redirect_stdout", "pycocotools.coco.COCO.loadRes", "detectron2.evaluation.fast_eval_api.COCOeval_opt", "params.items", "detectron2.evaluation.fast_eval_api.COCOeval_opt.evaluate", "detectron2.evaluation.fast_eval_api.COCOeval_opt.accumulate", "detectron2.evaluation.fast_eval_api.COCOeval_opt.summarize", "numpy.abs", "test_coco_evaluation.TestCOCOeval.assertTrue", "io.StringIO", "io.StringIO", "setattr", "io.StringIO", "setattr", "type", "type", "numpy.max"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.evaluate", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.fast_eval_api.COCOeval_opt.accumulate", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.evaluate", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.fast_eval_api.COCOeval_opt.accumulate"], ["    ", "def", "test_fast_eval", "(", "self", ")", ":", "\n", "# A small set of images/categories from COCO val", "\n", "# fmt: off", "\n", "        ", "detections", "=", "[", "{", "\"image_id\"", ":", "139", ",", "\"category_id\"", ":", "1", ",", "\"bbox\"", ":", "[", "417.3332824707031", ",", "159.27003479003906", ",", "47.66064453125", ",", "143.00193786621094", "]", ",", "\"score\"", ":", "0.9949821829795837", ",", "\"segmentation\"", ":", "{", "\"size\"", ":", "[", "426", ",", "640", "]", ",", "\"counts\"", ":", "\"Tc`52W=3N0N4aNN^E7]:4XE1g:8kDMT;U100000001O1gE[Nk8h1dFiNY9Z1aFkN]9g2J3NdN`FlN`9S1cFRN07]9g1bFoM6;X9c1cFoM=8R9g1bFQN>3U9Y30O01OO1O001N2O1N1O4L4L5UNoE3V:CVF6Q:@YF9l9@ZF<k9[O`F=];HYnX2\"", "}", "}", ",", "{", "\"image_id\"", ":", "139", ",", "\"category_id\"", ":", "1", ",", "\"bbox\"", ":", "[", "383.5909118652344", ",", "172.0777587890625", ",", "17.959075927734375", ",", "36.94813537597656", "]", ",", "\"score\"", ":", "0.7685421705245972", ",", "\"segmentation\"", ":", "{", "\"size\"", ":", "[", "426", ",", "640", "]", ",", "\"counts\"", ":", "\"lZP5m0Z<300O100O100000001O00]OlC0T<OnCOT<OnCNX<JnC2bQT3\"", "}", "}", ",", "{", "\"image_id\"", ":", "139", ",", "\"category_id\"", ":", "1", ",", "\"bbox\"", ":", "[", "457.8359069824219", ",", "158.88027954101562", ",", "9.89764404296875", ",", "8.771820068359375", "]", ",", "\"score\"", ":", "0.07092753797769547", ",", "\"segmentation\"", ":", "{", "\"size\"", ":", "[", "426", ",", "640", "]", ",", "\"counts\"", ":", "\"bSo54T=2N2O1001O006ImiW2\"", "}", "}", "]", "# noqa", "\n", "gt_annotations", "=", "{", "\"categories\"", ":", "[", "{", "\"supercategory\"", ":", "\"person\"", ",", "\"id\"", ":", "1", ",", "\"name\"", ":", "\"person\"", "}", ",", "{", "\"supercategory\"", ":", "\"furniture\"", ",", "\"id\"", ":", "65", ",", "\"name\"", ":", "\"bed\"", "}", "]", ",", "\"images\"", ":", "[", "{", "\"license\"", ":", "4", ",", "\"file_name\"", ":", "\"000000000285.jpg\"", ",", "\"coco_url\"", ":", "\"http://images.cocodataset.org/val2017/000000000285.jpg\"", ",", "\"height\"", ":", "640", ",", "\"width\"", ":", "586", ",", "\"date_captured\"", ":", "\"2013-11-18 13:09:47\"", ",", "\"flickr_url\"", ":", "\"http://farm8.staticflickr.com/7434/9138147604_c6225224b8_z.jpg\"", ",", "\"id\"", ":", "285", "}", ",", "{", "\"license\"", ":", "2", ",", "\"file_name\"", ":", "\"000000000139.jpg\"", ",", "\"coco_url\"", ":", "\"http://images.cocodataset.org/val2017/000000000139.jpg\"", ",", "\"height\"", ":", "426", ",", "\"width\"", ":", "640", ",", "\"date_captured\"", ":", "\"2013-11-21 01:34:01\"", ",", "\"flickr_url\"", ":", "\"http://farm9.staticflickr.com/8035/8024364858_9c41dc1666_z.jpg\"", ",", "\"id\"", ":", "139", "}", "]", ",", "\"annotations\"", ":", "[", "{", "\"segmentation\"", ":", "[", "[", "428.19", ",", "219.47", ",", "430.94", ",", "209.57", ",", "430.39", ",", "210.12", ",", "421.32", ",", "216.17", ",", "412.8", ",", "217.27", ",", "413.9", ",", "214.24", ",", "422.42", ",", "211.22", ",", "429.29", ",", "201.6", ",", "430.67", ",", "181.8", ",", "430.12", ",", "175.2", ",", "427.09", ",", "168.06", ",", "426.27", ",", "164.21", ",", "430.94", ",", "159.26", ",", "440.29", ",", "157.61", ",", "446.06", ",", "163.93", ",", "448.53", ",", "168.06", ",", "448.53", ",", "173.01", ",", "449.08", ",", "174.93", ",", "454.03", ",", "185.1", ",", "455.41", ",", "188.4", ",", "458.43", ",", "195.0", ",", "460.08", ",", "210.94", ",", "462.28", ",", "226.61", ",", "460.91", ",", "233.76", ",", "454.31", ",", "234.04", ",", "460.08", ",", "256.85", ",", "462.56", ",", "268.13", ",", "465.58", ",", "290.67", ",", "465.85", ",", "293.14", ",", "463.38", ",", "295.62", ",", "452.66", ",", "295.34", ",", "448.26", ",", "294.52", ",", "443.59", ",", "282.7", ",", "446.06", ",", "235.14", ",", "446.34", ",", "230.19", ",", "438.09", ",", "232.39", ",", "438.09", ",", "221.67", ",", "434.24", ",", "221.12", ",", "427.09", ",", "219.74", "]", "]", ",", "\"area\"", ":", "2913.1103999999987", ",", "\"iscrowd\"", ":", "0", ",", "\"image_id\"", ":", "139", ",", "\"bbox\"", ":", "[", "412.8", ",", "157.61", ",", "53.05", ",", "138.01", "]", ",", "\"category_id\"", ":", "1", ",", "\"id\"", ":", "230831", "}", ",", "{", "\"segmentation\"", ":", "[", "[", "384.98", ",", "206.58", ",", "384.43", ",", "199.98", ",", "385.25", ",", "193.66", ",", "385.25", ",", "190.08", ",", "387.18", ",", "185.13", ",", "387.18", ",", "182.93", ",", "386.08", ",", "181.01", ",", "385.25", ",", "178.81", ",", "385.25", ",", "175.79", ",", "388.0", ",", "172.76", ",", "394.88", ",", "172.21", ",", "398.72", ",", "173.31", ",", "399.27", ",", "176.06", ",", "399.55", ",", "183.48", ",", "397.9", ",", "185.68", ",", "395.15", ",", "188.98", ",", "396.8", ",", "193.38", ",", "398.45", ",", "194.48", ",", "399.0", ",", "205.75", ",", "395.43", ",", "207.95", ",", "388.83", ",", "206.03", "]", "]", ",", "\"area\"", ":", "435.1449499999997", ",", "\"iscrowd\"", ":", "0", ",", "\"image_id\"", ":", "139", ",", "\"bbox\"", ":", "[", "384.43", ",", "172.21", ",", "15.12", ",", "35.74", "]", ",", "\"category_id\"", ":", "1", ",", "\"id\"", ":", "233201", "}", "]", "}", "# noqa", "\n", "# fmt: on", "\n", "\n", "# Test a small dataset for typical COCO format", "\n", "experiments", "=", "{", "\"full\"", ":", "(", "detections", ",", "gt_annotations", ",", "{", "}", ")", "}", "\n", "\n", "# Test what happens if the list of detections or ground truth annotations is empty", "\n", "experiments", "[", "\"empty_dt\"", "]", "=", "(", "[", "]", ",", "gt_annotations", ",", "{", "}", ")", "\n", "gt", "=", "copy", ".", "deepcopy", "(", "gt_annotations", ")", "\n", "gt", "[", "\"annotations\"", "]", "=", "[", "]", "\n", "experiments", "[", "\"empty_gt\"", "]", "=", "(", "detections", ",", "gt", ",", "{", "}", ")", "\n", "\n", "# Test changing parameter settings", "\n", "experiments", "[", "\"no_categories\"", "]", "=", "(", "detections", ",", "gt_annotations", ",", "{", "\"useCats\"", ":", "0", "}", ")", "\n", "experiments", "[", "\"no_ious\"", "]", "=", "(", "detections", ",", "gt_annotations", ",", "{", "\"iouThrs\"", ":", "[", "]", "}", ")", "\n", "experiments", "[", "\"no_rec_thrs\"", "]", "=", "(", "detections", ",", "gt_annotations", ",", "{", "\"recThrs\"", ":", "[", "]", "}", ")", "\n", "experiments", "[", "\"no_max_dets\"", "]", "=", "(", "detections", ",", "gt_annotations", ",", "{", "\"maxDets\"", ":", "[", "]", "}", ")", "\n", "experiments", "[", "\"one_max_det\"", "]", "=", "(", "detections", ",", "gt_annotations", ",", "{", "\"maxDets\"", ":", "[", "1", "]", "}", ")", "\n", "experiments", "[", "\"no_area\"", "]", "=", "(", "detections", ",", "gt_annotations", ",", "{", "\"areaRng\"", ":", "[", "]", ",", "\"areaRngLbl\"", ":", "[", "]", "}", ")", "\n", "\n", "# Test what happens if one omits different fields from the annotation structure", "\n", "annotation_fields", "=", "[", "\n", "\"id\"", ",", "\n", "\"image_id\"", ",", "\n", "\"category_id\"", ",", "\n", "\"score\"", ",", "\n", "\"area\"", ",", "\n", "\"iscrowd\"", ",", "\n", "\"ignore\"", ",", "\n", "\"bbox\"", ",", "\n", "\"segmentation\"", ",", "\n", "]", "\n", "for", "a", "in", "annotation_fields", ":", "\n", "            ", "gt", "=", "copy", ".", "deepcopy", "(", "gt_annotations", ")", "\n", "for", "g", "in", "gt", "[", "\"annotations\"", "]", ":", "\n", "                ", "if", "a", "in", "g", ":", "\n", "                    ", "del", "g", "[", "a", "]", "\n", "", "", "dt", "=", "copy", ".", "deepcopy", "(", "detections", ")", "\n", "for", "d", "in", "dt", ":", "\n", "                ", "if", "a", "in", "d", ":", "\n", "                    ", "del", "d", "[", "a", "]", "\n", "", "", "experiments", "[", "\"omit_gt_\"", "+", "a", "]", "=", "(", "detections", ",", "gt", ",", "{", "}", ")", "\n", "experiments", "[", "\"omit_dt_\"", "+", "a", "]", "=", "(", "dt", ",", "gt_annotations", ",", "{", "}", ")", "\n", "\n", "# Compare precision/recall for original COCO PythonAPI to custom optimized one", "\n", "", "for", "name", ",", "(", "dt", ",", "gt", ",", "params", ")", "in", "experiments", ".", "items", "(", ")", ":", "\n", "# Dump to json.", "\n", "            ", "try", ":", "\n", "                ", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "tmpdir", ":", "\n", "                    ", "json_file_name", "=", "os", ".", "path", ".", "join", "(", "tmpdir", ",", "\"gt_\"", "+", "name", "+", "\".json\"", ")", "\n", "with", "open", "(", "json_file_name", ",", "\"w\"", ")", "as", "f", ":", "\n", "                        ", "json", ".", "dump", "(", "gt", ",", "f", ")", "\n", "", "with", "contextlib", ".", "redirect_stdout", "(", "io", ".", "StringIO", "(", ")", ")", ":", "\n", "                        ", "coco_api", "=", "COCO", "(", "json_file_name", ")", "\n", "", "", "", "except", "Exception", ":", "\n", "                ", "pass", "\n", "\n", "", "for", "iou_type", "in", "[", "\"bbox\"", ",", "\"segm\"", ",", "\"keypoints\"", "]", ":", "\n", "# Run original COCOeval PythonAPI", "\n", "                ", "api_exception", "=", "None", "\n", "try", ":", "\n", "                    ", "with", "contextlib", ".", "redirect_stdout", "(", "io", ".", "StringIO", "(", ")", ")", ":", "\n", "                        ", "coco_dt", "=", "coco_api", ".", "loadRes", "(", "dt", ")", "\n", "coco_eval", "=", "COCOeval", "(", "coco_api", ",", "coco_dt", ",", "iou_type", ")", "\n", "for", "p", ",", "v", "in", "params", ".", "items", "(", ")", ":", "\n", "                            ", "setattr", "(", "coco_eval", ".", "params", ",", "p", ",", "v", ")", "\n", "", "coco_eval", ".", "evaluate", "(", ")", "\n", "coco_eval", ".", "accumulate", "(", ")", "\n", "coco_eval", ".", "summarize", "(", ")", "\n", "", "", "except", "Exception", "as", "ex", ":", "\n", "                    ", "api_exception", "=", "ex", "\n", "\n", "# Run optimized COCOeval_opt API", "\n", "", "opt_exception", "=", "None", "\n", "try", ":", "\n", "                    ", "with", "contextlib", ".", "redirect_stdout", "(", "io", ".", "StringIO", "(", ")", ")", ":", "\n", "                        ", "coco_dt", "=", "coco_api", ".", "loadRes", "(", "dt", ")", "\n", "coco_eval_opt", "=", "COCOeval_opt", "(", "coco_api", ",", "coco_dt", ",", "iou_type", ")", "\n", "for", "p", ",", "v", "in", "params", ".", "items", "(", ")", ":", "\n", "                            ", "setattr", "(", "coco_eval_opt", ".", "params", ",", "p", ",", "v", ")", "\n", "", "coco_eval_opt", ".", "evaluate", "(", ")", "\n", "coco_eval_opt", ".", "accumulate", "(", ")", "\n", "coco_eval_opt", ".", "summarize", "(", ")", "\n", "", "", "except", "Exception", "as", "ex", ":", "\n", "                    ", "opt_exception", "=", "ex", "\n", "\n", "", "if", "api_exception", "is", "not", "None", "and", "opt_exception", "is", "not", "None", ":", "\n", "# Original API and optimized API should throw the same exception if annotation", "\n", "# format is bad", "\n", "                    ", "api_error", "=", "\"\"", "if", "api_exception", "is", "None", "else", "type", "(", "api_exception", ")", ".", "__name__", "\n", "opt_error", "=", "\"\"", "if", "opt_exception", "is", "None", "else", "type", "(", "opt_exception", ")", ".", "__name__", "\n", "msg", "=", "\"%s: comparing COCO APIs, '%s' != '%s'\"", "%", "(", "name", ",", "api_error", ",", "opt_error", ")", "\n", "self", ".", "assertTrue", "(", "api_error", "==", "opt_error", ",", "msg", "=", "msg", ")", "\n", "", "else", ":", "\n", "# Original API and optimized API should produce the same precision/recalls", "\n", "                    ", "for", "k", "in", "[", "\"precision\"", ",", "\"recall\"", "]", ":", "\n", "                        ", "diff", "=", "np", ".", "abs", "(", "coco_eval", ".", "eval", "[", "k", "]", "-", "coco_eval_opt", ".", "eval", "[", "k", "]", ")", "\n", "abs_diff", "=", "np", ".", "max", "(", "diff", ")", "if", "diff", ".", "size", ">", "0", "else", "0.0", "\n", "msg", "=", "\"%s: comparing COCO APIs, %s differs by %f\"", "%", "(", "name", ",", "k", ",", "abs_diff", ")", "\n", "self", ".", "assertTrue", "(", "abs_diff", "<", "1e-4", ",", "msg", "=", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco_evaluation.TestCOCOeval.test_unknown_category": [[126, 140], ["unittest.skipIf", "detectron2.evaluation.COCOEvaluator", "detectron2.evaluation.COCOEvaluator.reset", "detectron2.structures.Instances", "detectron2.structures.Boxes", "torch.rand", "torch.tensor", "detectron2.evaluation.COCOEvaluator.process", "os.environ.get", "detectron2.data.DatasetCatalog.get", "torch.rand", "test_coco_evaluation.TestCOCOeval.assertRaises", "detectron2.evaluation.COCOEvaluator.evaluate"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.reset", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.process", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.evaluate"], ["", "", "", "", "", "@", "unittest", ".", "skipIf", "(", "os", ".", "environ", ".", "get", "(", "\"CI\"", ")", ",", "\"Require COCO data.\"", ")", "\n", "def", "test_unknown_category", "(", "self", ")", ":", "\n", "        ", "dataset", "=", "\"coco_2017_val_100\"", "\n", "evaluator", "=", "COCOEvaluator", "(", "dataset", ")", "\n", "evaluator", ".", "reset", "(", ")", "\n", "inputs", "=", "DatasetCatalog", ".", "get", "(", "dataset", ")", "[", ":", "2", "]", "\n", "pred", "=", "Instances", "(", "(", "100", ",", "100", ")", ")", "\n", "pred", ".", "pred_boxes", "=", "Boxes", "(", "torch", ".", "rand", "(", "2", ",", "4", ")", ")", "\n", "pred", ".", "scores", "=", "torch", ".", "rand", "(", "2", ")", "\n", "pred", ".", "pred_classes", "=", "torch", ".", "tensor", "(", "[", "10", ",", "80", "]", ")", "\n", "output", "=", "{", "\"instances\"", ":", "pred", "}", "\n", "evaluator", ".", "process", "(", "inputs", ",", "[", "output", ",", "output", "]", ")", "\n", "with", "self", ".", "assertRaises", "(", "AssertionError", ")", ":", "\n", "            ", "evaluator", ".", "evaluate", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_dataset.TestDatasetFromList.test_using_lazy_path": [[16, 26], ["range", "detectron2.data.build.DatasetFromList", "range", "detectron2.data.build.DatasetFromList.append", "test_dataset.TestDatasetFromList.assertTrue", "test_dataset.TestDatasetFromList.assertEqual", "isinstance", "os.fspath", "test_dataset._a_slow_func", "iopath.common.file_io.LazyPath", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.test_dataset._a_slow_func"], ["    ", "def", "test_using_lazy_path", "(", "self", ")", ":", "\n", "        ", "dataset", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "            ", "dataset", ".", "append", "(", "{", "\"file_name\"", ":", "LazyPath", "(", "partial", "(", "_a_slow_func", ",", "i", ")", ")", "}", ")", "\n", "\n", "", "dataset", "=", "DatasetFromList", "(", "dataset", ")", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "            ", "path", "=", "dataset", "[", "i", "]", "[", "\"file_name\"", "]", "\n", "self", ".", "assertTrue", "(", "isinstance", "(", "path", ",", "LazyPath", ")", ")", "\n", "self", ".", "assertEqual", "(", "os", ".", "fspath", "(", "path", ")", ",", "_a_slow_func", "(", "i", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_dataset._a_slow_func": [[11, 13], ["None"], "function", ["None"], ["def", "_a_slow_func", "(", "x", ")", ":", "\n", "    ", "return", "\"path/{}\"", ".", "format", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.TestRLEToJson.test": [[78, 99], ["test_coco.make_mask", "detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "detectron2.data.datasets.coco.convert_to_coco_dict", "pycocotools.decode", "test_coco.TestRLEToJson.assertTrue", "detectron2.data.DatasetCatalog.pop", "detectron2.data.MetadataCatalog.pop", "tempfile.TemporaryDirectory", "os.path.join", "detectron2.data.datasets.coco.load_coco_json", "numpy.array_equal", "test_coco.make_dataset_dicts", "detectron2.data.MetadataCatalog.get", "open", "json.dump"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.make_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.convert_to_coco_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.load_coco_json", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.make_dataset_dicts", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump"], ["    ", "def", "test", "(", "self", ")", ":", "\n", "# Make a dummy dataset.", "\n", "        ", "mask", "=", "make_mask", "(", ")", "\n", "DatasetCatalog", ".", "register", "(", "\"test_dataset\"", ",", "lambda", ":", "make_dataset_dicts", "(", "mask", ")", ")", "\n", "MetadataCatalog", ".", "get", "(", "\"test_dataset\"", ")", ".", "set", "(", "thing_classes", "=", "[", "\"test_label\"", "]", ")", "\n", "\n", "# Dump to json.", "\n", "json_dict", "=", "convert_to_coco_dict", "(", "\"test_dataset\"", ")", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "tmpdir", ":", "\n", "            ", "json_file_name", "=", "os", ".", "path", ".", "join", "(", "tmpdir", ",", "\"test.json\"", ")", "\n", "with", "open", "(", "json_file_name", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "json", ".", "dump", "(", "json_dict", ",", "f", ")", "\n", "# Load from json.", "\n", "", "dicts", "=", "load_coco_json", "(", "json_file_name", ",", "\"\"", ")", "\n", "\n", "# Check the loaded mask matches the original.", "\n", "", "anno", "=", "dicts", "[", "0", "]", "[", "\"annotations\"", "]", "[", "0", "]", "\n", "loaded_mask", "=", "mask_util", ".", "decode", "(", "anno", "[", "\"segmentation\"", "]", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "array_equal", "(", "loaded_mask", ",", "mask", ")", ")", "\n", "DatasetCatalog", ".", "pop", "(", "\"test_dataset\"", ")", "\n", "MetadataCatalog", ".", "pop", "(", "\"test_dataset\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.TestRLEToJson.test_uncompressed_RLE": [[100, 106], ["test_coco.make_mask", "pycocotools.encode", "test_coco.uncompressed_rle", "pycocotools.frPyObjects", "test_coco.TestRLEToJson.assertEqual", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.make_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.uncompressed_rle"], ["", "def", "test_uncompressed_RLE", "(", "self", ")", ":", "\n", "        ", "mask", "=", "make_mask", "(", ")", "\n", "rle", "=", "mask_util", ".", "encode", "(", "np", ".", "asarray", "(", "mask", ",", "order", "=", "\"F\"", ")", ")", "\n", "uncompressed", "=", "uncompressed_rle", "(", "mask", ")", "\n", "compressed", "=", "mask_util", ".", "frPyObjects", "(", "uncompressed", ",", "*", "rle", "[", "\"size\"", "]", ")", "\n", "self", ".", "assertEqual", "(", "rle", ",", "compressed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.TestConvertCOCO.generate_data": [[109, 133], ["None"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "generate_data", "(", ")", ":", "\n", "        ", "record", "=", "{", "\n", "\"file_name\"", ":", "\"test\"", ",", "\n", "\"image_id\"", ":", "0", ",", "\n", "\"height\"", ":", "100", ",", "\n", "\"width\"", ":", "100", ",", "\n", "\"annotations\"", ":", "[", "\n", "{", "\n", "\"bbox\"", ":", "[", "10", ",", "10", ",", "10", ",", "10", ",", "5", "]", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYWHA_ABS", ",", "\n", "\"category_id\"", ":", "0", ",", "\n", "\"iscrowd\"", ":", "0", ",", "\n", "}", ",", "\n", "{", "\n", "\"bbox\"", ":", "[", "15", ",", "15", ",", "3", ",", "3", "]", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", ",", "\n", "\"category_id\"", ":", "0", ",", "\n", "\"iscrowd\"", ":", "0", ",", "\n", "}", ",", "\n", "]", ",", "\n", "}", "\n", "\n", "return", "[", "record", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.TestConvertCOCO.test_convert_to_coco": [[134, 140], ["detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "detectron2.data.datasets.coco.convert_to_coco_dict", "detectron2.data.DatasetCatalog.pop", "detectron2.data.MetadataCatalog.pop", "test_coco.TestConvertCOCO.generate_data", "detectron2.data.MetadataCatalog.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.convert_to_coco_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.TestConvertCOCO.generate_data", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "test_convert_to_coco", "(", "self", ")", ":", "\n", "        ", "DatasetCatalog", ".", "register", "(", "\"test_dataset\"", ",", "lambda", ":", "TestConvertCOCO", ".", "generate_data", "(", ")", ")", "\n", "MetadataCatalog", ".", "get", "(", "\"test_dataset\"", ")", ".", "set", "(", "thing_classes", "=", "[", "\"test_label\"", "]", ")", "\n", "convert_to_coco_dict", "(", "\"test_dataset\"", ")", "\n", "DatasetCatalog", ".", "pop", "(", "\"test_dataset\"", ")", "\n", "MetadataCatalog", ".", "pop", "(", "\"test_dataset\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.make_mask": [[14, 27], ["numpy.zeros", "range", "range", "numpy.linalg.norm", "numpy.array", "numpy.array"], "function", ["None"], ["def", "make_mask", "(", ")", ":", "\n", "    ", "\"\"\"\n    Makes a donut shaped binary mask.\n    \"\"\"", "\n", "H", "=", "100", "\n", "W", "=", "100", "\n", "mask", "=", "np", ".", "zeros", "(", "[", "H", ",", "W", "]", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "for", "x", "in", "range", "(", "W", ")", ":", "\n", "        ", "for", "y", "in", "range", "(", "H", ")", ":", "\n", "            ", "d", "=", "np", ".", "linalg", ".", "norm", "(", "np", ".", "array", "(", "[", "W", ",", "H", "]", ")", "/", "2", "-", "np", ".", "array", "(", "[", "x", ",", "y", "]", ")", ")", "\n", "if", "d", ">", "10", "and", "d", "<", "20", ":", "\n", "                ", "mask", "[", "y", ",", "x", "]", "=", "1", "\n", "", "", "", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.uncompressed_rle": [[29, 43], ["mask.flatten().tolist", "counts.append", "mask.flatten", "counts.append"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "def", "uncompressed_rle", "(", "mask", ")", ":", "\n", "    ", "l", "=", "mask", ".", "flatten", "(", "order", "=", "\"F\"", ")", ".", "tolist", "(", ")", "\n", "counts", "=", "[", "]", "\n", "p", "=", "False", "\n", "cnt", "=", "0", "\n", "for", "i", "in", "l", ":", "\n", "        ", "if", "i", "==", "p", ":", "\n", "            ", "cnt", "+=", "1", "\n", "", "else", ":", "\n", "            ", "counts", ".", "append", "(", "cnt", ")", "\n", "p", "=", "i", "\n", "cnt", "=", "1", "\n", "", "", "counts", ".", "append", "(", "cnt", ")", "\n", "return", "{", "\"counts\"", ":", "counts", ",", "\"size\"", ":", "[", "mask", ".", "shape", "[", "0", "]", ",", "mask", ".", "shape", "[", "1", "]", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.make_dataset_dicts": [[45, 75], ["numpy.nonzero", "numpy.min", "numpy.max", "numpy.min", "numpy.max", "pycocotools.encode", "test_coco.uncompressed_rle", "numpy.asarray"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode", "home.repos.pwc.inspect_result.microsoft_regionclip.data.test_coco.uncompressed_rle"], ["", "def", "make_dataset_dicts", "(", "mask", ",", "compressed", ":", "bool", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Returns a list of dicts that represents a single COCO data point for\n    object detection. The single instance given by `mask` is represented by\n    RLE, either compressed or uncompressed.\n    \"\"\"", "\n", "record", "=", "{", "}", "\n", "record", "[", "\"file_name\"", "]", "=", "\"test\"", "\n", "record", "[", "\"image_id\"", "]", "=", "0", "\n", "record", "[", "\"height\"", "]", "=", "mask", ".", "shape", "[", "0", "]", "\n", "record", "[", "\"width\"", "]", "=", "mask", ".", "shape", "[", "1", "]", "\n", "\n", "y", ",", "x", "=", "np", ".", "nonzero", "(", "mask", ")", "\n", "if", "compressed", ":", "\n", "        ", "segmentation", "=", "mask_util", ".", "encode", "(", "np", ".", "asarray", "(", "mask", ",", "order", "=", "\"F\"", ")", ")", "\n", "", "else", ":", "\n", "        ", "segmentation", "=", "uncompressed_rle", "(", "mask", ")", "\n", "", "min_x", "=", "np", ".", "min", "(", "x", ")", "\n", "max_x", "=", "np", ".", "max", "(", "x", ")", "\n", "min_y", "=", "np", ".", "min", "(", "y", ")", "\n", "max_y", "=", "np", ".", "max", "(", "y", ")", "\n", "obj", "=", "{", "\n", "\"bbox\"", ":", "[", "min_x", ",", "min_y", ",", "max_x", ",", "max_y", "]", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", ",", "\n", "\"category_id\"", ":", "0", ",", "\n", "\"iscrowd\"", ":", "0", ",", "\n", "\"segmentation\"", ":", "segmentation", ",", "\n", "}", "\n", "record", "[", "\"annotations\"", "]", "=", "[", "obj", "]", "\n", "return", "[", "record", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes_panoptic.get_cityscapes_panoptic_files": [[18, 49], ["detectron2.utils.file_io.PathManager.ls", "logger.info", "len", "detectron2.utils.file_io.PathManager.isfile", "detectron2.utils.file_io.PathManager.isfile", "os.path.join", "detectron2.utils.file_io.PathManager.ls", "image_dict.get", "os.path.join", "files.append", "os.path.join", "basename.endswith", "len", "os.path.basename", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "get_cityscapes_panoptic_files", "(", "image_dir", ",", "gt_dir", ",", "json_info", ")", ":", "\n", "    ", "files", "=", "[", "]", "\n", "# scan through the directory", "\n", "cities", "=", "PathManager", ".", "ls", "(", "image_dir", ")", "\n", "logger", ".", "info", "(", "f\"{len(cities)} cities found in '{image_dir}'.\"", ")", "\n", "image_dict", "=", "{", "}", "\n", "for", "city", "in", "cities", ":", "\n", "        ", "city_img_dir", "=", "os", ".", "path", ".", "join", "(", "image_dir", ",", "city", ")", "\n", "for", "basename", "in", "PathManager", ".", "ls", "(", "city_img_dir", ")", ":", "\n", "            ", "image_file", "=", "os", ".", "path", ".", "join", "(", "city_img_dir", ",", "basename", ")", "\n", "\n", "suffix", "=", "\"_leftImg8bit.png\"", "\n", "assert", "basename", ".", "endswith", "(", "suffix", ")", ",", "basename", "\n", "basename", "=", "os", ".", "path", ".", "basename", "(", "basename", ")", "[", ":", "-", "len", "(", "suffix", ")", "]", "\n", "\n", "image_dict", "[", "basename", "]", "=", "image_file", "\n", "\n", "", "", "for", "ann", "in", "json_info", "[", "\"annotations\"", "]", ":", "\n", "        ", "image_file", "=", "image_dict", ".", "get", "(", "ann", "[", "\"image_id\"", "]", ",", "None", ")", "\n", "assert", "image_file", "is", "not", "None", ",", "\"No image {} found for annotation {}\"", ".", "format", "(", "\n", "ann", "[", "\"image_id\"", "]", ",", "ann", "[", "\"file_name\"", "]", "\n", ")", "\n", "label_file", "=", "os", ".", "path", ".", "join", "(", "gt_dir", ",", "ann", "[", "\"file_name\"", "]", ")", "\n", "segments_info", "=", "ann", "[", "\"segments_info\"", "]", "\n", "\n", "files", ".", "append", "(", "(", "image_file", ",", "label_file", ",", "segments_info", ")", ")", "\n", "\n", "", "assert", "len", "(", "files", ")", ",", "\"No images found in {}\"", ".", "format", "(", "image_dir", ")", "\n", "assert", "PathManager", ".", "isfile", "(", "files", "[", "0", "]", "[", "0", "]", ")", ",", "files", "[", "0", "]", "[", "0", "]", "\n", "assert", "PathManager", ".", "isfile", "(", "files", "[", "0", "]", "[", "1", "]", ")", ",", "files", "[", "0", "]", "[", "1", "]", "\n", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes_panoptic.load_cityscapes_panoptic": [[51, 110], ["os.path.exists", "cityscapes_panoptic.get_cityscapes_panoptic_files", "len", "detectron2.utils.file_io.PathManager.isfile", "detectron2.utils.file_io.PathManager.isfile", "open", "json.load", "ret.append", "cityscapes_panoptic.load_cityscapes_panoptic._convert_category_id"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes_panoptic.get_cityscapes_panoptic_files", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation.COCOPanopticEvaluator._convert_category_id"], ["", "def", "load_cityscapes_panoptic", "(", "image_dir", ",", "gt_dir", ",", "gt_json", ",", "meta", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        image_dir (str): path to the raw dataset. e.g., \"~/cityscapes/leftImg8bit/train\".\n        gt_dir (str): path to the raw annotations. e.g.,\n            \"~/cityscapes/gtFine/cityscapes_panoptic_train\".\n        gt_json (str): path to the json file. e.g.,\n            \"~/cityscapes/gtFine/cityscapes_panoptic_train.json\".\n        meta (dict): dictionary containing \"thing_dataset_id_to_contiguous_id\"\n            and \"stuff_dataset_id_to_contiguous_id\" to map category ids to\n            contiguous ids for training.\n\n    Returns:\n        list[dict]: a list of dicts in Detectron2 standard format. (See\n        `Using Custom Datasets </tutorials/datasets.html>`_ )\n    \"\"\"", "\n", "\n", "def", "_convert_category_id", "(", "segment_info", ",", "meta", ")", ":", "\n", "        ", "if", "segment_info", "[", "\"category_id\"", "]", "in", "meta", "[", "\"thing_dataset_id_to_contiguous_id\"", "]", ":", "\n", "            ", "segment_info", "[", "\"category_id\"", "]", "=", "meta", "[", "\"thing_dataset_id_to_contiguous_id\"", "]", "[", "\n", "segment_info", "[", "\"category_id\"", "]", "\n", "]", "\n", "", "else", ":", "\n", "            ", "segment_info", "[", "\"category_id\"", "]", "=", "meta", "[", "\"stuff_dataset_id_to_contiguous_id\"", "]", "[", "\n", "segment_info", "[", "\"category_id\"", "]", "\n", "]", "\n", "", "return", "segment_info", "\n", "\n", "", "assert", "os", ".", "path", ".", "exists", "(", "\n", "gt_json", "\n", ")", ",", "\"Please run `python cityscapesscripts/preparation/createPanopticImgs.py` to generate label files.\"", "# noqa", "\n", "with", "open", "(", "gt_json", ")", "as", "f", ":", "\n", "        ", "json_info", "=", "json", ".", "load", "(", "f", ")", "\n", "", "files", "=", "get_cityscapes_panoptic_files", "(", "image_dir", ",", "gt_dir", ",", "json_info", ")", "\n", "ret", "=", "[", "]", "\n", "for", "image_file", ",", "label_file", ",", "segments_info", "in", "files", ":", "\n", "        ", "sem_label_file", "=", "(", "\n", "image_file", ".", "replace", "(", "\"leftImg8bit\"", ",", "\"gtFine\"", ")", ".", "split", "(", "\".\"", ")", "[", "0", "]", "+", "\"_labelTrainIds.png\"", "\n", ")", "\n", "segments_info", "=", "[", "_convert_category_id", "(", "x", ",", "meta", ")", "for", "x", "in", "segments_info", "]", "\n", "ret", ".", "append", "(", "\n", "{", "\n", "\"file_name\"", ":", "image_file", ",", "\n", "\"image_id\"", ":", "\"_\"", ".", "join", "(", "\n", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "image_file", ")", ")", "[", "0", "]", ".", "split", "(", "\"_\"", ")", "[", ":", "3", "]", "\n", ")", ",", "\n", "\"sem_seg_file_name\"", ":", "sem_label_file", ",", "\n", "\"pan_seg_file_name\"", ":", "label_file", ",", "\n", "\"segments_info\"", ":", "segments_info", ",", "\n", "}", "\n", ")", "\n", "", "assert", "len", "(", "ret", ")", ",", "f\"No images found in {image_dir}!\"", "\n", "assert", "PathManager", ".", "isfile", "(", "\n", "ret", "[", "0", "]", "[", "\"sem_seg_file_name\"", "]", "\n", ")", ",", "\"Please generate labelTrainIds.png with cityscapesscripts/preparation/createTrainIdLabelImgs.py\"", "# noqa", "\n", "assert", "PathManager", ".", "isfile", "(", "\n", "ret", "[", "0", "]", "[", "\"pan_seg_file_name\"", "]", "\n", ")", ",", "\"Please generate panoptic annotation with python cityscapesscripts/preparation/createPanopticImgs.py\"", "# noqa", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes_panoptic.register_all_cityscapes_panoptic": [[127, 187], ["_RAW_CITYSCAPES_PANOPTIC_SPLITS.items", "os.path.join", "os.path.join", "os.path.join", "detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "cityscapes_panoptic.load_cityscapes_panoptic", "detectron2.data.MetadataCatalog.get", "os.path.join.replace"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes_panoptic.load_cityscapes_panoptic", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "register_all_cityscapes_panoptic", "(", "root", ")", ":", "\n", "    ", "meta", "=", "{", "}", "\n", "# The following metadata maps contiguous id from [0, #thing categories +", "\n", "# #stuff categories) to their names and colors. We have to replica of the", "\n", "# same name and color under \"thing_*\" and \"stuff_*\" because the current", "\n", "# visualization function in D2 handles thing and class classes differently", "\n", "# due to some heuristic used in Panoptic FPN. We keep the same naming to", "\n", "# enable reusing existing visualization functions.", "\n", "thing_classes", "=", "[", "k", "[", "\"name\"", "]", "for", "k", "in", "CITYSCAPES_CATEGORIES", "]", "\n", "thing_colors", "=", "[", "k", "[", "\"color\"", "]", "for", "k", "in", "CITYSCAPES_CATEGORIES", "]", "\n", "stuff_classes", "=", "[", "k", "[", "\"name\"", "]", "for", "k", "in", "CITYSCAPES_CATEGORIES", "]", "\n", "stuff_colors", "=", "[", "k", "[", "\"color\"", "]", "for", "k", "in", "CITYSCAPES_CATEGORIES", "]", "\n", "\n", "meta", "[", "\"thing_classes\"", "]", "=", "thing_classes", "\n", "meta", "[", "\"thing_colors\"", "]", "=", "thing_colors", "\n", "meta", "[", "\"stuff_classes\"", "]", "=", "stuff_classes", "\n", "meta", "[", "\"stuff_colors\"", "]", "=", "stuff_colors", "\n", "\n", "# There are three types of ids in cityscapes panoptic segmentation:", "\n", "# (1) category id: like semantic segmentation, it is the class id for each", "\n", "#   pixel. Since there are some classes not used in evaluation, the category", "\n", "#   id is not always contiguous and thus we have two set of category ids:", "\n", "#       - original category id: category id in the original dataset, mainly", "\n", "#           used for evaluation.", "\n", "#       - contiguous category id: [0, #classes), in order to train the classifier", "\n", "# (2) instance id: this id is used to differentiate different instances from", "\n", "#   the same category. For \"stuff\" classes, the instance id is always 0; for", "\n", "#   \"thing\" classes, the instance id starts from 1 and 0 is reserved for", "\n", "#   ignored instances (e.g. crowd annotation).", "\n", "# (3) panoptic id: this is the compact id that encode both category and", "\n", "#   instance id by: category_id * 1000 + instance_id.", "\n", "thing_dataset_id_to_contiguous_id", "=", "{", "}", "\n", "stuff_dataset_id_to_contiguous_id", "=", "{", "}", "\n", "\n", "for", "k", "in", "CITYSCAPES_CATEGORIES", ":", "\n", "        ", "if", "k", "[", "\"isthing\"", "]", "==", "1", ":", "\n", "            ", "thing_dataset_id_to_contiguous_id", "[", "k", "[", "\"id\"", "]", "]", "=", "k", "[", "\"trainId\"", "]", "\n", "", "else", ":", "\n", "            ", "stuff_dataset_id_to_contiguous_id", "[", "k", "[", "\"id\"", "]", "]", "=", "k", "[", "\"trainId\"", "]", "\n", "\n", "", "", "meta", "[", "\"thing_dataset_id_to_contiguous_id\"", "]", "=", "thing_dataset_id_to_contiguous_id", "\n", "meta", "[", "\"stuff_dataset_id_to_contiguous_id\"", "]", "=", "stuff_dataset_id_to_contiguous_id", "\n", "\n", "for", "key", ",", "(", "image_dir", ",", "gt_dir", ",", "gt_json", ")", "in", "_RAW_CITYSCAPES_PANOPTIC_SPLITS", ".", "items", "(", ")", ":", "\n", "        ", "image_dir", "=", "os", ".", "path", ".", "join", "(", "root", ",", "image_dir", ")", "\n", "gt_dir", "=", "os", ".", "path", ".", "join", "(", "root", ",", "gt_dir", ")", "\n", "gt_json", "=", "os", ".", "path", ".", "join", "(", "root", ",", "gt_json", ")", "\n", "\n", "DatasetCatalog", ".", "register", "(", "\n", "key", ",", "lambda", "x", "=", "image_dir", ",", "y", "=", "gt_dir", ",", "z", "=", "gt_json", ":", "load_cityscapes_panoptic", "(", "x", ",", "y", ",", "z", ",", "meta", ")", "\n", ")", "\n", "MetadataCatalog", ".", "get", "(", "key", ")", ".", "set", "(", "\n", "panoptic_root", "=", "gt_dir", ",", "\n", "image_root", "=", "image_dir", ",", "\n", "panoptic_json", "=", "gt_json", ",", "\n", "gt_dir", "=", "gt_dir", ".", "replace", "(", "\"cityscapes_panoptic_\"", ",", "\"\"", ")", ",", "\n", "evaluator_type", "=", "\"cityscapes_panoptic_seg\"", ",", "\n", "ignore_label", "=", "255", ",", "\n", "label_divisor", "=", "1000", ",", "\n", "**", "meta", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.load_coco_json": [[30, 228], ["fvcore.common.timer.Timer", "detectron2.utils.file_io.PathManager.get_local_path", "sorted", "COCO.loadImgs", "sum", "len", "list", "logger.info", "contextlib.redirect_stdout", "COCO", "fvcore.common.timer.Timer.seconds", "logger.info", "MetadataCatalog.get", "sorted", "COCO.loadCats", "COCO.imgs.keys", "logger.warning", "zip", "os.path.join", "dataset_dicts.append", "logger.warning", "io.StringIO", "COCO.getCatIds", "len", "len", "len", "len", "anno.get", "anno.get", "objs.append", "fvcore.common.timer.Timer.seconds", "sorted", "logger.warning", "enumerate", "set", "anno.get", "ValueError", "isinstance", "enumerate", "min", "max", "len", "len", "isinstance", "pycocotools.frPyObjects", "len", "KeyError", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "load_coco_json", "(", "json_file", ",", "image_root", ",", "dataset_name", "=", "None", ",", "extra_annotation_keys", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Load a json file with COCO's instances annotation format.\n    Currently supports instance detection, instance segmentation,\n    and person keypoints annotations.\n\n    Args:\n        json_file (str): full path to the json file in COCO instances annotation format.\n        image_root (str or path-like): the directory where the images in this json file exists.\n        dataset_name (str or None): the name of the dataset (e.g., coco_2017_train).\n            When provided, this function will also do the following:\n\n            * Put \"thing_classes\" into the metadata associated with this dataset.\n            * Map the category ids into a contiguous range (needed by standard dataset format),\n              and add \"thing_dataset_id_to_contiguous_id\" to the metadata associated\n              with this dataset.\n\n            This option should usually be provided, unless users need to load\n            the original json content and apply more processing manually.\n        extra_annotation_keys (list[str]): list of per-annotation keys that should also be\n            loaded into the dataset dict (besides \"iscrowd\", \"bbox\", \"keypoints\",\n            \"category_id\", \"segmentation\"). The values for these keys will be returned as-is.\n            For example, the densepose annotations are loaded in this way.\n\n    Returns:\n        list[dict]: a list of dicts in Detectron2 standard dataset dicts format (See\n        `Using Custom Datasets </tutorials/datasets.html>`_ ) when `dataset_name` is not None.\n        If `dataset_name` is None, the returned `category_ids` may be\n        incontiguous and may not conform to the Detectron2 standard format.\n\n    Notes:\n        1. This function does not read the image files.\n           The results do not have the \"image\" field.\n    \"\"\"", "\n", "from", "pycocotools", ".", "coco", "import", "COCO", "\n", "\n", "timer", "=", "Timer", "(", ")", "\n", "json_file", "=", "PathManager", ".", "get_local_path", "(", "json_file", ")", "\n", "with", "contextlib", ".", "redirect_stdout", "(", "io", ".", "StringIO", "(", ")", ")", ":", "\n", "        ", "coco_api", "=", "COCO", "(", "json_file", ")", "\n", "", "if", "timer", ".", "seconds", "(", ")", ">", "1", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading {} takes {:.2f} seconds.\"", ".", "format", "(", "json_file", ",", "timer", ".", "seconds", "(", ")", ")", ")", "\n", "\n", "", "id_map", "=", "None", "\n", "if", "dataset_name", "is", "not", "None", ":", "\n", "        ", "meta", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", "\n", "cat_ids", "=", "sorted", "(", "coco_api", ".", "getCatIds", "(", ")", ")", "\n", "cats", "=", "coco_api", ".", "loadCats", "(", "cat_ids", ")", "\n", "# The categories in a custom json file may not be sorted.", "\n", "thing_classes", "=", "[", "c", "[", "\"name\"", "]", "for", "c", "in", "sorted", "(", "cats", ",", "key", "=", "lambda", "x", ":", "x", "[", "\"id\"", "]", ")", "]", "\n", "meta", ".", "thing_classes", "=", "thing_classes", "\n", "\n", "# In COCO, certain category ids are artificially removed,", "\n", "# and by convention they are always ignored.", "\n", "# We deal with COCO's id issue and translate", "\n", "# the category ids to contiguous ids in [0, 80).", "\n", "\n", "# It works by looking at the \"categories\" field in the json, therefore", "\n", "# if users' own json also have incontiguous ids, we'll", "\n", "# apply this mapping as well but print a warning.", "\n", "if", "not", "(", "min", "(", "cat_ids", ")", "==", "1", "and", "max", "(", "cat_ids", ")", "==", "len", "(", "cat_ids", ")", ")", ":", "\n", "            ", "if", "\"coco\"", "not", "in", "dataset_name", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"\"\"\nCategory ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n\"\"\"", "\n", ")", "\n", "", "", "id_map", "=", "{", "v", ":", "i", "for", "i", ",", "v", "in", "enumerate", "(", "cat_ids", ")", "}", "\n", "meta", ".", "thing_dataset_id_to_contiguous_id", "=", "id_map", "\n", "\n", "# sort indices for reproducible results", "\n", "", "img_ids", "=", "sorted", "(", "coco_api", ".", "imgs", ".", "keys", "(", ")", ")", "\n", "# imgs is a list of dicts, each looks something like:", "\n", "# {'license': 4,", "\n", "#  'url': 'http://farm6.staticflickr.com/5454/9413846304_881d5e5c3b_z.jpg',", "\n", "#  'file_name': 'COCO_val2014_000000001268.jpg',", "\n", "#  'height': 427,", "\n", "#  'width': 640,", "\n", "#  'date_captured': '2013-11-17 05:57:24',", "\n", "#  'id': 1268}", "\n", "imgs", "=", "coco_api", ".", "loadImgs", "(", "img_ids", ")", "\n", "# anns is a list[list[dict]], where each dict is an annotation", "\n", "# record for an object. The inner list enumerates the objects in an image", "\n", "# and the outer list enumerates over images. Example of anns[0]:", "\n", "# [{'segmentation': [[192.81,", "\n", "#     247.09,", "\n", "#     ...", "\n", "#     219.03,", "\n", "#     249.06]],", "\n", "#   'area': 1035.749,", "\n", "#   'iscrowd': 0,", "\n", "#   'image_id': 1268,", "\n", "#   'bbox': [192.81, 224.8, 74.73, 33.43],", "\n", "#   'category_id': 16,", "\n", "#   'id': 42986},", "\n", "#  ...]", "\n", "anns", "=", "[", "coco_api", ".", "imgToAnns", "[", "img_id", "]", "for", "img_id", "in", "img_ids", "]", "\n", "total_num_valid_anns", "=", "sum", "(", "[", "len", "(", "x", ")", "for", "x", "in", "anns", "]", ")", "\n", "total_num_anns", "=", "len", "(", "coco_api", ".", "anns", ")", "\n", "if", "total_num_valid_anns", "<", "total_num_anns", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "f\"{json_file} contains {total_num_anns} annotations, but only \"", "\n", "f\"{total_num_valid_anns} of them match to images in the file.\"", "\n", ")", "\n", "\n", "", "if", "\"minival\"", "not", "in", "json_file", ":", "\n", "# The popular valminusminival & minival annotations for COCO2014 contain this bug.", "\n", "# However the ratio of buggy annotations there is tiny and does not affect accuracy.", "\n", "# Therefore we explicitly white-list them.", "\n", "        ", "ann_ids", "=", "[", "ann", "[", "\"id\"", "]", "for", "anns_per_image", "in", "anns", "for", "ann", "in", "anns_per_image", "]", "\n", "assert", "len", "(", "set", "(", "ann_ids", ")", ")", "==", "len", "(", "ann_ids", ")", ",", "\"Annotation ids in '{}' are not unique!\"", ".", "format", "(", "\n", "json_file", "\n", ")", "\n", "\n", "", "imgs_anns", "=", "list", "(", "zip", "(", "imgs", ",", "anns", ")", ")", "\n", "logger", ".", "info", "(", "\"Loaded {} images in COCO format from {}\"", ".", "format", "(", "len", "(", "imgs_anns", ")", ",", "json_file", ")", ")", "\n", "\n", "dataset_dicts", "=", "[", "]", "\n", "\n", "ann_keys", "=", "[", "\"iscrowd\"", ",", "\"bbox\"", ",", "\"keypoints\"", ",", "\"category_id\"", "]", "+", "(", "extra_annotation_keys", "or", "[", "]", ")", "\n", "\n", "num_instances_without_valid_segmentation", "=", "0", "\n", "\n", "for", "(", "img_dict", ",", "anno_dict_list", ")", "in", "imgs_anns", ":", "\n", "        ", "record", "=", "{", "}", "\n", "record", "[", "\"file_name\"", "]", "=", "os", ".", "path", ".", "join", "(", "image_root", ",", "img_dict", "[", "\"file_name\"", "]", ")", "\n", "record", "[", "\"height\"", "]", "=", "img_dict", "[", "\"height\"", "]", "\n", "record", "[", "\"width\"", "]", "=", "img_dict", "[", "\"width\"", "]", "\n", "image_id", "=", "record", "[", "\"image_id\"", "]", "=", "img_dict", "[", "\"id\"", "]", "\n", "\n", "objs", "=", "[", "]", "\n", "for", "anno", "in", "anno_dict_list", ":", "\n", "# Check that the image_id in this annotation is the same as", "\n", "# the image_id we're looking at.", "\n", "# This fails only when the data parsing logic or the annotation file is buggy.", "\n", "\n", "# The original COCO valminusminival2014 & minival2014 annotation files", "\n", "# actually contains bugs that, together with certain ways of using COCO API,", "\n", "# can trigger this assertion.", "\n", "            ", "assert", "anno", "[", "\"image_id\"", "]", "==", "image_id", "\n", "\n", "assert", "anno", ".", "get", "(", "\"ignore\"", ",", "0", ")", "==", "0", ",", "'\"ignore\" in COCO json file is not supported.'", "\n", "\n", "obj", "=", "{", "key", ":", "anno", "[", "key", "]", "for", "key", "in", "ann_keys", "if", "key", "in", "anno", "}", "\n", "if", "\"bbox\"", "in", "obj", "and", "len", "(", "obj", "[", "\"bbox\"", "]", ")", "==", "0", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"One annotation of image {image_id} contains empty 'bbox' value! \"", "\n", "\"This json does not have valid COCO format.\"", "\n", ")", "\n", "\n", "", "segm", "=", "anno", ".", "get", "(", "\"segmentation\"", ",", "None", ")", "\n", "if", "segm", ":", "# either list[list[float]] or dict(RLE)", "\n", "                ", "if", "isinstance", "(", "segm", ",", "dict", ")", ":", "\n", "                    ", "if", "isinstance", "(", "segm", "[", "\"counts\"", "]", ",", "list", ")", ":", "\n", "# convert to compressed RLE", "\n", "                        ", "segm", "=", "mask_util", ".", "frPyObjects", "(", "segm", ",", "*", "segm", "[", "\"size\"", "]", ")", "\n", "", "", "else", ":", "\n", "# filter out invalid polygons (< 3 points)", "\n", "                    ", "segm", "=", "[", "poly", "for", "poly", "in", "segm", "if", "len", "(", "poly", ")", "%", "2", "==", "0", "and", "len", "(", "poly", ")", ">=", "6", "]", "\n", "if", "len", "(", "segm", ")", "==", "0", ":", "\n", "                        ", "num_instances_without_valid_segmentation", "+=", "1", "\n", "continue", "# ignore this instance", "\n", "", "", "obj", "[", "\"segmentation\"", "]", "=", "segm", "\n", "\n", "", "keypts", "=", "anno", ".", "get", "(", "\"keypoints\"", ",", "None", ")", "\n", "if", "keypts", ":", "# list[int]", "\n", "                ", "for", "idx", ",", "v", "in", "enumerate", "(", "keypts", ")", ":", "\n", "                    ", "if", "idx", "%", "3", "!=", "2", ":", "\n", "# COCO's segmentation coordinates are floating points in [0, H or W],", "\n", "# but keypoint coordinates are integers in [0, H-1 or W-1]", "\n", "# Therefore we assume the coordinates are \"pixel indices\" and", "\n", "# add 0.5 to convert to floating point coordinates.", "\n", "                        ", "keypts", "[", "idx", "]", "=", "v", "+", "0.5", "\n", "", "", "obj", "[", "\"keypoints\"", "]", "=", "keypts", "\n", "\n", "", "obj", "[", "\"bbox_mode\"", "]", "=", "BoxMode", ".", "XYWH_ABS", "\n", "if", "id_map", ":", "\n", "                ", "annotation_category_id", "=", "obj", "[", "\"category_id\"", "]", "\n", "try", ":", "\n", "                    ", "obj", "[", "\"category_id\"", "]", "=", "id_map", "[", "annotation_category_id", "]", "\n", "", "except", "KeyError", "as", "e", ":", "\n", "                    ", "raise", "KeyError", "(", "\n", "f\"Encountered category_id={annotation_category_id} \"", "\n", "\"but this id does not exist in 'categories' of the json file.\"", "\n", ")", "from", "e", "\n", "", "", "objs", ".", "append", "(", "obj", ")", "\n", "", "record", "[", "\"annotations\"", "]", "=", "objs", "\n", "dataset_dicts", ".", "append", "(", "record", ")", "\n", "\n", "", "if", "num_instances_without_valid_segmentation", ">", "0", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"Filtered out {} instances without valid segmentation. \"", ".", "format", "(", "\n", "num_instances_without_valid_segmentation", "\n", ")", "\n", "+", "\"There might be issues in your dataset generation process.  Please \"", "\n", "\"check https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html carefully\"", "\n", ")", "\n", "", "return", "dataset_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.load_sem_seg": [[230, 304], ["sorted", "sorted", "logger.info", "zip", "os.path.normpath", "len", "len", "len", "logger.warn", "list", "sorted", "logger.warn", "dataset_dicts.append", "os.path.relpath", "os.path.splitext", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "detectron2.utils.file_io.PathManager.ls", "f.endswith", "coco.load_sem_seg.file2id"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "load_sem_seg", "(", "gt_root", ",", "image_root", ",", "gt_ext", "=", "\"png\"", ",", "image_ext", "=", "\"jpg\"", ")", ":", "\n", "    ", "\"\"\"\n    Load semantic segmentation datasets. All files under \"gt_root\" with \"gt_ext\" extension are\n    treated as ground truth annotations and all files under \"image_root\" with \"image_ext\" extension\n    as input images. Ground truth and input images are matched using file paths relative to\n    \"gt_root\" and \"image_root\" respectively without taking into account file extensions.\n    This works for COCO as well as some other datasets.\n\n    Args:\n        gt_root (str): full path to ground truth semantic segmentation files. Semantic segmentation\n            annotations are stored as images with integer values in pixels that represent\n            corresponding semantic labels.\n        image_root (str): the directory where the input images are.\n        gt_ext (str): file extension for ground truth annotations.\n        image_ext (str): file extension for input images.\n\n    Returns:\n        list[dict]:\n            a list of dicts in detectron2 standard format without instance-level\n            annotation.\n\n    Notes:\n        1. This function does not read the image and ground truth files.\n           The results do not have the \"image\" and \"sem_seg\" fields.\n    \"\"\"", "\n", "\n", "# We match input images with ground truth based on their relative filepaths (without file", "\n", "# extensions) starting from 'image_root' and 'gt_root' respectively.", "\n", "def", "file2id", "(", "folder_path", ",", "file_path", ")", ":", "\n", "# extract relative path starting from `folder_path`", "\n", "        ", "image_id", "=", "os", ".", "path", ".", "normpath", "(", "os", ".", "path", ".", "relpath", "(", "file_path", ",", "start", "=", "folder_path", ")", ")", "\n", "# remove file extension", "\n", "image_id", "=", "os", ".", "path", ".", "splitext", "(", "image_id", ")", "[", "0", "]", "\n", "return", "image_id", "\n", "\n", "", "input_files", "=", "sorted", "(", "\n", "(", "os", ".", "path", ".", "join", "(", "image_root", ",", "f", ")", "for", "f", "in", "PathManager", ".", "ls", "(", "image_root", ")", "if", "f", ".", "endswith", "(", "image_ext", ")", ")", ",", "\n", "key", "=", "lambda", "file_path", ":", "file2id", "(", "image_root", ",", "file_path", ")", ",", "\n", ")", "\n", "gt_files", "=", "sorted", "(", "\n", "(", "os", ".", "path", ".", "join", "(", "gt_root", ",", "f", ")", "for", "f", "in", "PathManager", ".", "ls", "(", "gt_root", ")", "if", "f", ".", "endswith", "(", "gt_ext", ")", ")", ",", "\n", "key", "=", "lambda", "file_path", ":", "file2id", "(", "gt_root", ",", "file_path", ")", ",", "\n", ")", "\n", "\n", "assert", "len", "(", "gt_files", ")", ">", "0", ",", "\"No annotations found in {}.\"", ".", "format", "(", "gt_root", ")", "\n", "\n", "# Use the intersection, so that val2017_100 annotations can run smoothly with val2017 images", "\n", "if", "len", "(", "input_files", ")", "!=", "len", "(", "gt_files", ")", ":", "\n", "        ", "logger", ".", "warn", "(", "\n", "\"Directory {} and {} has {} and {} files, respectively.\"", ".", "format", "(", "\n", "image_root", ",", "gt_root", ",", "len", "(", "input_files", ")", ",", "len", "(", "gt_files", ")", "\n", ")", "\n", ")", "\n", "input_basenames", "=", "[", "os", ".", "path", ".", "basename", "(", "f", ")", "[", ":", "-", "len", "(", "image_ext", ")", "]", "for", "f", "in", "input_files", "]", "\n", "gt_basenames", "=", "[", "os", ".", "path", ".", "basename", "(", "f", ")", "[", ":", "-", "len", "(", "gt_ext", ")", "]", "for", "f", "in", "gt_files", "]", "\n", "intersect", "=", "list", "(", "set", "(", "input_basenames", ")", "&", "set", "(", "gt_basenames", ")", ")", "\n", "# sort, otherwise each worker may obtain a list[dict] in different order", "\n", "intersect", "=", "sorted", "(", "intersect", ")", "\n", "logger", ".", "warn", "(", "\"Will use their intersection of {} files.\"", ".", "format", "(", "len", "(", "intersect", ")", ")", ")", "\n", "input_files", "=", "[", "os", ".", "path", ".", "join", "(", "image_root", ",", "f", "+", "image_ext", ")", "for", "f", "in", "intersect", "]", "\n", "gt_files", "=", "[", "os", ".", "path", ".", "join", "(", "gt_root", ",", "f", "+", "gt_ext", ")", "for", "f", "in", "intersect", "]", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"Loaded {} images with semantic segmentation from {}\"", ".", "format", "(", "len", "(", "input_files", ")", ",", "image_root", ")", "\n", ")", "\n", "\n", "dataset_dicts", "=", "[", "]", "\n", "for", "(", "img_path", ",", "gt_path", ")", "in", "zip", "(", "input_files", ",", "gt_files", ")", ":", "\n", "        ", "record", "=", "{", "}", "\n", "record", "[", "\"file_name\"", "]", "=", "img_path", "\n", "record", "[", "\"sem_seg_file_name\"", "]", "=", "gt_path", "\n", "dataset_dicts", ".", "append", "(", "record", ")", "\n", "\n", "", "return", "dataset_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.convert_to_coco_dict": [[306, 443], ["DatasetCatalog.get", "MetadataCatalog.get", "hasattr", "logger.info", "enumerate", "logger.info", "coco_images.append", "image_dict.get", "str", "len", "reverse_id_mapper", "enumerate", "image_dict.get", "int", "int", "str", "isinstance", "detectron2.structures.BoxMode.convert", "float", "int", "int", "coco_annotations.append", "datetime.datetime.now", "MetadataCatalog.get.thing_dataset_id_to_contiguous_id.items", "bbox.tolist.tolist", "len", "ValueError", "isinstance", "enumerate", "len", "round", "annotation.get", "reverse_id_mapper", "isinstance", "len", "len", "ValueError", "len", "detectron2.structures.PolygonMasks", "[].item", "isinstance", "detectron2.structures.BoxMode.convert", "[].item", "[].item", "sum", "float", "pycocotools.area().item", "TypeError", "isinstance", "counts.decode", "detectron2.structures.PolygonMasks.area", "pycocotools.area", "detectron2.structures.Boxes().area", "detectron2.structures.RotatedBoxes().area", "type", "detectron2.structures.Boxes", "detectron2.structures.RotatedBoxes"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.area"], ["", "def", "convert_to_coco_dict", "(", "dataset_name", ")", ":", "\n", "    ", "\"\"\"\n    Convert an instance detection/segmentation or keypoint detection dataset\n    in detectron2's standard format into COCO json format.\n\n    Generic dataset description can be found here:\n    https://detectron2.readthedocs.io/tutorials/datasets.html#register-a-dataset\n\n    COCO data format description can be found here:\n    http://cocodataset.org/#format-data\n\n    Args:\n        dataset_name (str):\n            name of the source dataset\n            Must be registered in DatastCatalog and in detectron2's standard format.\n            Must have corresponding metadata \"thing_classes\"\n    Returns:\n        coco_dict: serializable dict in COCO json format\n    \"\"\"", "\n", "\n", "dataset_dicts", "=", "DatasetCatalog", ".", "get", "(", "dataset_name", ")", "\n", "metadata", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", "\n", "\n", "# unmap the category mapping ids for COCO", "\n", "if", "hasattr", "(", "metadata", ",", "\"thing_dataset_id_to_contiguous_id\"", ")", ":", "\n", "        ", "reverse_id_mapping", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "metadata", ".", "thing_dataset_id_to_contiguous_id", ".", "items", "(", ")", "}", "\n", "reverse_id_mapper", "=", "lambda", "contiguous_id", ":", "reverse_id_mapping", "[", "contiguous_id", "]", "# noqa", "\n", "", "else", ":", "\n", "        ", "reverse_id_mapper", "=", "lambda", "contiguous_id", ":", "contiguous_id", "# noqa", "\n", "\n", "", "categories", "=", "[", "\n", "{", "\"id\"", ":", "reverse_id_mapper", "(", "id", ")", ",", "\"name\"", ":", "name", "}", "\n", "for", "id", ",", "name", "in", "enumerate", "(", "metadata", ".", "thing_classes", ")", "\n", "]", "\n", "\n", "logger", ".", "info", "(", "\"Converting dataset dicts into COCO format\"", ")", "\n", "coco_images", "=", "[", "]", "\n", "coco_annotations", "=", "[", "]", "\n", "\n", "for", "image_id", ",", "image_dict", "in", "enumerate", "(", "dataset_dicts", ")", ":", "\n", "        ", "coco_image", "=", "{", "\n", "\"id\"", ":", "image_dict", ".", "get", "(", "\"image_id\"", ",", "image_id", ")", ",", "\n", "\"width\"", ":", "int", "(", "image_dict", "[", "\"width\"", "]", ")", ",", "\n", "\"height\"", ":", "int", "(", "image_dict", "[", "\"height\"", "]", ")", ",", "\n", "\"file_name\"", ":", "str", "(", "image_dict", "[", "\"file_name\"", "]", ")", ",", "\n", "}", "\n", "coco_images", ".", "append", "(", "coco_image", ")", "\n", "\n", "anns_per_image", "=", "image_dict", ".", "get", "(", "\"annotations\"", ",", "[", "]", ")", "\n", "for", "annotation", "in", "anns_per_image", ":", "\n", "# create a new dict with only COCO fields", "\n", "            ", "coco_annotation", "=", "{", "}", "\n", "\n", "# COCO requirement: XYWH box format for axis-align and XYWHA for rotated", "\n", "bbox", "=", "annotation", "[", "\"bbox\"", "]", "\n", "if", "isinstance", "(", "bbox", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "if", "bbox", ".", "ndim", "!=", "1", ":", "\n", "                    ", "raise", "ValueError", "(", "f\"bbox has to be 1-dimensional. Got shape={bbox.shape}.\"", ")", "\n", "", "bbox", "=", "bbox", ".", "tolist", "(", ")", "\n", "", "if", "len", "(", "bbox", ")", "not", "in", "[", "4", ",", "5", "]", ":", "\n", "                ", "raise", "ValueError", "(", "f\"bbox has to has length 4 or 5. Got {bbox}.\"", ")", "\n", "", "from_bbox_mode", "=", "annotation", "[", "\"bbox_mode\"", "]", "\n", "to_bbox_mode", "=", "BoxMode", ".", "XYWH_ABS", "if", "len", "(", "bbox", ")", "==", "4", "else", "BoxMode", ".", "XYWHA_ABS", "\n", "bbox", "=", "BoxMode", ".", "convert", "(", "bbox", ",", "from_bbox_mode", ",", "to_bbox_mode", ")", "\n", "\n", "# COCO requirement: instance area", "\n", "if", "\"segmentation\"", "in", "annotation", ":", "\n", "# Computing areas for instances by counting the pixels", "\n", "                ", "segmentation", "=", "annotation", "[", "\"segmentation\"", "]", "\n", "# TODO: check segmentation type: RLE, BinaryMask or Polygon", "\n", "if", "isinstance", "(", "segmentation", ",", "list", ")", ":", "\n", "                    ", "polygons", "=", "PolygonMasks", "(", "[", "segmentation", "]", ")", "\n", "area", "=", "polygons", ".", "area", "(", ")", "[", "0", "]", ".", "item", "(", ")", "\n", "", "elif", "isinstance", "(", "segmentation", ",", "dict", ")", ":", "# RLE", "\n", "                    ", "area", "=", "mask_util", ".", "area", "(", "segmentation", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "TypeError", "(", "f\"Unknown segmentation type {type(segmentation)}!\"", ")", "\n", "", "", "else", ":", "\n", "# Computing areas using bounding boxes", "\n", "                ", "if", "to_bbox_mode", "==", "BoxMode", ".", "XYWH_ABS", ":", "\n", "                    ", "bbox_xy", "=", "BoxMode", ".", "convert", "(", "bbox", ",", "to_bbox_mode", ",", "BoxMode", ".", "XYXY_ABS", ")", "\n", "area", "=", "Boxes", "(", "[", "bbox_xy", "]", ")", ".", "area", "(", ")", "[", "0", "]", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                    ", "area", "=", "RotatedBoxes", "(", "[", "bbox", "]", ")", ".", "area", "(", ")", "[", "0", "]", ".", "item", "(", ")", "\n", "\n", "", "", "if", "\"keypoints\"", "in", "annotation", ":", "\n", "                ", "keypoints", "=", "annotation", "[", "\"keypoints\"", "]", "# list[int]", "\n", "for", "idx", ",", "v", "in", "enumerate", "(", "keypoints", ")", ":", "\n", "                    ", "if", "idx", "%", "3", "!=", "2", ":", "\n", "# COCO's segmentation coordinates are floating points in [0, H or W],", "\n", "# but keypoint coordinates are integers in [0, H-1 or W-1]", "\n", "# For COCO format consistency we substract 0.5", "\n", "# https://github.com/facebookresearch/detectron2/pull/175#issuecomment-551202163", "\n", "                        ", "keypoints", "[", "idx", "]", "=", "v", "-", "0.5", "\n", "", "", "if", "\"num_keypoints\"", "in", "annotation", ":", "\n", "                    ", "num_keypoints", "=", "annotation", "[", "\"num_keypoints\"", "]", "\n", "", "else", ":", "\n", "                    ", "num_keypoints", "=", "sum", "(", "kp", ">", "0", "for", "kp", "in", "keypoints", "[", "2", ":", ":", "3", "]", ")", "\n", "\n", "# COCO requirement:", "\n", "#   linking annotations to images", "\n", "#   \"id\" field must start with 1", "\n", "", "", "coco_annotation", "[", "\"id\"", "]", "=", "len", "(", "coco_annotations", ")", "+", "1", "\n", "coco_annotation", "[", "\"image_id\"", "]", "=", "coco_image", "[", "\"id\"", "]", "\n", "coco_annotation", "[", "\"bbox\"", "]", "=", "[", "round", "(", "float", "(", "x", ")", ",", "3", ")", "for", "x", "in", "bbox", "]", "\n", "coco_annotation", "[", "\"area\"", "]", "=", "float", "(", "area", ")", "\n", "coco_annotation", "[", "\"iscrowd\"", "]", "=", "int", "(", "annotation", ".", "get", "(", "\"iscrowd\"", ",", "0", ")", ")", "\n", "coco_annotation", "[", "\"category_id\"", "]", "=", "int", "(", "reverse_id_mapper", "(", "annotation", "[", "\"category_id\"", "]", ")", ")", "\n", "\n", "# Add optional fields", "\n", "if", "\"keypoints\"", "in", "annotation", ":", "\n", "                ", "coco_annotation", "[", "\"keypoints\"", "]", "=", "keypoints", "\n", "coco_annotation", "[", "\"num_keypoints\"", "]", "=", "num_keypoints", "\n", "\n", "", "if", "\"segmentation\"", "in", "annotation", ":", "\n", "                ", "seg", "=", "coco_annotation", "[", "\"segmentation\"", "]", "=", "annotation", "[", "\"segmentation\"", "]", "\n", "if", "isinstance", "(", "seg", ",", "dict", ")", ":", "# RLE", "\n", "                    ", "counts", "=", "seg", "[", "\"counts\"", "]", "\n", "if", "not", "isinstance", "(", "counts", ",", "str", ")", ":", "\n", "# make it json-serializable", "\n", "                        ", "seg", "[", "\"counts\"", "]", "=", "counts", ".", "decode", "(", "\"ascii\"", ")", "\n", "\n", "", "", "", "coco_annotations", ".", "append", "(", "coco_annotation", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\n", "\"Conversion finished, \"", "\n", "f\"#images: {len(coco_images)}, #annotations: {len(coco_annotations)}\"", "\n", ")", "\n", "\n", "info", "=", "{", "\n", "\"date_created\"", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", ",", "\n", "\"description\"", ":", "\"Automatically generated COCO json file for Detectron2.\"", ",", "\n", "}", "\n", "coco_dict", "=", "{", "\"info\"", ":", "info", ",", "\"images\"", ":", "coco_images", ",", "\"categories\"", ":", "categories", ",", "\"licenses\"", ":", "None", "}", "\n", "if", "len", "(", "coco_annotations", ")", ">", "0", ":", "\n", "        ", "coco_dict", "[", "\"annotations\"", "]", "=", "coco_annotations", "\n", "", "return", "coco_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.convert_to_coco_json": [[445, 477], ["detectron2.utils.file_io.PathManager.mkdirs", "os.path.dirname", "iopath.common.file_io.file_lock", "detectron2.utils.file_io.PathManager.exists", "logger.warning", "logger.info", "coco.convert_to_coco_dict", "logger.info", "shutil.move", "detectron2.utils.file_io.PathManager.open", "json.dump"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.convert_to_coco_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump"], ["", "def", "convert_to_coco_json", "(", "dataset_name", ",", "output_file", ",", "allow_cached", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Converts dataset into COCO format and saves it to a json file.\n    dataset_name must be registered in DatasetCatalog and in detectron2's standard format.\n\n    Args:\n        dataset_name:\n            reference from the config file to the catalogs\n            must be registered in DatasetCatalog and in detectron2's standard format\n        output_file: path of json file that will be saved to\n        allow_cached: if json file is already present then skip conversion\n    \"\"\"", "\n", "\n", "# TODO: The dataset or the conversion script *may* change,", "\n", "# a checksum would be useful for validating the cached data", "\n", "\n", "PathManager", ".", "mkdirs", "(", "os", ".", "path", ".", "dirname", "(", "output_file", ")", ")", "\n", "with", "file_lock", "(", "output_file", ")", ":", "\n", "        ", "if", "PathManager", ".", "exists", "(", "output_file", ")", "and", "allow_cached", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Using previously cached COCO format annotations at '{output_file}'. \"", "\n", "\"You need to clear the cache file if your dataset has been modified.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "f\"Converting annotations of dataset '{dataset_name}' to COCO format ...)\"", ")", "\n", "coco_dict", "=", "convert_to_coco_dict", "(", "dataset_name", ")", "\n", "\n", "logger", ".", "info", "(", "f\"Caching COCO format annotations at '{output_file}' ...\"", ")", "\n", "tmp_file", "=", "output_file", "+", "\".tmp\"", "\n", "with", "PathManager", ".", "open", "(", "tmp_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "json", ".", "dump", "(", "coco_dict", ",", "f", ")", "\n", "", "shutil", ".", "move", "(", "tmp_file", ",", "output_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.register_coco_instances": [[479, 506], ["isinstance", "isinstance", "isinstance", "DatasetCatalog.register", "MetadataCatalog.get().set", "coco.load_coco_json", "MetadataCatalog.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.load_coco_json", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "", "def", "register_coco_instances", "(", "name", ",", "metadata", ",", "json_file", ",", "image_root", ")", ":", "\n", "    ", "\"\"\"\n    Register a dataset in COCO's json annotation format for\n    instance detection, instance segmentation and keypoint detection.\n    (i.e., Type 1 and 2 in http://cocodataset.org/#format-data.\n    `instances*.json` and `person_keypoints*.json` in the dataset).\n\n    This is an example of how to register a new dataset.\n    You can do something similar to this function, to register new datasets.\n\n    Args:\n        name (str): the name that identifies a dataset, e.g. \"coco_2014_train\".\n        metadata (dict): extra metadata associated with this dataset.  You can\n            leave it as an empty dict.\n        json_file (str): path to the json instance annotation file.\n        image_root (str or path-like): directory which contains all the images.\n    \"\"\"", "\n", "assert", "isinstance", "(", "name", ",", "str", ")", ",", "name", "\n", "assert", "isinstance", "(", "json_file", ",", "(", "str", ",", "os", ".", "PathLike", ")", ")", ",", "json_file", "\n", "assert", "isinstance", "(", "image_root", ",", "(", "str", ",", "os", ".", "PathLike", ")", ")", ",", "image_root", "\n", "# 1. register a function which returns dicts", "\n", "DatasetCatalog", ".", "register", "(", "name", ",", "lambda", ":", "load_coco_json", "(", "json_file", ",", "image_root", ",", "name", ")", ")", "\n", "\n", "# 2. Optionally, add metadata about this dataset,", "\n", "# since they might be useful in evaluation, visualization or logging", "\n", "MetadataCatalog", ".", "get", "(", "name", ")", ".", "set", "(", "\n", "json_file", "=", "json_file", ",", "image_root", "=", "image_root", ",", "evaluator_type", "=", "\"coco\"", ",", "**", "metadata", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco_panoptic.load_coco_panoptic_json": [[14, 64], ["len", "detectron2.utils.file_io.PathManager.isfile", "detectron2.utils.file_io.PathManager.isfile", "detectron2.utils.file_io.PathManager.open", "json.load", "int", "os.path.join", "os.path.join", "ret.append", "coco_panoptic.load_coco_panoptic_json._convert_category_id"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation.COCOPanopticEvaluator._convert_category_id"], ["def", "load_coco_panoptic_json", "(", "json_file", ",", "image_dir", ",", "gt_dir", ",", "meta", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        image_dir (str): path to the raw dataset. e.g., \"~/coco/train2017\".\n        gt_dir (str): path to the raw annotations. e.g., \"~/coco/panoptic_train2017\".\n        json_file (str): path to the json file. e.g., \"~/coco/annotations/panoptic_train2017.json\".\n\n    Returns:\n        list[dict]: a list of dicts in Detectron2 standard format. (See\n        `Using Custom Datasets </tutorials/datasets.html>`_ )\n    \"\"\"", "\n", "\n", "def", "_convert_category_id", "(", "segment_info", ",", "meta", ")", ":", "\n", "        ", "if", "segment_info", "[", "\"category_id\"", "]", "in", "meta", "[", "\"thing_dataset_id_to_contiguous_id\"", "]", ":", "\n", "            ", "segment_info", "[", "\"category_id\"", "]", "=", "meta", "[", "\"thing_dataset_id_to_contiguous_id\"", "]", "[", "\n", "segment_info", "[", "\"category_id\"", "]", "\n", "]", "\n", "segment_info", "[", "\"isthing\"", "]", "=", "True", "\n", "", "else", ":", "\n", "            ", "segment_info", "[", "\"category_id\"", "]", "=", "meta", "[", "\"stuff_dataset_id_to_contiguous_id\"", "]", "[", "\n", "segment_info", "[", "\"category_id\"", "]", "\n", "]", "\n", "segment_info", "[", "\"isthing\"", "]", "=", "False", "\n", "", "return", "segment_info", "\n", "\n", "", "with", "PathManager", ".", "open", "(", "json_file", ")", "as", "f", ":", "\n", "        ", "json_info", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "ret", "=", "[", "]", "\n", "for", "ann", "in", "json_info", "[", "\"annotations\"", "]", ":", "\n", "        ", "image_id", "=", "int", "(", "ann", "[", "\"image_id\"", "]", ")", "\n", "# TODO: currently we assume image and label has the same filename but", "\n", "# different extension, and images have extension \".jpg\" for COCO. Need", "\n", "# to make image extension a user-provided argument if we extend this", "\n", "# function to support other COCO-like datasets.", "\n", "image_file", "=", "os", ".", "path", ".", "join", "(", "image_dir", ",", "os", ".", "path", ".", "splitext", "(", "ann", "[", "\"file_name\"", "]", ")", "[", "0", "]", "+", "\".jpg\"", ")", "\n", "label_file", "=", "os", ".", "path", ".", "join", "(", "gt_dir", ",", "ann", "[", "\"file_name\"", "]", ")", "\n", "segments_info", "=", "[", "_convert_category_id", "(", "x", ",", "meta", ")", "for", "x", "in", "ann", "[", "\"segments_info\"", "]", "]", "\n", "ret", ".", "append", "(", "\n", "{", "\n", "\"file_name\"", ":", "image_file", ",", "\n", "\"image_id\"", ":", "image_id", ",", "\n", "\"pan_seg_file_name\"", ":", "label_file", ",", "\n", "\"segments_info\"", ":", "segments_info", ",", "\n", "}", "\n", ")", "\n", "", "assert", "len", "(", "ret", ")", ",", "f\"No images found in {image_dir}!\"", "\n", "assert", "PathManager", ".", "isfile", "(", "ret", "[", "0", "]", "[", "\"file_name\"", "]", ")", ",", "ret", "[", "0", "]", "[", "\"file_name\"", "]", "\n", "assert", "PathManager", ".", "isfile", "(", "ret", "[", "0", "]", "[", "\"pan_seg_file_name\"", "]", ")", ",", "ret", "[", "0", "]", "[", "\"pan_seg_file_name\"", "]", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco_panoptic.register_coco_panoptic": [[66, 99], ["detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "coco_panoptic.load_coco_panoptic_json", "detectron2.data.MetadataCatalog.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco_panoptic.load_coco_panoptic_json", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "register_coco_panoptic", "(", "\n", "name", ",", "metadata", ",", "image_root", ",", "panoptic_root", ",", "panoptic_json", ",", "instances_json", "=", "None", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Register a \"standard\" version of COCO panoptic segmentation dataset named `name`.\n    The dictionaries in this registered dataset follows detectron2's standard format.\n    Hence it's called \"standard\".\n\n    Args:\n        name (str): the name that identifies a dataset,\n            e.g. \"coco_2017_train_panoptic\"\n        metadata (dict): extra metadata associated with this dataset.\n        image_root (str): directory which contains all the images\n        panoptic_root (str): directory which contains panoptic annotation images in COCO format\n        panoptic_json (str): path to the json panoptic annotation file in COCO format\n        sem_seg_root (none): not used, to be consistent with\n            `register_coco_panoptic_separated`.\n        instances_json (str): path to the json instance annotation file\n    \"\"\"", "\n", "panoptic_name", "=", "name", "\n", "DatasetCatalog", ".", "register", "(", "\n", "panoptic_name", ",", "\n", "lambda", ":", "load_coco_panoptic_json", "(", "panoptic_json", ",", "image_root", ",", "panoptic_root", ",", "metadata", ")", ",", "\n", ")", "\n", "MetadataCatalog", ".", "get", "(", "panoptic_name", ")", ".", "set", "(", "\n", "panoptic_root", "=", "panoptic_root", ",", "\n", "image_root", "=", "image_root", ",", "\n", "panoptic_json", "=", "panoptic_json", ",", "\n", "json_file", "=", "instances_json", ",", "\n", "evaluator_type", "=", "\"coco_panoptic_seg\"", ",", "\n", "ignore_label", "=", "255", ",", "\n", "label_divisor", "=", "1000", ",", "\n", "**", "metadata", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco_panoptic.register_coco_panoptic_separated": [[102, 165], ["detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "coco_panoptic.merge_to_panoptic", "detectron2.data.MetadataCatalog.get", "coco.load_sem_seg", "detectron2.data.MetadataCatalog.get", "coco.load_coco_json", "coco.load_sem_seg"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco_panoptic.merge_to_panoptic", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.load_sem_seg", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.load_coco_json", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.load_sem_seg"], ["", "def", "register_coco_panoptic_separated", "(", "\n", "name", ",", "metadata", ",", "image_root", ",", "panoptic_root", ",", "panoptic_json", ",", "sem_seg_root", ",", "instances_json", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Register a \"separated\" version of COCO panoptic segmentation dataset named `name`.\n    The annotations in this registered dataset will contain both instance annotations and\n    semantic annotations, each with its own contiguous ids. Hence it's called \"separated\".\n\n    It follows the setting used by the PanopticFPN paper:\n\n    1. The instance annotations directly come from polygons in the COCO\n       instances annotation task, rather than from the masks in the COCO panoptic annotations.\n\n       The two format have small differences:\n       Polygons in the instance annotations may have overlaps.\n       The mask annotations are produced by labeling the overlapped polygons\n       with depth ordering.\n\n    2. The semantic annotations are converted from panoptic annotations, where\n       all \"things\" are assigned a semantic id of 0.\n       All semantic categories will therefore have ids in contiguous\n       range [1, #stuff_categories].\n\n    This function will also register a pure semantic segmentation dataset\n    named ``name + '_stuffonly'``.\n\n    Args:\n        name (str): the name that identifies a dataset,\n            e.g. \"coco_2017_train_panoptic\"\n        metadata (dict): extra metadata associated with this dataset.\n        image_root (str): directory which contains all the images\n        panoptic_root (str): directory which contains panoptic annotation images\n        panoptic_json (str): path to the json panoptic annotation file\n        sem_seg_root (str): directory which contains all the ground truth segmentation annotations.\n        instances_json (str): path to the json instance annotation file\n    \"\"\"", "\n", "panoptic_name", "=", "name", "+", "\"_separated\"", "\n", "DatasetCatalog", ".", "register", "(", "\n", "panoptic_name", ",", "\n", "lambda", ":", "merge_to_panoptic", "(", "\n", "load_coco_json", "(", "instances_json", ",", "image_root", ",", "panoptic_name", ")", ",", "\n", "load_sem_seg", "(", "sem_seg_root", ",", "image_root", ")", ",", "\n", ")", ",", "\n", ")", "\n", "MetadataCatalog", ".", "get", "(", "panoptic_name", ")", ".", "set", "(", "\n", "panoptic_root", "=", "panoptic_root", ",", "\n", "image_root", "=", "image_root", ",", "\n", "panoptic_json", "=", "panoptic_json", ",", "\n", "sem_seg_root", "=", "sem_seg_root", ",", "\n", "json_file", "=", "instances_json", ",", "# TODO rename", "\n", "evaluator_type", "=", "\"coco_panoptic_seg\"", ",", "\n", "ignore_label", "=", "255", ",", "\n", "**", "metadata", ",", "\n", ")", "\n", "\n", "semantic_name", "=", "name", "+", "\"_stuffonly\"", "\n", "DatasetCatalog", ".", "register", "(", "semantic_name", ",", "lambda", ":", "load_sem_seg", "(", "sem_seg_root", ",", "image_root", ")", ")", "\n", "MetadataCatalog", ".", "get", "(", "semantic_name", ")", ".", "set", "(", "\n", "sem_seg_root", "=", "sem_seg_root", ",", "\n", "image_root", "=", "image_root", ",", "\n", "evaluator_type", "=", "\"sem_seg\"", ",", "\n", "ignore_label", "=", "255", ",", "\n", "**", "metadata", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco_panoptic.merge_to_panoptic": [[168, 191], ["len", "copy.copy", "copy.copy.update", "results.append"], "function", ["None"], ["", "def", "merge_to_panoptic", "(", "detection_dicts", ",", "sem_seg_dicts", ")", ":", "\n", "    ", "\"\"\"\n    Create dataset dicts for panoptic segmentation, by\n    merging two dicts using \"file_name\" field to match their entries.\n\n    Args:\n        detection_dicts (list[dict]): lists of dicts for object detection or instance segmentation.\n        sem_seg_dicts (list[dict]): lists of dicts for semantic segmentation.\n\n    Returns:\n        list[dict] (one per input image): Each dict contains all (key, value) pairs from dicts in\n            both detection_dicts and sem_seg_dicts that correspond to the same image.\n            The function assumes that the same key in different dicts has the same value.\n    \"\"\"", "\n", "results", "=", "[", "]", "\n", "sem_seg_file_to_entry", "=", "{", "x", "[", "\"file_name\"", "]", ":", "x", "for", "x", "in", "sem_seg_dicts", "}", "\n", "assert", "len", "(", "sem_seg_file_to_entry", ")", ">", "0", "\n", "\n", "for", "det_dict", "in", "detection_dicts", ":", "\n", "        ", "dic", "=", "copy", ".", "copy", "(", "det_dict", ")", "\n", "dic", ".", "update", "(", "sem_seg_file_to_entry", "[", "dic", "[", "\"file_name\"", "]", "]", ")", "\n", "results", ".", "append", "(", "dic", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_coco_instances_meta": [[445, 458], ["len", "len", "enumerate"], "function", ["None"], ["def", "_get_coco_instances_meta", "(", ")", ":", "\n", "    ", "thing_ids", "=", "[", "k", "[", "\"id\"", "]", "for", "k", "in", "COCO_CATEGORIES", "if", "k", "[", "\"isthing\"", "]", "==", "1", "]", "\n", "thing_colors", "=", "[", "k", "[", "\"color\"", "]", "for", "k", "in", "COCO_CATEGORIES", "if", "k", "[", "\"isthing\"", "]", "==", "1", "]", "\n", "assert", "len", "(", "thing_ids", ")", "==", "80", ",", "len", "(", "thing_ids", ")", "\n", "# Mapping from the incontiguous COCO category id to an id in [0, 79]", "\n", "thing_dataset_id_to_contiguous_id", "=", "{", "k", ":", "i", "for", "i", ",", "k", "in", "enumerate", "(", "thing_ids", ")", "}", "\n", "thing_classes", "=", "[", "k", "[", "\"name\"", "]", "for", "k", "in", "COCO_CATEGORIES", "if", "k", "[", "\"isthing\"", "]", "==", "1", "]", "\n", "ret", "=", "{", "\n", "\"thing_dataset_id_to_contiguous_id\"", ":", "thing_dataset_id_to_contiguous_id", ",", "\n", "\"thing_classes\"", ":", "thing_classes", ",", "\n", "\"thing_colors\"", ":", "thing_colors", ",", "\n", "}", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_coco_panoptic_separated_meta": [[460, 491], ["len", "ret.update", "len", "builtin_meta._get_coco_instances_meta", "enumerate", "k[].replace().replace", "k[].replace"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_coco_instances_meta"], ["", "def", "_get_coco_panoptic_separated_meta", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns metadata for \"separated\" version of the panoptic segmentation dataset.\n    \"\"\"", "\n", "stuff_ids", "=", "[", "k", "[", "\"id\"", "]", "for", "k", "in", "COCO_CATEGORIES", "if", "k", "[", "\"isthing\"", "]", "==", "0", "]", "\n", "assert", "len", "(", "stuff_ids", ")", "==", "53", ",", "len", "(", "stuff_ids", ")", "\n", "\n", "# For semantic segmentation, this mapping maps from contiguous stuff id", "\n", "# (in [0, 53], used in models) to ids in the dataset (used for processing results)", "\n", "# The id 0 is mapped to an extra category \"thing\".", "\n", "stuff_dataset_id_to_contiguous_id", "=", "{", "k", ":", "i", "+", "1", "for", "i", ",", "k", "in", "enumerate", "(", "stuff_ids", ")", "}", "\n", "# When converting COCO panoptic annotations to semantic annotations", "\n", "# We label the \"thing\" category to 0", "\n", "stuff_dataset_id_to_contiguous_id", "[", "0", "]", "=", "0", "\n", "\n", "# 54 names for COCO stuff categories (including \"things\")", "\n", "stuff_classes", "=", "[", "\"things\"", "]", "+", "[", "\n", "k", "[", "\"name\"", "]", ".", "replace", "(", "\"-other\"", ",", "\"\"", ")", ".", "replace", "(", "\"-merged\"", ",", "\"\"", ")", "\n", "for", "k", "in", "COCO_CATEGORIES", "\n", "if", "k", "[", "\"isthing\"", "]", "==", "0", "\n", "]", "\n", "\n", "# NOTE: I randomly picked a color for things", "\n", "stuff_colors", "=", "[", "[", "82", ",", "18", ",", "128", "]", "]", "+", "[", "k", "[", "\"color\"", "]", "for", "k", "in", "COCO_CATEGORIES", "if", "k", "[", "\"isthing\"", "]", "==", "0", "]", "\n", "ret", "=", "{", "\n", "\"stuff_dataset_id_to_contiguous_id\"", ":", "stuff_dataset_id_to_contiguous_id", ",", "\n", "\"stuff_classes\"", ":", "stuff_classes", ",", "\n", "\"stuff_colors\"", ":", "stuff_colors", ",", "\n", "}", "\n", "ret", ".", "update", "(", "_get_coco_instances_meta", "(", ")", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_builtin_metadata": [[493, 561], ["KeyError", "builtin_meta._get_coco_instances_meta", "builtin_meta._get_coco_panoptic_separated_meta", "enumerate"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_coco_instances_meta", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_coco_panoptic_separated_meta"], ["", "def", "_get_builtin_metadata", "(", "dataset_name", ")", ":", "\n", "    ", "if", "dataset_name", "==", "\"coco\"", ":", "\n", "        ", "return", "_get_coco_instances_meta", "(", ")", "\n", "", "if", "dataset_name", "==", "\"coco_panoptic_separated\"", ":", "\n", "        ", "return", "_get_coco_panoptic_separated_meta", "(", ")", "\n", "", "elif", "dataset_name", "==", "\"coco_panoptic_standard\"", ":", "\n", "        ", "meta", "=", "{", "}", "\n", "# The following metadata maps contiguous id from [0, #thing categories +", "\n", "# #stuff categories) to their names and colors. We have to replica of the", "\n", "# same name and color under \"thing_*\" and \"stuff_*\" because the current", "\n", "# visualization function in D2 handles thing and class classes differently", "\n", "# due to some heuristic used in Panoptic FPN. We keep the same naming to", "\n", "# enable reusing existing visualization functions.", "\n", "thing_classes", "=", "[", "k", "[", "\"name\"", "]", "for", "k", "in", "COCO_CATEGORIES", "]", "\n", "thing_colors", "=", "[", "k", "[", "\"color\"", "]", "for", "k", "in", "COCO_CATEGORIES", "]", "\n", "stuff_classes", "=", "[", "k", "[", "\"name\"", "]", "for", "k", "in", "COCO_CATEGORIES", "]", "\n", "stuff_colors", "=", "[", "k", "[", "\"color\"", "]", "for", "k", "in", "COCO_CATEGORIES", "]", "\n", "\n", "meta", "[", "\"thing_classes\"", "]", "=", "thing_classes", "\n", "meta", "[", "\"thing_colors\"", "]", "=", "thing_colors", "\n", "meta", "[", "\"stuff_classes\"", "]", "=", "stuff_classes", "\n", "meta", "[", "\"stuff_colors\"", "]", "=", "stuff_colors", "\n", "\n", "# Convert category id for training:", "\n", "#   category id: like semantic segmentation, it is the class id for each", "\n", "#   pixel. Since there are some classes not used in evaluation, the category", "\n", "#   id is not always contiguous and thus we have two set of category ids:", "\n", "#       - original category id: category id in the original dataset, mainly", "\n", "#           used for evaluation.", "\n", "#       - contiguous category id: [0, #classes), in order to train the linear", "\n", "#           softmax classifier.", "\n", "thing_dataset_id_to_contiguous_id", "=", "{", "}", "\n", "stuff_dataset_id_to_contiguous_id", "=", "{", "}", "\n", "\n", "for", "i", ",", "cat", "in", "enumerate", "(", "COCO_CATEGORIES", ")", ":", "\n", "            ", "if", "cat", "[", "\"isthing\"", "]", ":", "\n", "                ", "thing_dataset_id_to_contiguous_id", "[", "cat", "[", "\"id\"", "]", "]", "=", "i", "\n", "", "else", ":", "\n", "                ", "stuff_dataset_id_to_contiguous_id", "[", "cat", "[", "\"id\"", "]", "]", "=", "i", "\n", "\n", "", "", "meta", "[", "\"thing_dataset_id_to_contiguous_id\"", "]", "=", "thing_dataset_id_to_contiguous_id", "\n", "meta", "[", "\"stuff_dataset_id_to_contiguous_id\"", "]", "=", "stuff_dataset_id_to_contiguous_id", "\n", "\n", "return", "meta", "\n", "", "elif", "dataset_name", "==", "\"coco_person\"", ":", "\n", "        ", "return", "{", "\n", "\"thing_classes\"", ":", "[", "\"person\"", "]", ",", "\n", "\"keypoint_names\"", ":", "COCO_PERSON_KEYPOINT_NAMES", ",", "\n", "\"keypoint_flip_map\"", ":", "COCO_PERSON_KEYPOINT_FLIP_MAP", ",", "\n", "\"keypoint_connection_rules\"", ":", "KEYPOINT_CONNECTION_RULES", ",", "\n", "}", "\n", "", "elif", "dataset_name", "==", "\"cityscapes\"", ":", "\n", "# fmt: off", "\n", "        ", "CITYSCAPES_THING_CLASSES", "=", "[", "\n", "\"person\"", ",", "\"rider\"", ",", "\"car\"", ",", "\"truck\"", ",", "\n", "\"bus\"", ",", "\"train\"", ",", "\"motorcycle\"", ",", "\"bicycle\"", ",", "\n", "]", "\n", "CITYSCAPES_STUFF_CLASSES", "=", "[", "\n", "\"road\"", ",", "\"sidewalk\"", ",", "\"building\"", ",", "\"wall\"", ",", "\"fence\"", ",", "\"pole\"", ",", "\"traffic light\"", ",", "\n", "\"traffic sign\"", ",", "\"vegetation\"", ",", "\"terrain\"", ",", "\"sky\"", ",", "\"person\"", ",", "\"rider\"", ",", "\"car\"", ",", "\n", "\"truck\"", ",", "\"bus\"", ",", "\"train\"", ",", "\"motorcycle\"", ",", "\"bicycle\"", ",", "\n", "]", "\n", "# fmt: on", "\n", "return", "{", "\n", "\"thing_classes\"", ":", "CITYSCAPES_THING_CLASSES", ",", "\n", "\"stuff_classes\"", ":", "CITYSCAPES_STUFF_CLASSES", ",", "\n", "}", "\n", "", "raise", "KeyError", "(", "\"No built-in metadata for dataset {}\"", ".", "format", "(", "dataset_name", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes._get_cityscapes_files": [[27, 51], ["detectron2.utils.file_io.PathManager.ls", "logger.info", "len", "os.path.join", "os.path.join", "detectron2.utils.file_io.PathManager.ls", "detectron2.utils.file_io.PathManager.isfile", "os.path.join", "basename.endswith", "os.path.join", "os.path.join", "os.path.join", "files.append", "len", "len"], "function", ["None"], ["def", "_get_cityscapes_files", "(", "image_dir", ",", "gt_dir", ")", ":", "\n", "    ", "files", "=", "[", "]", "\n", "# scan through the directory", "\n", "cities", "=", "PathManager", ".", "ls", "(", "image_dir", ")", "\n", "logger", ".", "info", "(", "f\"{len(cities)} cities found in '{image_dir}'.\"", ")", "\n", "for", "city", "in", "cities", ":", "\n", "        ", "city_img_dir", "=", "os", ".", "path", ".", "join", "(", "image_dir", ",", "city", ")", "\n", "city_gt_dir", "=", "os", ".", "path", ".", "join", "(", "gt_dir", ",", "city", ")", "\n", "for", "basename", "in", "PathManager", ".", "ls", "(", "city_img_dir", ")", ":", "\n", "            ", "image_file", "=", "os", ".", "path", ".", "join", "(", "city_img_dir", ",", "basename", ")", "\n", "\n", "suffix", "=", "\"leftImg8bit.png\"", "\n", "assert", "basename", ".", "endswith", "(", "suffix", ")", ",", "basename", "\n", "basename", "=", "basename", "[", ":", "-", "len", "(", "suffix", ")", "]", "\n", "\n", "instance_file", "=", "os", ".", "path", ".", "join", "(", "city_gt_dir", ",", "basename", "+", "\"gtFine_instanceIds.png\"", ")", "\n", "label_file", "=", "os", ".", "path", ".", "join", "(", "city_gt_dir", ",", "basename", "+", "\"gtFine_labelIds.png\"", ")", "\n", "json_file", "=", "os", ".", "path", ".", "join", "(", "city_gt_dir", ",", "basename", "+", "\"gtFine_polygons.json\"", ")", "\n", "\n", "files", ".", "append", "(", "(", "image_file", ",", "instance_file", ",", "label_file", ",", "json_file", ")", ")", "\n", "", "", "assert", "len", "(", "files", ")", ",", "\"No images found in {}\"", ".", "format", "(", "image_dir", ")", "\n", "for", "f", "in", "files", "[", "0", "]", ":", "\n", "        ", "assert", "PathManager", ".", "isfile", "(", "f", ")", ",", "f", "\n", "", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes.load_cityscapes_instances": [[53, 93], ["cityscapes._get_cityscapes_files", "logger.info", "multiprocessing.Pool", "mp.Pool.map", "logger.info", "functools.partial", "max", "len", "enumerate", "multiprocessing.cpu_count", "detectron2.utils.comm.get_world_size"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes._get_cityscapes_files", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], ["", "def", "load_cityscapes_instances", "(", "image_dir", ",", "gt_dir", ",", "from_json", "=", "True", ",", "to_polygons", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        image_dir (str): path to the raw dataset. e.g., \"~/cityscapes/leftImg8bit/train\".\n        gt_dir (str): path to the raw annotations. e.g., \"~/cityscapes/gtFine/train\".\n        from_json (bool): whether to read annotations from the raw json file or the png files.\n        to_polygons (bool): whether to represent the segmentation as polygons\n            (COCO's format) instead of masks (cityscapes's format).\n\n    Returns:\n        list[dict]: a list of dicts in Detectron2 standard format. (See\n        `Using Custom Datasets </tutorials/datasets.html>`_ )\n    \"\"\"", "\n", "if", "from_json", ":", "\n", "        ", "assert", "to_polygons", ",", "(", "\n", "\"Cityscapes's json annotations are in polygon format. \"", "\n", "\"Converting to mask format is not supported now.\"", "\n", ")", "\n", "", "files", "=", "_get_cityscapes_files", "(", "image_dir", ",", "gt_dir", ")", "\n", "\n", "logger", ".", "info", "(", "\"Preprocessing cityscapes annotations ...\"", ")", "\n", "# This is still not fast: all workers will execute duplicate works and will", "\n", "# take up to 10m on a 8GPU server.", "\n", "pool", "=", "mp", ".", "Pool", "(", "processes", "=", "max", "(", "mp", ".", "cpu_count", "(", ")", "//", "get_world_size", "(", ")", "//", "2", ",", "4", ")", ")", "\n", "\n", "ret", "=", "pool", ".", "map", "(", "\n", "functools", ".", "partial", "(", "_cityscapes_files_to_dict", ",", "from_json", "=", "from_json", ",", "to_polygons", "=", "to_polygons", ")", ",", "\n", "files", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Loaded {} images from {}\"", ".", "format", "(", "len", "(", "ret", ")", ",", "image_dir", ")", ")", "\n", "\n", "# Map cityscape ids to contiguous ids", "\n", "from", "cityscapesscripts", ".", "helpers", ".", "labels", "import", "labels", "\n", "\n", "labels", "=", "[", "l", "for", "l", "in", "labels", "if", "l", ".", "hasInstances", "and", "not", "l", ".", "ignoreInEval", "]", "\n", "dataset_id_to_contiguous_id", "=", "{", "l", ".", "id", ":", "idx", "for", "idx", ",", "l", "in", "enumerate", "(", "labels", ")", "}", "\n", "for", "dict_per_image", "in", "ret", ":", "\n", "        ", "for", "anno", "in", "dict_per_image", "[", "\"annotations\"", "]", ":", "\n", "            ", "anno", "[", "\"category_id\"", "]", "=", "dataset_id_to_contiguous_id", "[", "anno", "[", "\"category_id\"", "]", "]", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes.load_cityscapes_semantic": [[95, 126], ["detectron2.utils.file_io.PathManager.get_local_path", "cityscapes._get_cityscapes_files", "len", "detectron2.utils.file_io.PathManager.isfile", "label_file.replace.replace", "ret.append", "detectron2.utils.file_io.PathManager.open", "json.load"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes._get_cityscapes_files", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load"], ["", "def", "load_cityscapes_semantic", "(", "image_dir", ",", "gt_dir", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        image_dir (str): path to the raw dataset. e.g., \"~/cityscapes/leftImg8bit/train\".\n        gt_dir (str): path to the raw annotations. e.g., \"~/cityscapes/gtFine/train\".\n\n    Returns:\n        list[dict]: a list of dict, each has \"file_name\" and\n            \"sem_seg_file_name\".\n    \"\"\"", "\n", "ret", "=", "[", "]", "\n", "# gt_dir is small and contain many small files. make sense to fetch to local first", "\n", "gt_dir", "=", "PathManager", ".", "get_local_path", "(", "gt_dir", ")", "\n", "for", "image_file", ",", "_", ",", "label_file", ",", "json_file", "in", "_get_cityscapes_files", "(", "image_dir", ",", "gt_dir", ")", ":", "\n", "        ", "label_file", "=", "label_file", ".", "replace", "(", "\"labelIds\"", ",", "\"labelTrainIds\"", ")", "\n", "\n", "with", "PathManager", ".", "open", "(", "json_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "jsonobj", "=", "json", ".", "load", "(", "f", ")", "\n", "", "ret", ".", "append", "(", "\n", "{", "\n", "\"file_name\"", ":", "image_file", ",", "\n", "\"sem_seg_file_name\"", ":", "label_file", ",", "\n", "\"height\"", ":", "jsonobj", "[", "\"imgHeight\"", "]", ",", "\n", "\"width\"", ":", "jsonobj", "[", "\"imgWidth\"", "]", ",", "\n", "}", "\n", ")", "\n", "", "assert", "len", "(", "ret", ")", ",", "f\"No images found in {image_dir}!\"", "\n", "assert", "PathManager", ".", "isfile", "(", "\n", "ret", "[", "0", "]", "[", "\"sem_seg_file_name\"", "]", "\n", ")", ",", "\"Please generate labelTrainIds.png with cityscapesscripts/preparation/createTrainIdLabelImgs.py\"", "# noqa", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes._cityscapes_files_to_dict": [[128, 279], ["Polygon", "numpy.unique", "detectron2.utils.file_io.PathManager.open", "json.load", "os.path.basename", "Polygon().buffer", "Polygon().buffer.difference", "polygons_union.union.union", "label_name.endswith", "isinstance", "annos.append", "detectron2.utils.file_io.PathManager.open", "numpy.asarray", "os.path.basename", "numpy.asarray", "numpy.nonzero", "annos.append", "numpy.asarray", "polygons_union.union.union", "isinstance", "poly_coord.append", "PIL.Image.open", "inds[].min", "inds[].max", "inds[].min", "inds[].max", "label_name.endswith", "Polygon", "NotImplementedError", "list", "cv2.findContours", "c.reshape().tolist", "len", "pycocotools.encode", "itertools.chain", "np.asarray.copy", "c.reshape", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "def", "_cityscapes_files_to_dict", "(", "files", ",", "from_json", ",", "to_polygons", ")", ":", "\n", "    ", "\"\"\"\n    Parse cityscapes annotation files to a instance segmentation dataset dict.\n\n    Args:\n        files (tuple): consists of (image_file, instance_id_file, label_id_file, json_file)\n        from_json (bool): whether to read annotations from the raw json file or the png files.\n        to_polygons (bool): whether to represent the segmentation as polygons\n            (COCO's format) instead of masks (cityscapes's format).\n\n    Returns:\n        A dict in Detectron2 Dataset format.\n    \"\"\"", "\n", "from", "cityscapesscripts", ".", "helpers", ".", "labels", "import", "id2label", ",", "name2label", "\n", "\n", "image_file", ",", "instance_id_file", ",", "_", ",", "json_file", "=", "files", "\n", "\n", "annos", "=", "[", "]", "\n", "\n", "if", "from_json", ":", "\n", "        ", "from", "shapely", ".", "geometry", "import", "MultiPolygon", ",", "Polygon", "\n", "\n", "with", "PathManager", ".", "open", "(", "json_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "jsonobj", "=", "json", ".", "load", "(", "f", ")", "\n", "", "ret", "=", "{", "\n", "\"file_name\"", ":", "image_file", ",", "\n", "\"image_id\"", ":", "os", ".", "path", ".", "basename", "(", "image_file", ")", ",", "\n", "\"height\"", ":", "jsonobj", "[", "\"imgHeight\"", "]", ",", "\n", "\"width\"", ":", "jsonobj", "[", "\"imgWidth\"", "]", ",", "\n", "}", "\n", "\n", "# `polygons_union` contains the union of all valid polygons.", "\n", "polygons_union", "=", "Polygon", "(", ")", "\n", "\n", "# CityscapesScripts draw the polygons in sequential order", "\n", "# and each polygon *overwrites* existing ones. See", "\n", "# (https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/preparation/json2instanceImg.py) # noqa", "\n", "# We use reverse order, and each polygon *avoids* early ones.", "\n", "# This will resolve the ploygon overlaps in the same way as CityscapesScripts.", "\n", "for", "obj", "in", "jsonobj", "[", "\"objects\"", "]", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "if", "\"deleted\"", "in", "obj", ":", "# cityscapes data format specific", "\n", "                ", "continue", "\n", "", "label_name", "=", "obj", "[", "\"label\"", "]", "\n", "\n", "try", ":", "\n", "                ", "label", "=", "name2label", "[", "label_name", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "if", "label_name", ".", "endswith", "(", "\"group\"", ")", ":", "# crowd area", "\n", "                    ", "label", "=", "name2label", "[", "label_name", "[", ":", "-", "len", "(", "\"group\"", ")", "]", "]", "\n", "", "else", ":", "\n", "                    ", "raise", "\n", "", "", "if", "label", ".", "id", "<", "0", ":", "# cityscapes data format", "\n", "                ", "continue", "\n", "\n", "# Cityscapes's raw annotations uses integer coordinates", "\n", "# Therefore +0.5 here", "\n", "", "poly_coord", "=", "np", ".", "asarray", "(", "obj", "[", "\"polygon\"", "]", ",", "dtype", "=", "\"f4\"", ")", "+", "0.5", "\n", "# CityscapesScript uses PIL.ImageDraw.polygon to rasterize", "\n", "# polygons for evaluation. This function operates in integer space", "\n", "# and draws each pixel whose center falls into the polygon.", "\n", "# Therefore it draws a polygon which is 0.5 \"fatter\" in expectation.", "\n", "# We therefore dilate the input polygon by 0.5 as our input.", "\n", "poly", "=", "Polygon", "(", "poly_coord", ")", ".", "buffer", "(", "0.5", ",", "resolution", "=", "4", ")", "\n", "\n", "if", "not", "label", ".", "hasInstances", "or", "label", ".", "ignoreInEval", ":", "\n", "# even if we won't store the polygon it still contributes to overlaps resolution", "\n", "                ", "polygons_union", "=", "polygons_union", ".", "union", "(", "poly", ")", "\n", "continue", "\n", "\n", "# Take non-overlapping part of the polygon", "\n", "", "poly_wo_overlaps", "=", "poly", ".", "difference", "(", "polygons_union", ")", "\n", "if", "poly_wo_overlaps", ".", "is_empty", ":", "\n", "                ", "continue", "\n", "", "polygons_union", "=", "polygons_union", ".", "union", "(", "poly", ")", "\n", "\n", "anno", "=", "{", "}", "\n", "anno", "[", "\"iscrowd\"", "]", "=", "label_name", ".", "endswith", "(", "\"group\"", ")", "\n", "anno", "[", "\"category_id\"", "]", "=", "label", ".", "id", "\n", "\n", "if", "isinstance", "(", "poly_wo_overlaps", ",", "Polygon", ")", ":", "\n", "                ", "poly_list", "=", "[", "poly_wo_overlaps", "]", "\n", "", "elif", "isinstance", "(", "poly_wo_overlaps", ",", "MultiPolygon", ")", ":", "\n", "                ", "poly_list", "=", "poly_wo_overlaps", ".", "geoms", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\"Unknown geometric structure {}\"", ".", "format", "(", "poly_wo_overlaps", ")", ")", "\n", "\n", "", "poly_coord", "=", "[", "]", "\n", "for", "poly_el", "in", "poly_list", ":", "\n", "# COCO API can work only with exterior boundaries now, hence we store only them.", "\n", "# TODO: store both exterior and interior boundaries once other parts of the", "\n", "# codebase support holes in polygons.", "\n", "                ", "poly_coord", ".", "append", "(", "list", "(", "chain", "(", "*", "poly_el", ".", "exterior", ".", "coords", ")", ")", ")", "\n", "", "anno", "[", "\"segmentation\"", "]", "=", "poly_coord", "\n", "(", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", ")", "=", "poly_wo_overlaps", ".", "bounds", "\n", "\n", "anno", "[", "\"bbox\"", "]", "=", "(", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", ")", "\n", "anno", "[", "\"bbox_mode\"", "]", "=", "BoxMode", ".", "XYXY_ABS", "\n", "\n", "annos", ".", "append", "(", "anno", ")", "\n", "", "", "else", ":", "\n", "# See also the official annotation parsing scripts at", "\n", "# https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/evaluation/instances2dict.py  # noqa", "\n", "        ", "with", "PathManager", ".", "open", "(", "instance_id_file", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "inst_image", "=", "np", ".", "asarray", "(", "Image", ".", "open", "(", "f", ")", ",", "order", "=", "\"F\"", ")", "\n", "# ids < 24 are stuff labels (filtering them first is about 5% faster)", "\n", "", "flattened_ids", "=", "np", ".", "unique", "(", "inst_image", "[", "inst_image", ">=", "24", "]", ")", "\n", "\n", "ret", "=", "{", "\n", "\"file_name\"", ":", "image_file", ",", "\n", "\"image_id\"", ":", "os", ".", "path", ".", "basename", "(", "image_file", ")", ",", "\n", "\"height\"", ":", "inst_image", ".", "shape", "[", "0", "]", ",", "\n", "\"width\"", ":", "inst_image", ".", "shape", "[", "1", "]", ",", "\n", "}", "\n", "\n", "for", "instance_id", "in", "flattened_ids", ":", "\n", "# For non-crowd annotations, instance_id // 1000 is the label_id", "\n", "# Crowd annotations have <1000 instance ids", "\n", "            ", "label_id", "=", "instance_id", "//", "1000", "if", "instance_id", ">=", "1000", "else", "instance_id", "\n", "label", "=", "id2label", "[", "label_id", "]", "\n", "if", "not", "label", ".", "hasInstances", "or", "label", ".", "ignoreInEval", ":", "\n", "                ", "continue", "\n", "\n", "", "anno", "=", "{", "}", "\n", "anno", "[", "\"iscrowd\"", "]", "=", "instance_id", "<", "1000", "\n", "anno", "[", "\"category_id\"", "]", "=", "label", ".", "id", "\n", "\n", "mask", "=", "np", ".", "asarray", "(", "inst_image", "==", "instance_id", ",", "dtype", "=", "np", ".", "uint8", ",", "order", "=", "\"F\"", ")", "\n", "\n", "inds", "=", "np", ".", "nonzero", "(", "mask", ")", "\n", "ymin", ",", "ymax", "=", "inds", "[", "0", "]", ".", "min", "(", ")", ",", "inds", "[", "0", "]", ".", "max", "(", ")", "\n", "xmin", ",", "xmax", "=", "inds", "[", "1", "]", ".", "min", "(", ")", ",", "inds", "[", "1", "]", ".", "max", "(", ")", "\n", "anno", "[", "\"bbox\"", "]", "=", "(", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", ")", "\n", "if", "xmax", "<=", "xmin", "or", "ymax", "<=", "ymin", ":", "\n", "                ", "continue", "\n", "", "anno", "[", "\"bbox_mode\"", "]", "=", "BoxMode", ".", "XYXY_ABS", "\n", "if", "to_polygons", ":", "\n", "# This conversion comes from D4809743 and D5171122,", "\n", "# when Mask-RCNN was first developed.", "\n", "                ", "contours", "=", "cv2", ".", "findContours", "(", "mask", ".", "copy", "(", ")", ",", "cv2", ".", "RETR_EXTERNAL", ",", "cv2", ".", "CHAIN_APPROX_NONE", ")", "[", "\n", "-", "2", "\n", "]", "\n", "polygons", "=", "[", "c", ".", "reshape", "(", "-", "1", ")", ".", "tolist", "(", ")", "for", "c", "in", "contours", "if", "len", "(", "c", ")", ">=", "3", "]", "\n", "# opencv's can produce invalid polygons", "\n", "if", "len", "(", "polygons", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "anno", "[", "\"segmentation\"", "]", "=", "polygons", "\n", "", "else", ":", "\n", "                ", "anno", "[", "\"segmentation\"", "]", "=", "mask_util", ".", "encode", "(", "mask", "[", ":", ",", ":", ",", "None", "]", ")", "[", "0", "]", "\n", "", "annos", ".", "append", "(", "anno", ")", "\n", "", "", "ret", "[", "\"annotations\"", "]", "=", "annos", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.SimpleTokenizer.__init__": [[69, 86], ["clip_prompt_utils.default_bpe", "clip_prompt_utils.bytes_to_unicode", "gzip.open().read().decode().split", "list", "list.extend", "dict", "dict", "regex.compile", "tuple", "bytes_to_unicode().values", "list.append", "zip", "zip", "clip_prompt_utils.SimpleTokenizer.byte_encoder.items", "gzip.open().read().decode", "merge.split", "range", "clip_prompt_utils.SimpleTokenizer.encoder.items", "range", "clip_prompt_utils.bytes_to_unicode", "len", "len", "gzip.open().read", "gzip.open"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.default_bpe", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.bytes_to_unicode", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.bytes_to_unicode"], ["    ", "def", "__init__", "(", "self", ",", "bpe_path", ":", "str", "=", "default_bpe", "(", ")", ")", ":", "\n", "        ", "self", ".", "byte_encoder", "=", "bytes_to_unicode", "(", ")", "\n", "self", ".", "byte_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "byte_encoder", ".", "items", "(", ")", "}", "\n", "merges", "=", "gzip", ".", "open", "(", "bpe_path", ")", ".", "read", "(", ")", ".", "decode", "(", "\"utf-8\"", ")", ".", "split", "(", "'\\n'", ")", "\n", "merges", "=", "merges", "[", "1", ":", "49152", "-", "256", "-", "2", "+", "1", "]", "\n", "merges", "=", "[", "tuple", "(", "merge", ".", "split", "(", ")", ")", "for", "merge", "in", "merges", "]", "\n", "vocab", "=", "list", "(", "bytes_to_unicode", "(", ")", ".", "values", "(", ")", ")", "\n", "vocab", "=", "vocab", "+", "[", "v", "+", "'</w>'", "for", "v", "in", "vocab", "]", "\n", "self", ".", "vocab", "=", "vocab", "\n", "for", "merge", "in", "merges", ":", "\n", "            ", "vocab", ".", "append", "(", "''", ".", "join", "(", "merge", ")", ")", "\n", "", "vocab", ".", "extend", "(", "[", "'<|startoftext|>'", ",", "'<|endoftext|>'", "]", ")", "\n", "self", ".", "encoder", "=", "dict", "(", "zip", "(", "vocab", ",", "range", "(", "len", "(", "vocab", ")", ")", ")", ")", "\n", "self", ".", "decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "encoder", ".", "items", "(", ")", "}", "\n", "self", ".", "bpe_ranks", "=", "dict", "(", "zip", "(", "merges", ",", "range", "(", "len", "(", "merges", ")", ")", ")", ")", "\n", "self", ".", "cache", "=", "{", "'<|startoftext|>'", ":", "'<|startoftext|>'", ",", "'<|endoftext|>'", ":", "'<|endoftext|>'", "}", "\n", "self", ".", "pat", "=", "re", ".", "compile", "(", "r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\"", ",", "re", ".", "IGNORECASE", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.SimpleTokenizer.bpe": [[87, 127], ["clip_prompt_utils.get_pairs", "tuple", "min", "tuple", "len", "len", "clip_prompt_utils.get_pairs", "word.index", "tuple.extend", "tuple.append", "tuple.append", "clip_prompt_utils.SimpleTokenizer.bpe_ranks.get", "tuple.extend", "float", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.get_pairs", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.get_pairs", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "bpe", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "token", "in", "self", ".", "cache", ":", "\n", "            ", "return", "self", ".", "cache", "[", "token", "]", "\n", "", "word", "=", "tuple", "(", "token", "[", ":", "-", "1", "]", ")", "+", "(", "token", "[", "-", "1", "]", "+", "'</w>'", ",", ")", "\n", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "\n", "if", "not", "pairs", ":", "\n", "            ", "return", "token", "+", "'</w>'", "\n", "\n", "", "while", "True", ":", "\n", "            ", "bigram", "=", "min", "(", "pairs", ",", "key", "=", "lambda", "pair", ":", "self", ".", "bpe_ranks", ".", "get", "(", "pair", ",", "float", "(", "'inf'", ")", ")", ")", "\n", "if", "bigram", "not", "in", "self", ".", "bpe_ranks", ":", "\n", "                ", "break", "\n", "", "first", ",", "second", "=", "bigram", "\n", "new_word", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "word", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "j", "=", "word", ".", "index", "(", "first", ",", "i", ")", "\n", "new_word", ".", "extend", "(", "word", "[", "i", ":", "j", "]", ")", "\n", "i", "=", "j", "\n", "", "except", ":", "\n", "                    ", "new_word", ".", "extend", "(", "word", "[", "i", ":", "]", ")", "\n", "break", "\n", "\n", "", "if", "word", "[", "i", "]", "==", "first", "and", "i", "<", "len", "(", "word", ")", "-", "1", "and", "word", "[", "i", "+", "1", "]", "==", "second", ":", "\n", "                    ", "new_word", ".", "append", "(", "first", "+", "second", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                    ", "new_word", ".", "append", "(", "word", "[", "i", "]", ")", "\n", "i", "+=", "1", "\n", "", "", "new_word", "=", "tuple", "(", "new_word", ")", "\n", "word", "=", "new_word", "\n", "if", "len", "(", "word", ")", "==", "1", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "", "", "word", "=", "' '", ".", "join", "(", "word", ")", "\n", "self", ".", "cache", "[", "token", "]", "=", "word", "\n", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.SimpleTokenizer.encode": [[128, 142], ["whitespace_clean().lower", "regex.findall", "bpe_tokens.extend", "this_link.append", "str2id_links.append", "clip_prompt_utils.whitespace_clean", "clip_prompt_utils.basic_clean", "clip_prompt_utils.SimpleTokenizer.bpe().split", "token.encode", "clip_prompt_utils.SimpleTokenizer.bpe"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.whitespace_clean", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.basic_clean", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.bpe"], ["", "def", "encode", "(", "self", ",", "text", ",", "return_link", "=", "False", ")", ":", "\n", "        ", "bpe_tokens", "=", "[", "]", "\n", "text", "=", "whitespace_clean", "(", "basic_clean", "(", "text", ")", ")", ".", "lower", "(", ")", "\n", "str2id_links", "=", "[", "]", "#  link original sentence word to the tokenized ids of its subwords", "\n", "for", "token", "in", "re", ".", "findall", "(", "self", ".", "pat", ",", "text", ")", ":", "\n", "            ", "this_link", "=", "[", "token", "]", "\n", "token", "=", "''", ".", "join", "(", "self", ".", "byte_encoder", "[", "b", "]", "for", "b", "in", "token", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "ids", "=", "[", "self", ".", "encoder", "[", "bpe_token", "]", "for", "bpe_token", "in", "self", ".", "bpe", "(", "token", ")", ".", "split", "(", "' '", ")", "]", "\n", "bpe_tokens", ".", "extend", "(", "ids", ")", "\n", "this_link", ".", "append", "(", "ids", ")", "\n", "str2id_links", ".", "append", "(", "this_link", ")", "\n", "", "if", "return_link", ":", "\n", "            ", "return", "bpe_tokens", ",", "str2id_links", "\n", "", "return", "bpe_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.SimpleTokenizer.decode": [[143, 147], ["bytearray().decode().replace", "bytearray().decode", "bytearray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode"], ["", "def", "decode", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "text", "=", "''", ".", "join", "(", "[", "self", ".", "decoder", "[", "token", "]", "for", "token", "in", "tokens", "]", ")", "\n", "text", "=", "bytearray", "(", "[", "self", ".", "byte_decoder", "[", "c", "]", "for", "c", "in", "text", "]", ")", ".", "decode", "(", "'utf-8'", ",", "errors", "=", "\"replace\"", ")", ".", "replace", "(", "'</w>'", ",", "' '", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.default_bpe": [[16, 19], ["functools.lru_cache", "os.path.join", "os.path.dirname", "os.path.abspath"], "function", ["None"], ["@", "lru_cache", "(", ")", "\n", "def", "default_bpe", "(", ")", ":", "\n", "    ", "return", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", ",", "\"bpe_simple_vocab_16e6.txt.gz\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.bytes_to_unicode": [[21, 42], ["functools.lru_cache", "range", "dict", "list", "chr", "zip", "list", "list", "range", "bs.append", "cs.append", "range", "range", "ord", "ord", "ord", "ord", "ord", "ord"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "@", "lru_cache", "(", ")", "\n", "def", "bytes_to_unicode", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"", "\n", "bs", "=", "list", "(", "range", "(", "ord", "(", "\"!\"", ")", ",", "ord", "(", "\"~\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00a1\"", ")", ",", "ord", "(", "\"\u00ac\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00ae\"", ")", ",", "ord", "(", "\"\u00ff\"", ")", "+", "1", ")", ")", "\n", "cs", "=", "bs", "[", ":", "]", "\n", "n", "=", "0", "\n", "for", "b", "in", "range", "(", "2", "**", "8", ")", ":", "\n", "        ", "if", "b", "not", "in", "bs", ":", "\n", "            ", "bs", ".", "append", "(", "b", ")", "\n", "cs", ".", "append", "(", "2", "**", "8", "+", "n", ")", "\n", "n", "+=", "1", "\n", "", "", "cs", "=", "[", "chr", "(", "n", ")", "for", "n", "in", "cs", "]", "\n", "return", "dict", "(", "zip", "(", "bs", ",", "cs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.get_pairs": [[44, 54], ["set", "set.add"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "get_pairs", "(", "word", ")", ":", "\n", "    ", "\"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"", "\n", "pairs", "=", "set", "(", ")", "\n", "prev_char", "=", "word", "[", "0", "]", "\n", "for", "char", "in", "word", "[", "1", ":", "]", ":", "\n", "        ", "pairs", ".", "add", "(", "(", "prev_char", ",", "char", ")", ")", "\n", "prev_char", "=", "char", "\n", "", "return", "pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.basic_clean": [[56, 60], ["ftfy.fix_text", "html.unescape", "html.unescape.strip", "html.unescape"], "function", ["None"], ["", "def", "basic_clean", "(", "text", ")", ":", "\n", "    ", "text", "=", "ftfy", ".", "fix_text", "(", "text", ")", "\n", "text", "=", "html", ".", "unescape", "(", "html", ".", "unescape", "(", "text", ")", ")", "\n", "return", "text", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.whitespace_clean": [[62, 66], ["regex.sub", "text.strip.strip"], "function", ["None"], ["", "def", "whitespace_clean", "(", "text", ")", ":", "\n", "    ", "text", "=", "re", ".", "sub", "(", "r'\\s+'", ",", "' '", ",", "text", ")", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.tokenize": [[152, 167], ["isinstance", "torch.zeros", "enumerate", "len", "torch.tensor", "len", "RuntimeError", "_tokenizer.encode", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "", "def", "tokenize", "(", "texts", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ",", "context_length", ":", "int", "=", "77", ")", ":", "\n", "    ", "if", "isinstance", "(", "texts", ",", "str", ")", ":", "\n", "        ", "texts", "=", "[", "texts", "]", "\n", "\n", "", "sot_token", "=", "_tokenizer", ".", "encoder", "[", "\"<|startoftext|>\"", "]", "\n", "eot_token", "=", "_tokenizer", ".", "encoder", "[", "\"<|endoftext|>\"", "]", "\n", "all_tokens", "=", "[", "[", "sot_token", "]", "+", "_tokenizer", ".", "encode", "(", "text", ")", "+", "[", "eot_token", "]", "for", "text", "in", "texts", "]", "\n", "result", "=", "torch", ".", "zeros", "(", "len", "(", "all_tokens", ")", ",", "context_length", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "for", "i", ",", "tokens", "in", "enumerate", "(", "all_tokens", ")", ":", "\n", "        ", "if", "len", "(", "tokens", ")", ">", "context_length", ":", "\n", "            ", "raise", "RuntimeError", "(", "f\"Input {texts[i]} is too long for context length {context_length}\"", ")", "\n", "", "result", "[", "i", ",", ":", "len", "(", "tokens", ")", "]", "=", "torch", ".", "tensor", "(", "tokens", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.get_prompt_templates": [[170, 332], ["None"], "function", ["None"], ["", "def", "get_prompt_templates", "(", ")", ":", "\n", "# prompt_templates = [", "\n", "#     'There is a {} in the scene.',", "\n", "#     'There is the {} in the scene.',", "\n", "#     'a photo of a {} in the scene.',", "\n", "#     'a photo of the {} in the scene.',", "\n", "#     'a photo of one {} in the scene.',", "\n", "\n", "#     'itap of a {}.',", "\n", "#     'itap of my {}.',  # itap: I took a picture of", "\n", "#     'itap of the {}.',", "\n", "#     'a photo of a {}.',", "\n", "#     'a photo of my {}.',", "\n", "#     'a photo of the {}.',", "\n", "#     'a photo of one {}.',", "\n", "#     'a photo of many {}.',", "\n", "\n", "#     'a good photo of a {}.',", "\n", "#     'a good photo of the {}.',", "\n", "#     'a bad photo of a {}.',", "\n", "#     'a bad photo of the {}.',", "\n", "#     'a photo of a nice {}.',", "\n", "#     'a photo of the nice {}.',", "\n", "#     'a photo of a cool {}.',", "\n", "#     'a photo of the cool {}.',", "\n", "#     'a photo of a weird {}.',", "\n", "#     'a photo of the weird {}.',", "\n", "\n", "#     'a photo of a small {}.',", "\n", "#     'a photo of the small {}.',", "\n", "#     'a photo of a large {}.',", "\n", "#     'a photo of the large {}.',", "\n", "\n", "#     'a photo of a clean {}.',", "\n", "#     'a photo of the clean {}.',", "\n", "#     'a photo of a dirty {}.',", "\n", "#     'a photo of the dirty {}.',", "\n", "\n", "#     'a bright photo of a {}.',", "\n", "#     'a bright photo of the {}.',", "\n", "#     'a dark photo of a {}.',", "\n", "#     'a dark photo of the {}.',", "\n", "\n", "#     'a photo of a hard to see {}.',", "\n", "#     'a photo of the hard to see {}.',", "\n", "#     'a low resolution photo of a {}.',", "\n", "#     'a low resolution photo of the {}.',", "\n", "#     'a cropped photo of a {}.',", "\n", "#     'a cropped photo of the {}.',", "\n", "#     'a close-up photo of a {}.',", "\n", "#     'a close-up photo of the {}.',", "\n", "#     'a jpeg corrupted photo of a {}.',", "\n", "#     'a jpeg corrupted photo of the {}.',", "\n", "#     'a blurry photo of a {}.',", "\n", "#     'a blurry photo of the {}.',", "\n", "#     'a pixelated photo of a {}.',", "\n", "#     'a pixelated photo of the {}.',", "\n", "\n", "#     'a black and white photo of the {}.',", "\n", "#     'a black and white photo of a {}.',", "\n", "\n", "#     'a plastic {}.',", "\n", "#     'the plastic {}.',", "\n", "\n", "#     'a toy {}.',", "\n", "#     'the toy {}.',", "\n", "#     'a plushie {}.',", "\n", "#     'the plushie {}.',", "\n", "#     'a cartoon {}.',", "\n", "#     'the cartoon {}.',", "\n", "\n", "#     'an embroidered {}.',", "\n", "#     'the embroidered {}.',", "\n", "\n", "#     'a painting of the {}.',", "\n", "#     'a painting of a {}.',", "\n", "# ]", "\n", "\n", "    ", "prompt_templates", "=", "[", "\n", "'{}.'", ",", "\n", "'a photo of a {}.'", ",", "\n", "'a bad photo of a {}.'", ",", "\n", "'a photo of many {}.'", ",", "\n", "'a sculpture of a {}.'", ",", "\n", "'a photo of the hard to see {}.'", ",", "\n", "'a low resolution photo of the {}.'", ",", "\n", "'a rendering of a {}.'", ",", "\n", "'graffiti of a {}.'", ",", "\n", "'a bad photo of the {}.'", ",", "\n", "'a cropped photo of the {}.'", ",", "\n", "'a tattoo of a {}.'", ",", "\n", "'the embroidered {}.'", ",", "\n", "'a photo of a hard to see {}.'", ",", "\n", "'a bright photo of a {}.'", ",", "\n", "'a photo of a clean {}.'", ",", "\n", "'a photo of a dirty {}.'", ",", "\n", "'a dark photo of the {}.'", ",", "\n", "'a drawing of a {}.'", ",", "\n", "'a photo of my {}.'", ",", "\n", "'the plastic {}.'", ",", "\n", "'a photo of the cool {}.'", ",", "\n", "'a close-up photo of a {}.'", ",", "\n", "'a black and white photo of the {}.'", ",", "\n", "'a painting of the {}.'", ",", "\n", "'a painting of a {}.'", ",", "\n", "'a pixelated photo of the {}.'", ",", "\n", "'a sculpture of the {}.'", ",", "\n", "'a bright photo of the {}.'", ",", "\n", "'a cropped photo of a {}.'", ",", "\n", "'a plastic {}.'", ",", "\n", "'a photo of the dirty {}.'", ",", "\n", "'a jpeg corrupted photo of a {}.'", ",", "\n", "'a blurry photo of the {}.'", ",", "\n", "'a photo of the {}.'", ",", "\n", "'a good photo of the {}.'", ",", "\n", "'a rendering of the {}.'", ",", "\n", "'a {} in a video game.'", ",", "\n", "'a photo of one {}.'", ",", "\n", "'a doodle of a {}.'", ",", "\n", "'a close-up photo of the {}.'", ",", "\n", "'the origami {}.'", ",", "\n", "'the {} in a video game.'", ",", "\n", "'a sketch of a {}.'", ",", "\n", "'a doodle of the {}.'", ",", "\n", "'a origami {}.'", ",", "\n", "'a low resolution photo of a {}.'", ",", "\n", "'the toy {}.'", ",", "\n", "'a rendition of the {}.'", ",", "\n", "'a photo of the clean {}.'", ",", "\n", "'a photo of a large {}.'", ",", "\n", "'a rendition of a {}.'", ",", "\n", "'a photo of a nice {}.'", ",", "\n", "'a photo of a weird {}.'", ",", "\n", "'a blurry photo of a {}.'", ",", "\n", "'a cartoon {}.'", ",", "\n", "'art of a {}.'", ",", "\n", "'a sketch of the {}.'", ",", "\n", "'a embroidered {}.'", ",", "\n", "'a pixelated photo of a {}.'", ",", "\n", "'itap of the {}.'", ",", "\n", "'a jpeg corrupted photo of the {}.'", ",", "\n", "'a good photo of a {}.'", ",", "\n", "'a plushie {}.'", ",", "\n", "'a photo of the nice {}.'", ",", "\n", "'a photo of the small {}.'", ",", "\n", "'a photo of the weird {}.'", ",", "\n", "'the cartoon {}.'", ",", "\n", "'art of the {}.'", ",", "\n", "'a drawing of the {}.'", ",", "\n", "'a photo of the large {}.'", ",", "\n", "'a black and white photo of a {}.'", ",", "\n", "'the plushie {}.'", ",", "\n", "'a dark photo of a {}.'", ",", "\n", "'itap of a {}.'", ",", "\n", "'graffiti of the {}.'", ",", "\n", "'a toy {}.'", ",", "\n", "'itap of my {}.'", ",", "\n", "'a photo of a cool {}.'", ",", "\n", "'a photo of a small {}.'", ",", "\n", "'a tattoo of the {}.'", ",", "\n", "]", "\n", "return", "prompt_templates", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.prompt_engineering": [[333, 335], ["template.replace", "classnames.replace().replace", "classnames.replace"], "function", ["None"], ["", "def", "prompt_engineering", "(", "classnames", ",", "template", "=", "\"\"", ")", ":", "\n", "    ", "return", "template", ".", "replace", "(", "'{}'", ",", "classnames", ".", "replace", "(", "','", ",", "''", ")", ".", "replace", "(", "'+'", ",", "' '", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.convert_example_to_features_bpe": [[337, 353], ["isinstance", "numpy.array", "numpy.zeros", "len", "tokenizer.encode"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "def", "convert_example_to_features_bpe", "(", "text", ",", "tokenizer", ",", "sot_token", ",", "eot_token", ",", "context_length", "=", "77", ")", ":", "\n", "    ", "\"\"\"\n    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample.\n    :param tokenizer: Tokenizer\n    :return: List, a list containing token id, padded by 0\n    \"\"\"", "\n", "assert", "isinstance", "(", "text", ",", "str", ")", "\n", "input_ids", "=", "[", "sot_token", "]", "+", "tokenizer", ".", "encode", "(", "text", ")", "+", "[", "eot_token", "]", "\n", "if", "len", "(", "input_ids", ")", ">", "context_length", ":", "\n", "        ", "input_ids", "=", "input_ids", "[", ":", "context_length", "]", "\n", "", "input_ids", "=", "np", ".", "array", "(", "input_ids", ")", "\n", "\n", "pad_input_ids", "=", "np", ".", "zeros", "(", "context_length", ")", "\n", "pad_input_ids", "[", ":", "input_ids", ".", "shape", "[", "0", "]", "]", "=", "input_ids", "\n", "\n", "return", "pad_input_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.get_cls_names": [[354, 403], ["sorted", "len", "cls_n.replace", "cls_n.replace", "cls_n.replace", "open", "min", "max", "len", "names.append", "row.split", "range"], "function", ["None"], ["", "def", "get_cls_names", "(", "filter_novel", "=", "False", ",", "coco", "=", "None", ",", "from_file", "=", "False", ")", ":", "\n", "    ", "\"\"\" return a list of strings with each string as name of a class\n    \"\"\"", "\n", "# the names are stored in a txt file", "\n", "if", "from_file", ":", "\n", "# coco_det_cls = {COCO_80_ALL_CLS[key]: key for key in COCO_80_ALL_CLS} ", "\n", "# # not found in nouns {'skis': 31, 'sports ball': 33, 'hot dog': 53, 'potted plant': 59, 'scissors': 77, 'hair drier': 79}", "\n", "# coco_det_cls['ski'] = 81", "\n", "# coco_det_cls['scissor'] = 82", "\n", "# with open('/home/v-yiwuzhong/projects/azureblobs/vyiwuzhong_phillytools/trained_models/concept_pool/COCO_Caption_nouns_4688.txt','w') as g:", "\n", "#     with open(from_file, 'r') as f:", "\n", "#         cnt = 0", "\n", "#         for row in f:", "\n", "#             if row.split(\",\")[0] not in coco_det_cls:", "\n", "#                 g.write(row)", "\n", "#                 cnt += 1", "\n", "#             else:", "\n", "#                 coco_det_cls.pop(row.split(\",\")[0])", "\n", "        ", "names", "=", "[", "]", "\n", "with", "open", "(", "from_file", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "row", "in", "f", ":", "\n", "                ", "names", ".", "append", "(", "row", ".", "split", "(", "\",\"", ")", "[", "0", "]", ")", "\n", "", "", "return", "names", "\n", "# classes' names", "\n", "", "if", "coco", "==", "'target'", ":", "\n", "        ", "return", "COCO_UNSEEN_CLS", "\n", "", "elif", "coco", "==", "'base'", ":", "\n", "        ", "return", "COCO_SEEN_CLS", "\n", "", "elif", "coco", "==", "'all'", ":", "\n", "        ", "return", "COCO_OVD_ALL_CLS", "\n", "", "elif", "coco", "==", "'all_80'", ":", "\n", "        ", "return", "[", "COCO_80_ALL_CLS", "[", "i", "+", "1", "]", "for", "i", "in", "range", "(", "80", ")", "]", "\n", "", "assert", "len", "(", "LVIS_V1_CATEGORIES", ")", "==", "1203", "\n", "cat_ids", "=", "[", "k", "[", "\"id\"", "]", "for", "k", "in", "LVIS_V1_CATEGORIES", "]", "\n", "assert", "min", "(", "cat_ids", ")", "==", "1", "and", "max", "(", "cat_ids", ")", "==", "len", "(", "\n", "cat_ids", "\n", ")", ",", "\"Category ids are not in [1, #categories], as expected\"", "\n", "# Ensure that the category list is sorted by id", "\n", "lvis_categories", "=", "sorted", "(", "LVIS_V1_CATEGORIES", ",", "key", "=", "lambda", "x", ":", "x", "[", "\"id\"", "]", ")", "\n", "if", "filter_novel", ":", "\n", "        ", "class_names", "=", "[", "cls_meta", "[", "'name'", "]", "for", "cls_meta", "in", "lvis_categories", "if", "cls_meta", "[", "'frequency'", "]", "!=", "'r'", "]", "\n", "", "else", ":", "\n", "        ", "class_names", "=", "[", "cls_meta", "[", "'name'", "]", "for", "cls_meta", "in", "lvis_categories", "]", "\n", "\n", "# remove or replace special symbols", "\n", "", "class_names", "=", "[", "cls_n", ".", "replace", "(", "\"_\"", ",", "\" \"", ")", "for", "cls_n", "in", "class_names", "]", "\n", "class_names", "=", "[", "cls_n", ".", "replace", "(", "\"(\"", ",", "\"\"", ")", "for", "cls_n", "in", "class_names", "]", "\n", "class_names", "=", "[", "cls_n", ".", "replace", "(", "\")\"", ",", "\"\"", ")", "for", "cls_n", "in", "class_names", "]", "\n", "return", "class_names", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.clip_prompt_utils.pre_tokenize": [[404, 438], ["clip_prompt_utils.SimpleTokenizer", "clip_prompt_utils.get_prompt_templates", "range", "torch.stack", "len", "isinstance", "input_ids_all.append", "isinstance", "clip_prompt_utils.convert_example_to_features_bpe", "input_ids.append", "torch.stack", "t1s.append", "torch.tensor", "clip_prompt_utils.prompt_engineering"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.get_prompt_templates", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.convert_example_to_features_bpe", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.prompt_engineering"], ["", "def", "pre_tokenize", "(", "class_names", ")", ":", "\n", "    ", "\"\"\"\n    pre-tokenize class names\n    :param class_names: List, a list of class names\n    :param tokenizer: Tokenizer, SimpleTokenizer()\n    :return: Tensor, containing all prompts for all classes, [#cls, #prompts, context_length]\n    \"\"\"", "\n", "# tokenizer", "\n", "tokenizer", "=", "SimpleTokenizer", "(", ")", "\n", "sot_token", "=", "tokenizer", ".", "encoder", "[", "\"<|startoftext|>\"", "]", "\n", "eot_token", "=", "tokenizer", ".", "encoder", "[", "\"<|endoftext|>\"", "]", "\n", "\n", "# prompt engineering", "\n", "prompt_templates", "=", "get_prompt_templates", "(", ")", "\n", "input_ids_all", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "len", "(", "class_names", ")", ")", ":", "\n", "        ", "v", "=", "class_names", "[", "k", "]", "\n", "if", "isinstance", "(", "v", ",", "str", ")", ":", "\n", "            ", "vs", "=", "[", "v", "]", "\n", "", "elif", "isinstance", "(", "v", ",", "list", ")", ":", "\n", "            ", "vs", "=", "v", "\n", "", "t1s", "=", "[", "]", "\n", "for", "v", "in", "vs", ":", "\n", "            ", "for", "pt", "in", "prompt_templates", ":", "\n", "                ", "t1s", ".", "append", "(", "prompt_engineering", "(", "v", ",", "template", "=", "pt", ")", ")", "\n", "", "", "input_ids", "=", "[", "]", "\n", "for", "t1", "in", "t1s", ":", "\n", "            ", "this_input_ids", "=", "convert_example_to_features_bpe", "(", "t1", ",", "tokenizer", ",", "sot_token", ",", "eot_token", ")", "\n", "input_ids", ".", "append", "(", "torch", ".", "tensor", "(", "this_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "\n", "", "input_ids_all", ".", "append", "(", "torch", ".", "stack", "(", "input_ids", ",", "0", ")", ")", "\n", "\n", "", "input_ids_all_classes", "=", "torch", ".", "stack", "(", "input_ids_all", ",", "0", ")", "\n", "return", "input_ids_all_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis.register_lvis_instances": [[27, 45], ["detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "lvis.load_lvis_json", "detectron2.data.MetadataCatalog.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis.load_lvis_json", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "register_lvis_instances", "(", "name", ",", "metadata", ",", "json_file", ",", "image_root", ",", "args", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Register a dataset in LVIS's json annotation format for instance detection and segmentation.\n\n    Args:\n        name (str): a name that identifies the dataset, e.g. \"lvis_v0.5_train\".\n        metadata (dict): extra metadata associated with this dataset. It can be an empty dict.\n        json_file (str): path to the json instance annotation file.\n        image_root (str or path-like): directory which contains all the images.\n    \"\"\"", "\n", "if", "args", "is", "not", "None", ":", "\n", "        ", "filter_open_cls", "=", "args", "[", "'filter_open_cls'", "]", "\n", "run_custom_img", "=", "args", "[", "'run_custom_img'", "]", "\n", "\n", "", "DatasetCatalog", ".", "register", "(", "name", ",", "lambda", ":", "load_lvis_json", "(", "json_file", ",", "image_root", ",", "name", ",", "filter_open_cls", "=", "filter_open_cls", ",", "run_custom_img", "=", "run_custom_img", ")", ")", "\n", "MetadataCatalog", ".", "get", "(", "name", ")", ".", "set", "(", "\n", "json_file", "=", "json_file", ",", "image_root", "=", "image_root", ",", "evaluator_type", "=", "\"lvis\"", ",", "**", "metadata", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis.load_lvis_json": [[48, 203], ["detectron2.utils.file_io.PathManager.get_local_path", "fvcore.common.timer.Timer", "LVIS", "sorted", "LVIS.load_imgs", "list", "logger.info", "lvis.load_lvis_custom_img", "fvcore.common.timer.Timer.seconds", "logger.info", "lvis.get_lvis_instances_meta", "detectron2.data.MetadataCatalog.get().set", "LVIS.imgs.keys", "len", "len", "zip", "os.path.join", "lvis.load_lvis_json.get_file_name"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis.load_lvis_custom_img", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis.get_lvis_instances_meta", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "load_lvis_json", "(", "json_file", ",", "image_root", ",", "dataset_name", "=", "None", ",", "filter_open_cls", "=", "False", ",", "clip_gt_crop", "=", "True", ",", "max_gt_per_img", "=", "500", ",", "run_custom_img", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Load a json file in LVIS's annotation format.\n\n    Args:\n        json_file (str): full path to the LVIS json annotation file.\n        image_root (str): the directory where the images in this json file exists.\n        dataset_name (str): the name of the dataset (e.g., \"lvis_v0.5_train\").\n            If provided, this function will put \"thing_classes\" into the metadata\n            associated with this dataset.\n        filter_open_cls: open-set setting, filter the rare (novel) categories during training\n        clip_gt_crop: whether apply CLIP on GT regions; must filter images with empty annotations or too many GT bbox\n    Returns:\n        list[dict]: a list of dicts in Detectron2 standard format. (See\n        `Using Custom Datasets </tutorials/datasets.html>`_ )\n\n    Notes:\n        1. This function does not read the image files.\n           The results do not have the \"image\" field.\n    \"\"\"", "\n", "if", "run_custom_img", ":", "\n", "        ", "return", "load_lvis_custom_img", "(", ")", "\n", "\n", "", "from", "lvis", "import", "LVIS", "\n", "\n", "if", "'train'", "in", "dataset_name", "and", "filter_open_cls", ":", "# openset training setting, filter the novel classes during training", "\n", "        ", "filter_open_cls", "=", "True", "\n", "", "else", ":", "\n", "        ", "filter_open_cls", "=", "False", "\n", "\n", "", "json_file", "=", "PathManager", ".", "get_local_path", "(", "json_file", ")", "\n", "\n", "timer", "=", "Timer", "(", ")", "\n", "lvis_api", "=", "LVIS", "(", "json_file", ")", "\n", "if", "timer", ".", "seconds", "(", ")", ">", "1", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading {} takes {:.2f} seconds.\"", ".", "format", "(", "json_file", ",", "timer", ".", "seconds", "(", ")", ")", ")", "\n", "\n", "", "if", "dataset_name", "is", "not", "None", ":", "\n", "        ", "meta", "=", "get_lvis_instances_meta", "(", "dataset_name", ")", "\n", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", ".", "set", "(", "**", "meta", ")", "\n", "\n", "# sort indices for reproducible results", "\n", "", "img_ids", "=", "sorted", "(", "lvis_api", ".", "imgs", ".", "keys", "(", ")", ")", "\n", "# imgs is a list of dicts, each looks something like:", "\n", "# {'license': 4,", "\n", "#  'url': 'http://farm6.staticflickr.com/5454/9413846304_881d5e5c3b_z.jpg',", "\n", "#  'file_name': 'COCO_val2014_000000001268.jpg',", "\n", "#  'height': 427,", "\n", "#  'width': 640,", "\n", "#  'date_captured': '2013-11-17 05:57:24',", "\n", "#  'id': 1268}", "\n", "imgs", "=", "lvis_api", ".", "load_imgs", "(", "img_ids", ")", "\n", "# anns is a list[list[dict]], where each dict is an annotation", "\n", "# record for an object. The inner list enumerates the objects in an image", "\n", "# and the outer list enumerates over images. Example of anns[0]:", "\n", "# [{'segmentation': [[192.81,", "\n", "#     247.09,", "\n", "#     ...", "\n", "#     219.03,", "\n", "#     249.06]],", "\n", "#   'area': 1035.749,", "\n", "#   'image_id': 1268,", "\n", "#   'bbox': [192.81, 224.8, 74.73, 33.43],", "\n", "#   'category_id': 16,", "\n", "#   'id': 42986},", "\n", "#  ...]", "\n", "anns", "=", "[", "lvis_api", ".", "img_ann_map", "[", "img_id", "]", "for", "img_id", "in", "img_ids", "]", "\n", "\n", "# Sanity check that each annotation has a unique id", "\n", "ann_ids", "=", "[", "ann", "[", "\"id\"", "]", "for", "anns_per_image", "in", "anns", "for", "ann", "in", "anns_per_image", "]", "\n", "assert", "len", "(", "set", "(", "ann_ids", ")", ")", "==", "len", "(", "ann_ids", ")", ",", "\"Annotation ids in '{}' are not unique\"", ".", "format", "(", "\n", "json_file", "\n", ")", "\n", "\n", "imgs_anns", "=", "list", "(", "zip", "(", "imgs", ",", "anns", ")", ")", "\n", "\n", "logger", ".", "info", "(", "\"Loaded {} images in the LVIS format from {}\"", ".", "format", "(", "len", "(", "imgs_anns", ")", ",", "json_file", ")", ")", "\n", "\n", "def", "get_file_name", "(", "img_root", ",", "img_dict", ")", ":", "\n", "# Determine the path including the split folder (\"train2017\", \"val2017\", \"test2017\") from", "\n", "# the coco_url field. Example:", "\n", "#   'coco_url': 'http://images.cocodataset.org/train2017/000000155379.jpg'", "\n", "        ", "split_folder", ",", "file_name", "=", "img_dict", "[", "\"coco_url\"", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "2", ":", "]", "\n", "return", "os", ".", "path", ".", "join", "(", "img_root", "+", "split_folder", ",", "file_name", ")", "\n", "\n", "", "dataset_dicts", "=", "[", "]", "\n", "cls_type_dict", "=", "{", "cls_meta", "[", "'id'", "]", ":", "cls_meta", "[", "'frequency'", "]", "for", "cls_meta", "in", "lvis_api", ".", "dataset", "[", "'categories'", "]", "}", "# map cls id to cls type", "\n", "area_dict", "=", "{", "'r'", ":", "[", "]", ",", "'c'", ":", "[", "]", ",", "'f'", ":", "[", "]", "}", "# calculate box area for each type of class", "\n", "\n", "for", "(", "img_dict", ",", "anno_dict_list", ")", "in", "imgs_anns", ":", "\n", "        ", "record", "=", "{", "}", "\n", "record", "[", "\"file_name\"", "]", "=", "get_file_name", "(", "image_root", ",", "img_dict", ")", "\n", "record", "[", "\"height\"", "]", "=", "img_dict", "[", "\"height\"", "]", "\n", "record", "[", "\"width\"", "]", "=", "img_dict", "[", "\"width\"", "]", "\n", "record", "[", "\"not_exhaustive_category_ids\"", "]", "=", "img_dict", ".", "get", "(", "\"not_exhaustive_category_ids\"", ",", "[", "]", ")", "\n", "record", "[", "\"neg_category_ids\"", "]", "=", "img_dict", ".", "get", "(", "\"neg_category_ids\"", ",", "[", "]", ")", "\n", "image_id", "=", "record", "[", "\"image_id\"", "]", "=", "img_dict", "[", "\"id\"", "]", "\n", "\n", "objs", "=", "[", "]", "\n", "for", "anno", "in", "anno_dict_list", ":", "\n", "# Check that the image_id in this annotation is the same as", "\n", "# the image_id we're looking at.", "\n", "# This fails only when the data parsing logic or the annotation file is buggy.", "\n", "            ", "assert", "anno", "[", "\"image_id\"", "]", "==", "image_id", "\n", "obj", "=", "{", "\"bbox\"", ":", "anno", "[", "\"bbox\"", "]", ",", "\"bbox_mode\"", ":", "BoxMode", ".", "XYWH_ABS", "}", "\n", "# LVIS data loader can be used to load COCO dataset categories. In this case `meta`", "\n", "# variable will have a field with COCO-specific category mapping.", "\n", "if", "dataset_name", "is", "not", "None", "and", "\"thing_dataset_id_to_contiguous_id\"", "in", "meta", ":", "\n", "                ", "obj", "[", "\"category_id\"", "]", "=", "meta", "[", "\"thing_dataset_id_to_contiguous_id\"", "]", "[", "anno", "[", "\"category_id\"", "]", "]", "\n", "", "else", ":", "\n", "                ", "obj", "[", "\"category_id\"", "]", "=", "anno", "[", "\"category_id\"", "]", "-", "1", "# Convert 1-indexed to 0-indexed", "\n", "", "obj", "[", "'frequency'", "]", "=", "cls_type_dict", "[", "anno", "[", "\"category_id\"", "]", "]", "# used for open-set filtering", "\n", "\n", "if", "filter_open_cls", ":", "# filter categories for open-set training", "\n", "                ", "if", "obj", "[", "'frequency'", "]", "==", "'r'", ":", "\n", "                    ", "continue", "\n", "", "", "area_dict", "[", "obj", "[", "'frequency'", "]", "]", ".", "append", "(", "anno", "[", "\"bbox\"", "]", "[", "2", "]", "*", "anno", "[", "\"bbox\"", "]", "[", "3", "]", ")", "\n", "\n", "segm", "=", "anno", "[", "\"segmentation\"", "]", "# list[list[float]]", "\n", "# filter out invalid polygons (< 3 points)", "\n", "valid_segm", "=", "[", "poly", "for", "poly", "in", "segm", "if", "len", "(", "poly", ")", "%", "2", "==", "0", "and", "len", "(", "poly", ")", ">=", "6", "]", "\n", "assert", "len", "(", "segm", ")", "==", "len", "(", "\n", "valid_segm", "\n", ")", ",", "\"Annotation contains an invalid polygon with < 3 points\"", "\n", "assert", "len", "(", "segm", ")", ">", "0", "\n", "obj", "[", "\"segmentation\"", "]", "=", "segm", "\n", "objs", ".", "append", "(", "obj", ")", "\n", "", "if", "(", "filter_open_cls", "or", "clip_gt_crop", ")", "and", "len", "(", "objs", ")", "==", "0", ":", "# no annotation for this image", "\n", "            ", "continue", "\n", "", "record", "[", "\"annotations\"", "]", "=", "objs", "\n", "dataset_dicts", ".", "append", "(", "record", ")", "\n", "\n", "# For the training in open-set setting, map original category id to new category id number (base categories)", "\n", "", "if", "filter_open_cls", ":", "\n", "# get new category id in order", "\n", "        ", "old_to_new", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "cls_type_dict", ")", ")", ":", "\n", "            ", "if", "cls_type_dict", "[", "i", "+", "1", "]", "!=", "'r'", ":", "# cls_type_dict is 1-indexed", "\n", "                ", "old_to_new", "[", "i", "]", "=", "len", "(", "old_to_new", ")", "\n", "# map annotation to new category id", "\n", "", "", "for", "record", "in", "dataset_dicts", ":", "\n", "            ", "record", ".", "pop", "(", "'not_exhaustive_category_ids'", ")", "# won't be used", "\n", "record", ".", "pop", "(", "'neg_category_ids'", ")", "# won't be used", "\n", "for", "obj", "in", "record", "[", "'annotations'", "]", ":", "\n", "                ", "obj", "[", "'category_id'", "]", "=", "old_to_new", "[", "obj", "[", "'category_id'", "]", "]", "# 0-indexed id", "\n", "assert", "obj", "[", "'frequency'", "]", "!=", "'r'", "\n", "", "", "logger", ".", "info", "(", "\"\\n\\nModel will be trained in the open-set setting! {} / {} categories are kept.\\n\"", ".", "format", "(", "len", "(", "old_to_new", ")", ",", "len", "(", "cls_type_dict", ")", ")", ")", "\n", "\n", "# apply CLIP on GT regions: some images has large number of GT bbox (eg, 759), remove them, otherwise, OOM", "\n", "", "if", "clip_gt_crop", ":", "\n", "        ", "dataset_dicts", "=", "sorted", "(", "dataset_dicts", ",", "key", "=", "lambda", "x", ":", "len", "(", "x", "[", "\"annotations\"", "]", ")", ",", "reverse", "=", "True", ")", "\n", "for", "record", "in", "dataset_dicts", ":", "\n", "            ", "record", "[", "\"annotations\"", "]", "=", "record", "[", "\"annotations\"", "]", "[", ":", "max_gt_per_img", "]", "# only <10 per 20k images in test have >300 GT boxes", "\n", "\n", "", "", "return", "dataset_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis.load_lvis_custom_img": [[204, 223], ["enumerate", "os.path.join", "PIL.Image.open", "dataset_dicts.append", "os.listdir"], "function", ["None"], ["", "def", "load_lvis_custom_img", "(", "custom_img_path", "=", "'datasets/custom_images'", ")", ":", "\n", "    ", "\"\"\"\n    This is a tentitive function for loading custom images.\n    Given a folder of images (eg, 'datasets/custom_images'), load their meta data into a dictionary\n    \"\"\"", "\n", "custom_img_list", "=", "[", "os", ".", "path", ".", "join", "(", "custom_img_path", ",", "item", ")", "for", "item", "in", "os", ".", "listdir", "(", "custom_img_path", ")", "]", "\n", "\n", "dataset_dicts", "=", "[", "]", "\n", "for", "f_i", ",", "file_name", "in", "enumerate", "(", "custom_img_list", ")", ":", "\n", "        ", "record", "=", "{", "}", "\n", "record", "[", "\"file_name\"", "]", "=", "file_name", "\n", "img_file", "=", "Image", ".", "open", "(", "record", "[", "\"file_name\"", "]", ")", "\n", "record", "[", "\"height\"", "]", "=", "img_file", ".", "size", "[", "1", "]", "\n", "record", "[", "\"width\"", "]", "=", "img_file", ".", "size", "[", "0", "]", "\n", "record", "[", "\"image_id\"", "]", "=", "f_i", "\n", "\n", "dataset_dicts", ".", "append", "(", "record", ")", "\n", "\n", "", "return", "dataset_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis.get_lvis_instances_meta": [[224, 241], ["ValueError", "builtin_meta._get_coco_instances_meta", "lvis._get_lvis_instances_meta_v0_5", "lvis._get_lvis_instances_meta_v1"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_coco_instances_meta", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis._get_lvis_instances_meta_v0_5", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis._get_lvis_instances_meta_v1"], ["", "def", "get_lvis_instances_meta", "(", "dataset_name", ")", ":", "\n", "    ", "\"\"\"\n    Load LVIS metadata.\n\n    Args:\n        dataset_name (str): LVIS dataset name without the split name (e.g., \"lvis_v0.5\").\n\n    Returns:\n        dict: LVIS metadata with keys: thing_classes\n    \"\"\"", "\n", "if", "\"cocofied\"", "in", "dataset_name", ":", "\n", "        ", "return", "_get_coco_instances_meta", "(", ")", "\n", "", "if", "\"v0.5\"", "in", "dataset_name", ":", "\n", "        ", "return", "_get_lvis_instances_meta_v0_5", "(", ")", "\n", "", "elif", "\"v1\"", "in", "dataset_name", ":", "\n", "        ", "return", "_get_lvis_instances_meta_v1", "(", ")", "\n", "", "raise", "ValueError", "(", "\"No built-in metadata for dataset {}\"", ".", "format", "(", "dataset_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis._get_lvis_instances_meta_v0_5": [[243, 254], ["sorted", "len", "min", "max", "len"], "function", ["None"], ["", "def", "_get_lvis_instances_meta_v0_5", "(", ")", ":", "\n", "    ", "assert", "len", "(", "LVIS_V0_5_CATEGORIES", ")", "==", "1230", "\n", "cat_ids", "=", "[", "k", "[", "\"id\"", "]", "for", "k", "in", "LVIS_V0_5_CATEGORIES", "]", "\n", "assert", "min", "(", "cat_ids", ")", "==", "1", "and", "max", "(", "cat_ids", ")", "==", "len", "(", "\n", "cat_ids", "\n", ")", ",", "\"Category ids are not in [1, #categories], as expected\"", "\n", "# Ensure that the category list is sorted by id", "\n", "lvis_categories", "=", "sorted", "(", "LVIS_V0_5_CATEGORIES", ",", "key", "=", "lambda", "x", ":", "x", "[", "\"id\"", "]", ")", "\n", "thing_classes", "=", "[", "k", "[", "\"synonyms\"", "]", "[", "0", "]", "for", "k", "in", "lvis_categories", "]", "\n", "meta", "=", "{", "\"thing_classes\"", ":", "thing_classes", "}", "\n", "return", "meta", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis._get_lvis_instances_meta_v1": [[256, 267], ["sorted", "len", "min", "max", "len"], "function", ["None"], ["", "def", "_get_lvis_instances_meta_v1", "(", ")", ":", "\n", "    ", "assert", "len", "(", "LVIS_V1_CATEGORIES", ")", "==", "1203", "\n", "cat_ids", "=", "[", "k", "[", "\"id\"", "]", "for", "k", "in", "LVIS_V1_CATEGORIES", "]", "\n", "assert", "min", "(", "cat_ids", ")", "==", "1", "and", "max", "(", "cat_ids", ")", "==", "len", "(", "\n", "cat_ids", "\n", ")", ",", "\"Category ids are not in [1, #categories], as expected\"", "\n", "# Ensure that the category list is sorted by id", "\n", "lvis_categories", "=", "sorted", "(", "LVIS_V1_CATEGORIES", ",", "key", "=", "lambda", "x", ":", "x", "[", "\"id\"", "]", ")", "\n", "thing_classes", "=", "[", "k", "[", "\"synonyms\"", "]", "[", "0", "]", "for", "k", "in", "lvis_categories", "]", "\n", "meta", "=", "{", "\"thing_classes\"", ":", "thing_classes", "}", "\n", "return", "meta", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin.register_all_coco": [[115, 163], ["_PREDEFINED_SPLITS_COCO.items", "_PREDEFINED_SPLITS_COCO_PANOPTIC.items", "detectron2.data.MetadataCatalog.get", "coco_panoptic.register_coco_panoptic_separated", "coco_panoptic.register_coco_panoptic", "splits_per_dataset.items", "splits_per_dataset.items", "builtin_meta._get_builtin_metadata", "os.path.join", "os.path.join", "os.path.join", "builtin_meta._get_builtin_metadata", "os.path.join", "os.path.join", "coco.register_coco_instances", "coco.register_coco_instances", "os.path.join", "builtin_meta._get_builtin_metadata", "os.path.join", "len", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco_panoptic.register_coco_panoptic_separated", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco_panoptic.register_coco_panoptic", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_builtin_metadata", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_builtin_metadata", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.register_coco_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.register_coco_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_builtin_metadata"], ["def", "register_all_coco", "(", "root", ")", ":", "\n", "    ", "for", "dataset_name", ",", "splits_per_dataset", "in", "_PREDEFINED_SPLITS_COCO", ".", "items", "(", ")", ":", "\n", "        ", "if", "dataset_name", "==", "'coco_ovd'", ":", "# for zero-shot split", "\n", "            ", "for", "key", ",", "(", "image_root", ",", "json_file", ")", "in", "splits_per_dataset", ".", "items", "(", ")", ":", "\n", "# Assume pre-defined datasets live in `./datasets`.", "\n", "                ", "register_coco_instances", "(", "\n", "key", ",", "\n", "{", "}", ",", "# empty metadata, it will be overwritten in load_coco_json() function", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "json_file", ")", "if", "\"://\"", "not", "in", "json_file", "else", "json_file", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "image_root", ")", ",", "\n", ")", "\n", "", "", "else", ":", "# default splits", "\n", "            ", "for", "key", ",", "(", "image_root", ",", "json_file", ")", "in", "splits_per_dataset", ".", "items", "(", ")", ":", "\n", "# Assume pre-defined datasets live in `./datasets`.", "\n", "                ", "register_coco_instances", "(", "\n", "key", ",", "\n", "_get_builtin_metadata", "(", "dataset_name", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "json_file", ")", "if", "\"://\"", "not", "in", "json_file", "else", "json_file", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "image_root", ")", ",", "\n", ")", "\n", "\n", "", "", "", "for", "(", "\n", "prefix", ",", "\n", "(", "panoptic_root", ",", "panoptic_json", ",", "semantic_root", ")", ",", "\n", ")", "in", "_PREDEFINED_SPLITS_COCO_PANOPTIC", ".", "items", "(", ")", ":", "\n", "        ", "prefix_instances", "=", "prefix", "[", ":", "-", "len", "(", "\"_panoptic\"", ")", "]", "\n", "instances_meta", "=", "MetadataCatalog", ".", "get", "(", "prefix_instances", ")", "\n", "image_root", ",", "instances_json", "=", "instances_meta", ".", "image_root", ",", "instances_meta", ".", "json_file", "\n", "# The \"separated\" version of COCO panoptic segmentation dataset,", "\n", "# e.g. used by Panoptic FPN", "\n", "register_coco_panoptic_separated", "(", "\n", "prefix", ",", "\n", "_get_builtin_metadata", "(", "\"coco_panoptic_separated\"", ")", ",", "\n", "image_root", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "panoptic_root", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "panoptic_json", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "semantic_root", ")", ",", "\n", "instances_json", ",", "\n", ")", "\n", "# The \"standard\" version of COCO panoptic segmentation dataset,", "\n", "# e.g. used by Panoptic-DeepLab", "\n", "register_coco_panoptic", "(", "\n", "prefix", ",", "\n", "_get_builtin_metadata", "(", "\"coco_panoptic_standard\"", ")", ",", "\n", "image_root", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "panoptic_root", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "panoptic_json", ")", ",", "\n", "instances_json", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin.register_all_lvis": [[203, 218], ["_PREDEFINED_SPLITS_LVIS.items", "splits_per_dataset.items", "lvis.register_lvis_instances", "lvis.get_lvis_instances_meta", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis.register_lvis_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.lvis.get_lvis_instances_meta"], ["def", "register_all_lvis", "(", "root", ")", ":", "\n", "    ", "for", "dataset_name", ",", "splits_per_dataset", "in", "_PREDEFINED_SPLITS_LVIS", ".", "items", "(", ")", ":", "\n", "        ", "for", "key", ",", "(", "image_root", ",", "json_file", ")", "in", "splits_per_dataset", ".", "items", "(", ")", ":", "\n", "            ", "if", "dataset_name", "==", "\"lvis_v1\"", ":", "\n", "                ", "args", "=", "{", "'filter_open_cls'", ":", "True", ",", "'run_custom_img'", ":", "False", "}", "\n", "", "elif", "dataset_name", "==", "'lvis_v1_custom_img'", ":", "\n", "                ", "args", "=", "{", "'filter_open_cls'", ":", "False", ",", "'run_custom_img'", ":", "True", "}", "\n", "", "elif", "dataset_name", "==", "'lvis_v1_fullysup'", ":", "\n", "                ", "args", "=", "{", "'filter_open_cls'", ":", "False", ",", "'run_custom_img'", ":", "False", "}", "\n", "", "register_lvis_instances", "(", "\n", "key", ",", "\n", "get_lvis_instances_meta", "(", "dataset_name", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "json_file", ")", "if", "\"://\"", "not", "in", "json_file", "else", "json_file", ",", "\n", "os", ".", "path", ".", "join", "(", "root", ",", "image_root", ")", ",", "\n", "args", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin.register_all_cityscapes": [[229, 256], ["_RAW_CITYSCAPES_SPLITS.items", "builtin_meta._get_builtin_metadata", "os.path.join", "os.path.join", "key.format", "detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "key.format", "detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "cityscapes.load_cityscapes_instances", "detectron2.data.MetadataCatalog.get", "cityscapes.load_cityscapes_semantic", "detectron2.data.MetadataCatalog.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin_meta._get_builtin_metadata", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes.load_cityscapes_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.cityscapes.load_cityscapes_semantic", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "register_all_cityscapes", "(", "root", ")", ":", "\n", "    ", "for", "key", ",", "(", "image_dir", ",", "gt_dir", ")", "in", "_RAW_CITYSCAPES_SPLITS", ".", "items", "(", ")", ":", "\n", "        ", "meta", "=", "_get_builtin_metadata", "(", "\"cityscapes\"", ")", "\n", "image_dir", "=", "os", ".", "path", ".", "join", "(", "root", ",", "image_dir", ")", "\n", "gt_dir", "=", "os", ".", "path", ".", "join", "(", "root", ",", "gt_dir", ")", "\n", "\n", "inst_key", "=", "key", ".", "format", "(", "task", "=", "\"instance_seg\"", ")", "\n", "DatasetCatalog", ".", "register", "(", "\n", "inst_key", ",", "\n", "lambda", "x", "=", "image_dir", ",", "y", "=", "gt_dir", ":", "load_cityscapes_instances", "(", "\n", "x", ",", "y", ",", "from_json", "=", "True", ",", "to_polygons", "=", "True", "\n", ")", ",", "\n", ")", "\n", "MetadataCatalog", ".", "get", "(", "inst_key", ")", ".", "set", "(", "\n", "image_dir", "=", "image_dir", ",", "gt_dir", "=", "gt_dir", ",", "evaluator_type", "=", "\"cityscapes_instance\"", ",", "**", "meta", "\n", ")", "\n", "\n", "sem_key", "=", "key", ".", "format", "(", "task", "=", "\"sem_seg\"", ")", "\n", "DatasetCatalog", ".", "register", "(", "\n", "sem_key", ",", "lambda", "x", "=", "image_dir", ",", "y", "=", "gt_dir", ":", "load_cityscapes_semantic", "(", "x", ",", "y", ")", "\n", ")", "\n", "MetadataCatalog", ".", "get", "(", "sem_key", ")", ".", "set", "(", "\n", "image_dir", "=", "image_dir", ",", "\n", "gt_dir", "=", "gt_dir", ",", "\n", "evaluator_type", "=", "\"cityscapes_sem_seg\"", ",", "\n", "ignore_label", "=", "255", ",", "\n", "**", "meta", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin.register_all_pascal_voc": [[260, 274], ["pascal_voc.register_pascal_voc", "os.path.join", "detectron2.data.MetadataCatalog.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.datasets.pascal_voc.register_pascal_voc", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "register_all_pascal_voc", "(", "root", ")", ":", "\n", "    ", "SPLITS", "=", "[", "\n", "(", "\"voc_2007_trainval\"", ",", "\"VOC2007\"", ",", "\"trainval\"", ")", ",", "\n", "(", "\"voc_2007_train\"", ",", "\"VOC2007\"", ",", "\"train\"", ")", ",", "\n", "(", "\"voc_2007_val\"", ",", "\"VOC2007\"", ",", "\"val\"", ")", ",", "\n", "(", "\"voc_2007_test\"", ",", "\"VOC2007\"", ",", "\"test\"", ")", ",", "\n", "(", "\"voc_2012_trainval\"", ",", "\"VOC2012\"", ",", "\"trainval\"", ")", ",", "\n", "(", "\"voc_2012_train\"", ",", "\"VOC2012\"", ",", "\"train\"", ")", ",", "\n", "(", "\"voc_2012_val\"", ",", "\"VOC2012\"", ",", "\"val\"", ")", ",", "\n", "]", "\n", "for", "name", ",", "dirname", ",", "split", "in", "SPLITS", ":", "\n", "        ", "year", "=", "2007", "if", "\"2007\"", "in", "name", "else", "2012", "\n", "register_pascal_voc", "(", "name", ",", "os", ".", "path", ".", "join", "(", "root", ",", "dirname", ")", ",", "split", ",", "year", ")", "\n", "MetadataCatalog", ".", "get", "(", "name", ")", ".", "evaluator_type", "=", "\"pascal_voc\"", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.builtin.register_all_ade20k": [[276, 291], ["os.path.join", "os.path.join", "os.path.join", "detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "coco.load_sem_seg", "detectron2.data.MetadataCatalog.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.load_sem_seg", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "def", "register_all_ade20k", "(", "root", ")", ":", "\n", "    ", "root", "=", "os", ".", "path", ".", "join", "(", "root", ",", "\"ADEChallengeData2016\"", ")", "\n", "for", "name", ",", "dirname", "in", "[", "(", "\"train\"", ",", "\"training\"", ")", ",", "(", "\"val\"", ",", "\"validation\"", ")", "]", ":", "\n", "        ", "image_dir", "=", "os", ".", "path", ".", "join", "(", "root", ",", "\"images\"", ",", "dirname", ")", "\n", "gt_dir", "=", "os", ".", "path", ".", "join", "(", "root", ",", "\"annotations_detectron2\"", ",", "dirname", ")", "\n", "name", "=", "f\"ade20k_sem_seg_{name}\"", "\n", "DatasetCatalog", ".", "register", "(", "\n", "name", ",", "lambda", "x", "=", "image_dir", ",", "y", "=", "gt_dir", ":", "load_sem_seg", "(", "y", ",", "x", ",", "gt_ext", "=", "\"png\"", ",", "image_ext", "=", "\"jpg\"", ")", "\n", ")", "\n", "MetadataCatalog", ".", "get", "(", "name", ")", ".", "set", "(", "\n", "stuff_classes", "=", "ADE20K_SEM_SEG_CATEGORIES", "[", ":", "]", ",", "\n", "image_root", "=", "image_dir", ",", "\n", "sem_seg_root", "=", "gt_dir", ",", "\n", "evaluator_type", "=", "\"sem_seg\"", ",", "\n", "ignore_label", "=", "255", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.pascal_voc.load_voc_instances": [[25, 76], ["detectron2.utils.file_io.PathManager.get_local_path", "detectron2.utils.file_io.PathManager.open", "numpy.loadtxt", "os.path.join", "os.path.join", "os.path.join", "ET.parse.findall", "dicts.append", "os.path.join", "detectron2.utils.file_io.PathManager.open", "xml.parse", "int", "int", "obj.find", "instances.append", "obj.find", "float", "class_names.index", "ET.parse.findall", "ET.parse.findall", "obj.find.find"], "function", ["None"], ["def", "load_voc_instances", "(", "dirname", ":", "str", ",", "split", ":", "str", ",", "class_names", ":", "Union", "[", "List", "[", "str", "]", ",", "Tuple", "[", "str", ",", "...", "]", "]", ")", ":", "\n", "    ", "\"\"\"\n    Load Pascal VOC detection annotations to Detectron2 format.\n\n    Args:\n        dirname: Contain \"Annotations\", \"ImageSets\", \"JPEGImages\"\n        split (str): one of \"train\", \"test\", \"val\", \"trainval\"\n        class_names: list or tuple of class names\n    \"\"\"", "\n", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "dirname", ",", "\"ImageSets\"", ",", "\"Main\"", ",", "split", "+", "\".txt\"", ")", ")", "as", "f", ":", "\n", "        ", "fileids", "=", "np", ".", "loadtxt", "(", "f", ",", "dtype", "=", "np", ".", "str", ")", "\n", "\n", "# Needs to read many small annotation files. Makes sense at local", "\n", "", "annotation_dirname", "=", "PathManager", ".", "get_local_path", "(", "os", ".", "path", ".", "join", "(", "dirname", ",", "\"Annotations/\"", ")", ")", "\n", "dicts", "=", "[", "]", "\n", "for", "fileid", "in", "fileids", ":", "\n", "        ", "anno_file", "=", "os", ".", "path", ".", "join", "(", "annotation_dirname", ",", "fileid", "+", "\".xml\"", ")", "\n", "jpeg_file", "=", "os", ".", "path", ".", "join", "(", "dirname", ",", "\"JPEGImages\"", ",", "fileid", "+", "\".jpg\"", ")", "\n", "\n", "with", "PathManager", ".", "open", "(", "anno_file", ")", "as", "f", ":", "\n", "            ", "tree", "=", "ET", ".", "parse", "(", "f", ")", "\n", "\n", "", "r", "=", "{", "\n", "\"file_name\"", ":", "jpeg_file", ",", "\n", "\"image_id\"", ":", "fileid", ",", "\n", "\"height\"", ":", "int", "(", "tree", ".", "findall", "(", "\"./size/height\"", ")", "[", "0", "]", ".", "text", ")", ",", "\n", "\"width\"", ":", "int", "(", "tree", ".", "findall", "(", "\"./size/width\"", ")", "[", "0", "]", ".", "text", ")", ",", "\n", "}", "\n", "instances", "=", "[", "]", "\n", "\n", "for", "obj", "in", "tree", ".", "findall", "(", "\"object\"", ")", ":", "\n", "            ", "cls", "=", "obj", ".", "find", "(", "\"name\"", ")", ".", "text", "\n", "# We include \"difficult\" samples in training.", "\n", "# Based on limited experiments, they don't hurt accuracy.", "\n", "# difficult = int(obj.find(\"difficult\").text)", "\n", "# if difficult == 1:", "\n", "# continue", "\n", "bbox", "=", "obj", ".", "find", "(", "\"bndbox\"", ")", "\n", "bbox", "=", "[", "float", "(", "bbox", ".", "find", "(", "x", ")", ".", "text", ")", "for", "x", "in", "[", "\"xmin\"", ",", "\"ymin\"", ",", "\"xmax\"", ",", "\"ymax\"", "]", "]", "\n", "# Original annotations are integers in the range [1, W or H]", "\n", "# Assuming they mean 1-based pixel indices (inclusive),", "\n", "# a box with annotation (xmin=1, xmax=W) covers the whole image.", "\n", "# In coordinate space this is represented by (xmin=0, xmax=W)", "\n", "bbox", "[", "0", "]", "-=", "1.0", "\n", "bbox", "[", "1", "]", "-=", "1.0", "\n", "instances", ".", "append", "(", "\n", "{", "\"category_id\"", ":", "class_names", ".", "index", "(", "cls", ")", ",", "\"bbox\"", ":", "bbox", ",", "\"bbox_mode\"", ":", "BoxMode", ".", "XYXY_ABS", "}", "\n", ")", "\n", "", "r", "[", "\"annotations\"", "]", "=", "instances", "\n", "dicts", ".", "append", "(", "r", ")", "\n", "", "return", "dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.pascal_voc.register_pascal_voc": [[78, 82], ["detectron2.data.DatasetCatalog.register", "detectron2.data.MetadataCatalog.get().set", "pascal_voc.load_voc_instances", "detectron2.data.MetadataCatalog.get", "list"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._DatasetCatalog.register", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.pascal_voc.load_voc_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "register_pascal_voc", "(", "name", ",", "dirname", ",", "split", ",", "year", ",", "class_names", "=", "CLASS_NAMES", ")", ":", "\n", "    ", "DatasetCatalog", ".", "register", "(", "name", ",", "lambda", ":", "load_voc_instances", "(", "dirname", ",", "split", ",", "class_names", ")", ")", "\n", "MetadataCatalog", ".", "get", "(", "name", ")", ".", "set", "(", "\n", "thing_classes", "=", "list", "(", "class_names", ")", ",", "dirname", "=", "dirname", ",", "year", "=", "year", ",", "split", "=", "split", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.grouped_batch_sampler.GroupedBatchSampler.__init__": [[14, 36], ["numpy.asarray", "numpy.unique().tolist", "isinstance", "ValueError", "numpy.unique"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sampler", ",", "group_ids", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            sampler (Sampler): Base sampler.\n            group_ids (list[int]): If the sampler produces indices in range [0, N),\n                `group_ids` must be a list of `N` ints which contains the group id of each sample.\n                The group ids must be a set of integers in the range [0, num_groups).\n            batch_size (int): Size of mini-batch.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "sampler", ",", "Sampler", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"sampler should be an instance of \"", "\n", "\"torch.utils.data.Sampler, but got sampler={}\"", ".", "format", "(", "sampler", ")", "\n", ")", "\n", "", "self", ".", "sampler", "=", "sampler", "\n", "self", ".", "group_ids", "=", "np", ".", "asarray", "(", "group_ids", ")", "\n", "assert", "self", ".", "group_ids", ".", "ndim", "==", "1", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "groups", "=", "np", ".", "unique", "(", "self", ".", "group_ids", ")", ".", "tolist", "(", ")", "\n", "\n", "# buffer the indices of each group until batch size is reached", "\n", "self", ".", "buffer_per_group", "=", "{", "k", ":", "[", "]", "for", "k", "in", "groups", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.grouped_batch_sampler.GroupedBatchSampler.__iter__": [[37, 45], ["group_buffer.append", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "idx", "in", "self", ".", "sampler", ":", "\n", "            ", "group_id", "=", "self", ".", "group_ids", "[", "idx", "]", "\n", "group_buffer", "=", "self", ".", "buffer_per_group", "[", "group_id", "]", "\n", "group_buffer", ".", "append", "(", "idx", ")", "\n", "if", "len", "(", "group_buffer", ")", "==", "self", ".", "batch_size", ":", "\n", "                ", "yield", "group_buffer", "[", ":", "]", "# yield a copy of the list", "\n", "del", "group_buffer", "[", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.grouped_batch_sampler.GroupedBatchSampler.__len__": [[46, 48], ["NotImplementedError"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"len() of GroupedBatchSampler is not well-defined.\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.TrainingSampler.__init__": [[24, 42], ["int", "detectron2.utils.comm.get_rank", "detectron2.utils.comm.get_world_size", "detectron2.utils.comm.shared_random_seed"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.shared_random_seed"], ["def", "__init__", "(", "self", ",", "size", ":", "int", ",", "shuffle", ":", "bool", "=", "True", ",", "seed", ":", "Optional", "[", "int", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            size (int): the total number of data of the underlying dataset to sample from\n            shuffle (bool): whether to shuffle the indices or not\n            seed (int): the initial seed of the shuffle. Must be the same\n                across all workers. If None, will use a random seed shared\n                among workers (require synchronization among all workers).\n        \"\"\"", "\n", "self", ".", "_size", "=", "size", "\n", "assert", "size", ">", "0", "\n", "self", ".", "_shuffle", "=", "shuffle", "\n", "if", "seed", "is", "None", ":", "\n", "            ", "seed", "=", "comm", ".", "shared_random_seed", "(", ")", "\n", "", "self", ".", "_seed", "=", "int", "(", "seed", ")", "\n", "\n", "self", ".", "_rank", "=", "comm", ".", "get_rank", "(", ")", "\n", "self", ".", "_world_size", "=", "comm", ".", "get_world_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.TrainingSampler.__iter__": [[43, 46], ["itertools.islice", "distributed_sampler.TrainingSampler._infinite_indices"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler._infinite_indices"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "start", "=", "self", ".", "_rank", "\n", "yield", "from", "itertools", ".", "islice", "(", "self", ".", "_infinite_indices", "(", ")", ",", "start", ",", "None", ",", "self", ".", "_world_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.TrainingSampler._infinite_indices": [[47, 55], ["torch.Generator", "torch.Generator.manual_seed", "torch.randperm().tolist", "torch.arange().tolist", "torch.randperm", "torch.arange"], "methods", ["None"], ["", "def", "_infinite_indices", "(", "self", ")", ":", "\n", "        ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "_seed", ")", "\n", "while", "True", ":", "\n", "            ", "if", "self", ".", "_shuffle", ":", "\n", "                ", "yield", "from", "torch", ".", "randperm", "(", "self", ".", "_size", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "                ", "yield", "from", "torch", ".", "arange", "(", "self", ".", "_size", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler.__init__": [[63, 84], ["int", "detectron2.utils.comm.get_rank", "detectron2.utils.comm.get_world_size", "torch.trunc", "detectron2.utils.comm.shared_random_seed"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.shared_random_seed"], ["def", "__init__", "(", "self", ",", "repeat_factors", ",", "*", ",", "shuffle", "=", "True", ",", "seed", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            repeat_factors (Tensor): a float vector, the repeat factor for each indice. When it's\n                full of ones, it is equivalent to ``TrainingSampler(len(repeat_factors), ...)``.\n            shuffle (bool): whether to shuffle the indices or not\n            seed (int): the initial seed of the shuffle. Must be the same\n                across all workers. If None, will use a random seed shared\n                among workers (require synchronization among all workers).\n        \"\"\"", "\n", "self", ".", "_shuffle", "=", "shuffle", "\n", "if", "seed", "is", "None", ":", "\n", "            ", "seed", "=", "comm", ".", "shared_random_seed", "(", ")", "\n", "", "self", ".", "_seed", "=", "int", "(", "seed", ")", "\n", "\n", "self", ".", "_rank", "=", "comm", ".", "get_rank", "(", ")", "\n", "self", ".", "_world_size", "=", "comm", ".", "get_world_size", "(", ")", "\n", "\n", "# Split into whole number (_int_part) and fractional (_frac_part) parts.", "\n", "self", ".", "_int_part", "=", "torch", ".", "trunc", "(", "repeat_factors", ")", "\n", "self", ".", "_frac_part", "=", "repeat_factors", "-", "self", ".", "_int_part", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler.repeat_factors_from_category_frequency": [[85, 131], ["collections.defaultdict", "len", "collections.defaultdict.items", "torch.tensor", "max", "max", "rep_factors.append", "math.sqrt", "collections.defaultdict.items"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "repeat_factors_from_category_frequency", "(", "dataset_dicts", ",", "repeat_thresh", ")", ":", "\n", "        ", "\"\"\"\n        Compute (fractional) per-image repeat factors based on category frequency.\n        The repeat factor for an image is a function of the frequency of the rarest\n        category labeled in that image. The \"frequency of category c\" in [0, 1] is defined\n        as the fraction of images in the training set (without repeats) in which category c\n        appears.\n        See :paper:`lvis` (>= v2) Appendix B.2.\n\n        Args:\n            dataset_dicts (list[dict]): annotations in Detectron2 dataset format.\n            repeat_thresh (float): frequency threshold below which data is repeated.\n                If the frequency is half of `repeat_thresh`, the image will be\n                repeated twice.\n\n        Returns:\n            torch.Tensor:\n                the i-th element is the repeat factor for the dataset image at index i.\n        \"\"\"", "\n", "# 1. For each category c, compute the fraction of images that contain it: f(c)", "\n", "category_freq", "=", "defaultdict", "(", "int", ")", "\n", "for", "dataset_dict", "in", "dataset_dicts", ":", "# For each image (without repeats)", "\n", "            ", "cat_ids", "=", "{", "ann", "[", "\"category_id\"", "]", "for", "ann", "in", "dataset_dict", "[", "\"annotations\"", "]", "}", "\n", "for", "cat_id", "in", "cat_ids", ":", "\n", "                ", "category_freq", "[", "cat_id", "]", "+=", "1", "\n", "", "", "num_images", "=", "len", "(", "dataset_dicts", ")", "\n", "for", "k", ",", "v", "in", "category_freq", ".", "items", "(", ")", ":", "\n", "            ", "category_freq", "[", "k", "]", "=", "v", "/", "num_images", "\n", "\n", "# 2. For each category c, compute the category-level repeat factor:", "\n", "#    r(c) = max(1, sqrt(t / f(c)))", "\n", "", "category_rep", "=", "{", "\n", "cat_id", ":", "max", "(", "1.0", ",", "math", ".", "sqrt", "(", "repeat_thresh", "/", "cat_freq", ")", ")", "\n", "for", "cat_id", ",", "cat_freq", "in", "category_freq", ".", "items", "(", ")", "\n", "}", "\n", "\n", "# 3. For each image I, compute the image-level repeat factor:", "\n", "#    r(I) = max_{c in I} r(c)", "\n", "rep_factors", "=", "[", "]", "\n", "for", "dataset_dict", "in", "dataset_dicts", ":", "\n", "            ", "cat_ids", "=", "{", "ann", "[", "\"category_id\"", "]", "for", "ann", "in", "dataset_dict", "[", "\"annotations\"", "]", "}", "\n", "rep_factor", "=", "max", "(", "{", "category_rep", "[", "cat_id", "]", "for", "cat_id", "in", "cat_ids", "}", ",", "default", "=", "1.0", ")", "\n", "rep_factors", ".", "append", "(", "rep_factor", ")", "\n", "\n", "", "return", "torch", ".", "tensor", "(", "rep_factors", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler._get_epoch_indices": [[132, 154], ["torch.rand", "enumerate", "torch.tensor", "len", "indices.extend", "int", "rep_factor.item"], "methods", ["None"], ["", "def", "_get_epoch_indices", "(", "self", ",", "generator", ")", ":", "\n", "        ", "\"\"\"\n        Create a list of dataset indices (with repeats) to use for one epoch.\n\n        Args:\n            generator (torch.Generator): pseudo random number generator used for\n                stochastic rounding.\n\n        Returns:\n            torch.Tensor: list of dataset indices to use in one epoch. Each index\n                is repeated based on its calculated repeat factor.\n        \"\"\"", "\n", "# Since repeat factors are fractional, we use stochastic rounding so", "\n", "# that the target repeat factor is achieved in expectation over the", "\n", "# course of training", "\n", "rands", "=", "torch", ".", "rand", "(", "len", "(", "self", ".", "_frac_part", ")", ",", "generator", "=", "generator", ")", "\n", "rep_factors", "=", "self", ".", "_int_part", "+", "(", "rands", "<", "self", ".", "_frac_part", ")", ".", "float", "(", ")", "\n", "# Construct a list of indices in which we repeat images as specified", "\n", "indices", "=", "[", "]", "\n", "for", "dataset_index", ",", "rep_factor", "in", "enumerate", "(", "rep_factors", ")", ":", "\n", "            ", "indices", ".", "extend", "(", "[", "dataset_index", "]", "*", "int", "(", "rep_factor", ".", "item", "(", ")", ")", ")", "\n", "", "return", "torch", ".", "tensor", "(", "indices", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler.__iter__": [[155, 158], ["itertools.islice", "distributed_sampler.RepeatFactorTrainingSampler._infinite_indices"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler._infinite_indices"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "start", "=", "self", ".", "_rank", "\n", "yield", "from", "itertools", ".", "islice", "(", "self", ".", "_infinite_indices", "(", ")", ",", "start", ",", "None", ",", "self", ".", "_world_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler._infinite_indices": [[159, 171], ["torch.Generator", "torch.Generator.manual_seed", "distributed_sampler.RepeatFactorTrainingSampler._get_epoch_indices", "torch.randperm", "len", "indices[].tolist", "distributed_sampler.RepeatFactorTrainingSampler.tolist"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.RepeatFactorTrainingSampler._get_epoch_indices"], ["", "def", "_infinite_indices", "(", "self", ")", ":", "\n", "        ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "_seed", ")", "\n", "while", "True", ":", "\n", "# Sample indices with repeats determined by stochastic rounding; each", "\n", "# \"epoch\" may have a slightly different size due to the rounding.", "\n", "            ", "indices", "=", "self", ".", "_get_epoch_indices", "(", "g", ")", "\n", "if", "self", ".", "_shuffle", ":", "\n", "                ", "randperm", "=", "torch", ".", "randperm", "(", "len", "(", "indices", ")", ",", "generator", "=", "g", ")", "\n", "yield", "from", "indices", "[", "randperm", "]", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "                ", "yield", "from", "indices", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.InferenceSampler.__init__": [[181, 195], ["detectron2.utils.comm.get_rank", "detectron2.utils.comm.get_world_size", "min", "range"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size"], ["def", "__init__", "(", "self", ",", "size", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            size (int): the total number of data of the underlying dataset to sample from\n        \"\"\"", "\n", "self", ".", "_size", "=", "size", "\n", "assert", "size", ">", "0", "\n", "self", ".", "_rank", "=", "comm", ".", "get_rank", "(", ")", "\n", "self", ".", "_world_size", "=", "comm", ".", "get_world_size", "(", ")", "\n", "\n", "shard_size", "=", "(", "self", ".", "_size", "-", "1", ")", "//", "self", ".", "_world_size", "+", "1", "\n", "begin", "=", "shard_size", "*", "self", ".", "_rank", "\n", "end", "=", "min", "(", "shard_size", "*", "(", "self", ".", "_rank", "+", "1", ")", ",", "self", ".", "_size", ")", "\n", "self", ".", "_local_indices", "=", "range", "(", "begin", ",", "end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.InferenceSampler.__iter__": [[196, 198], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "yield", "from", "self", ".", "_local_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.samplers.distributed_sampler.InferenceSampler.__len__": [[199, 201], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_local_indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv.InputExample.__init__": [[7, 28], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "tokens_a", ",", "tokens_b", "=", "None", ",", "is_next", "=", "None", ",", "\n", "lm_labels", "=", "None", ",", "img_id", "=", "None", ",", "is_img_match", "=", "None", ",", "\n", "img_label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            tokens_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            tokens_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "tokens_a", "=", "tokens_a", "\n", "self", ".", "tokens_b", "=", "tokens_b", "\n", "self", ".", "is_next", "=", "is_next", "# nextSentence", "\n", "self", ".", "lm_labels", "=", "lm_labels", "# masked words for language model", "\n", "\n", "self", ".", "img_id", "=", "img_id", "\n", "self", ".", "is_img_match", "=", "is_img_match", "\n", "self", ".", "img_label", "=", "img_label", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv.InputFeatures.__init__": [[32, 42], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "is_next", ",", "\n", "lm_label_ids", ",", "img_feat_len", ",", "is_img_match", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "is_next", "=", "is_next", "\n", "self", ".", "lm_label_ids", "=", "lm_label_ids", "\n", "\n", "self", ".", "img_feat_len", "=", "img_feat_len", "\n", "self", ".", "is_img_match", "=", "is_img_match", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv.random_word": [[44, 83], ["enumerate", "random.random", "output_label.append", "output_label.append", "output_label.append", "logging.warning", "random.choice", "list", "tokenizer.vocab.items"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "", "def", "random_word", "(", "tokens", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"\n    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n    :param tokens: list of str, tokenized sentence.\n    :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here)\n    :return: (list of str, list of int), masked tokens and related labels for LM prediction\n    \"\"\"", "\n", "output_label", "=", "[", "]", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "prob", "=", "random", ".", "random", "(", ")", "\n", "# mask token with 15% probability", "\n", "if", "prob", "<", "0.15", ":", "\n", "            ", "prob", "/=", "0.15", "\n", "\n", "# 80% randomly change token to mask token", "\n", "if", "prob", "<", "0.8", ":", "\n", "                ", "tokens", "[", "i", "]", "=", "\"[MASK]\"", "\n", "\n", "# 10% randomly change token to random token", "\n", "", "elif", "prob", "<", "0.9", ":", "\n", "                ", "tokens", "[", "i", "]", "=", "random", ".", "choice", "(", "list", "(", "tokenizer", ".", "vocab", ".", "items", "(", ")", ")", ")", "[", "0", "]", "\n", "\n", "# -> rest 10% randomly keep current token", "\n", "\n", "# append current token to output (we will predict these later)", "\n", "", "try", ":", "\n", "                ", "output_label", ".", "append", "(", "tokenizer", ".", "vocab", "[", "token", "]", ")", "\n", "", "except", "KeyError", ":", "\n", "# For unknown words (should not occur with BPE vocab)", "\n", "                ", "output_label", ".", "append", "(", "tokenizer", ".", "vocab", "[", "\"[UNK]\"", "]", ")", "\n", "logging", ".", "warning", "(", "\n", "\"Cannot find token '{}' in vocab. Using [UNK] insetad\"", ".", "format", "(", "\n", "token", ")", ")", "\n", "", "", "else", ":", "\n", "# no masking token (will be ignored by loss function later)", "\n", "            ", "output_label", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "", "return", "tokens", ",", "output_label", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv.convert_example_to_features": [[85, 202], ["clip_tsv.random_word", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "tokenizer.convert_tokens_to_ids", "clip_tsv.InputFeatures", "clip_tsv._truncate_seq_pair", "clip_tsv.random_word", "tokens.append", "segment_ids.append", "tokens.append", "segment_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "lm_label_ids.append", "len", "len", "len", "len", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "len", "len", "tokens.append", "segment_ids.append", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv.random_word", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv._truncate_seq_pair", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv.random_word"], ["", "def", "convert_example_to_features", "(", "args", ",", "example", ",", "max_seq_length", ",", "tokenizer", ",", "\n", "img_feat_len", ")", ":", "\n", "    ", "\"\"\"\n    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample with\n    IDs, LM labels, input_mask, CLS and SEP tokens etc.\n    :param args: parameter settings\n    :param img_feat_len: lens of actual img features\n    :param example: InputExample, containing sentence input as strings and is_next label\n    :param max_seq_length: int, maximum length of sequence.\n    :param tokenizer: Tokenizer\n    :return: InputFeatures, containing all inputs and labels of one sample as IDs (as used for model training)\n    \"\"\"", "\n", "\n", "tokens_a", "=", "example", ".", "tokens_a", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "tokens_b", ":", "\n", "        ", "tokens_b", "=", "example", ".", "tokens_b", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "        ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "            ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "", "tokens_a", ",", "t1_label", "=", "random_word", "(", "tokens_a", ",", "tokenizer", ")", "\n", "if", "tokens_b", ":", "\n", "        ", "tokens_b", ",", "t2_label", "=", "random_word", "(", "tokens_b", ",", "tokenizer", ")", "\n", "\n", "# concatenate lm labels and account for CLS, SEP, SEP", "\n", "", "if", "tokens_b", ":", "\n", "        ", "lm_label_ids", "=", "(", "[", "-", "1", "]", "+", "t1_label", "+", "[", "-", "1", "]", "+", "t2_label", "+", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "lm_label_ids", "=", "(", "[", "-", "1", "]", "+", "t1_label", "+", "[", "-", "1", "]", ")", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "tokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "tokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "for", "token", "in", "tokens_a", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "        ", "assert", "len", "(", "tokens_b", ")", ">", "0", "\n", "for", "token", "in", "tokens_b", ":", "\n", "            ", "tokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "lm_label_ids", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "lm_label_ids", ")", "==", "max_seq_length", "\n", "\n", "# image features", "\n", "if", "args", ".", "max_img_seq_length", ">", "0", ":", "\n", "        ", "if", "img_feat_len", ">", "args", ".", "max_img_seq_length", ":", "\n", "            ", "input_mask", "=", "input_mask", "+", "[", "1", "]", "*", "img_feat_len", "\n", "", "else", ":", "\n", "            ", "input_mask", "=", "input_mask", "+", "[", "1", "]", "*", "img_feat_len", "\n", "pad_img_feat_len", "=", "args", ".", "max_img_seq_length", "-", "img_feat_len", "\n", "input_mask", "=", "input_mask", "+", "(", "[", "0", "]", "*", "pad_img_feat_len", ")", "\n", "\n", "", "", "lm_label_ids", "=", "lm_label_ids", "+", "[", "-", "1", "]", "*", "args", ".", "max_img_seq_length", "\n", "\n", "if", "example", ".", "guid", "<", "1", ":", "\n", "        ", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logging", ".", "info", "(", "\"guid: %s\"", "%", "example", ".", "guid", ")", "\n", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logging", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logging", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logging", ".", "info", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logging", ".", "info", "(", "\"LM label: %s \"", "%", "lm_label_ids", ")", "\n", "logging", ".", "info", "(", "\"Is next sentence label: %s \"", "%", "example", ".", "is_next", ")", "\n", "\n", "", "features", "=", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "lm_label_ids", "=", "lm_label_ids", ",", "\n", "is_next", "=", "example", ".", "is_next", ",", "\n", "img_feat_len", "=", "img_feat_len", ",", "\n", "is_img_match", "=", "example", ".", "is_img_match", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv._truncate_seq_pair": [[204, 219], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.__init__": [[66, 83], ["clip_prompt_engineering.default_bpe", "clip_prompt_engineering.bytes_to_unicode", "gzip.open().read().decode().split", "list", "list.extend", "dict", "dict", "regex.compile", "tuple", "bytes_to_unicode().values", "list.append", "zip", "zip", "clip_prompt_engineering.SimpleTokenizer.byte_encoder.items", "gzip.open().read().decode", "merge.split", "range", "clip_prompt_engineering.SimpleTokenizer.encoder.items", "range", "clip_prompt_engineering.bytes_to_unicode", "len", "len", "gzip.open().read", "gzip.open"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.default_bpe", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.bytes_to_unicode", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.bytes_to_unicode"], ["    ", "def", "__init__", "(", "self", ",", "bpe_path", ":", "str", "=", "default_bpe", "(", ")", ")", ":", "\n", "        ", "self", ".", "byte_encoder", "=", "bytes_to_unicode", "(", ")", "\n", "self", ".", "byte_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "byte_encoder", ".", "items", "(", ")", "}", "\n", "merges", "=", "gzip", ".", "open", "(", "bpe_path", ")", ".", "read", "(", ")", ".", "decode", "(", "\"utf-8\"", ")", ".", "split", "(", "'\\n'", ")", "\n", "merges", "=", "merges", "[", "1", ":", "49152", "-", "256", "-", "2", "+", "1", "]", "\n", "merges", "=", "[", "tuple", "(", "merge", ".", "split", "(", ")", ")", "for", "merge", "in", "merges", "]", "\n", "vocab", "=", "list", "(", "bytes_to_unicode", "(", ")", ".", "values", "(", ")", ")", "\n", "vocab", "=", "vocab", "+", "[", "v", "+", "'</w>'", "for", "v", "in", "vocab", "]", "\n", "self", ".", "vocab", "=", "vocab", "\n", "for", "merge", "in", "merges", ":", "\n", "            ", "vocab", ".", "append", "(", "''", ".", "join", "(", "merge", ")", ")", "\n", "", "vocab", ".", "extend", "(", "[", "'<|startoftext|>'", ",", "'<|endoftext|>'", "]", ")", "\n", "self", ".", "encoder", "=", "dict", "(", "zip", "(", "vocab", ",", "range", "(", "len", "(", "vocab", ")", ")", ")", ")", "\n", "self", ".", "decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "encoder", ".", "items", "(", ")", "}", "\n", "self", ".", "bpe_ranks", "=", "dict", "(", "zip", "(", "merges", ",", "range", "(", "len", "(", "merges", ")", ")", ")", ")", "\n", "self", ".", "cache", "=", "{", "'<|startoftext|>'", ":", "'<|startoftext|>'", ",", "'<|endoftext|>'", ":", "'<|endoftext|>'", "}", "\n", "self", ".", "pat", "=", "re", ".", "compile", "(", "r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\"", ",", "re", ".", "IGNORECASE", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.bpe": [[84, 124], ["clip_prompt_engineering.get_pairs", "tuple", "min", "tuple", "len", "len", "clip_prompt_engineering.get_pairs", "word.index", "tuple.extend", "tuple.append", "tuple.append", "clip_prompt_engineering.SimpleTokenizer.bpe_ranks.get", "tuple.extend", "float", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.get_pairs", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.get_pairs", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "bpe", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "token", "in", "self", ".", "cache", ":", "\n", "            ", "return", "self", ".", "cache", "[", "token", "]", "\n", "", "word", "=", "tuple", "(", "token", "[", ":", "-", "1", "]", ")", "+", "(", "token", "[", "-", "1", "]", "+", "'</w>'", ",", ")", "\n", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "\n", "if", "not", "pairs", ":", "\n", "            ", "return", "token", "+", "'</w>'", "\n", "\n", "", "while", "True", ":", "\n", "            ", "bigram", "=", "min", "(", "pairs", ",", "key", "=", "lambda", "pair", ":", "self", ".", "bpe_ranks", ".", "get", "(", "pair", ",", "float", "(", "'inf'", ")", ")", ")", "\n", "if", "bigram", "not", "in", "self", ".", "bpe_ranks", ":", "\n", "                ", "break", "\n", "", "first", ",", "second", "=", "bigram", "\n", "new_word", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "word", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "j", "=", "word", ".", "index", "(", "first", ",", "i", ")", "\n", "new_word", ".", "extend", "(", "word", "[", "i", ":", "j", "]", ")", "\n", "i", "=", "j", "\n", "", "except", ":", "\n", "                    ", "new_word", ".", "extend", "(", "word", "[", "i", ":", "]", ")", "\n", "break", "\n", "\n", "", "if", "word", "[", "i", "]", "==", "first", "and", "i", "<", "len", "(", "word", ")", "-", "1", "and", "word", "[", "i", "+", "1", "]", "==", "second", ":", "\n", "                    ", "new_word", ".", "append", "(", "first", "+", "second", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                    ", "new_word", ".", "append", "(", "word", "[", "i", "]", ")", "\n", "i", "+=", "1", "\n", "", "", "new_word", "=", "tuple", "(", "new_word", ")", "\n", "word", "=", "new_word", "\n", "if", "len", "(", "word", ")", "==", "1", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "", "", "word", "=", "' '", ".", "join", "(", "word", ")", "\n", "self", ".", "cache", "[", "token", "]", "=", "word", "\n", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode": [[125, 132], ["whitespace_clean().lower", "regex.findall", "bpe_tokens.extend", "clip_prompt_engineering.whitespace_clean", "clip_prompt_engineering.basic_clean", "token.encode", "clip_prompt_engineering.SimpleTokenizer.bpe().split", "clip_prompt_engineering.SimpleTokenizer.bpe"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.whitespace_clean", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.basic_clean", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.bpe"], ["", "def", "encode", "(", "self", ",", "text", ")", ":", "\n", "        ", "bpe_tokens", "=", "[", "]", "\n", "text", "=", "whitespace_clean", "(", "basic_clean", "(", "text", ")", ")", ".", "lower", "(", ")", "\n", "for", "token", "in", "re", ".", "findall", "(", "self", ".", "pat", ",", "text", ")", ":", "\n", "            ", "token", "=", "''", ".", "join", "(", "self", ".", "byte_encoder", "[", "b", "]", "for", "b", "in", "token", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "bpe_tokens", ".", "extend", "(", "self", ".", "encoder", "[", "bpe_token", "]", "for", "bpe_token", "in", "self", ".", "bpe", "(", "token", ")", ".", "split", "(", "' '", ")", ")", "\n", "", "return", "bpe_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode": [[133, 137], ["bytearray().decode().replace", "bytearray().decode", "bytearray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode"], ["", "def", "decode", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "text", "=", "''", ".", "join", "(", "[", "self", ".", "decoder", "[", "token", "]", "for", "token", "in", "tokens", "]", ")", "\n", "text", "=", "bytearray", "(", "[", "self", ".", "byte_decoder", "[", "c", "]", "for", "c", "in", "text", "]", ")", ".", "decode", "(", "'utf-8'", ",", "errors", "=", "\"replace\"", ")", ".", "replace", "(", "'</w>'", ",", "' '", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.default_bpe": [[13, 16], ["functools.lru_cache", "os.path.join", "os.path.dirname", "os.path.abspath"], "function", ["None"], ["@", "lru_cache", "(", ")", "\n", "def", "default_bpe", "(", ")", ":", "\n", "    ", "return", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", ",", "\"bpe_simple_vocab_16e6.txt.gz\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.bytes_to_unicode": [[18, 39], ["functools.lru_cache", "range", "dict", "list", "chr", "zip", "list", "list", "range", "bs.append", "cs.append", "range", "range", "ord", "ord", "ord", "ord", "ord", "ord"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "@", "lru_cache", "(", ")", "\n", "def", "bytes_to_unicode", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"", "\n", "bs", "=", "list", "(", "range", "(", "ord", "(", "\"!\"", ")", ",", "ord", "(", "\"~\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00a1\"", ")", ",", "ord", "(", "\"\u00ac\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00ae\"", ")", ",", "ord", "(", "\"\u00ff\"", ")", "+", "1", ")", ")", "\n", "cs", "=", "bs", "[", ":", "]", "\n", "n", "=", "0", "\n", "for", "b", "in", "range", "(", "2", "**", "8", ")", ":", "\n", "        ", "if", "b", "not", "in", "bs", ":", "\n", "            ", "bs", ".", "append", "(", "b", ")", "\n", "cs", ".", "append", "(", "2", "**", "8", "+", "n", ")", "\n", "n", "+=", "1", "\n", "", "", "cs", "=", "[", "chr", "(", "n", ")", "for", "n", "in", "cs", "]", "\n", "return", "dict", "(", "zip", "(", "bs", ",", "cs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.get_pairs": [[41, 51], ["set", "set.add"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set"], ["", "def", "get_pairs", "(", "word", ")", ":", "\n", "    ", "\"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"", "\n", "pairs", "=", "set", "(", ")", "\n", "prev_char", "=", "word", "[", "0", "]", "\n", "for", "char", "in", "word", "[", "1", ":", "]", ":", "\n", "        ", "pairs", ".", "add", "(", "(", "prev_char", ",", "char", ")", ")", "\n", "prev_char", "=", "char", "\n", "", "return", "pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.basic_clean": [[53, 57], ["ftfy.fix_text", "html.unescape", "html.unescape.strip", "html.unescape"], "function", ["None"], ["", "def", "basic_clean", "(", "text", ")", ":", "\n", "    ", "text", "=", "ftfy", ".", "fix_text", "(", "text", ")", "\n", "text", "=", "html", ".", "unescape", "(", "html", ".", "unescape", "(", "text", ")", ")", "\n", "return", "text", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.whitespace_clean": [[59, 63], ["regex.sub", "text.strip.strip"], "function", ["None"], ["", "def", "whitespace_clean", "(", "text", ")", ":", "\n", "    ", "text", "=", "re", ".", "sub", "(", "r'\\s+'", ",", "' '", ",", "text", ")", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.tokenize": [[142, 157], ["isinstance", "torch.zeros", "enumerate", "len", "torch.tensor", "len", "RuntimeError", "_tokenizer.encode", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "", "def", "tokenize", "(", "texts", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ",", "context_length", ":", "int", "=", "77", ")", ":", "\n", "    ", "if", "isinstance", "(", "texts", ",", "str", ")", ":", "\n", "        ", "texts", "=", "[", "texts", "]", "\n", "\n", "", "sot_token", "=", "_tokenizer", ".", "encoder", "[", "\"<|startoftext|>\"", "]", "\n", "eot_token", "=", "_tokenizer", ".", "encoder", "[", "\"<|endoftext|>\"", "]", "\n", "all_tokens", "=", "[", "[", "sot_token", "]", "+", "_tokenizer", ".", "encode", "(", "text", ")", "+", "[", "eot_token", "]", "for", "text", "in", "texts", "]", "\n", "result", "=", "torch", ".", "zeros", "(", "len", "(", "all_tokens", ")", ",", "context_length", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "for", "i", ",", "tokens", "in", "enumerate", "(", "all_tokens", ")", ":", "\n", "        ", "if", "len", "(", "tokens", ")", ">", "context_length", ":", "\n", "            ", "raise", "RuntimeError", "(", "f\"Input {texts[i]} is too long for context length {context_length}\"", ")", "\n", "", "result", "[", "i", ",", ":", "len", "(", "tokens", ")", "]", "=", "torch", ".", "tensor", "(", "tokens", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.get_prompt_templates": [[160, 322], ["None"], "function", ["None"], ["", "def", "get_prompt_templates", "(", ")", ":", "\n", "# prompt_templates = [", "\n", "#     'There is a {} in the scene.',", "\n", "#     'There is the {} in the scene.',", "\n", "#     'a photo of a {} in the scene.',", "\n", "#     'a photo of the {} in the scene.',", "\n", "#     'a photo of one {} in the scene.',", "\n", "\n", "#     'itap of a {}.',", "\n", "#     'itap of my {}.',  # itap: I took a picture of", "\n", "#     'itap of the {}.',", "\n", "#     'a photo of a {}.',", "\n", "#     'a photo of my {}.',", "\n", "#     'a photo of the {}.',", "\n", "#     'a photo of one {}.',", "\n", "#     'a photo of many {}.',", "\n", "\n", "#     'a good photo of a {}.',", "\n", "#     'a good photo of the {}.',", "\n", "#     'a bad photo of a {}.',", "\n", "#     'a bad photo of the {}.',", "\n", "#     'a photo of a nice {}.',", "\n", "#     'a photo of the nice {}.',", "\n", "#     'a photo of a cool {}.',", "\n", "#     'a photo of the cool {}.',", "\n", "#     'a photo of a weird {}.',", "\n", "#     'a photo of the weird {}.',", "\n", "\n", "#     'a photo of a small {}.',", "\n", "#     'a photo of the small {}.',", "\n", "#     'a photo of a large {}.',", "\n", "#     'a photo of the large {}.',", "\n", "\n", "#     'a photo of a clean {}.',", "\n", "#     'a photo of the clean {}.',", "\n", "#     'a photo of a dirty {}.',", "\n", "#     'a photo of the dirty {}.',", "\n", "\n", "#     'a bright photo of a {}.',", "\n", "#     'a bright photo of the {}.',", "\n", "#     'a dark photo of a {}.',", "\n", "#     'a dark photo of the {}.',", "\n", "\n", "#     'a photo of a hard to see {}.',", "\n", "#     'a photo of the hard to see {}.',", "\n", "#     'a low resolution photo of a {}.',", "\n", "#     'a low resolution photo of the {}.',", "\n", "#     'a cropped photo of a {}.',", "\n", "#     'a cropped photo of the {}.',", "\n", "#     'a close-up photo of a {}.',", "\n", "#     'a close-up photo of the {}.',", "\n", "#     'a jpeg corrupted photo of a {}.',", "\n", "#     'a jpeg corrupted photo of the {}.',", "\n", "#     'a blurry photo of a {}.',", "\n", "#     'a blurry photo of the {}.',", "\n", "#     'a pixelated photo of a {}.',", "\n", "#     'a pixelated photo of the {}.',", "\n", "\n", "#     'a black and white photo of the {}.',", "\n", "#     'a black and white photo of a {}.',", "\n", "\n", "#     'a plastic {}.',", "\n", "#     'the plastic {}.',", "\n", "\n", "#     'a toy {}.',", "\n", "#     'the toy {}.',", "\n", "#     'a plushie {}.',", "\n", "#     'the plushie {}.',", "\n", "#     'a cartoon {}.',", "\n", "#     'the cartoon {}.',", "\n", "\n", "#     'an embroidered {}.',", "\n", "#     'the embroidered {}.',", "\n", "\n", "#     'a painting of the {}.',", "\n", "#     'a painting of a {}.',", "\n", "# ]", "\n", "\n", "    ", "prompt_templates", "=", "[", "\n", "'{}.'", ",", "\n", "'a photo of a {}.'", ",", "\n", "'a bad photo of a {}.'", ",", "\n", "'a photo of many {}.'", ",", "\n", "'a sculpture of a {}.'", ",", "\n", "'a photo of the hard to see {}.'", ",", "\n", "'a low resolution photo of the {}.'", ",", "\n", "'a rendering of a {}.'", ",", "\n", "'graffiti of a {}.'", ",", "\n", "'a bad photo of the {}.'", ",", "\n", "'a cropped photo of the {}.'", ",", "\n", "'a tattoo of a {}.'", ",", "\n", "'the embroidered {}.'", ",", "\n", "'a photo of a hard to see {}.'", ",", "\n", "'a bright photo of a {}.'", ",", "\n", "'a photo of a clean {}.'", ",", "\n", "'a photo of a dirty {}.'", ",", "\n", "'a dark photo of the {}.'", ",", "\n", "'a drawing of a {}.'", ",", "\n", "'a photo of my {}.'", ",", "\n", "'the plastic {}.'", ",", "\n", "'a photo of the cool {}.'", ",", "\n", "'a close-up photo of a {}.'", ",", "\n", "'a black and white photo of the {}.'", ",", "\n", "'a painting of the {}.'", ",", "\n", "'a painting of a {}.'", ",", "\n", "'a pixelated photo of the {}.'", ",", "\n", "'a sculpture of the {}.'", ",", "\n", "'a bright photo of the {}.'", ",", "\n", "'a cropped photo of a {}.'", ",", "\n", "'a plastic {}.'", ",", "\n", "'a photo of the dirty {}.'", ",", "\n", "'a jpeg corrupted photo of a {}.'", ",", "\n", "'a blurry photo of the {}.'", ",", "\n", "'a photo of the {}.'", ",", "\n", "'a good photo of the {}.'", ",", "\n", "'a rendering of the {}.'", ",", "\n", "'a {} in a video game.'", ",", "\n", "'a photo of one {}.'", ",", "\n", "'a doodle of a {}.'", ",", "\n", "'a close-up photo of the {}.'", ",", "\n", "'the origami {}.'", ",", "\n", "'the {} in a video game.'", ",", "\n", "'a sketch of a {}.'", ",", "\n", "'a doodle of the {}.'", ",", "\n", "'a origami {}.'", ",", "\n", "'a low resolution photo of a {}.'", ",", "\n", "'the toy {}.'", ",", "\n", "'a rendition of the {}.'", ",", "\n", "'a photo of the clean {}.'", ",", "\n", "'a photo of a large {}.'", ",", "\n", "'a rendition of a {}.'", ",", "\n", "'a photo of a nice {}.'", ",", "\n", "'a photo of a weird {}.'", ",", "\n", "'a blurry photo of a {}.'", ",", "\n", "'a cartoon {}.'", ",", "\n", "'art of a {}.'", ",", "\n", "'a sketch of the {}.'", ",", "\n", "'a embroidered {}.'", ",", "\n", "'a pixelated photo of a {}.'", ",", "\n", "'itap of the {}.'", ",", "\n", "'a jpeg corrupted photo of the {}.'", ",", "\n", "'a good photo of a {}.'", ",", "\n", "'a plushie {}.'", ",", "\n", "'a photo of the nice {}.'", ",", "\n", "'a photo of the small {}.'", ",", "\n", "'a photo of the weird {}.'", ",", "\n", "'the cartoon {}.'", ",", "\n", "'art of the {}.'", ",", "\n", "'a drawing of the {}.'", ",", "\n", "'a photo of the large {}.'", ",", "\n", "'a black and white photo of a {}.'", ",", "\n", "'the plushie {}.'", ",", "\n", "'a dark photo of a {}.'", ",", "\n", "'itap of a {}.'", ",", "\n", "'graffiti of the {}.'", ",", "\n", "'a toy {}.'", ",", "\n", "'itap of my {}.'", ",", "\n", "'a photo of a cool {}.'", ",", "\n", "'a photo of a small {}.'", ",", "\n", "'a tattoo of the {}.'", ",", "\n", "]", "\n", "return", "prompt_templates", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.prompt_engineering": [[323, 325], ["template.replace", "classnames.replace().replace", "classnames.replace"], "function", ["None"], ["", "def", "prompt_engineering", "(", "classnames", ",", "template", "=", "\"\"", ")", ":", "\n", "    ", "return", "template", ".", "replace", "(", "'{}'", ",", "classnames", ".", "replace", "(", "','", ",", "''", ")", ".", "replace", "(", "'+'", ",", "' '", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.convert_example_to_features_bpe": [[327, 343], ["isinstance", "numpy.array", "numpy.zeros", "len", "tokenizer.encode"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "def", "convert_example_to_features_bpe", "(", "text", ",", "tokenizer", ",", "sot_token", ",", "eot_token", ",", "context_length", "=", "77", ")", ":", "\n", "    ", "\"\"\"\n    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample.\n    :param tokenizer: Tokenizer\n    :return: List, a list containing token id, padded by 0\n    \"\"\"", "\n", "assert", "isinstance", "(", "text", ",", "str", ")", "\n", "input_ids", "=", "[", "sot_token", "]", "+", "tokenizer", ".", "encode", "(", "text", ")", "+", "[", "eot_token", "]", "\n", "if", "len", "(", "input_ids", ")", ">", "context_length", ":", "\n", "        ", "input_ids", "=", "input_ids", "[", ":", "context_length", "]", "\n", "", "input_ids", "=", "np", ".", "array", "(", "input_ids", ")", "\n", "\n", "pad_input_ids", "=", "np", ".", "zeros", "(", "context_length", ")", "\n", "pad_input_ids", "[", ":", "input_ids", ".", "shape", "[", "0", "]", "]", "=", "input_ids", "\n", "\n", "return", "pad_input_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.pre_tokenize": [[344, 378], ["clip_prompt_engineering.SimpleTokenizer", "clip_prompt_engineering.get_prompt_templates", "range", "torch.stack", "len", "isinstance", "input_ids_all.append", "isinstance", "clip_prompt_engineering.convert_example_to_features_bpe", "input_ids.append", "torch.stack", "t1s.append", "torch.tensor", "clip_prompt_engineering.prompt_engineering"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.get_prompt_templates", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.convert_example_to_features_bpe", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.prompt_engineering"], ["", "def", "pre_tokenize", "(", "class_names", ")", ":", "\n", "    ", "\"\"\"\n    pre-tokenize class names\n    :param class_names: List, a list of class names\n    :param tokenizer: Tokenizer, SimpleTokenizer()\n    :return: Tensor, containing all prompts for all classes, [#cls, #prompts, context_length]\n    \"\"\"", "\n", "# tokenizer", "\n", "tokenizer", "=", "SimpleTokenizer", "(", ")", "\n", "sot_token", "=", "tokenizer", ".", "encoder", "[", "\"<|startoftext|>\"", "]", "\n", "eot_token", "=", "tokenizer", ".", "encoder", "[", "\"<|endoftext|>\"", "]", "\n", "\n", "# prompt engineering", "\n", "prompt_templates", "=", "get_prompt_templates", "(", ")", "\n", "input_ids_all", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "len", "(", "class_names", ")", ")", ":", "\n", "        ", "v", "=", "class_names", "[", "k", "]", "\n", "if", "isinstance", "(", "v", ",", "str", ")", ":", "\n", "            ", "vs", "=", "[", "v", "]", "\n", "", "elif", "isinstance", "(", "v", ",", "list", ")", ":", "\n", "            ", "vs", "=", "v", "\n", "", "t1s", "=", "[", "]", "\n", "for", "v", "in", "vs", ":", "\n", "            ", "for", "pt", "in", "prompt_templates", ":", "\n", "                ", "t1s", ".", "append", "(", "prompt_engineering", "(", "v", ",", "template", "=", "pt", ")", ")", "\n", "", "", "input_ids", "=", "[", "]", "\n", "for", "t1", "in", "t1s", ":", "\n", "            ", "this_input_ids", "=", "convert_example_to_features_bpe", "(", "t1", ",", "tokenizer", ",", "sot_token", ",", "eot_token", ")", "\n", "input_ids", ".", "append", "(", "torch", ".", "tensor", "(", "this_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "\n", "", "input_ids_all", ".", "append", "(", "torch", ".", "stack", "(", "input_ids", ",", "0", ")", ")", "\n", "\n", "", "input_ids_all_classes", "=", "torch", ".", "stack", "(", "input_ids_all", ",", "0", ")", "\n", "return", "input_ids_all_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.__init__": [[32, 109], ["detectron2.data.clip_datasets.clip_prompt_engineering.get_prompt_templates", "sum", "isinstance", "isinstance", "len", "len", "len", "len", "len", "detectron2.structures.tsv_file.TSVFile", "detectron2.structures.tsv_file.TSVFile", "isinstance", "isinstance", "detectron2.structures.tsv_file.CompositeTSVFile", "detectron2.structures.tsv_file.CompositeTSVFile", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.image_tsv_file.get_chunk_size", "numpy.cumsum().tolist", "ValueError", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.classnames.values", "[].lower", "[].lower", "detectron2.structures.tsv_file.CompositeTSVFile", "detectron2.structures.tsv_file.CompositeTSVFile", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.image_tsv_file.get_chunk_size", "ValueError", "len", "len", "len", "len", "[].lower", "[].lower", "numpy.cumsum", "os.path.splitext", "os.path.splitext", "os.path.splitext", "os.path.splitext"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.get_prompt_templates", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.get_chunk_size", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.tsv_file.CompositeTSVFile.get_chunk_size"], ["def", "__init__", "(", "self", ",", "\n", "image_tsv_file", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ",", "\n", "text_tsv_file", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", ",", "\n", "transforms", ":", "Callable", "=", "None", ",", "\n", "tokenizer", ":", "Callable", "=", "None", ",", "\n", "seq_len", "=", "0", ",", "context_length", "=", "77", ",", "target_offset", "=", "0", ",", "\n", "args", "=", "None", ",", "\n", "dataset_name", "=", "\"\"", ",", "\n", "tokenizer_type", "=", "\"bert\"", ",", "\n", "is_train", "=", "True", ",", "\n", "map_file", "=", "None", ",", "\n", "filtered_datasets", "=", "''", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "is_train", "=", "is_train", "\n", "self", ".", "dataset_names", "=", "dataset_name", "\n", "self", ".", "tokenizer_type", "=", "tokenizer_type", "\n", "self", ".", "target_offset", "=", "target_offset", "\n", "self", ".", "seq_len", "=", "seq_len", "\n", "\n", "self", ".", "transforms", "=", "transforms", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "_chunk_sizes", "=", "None", "\n", "self", ".", "context_length", "=", "context_length", "\n", "\n", "self", ".", "prompt_templates", "=", "get_prompt_templates", "(", ")", "# [:2]", "\n", "self", ".", "spacy_nlp", "=", "None", "# spacy.load('en_core_web_sm')", "\n", "\n", "self", ".", "class_selector", "=", "None", "\n", "\n", "self", ".", "label2idx", "=", "{", "}", "\n", "self", ".", "idx2label", "=", "{", "}", "\n", "self", ".", "classnames", "=", "{", "}", "\n", "self", ".", "dataset_target_offsets", "=", "{", "}", ";", "offset", "=", "0", "\n", "\n", "self", ".", "num_classes", "=", "sum", "(", "[", "len", "(", "val", ")", "for", "val", "in", "self", ".", "classnames", ".", "values", "(", ")", "]", ")", "\n", "\n", "self", ".", "filtered_classnames", "=", "[", "]", "\n", "\n", "if", "isinstance", "(", "image_tsv_file", ",", "str", ")", "and", "isinstance", "(", "text_tsv_file", ",", "str", ")", ":", "\n", "# single tsv file", "\n", "            ", "if", "(", "\n", "os", ".", "path", ".", "splitext", "(", "image_tsv_file", ")", "[", "1", "]", ".", "lower", "(", ")", "==", "'.tsv'", "\n", "and", "os", ".", "path", ".", "splitext", "(", "text_tsv_file", ")", "[", "1", "]", ".", "lower", "(", ")", "==", "'.tsv'", "\n", ")", ":", "\n", "                ", "self", ".", "image_tsv_file", "=", "TSVFile", "(", "image_tsv_file", ",", "if_generate_lineidx", "=", "True", ")", "\n", "self", ".", "text_tsv_file", "=", "TSVFile", "(", "text_tsv_file", ",", "if_generate_lineidx", "=", "True", ")", "\n", "# multiple tsv files specified in a text file", "\n", "", "elif", "(", "\n", "os", ".", "path", ".", "splitext", "(", "image_tsv_file", ")", "[", "1", "]", ".", "lower", "(", ")", "==", "'.txt'", "\n", "and", "os", ".", "path", ".", "splitext", "(", "text_tsv_file", ")", "[", "1", "]", ".", "lower", "(", ")", "==", "'.txt'", "\n", ")", ":", "\n", "                ", "self", ".", "image_tsv_file", "=", "CompositeTSVFile", "(", "image_tsv_file", ")", "\n", "self", ".", "text_tsv_file", "=", "CompositeTSVFile", "(", "text_tsv_file", ")", "\n", "self", ".", "_chunk_sizes", "=", "self", ".", "image_tsv_file", ".", "get_chunk_size", "(", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid input! Please check the tsv filenames.\"", ")", "\n", "# multiple tsv files specified in a list", "\n", "", "", "elif", "(", "\n", "isinstance", "(", "image_tsv_file", ",", "list", ")", "\n", "and", "isinstance", "(", "text_tsv_file", ",", "list", ")", "\n", ")", ":", "\n", "            ", "assert", "len", "(", "image_tsv_file", ")", "==", "len", "(", "text_tsv_file", ")", ",", "\"Inconsistent number of Image/Text tsv files!\"", "\n", "assert", "len", "(", "image_tsv_file", ")", "==", "len", "(", "text_tsv_file", ")", ",", "\"Inconsistent number of Image/Text tsv files!\"", "\n", "self", ".", "image_tsv_path", "=", "image_tsv_file", "\n", "self", ".", "text_tsv_path", "=", "text_tsv_file", "\n", "self", ".", "image_tsv_file", "=", "CompositeTSVFile", "(", "image_tsv_file", ",", "class_selector", "=", "self", ".", "class_selector", ")", "\n", "self", ".", "text_tsv_file", "=", "CompositeTSVFile", "(", "text_tsv_file", ",", "class_selector", "=", "self", ".", "class_selector", ")", "\n", "self", ".", "_chunk_sizes", "=", "self", ".", "image_tsv_file", ".", "get_chunk_size", "(", ")", "\n", "self", ".", "_accumulated_chunk_sizes", "=", "np", ".", "cumsum", "(", "self", ".", "_chunk_sizes", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid input! Please check the tsv filenames.\"", ")", "\n", "\n", "", "assert", "len", "(", "self", ".", "image_tsv_file", ")", "==", "len", "(", "self", ".", "text_tsv_file", ")", ",", "\"Inconsistent size of Image/Text ({}/{}) data!\"", ".", "format", "(", "\n", "len", "(", "self", ".", "image_tsv_file", ")", ",", "len", "(", "self", ".", "text_tsv_file", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_chunk_sizes": [[111, 113], ["None"], "methods", ["None"], ["", "def", "get_chunk_sizes", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_chunk_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_class_boundaries": [[114, 118], ["clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.image_tsv_file.get_class_boundaries"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_class_boundaries"], ["", "def", "get_class_boundaries", "(", "self", ")", ":", "\n", "# The samples of each class are organized class-by-class.", "\n", "# _class_boundaries stores the lower- and upper-bound of each class.", "\n", "        ", "return", "self", ".", "image_tsv_file", ".", "get_class_boundaries", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._load_map": [[119, 130], ["open", "line.strip().split", "int", "line.strip"], "methods", ["None"], ["", "def", "_load_map", "(", "self", ",", "map_file", ":", "str", ")", ":", "\n", "        ", "if", "not", "map_file", ":", "\n", "            ", "return", "None", "\n", "\n", "", "label2idx", "=", "{", "}", "\n", "with", "open", "(", "map_file", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "items", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "label2idx", "[", "items", "[", "0", "]", "]", "=", "int", "(", "items", "[", "1", "]", ")", "\n", "\n", "", "", "return", "label2idx", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._load_darknet_map": [[131, 144], ["open", "l.strip"], "methods", ["None"], ["", "def", "_load_darknet_map", "(", "self", ",", "map_file", ")", ":", "\n", "        ", "if", "not", "map_file", ":", "\n", "            ", "return", "None", "\n", "\n", "", "label2idx", "=", "{", "}", "\n", "with", "open", "(", "map_file", ")", "as", "f", ":", "\n", "            ", "linenum", "=", "0", "\n", "for", "l", "in", "f", ":", "\n", "                ", "item", "=", "l", ".", "strip", "(", ")", "\n", "label2idx", "[", "item", "]", "=", "linenum", "\n", "linenum", "+=", "1", "\n", "\n", "", "", "return", "label2idx", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._pre_tokenize": [[145, 218], ["range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "len", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.classnames[].label_to_name", "isinstance", "zip", "input_ids_all.append", "input_masks_all.append", "segment_ids_all.append", "isinstance", "len", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "t1s.append", "t2s.append", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.tokenizer.tokenize", "clip_tsv.InputExample", "clip_tsv.convert_example_to_features", "input_ids.append", "input_masks.append", "segment_ids.append", "detectron2.data.clip_datasets.clip_prompt_engineering.prompt_engineering", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "clip_tsv.InputExample", "clip_img_txt_pair_tsv.convert_example_to_features_bpe", "input_ids.append", "input_masks.append", "segment_ids.append", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.tokenize", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv.convert_example_to_features", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.prompt_engineering", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.convert_example_to_features_bpe"], ["", "def", "_pre_tokenize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        pre-tokenize class names\n        \"\"\"", "\n", "input_ids_all", "=", "[", "]", "\n", "input_masks_all", "=", "[", "]", "\n", "segment_ids_all", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "len", "(", "self", ".", "classnames", "[", "\"imagenet\"", "]", ")", ")", ":", "\n", "            ", "cur_id", "=", "0", ";", "img_id", "=", "0", "\n", "scale", "=", "1.0", "\n", "\n", "v", "=", "self", ".", "classnames", "[", "\"imagenet\"", "]", ".", "label_to_name", "(", "k", ")", "\n", "if", "isinstance", "(", "v", ",", "str", ")", ":", "\n", "                ", "vs", "=", "[", "v", "]", "\n", "", "elif", "isinstance", "(", "v", ",", "list", ")", ":", "\n", "                ", "vs", "=", "v", "\n", "", "t1s", "=", "[", "]", "\n", "t2s", "=", "[", "]", "\n", "for", "v", "in", "vs", ":", "\n", "                ", "for", "pt", "in", "self", ".", "prompt_templates", ":", "\n", "                    ", "t1s", ".", "append", "(", "prompt_engineering", "(", "v", ",", "template", "=", "pt", ")", ")", "\n", "t2s", ".", "append", "(", "\"\"", ")", "\n", "", "", "input_ids", "=", "[", "]", "\n", "input_masks", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "is_next_labels", "=", "[", "0", "]", "*", "len", "(", "t1s", ")", "\n", "is_img_matchs", "=", "[", "1", "]", "*", "len", "(", "t1s", ")", "\n", "img_feat_len", "=", "0", "\n", "for", "t1", ",", "t2", ",", "is_next_label", ",", "is_img_match", "in", "zip", "(", "t1s", ",", "t2s", ",", "is_next_labels", ",", "is_img_matchs", ")", ":", "\n", "                ", "if", "self", ".", "tokenizer_type", "==", "\"bert\"", ":", "\n", "# tokenize", "\n", "                    ", "tokens_a", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "t1", ")", "\n", "tokens_b", "=", "None", "\n", "\n", "# combine to one sample", "\n", "cur_example", "=", "InputExample", "(", "guid", "=", "cur_id", ",", "tokens_a", "=", "tokens_a", ",", "\n", "tokens_b", "=", "tokens_b", ",", "is_next", "=", "is_next_label", ",", "\n", "img_id", "=", "img_id", ",", "is_img_match", "=", "is_img_match", ")", "\n", "\n", "# transform sample to features", "\n", "cur_features", "=", "convert_example_to_features", "(", "self", ".", "args", ",", "cur_example", ",", "\n", "self", ".", "seq_len", ",", "self", ".", "tokenizer", ",", "\n", "img_feat_len", ")", "\n", "\n", "input_ids", ".", "append", "(", "torch", ".", "tensor", "(", "cur_features", ".", "input_ids", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "input_masks", ".", "append", "(", "torch", ".", "tensor", "(", "cur_features", ".", "input_mask", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "segment_ids", ".", "append", "(", "torch", ".", "tensor", "(", "cur_features", ".", "segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "\n", "", "elif", "self", ".", "tokenizer_type", "==", "\"bpe\"", ":", "\n", "                    ", "tokens_a", "=", "t1", ";", "tokens_b", "=", "None", "\n", "# combine to one sample", "\n", "cur_example", "=", "InputExample", "(", "guid", "=", "cur_id", ",", "tokens_a", "=", "tokens_a", ",", "\n", "tokens_b", "=", "tokens_b", ",", "is_next", "=", "is_next_label", ",", "\n", "img_id", "=", "img_id", ",", "is_img_match", "=", "is_img_match", ")", "\n", "\n", "# transform sample to features", "\n", "cur_features", "=", "convert_example_to_features_bpe", "(", "self", ".", "args", ",", "cur_example", ",", "\n", "self", ".", "seq_len", ",", "self", ".", "tokenizer", ",", "\n", "img_feat_len", ")", "\n", "\n", "input_ids", ".", "append", "(", "torch", ".", "tensor", "(", "cur_features", ".", "input_ids", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "input_masks", ".", "append", "(", "torch", ".", "tensor", "(", "cur_features", ".", "input_mask", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "segment_ids", ".", "append", "(", "torch", ".", "tensor", "(", "cur_features", ".", "segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "\n", "", "", "input_ids_all", ".", "append", "(", "torch", ".", "stack", "(", "input_ids", ",", "0", ")", ")", "\n", "input_masks_all", ".", "append", "(", "torch", ".", "stack", "(", "input_masks", ",", "0", ")", ")", "\n", "segment_ids_all", ".", "append", "(", "torch", ".", "stack", "(", "segment_ids", ",", "0", ")", ")", "\n", "\n", "", "self", ".", "input_ids_all_classes", "=", "torch", ".", "stack", "(", "input_ids_all", ",", "0", ")", "\n", "self", ".", "input_mask_all_classes", "=", "torch", ".", "stack", "(", "input_masks_all", ",", "0", ")", "\n", "self", ".", "segment_ids_all_classes", "=", "torch", ".", "stack", "(", "segment_ids_all", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._online_tokenize": [[219, 267], ["numpy.random.randint", "text.split", "random.sample", "detectron2.data.clip_datasets.clip_prompt_engineering.prompt_engineering", "len", "numpy.random.randint", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.tokenizer.tokenize", "clip_tsv.InputExample", "clip_tsv.convert_example_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "clip_tsv.InputExample", "clip_img_txt_pair_tsv.convert_example_to_features_bpe"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.prompt_engineering", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.tokenize", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_tsv.convert_example_to_features", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.convert_example_to_features_bpe"], ["", "def", "_online_tokenize", "(", "self", ",", "text", ")", ":", "\n", "\n", "# random select a prompt template", "\n", "        ", "temp_idx", "=", "np", ".", "random", ".", "randint", "(", "len", "(", "self", ".", "prompt_templates", ")", ")", "\n", "pt", "=", "self", ".", "prompt_templates", "[", "temp_idx", "]", "\n", "\n", "names", "=", "text", ".", "split", "(", "\";\"", ")", "\n", "num_names", "=", "np", ".", "random", ".", "randint", "(", "len", "(", "names", ")", ")", "+", "1", "\n", "names_sampled", "=", "random", ".", "sample", "(", "names", ",", "num_names", ")", "\n", "text", "=", "\", \"", ".", "join", "(", "names_sampled", ")", "\n", "\n", "t1", "=", "prompt_engineering", "(", "text", ",", "template", "=", "pt", ")", "\n", "\n", "cur_id", "=", "0", ";", "img_id", "=", "0", ";", "scale", "=", "1.0", "\n", "is_next_label", "=", "0", ";", "is_img_match", "=", "1", "\n", "img_feat_len", "=", "0", "\n", "\n", "if", "self", ".", "tokenizer_type", "==", "\"bert\"", ":", "\n", "# tokenize", "\n", "            ", "tokens_a", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "t1", ")", "\n", "tokens_b", "=", "None", "\n", "\n", "# combine to one sample", "\n", "cur_example", "=", "InputExample", "(", "guid", "=", "cur_id", ",", "tokens_a", "=", "tokens_a", ",", "\n", "tokens_b", "=", "tokens_b", ",", "is_next", "=", "is_next_label", ",", "\n", "img_id", "=", "img_id", ",", "is_img_match", "=", "is_img_match", ")", "\n", "\n", "# transform sample to features", "\n", "cur_features", "=", "convert_example_to_features", "(", "self", ".", "args", ",", "cur_example", ",", "\n", "self", ".", "context_length", ",", "self", ".", "tokenizer", ",", "\n", "img_feat_len", ")", "\n", "\n", "\n", "", "elif", "self", ".", "tokenizer_type", "==", "\"bpe\"", ":", "\n", "            ", "tokens_a", "=", "t1", ";", "tokens_b", "=", "None", "\n", "# combine to one sample", "\n", "cur_example", "=", "InputExample", "(", "guid", "=", "cur_id", ",", "tokens_a", "=", "tokens_a", ",", "\n", "tokens_b", "=", "tokens_b", ",", "is_next", "=", "is_next_label", ",", "\n", "img_id", "=", "img_id", ",", "is_img_match", "=", "is_img_match", ")", "\n", "\n", "# transform sample to features", "\n", "cur_features", "=", "convert_example_to_features_bpe", "(", "self", ".", "args", ",", "cur_example", ",", "\n", "self", ".", "context_length", ",", "self", ".", "tokenizer", ",", "\n", "img_feat_len", ")", "\n", "\n", "", "return", "torch", ".", "tensor", "(", "cur_features", ".", "input_ids", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "cur_features", ".", "input_mask", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "cur_features", ".", "segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_dataset_name": [[268, 276], ["enumerate"], "methods", ["None"], ["", "def", "get_dataset_name", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        get dataset name according to index\n        \"\"\"", "\n", "assert", "index", "<", "self", ".", "_accumulated_chunk_sizes", "[", "-", "1", "]", ",", "\"index must in the range of accumulated data size\"", "\n", "for", "k", ",", "boundary", "in", "enumerate", "(", "self", ".", "_accumulated_chunk_sizes", ")", ":", "\n", "            ", "if", "index", "<", "boundary", ":", "\n", "                ", "return", "self", ".", "dataset_names", "[", "k", "]", ",", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_target_offset": [[277, 279], ["None"], "methods", ["None"], ["", "", "", "def", "get_target_offset", "(", "self", ",", "dataset_name", ")", ":", "\n", "        ", "return", "self", ".", "dataset_target_offsets", "[", "dataset_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_img_label_pair": [[280, 308], ["clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_dataset_name", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_target_offset", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._decode_data", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.transforms", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._online_tokenize", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._online_tokenize", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._online_tokenize"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_dataset_name", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_target_offset", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._decode_data", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._online_tokenize", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._online_tokenize", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._online_tokenize"], ["", "def", "get_img_label_pair", "(", "self", ",", "items_image", ",", "index", ")", ":", "\n", "        ", "dataset_name", ",", "chunk_id", "=", "self", ".", "get_dataset_name", "(", "index", ")", "\n", "target_offset", "=", "self", ".", "get_target_offset", "(", "dataset_name", ")", "\n", "_", ",", "target", ",", "img", "=", "self", ".", "_decode_data", "(", "items_image", ",", "dataset_name", ")", "\n", "\n", "if", "self", ".", "transforms", ":", "\n", "            ", "img", "=", "self", ".", "transforms", "(", "img", ")", "\n", "\n", "", "if", "target", "==", "-", "1", ":", "\n", "            ", "input_ids", ",", "input_mask", ",", "segment_ids", "=", "self", ".", "_online_tokenize", "(", "\"uncovered image\"", ")", "\n", "", "else", ":", "\n", "            ", "classname", "=", "self", ".", "classnames", "[", "dataset_name", "]", ".", "labels2names", "[", "self", ".", "idx2label", "[", "dataset_name", "]", "[", "target", "]", "]", "\n", "if", "classname", "in", "self", ".", "filtered_classnames", ":", "\n", "# we filter these classnames for training", "\n", "                ", "target", "=", "-", "1", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", "=", "self", ".", "_online_tokenize", "(", "\"uncovered image\"", ")", "\n", "", "else", ":", "\n", "                ", "input_ids", ",", "input_mask", ",", "segment_ids", "=", "self", ".", "_online_tokenize", "(", "classname", ")", "\n", "target", "+=", "target_offset", "\n", "", "", "return", "img", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "torch", ".", "LongTensor", "(", "[", "target", "]", ")", ",", "dataset_name", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_img_txt_pair": [[309, 364], ["clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_dataset_name", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._decode_image", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._decode_text", "isinstance", "len", "len", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.create_phrase_text", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.transforms", "clip_img_txt_pair_tsv.convert_txt_to_tokens_bpe", "isinstance", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.tensor().long().view", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "clip_img_txt_pair_tsv.convert_txt_to_tokens_bpe", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_dataset_name", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._decode_image", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._decode_text", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.create_phrase_text", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.convert_txt_to_tokens_bpe", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.convert_txt_to_tokens_bpe"], ["", "def", "get_img_txt_pair", "(", "self", ",", "items_image", ",", "items_text", ",", "index", ")", ":", "\n", "        ", "dataset_name", ",", "chunk_id", "=", "self", ".", "get_dataset_name", "(", "index", ")", "\n", "assert", "items_text", "[", "0", "]", "==", "items_image", "[", "0", "]", ",", "'keys do not match for image ({}) and text ({}) for {} at chunk {}-{}'", ".", "format", "(", "\n", "len", "(", "items_text", "[", "0", "]", ")", ",", "len", "(", "items_image", "[", "0", "]", ")", ",", "dataset_name", ",", "chunk_id", ",", "self", ".", "image_tsv_path", "[", "chunk_id", "]", "\n", ")", "\n", "\n", "img", "=", "self", ".", "_decode_image", "(", "items_image", ",", "dataset_name", ")", "\n", "#     print(\"index {}, chunk id {}, name {}\".format(index, chunk_id, self.image_tsv_path[chunk_id]))", "\n", "#     raise TypeError(\"cannot decode current item\") ", "\n", "img_width", ",", "img_height", "=", "img", ".", "size", "# img_height, img_width = np.array(img).shape", "\n", "\n", "txts", "=", "self", ".", "_decode_text", "(", "items_text", ")", "\n", "if", "self", ".", "spacy_nlp", "is", "not", "None", ":", "\n", "            ", "np_input_ids", ",", "np_input_masks", ",", "np_segment_ids", "=", "self", ".", "create_phrase_text", "(", "txts", ")", "\n", "\n", "", "if", "self", ".", "transforms", ":", "\n", "            ", "img", "=", "self", ".", "transforms", "(", "img", ")", "\n", "\n", "", "if", "isinstance", "(", "txts", ",", "str", ")", ":", "\n", "            ", "input_ids", ",", "input_masks", ",", "segment_ids", "=", "convert_txt_to_tokens_bpe", "(", "txts", ",", "self", ".", "tokenizer", ",", "self", ".", "context_length", ")", "\n", "all_str2id_links", "=", "[", "]", "\n", "", "elif", "isinstance", "(", "txts", ",", "list", ")", ":", "\n", "            ", "input_ids", "=", "[", "]", "\n", "input_masks", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "all_str2id_links", "=", "[", "]", "\n", "for", "txt", "in", "txts", ":", "\n", "                ", "input_id", ",", "input_mask", ",", "segment_id", ",", "str2id_links", "=", "convert_txt_to_tokens_bpe", "(", "txt", ",", "self", ".", "tokenizer", ",", "self", ".", "context_length", ",", "return_link", "=", "True", ")", "\n", "input_ids", "+=", "input_id", "\n", "input_masks", "+=", "input_mask", "\n", "segment_ids", "+=", "segment_id", "\n", "all_str2id_links", "+=", "[", "str2id_links", "]", "\n", "", "", "scale", "=", "1.0", "\n", "img_id", "=", "0", "\n", "\n", "if", "self", ".", "spacy_nlp", "is", "not", "None", ":", "\n", "            ", "return", "img", ",", "torch", ".", "tensor", "(", "input_ids", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "input_masks", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "segment_ids", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "LongTensor", "(", "[", "1e5", "]", ")", ",", "dataset_name", ",", "torch", ".", "tensor", "(", "np_input_ids", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "np_input_masks", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "np_segment_ids", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "img", ",", "torch", ".", "tensor", "(", "input_ids", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "input_masks", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "segment_ids", ")", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "LongTensor", "(", "[", "1e5", "]", ")", ",", "(", "dataset_name", ",", "items_text", "[", "0", "]", ",", "(", "img_height", ",", "img_width", ")", ",", "all_str2id_links", ")", "# dataset name, image id, image height&width, links bet string and tokenized texts", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.create_phrase_text": [[365, 395], ["isinstance", "list", "random.sample", "enumerate", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.spacy_nlp", "list.extend", "set", "len", "text_list.append", "clip_img_txt_pair_tsv.convert_txt_to_tokens_bpe", "txt.lower", "nc.replace().replace", "detectron2.data.clip_datasets.clip_prompt_engineering.prompt_engineering", "nc.replace"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.convert_txt_to_tokens_bpe", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.prompt_engineering"], ["", "", "def", "create_phrase_text", "(", "self", ",", "txt_list", ")", ":", "\n", "        ", "\"\"\" Use NLP tool to detect noun phrases in captions, fill each identified phrase into a random prompt to create a sentence,\n            and convert each sentence to bpe tokens\n        \"\"\"", "\n", "if", "isinstance", "(", "txt_list", ",", "str", ")", ":", "\n", "            ", "txt_list", "=", "[", "txt_list", "]", "\n", "# detect noun phrase", "\n", "", "noun_phrase", "=", "[", "]", "\n", "for", "txt", "in", "txt_list", ":", "\n", "            ", "doc", "=", "self", ".", "spacy_nlp", "(", "txt", ".", "lower", "(", ")", ")", "\n", "this_text", "=", "[", "nc", ".", "text", "for", "nc", "in", "doc", ".", "noun_chunks", "]", "\n", "this_text", "=", "[", "nc", ".", "replace", "(", "'a '", ",", "''", ")", ".", "replace", "(", "'the '", ",", "''", ")", "for", "nc", "in", "this_text", "]", "\n", "noun_phrase", ".", "extend", "(", "this_text", ")", "\n", "", "noun_phrase", "=", "list", "(", "set", "(", "noun_phrase", ")", ")", "\n", "# fill each phrase into a random prompt", "\n", "text_list", "=", "[", "]", "\n", "pts", "=", "random", ".", "sample", "(", "self", ".", "prompt_templates", ",", "len", "(", "noun_phrase", ")", ")", "\n", "for", "i", ",", "np", "in", "enumerate", "(", "noun_phrase", ")", ":", "\n", "            ", "text_list", ".", "append", "(", "prompt_engineering", "(", "np", ",", "pts", "[", "i", "]", ")", ")", "\n", "# convert string into bpe tokens", "\n", "", "input_ids", "=", "[", "]", "\n", "input_masks", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "for", "txt", "in", "text_list", ":", "\n", "            ", "input_id", ",", "input_mask", ",", "segment_id", "=", "convert_txt_to_tokens_bpe", "(", "txt", ",", "self", ".", "tokenizer", ",", "self", ".", "context_length", ")", "\n", "input_ids", "+=", "input_id", "\n", "input_masks", "+=", "input_mask", "\n", "segment_ids", "+=", "segment_id", "\n", "", "return", "input_ids", ",", "input_masks", ",", "segment_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.__getitem__": [[396, 423], ["isinstance", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_img_label_pair", "clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_img_txt_pair", "threading.Thread", "threading.Thread.start"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_img_label_pair", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.get_img_txt_pair"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "Union", "[", "int", ",", "Tuple", "[", "int", ",", "int", "]", "]", ")", ":", "\n", "        ", "if", "isinstance", "(", "index", ",", "tuple", ")", ":", "\n", "            ", "items_image", "=", "self", ".", "image_tsv_file", "[", "index", "[", "0", "]", "]", "\n", "items_text", "=", "self", ".", "text_tsv_file", "[", "index", "[", "0", "]", "]", "\n", "if", "index", "[", "1", "]", ">=", "0", ":", "\n", "                ", "tsv_filename", "=", "self", ".", "image_tsv_file", ".", "file_list", "[", "index", "[", "1", "]", "]", "\n", "\n", "# Python threads are not truly parallel. Spawn a new process instead.", "\n", "# logging.info('Pre-loading %s ...' % tsv_filename)", "\n", "# os.system('cat ' + tsv_filename + ' > /dev/null &')", "\n", "x", "=", "threading", ".", "Thread", "(", "\n", "target", "=", "pre_fetch", ",", "args", "=", "(", "tsv_filename", ",", ")", ",", "daemon", "=", "True", "\n", ")", "\n", "x", ".", "start", "(", ")", "\n", "", "curr_index", "=", "index", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "items_image", "=", "self", ".", "image_tsv_file", "[", "index", "]", "\n", "items_text", "=", "self", ".", "text_tsv_file", "[", "index", "]", "\n", "curr_index", "=", "index", "\n", "\n", "# NOTE: since we duplicate image tsv to text tsv for image-label data,", "\n", "# we can determine whether the current instance is an image-label pair or ", "\n", "# a image-text pair data based on whether items_image is identical to items_text or not.", "\n", "", "if", "items_image", "==", "items_text", ":", "\n", "            ", "return", "self", ".", "get_img_label_pair", "(", "items_image", ",", "curr_index", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "get_img_txt_pair", "(", "items_image", ",", "items_text", ",", "curr_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._decode_image": [[424, 428], ["PIL.Image.open().convert", "PIL.Image.open", "io.BytesIO", "base64.b64decode"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "", "def", "_decode_image", "(", "self", ",", "items", ":", "Tuple", "[", "str", ",", "str", "]", ",", "dataset_name", "=", "\"\"", ")", ":", "\n", "        ", "key", "=", "items", "[", "0", "]", "\n", "image", "=", "Image", ".", "open", "(", "BytesIO", "(", "base64", ".", "b64decode", "(", "items", "[", "1", "]", ")", ")", ")", ".", "convert", "(", "'RGB'", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._decode_text": [[429, 465], ["isinstance", "isinstance", "json.loads", "isinstance", "random.sample", "random.choice", "isinstance", "json.loads.keys", "tags.split", "random.shuffle", "detectron2.data.clip_datasets.clip_prompt_engineering.prompt_engineering", "ValueError", "random.sample"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.prompt_engineering"], ["", "def", "_decode_text", "(", "self", ",", "items", ":", "Tuple", "[", "str", ",", "Union", "[", "str", ",", "dict", "]", "]", ")", ":", "\n", "        ", "key", "=", "items", "[", "0", "]", "\n", "text", "=", "''", "\n", "if", "isinstance", "(", "items", "[", "1", "]", ",", "str", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "str_dict", "=", "json", ".", "loads", "(", "items", "[", "1", "]", ")", "\n", "# in this dict, it may contain either \"tags\" or \"captions\" or both", "\n", "keys", "=", "[", "key", "for", "key", "in", "str_dict", ".", "keys", "(", ")", "]", "\n", "selected_key", "=", "random", ".", "sample", "(", "keys", ",", "1", ")", "[", "0", "]", "\n", "if", "selected_key", "==", "\"captions\"", ":", "\n", "# if this is a caption, we sample a caption", "\n", "                    ", "captions", "=", "str_dict", "[", "selected_key", "]", "\n", "text", "=", "captions", "[", ":", "5", "]", "\n", "# text = random.sample(captions, 1)[0]", "\n", "", "elif", "selected_key", "==", "\"tags\"", ":", "\n", "# for tags, we randomly disorder it", "\n", "                    ", "tags", "=", "str_dict", "[", "selected_key", "]", "\n", "tag_words", "=", "tags", ".", "split", "(", "' '", ")", "\n", "random", ".", "shuffle", "(", "tag_words", ")", "\n", "tags_shuffled", "=", "\" \"", ".", "join", "(", "tag_words", ")", "\n", "# add prompt template", "\n", "pt", "=", "random", ".", "sample", "(", "self", ".", "prompt_templates", ",", "1", ")", "[", "0", "]", "\n", "text", "=", "prompt_engineering", "(", "tags_shuffled", ",", "pt", ")", "\n", "", "", "except", ":", "\n", "                ", "text", "=", "items", "[", "1", "]", "\n", "", "", "elif", "isinstance", "(", "items", "[", "1", "]", ",", "dict", ")", ":", "\n", "            ", "assert", "'captions'", "in", "items", "[", "1", "]", ",", "'\"captions\" does not in {}'", ".", "format", "(", "items", "[", "1", "]", ")", "\n", "captions", "=", "items", "[", "1", "]", "[", "'captions'", "]", "\n", "if", "isinstance", "(", "captions", ",", "list", ")", ":", "\n", "                ", "text", "=", "random", ".", "choice", "(", "captions", ")", "\n", "", "elif", "isinstance", "(", "captions", ",", "str", ")", ":", "\n", "                ", "text", "=", "captions", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'captions should be str or list'", ")", "\n", "\n", "", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._decode_data": [[466, 475], ["clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._get_label", "PIL.Image.open", "PIL.Image.open.convert", "io.BytesIO", "base64.b64decode"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._get_label", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "def", "_decode_data", "(", "self", ",", "items", ",", "dataset_name", ")", ":", "\n", "        ", "key", "=", "items", "[", "0", "]", "\n", "label", "=", "self", ".", "_get_label", "(", "items", "[", "1", "]", ",", "dataset_name", ")", "\n", "try", ":", "\n", "            ", "image", "=", "Image", ".", "open", "(", "BytesIO", "(", "base64", ".", "b64decode", "(", "items", "[", "2", "]", ")", ")", ")", "\n", "", "except", ":", "\n", "            ", "return", "None", "\n", "\n", "", "return", "key", ",", "label", ",", "image", ".", "convert", "(", "'RGB'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset._get_label": [[476, 488], ["int", "json.loads"], "methods", ["None"], ["", "def", "_get_label", "(", "self", ",", "item", ",", "dataset_name", ")", ":", "\n", "        ", "if", "not", "self", ".", "label2idx", "[", "dataset_name", "]", ":", "\n", "            ", "return", "int", "(", "item", ")", "\n", "\n", "", "if", "item", "in", "self", ".", "label2idx", "[", "dataset_name", "]", ":", "\n", "            ", "return", "self", ".", "label2idx", "[", "dataset_name", "]", "[", "item", "]", "\n", "\n", "", "label", "=", "json", ".", "loads", "(", "item", ")", "[", "0", "]", "[", "'class'", "]", "\n", "if", "label", "in", "self", ".", "label2idx", "[", "dataset_name", "]", ":", "\n", "            ", "return", "self", ".", "label2idx", "[", "dataset_name", "]", "[", "label", "]", "\n", "", "else", ":", "\n", "            ", "return", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.CLIPImgTxtPairTSVDataset.__len__": [[489, 491], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "image_tsv_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.InputFeatures.__init__": [[588, 598], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "is_next", ",", "\n", "lm_label_ids", ",", "img_feat_len", ",", "is_img_match", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "is_next", "=", "is_next", "\n", "self", ".", "lm_label_ids", "=", "lm_label_ids", "\n", "\n", "self", ".", "img_feat_len", "=", "img_feat_len", "\n", "self", ".", "is_img_match", "=", "is_img_match", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.pre_fetch": [[24, 28], ["logging.info", "open", "logging.info"], "function", ["None"], ["def", "pre_fetch", "(", "tsv_filename", ":", "str", ")", ":", "\n", "    ", "logging", ".", "info", "(", "'Pre-loading %s ...'", "%", "tsv_filename", ")", "\n", "with", "open", "(", "tsv_filename", ",", "'r'", ")", ":", "\n", "        ", "logging", ".", "info", "(", "'Pre-loading %s ended.'", "%", "tsv_filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.convert_txt_to_tokens_bpe": [[492, 526], ["tokenizer.encode", "tokenizer.encode", "len", "len", "len", "len", "len", "input_ids.append", "input_mask.append", "segment_ids.append", "lm_label_ids.append", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "", "def", "convert_txt_to_tokens_bpe", "(", "text", ",", "tokenizer", ",", "context_length", ",", "return_link", "=", "False", ")", ":", "\n", "\n", "    ", "sot_token", "=", "tokenizer", ".", "encoder", "[", "\"<|startoftext|>\"", "]", "\n", "eot_token", "=", "tokenizer", ".", "encoder", "[", "\"<|endoftext|>\"", "]", "\n", "if", "return_link", ":", "\n", "        ", "bpe_tokens", ",", "str2id_links", "=", "tokenizer", ".", "encode", "(", "text", ",", "return_link", "=", "return_link", ")", "\n", "str2id_links", "=", "[", "[", "\"<|startoftext|>\"", ",", "[", "sot_token", "]", "]", "]", "+", "str2id_links", "+", "[", "[", "\"<|endoftext|>\"", ",", "[", "eot_token", "]", "]", "]", "\n", "", "else", ":", "\n", "        ", "bpe_tokens", "=", "tokenizer", ".", "encode", "(", "text", ",", "return_link", "=", "return_link", ")", "\n", "", "input_ids", "=", "[", "sot_token", "]", "+", "bpe_tokens", "+", "[", "eot_token", "]", "\n", "\n", "if", "len", "(", "input_ids", ")", ">", "context_length", ":", "\n", "        ", "input_ids", "=", "input_ids", "[", ":", "context_length", "]", "\n", "", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "lm_label_ids", "=", "[", "-", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "context_length", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "lm_label_ids", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "context_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "context_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "context_length", "\n", "assert", "len", "(", "lm_label_ids", ")", "==", "context_length", "\n", "\n", "if", "return_link", ":", "\n", "        ", "return", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "str2id_links", "\n", "", "return", "input_ids", ",", "input_mask", ",", "segment_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_img_txt_pair_tsv.convert_example_to_features_bpe": [[527, 584], ["isinstance", "clip_img_txt_pair_tsv.InputFeatures", "len", "len", "len", "len", "len", "input_ids.append", "input_mask.append", "segment_ids.append", "lm_label_ids.append", "len", "len", "len", "len", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "tokenizer.encode", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "def", "convert_example_to_features_bpe", "(", "args", ",", "example", ",", "max_seq_length", ",", "tokenizer", ",", "\n", "img_feat_len", ",", "context_length", "=", "77", ")", ":", "\n", "    ", "\"\"\"\n    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample with\n    IDs, LM labels, input_mask, CLS and SEP tokens etc.\n    :param args: parameter settings\n    :param img_feat_len: lens of actual img features\n    :param example: InputExample, containing sentence input as strings and is_next label\n    :param max_seq_length: int, maximum length of sequence.\n    :param tokenizer: Tokenizer\n    :return: InputFeatures, containing all inputs and labels of one sample as IDs (as used for model training)\n    \"\"\"", "\n", "# we do not consider tokens_b for now in original CLIP", "\n", "text", "=", "example", ".", "tokens_a", "\n", "assert", "isinstance", "(", "text", ",", "str", ")", "\n", "\n", "sot_token", "=", "tokenizer", ".", "encoder", "[", "\"<|startoftext|>\"", "]", "\n", "eot_token", "=", "tokenizer", ".", "encoder", "[", "\"<|endoftext|>\"", "]", "\n", "input_ids", "=", "[", "sot_token", "]", "+", "tokenizer", ".", "encode", "(", "text", ")", "+", "[", "eot_token", "]", "\n", "\n", "if", "len", "(", "input_ids", ")", ">", "context_length", ":", "\n", "        ", "input_ids", "=", "input_ids", "[", ":", "context_length", "]", "\n", "", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "input_ids", ")", "\n", "lm_label_ids", "=", "[", "-", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "context_length", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "lm_label_ids", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "context_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "context_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "context_length", "\n", "assert", "len", "(", "lm_label_ids", ")", "==", "context_length", "\n", "\n", "if", "example", ".", "guid", "<", "1", ":", "\n", "        ", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logging", ".", "info", "(", "\"guid: %s\"", "%", "example", ".", "guid", ")", "\n", "logging", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logging", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logging", ".", "info", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logging", ".", "info", "(", "\"LM label: %s \"", "%", "lm_label_ids", ")", "\n", "logging", ".", "info", "(", "\"Is next sentence label: %s \"", "%", "example", ".", "is_next", ")", "\n", "\n", "", "features", "=", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "lm_label_ids", "=", "lm_label_ids", ",", "\n", "is_next", "=", "example", ".", "is_next", ",", "\n", "img_feat_len", "=", "img_feat_len", ",", "\n", "is_img_match", "=", "example", ".", "is_img_match", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomApply.__init__": [[47, 60], ["augmentation.Augmentation.__init__", "augmentation._transform_to_aug"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation._transform_to_aug"], ["def", "__init__", "(", "self", ",", "tfm_or_aug", ",", "prob", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tfm_or_aug (Transform, Augmentation): the transform or augmentation\n                to be applied. It can either be a `Transform` or `Augmentation`\n                instance.\n            prob (float): probability between 0.0 and 1.0 that\n                the wrapper transformation is applied\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "aug", "=", "_transform_to_aug", "(", "tfm_or_aug", ")", "\n", "assert", "0.0", "<=", "prob", "<=", "1.0", ",", "f\"Probablity must be between 0.0 and 1.0 (given: {prob})\"", "\n", "self", ".", "prob", "=", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomApply.get_transform": [[61, 67], ["augmentation_impl.RandomApply._rand_range", "augmentation_impl.RandomApply.aug.get_transform", "fvcore.transforms.transform.NoOpTransform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._rand_range", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.get_transform"], ["", "def", "get_transform", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "do", "=", "self", ".", "_rand_range", "(", ")", "<", "self", ".", "prob", "\n", "if", "do", ":", "\n", "            ", "return", "self", ".", "aug", ".", "get_transform", "(", "*", "args", ")", "\n", "", "else", ":", "\n", "            ", "return", "NoOpTransform", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomApply.__call__": [[68, 74], ["augmentation_impl.RandomApply._rand_range", "augmentation_impl.RandomApply.aug", "fvcore.transforms.transform.NoOpTransform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._rand_range"], ["", "", "def", "__call__", "(", "self", ",", "aug_input", ")", ":", "\n", "        ", "do", "=", "self", ".", "_rand_range", "(", ")", "<", "self", ".", "prob", "\n", "if", "do", ":", "\n", "            ", "return", "self", ".", "aug", "(", "aug_input", ")", "\n", "", "else", ":", "\n", "            ", "return", "NoOpTransform", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomFlip.__init__": [[81, 95], ["augmentation.Augmentation.__init__", "augmentation_impl.RandomFlip._init", "ValueError", "ValueError", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "prob", "=", "0.5", ",", "*", ",", "horizontal", "=", "True", ",", "vertical", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prob (float): probability of flip.\n            horizontal (boolean): whether to apply horizontal flipping\n            vertical (boolean): whether to apply vertical flipping\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "horizontal", "and", "vertical", ":", "\n", "            ", "raise", "ValueError", "(", "\"Cannot do both horiz and vert. Please use two Flip instead.\"", ")", "\n", "", "if", "not", "horizontal", "and", "not", "vertical", ":", "\n", "            ", "raise", "ValueError", "(", "\"At least one of horiz or vert has to be True!\"", ")", "\n", "", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomFlip.get_transform": [[96, 106], ["augmentation_impl.RandomFlip._rand_range", "fvcore.transforms.transform.NoOpTransform", "fvcore.transforms.transform.HFlipTransform", "fvcore.transforms.transform.VFlipTransform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._rand_range"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "h", ",", "w", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "do", "=", "self", ".", "_rand_range", "(", ")", "<", "self", ".", "prob", "\n", "if", "do", ":", "\n", "            ", "if", "self", ".", "horizontal", ":", "\n", "                ", "return", "HFlipTransform", "(", "w", ")", "\n", "", "elif", "self", ".", "vertical", ":", "\n", "                ", "return", "VFlipTransform", "(", "h", ")", "\n", "", "", "else", ":", "\n", "            ", "return", "NoOpTransform", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.Resize.__init__": [[111, 121], ["isinstance", "tuple", "augmentation_impl.Resize._init", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "shape", ",", "interp", "=", "Image", ".", "BILINEAR", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            shape: (h, w) tuple or a int\n            interp: PIL interpolation method\n        \"\"\"", "\n", "if", "isinstance", "(", "shape", ",", "int", ")", ":", "\n", "            ", "shape", "=", "(", "shape", ",", "shape", ")", "\n", "", "shape", "=", "tuple", "(", "shape", ")", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.Resize.get_transform": [[122, 125], ["transform.ResizeTransform"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "return", "ResizeTransform", "(", "\n", "image", ".", "shape", "[", "0", "]", ",", "image", ".", "shape", "[", "1", "]", ",", "self", ".", "shape", "[", "0", "]", ",", "self", ".", "shape", "[", "1", "]", ",", "self", ".", "interp", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.ResizeShortestEdge.__init__": [[134, 157], ["augmentation.Augmentation.__init__", "isinstance", "augmentation_impl.ResizeShortestEdge._init", "locals", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "\n", "self", ",", "short_edge_length", ",", "max_size", "=", "sys", ".", "maxsize", ",", "sample_style", "=", "\"range\"", ",", "interp", "=", "Image", ".", "BILINEAR", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            short_edge_length (list[int]): If ``sample_style==\"range\"``,\n                a [min, max] interval from which to sample the shortest edge length.\n                If ``sample_style==\"choice\"``, a list of shortest edge lengths to sample from.\n            max_size (int): maximum allowed longest edge length.\n            sample_style (str): either \"range\" or \"choice\".\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "sample_style", "in", "[", "\"range\"", ",", "\"choice\"", "]", ",", "sample_style", "\n", "\n", "self", ".", "is_range", "=", "sample_style", "==", "\"range\"", "\n", "if", "isinstance", "(", "short_edge_length", ",", "int", ")", ":", "\n", "            ", "short_edge_length", "=", "(", "short_edge_length", ",", "short_edge_length", ")", "\n", "", "if", "self", ".", "is_range", ":", "\n", "            ", "assert", "len", "(", "short_edge_length", ")", "==", "2", ",", "(", "\n", "\"short_edge_length must be two values using 'range' sample style.\"", "\n", "f\" Got {short_edge_length}!\"", "\n", ")", "\n", "", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.ResizeShortestEdge.get_transform": [[158, 179], ["int", "int", "transform.ResizeTransform", "numpy.random.randint", "numpy.random.choice", "fvcore.transforms.transform.NoOpTransform", "min", "max", "max"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "h", ",", "w", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "if", "self", ".", "is_range", ":", "\n", "            ", "size", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "short_edge_length", "[", "0", "]", ",", "self", ".", "short_edge_length", "[", "1", "]", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "size", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "short_edge_length", ")", "\n", "", "if", "size", "==", "0", ":", "\n", "            ", "return", "NoOpTransform", "(", ")", "\n", "\n", "", "scale", "=", "size", "*", "1.0", "/", "min", "(", "h", ",", "w", ")", "\n", "if", "h", "<", "w", ":", "\n", "            ", "newh", ",", "neww", "=", "size", ",", "scale", "*", "w", "\n", "", "else", ":", "\n", "            ", "newh", ",", "neww", "=", "scale", "*", "h", ",", "size", "\n", "", "if", "max", "(", "newh", ",", "neww", ")", ">", "self", ".", "max_size", ":", "\n", "            ", "scale", "=", "self", ".", "max_size", "*", "1.0", "/", "max", "(", "newh", ",", "neww", ")", "\n", "newh", "=", "newh", "*", "scale", "\n", "neww", "=", "neww", "*", "scale", "\n", "", "neww", "=", "int", "(", "neww", "+", "0.5", ")", "\n", "newh", "=", "int", "(", "newh", "+", "0.5", ")", "\n", "return", "ResizeTransform", "(", "h", ",", "w", ",", "newh", ",", "neww", ",", "self", ".", "interp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.ResizeScale.__init__": [[190, 208], ["augmentation.Augmentation.__init__", "augmentation_impl.ResizeScale._init", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "\n", "self", ",", "\n", "min_scale", ":", "float", ",", "\n", "max_scale", ":", "float", ",", "\n", "target_height", ":", "int", ",", "\n", "target_width", ":", "int", ",", "\n", "interp", ":", "int", "=", "Image", ".", "BILINEAR", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            min_scale: minimum image scale range.\n            max_scale: maximum image scale range.\n            target_height: target image height.\n            target_width: target image width.\n            interp: image interpolation method.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.ResizeScale.get_transform": [[209, 221], ["numpy.random.uniform", "numpy.multiply", "numpy.minimum", "numpy.round().astype", "transform.ResizeTransform", "numpy.round", "numpy.multiply"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ":", "np", ".", "ndarray", ")", "->", "Transform", ":", "\n", "# Compute the image scale and scaled size.", "\n", "        ", "input_size", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "output_size", "=", "(", "self", ".", "target_height", ",", "self", ".", "target_width", ")", "\n", "random_scale", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "min_scale", ",", "self", ".", "max_scale", ")", "\n", "random_scale_size", "=", "np", ".", "multiply", "(", "output_size", ",", "random_scale", ")", "\n", "scale", "=", "np", ".", "minimum", "(", "\n", "random_scale_size", "[", "0", "]", "/", "input_size", "[", "0", "]", ",", "random_scale_size", "[", "1", "]", "/", "input_size", "[", "1", "]", "\n", ")", "\n", "scaled_size", "=", "np", ".", "round", "(", "np", ".", "multiply", "(", "input_size", ",", "scale", ")", ")", ".", "astype", "(", "int", ")", "\n", "return", "ResizeTransform", "(", "\n", "input_size", "[", "0", "]", ",", "input_size", "[", "1", "]", ",", "scaled_size", "[", "0", "]", ",", "scaled_size", "[", "1", "]", ",", "self", ".", "interp", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomRotation.__init__": [[230, 253], ["augmentation.Augmentation.__init__", "isinstance", "augmentation_impl.RandomRotation._init", "isinstance", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "angle", ",", "expand", "=", "True", ",", "center", "=", "None", ",", "sample_style", "=", "\"range\"", ",", "interp", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            angle (list[float]): If ``sample_style==\"range\"``,\n                a [min, max] interval from which to sample the angle (in degrees).\n                If ``sample_style==\"choice\"``, a list of angles to sample from\n            expand (bool): choose if the image should be resized to fit the whole\n                rotated image (default), or simply cropped\n            center (list[[float, float]]):  If ``sample_style==\"range\"``,\n                a [[minx, miny], [maxx, maxy]] relative interval from which to sample the center,\n                [0, 0] being the top left of the image and [1, 1] the bottom right.\n                If ``sample_style==\"choice\"``, a list of centers to sample from\n                Default: None, which means that the center of rotation is the center of the image\n                center has no effect if expand=True because it only affects shifting\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "sample_style", "in", "[", "\"range\"", ",", "\"choice\"", "]", ",", "sample_style", "\n", "self", ".", "is_range", "=", "sample_style", "==", "\"range\"", "\n", "if", "isinstance", "(", "angle", ",", "(", "float", ",", "int", ")", ")", ":", "\n", "            ", "angle", "=", "(", "angle", ",", "angle", ")", "\n", "", "if", "center", "is", "not", "None", "and", "isinstance", "(", "center", "[", "0", "]", ",", "(", "float", ",", "int", ")", ")", ":", "\n", "            ", "center", "=", "(", "center", ",", "center", ")", "\n", "", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomRotation.get_transform": [[254, 276], ["transform.RotationTransform", "numpy.random.uniform", "numpy.random.choice", "fvcore.transforms.transform.NoOpTransform", "numpy.random.choice", "numpy.random.uniform", "numpy.random.uniform"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "h", ",", "w", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "center", "=", "None", "\n", "if", "self", ".", "is_range", ":", "\n", "            ", "angle", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "angle", "[", "0", "]", ",", "self", ".", "angle", "[", "1", "]", ")", "\n", "if", "self", ".", "center", "is", "not", "None", ":", "\n", "                ", "center", "=", "(", "\n", "np", ".", "random", ".", "uniform", "(", "self", ".", "center", "[", "0", "]", "[", "0", "]", ",", "self", ".", "center", "[", "1", "]", "[", "0", "]", ")", ",", "\n", "np", ".", "random", ".", "uniform", "(", "self", ".", "center", "[", "0", "]", "[", "1", "]", ",", "self", ".", "center", "[", "1", "]", "[", "1", "]", ")", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "angle", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "angle", ")", "\n", "if", "self", ".", "center", "is", "not", "None", ":", "\n", "                ", "center", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "center", ")", "\n", "\n", "", "", "if", "center", "is", "not", "None", ":", "\n", "            ", "center", "=", "(", "w", "*", "center", "[", "0", "]", ",", "h", "*", "center", "[", "1", "]", ")", "# Convert to absolute coordinates", "\n", "\n", "", "if", "angle", "%", "360", "==", "0", ":", "\n", "            ", "return", "NoOpTransform", "(", ")", "\n", "\n", "", "return", "RotationTransform", "(", "h", ",", "w", ",", "angle", ",", "expand", "=", "self", ".", "expand", ",", "center", "=", "center", ",", "interp", "=", "self", ".", "interp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.FixedSizeCrop.__init__": [[285, 293], ["augmentation.Augmentation.__init__", "augmentation_impl.FixedSizeCrop._init", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "crop_size", ":", "Tuple", "[", "int", "]", ",", "pad_value", ":", "float", "=", "128.0", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            crop_size: target image (height, width).\n            pad_value: the padding value.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.FixedSizeCrop.get_transform": [[294, 317], ["numpy.subtract", "numpy.maximum", "numpy.multiply", "numpy.round().astype", "fvcore.transforms.transform.CropTransform", "numpy.subtract", "numpy.maximum", "numpy.minimum", "fvcore.transforms.transform.PadTransform", "fvcore.transforms.transform.TransformList", "numpy.random.uniform", "numpy.round"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ":", "np", ".", "ndarray", ")", "->", "TransformList", ":", "\n", "# Compute the image scale and scaled size.", "\n", "        ", "input_size", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "output_size", "=", "self", ".", "crop_size", "\n", "\n", "# Add random crop if the image is scaled up.", "\n", "max_offset", "=", "np", ".", "subtract", "(", "input_size", ",", "output_size", ")", "\n", "max_offset", "=", "np", ".", "maximum", "(", "max_offset", ",", "0", ")", "\n", "offset", "=", "np", ".", "multiply", "(", "max_offset", ",", "np", ".", "random", ".", "uniform", "(", "0.0", ",", "1.0", ")", ")", "\n", "offset", "=", "np", ".", "round", "(", "offset", ")", ".", "astype", "(", "int", ")", "\n", "crop_transform", "=", "CropTransform", "(", "\n", "offset", "[", "1", "]", ",", "offset", "[", "0", "]", ",", "output_size", "[", "1", "]", ",", "output_size", "[", "0", "]", ",", "input_size", "[", "1", "]", ",", "input_size", "[", "0", "]", "\n", ")", "\n", "\n", "# Add padding if the image is scaled down.", "\n", "pad_size", "=", "np", ".", "subtract", "(", "output_size", ",", "input_size", ")", "\n", "pad_size", "=", "np", ".", "maximum", "(", "pad_size", ",", "0", ")", "\n", "original_size", "=", "np", ".", "minimum", "(", "input_size", ",", "output_size", ")", "\n", "pad_transform", "=", "PadTransform", "(", "\n", "0", ",", "0", ",", "pad_size", "[", "1", "]", ",", "pad_size", "[", "0", "]", ",", "original_size", "[", "1", "]", ",", "original_size", "[", "0", "]", ",", "self", ".", "pad_value", "\n", ")", "\n", "\n", "return", "TransformList", "(", "[", "crop_transform", ",", "pad_transform", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomCrop.__init__": [[324, 345], ["augmentation.Augmentation.__init__", "augmentation_impl.RandomCrop._init", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "crop_type", ":", "str", ",", "crop_size", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            crop_type (str): one of \"relative_range\", \"relative\", \"absolute\", \"absolute_range\".\n            crop_size (tuple[float, float]): two floats, explained below.\n\n        - \"relative\": crop a (H * crop_size[0], W * crop_size[1]) region from an input image of\n          size (H, W). crop size should be in (0, 1]\n        - \"relative_range\": uniformly sample two values from [crop_size[0], 1]\n          and [crop_size[1]], 1], and use them as in \"relative\" crop type.\n        - \"absolute\" crop a (crop_size[0], crop_size[1]) region from input image.\n          crop_size must be smaller than the input image size.\n        - \"absolute_range\", for an input of size (H, W), uniformly sample H_crop in\n          [crop_size[0], min(H, crop_size[1])] and W_crop in [crop_size[0], min(W, crop_size[1])].\n          Then crop a region (H_crop, W_crop).\n        \"\"\"", "\n", "# TODO style of relative_range and absolute_range are not consistent:", "\n", "# one takes (h, w) but another takes (min, max)", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "crop_type", "in", "[", "\"relative_range\"", ",", "\"relative\"", ",", "\"absolute\"", ",", "\"absolute_range\"", "]", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomCrop.get_transform": [[346, 353], ["augmentation_impl.RandomCrop.get_crop_size", "numpy.random.randint", "numpy.random.randint", "fvcore.transforms.transform.CropTransform"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomCrop.get_crop_size"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "h", ",", "w", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "croph", ",", "cropw", "=", "self", ".", "get_crop_size", "(", "(", "h", ",", "w", ")", ")", "\n", "assert", "h", ">=", "croph", "and", "w", ">=", "cropw", ",", "\"Shape computation in {} has bugs.\"", ".", "format", "(", "self", ")", "\n", "h0", "=", "np", ".", "random", ".", "randint", "(", "h", "-", "croph", "+", "1", ")", "\n", "w0", "=", "np", ".", "random", ".", "randint", "(", "w", "-", "cropw", "+", "1", ")", "\n", "return", "CropTransform", "(", "w0", ",", "h0", ",", "cropw", ",", "croph", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomCrop.get_crop_size": [[354, 379], ["int", "int", "numpy.asarray", "int", "int", "numpy.random.rand", "min", "min", "numpy.random.randint", "numpy.random.randint", "NotImplementedError", "min", "min", "min", "min"], "methods", ["None"], ["", "def", "get_crop_size", "(", "self", ",", "image_size", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            image_size (tuple): height, width\n\n        Returns:\n            crop_size (tuple): height, width in absolute pixels\n        \"\"\"", "\n", "h", ",", "w", "=", "image_size", "\n", "if", "self", ".", "crop_type", "==", "\"relative\"", ":", "\n", "            ", "ch", ",", "cw", "=", "self", ".", "crop_size", "\n", "return", "int", "(", "h", "*", "ch", "+", "0.5", ")", ",", "int", "(", "w", "*", "cw", "+", "0.5", ")", "\n", "", "elif", "self", ".", "crop_type", "==", "\"relative_range\"", ":", "\n", "            ", "crop_size", "=", "np", ".", "asarray", "(", "self", ".", "crop_size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "ch", ",", "cw", "=", "crop_size", "+", "np", ".", "random", ".", "rand", "(", "2", ")", "*", "(", "1", "-", "crop_size", ")", "\n", "return", "int", "(", "h", "*", "ch", "+", "0.5", ")", ",", "int", "(", "w", "*", "cw", "+", "0.5", ")", "\n", "", "elif", "self", ".", "crop_type", "==", "\"absolute\"", ":", "\n", "            ", "return", "(", "min", "(", "self", ".", "crop_size", "[", "0", "]", ",", "h", ")", ",", "min", "(", "self", ".", "crop_size", "[", "1", "]", ",", "w", ")", ")", "\n", "", "elif", "self", ".", "crop_type", "==", "\"absolute_range\"", ":", "\n", "            ", "assert", "self", ".", "crop_size", "[", "0", "]", "<=", "self", ".", "crop_size", "[", "1", "]", "\n", "ch", "=", "np", ".", "random", ".", "randint", "(", "min", "(", "h", ",", "self", ".", "crop_size", "[", "0", "]", ")", ",", "min", "(", "h", ",", "self", ".", "crop_size", "[", "1", "]", ")", "+", "1", ")", "\n", "cw", "=", "np", ".", "random", ".", "randint", "(", "min", "(", "w", ",", "self", ".", "crop_size", "[", "0", "]", ")", ",", "min", "(", "w", ",", "self", ".", "crop_size", "[", "1", "]", ")", "+", "1", ")", "\n", "return", "ch", ",", "cw", "\n", "", "else", ":", "\n", "            ", "NotImplementedError", "(", "\"Unknown crop type {}\"", ".", "format", "(", "self", ".", "crop_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomCrop_CategoryAreaConstraint.__init__": [[389, 407], ["augmentation_impl.RandomCrop", "augmentation_impl.RandomCrop_CategoryAreaConstraint._init", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "\n", "self", ",", "\n", "crop_type", ":", "str", ",", "\n", "crop_size", ",", "\n", "single_category_max_area", ":", "float", "=", "1.0", ",", "\n", "ignored_category", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            crop_type, crop_size: same as in :class:`RandomCrop`\n            single_category_max_area: the maximum allowed area ratio of a\n                category. Set to 1.0 to disable\n            ignored_category: allow this category in the semantic segmentation\n                ground truth to exceed the area ratio. Usually set to the category\n                that's ignored in training.\n        \"\"\"", "\n", "self", ".", "crop_aug", "=", "RandomCrop", "(", "crop_type", ",", "crop_size", ")", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomCrop_CategoryAreaConstraint.get_transform": [[408, 425], ["augmentation_impl.RandomCrop_CategoryAreaConstraint.crop_aug.get_transform", "range", "fvcore.transforms.transform.CropTransform", "augmentation_impl.RandomCrop_CategoryAreaConstraint.crop_aug.get_crop_size", "numpy.random.randint", "numpy.random.randint", "numpy.unique", "len", "numpy.max", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.get_transform", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomCrop.get_crop_size"], ["", "def", "get_transform", "(", "self", ",", "image", ",", "sem_seg", ")", ":", "\n", "        ", "if", "self", ".", "single_category_max_area", ">=", "1.0", ":", "\n", "            ", "return", "self", ".", "crop_aug", ".", "get_transform", "(", "image", ")", "\n", "", "else", ":", "\n", "            ", "h", ",", "w", "=", "sem_seg", ".", "shape", "\n", "for", "_", "in", "range", "(", "10", ")", ":", "\n", "                ", "crop_size", "=", "self", ".", "crop_aug", ".", "get_crop_size", "(", "(", "h", ",", "w", ")", ")", "\n", "y0", "=", "np", ".", "random", ".", "randint", "(", "h", "-", "crop_size", "[", "0", "]", "+", "1", ")", "\n", "x0", "=", "np", ".", "random", ".", "randint", "(", "w", "-", "crop_size", "[", "1", "]", "+", "1", ")", "\n", "sem_seg_temp", "=", "sem_seg", "[", "y0", ":", "y0", "+", "crop_size", "[", "0", "]", ",", "x0", ":", "x0", "+", "crop_size", "[", "1", "]", "]", "\n", "labels", ",", "cnt", "=", "np", ".", "unique", "(", "sem_seg_temp", ",", "return_counts", "=", "True", ")", "\n", "if", "self", ".", "ignored_category", "is", "not", "None", ":", "\n", "                    ", "cnt", "=", "cnt", "[", "labels", "!=", "self", ".", "ignored_category", "]", "\n", "", "if", "len", "(", "cnt", ")", ">", "1", "and", "np", ".", "max", "(", "cnt", ")", "<", "np", ".", "sum", "(", "cnt", ")", "*", "self", ".", "single_category_max_area", ":", "\n", "                    ", "break", "\n", "", "", "crop_tfm", "=", "CropTransform", "(", "x0", ",", "y0", ",", "crop_size", "[", "1", "]", ",", "crop_size", "[", "0", "]", ")", "\n", "return", "crop_tfm", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomExtent.__init__": [[436, 448], ["augmentation.Augmentation.__init__", "augmentation_impl.RandomExtent._init", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "scale_range", ",", "shift_range", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            output_size (h, w): Dimensions of output image\n            scale_range (l, h): Range of input-to-output size scaling factor\n            shift_range (x, y): Range of shifts of the cropped subrect. The rect\n                is shifted by [w / 2 * Uniform(-x, x), h / 2 * Uniform(-y, y)],\n                where (w, h) is the (width, height) of the input image. Set each\n                component to zero to crop at the image's center.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomExtent.get_transform": [[449, 469], ["numpy.array", "numpy.random.uniform", "transform.ExtentTransform", "numpy.random.rand", "numpy.random.rand", "int", "int"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "img_h", ",", "img_w", "=", "image", ".", "shape", "[", ":", "2", "]", "\n", "\n", "# Initialize src_rect to fit the input image.", "\n", "src_rect", "=", "np", ".", "array", "(", "[", "-", "0.5", "*", "img_w", ",", "-", "0.5", "*", "img_h", ",", "0.5", "*", "img_w", ",", "0.5", "*", "img_h", "]", ")", "\n", "\n", "# Apply a random scaling to the src_rect.", "\n", "src_rect", "*=", "np", ".", "random", ".", "uniform", "(", "self", ".", "scale_range", "[", "0", "]", ",", "self", ".", "scale_range", "[", "1", "]", ")", "\n", "\n", "# Apply a random shift to the coordinates origin.", "\n", "src_rect", "[", "0", ":", ":", "2", "]", "+=", "self", ".", "shift_range", "[", "0", "]", "*", "img_w", "*", "(", "np", ".", "random", ".", "rand", "(", ")", "-", "0.5", ")", "\n", "src_rect", "[", "1", ":", ":", "2", "]", "+=", "self", ".", "shift_range", "[", "1", "]", "*", "img_h", "*", "(", "np", ".", "random", ".", "rand", "(", ")", "-", "0.5", ")", "\n", "\n", "# Map src_rect coordinates into image coordinates (center at corner).", "\n", "src_rect", "[", "0", ":", ":", "2", "]", "+=", "0.5", "*", "img_w", "\n", "src_rect", "[", "1", ":", ":", "2", "]", "+=", "0.5", "*", "img_h", "\n", "\n", "return", "ExtentTransform", "(", "\n", "src_rect", "=", "(", "src_rect", "[", "0", "]", ",", "src_rect", "[", "1", "]", ",", "src_rect", "[", "2", "]", ",", "src_rect", "[", "3", "]", ")", ",", "\n", "output_size", "=", "(", "int", "(", "src_rect", "[", "3", "]", "-", "src_rect", "[", "1", "]", ")", ",", "int", "(", "src_rect", "[", "2", "]", "-", "src_rect", "[", "0", "]", ")", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomContrast.__init__": [[484, 492], ["augmentation.Augmentation.__init__", "augmentation_impl.RandomContrast._init", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "intensity_min", ",", "intensity_max", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            intensity_min (float): Minimum augmentation\n            intensity_max (float): Maximum augmentation\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomContrast.get_transform": [[493, 496], ["numpy.random.uniform", "fvcore.transforms.transform.BlendTransform", "image.mean"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "w", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "intensity_min", ",", "self", ".", "intensity_max", ")", "\n", "return", "BlendTransform", "(", "src_image", "=", "image", ".", "mean", "(", ")", ",", "src_weight", "=", "1", "-", "w", ",", "dst_weight", "=", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomBrightness.__init__": [[510, 518], ["augmentation.Augmentation.__init__", "augmentation_impl.RandomBrightness._init", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "intensity_min", ",", "intensity_max", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            intensity_min (float): Minimum augmentation\n            intensity_max (float): Maximum augmentation\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomBrightness.get_transform": [[519, 522], ["numpy.random.uniform", "fvcore.transforms.transform.BlendTransform"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "w", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "intensity_min", ",", "self", ".", "intensity_max", ")", "\n", "return", "BlendTransform", "(", "src_image", "=", "0", ",", "src_weight", "=", "1", "-", "w", ",", "dst_weight", "=", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomSaturation.__init__": [[537, 545], ["augmentation.Augmentation.__init__", "augmentation_impl.RandomSaturation._init", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "intensity_min", ",", "intensity_max", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            intensity_min (float): Minimum augmentation (1 preserves input).\n            intensity_max (float): Maximum augmentation (1 preserves input).\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomSaturation.get_transform": [[546, 551], ["numpy.random.uniform", "fvcore.transforms.transform.BlendTransform", "image.dot"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "assert", "image", ".", "shape", "[", "-", "1", "]", "==", "3", ",", "\"RandomSaturation only works on RGB images\"", "\n", "w", "=", "np", ".", "random", ".", "uniform", "(", "self", ".", "intensity_min", ",", "self", ".", "intensity_max", ")", "\n", "grayscale", "=", "image", ".", "dot", "(", "[", "0.299", ",", "0.587", ",", "0.114", "]", ")", "[", ":", ",", ":", ",", "np", ".", "newaxis", "]", "\n", "return", "BlendTransform", "(", "src_image", "=", "grayscale", ",", "src_weight", "=", "1", "-", "w", ",", "dst_weight", "=", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomLighting.__init__": [[562, 573], ["augmentation.Augmentation.__init__", "augmentation_impl.RandomLighting._init", "numpy.array", "numpy.array", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init"], ["def", "__init__", "(", "self", ",", "scale", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            scale (float): Standard deviation of principal component weighting.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_init", "(", "locals", "(", ")", ")", "\n", "self", ".", "eigen_vecs", "=", "np", ".", "array", "(", "\n", "[", "[", "-", "0.5675", ",", "0.7192", ",", "0.4009", "]", ",", "[", "-", "0.5808", ",", "-", "0.0045", ",", "-", "0.8140", "]", ",", "[", "-", "0.5836", ",", "-", "0.6948", ",", "0.4203", "]", "]", "\n", ")", "\n", "self", ".", "eigen_vals", "=", "np", ".", "array", "(", "[", "0.2175", ",", "0.0188", ",", "0.0045", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation_impl.RandomLighting.get_transform": [[574, 579], ["numpy.random.normal", "fvcore.transforms.transform.BlendTransform", "augmentation_impl.RandomLighting.eigen_vecs.dot"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ",", "image", ")", ":", "\n", "        ", "assert", "image", ".", "shape", "[", "-", "1", "]", "==", "3", ",", "\"RandomLighting only works on RGB images\"", "\n", "weights", "=", "np", ".", "random", ".", "normal", "(", "scale", "=", "self", ".", "scale", ",", "size", "=", "3", ")", "\n", "return", "BlendTransform", "(", "\n", "src_image", "=", "self", ".", "eigen_vecs", ".", "dot", "(", "weights", "*", "self", ".", "eigen_vals", ")", ",", "src_weight", "=", "1.0", ",", "dst_weight", "=", "1.0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.build.build_clip_transforms": [[8, 61], ["print", "timm.data.create_transform", "ts.append", "torchvision.Compose", "len", "torchvision.ToTensor", "torchvision.RandomResizedCrop", "torchvision.RandomHorizontalFlip", "len", "torchvision_transforms.transforms.Resize", "torchvision.RandomHorizontalFlip"], "function", ["None"], ["META_ARCH_REGISTRY", ".", "__doc__", "=", "\"\"\"\nRegistry for meta-architectures, i.e. the whole model.\n\nThe registered object will be called with `obj(cfg)`\nand expected to return a `nn.Module` object.\n\"\"\"", "\n", "\n", "\n", "def", "build_model", "(", "cfg", ")", ":", "\n", "    ", "\"\"\"\n    Build the whole model architecture, defined by ``cfg.MODEL.META_ARCHITECTURE``.\n    Note that it does not load any weights from ``cfg``.\n    \"\"\"", "\n", "meta_arch", "=", "cfg", ".", "MODEL", ".", "META_ARCHITECTURE", "\n", "model", "=", "META_ARCH_REGISTRY", ".", "get", "(", "meta_arch", ")", "(", "cfg", ")", "\n", "model", ".", "to", "(", "torch", ".", "device", "(", "cfg", ".", "MODEL", ".", "DEVICE", ")", ")", "\n", "_log_api_usage", "(", "\"modeling.meta_arch.\"", "+", "meta_arch", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ExtentTransform.__init__": [[46, 56], ["fvcore.transforms.transform.Transform.__init__", "transform.ExtentTransform._set_attributes", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "src_rect", ",", "output_size", ",", "interp", "=", "Image", ".", "LINEAR", ",", "fill", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_rect (x0, y0, x1, y1): src coordinates\n            output_size (h, w): dst image size\n            interp: PIL interpolation methods\n            fill: Fill color used when src_rect extends outside image\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_set_attributes", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ExtentTransform.apply_image": [[57, 74], ["PIL.Image.fromarray.transform", "numpy.asarray", "PIL.Image.fromarray", "PIL.Image.fromarray", "numpy.expand_dims", "len", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugInput.transform"], ["", "def", "apply_image", "(", "self", ",", "img", ",", "interp", "=", "None", ")", ":", "\n", "        ", "h", ",", "w", "=", "self", ".", "output_size", "\n", "if", "len", "(", "img", ".", "shape", ")", ">", "2", "and", "img", ".", "shape", "[", "2", "]", "==", "1", ":", "\n", "            ", "pil_image", "=", "Image", ".", "fromarray", "(", "img", "[", ":", ",", ":", ",", "0", "]", ",", "mode", "=", "\"L\"", ")", "\n", "", "else", ":", "\n", "            ", "pil_image", "=", "Image", ".", "fromarray", "(", "img", ")", "\n", "", "pil_image", "=", "pil_image", ".", "transform", "(", "\n", "size", "=", "(", "w", ",", "h", ")", ",", "\n", "method", "=", "Image", ".", "EXTENT", ",", "\n", "data", "=", "self", ".", "src_rect", ",", "\n", "resample", "=", "interp", "if", "interp", "else", "self", ".", "interp", ",", "\n", "fill", "=", "self", ".", "fill", ",", "\n", ")", "\n", "ret", "=", "np", ".", "asarray", "(", "pil_image", ")", "\n", "if", "len", "(", "img", ".", "shape", ")", ">", "2", "and", "img", ".", "shape", "[", "2", "]", "==", "1", ":", "\n", "            ", "ret", "=", "np", ".", "expand_dims", "(", "ret", ",", "-", "1", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ExtentTransform.apply_coords": [[75, 88], ["coords.astype"], "methods", ["None"], ["", "def", "apply_coords", "(", "self", ",", "coords", ")", ":", "\n", "# Transform image center from source coordinates into output coordinates", "\n", "# and then map the new origin to the corner of the output image.", "\n", "        ", "h", ",", "w", "=", "self", ".", "output_size", "\n", "x0", ",", "y0", ",", "x1", ",", "y1", "=", "self", ".", "src_rect", "\n", "new_coords", "=", "coords", ".", "astype", "(", "np", ".", "float32", ")", "\n", "new_coords", "[", ":", ",", "0", "]", "-=", "0.5", "*", "(", "x0", "+", "x1", ")", "\n", "new_coords", "[", ":", ",", "1", "]", "-=", "0.5", "*", "(", "y0", "+", "y1", ")", "\n", "new_coords", "[", ":", ",", "0", "]", "*=", "w", "/", "(", "x1", "-", "x0", ")", "\n", "new_coords", "[", ":", ",", "1", "]", "*=", "h", "/", "(", "y1", "-", "y0", ")", "\n", "new_coords", "[", ":", ",", "0", "]", "+=", "0.5", "*", "w", "\n", "new_coords", "[", ":", ",", "1", "]", "+=", "0.5", "*", "h", "\n", "return", "new_coords", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ExtentTransform.apply_segmentation": [[89, 92], ["transform.ExtentTransform.apply_image"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image"], ["", "def", "apply_segmentation", "(", "self", ",", "segmentation", ")", ":", "\n", "        ", "segmentation", "=", "self", ".", "apply_image", "(", "segmentation", ",", "interp", "=", "Image", ".", "NEAREST", ")", "\n", "return", "segmentation", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ResizeTransform.__init__": [[99, 111], ["fvcore.transforms.transform.Transform.__init__", "transform.ResizeTransform._set_attributes", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "h", ",", "w", ",", "new_h", ",", "new_w", ",", "interp", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            h, w (int): original image size\n            new_h, new_w (int): new image size\n            interp: PIL interpolation methods, defaults to bilinear.\n        \"\"\"", "\n", "# TODO decide on PIL vs opencv", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "interp", "is", "None", ":", "\n", "            ", "interp", "=", "Image", ".", "BILINEAR", "\n", "", "self", ".", "_set_attributes", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ResizeTransform.apply_image": [[112, 148], ["len", "PIL.Image.fromarray.resize", "numpy.asarray", "any", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "list", "numpy.ascontiguousarray.view().permute", "torch.interpolate", "torch.interpolate", "numpy.ascontiguousarray.permute().view().numpy", "PIL.Image.fromarray", "PIL.Image.fromarray", "numpy.expand_dims", "numpy.ascontiguousarray", "len", "len", "numpy.ascontiguousarray.view", "numpy.ascontiguousarray.permute().view", "len", "numpy.ascontiguousarray.permute"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "apply_image", "(", "self", ",", "img", ",", "interp", "=", "None", ")", ":", "\n", "        ", "assert", "img", ".", "shape", "[", ":", "2", "]", "==", "(", "self", ".", "h", ",", "self", ".", "w", ")", "\n", "assert", "len", "(", "img", ".", "shape", ")", "<=", "4", "\n", "interp_method", "=", "interp", "if", "interp", "is", "not", "None", "else", "self", ".", "interp", "\n", "\n", "if", "img", ".", "dtype", "==", "np", ".", "uint8", ":", "\n", "            ", "if", "len", "(", "img", ".", "shape", ")", ">", "2", "and", "img", ".", "shape", "[", "2", "]", "==", "1", ":", "\n", "                ", "pil_image", "=", "Image", ".", "fromarray", "(", "img", "[", ":", ",", ":", ",", "0", "]", ",", "mode", "=", "\"L\"", ")", "\n", "", "else", ":", "\n", "                ", "pil_image", "=", "Image", ".", "fromarray", "(", "img", ")", "\n", "", "pil_image", "=", "pil_image", ".", "resize", "(", "(", "self", ".", "new_w", ",", "self", ".", "new_h", ")", ",", "interp_method", ")", "\n", "ret", "=", "np", ".", "asarray", "(", "pil_image", ")", "\n", "if", "len", "(", "img", ".", "shape", ")", ">", "2", "and", "img", ".", "shape", "[", "2", "]", "==", "1", ":", "\n", "                ", "ret", "=", "np", ".", "expand_dims", "(", "ret", ",", "-", "1", ")", "\n", "", "", "else", ":", "\n", "# PIL only supports uint8", "\n", "            ", "if", "any", "(", "x", "<", "0", "for", "x", "in", "img", ".", "strides", ")", ":", "\n", "                ", "img", "=", "np", ".", "ascontiguousarray", "(", "img", ")", "\n", "", "img", "=", "torch", ".", "from_numpy", "(", "img", ")", "\n", "shape", "=", "list", "(", "img", ".", "shape", ")", "\n", "shape_4d", "=", "shape", "[", ":", "2", "]", "+", "[", "1", "]", "*", "(", "4", "-", "len", "(", "shape", ")", ")", "+", "shape", "[", "2", ":", "]", "\n", "img", "=", "img", ".", "view", "(", "shape_4d", ")", ".", "permute", "(", "2", ",", "3", ",", "0", ",", "1", ")", "# hw(c) -> nchw", "\n", "_PIL_RESIZE_TO_INTERPOLATE_MODE", "=", "{", "\n", "Image", ".", "NEAREST", ":", "\"nearest\"", ",", "\n", "Image", ".", "BILINEAR", ":", "\"bilinear\"", ",", "\n", "Image", ".", "BICUBIC", ":", "\"bicubic\"", ",", "\n", "}", "\n", "mode", "=", "_PIL_RESIZE_TO_INTERPOLATE_MODE", "[", "interp_method", "]", "\n", "align_corners", "=", "None", "if", "mode", "==", "\"nearest\"", "else", "False", "\n", "img", "=", "F", ".", "interpolate", "(", "\n", "img", ",", "(", "self", ".", "new_h", ",", "self", ".", "new_w", ")", ",", "mode", "=", "mode", ",", "align_corners", "=", "align_corners", "\n", ")", "\n", "shape", "[", ":", "2", "]", "=", "(", "self", ".", "new_h", ",", "self", ".", "new_w", ")", "\n", "ret", "=", "img", ".", "permute", "(", "2", ",", "3", ",", "0", ",", "1", ")", ".", "view", "(", "shape", ")", ".", "numpy", "(", ")", "# nchw -> hw(c)", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ResizeTransform.apply_coords": [[149, 153], ["None"], "methods", ["None"], ["", "def", "apply_coords", "(", "self", ",", "coords", ")", ":", "\n", "        ", "coords", "[", ":", ",", "0", "]", "=", "coords", "[", ":", ",", "0", "]", "*", "(", "self", ".", "new_w", "*", "1.0", "/", "self", ".", "w", ")", "\n", "coords", "[", ":", ",", "1", "]", "=", "coords", "[", ":", ",", "1", "]", "*", "(", "self", ".", "new_h", "*", "1.0", "/", "self", ".", "h", ")", "\n", "return", "coords", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ResizeTransform.apply_segmentation": [[154, 157], ["transform.ResizeTransform.apply_image"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image"], ["", "def", "apply_segmentation", "(", "self", ",", "segmentation", ")", ":", "\n", "        ", "segmentation", "=", "self", ".", "apply_image", "(", "segmentation", ",", "interp", "=", "Image", ".", "NEAREST", ")", "\n", "return", "segmentation", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ResizeTransform.inverse": [[158, 160], ["transform.ResizeTransform"], "methods", ["None"], ["", "def", "inverse", "(", "self", ")", ":", "\n", "        ", "return", "ResizeTransform", "(", "self", ".", "new_h", ",", "self", ".", "new_w", ",", "self", ".", "h", ",", "self", ".", "w", ",", "self", ".", "interp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.RotationTransform.__init__": [[168, 199], ["fvcore.transforms.transform.Transform.__init__", "numpy.array", "transform.RotationTransform._set_attributes", "transform.RotationTransform.create_rotation_matrix", "transform.RotationTransform.create_rotation_matrix", "abs", "abs", "numpy.rint().astype", "locals", "numpy.cos", "numpy.sin", "numpy.deg2rad", "numpy.deg2rad", "numpy.rint"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.RotationTransform.create_rotation_matrix", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.RotationTransform.create_rotation_matrix"], ["def", "__init__", "(", "self", ",", "h", ",", "w", ",", "angle", ",", "expand", "=", "True", ",", "center", "=", "None", ",", "interp", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            h, w (int): original image size\n            angle (float): degrees for rotation\n            expand (bool): choose if the image should be resized to fit the whole\n                rotated image (default), or simply cropped\n            center (tuple (width, height)): coordinates of the rotation center\n                if left to None, the center will be fit to the center of each image\n                center has no effect if expand=True because it only affects shifting\n            interp: cv2 interpolation method, default cv2.INTER_LINEAR\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "image_center", "=", "np", ".", "array", "(", "(", "w", "/", "2", ",", "h", "/", "2", ")", ")", "\n", "if", "center", "is", "None", ":", "\n", "            ", "center", "=", "image_center", "\n", "", "if", "interp", "is", "None", ":", "\n", "            ", "interp", "=", "cv2", ".", "INTER_LINEAR", "\n", "", "abs_cos", ",", "abs_sin", "=", "(", "abs", "(", "np", ".", "cos", "(", "np", ".", "deg2rad", "(", "angle", ")", ")", ")", ",", "abs", "(", "np", ".", "sin", "(", "np", ".", "deg2rad", "(", "angle", ")", ")", ")", ")", "\n", "if", "expand", ":", "\n", "# find the new width and height bounds", "\n", "            ", "bound_w", ",", "bound_h", "=", "np", ".", "rint", "(", "\n", "[", "h", "*", "abs_sin", "+", "w", "*", "abs_cos", ",", "h", "*", "abs_cos", "+", "w", "*", "abs_sin", "]", "\n", ")", ".", "astype", "(", "int", ")", "\n", "", "else", ":", "\n", "            ", "bound_w", ",", "bound_h", "=", "w", ",", "h", "\n", "\n", "", "self", ".", "_set_attributes", "(", "locals", "(", ")", ")", "\n", "self", ".", "rm_coords", "=", "self", ".", "create_rotation_matrix", "(", ")", "\n", "# Needed because of this problem https://github.com/opencv/opencv/issues/11784", "\n", "self", ".", "rm_image", "=", "self", ".", "create_rotation_matrix", "(", "offset", "=", "-", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.RotationTransform.apply_image": [[200, 209], ["cv2.warpAffine", "len"], "methods", ["None"], ["", "def", "apply_image", "(", "self", ",", "img", ",", "interp", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        img should be a numpy array, formatted as Height * Width * Nchannels\n        \"\"\"", "\n", "if", "len", "(", "img", ")", "==", "0", "or", "self", ".", "angle", "%", "360", "==", "0", ":", "\n", "            ", "return", "img", "\n", "", "assert", "img", ".", "shape", "[", ":", "2", "]", "==", "(", "self", ".", "h", ",", "self", ".", "w", ")", "\n", "interp", "=", "interp", "if", "interp", "is", "not", "None", "else", "self", ".", "interp", "\n", "return", "cv2", ".", "warpAffine", "(", "img", ",", "self", ".", "rm_image", ",", "(", "self", ".", "bound_w", ",", "self", ".", "bound_h", ")", ",", "flags", "=", "interp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.RotationTransform.apply_coords": [[210, 218], ["numpy.asarray", "cv2.transform", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugInput.transform"], ["", "def", "apply_coords", "(", "self", ",", "coords", ")", ":", "\n", "        ", "\"\"\"\n        coords should be a N * 2 array-like, containing N couples of (x, y) points\n        \"\"\"", "\n", "coords", "=", "np", ".", "asarray", "(", "coords", ",", "dtype", "=", "float", ")", "\n", "if", "len", "(", "coords", ")", "==", "0", "or", "self", ".", "angle", "%", "360", "==", "0", ":", "\n", "            ", "return", "coords", "\n", "", "return", "cv2", ".", "transform", "(", "coords", "[", ":", ",", "np", ".", "newaxis", ",", ":", "]", ",", "self", ".", "rm_coords", ")", "[", ":", ",", "0", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.RotationTransform.apply_segmentation": [[219, 222], ["transform.RotationTransform.apply_image"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image"], ["", "def", "apply_segmentation", "(", "self", ",", "segmentation", ")", ":", "\n", "        ", "segmentation", "=", "self", ".", "apply_image", "(", "segmentation", ",", "interp", "=", "cv2", ".", "INTER_NEAREST", ")", "\n", "return", "segmentation", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.RotationTransform.create_rotation_matrix": [[223, 234], ["cv2.getRotationMatrix2D", "tuple", "cv2.transform", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugInput.transform"], ["", "def", "create_rotation_matrix", "(", "self", ",", "offset", "=", "0", ")", ":", "\n", "        ", "center", "=", "(", "self", ".", "center", "[", "0", "]", "+", "offset", ",", "self", ".", "center", "[", "1", "]", "+", "offset", ")", "\n", "rm", "=", "cv2", ".", "getRotationMatrix2D", "(", "tuple", "(", "center", ")", ",", "self", ".", "angle", ",", "1", ")", "\n", "if", "self", ".", "expand", ":", "\n", "# Find the coordinates of the center of rotation in the new image", "\n", "# The only point for which we know the future coordinates is the center of the image", "\n", "            ", "rot_im_center", "=", "cv2", ".", "transform", "(", "self", ".", "image_center", "[", "None", ",", "None", ",", ":", "]", "+", "offset", ",", "rm", ")", "[", "0", ",", "0", ",", ":", "]", "\n", "new_center", "=", "np", ".", "array", "(", "[", "self", ".", "bound_w", "/", "2", ",", "self", ".", "bound_h", "/", "2", "]", ")", "+", "offset", "-", "rot_im_center", "\n", "# shift the rotation center to the new coordinates", "\n", "rm", "[", ":", ",", "2", "]", "+=", "new_center", "\n", "", "return", "rm", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.RotationTransform.inverse": [[235, 248], ["transform.RotationTransform", "fvcore.transforms.transform.CropTransform", "fvcore.transforms.transform.TransformList", "NotImplementedError"], "methods", ["None"], ["", "def", "inverse", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The inverse is to rotate it back with expand, and crop to get the original shape.\n        \"\"\"", "\n", "if", "not", "self", ".", "expand", ":", "# Not possible to inverse if a part of the image is lost", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "", "rotation", "=", "RotationTransform", "(", "\n", "self", ".", "bound_h", ",", "self", ".", "bound_w", ",", "-", "self", ".", "angle", ",", "True", ",", "None", ",", "self", ".", "interp", "\n", ")", "\n", "crop", "=", "CropTransform", "(", "\n", "(", "rotation", ".", "bound_w", "-", "self", ".", "w", ")", "//", "2", ",", "(", "rotation", ".", "bound_h", "-", "self", ".", "h", ")", "//", "2", ",", "self", ".", "w", ",", "self", ".", "h", "\n", ")", "\n", "return", "TransformList", "(", "[", "rotation", ",", "crop", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.__init__": [[258, 268], ["fvcore.transforms.transform.Transform.__init__", "transform.ColorTransform._set_attributes", "callable", "ValueError", "locals"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "op", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            op (Callable): operation to be applied to the image,\n                which takes in an ndarray and returns an ndarray.\n        \"\"\"", "\n", "if", "not", "callable", "(", "op", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"op parameter should be callable\"", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_set_attributes", "(", "locals", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_image": [[269, 271], ["transform.ColorTransform.op"], "methods", ["None"], ["", "def", "apply_image", "(", "self", ",", "img", ")", ":", "\n", "        ", "return", "self", ".", "op", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_coords": [[272, 274], ["None"], "methods", ["None"], ["", "def", "apply_coords", "(", "self", ",", "coords", ")", ":", "\n", "        ", "return", "coords", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.inverse": [[275, 277], ["fvcore.transforms.transform.NoOpTransform"], "methods", ["None"], ["", "def", "inverse", "(", "self", ")", ":", "\n", "        ", "return", "NoOpTransform", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_segmentation": [[278, 280], ["None"], "methods", ["None"], ["", "def", "apply_segmentation", "(", "self", ",", "segmentation", ")", ":", "\n", "        ", "return", "segmentation", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.__init__": [[289, 301], ["transform.ColorTransform.__init__", "callable", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "op", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            op (Callable): operation to be applied to the image,\n                which takes in a PIL Image and returns a transformed\n                PIL Image.\n                For reference on possible operations see:\n                - https://pillow.readthedocs.io/en/stable/\n        \"\"\"", "\n", "if", "not", "callable", "(", "op", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"op parameter should be callable\"", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image": [[302, 305], ["PIL.Image.fromarray", "numpy.asarray", "transform.ColorTransform.apply_image"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image"], ["", "def", "apply_image", "(", "self", ",", "img", ")", ":", "\n", "        ", "img", "=", "Image", ".", "fromarray", "(", "img", ")", "\n", "return", "np", ".", "asarray", "(", "super", "(", ")", ".", "apply_image", "(", "img", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.HFlip_rotated_box": [[307, 321], ["None"], "function", ["None"], ["", "", "def", "HFlip_rotated_box", "(", "transform", ",", "rotated_boxes", ")", ":", "\n", "    ", "\"\"\"\n    Apply the horizontal flip transform on rotated boxes.\n\n    Args:\n        rotated_boxes (ndarray): Nx5 floating point array of\n            (x_center, y_center, width, height, angle_degrees) format\n            in absolute coordinates.\n    \"\"\"", "\n", "# Transform x_center", "\n", "rotated_boxes", "[", ":", ",", "0", "]", "=", "transform", ".", "width", "-", "rotated_boxes", "[", ":", ",", "0", "]", "\n", "# Transform angle", "\n", "rotated_boxes", "[", ":", ",", "4", "]", "=", "-", "rotated_boxes", "[", ":", ",", "4", "]", "\n", "return", "rotated_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.Resize_rotated_box": [[323, 345], ["numpy.cos", "numpy.sin", "numpy.sqrt", "numpy.sqrt", "numpy.square", "numpy.square", "numpy.square", "numpy.square", "numpy.arctan2"], "function", ["None"], ["", "def", "Resize_rotated_box", "(", "transform", ",", "rotated_boxes", ")", ":", "\n", "    ", "\"\"\"\n    Apply the resizing transform on rotated boxes. For details of how these (approximation)\n    formulas are derived, please refer to :meth:`RotatedBoxes.scale`.\n\n    Args:\n        rotated_boxes (ndarray): Nx5 floating point array of\n            (x_center, y_center, width, height, angle_degrees) format\n            in absolute coordinates.\n    \"\"\"", "\n", "scale_factor_x", "=", "transform", ".", "new_w", "*", "1.0", "/", "transform", ".", "w", "\n", "scale_factor_y", "=", "transform", ".", "new_h", "*", "1.0", "/", "transform", ".", "h", "\n", "rotated_boxes", "[", ":", ",", "0", "]", "*=", "scale_factor_x", "\n", "rotated_boxes", "[", ":", ",", "1", "]", "*=", "scale_factor_y", "\n", "theta", "=", "rotated_boxes", "[", ":", ",", "4", "]", "*", "np", ".", "pi", "/", "180.0", "\n", "c", "=", "np", ".", "cos", "(", "theta", ")", "\n", "s", "=", "np", ".", "sin", "(", "theta", ")", "\n", "rotated_boxes", "[", ":", ",", "2", "]", "*=", "np", ".", "sqrt", "(", "np", ".", "square", "(", "scale_factor_x", "*", "c", ")", "+", "np", ".", "square", "(", "scale_factor_y", "*", "s", ")", ")", "\n", "rotated_boxes", "[", ":", ",", "3", "]", "*=", "np", ".", "sqrt", "(", "np", ".", "square", "(", "scale_factor_x", "*", "s", ")", "+", "np", ".", "square", "(", "scale_factor_y", "*", "c", ")", ")", "\n", "rotated_boxes", "[", ":", ",", "4", "]", "=", "np", ".", "arctan2", "(", "scale_factor_x", "*", "s", ",", "scale_factor_y", "*", "c", ")", "*", "180", "/", "np", ".", "pi", "\n", "\n", "return", "rotated_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._init": [[106, 111], ["params.items", "setattr", "k.startswith"], "methods", ["None"], ["def", "_init", "(", "self", ",", "params", "=", "None", ")", ":", "\n", "        ", "if", "params", ":", "\n", "            ", "for", "k", ",", "v", "in", "params", ".", "items", "(", ")", ":", "\n", "                ", "if", "k", "!=", "\"self\"", "and", "not", "k", ".", "startswith", "(", "\"_\"", ")", ":", "\n", "                    ", "setattr", "(", "self", ",", "k", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.get_transform": [[112, 147], ["None"], "methods", ["None"], ["", "", "", "", "def", "get_transform", "(", "self", ",", "*", "args", ")", "->", "Transform", ":", "\n", "        ", "\"\"\"\n        Execute the policy based on input data, and decide what transform to apply to inputs.\n\n        Args:\n            args: Any fixed-length positional arguments. By default, the name of the arguments\n                should exist in the :class:`AugInput` to be used.\n\n        Returns:\n            Transform: Returns the deterministic transform to apply to the input.\n\n        Examples:\n        ::\n            class MyAug:\n                # if a policy needs to know both image and semantic segmentation\n                def get_transform(image, sem_seg) -> T.Transform:\n                    pass\n            tfm: Transform = MyAug().get_transform(image, sem_seg)\n            new_image = tfm.apply_image(image)\n\n        Notes:\n            Users can freely use arbitrary new argument names in custom\n            :meth:`get_transform` method, as long as they are available in the\n            input data. In detectron2 we use the following convention:\n\n            * image: (H,W) or (H,W,C) ndarray of type uint8 in range [0, 255], or\n              floating point in range [0, 1] or [0, 255].\n            * boxes: (N,4) ndarray of float32. It represents the instance bounding boxes\n              of N instances. Each is in XYXY format in unit of absolute coordinates.\n            * sem_seg: (H,W) ndarray of type uint8. Each element is an integer label of pixel.\n\n            We do not specify convention for other types and do not include builtin\n            :class:`Augmentation` that uses other types in detectron2.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.__call__": [[148, 172], ["augmentation._get_aug_input_args", "augmentation.Augmentation.get_transform", "isinstance", "aug_input.transform", "type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation._get_aug_input_args", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.get_transform", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugInput.transform"], ["", "def", "__call__", "(", "self", ",", "aug_input", ")", "->", "Transform", ":", "\n", "        ", "\"\"\"\n        Augment the given `aug_input` **in-place**, and return the transform that's used.\n\n        This method will be called to apply the augmentation. In most augmentation, it\n        is enough to use the default implementation, which calls :meth:`get_transform`\n        using the inputs. But a subclass can overwrite it to have more complicated logic.\n\n        Args:\n            aug_input (AugInput): an object that has attributes needed by this augmentation\n                (defined by ``self.get_transform``). Its ``transform`` method will be called\n                to in-place transform it.\n\n        Returns:\n            Transform: the transform that is applied on the input.\n        \"\"\"", "\n", "args", "=", "_get_aug_input_args", "(", "self", ",", "aug_input", ")", "\n", "tfm", "=", "self", ".", "get_transform", "(", "*", "args", ")", "\n", "assert", "isinstance", "(", "tfm", ",", "(", "Transform", ",", "TransformList", ")", ")", ",", "(", "\n", "f\"{type(self)}.get_transform must return an instance of Transform! \"", "\n", "\"Got {type(tfm)} instead.\"", "\n", ")", "\n", "aug_input", ".", "transform", "(", "tfm", ")", "\n", "return", "tfm", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation._rand_range": [[173, 182], ["numpy.random.uniform"], "methods", ["None"], ["", "def", "_rand_range", "(", "self", ",", "low", "=", "1.0", ",", "high", "=", "None", ",", "size", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Uniform float random number between low and high.\n        \"\"\"", "\n", "if", "high", "is", "None", ":", "\n", "            ", "low", ",", "high", "=", "0", ",", "low", "\n", "", "if", "size", "is", "None", ":", "\n", "            ", "size", "=", "[", "]", "\n", "", "return", "np", ".", "random", ".", "uniform", "(", "low", ",", "high", ",", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.Augmentation.__repr__": [[183, 212], ["inspect.signature", "inspect.signature.parameters.items", "type", "hasattr", "getattr", "pprint.pformat", "argstr.append", "super().__repr__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomEqualize.__repr__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Produce something like:\n        \"MyAugmentation(field1={self.field1}, field2={self.field2})\"\n        \"\"\"", "\n", "try", ":", "\n", "            ", "sig", "=", "inspect", ".", "signature", "(", "self", ".", "__init__", ")", "\n", "classname", "=", "type", "(", "self", ")", ".", "__name__", "\n", "argstr", "=", "[", "]", "\n", "for", "name", ",", "param", "in", "sig", ".", "parameters", ".", "items", "(", ")", ":", "\n", "                ", "assert", "(", "\n", "param", ".", "kind", "!=", "param", ".", "VAR_POSITIONAL", "and", "param", ".", "kind", "!=", "param", ".", "VAR_KEYWORD", "\n", ")", ",", "\"The default __repr__ doesn't support *args or **kwargs\"", "\n", "assert", "hasattr", "(", "self", ",", "name", ")", ",", "(", "\n", "\"Attribute {} not found! \"", "\n", "\"Default __repr__ only works if attributes match the constructor.\"", ".", "format", "(", "name", ")", "\n", ")", "\n", "attr", "=", "getattr", "(", "self", ",", "name", ")", "\n", "default", "=", "param", ".", "default", "\n", "if", "default", "is", "attr", ":", "\n", "                    ", "continue", "\n", "", "attr_str", "=", "pprint", ".", "pformat", "(", "attr", ")", "\n", "if", "\"\\n\"", "in", "attr_str", ":", "\n", "# don't show it if pformat decides to use >1 lines", "\n", "                    ", "attr_str", "=", "\"...\"", "\n", "", "argstr", ".", "append", "(", "\"{}={}\"", ".", "format", "(", "name", ",", "attr_str", ")", ")", "\n", "", "return", "\"{}({})\"", ".", "format", "(", "classname", ",", "\", \"", ".", "join", "(", "argstr", ")", ")", "\n", "", "except", "AssertionError", ":", "\n", "            ", "return", "super", "(", ")", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugmentationList.__init__": [[253, 260], ["super().__init__", "augmentation._transform_to_aug"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation._transform_to_aug"], ["def", "__init__", "(", "self", ",", "augs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            augs (list[Augmentation or Transform]):\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "augs", "=", "[", "_transform_to_aug", "(", "x", ")", "for", "x", "in", "augs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugmentationList.__call__": [[261, 267], ["fvcore.transforms.transform.TransformList", "x", "tfms.append"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "aug_input", ")", "->", "Transform", ":", "\n", "        ", "tfms", "=", "[", "]", "\n", "for", "x", "in", "self", ".", "augs", ":", "\n", "            ", "tfm", "=", "x", "(", "aug_input", ")", "\n", "tfms", ".", "append", "(", "tfm", ")", "\n", "", "return", "TransformList", "(", "tfms", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugmentationList.__repr__": [[268, 271], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "msgs", "=", "[", "str", "(", "x", ")", "for", "x", "in", "self", ".", "augs", "]", "\n", "return", "\"AugmentationList[{}]\"", ".", "format", "(", "\", \"", ".", "join", "(", "msgs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugInput.__init__": [[307, 327], ["augmentation._check_img_dtype"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation._check_img_dtype"], ["def", "__init__", "(", "\n", "self", ",", "\n", "image", ":", "np", ".", "ndarray", ",", "\n", "*", ",", "\n", "boxes", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "sem_seg", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            image (ndarray): (H,W) or (H,W,C) ndarray of type uint8 in range [0, 255], or\n                floating point in range [0, 1] or [0, 255]. The meaning of C is up\n                to users.\n            boxes (ndarray or None): Nx4 float32 boxes in XYXY_ABS mode\n            sem_seg (ndarray or None): HxW uint8 semantic segmentation mask. Each element\n                is an integer label of pixel.\n        \"\"\"", "\n", "_check_img_dtype", "(", "image", ")", "\n", "self", ".", "image", "=", "image", "\n", "self", ".", "boxes", "=", "boxes", "\n", "self", ".", "sem_seg", "=", "sem_seg", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugInput.transform": [[328, 340], ["tfm.apply_image", "tfm.apply_box", "tfm.apply_segmentation"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.PILColorTransform.apply_image", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.transform.ColorTransform.apply_segmentation"], ["", "def", "transform", "(", "self", ",", "tfm", ":", "Transform", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        In-place transform all attributes of this class.\n\n        By \"in-place\", it means after calling this method, accessing an attribute such\n        as ``self.image`` will return transformed data.\n        \"\"\"", "\n", "self", ".", "image", "=", "tfm", ".", "apply_image", "(", "self", ".", "image", ")", "\n", "if", "self", ".", "boxes", "is", "not", "None", ":", "\n", "            ", "self", ".", "boxes", "=", "tfm", ".", "apply_box", "(", "self", ".", "boxes", ")", "\n", "", "if", "self", ".", "sem_seg", "is", "not", "None", ":", "\n", "            ", "self", ".", "sem_seg", "=", "tfm", ".", "apply_segmentation", "(", "self", ".", "sem_seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugInput.apply_augmentations": [[341, 348], ["augmentation.AugmentationList"], "methods", ["None"], ["", "", "def", "apply_augmentations", "(", "\n", "self", ",", "augmentations", ":", "List", "[", "Union", "[", "Augmentation", ",", "Transform", "]", "]", "\n", ")", "->", "TransformList", ":", "\n", "        ", "\"\"\"\n        Equivalent of ``AugmentationList(augmentations)(self)``\n        \"\"\"", "\n", "return", "AugmentationList", "(", "augmentations", ")", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation._check_img_dtype": [[27, 37], ["isinstance", "type", "isinstance"], "function", ["None"], ["def", "_check_img_dtype", "(", "img", ")", ":", "\n", "    ", "assert", "isinstance", "(", "img", ",", "np", ".", "ndarray", ")", ",", "\"[Augmentation] Needs an numpy array, but got a {}!\"", ".", "format", "(", "\n", "type", "(", "img", ")", "\n", ")", "\n", "assert", "not", "isinstance", "(", "img", ".", "dtype", ",", "np", ".", "integer", ")", "or", "(", "\n", "img", ".", "dtype", "==", "np", ".", "uint8", "\n", ")", ",", "\"[Augmentation] Got image of type {}, use uint8 or floating points instead!\"", ".", "format", "(", "\n", "img", ".", "dtype", "\n", ")", "\n", "assert", "img", ".", "ndim", "in", "[", "2", ",", "3", "]", ",", "img", ".", "ndim", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation._get_aug_input_args": [[39, 75], ["list", "tuple", "inspect.signature().parameters.items", "len", "args.append", "names.append", "getattr", "AttributeError", "TypeError", "inspect.signature", "type", "type", "type", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "_get_aug_input_args", "(", "aug", ",", "aug_input", ")", "->", "List", "[", "Any", "]", ":", "\n", "    ", "\"\"\"\n    Get the arguments to be passed to ``aug.get_transform`` from the input ``aug_input``.\n    \"\"\"", "\n", "if", "aug", ".", "input_args", "is", "None", ":", "\n", "# Decide what attributes are needed automatically", "\n", "        ", "prms", "=", "list", "(", "inspect", ".", "signature", "(", "aug", ".", "get_transform", ")", ".", "parameters", ".", "items", "(", ")", ")", "\n", "# The default behavior is: if there is one parameter, then its \"image\"", "\n", "# (work automatically for majority of use cases, and also avoid BC breaking),", "\n", "# Otherwise, use the argument names.", "\n", "if", "len", "(", "prms", ")", "==", "1", ":", "\n", "            ", "names", "=", "(", "\"image\"", ",", ")", "\n", "", "else", ":", "\n", "            ", "names", "=", "[", "]", "\n", "for", "name", ",", "prm", "in", "prms", ":", "\n", "                ", "if", "prm", ".", "kind", "in", "(", "inspect", ".", "Parameter", ".", "VAR_POSITIONAL", ",", "inspect", ".", "Parameter", ".", "VAR_KEYWORD", ")", ":", "\n", "                    ", "raise", "TypeError", "(", "\n", "f\"\"\" \\\nThe default implementation of `{type(aug)}.__call__` does not allow \\\n`{type(aug)}.get_transform` to use variable-length arguments (*args, **kwargs)! \\\nIf arguments are unknown, reimplement `__call__` instead. \\\n\"\"\"", "\n", ")", "\n", "", "names", ".", "append", "(", "name", ")", "\n", "", "", "aug", ".", "input_args", "=", "tuple", "(", "names", ")", "\n", "\n", "", "args", "=", "[", "]", "\n", "for", "f", "in", "aug", ".", "input_args", ":", "\n", "        ", "try", ":", "\n", "            ", "args", ".", "append", "(", "getattr", "(", "aug_input", ",", "f", ")", ")", "\n", "", "except", "AttributeError", "as", "e", ":", "\n", "            ", "raise", "AttributeError", "(", "\n", "f\"{type(aug)}.get_transform needs input attribute '{f}', \"", "\n", "f\"but it is not an attribute of {type(aug_input)}!\"", "\n", ")", "from", "e", "\n", "", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation._transform_to_aug": [[216, 239], ["isinstance", "isinstance", "_TransformToAug", "repr"], "function", ["None"], ["", "def", "_transform_to_aug", "(", "tfm_or_aug", ")", ":", "\n", "    ", "\"\"\"\n    Wrap Transform into Augmentation.\n    Private, used internally to implement augmentations.\n    \"\"\"", "\n", "assert", "isinstance", "(", "tfm_or_aug", ",", "(", "Transform", ",", "Augmentation", ")", ")", ",", "tfm_or_aug", "\n", "if", "isinstance", "(", "tfm_or_aug", ",", "Augmentation", ")", ":", "\n", "        ", "return", "tfm_or_aug", "\n", "", "else", ":", "\n", "\n", "        ", "class", "_TransformToAug", "(", "Augmentation", ")", ":", "\n", "            ", "def", "__init__", "(", "self", ",", "tfm", ":", "Transform", ")", ":", "\n", "                ", "self", ".", "tfm", "=", "tfm", "\n", "\n", "", "def", "get_transform", "(", "self", ",", "*", "args", ")", ":", "\n", "                ", "return", "self", ".", "tfm", "\n", "\n", "", "def", "__repr__", "(", "self", ")", ":", "\n", "                ", "return", "repr", "(", "self", ".", "tfm", ")", "\n", "\n", "", "__str__", "=", "__repr__", "\n", "\n", "", "return", "_TransformToAug", "(", "tfm_or_aug", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.apply_augmentations": [[350, 362], ["isinstance", "AugInput.apply_augmentations", "augmentation.AugInput"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.apply_augmentations"], ["", "", "def", "apply_augmentations", "(", "augmentations", ":", "List", "[", "Union", "[", "Transform", ",", "Augmentation", "]", "]", ",", "inputs", ")", ":", "\n", "    ", "\"\"\"\n    Use ``T.AugmentationList(augmentations)(inputs)`` instead.\n    \"\"\"", "\n", "if", "isinstance", "(", "inputs", ",", "np", ".", "ndarray", ")", ":", "\n", "# handle the common case of image-only Augmentation, also for backward compatibility", "\n", "        ", "image_only", "=", "True", "\n", "inputs", "=", "AugInput", "(", "inputs", ")", "\n", "", "else", ":", "\n", "        ", "image_only", "=", "False", "\n", "", "tfms", "=", "inputs", ".", "apply_augmentations", "(", "augmentations", ")", "\n", "return", "inputs", ".", "image", "if", "image_only", "else", "inputs", ",", "tfms", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Compose.__init__": [[55, 57], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "transforms", ")", ":", "\n", "        ", "self", ".", "transforms", "=", "transforms", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Compose.__call__": [[58, 62], ["t"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "img", "=", "t", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Compose.__repr__": [[63, 70], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'('", "\n", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "format_string", "+=", "'\\n'", "\n", "format_string", "+=", "'    {0}'", ".", "format", "(", "t", ")", "\n", "", "format_string", "+=", "'\\n)'", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ToTensor.__call__": [[89, 98], ["functional.to_tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.to_tensor"], ["def", "__call__", "(", "self", ",", "pic", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n\n        Returns:\n            Tensor: Converted image.\n        \"\"\"", "\n", "return", "F", ".", "to_tensor", "(", "pic", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ToTensor.__repr__": [[99, 101], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.PILToTensor.__call__": [[109, 118], ["functional.pil_to_tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.pil_to_tensor"], ["def", "__call__", "(", "self", ",", "pic", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            pic (PIL Image): Image to be converted to tensor.\n\n        Returns:\n            Tensor: Converted image.\n        \"\"\"", "\n", "return", "F", ".", "pil_to_tensor", "(", "pic", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.PILToTensor.__repr__": [[119, 121], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ConvertImageDtype.__init__": [[142, 145], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "dtype", ":", "torch", ".", "dtype", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dtype", "=", "dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ConvertImageDtype.forward": [[146, 148], ["functional.convert_image_dtype"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.convert_image_dtype"], ["", "def", "forward", "(", "self", ",", "image", ")", ":", "\n", "        ", "return", "F", ".", "convert_image_dtype", "(", "image", ",", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ToPILImage.__init__": [[167, 169], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mode", "=", "None", ")", ":", "\n", "        ", "self", ".", "mode", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ToPILImage.__call__": [[170, 180], ["functional.to_pil_image"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.to_pil_image"], ["", "def", "__call__", "(", "self", ",", "pic", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\n\n        Returns:\n            PIL Image: Image converted to PIL Image.\n\n        \"\"\"", "\n", "return", "F", ".", "to_pil_image", "(", "pic", ",", "self", ".", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ToPILImage.__repr__": [[181, 187], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'('", "\n", "if", "self", ".", "mode", "is", "not", "None", ":", "\n", "            ", "format_string", "+=", "'mode={0}'", ".", "format", "(", "self", ".", "mode", ")", "\n", "", "format_string", "+=", "')'", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Normalize.__init__": [[207, 212], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "mean", ",", "std", ",", "inplace", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mean", "=", "mean", "\n", "self", ".", "std", "=", "std", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Normalize.forward": [[213, 222], ["functional.normalize"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.normalize"], ["", "def", "forward", "(", "self", ",", "tensor", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "\"\"\"\n        Args:\n            tensor (Tensor): Tensor image to be normalized.\n\n        Returns:\n            Tensor: Normalized Tensor image.\n        \"\"\"", "\n", "return", "F", ".", "normalize", "(", "tensor", ",", "self", ".", "mean", ",", "self", ".", "std", ",", "self", ".", "inplace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Normalize.__repr__": [[223, 225], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(mean={0}, std={1})'", ".", "format", "(", "self", ".", "mean", ",", "self", ".", "std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Resize.__init__": [[271, 290], ["super().__init__", "isinstance", "isinstance", "TypeError", "isinstance", "ValueError", "warnings.warn", "functional._interpolation_modes_from_int", "len", "type"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int"], ["def", "__init__", "(", "self", ",", "size", ",", "interpolation", "=", "InterpolationMode", ".", "BILINEAR", ",", "max_size", "=", "None", ",", "antialias", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "size", ",", "(", "int", ",", "Sequence", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Size should be int or sequence. Got {}\"", ".", "format", "(", "type", "(", "size", ")", ")", ")", "\n", "", "if", "isinstance", "(", "size", ",", "Sequence", ")", "and", "len", "(", "size", ")", "not", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"If size is a sequence, it should have 1 or 2 values\"", ")", "\n", "", "self", ".", "size", "=", "size", "\n", "self", ".", "max_size", "=", "max_size", "\n", "\n", "# Backward compatibility with integer value", "\n", "if", "isinstance", "(", "interpolation", ",", "int", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Argument interpolation should be of type InterpolationMode instead of int. \"", "\n", "\"Please, use InterpolationMode enum.\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "interpolation", ")", "\n", "\n", "", "self", ".", "interpolation", "=", "interpolation", "\n", "self", ".", "antialias", "=", "antialias", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Resize.forward": [[291, 300], ["functional.resize"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be scaled.\n\n        Returns:\n            PIL Image or Tensor: Rescaled image.\n        \"\"\"", "\n", "return", "F", ".", "resize", "(", "img", ",", "self", ".", "size", ",", "self", ".", "interpolation", ",", "self", ".", "max_size", ",", "self", ".", "antialias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Resize.__repr__": [[301, 305], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "interpolate_str", "=", "self", ".", "interpolation", ".", "value", "\n", "return", "self", ".", "__class__", ".", "__name__", "+", "'(size={0}, interpolation={1}, max_size={2}, antialias={3})'", ".", "format", "(", "\n", "self", ".", "size", ",", "interpolate_str", ",", "self", ".", "max_size", ",", "self", ".", "antialias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Scale.__init__": [[311, 315], ["warnings.warn", "transforms.Resize.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\"The use of the transforms.Scale transform is deprecated, \"", "+", "\n", "\"please use transforms.Resize instead.\"", ")", "\n", "super", "(", "Scale", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.CenterCrop.__init__": [[329, 332], ["super().__init__", "transforms._setup_size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_size"], ["def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "size", "=", "_setup_size", "(", "size", ",", "error_msg", "=", "\"Please provide only two dimensions (h, w) for size.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.CenterCrop.forward": [[333, 342], ["functional.center_crop"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.center_crop"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be cropped.\n\n        Returns:\n            PIL Image or Tensor: Cropped image.\n        \"\"\"", "\n", "return", "F", ".", "center_crop", "(", "img", ",", "self", ".", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.CenterCrop.__repr__": [[343, 345], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(size={0})'", ".", "format", "(", "self", ".", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Pad.__init__": [[385, 403], ["super().__init__", "isinstance", "TypeError", "isinstance", "TypeError", "ValueError", "isinstance", "ValueError", "len", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "padding", ",", "fill", "=", "0", ",", "padding_mode", "=", "\"constant\"", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "padding", ",", "(", "numbers", ".", "Number", ",", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Got inappropriate padding arg\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "fill", ",", "(", "numbers", ".", "Number", ",", "str", ",", "tuple", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Got inappropriate fill arg\"", ")", "\n", "\n", "", "if", "padding_mode", "not", "in", "[", "\"constant\"", ",", "\"edge\"", ",", "\"reflect\"", ",", "\"symmetric\"", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"Padding mode should be either constant, edge, reflect or symmetric\"", ")", "\n", "\n", "", "if", "isinstance", "(", "padding", ",", "Sequence", ")", "and", "len", "(", "padding", ")", "not", "in", "[", "1", ",", "2", ",", "4", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"Padding must be an int or a 1, 2, or 4 element tuple, not a \"", "+", "\n", "\"{} element tuple\"", ".", "format", "(", "len", "(", "padding", ")", ")", ")", "\n", "\n", "", "self", ".", "padding", "=", "padding", "\n", "self", ".", "fill", "=", "fill", "\n", "self", ".", "padding_mode", "=", "padding_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Pad.forward": [[404, 413], ["functional.pad"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be padded.\n\n        Returns:\n            PIL Image or Tensor: Padded image.\n        \"\"\"", "\n", "return", "F", ".", "pad", "(", "img", ",", "self", ".", "padding", ",", "self", ".", "fill", ",", "self", ".", "padding_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Pad.__repr__": [[414, 417], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(padding={0}, fill={1}, padding_mode={2})'", ".", "format", "(", "self", ".", "padding", ",", "self", ".", "fill", ",", "self", ".", "padding_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Lambda.__init__": [[426, 430], ["callable", "TypeError", "repr", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lambd", ")", ":", "\n", "        ", "if", "not", "callable", "(", "lambd", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Argument lambd should be callable, got {}\"", ".", "format", "(", "repr", "(", "type", "(", "lambd", ")", ".", "__name__", ")", ")", ")", "\n", "", "self", ".", "lambd", "=", "lambd", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Lambda.__call__": [[431, 433], ["transforms.Lambda.lambd"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "return", "self", ".", "lambd", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Lambda.__repr__": [[434, 436], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'()'", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomTransforms.__init__": [[445, 449], ["isinstance", "TypeError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "transforms", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "transforms", ",", "Sequence", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Argument transforms should be a sequence\"", ")", "\n", "", "self", ".", "transforms", "=", "transforms", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomTransforms.__call__": [[450, 452], ["NotImplementedError"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomTransforms.__repr__": [[453, 460], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'('", "\n", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "format_string", "+=", "'\\n'", "\n", "format_string", "+=", "'    {0}'", ".", "format", "(", "t", ")", "\n", "", "format_string", "+=", "'\\n)'", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomApply.__init__": [[482, 486], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "transforms", ",", "p", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transforms", "=", "transforms", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomApply.forward": [[487, 493], ["torch.rand", "t"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "if", "self", ".", "p", "<", "torch", ".", "rand", "(", "1", ")", ":", "\n", "            ", "return", "img", "\n", "", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "img", "=", "t", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomApply.__repr__": [[494, 502], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'('", "\n", "format_string", "+=", "'\\n    p={}'", ".", "format", "(", "self", ".", "p", ")", "\n", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "format_string", "+=", "'\\n'", "\n", "format_string", "+=", "'    {0}'", ".", "format", "(", "t", ")", "\n", "", "format_string", "+=", "'\\n)'", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomOrder.__call__": [[507, 513], ["list", "random.shuffle", "range", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "order", "=", "list", "(", "range", "(", "len", "(", "self", ".", "transforms", ")", ")", ")", "\n", "random", ".", "shuffle", "(", "order", ")", "\n", "for", "i", "in", "order", ":", "\n", "            ", "img", "=", "self", ".", "transforms", "[", "i", "]", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomChoice.__call__": [[518, 521], ["random.choice", "random.choice."], "methods", ["None"], ["def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "t", "=", "random", ".", "choice", "(", "self", ".", "transforms", ")", "\n", "return", "t", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomCrop.get_params": [[567, 592], ["functional._get_image_size", "torch.randint().item", "torch.randint().item", "ValueError", "torch.randint", "torch.randint"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size"], ["@", "staticmethod", "\n", "def", "get_params", "(", "img", ":", "Tensor", ",", "output_size", ":", "Tuple", "[", "int", ",", "int", "]", ")", "->", "Tuple", "[", "int", ",", "int", ",", "int", ",", "int", "]", ":", "\n", "        ", "\"\"\"Get parameters for ``crop`` for a random crop.\n\n        Args:\n            img (PIL Image or Tensor): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n        \"\"\"", "\n", "w", ",", "h", "=", "F", ".", "_get_image_size", "(", "img", ")", "\n", "th", ",", "tw", "=", "output_size", "\n", "\n", "if", "h", "+", "1", "<", "th", "or", "w", "+", "1", "<", "tw", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Required crop size {} is larger then input image size {}\"", ".", "format", "(", "(", "th", ",", "tw", ")", ",", "(", "h", ",", "w", ")", ")", "\n", ")", "\n", "\n", "", "if", "w", "==", "tw", "and", "h", "==", "th", ":", "\n", "            ", "return", "0", ",", "0", ",", "h", ",", "w", "\n", "\n", "", "i", "=", "torch", ".", "randint", "(", "0", ",", "h", "-", "th", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", "\n", "j", "=", "torch", ".", "randint", "(", "0", ",", "w", "-", "tw", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", "\n", "return", "i", ",", "j", ",", "th", ",", "tw", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomCrop.__init__": [[593, 604], ["super().__init__", "tuple", "transforms._setup_size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_size"], ["", "def", "__init__", "(", "self", ",", "size", ",", "padding", "=", "None", ",", "pad_if_needed", "=", "False", ",", "fill", "=", "0", ",", "padding_mode", "=", "\"constant\"", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "size", "=", "tuple", "(", "_setup_size", "(", "\n", "size", ",", "error_msg", "=", "\"Please provide only two dimensions (h, w) for size.\"", "\n", ")", ")", "\n", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "pad_if_needed", "=", "pad_if_needed", "\n", "self", ".", "fill", "=", "fill", "\n", "self", ".", "padding_mode", "=", "padding_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomCrop.forward": [[605, 629], ["functional._get_image_size", "transforms.RandomCrop.get_params", "functional.crop", "functional.pad", "functional.pad", "functional.pad"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.get_params", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be cropped.\n\n        Returns:\n            PIL Image or Tensor: Cropped image.\n        \"\"\"", "\n", "if", "self", ".", "padding", "is", "not", "None", ":", "\n", "            ", "img", "=", "F", ".", "pad", "(", "img", ",", "self", ".", "padding", ",", "self", ".", "fill", ",", "self", ".", "padding_mode", ")", "\n", "\n", "", "width", ",", "height", "=", "F", ".", "_get_image_size", "(", "img", ")", "\n", "# pad the width if needed", "\n", "if", "self", ".", "pad_if_needed", "and", "width", "<", "self", ".", "size", "[", "1", "]", ":", "\n", "            ", "padding", "=", "[", "self", ".", "size", "[", "1", "]", "-", "width", ",", "0", "]", "\n", "img", "=", "F", ".", "pad", "(", "img", ",", "padding", ",", "self", ".", "fill", ",", "self", ".", "padding_mode", ")", "\n", "# pad the height if needed", "\n", "", "if", "self", ".", "pad_if_needed", "and", "height", "<", "self", ".", "size", "[", "0", "]", ":", "\n", "            ", "padding", "=", "[", "0", ",", "self", ".", "size", "[", "0", "]", "-", "height", "]", "\n", "img", "=", "F", ".", "pad", "(", "img", ",", "padding", ",", "self", ".", "fill", ",", "self", ".", "padding_mode", ")", "\n", "\n", "", "i", ",", "j", ",", "h", ",", "w", "=", "self", ".", "get_params", "(", "img", ",", "self", ".", "size", ")", "\n", "\n", "return", "F", ".", "crop", "(", "img", ",", "i", ",", "j", ",", "h", ",", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomCrop.__repr__": [[630, 632], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "\"(size={0}, padding={1})\"", ".", "format", "(", "self", ".", "size", ",", "self", ".", "padding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomHorizontalFlip.__init__": [[644, 647], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "p", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomHorizontalFlip.forward": [[648, 659], ["torch.rand", "functional.hflip"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.hflip"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be flipped.\n\n        Returns:\n            PIL Image or Tensor: Randomly flipped image.\n        \"\"\"", "\n", "if", "torch", ".", "rand", "(", "1", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "F", ".", "hflip", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomHorizontalFlip.__repr__": [[660, 662], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(p={})'", ".", "format", "(", "self", ".", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomVerticalFlip.__init__": [[674, 677], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "p", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomVerticalFlip.forward": [[678, 689], ["torch.rand", "functional.vflip"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.vflip"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be flipped.\n\n        Returns:\n            PIL Image or Tensor: Randomly flipped image.\n        \"\"\"", "\n", "if", "torch", ".", "rand", "(", "1", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "F", ".", "vflip", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomVerticalFlip.__repr__": [[690, 692], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(p={})'", ".", "format", "(", "self", ".", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomPerspective.__init__": [[711, 732], ["super().__init__", "isinstance", "warnings.warn", "functional._interpolation_modes_from_int", "isinstance", "TypeError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int"], ["def", "__init__", "(", "self", ",", "distortion_scale", "=", "0.5", ",", "p", "=", "0.5", ",", "interpolation", "=", "InterpolationMode", ".", "BILINEAR", ",", "fill", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "p", "=", "p", "\n", "\n", "# Backward compatibility with integer value", "\n", "if", "isinstance", "(", "interpolation", ",", "int", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Argument interpolation should be of type InterpolationMode instead of int. \"", "\n", "\"Please, use InterpolationMode enum.\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "interpolation", ")", "\n", "\n", "", "self", ".", "interpolation", "=", "interpolation", "\n", "self", ".", "distortion_scale", "=", "distortion_scale", "\n", "\n", "if", "fill", "is", "None", ":", "\n", "            ", "fill", "=", "0", "\n", "", "elif", "not", "isinstance", "(", "fill", ",", "(", "Sequence", ",", "numbers", ".", "Number", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Fill should be either a sequence or a number.\"", ")", "\n", "\n", "", "self", ".", "fill", "=", "fill", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomPerspective.forward": [[733, 754], ["isinstance", "isinstance", "torch.rand", "functional._get_image_size", "transforms.RandomPerspective.get_params", "functional.perspective", "functional._get_image_num_channels", "float", "float"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.get_params", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.perspective", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be Perspectively transformed.\n\n        Returns:\n            PIL Image or Tensor: Randomly transformed image.\n        \"\"\"", "\n", "\n", "fill", "=", "self", ".", "fill", "\n", "if", "isinstance", "(", "img", ",", "Tensor", ")", ":", "\n", "            ", "if", "isinstance", "(", "fill", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "fill", "=", "[", "float", "(", "fill", ")", "]", "*", "F", ".", "_get_image_num_channels", "(", "img", ")", "\n", "", "else", ":", "\n", "                ", "fill", "=", "[", "float", "(", "f", ")", "for", "f", "in", "fill", "]", "\n", "\n", "", "", "if", "torch", ".", "rand", "(", "1", ")", "<", "self", ".", "p", ":", "\n", "            ", "width", ",", "height", "=", "F", ".", "_get_image_size", "(", "img", ")", "\n", "startpoints", ",", "endpoints", "=", "self", ".", "get_params", "(", "width", ",", "height", ",", "self", ".", "distortion_scale", ")", "\n", "return", "F", ".", "perspective", "(", "img", ",", "startpoints", ",", "endpoints", ",", "self", ".", "interpolation", ",", "fill", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomPerspective.get_params": [[755, 789], ["int", "int", "int", "int", "int", "int", "int", "int", "torch.randint().item", "torch.randint().item", "torch.randint().item", "torch.randint().item", "torch.randint().item", "torch.randint().item", "torch.randint().item", "torch.randint().item", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "int", "int", "int", "int", "int", "int", "int", "int"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "width", ":", "int", ",", "height", ":", "int", ",", "distortion_scale", ":", "float", ")", "->", "Tuple", "[", "List", "[", "List", "[", "int", "]", "]", ",", "List", "[", "List", "[", "int", "]", "]", "]", ":", "\n", "        ", "\"\"\"Get parameters for ``perspective`` for a random perspective transform.\n\n        Args:\n            width (int): width of the image.\n            height (int): height of the image.\n            distortion_scale (float): argument to control the degree of distortion and ranges from 0 to 1.\n\n        Returns:\n            List containing [top-left, top-right, bottom-right, bottom-left] of the original image,\n            List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image.\n        \"\"\"", "\n", "half_height", "=", "height", "//", "2", "\n", "half_width", "=", "width", "//", "2", "\n", "topleft", "=", "[", "\n", "int", "(", "torch", ".", "randint", "(", "0", ",", "int", "(", "distortion_scale", "*", "half_width", ")", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", ")", ",", "\n", "int", "(", "torch", ".", "randint", "(", "0", ",", "int", "(", "distortion_scale", "*", "half_height", ")", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", ")", "\n", "]", "\n", "topright", "=", "[", "\n", "int", "(", "torch", ".", "randint", "(", "width", "-", "int", "(", "distortion_scale", "*", "half_width", ")", "-", "1", ",", "width", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", ")", ",", "\n", "int", "(", "torch", ".", "randint", "(", "0", ",", "int", "(", "distortion_scale", "*", "half_height", ")", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", ")", "\n", "]", "\n", "botright", "=", "[", "\n", "int", "(", "torch", ".", "randint", "(", "width", "-", "int", "(", "distortion_scale", "*", "half_width", ")", "-", "1", ",", "width", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", ")", ",", "\n", "int", "(", "torch", ".", "randint", "(", "height", "-", "int", "(", "distortion_scale", "*", "half_height", ")", "-", "1", ",", "height", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", ")", "\n", "]", "\n", "botleft", "=", "[", "\n", "int", "(", "torch", ".", "randint", "(", "0", ",", "int", "(", "distortion_scale", "*", "half_width", ")", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", ")", ",", "\n", "int", "(", "torch", ".", "randint", "(", "height", "-", "int", "(", "distortion_scale", "*", "half_height", ")", "-", "1", ",", "height", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", ")", "\n", "]", "\n", "startpoints", "=", "[", "[", "0", ",", "0", "]", ",", "[", "width", "-", "1", ",", "0", "]", ",", "[", "width", "-", "1", ",", "height", "-", "1", "]", ",", "[", "0", ",", "height", "-", "1", "]", "]", "\n", "endpoints", "=", "[", "topleft", ",", "topright", ",", "botright", ",", "botleft", "]", "\n", "return", "startpoints", ",", "endpoints", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomPerspective.__repr__": [[790, 792], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(p={})'", ".", "format", "(", "self", ".", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomResizedCrop.__init__": [[823, 845], ["super().__init__", "transforms._setup_size", "isinstance", "isinstance", "TypeError", "isinstance", "TypeError", "warnings.warn", "warnings.warn", "functional._interpolation_modes_from_int"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int"], ["def", "__init__", "(", "self", ",", "size", ",", "scale", "=", "(", "0.08", ",", "1.0", ")", ",", "ratio", "=", "(", "3.", "/", "4.", ",", "4.", "/", "3.", ")", ",", "interpolation", "=", "InterpolationMode", ".", "BILINEAR", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "size", "=", "_setup_size", "(", "size", ",", "error_msg", "=", "\"Please provide only two dimensions (h, w) for size.\"", ")", "\n", "\n", "if", "not", "isinstance", "(", "scale", ",", "Sequence", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Scale should be a sequence\"", ")", "\n", "", "if", "not", "isinstance", "(", "ratio", ",", "Sequence", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Ratio should be a sequence\"", ")", "\n", "", "if", "(", "scale", "[", "0", "]", ">", "scale", "[", "1", "]", ")", "or", "(", "ratio", "[", "0", "]", ">", "ratio", "[", "1", "]", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"Scale and ratio should be of kind (min, max)\"", ")", "\n", "\n", "# Backward compatibility with integer value", "\n", "", "if", "isinstance", "(", "interpolation", ",", "int", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Argument interpolation should be of type InterpolationMode instead of int. \"", "\n", "\"Please, use InterpolationMode enum.\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "interpolation", ")", "\n", "\n", "", "self", ".", "interpolation", "=", "interpolation", "\n", "self", ".", "scale", "=", "scale", "\n", "self", ".", "ratio", "=", "ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomResizedCrop.get_params": [[846, 893], ["functional._get_image_size", "torch.log", "range", "torch.tensor", "torch.exp().item", "int", "int", "float", "float", "min", "int", "torch.empty().uniform_().item", "round", "round", "torch.randint().item", "torch.randint().item", "round", "max", "int", "torch.exp", "math.sqrt", "math.sqrt", "round", "torch.empty().uniform_", "torch.empty().uniform_", "torch.randint", "torch.randint", "min", "max", "torch.empty", "torch.empty"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "\n", "img", ":", "Tensor", ",", "scale", ":", "List", "[", "float", "]", ",", "ratio", ":", "List", "[", "float", "]", "\n", ")", "->", "Tuple", "[", "int", ",", "int", ",", "int", ",", "int", "]", ":", "\n", "        ", "\"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image or Tensor): Input image.\n            scale (list): range of scale of the origin size cropped\n            ratio (list): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n            sized crop.\n        \"\"\"", "\n", "width", ",", "height", "=", "F", ".", "_get_image_size", "(", "img", ")", "\n", "area", "=", "height", "*", "width", "\n", "\n", "log_ratio", "=", "torch", ".", "log", "(", "torch", ".", "tensor", "(", "ratio", ")", ")", "\n", "for", "_", "in", "range", "(", "10", ")", ":", "\n", "            ", "target_area", "=", "area", "*", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "scale", "[", "0", "]", ",", "scale", "[", "1", "]", ")", ".", "item", "(", ")", "\n", "aspect_ratio", "=", "torch", ".", "exp", "(", "\n", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "log_ratio", "[", "0", "]", ",", "log_ratio", "[", "1", "]", ")", "\n", ")", ".", "item", "(", ")", "\n", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "*", "aspect_ratio", ")", ")", ")", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "/", "aspect_ratio", ")", ")", ")", "\n", "\n", "if", "0", "<", "w", "<=", "width", "and", "0", "<", "h", "<=", "height", ":", "\n", "                ", "i", "=", "torch", ".", "randint", "(", "0", ",", "height", "-", "h", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", "\n", "j", "=", "torch", ".", "randint", "(", "0", ",", "width", "-", "w", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "\n", "# Fallback to central crop", "\n", "", "", "in_ratio", "=", "float", "(", "width", ")", "/", "float", "(", "height", ")", "\n", "if", "in_ratio", "<", "min", "(", "ratio", ")", ":", "\n", "            ", "w", "=", "width", "\n", "h", "=", "int", "(", "round", "(", "w", "/", "min", "(", "ratio", ")", ")", ")", "\n", "", "elif", "in_ratio", ">", "max", "(", "ratio", ")", ":", "\n", "            ", "h", "=", "height", "\n", "w", "=", "int", "(", "round", "(", "h", "*", "max", "(", "ratio", ")", ")", ")", "\n", "", "else", ":", "# whole image", "\n", "            ", "w", "=", "width", "\n", "h", "=", "height", "\n", "", "i", "=", "(", "height", "-", "h", ")", "//", "2", "\n", "j", "=", "(", "width", "-", "w", ")", "//", "2", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomResizedCrop.forward": [[894, 904], ["transforms.RandomResizedCrop.get_params", "functional.resized_crop"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.get_params", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.resized_crop"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be cropped and resized.\n\n        Returns:\n            PIL Image or Tensor: Randomly cropped and resized image.\n        \"\"\"", "\n", "i", ",", "j", ",", "h", ",", "w", "=", "self", ".", "get_params", "(", "img", ",", "self", ".", "scale", ",", "self", ".", "ratio", ")", "\n", "return", "F", ".", "resized_crop", "(", "img", ",", "i", ",", "j", ",", "h", ",", "w", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomResizedCrop.__repr__": [[905, 912], ["tuple", "tuple", "round", "round"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "interpolate_str", "=", "self", ".", "interpolation", ".", "value", "\n", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'(size={0}'", ".", "format", "(", "self", ".", "size", ")", "\n", "format_string", "+=", "', scale={0}'", ".", "format", "(", "tuple", "(", "round", "(", "s", ",", "4", ")", "for", "s", "in", "self", ".", "scale", ")", ")", "\n", "format_string", "+=", "', ratio={0}'", ".", "format", "(", "tuple", "(", "round", "(", "r", ",", "4", ")", "for", "r", "in", "self", ".", "ratio", ")", ")", "\n", "format_string", "+=", "', interpolation={0})'", ".", "format", "(", "interpolate_str", ")", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomSizedCrop.__init__": [[918, 922], ["warnings.warn", "transforms.RandomResizedCrop.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\"The use of the transforms.RandomSizedCrop transform is deprecated, \"", "+", "\n", "\"please use transforms.RandomResizedCrop instead.\"", ")", "\n", "super", "(", "RandomSizedCrop", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.FiveCrop.__init__": [[952, 955], ["super().__init__", "transforms._setup_size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_size"], ["def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "size", "=", "_setup_size", "(", "size", ",", "error_msg", "=", "\"Please provide only two dimensions (h, w) for size.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.FiveCrop.forward": [[956, 965], ["functional.five_crop"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.five_crop"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be cropped.\n\n        Returns:\n            tuple of 5 images. Image can be PIL Image or Tensor\n        \"\"\"", "\n", "return", "F", ".", "five_crop", "(", "img", ",", "self", ".", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.FiveCrop.__repr__": [[966, 968], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(size={0})'", ".", "format", "(", "self", ".", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.TenCrop.__init__": [[1000, 1004], ["super().__init__", "transforms._setup_size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_size"], ["def", "__init__", "(", "self", ",", "size", ",", "vertical_flip", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "size", "=", "_setup_size", "(", "size", ",", "error_msg", "=", "\"Please provide only two dimensions (h, w) for size.\"", ")", "\n", "self", ".", "vertical_flip", "=", "vertical_flip", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.TenCrop.forward": [[1005, 1014], ["functional.ten_crop"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.ten_crop"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be cropped.\n\n        Returns:\n            tuple of 10 images. Image can be PIL Image or Tensor\n        \"\"\"", "\n", "return", "F", ".", "ten_crop", "(", "img", ",", "self", ".", "size", ",", "self", ".", "vertical_flip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.TenCrop.__repr__": [[1015, 1017], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(size={0}, vertical_flip={1})'", ".", "format", "(", "self", ".", "size", ",", "self", ".", "vertical_flip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.LinearTransformation.__init__": [[1038, 1055], ["super().__init__", "transformation_matrix.size", "transformation_matrix.size", "ValueError", "mean_vector.size", "transformation_matrix.size", "ValueError", "ValueError", "mean_vector.size", "tuple", "transformation_matrix.size", "transformation_matrix.size"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "transformation_matrix", ",", "mean_vector", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "transformation_matrix", ".", "size", "(", "0", ")", "!=", "transformation_matrix", ".", "size", "(", "1", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"transformation_matrix should be square. Got \"", "+", "\n", "\"[{} x {}] rectangular matrix.\"", ".", "format", "(", "*", "transformation_matrix", ".", "size", "(", ")", ")", ")", "\n", "\n", "", "if", "mean_vector", ".", "size", "(", "0", ")", "!=", "transformation_matrix", ".", "size", "(", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"mean_vector should have the same length {}\"", ".", "format", "(", "mean_vector", ".", "size", "(", "0", ")", ")", "+", "\n", "\" as any one of the dimensions of the transformation_matrix [{}]\"", "\n", ".", "format", "(", "tuple", "(", "transformation_matrix", ".", "size", "(", ")", ")", ")", ")", "\n", "\n", "", "if", "transformation_matrix", ".", "device", "!=", "mean_vector", ".", "device", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input tensors should be on the same device. Got {} and {}\"", "\n", ".", "format", "(", "transformation_matrix", ".", "device", ",", "mean_vector", ".", "device", ")", ")", "\n", "\n", "", "self", ".", "transformation_matrix", "=", "transformation_matrix", "\n", "self", ".", "mean_vector", "=", "mean_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.LinearTransformation.forward": [[1056, 1079], ["torch.mm", "torch.mm.view", "ValueError", "ValueError", "torch.mm.view.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tensor", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "\"\"\"\n        Args:\n            tensor (Tensor): Tensor image to be whitened.\n\n        Returns:\n            Tensor: Transformed image.\n        \"\"\"", "\n", "shape", "=", "tensor", ".", "shape", "\n", "n", "=", "shape", "[", "-", "3", "]", "*", "shape", "[", "-", "2", "]", "*", "shape", "[", "-", "1", "]", "\n", "if", "n", "!=", "self", ".", "transformation_matrix", ".", "shape", "[", "0", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input tensor and transformation matrix have incompatible shape.\"", "+", "\n", "\"[{} x {} x {}] != \"", ".", "format", "(", "shape", "[", "-", "3", "]", ",", "shape", "[", "-", "2", "]", ",", "shape", "[", "-", "1", "]", ")", "+", "\n", "\"{}\"", ".", "format", "(", "self", ".", "transformation_matrix", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n", "", "if", "tensor", ".", "device", ".", "type", "!=", "self", ".", "mean_vector", ".", "device", ".", "type", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input tensor should be on the same device as transformation matrix and mean vector. \"", "\n", "\"Got {} vs {}\"", ".", "format", "(", "tensor", ".", "device", ",", "self", ".", "mean_vector", ".", "device", ")", ")", "\n", "\n", "", "flat_tensor", "=", "tensor", ".", "view", "(", "-", "1", ",", "n", ")", "-", "self", ".", "mean_vector", "\n", "transformed_tensor", "=", "torch", ".", "mm", "(", "flat_tensor", ",", "self", ".", "transformation_matrix", ")", "\n", "tensor", "=", "transformed_tensor", ".", "view", "(", "shape", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.LinearTransformation.__repr__": [[1080, 1085], ["str", "transforms.LinearTransformation.transformation_matrix.tolist", "str", "transforms.LinearTransformation.mean_vector.tolist"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'(transformation_matrix='", "\n", "format_string", "+=", "(", "str", "(", "self", ".", "transformation_matrix", ".", "tolist", "(", ")", ")", "+", "')'", ")", "\n", "format_string", "+=", "(", "\", (mean_vector=\"", "+", "str", "(", "self", ".", "mean_vector", ".", "tolist", "(", ")", ")", "+", "')'", ")", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ColorJitter.__init__": [[1108, 1115], ["super().__init__", "transforms.ColorJitter._check_input", "transforms.ColorJitter._check_input", "transforms.ColorJitter._check_input", "transforms.ColorJitter._check_input"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ColorJitter._check_input", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ColorJitter._check_input", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ColorJitter._check_input", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ColorJitter._check_input"], ["def", "__init__", "(", "self", ",", "brightness", "=", "0", ",", "contrast", "=", "0", ",", "saturation", "=", "0", ",", "hue", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "brightness", "=", "self", ".", "_check_input", "(", "brightness", ",", "'brightness'", ")", "\n", "self", ".", "contrast", "=", "self", ".", "_check_input", "(", "contrast", ",", "'contrast'", ")", "\n", "self", ".", "saturation", "=", "self", ".", "_check_input", "(", "saturation", ",", "'saturation'", ")", "\n", "self", ".", "hue", "=", "self", ".", "_check_input", "(", "hue", ",", "'hue'", ",", "center", "=", "0", ",", "bound", "=", "(", "-", "0.5", ",", "0.5", ")", ",", "\n", "clip_first_on_zero", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ColorJitter._check_input": [[1116, 1135], ["isinstance", "float", "ValueError", "max", "isinstance", "TypeError", "float", "float", "len", "ValueError"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "_check_input", "(", "self", ",", "value", ",", "name", ",", "center", "=", "1", ",", "bound", "=", "(", "0", ",", "float", "(", "'inf'", ")", ")", ",", "clip_first_on_zero", "=", "True", ")", ":", "\n", "        ", "if", "isinstance", "(", "value", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "if", "value", "<", "0", ":", "\n", "                ", "raise", "ValueError", "(", "\"If {} is a single number, it must be non negative.\"", ".", "format", "(", "name", ")", ")", "\n", "", "value", "=", "[", "center", "-", "float", "(", "value", ")", ",", "center", "+", "float", "(", "value", ")", "]", "\n", "if", "clip_first_on_zero", ":", "\n", "                ", "value", "[", "0", "]", "=", "max", "(", "value", "[", "0", "]", ",", "0.0", ")", "\n", "", "", "elif", "isinstance", "(", "value", ",", "(", "tuple", ",", "list", ")", ")", "and", "len", "(", "value", ")", "==", "2", ":", "\n", "            ", "if", "not", "bound", "[", "0", "]", "<=", "value", "[", "0", "]", "<=", "value", "[", "1", "]", "<=", "bound", "[", "1", "]", ":", "\n", "                ", "raise", "ValueError", "(", "\"{} values should be between {}\"", ".", "format", "(", "name", ",", "bound", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "\"{} should be a single number or a list/tuple with length 2.\"", ".", "format", "(", "name", ")", ")", "\n", "\n", "# if value is 0 or (1., 1.) for brightness/contrast/saturation", "\n", "# or (0., 0.) for hue, do nothing", "\n", "", "if", "value", "[", "0", "]", "==", "value", "[", "1", "]", "==", "center", ":", "\n", "            ", "value", "=", "None", "\n", "", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ColorJitter.get_params": [[1136, 1166], ["torch.randperm", "float", "float", "float", "float", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty", "torch.empty", "torch.empty", "torch.empty"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "brightness", ":", "Optional", "[", "List", "[", "float", "]", "]", ",", "\n", "contrast", ":", "Optional", "[", "List", "[", "float", "]", "]", ",", "\n", "saturation", ":", "Optional", "[", "List", "[", "float", "]", "]", ",", "\n", "hue", ":", "Optional", "[", "List", "[", "float", "]", "]", "\n", ")", "->", "Tuple", "[", "Tensor", ",", "Optional", "[", "float", "]", ",", "Optional", "[", "float", "]", ",", "Optional", "[", "float", "]", ",", "Optional", "[", "float", "]", "]", ":", "\n", "        ", "\"\"\"Get the parameters for the randomized transform to be applied on image.\n\n        Args:\n            brightness (tuple of float (min, max), optional): The range from which the brightness_factor is chosen\n                uniformly. Pass None to turn off the transformation.\n            contrast (tuple of float (min, max), optional): The range from which the contrast_factor is chosen\n                uniformly. Pass None to turn off the transformation.\n            saturation (tuple of float (min, max), optional): The range from which the saturation_factor is chosen\n                uniformly. Pass None to turn off the transformation.\n            hue (tuple of float (min, max), optional): The range from which the hue_factor is chosen uniformly.\n                Pass None to turn off the transformation.\n\n        Returns:\n            tuple: The parameters used to apply the randomized transform\n            along with their random order.\n        \"\"\"", "\n", "fn_idx", "=", "torch", ".", "randperm", "(", "4", ")", "\n", "\n", "b", "=", "None", "if", "brightness", "is", "None", "else", "float", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "brightness", "[", "0", "]", ",", "brightness", "[", "1", "]", ")", ")", "\n", "c", "=", "None", "if", "contrast", "is", "None", "else", "float", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "contrast", "[", "0", "]", ",", "contrast", "[", "1", "]", ")", ")", "\n", "s", "=", "None", "if", "saturation", "is", "None", "else", "float", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "saturation", "[", "0", "]", ",", "saturation", "[", "1", "]", ")", ")", "\n", "h", "=", "None", "if", "hue", "is", "None", "else", "float", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "hue", "[", "0", "]", ",", "hue", "[", "1", "]", ")", ")", "\n", "\n", "return", "fn_idx", ",", "b", ",", "c", ",", "s", ",", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ColorJitter.forward": [[1167, 1189], ["transforms.ColorJitter.get_params", "functional.adjust_brightness", "functional.adjust_contrast", "functional.adjust_saturation", "functional.adjust_hue"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.get_params", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_brightness", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_contrast", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_saturation", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_hue"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Input image.\n\n        Returns:\n            PIL Image or Tensor: Color jittered image.\n        \"\"\"", "\n", "fn_idx", ",", "brightness_factor", ",", "contrast_factor", ",", "saturation_factor", ",", "hue_factor", "=", "self", ".", "get_params", "(", "self", ".", "brightness", ",", "self", ".", "contrast", ",", "self", ".", "saturation", ",", "self", ".", "hue", ")", "\n", "\n", "for", "fn_id", "in", "fn_idx", ":", "\n", "            ", "if", "fn_id", "==", "0", "and", "brightness_factor", "is", "not", "None", ":", "\n", "                ", "img", "=", "F", ".", "adjust_brightness", "(", "img", ",", "brightness_factor", ")", "\n", "", "elif", "fn_id", "==", "1", "and", "contrast_factor", "is", "not", "None", ":", "\n", "                ", "img", "=", "F", ".", "adjust_contrast", "(", "img", ",", "contrast_factor", ")", "\n", "", "elif", "fn_id", "==", "2", "and", "saturation_factor", "is", "not", "None", ":", "\n", "                ", "img", "=", "F", ".", "adjust_saturation", "(", "img", ",", "saturation_factor", ")", "\n", "", "elif", "fn_id", "==", "3", "and", "hue_factor", "is", "not", "None", ":", "\n", "                ", "img", "=", "F", ".", "adjust_hue", "(", "img", ",", "hue_factor", ")", "\n", "\n", "", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.ColorJitter.__repr__": [[1190, 1197], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'('", "\n", "format_string", "+=", "'brightness={0}'", ".", "format", "(", "self", ".", "brightness", ")", "\n", "format_string", "+=", "', contrast={0}'", ".", "format", "(", "self", ".", "contrast", ")", "\n", "format_string", "+=", "', saturation={0}'", ".", "format", "(", "self", ".", "saturation", ")", "\n", "format_string", "+=", "', hue={0})'", ".", "format", "(", "self", ".", "hue", ")", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomRotation.__init__": [[1227, 1261], ["super().__init__", "isinstance", "transforms._setup_angle", "warnings.warn", "functional._interpolation_modes_from_int", "warnings.warn", "functional._interpolation_modes_from_int", "transforms._check_sequence_input", "isinstance", "TypeError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_angle", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._check_sequence_input"], ["def", "__init__", "(", "\n", "self", ",", "degrees", ",", "interpolation", "=", "InterpolationMode", ".", "NEAREST", ",", "expand", "=", "False", ",", "center", "=", "None", ",", "fill", "=", "0", ",", "resample", "=", "None", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "resample", "is", "not", "None", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "resample", ")", "\n", "\n", "# Backward compatibility with integer value", "\n", "", "if", "isinstance", "(", "interpolation", ",", "int", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Argument interpolation should be of type InterpolationMode instead of int. \"", "\n", "\"Please, use InterpolationMode enum.\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "interpolation", ")", "\n", "\n", "", "self", ".", "degrees", "=", "_setup_angle", "(", "degrees", ",", "name", "=", "\"degrees\"", ",", "req_sizes", "=", "(", "2", ",", ")", ")", "\n", "\n", "if", "center", "is", "not", "None", ":", "\n", "            ", "_check_sequence_input", "(", "center", ",", "\"center\"", ",", "req_sizes", "=", "(", "2", ",", ")", ")", "\n", "\n", "", "self", ".", "center", "=", "center", "\n", "\n", "self", ".", "resample", "=", "self", ".", "interpolation", "=", "interpolation", "\n", "self", ".", "expand", "=", "expand", "\n", "\n", "if", "fill", "is", "None", ":", "\n", "            ", "fill", "=", "0", "\n", "", "elif", "not", "isinstance", "(", "fill", ",", "(", "Sequence", ",", "numbers", ".", "Number", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Fill should be either a sequence or a number.\"", ")", "\n", "\n", "", "self", ".", "fill", "=", "fill", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomRotation.get_params": [[1262, 1271], ["float", "torch.empty().uniform_().item", "torch.empty().uniform_", "float", "float", "torch.empty"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "degrees", ":", "List", "[", "float", "]", ")", "->", "float", ":", "\n", "        ", "\"\"\"Get parameters for ``rotate`` for a random rotation.\n\n        Returns:\n            float: angle parameter to be passed to ``rotate`` for random rotation.\n        \"\"\"", "\n", "angle", "=", "float", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "float", "(", "degrees", "[", "0", "]", ")", ",", "float", "(", "degrees", "[", "1", "]", ")", ")", ".", "item", "(", ")", ")", "\n", "return", "angle", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomRotation.forward": [[1272, 1289], ["isinstance", "transforms.RandomRotation.get_params", "functional.rotate", "isinstance", "functional._get_image_num_channels", "float", "float"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.get_params", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rotate", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be rotated.\n\n        Returns:\n            PIL Image or Tensor: Rotated image.\n        \"\"\"", "\n", "fill", "=", "self", ".", "fill", "\n", "if", "isinstance", "(", "img", ",", "Tensor", ")", ":", "\n", "            ", "if", "isinstance", "(", "fill", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "fill", "=", "[", "float", "(", "fill", ")", "]", "*", "F", ".", "_get_image_num_channels", "(", "img", ")", "\n", "", "else", ":", "\n", "                ", "fill", "=", "[", "float", "(", "f", ")", "for", "f", "in", "fill", "]", "\n", "", "", "angle", "=", "self", ".", "get_params", "(", "self", ".", "degrees", ")", "\n", "\n", "return", "F", ".", "rotate", "(", "img", ",", "angle", ",", "self", ".", "resample", ",", "self", ".", "expand", ",", "self", ".", "center", ",", "fill", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomRotation.__repr__": [[1290, 1301], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "interpolate_str", "=", "self", ".", "interpolation", ".", "value", "\n", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'(degrees={0}'", ".", "format", "(", "self", ".", "degrees", ")", "\n", "format_string", "+=", "', interpolation={0}'", ".", "format", "(", "interpolate_str", ")", "\n", "format_string", "+=", "', expand={0}'", ".", "format", "(", "self", ".", "expand", ")", "\n", "if", "self", ".", "center", "is", "not", "None", ":", "\n", "            ", "format_string", "+=", "', center={0}'", ".", "format", "(", "self", ".", "center", ")", "\n", "", "if", "self", ".", "fill", "is", "not", "None", ":", "\n", "            ", "format_string", "+=", "', fill={0}'", ".", "format", "(", "self", ".", "fill", ")", "\n", "", "format_string", "+=", "')'", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAffine.__init__": [[1339, 1393], ["super().__init__", "isinstance", "transforms._setup_angle", "warnings.warn", "functional._interpolation_modes_from_int", "warnings.warn", "functional._interpolation_modes_from_int", "warnings.warn", "transforms._check_sequence_input", "transforms._check_sequence_input", "transforms._setup_angle", "isinstance", "TypeError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_angle", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._check_sequence_input", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._check_sequence_input", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_angle"], ["def", "__init__", "(", "\n", "self", ",", "degrees", ",", "translate", "=", "None", ",", "scale", "=", "None", ",", "shear", "=", "None", ",", "interpolation", "=", "InterpolationMode", ".", "NEAREST", ",", "fill", "=", "0", ",", "\n", "fillcolor", "=", "None", ",", "resample", "=", "None", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "resample", "is", "not", "None", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "resample", ")", "\n", "\n", "# Backward compatibility with integer value", "\n", "", "if", "isinstance", "(", "interpolation", ",", "int", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Argument interpolation should be of type InterpolationMode instead of int. \"", "\n", "\"Please, use InterpolationMode enum.\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "interpolation", ")", "\n", "\n", "", "if", "fillcolor", "is", "not", "None", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"", "\n", ")", "\n", "fill", "=", "fillcolor", "\n", "\n", "", "self", ".", "degrees", "=", "_setup_angle", "(", "degrees", ",", "name", "=", "\"degrees\"", ",", "req_sizes", "=", "(", "2", ",", ")", ")", "\n", "\n", "if", "translate", "is", "not", "None", ":", "\n", "            ", "_check_sequence_input", "(", "translate", ",", "\"translate\"", ",", "req_sizes", "=", "(", "2", ",", ")", ")", "\n", "for", "t", "in", "translate", ":", "\n", "                ", "if", "not", "(", "0.0", "<=", "t", "<=", "1.0", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "\"translation values should be between 0 and 1\"", ")", "\n", "", "", "", "self", ".", "translate", "=", "translate", "\n", "\n", "if", "scale", "is", "not", "None", ":", "\n", "            ", "_check_sequence_input", "(", "scale", ",", "\"scale\"", ",", "req_sizes", "=", "(", "2", ",", ")", ")", "\n", "for", "s", "in", "scale", ":", "\n", "                ", "if", "s", "<=", "0", ":", "\n", "                    ", "raise", "ValueError", "(", "\"scale values should be positive\"", ")", "\n", "", "", "", "self", ".", "scale", "=", "scale", "\n", "\n", "if", "shear", "is", "not", "None", ":", "\n", "            ", "self", ".", "shear", "=", "_setup_angle", "(", "shear", ",", "name", "=", "\"shear\"", ",", "req_sizes", "=", "(", "2", ",", "4", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "shear", "=", "shear", "\n", "\n", "", "self", ".", "resample", "=", "self", ".", "interpolation", "=", "interpolation", "\n", "\n", "if", "fill", "is", "None", ":", "\n", "            ", "fill", "=", "0", "\n", "", "elif", "not", "isinstance", "(", "fill", ",", "(", "Sequence", ",", "numbers", ".", "Number", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Fill should be either a sequence or a number.\"", ")", "\n", "\n", "", "self", ".", "fillcolor", "=", "self", ".", "fill", "=", "fill", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAffine.get_params": [[1394, 1431], ["float", "torch.empty().uniform_().item", "float", "float", "int", "int", "float", "float", "round", "round", "torch.empty().uniform_().item", "torch.empty().uniform_().item", "len", "float", "torch.empty().uniform_", "torch.empty().uniform_().item", "torch.empty().uniform_().item", "torch.empty().uniform_().item", "float", "float", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "\n", "degrees", ":", "List", "[", "float", "]", ",", "\n", "translate", ":", "Optional", "[", "List", "[", "float", "]", "]", ",", "\n", "scale_ranges", ":", "Optional", "[", "List", "[", "float", "]", "]", ",", "\n", "shears", ":", "Optional", "[", "List", "[", "float", "]", "]", ",", "\n", "img_size", ":", "List", "[", "int", "]", "\n", ")", "->", "Tuple", "[", "float", ",", "Tuple", "[", "int", ",", "int", "]", ",", "float", ",", "Tuple", "[", "float", ",", "float", "]", "]", ":", "\n", "        ", "\"\"\"Get parameters for affine transformation\n\n        Returns:\n            params to be passed to the affine transformation\n        \"\"\"", "\n", "angle", "=", "float", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "float", "(", "degrees", "[", "0", "]", ")", ",", "float", "(", "degrees", "[", "1", "]", ")", ")", ".", "item", "(", ")", ")", "\n", "if", "translate", "is", "not", "None", ":", "\n", "            ", "max_dx", "=", "float", "(", "translate", "[", "0", "]", "*", "img_size", "[", "0", "]", ")", "\n", "max_dy", "=", "float", "(", "translate", "[", "1", "]", "*", "img_size", "[", "1", "]", ")", "\n", "tx", "=", "int", "(", "round", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "-", "max_dx", ",", "max_dx", ")", ".", "item", "(", ")", ")", ")", "\n", "ty", "=", "int", "(", "round", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "-", "max_dy", ",", "max_dy", ")", ".", "item", "(", ")", ")", ")", "\n", "translations", "=", "(", "tx", ",", "ty", ")", "\n", "", "else", ":", "\n", "            ", "translations", "=", "(", "0", ",", "0", ")", "\n", "\n", "", "if", "scale_ranges", "is", "not", "None", ":", "\n", "            ", "scale", "=", "float", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "scale_ranges", "[", "0", "]", ",", "scale_ranges", "[", "1", "]", ")", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "scale", "=", "1.0", "\n", "\n", "", "shear_x", "=", "shear_y", "=", "0.0", "\n", "if", "shears", "is", "not", "None", ":", "\n", "            ", "shear_x", "=", "float", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "shears", "[", "0", "]", ",", "shears", "[", "1", "]", ")", ".", "item", "(", ")", ")", "\n", "if", "len", "(", "shears", ")", "==", "4", ":", "\n", "                ", "shear_y", "=", "float", "(", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "shears", "[", "2", "]", ",", "shears", "[", "3", "]", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "", "shear", "=", "(", "shear_x", ",", "shear_y", ")", "\n", "\n", "return", "angle", ",", "translations", ",", "scale", ",", "shear", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAffine.forward": [[1432, 1451], ["isinstance", "functional._get_image_size", "transforms.RandomAffine.get_params", "functional.affine", "isinstance", "functional._get_image_num_channels", "float", "float"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.get_params", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.affine", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n            img (PIL Image or Tensor): Image to be transformed.\n\n        Returns:\n            PIL Image or Tensor: Affine transformed image.\n        \"\"\"", "\n", "fill", "=", "self", ".", "fill", "\n", "if", "isinstance", "(", "img", ",", "Tensor", ")", ":", "\n", "            ", "if", "isinstance", "(", "fill", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "fill", "=", "[", "float", "(", "fill", ")", "]", "*", "F", ".", "_get_image_num_channels", "(", "img", ")", "\n", "", "else", ":", "\n", "                ", "fill", "=", "[", "float", "(", "f", ")", "for", "f", "in", "fill", "]", "\n", "\n", "", "", "img_size", "=", "F", ".", "_get_image_size", "(", "img", ")", "\n", "\n", "ret", "=", "self", ".", "get_params", "(", "self", ".", "degrees", ",", "self", ".", "translate", ",", "self", ".", "scale", ",", "self", ".", "shear", ",", "img_size", ")", "\n", "\n", "return", "F", ".", "affine", "(", "img", ",", "*", "ret", ",", "interpolation", "=", "self", ".", "interpolation", ",", "fill", "=", "fill", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAffine.__repr__": [[1452, 1468], ["dict", "s.format"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "'{name}(degrees={degrees}'", "\n", "if", "self", ".", "translate", "is", "not", "None", ":", "\n", "            ", "s", "+=", "', translate={translate}'", "\n", "", "if", "self", ".", "scale", "is", "not", "None", ":", "\n", "            ", "s", "+=", "', scale={scale}'", "\n", "", "if", "self", ".", "shear", "is", "not", "None", ":", "\n", "            ", "s", "+=", "', shear={shear}'", "\n", "", "if", "self", ".", "interpolation", "!=", "InterpolationMode", ".", "NEAREST", ":", "\n", "            ", "s", "+=", "', interpolation={interpolation}'", "\n", "", "if", "self", ".", "fill", "!=", "0", ":", "\n", "            ", "s", "+=", "', fill={fill}'", "\n", "", "s", "+=", "')'", "\n", "d", "=", "dict", "(", "self", ".", "__dict__", ")", "\n", "d", "[", "'interpolation'", "]", "=", "self", ".", "interpolation", ".", "value", "\n", "return", "s", ".", "format", "(", "name", "=", "self", ".", "__class__", ".", "__name__", ",", "**", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Grayscale.__init__": [[1486, 1489], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "num_output_channels", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_output_channels", "=", "num_output_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Grayscale.forward": [[1490, 1499], ["functional.rgb_to_grayscale"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rgb_to_grayscale"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be converted to grayscale.\n\n        Returns:\n            PIL Image or Tensor: Grayscaled image.\n        \"\"\"", "\n", "return", "F", ".", "rgb_to_grayscale", "(", "img", ",", "num_output_channels", "=", "self", ".", "num_output_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.Grayscale.__repr__": [[1500, 1502], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(num_output_channels={0})'", ".", "format", "(", "self", ".", "num_output_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomGrayscale.__init__": [[1520, 1523], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "p", "=", "0.1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomGrayscale.forward": [[1524, 1536], ["functional._get_image_num_channels", "torch.rand", "functional.rgb_to_grayscale"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rgb_to_grayscale"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be converted to grayscale.\n\n        Returns:\n            PIL Image or Tensor: Randomly grayscaled image.\n        \"\"\"", "\n", "num_output_channels", "=", "F", ".", "_get_image_num_channels", "(", "img", ")", "\n", "if", "torch", ".", "rand", "(", "1", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "F", ".", "rgb_to_grayscale", "(", "img", ",", "num_output_channels", "=", "num_output_channels", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomGrayscale.__repr__": [[1537, 1539], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(p={0})'", ".", "format", "(", "self", ".", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomErasing.__init__": [[1568, 1590], ["super().__init__", "isinstance", "TypeError", "isinstance", "ValueError", "isinstance", "TypeError", "isinstance", "TypeError", "warnings.warn", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "p", "=", "0.5", ",", "scale", "=", "(", "0.02", ",", "0.33", ")", ",", "ratio", "=", "(", "0.3", ",", "3.3", ")", ",", "value", "=", "0", ",", "inplace", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "value", ",", "(", "numbers", ".", "Number", ",", "str", ",", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Argument value should be either a number or str or a sequence\"", ")", "\n", "", "if", "isinstance", "(", "value", ",", "str", ")", "and", "value", "!=", "\"random\"", ":", "\n", "            ", "raise", "ValueError", "(", "\"If value is str, it should be 'random'\"", ")", "\n", "", "if", "not", "isinstance", "(", "scale", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Scale should be a sequence\"", ")", "\n", "", "if", "not", "isinstance", "(", "ratio", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"Ratio should be a sequence\"", ")", "\n", "", "if", "(", "scale", "[", "0", "]", ">", "scale", "[", "1", "]", ")", "or", "(", "ratio", "[", "0", "]", ">", "ratio", "[", "1", "]", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"Scale and ratio should be of kind (min, max)\"", ")", "\n", "", "if", "scale", "[", "0", "]", "<", "0", "or", "scale", "[", "1", "]", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Scale should be between 0 and 1\"", ")", "\n", "", "if", "p", "<", "0", "or", "p", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Random erasing probability should be between 0 and 1\"", ")", "\n", "\n", "", "self", ".", "p", "=", "p", "\n", "self", ".", "scale", "=", "scale", "\n", "self", ".", "ratio", "=", "ratio", "\n", "self", ".", "value", "=", "value", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomErasing.get_params": [[1591, 1634], ["torch.log", "range", "torch.tensor", "torch.exp().item", "int", "int", "torch.randint().item", "torch.randint().item", "torch.empty().uniform_().item", "round", "round", "torch.empty().normal_", "torch.exp", "math.sqrt", "math.sqrt", "torch.tensor", "torch.randint", "torch.randint", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty", "torch.empty", "torch.empty"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "\n", "img", ":", "Tensor", ",", "scale", ":", "Tuple", "[", "float", ",", "float", "]", ",", "ratio", ":", "Tuple", "[", "float", ",", "float", "]", ",", "value", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", "\n", ")", "->", "Tuple", "[", "int", ",", "int", ",", "int", ",", "int", ",", "Tensor", "]", ":", "\n", "        ", "\"\"\"Get parameters for ``erase`` for a random erasing.\n\n        Args:\n            img (Tensor): Tensor image to be erased.\n            scale (sequence): range of proportion of erased area against input image.\n            ratio (sequence): range of aspect ratio of erased area.\n            value (list, optional): erasing value. If None, it is interpreted as \"random\"\n                (erasing each pixel with random values). If ``len(value)`` is 1, it is interpreted as a number,\n                i.e. ``value[0]``.\n\n        Returns:\n            tuple: params (i, j, h, w, v) to be passed to ``erase`` for random erasing.\n        \"\"\"", "\n", "img_c", ",", "img_h", ",", "img_w", "=", "img", ".", "shape", "[", "-", "3", "]", ",", "img", ".", "shape", "[", "-", "2", "]", ",", "img", ".", "shape", "[", "-", "1", "]", "\n", "area", "=", "img_h", "*", "img_w", "\n", "\n", "log_ratio", "=", "torch", ".", "log", "(", "torch", ".", "tensor", "(", "ratio", ")", ")", "\n", "for", "_", "in", "range", "(", "10", ")", ":", "\n", "            ", "erase_area", "=", "area", "*", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "scale", "[", "0", "]", ",", "scale", "[", "1", "]", ")", ".", "item", "(", ")", "\n", "aspect_ratio", "=", "torch", ".", "exp", "(", "\n", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "log_ratio", "[", "0", "]", ",", "log_ratio", "[", "1", "]", ")", "\n", ")", ".", "item", "(", ")", "\n", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "erase_area", "*", "aspect_ratio", ")", ")", ")", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "erase_area", "/", "aspect_ratio", ")", ")", ")", "\n", "if", "not", "(", "h", "<", "img_h", "and", "w", "<", "img_w", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "value", "is", "None", ":", "\n", "                ", "v", "=", "torch", ".", "empty", "(", "[", "img_c", ",", "h", ",", "w", "]", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "normal_", "(", ")", "\n", "", "else", ":", "\n", "                ", "v", "=", "torch", ".", "tensor", "(", "value", ")", "[", ":", ",", "None", ",", "None", "]", "\n", "\n", "", "i", "=", "torch", ".", "randint", "(", "0", ",", "img_h", "-", "h", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", "\n", "j", "=", "torch", ".", "randint", "(", "0", ",", "img_w", "-", "w", "+", "1", ",", "size", "=", "(", "1", ",", ")", ")", ".", "item", "(", ")", "\n", "return", "i", ",", "j", ",", "h", ",", "w", ",", "v", "\n", "\n", "# Return original image", "\n", "", "return", "0", ",", "0", ",", "img_h", ",", "img_w", ",", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomErasing.forward": [[1635, 1664], ["torch.rand", "isinstance", "transforms.RandomErasing.get_params", "functional.erase", "isinstance", "ValueError", "isinstance", "list", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.get_params", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.erase", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (Tensor): Tensor image to be erased.\n\n        Returns:\n            img (Tensor): Erased Tensor image.\n        \"\"\"", "\n", "if", "torch", ".", "rand", "(", "1", ")", "<", "self", ".", "p", ":", "\n", "\n", "# cast self.value to script acceptable type", "\n", "            ", "if", "isinstance", "(", "self", ".", "value", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "value", "=", "[", "self", ".", "value", ",", "]", "\n", "", "elif", "isinstance", "(", "self", ".", "value", ",", "str", ")", ":", "\n", "                ", "value", "=", "None", "\n", "", "elif", "isinstance", "(", "self", ".", "value", ",", "tuple", ")", ":", "\n", "                ", "value", "=", "list", "(", "self", ".", "value", ")", "\n", "", "else", ":", "\n", "                ", "value", "=", "self", ".", "value", "\n", "\n", "", "if", "value", "is", "not", "None", "and", "not", "(", "len", "(", "value", ")", "in", "(", "1", ",", "img", ".", "shape", "[", "-", "3", "]", ")", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"If value is a sequence, it should have either a single value or \"", "\n", "\"{} (number of input channels)\"", ".", "format", "(", "img", ".", "shape", "[", "-", "3", "]", ")", "\n", ")", "\n", "\n", "", "x", ",", "y", ",", "h", ",", "w", ",", "v", "=", "self", ".", "get_params", "(", "img", ",", "scale", "=", "self", ".", "scale", ",", "ratio", "=", "self", ".", "ratio", ",", "value", "=", "value", ")", "\n", "return", "F", ".", "erase", "(", "img", ",", "x", ",", "y", ",", "h", ",", "w", ",", "v", ",", "self", ".", "inplace", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomErasing.__repr__": [[1665, 1672], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "'(p={}, '", ".", "format", "(", "self", ".", "p", ")", "\n", "s", "+=", "'scale={}, '", ".", "format", "(", "self", ".", "scale", ")", "\n", "s", "+=", "'ratio={}, '", ".", "format", "(", "self", ".", "ratio", ")", "\n", "s", "+=", "'value={}, '", ".", "format", "(", "self", ".", "value", ")", "\n", "s", "+=", "'inplace={})'", ".", "format", "(", "self", ".", "inplace", ")", "\n", "return", "self", ".", "__class__", ".", "__name__", "+", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.__init__": [[1691, 1709], ["super().__init__", "transforms._setup_size", "isinstance", "ValueError", "ValueError", "isinstance", "ValueError", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_size"], ["def", "__init__", "(", "self", ",", "kernel_size", ",", "sigma", "=", "(", "0.1", ",", "2.0", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "kernel_size", "=", "_setup_size", "(", "kernel_size", ",", "\"Kernel size should be a tuple/list of two integers\"", ")", "\n", "for", "ks", "in", "self", ".", "kernel_size", ":", "\n", "            ", "if", "ks", "<=", "0", "or", "ks", "%", "2", "==", "0", ":", "\n", "                ", "raise", "ValueError", "(", "\"Kernel size value should be an odd and positive number.\"", ")", "\n", "\n", "", "", "if", "isinstance", "(", "sigma", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "if", "sigma", "<=", "0", ":", "\n", "                ", "raise", "ValueError", "(", "\"If sigma is a single number, it must be positive.\"", ")", "\n", "", "sigma", "=", "(", "sigma", ",", "sigma", ")", "\n", "", "elif", "isinstance", "(", "sigma", ",", "Sequence", ")", "and", "len", "(", "sigma", ")", "==", "2", ":", "\n", "            ", "if", "not", "0.", "<", "sigma", "[", "0", "]", "<=", "sigma", "[", "1", "]", ":", "\n", "                ", "raise", "ValueError", "(", "\"sigma values should be positive and of the form (min, max).\"", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"sigma should be a single number or a list/tuple with length 2.\"", ")", "\n", "\n", "", "self", ".", "sigma", "=", "sigma", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.get_params": [[1710, 1722], ["torch.empty().uniform_().item", "torch.empty().uniform_", "torch.empty"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "sigma_min", ":", "float", ",", "sigma_max", ":", "float", ")", "->", "float", ":", "\n", "        ", "\"\"\"Choose sigma for random gaussian blurring.\n\n        Args:\n            sigma_min (float): Minimum standard deviation that can be chosen for blurring kernel.\n            sigma_max (float): Maximum standard deviation that can be chosen for blurring kernel.\n\n        Returns:\n            float: Standard deviation to be passed to calculate kernel for gaussian blurring.\n        \"\"\"", "\n", "return", "torch", ".", "empty", "(", "1", ")", ".", "uniform_", "(", "sigma_min", ",", "sigma_max", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.forward": [[1723, 1733], ["transforms.GaussianBlur.get_params", "functional.gaussian_blur"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.get_params", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.gaussian_blur"], ["", "def", "forward", "(", "self", ",", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): image to be blurred.\n\n        Returns:\n            PIL Image or Tensor: Gaussian blurred image\n        \"\"\"", "\n", "sigma", "=", "self", ".", "get_params", "(", "self", ".", "sigma", "[", "0", "]", ",", "self", ".", "sigma", "[", "1", "]", ")", "\n", "return", "F", ".", "gaussian_blur", "(", "img", ",", "self", ".", "kernel_size", ",", "[", "sigma", ",", "sigma", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.GaussianBlur.__repr__": [[1734, 1738], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "'(kernel_size={}, '", ".", "format", "(", "self", ".", "kernel_size", ")", "\n", "s", "+=", "'sigma={})'", ".", "format", "(", "self", ".", "sigma", ")", "\n", "return", "self", ".", "__class__", ".", "__name__", "+", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomInvert.__init__": [[1782, 1785], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "p", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomInvert.forward": [[1786, 1797], ["torch.rand().item", "functional.invert", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.invert"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be inverted.\n\n        Returns:\n            PIL Image or Tensor: Randomly color inverted image.\n        \"\"\"", "\n", "if", "torch", ".", "rand", "(", "1", ")", ".", "item", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "F", ".", "invert", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomInvert.__repr__": [[1798, 1800], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(p={})'", ".", "format", "(", "self", ".", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomPosterize.__init__": [[1813, 1817], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "bits", ",", "p", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bits", "=", "bits", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomPosterize.forward": [[1818, 1829], ["torch.rand().item", "functional.posterize", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.posterize"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be posterized.\n\n        Returns:\n            PIL Image or Tensor: Randomly posterized image.\n        \"\"\"", "\n", "if", "torch", ".", "rand", "(", "1", ")", ".", "item", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "F", ".", "posterize", "(", "img", ",", "self", ".", "bits", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomPosterize.__repr__": [[1830, 1832], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(bits={},p={})'", ".", "format", "(", "self", ".", "bits", ",", "self", ".", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomSolarize.__init__": [[1845, 1849], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "threshold", ",", "p", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "threshold", "=", "threshold", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomSolarize.forward": [[1850, 1861], ["torch.rand().item", "functional.solarize", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.solarize"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be solarized.\n\n        Returns:\n            PIL Image or Tensor: Randomly solarized image.\n        \"\"\"", "\n", "if", "torch", ".", "rand", "(", "1", ")", ".", "item", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "F", ".", "solarize", "(", "img", ",", "self", ".", "threshold", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomSolarize.__repr__": [[1862, 1864], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(threshold={},p={})'", ".", "format", "(", "self", ".", "threshold", ",", "self", ".", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAdjustSharpness.__init__": [[1877, 1881], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "sharpness_factor", ",", "p", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sharpness_factor", "=", "sharpness_factor", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAdjustSharpness.forward": [[1882, 1893], ["torch.rand().item", "functional.adjust_sharpness", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_sharpness"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be sharpened.\n\n        Returns:\n            PIL Image or Tensor: Randomly sharpened image.\n        \"\"\"", "\n", "if", "torch", ".", "rand", "(", "1", ")", ".", "item", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "F", ".", "adjust_sharpness", "(", "img", ",", "self", ".", "sharpness_factor", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAdjustSharpness.__repr__": [[1894, 1896], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(sharpness_factor={},p={})'", ".", "format", "(", "self", ".", "sharpness_factor", ",", "self", ".", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAutocontrast.__init__": [[1908, 1911], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "p", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAutocontrast.forward": [[1912, 1923], ["torch.rand().item", "functional.autocontrast", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.autocontrast"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be autocontrasted.\n\n        Returns:\n            PIL Image or Tensor: Randomly autocontrasted image.\n        \"\"\"", "\n", "if", "torch", ".", "rand", "(", "1", ")", ".", "item", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "F", ".", "autocontrast", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomAutocontrast.__repr__": [[1924, 1926], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(p={})'", ".", "format", "(", "self", ".", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomEqualize.__init__": [[1938, 1941], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "p", "=", "0.5", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "p", "=", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomEqualize.forward": [[1942, 1953], ["torch.rand().item", "functional.equalize", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.equalize"], ["", "def", "forward", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image or Tensor): Image to be equalized.\n\n        Returns:\n            PIL Image or Tensor: Randomly equalized image.\n        \"\"\"", "\n", "if", "torch", ".", "rand", "(", "1", ")", ".", "item", "(", ")", "<", "self", ".", "p", ":", "\n", "            ", "return", "F", ".", "equalize", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms.RandomEqualize.__repr__": [[1954, 1956], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "'(p={})'", ".", "format", "(", "self", ".", "p", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_size": [[1740, 1751], ["isinstance", "isinstance", "len", "ValueError", "int", "int", "len"], "function", ["None"], ["", "", "def", "_setup_size", "(", "size", ",", "error_msg", ")", ":", "\n", "    ", "if", "isinstance", "(", "size", ",", "numbers", ".", "Number", ")", ":", "\n", "        ", "return", "int", "(", "size", ")", ",", "int", "(", "size", ")", "\n", "\n", "", "if", "isinstance", "(", "size", ",", "Sequence", ")", "and", "len", "(", "size", ")", "==", "1", ":", "\n", "        ", "return", "size", "[", "0", "]", ",", "size", "[", "0", "]", "\n", "\n", "", "if", "len", "(", "size", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "error_msg", ")", "\n", "\n", "", "return", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._check_sequence_input": [[1753, 1759], ["isinstance", "TypeError", "len", "ValueError", "len", "str"], "function", ["None"], ["", "def", "_check_sequence_input", "(", "x", ",", "name", ",", "req_sizes", ")", ":", "\n", "    ", "msg", "=", "req_sizes", "[", "0", "]", "if", "len", "(", "req_sizes", ")", "<", "2", "else", "\" or \"", ".", "join", "(", "[", "str", "(", "s", ")", "for", "s", "in", "req_sizes", "]", ")", "\n", "if", "not", "isinstance", "(", "x", ",", "Sequence", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"{} should be a sequence of length {}.\"", ".", "format", "(", "name", ",", "msg", ")", ")", "\n", "", "if", "len", "(", "x", ")", "not", "in", "req_sizes", ":", "\n", "        ", "raise", "ValueError", "(", "\"{} should be sequence of length {}.\"", ".", "format", "(", "name", ",", "msg", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._setup_angle": [[1761, 1770], ["isinstance", "transforms._check_sequence_input", "float", "ValueError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.transforms._check_sequence_input"], ["", "", "def", "_setup_angle", "(", "x", ",", "name", ",", "req_sizes", "=", "(", "2", ",", ")", ")", ":", "\n", "    ", "if", "isinstance", "(", "x", ",", "numbers", ".", "Number", ")", ":", "\n", "        ", "if", "x", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"If {} is a single number, it must be positive.\"", ".", "format", "(", "name", ")", ")", "\n", "", "x", "=", "[", "-", "x", ",", "x", "]", "\n", "", "else", ":", "\n", "        ", "_check_sequence_input", "(", "x", ",", "name", ",", "req_sizes", ")", "\n", "\n", "", "return", "[", "float", "(", "d", ")", "for", "d", "in", "x", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image": [[14, 20], ["isinstance", "isinstance"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "_is_pil_image", "(", "img", ":", "Any", ")", "->", "bool", ":", "\n", "    ", "if", "accimage", "is", "not", "None", ":", "\n", "        ", "return", "isinstance", "(", "img", ",", "(", "Image", ".", "Image", ",", "accimage", ".", "Image", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "isinstance", "(", "img", ",", "Image", ".", "Image", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._get_image_size": [[22, 27], ["functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "_get_image_size", "(", "img", ":", "Any", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "if", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "return", "img", ".", "size", "\n", "", "raise", "TypeError", "(", "\"Unexpected type {}\"", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._get_image_num_channels": [[29, 34], ["functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "_get_image_num_channels", "(", "img", ":", "Any", ")", "->", "int", ":", "\n", "    ", "if", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "return", "1", "if", "img", ".", "mode", "==", "'L'", "else", "3", "\n", "", "raise", "TypeError", "(", "\"Unexpected type {}\"", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.hflip": [[36, 42], ["img.transpose", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "hflip", "(", "img", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "return", "img", ".", "transpose", "(", "Image", ".", "FLIP_LEFT_RIGHT", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.vflip": [[44, 50], ["img.transpose", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "vflip", "(", "img", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "return", "img", ".", "transpose", "(", "Image", ".", "FLIP_TOP_BOTTOM", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.adjust_brightness": [[52, 60], ["PIL.ImageEnhance.Brightness", "ImageEnhance.Brightness.enhance", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "adjust_brightness", "(", "img", ",", "brightness_factor", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "enhancer", "=", "ImageEnhance", ".", "Brightness", "(", "img", ")", "\n", "img", "=", "enhancer", ".", "enhance", "(", "brightness_factor", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.adjust_contrast": [[62, 70], ["PIL.ImageEnhance.Contrast", "ImageEnhance.Contrast.enhance", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "adjust_contrast", "(", "img", ",", "contrast_factor", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "enhancer", "=", "ImageEnhance", ".", "Contrast", "(", "img", ")", "\n", "img", "=", "enhancer", ".", "enhance", "(", "contrast_factor", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.adjust_saturation": [[72, 80], ["PIL.ImageEnhance.Color", "ImageEnhance.Color.enhance", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "adjust_saturation", "(", "img", ",", "saturation_factor", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "enhancer", "=", "ImageEnhance", ".", "Color", "(", "img", ")", "\n", "img", "=", "enhancer", ".", "enhance", "(", "saturation_factor", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.adjust_hue": [[82, 104], ["Image.merge().convert.convert().split", "numpy.array", "PIL.Image.fromarray", "PIL.Image.merge().convert", "ValueError", "functional_pil._is_pil_image", "TypeError", "numpy.errstate", "numpy.uint8", "Image.merge().convert.convert", "PIL.Image.merge", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "adjust_hue", "(", "img", ",", "hue_factor", ")", ":", "\n", "    ", "if", "not", "(", "-", "0.5", "<=", "hue_factor", "<=", "0.5", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'hue_factor ({}) is not in [-0.5, 0.5].'", ".", "format", "(", "hue_factor", ")", ")", "\n", "\n", "", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "input_mode", "=", "img", ".", "mode", "\n", "if", "input_mode", "in", "{", "'L'", ",", "'1'", ",", "'I'", ",", "'F'", "}", ":", "\n", "        ", "return", "img", "\n", "\n", "", "h", ",", "s", ",", "v", "=", "img", ".", "convert", "(", "'HSV'", ")", ".", "split", "(", ")", "\n", "\n", "np_h", "=", "np", ".", "array", "(", "h", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "# uint8 addition take cares of rotation across boundaries", "\n", "with", "np", ".", "errstate", "(", "over", "=", "'ignore'", ")", ":", "\n", "        ", "np_h", "+=", "np", ".", "uint8", "(", "hue_factor", "*", "255", ")", "\n", "", "h", "=", "Image", ".", "fromarray", "(", "np_h", ",", "'L'", ")", "\n", "\n", "img", "=", "Image", ".", "merge", "(", "'HSV'", ",", "(", "h", ",", "s", ",", "v", ")", ")", ".", "convert", "(", "input_mode", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.adjust_gamma": [[106, 121], ["img.convert.convert", "img.convert.point", "img.convert.convert", "functional_pil._is_pil_image", "TypeError", "ValueError", "type", "pow", "range"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "adjust_gamma", "(", "img", ",", "gamma", ",", "gain", "=", "1", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "if", "gamma", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "'Gamma should be a non-negative real number'", ")", "\n", "\n", "", "input_mode", "=", "img", ".", "mode", "\n", "img", "=", "img", ".", "convert", "(", "'RGB'", ")", "\n", "gamma_map", "=", "[", "(", "255", "+", "1", "-", "1e-3", ")", "*", "gain", "*", "pow", "(", "ele", "/", "255.", ",", "gamma", ")", "for", "ele", "in", "range", "(", "256", ")", "]", "*", "3", "\n", "img", "=", "img", ".", "point", "(", "gamma_map", ")", "# use PIL's point-function to accelerate this part", "\n", "\n", "img", "=", "img", ".", "convert", "(", "input_mode", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.pad": [[123, 196], ["isinstance", "functional_pil._is_pil_image", "TypeError", "isinstance", "TypeError", "isinstance", "TypeError", "isinstance", "TypeError", "tuple", "isinstance", "ValueError", "isinstance", "ValueError", "functional_pil._parse_fill", "PIL.ImageOps.expand", "isinstance", "cropping.any", "numpy.maximum", "numpy.asarray", "PIL.Image.fromarray", "len", "len", "np.pad.getpalette", "PIL.ImageOps.expand", "ImageOps.expand.putpalette", "isinstance", "isinstance", "numpy.minimum", "np.pad.crop", "np.pad.getpalette", "numpy.asarray", "numpy.pad", "PIL.Image.fromarray", "np.pad.putpalette", "len", "numpy.pad", "len", "numpy.pad", "type", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._parse_fill", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "pad", "(", "img", ",", "padding", ",", "fill", "=", "0", ",", "padding_mode", "=", "\"constant\"", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"img should be PIL Image. Got {}\"", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "padding", ",", "(", "numbers", ".", "Number", ",", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Got inappropriate padding arg\"", ")", "\n", "", "if", "not", "isinstance", "(", "fill", ",", "(", "numbers", ".", "Number", ",", "str", ",", "tuple", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Got inappropriate fill arg\"", ")", "\n", "", "if", "not", "isinstance", "(", "padding_mode", ",", "str", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Got inappropriate padding_mode arg\"", ")", "\n", "\n", "", "if", "isinstance", "(", "padding", ",", "list", ")", ":", "\n", "        ", "padding", "=", "tuple", "(", "padding", ")", "\n", "\n", "", "if", "isinstance", "(", "padding", ",", "tuple", ")", "and", "len", "(", "padding", ")", "not", "in", "[", "1", ",", "2", ",", "4", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Padding must be an int or a 1, 2, or 4 element tuple, not a \"", "+", "\n", "\"{} element tuple\"", ".", "format", "(", "len", "(", "padding", ")", ")", ")", "\n", "\n", "", "if", "isinstance", "(", "padding", ",", "tuple", ")", "and", "len", "(", "padding", ")", "==", "1", ":", "\n", "# Compatibility with `functional_tensor.pad`", "\n", "        ", "padding", "=", "padding", "[", "0", "]", "\n", "\n", "", "if", "padding_mode", "not", "in", "[", "\"constant\"", ",", "\"edge\"", ",", "\"reflect\"", ",", "\"symmetric\"", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Padding mode should be either constant, edge, reflect or symmetric\"", ")", "\n", "\n", "", "if", "padding_mode", "==", "\"constant\"", ":", "\n", "        ", "opts", "=", "_parse_fill", "(", "fill", ",", "img", ",", "name", "=", "\"fill\"", ")", "\n", "if", "img", ".", "mode", "==", "\"P\"", ":", "\n", "            ", "palette", "=", "img", ".", "getpalette", "(", ")", "\n", "image", "=", "ImageOps", ".", "expand", "(", "img", ",", "border", "=", "padding", ",", "**", "opts", ")", "\n", "image", ".", "putpalette", "(", "palette", ")", "\n", "return", "image", "\n", "\n", "", "return", "ImageOps", ".", "expand", "(", "img", ",", "border", "=", "padding", ",", "**", "opts", ")", "\n", "", "else", ":", "\n", "        ", "if", "isinstance", "(", "padding", ",", "int", ")", ":", "\n", "            ", "pad_left", "=", "pad_right", "=", "pad_top", "=", "pad_bottom", "=", "padding", "\n", "", "if", "isinstance", "(", "padding", ",", "tuple", ")", "and", "len", "(", "padding", ")", "==", "2", ":", "\n", "            ", "pad_left", "=", "pad_right", "=", "padding", "[", "0", "]", "\n", "pad_top", "=", "pad_bottom", "=", "padding", "[", "1", "]", "\n", "", "if", "isinstance", "(", "padding", ",", "tuple", ")", "and", "len", "(", "padding", ")", "==", "4", ":", "\n", "            ", "pad_left", "=", "padding", "[", "0", "]", "\n", "pad_top", "=", "padding", "[", "1", "]", "\n", "pad_right", "=", "padding", "[", "2", "]", "\n", "pad_bottom", "=", "padding", "[", "3", "]", "\n", "\n", "", "p", "=", "[", "pad_left", ",", "pad_top", ",", "pad_right", ",", "pad_bottom", "]", "\n", "cropping", "=", "-", "np", ".", "minimum", "(", "p", ",", "0", ")", "\n", "\n", "if", "cropping", ".", "any", "(", ")", ":", "\n", "            ", "crop_left", ",", "crop_top", ",", "crop_right", ",", "crop_bottom", "=", "cropping", "\n", "img", "=", "img", ".", "crop", "(", "(", "crop_left", ",", "crop_top", ",", "img", ".", "width", "-", "crop_right", ",", "img", ".", "height", "-", "crop_bottom", ")", ")", "\n", "\n", "", "pad_left", ",", "pad_top", ",", "pad_right", ",", "pad_bottom", "=", "np", ".", "maximum", "(", "p", ",", "0", ")", "\n", "\n", "if", "img", ".", "mode", "==", "'P'", ":", "\n", "            ", "palette", "=", "img", ".", "getpalette", "(", ")", "\n", "img", "=", "np", ".", "asarray", "(", "img", ")", "\n", "img", "=", "np", ".", "pad", "(", "img", ",", "(", "(", "pad_top", ",", "pad_bottom", ")", ",", "(", "pad_left", ",", "pad_right", ")", ")", ",", "padding_mode", ")", "\n", "img", "=", "Image", ".", "fromarray", "(", "img", ")", "\n", "img", ".", "putpalette", "(", "palette", ")", "\n", "return", "img", "\n", "\n", "", "img", "=", "np", ".", "asarray", "(", "img", ")", "\n", "# RGB image", "\n", "if", "len", "(", "img", ".", "shape", ")", "==", "3", ":", "\n", "            ", "img", "=", "np", ".", "pad", "(", "img", ",", "(", "(", "pad_top", ",", "pad_bottom", ")", ",", "(", "pad_left", ",", "pad_right", ")", ",", "(", "0", ",", "0", ")", ")", ",", "padding_mode", ")", "\n", "# Grayscale image", "\n", "", "if", "len", "(", "img", ".", "shape", ")", "==", "2", ":", "\n", "            ", "img", "=", "np", ".", "pad", "(", "img", ",", "(", "(", "pad_top", ",", "pad_bottom", ")", ",", "(", "pad_left", ",", "pad_right", ")", ")", ",", "padding_mode", ")", "\n", "\n", "", "return", "Image", ".", "fromarray", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.crop": [[198, 204], ["img.crop", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "crop", "(", "img", ":", "Image", ".", "Image", ",", "top", ":", "int", ",", "left", ":", "int", ",", "height", ":", "int", ",", "width", ":", "int", ")", "->", "Image", ".", "Image", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "return", "img", ".", "crop", "(", "(", "left", ",", "top", ",", "left", "+", "width", ",", "top", "+", "height", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.resize": [[206, 242], ["isinstance", "functional_pil._is_pil_image", "TypeError", "TypeError", "isinstance", "img.resize", "img.resize", "isinstance", "len", "int", "ValueError", "type", "isinstance", "ValueError", "len", "int"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "resize", "(", "img", ",", "size", ",", "interpolation", "=", "Image", ".", "BILINEAR", ",", "max_size", "=", "None", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "", "if", "not", "(", "isinstance", "(", "size", ",", "int", ")", "or", "(", "isinstance", "(", "size", ",", "Sequence", ")", "and", "len", "(", "size", ")", "in", "(", "1", ",", "2", ")", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'Got inappropriate size arg: {}'", ".", "format", "(", "size", ")", ")", "\n", "\n", "", "if", "isinstance", "(", "size", ",", "Sequence", ")", "and", "len", "(", "size", ")", "==", "1", ":", "\n", "        ", "size", "=", "size", "[", "0", "]", "\n", "", "if", "isinstance", "(", "size", ",", "int", ")", ":", "\n", "        ", "w", ",", "h", "=", "img", ".", "size", "\n", "\n", "short", ",", "long", "=", "(", "w", ",", "h", ")", "if", "w", "<=", "h", "else", "(", "h", ",", "w", ")", "\n", "if", "short", "==", "size", ":", "\n", "            ", "return", "img", "\n", "\n", "", "new_short", ",", "new_long", "=", "size", ",", "int", "(", "size", "*", "long", "/", "short", ")", "\n", "\n", "if", "max_size", "is", "not", "None", ":", "\n", "            ", "if", "max_size", "<=", "size", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"max_size = {max_size} must be strictly greater than the requested \"", "\n", "f\"size for the smaller edge size = {size}\"", "\n", ")", "\n", "", "if", "new_long", ">", "max_size", ":", "\n", "                ", "new_short", ",", "new_long", "=", "int", "(", "max_size", "*", "new_short", "/", "new_long", ")", ",", "max_size", "\n", "\n", "", "", "new_w", ",", "new_h", "=", "(", "new_short", ",", "new_long", ")", "if", "w", "<=", "h", "else", "(", "new_long", ",", "new_short", ")", "\n", "return", "img", ".", "resize", "(", "(", "new_w", ",", "new_h", ")", ",", "interpolation", ")", "\n", "", "else", ":", "\n", "        ", "if", "max_size", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"max_size should only be passed if size specifies the length of the smaller edge, \"", "\n", "\"i.e. size should be an int or a sequence of length 1 in torchscript mode.\"", "\n", ")", "\n", "", "return", "img", ".", "resize", "(", "size", "[", ":", ":", "-", "1", "]", ",", "interpolation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._parse_fill": [[244, 261], ["len", "isinstance", "img.getbands", "isinstance", "tuple", "tuple", "len", "ValueError", "msg.format", "len"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "_parse_fill", "(", "fill", ",", "img", ",", "name", "=", "\"fillcolor\"", ")", ":", "\n", "# Process fill color for affine transforms", "\n", "    ", "num_bands", "=", "len", "(", "img", ".", "getbands", "(", ")", ")", "\n", "if", "fill", "is", "None", ":", "\n", "        ", "fill", "=", "0", "\n", "", "if", "isinstance", "(", "fill", ",", "(", "int", ",", "float", ")", ")", "and", "num_bands", ">", "1", ":", "\n", "        ", "fill", "=", "tuple", "(", "[", "fill", "]", "*", "num_bands", ")", "\n", "", "if", "isinstance", "(", "fill", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "if", "len", "(", "fill", ")", "!=", "num_bands", ":", "\n", "            ", "msg", "=", "(", "\"The number of elements in 'fill' does not match the number of \"", "\n", "\"bands of the image ({} != {})\"", ")", "\n", "raise", "ValueError", "(", "msg", ".", "format", "(", "len", "(", "fill", ")", ",", "num_bands", ")", ")", "\n", "\n", "", "fill", "=", "tuple", "(", "fill", ")", "\n", "\n", "", "return", "{", "name", ":", "fill", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.affine": [[263, 271], ["functional_pil._parse_fill", "img.transform", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._parse_fill", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugInput.transform", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "affine", "(", "img", ",", "matrix", ",", "interpolation", "=", "0", ",", "fill", "=", "None", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "output_size", "=", "img", ".", "size", "\n", "opts", "=", "_parse_fill", "(", "fill", ",", "img", ")", "\n", "return", "img", ".", "transform", "(", "output_size", ",", "Image", ".", "AFFINE", ",", "matrix", ",", "interpolation", ",", "**", "opts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.rotate": [[273, 280], ["functional_pil._parse_fill", "img.rotate", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._parse_fill", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rotate", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "rotate", "(", "img", ",", "angle", ",", "interpolation", "=", "0", ",", "expand", "=", "False", ",", "center", "=", "None", ",", "fill", "=", "None", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"img should be PIL Image. Got {}\"", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "opts", "=", "_parse_fill", "(", "fill", ",", "img", ")", "\n", "return", "img", ".", "rotate", "(", "angle", ",", "interpolation", ",", "expand", ",", "center", ",", "**", "opts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.perspective": [[282, 290], ["functional_pil._parse_fill", "img.transform", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._parse_fill", "home.repos.pwc.inspect_result.microsoft_regionclip.transforms.augmentation.AugInput.transform", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "perspective", "(", "img", ",", "perspective_coeffs", ",", "interpolation", "=", "Image", ".", "BICUBIC", ",", "fill", "=", "None", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "opts", "=", "_parse_fill", "(", "fill", ",", "img", ")", "\n", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "PERSPECTIVE", ",", "perspective_coeffs", ",", "interpolation", ",", "**", "opts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.to_grayscale": [[292, 308], ["functional_pil._is_pil_image", "TypeError", "Image.fromarray.convert", "Image.fromarray.convert", "numpy.array", "numpy.dstack", "PIL.Image.fromarray", "ValueError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "to_grayscale", "(", "img", ",", "num_output_channels", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "if", "num_output_channels", "==", "1", ":", "\n", "        ", "img", "=", "img", ".", "convert", "(", "'L'", ")", "\n", "", "elif", "num_output_channels", "==", "3", ":", "\n", "        ", "img", "=", "img", ".", "convert", "(", "'L'", ")", "\n", "np_img", "=", "np", ".", "array", "(", "img", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "np_img", "=", "np", ".", "dstack", "(", "[", "np_img", ",", "np_img", ",", "np_img", "]", ")", "\n", "img", "=", "Image", ".", "fromarray", "(", "np_img", ",", "'RGB'", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'num_output_channels should be either 1 or 3'", ")", "\n", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.invert": [[310, 315], ["PIL.ImageOps.invert", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.invert", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "invert", "(", "img", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "", "return", "ImageOps", ".", "invert", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.posterize": [[317, 322], ["PIL.ImageOps.posterize", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.posterize", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "posterize", "(", "img", ",", "bits", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "", "return", "ImageOps", ".", "posterize", "(", "img", ",", "bits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.solarize": [[324, 329], ["PIL.ImageOps.solarize", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.solarize", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "solarize", "(", "img", ",", "threshold", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "", "return", "ImageOps", ".", "solarize", "(", "img", ",", "threshold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.adjust_sharpness": [[331, 339], ["PIL.ImageEnhance.Sharpness", "ImageEnhance.Sharpness.enhance", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "adjust_sharpness", "(", "img", ",", "sharpness_factor", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "enhancer", "=", "ImageEnhance", ".", "Sharpness", "(", "img", ")", "\n", "img", "=", "enhancer", ".", "enhance", "(", "sharpness_factor", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.autocontrast": [[341, 346], ["PIL.ImageOps.autocontrast", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.autocontrast", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "autocontrast", "(", "img", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "", "return", "ImageOps", ".", "autocontrast", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil.equalize": [[348, 353], ["PIL.ImageOps.equalize", "functional_pil._is_pil_image", "TypeError", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.equalize", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "equalize", "(", "img", ")", ":", "\n", "    ", "if", "not", "_is_pil_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "", "return", "ImageOps", ".", "equalize", "(", "img", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int": [[37, 47], ["None"], "function", ["None"], ["", "def", "_interpolation_modes_from_int", "(", "i", ":", "int", ")", "->", "InterpolationMode", ":", "\n", "    ", "inverse_modes_mapping", "=", "{", "\n", "0", ":", "InterpolationMode", ".", "NEAREST", ",", "\n", "2", ":", "InterpolationMode", ".", "BILINEAR", ",", "\n", "3", ":", "InterpolationMode", ".", "BICUBIC", ",", "\n", "4", ":", "InterpolationMode", ".", "BOX", ",", "\n", "5", ":", "InterpolationMode", ".", "HAMMING", ",", "\n", "1", ":", "InterpolationMode", ".", "LANCZOS", ",", "\n", "}", "\n", "return", "inverse_modes_mapping", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._get_image_size": [[61, 68], ["isinstance", "functional_pil._get_image_size", "functional_tensor._get_image_size"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size"], ["def", "_get_image_size", "(", "img", ":", "Tensor", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\"Returns image size as [w, h]\n    \"\"\"", "\n", "if", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_t", ".", "_get_image_size", "(", "img", ")", "\n", "\n", "", "return", "F_pil", ".", "_get_image_size", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._get_image_num_channels": [[70, 77], ["isinstance", "functional_pil._get_image_num_channels", "functional_tensor._get_image_num_channels"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels"], ["", "def", "_get_image_num_channels", "(", "img", ":", "Tensor", ")", "->", "int", ":", "\n", "    ", "\"\"\"Returns number of image channels\n    \"\"\"", "\n", "if", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_t", ".", "_get_image_num_channels", "(", "img", ")", "\n", "\n", "", "return", "F_pil", ".", "_get_image_num_channels", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._is_numpy": [[79, 82], ["isinstance"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "_is_numpy", "(", "img", ":", "Any", ")", "->", "bool", ":", "\n", "    ", "return", "isinstance", "(", "img", ",", "np", ".", "ndarray", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._is_numpy_image": [[84, 87], ["None"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "_is_numpy_image", "(", "img", ":", "Any", ")", "->", "bool", ":", "\n", "    ", "return", "img", ".", "ndim", "in", "{", "2", ",", "3", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.to_tensor": [[89, 141], ["torch.get_default_dtype", "isinstance", "torch.from_numpy", "torch.from_numpy().contiguous.view", "torch.from_numpy().contiguous.permute().contiguous", "isinstance", "TypeError", "functional._is_numpy", "ValueError", "torch.from_numpy().contiguous", "isinstance", "isinstance", "numpy.zeros", "pic.copyto", "torch.from_numpy().to", "numpy.array", "len", "torch.from_numpy().contiguous.to().div", "functional_pil._is_pil_image", "functional._is_numpy", "functional._is_numpy_image", "torch.from_numpy().contiguous.to().div", "mode_to_nptype.get", "pic.getbands", "torch.from_numpy().contiguous.permute", "type", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().contiguous.to", "pic.transpose", "torch.from_numpy().contiguous.to"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._is_numpy", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._is_numpy", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._is_numpy_image", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "to_tensor", "(", "pic", ")", ":", "\n", "    ", "\"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n    This function does not support torchscript.\n\n    See :class:`~torchvision.transforms.ToTensor` for more details.\n\n    Args:\n        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n\n    Returns:\n        Tensor: Converted image.\n    \"\"\"", "\n", "if", "not", "(", "F_pil", ".", "_is_pil_image", "(", "pic", ")", "or", "_is_numpy", "(", "pic", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'pic should be PIL Image or ndarray. Got {}'", ".", "format", "(", "type", "(", "pic", ")", ")", ")", "\n", "\n", "", "if", "_is_numpy", "(", "pic", ")", "and", "not", "_is_numpy_image", "(", "pic", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'pic should be 2/3 dimensional. Got {} dimensions.'", ".", "format", "(", "pic", ".", "ndim", ")", ")", "\n", "\n", "", "default_float_dtype", "=", "torch", ".", "get_default_dtype", "(", ")", "\n", "\n", "if", "isinstance", "(", "pic", ",", "np", ".", "ndarray", ")", ":", "\n", "# handle numpy array", "\n", "        ", "if", "pic", ".", "ndim", "==", "2", ":", "\n", "            ", "pic", "=", "pic", "[", ":", ",", ":", ",", "None", "]", "\n", "\n", "", "img", "=", "torch", ".", "from_numpy", "(", "pic", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", ")", ".", "contiguous", "(", ")", "\n", "# backward compatibility", "\n", "if", "isinstance", "(", "img", ",", "torch", ".", "ByteTensor", ")", ":", "\n", "            ", "return", "img", ".", "to", "(", "dtype", "=", "default_float_dtype", ")", ".", "div", "(", "255", ")", "\n", "", "else", ":", "\n", "            ", "return", "img", "\n", "\n", "", "", "if", "accimage", "is", "not", "None", "and", "isinstance", "(", "pic", ",", "accimage", ".", "Image", ")", ":", "\n", "        ", "nppic", "=", "np", ".", "zeros", "(", "[", "pic", ".", "channels", ",", "pic", ".", "height", ",", "pic", ".", "width", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "pic", ".", "copyto", "(", "nppic", ")", "\n", "return", "torch", ".", "from_numpy", "(", "nppic", ")", ".", "to", "(", "dtype", "=", "default_float_dtype", ")", "\n", "\n", "# handle PIL Image", "\n", "", "mode_to_nptype", "=", "{", "'I'", ":", "np", ".", "int32", ",", "'I;16'", ":", "np", ".", "int16", ",", "'F'", ":", "np", ".", "float32", "}", "\n", "img", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "array", "(", "pic", ",", "mode_to_nptype", ".", "get", "(", "pic", ".", "mode", ",", "np", ".", "uint8", ")", ",", "copy", "=", "True", ")", "\n", ")", "\n", "\n", "if", "pic", ".", "mode", "==", "'1'", ":", "\n", "        ", "img", "=", "255", "*", "img", "\n", "", "img", "=", "img", ".", "view", "(", "pic", ".", "size", "[", "1", "]", ",", "pic", ".", "size", "[", "0", "]", ",", "len", "(", "pic", ".", "getbands", "(", ")", ")", ")", "\n", "# put it from HWC to CHW format", "\n", "img", "=", "img", ".", "permute", "(", "(", "2", ",", "0", ",", "1", ")", ")", ".", "contiguous", "(", ")", "\n", "if", "isinstance", "(", "img", ",", "torch", ".", "ByteTensor", ")", ":", "\n", "        ", "return", "img", ".", "to", "(", "dtype", "=", "default_float_dtype", ")", ".", "div", "(", "255", ")", "\n", "", "else", ":", "\n", "        ", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.pil_to_tensor": [[143, 170], ["torch.as_tensor", "img.permute.view", "img.permute.permute", "functional_pil._is_pil_image", "TypeError", "isinstance", "numpy.zeros", "pic.copyto", "torch.as_tensor", "numpy.asarray", "len", "pic.getbands", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "", "def", "pil_to_tensor", "(", "pic", ")", ":", "\n", "    ", "\"\"\"Convert a ``PIL Image`` to a tensor of the same type.\n    This function does not support torchscript.\n\n    See :class:`~torchvision.transforms.PILToTensor` for more details.\n\n    Args:\n        pic (PIL Image): Image to be converted to tensor.\n\n    Returns:\n        Tensor: Converted image.\n    \"\"\"", "\n", "if", "not", "F_pil", ".", "_is_pil_image", "(", "pic", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'pic should be PIL Image. Got {}'", ".", "format", "(", "type", "(", "pic", ")", ")", ")", "\n", "\n", "", "if", "accimage", "is", "not", "None", "and", "isinstance", "(", "pic", ",", "accimage", ".", "Image", ")", ":", "\n", "# accimage format is always uint8 internally, so always return uint8 here", "\n", "        ", "nppic", "=", "np", ".", "zeros", "(", "[", "pic", ".", "channels", ",", "pic", ".", "height", ",", "pic", ".", "width", "]", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "pic", ".", "copyto", "(", "nppic", ")", "\n", "return", "torch", ".", "as_tensor", "(", "nppic", ")", "\n", "\n", "# handle PIL Image", "\n", "", "img", "=", "torch", ".", "as_tensor", "(", "np", ".", "asarray", "(", "pic", ")", ")", "\n", "img", "=", "img", ".", "view", "(", "pic", ".", "size", "[", "1", "]", ",", "pic", ".", "size", "[", "0", "]", ",", "len", "(", "pic", ".", "getbands", "(", ")", ")", ")", "\n", "# put it from HWC to CHW format", "\n", "img", "=", "img", ".", "permute", "(", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.convert_image_dtype": [[172, 198], ["functional_tensor.convert_image_dtype", "isinstance", "TypeError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.convert_image_dtype"], ["", "def", "convert_image_dtype", "(", "image", ":", "torch", ".", "Tensor", ",", "dtype", ":", "torch", ".", "dtype", "=", "torch", ".", "float", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"Convert a tensor image to the given ``dtype`` and scale the values accordingly\n    This function does not support PIL Image.\n\n    Args:\n        image (torch.Tensor): Image to be converted\n        dtype (torch.dtype): Desired data type of the output\n\n    Returns:\n        Tensor: Converted image\n\n    .. note::\n\n        When converting from a smaller to a larger integer ``dtype`` the maximum values are **not** mapped exactly.\n        If converted back and forth, this mismatch has no effect.\n\n    Raises:\n        RuntimeError: When trying to cast :class:`torch.float32` to :class:`torch.int32` or :class:`torch.int64` as\n            well as for trying to cast :class:`torch.float64` to :class:`torch.int64`. These conversions might lead to\n            overflow errors since the floating point ``dtype`` cannot store consecutive integers over the whole range\n            of the integer ``dtype``.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "image", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'Input img should be Tensor Image'", ")", "\n", "\n", "", "return", "F_t", ".", "convert_image_dtype", "(", "image", ",", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.to_pil_image": [[200, 293], ["isinstance", "PIL.Image.fromarray", "TypeError", "isinstance", "numpy.transpose", "isinstance", "TypeError", "TypeError", "isinstance", "isinstance", "isinstance", "np.expand_dims.is_floating_point", "np.expand_dims.mul().byte", "np.expand_dims.cpu().numpy", "ValueError", "type", "np.expand_dims.ndimension", "ValueError", "ValueError", "ValueError", "np.expand_dims.ndimension", "np.expand_dims.unsqueeze", "ValueError", "ValueError", "np.expand_dims.mul", "np.expand_dims.cpu", "type", "ValueError", "ValueError", "np.expand_dims.ndimension", "numpy.expand_dims"], "function", ["None"], ["", "def", "to_pil_image", "(", "pic", ",", "mode", "=", "None", ")", ":", "\n", "    ", "\"\"\"Convert a tensor or an ndarray to PIL Image. This function does not support torchscript.\n\n    See :class:`~torchvision.transforms.ToPILImage` for more details.\n\n    Args:\n        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\n        mode (`PIL.Image mode`_): color space and pixel depth of input data (optional).\n\n    .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes\n\n    Returns:\n        PIL Image: Image converted to PIL Image.\n    \"\"\"", "\n", "if", "not", "(", "isinstance", "(", "pic", ",", "torch", ".", "Tensor", ")", "or", "isinstance", "(", "pic", ",", "np", ".", "ndarray", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'pic should be Tensor or ndarray. Got {}.'", ".", "format", "(", "type", "(", "pic", ")", ")", ")", "\n", "\n", "", "elif", "isinstance", "(", "pic", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "pic", ".", "ndimension", "(", ")", "not", "in", "{", "2", ",", "3", "}", ":", "\n", "            ", "raise", "ValueError", "(", "'pic should be 2/3 dimensional. Got {} dimensions.'", ".", "format", "(", "pic", ".", "ndimension", "(", ")", ")", ")", "\n", "\n", "", "elif", "pic", ".", "ndimension", "(", ")", "==", "2", ":", "\n", "# if 2D image, add channel dimension (CHW)", "\n", "            ", "pic", "=", "pic", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# check number of channels", "\n", "", "if", "pic", ".", "shape", "[", "-", "3", "]", ">", "4", ":", "\n", "            ", "raise", "ValueError", "(", "'pic should not have > 4 channels. Got {} channels.'", ".", "format", "(", "pic", ".", "shape", "[", "-", "3", "]", ")", ")", "\n", "\n", "", "", "elif", "isinstance", "(", "pic", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "if", "pic", ".", "ndim", "not", "in", "{", "2", ",", "3", "}", ":", "\n", "            ", "raise", "ValueError", "(", "'pic should be 2/3 dimensional. Got {} dimensions.'", ".", "format", "(", "pic", ".", "ndim", ")", ")", "\n", "\n", "", "elif", "pic", ".", "ndim", "==", "2", ":", "\n", "# if 2D image, add channel dimension (HWC)", "\n", "            ", "pic", "=", "np", ".", "expand_dims", "(", "pic", ",", "2", ")", "\n", "\n", "# check number of channels", "\n", "", "if", "pic", ".", "shape", "[", "-", "1", "]", ">", "4", ":", "\n", "            ", "raise", "ValueError", "(", "'pic should not have > 4 channels. Got {} channels.'", ".", "format", "(", "pic", ".", "shape", "[", "-", "1", "]", ")", ")", "\n", "\n", "", "", "npimg", "=", "pic", "\n", "if", "isinstance", "(", "pic", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "pic", ".", "is_floating_point", "(", ")", "and", "mode", "!=", "'F'", ":", "\n", "            ", "pic", "=", "pic", ".", "mul", "(", "255", ")", ".", "byte", "(", ")", "\n", "", "npimg", "=", "np", ".", "transpose", "(", "pic", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "npimg", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'Input pic must be a torch.Tensor or NumPy ndarray, '", "+", "\n", "'not {}'", ".", "format", "(", "type", "(", "npimg", ")", ")", ")", "\n", "\n", "", "if", "npimg", ".", "shape", "[", "2", "]", "==", "1", ":", "\n", "        ", "expected_mode", "=", "None", "\n", "npimg", "=", "npimg", "[", ":", ",", ":", ",", "0", "]", "\n", "if", "npimg", ".", "dtype", "==", "np", ".", "uint8", ":", "\n", "            ", "expected_mode", "=", "'L'", "\n", "", "elif", "npimg", ".", "dtype", "==", "np", ".", "int16", ":", "\n", "            ", "expected_mode", "=", "'I;16'", "\n", "", "elif", "npimg", ".", "dtype", "==", "np", ".", "int32", ":", "\n", "            ", "expected_mode", "=", "'I'", "\n", "", "elif", "npimg", ".", "dtype", "==", "np", ".", "float32", ":", "\n", "            ", "expected_mode", "=", "'F'", "\n", "", "if", "mode", "is", "not", "None", "and", "mode", "!=", "expected_mode", ":", "\n", "            ", "raise", "ValueError", "(", "\"Incorrect mode ({}) supplied for input type {}. Should be {}\"", "\n", ".", "format", "(", "mode", ",", "np", ".", "dtype", ",", "expected_mode", ")", ")", "\n", "", "mode", "=", "expected_mode", "\n", "\n", "", "elif", "npimg", ".", "shape", "[", "2", "]", "==", "2", ":", "\n", "        ", "permitted_2_channel_modes", "=", "[", "'LA'", "]", "\n", "if", "mode", "is", "not", "None", "and", "mode", "not", "in", "permitted_2_channel_modes", ":", "\n", "            ", "raise", "ValueError", "(", "\"Only modes {} are supported for 2D inputs\"", ".", "format", "(", "permitted_2_channel_modes", ")", ")", "\n", "\n", "", "if", "mode", "is", "None", "and", "npimg", ".", "dtype", "==", "np", ".", "uint8", ":", "\n", "            ", "mode", "=", "'LA'", "\n", "\n", "", "", "elif", "npimg", ".", "shape", "[", "2", "]", "==", "4", ":", "\n", "        ", "permitted_4_channel_modes", "=", "[", "'RGBA'", ",", "'CMYK'", ",", "'RGBX'", "]", "\n", "if", "mode", "is", "not", "None", "and", "mode", "not", "in", "permitted_4_channel_modes", ":", "\n", "            ", "raise", "ValueError", "(", "\"Only modes {} are supported for 4D inputs\"", ".", "format", "(", "permitted_4_channel_modes", ")", ")", "\n", "\n", "", "if", "mode", "is", "None", "and", "npimg", ".", "dtype", "==", "np", ".", "uint8", ":", "\n", "            ", "mode", "=", "'RGBA'", "\n", "", "", "else", ":", "\n", "        ", "permitted_3_channel_modes", "=", "[", "'RGB'", ",", "'YCbCr'", ",", "'HSV'", "]", "\n", "if", "mode", "is", "not", "None", "and", "mode", "not", "in", "permitted_3_channel_modes", ":", "\n", "            ", "raise", "ValueError", "(", "\"Only modes {} are supported for 3D inputs\"", ".", "format", "(", "permitted_3_channel_modes", ")", ")", "\n", "", "if", "mode", "is", "None", "and", "npimg", ".", "dtype", "==", "np", ".", "uint8", ":", "\n", "            ", "mode", "=", "'RGB'", "\n", "\n", "", "", "if", "mode", "is", "None", ":", "\n", "        ", "raise", "TypeError", "(", "'Input type {} is not supported'", ".", "format", "(", "npimg", ".", "dtype", ")", ")", "\n", "\n", "", "return", "Image", ".", "fromarray", "(", "npimg", ",", "mode", "=", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.normalize": [[295, 337], ["torch.as_tensor", "torch.as_tensor", "tensor.clone.sub_().div_", "isinstance", "TypeError", "tensor.clone.is_floating_point", "TypeError", "ValueError", "tensor.clone.clone", "ValueError", "mean.view.view", "std.view.view", "tensor.clone.sub_", "type", "tensor.clone.size"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["", "def", "normalize", "(", "tensor", ":", "Tensor", ",", "mean", ":", "List", "[", "float", "]", ",", "std", ":", "List", "[", "float", "]", ",", "inplace", ":", "bool", "=", "False", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Normalize a float tensor image with mean and standard deviation.\n    This transform does not support PIL Image.\n\n    .. note::\n        This transform acts out of place by default, i.e., it does not mutates the input tensor.\n\n    See :class:`~torchvision.transforms.Normalize` for more details.\n\n    Args:\n        tensor (Tensor): Float tensor image of size (C, H, W) or (B, C, H, W) to be normalized.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n        inplace(bool,optional): Bool to make this operation inplace.\n\n    Returns:\n        Tensor: Normalized Tensor image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "tensor", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'Input tensor should be a torch tensor. Got {}.'", ".", "format", "(", "type", "(", "tensor", ")", ")", ")", "\n", "\n", "", "if", "not", "tensor", ".", "is_floating_point", "(", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'Input tensor should be a float tensor. Got {}.'", ".", "format", "(", "tensor", ".", "dtype", ")", ")", "\n", "\n", "", "if", "tensor", ".", "ndim", "<", "3", ":", "\n", "        ", "raise", "ValueError", "(", "'Expected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = '", "\n", "'{}.'", ".", "format", "(", "tensor", ".", "size", "(", ")", ")", ")", "\n", "\n", "", "if", "not", "inplace", ":", "\n", "        ", "tensor", "=", "tensor", ".", "clone", "(", ")", "\n", "\n", "", "dtype", "=", "tensor", ".", "dtype", "\n", "mean", "=", "torch", ".", "as_tensor", "(", "mean", ",", "dtype", "=", "dtype", ",", "device", "=", "tensor", ".", "device", ")", "\n", "std", "=", "torch", ".", "as_tensor", "(", "std", ",", "dtype", "=", "dtype", ",", "device", "=", "tensor", ".", "device", ")", "\n", "if", "(", "std", "==", "0", ")", ".", "any", "(", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'std evaluated to zero after conversion to {}, leading to division by zero.'", ".", "format", "(", "dtype", ")", ")", "\n", "", "if", "mean", ".", "ndim", "==", "1", ":", "\n", "        ", "mean", "=", "mean", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", "\n", "", "if", "std", ".", "ndim", "==", "1", ":", "\n", "        ", "std", "=", "std", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", "\n", "", "tensor", ".", "sub_", "(", "mean", ")", ".", "div_", "(", "std", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.resize": [[339, 406], ["isinstance", "functional_tensor.resize", "warnings.warn", "functional._interpolation_modes_from_int", "isinstance", "TypeError", "isinstance", "functional_pil.resize", "warnings.warn"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize"], ["", "def", "resize", "(", "img", ":", "Tensor", ",", "size", ":", "List", "[", "int", "]", ",", "interpolation", ":", "InterpolationMode", "=", "InterpolationMode", ".", "BILINEAR", ",", "\n", "max_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "antialias", ":", "Optional", "[", "bool", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "    ", "r\"\"\"Resize the input image to the given size.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n\n    .. warning::\n        The output image might be different depending on its type: when downsampling, the interpolation of PIL images\n        and tensors is slightly different, because PIL applies antialiasing. This may lead to significant differences\n        in the performance of a network. Therefore, it is preferable to train and serve a model with the same input\n        types. See also below the ``antialias`` parameter, which can help making the output of PIL images and tensors\n        closer.\n\n    Args:\n        img (PIL Image or Tensor): Image to be resized.\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), the output size will be matched to this. If size is an int,\n            the smaller edge of the image will be matched to this number maintaining\n            the aspect ratio. i.e, if height > width, then image will be rescaled to\n            :math:`\\left(\\text{size} \\times \\frac{\\text{height}}{\\text{width}}, \\text{size}\\right)`.\n\n            .. note::\n                In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`.\n            Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``,\n            ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported.\n            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n        max_size (int, optional): The maximum allowed for the longer edge of\n            the resized image: if the longer edge of the image is greater\n            than ``max_size`` after being resized according to ``size``, then\n            the image is resized again so that the longer edge is equal to\n            ``max_size``. As a result, ``size`` might be overruled, i.e the\n            smaller edge may be shorter than ``size``. This is only supported\n            if ``size`` is an int (or a sequence of length 1 in torchscript\n            mode).\n        antialias (bool, optional): antialias flag. If ``img`` is PIL Image, the flag is ignored and anti-alias\n            is always used. If ``img`` is Tensor, the flag is False by default and can be set to True for\n            ``InterpolationMode.BILINEAR`` only mode. This can help making the output for PIL images and tensors\n            closer.\n\n            .. warning::\n                There is no autodiff support for ``antialias=True`` option with input ``img`` as Tensor.\n\n    Returns:\n        PIL Image or Tensor: Resized image.\n    \"\"\"", "\n", "# Backward compatibility with integer value", "\n", "if", "isinstance", "(", "interpolation", ",", "int", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\n", "\"Argument interpolation should be of type InterpolationMode instead of int. \"", "\n", "\"Please, use InterpolationMode enum.\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "interpolation", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "interpolation", ",", "InterpolationMode", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Argument interpolation should be a InterpolationMode\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "antialias", "is", "not", "None", "and", "not", "antialias", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"", "\n", ")", "\n", "", "pil_interpolation", "=", "pil_modes_mapping", "[", "interpolation", "]", "\n", "return", "F_pil", ".", "resize", "(", "img", ",", "size", "=", "size", ",", "interpolation", "=", "pil_interpolation", ",", "max_size", "=", "max_size", ")", "\n", "\n", "", "return", "F_t", ".", "resize", "(", "img", ",", "size", "=", "size", ",", "interpolation", "=", "interpolation", ".", "value", ",", "max_size", "=", "max_size", ",", "antialias", "=", "antialias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.scale": [[408, 412], ["warnings.warn", "functional.resize"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize"], ["", "def", "scale", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "warnings", ".", "warn", "(", "\"The use of the transforms.Scale transform is deprecated, \"", "+", "\n", "\"please use transforms.Resize instead.\"", ")", "\n", "return", "resize", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.pad": [[414, 459], ["functional_tensor.pad", "isinstance", "functional_pil.pad"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad"], ["", "def", "pad", "(", "img", ":", "Tensor", ",", "padding", ":", "List", "[", "int", "]", ",", "fill", ":", "int", "=", "0", ",", "padding_mode", ":", "str", "=", "\"constant\"", ")", "->", "Tensor", ":", "\n", "    ", "r\"\"\"Pad the given image on all sides with the given \"pad\" value.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means at most 2 leading dimensions for mode reflect and symmetric,\n    at most 3 leading dimensions for mode edge,\n    and an arbitrary number of leading dimensions for mode constant\n\n    Args:\n        img (PIL Image or Tensor): Image to be padded.\n        padding (int or sequence): Padding on each border. If a single int is provided this\n            is used to pad all borders. If sequence of length 2 is provided this is the padding\n            on left/right and top/bottom respectively. If a sequence of length 4 is provided\n            this is the padding for the left, top, right and bottom borders respectively.\n\n            .. note::\n                In torchscript mode padding as single int is not supported, use a sequence of\n                length 1: ``[padding, ]``.\n        fill (number or str or tuple): Pixel fill value for constant fill. Default is 0.\n            If a tuple of length 3, it is used to fill R, G, B channels respectively.\n            This value is only used when the padding_mode is constant.\n            Only number is supported for torch Tensor.\n            Only int or str or tuple value is supported for PIL Image.\n        padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric.\n            Default is constant.\n\n            - constant: pads with a constant value, this value is specified with fill\n\n            - edge: pads with the last value at the edge of the image.\n              If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2\n\n            - reflect: pads with reflection of image without repeating the last value on the edge.\n              For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n              will result in [3, 2, 1, 2, 3, 4, 3, 2]\n\n            - symmetric: pads with reflection of image repeating the last value on the edge.\n              For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n              will result in [2, 1, 1, 2, 3, 4, 4, 3]\n\n    Returns:\n        PIL Image or Tensor: Padded image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "pad", "(", "img", ",", "padding", "=", "padding", ",", "fill", "=", "fill", ",", "padding_mode", "=", "padding_mode", ")", "\n", "\n", "", "return", "F_t", ".", "pad", "(", "img", ",", "padding", "=", "padding", ",", "fill", "=", "fill", ",", "padding_mode", "=", "padding_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.crop": [[461, 482], ["functional_tensor.crop", "isinstance", "functional_pil.crop"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop"], ["", "def", "crop", "(", "img", ":", "Tensor", ",", "top", ":", "int", ",", "left", ":", "int", ",", "height", ":", "int", ",", "width", ":", "int", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Crop the given image at specified location and output size.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n    If image size is smaller than output size along any edge, image is padded with 0 and then cropped.\n\n    Args:\n        img (PIL Image or Tensor): Image to be cropped. (0,0) denotes the top left corner of the image.\n        top (int): Vertical component of the top left corner of the crop box.\n        left (int): Horizontal component of the top left corner of the crop box.\n        height (int): Height of the crop box.\n        width (int): Width of the crop box.\n\n    Returns:\n        PIL Image or Tensor: Cropped image.\n    \"\"\"", "\n", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "crop", "(", "img", ",", "top", ",", "left", ",", "height", ",", "width", ")", "\n", "\n", "", "return", "F_t", ".", "crop", "(", "img", ",", "top", ",", "left", ",", "height", ",", "width", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.center_crop": [[484, 521], ["isinstance", "functional._get_image_size", "int", "int", "functional.crop", "functional.pad", "functional._get_image_size", "round", "round", "int", "int", "isinstance", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size"], ["", "def", "center_crop", "(", "img", ":", "Tensor", ",", "output_size", ":", "List", "[", "int", "]", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Crops the given image at the center.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n    If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.\n\n    Args:\n        img (PIL Image or Tensor): Image to be cropped.\n        output_size (sequence or int): (height, width) of the crop box. If int or sequence with single int,\n            it is used for both directions.\n\n    Returns:\n        PIL Image or Tensor: Cropped image.\n    \"\"\"", "\n", "if", "isinstance", "(", "output_size", ",", "numbers", ".", "Number", ")", ":", "\n", "        ", "output_size", "=", "(", "int", "(", "output_size", ")", ",", "int", "(", "output_size", ")", ")", "\n", "", "elif", "isinstance", "(", "output_size", ",", "(", "tuple", ",", "list", ")", ")", "and", "len", "(", "output_size", ")", "==", "1", ":", "\n", "        ", "output_size", "=", "(", "output_size", "[", "0", "]", ",", "output_size", "[", "0", "]", ")", "\n", "\n", "", "image_width", ",", "image_height", "=", "_get_image_size", "(", "img", ")", "\n", "crop_height", ",", "crop_width", "=", "output_size", "\n", "\n", "if", "crop_width", ">", "image_width", "or", "crop_height", ">", "image_height", ":", "\n", "        ", "padding_ltrb", "=", "[", "\n", "(", "crop_width", "-", "image_width", ")", "//", "2", "if", "crop_width", ">", "image_width", "else", "0", ",", "\n", "(", "crop_height", "-", "image_height", ")", "//", "2", "if", "crop_height", ">", "image_height", "else", "0", ",", "\n", "(", "crop_width", "-", "image_width", "+", "1", ")", "//", "2", "if", "crop_width", ">", "image_width", "else", "0", ",", "\n", "(", "crop_height", "-", "image_height", "+", "1", ")", "//", "2", "if", "crop_height", ">", "image_height", "else", "0", ",", "\n", "]", "\n", "img", "=", "pad", "(", "img", ",", "padding_ltrb", ",", "fill", "=", "0", ")", "# PIL uses fill value 0", "\n", "image_width", ",", "image_height", "=", "_get_image_size", "(", "img", ")", "\n", "if", "crop_width", "==", "image_width", "and", "crop_height", "==", "image_height", ":", "\n", "            ", "return", "img", "\n", "\n", "", "", "crop_top", "=", "int", "(", "round", "(", "(", "image_height", "-", "crop_height", ")", "/", "2.", ")", ")", "\n", "crop_left", "=", "int", "(", "round", "(", "(", "image_width", "-", "crop_width", ")", "/", "2.", ")", ")", "\n", "return", "crop", "(", "img", ",", "crop_top", ",", "crop_left", ",", "crop_height", ",", "crop_width", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.resized_crop": [[523, 552], ["functional.crop", "functional.resize"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize"], ["", "def", "resized_crop", "(", "\n", "img", ":", "Tensor", ",", "top", ":", "int", ",", "left", ":", "int", ",", "height", ":", "int", ",", "width", ":", "int", ",", "size", ":", "List", "[", "int", "]", ",", "\n", "interpolation", ":", "InterpolationMode", "=", "InterpolationMode", ".", "BILINEAR", "\n", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Crop the given image and resize it to desired size.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n\n    Notably used in :class:`~torchvision.transforms.RandomResizedCrop`.\n\n    Args:\n        img (PIL Image or Tensor): Image to be cropped. (0,0) denotes the top left corner of the image.\n        top (int): Vertical component of the top left corner of the crop box.\n        left (int): Horizontal component of the top left corner of the crop box.\n        height (int): Height of the crop box.\n        width (int): Width of the crop box.\n        size (sequence or int): Desired output size. Same semantics as ``resize``.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`.\n            Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``,\n            ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported.\n            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n\n    Returns:\n        PIL Image or Tensor: Cropped image.\n    \"\"\"", "\n", "img", "=", "crop", "(", "img", ",", "top", ",", "left", ",", "height", ",", "width", ")", "\n", "img", "=", "resize", "(", "img", ",", "size", ",", "interpolation", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.hflip": [[554, 570], ["functional_tensor.hflip", "isinstance", "functional_pil.hflip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.hflip", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.hflip"], ["", "def", "hflip", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Horizontally flip the given image.\n\n    Args:\n        img (PIL Image or Tensor): Image to be flipped. If img\n            is a Tensor, it is expected to be in [..., H, W] format,\n            where ... means it can have an arbitrary number of leading\n            dimensions.\n\n    Returns:\n        PIL Image or Tensor:  Horizontally flipped image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "hflip", "(", "img", ")", "\n", "\n", "", "return", "F_t", ".", "hflip", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._get_perspective_coeffs": [[572, 600], ["torch.zeros", "enumerate", "torch.tensor().view", "res.tolist", "zip", "torch.tensor", "torch.tensor", "torch.linalg.lstsq", "len", "torch.tensor"], "function", ["None"], ["", "def", "_get_perspective_coeffs", "(", "\n", "startpoints", ":", "List", "[", "List", "[", "int", "]", "]", ",", "endpoints", ":", "List", "[", "List", "[", "int", "]", "]", "\n", ")", "->", "List", "[", "float", "]", ":", "\n", "    ", "\"\"\"Helper function to get the coefficients (a, b, c, d, e, f, g, h) for the perspective transforms.\n\n    In Perspective Transform each pixel (x, y) in the original image gets transformed as,\n     (x, y) -> ( (ax + by + c) / (gx + hy + 1), (dx + ey + f) / (gx + hy + 1) )\n\n    Args:\n        startpoints (list of list of ints): List containing four lists of two integers corresponding to four corners\n            ``[top-left, top-right, bottom-right, bottom-left]`` of the original image.\n        endpoints (list of list of ints): List containing four lists of two integers corresponding to four corners\n            ``[top-left, top-right, bottom-right, bottom-left]`` of the transformed image.\n\n    Returns:\n        octuple (a, b, c, d, e, f, g, h) for transforming each pixel.\n    \"\"\"", "\n", "a_matrix", "=", "torch", ".", "zeros", "(", "2", "*", "len", "(", "startpoints", ")", ",", "8", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n", "for", "i", ",", "(", "p1", ",", "p2", ")", "in", "enumerate", "(", "zip", "(", "endpoints", ",", "startpoints", ")", ")", ":", "\n", "        ", "a_matrix", "[", "2", "*", "i", ",", ":", "]", "=", "torch", ".", "tensor", "(", "[", "p1", "[", "0", "]", ",", "p1", "[", "1", "]", ",", "1", ",", "0", ",", "0", ",", "0", ",", "-", "p2", "[", "0", "]", "*", "p1", "[", "0", "]", ",", "-", "p2", "[", "0", "]", "*", "p1", "[", "1", "]", "]", ")", "\n", "a_matrix", "[", "2", "*", "i", "+", "1", ",", ":", "]", "=", "torch", ".", "tensor", "(", "[", "0", ",", "0", ",", "0", ",", "p1", "[", "0", "]", ",", "p1", "[", "1", "]", ",", "1", ",", "-", "p2", "[", "1", "]", "*", "p1", "[", "0", "]", ",", "-", "p2", "[", "1", "]", "*", "p1", "[", "1", "]", "]", ")", "\n", "\n", "", "b_matrix", "=", "torch", ".", "tensor", "(", "startpoints", ",", "dtype", "=", "torch", ".", "float", ")", ".", "view", "(", "8", ")", "\n", "res", "=", "torch", ".", "linalg", ".", "lstsq", "(", "a_matrix", ",", "b_matrix", ",", "driver", "=", "'gels'", ")", ".", "solution", "\n", "\n", "output", ":", "List", "[", "float", "]", "=", "res", ".", "tolist", "(", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.perspective": [[602, 652], ["functional._get_perspective_coeffs", "isinstance", "functional_tensor.perspective", "warnings.warn", "functional._interpolation_modes_from_int", "isinstance", "TypeError", "isinstance", "functional_pil.perspective"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._get_perspective_coeffs", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.perspective", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.perspective"], ["", "def", "perspective", "(", "\n", "img", ":", "Tensor", ",", "\n", "startpoints", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "endpoints", ":", "List", "[", "List", "[", "int", "]", "]", ",", "\n", "interpolation", ":", "InterpolationMode", "=", "InterpolationMode", ".", "BILINEAR", ",", "\n", "fill", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", "\n", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Perform perspective transform of the given image.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n\n    Args:\n        img (PIL Image or Tensor): Image to be transformed.\n        startpoints (list of list of ints): List containing four lists of two integers corresponding to four corners\n            ``[top-left, top-right, bottom-right, bottom-left]`` of the original image.\n        endpoints (list of list of ints): List containing four lists of two integers corresponding to four corners\n            ``[top-left, top-right, bottom-right, bottom-left]`` of the transformed image.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.\n            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n        fill (sequence or number, optional): Pixel fill value for the area outside the transformed\n            image. If given a number, the value is used for all bands respectively.\n\n            .. note::\n                In torchscript mode single int/float value is not supported, please use a sequence\n                of length 1: ``[value, ]``.\n\n    Returns:\n        PIL Image or Tensor: transformed Image.\n    \"\"\"", "\n", "\n", "coeffs", "=", "_get_perspective_coeffs", "(", "startpoints", ",", "endpoints", ")", "\n", "\n", "# Backward compatibility with integer value", "\n", "if", "isinstance", "(", "interpolation", ",", "int", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\n", "\"Argument interpolation should be of type InterpolationMode instead of int. \"", "\n", "\"Please, use InterpolationMode enum.\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "interpolation", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "interpolation", ",", "InterpolationMode", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Argument interpolation should be a InterpolationMode\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "pil_interpolation", "=", "pil_modes_mapping", "[", "interpolation", "]", "\n", "return", "F_pil", ".", "perspective", "(", "img", ",", "coeffs", ",", "interpolation", "=", "pil_interpolation", ",", "fill", "=", "fill", ")", "\n", "\n", "", "return", "F_t", ".", "perspective", "(", "img", ",", "coeffs", ",", "interpolation", "=", "interpolation", ".", "value", ",", "fill", "=", "fill", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.vflip": [[654, 670], ["functional_tensor.vflip", "isinstance", "functional_pil.vflip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.vflip", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.vflip"], ["", "def", "vflip", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Vertically flip the given image.\n\n    Args:\n        img (PIL Image or Tensor): Image to be flipped. If img\n            is a Tensor, it is expected to be in [..., H, W] format,\n            where ... means it can have an arbitrary number of leading\n            dimensions.\n\n    Returns:\n        PIL Image or Tensor:  Vertically flipped image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "vflip", "(", "img", ")", "\n", "\n", "", "return", "F_t", ".", "vflip", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.five_crop": [[672, 713], ["isinstance", "functional._get_image_size", "functional.crop", "functional.crop", "functional.crop", "functional.crop", "functional.center_crop", "len", "ValueError", "ValueError", "int", "int", "isinstance", "msg.format", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.center_crop"], ["", "def", "five_crop", "(", "img", ":", "Tensor", ",", "size", ":", "List", "[", "int", "]", ")", "->", "Tuple", "[", "Tensor", ",", "Tensor", ",", "Tensor", ",", "Tensor", ",", "Tensor", "]", ":", "\n", "    ", "\"\"\"Crop the given image into four corners and the central crop.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n\n    .. Note::\n        This transform returns a tuple of images and there may be a\n        mismatch in the number of inputs and targets your ``Dataset`` returns.\n\n    Args:\n        img (PIL Image or Tensor): Image to be cropped.\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n\n    Returns:\n       tuple: tuple (tl, tr, bl, br, center)\n       Corresponding top left, top right, bottom left, bottom right and center crop.\n    \"\"\"", "\n", "if", "isinstance", "(", "size", ",", "numbers", ".", "Number", ")", ":", "\n", "        ", "size", "=", "(", "int", "(", "size", ")", ",", "int", "(", "size", ")", ")", "\n", "", "elif", "isinstance", "(", "size", ",", "(", "tuple", ",", "list", ")", ")", "and", "len", "(", "size", ")", "==", "1", ":", "\n", "        ", "size", "=", "(", "size", "[", "0", "]", ",", "size", "[", "0", "]", ")", "\n", "\n", "", "if", "len", "(", "size", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Please provide only two dimensions (h, w) for size.\"", ")", "\n", "\n", "", "image_width", ",", "image_height", "=", "_get_image_size", "(", "img", ")", "\n", "crop_height", ",", "crop_width", "=", "size", "\n", "if", "crop_width", ">", "image_width", "or", "crop_height", ">", "image_height", ":", "\n", "        ", "msg", "=", "\"Requested crop size {} is bigger than input size {}\"", "\n", "raise", "ValueError", "(", "msg", ".", "format", "(", "size", ",", "(", "image_height", ",", "image_width", ")", ")", ")", "\n", "\n", "", "tl", "=", "crop", "(", "img", ",", "0", ",", "0", ",", "crop_height", ",", "crop_width", ")", "\n", "tr", "=", "crop", "(", "img", ",", "0", ",", "image_width", "-", "crop_width", ",", "crop_height", ",", "crop_width", ")", "\n", "bl", "=", "crop", "(", "img", ",", "image_height", "-", "crop_height", ",", "0", ",", "crop_height", ",", "crop_width", ")", "\n", "br", "=", "crop", "(", "img", ",", "image_height", "-", "crop_height", ",", "image_width", "-", "crop_width", ",", "crop_height", ",", "crop_width", ")", "\n", "\n", "center", "=", "center_crop", "(", "img", ",", "[", "crop_height", ",", "crop_width", "]", ")", "\n", "\n", "return", "tl", ",", "tr", ",", "bl", ",", "br", ",", "center", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.ten_crop": [[715, 755], ["isinstance", "functional.five_crop", "functional.five_crop", "len", "ValueError", "functional.vflip", "functional.hflip", "int", "int", "isinstance", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.five_crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.five_crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.vflip", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.hflip"], ["", "def", "ten_crop", "(", "img", ":", "Tensor", ",", "size", ":", "List", "[", "int", "]", ",", "vertical_flip", ":", "bool", "=", "False", ")", "->", "List", "[", "Tensor", "]", ":", "\n", "    ", "\"\"\"Generate ten cropped images from the given image.\n    Crop the given image into four corners and the central crop plus the\n    flipped version of these (horizontal flipping is used by default).\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions\n\n    .. Note::\n        This transform returns a tuple of images and there may be a\n        mismatch in the number of inputs and targets your ``Dataset`` returns.\n\n    Args:\n        img (PIL Image or Tensor): Image to be cropped.\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n        vertical_flip (bool): Use vertical flipping instead of horizontal\n\n    Returns:\n        tuple: tuple (tl, tr, bl, br, center, tl_flip, tr_flip, bl_flip, br_flip, center_flip)\n        Corresponding top left, top right, bottom left, bottom right and\n        center crop and same for the flipped image.\n    \"\"\"", "\n", "if", "isinstance", "(", "size", ",", "numbers", ".", "Number", ")", ":", "\n", "        ", "size", "=", "(", "int", "(", "size", ")", ",", "int", "(", "size", ")", ")", "\n", "", "elif", "isinstance", "(", "size", ",", "(", "tuple", ",", "list", ")", ")", "and", "len", "(", "size", ")", "==", "1", ":", "\n", "        ", "size", "=", "(", "size", "[", "0", "]", ",", "size", "[", "0", "]", ")", "\n", "\n", "", "if", "len", "(", "size", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Please provide only two dimensions (h, w) for size.\"", ")", "\n", "\n", "", "first_five", "=", "five_crop", "(", "img", ",", "size", ")", "\n", "\n", "if", "vertical_flip", ":", "\n", "        ", "img", "=", "vflip", "(", "img", ")", "\n", "", "else", ":", "\n", "        ", "img", "=", "hflip", "(", "img", ")", "\n", "\n", "", "second_five", "=", "five_crop", "(", "img", ",", "size", ")", "\n", "return", "first_five", "+", "second_five", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.adjust_brightness": [[757, 775], ["functional_tensor.adjust_brightness", "isinstance", "functional_pil.adjust_brightness"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_brightness", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_brightness"], ["", "def", "adjust_brightness", "(", "img", ":", "Tensor", ",", "brightness_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Adjust brightness of an image.\n\n    Args:\n        img (PIL Image or Tensor): Image to be adjusted.\n            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n        brightness_factor (float):  How much to adjust the brightness. Can be\n            any non negative number. 0 gives a black image, 1 gives the\n            original image while 2 increases the brightness by a factor of 2.\n\n    Returns:\n        PIL Image or Tensor: Brightness adjusted image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "adjust_brightness", "(", "img", ",", "brightness_factor", ")", "\n", "\n", "", "return", "F_t", ".", "adjust_brightness", "(", "img", ",", "brightness_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.adjust_contrast": [[777, 795], ["functional_tensor.adjust_contrast", "isinstance", "functional_pil.adjust_contrast"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_contrast", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_contrast"], ["", "def", "adjust_contrast", "(", "img", ":", "Tensor", ",", "contrast_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Adjust contrast of an image.\n\n    Args:\n        img (PIL Image or Tensor): Image to be adjusted.\n            If img is torch Tensor, it is expected to be in [..., 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n        contrast_factor (float): How much to adjust the contrast. Can be any\n            non negative number. 0 gives a solid gray image, 1 gives the\n            original image while 2 increases the contrast by a factor of 2.\n\n    Returns:\n        PIL Image or Tensor: Contrast adjusted image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "adjust_contrast", "(", "img", ",", "contrast_factor", ")", "\n", "\n", "", "return", "F_t", ".", "adjust_contrast", "(", "img", ",", "contrast_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.adjust_saturation": [[797, 815], ["functional_tensor.adjust_saturation", "isinstance", "functional_pil.adjust_saturation"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_saturation", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_saturation"], ["", "def", "adjust_saturation", "(", "img", ":", "Tensor", ",", "saturation_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Adjust color saturation of an image.\n\n    Args:\n        img (PIL Image or Tensor): Image to be adjusted.\n            If img is torch Tensor, it is expected to be in [..., 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n        saturation_factor (float):  How much to adjust the saturation. 0 will\n            give a black and white image, 1 will give the original image while\n            2 will enhance the saturation by a factor of 2.\n\n    Returns:\n        PIL Image or Tensor: Saturation adjusted image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "adjust_saturation", "(", "img", ",", "saturation_factor", ")", "\n", "\n", "", "return", "F_t", ".", "adjust_saturation", "(", "img", ",", "saturation_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.adjust_hue": [[817, 849], ["functional_tensor.adjust_hue", "isinstance", "functional_pil.adjust_hue"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_hue", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_hue"], ["", "def", "adjust_hue", "(", "img", ":", "Tensor", ",", "hue_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Adjust hue of an image.\n\n    The image hue is adjusted by converting the image to HSV and\n    cyclically shifting the intensities in the hue channel (H).\n    The image is then converted back to original image mode.\n\n    `hue_factor` is the amount of shift in H channel and must be in the\n    interval `[-0.5, 0.5]`.\n\n    See `Hue`_ for more details.\n\n    .. _Hue: https://en.wikipedia.org/wiki/Hue\n\n    Args:\n        img (PIL Image or Tensor): Image to be adjusted.\n            If img is torch Tensor, it is expected to be in [..., 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n            If img is PIL Image mode \"1\", \"L\", \"I\", \"F\" and modes with transparency (alpha channel) are not supported.\n        hue_factor (float):  How much to shift the hue channel. Should be in\n            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n            HSV space in positive and negative direction respectively.\n            0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n            with complementary colors while 0 gives the original image.\n\n    Returns:\n        PIL Image or Tensor: Hue adjusted image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "adjust_hue", "(", "img", ",", "hue_factor", ")", "\n", "\n", "", "return", "F_t", ".", "adjust_hue", "(", "img", ",", "hue_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.adjust_gamma": [[851, 880], ["functional_tensor.adjust_gamma", "isinstance", "functional_pil.adjust_gamma"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_gamma", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_gamma"], ["", "def", "adjust_gamma", "(", "img", ":", "Tensor", ",", "gamma", ":", "float", ",", "gain", ":", "float", "=", "1", ")", "->", "Tensor", ":", "\n", "    ", "r\"\"\"Perform gamma correction on an image.\n\n    Also known as Power Law Transform. Intensities in RGB mode are adjusted\n    based on the following equation:\n\n    .. math::\n        I_{\\text{out}} = 255 \\times \\text{gain} \\times \\left(\\frac{I_{\\text{in}}}{255}\\right)^{\\gamma}\n\n    See `Gamma Correction`_ for more details.\n\n    .. _Gamma Correction: https://en.wikipedia.org/wiki/Gamma_correction\n\n    Args:\n        img (PIL Image or Tensor): PIL Image to be adjusted.\n            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n            If img is PIL Image, modes with transparency (alpha channel) are not supported.\n        gamma (float): Non negative real number, same as :math:`\\gamma` in the equation.\n            gamma larger than 1 make the shadows darker,\n            while gamma smaller than 1 make dark regions lighter.\n        gain (float): The constant multiplier.\n    Returns:\n        PIL Image or Tensor: Gamma correction adjusted image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "adjust_gamma", "(", "img", ",", "gamma", ",", "gain", ")", "\n", "\n", "", "return", "F_t", ".", "adjust_gamma", "(", "img", ",", "gamma", ",", "gain", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._get_inverse_affine_matrix": [[882, 930], ["math.radians", "math.radians", "math.cos", "math.cos", "math.sin", "math.sin", "math.cos", "math.cos", "math.cos", "math.cos", "math.tan", "math.tan", "math.cos", "math.sin", "functional.scale", "functional.scale"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.scale", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.scale"], ["", "def", "_get_inverse_affine_matrix", "(", "\n", "center", ":", "List", "[", "float", "]", ",", "angle", ":", "float", ",", "translate", ":", "List", "[", "float", "]", ",", "scale", ":", "float", ",", "shear", ":", "List", "[", "float", "]", "\n", ")", "->", "List", "[", "float", "]", ":", "\n", "# Helper method to compute inverse matrix for affine transformation", "\n", "\n", "# As it is explained in PIL.Image.rotate", "\n", "# We need compute INVERSE of affine transformation matrix: M = T * C * RSS * C^-1", "\n", "# where T is translation matrix: [1, 0, tx | 0, 1, ty | 0, 0, 1]", "\n", "#       C is translation matrix to keep center: [1, 0, cx | 0, 1, cy | 0, 0, 1]", "\n", "#       RSS is rotation with scale and shear matrix", "\n", "#       RSS(a, s, (sx, sy)) =", "\n", "#       = R(a) * S(s) * SHy(sy) * SHx(sx)", "\n", "#       = [ s*cos(a - sy)/cos(sy), s*(-cos(a - sy)*tan(x)/cos(y) - sin(a)), 0 ]", "\n", "#         [ s*sin(a + sy)/cos(sy), s*(-sin(a - sy)*tan(x)/cos(y) + cos(a)), 0 ]", "\n", "#         [ 0                    , 0                                      , 1 ]", "\n", "#", "\n", "# where R is a rotation matrix, S is a scaling matrix, and SHx and SHy are the shears:", "\n", "# SHx(s) = [1, -tan(s)] and SHy(s) = [1      , 0]", "\n", "#          [0, 1      ]              [-tan(s), 1]", "\n", "#", "\n", "# Thus, the inverse is M^-1 = C * RSS^-1 * C^-1 * T^-1", "\n", "\n", "    ", "rot", "=", "math", ".", "radians", "(", "angle", ")", "\n", "sx", ",", "sy", "=", "[", "math", ".", "radians", "(", "s", ")", "for", "s", "in", "shear", "]", "\n", "\n", "cx", ",", "cy", "=", "center", "\n", "tx", ",", "ty", "=", "translate", "\n", "\n", "# RSS without scaling", "\n", "a", "=", "math", ".", "cos", "(", "rot", "-", "sy", ")", "/", "math", ".", "cos", "(", "sy", ")", "\n", "b", "=", "-", "math", ".", "cos", "(", "rot", "-", "sy", ")", "*", "math", ".", "tan", "(", "sx", ")", "/", "math", ".", "cos", "(", "sy", ")", "-", "math", ".", "sin", "(", "rot", ")", "\n", "c", "=", "math", ".", "sin", "(", "rot", "-", "sy", ")", "/", "math", ".", "cos", "(", "sy", ")", "\n", "d", "=", "-", "math", ".", "sin", "(", "rot", "-", "sy", ")", "*", "math", ".", "tan", "(", "sx", ")", "/", "math", ".", "cos", "(", "sy", ")", "+", "math", ".", "cos", "(", "rot", ")", "\n", "\n", "# Inverted rotation matrix with scale and shear", "\n", "# det([[a, b], [c, d]]) == 1, since det(rotation) = 1 and det(shear) = 1", "\n", "matrix", "=", "[", "d", ",", "-", "b", ",", "0.0", ",", "-", "c", ",", "a", ",", "0.0", "]", "\n", "matrix", "=", "[", "x", "/", "scale", "for", "x", "in", "matrix", "]", "\n", "\n", "# Apply inverse of translation and of center translation: RSS^-1 * C^-1 * T^-1", "\n", "matrix", "[", "2", "]", "+=", "matrix", "[", "0", "]", "*", "(", "-", "cx", "-", "tx", ")", "+", "matrix", "[", "1", "]", "*", "(", "-", "cy", "-", "ty", ")", "\n", "matrix", "[", "5", "]", "+=", "matrix", "[", "3", "]", "*", "(", "-", "cx", "-", "tx", ")", "+", "matrix", "[", "4", "]", "*", "(", "-", "cy", "-", "ty", ")", "\n", "\n", "# Apply center translation: C * RSS^-1 * C^-1 * T^-1", "\n", "matrix", "[", "2", "]", "+=", "cx", "\n", "matrix", "[", "5", "]", "+=", "cy", "\n", "\n", "return", "matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.rotate": [[932, 1004], ["isinstance", "functional._get_inverse_affine_matrix", "functional_tensor.rotate", "warnings.warn", "functional._interpolation_modes_from_int", "warnings.warn", "functional._interpolation_modes_from_int", "isinstance", "TypeError", "TypeError", "isinstance", "TypeError", "isinstance", "functional_pil.rotate", "functional._get_image_size", "isinstance", "zip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._get_inverse_affine_matrix", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rotate", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rotate", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size"], ["", "def", "rotate", "(", "\n", "img", ":", "Tensor", ",", "angle", ":", "float", ",", "interpolation", ":", "InterpolationMode", "=", "InterpolationMode", ".", "NEAREST", ",", "\n", "expand", ":", "bool", "=", "False", ",", "center", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "fill", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", ",", "resample", ":", "Optional", "[", "int", "]", "=", "None", "\n", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Rotate the image by angle.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n\n    Args:\n        img (PIL Image or Tensor): image to be rotated.\n        angle (number): rotation angle value in degrees, counter-clockwise.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n        expand (bool, optional): Optional expansion flag.\n            If true, expands the output image to make it large enough to hold the entire rotated image.\n            If false or omitted, make the output image the same size as the input image.\n            Note that the expand flag assumes rotation around the center and no translation.\n        center (sequence, optional): Optional center of rotation. Origin is the upper left corner.\n            Default is the center of the image.\n        fill (sequence or number, optional): Pixel fill value for the area outside the transformed\n            image. If given a number, the value is used for all bands respectively.\n\n            .. note::\n                In torchscript mode single int/float value is not supported, please use a sequence\n                of length 1: ``[value, ]``.\n\n    Returns:\n        PIL Image or Tensor: Rotated image.\n\n    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\n\n    \"\"\"", "\n", "if", "resample", "is", "not", "None", ":", "\n", "        ", "warnings", ".", "warn", "(", "\n", "\"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "resample", ")", "\n", "\n", "# Backward compatibility with integer value", "\n", "", "if", "isinstance", "(", "interpolation", ",", "int", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\n", "\"Argument interpolation should be of type InterpolationMode instead of int. \"", "\n", "\"Please, use InterpolationMode enum.\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "interpolation", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "angle", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Argument angle should be int or float\"", ")", "\n", "\n", "", "if", "center", "is", "not", "None", "and", "not", "isinstance", "(", "center", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Argument center should be a sequence\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "interpolation", ",", "InterpolationMode", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Argument interpolation should be a InterpolationMode\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "pil_interpolation", "=", "pil_modes_mapping", "[", "interpolation", "]", "\n", "return", "F_pil", ".", "rotate", "(", "img", ",", "angle", "=", "angle", ",", "interpolation", "=", "pil_interpolation", ",", "expand", "=", "expand", ",", "center", "=", "center", ",", "fill", "=", "fill", ")", "\n", "\n", "", "center_f", "=", "[", "0.0", ",", "0.0", "]", "\n", "if", "center", "is", "not", "None", ":", "\n", "        ", "img_size", "=", "_get_image_size", "(", "img", ")", "\n", "# Center values should be in pixel coordinates but translated such that (0, 0) corresponds to image center.", "\n", "center_f", "=", "[", "1.0", "*", "(", "c", "-", "s", "*", "0.5", ")", "for", "c", ",", "s", "in", "zip", "(", "center", ",", "img_size", ")", "]", "\n", "\n", "# due to current incoherence of rotation angle direction between affine and rotate implementations", "\n", "# we need to set -angle.", "\n", "", "matrix", "=", "_get_inverse_affine_matrix", "(", "center_f", ",", "-", "angle", ",", "[", "0.0", ",", "0.0", "]", ",", "1.0", ",", "[", "0.0", ",", "0.0", "]", ")", "\n", "return", "F_t", ".", "rotate", "(", "img", ",", "matrix", "=", "matrix", ",", "interpolation", "=", "interpolation", ".", "value", ",", "expand", "=", "expand", ",", "fill", "=", "fill", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.affine": [[1006, 1110], ["isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "functional._get_image_size", "functional._get_inverse_affine_matrix", "functional_tensor.affine", "warnings.warn", "functional._interpolation_modes_from_int", "warnings.warn", "functional._interpolation_modes_from_int", "warnings.warn", "isinstance", "TypeError", "isinstance", "TypeError", "len", "ValueError", "ValueError", "isinstance", "TypeError", "isinstance", "TypeError", "float", "list", "list", "len", "len", "ValueError", "isinstance", "functional._get_inverse_affine_matrix", "functional_pil.affine"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._get_inverse_affine_matrix", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.affine", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._interpolation_modes_from_int", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional._get_inverse_affine_matrix", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.affine"], ["", "def", "affine", "(", "\n", "img", ":", "Tensor", ",", "angle", ":", "float", ",", "translate", ":", "List", "[", "int", "]", ",", "scale", ":", "float", ",", "shear", ":", "List", "[", "float", "]", ",", "\n", "interpolation", ":", "InterpolationMode", "=", "InterpolationMode", ".", "NEAREST", ",", "fill", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", ",", "\n", "resample", ":", "Optional", "[", "int", "]", "=", "None", ",", "fillcolor", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", "\n", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Apply affine transformation on the image keeping image center invariant.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n\n    Args:\n        img (PIL Image or Tensor): image to transform.\n        angle (number): rotation angle in degrees between -180 and 180, clockwise direction.\n        translate (sequence of integers): horizontal and vertical translations (post-rotation translation)\n        scale (float): overall scale\n        shear (float or sequence): shear angle value in degrees between -180 to 180, clockwise direction.\n            If a sequence is specified, the first value corresponds to a shear parallel to the x axis, while\n            the second value corresponds to a shear parallel to the y axis.\n        interpolation (InterpolationMode): Desired interpolation enum defined by\n            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.\n        fill (sequence or number, optional): Pixel fill value for the area outside the transformed\n            image. If given a number, the value is used for all bands respectively.\n\n            .. note::\n                In torchscript mode single int/float value is not supported, please use a sequence\n                of length 1: ``[value, ]``.\n        fillcolor (sequence, int, float): deprecated argument and will be removed since v0.10.0.\n            Please use the ``fill`` parameter instead.\n        resample (int, optional): deprecated argument and will be removed since v0.10.0.\n            Please use the ``interpolation`` parameter instead.\n\n    Returns:\n        PIL Image or Tensor: Transformed image.\n    \"\"\"", "\n", "if", "resample", "is", "not", "None", ":", "\n", "        ", "warnings", ".", "warn", "(", "\n", "\"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "resample", ")", "\n", "\n", "# Backward compatibility with integer value", "\n", "", "if", "isinstance", "(", "interpolation", ",", "int", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\n", "\"Argument interpolation should be of type InterpolationMode instead of int. \"", "\n", "\"Please, use InterpolationMode enum.\"", "\n", ")", "\n", "interpolation", "=", "_interpolation_modes_from_int", "(", "interpolation", ")", "\n", "\n", "", "if", "fillcolor", "is", "not", "None", ":", "\n", "        ", "warnings", ".", "warn", "(", "\n", "\"Argument fillcolor is deprecated and will be removed since v0.10.0. Please, use fill instead\"", "\n", ")", "\n", "fill", "=", "fillcolor", "\n", "\n", "", "if", "not", "isinstance", "(", "angle", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Argument angle should be int or float\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "translate", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Argument translate should be a sequence\"", ")", "\n", "\n", "", "if", "len", "(", "translate", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Argument translate should be a sequence of length 2\"", ")", "\n", "\n", "", "if", "scale", "<=", "0.0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Argument scale should be positive\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "shear", ",", "(", "numbers", ".", "Number", ",", "(", "list", ",", "tuple", ")", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Shear should be either a single value or a sequence of two values\"", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "interpolation", ",", "InterpolationMode", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Argument interpolation should be a InterpolationMode\"", ")", "\n", "\n", "", "if", "isinstance", "(", "angle", ",", "int", ")", ":", "\n", "        ", "angle", "=", "float", "(", "angle", ")", "\n", "\n", "", "if", "isinstance", "(", "translate", ",", "tuple", ")", ":", "\n", "        ", "translate", "=", "list", "(", "translate", ")", "\n", "\n", "", "if", "isinstance", "(", "shear", ",", "numbers", ".", "Number", ")", ":", "\n", "        ", "shear", "=", "[", "shear", ",", "0.0", "]", "\n", "\n", "", "if", "isinstance", "(", "shear", ",", "tuple", ")", ":", "\n", "        ", "shear", "=", "list", "(", "shear", ")", "\n", "\n", "", "if", "len", "(", "shear", ")", "==", "1", ":", "\n", "        ", "shear", "=", "[", "shear", "[", "0", "]", ",", "shear", "[", "0", "]", "]", "\n", "\n", "", "if", "len", "(", "shear", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Shear should be a sequence containing two values. Got {}\"", ".", "format", "(", "shear", ")", ")", "\n", "\n", "", "img_size", "=", "_get_image_size", "(", "img", ")", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "# center = (img_size[0] * 0.5 + 0.5, img_size[1] * 0.5 + 0.5)", "\n", "# it is visually better to estimate the center without 0.5 offset", "\n", "# otherwise image rotated by 90 degrees is shifted vs output image of torch.rot90 or F_t.affine", "\n", "        ", "center", "=", "[", "img_size", "[", "0", "]", "*", "0.5", ",", "img_size", "[", "1", "]", "*", "0.5", "]", "\n", "matrix", "=", "_get_inverse_affine_matrix", "(", "center", ",", "angle", ",", "translate", ",", "scale", ",", "shear", ")", "\n", "pil_interpolation", "=", "pil_modes_mapping", "[", "interpolation", "]", "\n", "return", "F_pil", ".", "affine", "(", "img", ",", "matrix", "=", "matrix", ",", "interpolation", "=", "pil_interpolation", ",", "fill", "=", "fill", ")", "\n", "\n", "", "translate_f", "=", "[", "1.0", "*", "t", "for", "t", "in", "translate", "]", "\n", "matrix", "=", "_get_inverse_affine_matrix", "(", "[", "0.0", ",", "0.0", "]", ",", "angle", ",", "translate_f", ",", "scale", ",", "shear", ")", "\n", "return", "F_t", ".", "affine", "(", "img", ",", "matrix", "=", "matrix", ",", "interpolation", "=", "interpolation", ".", "value", ",", "fill", "=", "fill", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.to_grayscale": [[1112, 1131], ["isinstance", "TypeError", "functional_pil.to_grayscale"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.to_grayscale"], ["", "@", "torch", ".", "jit", ".", "unused", "\n", "def", "to_grayscale", "(", "img", ",", "num_output_channels", "=", "1", ")", ":", "\n", "    ", "\"\"\"Convert PIL image of any mode (RGB, HSV, LAB, etc) to grayscale version of image.\n    This transform does not support torch Tensor.\n\n    Args:\n        img (PIL Image): PIL Image to be converted to grayscale.\n        num_output_channels (int): number of channels of the output image. Value can be 1 or 3. Default is 1.\n\n    Returns:\n        PIL Image: Grayscale version of the image.\n\n        - if num_output_channels = 1 : returned image is single channel\n        - if num_output_channels = 3 : returned image is 3 channel with r = g = b\n    \"\"\"", "\n", "if", "isinstance", "(", "img", ",", "Image", ".", "Image", ")", ":", "\n", "        ", "return", "F_pil", ".", "to_grayscale", "(", "img", ",", "num_output_channels", ")", "\n", "\n", "", "raise", "TypeError", "(", "\"Input should be PIL Image\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.rgb_to_grayscale": [[1133, 1156], ["functional_tensor.rgb_to_grayscale", "isinstance", "functional_pil.to_grayscale"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rgb_to_grayscale", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.to_grayscale"], ["", "def", "rgb_to_grayscale", "(", "img", ":", "Tensor", ",", "num_output_channels", ":", "int", "=", "1", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Convert RGB image to grayscale version of image.\n    If the image is torch Tensor, it is expected\n    to have [..., 3, H, W] shape, where ... means an arbitrary number of leading dimensions\n\n    Note:\n        Please, note that this method supports only RGB images as input. For inputs in other color spaces,\n        please, consider using meth:`~torchvision.transforms.functional.to_grayscale` with PIL Image.\n\n    Args:\n        img (PIL Image or Tensor): RGB Image to be converted to grayscale.\n        num_output_channels (int): number of channels of the output image. Value can be 1 or 3. Default, 1.\n\n    Returns:\n        PIL Image or Tensor: Grayscale version of the image.\n\n        - if num_output_channels = 1 : returned image is single channel\n        - if num_output_channels = 3 : returned image is 3 channel with r = g = b\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "to_grayscale", "(", "img", ",", "num_output_channels", ")", "\n", "\n", "", "return", "F_t", ".", "rgb_to_grayscale", "(", "img", ",", "num_output_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.erase": [[1158, 1182], ["isinstance", "TypeError", "img.clone.clone", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["", "def", "erase", "(", "img", ":", "Tensor", ",", "i", ":", "int", ",", "j", ":", "int", ",", "h", ":", "int", ",", "w", ":", "int", ",", "v", ":", "Tensor", ",", "inplace", ":", "bool", "=", "False", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\" Erase the input Tensor Image with given value.\n    This transform does not support PIL Image.\n\n    Args:\n        img (Tensor Image): Tensor image of size (C, H, W) to be erased\n        i (int): i in (i,j) i.e coordinates of the upper left corner.\n        j (int): j in (i,j) i.e coordinates of the upper left corner.\n        h (int): Height of the erased region.\n        w (int): Width of the erased region.\n        v: Erasing value.\n        inplace(bool, optional): For in-place operations. By default is set False.\n\n    Returns:\n        Tensor Image: Erased image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be Tensor Image. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "if", "not", "inplace", ":", "\n", "        ", "img", "=", "img", ".", "clone", "(", ")", "\n", "\n", "", "img", "[", "...", ",", "i", ":", "i", "+", "h", ",", "j", ":", "j", "+", "w", "]", "=", "v", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.gaussian_blur": [[1184, 1247], ["isinstance", "isinstance", "functional_tensor.gaussian_blur", "isinstance", "TypeError", "len", "ValueError", "TypeError", "isinstance", "len", "ValueError", "isinstance", "functional.to_tensor", "isinstance", "functional.to_pil_image", "ValueError", "isinstance", "float", "float", "len", "ValueError", "functional_pil._is_pil_image", "TypeError", "type", "len", "type", "len", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.gaussian_blur", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.to_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.to_pil_image", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_pil._is_pil_image"], ["", "def", "gaussian_blur", "(", "img", ":", "Tensor", ",", "kernel_size", ":", "List", "[", "int", "]", ",", "sigma", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Performs Gaussian blurring on the image by given kernel.\n    If the image is torch Tensor, it is expected\n    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n\n    Args:\n        img (PIL Image or Tensor): Image to be blurred\n        kernel_size (sequence of ints or int): Gaussian kernel size. Can be a sequence of integers\n            like ``(kx, ky)`` or a single integer for square kernels.\n\n            .. note::\n                In torchscript mode kernel_size as single int is not supported, use a sequence of\n                length 1: ``[ksize, ]``.\n        sigma (sequence of floats or float, optional): Gaussian kernel standard deviation. Can be a\n            sequence of floats like ``(sigma_x, sigma_y)`` or a single float to define the\n            same sigma in both X/Y directions. If None, then it is computed using\n            ``kernel_size`` as ``sigma = 0.3 * ((kernel_size - 1) * 0.5 - 1) + 0.8``.\n            Default, None.\n\n            .. note::\n                In torchscript mode sigma as single float is\n                not supported, use a sequence of length 1: ``[sigma, ]``.\n\n    Returns:\n        PIL Image or Tensor: Gaussian Blurred version of the image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "kernel_size", ",", "(", "int", ",", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'kernel_size should be int or a sequence of integers. Got {}'", ".", "format", "(", "type", "(", "kernel_size", ")", ")", ")", "\n", "", "if", "isinstance", "(", "kernel_size", ",", "int", ")", ":", "\n", "        ", "kernel_size", "=", "[", "kernel_size", ",", "kernel_size", "]", "\n", "", "if", "len", "(", "kernel_size", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "'If kernel_size is a sequence its length should be 2. Got {}'", ".", "format", "(", "len", "(", "kernel_size", ")", ")", ")", "\n", "", "for", "ksize", "in", "kernel_size", ":", "\n", "        ", "if", "ksize", "%", "2", "==", "0", "or", "ksize", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "'kernel_size should have odd and positive integers. Got {}'", ".", "format", "(", "kernel_size", ")", ")", "\n", "\n", "", "", "if", "sigma", "is", "None", ":", "\n", "        ", "sigma", "=", "[", "ksize", "*", "0.15", "+", "0.35", "for", "ksize", "in", "kernel_size", "]", "\n", "\n", "", "if", "sigma", "is", "not", "None", "and", "not", "isinstance", "(", "sigma", ",", "(", "int", ",", "float", ",", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'sigma should be either float or sequence of floats. Got {}'", ".", "format", "(", "type", "(", "sigma", ")", ")", ")", "\n", "", "if", "isinstance", "(", "sigma", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "        ", "sigma", "=", "[", "float", "(", "sigma", ")", ",", "float", "(", "sigma", ")", "]", "\n", "", "if", "isinstance", "(", "sigma", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "sigma", ")", "==", "1", ":", "\n", "        ", "sigma", "=", "[", "sigma", "[", "0", "]", ",", "sigma", "[", "0", "]", "]", "\n", "", "if", "len", "(", "sigma", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "'If sigma is a sequence, its length should be 2. Got {}'", ".", "format", "(", "len", "(", "sigma", ")", ")", ")", "\n", "", "for", "s", "in", "sigma", ":", "\n", "        ", "if", "s", "<=", "0.", ":", "\n", "            ", "raise", "ValueError", "(", "'sigma should have positive values. Got {}'", ".", "format", "(", "sigma", ")", ")", "\n", "\n", "", "", "t_img", "=", "img", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "not", "F_pil", ".", "_is_pil_image", "(", "img", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'img should be PIL Image or Tensor. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "t_img", "=", "to_tensor", "(", "img", ")", "\n", "\n", "", "output", "=", "F_t", ".", "gaussian_blur", "(", "t_img", ",", "kernel_size", ",", "sigma", ")", "\n", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "output", "=", "to_pil_image", "(", "output", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.invert": [[1249, 1265], ["functional_tensor.invert", "isinstance", "functional_pil.invert"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.invert", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.invert"], ["", "def", "invert", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Invert the colors of an RGB/grayscale image.\n\n    Args:\n        img (PIL Image or Tensor): Image to have its colors inverted.\n            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n            If img is PIL Image, it is expected to be in mode \"L\" or \"RGB\".\n\n    Returns:\n        PIL Image or Tensor: Color inverted image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "invert", "(", "img", ")", "\n", "\n", "", "return", "F_t", ".", "invert", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.posterize": [[1267, 1287], ["functional_tensor.posterize", "ValueError", "isinstance", "functional_pil.posterize"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.posterize", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.posterize"], ["", "def", "posterize", "(", "img", ":", "Tensor", ",", "bits", ":", "int", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Posterize an image by reducing the number of bits for each color channel.\n\n    Args:\n        img (PIL Image or Tensor): Image to have its colors posterized.\n            If img is torch Tensor, it should be of type torch.uint8 and\n            it is expected to be in [..., 1 or 3, H, W] format, where ... means\n            it can have an arbitrary number of leading dimensions.\n            If img is PIL Image, it is expected to be in mode \"L\" or \"RGB\".\n        bits (int): The number of bits to keep for each channel (0-8).\n    Returns:\n        PIL Image or Tensor: Posterized image.\n    \"\"\"", "\n", "if", "not", "(", "0", "<=", "bits", "<=", "8", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'The number if bits should be between 0 and 8. Got {}'", ".", "format", "(", "bits", ")", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "posterize", "(", "img", ",", "bits", ")", "\n", "\n", "", "return", "F_t", ".", "posterize", "(", "img", ",", "bits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.solarize": [[1289, 1305], ["functional_tensor.solarize", "isinstance", "functional_pil.solarize"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.solarize", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.solarize"], ["", "def", "solarize", "(", "img", ":", "Tensor", ",", "threshold", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Solarize an RGB/grayscale image by inverting all pixel values above a threshold.\n\n    Args:\n        img (PIL Image or Tensor): Image to have its colors inverted.\n            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n            If img is PIL Image, it is expected to be in mode \"L\" or \"RGB\".\n        threshold (float): All pixels equal or above this value are inverted.\n    Returns:\n        PIL Image or Tensor: Solarized image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "solarize", "(", "img", ",", "threshold", ")", "\n", "\n", "", "return", "F_t", ".", "solarize", "(", "img", ",", "threshold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.adjust_sharpness": [[1307, 1325], ["functional_tensor.adjust_sharpness", "isinstance", "functional_pil.adjust_sharpness"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_sharpness", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_sharpness"], ["", "def", "adjust_sharpness", "(", "img", ":", "Tensor", ",", "sharpness_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Adjust the sharpness of an image.\n\n    Args:\n        img (PIL Image or Tensor): Image to be adjusted.\n            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n        sharpness_factor (float):  How much to adjust the sharpness. Can be\n            any non negative number. 0 gives a blurred image, 1 gives the\n            original image while 2 increases the sharpness by a factor of 2.\n\n    Returns:\n        PIL Image or Tensor: Sharpness adjusted image.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "adjust_sharpness", "(", "img", ",", "sharpness_factor", ")", "\n", "\n", "", "return", "F_t", ".", "adjust_sharpness", "(", "img", ",", "sharpness_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.autocontrast": [[1327, 1345], ["functional_tensor.autocontrast", "isinstance", "functional_pil.autocontrast"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.autocontrast", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.autocontrast"], ["", "def", "autocontrast", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Maximize contrast of an image by remapping its\n    pixels per channel so that the lowest becomes black and the lightest\n    becomes white.\n\n    Args:\n        img (PIL Image or Tensor): Image on which autocontrast is applied.\n            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n            If img is PIL Image, it is expected to be in mode \"L\" or \"RGB\".\n\n    Returns:\n        PIL Image or Tensor: An image that was autocontrasted.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "autocontrast", "(", "img", ")", "\n", "\n", "", "return", "F_t", ".", "autocontrast", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional.equalize": [[1347, 1366], ["functional_tensor.equalize", "isinstance", "functional_pil.equalize"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.equalize", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.equalize"], ["", "def", "equalize", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"Equalize the histogram of an image by applying\n    a non-linear mapping to the input in order to create a uniform\n    distribution of grayscale values in the output.\n\n    Args:\n        img (PIL Image or Tensor): Image on which equalize is applied.\n            If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format,\n            where ... means it can have an arbitrary number of leading dimensions.\n            The tensor dtype must be ``torch.uint8`` and values are expected to be in ``[0, 255]``.\n            If img is PIL Image, it is expected to be in mode \"P\", \"L\" or \"RGB\".\n\n    Returns:\n        PIL Image or Tensor: An image that was equalized.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "F_pil", ".", "equalize", "(", "img", ")", "\n", "\n", "", "return", "F_t", ".", "equalize", "(", "img", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._is_tensor_a_torch_image": [[10, 12], ["None"], "function", ["None"], ["def", "_is_tensor_a_torch_image", "(", "x", ":", "Tensor", ")", "->", "bool", ":", "\n", "    ", "return", "x", ".", "ndim", ">=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor": [[14, 17], ["functional_tensor._is_tensor_a_torch_image", "TypeError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._is_tensor_a_torch_image"], ["", "def", "_assert_image_tensor", "(", "img", ")", ":", "\n", "    ", "if", "not", "_is_tensor_a_torch_image", "(", "img", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Tensor is not a torch image.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size": [[19, 23], ["functional_tensor._assert_image_tensor"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor"], ["", "", "def", "_get_image_size", "(", "img", ":", "Tensor", ")", "->", "List", "[", "int", "]", ":", "\n", "# Returns (w, h) of tensor image", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "return", "[", "img", ".", "shape", "[", "-", "1", "]", ",", "img", ".", "shape", "[", "-", "2", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels": [[25, 32], ["TypeError"], "function", ["None"], ["", "def", "_get_image_num_channels", "(", "img", ":", "Tensor", ")", "->", "int", ":", "\n", "    ", "if", "img", ".", "ndim", "==", "2", ":", "\n", "        ", "return", "1", "\n", "", "elif", "img", ".", "ndim", ">", "2", ":", "\n", "        ", "return", "img", ".", "shape", "[", "-", "3", "]", "\n", "\n", "", "raise", "TypeError", "(", "\"Input ndim should be 2 or more. Got {}\"", ".", "format", "(", "img", ".", "ndim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._max_value": [[34, 50], ["torch.tensor", "torch.tensor", "torch.tensor.item", "torch.tensor().is_signed", "torch.tensor.pow().sub", "torch.tensor", "torch.tensor.pow"], "function", ["None"], ["", "def", "_max_value", "(", "dtype", ":", "torch", ".", "dtype", ")", "->", "float", ":", "\n", "# TODO: replace this method with torch.iinfo when it gets torchscript support.", "\n", "# https://github.com/pytorch/pytorch/issues/41492", "\n", "\n", "    ", "a", "=", "torch", ".", "tensor", "(", "2", ",", "dtype", "=", "dtype", ")", "\n", "signed", "=", "1", "if", "torch", ".", "tensor", "(", "0", ",", "dtype", "=", "dtype", ")", ".", "is_signed", "(", ")", "else", "0", "\n", "bits", "=", "1", "\n", "max_value", "=", "torch", ".", "tensor", "(", "-", "signed", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "while", "True", ":", "\n", "        ", "next_value", "=", "a", ".", "pow", "(", "bits", "-", "signed", ")", ".", "sub", "(", "1", ")", "\n", "if", "next_value", ">", "max_value", ":", "\n", "            ", "max_value", "=", "next_value", "\n", "bits", "*=", "2", "\n", "", "else", ":", "\n", "            ", "break", "\n", "", "", "return", "max_value", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels": [[52, 56], ["functional_tensor._get_image_num_channels", "TypeError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels"], ["", "def", "_assert_channels", "(", "img", ":", "Tensor", ",", "permitted", ":", "List", "[", "int", "]", ")", "->", "None", ":", "\n", "    ", "c", "=", "_get_image_num_channels", "(", "img", ")", "\n", "if", "c", "not", "in", "permitted", ":", "\n", "        ", "raise", "TypeError", "(", "\"Input image tensor permitted channel values are {}, but found {}\"", ".", "format", "(", "permitted", ",", "c", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.convert_image_dtype": [[58, 108], ["image.to.is_floating_point", "torch.tensor().is_floating_point", "functional_tensor._max_value", "image.to.mul", "image.mul.to", "functional_tensor._max_value", "torch.tensor().is_floating_point", "functional_tensor._max_value", "image.to.to", "RuntimeError", "image.to.to", "int", "torch.div", "image.to.to", "int", "image.to.to", "torch.tensor", "torch.tensor", "torch.float32"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._max_value", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._max_value", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._max_value", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "", "def", "convert_image_dtype", "(", "image", ":", "torch", ".", "Tensor", ",", "dtype", ":", "torch", ".", "dtype", "=", "torch", ".", "float", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "if", "image", ".", "dtype", "==", "dtype", ":", "\n", "        ", "return", "image", "\n", "\n", "", "if", "image", ".", "is_floating_point", "(", ")", ":", "\n", "\n", "# TODO: replace with dtype.is_floating_point when torchscript supports it", "\n", "        ", "if", "torch", ".", "tensor", "(", "0", ",", "dtype", "=", "dtype", ")", ".", "is_floating_point", "(", ")", ":", "\n", "            ", "return", "image", ".", "to", "(", "dtype", ")", "\n", "\n", "# float to int", "\n", "", "if", "(", "image", ".", "dtype", "==", "torch", ".", "float32", "and", "dtype", "in", "(", "torch", ".", "int32", ",", "torch", ".", "int64", ")", ")", "or", "(", "\n", "image", ".", "dtype", "==", "torch", ".", "float64", "and", "dtype", "==", "torch", ".", "int64", "\n", ")", ":", "\n", "            ", "msg", "=", "f\"The cast from {image.dtype} to {dtype} cannot be performed safely.\"", "\n", "raise", "RuntimeError", "(", "msg", ")", "\n", "\n", "# https://github.com/pytorch/vision/pull/2078#issuecomment-612045321", "\n", "# For data in the range 0-1, (float * 255).to(uint) is only 255", "\n", "# when float is exactly 1.0.", "\n", "# `max + 1 - epsilon` provides more evenly distributed mapping of", "\n", "# ranges of floats to ints.", "\n", "", "eps", "=", "1e-3", "\n", "max_val", "=", "_max_value", "(", "dtype", ")", "\n", "result", "=", "image", ".", "mul", "(", "max_val", "+", "1.0", "-", "eps", ")", "\n", "return", "result", ".", "to", "(", "dtype", ")", "\n", "", "else", ":", "\n", "        ", "input_max", "=", "_max_value", "(", "image", ".", "dtype", ")", "\n", "\n", "# int to float", "\n", "# TODO: replace with dtype.is_floating_point when torchscript supports it", "\n", "if", "torch", ".", "tensor", "(", "0", ",", "dtype", "=", "dtype", ")", ".", "is_floating_point", "(", ")", ":", "\n", "            ", "image", "=", "image", ".", "to", "(", "dtype", ")", "\n", "return", "image", "/", "input_max", "\n", "\n", "", "output_max", "=", "_max_value", "(", "dtype", ")", "\n", "\n", "# int to int", "\n", "if", "input_max", ">", "output_max", ":", "\n", "# factor should be forced to int for torch jit script", "\n", "# otherwise factor is a float and image // factor can produce different results", "\n", "            ", "factor", "=", "int", "(", "(", "input_max", "+", "1", ")", "//", "(", "output_max", "+", "1", ")", ")", "\n", "image", "=", "torch", ".", "div", "(", "image", ",", "factor", ",", "rounding_mode", "=", "'floor'", ")", "\n", "return", "image", ".", "to", "(", "dtype", ")", "\n", "", "else", ":", "\n", "# factor should be forced to int for torch jit script", "\n", "# otherwise factor is a float and image * factor can produce different results", "\n", "            ", "factor", "=", "int", "(", "(", "output_max", "+", "1", ")", "//", "(", "input_max", "+", "1", ")", ")", "\n", "image", "=", "image", ".", "to", "(", "dtype", ")", "\n", "return", "image", "*", "factor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.vflip": [[110, 114], ["functional_tensor._assert_image_tensor", "img.flip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor"], ["", "", "", "def", "vflip", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "return", "img", ".", "flip", "(", "-", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.hflip": [[116, 120], ["functional_tensor._assert_image_tensor", "img.flip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor"], ["", "def", "hflip", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "return", "img", ".", "flip", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop": [[122, 133], ["functional_tensor._assert_image_tensor", "functional_tensor._get_image_size", "torch.nn.functional.pad", "max", "max", "max", "max", "max", "max"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad"], ["", "def", "crop", "(", "img", ":", "Tensor", ",", "top", ":", "int", ",", "left", ":", "int", ",", "height", ":", "int", ",", "width", ":", "int", ")", "->", "Tensor", ":", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "w", ",", "h", "=", "_get_image_size", "(", "img", ")", "\n", "right", "=", "left", "+", "width", "\n", "bottom", "=", "top", "+", "height", "\n", "\n", "if", "left", "<", "0", "or", "top", "<", "0", "or", "right", ">", "w", "or", "bottom", ">", "h", ":", "\n", "        ", "padding_ltrb", "=", "[", "max", "(", "-", "left", ",", "0", ")", ",", "max", "(", "-", "top", ",", "0", ")", ",", "max", "(", "right", "-", "w", ",", "0", ")", ",", "max", "(", "bottom", "-", "h", ",", "0", ")", "]", "\n", "return", "pad", "(", "img", "[", "...", ",", "max", "(", "top", ",", "0", ")", ":", "bottom", ",", "max", "(", "left", ",", "0", ")", ":", "right", "]", ",", "padding_ltrb", ",", "fill", "=", "0", ")", "\n", "", "return", "img", "[", "...", ",", "top", ":", "bottom", ",", "left", ":", "right", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rgb_to_grayscale": [[135, 153], ["functional_tensor._assert_channels", "img.unbind", "l_img.unsqueeze.unsqueeze", "TypeError", "ValueError", "l_img.unsqueeze.expand"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels"], ["", "def", "rgb_to_grayscale", "(", "img", ":", "Tensor", ",", "num_output_channels", ":", "int", "=", "1", ")", "->", "Tensor", ":", "\n", "    ", "if", "img", ".", "ndim", "<", "3", ":", "\n", "        ", "raise", "TypeError", "(", "\"Input image tensor should have at least 3 dimensions, but found {}\"", ".", "format", "(", "img", ".", "ndim", ")", ")", "\n", "", "_assert_channels", "(", "img", ",", "[", "3", "]", ")", "\n", "\n", "if", "num_output_channels", "not", "in", "(", "1", ",", "3", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'num_output_channels should be either 1 or 3'", ")", "\n", "\n", "", "r", ",", "g", ",", "b", "=", "img", ".", "unbind", "(", "dim", "=", "-", "3", ")", "\n", "# This implementation closely follows the TF one:", "\n", "# https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/image_ops_impl.py#L2105-L2138", "\n", "l_img", "=", "(", "0.2989", "*", "r", "+", "0.587", "*", "g", "+", "0.114", "*", "b", ")", ".", "to", "(", "img", ".", "dtype", ")", "\n", "l_img", "=", "l_img", ".", "unsqueeze", "(", "dim", "=", "-", "3", ")", "\n", "\n", "if", "num_output_channels", "==", "3", ":", "\n", "        ", "return", "l_img", ".", "expand", "(", "img", ".", "shape", ")", "\n", "\n", "", "return", "l_img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_brightness": [[155, 164], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "functional_tensor._blend", "ValueError", "torch.zeros_like"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._blend"], ["", "def", "adjust_brightness", "(", "img", ":", "Tensor", ",", "brightness_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "if", "brightness_factor", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "'brightness_factor ({}) is not non-negative.'", ".", "format", "(", "brightness_factor", ")", ")", "\n", "\n", "", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "_assert_channels", "(", "img", ",", "[", "1", ",", "3", "]", ")", "\n", "\n", "return", "_blend", "(", "img", ",", "torch", ".", "zeros_like", "(", "img", ")", ",", "brightness_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_contrast": [[166, 178], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "torch.mean", "functional_tensor._blend", "ValueError", "torch.is_floating_point", "rgb_to_grayscale().to", "functional_tensor.rgb_to_grayscale"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._blend", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rgb_to_grayscale"], ["", "def", "adjust_contrast", "(", "img", ":", "Tensor", ",", "contrast_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "if", "contrast_factor", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "'contrast_factor ({}) is not non-negative.'", ".", "format", "(", "contrast_factor", ")", ")", "\n", "\n", "", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "_assert_channels", "(", "img", ",", "[", "3", "]", ")", "\n", "\n", "dtype", "=", "img", ".", "dtype", "if", "torch", ".", "is_floating_point", "(", "img", ")", "else", "torch", ".", "float32", "\n", "mean", "=", "torch", ".", "mean", "(", "rgb_to_grayscale", "(", "img", ")", ".", "to", "(", "dtype", ")", ",", "dim", "=", "(", "-", "3", ",", "-", "2", ",", "-", "1", ")", ",", "keepdim", "=", "True", ")", "\n", "\n", "return", "_blend", "(", "img", ",", "mean", ",", "contrast_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_hue": [[180, 207], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "functional_tensor._rgb2hsv", "torch.stack.unbind", "torch.stack", "functional_tensor._hsv2rgb", "ValueError", "isinstance", "TypeError", "functional_tensor._get_image_num_channels", "torch.stack.to"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._rgb2hsv", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._hsv2rgb", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "adjust_hue", "(", "img", ":", "Tensor", ",", "hue_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "if", "not", "(", "-", "0.5", "<=", "hue_factor", "<=", "0.5", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'hue_factor ({}) is not in [-0.5, 0.5].'", ".", "format", "(", "hue_factor", ")", ")", "\n", "\n", "", "if", "not", "(", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'Input img should be Tensor image'", ")", "\n", "\n", "", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "_assert_channels", "(", "img", ",", "[", "1", ",", "3", "]", ")", "\n", "if", "_get_image_num_channels", "(", "img", ")", "==", "1", ":", "# Match PIL behaviour", "\n", "        ", "return", "img", "\n", "\n", "", "orig_dtype", "=", "img", ".", "dtype", "\n", "if", "img", ".", "dtype", "==", "torch", ".", "uint8", ":", "\n", "        ", "img", "=", "img", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", "/", "255.0", "\n", "\n", "", "img", "=", "_rgb2hsv", "(", "img", ")", "\n", "h", ",", "s", ",", "v", "=", "img", ".", "unbind", "(", "dim", "=", "-", "3", ")", "\n", "h", "=", "(", "h", "+", "hue_factor", ")", "%", "1.0", "\n", "img", "=", "torch", ".", "stack", "(", "(", "h", ",", "s", ",", "v", ")", ",", "dim", "=", "-", "3", ")", "\n", "img_hue_adj", "=", "_hsv2rgb", "(", "img", ")", "\n", "\n", "if", "orig_dtype", "==", "torch", ".", "uint8", ":", "\n", "        ", "img_hue_adj", "=", "(", "img_hue_adj", "*", "255.0", ")", ".", "to", "(", "dtype", "=", "orig_dtype", ")", "\n", "\n", "", "return", "img_hue_adj", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_saturation": [[209, 218], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "functional_tensor._blend", "ValueError", "functional_tensor.rgb_to_grayscale"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._blend", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rgb_to_grayscale"], ["", "def", "adjust_saturation", "(", "img", ":", "Tensor", ",", "saturation_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "if", "saturation_factor", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "'saturation_factor ({}) is not non-negative.'", ".", "format", "(", "saturation_factor", ")", ")", "\n", "\n", "", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "_assert_channels", "(", "img", ",", "[", "3", "]", ")", "\n", "\n", "return", "_blend", "(", "img", ",", "rgb_to_grayscale", "(", "img", ")", ",", "saturation_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_gamma": [[220, 238], ["functional_tensor._assert_channels", "functional_tensor.convert_image_dtype", "isinstance", "TypeError", "ValueError", "torch.is_floating_point", "functional_tensor.convert_image_dtype"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.convert_image_dtype", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.convert_image_dtype"], ["", "def", "adjust_gamma", "(", "img", ":", "Tensor", ",", "gamma", ":", "float", ",", "gain", ":", "float", "=", "1", ")", "->", "Tensor", ":", "\n", "    ", "if", "not", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'Input img should be a Tensor.'", ")", "\n", "\n", "", "_assert_channels", "(", "img", ",", "[", "1", ",", "3", "]", ")", "\n", "\n", "if", "gamma", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "'Gamma should be a non-negative real number'", ")", "\n", "\n", "", "result", "=", "img", "\n", "dtype", "=", "img", ".", "dtype", "\n", "if", "not", "torch", ".", "is_floating_point", "(", "img", ")", ":", "\n", "        ", "result", "=", "convert_image_dtype", "(", "result", ",", "torch", ".", "float32", ")", "\n", "\n", "", "result", "=", "(", "gain", "*", "result", "**", "gamma", ")", ".", "clamp", "(", "0", ",", "1", ")", "\n", "\n", "result", "=", "convert_image_dtype", "(", "result", ",", "dtype", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.center_crop": [[240, 262], ["warnings.warn", "functional_tensor._assert_image_tensor", "img.size", "int", "int", "functional_tensor.crop"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop"], ["", "def", "center_crop", "(", "img", ":", "Tensor", ",", "output_size", ":", "BroadcastingList2", "[", "int", "]", ")", "->", "Tensor", ":", "\n", "    ", "\"\"\"DEPRECATED\n    \"\"\"", "\n", "warnings", ".", "warn", "(", "\n", "\"This method is deprecated and will be removed in future releases. \"", "\n", "\"Please, use ``F.center_crop`` instead.\"", "\n", ")", "\n", "\n", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "_", ",", "image_width", ",", "image_height", "=", "img", ".", "size", "(", ")", "\n", "crop_height", ",", "crop_width", "=", "output_size", "\n", "# crop_top = int(round((image_height - crop_height) / 2.))", "\n", "# Result can be different between python func and scripted func", "\n", "# Temporary workaround:", "\n", "crop_top", "=", "int", "(", "(", "image_height", "-", "crop_height", "+", "1", ")", "*", "0.5", ")", "\n", "# crop_left = int(round((image_width - crop_width) / 2.))", "\n", "# Result can be different between python func and scripted func", "\n", "# Temporary workaround:", "\n", "crop_left", "=", "int", "(", "(", "image_width", "-", "crop_width", "+", "1", ")", "*", "0.5", ")", "\n", "\n", "return", "crop", "(", "img", ",", "crop_top", ",", "crop_left", ",", "crop_height", ",", "crop_width", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.five_crop": [[264, 289], ["warnings.warn", "functional_tensor._assert_image_tensor", "img.size", "functional_tensor.crop", "functional_tensor.crop", "functional_tensor.crop", "functional_tensor.crop", "functional_tensor.center_crop", "len", "ValueError", "msg.format"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.center_crop"], ["", "def", "five_crop", "(", "img", ":", "Tensor", ",", "size", ":", "BroadcastingList2", "[", "int", "]", ")", "->", "List", "[", "Tensor", "]", ":", "\n", "    ", "\"\"\"DEPRECATED\n    \"\"\"", "\n", "warnings", ".", "warn", "(", "\n", "\"This method is deprecated and will be removed in future releases. \"", "\n", "\"Please, use ``F.five_crop`` instead.\"", "\n", ")", "\n", "\n", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "assert", "len", "(", "size", ")", "==", "2", ",", "\"Please provide only two dimensions (h, w) for size.\"", "\n", "\n", "_", ",", "image_width", ",", "image_height", "=", "img", ".", "size", "(", ")", "\n", "crop_height", ",", "crop_width", "=", "size", "\n", "if", "crop_width", ">", "image_width", "or", "crop_height", ">", "image_height", ":", "\n", "        ", "msg", "=", "\"Requested crop size {} is bigger than input size {}\"", "\n", "raise", "ValueError", "(", "msg", ".", "format", "(", "size", ",", "(", "image_height", ",", "image_width", ")", ")", ")", "\n", "\n", "", "tl", "=", "crop", "(", "img", ",", "0", ",", "0", ",", "crop_width", ",", "crop_height", ")", "\n", "tr", "=", "crop", "(", "img", ",", "image_width", "-", "crop_width", ",", "0", ",", "image_width", ",", "crop_height", ")", "\n", "bl", "=", "crop", "(", "img", ",", "0", ",", "image_height", "-", "crop_height", ",", "crop_width", ",", "image_height", ")", "\n", "br", "=", "crop", "(", "img", ",", "image_width", "-", "crop_width", ",", "image_height", "-", "crop_height", ",", "image_width", ",", "image_height", ")", "\n", "center", "=", "center_crop", "(", "img", ",", "(", "crop_height", ",", "crop_width", ")", ")", "\n", "\n", "return", "[", "tl", ",", "tr", ",", "bl", ",", "br", ",", "center", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.ten_crop": [[291, 312], ["warnings.warn", "functional_tensor._assert_image_tensor", "functional_tensor.five_crop", "functional_tensor.five_crop", "len", "functional_tensor.vflip", "functional_tensor.hflip"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.five_crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.five_crop", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.vflip", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.hflip"], ["", "def", "ten_crop", "(", "img", ":", "Tensor", ",", "size", ":", "BroadcastingList2", "[", "int", "]", ",", "vertical_flip", ":", "bool", "=", "False", ")", "->", "List", "[", "Tensor", "]", ":", "\n", "    ", "\"\"\"DEPRECATED\n    \"\"\"", "\n", "warnings", ".", "warn", "(", "\n", "\"This method is deprecated and will be removed in future releases. \"", "\n", "\"Please, use ``F.ten_crop`` instead.\"", "\n", ")", "\n", "\n", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "assert", "len", "(", "size", ")", "==", "2", ",", "\"Please provide only two dimensions (h, w) for size.\"", "\n", "first_five", "=", "five_crop", "(", "img", ",", "size", ")", "\n", "\n", "if", "vertical_flip", ":", "\n", "        ", "img", "=", "vflip", "(", "img", ")", "\n", "", "else", ":", "\n", "        ", "img", "=", "hflip", "(", "img", ")", "\n", "\n", "", "second_five", "=", "five_crop", "(", "img", ",", "size", ")", "\n", "\n", "return", "first_five", "+", "second_five", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._blend": [[314, 318], ["float", "img1.is_floating_point"], "function", ["None"], ["", "def", "_blend", "(", "img1", ":", "Tensor", ",", "img2", ":", "Tensor", ",", "ratio", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "ratio", "=", "float", "(", "ratio", ")", "\n", "bound", "=", "1.0", "if", "img1", ".", "is_floating_point", "(", ")", "else", "255.0", "\n", "return", "(", "ratio", "*", "img1", "+", "(", "1.0", "-", "ratio", ")", "*", "img2", ")", ".", "clamp", "(", "0", ",", "bound", ")", ".", "to", "(", "img1", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._rgb2hsv": [[320, 357], ["img.unbind", "torch.ones_like", "torch.where", "torch.fmod", "torch.stack", "torch.max", "torch.min", "torch.where"], "function", ["None"], ["", "def", "_rgb2hsv", "(", "img", ")", ":", "\n", "    ", "r", ",", "g", ",", "b", "=", "img", ".", "unbind", "(", "dim", "=", "-", "3", ")", "\n", "\n", "# Implementation is based on https://github.com/python-pillow/Pillow/blob/4174d4267616897df3746d315d5a2d0f82c656ee/", "\n", "# src/libImaging/Convert.c#L330", "\n", "maxc", "=", "torch", ".", "max", "(", "img", ",", "dim", "=", "-", "3", ")", ".", "values", "\n", "minc", "=", "torch", ".", "min", "(", "img", ",", "dim", "=", "-", "3", ")", ".", "values", "\n", "\n", "# The algorithm erases S and H channel where `maxc = minc`. This avoids NaN", "\n", "# from happening in the results, because", "\n", "#   + S channel has division by `maxc`, which is zero only if `maxc = minc`", "\n", "#   + H channel has division by `(maxc - minc)`.", "\n", "#", "\n", "# Instead of overwriting NaN afterwards, we just prevent it from occuring so", "\n", "# we don't need to deal with it in case we save the NaN in a buffer in", "\n", "# backprop, if it is ever supported, but it doesn't hurt to do so.", "\n", "eqc", "=", "maxc", "==", "minc", "\n", "\n", "cr", "=", "maxc", "-", "minc", "\n", "# Since `eqc => cr = 0`, replacing denominator with 1 when `eqc` is fine.", "\n", "ones", "=", "torch", ".", "ones_like", "(", "maxc", ")", "\n", "s", "=", "cr", "/", "torch", ".", "where", "(", "eqc", ",", "ones", ",", "maxc", ")", "\n", "# Note that `eqc => maxc = minc = r = g = b`. So the following calculation", "\n", "# of `h` would reduce to `bc - gc + 2 + rc - bc + 4 + rc - bc = 6` so it", "\n", "# would not matter what values `rc`, `gc`, and `bc` have here, and thus", "\n", "# replacing denominator with 1 when `eqc` is fine.", "\n", "cr_divisor", "=", "torch", ".", "where", "(", "eqc", ",", "ones", ",", "cr", ")", "\n", "rc", "=", "(", "maxc", "-", "r", ")", "/", "cr_divisor", "\n", "gc", "=", "(", "maxc", "-", "g", ")", "/", "cr_divisor", "\n", "bc", "=", "(", "maxc", "-", "b", ")", "/", "cr_divisor", "\n", "\n", "hr", "=", "(", "maxc", "==", "r", ")", "*", "(", "bc", "-", "gc", ")", "\n", "hg", "=", "(", "(", "maxc", "==", "g", ")", "&", "(", "maxc", "!=", "r", ")", ")", "*", "(", "2.0", "+", "rc", "-", "bc", ")", "\n", "hb", "=", "(", "(", "maxc", "!=", "g", ")", "&", "(", "maxc", "!=", "r", ")", ")", "*", "(", "4.0", "+", "gc", "-", "rc", ")", "\n", "h", "=", "(", "hr", "+", "hg", "+", "hb", ")", "\n", "h", "=", "torch", ".", "fmod", "(", "(", "h", "/", "6.0", "+", "1.0", ")", ",", "1.0", ")", "\n", "return", "torch", ".", "stack", "(", "(", "h", ",", "s", ",", "maxc", ")", ",", "dim", "=", "-", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._hsv2rgb": [[359, 378], ["img.unbind", "torch.floor", "i.to.to", "torch.clamp", "torch.clamp", "torch.clamp", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.einsum", "i.to.unsqueeze", "torch.arange().view", "mask.to", "torch.arange"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "_hsv2rgb", "(", "img", ")", ":", "\n", "    ", "h", ",", "s", ",", "v", "=", "img", ".", "unbind", "(", "dim", "=", "-", "3", ")", "\n", "i", "=", "torch", ".", "floor", "(", "h", "*", "6.0", ")", "\n", "f", "=", "(", "h", "*", "6.0", ")", "-", "i", "\n", "i", "=", "i", ".", "to", "(", "dtype", "=", "torch", ".", "int32", ")", "\n", "\n", "p", "=", "torch", ".", "clamp", "(", "(", "v", "*", "(", "1.0", "-", "s", ")", ")", ",", "0.0", ",", "1.0", ")", "\n", "q", "=", "torch", ".", "clamp", "(", "(", "v", "*", "(", "1.0", "-", "s", "*", "f", ")", ")", ",", "0.0", ",", "1.0", ")", "\n", "t", "=", "torch", ".", "clamp", "(", "(", "v", "*", "(", "1.0", "-", "s", "*", "(", "1.0", "-", "f", ")", ")", ")", ",", "0.0", ",", "1.0", ")", "\n", "i", "=", "i", "%", "6", "\n", "\n", "mask", "=", "i", ".", "unsqueeze", "(", "dim", "=", "-", "3", ")", "==", "torch", ".", "arange", "(", "6", ",", "device", "=", "i", ".", "device", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", "\n", "\n", "a1", "=", "torch", ".", "stack", "(", "(", "v", ",", "q", ",", "p", ",", "p", ",", "t", ",", "v", ")", ",", "dim", "=", "-", "3", ")", "\n", "a2", "=", "torch", ".", "stack", "(", "(", "t", ",", "v", ",", "v", ",", "q", ",", "p", ",", "p", ")", ",", "dim", "=", "-", "3", ")", "\n", "a3", "=", "torch", ".", "stack", "(", "(", "p", ",", "p", ",", "t", ",", "v", ",", "v", ",", "q", ")", ",", "dim", "=", "-", "3", ")", "\n", "a4", "=", "torch", ".", "stack", "(", "(", "a1", ",", "a2", ",", "a3", ")", ",", "dim", "=", "-", "4", ")", "\n", "\n", "return", "torch", ".", "einsum", "(", "\"...ijk, ...xijk -> ...xjk\"", ",", "mask", ".", "to", "(", "dtype", "=", "img", ".", "dtype", ")", ",", "a4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._pad_symmetric": [[380, 408], ["img.size", "torch.tensor", "torch.tensor", "max", "range", "range", "range", "range", "range", "range", "RuntimeError", "min"], "function", ["None"], ["", "def", "_pad_symmetric", "(", "img", ":", "Tensor", ",", "padding", ":", "List", "[", "int", "]", ")", "->", "Tensor", ":", "\n", "# padding is left, right, top, bottom", "\n", "\n", "# crop if needed", "\n", "    ", "if", "padding", "[", "0", "]", "<", "0", "or", "padding", "[", "1", "]", "<", "0", "or", "padding", "[", "2", "]", "<", "0", "or", "padding", "[", "3", "]", "<", "0", ":", "\n", "        ", "crop_left", ",", "crop_right", ",", "crop_top", ",", "crop_bottom", "=", "[", "-", "min", "(", "x", ",", "0", ")", "for", "x", "in", "padding", "]", "\n", "img", "=", "img", "[", "...", ",", "crop_top", ":", "img", ".", "shape", "[", "-", "2", "]", "-", "crop_bottom", ",", "crop_left", ":", "img", ".", "shape", "[", "-", "1", "]", "-", "crop_right", "]", "\n", "padding", "=", "[", "max", "(", "x", ",", "0", ")", "for", "x", "in", "padding", "]", "\n", "\n", "", "in_sizes", "=", "img", ".", "size", "(", ")", "\n", "\n", "x_indices", "=", "[", "i", "for", "i", "in", "range", "(", "in_sizes", "[", "-", "1", "]", ")", "]", "# [0, 1, 2, 3, ...]", "\n", "left_indices", "=", "[", "i", "for", "i", "in", "range", "(", "padding", "[", "0", "]", "-", "1", ",", "-", "1", ",", "-", "1", ")", "]", "# e.g. [3, 2, 1, 0]", "\n", "right_indices", "=", "[", "-", "(", "i", "+", "1", ")", "for", "i", "in", "range", "(", "padding", "[", "1", "]", ")", "]", "# e.g. [-1, -2, -3]", "\n", "x_indices", "=", "torch", ".", "tensor", "(", "left_indices", "+", "x_indices", "+", "right_indices", ",", "device", "=", "img", ".", "device", ")", "\n", "\n", "y_indices", "=", "[", "i", "for", "i", "in", "range", "(", "in_sizes", "[", "-", "2", "]", ")", "]", "\n", "top_indices", "=", "[", "i", "for", "i", "in", "range", "(", "padding", "[", "2", "]", "-", "1", ",", "-", "1", ",", "-", "1", ")", "]", "\n", "bottom_indices", "=", "[", "-", "(", "i", "+", "1", ")", "for", "i", "in", "range", "(", "padding", "[", "3", "]", ")", "]", "\n", "y_indices", "=", "torch", ".", "tensor", "(", "top_indices", "+", "y_indices", "+", "bottom_indices", ",", "device", "=", "img", ".", "device", ")", "\n", "\n", "ndim", "=", "img", ".", "ndim", "\n", "if", "ndim", "==", "3", ":", "\n", "        ", "return", "img", "[", ":", ",", "y_indices", "[", ":", ",", "None", "]", ",", "x_indices", "[", "None", ",", ":", "]", "]", "\n", "", "elif", "ndim", "==", "4", ":", "\n", "        ", "return", "img", "[", ":", ",", ":", ",", "y_indices", "[", ":", ",", "None", "]", ",", "x_indices", "[", "None", ",", ":", "]", "]", "\n", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Symmetric padding of N-D tensors are not supported yet\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad": [[410, 478], ["functional_tensor._assert_image_tensor", "isinstance", "isinstance", "torch.nn.functional.pad", "isinstance", "TypeError", "isinstance", "TypeError", "isinstance", "TypeError", "list", "isinstance", "ValueError", "ValueError", "torch.jit.is_scripting", "img.to.unsqueeze", "img.to.to", "img.to.squeeze", "img.to.to", "len", "ValueError", "len", "functional_tensor._pad_symmetric", "float", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._pad_symmetric"], ["", "", "def", "pad", "(", "img", ":", "Tensor", ",", "padding", ":", "List", "[", "int", "]", ",", "fill", ":", "int", "=", "0", ",", "padding_mode", ":", "str", "=", "\"constant\"", ")", "->", "Tensor", ":", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "if", "not", "isinstance", "(", "padding", ",", "(", "int", ",", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Got inappropriate padding arg\"", ")", "\n", "", "if", "not", "isinstance", "(", "fill", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Got inappropriate fill arg\"", ")", "\n", "", "if", "not", "isinstance", "(", "padding_mode", ",", "str", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Got inappropriate padding_mode arg\"", ")", "\n", "\n", "", "if", "isinstance", "(", "padding", ",", "tuple", ")", ":", "\n", "        ", "padding", "=", "list", "(", "padding", ")", "\n", "\n", "", "if", "isinstance", "(", "padding", ",", "list", ")", "and", "len", "(", "padding", ")", "not", "in", "[", "1", ",", "2", ",", "4", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Padding must be an int or a 1, 2, or 4 element tuple, not a \"", "+", "\n", "\"{} element tuple\"", ".", "format", "(", "len", "(", "padding", ")", ")", ")", "\n", "\n", "", "if", "padding_mode", "not", "in", "[", "\"constant\"", ",", "\"edge\"", ",", "\"reflect\"", ",", "\"symmetric\"", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Padding mode should be either constant, edge, reflect or symmetric\"", ")", "\n", "\n", "", "if", "isinstance", "(", "padding", ",", "int", ")", ":", "\n", "        ", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "# This maybe unreachable", "\n", "            ", "raise", "ValueError", "(", "\"padding can't be an int while torchscripting, set it as a list [value, ]\"", ")", "\n", "", "pad_left", "=", "pad_right", "=", "pad_top", "=", "pad_bottom", "=", "padding", "\n", "", "elif", "len", "(", "padding", ")", "==", "1", ":", "\n", "        ", "pad_left", "=", "pad_right", "=", "pad_top", "=", "pad_bottom", "=", "padding", "[", "0", "]", "\n", "", "elif", "len", "(", "padding", ")", "==", "2", ":", "\n", "        ", "pad_left", "=", "pad_right", "=", "padding", "[", "0", "]", "\n", "pad_top", "=", "pad_bottom", "=", "padding", "[", "1", "]", "\n", "", "else", ":", "\n", "        ", "pad_left", "=", "padding", "[", "0", "]", "\n", "pad_top", "=", "padding", "[", "1", "]", "\n", "pad_right", "=", "padding", "[", "2", "]", "\n", "pad_bottom", "=", "padding", "[", "3", "]", "\n", "\n", "", "p", "=", "[", "pad_left", ",", "pad_right", ",", "pad_top", ",", "pad_bottom", "]", "\n", "\n", "if", "padding_mode", "==", "\"edge\"", ":", "\n", "# remap padding_mode str", "\n", "        ", "padding_mode", "=", "\"replicate\"", "\n", "", "elif", "padding_mode", "==", "\"symmetric\"", ":", "\n", "# route to another implementation", "\n", "        ", "return", "_pad_symmetric", "(", "img", ",", "p", ")", "\n", "\n", "", "need_squeeze", "=", "False", "\n", "if", "img", ".", "ndim", "<", "4", ":", "\n", "        ", "img", "=", "img", ".", "unsqueeze", "(", "dim", "=", "0", ")", "\n", "need_squeeze", "=", "True", "\n", "\n", "", "out_dtype", "=", "img", ".", "dtype", "\n", "need_cast", "=", "False", "\n", "if", "(", "padding_mode", "!=", "\"constant\"", ")", "and", "img", ".", "dtype", "not", "in", "(", "torch", ".", "float32", ",", "torch", ".", "float64", ")", ":", "\n", "# Here we temporary cast input tensor to float", "\n", "# until pytorch issue is resolved :", "\n", "# https://github.com/pytorch/pytorch/issues/40763", "\n", "        ", "need_cast", "=", "True", "\n", "img", "=", "img", ".", "to", "(", "torch", ".", "float32", ")", "\n", "\n", "", "img", "=", "torch_pad", "(", "img", ",", "p", ",", "mode", "=", "padding_mode", ",", "value", "=", "float", "(", "fill", ")", ")", "\n", "\n", "if", "need_squeeze", ":", "\n", "        ", "img", "=", "img", ".", "squeeze", "(", "dim", "=", "0", ")", "\n", "\n", "", "if", "need_cast", ":", "\n", "        ", "img", "=", "img", ".", "to", "(", "out_dtype", ")", "\n", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.resize": [[480, 560], ["functional_tensor._assert_image_tensor", "isinstance", "isinstance", "functional_tensor._get_image_size", "functional_tensor._cast_squeeze_in", "functional_tensor._cast_squeeze_out", "isinstance", "TypeError", "isinstance", "TypeError", "ValueError", "list", "ValueError", "isinstance", "torch.nn.functional.interpolate", "torch.ops.torchvision._interpolate_bicubic2d_aa.clamp", "len", "ValueError", "ValueError", "len", "isinstance", "int", "torch.ops.torchvision._interpolate_bilinear2d_aa", "len", "ValueError", "torch.ops.torchvision._interpolate_bicubic2d_aa", "len", "int"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_size", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_in", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_out", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "def", "resize", "(", "\n", "img", ":", "Tensor", ",", "\n", "size", ":", "List", "[", "int", "]", ",", "\n", "interpolation", ":", "str", "=", "\"bilinear\"", ",", "\n", "max_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "antialias", ":", "Optional", "[", "bool", "]", "=", "None", "\n", ")", "->", "Tensor", ":", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "if", "not", "isinstance", "(", "size", ",", "(", "int", ",", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Got inappropriate size arg\"", ")", "\n", "", "if", "not", "isinstance", "(", "interpolation", ",", "str", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Got inappropriate interpolation arg\"", ")", "\n", "\n", "", "if", "interpolation", "not", "in", "[", "\"nearest\"", ",", "\"bilinear\"", ",", "\"bicubic\"", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"This interpolation mode is unsupported with Tensor input\"", ")", "\n", "\n", "", "if", "isinstance", "(", "size", ",", "tuple", ")", ":", "\n", "        ", "size", "=", "list", "(", "size", ")", "\n", "\n", "", "if", "isinstance", "(", "size", ",", "list", ")", ":", "\n", "        ", "if", "len", "(", "size", ")", "not", "in", "[", "1", ",", "2", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"Size must be an int or a 1 or 2 element tuple/list, not a \"", "\n", "\"{} element tuple/list\"", ".", "format", "(", "len", "(", "size", ")", ")", ")", "\n", "", "if", "max_size", "is", "not", "None", "and", "len", "(", "size", ")", "!=", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"max_size should only be passed if size specifies the length of the smaller edge, \"", "\n", "\"i.e. size should be an int or a sequence of length 1 in torchscript mode.\"", "\n", ")", "\n", "\n", "", "", "if", "antialias", "is", "None", ":", "\n", "        ", "antialias", "=", "False", "\n", "\n", "", "if", "antialias", "and", "interpolation", "not", "in", "[", "\"bilinear\"", ",", "\"bicubic\"", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Antialias option is supported for bilinear and bicubic interpolation modes only\"", ")", "\n", "\n", "", "w", ",", "h", "=", "_get_image_size", "(", "img", ")", "\n", "\n", "if", "isinstance", "(", "size", ",", "int", ")", "or", "len", "(", "size", ")", "==", "1", ":", "# specified size only for the smallest edge", "\n", "        ", "short", ",", "long", "=", "(", "w", ",", "h", ")", "if", "w", "<=", "h", "else", "(", "h", ",", "w", ")", "\n", "requested_new_short", "=", "size", "if", "isinstance", "(", "size", ",", "int", ")", "else", "size", "[", "0", "]", "\n", "\n", "if", "short", "==", "requested_new_short", ":", "\n", "            ", "return", "img", "\n", "\n", "", "new_short", ",", "new_long", "=", "requested_new_short", ",", "int", "(", "requested_new_short", "*", "long", "/", "short", ")", "\n", "\n", "if", "max_size", "is", "not", "None", ":", "\n", "            ", "if", "max_size", "<=", "requested_new_short", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"max_size = {max_size} must be strictly greater than the requested \"", "\n", "f\"size for the smaller edge size = {size}\"", "\n", ")", "\n", "", "if", "new_long", ">", "max_size", ":", "\n", "                ", "new_short", ",", "new_long", "=", "int", "(", "max_size", "*", "new_short", "/", "new_long", ")", ",", "max_size", "\n", "\n", "", "", "new_w", ",", "new_h", "=", "(", "new_short", ",", "new_long", ")", "if", "w", "<=", "h", "else", "(", "new_long", ",", "new_short", ")", "\n", "\n", "", "else", ":", "# specified both h and w", "\n", "        ", "new_w", ",", "new_h", "=", "size", "[", "1", "]", ",", "size", "[", "0", "]", "\n", "\n", "", "img", ",", "need_cast", ",", "need_squeeze", ",", "out_dtype", "=", "_cast_squeeze_in", "(", "img", ",", "[", "torch", ".", "float32", ",", "torch", ".", "float64", "]", ")", "\n", "\n", "# Define align_corners to avoid warnings", "\n", "align_corners", "=", "False", "if", "interpolation", "in", "[", "\"bilinear\"", ",", "\"bicubic\"", "]", "else", "None", "\n", "\n", "if", "antialias", ":", "\n", "        ", "if", "interpolation", "==", "\"bilinear\"", ":", "\n", "            ", "img", "=", "torch", ".", "ops", ".", "torchvision", ".", "_interpolate_bilinear2d_aa", "(", "img", ",", "[", "new_h", ",", "new_w", "]", ",", "align_corners", "=", "False", ")", "\n", "", "elif", "interpolation", "==", "\"bicubic\"", ":", "\n", "            ", "img", "=", "torch", ".", "ops", ".", "torchvision", ".", "_interpolate_bicubic2d_aa", "(", "img", ",", "[", "new_h", ",", "new_w", "]", ",", "align_corners", "=", "False", ")", "\n", "", "", "else", ":", "\n", "        ", "img", "=", "interpolate", "(", "img", ",", "size", "=", "[", "new_h", ",", "new_w", "]", ",", "mode", "=", "interpolation", ",", "align_corners", "=", "align_corners", ")", "\n", "\n", "", "if", "interpolation", "==", "\"bicubic\"", "and", "out_dtype", "==", "torch", ".", "uint8", ":", "\n", "        ", "img", "=", "img", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "255", ")", "\n", "\n", "", "img", "=", "_cast_squeeze_out", "(", "img", ",", "need_cast", "=", "need_cast", ",", "need_squeeze", "=", "need_squeeze", ",", "out_dtype", "=", "out_dtype", ")", "\n", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_grid_transform_inputs": [[562, 597], ["functional_tensor._assert_image_tensor", "functional_tensor._get_image_num_channels", "isinstance", "TypeError", "TypeError", "ValueError", "ValueError", "warnings.warn", "isinstance", "ValueError", "ValueError", "isinstance", "len", "len", "isinstance", "msg.format", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_image_num_channels"], ["", "def", "_assert_grid_transform_inputs", "(", "\n", "img", ":", "Tensor", ",", "\n", "matrix", ":", "Optional", "[", "List", "[", "float", "]", "]", ",", "\n", "interpolation", ":", "str", ",", "\n", "fill", ":", "Optional", "[", "List", "[", "float", "]", "]", ",", "\n", "supported_interpolation_modes", ":", "List", "[", "str", "]", ",", "\n", "coeffs", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", ",", "\n", ")", ":", "\n", "\n", "    ", "if", "not", "(", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Input img should be Tensor\"", ")", "\n", "\n", "", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "if", "matrix", "is", "not", "None", "and", "not", "isinstance", "(", "matrix", ",", "list", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Argument matrix should be a list\"", ")", "\n", "\n", "", "if", "matrix", "is", "not", "None", "and", "len", "(", "matrix", ")", "!=", "6", ":", "\n", "        ", "raise", "ValueError", "(", "\"Argument matrix should have 6 float values\"", ")", "\n", "\n", "", "if", "coeffs", "is", "not", "None", "and", "len", "(", "coeffs", ")", "!=", "8", ":", "\n", "        ", "raise", "ValueError", "(", "\"Argument coeffs should have 8 float values\"", ")", "\n", "\n", "", "if", "fill", "is", "not", "None", "and", "not", "isinstance", "(", "fill", ",", "(", "int", ",", "float", ",", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\"Argument fill should be either int, float, tuple or list\"", ")", "\n", "\n", "# Check fill", "\n", "", "num_channels", "=", "_get_image_num_channels", "(", "img", ")", "\n", "if", "isinstance", "(", "fill", ",", "(", "tuple", ",", "list", ")", ")", "and", "(", "len", "(", "fill", ")", ">", "1", "and", "len", "(", "fill", ")", "!=", "num_channels", ")", ":", "\n", "        ", "msg", "=", "(", "\"The number of elements in 'fill' cannot broadcast to match the number of \"", "\n", "\"channels of the image ({} != {})\"", ")", "\n", "raise", "ValueError", "(", "msg", ".", "format", "(", "len", "(", "fill", ")", ",", "num_channels", ")", ")", "\n", "\n", "", "if", "interpolation", "not", "in", "supported_interpolation_modes", ":", "\n", "        ", "raise", "ValueError", "(", "\"Interpolation mode '{}' is unsupported with Tensor input\"", ".", "format", "(", "interpolation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_in": [[599, 613], ["img.to.unsqueeze", "img.to.to"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "", "def", "_cast_squeeze_in", "(", "img", ":", "Tensor", ",", "req_dtypes", ":", "List", "[", "torch", ".", "dtype", "]", ")", "->", "Tuple", "[", "Tensor", ",", "bool", ",", "bool", ",", "torch", ".", "dtype", "]", ":", "\n", "    ", "need_squeeze", "=", "False", "\n", "# make image NCHW", "\n", "if", "img", ".", "ndim", "<", "4", ":", "\n", "        ", "img", "=", "img", ".", "unsqueeze", "(", "dim", "=", "0", ")", "\n", "need_squeeze", "=", "True", "\n", "\n", "", "out_dtype", "=", "img", ".", "dtype", "\n", "need_cast", "=", "False", "\n", "if", "out_dtype", "not", "in", "req_dtypes", ":", "\n", "        ", "need_cast", "=", "True", "\n", "req_dtype", "=", "req_dtypes", "[", "0", "]", "\n", "img", "=", "img", ".", "to", "(", "req_dtype", ")", "\n", "", "return", "img", ",", "need_cast", ",", "need_squeeze", ",", "out_dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_out": [[615, 626], ["torch.round.squeeze", "torch.round.to", "torch.round"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "_cast_squeeze_out", "(", "img", ":", "Tensor", ",", "need_cast", ":", "bool", ",", "need_squeeze", ":", "bool", ",", "out_dtype", ":", "torch", ".", "dtype", ")", ":", "\n", "    ", "if", "need_squeeze", ":", "\n", "        ", "img", "=", "img", ".", "squeeze", "(", "dim", "=", "0", ")", "\n", "\n", "", "if", "need_cast", ":", "\n", "        ", "if", "out_dtype", "in", "(", "torch", ".", "uint8", ",", "torch", ".", "int8", ",", "torch", ".", "int16", ",", "torch", ".", "int32", ",", "torch", ".", "int64", ")", ":", "\n", "# it is better to round before cast", "\n", "            ", "img", "=", "torch", ".", "round", "(", "img", ")", "\n", "", "img", "=", "img", ".", "to", "(", "out_dtype", ")", "\n", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._apply_grid_transform": [[628, 658], ["functional_tensor._cast_squeeze_in", "torch.nn.functional.grid_sample", "functional_tensor._cast_squeeze_out", "grid.expand.expand", "torch.ones", "torch.cat", "mask.expand_as.expand_as", "torch.tensor().view().expand_as", "isinstance", "len", "torch.tensor().view", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_in", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_out", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat"], ["", "def", "_apply_grid_transform", "(", "img", ":", "Tensor", ",", "grid", ":", "Tensor", ",", "mode", ":", "str", ",", "fill", ":", "Optional", "[", "List", "[", "float", "]", "]", ")", "->", "Tensor", ":", "\n", "\n", "    ", "img", ",", "need_cast", ",", "need_squeeze", ",", "out_dtype", "=", "_cast_squeeze_in", "(", "img", ",", "[", "grid", ".", "dtype", ",", "]", ")", "\n", "\n", "if", "img", ".", "shape", "[", "0", "]", ">", "1", ":", "\n", "# Apply same grid to a batch of images", "\n", "        ", "grid", "=", "grid", ".", "expand", "(", "img", ".", "shape", "[", "0", "]", ",", "grid", ".", "shape", "[", "1", "]", ",", "grid", ".", "shape", "[", "2", "]", ",", "grid", ".", "shape", "[", "3", "]", ")", "\n", "\n", "# Append a dummy mask for customized fill colors, should be faster than grid_sample() twice", "\n", "", "if", "fill", "is", "not", "None", ":", "\n", "        ", "dummy", "=", "torch", ".", "ones", "(", "(", "img", ".", "shape", "[", "0", "]", ",", "1", ",", "img", ".", "shape", "[", "2", "]", ",", "img", ".", "shape", "[", "3", "]", ")", ",", "dtype", "=", "img", ".", "dtype", ",", "device", "=", "img", ".", "device", ")", "\n", "img", "=", "torch", ".", "cat", "(", "(", "img", ",", "dummy", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "img", "=", "grid_sample", "(", "img", ",", "grid", ",", "mode", "=", "mode", ",", "padding_mode", "=", "\"zeros\"", ",", "align_corners", "=", "False", ")", "\n", "\n", "# Fill with required color", "\n", "if", "fill", "is", "not", "None", ":", "\n", "        ", "mask", "=", "img", "[", ":", ",", "-", "1", ":", ",", ":", ",", ":", "]", "# N * 1 * H * W", "\n", "img", "=", "img", "[", ":", ",", ":", "-", "1", ",", ":", ",", ":", "]", "# N * C * H * W", "\n", "mask", "=", "mask", ".", "expand_as", "(", "img", ")", "\n", "len_fill", "=", "len", "(", "fill", ")", "if", "isinstance", "(", "fill", ",", "(", "tuple", ",", "list", ")", ")", "else", "1", "\n", "fill_img", "=", "torch", ".", "tensor", "(", "fill", ",", "dtype", "=", "img", ".", "dtype", ",", "device", "=", "img", ".", "device", ")", ".", "view", "(", "1", ",", "len_fill", ",", "1", ",", "1", ")", ".", "expand_as", "(", "img", ")", "\n", "if", "mode", "==", "'nearest'", ":", "\n", "            ", "mask", "=", "mask", "<", "0.5", "\n", "img", "[", "mask", "]", "=", "fill_img", "[", "mask", "]", "\n", "", "else", ":", "# 'bilinear'", "\n", "            ", "img", "=", "img", "*", "mask", "+", "(", "1.0", "-", "mask", ")", "*", "fill_img", "\n", "\n", "", "", "img", "=", "_cast_squeeze_out", "(", "img", ",", "need_cast", ",", "need_squeeze", ",", "out_dtype", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._gen_affine_grid": [[660, 680], ["torch.empty", "torch.linspace", "base_grid[].copy_", "torch.linspace().unsqueeze_", "base_grid[].copy_", "base_grid[].fill_", "torch.empty.view().bmm", "base_grid.view().bmm.view", "theta.transpose", "torch.tensor", "torch.linspace", "torch.empty.view"], "function", ["None"], ["", "def", "_gen_affine_grid", "(", "\n", "theta", ":", "Tensor", ",", "w", ":", "int", ",", "h", ":", "int", ",", "ow", ":", "int", ",", "oh", ":", "int", ",", "\n", ")", "->", "Tensor", ":", "\n", "# https://github.com/pytorch/pytorch/blob/74b65c32be68b15dc7c9e8bb62459efbfbde33d8/aten/src/ATen/native/", "\n", "# AffineGridGenerator.cpp#L18", "\n", "# Difference with AffineGridGenerator is that:", "\n", "# 1) we normalize grid values after applying theta", "\n", "# 2) we can normalize by other image size, such that it covers \"extend\" option like in PIL.Image.rotate", "\n", "\n", "    ", "d", "=", "0.5", "\n", "base_grid", "=", "torch", ".", "empty", "(", "1", ",", "oh", ",", "ow", ",", "3", ",", "dtype", "=", "theta", ".", "dtype", ",", "device", "=", "theta", ".", "device", ")", "\n", "x_grid", "=", "torch", ".", "linspace", "(", "-", "ow", "*", "0.5", "+", "d", ",", "ow", "*", "0.5", "+", "d", "-", "1", ",", "steps", "=", "ow", ",", "device", "=", "theta", ".", "device", ")", "\n", "base_grid", "[", "...", ",", "0", "]", ".", "copy_", "(", "x_grid", ")", "\n", "y_grid", "=", "torch", ".", "linspace", "(", "-", "oh", "*", "0.5", "+", "d", ",", "oh", "*", "0.5", "+", "d", "-", "1", ",", "steps", "=", "oh", ",", "device", "=", "theta", ".", "device", ")", ".", "unsqueeze_", "(", "-", "1", ")", "\n", "base_grid", "[", "...", ",", "1", "]", ".", "copy_", "(", "y_grid", ")", "\n", "base_grid", "[", "...", ",", "2", "]", ".", "fill_", "(", "1", ")", "\n", "\n", "rescaled_theta", "=", "theta", ".", "transpose", "(", "1", ",", "2", ")", "/", "torch", ".", "tensor", "(", "[", "0.5", "*", "w", ",", "0.5", "*", "h", "]", ",", "dtype", "=", "theta", ".", "dtype", ",", "device", "=", "theta", ".", "device", ")", "\n", "output_grid", "=", "base_grid", ".", "view", "(", "1", ",", "oh", "*", "ow", ",", "3", ")", ".", "bmm", "(", "rescaled_theta", ")", "\n", "return", "output_grid", ".", "view", "(", "1", ",", "oh", ",", "ow", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.affine": [[682, 693], ["functional_tensor._assert_grid_transform_inputs", "torch.tensor().reshape", "functional_tensor._gen_affine_grid", "functional_tensor._apply_grid_transform", "torch.is_floating_point", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_grid_transform_inputs", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._gen_affine_grid", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._apply_grid_transform"], ["", "def", "affine", "(", "\n", "img", ":", "Tensor", ",", "matrix", ":", "List", "[", "float", "]", ",", "interpolation", ":", "str", "=", "\"nearest\"", ",", "fill", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", "\n", ")", "->", "Tensor", ":", "\n", "    ", "_assert_grid_transform_inputs", "(", "img", ",", "matrix", ",", "interpolation", ",", "fill", ",", "[", "\"nearest\"", ",", "\"bilinear\"", "]", ")", "\n", "\n", "dtype", "=", "img", ".", "dtype", "if", "torch", ".", "is_floating_point", "(", "img", ")", "else", "torch", ".", "float32", "\n", "theta", "=", "torch", ".", "tensor", "(", "matrix", ",", "dtype", "=", "dtype", ",", "device", "=", "img", ".", "device", ")", ".", "reshape", "(", "1", ",", "2", ",", "3", ")", "\n", "shape", "=", "img", ".", "shape", "\n", "# grid will be generated on the same device as theta and img", "\n", "grid", "=", "_gen_affine_grid", "(", "theta", ",", "w", "=", "shape", "[", "-", "1", "]", ",", "h", "=", "shape", "[", "-", "2", "]", ",", "ow", "=", "shape", "[", "-", "1", "]", ",", "oh", "=", "shape", "[", "-", "2", "]", ")", "\n", "return", "_apply_grid_transform", "(", "img", ",", "grid", ",", "interpolation", ",", "fill", "=", "fill", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._compute_output_size": [[695, 718], ["torch.tensor", "torch.tensor().reshape", "torch.tensor.view().bmm().view", "pts.view().bmm().view.min", "pts.view().bmm().view.max", "torch.ceil", "torch.floor", "int", "int", "torch.tensor", "torch.tensor.view().bmm", "torch.tensor().reshape.transpose", "torch.tensor.view"], "function", ["None"], ["", "def", "_compute_output_size", "(", "matrix", ":", "List", "[", "float", "]", ",", "w", ":", "int", ",", "h", ":", "int", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "\n", "# Inspired of PIL implementation:", "\n", "# https://github.com/python-pillow/Pillow/blob/11de3318867e4398057373ee9f12dcb33db7335c/src/PIL/Image.py#L2054", "\n", "\n", "# pts are Top-Left, Top-Right, Bottom-Left, Bottom-Right points.", "\n", "    ", "pts", "=", "torch", ".", "tensor", "(", "[", "\n", "[", "-", "0.5", "*", "w", ",", "-", "0.5", "*", "h", ",", "1.0", "]", ",", "\n", "[", "-", "0.5", "*", "w", ",", "0.5", "*", "h", ",", "1.0", "]", ",", "\n", "[", "0.5", "*", "w", ",", "0.5", "*", "h", ",", "1.0", "]", ",", "\n", "[", "0.5", "*", "w", ",", "-", "0.5", "*", "h", ",", "1.0", "]", ",", "\n", "]", ")", "\n", "theta", "=", "torch", ".", "tensor", "(", "matrix", ",", "dtype", "=", "torch", ".", "float", ")", ".", "reshape", "(", "1", ",", "2", ",", "3", ")", "\n", "new_pts", "=", "pts", ".", "view", "(", "1", ",", "4", ",", "3", ")", ".", "bmm", "(", "theta", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "view", "(", "4", ",", "2", ")", "\n", "min_vals", ",", "_", "=", "new_pts", ".", "min", "(", "dim", "=", "0", ")", "\n", "max_vals", ",", "_", "=", "new_pts", ".", "max", "(", "dim", "=", "0", ")", "\n", "\n", "# Truncate precision to 1e-4 to avoid ceil of Xe-15 to 1.0", "\n", "tol", "=", "1e-4", "\n", "cmax", "=", "torch", ".", "ceil", "(", "(", "max_vals", "/", "tol", ")", ".", "trunc_", "(", ")", "*", "tol", ")", "\n", "cmin", "=", "torch", ".", "floor", "(", "(", "min_vals", "/", "tol", ")", ".", "trunc_", "(", ")", "*", "tol", ")", "\n", "size", "=", "cmax", "-", "cmin", "\n", "return", "int", "(", "size", "[", "0", "]", ")", ",", "int", "(", "size", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.rotate": [[720, 733], ["functional_tensor._assert_grid_transform_inputs", "torch.tensor().reshape", "functional_tensor._gen_affine_grid", "functional_tensor._apply_grid_transform", "functional_tensor._compute_output_size", "torch.is_floating_point", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_grid_transform_inputs", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._gen_affine_grid", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._apply_grid_transform", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._compute_output_size"], ["", "def", "rotate", "(", "\n", "img", ":", "Tensor", ",", "matrix", ":", "List", "[", "float", "]", ",", "interpolation", ":", "str", "=", "\"nearest\"", ",", "\n", "expand", ":", "bool", "=", "False", ",", "fill", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", "\n", ")", "->", "Tensor", ":", "\n", "    ", "_assert_grid_transform_inputs", "(", "img", ",", "matrix", ",", "interpolation", ",", "fill", ",", "[", "\"nearest\"", ",", "\"bilinear\"", "]", ")", "\n", "w", ",", "h", "=", "img", ".", "shape", "[", "-", "1", "]", ",", "img", ".", "shape", "[", "-", "2", "]", "\n", "ow", ",", "oh", "=", "_compute_output_size", "(", "matrix", ",", "w", ",", "h", ")", "if", "expand", "else", "(", "w", ",", "h", ")", "\n", "dtype", "=", "img", ".", "dtype", "if", "torch", ".", "is_floating_point", "(", "img", ")", "else", "torch", ".", "float32", "\n", "theta", "=", "torch", ".", "tensor", "(", "matrix", ",", "dtype", "=", "dtype", ",", "device", "=", "img", ".", "device", ")", ".", "reshape", "(", "1", ",", "2", ",", "3", ")", "\n", "# grid will be generated on the same device as theta and img", "\n", "grid", "=", "_gen_affine_grid", "(", "theta", ",", "w", "=", "w", ",", "h", "=", "h", ",", "ow", "=", "ow", ",", "oh", "=", "oh", ")", "\n", "\n", "return", "_apply_grid_transform", "(", "img", ",", "grid", ",", "interpolation", ",", "fill", "=", "fill", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._perspective_grid": [[735, 766], ["torch.tensor", "torch.tensor", "torch.empty", "torch.linspace", "base_grid[].copy_", "torch.linspace().unsqueeze_", "base_grid[].copy_", "base_grid[].fill_", "torch.empty.view().bmm", "torch.empty.view().bmm", "output_grid.view", "torch.tensor.transpose", "torch.tensor", "torch.tensor.transpose", "torch.linspace", "torch.empty.view", "torch.empty.view"], "function", ["None"], ["", "def", "_perspective_grid", "(", "coeffs", ":", "List", "[", "float", "]", ",", "ow", ":", "int", ",", "oh", ":", "int", ",", "dtype", ":", "torch", ".", "dtype", ",", "device", ":", "torch", ".", "device", ")", ":", "\n", "# https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/", "\n", "# src/libImaging/Geometry.c#L394", "\n", "\n", "#", "\n", "# x_out = (coeffs[0] * x + coeffs[1] * y + coeffs[2]) / (coeffs[6] * x + coeffs[7] * y + 1)", "\n", "# y_out = (coeffs[3] * x + coeffs[4] * y + coeffs[5]) / (coeffs[6] * x + coeffs[7] * y + 1)", "\n", "#", "\n", "    ", "theta1", "=", "torch", ".", "tensor", "(", "[", "[", "\n", "[", "coeffs", "[", "0", "]", ",", "coeffs", "[", "1", "]", ",", "coeffs", "[", "2", "]", "]", ",", "\n", "[", "coeffs", "[", "3", "]", ",", "coeffs", "[", "4", "]", ",", "coeffs", "[", "5", "]", "]", "\n", "]", "]", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "theta2", "=", "torch", ".", "tensor", "(", "[", "[", "\n", "[", "coeffs", "[", "6", "]", ",", "coeffs", "[", "7", "]", ",", "1.0", "]", ",", "\n", "[", "coeffs", "[", "6", "]", ",", "coeffs", "[", "7", "]", ",", "1.0", "]", "\n", "]", "]", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "\n", "d", "=", "0.5", "\n", "base_grid", "=", "torch", ".", "empty", "(", "1", ",", "oh", ",", "ow", ",", "3", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "x_grid", "=", "torch", ".", "linspace", "(", "d", ",", "ow", "*", "1.0", "+", "d", "-", "1.0", ",", "steps", "=", "ow", ",", "device", "=", "device", ")", "\n", "base_grid", "[", "...", ",", "0", "]", ".", "copy_", "(", "x_grid", ")", "\n", "y_grid", "=", "torch", ".", "linspace", "(", "d", ",", "oh", "*", "1.0", "+", "d", "-", "1.0", ",", "steps", "=", "oh", ",", "device", "=", "device", ")", ".", "unsqueeze_", "(", "-", "1", ")", "\n", "base_grid", "[", "...", ",", "1", "]", ".", "copy_", "(", "y_grid", ")", "\n", "base_grid", "[", "...", ",", "2", "]", ".", "fill_", "(", "1", ")", "\n", "\n", "rescaled_theta1", "=", "theta1", ".", "transpose", "(", "1", ",", "2", ")", "/", "torch", ".", "tensor", "(", "[", "0.5", "*", "ow", ",", "0.5", "*", "oh", "]", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "output_grid1", "=", "base_grid", ".", "view", "(", "1", ",", "oh", "*", "ow", ",", "3", ")", ".", "bmm", "(", "rescaled_theta1", ")", "\n", "output_grid2", "=", "base_grid", ".", "view", "(", "1", ",", "oh", "*", "ow", ",", "3", ")", ".", "bmm", "(", "theta2", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "output_grid", "=", "output_grid1", "/", "output_grid2", "-", "1.0", "\n", "return", "output_grid", ".", "view", "(", "1", ",", "oh", ",", "ow", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.perspective": [[768, 789], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_grid_transform_inputs", "functional_tensor._perspective_grid", "functional_tensor._apply_grid_transform", "isinstance", "TypeError", "torch.is_floating_point"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_grid_transform_inputs", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._perspective_grid", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._apply_grid_transform"], ["", "def", "perspective", "(", "\n", "img", ":", "Tensor", ",", "perspective_coeffs", ":", "List", "[", "float", "]", ",", "interpolation", ":", "str", "=", "\"bilinear\"", ",", "fill", ":", "Optional", "[", "List", "[", "float", "]", "]", "=", "None", "\n", ")", "->", "Tensor", ":", "\n", "    ", "if", "not", "(", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'Input img should be Tensor.'", ")", "\n", "\n", "", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "_assert_grid_transform_inputs", "(", "\n", "img", ",", "\n", "matrix", "=", "None", ",", "\n", "interpolation", "=", "interpolation", ",", "\n", "fill", "=", "fill", ",", "\n", "supported_interpolation_modes", "=", "[", "\"nearest\"", ",", "\"bilinear\"", "]", ",", "\n", "coeffs", "=", "perspective_coeffs", "\n", ")", "\n", "\n", "ow", ",", "oh", "=", "img", ".", "shape", "[", "-", "1", "]", ",", "img", ".", "shape", "[", "-", "2", "]", "\n", "dtype", "=", "img", ".", "dtype", "if", "torch", ".", "is_floating_point", "(", "img", ")", "else", "torch", ".", "float32", "\n", "grid", "=", "_perspective_grid", "(", "perspective_coeffs", ",", "ow", "=", "ow", ",", "oh", "=", "oh", ",", "dtype", "=", "dtype", ",", "device", "=", "img", ".", "device", ")", "\n", "return", "_apply_grid_transform", "(", "img", ",", "grid", ",", "interpolation", ",", "fill", "=", "fill", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_gaussian_kernel1d": [[791, 799], ["torch.linspace", "torch.exp", "torch.exp.sum"], "function", ["None"], ["", "def", "_get_gaussian_kernel1d", "(", "kernel_size", ":", "int", ",", "sigma", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "ksize_half", "=", "(", "kernel_size", "-", "1", ")", "*", "0.5", "\n", "\n", "x", "=", "torch", ".", "linspace", "(", "-", "ksize_half", ",", "ksize_half", ",", "steps", "=", "kernel_size", ")", "\n", "pdf", "=", "torch", ".", "exp", "(", "-", "0.5", "*", "(", "x", "/", "sigma", ")", ".", "pow", "(", "2", ")", ")", "\n", "kernel1d", "=", "pdf", "/", "pdf", ".", "sum", "(", ")", "\n", "\n", "return", "kernel1d", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_gaussian_kernel2d": [[801, 808], ["_get_gaussian_kernel1d().to", "_get_gaussian_kernel1d().to", "torch.mm", "functional_tensor._get_gaussian_kernel1d", "functional_tensor._get_gaussian_kernel1d"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_gaussian_kernel1d", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_gaussian_kernel1d"], ["", "def", "_get_gaussian_kernel2d", "(", "\n", "kernel_size", ":", "List", "[", "int", "]", ",", "sigma", ":", "List", "[", "float", "]", ",", "dtype", ":", "torch", ".", "dtype", ",", "device", ":", "torch", ".", "device", "\n", ")", "->", "Tensor", ":", "\n", "    ", "kernel1d_x", "=", "_get_gaussian_kernel1d", "(", "kernel_size", "[", "0", "]", ",", "sigma", "[", "0", "]", ")", ".", "to", "(", "device", ",", "dtype", "=", "dtype", ")", "\n", "kernel1d_y", "=", "_get_gaussian_kernel1d", "(", "kernel_size", "[", "1", "]", ",", "sigma", "[", "1", "]", ")", ".", "to", "(", "device", ",", "dtype", "=", "dtype", ")", "\n", "kernel2d", "=", "torch", ".", "mm", "(", "kernel1d_y", "[", ":", ",", "None", "]", ",", "kernel1d_x", "[", "None", ",", ":", "]", ")", "\n", "return", "kernel2d", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.gaussian_blur": [[810, 829], ["functional_tensor._assert_image_tensor", "functional_tensor._get_gaussian_kernel2d", "kernel.expand.expand", "functional_tensor._cast_squeeze_in", "torch.nn.functional.pad", "torch.nn.functional.conv2d", "functional_tensor._cast_squeeze_out", "isinstance", "TypeError", "torch.is_floating_point", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._get_gaussian_kernel2d", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_in", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_out"], ["", "def", "gaussian_blur", "(", "img", ":", "Tensor", ",", "kernel_size", ":", "List", "[", "int", "]", ",", "sigma", ":", "List", "[", "float", "]", ")", "->", "Tensor", ":", "\n", "    ", "if", "not", "(", "isinstance", "(", "img", ",", "torch", ".", "Tensor", ")", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'img should be Tensor. Got {}'", ".", "format", "(", "type", "(", "img", ")", ")", ")", "\n", "\n", "", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "dtype", "=", "img", ".", "dtype", "if", "torch", ".", "is_floating_point", "(", "img", ")", "else", "torch", ".", "float32", "\n", "kernel", "=", "_get_gaussian_kernel2d", "(", "kernel_size", ",", "sigma", ",", "dtype", "=", "dtype", ",", "device", "=", "img", ".", "device", ")", "\n", "kernel", "=", "kernel", ".", "expand", "(", "img", ".", "shape", "[", "-", "3", "]", ",", "1", ",", "kernel", ".", "shape", "[", "0", "]", ",", "kernel", ".", "shape", "[", "1", "]", ")", "\n", "\n", "img", ",", "need_cast", ",", "need_squeeze", ",", "out_dtype", "=", "_cast_squeeze_in", "(", "img", ",", "[", "kernel", ".", "dtype", ",", "]", ")", "\n", "\n", "# padding = (left, right, top, bottom)", "\n", "padding", "=", "[", "kernel_size", "[", "0", "]", "//", "2", ",", "kernel_size", "[", "0", "]", "//", "2", ",", "kernel_size", "[", "1", "]", "//", "2", ",", "kernel_size", "[", "1", "]", "//", "2", "]", "\n", "img", "=", "torch_pad", "(", "img", ",", "padding", ",", "mode", "=", "\"reflect\"", ")", "\n", "img", "=", "conv2d", "(", "img", ",", "kernel", ",", "groups", "=", "img", ".", "shape", "[", "-", "3", "]", ")", "\n", "\n", "img", "=", "_cast_squeeze_out", "(", "img", ",", "need_cast", ",", "need_squeeze", ",", "out_dtype", ")", "\n", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.invert": [[831, 842], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "torch.tensor", "TypeError", "img.is_floating_point"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels"], ["", "def", "invert", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "if", "img", ".", "ndim", "<", "3", ":", "\n", "        ", "raise", "TypeError", "(", "\"Input image tensor should have at least 3 dimensions, but found {}\"", ".", "format", "(", "img", ".", "ndim", ")", ")", "\n", "\n", "", "_assert_channels", "(", "img", ",", "[", "1", ",", "3", "]", ")", "\n", "\n", "bound", "=", "torch", ".", "tensor", "(", "1", "if", "img", ".", "is_floating_point", "(", ")", "else", "255", ",", "dtype", "=", "img", ".", "dtype", ",", "device", "=", "img", ".", "device", ")", "\n", "return", "bound", "-", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.posterize": [[844, 856], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "TypeError", "TypeError", "int"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels"], ["", "def", "posterize", "(", "img", ":", "Tensor", ",", "bits", ":", "int", ")", "->", "Tensor", ":", "\n", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "if", "img", ".", "ndim", "<", "3", ":", "\n", "        ", "raise", "TypeError", "(", "\"Input image tensor should have at least 3 dimensions, but found {}\"", ".", "format", "(", "img", ".", "ndim", ")", ")", "\n", "", "if", "img", ".", "dtype", "!=", "torch", ".", "uint8", ":", "\n", "        ", "raise", "TypeError", "(", "\"Only torch.uint8 image tensors are supported, but found {}\"", ".", "format", "(", "img", ".", "dtype", ")", ")", "\n", "\n", "", "_assert_channels", "(", "img", ",", "[", "1", ",", "3", "]", ")", "\n", "mask", "=", "-", "int", "(", "2", "**", "(", "8", "-", "bits", ")", ")", "# JIT-friendly for: ~(2 ** (8 - bits) - 1)", "\n", "return", "img", "&", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.solarize": [[858, 869], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "functional_tensor.invert", "torch.where", "TypeError"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.invert"], ["", "def", "solarize", "(", "img", ":", "Tensor", ",", "threshold", ":", "float", ")", "->", "Tensor", ":", "\n", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "if", "img", ".", "ndim", "<", "3", ":", "\n", "        ", "raise", "TypeError", "(", "\"Input image tensor should have at least 3 dimensions, but found {}\"", ".", "format", "(", "img", ".", "ndim", ")", ")", "\n", "\n", "", "_assert_channels", "(", "img", ",", "[", "1", ",", "3", "]", ")", "\n", "\n", "inverted_img", "=", "invert", "(", "img", ")", "\n", "return", "torch", ".", "where", "(", "img", ">=", "threshold", ",", "inverted_img", ",", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._blurred_degenerate_image": [[871, 887], ["torch.ones", "kernel.expand.sum", "kernel.expand.expand", "functional_tensor._cast_squeeze_in", "torch.nn.functional.conv2d", "functional_tensor._cast_squeeze_out", "img.clone", "torch.is_floating_point"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_in", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.regnet.conv2d", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._cast_squeeze_out", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.clone"], ["", "def", "_blurred_degenerate_image", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "dtype", "=", "img", ".", "dtype", "if", "torch", ".", "is_floating_point", "(", "img", ")", "else", "torch", ".", "float32", "\n", "\n", "kernel", "=", "torch", ".", "ones", "(", "(", "3", ",", "3", ")", ",", "dtype", "=", "dtype", ",", "device", "=", "img", ".", "device", ")", "\n", "kernel", "[", "1", ",", "1", "]", "=", "5.0", "\n", "kernel", "/=", "kernel", ".", "sum", "(", ")", "\n", "kernel", "=", "kernel", ".", "expand", "(", "img", ".", "shape", "[", "-", "3", "]", ",", "1", ",", "kernel", ".", "shape", "[", "0", "]", ",", "kernel", ".", "shape", "[", "1", "]", ")", "\n", "\n", "result_tmp", ",", "need_cast", ",", "need_squeeze", ",", "out_dtype", "=", "_cast_squeeze_in", "(", "img", ",", "[", "kernel", ".", "dtype", ",", "]", ")", "\n", "result_tmp", "=", "conv2d", "(", "result_tmp", ",", "kernel", ",", "groups", "=", "result_tmp", ".", "shape", "[", "-", "3", "]", ")", "\n", "result_tmp", "=", "_cast_squeeze_out", "(", "result_tmp", ",", "need_cast", ",", "need_squeeze", ",", "out_dtype", ")", "\n", "\n", "result", "=", "img", ".", "clone", "(", ")", "\n", "result", "[", "...", ",", "1", ":", "-", "1", ",", "1", ":", "-", "1", "]", "=", "result_tmp", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.adjust_sharpness": [[889, 901], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "functional_tensor._blend", "ValueError", "functional_tensor._blurred_degenerate_image", "img.size", "img.size"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._blend", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._blurred_degenerate_image"], ["", "def", "adjust_sharpness", "(", "img", ":", "Tensor", ",", "sharpness_factor", ":", "float", ")", "->", "Tensor", ":", "\n", "    ", "if", "sharpness_factor", "<", "0", ":", "\n", "        ", "raise", "ValueError", "(", "'sharpness_factor ({}) is not non-negative.'", ".", "format", "(", "sharpness_factor", ")", ")", "\n", "\n", "", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "_assert_channels", "(", "img", ",", "[", "1", ",", "3", "]", ")", "\n", "\n", "if", "img", ".", "size", "(", "-", "1", ")", "<=", "2", "or", "img", ".", "size", "(", "-", "2", ")", "<=", "2", ":", "\n", "        ", "return", "img", "\n", "\n", "", "return", "_blend", "(", "img", ",", "_blurred_degenerate_image", "(", "img", ")", ",", "sharpness_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.autocontrast": [[903, 923], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "img.amin().to", "img.amax().to", "TypeError", "img.is_floating_point", "torch.is_floating_point", "torch.where", "img.amin", "img.amax"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "autocontrast", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "if", "img", ".", "ndim", "<", "3", ":", "\n", "        ", "raise", "TypeError", "(", "\"Input image tensor should have at least 3 dimensions, but found {}\"", ".", "format", "(", "img", ".", "ndim", ")", ")", "\n", "\n", "", "_assert_channels", "(", "img", ",", "[", "1", ",", "3", "]", ")", "\n", "\n", "bound", "=", "1.0", "if", "img", ".", "is_floating_point", "(", ")", "else", "255.0", "\n", "dtype", "=", "img", ".", "dtype", "if", "torch", ".", "is_floating_point", "(", "img", ")", "else", "torch", ".", "float32", "\n", "\n", "minimum", "=", "img", ".", "amin", "(", "dim", "=", "(", "-", "2", ",", "-", "1", ")", ",", "keepdim", "=", "True", ")", ".", "to", "(", "dtype", ")", "\n", "maximum", "=", "img", ".", "amax", "(", "dim", "=", "(", "-", "2", ",", "-", "1", ")", ",", "keepdim", "=", "True", ")", ".", "to", "(", "dtype", ")", "\n", "eq_idxs", "=", "torch", ".", "where", "(", "minimum", "==", "maximum", ")", "[", "0", "]", "\n", "minimum", "[", "eq_idxs", "]", "=", "0", "\n", "maximum", "[", "eq_idxs", "]", "=", "bound", "\n", "scale", "=", "bound", "/", "(", "maximum", "-", "minimum", ")", "\n", "\n", "return", "(", "(", "img", "-", "minimum", ")", "*", "scale", ")", ".", "clamp", "(", "0", ",", "bound", ")", ".", "to", "(", "img", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._scale_channel": [[925, 946], ["torch.div", "torch.div", "[].clamp", "lut[].to", "torch.histc", "torch.bincount", "nonzero_hist[].sum", "img_chan.to", "img_chan.view", "torch.cumsum", "torch.div", "torch.nn.functional.pad", "img_chan.to"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.pad", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "_scale_channel", "(", "img_chan", ")", ":", "\n", "# TODO: we should expect bincount to always be faster than histc, but this", "\n", "# isn't always the case. Once", "\n", "# https://github.com/pytorch/pytorch/issues/53194 is fixed, remove the if", "\n", "# block and only use bincount.", "\n", "    ", "if", "img_chan", ".", "is_cuda", ":", "\n", "        ", "hist", "=", "torch", ".", "histc", "(", "img_chan", ".", "to", "(", "torch", ".", "float32", ")", ",", "bins", "=", "256", ",", "min", "=", "0", ",", "max", "=", "255", ")", "\n", "", "else", ":", "\n", "        ", "hist", "=", "torch", ".", "bincount", "(", "img_chan", ".", "view", "(", "-", "1", ")", ",", "minlength", "=", "256", ")", "\n", "\n", "", "nonzero_hist", "=", "hist", "[", "hist", "!=", "0", "]", "\n", "step", "=", "torch", ".", "div", "(", "nonzero_hist", "[", ":", "-", "1", "]", ".", "sum", "(", ")", ",", "255", ",", "rounding_mode", "=", "'floor'", ")", "\n", "if", "step", "==", "0", ":", "\n", "        ", "return", "img_chan", "\n", "\n", "", "lut", "=", "torch", ".", "div", "(", "\n", "torch", ".", "cumsum", "(", "hist", ",", "0", ")", "+", "torch", ".", "div", "(", "step", ",", "2", ",", "rounding_mode", "=", "'floor'", ")", ",", "\n", "step", ",", "rounding_mode", "=", "'floor'", ")", "\n", "lut", "=", "torch", ".", "nn", ".", "functional", ".", "pad", "(", "lut", ",", "[", "1", ",", "0", "]", ")", "[", ":", "-", "1", "]", ".", "clamp", "(", "0", ",", "255", ")", "\n", "\n", "return", "lut", "[", "img_chan", ".", "to", "(", "torch", ".", "int64", ")", "]", ".", "to", "(", "torch", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._equalize_single_image": [[948, 950], ["torch.stack", "functional_tensor._scale_channel", "range", "img.size"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._scale_channel"], ["", "def", "_equalize_single_image", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "    ", "return", "torch", ".", "stack", "(", "[", "_scale_channel", "(", "img", "[", "c", "]", ")", "for", "c", "in", "range", "(", "img", ".", "size", "(", "0", ")", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor.equalize": [[952, 967], ["functional_tensor._assert_image_tensor", "functional_tensor._assert_channels", "torch.stack", "TypeError", "TypeError", "functional_tensor._equalize_single_image", "functional_tensor._equalize_single_image"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_image_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._assert_channels", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._equalize_single_image", "home.repos.pwc.inspect_result.microsoft_regionclip.torchvision_transforms.functional_tensor._equalize_single_image"], ["", "def", "equalize", "(", "img", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "\n", "    ", "_assert_image_tensor", "(", "img", ")", "\n", "\n", "if", "not", "(", "3", "<=", "img", ".", "ndim", "<=", "4", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\"Input image tensor should have 3 or 4 dimensions, but found {}\"", ".", "format", "(", "img", ".", "ndim", ")", ")", "\n", "", "if", "img", ".", "dtype", "!=", "torch", ".", "uint8", ":", "\n", "        ", "raise", "TypeError", "(", "\"Only torch.uint8 image tensors are supported, but found {}\"", ".", "format", "(", "img", ".", "dtype", ")", ")", "\n", "\n", "", "_assert_channels", "(", "img", ",", "[", "1", ",", "3", "]", ")", "\n", "\n", "if", "img", ".", "ndim", "==", "3", ":", "\n", "        ", "return", "_equalize_single_image", "(", "img", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "[", "_equalize_single_image", "(", "x", ")", "for", "x", "in", "img", "]", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.fast_eval_api.COCOeval_opt.evaluate": [[19, 96], ["time.time", "logger.info", "list", "sorted", "fast_eval_api.COCOeval_opt._prepare", "detectron2._C.COCOevalEvaluateImages", "copy.deepcopy", "time.time", "logger.info", "numpy.unique", "list", "computeIoU", "numpy.unique", "detectron2._C.InstanceAnnotation", "instances_cpp.append", "fast_eval_api.COCOeval_opt.evaluate.convert_instances_to_cpp"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.computeIoU"], ["def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Run per image evaluation on given images and store results in self.evalImgs_cpp, a\n        datastructure that isn't readable from Python but is used by a c++ implementation of\n        accumulate().  Unlike the original COCO PythonAPI, we don't populate the datastructure\n        self.evalImgs because this datastructure is a computational bottleneck.\n        :return: None\n        \"\"\"", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "\n", "p", "=", "self", ".", "params", "\n", "# add backward compatibility if useSegm is specified in params", "\n", "if", "p", ".", "useSegm", "is", "not", "None", ":", "\n", "            ", "p", ".", "iouType", "=", "\"segm\"", "if", "p", ".", "useSegm", "==", "1", "else", "\"bbox\"", "\n", "", "logger", ".", "info", "(", "\"Evaluate annotation type *{}*\"", ".", "format", "(", "p", ".", "iouType", ")", ")", "\n", "p", ".", "imgIds", "=", "list", "(", "np", ".", "unique", "(", "p", ".", "imgIds", ")", ")", "\n", "if", "p", ".", "useCats", ":", "\n", "            ", "p", ".", "catIds", "=", "list", "(", "np", ".", "unique", "(", "p", ".", "catIds", ")", ")", "\n", "", "p", ".", "maxDets", "=", "sorted", "(", "p", ".", "maxDets", ")", "\n", "self", ".", "params", "=", "p", "\n", "\n", "self", ".", "_prepare", "(", ")", "# bottleneck", "\n", "\n", "# loop through images, area range, max detection number", "\n", "catIds", "=", "p", ".", "catIds", "if", "p", ".", "useCats", "else", "[", "-", "1", "]", "\n", "\n", "if", "p", ".", "iouType", "==", "\"segm\"", "or", "p", ".", "iouType", "==", "\"bbox\"", ":", "\n", "            ", "computeIoU", "=", "self", ".", "computeIoU", "\n", "", "elif", "p", ".", "iouType", "==", "\"keypoints\"", ":", "\n", "            ", "computeIoU", "=", "self", ".", "computeOks", "\n", "", "self", ".", "ious", "=", "{", "\n", "(", "imgId", ",", "catId", ")", ":", "computeIoU", "(", "imgId", ",", "catId", ")", "for", "imgId", "in", "p", ".", "imgIds", "for", "catId", "in", "catIds", "\n", "}", "# bottleneck", "\n", "\n", "maxDet", "=", "p", ".", "maxDets", "[", "-", "1", "]", "\n", "\n", "# <<<< Beginning of code differences with original COCO API", "\n", "def", "convert_instances_to_cpp", "(", "instances", ",", "is_det", "=", "False", ")", ":", "\n", "# Convert annotations for a list of instances in an image to a format that's fast", "\n", "# to access in C++", "\n", "            ", "instances_cpp", "=", "[", "]", "\n", "for", "instance", "in", "instances", ":", "\n", "                ", "instance_cpp", "=", "_C", ".", "InstanceAnnotation", "(", "\n", "int", "(", "instance", "[", "\"id\"", "]", ")", ",", "\n", "instance", "[", "\"score\"", "]", "if", "is_det", "else", "instance", ".", "get", "(", "\"score\"", ",", "0.0", ")", ",", "\n", "instance", "[", "\"area\"", "]", ",", "\n", "bool", "(", "instance", ".", "get", "(", "\"iscrowd\"", ",", "0", ")", ")", ",", "\n", "bool", "(", "instance", ".", "get", "(", "\"ignore\"", ",", "0", ")", ")", ",", "\n", ")", "\n", "instances_cpp", ".", "append", "(", "instance_cpp", ")", "\n", "", "return", "instances_cpp", "\n", "\n", "# Convert GT annotations, detections, and IOUs to a format that's fast to access in C++", "\n", "", "ground_truth_instances", "=", "[", "\n", "[", "convert_instances_to_cpp", "(", "self", ".", "_gts", "[", "imgId", ",", "catId", "]", ")", "for", "catId", "in", "p", ".", "catIds", "]", "\n", "for", "imgId", "in", "p", ".", "imgIds", "\n", "]", "\n", "detected_instances", "=", "[", "\n", "[", "convert_instances_to_cpp", "(", "self", ".", "_dts", "[", "imgId", ",", "catId", "]", ",", "is_det", "=", "True", ")", "for", "catId", "in", "p", ".", "catIds", "]", "\n", "for", "imgId", "in", "p", ".", "imgIds", "\n", "]", "\n", "ious", "=", "[", "[", "self", ".", "ious", "[", "imgId", ",", "catId", "]", "for", "catId", "in", "catIds", "]", "for", "imgId", "in", "p", ".", "imgIds", "]", "\n", "\n", "if", "not", "p", ".", "useCats", ":", "\n", "# For each image, flatten per-category lists into a single list", "\n", "            ", "ground_truth_instances", "=", "[", "[", "[", "o", "for", "c", "in", "i", "for", "o", "in", "c", "]", "]", "for", "i", "in", "ground_truth_instances", "]", "\n", "detected_instances", "=", "[", "[", "[", "o", "for", "c", "in", "i", "for", "o", "in", "c", "]", "]", "for", "i", "in", "detected_instances", "]", "\n", "\n", "# Call C++ implementation of self.evaluateImgs()", "\n", "", "self", ".", "_evalImgs_cpp", "=", "_C", ".", "COCOevalEvaluateImages", "(", "\n", "p", ".", "areaRng", ",", "maxDet", ",", "p", ".", "iouThrs", ",", "ious", ",", "ground_truth_instances", ",", "detected_instances", "\n", ")", "\n", "self", ".", "_evalImgs", "=", "None", "\n", "\n", "self", ".", "_paramsEval", "=", "copy", ".", "deepcopy", "(", "self", ".", "params", ")", "\n", "toc", "=", "time", ".", "time", "(", ")", "\n", "logger", ".", "info", "(", "\"COCOeval_opt.evaluate() finished in {:0.2f} seconds.\"", ".", "format", "(", "toc", "-", "tic", ")", ")", "\n", "# >>>> End of code differences with original COCO API", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.fast_eval_api.COCOeval_opt.accumulate": [[98, 122], ["logger.info", "time.time", "hasattr", "detectron2._C.COCOevalAccumulate", "numpy.array().reshape", "numpy.array().reshape", "numpy.array().reshape", "time.time", "logger.info", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "accumulate", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Accumulate per image evaluation results and store the result in self.eval.  Does not\n        support changing parameter settings from those used by self.evaluate()\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Accumulating evaluation results...\"", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "assert", "hasattr", "(", "\n", "self", ",", "\"_evalImgs_cpp\"", "\n", ")", ",", "\"evaluate() must be called before accmulate() is called.\"", "\n", "\n", "self", ".", "eval", "=", "_C", ".", "COCOevalAccumulate", "(", "self", ".", "_paramsEval", ",", "self", ".", "_evalImgs_cpp", ")", "\n", "\n", "# recall is num_iou_thresholds X num_categories X num_area_ranges X num_max_detections", "\n", "self", ".", "eval", "[", "\"recall\"", "]", "=", "np", ".", "array", "(", "self", ".", "eval", "[", "\"recall\"", "]", ")", ".", "reshape", "(", "\n", "self", ".", "eval", "[", "\"counts\"", "]", "[", ":", "1", "]", "+", "self", ".", "eval", "[", "\"counts\"", "]", "[", "2", ":", "]", "\n", ")", "\n", "\n", "# precision and scores are num_iou_thresholds X num_recall_thresholds X num_categories X", "\n", "# num_area_ranges X num_max_detections", "\n", "self", ".", "eval", "[", "\"precision\"", "]", "=", "np", ".", "array", "(", "self", ".", "eval", "[", "\"precision\"", "]", ")", ".", "reshape", "(", "self", ".", "eval", "[", "\"counts\"", "]", ")", "\n", "self", ".", "eval", "[", "\"scores\"", "]", "=", "np", ".", "array", "(", "self", ".", "eval", "[", "\"scores\"", "]", ")", ".", "reshape", "(", "self", ".", "eval", "[", "\"counts\"", "]", ")", "\n", "toc", "=", "time", ".", "time", "(", ")", "\n", "logger", ".", "info", "(", "\"COCOeval_opt.accumulate() finished in {:0.2f} seconds.\"", ".", "format", "(", "toc", "-", "tic", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.sem_seg_evaluation.SemSegEvaluator.__init__": [[24, 73], ["logging.getLogger", "torch.device", "detectron2.data.MetadataCatalog.get", "len", "sem_seg_evaluation.SemSegEvaluator._logger.warn", "sem_seg_evaluation.SemSegEvaluator._logger.warn", "detectron2.data.DatasetCatalog.get", "c2d.items"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset_name", ",", "\n", "distributed", "=", "True", ",", "\n", "output_dir", "=", "None", ",", "\n", "*", ",", "\n", "num_classes", "=", "None", ",", "\n", "ignore_label", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dataset_name (str): name of the dataset to be evaluated.\n            distributed (bool): if True, will collect results from all ranks for evaluation.\n                Otherwise, will evaluate the results in the current process.\n            output_dir (str): an output directory to dump results.\n            num_classes, ignore_label: deprecated argument\n        \"\"\"", "\n", "self", ".", "_logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "if", "num_classes", "is", "not", "None", ":", "\n", "            ", "self", ".", "_logger", ".", "warn", "(", "\n", "\"SemSegEvaluator(num_classes) is deprecated! It should be obtained from metadata.\"", "\n", ")", "\n", "", "if", "ignore_label", "is", "not", "None", ":", "\n", "            ", "self", ".", "_logger", ".", "warn", "(", "\n", "\"SemSegEvaluator(ignore_label) is deprecated! It should be obtained from metadata.\"", "\n", ")", "\n", "", "self", ".", "_dataset_name", "=", "dataset_name", "\n", "self", ".", "_distributed", "=", "distributed", "\n", "self", ".", "_output_dir", "=", "output_dir", "\n", "\n", "self", ".", "_cpu_device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "\n", "self", ".", "input_file_to_gt_file", "=", "{", "\n", "dataset_record", "[", "\"file_name\"", "]", ":", "dataset_record", "[", "\"sem_seg_file_name\"", "]", "\n", "for", "dataset_record", "in", "DatasetCatalog", ".", "get", "(", "dataset_name", ")", "\n", "}", "\n", "\n", "meta", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", "\n", "# Dict that maps contiguous training ids to COCO category ids", "\n", "try", ":", "\n", "            ", "c2d", "=", "meta", ".", "stuff_dataset_id_to_contiguous_id", "\n", "self", ".", "_contiguous_id_to_dataset_id", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "c2d", ".", "items", "(", ")", "}", "\n", "", "except", "AttributeError", ":", "\n", "            ", "self", ".", "_contiguous_id_to_dataset_id", "=", "None", "\n", "", "self", ".", "_class_names", "=", "meta", ".", "stuff_classes", "\n", "self", ".", "_num_classes", "=", "len", "(", "meta", ".", "stuff_classes", ")", "\n", "if", "num_classes", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "_num_classes", "==", "num_classes", ",", "f\"{self._num_classes} != {num_classes}\"", "\n", "", "self", ".", "_ignore_label", "=", "ignore_label", "if", "ignore_label", "is", "not", "None", "else", "meta", ".", "ignore_label", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.sem_seg_evaluation.SemSegEvaluator.reset": [[74, 77], ["numpy.zeros"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_conf_matrix", "=", "np", ".", "zeros", "(", "(", "self", ".", "_num_classes", "+", "1", ",", "self", ".", "_num_classes", "+", "1", ")", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "_predictions", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.sem_seg_evaluation.SemSegEvaluator.process": [[78, 102], ["zip", "output[].argmax().to", "numpy.array", "numpy.bincount().reshape", "sem_seg_evaluation.SemSegEvaluator._predictions.extend", "detectron2.utils.file_io.PathManager.open", "numpy.array", "sem_seg_evaluation.SemSegEvaluator.encode_json_sem_seg", "output[].argmax", "PIL.open", "numpy.bincount", "numpy.array.reshape", "numpy.array.reshape"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.sem_seg_evaluation.SemSegEvaluator.encode_json_sem_seg"], ["", "def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            inputs: the inputs to a model.\n                It is a list of dicts. Each dict corresponds to an image and\n                contains keys like \"height\", \"width\", \"file_name\".\n            outputs: the outputs of a model. It is either list of semantic segmentation predictions\n                (Tensor [H, W]) or list of dicts with key \"sem_seg\" that contains semantic\n                segmentation prediction in the same format.\n        \"\"\"", "\n", "for", "input", ",", "output", "in", "zip", "(", "inputs", ",", "outputs", ")", ":", "\n", "            ", "output", "=", "output", "[", "\"sem_seg\"", "]", ".", "argmax", "(", "dim", "=", "0", ")", ".", "to", "(", "self", ".", "_cpu_device", ")", "\n", "pred", "=", "np", ".", "array", "(", "output", ",", "dtype", "=", "np", ".", "int", ")", "\n", "with", "PathManager", ".", "open", "(", "self", ".", "input_file_to_gt_file", "[", "input", "[", "\"file_name\"", "]", "]", ",", "\"rb\"", ")", "as", "f", ":", "\n", "                ", "gt", "=", "np", ".", "array", "(", "Image", ".", "open", "(", "f", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "", "gt", "[", "gt", "==", "self", ".", "_ignore_label", "]", "=", "self", ".", "_num_classes", "\n", "\n", "self", ".", "_conf_matrix", "+=", "np", ".", "bincount", "(", "\n", "(", "self", ".", "_num_classes", "+", "1", ")", "*", "pred", ".", "reshape", "(", "-", "1", ")", "+", "gt", ".", "reshape", "(", "-", "1", ")", ",", "\n", "minlength", "=", "self", ".", "_conf_matrix", ".", "size", ",", "\n", ")", ".", "reshape", "(", "self", ".", "_conf_matrix", ".", "shape", ")", "\n", "\n", "self", ".", "_predictions", ".", "extend", "(", "self", ".", "encode_json_sem_seg", "(", "pred", ",", "input", "[", "\"file_name\"", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.sem_seg_evaluation.SemSegEvaluator.evaluate": [[103, 163], ["numpy.full", "numpy.full", "[].astype", "numpy.sum().astype", "numpy.sum().astype", "numpy.sum", "enumerate", "enumerate", "collections.OrderedDict", "sem_seg_evaluation.SemSegEvaluator._logger.info", "detectron2.utils.comm.synchronize", "detectron2.utils.comm.all_gather", "detectron2.utils.comm.all_gather", "list", "numpy.zeros_like", "detectron2.utils.file_io.PathManager.mkdirs", "os.path.join", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "os.path.join", "itertools.chain", "detectron2.utils.comm.is_main_process", "detectron2.utils.file_io.PathManager.open", "f.write", "numpy.sum", "numpy.sum", "detectron2.utils.file_io.PathManager.open", "torch.save", "json.dumps", "sem_seg_evaluation.SemSegEvaluator._conf_matrix.diagonal"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save"], ["", "", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Evaluates standard semantic segmentation metrics (http://cocodataset.org/#stuff-eval):\n\n        * Mean intersection-over-union averaged across classes (mIoU)\n        * Frequency Weighted IoU (fwIoU)\n        * Mean pixel accuracy averaged across classes (mACC)\n        * Pixel Accuracy (pACC)\n        \"\"\"", "\n", "if", "self", ".", "_distributed", ":", "\n", "            ", "synchronize", "(", ")", "\n", "conf_matrix_list", "=", "all_gather", "(", "self", ".", "_conf_matrix", ")", "\n", "self", ".", "_predictions", "=", "all_gather", "(", "self", ".", "_predictions", ")", "\n", "self", ".", "_predictions", "=", "list", "(", "itertools", ".", "chain", "(", "*", "self", ".", "_predictions", ")", ")", "\n", "if", "not", "is_main_process", "(", ")", ":", "\n", "                ", "return", "\n", "\n", "", "self", ".", "_conf_matrix", "=", "np", ".", "zeros_like", "(", "self", ".", "_conf_matrix", ")", "\n", "for", "conf_matrix", "in", "conf_matrix_list", ":", "\n", "                ", "self", ".", "_conf_matrix", "+=", "conf_matrix", "\n", "\n", "", "", "if", "self", ".", "_output_dir", ":", "\n", "            ", "PathManager", ".", "mkdirs", "(", "self", ".", "_output_dir", ")", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_output_dir", ",", "\"sem_seg_predictions.json\"", ")", "\n", "with", "PathManager", ".", "open", "(", "file_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "json", ".", "dumps", "(", "self", ".", "_predictions", ")", ")", "\n", "\n", "", "", "acc", "=", "np", ".", "full", "(", "self", ".", "_num_classes", ",", "np", ".", "nan", ",", "dtype", "=", "np", ".", "float", ")", "\n", "iou", "=", "np", ".", "full", "(", "self", ".", "_num_classes", ",", "np", ".", "nan", ",", "dtype", "=", "np", ".", "float", ")", "\n", "tp", "=", "self", ".", "_conf_matrix", ".", "diagonal", "(", ")", "[", ":", "-", "1", "]", ".", "astype", "(", "np", ".", "float", ")", "\n", "pos_gt", "=", "np", ".", "sum", "(", "self", ".", "_conf_matrix", "[", ":", "-", "1", ",", ":", "-", "1", "]", ",", "axis", "=", "0", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "class_weights", "=", "pos_gt", "/", "np", ".", "sum", "(", "pos_gt", ")", "\n", "pos_pred", "=", "np", ".", "sum", "(", "self", ".", "_conf_matrix", "[", ":", "-", "1", ",", ":", "-", "1", "]", ",", "axis", "=", "1", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "acc_valid", "=", "pos_gt", ">", "0", "\n", "acc", "[", "acc_valid", "]", "=", "tp", "[", "acc_valid", "]", "/", "pos_gt", "[", "acc_valid", "]", "\n", "iou_valid", "=", "(", "pos_gt", "+", "pos_pred", ")", ">", "0", "\n", "union", "=", "pos_gt", "+", "pos_pred", "-", "tp", "\n", "iou", "[", "acc_valid", "]", "=", "tp", "[", "acc_valid", "]", "/", "union", "[", "acc_valid", "]", "\n", "macc", "=", "np", ".", "sum", "(", "acc", "[", "acc_valid", "]", ")", "/", "np", ".", "sum", "(", "acc_valid", ")", "\n", "miou", "=", "np", ".", "sum", "(", "iou", "[", "acc_valid", "]", ")", "/", "np", ".", "sum", "(", "iou_valid", ")", "\n", "fiou", "=", "np", ".", "sum", "(", "iou", "[", "acc_valid", "]", "*", "class_weights", "[", "acc_valid", "]", ")", "\n", "pacc", "=", "np", ".", "sum", "(", "tp", ")", "/", "np", ".", "sum", "(", "pos_gt", ")", "\n", "\n", "res", "=", "{", "}", "\n", "res", "[", "\"mIoU\"", "]", "=", "100", "*", "miou", "\n", "res", "[", "\"fwIoU\"", "]", "=", "100", "*", "fiou", "\n", "for", "i", ",", "name", "in", "enumerate", "(", "self", ".", "_class_names", ")", ":", "\n", "            ", "res", "[", "\"IoU-{}\"", ".", "format", "(", "name", ")", "]", "=", "100", "*", "iou", "[", "i", "]", "\n", "", "res", "[", "\"mACC\"", "]", "=", "100", "*", "macc", "\n", "res", "[", "\"pACC\"", "]", "=", "100", "*", "pacc", "\n", "for", "i", ",", "name", "in", "enumerate", "(", "self", ".", "_class_names", ")", ":", "\n", "            ", "res", "[", "\"ACC-{}\"", ".", "format", "(", "name", ")", "]", "=", "100", "*", "acc", "[", "i", "]", "\n", "\n", "", "if", "self", ".", "_output_dir", ":", "\n", "            ", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_output_dir", ",", "\"sem_seg_evaluation.pth\"", ")", "\n", "with", "PathManager", ".", "open", "(", "file_path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "torch", ".", "save", "(", "res", ",", "f", ")", "\n", "", "", "results", "=", "OrderedDict", "(", "{", "\"sem_seg\"", ":", "res", "}", ")", "\n", "self", ".", "_logger", ".", "info", "(", "results", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.sem_seg_evaluation.SemSegEvaluator.encode_json_sem_seg": [[164, 185], ["numpy.unique", "mask_rle[].decode", "json_list.append", "int", "pycocotools.encode", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode"], ["", "def", "encode_json_sem_seg", "(", "self", ",", "sem_seg", ",", "input_file_name", ")", ":", "\n", "        ", "\"\"\"\n        Convert semantic segmentation to COCO stuff format with segments encoded as RLEs.\n        See http://cocodataset.org/#format-results\n        \"\"\"", "\n", "json_list", "=", "[", "]", "\n", "for", "label", "in", "np", ".", "unique", "(", "sem_seg", ")", ":", "\n", "            ", "if", "self", ".", "_contiguous_id_to_dataset_id", "is", "not", "None", ":", "\n", "                ", "assert", "(", "\n", "label", "in", "self", ".", "_contiguous_id_to_dataset_id", "\n", ")", ",", "\"Label {} is not in the metadata info for {}\"", ".", "format", "(", "label", ",", "self", ".", "_dataset_name", ")", "\n", "dataset_id", "=", "self", ".", "_contiguous_id_to_dataset_id", "[", "label", "]", "\n", "", "else", ":", "\n", "                ", "dataset_id", "=", "int", "(", "label", ")", "\n", "", "mask", "=", "(", "sem_seg", "==", "label", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "mask_rle", "=", "mask_util", ".", "encode", "(", "np", ".", "array", "(", "mask", "[", ":", ",", ":", ",", "None", "]", ",", "order", "=", "\"F\"", ")", ")", "[", "0", "]", "\n", "mask_rle", "[", "\"counts\"", "]", "=", "mask_rle", "[", "\"counts\"", "]", ".", "decode", "(", "\"utf-8\"", ")", "\n", "json_list", ".", "append", "(", "\n", "{", "\"file_name\"", ":", "input_file_name", ",", "\"category_id\"", ":", "dataset_id", ",", "\"segmentation\"", ":", "mask_rle", "}", "\n", ")", "\n", "", "return", "json_list", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation.LVISEvaluator.__init__": [[28, 67], ["logging.getLogger", "torch.device", "detectron2.data.MetadataCatalog.get", "detectron2.utils.file_io.PathManager.get_local_path", "LVIS", "isinstance", "lvis_evaluation.LVISEvaluator._logger.warn", "len", "lvis_evaluation.LVISEvaluator._lvis_api.get_ann_ids"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "__init__", "(", "self", ",", "dataset_name", ",", "tasks", "=", "None", ",", "distributed", "=", "True", ",", "output_dir", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dataset_name (str): name of the dataset to be evaluated.\n                It must have the following corresponding metadata:\n                \"json_file\": the path to the LVIS format annotation\n            tasks (tuple[str]): tasks that can be evaluated under the given\n                configuration. A task is one of \"bbox\", \"segm\".\n                By default, will infer this automatically from predictions.\n            distributed (True): if True, will collect results from all ranks for evaluation.\n                Otherwise, will evaluate the results in the current process.\n            output_dir (str): optional, an output directory to dump results.\n        \"\"\"", "\n", "from", "lvis", "import", "LVIS", "\n", "\n", "self", ".", "_logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n", "if", "tasks", "is", "not", "None", "and", "isinstance", "(", "tasks", ",", "CfgNode", ")", ":", "\n", "            ", "self", ".", "_logger", ".", "warn", "(", "\n", "\"COCO Evaluator instantiated using config, this is deprecated behavior.\"", "\n", "\" Please pass in explicit arguments instead.\"", "\n", ")", "\n", "self", ".", "_tasks", "=", "None", "# Infering it from predictions should be better", "\n", "", "else", ":", "\n", "            ", "self", ".", "_tasks", "=", "tasks", "\n", "\n", "", "self", ".", "_distributed", "=", "distributed", "\n", "self", ".", "_output_dir", "=", "output_dir", "\n", "\n", "self", ".", "_cpu_device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "\n", "self", ".", "_metadata", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", "\n", "json_file", "=", "PathManager", ".", "get_local_path", "(", "self", ".", "_metadata", ".", "json_file", ")", "\n", "self", ".", "_lvis_api", "=", "LVIS", "(", "json_file", ")", "\n", "# Test set json files do not contain annotations (evaluation must be", "\n", "# performed using the LVIS evaluation server).", "\n", "self", ".", "_do_evaluation", "=", "len", "(", "self", ".", "_lvis_api", ".", "get_ann_ids", "(", ")", ")", ">", "0", "\n", "if", "dataset_name", "==", "'lvis_v1_val_custom_img'", ":", "\n", "            ", "self", ".", "_do_evaluation", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation.LVISEvaluator.reset": [[68, 70], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_predictions", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation.LVISEvaluator.process": [[71, 89], ["zip", "lvis_evaluation.LVISEvaluator._predictions.append", "output[].to", "coco_evaluation.instances_to_coco_json", "output[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.instances_to_coco_json", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            inputs: the inputs to a LVIS model (e.g., GeneralizedRCNN).\n                It is a list of dict. Each dict corresponds to an image and\n                contains keys like \"height\", \"width\", \"file_name\", \"image_id\".\n            outputs: the outputs of a LVIS model. It is a list of dicts with key\n                \"instances\" that contains :class:`Instances`.\n        \"\"\"", "\n", "for", "input", ",", "output", "in", "zip", "(", "inputs", ",", "outputs", ")", ":", "\n", "            ", "prediction", "=", "{", "\"image_id\"", ":", "input", "[", "\"image_id\"", "]", "}", "\n", "\n", "if", "\"instances\"", "in", "output", ":", "\n", "                ", "instances", "=", "output", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "_cpu_device", ")", "\n", "prediction", "[", "\"instances\"", "]", "=", "instances_to_coco_json", "(", "instances", ",", "input", "[", "\"image_id\"", "]", ")", "\n", "", "if", "\"proposals\"", "in", "output", ":", "\n", "                ", "prediction", "[", "\"proposals\"", "]", "=", "output", "[", "\"proposals\"", "]", ".", "to", "(", "self", ".", "_cpu_device", ")", "\n", "", "self", ".", "_predictions", ".", "append", "(", "prediction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation.LVISEvaluator.evaluate": [[90, 118], ["collections.OrderedDict", "copy.deepcopy", "detectron2.synchronize", "detectron2.gather", "list", "len", "lvis_evaluation.LVISEvaluator._logger.warning", "detectron2.utils.file_io.PathManager.mkdirs", "os.path.join", "lvis_evaluation.LVISEvaluator._eval_box_proposals", "lvis_evaluation.LVISEvaluator._eval_predictions", "itertools.chain", "detectron2.is_main_process", "detectron2.utils.file_io.PathManager.open", "torch.save"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._eval_box_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOEvaluator._eval_predictions", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save"], ["", "", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_distributed", ":", "\n", "            ", "comm", ".", "synchronize", "(", ")", "\n", "predictions", "=", "comm", ".", "gather", "(", "self", ".", "_predictions", ",", "dst", "=", "0", ")", "\n", "predictions", "=", "list", "(", "itertools", ".", "chain", "(", "*", "predictions", ")", ")", "\n", "\n", "if", "not", "comm", ".", "is_main_process", "(", ")", ":", "\n", "                ", "return", "\n", "", "", "else", ":", "\n", "            ", "predictions", "=", "self", ".", "_predictions", "\n", "\n", "", "if", "len", "(", "predictions", ")", "==", "0", ":", "\n", "            ", "self", ".", "_logger", ".", "warning", "(", "\"[LVISEvaluator] Did not receive valid predictions.\"", ")", "\n", "return", "{", "}", "\n", "\n", "", "if", "self", ".", "_output_dir", ":", "\n", "            ", "PathManager", ".", "mkdirs", "(", "self", ".", "_output_dir", ")", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_output_dir", ",", "\"instances_predictions.pth\"", ")", "\n", "with", "PathManager", ".", "open", "(", "file_path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "torch", ".", "save", "(", "predictions", ",", "f", ")", "\n", "\n", "", "", "self", ".", "_results", "=", "OrderedDict", "(", ")", "\n", "if", "\"proposals\"", "in", "predictions", "[", "0", "]", ":", "\n", "            ", "self", ".", "_eval_box_proposals", "(", "predictions", ")", "\n", "", "if", "\"instances\"", "in", "predictions", "[", "0", "]", ":", "\n", "            ", "self", ".", "_eval_predictions", "(", "predictions", ")", "\n", "# Copy so the caller can do whatever with results", "\n", "", "return", "copy", ".", "deepcopy", "(", "self", ".", "_results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation.LVISEvaluator._tasks_from_predictions": [[119, 124], ["None"], "methods", ["None"], ["", "def", "_tasks_from_predictions", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "for", "pred", "in", "predictions", ":", "\n", "            ", "if", "\"segmentation\"", "in", "pred", ":", "\n", "                ", "return", "(", "\"bbox\"", ",", "\"segm\"", ")", "\n", "", "", "return", "(", "\"bbox\"", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation.LVISEvaluator._eval_predictions": [[125, 166], ["lvis_evaluation.LVISEvaluator._logger.info", "list", "hasattr", "lvis_evaluation.LVISEvaluator._logger.info", "sorted", "itertools.chain", "lvis_evaluation.LVISEvaluator._tasks_from_predictions", "os.path.join", "lvis_evaluation.LVISEvaluator._logger.info", "lvis_evaluation.LVISEvaluator._logger.info", "lvis_evaluation._evaluate_predictions_on_lvis", "detectron2.utils.file_io.PathManager.open", "f.write", "f.flush", "lvis_evaluation.LVISEvaluator._metadata.thing_dataset_id_to_contiguous_id.items", "json.dumps", "lvis_evaluation.LVISEvaluator._metadata.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._tasks_from_predictions", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation._evaluate_predictions_on_lvis", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "_eval_predictions", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate predictions. Fill self._results with the metrics of the tasks.\n\n        Args:\n            predictions (list[dict]): list of outputs from the model\n        \"\"\"", "\n", "self", ".", "_logger", ".", "info", "(", "\"Preparing results in the LVIS format ...\"", ")", "\n", "lvis_results", "=", "list", "(", "itertools", ".", "chain", "(", "*", "[", "x", "[", "\"instances\"", "]", "for", "x", "in", "predictions", "]", ")", ")", "\n", "tasks", "=", "self", ".", "_tasks", "or", "self", ".", "_tasks_from_predictions", "(", "lvis_results", ")", "\n", "\n", "# LVIS evaluator can be used to evaluate results for COCO dataset categories.", "\n", "# In this case `_metadata` variable will have a field with COCO-specific category mapping.", "\n", "if", "hasattr", "(", "self", ".", "_metadata", ",", "\"thing_dataset_id_to_contiguous_id\"", ")", ":", "\n", "            ", "reverse_id_mapping", "=", "{", "\n", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "_metadata", ".", "thing_dataset_id_to_contiguous_id", ".", "items", "(", ")", "\n", "}", "\n", "for", "result", "in", "lvis_results", ":", "\n", "                ", "result", "[", "\"category_id\"", "]", "=", "reverse_id_mapping", "[", "result", "[", "\"category_id\"", "]", "]", "\n", "", "", "else", ":", "\n", "# unmap the category ids for LVIS (from 0-indexed to 1-indexed)", "\n", "            ", "for", "result", "in", "lvis_results", ":", "\n", "                ", "result", "[", "\"category_id\"", "]", "+=", "1", "\n", "\n", "", "", "if", "self", ".", "_output_dir", ":", "\n", "            ", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_output_dir", ",", "\"lvis_instances_results.json\"", ")", "\n", "self", ".", "_logger", ".", "info", "(", "\"Saving results to {}\"", ".", "format", "(", "file_path", ")", ")", "\n", "with", "PathManager", ".", "open", "(", "file_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "json", ".", "dumps", "(", "lvis_results", ")", ")", "\n", "f", ".", "flush", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "_do_evaluation", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\"Annotations are not available for evaluation.\"", ")", "\n", "return", "\n", "\n", "", "self", ".", "_logger", ".", "info", "(", "\"Evaluating predictions ...\"", ")", "\n", "for", "task", "in", "sorted", "(", "tasks", ")", ":", "\n", "            ", "res", "=", "_evaluate_predictions_on_lvis", "(", "\n", "self", ".", "_lvis_api", ",", "lvis_results", ",", "task", ",", "class_names", "=", "self", ".", "_metadata", ".", "get", "(", "\"thing_classes\"", ")", "\n", ")", "\n", "self", ".", "_results", "[", "task", "]", "=", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation.LVISEvaluator._eval_box_proposals": [[167, 205], ["lvis_evaluation.LVISEvaluator._logger.info", "lvis_evaluation.LVISEvaluator._logger.info", "lvis_evaluation.LVISEvaluator._logger.info", "areas.items", "ids.append", "boxes.append", "objectness_logits.append", "detectron2.utils.file_io.PathManager.open", "pickle.dump", "lvis_evaluation._evaluate_box_proposals", "float", "detectron2.utils.logger.create_small_table", "prediction[].proposal_boxes.tensor.numpy", "prediction[].objectness_logits.numpy", "os.path.join", "stats[].item"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation._evaluate_box_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.create_small_table"], ["", "", "def", "_eval_box_proposals", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate the box proposals in predictions.\n        Fill self._results with the metrics for \"box_proposals\" task.\n        \"\"\"", "\n", "if", "self", ".", "_output_dir", ":", "\n", "# Saving generated box proposals to file.", "\n", "# Predicted box_proposals are in XYXY_ABS mode.", "\n", "            ", "bbox_mode", "=", "BoxMode", ".", "XYXY_ABS", ".", "value", "\n", "ids", ",", "boxes", ",", "objectness_logits", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "prediction", "in", "predictions", ":", "\n", "                ", "ids", ".", "append", "(", "prediction", "[", "\"image_id\"", "]", ")", "\n", "boxes", ".", "append", "(", "prediction", "[", "\"proposals\"", "]", ".", "proposal_boxes", ".", "tensor", ".", "numpy", "(", ")", ")", "\n", "objectness_logits", ".", "append", "(", "prediction", "[", "\"proposals\"", "]", ".", "objectness_logits", ".", "numpy", "(", ")", ")", "\n", "\n", "", "proposal_data", "=", "{", "\n", "\"boxes\"", ":", "boxes", ",", "\n", "\"objectness_logits\"", ":", "objectness_logits", ",", "\n", "\"ids\"", ":", "ids", ",", "\n", "\"bbox_mode\"", ":", "bbox_mode", ",", "\n", "}", "\n", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "_output_dir", ",", "\"box_proposals.pkl\"", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "proposal_data", ",", "f", ")", "\n", "\n", "", "", "if", "not", "self", ".", "_do_evaluation", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\"Annotations are not available for evaluation.\"", ")", "\n", "return", "\n", "\n", "", "self", ".", "_logger", ".", "info", "(", "\"Evaluating bbox proposals ...\"", ")", "\n", "res", "=", "{", "}", "\n", "areas", "=", "{", "\"all\"", ":", "\"\"", ",", "\"small\"", ":", "\"s\"", ",", "\"medium\"", ":", "\"m\"", ",", "\"large\"", ":", "\"l\"", "}", "\n", "for", "limit", "in", "[", "100", ",", "1000", "]", ":", "\n", "            ", "for", "area", ",", "suffix", "in", "areas", ".", "items", "(", ")", ":", "\n", "                ", "stats", "=", "_evaluate_box_proposals", "(", "predictions", ",", "self", ".", "_lvis_api", ",", "area", "=", "area", ",", "limit", "=", "limit", ")", "\n", "key", "=", "\"AR{}@{:d}\"", ".", "format", "(", "suffix", ",", "limit", ")", "\n", "res", "[", "key", "]", "=", "float", "(", "stats", "[", "\"ar\"", "]", ".", "item", "(", ")", "*", "100", ")", "\n", "", "", "self", ".", "_logger", ".", "info", "(", "\"Proposal metrics: \\n\"", "+", "create_small_table", "(", "res", ")", ")", "\n", "self", ".", "_results", "[", "\"box_proposals\"", "]", "=", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation._evaluate_box_proposals": [[209, 315], ["torch.sort", "torch.zeros_like", "enumerate", "torch.zeros_like.mean", "lvis_api.get_ann_ids", "lvis_api.load_anns", "torch.as_tensor().reshape", "detectron2.structures.Boxes", "torch.as_tensor", "len", "detectron2.structures.pairwise_iou", "torch.zeros", "range", "gt_overlaps.append", "len", "torch.cat", "torch.zeros", "torch.arange", "predictions.objectness_logits.sort", "detectron2.structures.BoxMode.convert", "len", "len", "min", "detectron2.structures.pairwise_iou.max", "max_overlaps.max", "float", "torch.as_tensor", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "", "def", "_evaluate_box_proposals", "(", "dataset_predictions", ",", "lvis_api", ",", "thresholds", "=", "None", ",", "area", "=", "\"all\"", ",", "limit", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Evaluate detection proposal recall metrics. This function is a much\n    faster alternative to the official LVIS API recall evaluation code. However,\n    it produces slightly different results.\n    \"\"\"", "\n", "# Record max overlap value for each gt box", "\n", "# Return vector of overlap values", "\n", "areas", "=", "{", "\n", "\"all\"", ":", "0", ",", "\n", "\"small\"", ":", "1", ",", "\n", "\"medium\"", ":", "2", ",", "\n", "\"large\"", ":", "3", ",", "\n", "\"96-128\"", ":", "4", ",", "\n", "\"128-256\"", ":", "5", ",", "\n", "\"256-512\"", ":", "6", ",", "\n", "\"512-inf\"", ":", "7", ",", "\n", "}", "\n", "area_ranges", "=", "[", "\n", "[", "0", "**", "2", ",", "1e5", "**", "2", "]", ",", "# all", "\n", "[", "0", "**", "2", ",", "32", "**", "2", "]", ",", "# small", "\n", "[", "32", "**", "2", ",", "96", "**", "2", "]", ",", "# medium", "\n", "[", "96", "**", "2", ",", "1e5", "**", "2", "]", ",", "# large", "\n", "[", "96", "**", "2", ",", "128", "**", "2", "]", ",", "# 96-128", "\n", "[", "128", "**", "2", ",", "256", "**", "2", "]", ",", "# 128-256", "\n", "[", "256", "**", "2", ",", "512", "**", "2", "]", ",", "# 256-512", "\n", "[", "512", "**", "2", ",", "1e5", "**", "2", "]", ",", "\n", "]", "# 512-inf", "\n", "assert", "area", "in", "areas", ",", "\"Unknown area range: {}\"", ".", "format", "(", "area", ")", "\n", "area_range", "=", "area_ranges", "[", "areas", "[", "area", "]", "]", "\n", "gt_overlaps", "=", "[", "]", "\n", "num_pos", "=", "0", "\n", "\n", "for", "prediction_dict", "in", "dataset_predictions", ":", "\n", "        ", "predictions", "=", "prediction_dict", "[", "\"proposals\"", "]", "\n", "\n", "# sort predictions in descending order", "\n", "# TODO maybe remove this and make it explicit in the documentation", "\n", "inds", "=", "predictions", ".", "objectness_logits", ".", "sort", "(", "descending", "=", "True", ")", "[", "1", "]", "\n", "predictions", "=", "predictions", "[", "inds", "]", "\n", "\n", "ann_ids", "=", "lvis_api", ".", "get_ann_ids", "(", "img_ids", "=", "[", "prediction_dict", "[", "\"image_id\"", "]", "]", ")", "\n", "anno", "=", "lvis_api", ".", "load_anns", "(", "ann_ids", ")", "\n", "gt_boxes", "=", "[", "\n", "BoxMode", ".", "convert", "(", "obj", "[", "\"bbox\"", "]", ",", "BoxMode", ".", "XYWH_ABS", ",", "BoxMode", ".", "XYXY_ABS", ")", "for", "obj", "in", "anno", "\n", "]", "\n", "gt_boxes", "=", "torch", ".", "as_tensor", "(", "gt_boxes", ")", ".", "reshape", "(", "-", "1", ",", "4", ")", "# guard against no boxes", "\n", "gt_boxes", "=", "Boxes", "(", "gt_boxes", ")", "\n", "gt_areas", "=", "torch", ".", "as_tensor", "(", "[", "obj", "[", "\"area\"", "]", "for", "obj", "in", "anno", "]", ")", "\n", "\n", "if", "len", "(", "gt_boxes", ")", "==", "0", "or", "len", "(", "predictions", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "", "valid_gt_inds", "=", "(", "gt_areas", ">=", "area_range", "[", "0", "]", ")", "&", "(", "gt_areas", "<=", "area_range", "[", "1", "]", ")", "\n", "gt_boxes", "=", "gt_boxes", "[", "valid_gt_inds", "]", "\n", "\n", "num_pos", "+=", "len", "(", "gt_boxes", ")", "\n", "\n", "if", "len", "(", "gt_boxes", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "limit", "is", "not", "None", "and", "len", "(", "predictions", ")", ">", "limit", ":", "\n", "            ", "predictions", "=", "predictions", "[", ":", "limit", "]", "\n", "\n", "", "overlaps", "=", "pairwise_iou", "(", "predictions", ".", "proposal_boxes", ",", "gt_boxes", ")", "\n", "\n", "_gt_overlaps", "=", "torch", ".", "zeros", "(", "len", "(", "gt_boxes", ")", ")", "\n", "for", "j", "in", "range", "(", "min", "(", "len", "(", "predictions", ")", ",", "len", "(", "gt_boxes", ")", ")", ")", ":", "\n", "# find which proposal box maximally covers each gt box", "\n", "# and get the iou amount of coverage for each gt box", "\n", "            ", "max_overlaps", ",", "argmax_overlaps", "=", "overlaps", ".", "max", "(", "dim", "=", "0", ")", "\n", "\n", "# find which gt box is 'best' covered (i.e. 'best' = most iou)", "\n", "gt_ovr", ",", "gt_ind", "=", "max_overlaps", ".", "max", "(", "dim", "=", "0", ")", "\n", "assert", "gt_ovr", ">=", "0", "\n", "# find the proposal box that covers the best covered gt box", "\n", "box_ind", "=", "argmax_overlaps", "[", "gt_ind", "]", "\n", "# record the iou coverage of this gt box", "\n", "_gt_overlaps", "[", "j", "]", "=", "overlaps", "[", "box_ind", ",", "gt_ind", "]", "\n", "assert", "_gt_overlaps", "[", "j", "]", "==", "gt_ovr", "\n", "# mark the proposal box and the gt box as used", "\n", "overlaps", "[", "box_ind", ",", ":", "]", "=", "-", "1", "\n", "overlaps", "[", ":", ",", "gt_ind", "]", "=", "-", "1", "\n", "\n", "# append recorded iou coverage level", "\n", "", "gt_overlaps", ".", "append", "(", "_gt_overlaps", ")", "\n", "", "gt_overlaps", "=", "(", "\n", "torch", ".", "cat", "(", "gt_overlaps", ",", "dim", "=", "0", ")", "if", "len", "(", "gt_overlaps", ")", "else", "torch", ".", "zeros", "(", "0", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", ")", "\n", "gt_overlaps", ",", "_", "=", "torch", ".", "sort", "(", "gt_overlaps", ")", "\n", "\n", "if", "thresholds", "is", "None", ":", "\n", "        ", "step", "=", "0.05", "\n", "thresholds", "=", "torch", ".", "arange", "(", "0.5", ",", "0.95", "+", "1e-5", ",", "step", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "", "recalls", "=", "torch", ".", "zeros_like", "(", "thresholds", ")", "\n", "# compute recall for each iou threshold", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "thresholds", ")", ":", "\n", "        ", "recalls", "[", "i", "]", "=", "(", "gt_overlaps", ">=", "t", ")", ".", "float", "(", ")", ".", "sum", "(", ")", "/", "float", "(", "num_pos", ")", "\n", "# ar = 2 * np.trapz(recalls, thresholds)", "\n", "", "ar", "=", "recalls", ".", "mean", "(", ")", "\n", "return", "{", "\n", "\"ar\"", ":", "ar", ",", "\n", "\"recalls\"", ":", "recalls", ",", "\n", "\"thresholds\"", ":", "thresholds", ",", "\n", "\"gt_overlaps\"", ":", "gt_overlaps", ",", "\n", "\"num_pos\"", ":", "num_pos", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.lvis_evaluation._evaluate_predictions_on_lvis": [[318, 361], ["logging.getLogger", "LVISResults", "LVISEval", "LVISEval.run", "LVISEval.print_results", "LVISEval.get_results", "logging.getLogger.info", "len", "logging.getLogger.warn", "copy.deepcopy", "float", "float", "c.pop", "detectron2.utils.logger.create_small_table"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.create_small_table"], ["", "def", "_evaluate_predictions_on_lvis", "(", "lvis_gt", ",", "lvis_results", ",", "iou_type", ",", "class_names", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        iou_type (str):\n        kpt_oks_sigmas (list[float]):\n        class_names (None or list[str]): if provided, will use it to predict\n            per-category AP.\n\n    Returns:\n        a dict of {metric name: score}\n    \"\"\"", "\n", "metrics", "=", "{", "\n", "\"bbox\"", ":", "[", "\"AP\"", ",", "\"AP50\"", ",", "\"AP75\"", ",", "\"APs\"", ",", "\"APm\"", ",", "\"APl\"", ",", "\"APr\"", ",", "\"APc\"", ",", "\"APf\"", "]", ",", "\n", "\"segm\"", ":", "[", "\"AP\"", ",", "\"AP50\"", ",", "\"AP75\"", ",", "\"APs\"", ",", "\"APm\"", ",", "\"APl\"", ",", "\"APr\"", ",", "\"APc\"", ",", "\"APf\"", "]", ",", "\n", "}", "[", "iou_type", "]", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n", "if", "len", "(", "lvis_results", ")", "==", "0", ":", "# TODO: check if needed", "\n", "        ", "logger", ".", "warn", "(", "\"No predictions from the model!\"", ")", "\n", "return", "{", "metric", ":", "float", "(", "\"nan\"", ")", "for", "metric", "in", "metrics", "}", "\n", "\n", "", "if", "iou_type", "==", "\"segm\"", ":", "\n", "        ", "lvis_results", "=", "copy", ".", "deepcopy", "(", "lvis_results", ")", "\n", "# When evaluating mask AP, if the results contain bbox, LVIS API will", "\n", "# use the box area as the area of the instance, instead of the mask area.", "\n", "# This leads to a different definition of small/medium/large.", "\n", "# We remove the bbox field to let mask AP use mask area.", "\n", "for", "c", "in", "lvis_results", ":", "\n", "            ", "c", ".", "pop", "(", "\"bbox\"", ",", "None", ")", "\n", "\n", "", "", "from", "lvis", "import", "LVISEval", ",", "LVISResults", "\n", "\n", "lvis_results", "=", "LVISResults", "(", "lvis_gt", ",", "lvis_results", ")", "\n", "lvis_eval", "=", "LVISEval", "(", "lvis_gt", ",", "lvis_results", ",", "iou_type", ")", "\n", "lvis_eval", ".", "run", "(", ")", "\n", "lvis_eval", ".", "print_results", "(", ")", "\n", "\n", "# Pull the standard metrics from the LVIS results", "\n", "results", "=", "lvis_eval", ".", "get_results", "(", ")", "\n", "results", "=", "{", "metric", ":", "float", "(", "results", "[", "metric", "]", "*", "100", ")", "for", "metric", "in", "metrics", "}", "\n", "logger", ".", "info", "(", "\"Evaluation results for {}: \\n\"", ".", "format", "(", "iou_type", ")", "+", "create_small_table", "(", "results", ")", ")", "\n", "return", "results", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator.__init__": [[44, 123], ["logging.getLogger", "torch.device", "detectron2.data.MetadataCatalog.get", "detectron2.utils.file_io.PathManager.get_local_path", "isinstance", "coco_evaluation.COCOEvaluator._logger.warn", "hasattr", "coco_evaluation.COCOEvaluator._logger.info", "os.path.join", "detectron2.data.datasets.coco.convert_to_coco_json", "contextlib.redirect_stdout", "pycocotools.coco.COCO", "io.StringIO"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.datasets.coco.convert_to_coco_json"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset_name", ",", "\n", "tasks", "=", "None", ",", "\n", "distributed", "=", "True", ",", "\n", "output_dir", "=", "None", ",", "\n", "*", ",", "\n", "use_fast_impl", "=", "True", ",", "\n", "kpt_oks_sigmas", "=", "(", ")", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dataset_name (str): name of the dataset to be evaluated.\n                It must have either the following corresponding metadata:\n\n                    \"json_file\": the path to the COCO format annotation\n\n                Or it must be in detectron2's standard dataset format\n                so it can be converted to COCO format automatically.\n            tasks (tuple[str]): tasks that can be evaluated under the given\n                configuration. A task is one of \"bbox\", \"segm\", \"keypoints\".\n                By default, will infer this automatically from predictions.\n            distributed (True): if True, will collect results from all ranks and run evaluation\n                in the main process.\n                Otherwise, will only evaluate the results in the current process.\n            output_dir (str): optional, an output directory to dump all\n                results predicted on the dataset. The dump contains two files:\n\n                1. \"instances_predictions.pth\" a file that can be loaded with `torch.load` and\n                   contains all the results in the format they are produced by the model.\n                2. \"coco_instances_results.json\" a json file in COCO's result format.\n            use_fast_impl (bool): use a fast but **unofficial** implementation to compute AP.\n                Although the results should be very close to the official implementation in COCO\n                API, it is still recommended to compute results with the official API for use in\n                papers. The faster implementation also uses more RAM.\n            kpt_oks_sigmas (list[float]): The sigmas used to calculate keypoint OKS.\n                See http://cocodataset.org/#keypoints-eval\n                When empty, it will use the defaults in COCO.\n                Otherwise it should be the same length as ROI_KEYPOINT_HEAD.NUM_KEYPOINTS.\n        \"\"\"", "\n", "self", ".", "_logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "self", ".", "_distributed", "=", "distributed", "\n", "self", ".", "_output_dir", "=", "output_dir", "\n", "self", ".", "_use_fast_impl", "=", "use_fast_impl", "\n", "\n", "if", "tasks", "is", "not", "None", "and", "isinstance", "(", "tasks", ",", "CfgNode", ")", ":", "\n", "            ", "kpt_oks_sigmas", "=", "(", "\n", "tasks", ".", "TEST", ".", "KEYPOINT_OKS_SIGMAS", "if", "not", "kpt_oks_sigmas", "else", "kpt_oks_sigmas", "\n", ")", "\n", "self", ".", "_logger", ".", "warn", "(", "\n", "\"COCO Evaluator instantiated using config, this is deprecated behavior.\"", "\n", "\" Please pass in explicit arguments instead.\"", "\n", ")", "\n", "self", ".", "_tasks", "=", "None", "# Infering it from predictions should be better", "\n", "", "else", ":", "\n", "            ", "self", ".", "_tasks", "=", "tasks", "\n", "\n", "", "self", ".", "_cpu_device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "\n", "self", ".", "_metadata", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", "\n", "if", "not", "hasattr", "(", "self", ".", "_metadata", ",", "\"json_file\"", ")", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\n", "f\"'{dataset_name}' is not registered by `register_coco_instances`.\"", "\n", "\" Therefore trying to convert it to COCO format ...\"", "\n", ")", "\n", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "f\"{dataset_name}_coco_format.json\"", ")", "\n", "self", ".", "_metadata", ".", "json_file", "=", "cache_path", "\n", "convert_to_coco_json", "(", "dataset_name", ",", "cache_path", ")", "\n", "\n", "", "json_file", "=", "PathManager", ".", "get_local_path", "(", "self", ".", "_metadata", ".", "json_file", ")", "\n", "with", "contextlib", ".", "redirect_stdout", "(", "io", ".", "StringIO", "(", ")", ")", ":", "\n", "            ", "self", ".", "_coco_api", "=", "COCO", "(", "json_file", ")", "\n", "\n", "# Test set json files do not contain annotations (evaluation must be", "\n", "# performed using the COCO evaluation server).", "\n", "", "self", ".", "_do_evaluation", "=", "\"annotations\"", "in", "self", ".", "_coco_api", ".", "dataset", "\n", "if", "self", ".", "_do_evaluation", ":", "\n", "            ", "self", ".", "_kpt_oks_sigmas", "=", "kpt_oks_sigmas", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator.reset": [[124, 126], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_predictions", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator.process": [[127, 146], ["zip", "output[].to", "coco_evaluation.instances_to_coco_json", "output[].to", "len", "coco_evaluation.COCOEvaluator._predictions.append"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.instances_to_coco_json", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).\n                It is a list of dict. Each dict corresponds to an image and\n                contains keys like \"height\", \"width\", \"file_name\", \"image_id\".\n            outputs: the outputs of a COCO model. It is a list of dicts with key\n                \"instances\" that contains :class:`Instances`.\n        \"\"\"", "\n", "for", "input", ",", "output", "in", "zip", "(", "inputs", ",", "outputs", ")", ":", "\n", "            ", "prediction", "=", "{", "\"image_id\"", ":", "input", "[", "\"image_id\"", "]", "}", "\n", "\n", "if", "\"instances\"", "in", "output", ":", "\n", "                ", "instances", "=", "output", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "_cpu_device", ")", "\n", "prediction", "[", "\"instances\"", "]", "=", "instances_to_coco_json", "(", "instances", ",", "input", "[", "\"image_id\"", "]", ")", "\n", "", "if", "\"proposals\"", "in", "output", ":", "\n", "                ", "prediction", "[", "\"proposals\"", "]", "=", "output", "[", "\"proposals\"", "]", ".", "to", "(", "self", ".", "_cpu_device", ")", "\n", "", "if", "len", "(", "prediction", ")", ">", "1", ":", "\n", "                ", "self", ".", "_predictions", ".", "append", "(", "prediction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator.evaluate": [[147, 179], ["collections.OrderedDict", "copy.deepcopy", "detectron2.synchronize", "detectron2.gather", "list", "len", "coco_evaluation.COCOEvaluator._logger.warning", "detectron2.utils.file_io.PathManager.mkdirs", "os.path.join", "coco_evaluation.COCOEvaluator._eval_box_proposals", "coco_evaluation.COCOEvaluator._eval_predictions", "itertools.chain", "detectron2.is_main_process", "detectron2.utils.file_io.PathManager.open", "torch.save"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._eval_box_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOEvaluator._eval_predictions", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save"], ["", "", "", "def", "evaluate", "(", "self", ",", "img_ids", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img_ids: a list of image IDs to evaluate on. Default to None for the whole dataset\n        \"\"\"", "\n", "if", "self", ".", "_distributed", ":", "\n", "            ", "comm", ".", "synchronize", "(", ")", "\n", "predictions", "=", "comm", ".", "gather", "(", "self", ".", "_predictions", ",", "dst", "=", "0", ")", "\n", "predictions", "=", "list", "(", "itertools", ".", "chain", "(", "*", "predictions", ")", ")", "\n", "\n", "if", "not", "comm", ".", "is_main_process", "(", ")", ":", "\n", "                ", "return", "{", "}", "\n", "", "", "else", ":", "\n", "            ", "predictions", "=", "self", ".", "_predictions", "\n", "\n", "", "if", "len", "(", "predictions", ")", "==", "0", ":", "\n", "            ", "self", ".", "_logger", ".", "warning", "(", "\"[COCOEvaluator] Did not receive valid predictions.\"", ")", "\n", "return", "{", "}", "\n", "\n", "", "if", "self", ".", "_output_dir", ":", "\n", "            ", "PathManager", ".", "mkdirs", "(", "self", ".", "_output_dir", ")", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_output_dir", ",", "\"instances_predictions.pth\"", ")", "\n", "with", "PathManager", ".", "open", "(", "file_path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "torch", ".", "save", "(", "predictions", ",", "f", ")", "\n", "\n", "", "", "self", ".", "_results", "=", "OrderedDict", "(", ")", "\n", "if", "\"proposals\"", "in", "predictions", "[", "0", "]", ":", "\n", "            ", "self", ".", "_eval_box_proposals", "(", "predictions", ")", "\n", "", "if", "\"instances\"", "in", "predictions", "[", "0", "]", ":", "\n", "            ", "self", ".", "_eval_predictions", "(", "predictions", ",", "img_ids", "=", "img_ids", ")", "\n", "# Copy so the caller can do whatever with results", "\n", "", "return", "copy", ".", "deepcopy", "(", "self", ".", "_results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._tasks_from_predictions": [[180, 191], ["sorted", "tasks.add", "tasks.add"], "methods", ["None"], ["", "def", "_tasks_from_predictions", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "\"\"\"\n        Get COCO API \"tasks\" (i.e. iou_type) from COCO-format predictions.\n        \"\"\"", "\n", "tasks", "=", "{", "\"bbox\"", "}", "\n", "for", "pred", "in", "predictions", ":", "\n", "            ", "if", "\"segmentation\"", "in", "pred", ":", "\n", "                ", "tasks", ".", "add", "(", "\"segm\"", ")", "\n", "", "if", "\"keypoints\"", "in", "pred", ":", "\n", "                ", "tasks", ".", "add", "(", "\"keypoints\"", ")", "\n", "", "", "return", "sorted", "(", "tasks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._eval_predictions": [[192, 252], ["coco_evaluation.COCOEvaluator._logger.info", "list", "hasattr", "coco_evaluation.COCOEvaluator._logger.info", "sorted", "itertools.chain", "coco_evaluation.COCOEvaluator._tasks_from_predictions", "list", "len", "os.path.join", "coco_evaluation.COCOEvaluator._logger.info", "coco_evaluation.COCOEvaluator._logger.info", "coco_evaluation.COCOEvaluator._derive_coco_results", "dataset_id_to_contiguous_id.values", "detectron2.utils.file_io.PathManager.open", "f.write", "f.flush", "coco_evaluation._evaluate_predictions_on_coco", "min", "max", "dataset_id_to_contiguous_id.items", "json.dumps", "len", "coco_evaluation.COCOEvaluator._metadata.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._tasks_from_predictions", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._derive_coco_results", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOEvaluator._evaluate_predictions_on_coco", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "_eval_predictions", "(", "self", ",", "predictions", ",", "img_ids", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate predictions. Fill self._results with the metrics of the tasks.\n        \"\"\"", "\n", "self", ".", "_logger", ".", "info", "(", "\"Preparing results for COCO format ...\"", ")", "\n", "coco_results", "=", "list", "(", "itertools", ".", "chain", "(", "*", "[", "x", "[", "\"instances\"", "]", "for", "x", "in", "predictions", "]", ")", ")", "\n", "tasks", "=", "self", ".", "_tasks", "or", "self", ".", "_tasks_from_predictions", "(", "coco_results", ")", "\n", "\n", "# unmap the category ids for COCO", "\n", "if", "hasattr", "(", "self", ".", "_metadata", ",", "\"thing_dataset_id_to_contiguous_id\"", ")", ":", "\n", "            ", "dataset_id_to_contiguous_id", "=", "self", ".", "_metadata", ".", "thing_dataset_id_to_contiguous_id", "\n", "all_contiguous_ids", "=", "list", "(", "dataset_id_to_contiguous_id", ".", "values", "(", ")", ")", "\n", "num_classes", "=", "len", "(", "all_contiguous_ids", ")", "\n", "assert", "min", "(", "all_contiguous_ids", ")", "==", "0", "and", "max", "(", "all_contiguous_ids", ")", "==", "num_classes", "-", "1", "\n", "\n", "reverse_id_mapping", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "dataset_id_to_contiguous_id", ".", "items", "(", ")", "}", "\n", "for", "result", "in", "coco_results", ":", "\n", "                ", "category_id", "=", "result", "[", "\"category_id\"", "]", "\n", "assert", "category_id", "<", "num_classes", ",", "(", "\n", "f\"A prediction has class={category_id}, \"", "\n", "f\"but the dataset only has {num_classes} classes and \"", "\n", "f\"predicted class id should be in [0, {num_classes - 1}].\"", "\n", ")", "\n", "result", "[", "\"category_id\"", "]", "=", "reverse_id_mapping", "[", "category_id", "]", "\n", "\n", "", "", "if", "self", ".", "_output_dir", ":", "\n", "            ", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_output_dir", ",", "\"coco_instances_results.json\"", ")", "\n", "self", ".", "_logger", ".", "info", "(", "\"Saving results to {}\"", ".", "format", "(", "file_path", ")", ")", "\n", "with", "PathManager", ".", "open", "(", "file_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "json", ".", "dumps", "(", "coco_results", ")", ")", "\n", "f", ".", "flush", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "_do_evaluation", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\"Annotations are not available for evaluation.\"", ")", "\n", "return", "\n", "\n", "", "self", ".", "_logger", ".", "info", "(", "\n", "\"Evaluating predictions with {} COCO API...\"", ".", "format", "(", "\n", "\"unofficial\"", "if", "self", ".", "_use_fast_impl", "else", "\"official\"", "\n", ")", "\n", ")", "\n", "for", "task", "in", "sorted", "(", "tasks", ")", ":", "\n", "            ", "assert", "task", "in", "{", "\"bbox\"", ",", "\"segm\"", ",", "\"keypoints\"", "}", ",", "f\"Got unknown task: {task}!\"", "\n", "coco_eval", "=", "(", "\n", "_evaluate_predictions_on_coco", "(", "\n", "self", ".", "_coco_api", ",", "\n", "coco_results", ",", "\n", "task", ",", "\n", "kpt_oks_sigmas", "=", "self", ".", "_kpt_oks_sigmas", ",", "\n", "use_fast_impl", "=", "self", ".", "_use_fast_impl", ",", "\n", "img_ids", "=", "img_ids", ",", "\n", ")", "\n", "if", "len", "(", "coco_results", ")", ">", "0", "\n", "else", "None", "# cocoapi does not handle empty results very well", "\n", ")", "\n", "\n", "res", "=", "self", ".", "_derive_coco_results", "(", "\n", "coco_eval", ",", "task", ",", "class_names", "=", "self", ".", "_metadata", ".", "get", "(", "\"thing_classes\"", ")", "\n", ")", "\n", "self", ".", "_results", "[", "task", "]", "=", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._eval_box_proposals": [[253, 291], ["coco_evaluation.COCOEvaluator._logger.info", "coco_evaluation.COCOEvaluator._logger.info", "coco_evaluation.COCOEvaluator._logger.info", "areas.items", "ids.append", "boxes.append", "objectness_logits.append", "detectron2.utils.file_io.PathManager.open", "pickle.dump", "coco_evaluation._evaluate_box_proposals", "float", "detectron2.utils.logger.create_small_table", "prediction[].proposal_boxes.tensor.numpy", "prediction[].objectness_logits.numpy", "os.path.join", "stats[].item"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.CfgNode.dump", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation._evaluate_box_proposals", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.create_small_table"], ["", "", "def", "_eval_box_proposals", "(", "self", ",", "predictions", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate the box proposals in predictions.\n        Fill self._results with the metrics for \"box_proposals\" task.\n        \"\"\"", "\n", "if", "self", ".", "_output_dir", ":", "\n", "# Saving generated box proposals to file.", "\n", "# Predicted box_proposals are in XYXY_ABS mode.", "\n", "            ", "bbox_mode", "=", "BoxMode", ".", "XYXY_ABS", ".", "value", "\n", "ids", ",", "boxes", ",", "objectness_logits", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "prediction", "in", "predictions", ":", "\n", "                ", "ids", ".", "append", "(", "prediction", "[", "\"image_id\"", "]", ")", "\n", "boxes", ".", "append", "(", "prediction", "[", "\"proposals\"", "]", ".", "proposal_boxes", ".", "tensor", ".", "numpy", "(", ")", ")", "\n", "objectness_logits", ".", "append", "(", "prediction", "[", "\"proposals\"", "]", ".", "objectness_logits", ".", "numpy", "(", ")", ")", "\n", "\n", "", "proposal_data", "=", "{", "\n", "\"boxes\"", ":", "boxes", ",", "\n", "\"objectness_logits\"", ":", "objectness_logits", ",", "\n", "\"ids\"", ":", "ids", ",", "\n", "\"bbox_mode\"", ":", "bbox_mode", ",", "\n", "}", "\n", "with", "PathManager", ".", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "_output_dir", ",", "\"box_proposals.pkl\"", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "proposal_data", ",", "f", ")", "\n", "\n", "", "", "if", "not", "self", ".", "_do_evaluation", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\"Annotations are not available for evaluation.\"", ")", "\n", "return", "\n", "\n", "", "self", ".", "_logger", ".", "info", "(", "\"Evaluating bbox proposals ...\"", ")", "\n", "res", "=", "{", "}", "\n", "areas", "=", "{", "\"all\"", ":", "\"\"", ",", "\"small\"", ":", "\"s\"", ",", "\"medium\"", ":", "\"m\"", ",", "\"large\"", ":", "\"l\"", "}", "\n", "for", "limit", "in", "[", "100", ",", "1000", "]", ":", "\n", "            ", "for", "area", ",", "suffix", "in", "areas", ".", "items", "(", ")", ":", "\n", "                ", "stats", "=", "_evaluate_box_proposals", "(", "predictions", ",", "self", ".", "_coco_api", ",", "area", "=", "area", ",", "limit", "=", "limit", ")", "\n", "key", "=", "\"AR{}@{:d}\"", ".", "format", "(", "suffix", ",", "limit", ")", "\n", "res", "[", "key", "]", "=", "float", "(", "stats", "[", "\"ar\"", "]", ".", "item", "(", ")", "*", "100", ")", "\n", "", "", "self", ".", "_logger", ".", "info", "(", "\"Proposal metrics: \\n\"", "+", "create_small_table", "(", "res", ")", ")", "\n", "self", ".", "_results", "[", "\"box_proposals\"", "]", "=", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._derive_coco_results": [[292, 389], ["coco_evaluation.COCOEvaluator._logger.info", "enumerate", "min", "list", "itertools.zip_longest", "tabulate.tabulate.tabulate", "coco_evaluation.COCOEvaluator._logger.info", "results.update", "coco_evaluation.COCOEvaluator._logger.warn", "float", "numpy.isfinite", "coco_evaluation.COCOEvaluator._logger.info", "len", "results_per_category.append", "len", "itertools.chain", "float", "enumerate", "detectron2.utils.logger.create_small_table", "sum", "len", "numpy.mean", "float", "numpy.where", "coco_evaluation.COCOEvaluator._logger.info", "len", "results.values", "float", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "cinds.extend", "len", "numpy.mean", "range", "enumerate"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.create_small_table"], ["", "def", "_derive_coco_results", "(", "self", ",", "coco_eval", ",", "iou_type", ",", "class_names", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Derive the desired score numbers from summarized COCOeval.\n\n        Args:\n            coco_eval (None or COCOEval): None represents no predictions from model.\n            iou_type (str):\n            class_names (None or list[str]): if provided, will use it to predict\n                per-category AP.\n\n        Returns:\n            a dict of {metric name: score}\n        \"\"\"", "\n", "\n", "metrics", "=", "{", "\n", "\"bbox\"", ":", "[", "\"AP\"", ",", "\"AP50\"", ",", "\"AP75\"", ",", "\"APs\"", ",", "\"APm\"", ",", "\"APl\"", "]", ",", "\n", "\"segm\"", ":", "[", "\"AP\"", ",", "\"AP50\"", ",", "\"AP75\"", ",", "\"APs\"", ",", "\"APm\"", ",", "\"APl\"", "]", ",", "\n", "\"keypoints\"", ":", "[", "\"AP\"", ",", "\"AP50\"", ",", "\"AP75\"", ",", "\"APm\"", ",", "\"APl\"", "]", ",", "\n", "}", "[", "iou_type", "]", "\n", "\n", "if", "coco_eval", "is", "None", ":", "\n", "            ", "self", ".", "_logger", ".", "warn", "(", "\"No predictions from the model!\"", ")", "\n", "return", "{", "metric", ":", "float", "(", "\"nan\"", ")", "for", "metric", "in", "metrics", "}", "\n", "\n", "# the standard metrics", "\n", "", "results", "=", "{", "\n", "metric", ":", "float", "(", "coco_eval", ".", "stats", "[", "idx", "]", "*", "100", "if", "coco_eval", ".", "stats", "[", "idx", "]", ">=", "0", "else", "\"nan\"", ")", "\n", "for", "idx", ",", "metric", "in", "enumerate", "(", "metrics", ")", "\n", "}", "\n", "self", ".", "_logger", ".", "info", "(", "\n", "\"Evaluation results for {}: \\n\"", ".", "format", "(", "iou_type", ")", "+", "create_small_table", "(", "results", ")", "\n", ")", "\n", "if", "not", "np", ".", "isfinite", "(", "sum", "(", "results", ".", "values", "(", ")", ")", ")", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\"Some metrics cannot be computed and is shown as NaN.\"", ")", "\n", "\n", "", "if", "class_names", "is", "None", "or", "len", "(", "class_names", ")", "<=", "1", ":", "\n", "            ", "return", "results", "\n", "# Compute per-category AP", "\n", "# from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa", "\n", "", "precisions", "=", "coco_eval", ".", "eval", "[", "\"precision\"", "]", "\n", "# precision has dims (iou, recall, cls, area range, max dets)", "\n", "assert", "len", "(", "class_names", ")", "==", "precisions", ".", "shape", "[", "2", "]", "\n", "\n", "results_per_category", "=", "[", "]", "\n", "for", "idx", ",", "name", "in", "enumerate", "(", "class_names", ")", ":", "\n", "# area range index 0: all area ranges", "\n", "# max dets index -1: typically 100 per image", "\n", "            ", "precision", "=", "precisions", "[", ":", ",", ":", ",", "idx", ",", "0", ",", "-", "1", "]", "\n", "precision", "=", "precision", "[", "precision", ">", "-", "1", "]", "\n", "ap", "=", "np", ".", "mean", "(", "precision", ")", "if", "precision", ".", "size", "else", "float", "(", "\"nan\"", ")", "\n", "results_per_category", ".", "append", "(", "(", "\"{}\"", ".", "format", "(", "name", ")", ",", "float", "(", "ap", "*", "100", ")", ")", ")", "\n", "\n", "# Computing AP50 for (seen/unseen) split in generalized zeroshot setting (eg. all 65 categories)", "\n", "# from https://github.com/alirezazareian/ovr-cnn/blob/master/maskrcnn_benchmark/data/datasets/evaluation/coco/coco_eval.py", "\n", "", "if", "len", "(", "class_names", ")", "==", "65", ":", "\n", "            ", "p", "=", "coco_eval", ".", "params", "\n", "maxDets", "=", "p", ".", "maxDets", "[", "2", "]", "\n", "areaRng", "=", "'all'", "\n", "iouThr", "=", "0.5", "\n", "aind", "=", "[", "i", "for", "i", ",", "aRng", "in", "enumerate", "(", "p", ".", "areaRngLbl", ")", "if", "aRng", "==", "areaRng", "]", "\n", "mind", "=", "[", "i", "for", "i", ",", "mDet", "in", "enumerate", "(", "p", ".", "maxDets", ")", "if", "mDet", "==", "maxDets", "]", "\n", "t", "=", "np", ".", "where", "(", "iouThr", "==", "p", ".", "iouThrs", ")", "[", "0", "]", "\n", "s", "=", "coco_eval", ".", "eval", "[", "'precision'", "]", "\n", "s", "=", "s", "[", "t", ",", ":", ",", ":", ",", "aind", ",", "mind", "]", "\n", "\n", "unseen_cids", "=", "[", "p", ".", "catIds", "[", "i", "]", "for", "i", ",", "c", "in", "enumerate", "(", "class_names", ")", "if", "c", "in", "COCO_UNSEEN_CLS", "]", "\n", "seen_cids", "=", "[", "p", ".", "catIds", "[", "i", "]", "for", "i", ",", "c", "in", "enumerate", "(", "class_names", ")", "if", "c", "in", "COCO_SEEN_CLS", "]", "\n", "all_cids", "=", "[", "p", ".", "catIds", "[", "i", "]", "for", "i", ",", "c", "in", "enumerate", "(", "class_names", ")", "if", "c", "in", "COCO_OVD_ALL_CLS", "]", "\n", "res", "=", "{", "}", "\n", "for", "split", ",", "cid_list", "in", "[", "(", "'target'", ",", "unseen_cids", ")", ",", "(", "'base'", ",", "seen_cids", ")", ",", "(", "'all'", ",", "all_cids", ")", "]", ":", "\n", "                ", "cinds", "=", "[", "]", "\n", "for", "cid", "in", "cid_list", ":", "\n", "                    ", "cinds", ".", "extend", "(", "[", "i", "for", "i", ",", "c", "in", "enumerate", "(", "p", ".", "catIds", ")", "if", "c", "==", "cid", "]", ")", "\n", "", "s_split", "=", "s", "[", ":", ",", ":", ",", "cinds", "]", "\n", "if", "len", "(", "s_split", "[", "s_split", ">", "-", "1", "]", ")", "==", "0", ":", "\n", "                    ", "mean_s", "=", "-", "1", "\n", "", "else", ":", "\n", "                    ", "mean_s", "=", "np", ".", "mean", "(", "s_split", "[", "s_split", ">", "-", "1", "]", ")", "\n", "", "res", "[", "f'AP50_split_{split}'", "]", "=", "mean_s", "\n", "", "for", "res_item", "in", "res", ":", "\n", "                ", "self", ".", "_logger", ".", "info", "(", "\"{} AP: {}\\n\"", ".", "format", "(", "res_item", ",", "res", "[", "res_item", "]", ")", ")", "\n", "\n", "# tabulate it", "\n", "", "", "N_COLS", "=", "min", "(", "6", ",", "len", "(", "results_per_category", ")", "*", "2", ")", "\n", "results_flatten", "=", "list", "(", "itertools", ".", "chain", "(", "*", "results_per_category", ")", ")", "\n", "results_2d", "=", "itertools", ".", "zip_longest", "(", "*", "[", "results_flatten", "[", "i", ":", ":", "N_COLS", "]", "for", "i", "in", "range", "(", "N_COLS", ")", "]", ")", "\n", "table", "=", "tabulate", "(", "\n", "results_2d", ",", "\n", "tablefmt", "=", "\"pipe\"", ",", "\n", "floatfmt", "=", "\".3f\"", ",", "\n", "headers", "=", "[", "\"category\"", ",", "\"AP\"", "]", "*", "(", "N_COLS", "//", "2", ")", ",", "\n", "numalign", "=", "\"left\"", ",", "\n", ")", "\n", "self", ".", "_logger", ".", "info", "(", "\"Per-category {} AP: \\n\"", ".", "format", "(", "iou_type", ")", "+", "table", ")", "\n", "\n", "results", ".", "update", "(", "{", "\"AP-\"", "+", "name", ":", "ap", "for", "name", ",", "ap", "in", "results_per_category", "}", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.instances_to_coco_json": [[391, 451], ["len", "instances.pred_boxes.tensor.numpy", "detectron2.structures.BoxMode.convert", "boxes.tolist.tolist", "instances.scores.tolist", "instances.pred_classes.tolist", "instances.has", "instances.has", "range", "results.append", "rle[].decode", "keypoints[].flatten().tolist", "pycocotools.encode", "numpy.array", "keypoints[].flatten"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.export.c10.InstancesList.has", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.decode", "home.repos.pwc.inspect_result.microsoft_regionclip.clip_datasets.clip_prompt_engineering.SimpleTokenizer.encode", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["", "", "def", "instances_to_coco_json", "(", "instances", ",", "img_id", ")", ":", "\n", "    ", "\"\"\"\n    Dump an \"Instances\" object to a COCO-format json that's used for evaluation.\n\n    Args:\n        instances (Instances):\n        img_id (int): the image id\n\n    Returns:\n        list[dict]: list of json annotations in COCO format.\n    \"\"\"", "\n", "num_instance", "=", "len", "(", "instances", ")", "\n", "if", "num_instance", "==", "0", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "boxes", "=", "instances", ".", "pred_boxes", ".", "tensor", ".", "numpy", "(", ")", "\n", "boxes", "=", "BoxMode", ".", "convert", "(", "boxes", ",", "BoxMode", ".", "XYXY_ABS", ",", "BoxMode", ".", "XYWH_ABS", ")", "\n", "boxes", "=", "boxes", ".", "tolist", "(", ")", "\n", "scores", "=", "instances", ".", "scores", ".", "tolist", "(", ")", "\n", "classes", "=", "instances", ".", "pred_classes", ".", "tolist", "(", ")", "\n", "\n", "has_mask", "=", "instances", ".", "has", "(", "\"pred_masks\"", ")", "\n", "if", "has_mask", ":", "\n", "# use RLE to encode the masks, because they are too large and takes memory", "\n", "# since this evaluator stores outputs of the entire dataset", "\n", "        ", "rles", "=", "[", "\n", "mask_util", ".", "encode", "(", "np", ".", "array", "(", "mask", "[", ":", ",", ":", ",", "None", "]", ",", "order", "=", "\"F\"", ",", "dtype", "=", "\"uint8\"", ")", ")", "[", "0", "]", "\n", "for", "mask", "in", "instances", ".", "pred_masks", "\n", "]", "\n", "for", "rle", "in", "rles", ":", "\n", "# \"counts\" is an array encoded by mask_util as a byte-stream. Python3's", "\n", "# json writer which always produces strings cannot serialize a bytestream", "\n", "# unless you decode it. Thankfully, utf-8 works out (which is also what", "\n", "# the pycocotools/_mask.pyx does).", "\n", "            ", "rle", "[", "\"counts\"", "]", "=", "rle", "[", "\"counts\"", "]", ".", "decode", "(", "\"utf-8\"", ")", "\n", "\n", "", "", "has_keypoints", "=", "instances", ".", "has", "(", "\"pred_keypoints\"", ")", "\n", "if", "has_keypoints", ":", "\n", "        ", "keypoints", "=", "instances", ".", "pred_keypoints", "\n", "\n", "", "results", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "num_instance", ")", ":", "\n", "        ", "result", "=", "{", "\n", "\"image_id\"", ":", "img_id", ",", "\n", "\"category_id\"", ":", "classes", "[", "k", "]", ",", "\n", "\"bbox\"", ":", "boxes", "[", "k", "]", ",", "\n", "\"score\"", ":", "scores", "[", "k", "]", ",", "\n", "}", "\n", "if", "has_mask", ":", "\n", "            ", "result", "[", "\"segmentation\"", "]", "=", "rles", "[", "k", "]", "\n", "", "if", "has_keypoints", ":", "\n", "# In COCO annotations,", "\n", "# keypoints coordinates are pixel indices.", "\n", "# However our predictions are floating point coordinates.", "\n", "# Therefore we subtract 0.5 to be consistent with the annotation format.", "\n", "# This is the inverse of data loading logic in `datasets/coco.py`.", "\n", "            ", "keypoints", "[", "k", "]", "[", ":", ",", ":", "2", "]", "-=", "0.5", "\n", "result", "[", "\"keypoints\"", "]", "=", "keypoints", "[", "k", "]", ".", "flatten", "(", ")", ".", "tolist", "(", ")", "\n", "", "results", ".", "append", "(", "result", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation._evaluate_box_proposals": [[455, 563], ["torch.sort", "torch.zeros_like", "enumerate", "torch.zeros_like.mean", "coco_api.getAnnIds", "coco_api.loadAnns", "torch.as_tensor().reshape", "detectron2.structures.Boxes", "torch.as_tensor", "len", "detectron2.structures.pairwise_iou", "torch.zeros", "range", "gt_overlaps.append", "len", "torch.cat", "torch.zeros", "torch.arange", "predictions.objectness_logits.sort", "detectron2.structures.BoxMode.convert", "len", "len", "min", "detectron2.structures.pairwise_iou.max", "max_overlaps.max", "float", "torch.as_tensor", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.pairwise_iou", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cat", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "def", "_evaluate_box_proposals", "(", "dataset_predictions", ",", "coco_api", ",", "thresholds", "=", "None", ",", "area", "=", "\"all\"", ",", "limit", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Evaluate detection proposal recall metrics. This function is a much\n    faster alternative to the official COCO API recall evaluation code. However,\n    it produces slightly different results.\n    \"\"\"", "\n", "# Record max overlap value for each gt box", "\n", "# Return vector of overlap values", "\n", "areas", "=", "{", "\n", "\"all\"", ":", "0", ",", "\n", "\"small\"", ":", "1", ",", "\n", "\"medium\"", ":", "2", ",", "\n", "\"large\"", ":", "3", ",", "\n", "\"96-128\"", ":", "4", ",", "\n", "\"128-256\"", ":", "5", ",", "\n", "\"256-512\"", ":", "6", ",", "\n", "\"512-inf\"", ":", "7", ",", "\n", "}", "\n", "area_ranges", "=", "[", "\n", "[", "0", "**", "2", ",", "1e5", "**", "2", "]", ",", "# all", "\n", "[", "0", "**", "2", ",", "32", "**", "2", "]", ",", "# small", "\n", "[", "32", "**", "2", ",", "96", "**", "2", "]", ",", "# medium", "\n", "[", "96", "**", "2", ",", "1e5", "**", "2", "]", ",", "# large", "\n", "[", "96", "**", "2", ",", "128", "**", "2", "]", ",", "# 96-128", "\n", "[", "128", "**", "2", ",", "256", "**", "2", "]", ",", "# 128-256", "\n", "[", "256", "**", "2", ",", "512", "**", "2", "]", ",", "# 256-512", "\n", "[", "512", "**", "2", ",", "1e5", "**", "2", "]", ",", "\n", "]", "# 512-inf", "\n", "assert", "area", "in", "areas", ",", "\"Unknown area range: {}\"", ".", "format", "(", "area", ")", "\n", "area_range", "=", "area_ranges", "[", "areas", "[", "area", "]", "]", "\n", "gt_overlaps", "=", "[", "]", "\n", "num_pos", "=", "0", "\n", "\n", "for", "prediction_dict", "in", "dataset_predictions", ":", "\n", "        ", "predictions", "=", "prediction_dict", "[", "\"proposals\"", "]", "\n", "\n", "# sort predictions in descending order", "\n", "# TODO maybe remove this and make it explicit in the documentation", "\n", "inds", "=", "predictions", ".", "objectness_logits", ".", "sort", "(", "descending", "=", "True", ")", "[", "1", "]", "\n", "predictions", "=", "predictions", "[", "inds", "]", "\n", "\n", "ann_ids", "=", "coco_api", ".", "getAnnIds", "(", "imgIds", "=", "prediction_dict", "[", "\"image_id\"", "]", ")", "\n", "anno", "=", "coco_api", ".", "loadAnns", "(", "ann_ids", ")", "\n", "gt_boxes", "=", "[", "\n", "BoxMode", ".", "convert", "(", "obj", "[", "\"bbox\"", "]", ",", "BoxMode", ".", "XYWH_ABS", ",", "BoxMode", ".", "XYXY_ABS", ")", "\n", "for", "obj", "in", "anno", "\n", "if", "obj", "[", "\"iscrowd\"", "]", "==", "0", "\n", "]", "\n", "gt_boxes", "=", "torch", ".", "as_tensor", "(", "gt_boxes", ")", ".", "reshape", "(", "-", "1", ",", "4", ")", "# guard against no boxes", "\n", "gt_boxes", "=", "Boxes", "(", "gt_boxes", ")", "\n", "gt_areas", "=", "torch", ".", "as_tensor", "(", "[", "obj", "[", "\"area\"", "]", "for", "obj", "in", "anno", "if", "obj", "[", "\"iscrowd\"", "]", "==", "0", "]", ")", "\n", "\n", "if", "len", "(", "gt_boxes", ")", "==", "0", "or", "len", "(", "predictions", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "", "valid_gt_inds", "=", "(", "gt_areas", ">=", "area_range", "[", "0", "]", ")", "&", "(", "gt_areas", "<=", "area_range", "[", "1", "]", ")", "\n", "gt_boxes", "=", "gt_boxes", "[", "valid_gt_inds", "]", "\n", "\n", "num_pos", "+=", "len", "(", "gt_boxes", ")", "\n", "\n", "if", "len", "(", "gt_boxes", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "limit", "is", "not", "None", "and", "len", "(", "predictions", ")", ">", "limit", ":", "\n", "            ", "predictions", "=", "predictions", "[", ":", "limit", "]", "\n", "\n", "", "overlaps", "=", "pairwise_iou", "(", "predictions", ".", "proposal_boxes", ",", "gt_boxes", ")", "\n", "\n", "_gt_overlaps", "=", "torch", ".", "zeros", "(", "len", "(", "gt_boxes", ")", ")", "\n", "for", "j", "in", "range", "(", "min", "(", "len", "(", "predictions", ")", ",", "len", "(", "gt_boxes", ")", ")", ")", ":", "\n", "# find which proposal box maximally covers each gt box", "\n", "# and get the iou amount of coverage for each gt box", "\n", "            ", "max_overlaps", ",", "argmax_overlaps", "=", "overlaps", ".", "max", "(", "dim", "=", "0", ")", "\n", "\n", "# find which gt box is 'best' covered (i.e. 'best' = most iou)", "\n", "gt_ovr", ",", "gt_ind", "=", "max_overlaps", ".", "max", "(", "dim", "=", "0", ")", "\n", "assert", "gt_ovr", ">=", "0", "\n", "# find the proposal box that covers the best covered gt box", "\n", "box_ind", "=", "argmax_overlaps", "[", "gt_ind", "]", "\n", "# record the iou coverage of this gt box", "\n", "_gt_overlaps", "[", "j", "]", "=", "overlaps", "[", "box_ind", ",", "gt_ind", "]", "\n", "assert", "_gt_overlaps", "[", "j", "]", "==", "gt_ovr", "\n", "# mark the proposal box and the gt box as used", "\n", "overlaps", "[", "box_ind", ",", ":", "]", "=", "-", "1", "\n", "overlaps", "[", ":", ",", "gt_ind", "]", "=", "-", "1", "\n", "\n", "# append recorded iou coverage level", "\n", "", "gt_overlaps", ".", "append", "(", "_gt_overlaps", ")", "\n", "", "gt_overlaps", "=", "(", "\n", "torch", ".", "cat", "(", "gt_overlaps", ",", "dim", "=", "0", ")", "if", "len", "(", "gt_overlaps", ")", "else", "torch", ".", "zeros", "(", "0", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", ")", "\n", "gt_overlaps", ",", "_", "=", "torch", ".", "sort", "(", "gt_overlaps", ")", "\n", "\n", "if", "thresholds", "is", "None", ":", "\n", "        ", "step", "=", "0.05", "\n", "thresholds", "=", "torch", ".", "arange", "(", "0.5", ",", "0.95", "+", "1e-5", ",", "step", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "", "recalls", "=", "torch", ".", "zeros_like", "(", "thresholds", ")", "\n", "# compute recall for each iou threshold", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "thresholds", ")", ":", "\n", "        ", "recalls", "[", "i", "]", "=", "(", "gt_overlaps", ">=", "t", ")", ".", "float", "(", ")", ".", "sum", "(", ")", "/", "float", "(", "num_pos", ")", "\n", "# ar = 2 * np.trapz(recalls, thresholds)", "\n", "", "ar", "=", "recalls", ".", "mean", "(", ")", "\n", "return", "{", "\n", "\"ar\"", ":", "ar", ",", "\n", "\"recalls\"", ":", "recalls", ",", "\n", "\"thresholds\"", ":", "thresholds", ",", "\n", "\"gt_overlaps\"", ":", "gt_overlaps", ",", "\n", "\"num_pos\"", ":", "num_pos", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation._evaluate_predictions_on_coco": [[566, 611], ["coco_gt.loadRes", "coco_eval.evaluate", "coco_eval.accumulate", "coco_eval.summarize", "len", "copy.deepcopy", "len", "c.pop", "hasattr", "numpy.array", "len", "len", "next", "iter", "coco_gt.anns.values"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.evaluate", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.fast_eval_api.COCOeval_opt.accumulate", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.iter"], ["", "def", "_evaluate_predictions_on_coco", "(", "\n", "coco_gt", ",", "coco_results", ",", "iou_type", ",", "kpt_oks_sigmas", "=", "None", ",", "use_fast_impl", "=", "True", ",", "img_ids", "=", "None", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Evaluate the coco results using COCOEval API.\n    \"\"\"", "\n", "assert", "len", "(", "coco_results", ")", ">", "0", "\n", "\n", "if", "iou_type", "==", "\"segm\"", ":", "\n", "        ", "coco_results", "=", "copy", ".", "deepcopy", "(", "coco_results", ")", "\n", "# When evaluating mask AP, if the results contain bbox, cocoapi will", "\n", "# use the box area as the area of the instance, instead of the mask area.", "\n", "# This leads to a different definition of small/medium/large.", "\n", "# We remove the bbox field to let mask AP use mask area.", "\n", "for", "c", "in", "coco_results", ":", "\n", "            ", "c", ".", "pop", "(", "\"bbox\"", ",", "None", ")", "\n", "\n", "", "", "coco_dt", "=", "coco_gt", ".", "loadRes", "(", "coco_results", ")", "\n", "coco_eval", "=", "(", "COCOeval_opt", "if", "use_fast_impl", "else", "COCOeval", ")", "(", "coco_gt", ",", "coco_dt", ",", "iou_type", ")", "\n", "if", "img_ids", "is", "not", "None", ":", "\n", "        ", "coco_eval", ".", "params", ".", "imgIds", "=", "img_ids", "\n", "\n", "", "if", "iou_type", "==", "\"keypoints\"", ":", "\n", "# Use the COCO default keypoint OKS sigmas unless overrides are specified", "\n", "        ", "if", "kpt_oks_sigmas", ":", "\n", "            ", "assert", "hasattr", "(", "coco_eval", ".", "params", ",", "\"kpt_oks_sigmas\"", ")", ",", "\"pycocotools is too old!\"", "\n", "coco_eval", ".", "params", ".", "kpt_oks_sigmas", "=", "np", ".", "array", "(", "kpt_oks_sigmas", ")", "\n", "# COCOAPI requires every detection and every gt to have keypoints, so", "\n", "# we just take the first entry from both", "\n", "", "num_keypoints_dt", "=", "len", "(", "coco_results", "[", "0", "]", "[", "\"keypoints\"", "]", ")", "//", "3", "\n", "num_keypoints_gt", "=", "len", "(", "next", "(", "iter", "(", "coco_gt", ".", "anns", ".", "values", "(", ")", ")", ")", "[", "\"keypoints\"", "]", ")", "//", "3", "\n", "num_keypoints_oks", "=", "len", "(", "coco_eval", ".", "params", ".", "kpt_oks_sigmas", ")", "\n", "assert", "num_keypoints_oks", "==", "num_keypoints_dt", "==", "num_keypoints_gt", ",", "(", "\n", "f\"[COCOEvaluator] Prediction contain {num_keypoints_dt} keypoints. \"", "\n", "f\"Ground truth contains {num_keypoints_gt} keypoints. \"", "\n", "f\"The length of cfg.TEST.KEYPOINT_OKS_SIGMAS is {num_keypoints_oks}. \"", "\n", "\"They have to agree with each other. For meaning of OKS, please refer to \"", "\n", "\"http://cocodataset.org/#keypoints-eval.\"", "\n", ")", "\n", "\n", "", "coco_eval", ".", "evaluate", "(", ")", "\n", "coco_eval", ".", "accumulate", "(", ")", "\n", "coco_eval", ".", "summarize", "(", ")", "\n", "\n", "return", "coco_eval", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation.COCOPanopticEvaluator.__init__": [[32, 49], ["detectron2.data.MetadataCatalog.get", "detectron2.utils.file_io.PathManager.mkdirs", "panoptic_evaluation.COCOPanopticEvaluator._metadata.thing_dataset_id_to_contiguous_id.items", "panoptic_evaluation.COCOPanopticEvaluator._metadata.stuff_dataset_id_to_contiguous_id.items"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["def", "__init__", "(", "self", ",", "dataset_name", ":", "str", ",", "output_dir", ":", "Optional", "[", "str", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dataset_name: name of the dataset\n            output_dir: output directory to save results for evaluation.\n        \"\"\"", "\n", "self", ".", "_metadata", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", "\n", "self", ".", "_thing_contiguous_id_to_dataset_id", "=", "{", "\n", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "_metadata", ".", "thing_dataset_id_to_contiguous_id", ".", "items", "(", ")", "\n", "}", "\n", "self", ".", "_stuff_contiguous_id_to_dataset_id", "=", "{", "\n", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "_metadata", ".", "stuff_dataset_id_to_contiguous_id", ".", "items", "(", ")", "\n", "}", "\n", "\n", "self", ".", "_output_dir", "=", "output_dir", "\n", "if", "self", ".", "_output_dir", "is", "not", "None", ":", "\n", "            ", "PathManager", ".", "mkdirs", "(", "self", ".", "_output_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation.COCOPanopticEvaluator.reset": [[50, 52], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_predictions", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation.COCOPanopticEvaluator._convert_category_id": [[53, 67], ["segment_info.pop"], "methods", ["None"], ["", "def", "_convert_category_id", "(", "self", ",", "segment_info", ")", ":", "\n", "        ", "isthing", "=", "segment_info", ".", "pop", "(", "\"isthing\"", ",", "None", ")", "\n", "if", "isthing", "is", "None", ":", "\n", "# the model produces panoptic category id directly. No more conversion needed", "\n", "            ", "return", "segment_info", "\n", "", "if", "isthing", "is", "True", ":", "\n", "            ", "segment_info", "[", "\"category_id\"", "]", "=", "self", ".", "_thing_contiguous_id_to_dataset_id", "[", "\n", "segment_info", "[", "\"category_id\"", "]", "\n", "]", "\n", "", "else", ":", "\n", "            ", "segment_info", "[", "\"category_id\"", "]", "=", "self", ".", "_stuff_contiguous_id_to_dataset_id", "[", "\n", "segment_info", "[", "\"category_id\"", "]", "\n", "]", "\n", "", "return", "segment_info", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation.COCOPanopticEvaluator.process": [[68, 111], ["zip", "panoptic_img.cpu().numpy.cpu().numpy.cpu().numpy", "os.path.basename", "numpy.unique", "io.BytesIO", "PIL.Image.fromarray().save", "panoptic_evaluation.COCOPanopticEvaluator._predictions.append", "panoptic_img.cpu().numpy.cpu().numpy.cpu", "segments_info.append", "os.path.splitext", "panoptic_evaluation.COCOPanopticEvaluator._convert_category_id", "panoptic_evaluation.COCOPanopticEvaluator._metadata.thing_dataset_id_to_contiguous_id.values", "PIL.Image.fromarray", "out.getvalue", "int", "bool", "id2rgb", "int"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation.COCOPanopticEvaluator._convert_category_id"], ["", "def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "from", "panopticapi", ".", "utils", "import", "id2rgb", "\n", "\n", "for", "input", ",", "output", "in", "zip", "(", "inputs", ",", "outputs", ")", ":", "\n", "            ", "panoptic_img", ",", "segments_info", "=", "output", "[", "\"panoptic_seg\"", "]", "\n", "panoptic_img", "=", "panoptic_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "segments_info", "is", "None", ":", "\n", "# If \"segments_info\" is None, we assume \"panoptic_img\" is a", "\n", "# H*W int32 image storing the panoptic_id in the format of", "\n", "# category_id * label_divisor + instance_id. We reserve -1 for", "\n", "# VOID label, and add 1 to panoptic_img since the official", "\n", "# evaluation script uses 0 for VOID label.", "\n", "                ", "label_divisor", "=", "self", ".", "_metadata", ".", "label_divisor", "\n", "segments_info", "=", "[", "]", "\n", "for", "panoptic_label", "in", "np", ".", "unique", "(", "panoptic_img", ")", ":", "\n", "                    ", "if", "panoptic_label", "==", "-", "1", ":", "\n", "# VOID region.", "\n", "                        ", "continue", "\n", "", "pred_class", "=", "panoptic_label", "//", "label_divisor", "\n", "isthing", "=", "(", "\n", "pred_class", "in", "self", ".", "_metadata", ".", "thing_dataset_id_to_contiguous_id", ".", "values", "(", ")", "\n", ")", "\n", "segments_info", ".", "append", "(", "\n", "{", "\n", "\"id\"", ":", "int", "(", "panoptic_label", ")", "+", "1", ",", "\n", "\"category_id\"", ":", "int", "(", "pred_class", ")", ",", "\n", "\"isthing\"", ":", "bool", "(", "isthing", ")", ",", "\n", "}", "\n", ")", "\n", "# Official evaluation script uses 0 for VOID label.", "\n", "", "panoptic_img", "+=", "1", "\n", "\n", "", "file_name", "=", "os", ".", "path", ".", "basename", "(", "input", "[", "\"file_name\"", "]", ")", "\n", "file_name_png", "=", "os", ".", "path", ".", "splitext", "(", "file_name", ")", "[", "0", "]", "+", "\".png\"", "\n", "with", "io", ".", "BytesIO", "(", ")", "as", "out", ":", "\n", "                ", "Image", ".", "fromarray", "(", "id2rgb", "(", "panoptic_img", ")", ")", ".", "save", "(", "out", ",", "format", "=", "\"PNG\"", ")", "\n", "segments_info", "=", "[", "self", ".", "_convert_category_id", "(", "x", ")", "for", "x", "in", "segments_info", "]", "\n", "self", ".", "_predictions", ".", "append", "(", "\n", "{", "\n", "\"image_id\"", ":", "input", "[", "\"image_id\"", "]", ",", "\n", "\"file_name\"", ":", "file_name_png", ",", "\n", "\"png_string\"", ":", "out", ".", "getvalue", "(", ")", ",", "\n", "\"segments_info\"", ":", "segments_info", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation.COCOPanopticEvaluator.evaluate": [[114, 166], ["detectron2.utils.comm.synchronize", "detectron2.utils.comm.gather", "list", "detectron2.utils.file_io.PathManager.get_local_path", "detectron2.utils.file_io.PathManager.get_local_path", "collections.OrderedDict", "panoptic_evaluation._print_panoptic_results", "itertools.chain", "detectron2.utils.comm.is_main_process", "tempfile.TemporaryDirectory", "logger.info", "os.path.join", "open", "json.load", "detectron2.utils.file_io.PathManager.open", "f.write", "contextlib.redirect_stdout", "pq_compute", "open", "f.write", "json.dumps", "io.StringIO", "detectron2.utils.file_io.PathManager.get_local_path", "os.path.join", "p.pop"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation._print_panoptic_results", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.detection_checkpoint.DetectionCheckpointer.load", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["", "", "", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "comm", ".", "synchronize", "(", ")", "\n", "\n", "self", ".", "_predictions", "=", "comm", ".", "gather", "(", "self", ".", "_predictions", ")", "\n", "self", ".", "_predictions", "=", "list", "(", "itertools", ".", "chain", "(", "*", "self", ".", "_predictions", ")", ")", "\n", "if", "not", "comm", ".", "is_main_process", "(", ")", ":", "\n", "            ", "return", "\n", "\n", "# PanopticApi requires local files", "\n", "", "gt_json", "=", "PathManager", ".", "get_local_path", "(", "self", ".", "_metadata", ".", "panoptic_json", ")", "\n", "gt_folder", "=", "PathManager", ".", "get_local_path", "(", "self", ".", "_metadata", ".", "panoptic_root", ")", "\n", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"panoptic_eval\"", ")", "as", "pred_dir", ":", "\n", "            ", "logger", ".", "info", "(", "\"Writing all panoptic predictions to {} ...\"", ".", "format", "(", "pred_dir", ")", ")", "\n", "for", "p", "in", "self", ".", "_predictions", ":", "\n", "                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "pred_dir", ",", "p", "[", "\"file_name\"", "]", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "p", ".", "pop", "(", "\"png_string\"", ")", ")", "\n", "\n", "", "", "with", "open", "(", "gt_json", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "json_data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "json_data", "[", "\"annotations\"", "]", "=", "self", ".", "_predictions", "\n", "\n", "output_dir", "=", "self", ".", "_output_dir", "or", "pred_dir", "\n", "predictions_json", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"predictions.json\"", ")", "\n", "with", "PathManager", ".", "open", "(", "predictions_json", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "json", ".", "dumps", "(", "json_data", ")", ")", "\n", "\n", "", "from", "panopticapi", ".", "evaluation", "import", "pq_compute", "\n", "\n", "with", "contextlib", ".", "redirect_stdout", "(", "io", ".", "StringIO", "(", ")", ")", ":", "\n", "                ", "pq_res", "=", "pq_compute", "(", "\n", "gt_json", ",", "\n", "PathManager", ".", "get_local_path", "(", "predictions_json", ")", ",", "\n", "gt_folder", "=", "gt_folder", ",", "\n", "pred_folder", "=", "pred_dir", ",", "\n", ")", "\n", "\n", "", "", "res", "=", "{", "}", "\n", "res", "[", "\"PQ\"", "]", "=", "100", "*", "pq_res", "[", "\"All\"", "]", "[", "\"pq\"", "]", "\n", "res", "[", "\"SQ\"", "]", "=", "100", "*", "pq_res", "[", "\"All\"", "]", "[", "\"sq\"", "]", "\n", "res", "[", "\"RQ\"", "]", "=", "100", "*", "pq_res", "[", "\"All\"", "]", "[", "\"rq\"", "]", "\n", "res", "[", "\"PQ_th\"", "]", "=", "100", "*", "pq_res", "[", "\"Things\"", "]", "[", "\"pq\"", "]", "\n", "res", "[", "\"SQ_th\"", "]", "=", "100", "*", "pq_res", "[", "\"Things\"", "]", "[", "\"sq\"", "]", "\n", "res", "[", "\"RQ_th\"", "]", "=", "100", "*", "pq_res", "[", "\"Things\"", "]", "[", "\"rq\"", "]", "\n", "res", "[", "\"PQ_st\"", "]", "=", "100", "*", "pq_res", "[", "\"Stuff\"", "]", "[", "\"pq\"", "]", "\n", "res", "[", "\"SQ_st\"", "]", "=", "100", "*", "pq_res", "[", "\"Stuff\"", "]", "[", "\"sq\"", "]", "\n", "res", "[", "\"RQ_st\"", "]", "=", "100", "*", "pq_res", "[", "\"Stuff\"", "]", "[", "\"rq\"", "]", "\n", "\n", "results", "=", "OrderedDict", "(", "{", "\"panoptic_seg\"", ":", "res", "}", ")", "\n", "_print_panoptic_results", "(", "pq_res", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.panoptic_evaluation._print_panoptic_results": [[168, 178], ["tabulate.tabulate", "logger.info", "data.append"], "function", ["None"], ["", "", "def", "_print_panoptic_results", "(", "pq_res", ")", ":", "\n", "    ", "headers", "=", "[", "\"\"", ",", "\"PQ\"", ",", "\"SQ\"", ",", "\"RQ\"", ",", "\"#categories\"", "]", "\n", "data", "=", "[", "]", "\n", "for", "name", "in", "[", "\"All\"", ",", "\"Things\"", ",", "\"Stuff\"", "]", ":", "\n", "        ", "row", "=", "[", "name", "]", "+", "[", "pq_res", "[", "name", "]", "[", "k", "]", "*", "100", "for", "k", "in", "[", "\"pq\"", ",", "\"sq\"", ",", "\"rq\"", "]", "]", "+", "[", "pq_res", "[", "name", "]", "[", "\"n\"", "]", "]", "\n", "data", ".", "append", "(", "row", ")", "\n", "", "table", "=", "tabulate", "(", "\n", "data", ",", "headers", "=", "headers", ",", "tablefmt", "=", "\"pipe\"", ",", "floatfmt", "=", "\".3f\"", ",", "stralign", "=", "\"center\"", ",", "numalign", "=", "\"center\"", "\n", ")", "\n", "logger", ".", "info", "(", "\"Panoptic Evaluation Results:\\n\"", "+", "table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.PascalVOCDetectionEvaluator.__init__": [[31, 50], ["detectron2.data.MetadataCatalog.get", "detectron2.utils.file_io.PathManager.get_local_path", "os.path.join", "os.path.join", "torch.device", "logging.getLogger", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["def", "__init__", "(", "self", ",", "dataset_name", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dataset_name (str): name of the dataset, e.g., \"voc_2007_test\"\n        \"\"\"", "\n", "self", ".", "_dataset_name", "=", "dataset_name", "\n", "meta", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", "\n", "\n", "# Too many tiny files, download all to local for speed.", "\n", "annotation_dir_local", "=", "PathManager", ".", "get_local_path", "(", "\n", "os", ".", "path", ".", "join", "(", "meta", ".", "dirname", ",", "\"Annotations/\"", ")", "\n", ")", "\n", "self", ".", "_anno_file_template", "=", "os", ".", "path", ".", "join", "(", "annotation_dir_local", ",", "\"{}.xml\"", ")", "\n", "self", ".", "_image_set_path", "=", "os", ".", "path", ".", "join", "(", "meta", ".", "dirname", ",", "\"ImageSets\"", ",", "\"Main\"", ",", "meta", ".", "split", "+", "\".txt\"", ")", "\n", "self", ".", "_class_names", "=", "meta", ".", "thing_classes", "\n", "assert", "meta", ".", "year", "in", "[", "2007", ",", "2012", "]", ",", "meta", ".", "year", "\n", "self", ".", "_is_2007", "=", "meta", ".", "year", "==", "2007", "\n", "self", ".", "_cpu_device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "self", ".", "_logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.PascalVOCDetectionEvaluator.reset": [[51, 53], ["collections.defaultdict"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_predictions", "=", "defaultdict", "(", "list", ")", "# class name -> list of prediction strings", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.PascalVOCDetectionEvaluator.process": [[54, 68], ["zip", "output[].to", "output[].to.pred_boxes.tensor.numpy", "output[].to.scores.tolist", "output[].to.pred_classes.tolist", "zip", "pascal_voc_evaluation.PascalVOCDetectionEvaluator._predictions[].append"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "for", "input", ",", "output", "in", "zip", "(", "inputs", ",", "outputs", ")", ":", "\n", "            ", "image_id", "=", "input", "[", "\"image_id\"", "]", "\n", "instances", "=", "output", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "_cpu_device", ")", "\n", "boxes", "=", "instances", ".", "pred_boxes", ".", "tensor", ".", "numpy", "(", ")", "\n", "scores", "=", "instances", ".", "scores", ".", "tolist", "(", ")", "\n", "classes", "=", "instances", ".", "pred_classes", ".", "tolist", "(", ")", "\n", "for", "box", ",", "score", ",", "cls", "in", "zip", "(", "boxes", ",", "scores", ",", "classes", ")", ":", "\n", "                ", "xmin", ",", "ymin", ",", "xmax", ",", "ymax", "=", "box", "\n", "# The inverse of data loading logic in `datasets/pascal_voc.py`", "\n", "xmin", "+=", "1", "\n", "ymin", "+=", "1", "\n", "self", ".", "_predictions", "[", "cls", "]", ".", "append", "(", "\n", "f\"{image_id} {score:.3f} {xmin:.1f} {ymin:.1f} {xmax:.1f} {ymax:.1f}\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.PascalVOCDetectionEvaluator.evaluate": [[70, 116], ["detectron2.utils.comm.gather", "collections.defaultdict", "pascal_voc_evaluation.PascalVOCDetectionEvaluator._logger.info", "collections.OrderedDict", "detectron2.utils.comm.is_main_process", "predictions_per_rank.items", "tempfile.TemporaryDirectory", "os.path.join", "collections.defaultdict", "enumerate", "numpy.mean", "numpy.mean", "predictions[].extend", "collections.defaultdict.get", "range", "collections.defaultdict.items", "list", "open", "f.write", "pascal_voc_evaluation.voc_eval", "aps[].append", "mAP.values", "os.path.join.format"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.gather", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.voc_eval"], ["", "", "", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            dict: has a key \"segm\", whose value is a dict of \"AP\", \"AP50\", and \"AP75\".\n        \"\"\"", "\n", "all_predictions", "=", "comm", ".", "gather", "(", "self", ".", "_predictions", ",", "dst", "=", "0", ")", "\n", "if", "not", "comm", ".", "is_main_process", "(", ")", ":", "\n", "            ", "return", "\n", "", "predictions", "=", "defaultdict", "(", "list", ")", "\n", "for", "predictions_per_rank", "in", "all_predictions", ":", "\n", "            ", "for", "clsid", ",", "lines", "in", "predictions_per_rank", ".", "items", "(", ")", ":", "\n", "                ", "predictions", "[", "clsid", "]", ".", "extend", "(", "lines", ")", "\n", "", "", "del", "all_predictions", "\n", "\n", "self", ".", "_logger", ".", "info", "(", "\n", "\"Evaluating {} using {} metric. \"", "\n", "\"Note that results do not use the official Matlab API.\"", ".", "format", "(", "\n", "self", ".", "_dataset_name", ",", "2007", "if", "self", ".", "_is_2007", "else", "2012", "\n", ")", "\n", ")", "\n", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"pascal_voc_eval_\"", ")", "as", "dirname", ":", "\n", "            ", "res_file_template", "=", "os", ".", "path", ".", "join", "(", "dirname", ",", "\"{}.txt\"", ")", "\n", "\n", "aps", "=", "defaultdict", "(", "list", ")", "# iou -> ap per class", "\n", "for", "cls_id", ",", "cls_name", "in", "enumerate", "(", "self", ".", "_class_names", ")", ":", "\n", "                ", "lines", "=", "predictions", ".", "get", "(", "cls_id", ",", "[", "\"\"", "]", ")", "\n", "\n", "with", "open", "(", "res_file_template", ".", "format", "(", "cls_name", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "\"\\n\"", ".", "join", "(", "lines", ")", ")", "\n", "\n", "", "for", "thresh", "in", "range", "(", "50", ",", "100", ",", "5", ")", ":", "\n", "                    ", "rec", ",", "prec", ",", "ap", "=", "voc_eval", "(", "\n", "res_file_template", ",", "\n", "self", ".", "_anno_file_template", ",", "\n", "self", ".", "_image_set_path", ",", "\n", "cls_name", ",", "\n", "ovthresh", "=", "thresh", "/", "100.0", ",", "\n", "use_07_metric", "=", "self", ".", "_is_2007", ",", "\n", ")", "\n", "aps", "[", "thresh", "]", ".", "append", "(", "ap", "*", "100", ")", "\n", "\n", "", "", "", "ret", "=", "OrderedDict", "(", ")", "\n", "mAP", "=", "{", "iou", ":", "np", ".", "mean", "(", "x", ")", "for", "iou", ",", "x", "in", "aps", ".", "items", "(", ")", "}", "\n", "ret", "[", "\"bbox\"", "]", "=", "{", "\"AP\"", ":", "np", ".", "mean", "(", "list", "(", "mAP", ".", "values", "(", ")", ")", ")", ",", "\"AP50\"", ":", "mAP", "[", "50", "]", ",", "\"AP75\"", ":", "mAP", "[", "75", "]", "}", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.parse_rec": [[131, 153], ["functools.lru_cache", "ET.parse.findall", "detectron2.utils.file_io.PathManager.open", "xml.parse", "int", "int", "obj.find", "objects.append", "obj.find", "obj.find", "int", "int", "int", "int", "obj.find", "obj.find", "obj.find.find", "obj.find.find", "obj.find.find", "obj.find.find"], "function", ["None"], ["@", "lru_cache", "(", "maxsize", "=", "None", ")", "\n", "def", "parse_rec", "(", "filename", ")", ":", "\n", "    ", "\"\"\"Parse a PASCAL VOC xml file.\"\"\"", "\n", "with", "PathManager", ".", "open", "(", "filename", ")", "as", "f", ":", "\n", "        ", "tree", "=", "ET", ".", "parse", "(", "f", ")", "\n", "", "objects", "=", "[", "]", "\n", "for", "obj", "in", "tree", ".", "findall", "(", "\"object\"", ")", ":", "\n", "        ", "obj_struct", "=", "{", "}", "\n", "obj_struct", "[", "\"name\"", "]", "=", "obj", ".", "find", "(", "\"name\"", ")", ".", "text", "\n", "obj_struct", "[", "\"pose\"", "]", "=", "obj", ".", "find", "(", "\"pose\"", ")", ".", "text", "\n", "obj_struct", "[", "\"truncated\"", "]", "=", "int", "(", "obj", ".", "find", "(", "\"truncated\"", ")", ".", "text", ")", "\n", "obj_struct", "[", "\"difficult\"", "]", "=", "int", "(", "obj", ".", "find", "(", "\"difficult\"", ")", ".", "text", ")", "\n", "bbox", "=", "obj", ".", "find", "(", "\"bndbox\"", ")", "\n", "obj_struct", "[", "\"bbox\"", "]", "=", "[", "\n", "int", "(", "bbox", ".", "find", "(", "\"xmin\"", ")", ".", "text", ")", ",", "\n", "int", "(", "bbox", ".", "find", "(", "\"ymin\"", ")", ".", "text", ")", ",", "\n", "int", "(", "bbox", ".", "find", "(", "\"xmax\"", ")", ".", "text", ")", ",", "\n", "int", "(", "bbox", ".", "find", "(", "\"ymax\"", ")", ".", "text", ")", ",", "\n", "]", "\n", "objects", ".", "append", "(", "obj_struct", ")", "\n", "\n", "", "return", "objects", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.voc_ap": [[155, 185], ["numpy.arange", "numpy.concatenate", "numpy.concatenate", "range", "numpy.sum", "numpy.maximum", "numpy.where", "numpy.sum", "numpy.max"], "function", ["None"], ["", "def", "voc_ap", "(", "rec", ",", "prec", ",", "use_07_metric", "=", "False", ")", ":", "\n", "    ", "\"\"\"Compute VOC AP given precision and recall. If use_07_metric is true, uses\n    the VOC 07 11-point method (default:False).\n    \"\"\"", "\n", "if", "use_07_metric", ":", "\n", "# 11 point metric", "\n", "        ", "ap", "=", "0.0", "\n", "for", "t", "in", "np", ".", "arange", "(", "0.0", ",", "1.1", ",", "0.1", ")", ":", "\n", "            ", "if", "np", ".", "sum", "(", "rec", ">=", "t", ")", "==", "0", ":", "\n", "                ", "p", "=", "0", "\n", "", "else", ":", "\n", "                ", "p", "=", "np", ".", "max", "(", "prec", "[", "rec", ">=", "t", "]", ")", "\n", "", "ap", "=", "ap", "+", "p", "/", "11.0", "\n", "", "", "else", ":", "\n", "# correct AP calculation", "\n", "# first append sentinel values at the end", "\n", "        ", "mrec", "=", "np", ".", "concatenate", "(", "(", "[", "0.0", "]", ",", "rec", ",", "[", "1.0", "]", ")", ")", "\n", "mpre", "=", "np", ".", "concatenate", "(", "(", "[", "0.0", "]", ",", "prec", ",", "[", "0.0", "]", ")", ")", "\n", "\n", "# compute the precision envelope", "\n", "for", "i", "in", "range", "(", "mpre", ".", "size", "-", "1", ",", "0", ",", "-", "1", ")", ":", "\n", "            ", "mpre", "[", "i", "-", "1", "]", "=", "np", ".", "maximum", "(", "mpre", "[", "i", "-", "1", "]", ",", "mpre", "[", "i", "]", ")", "\n", "\n", "# to calculate area under PR curve, look for points", "\n", "# where X axis (recall) changes value", "\n", "", "i", "=", "np", ".", "where", "(", "mrec", "[", "1", ":", "]", "!=", "mrec", "[", ":", "-", "1", "]", ")", "[", "0", "]", "\n", "\n", "# and sum (\\Delta recall) * prec", "\n", "ap", "=", "np", ".", "sum", "(", "(", "mrec", "[", "i", "+", "1", "]", "-", "mrec", "[", "i", "]", ")", "*", "mpre", "[", "i", "+", "1", "]", ")", "\n", "", "return", "ap", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.voc_eval": [[187, 301], ["detpath.format", "numpy.array", "numpy.array().reshape", "numpy.argsort", "len", "numpy.zeros", "numpy.zeros", "range", "numpy.cumsum", "numpy.cumsum", "pascal_voc_evaluation.voc_ap", "detectron2.utils.file_io.PathManager.open", "f.readlines", "x.strip", "pascal_voc_evaluation.parse_rec", "numpy.array", "numpy.array().astype", "open", "f.readlines", "x.strip().split", "BB[].astype", "R[].astype", "float", "numpy.maximum", "annopath.format", "len", "sum", "float", "numpy.array", "numpy.maximum", "numpy.maximum", "numpy.minimum", "numpy.minimum", "numpy.maximum", "numpy.maximum", "numpy.max", "numpy.argmax", "numpy.array", "x.strip", "numpy.finfo", "float"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.voc_ap", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.pascal_voc_evaluation.parse_rec"], ["", "def", "voc_eval", "(", "detpath", ",", "annopath", ",", "imagesetfile", ",", "classname", ",", "ovthresh", "=", "0.5", ",", "use_07_metric", "=", "False", ")", ":", "\n", "    ", "\"\"\"rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07's 11 point AP computation\n        (default False)\n    \"\"\"", "\n", "# assumes detections are in detpath.format(classname)", "\n", "# assumes annotations are in annopath.format(imagename)", "\n", "# assumes imagesetfile is a text file with each line an image name", "\n", "\n", "# first load gt", "\n", "# read list of images", "\n", "with", "PathManager", ".", "open", "(", "imagesetfile", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "imagenames", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "lines", "]", "\n", "\n", "# load annots", "\n", "recs", "=", "{", "}", "\n", "for", "imagename", "in", "imagenames", ":", "\n", "        ", "recs", "[", "imagename", "]", "=", "parse_rec", "(", "annopath", ".", "format", "(", "imagename", ")", ")", "\n", "\n", "# extract gt objects for this class", "\n", "", "class_recs", "=", "{", "}", "\n", "npos", "=", "0", "\n", "for", "imagename", "in", "imagenames", ":", "\n", "        ", "R", "=", "[", "obj", "for", "obj", "in", "recs", "[", "imagename", "]", "if", "obj", "[", "\"name\"", "]", "==", "classname", "]", "\n", "bbox", "=", "np", ".", "array", "(", "[", "x", "[", "\"bbox\"", "]", "for", "x", "in", "R", "]", ")", "\n", "difficult", "=", "np", ".", "array", "(", "[", "x", "[", "\"difficult\"", "]", "for", "x", "in", "R", "]", ")", ".", "astype", "(", "np", ".", "bool", ")", "\n", "# difficult = np.array([False for x in R]).astype(np.bool)  # treat all \"difficult\" as GT", "\n", "det", "=", "[", "False", "]", "*", "len", "(", "R", ")", "\n", "npos", "=", "npos", "+", "sum", "(", "~", "difficult", ")", "\n", "class_recs", "[", "imagename", "]", "=", "{", "\"bbox\"", ":", "bbox", ",", "\"difficult\"", ":", "difficult", ",", "\"det\"", ":", "det", "}", "\n", "\n", "# read dets", "\n", "", "detfile", "=", "detpath", ".", "format", "(", "classname", ")", "\n", "with", "open", "(", "detfile", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "splitlines", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "for", "x", "in", "lines", "]", "\n", "image_ids", "=", "[", "x", "[", "0", "]", "for", "x", "in", "splitlines", "]", "\n", "confidence", "=", "np", ".", "array", "(", "[", "float", "(", "x", "[", "1", "]", ")", "for", "x", "in", "splitlines", "]", ")", "\n", "BB", "=", "np", ".", "array", "(", "[", "[", "float", "(", "z", ")", "for", "z", "in", "x", "[", "2", ":", "]", "]", "for", "x", "in", "splitlines", "]", ")", ".", "reshape", "(", "-", "1", ",", "4", ")", "\n", "\n", "# sort by confidence", "\n", "sorted_ind", "=", "np", ".", "argsort", "(", "-", "confidence", ")", "\n", "BB", "=", "BB", "[", "sorted_ind", ",", ":", "]", "\n", "image_ids", "=", "[", "image_ids", "[", "x", "]", "for", "x", "in", "sorted_ind", "]", "\n", "\n", "# go down dets and mark TPs and FPs", "\n", "nd", "=", "len", "(", "image_ids", ")", "\n", "tp", "=", "np", ".", "zeros", "(", "nd", ")", "\n", "fp", "=", "np", ".", "zeros", "(", "nd", ")", "\n", "for", "d", "in", "range", "(", "nd", ")", ":", "\n", "        ", "R", "=", "class_recs", "[", "image_ids", "[", "d", "]", "]", "\n", "bb", "=", "BB", "[", "d", ",", ":", "]", ".", "astype", "(", "float", ")", "\n", "ovmax", "=", "-", "np", ".", "inf", "\n", "BBGT", "=", "R", "[", "\"bbox\"", "]", ".", "astype", "(", "float", ")", "\n", "\n", "if", "BBGT", ".", "size", ">", "0", ":", "\n", "# compute overlaps", "\n", "# intersection", "\n", "            ", "ixmin", "=", "np", ".", "maximum", "(", "BBGT", "[", ":", ",", "0", "]", ",", "bb", "[", "0", "]", ")", "\n", "iymin", "=", "np", ".", "maximum", "(", "BBGT", "[", ":", ",", "1", "]", ",", "bb", "[", "1", "]", ")", "\n", "ixmax", "=", "np", ".", "minimum", "(", "BBGT", "[", ":", ",", "2", "]", ",", "bb", "[", "2", "]", ")", "\n", "iymax", "=", "np", ".", "minimum", "(", "BBGT", "[", ":", ",", "3", "]", ",", "bb", "[", "3", "]", ")", "\n", "iw", "=", "np", ".", "maximum", "(", "ixmax", "-", "ixmin", "+", "1.0", ",", "0.0", ")", "\n", "ih", "=", "np", ".", "maximum", "(", "iymax", "-", "iymin", "+", "1.0", ",", "0.0", ")", "\n", "inters", "=", "iw", "*", "ih", "\n", "\n", "# union", "\n", "uni", "=", "(", "\n", "(", "bb", "[", "2", "]", "-", "bb", "[", "0", "]", "+", "1.0", ")", "*", "(", "bb", "[", "3", "]", "-", "bb", "[", "1", "]", "+", "1.0", ")", "\n", "+", "(", "BBGT", "[", ":", ",", "2", "]", "-", "BBGT", "[", ":", ",", "0", "]", "+", "1.0", ")", "*", "(", "BBGT", "[", ":", ",", "3", "]", "-", "BBGT", "[", ":", ",", "1", "]", "+", "1.0", ")", "\n", "-", "inters", "\n", ")", "\n", "\n", "overlaps", "=", "inters", "/", "uni", "\n", "ovmax", "=", "np", ".", "max", "(", "overlaps", ")", "\n", "jmax", "=", "np", ".", "argmax", "(", "overlaps", ")", "\n", "\n", "", "if", "ovmax", ">", "ovthresh", ":", "\n", "            ", "if", "not", "R", "[", "\"difficult\"", "]", "[", "jmax", "]", ":", "\n", "                ", "if", "not", "R", "[", "\"det\"", "]", "[", "jmax", "]", ":", "\n", "                    ", "tp", "[", "d", "]", "=", "1.0", "\n", "R", "[", "\"det\"", "]", "[", "jmax", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "fp", "[", "d", "]", "=", "1.0", "\n", "", "", "", "else", ":", "\n", "            ", "fp", "[", "d", "]", "=", "1.0", "\n", "\n", "# compute precision recall", "\n", "", "", "fp", "=", "np", ".", "cumsum", "(", "fp", ")", "\n", "tp", "=", "np", ".", "cumsum", "(", "tp", ")", "\n", "rec", "=", "tp", "/", "float", "(", "npos", ")", "\n", "# avoid divide by zero in case the first detection matches a difficult", "\n", "# ground truth", "\n", "prec", "=", "tp", "/", "np", ".", "maximum", "(", "tp", "+", "fp", ",", "np", ".", "finfo", "(", "np", ".", "float64", ")", ".", "eps", ")", "\n", "ap", "=", "voc_ap", "(", "rec", ",", "prec", ",", "use_07_metric", ")", "\n", "\n", "return", "rec", ",", "prec", ",", "ap", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.print_csv_format": [[9, 29], ["logging.getLogger", "results.items", "isinstance", "isinstance", "len", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "res.items"], "function", ["None"], ["from", "detectron2", ".", "modeling", "import", "build_model", "\n", "from", "detectron2", ".", "structures", "import", "Boxes", ",", "Instances", ",", "ROIMasks", "\n", "from", "detectron2", ".", "utils", ".", "file_io", "import", "PathManager", "\n", "\n", "\n", "\"\"\"\nInternal utilities for tests. Don't use except for writing tests.\n\"\"\"", "\n", "\n", "\n", "def", "get_model_no_weights", "(", "config_path", ")", ":", "\n", "    ", "\"\"\"\n    Like model_zoo.get, but do not load any weights (even pretrained)\n    \"\"\"", "\n", "cfg", "=", "model_zoo", ".", "get_config", "(", "config_path", ")", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "cfg", ".", "MODEL", ".", "DEVICE", "=", "\"cpu\"", "\n", "", "return", "build_model", "(", "cfg", ")", "\n", "\n", "\n", "", "def", "random_boxes", "(", "num_boxes", ",", "max_coord", "=", "100", ",", "device", "=", "\"cpu\"", ")", ":", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.verify_results": [[31, 66], ["logging.getLogger", "len", "results[].get", "abs", "logging.getLogger.error", "logging.getLogger.error", "logging.getLogger.error", "sys.exit", "logging.getLogger.info", "numpy.isfinite", "str", "pprint.pformat"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["\n", "boxes", "=", "torch", ".", "rand", "(", "num_boxes", ",", "4", ",", "device", "=", "device", ")", "*", "(", "max_coord", "*", "0.5", ")", "\n", "boxes", ".", "clamp_", "(", "min", "=", "1.0", ")", "# tiny boxes cause numerical instability in box regression", "\n", "# Note: the implementation of this function in torchvision is:", "\n", "# boxes[:, 2:] += torch.rand(N, 2) * 100", "\n", "# but it does not guarantee non-negative widths/heights constraints:", "\n", "# boxes[:, 2] >= boxes[:, 0] and boxes[:, 3] >= boxes[:, 1]:", "\n", "boxes", "[", ":", ",", "2", ":", "]", "+=", "boxes", "[", ":", ",", ":", "2", "]", "\n", "return", "boxes", "\n", "\n", "\n", "", "def", "get_sample_coco_image", "(", "tensor", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        tensor (bool): if True, returns 3xHxW tensor.\n            else, returns a HxWx3 numpy array.\n\n    Returns:\n        an image, in BGR color.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "file_name", "=", "DatasetCatalog", ".", "get", "(", "\"coco_2017_val_100\"", ")", "[", "0", "]", "[", "\"file_name\"", "]", "\n", "if", "not", "PathManager", ".", "exists", "(", "file_name", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", ")", "\n", "", "", "except", "IOError", ":", "\n", "# for public CI to run", "\n", "        ", "file_name", "=", "\"http://images.cocodataset.org/train2017/000000000009.jpg\"", "\n", "", "ret", "=", "read_image", "(", "file_name", ",", "format", "=", "\"BGR\"", ")", "\n", "if", "tensor", ":", "\n", "        ", "ret", "=", "torch", ".", "from_numpy", "(", "np", ".", "ascontiguousarray", "(", "ret", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ")", ")", "\n", "", "return", "ret", "\n", "\n", "\n", "", "def", "convert_scripted_instances", "(", "instances", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.flatten_results_dict": [[68, 86], ["results.items", "isinstance", "testing.flatten_results_dict", "flatten_results_dict.items"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.testing.flatten_results_dict"], ["\n", "ret", "=", "Instances", "(", "instances", ".", "image_size", ")", "\n", "for", "name", "in", "instances", ".", "_field_names", ":", "\n", "        ", "val", "=", "getattr", "(", "instances", ",", "\"_\"", "+", "name", ",", "None", ")", "\n", "if", "val", "is", "not", "None", ":", "\n", "            ", "ret", ".", "set", "(", "name", ",", "val", ")", "\n", "", "", "return", "ret", "\n", "\n", "\n", "", "def", "assert_instances_allclose", "(", "input", ",", "other", ",", "*", ",", "rtol", "=", "1e-5", ",", "msg", "=", "\"\"", ",", "size_as_tensor", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        input, other (Instances):\n        size_as_tensor: compare image_size of the Instances as tensors (instead of tuples).\n             Useful for comparing outputs of tracing.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "input", ",", "Instances", ")", ":", "\n", "        ", "input", "=", "convert_scripted_instances", "(", "input", ")", "\n", "", "if", "not", "isinstance", "(", "other", ",", "Instances", ")", ":", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.DatasetEvaluator.reset": [[26, 32], ["None"], "methods", ["None"], ["def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Preparation for a new round of evaluation.\n        Should be called before starting a round of evaluation.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.DatasetEvaluator.process": [[33, 49], ["None"], "methods", ["None"], ["", "def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"\n        Process the pair of inputs and outputs.\n        If they contain batches, the pairs can be consumed one-by-one using `zip`:\n\n        .. code-block:: python\n\n            for input_, output in zip(inputs, outputs):\n                # do evaluation on single input/output pair\n                ...\n\n        Args:\n            inputs (list): the inputs that's used to call the model.\n            outputs (list): the return value of `model(inputs)`\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.DatasetEvaluator.evaluate": [[50, 64], ["None"], "methods", ["None"], ["", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate/summarize the performance, after processing all input/output pairs.\n\n        Returns:\n            dict:\n                A new evaluator class can return a dict of arbitrary format\n                as long as the user can process the results.\n                In our train_net.py, we expect the following format:\n\n                * key: the name of the task (e.g., bbox)\n                * value: a dict of {metric name: score}, e.g.: {\"AP50\": 80}\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.DatasetEvaluators.__init__": [[74, 81], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "self", ",", "evaluators", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            evaluators (list): the evaluators to combine.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_evaluators", "=", "evaluators", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.DatasetEvaluators.reset": [[82, 85], ["evaluator.reset"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "for", "evaluator", "in", "self", ".", "_evaluators", ":", "\n", "            ", "evaluator", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.DatasetEvaluators.process": [[86, 89], ["evaluator.process"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.process"], ["", "", "def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "for", "evaluator", "in", "self", ".", "_evaluators", ":", "\n", "            ", "evaluator", ".", "process", "(", "inputs", ",", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.DatasetEvaluators.evaluate": [[90, 101], ["collections.OrderedDict", "evaluator.evaluate", "detectron2.utils.comm.is_main_process", "evaluator.evaluate.items"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.evaluate", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.is_main_process"], ["", "", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "results", "=", "OrderedDict", "(", ")", "\n", "for", "evaluator", "in", "self", ".", "_evaluators", ":", "\n", "            ", "result", "=", "evaluator", ".", "evaluate", "(", ")", "\n", "if", "is_main_process", "(", ")", "and", "result", "is", "not", "None", ":", "\n", "                ", "for", "k", ",", "v", "in", "result", ".", "items", "(", ")", ":", "\n", "                    ", "assert", "(", "\n", "k", "not", "in", "results", "\n", ")", ",", "\"Different evaluators produce results with the same key {}\"", ".", "format", "(", "k", ")", "\n", "results", "[", "k", "]", "=", "v", "\n", "", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.inference_on_dataset": [[103, 210], ["detectron2.utils.comm.get_world_size", "logging.getLogger", "logging.getLogger.info", "len", "isinstance", "evaluator.DatasetEvaluators.reset", "min", "time.perf_counter", "str", "logging.getLogger.info", "str", "logging.getLogger.info", "evaluator.DatasetEvaluators.evaluate", "evaluator.DatasetEvaluators", "evaluator.DatasetEvaluators", "contextlib.ExitStack", "isinstance", "stack.enter_context", "time.perf_counter", "enumerate", "time.perf_counter", "datetime.timedelta", "datetime.timedelta", "len", "stack.enter_context", "torch.no_grad", "time.perf_counter", "model", "torch.cuda.is_available", "time.perf_counter", "evaluator.DatasetEvaluators.process", "time.perf_counter", "evaluator.inference_context", "time.perf_counter", "time.perf_counter", "torch.cuda.synchronize", "time.perf_counter", "time.perf_counter", "datetime.timedelta", "detectron2.utils.logger.log_every_n_seconds", "int", "int", "time.perf_counter", "int"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_world_size", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.reset", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.evaluate", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.process", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.inference_context", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.log_every_n_seconds"], ["", "", "def", "inference_on_dataset", "(", "\n", "model", ",", "data_loader", ",", "evaluator", ":", "Union", "[", "DatasetEvaluator", ",", "List", "[", "DatasetEvaluator", "]", ",", "None", "]", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Run model on the data_loader and evaluate the metrics with evaluator.\n    Also benchmark the inference speed of `model.__call__` accurately.\n    The model will be used in eval mode.\n\n    Args:\n        model (callable): a callable which takes an object from\n            `data_loader` and returns some outputs.\n\n            If it's an nn.Module, it will be temporarily set to `eval` mode.\n            If you wish to evaluate a model in `training` mode instead, you can\n            wrap the given model and override its behavior of `.eval()` and `.train()`.\n        data_loader: an iterable object with a length.\n            The elements it generates will be the inputs to the model.\n        evaluator: the evaluator(s) to run. Use `None` if you only want to benchmark,\n            but don't want to do any evaluation.\n\n    Returns:\n        The return value of `evaluator.evaluate()`\n    \"\"\"", "\n", "num_devices", "=", "get_world_size", "(", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "\"Start inference on {} batches\"", ".", "format", "(", "len", "(", "data_loader", ")", ")", ")", "\n", "\n", "total", "=", "len", "(", "data_loader", ")", "# inference data loader must have a fixed length", "\n", "if", "evaluator", "is", "None", ":", "\n", "# create a no-op evaluator", "\n", "        ", "evaluator", "=", "DatasetEvaluators", "(", "[", "]", ")", "\n", "", "if", "isinstance", "(", "evaluator", ",", "abc", ".", "MutableSequence", ")", ":", "\n", "        ", "evaluator", "=", "DatasetEvaluators", "(", "evaluator", ")", "\n", "", "evaluator", ".", "reset", "(", ")", "\n", "\n", "num_warmup", "=", "min", "(", "5", ",", "total", "-", "1", ")", "\n", "start_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "total_data_time", "=", "0", "\n", "total_compute_time", "=", "0", "\n", "total_eval_time", "=", "0", "\n", "with", "ExitStack", "(", ")", "as", "stack", ":", "\n", "        ", "if", "isinstance", "(", "model", ",", "nn", ".", "Module", ")", ":", "\n", "            ", "stack", ".", "enter_context", "(", "inference_context", "(", "model", ")", ")", "\n", "", "stack", ".", "enter_context", "(", "torch", ".", "no_grad", "(", ")", ")", "\n", "\n", "start_data_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "for", "idx", ",", "inputs", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "total_data_time", "+=", "time", ".", "perf_counter", "(", ")", "-", "start_data_time", "\n", "if", "idx", "==", "num_warmup", ":", "\n", "                ", "start_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "total_data_time", "=", "0", "\n", "total_compute_time", "=", "0", "\n", "total_eval_time", "=", "0", "\n", "\n", "", "start_compute_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "outputs", "=", "model", "(", "inputs", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "", "total_compute_time", "+=", "time", ".", "perf_counter", "(", ")", "-", "start_compute_time", "\n", "\n", "start_eval_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "evaluator", ".", "process", "(", "inputs", ",", "outputs", ")", "\n", "total_eval_time", "+=", "time", ".", "perf_counter", "(", ")", "-", "start_eval_time", "\n", "\n", "iters_after_start", "=", "idx", "+", "1", "-", "num_warmup", "*", "int", "(", "idx", ">=", "num_warmup", ")", "\n", "data_seconds_per_iter", "=", "total_data_time", "/", "iters_after_start", "\n", "compute_seconds_per_iter", "=", "total_compute_time", "/", "iters_after_start", "\n", "eval_seconds_per_iter", "=", "total_eval_time", "/", "iters_after_start", "\n", "total_seconds_per_iter", "=", "(", "time", ".", "perf_counter", "(", ")", "-", "start_time", ")", "/", "iters_after_start", "\n", "if", "idx", ">=", "num_warmup", "*", "2", "or", "compute_seconds_per_iter", ">", "5", ":", "\n", "                ", "eta", "=", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "total_seconds_per_iter", "*", "(", "total", "-", "idx", "-", "1", ")", ")", ")", "\n", "log_every_n_seconds", "(", "\n", "logging", ".", "INFO", ",", "\n", "(", "\n", "f\"Inference done {idx + 1}/{total}. \"", "\n", "f\"Dataloading: {data_seconds_per_iter:.4f} s / iter. \"", "\n", "f\"Inference: {compute_seconds_per_iter:.4f} s / iter. \"", "\n", "f\"Eval: {eval_seconds_per_iter:.4f} s / iter. \"", "\n", "f\"Total: {total_seconds_per_iter:.4f} s / iter. \"", "\n", "f\"ETA={eta}\"", "\n", ")", ",", "\n", "n", "=", "5", ",", "\n", ")", "\n", "", "start_data_time", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "# Measure the time only for this worker (before the synchronization barrier)", "\n", "", "", "total_time", "=", "time", ".", "perf_counter", "(", ")", "-", "start_time", "\n", "total_time_str", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "total_time", ")", ")", "\n", "# NOTE this format is parsed by grep", "\n", "logger", ".", "info", "(", "\n", "\"Total inference time: {} ({:.6f} s / iter per device, on {} devices)\"", ".", "format", "(", "\n", "total_time_str", ",", "total_time", "/", "(", "total", "-", "num_warmup", ")", ",", "num_devices", "\n", ")", "\n", ")", "\n", "total_compute_time_str", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "total_compute_time", ")", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"Total inference pure compute time: {} ({:.6f} s / iter per device, on {} devices)\"", ".", "format", "(", "\n", "total_compute_time_str", ",", "total_compute_time", "/", "(", "total", "-", "num_warmup", ")", ",", "num_devices", "\n", ")", "\n", ")", "\n", "\n", "results", "=", "evaluator", ".", "evaluate", "(", ")", "\n", "# An evaluator may return None when not in main process.", "\n", "# Replace it by an empty dict instead to make it easier for downstream code to handle", "\n", "if", "results", "is", "None", ":", "\n", "        ", "results", "=", "{", "}", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.evaluator.inference_context": [[212, 225], ["model.eval", "model.train"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train"], ["", "@", "contextmanager", "\n", "def", "inference_context", "(", "model", ")", ":", "\n", "    ", "\"\"\"\n    A context where the model is temporarily changed to eval mode,\n    and restored to previous mode afterwards.\n\n    Args:\n        model: a torch Module\n    \"\"\"", "\n", "training_mode", "=", "model", ".", "training", "\n", "model", ".", "eval", "(", ")", "\n", "yield", "\n", "model", ".", "train", "(", "training_mode", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.is_rotated": [[16, 32], ["type", "type", "numpy.all", "numpy.array", "len", "type", "type"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "is_rotated", "(", "box_list", ")", ":", "\n", "        ", "if", "type", "(", "box_list", ")", "==", "np", ".", "ndarray", ":", "\n", "            ", "return", "box_list", ".", "shape", "[", "1", "]", "==", "5", "\n", "", "elif", "type", "(", "box_list", ")", "==", "list", ":", "\n", "            ", "if", "box_list", "==", "[", "]", ":", "# cannot decide the box_dim", "\n", "                ", "return", "False", "\n", "", "return", "np", ".", "all", "(", "\n", "np", ".", "array", "(", "\n", "[", "\n", "(", "len", "(", "obj", ")", "==", "5", ")", "and", "(", "(", "type", "(", "obj", ")", "==", "list", ")", "or", "(", "type", "(", "obj", ")", "==", "np", ".", "ndarray", ")", ")", "\n", "for", "obj", "in", "box_list", "\n", "]", "\n", ")", "\n", ")", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.boxlist_to_tensor": [[33, 56], ["type", "torch.from_numpy", "type", "Exception", "detectron2.structures.BoxMode.convert", "Exception", "torch.zeros", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "@", "staticmethod", "\n", "def", "boxlist_to_tensor", "(", "boxlist", ",", "output_box_dim", ")", ":", "\n", "        ", "if", "type", "(", "boxlist", ")", "==", "np", ".", "ndarray", ":", "\n", "            ", "box_tensor", "=", "torch", ".", "from_numpy", "(", "boxlist", ")", "\n", "", "elif", "type", "(", "boxlist", ")", "==", "list", ":", "\n", "            ", "if", "boxlist", "==", "[", "]", ":", "\n", "                ", "return", "torch", ".", "zeros", "(", "(", "0", ",", "output_box_dim", ")", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "", "else", ":", "\n", "                ", "box_tensor", "=", "torch", ".", "FloatTensor", "(", "boxlist", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Unrecognized boxlist type\"", ")", "\n", "\n", "", "input_box_dim", "=", "box_tensor", ".", "shape", "[", "1", "]", "\n", "if", "input_box_dim", "!=", "output_box_dim", ":", "\n", "            ", "if", "input_box_dim", "==", "4", "and", "output_box_dim", "==", "5", ":", "\n", "                ", "box_tensor", "=", "BoxMode", ".", "convert", "(", "box_tensor", ",", "BoxMode", ".", "XYWH_ABS", ",", "BoxMode", ".", "XYWHA_ABS", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "\n", "\"Unable to convert from {}-dim box to {}-dim box\"", ".", "format", "(", "\n", "input_box_dim", ",", "output_box_dim", "\n", ")", "\n", ")", "\n", "", "", "return", "box_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.compute_iou_dt_gt": [[57, 67], ["rotated_coco_evaluation.RotatedCOCOeval.is_rotated", "rotated_coco_evaluation.RotatedCOCOeval.is_rotated", "all", "detectron2.structures.RotatedBoxes", "detectron2.structures.RotatedBoxes", "detectron2.structures.pairwise_iou_rotated", "pycocotools.cocoeval.maskUtils.iou", "rotated_coco_evaluation.RotatedCOCOeval.boxlist_to_tensor", "rotated_coco_evaluation.RotatedCOCOeval.boxlist_to_tensor"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.is_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.is_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.rotated_boxes.pairwise_iou_rotated", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.boxlist_to_tensor", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.boxlist_to_tensor"], ["", "def", "compute_iou_dt_gt", "(", "self", ",", "dt", ",", "gt", ",", "is_crowd", ")", ":", "\n", "        ", "if", "self", ".", "is_rotated", "(", "dt", ")", "or", "self", ".", "is_rotated", "(", "gt", ")", ":", "\n", "# TODO: take is_crowd into consideration", "\n", "            ", "assert", "all", "(", "c", "==", "0", "for", "c", "in", "is_crowd", ")", "\n", "dt", "=", "RotatedBoxes", "(", "self", ".", "boxlist_to_tensor", "(", "dt", ",", "output_box_dim", "=", "5", ")", ")", "\n", "gt", "=", "RotatedBoxes", "(", "self", ".", "boxlist_to_tensor", "(", "gt", ",", "output_box_dim", "=", "5", ")", ")", "\n", "return", "pairwise_iou_rotated", "(", "dt", ",", "gt", ")", "\n", "", "else", ":", "\n", "# This is the same as the classical COCO evaluation", "\n", "            ", "return", "maskUtils", ".", "iou", "(", "dt", ",", "gt", ",", "is_crowd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.computeIoU": [[68, 95], ["numpy.argsort", "rotated_coco_evaluation.RotatedCOCOeval.compute_iou_dt_gt", "len", "int", "len", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOeval.compute_iou_dt_gt"], ["", "", "def", "computeIoU", "(", "self", ",", "imgId", ",", "catId", ")", ":", "\n", "        ", "p", "=", "self", ".", "params", "\n", "if", "p", ".", "useCats", ":", "\n", "            ", "gt", "=", "self", ".", "_gts", "[", "imgId", ",", "catId", "]", "\n", "dt", "=", "self", ".", "_dts", "[", "imgId", ",", "catId", "]", "\n", "", "else", ":", "\n", "            ", "gt", "=", "[", "_", "for", "cId", "in", "p", ".", "catIds", "for", "_", "in", "self", ".", "_gts", "[", "imgId", ",", "cId", "]", "]", "\n", "dt", "=", "[", "_", "for", "cId", "in", "p", ".", "catIds", "for", "_", "in", "self", ".", "_dts", "[", "imgId", ",", "cId", "]", "]", "\n", "", "if", "len", "(", "gt", ")", "==", "0", "and", "len", "(", "dt", ")", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "", "inds", "=", "np", ".", "argsort", "(", "[", "-", "d", "[", "\"score\"", "]", "for", "d", "in", "dt", "]", ",", "kind", "=", "\"mergesort\"", ")", "\n", "dt", "=", "[", "dt", "[", "i", "]", "for", "i", "in", "inds", "]", "\n", "if", "len", "(", "dt", ")", ">", "p", ".", "maxDets", "[", "-", "1", "]", ":", "\n", "            ", "dt", "=", "dt", "[", "0", ":", "p", ".", "maxDets", "[", "-", "1", "]", "]", "\n", "\n", "", "assert", "p", ".", "iouType", "==", "\"bbox\"", ",", "\"unsupported iouType for iou computation\"", "\n", "\n", "g", "=", "[", "g", "[", "\"bbox\"", "]", "for", "g", "in", "gt", "]", "\n", "d", "=", "[", "d", "[", "\"bbox\"", "]", "for", "d", "in", "dt", "]", "\n", "\n", "# compute iou between each dt and gt region", "\n", "iscrowd", "=", "[", "int", "(", "o", "[", "\"iscrowd\"", "]", ")", "for", "o", "in", "gt", "]", "\n", "\n", "# Note: this function is copied from cocoeval.py in cocoapi", "\n", "# and the major difference is here.", "\n", "ious", "=", "self", ".", "compute_iou_dt_gt", "(", "d", ",", "g", ",", "iscrowd", ")", "\n", "return", "ious", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOEvaluator.process": [[104, 123], ["zip", "rotated_coco_evaluation.RotatedCOCOEvaluator._predictions.append", "output[].to", "rotated_coco_evaluation.RotatedCOCOEvaluator.instances_to_json", "output[].to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOEvaluator.instances_to_json", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).\n                It is a list of dict. Each dict corresponds to an image and\n                contains keys like \"height\", \"width\", \"file_name\", \"image_id\".\n            outputs: the outputs of a COCO model. It is a list of dicts with key\n                \"instances\" that contains :class:`Instances`.\n        \"\"\"", "\n", "for", "input", ",", "output", "in", "zip", "(", "inputs", ",", "outputs", ")", ":", "\n", "            ", "prediction", "=", "{", "\"image_id\"", ":", "input", "[", "\"image_id\"", "]", "}", "\n", "\n", "if", "\"instances\"", "in", "output", ":", "\n", "                ", "instances", "=", "output", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "_cpu_device", ")", "\n", "\n", "prediction", "[", "\"instances\"", "]", "=", "self", ".", "instances_to_json", "(", "instances", ",", "input", "[", "\"image_id\"", "]", ")", "\n", "", "if", "\"proposals\"", "in", "output", ":", "\n", "                ", "prediction", "[", "\"proposals\"", "]", "=", "output", "[", "\"proposals\"", "]", ".", "to", "(", "self", ".", "_cpu_device", ")", "\n", "", "self", ".", "_predictions", ".", "append", "(", "prediction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOEvaluator.instances_to_json": [[124, 147], ["len", "instances.pred_boxes.tensor.numpy", "detectron2.structures.BoxMode.convert.tolist", "instances.scores.tolist", "instances.pred_classes.tolist", "range", "detectron2.structures.BoxMode.convert", "results.append"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.BoxMode.convert"], ["", "", "def", "instances_to_json", "(", "self", ",", "instances", ",", "img_id", ")", ":", "\n", "        ", "num_instance", "=", "len", "(", "instances", ")", "\n", "if", "num_instance", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "", "boxes", "=", "instances", ".", "pred_boxes", ".", "tensor", ".", "numpy", "(", ")", "\n", "if", "boxes", ".", "shape", "[", "1", "]", "==", "4", ":", "\n", "            ", "boxes", "=", "BoxMode", ".", "convert", "(", "boxes", ",", "BoxMode", ".", "XYXY_ABS", ",", "BoxMode", ".", "XYWH_ABS", ")", "\n", "", "boxes", "=", "boxes", ".", "tolist", "(", ")", "\n", "scores", "=", "instances", ".", "scores", ".", "tolist", "(", ")", "\n", "classes", "=", "instances", ".", "pred_classes", ".", "tolist", "(", ")", "\n", "\n", "results", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "num_instance", ")", ":", "\n", "            ", "result", "=", "{", "\n", "\"image_id\"", ":", "img_id", ",", "\n", "\"category_id\"", ":", "classes", "[", "k", "]", ",", "\n", "\"bbox\"", ":", "boxes", "[", "k", "]", ",", "\n", "\"score\"", ":", "scores", "[", "k", "]", ",", "\n", "}", "\n", "\n", "results", ".", "append", "(", "result", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOEvaluator._eval_predictions": [[148, 191], ["rotated_coco_evaluation.RotatedCOCOEvaluator._logger.info", "list", "hasattr", "rotated_coco_evaluation.RotatedCOCOEvaluator._logger.info", "rotated_coco_evaluation.RotatedCOCOEvaluator._derive_coco_results", "itertools.chain", "os.path.join", "rotated_coco_evaluation.RotatedCOCOEvaluator._logger.info", "rotated_coco_evaluation.RotatedCOCOEvaluator._logger.info", "rotated_coco_evaluation.RotatedCOCOEvaluator._evaluate_predictions_on_coco", "detectron2.utils.file_io.PathManager.open", "f.write", "f.flush", "set", "len", "rotated_coco_evaluation.RotatedCOCOEvaluator._metadata.get", "rotated_coco_evaluation.RotatedCOCOEvaluator._metadata.thing_dataset_id_to_contiguous_id.items", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.coco_evaluation.COCOEvaluator._derive_coco_results", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOEvaluator._evaluate_predictions_on_coco", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "_eval_predictions", "(", "self", ",", "predictions", ",", "img_ids", "=", "None", ")", ":", "# img_ids: unused", "\n", "        ", "\"\"\"\n        Evaluate predictions on the given tasks.\n        Fill self._results with the metrics of the tasks.\n        \"\"\"", "\n", "self", ".", "_logger", ".", "info", "(", "\"Preparing results for COCO format ...\"", ")", "\n", "coco_results", "=", "list", "(", "itertools", ".", "chain", "(", "*", "[", "x", "[", "\"instances\"", "]", "for", "x", "in", "predictions", "]", ")", ")", "\n", "\n", "# unmap the category ids for COCO", "\n", "if", "hasattr", "(", "self", ".", "_metadata", ",", "\"thing_dataset_id_to_contiguous_id\"", ")", ":", "\n", "            ", "reverse_id_mapping", "=", "{", "\n", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "_metadata", ".", "thing_dataset_id_to_contiguous_id", ".", "items", "(", ")", "\n", "}", "\n", "for", "result", "in", "coco_results", ":", "\n", "                ", "result", "[", "\"category_id\"", "]", "=", "reverse_id_mapping", "[", "result", "[", "\"category_id\"", "]", "]", "\n", "\n", "", "", "if", "self", ".", "_output_dir", ":", "\n", "            ", "file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_output_dir", ",", "\"coco_instances_results.json\"", ")", "\n", "self", ".", "_logger", ".", "info", "(", "\"Saving results to {}\"", ".", "format", "(", "file_path", ")", ")", "\n", "with", "PathManager", ".", "open", "(", "file_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "json", ".", "dumps", "(", "coco_results", ")", ")", "\n", "f", ".", "flush", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "_do_evaluation", ":", "\n", "            ", "self", ".", "_logger", ".", "info", "(", "\"Annotations are not available for evaluation.\"", ")", "\n", "return", "\n", "\n", "", "self", ".", "_logger", ".", "info", "(", "\"Evaluating predictions ...\"", ")", "\n", "\n", "assert", "self", ".", "_tasks", "is", "None", "or", "set", "(", "self", ".", "_tasks", ")", "==", "{", "\n", "\"bbox\"", "\n", "}", ",", "\"[RotatedCOCOEvaluator] Only bbox evaluation is supported\"", "\n", "coco_eval", "=", "(", "\n", "self", ".", "_evaluate_predictions_on_coco", "(", "self", ".", "_coco_api", ",", "coco_results", ")", "\n", "if", "len", "(", "coco_results", ")", ">", "0", "\n", "else", "None", "# cocoapi does not handle empty results very well", "\n", ")", "\n", "\n", "task", "=", "\"bbox\"", "\n", "res", "=", "self", ".", "_derive_coco_results", "(", "\n", "coco_eval", ",", "task", ",", "class_names", "=", "self", ".", "_metadata", ".", "get", "(", "\"thing_classes\"", ")", "\n", ")", "\n", "self", ".", "_results", "[", "task", "]", "=", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.rotated_coco_evaluation.RotatedCOCOEvaluator._evaluate_predictions_on_coco": [[192, 208], ["coco_gt.loadRes", "rotated_coco_evaluation.RotatedCOCOeval", "RotatedCOCOeval.evaluate", "RotatedCOCOeval.accumulate", "RotatedCOCOeval.summarize", "len"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.evaluate", "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.fast_eval_api.COCOeval_opt.accumulate"], ["", "def", "_evaluate_predictions_on_coco", "(", "self", ",", "coco_gt", ",", "coco_results", ")", ":", "\n", "        ", "\"\"\"\n        Evaluate the coco results using COCOEval API.\n        \"\"\"", "\n", "assert", "len", "(", "coco_results", ")", ">", "0", "\n", "\n", "coco_dt", "=", "coco_gt", ".", "loadRes", "(", "coco_results", ")", "\n", "\n", "# Only bbox is supported for now", "\n", "coco_eval", "=", "RotatedCOCOeval", "(", "coco_gt", ",", "coco_dt", ",", "iouType", "=", "\"bbox\"", ")", "\n", "\n", "coco_eval", ".", "evaluate", "(", ")", "\n", "coco_eval", ".", "accumulate", "(", ")", "\n", "coco_eval", ".", "summarize", "(", ")", "\n", "\n", "return", "coco_eval", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.cityscapes_evaluation.CityscapesEvaluator.__init__": [[23, 33], ["detectron2.data.MetadataCatalog.get", "torch.device", "logging.getLogger"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device"], ["def", "__init__", "(", "self", ",", "dataset_name", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            dataset_name (str): the name of the dataset.\n                It must have the following metadata associated with it:\n                \"thing_classes\", \"gt_dir\".\n        \"\"\"", "\n", "self", ".", "_metadata", "=", "MetadataCatalog", ".", "get", "(", "dataset_name", ")", "\n", "self", ".", "_cpu_device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "self", ".", "_logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.cityscapes_evaluation.CityscapesEvaluator.reset": [[34, 44], ["tempfile.TemporaryDirectory", "cityscapes_evaluation.CityscapesEvaluator._logger.info", "detectron2.utils.comm.all_gather", "cityscapes_evaluation.CityscapesEvaluator._working_dir.cleanup"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_working_dir", "=", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"cityscapes_eval_\"", ")", "\n", "self", ".", "_temp_dir", "=", "self", ".", "_working_dir", ".", "name", "\n", "# All workers will write to the same results directory", "\n", "# TODO this does not work in distributed training", "\n", "self", ".", "_temp_dir", "=", "comm", ".", "all_gather", "(", "self", ".", "_temp_dir", ")", "[", "0", "]", "\n", "if", "self", ".", "_temp_dir", "!=", "self", ".", "_working_dir", ".", "name", ":", "\n", "            ", "self", ".", "_working_dir", ".", "cleanup", "(", ")", "\n", "", "self", ".", "_logger", ".", "info", "(", "\n", "\"Writing cityscapes results to temporary directory {} ...\"", ".", "format", "(", "self", ".", "_temp_dir", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.cityscapes_evaluation.CityscapesInstanceEvaluator.process": [[57, 87], ["zip", "os.path.join", "os.path.splitext", "output[].to", "len", "os.path.basename", "open", "range", "open", "output[].to.pred_masks[].numpy().astype", "os.path.join", "PIL.Image.fromarray().save", "fout.write", "output[].to.pred_masks[].numpy", "PIL.Image.fromarray", "os.path.basename"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "from", "cityscapesscripts", ".", "helpers", ".", "labels", "import", "name2label", "\n", "\n", "for", "input", ",", "output", "in", "zip", "(", "inputs", ",", "outputs", ")", ":", "\n", "            ", "file_name", "=", "input", "[", "\"file_name\"", "]", "\n", "basename", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "file_name", ")", ")", "[", "0", "]", "\n", "pred_txt", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_temp_dir", ",", "basename", "+", "\"_pred.txt\"", ")", "\n", "\n", "if", "\"instances\"", "in", "output", ":", "\n", "                ", "output", "=", "output", "[", "\"instances\"", "]", ".", "to", "(", "self", ".", "_cpu_device", ")", "\n", "num_instances", "=", "len", "(", "output", ")", "\n", "with", "open", "(", "pred_txt", ",", "\"w\"", ")", "as", "fout", ":", "\n", "                    ", "for", "i", "in", "range", "(", "num_instances", ")", ":", "\n", "                        ", "pred_class", "=", "output", ".", "pred_classes", "[", "i", "]", "\n", "classes", "=", "self", ".", "_metadata", ".", "thing_classes", "[", "pred_class", "]", "\n", "class_id", "=", "name2label", "[", "classes", "]", ".", "id", "\n", "score", "=", "output", ".", "scores", "[", "i", "]", "\n", "mask", "=", "output", ".", "pred_masks", "[", "i", "]", ".", "numpy", "(", ")", ".", "astype", "(", "\"uint8\"", ")", "\n", "png_filename", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_temp_dir", ",", "basename", "+", "\"_{}_{}.png\"", ".", "format", "(", "i", ",", "classes", ")", "\n", ")", "\n", "\n", "Image", ".", "fromarray", "(", "mask", "*", "255", ")", ".", "save", "(", "png_filename", ")", "\n", "fout", ".", "write", "(", "\n", "\"{} {} {}\\n\"", ".", "format", "(", "os", ".", "path", ".", "basename", "(", "png_filename", ")", ",", "class_id", ",", "score", ")", "\n", ")", "\n", "", "", "", "else", ":", "\n", "# Cityscapes requires a prediction file for every ground truth image.", "\n", "                ", "with", "open", "(", "pred_txt", ",", "\"w\"", ")", "as", "fout", ":", "\n", "                    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.cityscapes_evaluation.CityscapesInstanceEvaluator.evaluate": [[88, 127], ["detectron2.utils.comm.synchronize", "cityscapes_evaluation.CityscapesInstanceEvaluator._logger.info", "os.path.abspath", "os.path.join", "detectron2.utils.file_io.PathManager.get_local_path", "glob.glob", "len", "collections.OrderedDict", "cityscapes_evaluation.CityscapesInstanceEvaluator._working_dir.cleanup", "detectron2.utils.comm.get_rank", "os.path.join", "predictionImgList.append", "cityscapes_eval.evaluateImgLists", "cityscapes_eval.getPrediction"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["", "", "", "", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            dict: has a key \"segm\", whose value is a dict of \"AP\" and \"AP50\".\n        \"\"\"", "\n", "comm", ".", "synchronize", "(", ")", "\n", "if", "comm", ".", "get_rank", "(", ")", ">", "0", ":", "\n", "            ", "return", "\n", "", "import", "cityscapesscripts", ".", "evaluation", ".", "evalInstanceLevelSemanticLabeling", "as", "cityscapes_eval", "\n", "\n", "self", ".", "_logger", ".", "info", "(", "\"Evaluating results under {} ...\"", ".", "format", "(", "self", ".", "_temp_dir", ")", ")", "\n", "\n", "# set some global states in cityscapes evaluation API, before evaluating", "\n", "cityscapes_eval", ".", "args", ".", "predictionPath", "=", "os", ".", "path", ".", "abspath", "(", "self", ".", "_temp_dir", ")", "\n", "cityscapes_eval", ".", "args", ".", "predictionWalk", "=", "None", "\n", "cityscapes_eval", ".", "args", ".", "JSONOutput", "=", "False", "\n", "cityscapes_eval", ".", "args", ".", "colorized", "=", "False", "\n", "cityscapes_eval", ".", "args", ".", "gtInstancesFile", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_temp_dir", ",", "\"gtInstances.json\"", ")", "\n", "\n", "# These lines are adopted from", "\n", "# https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/evaluation/evalInstanceLevelSemanticLabeling.py # noqa", "\n", "gt_dir", "=", "PathManager", ".", "get_local_path", "(", "self", ".", "_metadata", ".", "gt_dir", ")", "\n", "groundTruthImgList", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "gt_dir", ",", "\"*\"", ",", "\"*_gtFine_instanceIds.png\"", ")", ")", "\n", "assert", "len", "(", "\n", "groundTruthImgList", "\n", ")", ",", "\"Cannot find any ground truth images to use for evaluation. Searched for: {}\"", ".", "format", "(", "\n", "cityscapes_eval", ".", "args", ".", "groundTruthSearch", "\n", ")", "\n", "predictionImgList", "=", "[", "]", "\n", "for", "gt", "in", "groundTruthImgList", ":", "\n", "            ", "predictionImgList", ".", "append", "(", "cityscapes_eval", ".", "getPrediction", "(", "gt", ",", "cityscapes_eval", ".", "args", ")", ")", "\n", "", "results", "=", "cityscapes_eval", ".", "evaluateImgLists", "(", "\n", "predictionImgList", ",", "groundTruthImgList", ",", "cityscapes_eval", ".", "args", "\n", ")", "[", "\"averages\"", "]", "\n", "\n", "ret", "=", "OrderedDict", "(", ")", "\n", "ret", "[", "\"segm\"", "]", "=", "{", "\"AP\"", ":", "results", "[", "\"allAp\"", "]", "*", "100", ",", "\"AP50\"", ":", "results", "[", "\"allAp50%\"", "]", "*", "100", "}", "\n", "self", ".", "_working_dir", ".", "cleanup", "(", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.cityscapes_evaluation.CityscapesSemSegEvaluator.process": [[139, 154], ["zip", "os.path.join", "output[].argmax().to().numpy", "trainId2label.items", "PIL.Image.fromarray().save", "os.path.splitext", "numpy.ones", "os.path.basename", "output[].argmax().to", "PIL.Image.fromarray", "output[].argmax"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "from", "cityscapesscripts", ".", "helpers", ".", "labels", "import", "trainId2label", "\n", "\n", "for", "input", ",", "output", "in", "zip", "(", "inputs", ",", "outputs", ")", ":", "\n", "            ", "file_name", "=", "input", "[", "\"file_name\"", "]", "\n", "basename", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "file_name", ")", ")", "[", "0", "]", "\n", "pred_filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_temp_dir", ",", "basename", "+", "\"_pred.png\"", ")", "\n", "\n", "output", "=", "output", "[", "\"sem_seg\"", "]", ".", "argmax", "(", "dim", "=", "0", ")", ".", "to", "(", "self", ".", "_cpu_device", ")", ".", "numpy", "(", ")", "\n", "pred", "=", "255", "*", "np", ".", "ones", "(", "output", ".", "shape", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "for", "train_id", ",", "label", "in", "trainId2label", ".", "items", "(", ")", ":", "\n", "                ", "if", "label", ".", "ignoreInEval", ":", "\n", "                    ", "continue", "\n", "", "pred", "[", "output", "==", "train_id", "]", "=", "label", ".", "id", "\n", "", "Image", ".", "fromarray", "(", "pred", ")", ".", "save", "(", "pred_filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.evaluation.cityscapes_evaluation.CityscapesSemSegEvaluator.evaluate": [[155, 195], ["detectron2.utils.comm.synchronize", "cityscapes_evaluation.CityscapesSemSegEvaluator._logger.info", "os.path.abspath", "detectron2.utils.file_io.PathManager.get_local_path", "glob.glob", "len", "cityscapes_eval.evaluateImgLists", "collections.OrderedDict", "cityscapes_evaluation.CityscapesSemSegEvaluator._working_dir.cleanup", "detectron2.utils.comm.get_rank", "os.path.join", "predictionImgList.append", "cityscapes_eval.getPrediction"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.synchronize", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.get_rank"], ["", "", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "comm", ".", "synchronize", "(", ")", "\n", "if", "comm", ".", "get_rank", "(", ")", ">", "0", ":", "\n", "            ", "return", "\n", "# Load the Cityscapes eval script *after* setting the required env var,", "\n", "# since the script reads CITYSCAPES_DATASET into global variables at load time.", "\n", "", "import", "cityscapesscripts", ".", "evaluation", ".", "evalPixelLevelSemanticLabeling", "as", "cityscapes_eval", "\n", "\n", "self", ".", "_logger", ".", "info", "(", "\"Evaluating results under {} ...\"", ".", "format", "(", "self", ".", "_temp_dir", ")", ")", "\n", "\n", "# set some global states in cityscapes evaluation API, before evaluating", "\n", "cityscapes_eval", ".", "args", ".", "predictionPath", "=", "os", ".", "path", ".", "abspath", "(", "self", ".", "_temp_dir", ")", "\n", "cityscapes_eval", ".", "args", ".", "predictionWalk", "=", "None", "\n", "cityscapes_eval", ".", "args", ".", "JSONOutput", "=", "False", "\n", "cityscapes_eval", ".", "args", ".", "colorized", "=", "False", "\n", "\n", "# These lines are adopted from", "\n", "# https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/evaluation/evalPixelLevelSemanticLabeling.py # noqa", "\n", "gt_dir", "=", "PathManager", ".", "get_local_path", "(", "self", ".", "_metadata", ".", "gt_dir", ")", "\n", "groundTruthImgList", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "gt_dir", ",", "\"*\"", ",", "\"*_gtFine_labelIds.png\"", ")", ")", "\n", "assert", "len", "(", "\n", "groundTruthImgList", "\n", ")", ",", "\"Cannot find any ground truth images to use for evaluation. Searched for: {}\"", ".", "format", "(", "\n", "cityscapes_eval", ".", "args", ".", "groundTruthSearch", "\n", ")", "\n", "predictionImgList", "=", "[", "]", "\n", "for", "gt", "in", "groundTruthImgList", ":", "\n", "            ", "predictionImgList", ".", "append", "(", "cityscapes_eval", ".", "getPrediction", "(", "cityscapes_eval", ".", "args", ",", "gt", ")", ")", "\n", "", "results", "=", "cityscapes_eval", ".", "evaluateImgLists", "(", "\n", "predictionImgList", ",", "groundTruthImgList", ",", "cityscapes_eval", ".", "args", "\n", ")", "\n", "ret", "=", "OrderedDict", "(", ")", "\n", "ret", "[", "\"sem_seg\"", "]", "=", "{", "\n", "\"IoU\"", ":", "100.0", "*", "results", "[", "\"averageScoreClasses\"", "]", ",", "\n", "\"iIoU\"", ":", "100.0", "*", "results", "[", "\"averageScoreInstClasses\"", "]", ",", "\n", "\"IoU_sup\"", ":", "100.0", "*", "results", "[", "\"averageScoreCategories\"", "]", ",", "\n", "\"iIoU_sup\"", ":", "100.0", "*", "results", "[", "\"averageScoreInstCategories\"", "]", ",", "\n", "}", "\n", "self", ".", "_working_dir", ".", "cleanup", "(", ")", "\n", "return", "ret", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build._create_gradient_clipper": [[23, 41], ["copy.deepcopy", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_value_", "build.GradientClipType"], "function", ["None"], ["model", ".", "to", "(", "torch", ".", "device", "(", "cfg", ".", "MODEL", ".", "DEVICE", ")", ")", "\n", "_log_api_usage", "(", "\"modeling.meta_arch.\"", "+", "meta_arch", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build._generate_optimizer_class_with_gradient_clipping": [[43, 75], ["type", "super().step", "itertools.chain", "global_clipper", "per_param_clipper", "type"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.maybe_add_gradient_clipping": [[77, 111], ["isinstance", "build._create_gradient_clipper", "build._generate_optimizer_class_with_gradient_clipping", "isinstance", "type", "issubclass", "torch.optim.SGD"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.build._create_gradient_clipper", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build._generate_optimizer_class_with_gradient_clipping"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.build_optimizer": [[113, 130], ["build.get_default_optimizer_params", "build.maybe_add_gradient_clipping"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.get_default_optimizer_params", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.maybe_add_gradient_clipping"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.get_default_optimizer_params": [[133, 218], ["len", "set", "model.modules", "module.named_parameters", "ValueError", "ValueError", "memo.add", "copy.copy", "copy.copy.update", "params.append", "isinstance", "overrides.get"], "function", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog.Metadata.set", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.build.build_lr_scheduler": [[220, 253], ["lr_scheduler.WarmupParamScheduler", "lr_scheduler.LRMultiplier", "fvcore.common.param_scheduler.MultiStepParamScheduler", "min", "len", "len", "logging.getLogger", "logging.getLogger.warning", "fvcore.common.param_scheduler.CosineParamScheduler", "ValueError", "range", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.WarmupParamScheduler.__init__": [[22, 49], ["scheduler", "fvcore.common.param_scheduler.CompositeParamScheduler.__init__", "scheduler", "fvcore.common.param_scheduler.ConstantParamScheduler", "fvcore.common.param_scheduler.LinearParamScheduler", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.LRScheduler.scheduler", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.hooks.LRScheduler.scheduler"], ["def", "__init__", "(", "\n", "self", ",", "\n", "scheduler", ":", "ParamScheduler", ",", "\n", "warmup_factor", ":", "float", ",", "\n", "warmup_length", ":", "float", ",", "\n", "warmup_method", ":", "str", "=", "\"linear\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            scheduler: warmup will be added at the beginning of this scheduler\n            warmup_factor: the factor w.r.t the initial value of ``scheduler``, e.g. 0.001\n            warmup_length: the relative length (in [0, 1]) of warmup steps w.r.t the entire\n                training, e.g. 0.01\n            warmup_method: one of \"linear\" or \"constant\"\n        \"\"\"", "\n", "end_value", "=", "scheduler", "(", "warmup_length", ")", "# the value to reach when warmup ends", "\n", "start_value", "=", "warmup_factor", "*", "scheduler", "(", "0.0", ")", "\n", "if", "warmup_method", "==", "\"constant\"", ":", "\n", "            ", "warmup", "=", "ConstantParamScheduler", "(", "start_value", ")", "\n", "", "elif", "warmup_method", "==", "\"linear\"", ":", "\n", "            ", "warmup", "=", "LinearParamScheduler", "(", "start_value", ",", "end_value", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown warmup method: {}\"", ".", "format", "(", "warmup_method", ")", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "\n", "[", "warmup", ",", "scheduler", "]", ",", "\n", "interval_scaling", "=", "[", "\"rescaled\"", ",", "\"fixed\"", "]", ",", "\n", "lengths", "=", "[", "warmup_length", ",", "1", "-", "warmup_length", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.__init__": [[86, 109], ["super().__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "multiplier", ":", "ParamScheduler", ",", "\n", "max_iter", ":", "int", ",", "\n", "last_iter", ":", "int", "=", "-", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            optimizer, last_iter: See ``torch.optim.lr_scheduler._LRScheduler``.\n                ``last_iter`` is the same as ``last_epoch``.\n            multiplier: a fvcore ParamScheduler that defines the multiplier on\n                every LR of the optimizer\n            max_iter: the total number of training iterations\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "multiplier", ",", "ParamScheduler", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"_LRMultiplier(multiplier=) must be an instance of fvcore \"", "\n", "f\"ParamScheduler. Got {multiplier} instead.\"", "\n", ")", "\n", "", "self", ".", "_multiplier", "=", "multiplier", "\n", "self", ".", "_max_iter", "=", "max_iter", "\n", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", "=", "last_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict": [[110, 113], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "# fvcore schedulers are stateless. Only keep pytorch scheduler states", "\n", "        ", "return", "{", "\"base_lrs\"", ":", "self", ".", "base_lrs", ",", "\"last_epoch\"", ":", "self", ".", "last_epoch", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.get_lr": [[114, 117], ["lr_scheduler.LRMultiplier._multiplier"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", "->", "List", "[", "float", "]", ":", "\n", "        ", "multiplier", "=", "self", ".", "_multiplier", "(", "self", ".", "last_epoch", "/", "self", ".", "_max_iter", ")", "\n", "return", "[", "base_lr", "*", "multiplier", "for", "base_lr", "in", "self", ".", "base_lrs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.WarmupMultiStepLR.__init__": [[133, 156], ["logger.warning", "super().__init__", "ValueError", "list", "sorted"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "milestones", ":", "List", "[", "int", "]", ",", "\n", "gamma", ":", "float", "=", "0.1", ",", "\n", "warmup_factor", ":", "float", "=", "0.001", ",", "\n", "warmup_iters", ":", "int", "=", "1000", ",", "\n", "warmup_method", ":", "str", "=", "\"linear\"", ",", "\n", "last_epoch", ":", "int", "=", "-", "1", ",", "\n", ")", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\"", "\n", ")", "\n", "if", "not", "list", "(", "milestones", ")", "==", "sorted", "(", "milestones", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Milestones should be a list of\"", "\" increasing integers. Got {}\"", ",", "milestones", "\n", ")", "\n", "", "self", ".", "milestones", "=", "milestones", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "warmup_factor", "=", "warmup_factor", "\n", "self", ".", "warmup_iters", "=", "warmup_iters", "\n", "self", ".", "warmup_method", "=", "warmup_method", "\n", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.WarmupMultiStepLR.get_lr": [[157, 164], ["lr_scheduler._get_warmup_factor_at_iter", "bisect.bisect_right"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler._get_warmup_factor_at_iter"], ["", "def", "get_lr", "(", "self", ")", "->", "List", "[", "float", "]", ":", "\n", "        ", "warmup_factor", "=", "_get_warmup_factor_at_iter", "(", "\n", "self", ".", "warmup_method", ",", "self", ".", "last_epoch", ",", "self", ".", "warmup_iters", ",", "self", ".", "warmup_factor", "\n", ")", "\n", "return", "[", "\n", "base_lr", "*", "warmup_factor", "*", "self", ".", "gamma", "**", "bisect_right", "(", "self", ".", "milestones", ",", "self", ".", "last_epoch", ")", "\n", "for", "base_lr", "in", "self", ".", "base_lrs", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.WarmupMultiStepLR._compute_values": [[166, 169], ["lr_scheduler.WarmupMultiStepLR.get_lr"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.WarmupCosineLR.get_lr"], ["", "def", "_compute_values", "(", "self", ")", "->", "List", "[", "float", "]", ":", "\n", "# The new interface", "\n", "        ", "return", "self", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.WarmupCosineLR.__init__": [[172, 189], ["logger.warning", "super().__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "max_iters", ":", "int", ",", "\n", "warmup_factor", ":", "float", "=", "0.001", ",", "\n", "warmup_iters", ":", "int", "=", "1000", ",", "\n", "warmup_method", ":", "str", "=", "\"linear\"", ",", "\n", "last_epoch", ":", "int", "=", "-", "1", ",", "\n", ")", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"WarmupCosineLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\"", "\n", ")", "\n", "self", ".", "max_iters", "=", "max_iters", "\n", "self", ".", "warmup_factor", "=", "warmup_factor", "\n", "self", ".", "warmup_iters", "=", "warmup_iters", "\n", "self", ".", "warmup_method", "=", "warmup_method", "\n", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.WarmupCosineLR.get_lr": [[190, 205], ["lr_scheduler._get_warmup_factor_at_iter", "math.cos"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler._get_warmup_factor_at_iter"], ["", "def", "get_lr", "(", "self", ")", "->", "List", "[", "float", "]", ":", "\n", "        ", "warmup_factor", "=", "_get_warmup_factor_at_iter", "(", "\n", "self", ".", "warmup_method", ",", "self", ".", "last_epoch", ",", "self", ".", "warmup_iters", ",", "self", ".", "warmup_factor", "\n", ")", "\n", "# Different definitions of half-cosine with warmup are possible. For", "\n", "# simplicity we multiply the standard half-cosine schedule by the warmup", "\n", "# factor. An alternative is to start the period of the cosine at warmup_iters", "\n", "# instead of at 0. In the case that warmup_iters << max_iters the two are", "\n", "# very close to each other.", "\n", "return", "[", "\n", "base_lr", "\n", "*", "warmup_factor", "\n", "*", "0.5", "\n", "*", "(", "1.0", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "self", ".", "last_epoch", "/", "self", ".", "max_iters", ")", ")", "\n", "for", "base_lr", "in", "self", ".", "base_lrs", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.WarmupCosineLR._compute_values": [[207, 210], ["lr_scheduler.WarmupCosineLR.get_lr"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.WarmupCosineLR.get_lr"], ["", "def", "_compute_values", "(", "self", ")", "->", "List", "[", "float", "]", ":", "\n", "# The new interface", "\n", "        ", "return", "self", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler._get_warmup_factor_at_iter": [[212, 239], ["ValueError"], "function", ["None"], ["", "", "def", "_get_warmup_factor_at_iter", "(", "\n", "method", ":", "str", ",", "iter", ":", "int", ",", "warmup_iters", ":", "int", ",", "warmup_factor", ":", "float", "\n", ")", "->", "float", ":", "\n", "    ", "\"\"\"\n    Return the learning rate warmup factor at a specific iteration.\n    See :paper:`ImageNet in 1h` for more details.\n\n    Args:\n        method (str): warmup method; either \"constant\" or \"linear\".\n        iter (int): iteration at which to calculate the warmup factor.\n        warmup_iters (int): the number of warmup iterations.\n        warmup_factor (float): the base warmup factor (the meaning changes according\n            to the method used).\n\n    Returns:\n        float: the effective warmup factor at the given iteration.\n    \"\"\"", "\n", "if", "iter", ">=", "warmup_iters", ":", "\n", "        ", "return", "1.0", "\n", "\n", "", "if", "method", "==", "\"constant\"", ":", "\n", "        ", "return", "warmup_factor", "\n", "", "elif", "method", "==", "\"linear\"", ":", "\n", "        ", "alpha", "=", "iter", "/", "warmup_iters", "\n", "return", "warmup_factor", "*", "(", "1", "-", "alpha", ")", "+", "alpha", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown warmup method: {}\"", ".", "format", "(", "method", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_packaging.TestProjects.test_import": [[8, 16], ["None"], "methods", ["None"], ["    ", "def", "test_import", "(", "self", ")", ":", "\n", "        ", "from", "detectron2", ".", "projects", "import", "point_rend", "\n", "\n", "_", "=", "point_rend", ".", "add_pointrend_config", "\n", "\n", "import", "detectron2", ".", "projects", ".", "deeplab", "as", "deeplab", "\n", "\n", "_", "=", "deeplab", ".", "add_deeplab_config", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_packaging.TestCollectEnv.test": [[23, 25], ["detectron2.utils.collect_env.collect_env_info"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.collect_env.collect_env_info"], ["    ", "def", "test", "(", "self", ")", ":", "\n", "        ", "_", "=", "collect_env_info", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_scheduler.TestScheduler.test_warmup_multistep": [[14, 44], ["torch.nn.Parameter", "torch.optim.SGD", "detectron2.solver.WarmupParamScheduler", "detectron2.solver.LRMultiplier", "torch.nn.Parameter.sum().backward", "torch.optim.SGD.step", "range", "test_scheduler.TestScheduler.assertTrue", "test_scheduler.TestScheduler.assertTrue", "test_scheduler.TestScheduler.assertTrue", "test_scheduler.TestScheduler.assertTrue", "test_scheduler.TestScheduler.assertTrue", "torch.zeros", "fvcore.common.param_scheduler.MultiStepParamScheduler", "detectron2.solver.LRMultiplier.step", "lrs.append", "numpy.allclose", "numpy.allclose", "numpy.allclose", "numpy.allclose", "numpy.allclose", "torch.nn.Parameter.sum"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step"], ["    ", "def", "test_warmup_multistep", "(", "self", ")", ":", "\n", "        ", "p", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "0", ")", ")", "\n", "opt", "=", "torch", ".", "optim", ".", "SGD", "(", "[", "p", "]", ",", "lr", "=", "5", ")", "\n", "\n", "multiplier", "=", "WarmupParamScheduler", "(", "\n", "MultiStepParamScheduler", "(", "\n", "[", "1", ",", "0.1", ",", "0.01", ",", "0.001", "]", ",", "\n", "milestones", "=", "[", "10", ",", "15", ",", "20", "]", ",", "\n", "num_updates", "=", "30", ",", "\n", ")", ",", "\n", "0.001", ",", "\n", "5", "/", "30", ",", "\n", ")", "\n", "sched", "=", "LRMultiplier", "(", "opt", ",", "multiplier", ",", "30", ")", "\n", "# This is an equivalent of:", "\n", "# sched = WarmupMultiStepLR(", "\n", "# opt, milestones=[10, 15, 20], gamma=0.1, warmup_factor=0.001, warmup_iters=5)", "\n", "\n", "p", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "opt", ".", "step", "(", ")", "\n", "\n", "lrs", "=", "[", "0.005", "]", "\n", "for", "_", "in", "range", "(", "30", ")", ":", "\n", "            ", "sched", ".", "step", "(", ")", "\n", "lrs", ".", "append", "(", "opt", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", ")", "\n", "", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "lrs", "[", ":", "5", "]", ",", "[", "0.005", ",", "1.004", ",", "2.003", ",", "3.002", ",", "4.001", "]", ")", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "lrs", "[", "5", ":", "10", "]", ",", "5.0", ")", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "lrs", "[", "10", ":", "15", "]", ",", "0.5", ")", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "lrs", "[", "15", ":", "20", "]", ",", "0.05", ")", ")", "\n", "self", ".", "assertTrue", "(", "np", ".", "allclose", "(", "lrs", "[", "20", ":", "]", ",", "0.005", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_scheduler.TestScheduler.test_warmup_cosine": [[45, 69], ["torch.nn.Parameter", "torch.optim.SGD", "detectron2.solver.WarmupParamScheduler", "detectron2.solver.LRMultiplier", "torch.nn.Parameter.sum().backward", "torch.optim.SGD.step", "test_scheduler.TestScheduler.assertEqual", "range", "enumerate", "torch.zeros", "fvcore.common.param_scheduler.CosineParamScheduler", "detectron2.solver.LRMultiplier.step", "lrs.append", "torch.nn.Parameter.sum", "test_scheduler.TestScheduler.assertAlmostEqual", "test_scheduler.TestScheduler.assertNotAlmostEqual", "math.cos"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.layers.roi_align_rotated._ROIAlignRotated.backward", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step"], ["", "def", "test_warmup_cosine", "(", "self", ")", ":", "\n", "        ", "p", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "0", ")", ")", "\n", "opt", "=", "torch", ".", "optim", ".", "SGD", "(", "[", "p", "]", ",", "lr", "=", "5", ")", "\n", "multiplier", "=", "WarmupParamScheduler", "(", "\n", "CosineParamScheduler", "(", "1", ",", "0", ")", ",", "\n", "0.001", ",", "\n", "5", "/", "30", ",", "\n", ")", "\n", "sched", "=", "LRMultiplier", "(", "opt", ",", "multiplier", ",", "30", ")", "\n", "\n", "p", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "opt", ".", "step", "(", ")", "\n", "self", ".", "assertEqual", "(", "opt", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", ",", "0.005", ")", "\n", "lrs", "=", "[", "0.005", "]", "\n", "\n", "for", "_", "in", "range", "(", "30", ")", ":", "\n", "            ", "sched", ".", "step", "(", ")", "\n", "lrs", ".", "append", "(", "opt", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", ")", "\n", "", "for", "idx", ",", "lr", "in", "enumerate", "(", "lrs", ")", ":", "\n", "            ", "expected_cosine", "=", "2.5", "*", "(", "1.0", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "idx", "/", "30", ")", ")", "\n", "if", "idx", ">=", "5", ":", "\n", "                ", "self", ".", "assertAlmostEqual", "(", "lr", ",", "expected_cosine", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "assertNotAlmostEqual", "(", "lr", ",", "expected_cosine", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo.test_get_returns_model": [[13, 17], ["detectron2.model_zoo.get", "test_model_zoo.TestModelZoo.assertIsInstance", "test_model_zoo.TestModelZoo.assertIsInstance"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["    ", "def", "test_get_returns_model", "(", "self", ")", ":", "\n", "        ", "model", "=", "model_zoo", ".", "get", "(", "\"Misc/scratch_mask_rcnn_R_50_FPN_3x_gn.yaml\"", ",", "trained", "=", "False", ")", "\n", "self", ".", "assertIsInstance", "(", "model", ",", "GeneralizedRCNN", ")", "\n", "self", ".", "assertIsInstance", "(", "model", ".", "backbone", ",", "FPN", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo.test_get_invalid_model": [[18, 20], ["test_model_zoo.TestModelZoo.assertRaises"], "methods", ["None"], ["", "def", "test_get_invalid_model", "(", "self", ")", ":", "\n", "        ", "self", ".", "assertRaises", "(", "RuntimeError", ",", "model_zoo", ".", "get", ",", "\"Invalid/config.yaml\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo.test_get_url": [[21, 29], ["detectron2.model_zoo.get_checkpoint_url", "test_model_zoo.TestModelZoo.assertEqual", "detectron2.model_zoo.get_checkpoint_url", "test_model_zoo.TestModelZoo.assertEqual"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_checkpoint_url", "home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_checkpoint_url"], ["", "def", "test_get_url", "(", "self", ")", ":", "\n", "        ", "url", "=", "model_zoo", ".", "get_checkpoint_url", "(", "\"Misc/scratch_mask_rcnn_R_50_FPN_3x_gn.yaml\"", ")", "\n", "self", ".", "assertEqual", "(", "\n", "url", ",", "\n", "\"https://dl.fbaipublicfiles.com/detectron2/Misc/scratch_mask_rcnn_R_50_FPN_3x_gn/138602908/model_final_01ca85.pkl\"", ",", "# noqa", "\n", ")", "\n", "url2", "=", "model_zoo", ".", "get_checkpoint_url", "(", "\"Misc/scratch_mask_rcnn_R_50_FPN_3x_gn.py\"", ")", "\n", "self", ".", "assertEqual", "(", "url", ",", "url2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo._build_lazy_model": [[30, 33], ["detectron2.model_zoo.get_config", "detectron2.config.instantiate"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate"], ["", "def", "_build_lazy_model", "(", "self", ",", "name", ")", ":", "\n", "        ", "cfg", "=", "model_zoo", ".", "get_config", "(", "\"common/models/\"", "+", "name", ")", "\n", "instantiate", "(", "cfg", ".", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo.test_mask_rcnn_fpn": [[34, 36], ["test_model_zoo.TestModelZoo._build_lazy_model"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo._build_lazy_model"], ["", "def", "test_mask_rcnn_fpn", "(", "self", ")", ":", "\n", "        ", "self", ".", "_build_lazy_model", "(", "\"mask_rcnn_fpn.py\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo.test_mask_rcnn_c4": [[37, 39], ["test_model_zoo.TestModelZoo._build_lazy_model"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo._build_lazy_model"], ["", "def", "test_mask_rcnn_c4", "(", "self", ")", ":", "\n", "        ", "self", ".", "_build_lazy_model", "(", "\"mask_rcnn_c4.py\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo.test_panoptic_fpn": [[40, 42], ["test_model_zoo.TestModelZoo._build_lazy_model"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo._build_lazy_model"], ["", "def", "test_panoptic_fpn", "(", "self", ")", ":", "\n", "        ", "self", ".", "_build_lazy_model", "(", "\"panoptic_fpn.py\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_zoo.TestModelZoo.test_schedule": [[43, 47], ["detectron2.model_zoo.get_config", "detectron2.model_zoo.get_config.items", "detectron2.config.instantiate"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate"], ["", "def", "test_schedule", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "model_zoo", ".", "get_config", "(", "\"common/coco_schedule.py\"", ")", "\n", "for", "_", ",", "v", "in", "cfg", ".", "items", "(", ")", ":", "\n", "            ", "instantiate", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate._test_obj": [[15, 19], ["detectron2.utils.registry._convert_target_to_string", "detectron2.utils.registry.locate", "test_registry.TestLocate.assertIs"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry._convert_target_to_string", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry.locate"], ["    ", "def", "_test_obj", "(", "self", ",", "obj", ")", ":", "\n", "        ", "name", "=", "_convert_target_to_string", "(", "obj", ")", "\n", "newobj", "=", "locate", "(", "name", ")", "\n", "self", ".", "assertIs", "(", "obj", ",", "newobj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate.test_basic": [[20, 22], ["test_registry.TestLocate._test_obj"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate._test_obj"], ["", "def", "test_basic", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_obj", "(", "GeneralizedRCNN", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate.test_inside_class": [[23, 26], ["test_registry.TestLocate._test_obj"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate._test_obj"], ["", "def", "test_inside_class", "(", "self", ")", ":", "\n", "# requires using __qualname__ instead of __name__", "\n", "        ", "self", ".", "_test_obj", "(", "A", ".", "B", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate.test_builtin": [[27, 30], ["test_registry.TestLocate._test_obj", "test_registry.TestLocate._test_obj"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate._test_obj", "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate._test_obj"], ["", "def", "test_builtin", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_obj", "(", "len", ")", "\n", "self", ".", "_test_obj", "(", "dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate.test_pytorch_optim": [[31, 34], ["test_registry.TestLocate._test_obj"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate._test_obj"], ["", "def", "test_pytorch_optim", "(", "self", ")", ":", "\n", "# pydoc.locate does not work for it", "\n", "        ", "self", ".", "_test_obj", "(", "torch", ".", "optim", ".", "SGD", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate.test_failure": [[35, 38], ["test_registry.TestLocate.assertRaises", "detectron2.utils.registry.locate"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry.locate"], ["", "def", "test_failure", "(", "self", ")", ":", "\n", "        ", "with", "self", ".", "assertRaises", "(", "ImportError", ")", ":", "\n", "            ", "locate", "(", "\"asdf\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_registry.TestLocate.test_compress_target": [[39, 46], ["detectron2.utils.registry._convert_target_to_string", "test_registry.TestLocate.assertEqual", "test_registry.TestLocate.assertIs", "detectron2.utils.registry.locate"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry._convert_target_to_string", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.registry.locate"], ["", "", "def", "test_compress_target", "(", "self", ")", ":", "\n", "        ", "from", "detectron2", ".", "data", ".", "transforms", "import", "RandomCrop", "\n", "\n", "name", "=", "_convert_target_to_string", "(", "RandomCrop", ")", "\n", "# name shouldn't contain 'augmentation_impl'", "\n", "self", ".", "assertEqual", "(", "name", ",", "\"detectron2.data.transforms.RandomCrop\"", ")", "\n", "self", ".", "assertIs", "(", "RandomCrop", ",", "locate", "(", "name", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_analysis.RetinaNetTest.setUp": [[12, 14], ["detectron2.utils.testing.get_model_no_weights"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.get_model_no_weights"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", "=", "get_model_no_weights", "(", "\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_analysis.RetinaNetTest.test_flop": [[15, 20], ["detectron2.utils.analysis.flop_count_operators", "test_model_analysis.RetinaNetTest.assertTrue", "int", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.analysis.flop_count_operators"], ["", "def", "test_flop", "(", "self", ")", ":", "\n", "# RetinaNet supports flop-counting with random inputs", "\n", "        ", "inputs", "=", "[", "{", "\"image\"", ":", "torch", ".", "rand", "(", "3", ",", "800", ",", "800", ")", ",", "\"test_unused\"", ":", "\"abcd\"", "}", "]", "\n", "res", "=", "flop_count_operators", "(", "self", ".", "model", ",", "inputs", ")", "\n", "self", ".", "assertTrue", "(", "int", "(", "res", "[", "\"conv\"", "]", ")", ",", "146", ")", "# 146B flops", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_analysis.RetinaNetTest.test_param_count": [[21, 25], ["detectron2.utils.analysis.parameter_count", "test_model_analysis.RetinaNetTest.assertTrue", "test_model_analysis.RetinaNetTest.assertTrue"], "methods", ["None"], ["", "def", "test_param_count", "(", "self", ")", ":", "\n", "        ", "res", "=", "parameter_count", "(", "self", ".", "model", ")", "\n", "self", ".", "assertTrue", "(", "res", "[", "\"\"", "]", ",", "37915572", ")", "\n", "self", ".", "assertTrue", "(", "res", "[", "\"backbone\"", "]", ",", "31452352", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_analysis.FasterRCNNTest.setUp": [[28, 30], ["detectron2.utils.testing.get_model_no_weights"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.get_model_no_weights"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", "=", "get_model_no_weights", "(", "\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_analysis.FasterRCNNTest.test_flop": [[31, 40], ["detectron2.utils.analysis.flop_count_operators", "test_model_analysis.FasterRCNNTest.assertTrue", "int", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.analysis.flop_count_operators"], ["", "def", "test_flop", "(", "self", ")", ":", "\n", "# Faster R-CNN supports flop-counting with random inputs", "\n", "        ", "inputs", "=", "[", "{", "\"image\"", ":", "torch", ".", "rand", "(", "3", ",", "800", ",", "800", ")", "}", "]", "\n", "res", "=", "flop_count_operators", "(", "self", ".", "model", ",", "inputs", ")", "\n", "\n", "# This only checks flops for backbone & proposal generator", "\n", "# Flops for box head is not conv, and depends on #proposals, which is", "\n", "# almost 0 for random inputs.", "\n", "self", ".", "assertTrue", "(", "int", "(", "res", "[", "\"conv\"", "]", ")", ",", "117", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_model_analysis.FasterRCNNTest.test_param_count": [[41, 45], ["detectron2.utils.analysis.parameter_count", "test_model_analysis.FasterRCNNTest.assertTrue", "test_model_analysis.FasterRCNNTest.assertTrue"], "methods", ["None"], ["", "def", "test_param_count", "(", "self", ")", ":", "\n", "        ", "res", "=", "parameter_count", "(", "self", ".", "model", ")", "\n", "self", ".", "assertTrue", "(", "res", "[", "\"\"", "]", ",", "41699936", ")", "\n", "self", ".", "assertTrue", "(", "res", "[", "\"backbone\"", "]", ",", "26799296", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestScripting.testMaskRCNN": [[38, 42], ["unittest.skipIf", "test_export_torchscript.TestScripting._test_rcnn_model", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestScripting._test_rcnn_model"], ["    ", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "testMaskRCNN", "(", "self", ")", ":", "\n", "# TODO: this test requires manifold access, see: T88318502", "\n", "        ", "self", ".", "_test_rcnn_model", "(", "\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestScripting.testRetinaNet": [[43, 47], ["unittest.skipIf", "test_export_torchscript.TestScripting._test_retinanet_model", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestScripting._test_retinanet_model"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "testRetinaNet", "(", "self", ")", ":", "\n", "# TODO: this test requires manifold access, see: T88318502", "\n", "        ", "self", ".", "_test_retinanet_model", "(", "\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestScripting._test_rcnn_model": [[48, 67], ["detectron2.model_zoo.get", "detectron2.model_zoo.get.eval", "detectron2.export.scripting_with_instances", "detectron2.utils.testing.assert_instances_allclose", "torch.no_grad", "detectron2.model_zoo.get.inference", "detectron2.export.scripting_with_instances.inference", "detectron2.utils.testing.get_sample_coco_image"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.scripting_with_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.assert_instances_allclose", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.get_sample_coco_image"], ["", "def", "_test_rcnn_model", "(", "self", ",", "config_path", ")", ":", "\n", "        ", "model", "=", "model_zoo", ".", "get", "(", "config_path", ",", "trained", "=", "True", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "fields", "=", "{", "\n", "\"proposal_boxes\"", ":", "Boxes", ",", "\n", "\"objectness_logits\"", ":", "Tensor", ",", "\n", "\"pred_boxes\"", ":", "Boxes", ",", "\n", "\"scores\"", ":", "Tensor", ",", "\n", "\"pred_classes\"", ":", "Tensor", ",", "\n", "\"pred_masks\"", ":", "Tensor", ",", "\n", "}", "\n", "script_model", "=", "scripting_with_instances", "(", "model", ",", "fields", ")", "\n", "\n", "inputs", "=", "[", "{", "\"image\"", ":", "get_sample_coco_image", "(", ")", "}", "]", "*", "2", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "instance", "=", "model", ".", "inference", "(", "inputs", ",", "do_postprocess", "=", "False", ")", "[", "0", "]", "\n", "scripted_instance", "=", "script_model", ".", "inference", "(", "inputs", ",", "do_postprocess", "=", "False", ")", "[", "0", "]", "\n", "", "assert_instances_allclose", "(", "instance", ",", "scripted_instance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestScripting._test_retinanet_model": [[68, 86], ["detectron2.model_zoo.get", "detectron2.model_zoo.get.eval", "detectron2.export.scripting_with_instances", "detectron2.utils.testing.get_sample_coco_image", "detectron2.utils.testing.assert_instances_allclose", "torch.no_grad", "detectron2.utils.testing.convert_scripted_instances", "detectron2.modeling.postprocessing.detector_postprocess", "detectron2.model_zoo.get.", "detectron2.export.scripting_with_instances."], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.scripting_with_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.get_sample_coco_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.assert_instances_allclose", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.convert_scripted_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.modeling.postprocessing.detector_postprocess"], ["", "def", "_test_retinanet_model", "(", "self", ",", "config_path", ")", ":", "\n", "        ", "model", "=", "model_zoo", ".", "get", "(", "config_path", ",", "trained", "=", "True", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "fields", "=", "{", "\n", "\"pred_boxes\"", ":", "Boxes", ",", "\n", "\"scores\"", ":", "Tensor", ",", "\n", "\"pred_classes\"", ":", "Tensor", ",", "\n", "}", "\n", "script_model", "=", "scripting_with_instances", "(", "model", ",", "fields", ")", "\n", "\n", "img", "=", "get_sample_coco_image", "(", ")", "\n", "inputs", "=", "[", "{", "\"image\"", ":", "img", "}", "]", "*", "2", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "instance", "=", "model", "(", "inputs", ")", "[", "0", "]", "[", "\"instances\"", "]", "\n", "scripted_instance", "=", "convert_scripted_instances", "(", "script_model", "(", "inputs", ")", "[", "0", "]", ")", "\n", "scripted_instance", "=", "detector_postprocess", "(", "scripted_instance", ",", "img", ".", "shape", "[", "1", "]", ",", "img", ".", "shape", "[", "2", "]", ")", "\n", "", "assert_instances_allclose", "(", "instance", ",", "scripted_instance", ")", "\n", "# Note that the model currently cannot be saved and loaded into a new process:", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTracing.testMaskRCNN": [[92, 100], ["unittest.skipIf", "test_export_torchscript.TestTracing._test_model", "torch.cuda.is_available", "model.inference"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export._test_model", "home.repos.pwc.inspect_result.microsoft_regionclip.roi_heads.rotated_fast_rcnn.RotatedFastRCNNOutputLayers.inference"], ["    ", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "testMaskRCNN", "(", "self", ")", ":", "\n", "# TODO: this test requires manifold access, see: T88318502", "\n", "        ", "def", "inference_func", "(", "model", ",", "image", ")", ":", "\n", "            ", "inputs", "=", "[", "{", "\"image\"", ":", "image", "}", "]", "\n", "return", "model", ".", "inference", "(", "inputs", ",", "do_postprocess", "=", "False", ")", "[", "0", "]", "\n", "\n", "", "self", ".", "_test_model", "(", "\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"", ",", "inference_func", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTracing.testRetinaNet": [[101, 108], ["unittest.skipIf", "test_export_torchscript.TestTracing._test_model", "torch.cuda.is_available", "model.forward"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export._test_model", "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "testRetinaNet", "(", "self", ")", ":", "\n", "# TODO: this test requires manifold access, see: T88318502", "\n", "        ", "def", "inference_func", "(", "model", ",", "image", ")", ":", "\n", "            ", "return", "model", ".", "forward", "(", "[", "{", "\"image\"", ":", "image", "}", "]", ")", "[", "0", "]", "[", "\"instances\"", "]", "\n", "\n", "", "self", ".", "_test_model", "(", "\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"", ",", "inference_func", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTracing._test_model": [[109, 123], ["detectron2.model_zoo.get", "detectron2.utils.testing.get_sample_coco_image", "detectron2.export.flatten.TracingAdapter", "detectron2.export.flatten.TracingAdapter.eval", "detectron2.utils.testing.assert_instances_allclose", "torch.no_grad", "torch.nn.functional.interpolate", "torch.jit.trace", "test_export_torchscript.TestTracing.testMaskRCNN.inference_func", "detectron2.export.flatten.TracingAdapter.outputs_schema", "torch.jit.trace."], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.get_sample_coco_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.assert_instances_allclose"], ["", "def", "_test_model", "(", "self", ",", "config_path", ",", "inference_func", ")", ":", "\n", "        ", "model", "=", "model_zoo", ".", "get", "(", "config_path", ",", "trained", "=", "True", ")", "\n", "image", "=", "get_sample_coco_image", "(", ")", "\n", "\n", "wrapper", "=", "TracingAdapter", "(", "model", ",", "image", ",", "inference_func", ")", "\n", "wrapper", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "small_image", "=", "nn", ".", "functional", ".", "interpolate", "(", "image", ",", "scale_factor", "=", "0.5", ")", "\n", "# trace with a different image, and the trace must still work", "\n", "traced_model", "=", "torch", ".", "jit", ".", "trace", "(", "wrapper", ",", "(", "small_image", ",", ")", ")", "\n", "\n", "output", "=", "inference_func", "(", "model", ",", "image", ")", "\n", "traced_output", "=", "wrapper", ".", "outputs_schema", "(", "traced_model", "(", "image", ")", ")", "\n", "", "assert_instances_allclose", "(", "output", ",", "traced_output", ",", "size_as_tensor", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTracing.testKeypointHead": [[124, 157], ["M", "M.eval", "torch.randn", "detectron2.utils.testing.random_boxes", "detectron2.utils.testing.random_boxes", "torch.no_grad", "detectron2.export.torchscript_patch.patch_builtin_len", "torch.jit.trace", "test_export_torchscript.TestTracing.testKeypointHead.gen_input"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.random_boxes", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript_patch.patch_builtin_len"], ["", "def", "testKeypointHead", "(", "self", ")", ":", "\n", "        ", "class", "M", "(", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "__init__", "(", "self", ")", ":", "\n", "                ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "KRCNNConvDeconvUpsampleHead", "(", "\n", "ShapeSpec", "(", "channels", "=", "4", ",", "height", "=", "14", ",", "width", "=", "14", ")", ",", "num_keypoints", "=", "17", ",", "conv_dims", "=", "(", "4", ",", ")", "\n", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "predbox1", ",", "predbox2", ")", ":", "\n", "                ", "inst", "=", "[", "\n", "Instances", "(", "(", "100", ",", "100", ")", ",", "pred_boxes", "=", "Boxes", "(", "predbox1", ")", ")", ",", "\n", "Instances", "(", "(", "100", ",", "100", ")", ",", "pred_boxes", "=", "Boxes", "(", "predbox2", ")", ")", ",", "\n", "]", "\n", "ret", "=", "self", ".", "model", "(", "x", ",", "inst", ")", "\n", "return", "tuple", "(", "x", ".", "pred_keypoints", "for", "x", "in", "ret", ")", "\n", "\n", "", "", "model", "=", "M", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "def", "gen_input", "(", "num1", ",", "num2", ")", ":", "\n", "            ", "feat", "=", "torch", ".", "randn", "(", "(", "num1", "+", "num2", ",", "4", ",", "14", ",", "14", ")", ")", "\n", "box1", "=", "random_boxes", "(", "num1", ")", "\n", "box2", "=", "random_boxes", "(", "num2", ")", "\n", "return", "feat", ",", "box1", ",", "box2", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ",", "patch_builtin_len", "(", ")", ":", "\n", "            ", "trace", "=", "torch", ".", "jit", ".", "trace", "(", "model", ",", "gen_input", "(", "15", ",", "15", ")", ",", "check_trace", "=", "False", ")", "\n", "\n", "inputs", "=", "gen_input", "(", "12", ",", "10", ")", "\n", "trace_outputs", "=", "trace", "(", "*", "inputs", ")", "\n", "true_outputs", "=", "model", "(", "*", "inputs", ")", "\n", "for", "trace_output", ",", "true_output", "in", "zip", "(", "trace_outputs", ",", "true_outputs", ")", ":", "\n", "                ", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "trace_output", ",", "true_output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTorchscriptUtils.test_dump_IR_tracing": [[161, 183], ["detectron2.config.get_cfg", "Mod", "detectron2.modeling.build_backbone", "Mod.eval", "torch.no_grad", "torch.jit.trace", "tempfile.TemporaryDirectory", "detectron2.export.dump_torchscript_IR", "tuple", "os.path.join", "test_export_torchscript.TestTorchscriptUtils.assertTrue", "test_export_torchscript.TestTorchscriptUtils.m().values", "torch.rand", "test_export_torchscript.TestTorchscriptUtils.m", "os.stat"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.backbone.build.build_backbone", "home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.dump_torchscript_IR"], ["    ", "def", "test_dump_IR_tracing", "(", "self", ")", ":", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "MODEL", ".", "RESNETS", ".", "DEPTH", "=", "18", "\n", "cfg", ".", "MODEL", ".", "RESNETS", ".", "RES2_OUT_CHANNELS", "=", "64", "\n", "\n", "class", "Mod", "(", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "                ", "return", "tuple", "(", "self", ".", "m", "(", "x", ")", ".", "values", "(", ")", ")", "\n", "\n", "", "", "model", "=", "Mod", "(", ")", "\n", "model", ".", "m", "=", "build_backbone", "(", "cfg", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "ts_model", "=", "torch", ".", "jit", ".", "trace", "(", "model", ",", "(", "torch", ".", "rand", "(", "2", ",", "3", ",", "224", ",", "224", ")", ",", ")", ")", "\n", "\n", "", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2_test\"", ")", "as", "d", ":", "\n", "            ", "dump_torchscript_IR", "(", "ts_model", ",", "d", ")", "\n", "# check that the files are created", "\n", "for", "name", "in", "[", "\"model_ts_code\"", ",", "\"model_ts_IR\"", ",", "\"model_ts_IR_inlined\"", ",", "\"model\"", "]", ":", "\n", "                ", "fname", "=", "os", ".", "path", ".", "join", "(", "d", ",", "name", "+", "\".txt\"", ")", "\n", "self", ".", "assertTrue", "(", "os", ".", "stat", "(", "fname", ")", ".", "st_size", ">", "0", ",", "fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTorchscriptUtils.test_dump_IR_function": [[184, 198], ["torch.jit.trace", "tempfile.TemporaryDirectory", "detectron2.export.dump_torchscript_IR", "test_export_torchscript.TestTorchscriptUtils.test_dump_IR_function.gunc"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.torchscript.dump_torchscript_IR"], ["", "", "", "def", "test_dump_IR_function", "(", "self", ")", ":", "\n", "        ", "@", "torch", ".", "jit", ".", "script", "\n", "def", "gunc", "(", "x", ",", "y", ")", ":", "\n", "            ", "return", "x", "+", "y", "\n", "\n", "", "def", "func", "(", "x", ",", "y", ")", ":", "\n", "            ", "return", "x", "+", "y", "+", "gunc", "(", "x", ",", "y", ")", "\n", "\n", "", "ts_model", "=", "torch", ".", "jit", ".", "trace", "(", "func", ",", "(", "torch", ".", "rand", "(", "3", ")", ",", "torch", ".", "rand", "(", "3", ")", ")", ")", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2_test\"", ")", "as", "d", ":", "\n", "            ", "dump_torchscript_IR", "(", "ts_model", ",", "d", ")", "\n", "for", "name", "in", "[", "\"model_ts_code\"", ",", "\"model_ts_IR\"", ",", "\"model_ts_IR_inlined\"", "]", ":", "\n", "                ", "fname", "=", "os", ".", "path", ".", "join", "(", "d", ",", "name", "+", "\".txt\"", ")", "\n", "self", ".", "assertTrue", "(", "os", ".", "stat", "(", "fname", ")", ".", "st_size", ">", "0", ",", "fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTorchscriptUtils.test_flatten_basic": [[199, 209], ["detectron2.export.flatten.flatten_to_tuple", "test_export_torchscript.TestTorchscriptUtils.assertEqual", "schema", "test_export_torchscript.TestTorchscriptUtils.assertEqual", "detectron2.export.flatten.flatten_to_tuple", "test_export_torchscript.TestTorchscriptUtils.assertEqual", "test_export_torchscript.TestTorchscriptUtils._check_schema"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.flatten_to_tuple", "home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.flatten_to_tuple", "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTorchscriptUtils._check_schema"], ["", "", "", "def", "test_flatten_basic", "(", "self", ")", ":", "\n", "        ", "obj", "=", "[", "3", ",", "(", "[", "5", ",", "6", "]", ",", "{", "\"name\"", ":", "[", "7", ",", "9", "]", ",", "\"name2\"", ":", "3", "}", ")", "]", "\n", "res", ",", "schema", "=", "flatten_to_tuple", "(", "obj", ")", "\n", "self", ".", "assertEqual", "(", "res", ",", "(", "3", ",", "5", ",", "6", ",", "7", ",", "9", ",", "3", ")", ")", "\n", "new_obj", "=", "schema", "(", "res", ")", "\n", "self", ".", "assertEqual", "(", "new_obj", ",", "obj", ")", "\n", "\n", "_", ",", "new_schema", "=", "flatten_to_tuple", "(", "new_obj", ")", "\n", "self", ".", "assertEqual", "(", "schema", ",", "new_schema", ")", "# test __eq__", "\n", "self", ".", "_check_schema", "(", "schema", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTorchscriptUtils._check_schema": [[210, 219], ["detectron2.config.instantiate.dump_dataclass", "json.dumps", "detectron2.config.instantiate.instantiate", "test_export_torchscript.TestTorchscriptUtils.assertEqual"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.dump_dataclass", "home.repos.pwc.inspect_result.microsoft_regionclip.config.instantiate.instantiate"], ["", "def", "_check_schema", "(", "self", ",", "schema", ")", ":", "\n", "        ", "dumped_schema", "=", "dump_dataclass", "(", "schema", ")", "\n", "# Check that the schema is json-serializable", "\n", "# Although in reality you might want to use yaml because it often has many levels", "\n", "json", ".", "dumps", "(", "dumped_schema", ")", "\n", "\n", "# Check that the schema can be deserialized", "\n", "new_schema", "=", "instantiate", "(", "dumped_schema", ")", "\n", "self", ".", "assertEqual", "(", "schema", ",", "new_schema", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTorchscriptUtils.test_flatten_instances_boxes": [[220, 233], ["detectron2.structures.Instances", "detectron2.export.flatten.flatten_to_tuple", "test_export_torchscript.TestTorchscriptUtils.assertEqual", "zip", "schema", "detectron2.utils.testing.assert_instances_allclose", "test_export_torchscript.TestTorchscriptUtils._check_schema", "torch.tensor", "test_export_torchscript.TestTorchscriptUtils.assertIs", "torch.tensor", "detectron2.structures.Boxes", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.flatten_to_tuple", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.assert_instances_allclose", "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_torchscript.TestTorchscriptUtils._check_schema"], ["", "def", "test_flatten_instances_boxes", "(", "self", ")", ":", "\n", "        ", "inst", "=", "Instances", "(", "\n", "torch", ".", "tensor", "(", "[", "5", ",", "8", "]", ")", ",", "pred_masks", "=", "torch", ".", "tensor", "(", "[", "3", "]", ")", ",", "pred_boxes", "=", "Boxes", "(", "torch", ".", "ones", "(", "(", "1", ",", "4", ")", ")", ")", "\n", ")", "\n", "obj", "=", "[", "3", ",", "(", "[", "5", ",", "6", "]", ",", "inst", ")", "]", "\n", "res", ",", "schema", "=", "flatten_to_tuple", "(", "obj", ")", "\n", "self", ".", "assertEqual", "(", "res", "[", ":", "3", "]", ",", "(", "3", ",", "5", ",", "6", ")", ")", "\n", "for", "r", ",", "expected", "in", "zip", "(", "res", "[", "3", ":", "]", ",", "(", "inst", ".", "pred_boxes", ".", "tensor", ",", "inst", ".", "pred_masks", ",", "inst", ".", "image_size", ")", ")", ":", "\n", "            ", "self", ".", "assertIs", "(", "r", ",", "expected", ")", "\n", "", "new_obj", "=", "schema", "(", "res", ")", "\n", "assert_instances_allclose", "(", "new_obj", "[", "1", "]", "[", "1", "]", ",", "inst", ",", "rtol", "=", "0.0", ",", "size_as_tensor", "=", "True", ")", "\n", "\n", "self", ".", "_check_schema", "(", "schema", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export.setUp": [[17, 19], ["detectron2.utils.logger.setup_logger"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "setup_logger", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export._test_model": [[20, 43], ["detectron2.model_zoo.get_config", "add_export_config", "detectron2.model_zoo.get", "Caffe2Tracer", "Caffe2Tracer.export_caffe2", "copy.deepcopy", "tempfile.TemporaryDirectory", "Caffe2Model.load_protobuf.save_protobuf", "Caffe2Model.load_protobuf.save_graph", "Caffe2Model.load_protobuf", "Caffe2Tracer.export_torchscript", "Caffe2Tracer.export_torchscript.save", "detectron2.utils.testing.get_sample_coco_image", "os.path.join", "os.path.join", "copy.deepcopy", "Caffe2Model.load_protobuf."], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.model_zoo.model_zoo.get_config", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.add_export_config", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_caffe2", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.save_protobuf", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.save_graph", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Model.load_protobuf", "home.repos.pwc.inspect_result.microsoft_regionclip.export.api.Caffe2Tracer.export_torchscript", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.testing.get_sample_coco_image"], ["", "def", "_test_model", "(", "self", ",", "config_path", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "# requires extra dependencies", "\n", "        ", "from", "detectron2", ".", "export", "import", "Caffe2Model", ",", "add_export_config", ",", "Caffe2Tracer", "\n", "\n", "cfg", "=", "model_zoo", ".", "get_config", "(", "config_path", ")", "\n", "add_export_config", "(", "cfg", ")", "\n", "cfg", ".", "MODEL", ".", "DEVICE", "=", "device", "\n", "model", "=", "model_zoo", ".", "get", "(", "config_path", ",", "trained", "=", "True", ",", "device", "=", "device", ")", "\n", "\n", "inputs", "=", "[", "{", "\"image\"", ":", "get_sample_coco_image", "(", ")", "}", "]", "\n", "tracer", "=", "Caffe2Tracer", "(", "cfg", ",", "model", ",", "copy", ".", "deepcopy", "(", "inputs", ")", ")", "\n", "\n", "c2_model", "=", "tracer", ".", "export_caffe2", "(", ")", "\n", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2_unittest\"", ")", "as", "d", ":", "\n", "            ", "c2_model", ".", "save_protobuf", "(", "d", ")", "\n", "c2_model", ".", "save_graph", "(", "os", ".", "path", ".", "join", "(", "d", ",", "\"test.svg\"", ")", ",", "inputs", "=", "copy", ".", "deepcopy", "(", "inputs", ")", ")", "\n", "\n", "c2_model", "=", "Caffe2Model", ".", "load_protobuf", "(", "d", ")", "\n", "c2_model", "(", "inputs", ")", "[", "0", "]", "[", "\"instances\"", "]", "\n", "\n", "ts_model", "=", "tracer", ".", "export_torchscript", "(", ")", "\n", "ts_model", ".", "save", "(", "os", ".", "path", ".", "join", "(", "d", ",", "\"model.ts\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export.testMaskRCNN": [[44, 47], ["test_export_caffe2.TestCaffe2Export._test_model"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export._test_model"], ["", "", "def", "testMaskRCNN", "(", "self", ")", ":", "\n", "# TODO: this test requires manifold access, see: T88318502", "\n", "        ", "self", ".", "_test_model", "(", "\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export.testMaskRCNNGPU": [[48, 52], ["unittest.skipIf", "test_export_caffe2.TestCaffe2Export._test_model", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export._test_model"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "testMaskRCNNGPU", "(", "self", ")", ":", "\n", "# TODO: this test requires manifold access, see: T88318502", "\n", "        ", "self", ".", "_test_model", "(", "\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"", ",", "device", "=", "\"cuda\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export.testRetinaNet": [[53, 56], ["test_export_caffe2.TestCaffe2Export._test_model"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export._test_model"], ["", "def", "testRetinaNet", "(", "self", ")", ":", "\n", "# TODO: this test requires manifold access, see: T88318502", "\n", "        ", "self", ".", "_test_model", "(", "\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export.testPanopticFPN": [[57, 60], ["test_export_caffe2.TestCaffe2Export._test_model"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_export_caffe2.TestCaffe2Export._test_model"], ["", "def", "testPanopticFPN", "(", "self", ")", ":", "\n", "# TODO: this test requires manifold access, see: T88318502", "\n", "        ", "self", ".", "_test_model", "(", "\"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_checkpoint.TestCheckpointer.setUp": [[12, 14], ["detectron2.utils.logger.setup_logger"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.logger.setup_logger"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "setup_logger", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_checkpoint.TestCheckpointer.create_complex_model": [[15, 31], ["torch.nn.Module", "torch.nn.Module", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Module", "torch.nn.Linear", "collections.OrderedDict", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["None"], ["", "def", "create_complex_model", "(", "self", ")", ":", "\n", "        ", "m", "=", "nn", ".", "Module", "(", ")", "\n", "m", ".", "block1", "=", "nn", ".", "Module", "(", ")", "\n", "m", ".", "block1", ".", "layer1", "=", "nn", ".", "Linear", "(", "2", ",", "3", ")", "\n", "m", ".", "layer2", "=", "nn", ".", "Linear", "(", "3", ",", "2", ")", "\n", "m", ".", "res", "=", "nn", ".", "Module", "(", ")", "\n", "m", ".", "res", ".", "layer2", "=", "nn", ".", "Linear", "(", "3", ",", "2", ")", "\n", "\n", "state_dict", "=", "OrderedDict", "(", ")", "\n", "state_dict", "[", "\"layer1.weight\"", "]", "=", "torch", ".", "rand", "(", "3", ",", "2", ")", "\n", "state_dict", "[", "\"layer1.bias\"", "]", "=", "torch", ".", "rand", "(", "3", ")", "\n", "state_dict", "[", "\"layer2.weight\"", "]", "=", "torch", ".", "rand", "(", "2", ",", "3", ")", "\n", "state_dict", "[", "\"layer2.bias\"", "]", "=", "torch", ".", "rand", "(", "2", ")", "\n", "state_dict", "[", "\"res.layer2.weight\"", "]", "=", "torch", ".", "rand", "(", "2", ",", "3", ")", "\n", "state_dict", "[", "\"res.layer2.bias\"", "]", "=", "torch", ".", "rand", "(", "2", ")", "\n", "return", "m", ",", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_checkpoint.TestCheckpointer.test_complex_model_loaded": [[32, 46], ["test_checkpoint.TestCheckpointer.create_complex_model", "torch.nn.DataParallel.state_dict", "detectron2.checkpoint.c2_model_loading.align_and_update_state_dicts", "torch.nn.DataParallel.load_state_dict", "zip", "torch.nn.DataParallel", "nn.DataParallel.state_dict.values", "state_dict.values", "test_checkpoint.TestCheckpointer.assertFalse", "test_checkpoint.TestCheckpointer.assertTrue", "loaded.to().equal", "id", "id", "loaded.to"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_checkpoint.TestCheckpointer.create_complex_model", "home.repos.pwc.inspect_result.microsoft_regionclip.solver.lr_scheduler.LRMultiplier.state_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.checkpoint.c2_model_loading.align_and_update_state_dicts", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.AMPTrainer.load_state_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["", "def", "test_complex_model_loaded", "(", "self", ")", ":", "\n", "        ", "for", "add_data_parallel", "in", "[", "False", ",", "True", "]", ":", "\n", "            ", "model", ",", "state_dict", "=", "self", ".", "create_complex_model", "(", ")", "\n", "if", "add_data_parallel", ":", "\n", "                ", "model", "=", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "model_sd", "=", "model", ".", "state_dict", "(", ")", "\n", "\n", "sd_to_load", "=", "align_and_update_state_dicts", "(", "model_sd", ",", "state_dict", ")", "\n", "model", ".", "load_state_dict", "(", "sd_to_load", ")", "\n", "for", "loaded", ",", "stored", "in", "zip", "(", "model_sd", ".", "values", "(", ")", ",", "state_dict", ".", "values", "(", ")", ")", ":", "\n", "# different tensor references", "\n", "                ", "self", ".", "assertFalse", "(", "id", "(", "loaded", ")", "==", "id", "(", "stored", ")", ")", "\n", "# same content", "\n", "self", ".", "assertTrue", "(", "loaded", ".", "to", "(", "stored", ")", ".", "equal", "(", "stored", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.__init__": [[21, 26], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "@", "configurable", "\n", "def", "__init__", "(", "self", ",", "sleep_sec", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mod", "=", "nn", ".", "Linear", "(", "10", ",", "20", ")", "\n", "self", ".", "sleep_sec", "=", "sleep_sec", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.from_config": [[27, 30], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_config", "(", "cls", ",", "cfg", ")", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine._SimpleModel.forward": [[31, 35], ["time.sleep", "x.sum", "sum", "x.mean", "test_engine._SimpleModel.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "sleep_sec", ">", "0", ":", "\n", "            ", "time", ".", "sleep", "(", "self", ".", "sleep_sec", ")", "\n", "", "return", "{", "\"loss\"", ":", "x", ".", "sum", "(", ")", "+", "sum", "(", "[", "x", ".", "mean", "(", ")", "for", "x", "in", "self", ".", "parameters", "(", ")", "]", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer._data_loader": [[38, 42], ["torch.device", "torch.rand().to", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to"], ["    ", "def", "_data_loader", "(", "self", ",", "device", ")", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "device", ")", "\n", "while", "True", ":", "\n", "            ", "yield", "torch", ".", "rand", "(", "3", ",", "3", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer.test_simple_trainer": [[43, 49], ["_SimpleModel().to", "detectron2.engine.SimpleTrainer", "detectron2.engine.SimpleTrainer.train", "test_engine.TestTrainer._data_loader", "torch.optim.SGD", "test_engine._SimpleModel", "_SimpleModel().to.parameters"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer._data_loader"], ["", "", "def", "test_simple_trainer", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "model", "=", "_SimpleModel", "(", ")", ".", "to", "(", "device", "=", "device", ")", "\n", "trainer", "=", "SimpleTrainer", "(", "\n", "model", ",", "self", ".", "_data_loader", "(", "device", ")", ",", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "0.1", ")", "\n", ")", "\n", "trainer", ".", "train", "(", "0", ",", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer.test_simple_trainer_cuda": [[50, 53], ["unittest.skipIf", "test_engine.TestTrainer.test_simple_trainer", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer.test_simple_trainer"], ["", "@", "unittest", ".", "skipIf", "(", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "\"CUDA not available\"", ")", "\n", "def", "test_simple_trainer_cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "test_simple_trainer", "(", "device", "=", "\"cuda\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer.test_writer_hooks": [[54, 84], ["test_engine._SimpleModel", "detectron2.engine.SimpleTrainer", "test_engine.TestTrainer._data_loader", "torch.optim.SGD", "tempfile.TemporaryDirectory", "os.path.join", "detectron2.engine.SimpleTrainer.register_hooks", "test_engine.TestTrainer.assertEqual", "zip", "test_engine.TestTrainer.assertIn", "_SimpleModel.parameters", "detectron2.utils.events.CommonMetricPrinter", "detectron2.utils.events.JSONWriter", "test_engine.TestTrainer.assertLogs", "detectron2.engine.SimpleTrainer.train", "open", "test_engine.TestTrainer.assertEqual", "test_engine.TestTrainer.assertIn", "len", "test_engine.TestTrainer.assertIn", "detectron2.engine.hooks.EvalHook", "detectron2.engine.hooks.PeriodicWriter", "json.loads", "line.strip"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer._data_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.register_hooks", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train"], ["", "def", "test_writer_hooks", "(", "self", ")", ":", "\n", "        ", "model", "=", "_SimpleModel", "(", "sleep_sec", "=", "0.1", ")", "\n", "trainer", "=", "SimpleTrainer", "(", "\n", "model", ",", "self", ".", "_data_loader", "(", "\"cpu\"", ")", ",", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "0.1", ")", "\n", ")", "\n", "\n", "max_iter", "=", "50", "\n", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2_test\"", ")", "as", "d", ":", "\n", "            ", "json_file", "=", "os", ".", "path", ".", "join", "(", "d", ",", "\"metrics.json\"", ")", "\n", "writers", "=", "[", "CommonMetricPrinter", "(", "max_iter", ")", ",", "JSONWriter", "(", "json_file", ")", "]", "\n", "\n", "trainer", ".", "register_hooks", "(", "\n", "[", "hooks", ".", "EvalHook", "(", "0", ",", "lambda", ":", "{", "\"metric\"", ":", "100", "}", ")", ",", "hooks", ".", "PeriodicWriter", "(", "writers", ")", "]", "\n", ")", "\n", "with", "self", ".", "assertLogs", "(", "writers", "[", "0", "]", ".", "logger", ")", "as", "logs", ":", "\n", "                ", "trainer", ".", "train", "(", "0", ",", "max_iter", ")", "\n", "\n", "", "with", "open", "(", "json_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "                ", "data", "=", "[", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "for", "line", "in", "f", "]", "\n", "self", ".", "assertEqual", "(", "[", "x", "[", "\"iteration\"", "]", "for", "x", "in", "data", "]", ",", "[", "19", ",", "39", ",", "49", ",", "50", "]", ")", "\n", "# the eval metric is in the last line with iter 50", "\n", "self", ".", "assertIn", "(", "\"metric\"", ",", "data", "[", "-", "1", "]", ",", "\"Eval metric must be in last line of JSON!\"", ")", "\n", "\n", "# test logged messages from CommonMetricPrinter", "\n", "", "self", ".", "assertEqual", "(", "len", "(", "logs", ".", "output", ")", ",", "3", ")", "\n", "for", "log", ",", "iter", "in", "zip", "(", "logs", ".", "output", ",", "[", "19", ",", "39", ",", "49", "]", ")", ":", "\n", "                ", "self", ".", "assertIn", "(", "f\"iter: {iter}\"", ",", "log", ")", "\n", "\n", "", "self", ".", "assertIn", "(", "\"eta: 0:00:00\"", ",", "logs", ".", "output", "[", "-", "1", "]", ",", "\"Last ETA must be 0!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer.test_default_trainer": [[85, 100], ["unittest.skipIf", "detectron2.config.get_cfg", "os.environ.get", "tempfile.TemporaryDirectory", "detectron2.engine.DefaultTrainer", "test_engine.TestTrainer.assertIs", "test_engine._SimpleModel", "test_engine.TestTrainer.assertIs"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.config.config.get_cfg", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "@", "unittest", ".", "skipIf", "(", "os", ".", "environ", ".", "get", "(", "\"CI\"", ")", ",", "\"Require COCO data.\"", ")", "\n", "def", "test_default_trainer", "(", "self", ")", ":", "\n", "# TODO: this test requires manifold access, so changed device to CPU. see: T88318502", "\n", "        ", "cfg", "=", "get_cfg", "(", ")", "\n", "cfg", ".", "MODEL", ".", "DEVICE", "=", "\"cpu\"", "\n", "cfg", ".", "MODEL", ".", "META_ARCHITECTURE", "=", "\"_SimpleModel\"", "\n", "cfg", ".", "DATASETS", ".", "TRAIN", "=", "(", "\"coco_2017_val_100\"", ",", ")", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2_test\"", ")", "as", "d", ":", "\n", "            ", "cfg", ".", "OUTPUT_DIR", "=", "d", "\n", "trainer", "=", "DefaultTrainer", "(", "cfg", ")", "\n", "\n", "# test property", "\n", "self", ".", "assertIs", "(", "trainer", ".", "model", ",", "trainer", ".", "_trainer", ".", "model", ")", "\n", "trainer", ".", "model", "=", "_SimpleModel", "(", ")", "\n", "self", ".", "assertIs", "(", "trainer", ".", "model", ",", "trainer", ".", "_trainer", ".", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer.test_checkpoint_resume": [[101, 138], ["test_engine._SimpleModel", "test_engine.TestTrainer._data_loader", "torch.optim.SGD", "torch.optim.lr_scheduler.StepLR", "_SimpleModel.parameters", "tempfile.TemporaryDirectory", "detectron2.engine.SimpleTrainer", "fvcore.common.checkpoint.Checkpointer", "detectron2.engine.SimpleTrainer.register_hooks", "detectron2.engine.SimpleTrainer.train", "test_engine.TestTrainer.assertAlmostEqual", "test_engine.TestTrainer.assertEqual", "torch.optim.SGD", "detectron2.engine.SimpleTrainer", "torch.optim.lr_scheduler.StepLR", "detectron2.engine.SimpleTrainer.register_hooks", "fvcore.common.checkpoint.Checkpointer", "fvcore.common.checkpoint.Checkpointer.resume_or_load", "test_engine.TestTrainer.assertEqual", "test_engine.TestTrainer.assertEqual", "test_engine.TestTrainer.assertAlmostEqual", "_SimpleModel.parameters", "detectron2.engine.hooks.LRScheduler", "detectron2.engine.hooks.PeriodicCheckpointer", "detectron2.engine.hooks.LRScheduler"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer._data_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.register_hooks", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.register_hooks", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.defaults.DefaultTrainer.resume_or_load"], ["", "", "def", "test_checkpoint_resume", "(", "self", ")", ":", "\n", "        ", "model", "=", "_SimpleModel", "(", ")", "\n", "dataloader", "=", "self", ".", "_data_loader", "(", "\"cpu\"", ")", "\n", "opt", "=", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "0.1", ")", "\n", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "opt", ",", "3", ")", "\n", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2_test\"", ")", "as", "d", ":", "\n", "            ", "trainer", "=", "SimpleTrainer", "(", "model", ",", "dataloader", ",", "opt", ")", "\n", "checkpointer", "=", "Checkpointer", "(", "model", ",", "d", ",", "opt", "=", "opt", ",", "trainer", "=", "trainer", ")", "\n", "\n", "trainer", ".", "register_hooks", "(", "\n", "[", "\n", "hooks", ".", "LRScheduler", "(", "scheduler", "=", "scheduler", ")", ",", "\n", "# checkpoint after scheduler to properly save the state of scheduler", "\n", "hooks", ".", "PeriodicCheckpointer", "(", "checkpointer", ",", "10", ")", ",", "\n", "]", "\n", ")", "\n", "\n", "trainer", ".", "train", "(", "0", ",", "12", ")", "\n", "self", ".", "assertAlmostEqual", "(", "opt", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", ",", "1e-5", ")", "\n", "self", ".", "assertEqual", "(", "scheduler", ".", "last_epoch", ",", "12", ")", "\n", "del", "trainer", "\n", "\n", "opt", "=", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "999", ")", "# lr will be loaded", "\n", "trainer", "=", "SimpleTrainer", "(", "model", ",", "dataloader", ",", "opt", ")", "\n", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "opt", ",", "3", ")", "\n", "trainer", ".", "register_hooks", "(", "\n", "[", "\n", "hooks", ".", "LRScheduler", "(", "scheduler", "=", "scheduler", ")", ",", "\n", "]", "\n", ")", "\n", "checkpointer", "=", "Checkpointer", "(", "model", ",", "d", ",", "opt", "=", "opt", ",", "trainer", "=", "trainer", ")", "\n", "checkpointer", ".", "resume_or_load", "(", "\"non_exist.pth\"", ")", "\n", "self", ".", "assertEqual", "(", "trainer", ".", "iter", ",", "11", ")", "# last finished iter number (0-based in Trainer)", "\n", "# number of times `scheduler.step()` was called (1-based)", "\n", "self", ".", "assertEqual", "(", "scheduler", ".", "last_epoch", ",", "12", ")", "\n", "self", ".", "assertAlmostEqual", "(", "opt", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", ",", "1e-5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer.test_eval_hook": [[139, 150], ["test_engine._SimpleModel", "test_engine.TestTrainer._data_loader", "torch.optim.SGD", "_SimpleModel.parameters", "unittest.mock.Mock", "detectron2.engine.SimpleTrainer", "detectron2.engine.SimpleTrainer.register_hooks", "detectron2.engine.SimpleTrainer.train", "test_engine.TestTrainer.assertEqual", "detectron2.engine.hooks.EvalHook"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_engine.TestTrainer._data_loader", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.register_hooks", "home.repos.pwc.inspect_result.microsoft_regionclip.engine.train_loop.TrainerBase.train"], ["", "", "def", "test_eval_hook", "(", "self", ")", ":", "\n", "        ", "model", "=", "_SimpleModel", "(", ")", "\n", "dataloader", "=", "self", ".", "_data_loader", "(", "\"cpu\"", ")", "\n", "opt", "=", "torch", ".", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "0.1", ")", "\n", "\n", "for", "total_iter", ",", "period", ",", "eval_count", "in", "[", "(", "30", ",", "15", ",", "2", ")", ",", "(", "31", ",", "15", ",", "3", ")", ",", "(", "20", ",", "0", ",", "1", ")", "]", ":", "\n", "            ", "test_func", "=", "mock", ".", "Mock", "(", "return_value", "=", "{", "\"metric\"", ":", "3.0", "}", ")", "\n", "trainer", "=", "SimpleTrainer", "(", "model", ",", "dataloader", ",", "opt", ")", "\n", "trainer", ".", "register_hooks", "(", "[", "hooks", ".", "EvalHook", "(", "period", ",", "test_func", ")", "]", ")", "\n", "trainer", ".", "train", "(", "0", ",", "total_iter", ")", "\n", "self", ".", "assertEqual", "(", "test_func", ".", "call_count", ",", "eval_count", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_events.TestEventWriter.testScalar": [[11, 26], ["tempfile.TemporaryDirectory", "detectron2.utils.events.EventStorage", "os.path.join", "detectron2.utils.events.JSONWriter", "range", "detectron2.utils.events.JSONWriter.close", "storage.put_scalar", "storage.step", "open", "test_events.TestEventWriter.assertTrue", "detectron2.utils.events.JSONWriter.write", "json.loads", "int"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["    ", "def", "testScalar", "(", "self", ")", ":", "\n", "        ", "with", "tempfile", ".", "TemporaryDirectory", "(", "\n", "prefix", "=", "\"detectron2_tests\"", "\n", ")", "as", "dir", ",", "EventStorage", "(", ")", "as", "storage", ":", "\n", "            ", "json_file", "=", "os", ".", "path", ".", "join", "(", "dir", ",", "\"test.json\"", ")", "\n", "writer", "=", "JSONWriter", "(", "json_file", ")", "\n", "for", "k", "in", "range", "(", "60", ")", ":", "\n", "                ", "storage", ".", "put_scalar", "(", "\"key\"", ",", "k", ",", "smoothing_hint", "=", "False", ")", "\n", "if", "(", "k", "+", "1", ")", "%", "20", "==", "0", ":", "\n", "                    ", "writer", ".", "write", "(", ")", "\n", "", "storage", ".", "step", "(", ")", "\n", "", "writer", ".", "close", "(", ")", "\n", "with", "open", "(", "json_file", ")", "as", "f", ":", "\n", "                ", "data", "=", "[", "json", ".", "loads", "(", "l", ")", "for", "l", "in", "f", "]", "\n", "self", ".", "assertTrue", "(", "[", "int", "(", "k", "[", "\"key\"", "]", ")", "for", "k", "in", "data", "]", "==", "[", "19", ",", "39", ",", "59", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_events.TestEventWriter.testScalarMismatchedPeriod": [[27, 47], ["tempfile.TemporaryDirectory", "detectron2.utils.events.EventStorage", "os.path.join", "detectron2.utils.events.JSONWriter", "range", "detectron2.utils.events.JSONWriter.close", "storage.put_scalar", "storage.step", "open", "test_events.TestEventWriter.assertTrue", "test_events.TestEventWriter.assertTrue", "test_events.TestEventWriter.assertTrue", "storage.put_scalar", "detectron2.utils.events.JSONWriter.write", "json.loads", "int", "int", "int", "k.get", "k.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.TensorboardXWriter.close", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "", "", "def", "testScalarMismatchedPeriod", "(", "self", ")", ":", "\n", "        ", "with", "tempfile", ".", "TemporaryDirectory", "(", "\n", "prefix", "=", "\"detectron2_tests\"", "\n", ")", "as", "dir", ",", "EventStorage", "(", ")", "as", "storage", ":", "\n", "            ", "json_file", "=", "os", ".", "path", ".", "join", "(", "dir", ",", "\"test.json\"", ")", "\n", "\n", "writer", "=", "JSONWriter", "(", "json_file", ")", "\n", "for", "k", "in", "range", "(", "60", ")", ":", "\n", "                ", "if", "k", "%", "17", "==", "0", ":", "# write in a differnt period", "\n", "                    ", "storage", ".", "put_scalar", "(", "\"key2\"", ",", "k", ",", "smoothing_hint", "=", "False", ")", "\n", "", "storage", ".", "put_scalar", "(", "\"key\"", ",", "k", ",", "smoothing_hint", "=", "False", ")", "\n", "if", "(", "k", "+", "1", ")", "%", "20", "==", "0", ":", "\n", "                    ", "writer", ".", "write", "(", ")", "\n", "", "storage", ".", "step", "(", ")", "\n", "", "writer", ".", "close", "(", ")", "\n", "with", "open", "(", "json_file", ")", "as", "f", ":", "\n", "                ", "data", "=", "[", "json", ".", "loads", "(", "l", ")", "for", "l", "in", "f", "]", "\n", "self", ".", "assertTrue", "(", "[", "int", "(", "k", ".", "get", "(", "\"key2\"", ",", "0", ")", ")", "for", "k", "in", "data", "]", "==", "[", "17", ",", "0", ",", "34", ",", "0", ",", "51", ",", "0", "]", ")", "\n", "self", ".", "assertTrue", "(", "[", "int", "(", "k", ".", "get", "(", "\"key\"", ",", "0", ")", ")", "for", "k", "in", "data", "]", "==", "[", "0", ",", "19", ",", "0", ",", "39", ",", "0", ",", "59", "]", ")", "\n", "self", ".", "assertTrue", "(", "[", "int", "(", "k", "[", "\"iteration\"", "]", ")", "for", "k", "in", "data", "]", "==", "[", "17", ",", "19", ",", "34", ",", "39", ",", "51", ",", "59", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_events.TestEventWriter.testPrintETA": [[48, 65], ["detectron2.utils.events.EventStorage", "detectron2.utils.events.CommonMetricPrinter", "detectron2.utils.events.CommonMetricPrinter", "s.put_scalar", "s.step", "s.put_scalar", "s.step", "test_events.TestEventWriter.assertIn", "test_events.TestEventWriter.assertNotIn", "test_events.TestEventWriter.assertLogs", "detectron2.utils.events.CommonMetricPrinter.write", "test_events.TestEventWriter.assertLogs", "detectron2.utils.events.CommonMetricPrinter.write"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.put_scalar", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.EventStorage.step", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.events.CommonMetricPrinter.write"], ["", "", "", "def", "testPrintETA", "(", "self", ")", ":", "\n", "        ", "with", "EventStorage", "(", ")", "as", "s", ":", "\n", "            ", "p1", "=", "CommonMetricPrinter", "(", "10", ")", "\n", "p2", "=", "CommonMetricPrinter", "(", ")", "\n", "\n", "s", ".", "put_scalar", "(", "\"time\"", ",", "1.0", ")", "\n", "s", ".", "step", "(", ")", "\n", "s", ".", "put_scalar", "(", "\"time\"", ",", "1.0", ")", "\n", "s", ".", "step", "(", ")", "\n", "\n", "with", "self", ".", "assertLogs", "(", "\"detectron2.utils.events\"", ")", "as", "logs", ":", "\n", "                ", "p1", ".", "write", "(", ")", "\n", "", "self", ".", "assertIn", "(", "\"eta\"", ",", "logs", ".", "output", "[", "0", "]", ")", "\n", "\n", "with", "self", ".", "assertLogs", "(", "\"detectron2.utils.events\"", ")", "as", "logs", ":", "\n", "                ", "p2", ".", "write", "(", ")", "\n", "", "self", ".", "assertNotIn", "(", "\"eta\"", ",", "logs", ".", "output", "[", "0", "]", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer._random_data": [[17, 34], ["numpy.concatenate", "numpy.zeros_like", "numpy.random.rand", "numpy.random.rand", "str", "numpy.random.rand().flatten", "test_visualizer.TestVisualizer._random_data._rand_poly"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.export.flatten.TensorWrapSchema.flatten"], ["    ", "def", "_random_data", "(", "self", ")", ":", "\n", "        ", "H", ",", "W", "=", "100", ",", "100", "\n", "N", "=", "10", "\n", "img", "=", "np", ".", "random", ".", "rand", "(", "H", ",", "W", ",", "3", ")", "*", "255", "\n", "boxxy", "=", "np", ".", "random", ".", "rand", "(", "N", ",", "2", ")", "*", "(", "H", "//", "2", ")", "\n", "boxes", "=", "np", ".", "concatenate", "(", "(", "boxxy", ",", "boxxy", "+", "H", "//", "2", ")", ",", "axis", "=", "1", ")", "\n", "\n", "def", "_rand_poly", "(", ")", ":", "\n", "            ", "return", "np", ".", "random", ".", "rand", "(", "3", ",", "2", ")", ".", "flatten", "(", ")", "*", "H", "\n", "\n", "", "polygons", "=", "[", "[", "_rand_poly", "(", ")", "for", "_", "in", "range", "(", "np", ".", "random", ".", "randint", "(", "1", ",", "5", ")", ")", "]", "for", "_", "in", "range", "(", "N", ")", "]", "\n", "\n", "mask", "=", "np", ".", "zeros_like", "(", "img", "[", ":", ",", ":", ",", "0", "]", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "mask", "[", ":", "40", ",", "10", ":", "20", "]", "=", "1", "\n", "\n", "labels", "=", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "N", ")", "]", "\n", "return", "img", ",", "boxes", ",", "labels", ",", "polygons", ",", "[", "mask", "]", "*", "N", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.metadata": [[35, 38], ["detectron2.data.MetadataCatalog.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "@", "property", "\n", "def", "metadata", "(", "self", ")", ":", "\n", "        ", "return", "MetadataCatalog", ".", "get", "(", "\"coco_2017_train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_draw_dataset_dict": [[39, 68], ["detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_dataset_dict", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_dataset_dict"], ["", "def", "test_draw_dataset_dict", "(", "self", ")", ":", "\n", "        ", "img", "=", "np", ".", "random", ".", "rand", "(", "512", ",", "512", ",", "3", ")", "*", "255", "\n", "dic", "=", "{", "\n", "\"annotations\"", ":", "[", "\n", "{", "\n", "\"bbox\"", ":", "[", "\n", "368.9946492271106", ",", "\n", "330.891438763377", ",", "\n", "13.148537455410235", ",", "\n", "13.644708680142685", ",", "\n", "]", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYWH_ABS", ",", "\n", "\"category_id\"", ":", "0", ",", "\n", "\"iscrowd\"", ":", "1", ",", "\n", "\"segmentation\"", ":", "{", "\n", "\"counts\"", ":", "\"_jh52m?2N2N2N2O100O10O001N1O2MceP2\"", ",", "\n", "\"size\"", ":", "[", "512", ",", "512", "]", ",", "\n", "}", ",", "\n", "}", "\n", "]", ",", "\n", "\"height\"", ":", "512", ",", "\n", "\"image_id\"", ":", "1", ",", "\n", "\"width\"", ":", "512", ",", "\n", "}", "\n", "v", "=", "Visualizer", "(", "img", ")", "\n", "v", ".", "draw_dataset_dict", "(", "dic", ")", "\n", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ")", "\n", "v", ".", "draw_dataset_dict", "(", "dic", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_draw_rotated_dataset_dict": [[69, 92], ["detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_dataset_dict"], ["", "def", "test_draw_rotated_dataset_dict", "(", "self", ")", ":", "\n", "        ", "img", "=", "np", ".", "random", ".", "rand", "(", "512", ",", "512", ",", "3", ")", "*", "255", "\n", "dic", "=", "{", "\n", "\"annotations\"", ":", "[", "\n", "{", "\n", "\"bbox\"", ":", "[", "\n", "368.9946492271106", ",", "\n", "330.891438763377", ",", "\n", "13.148537455410235", ",", "\n", "13.644708680142685", ",", "\n", "45.0", ",", "\n", "]", ",", "\n", "\"bbox_mode\"", ":", "BoxMode", ".", "XYWHA_ABS", ",", "\n", "\"category_id\"", ":", "0", ",", "\n", "\"iscrowd\"", ":", "1", ",", "\n", "}", "\n", "]", ",", "\n", "\"height\"", ":", "512", ",", "\n", "\"image_id\"", ":", "1", ",", "\n", "\"width\"", ":", "512", ",", "\n", "}", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ")", "\n", "v", ".", "draw_dataset_dict", "(", "dic", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_overlay_instances": [[93, 109], ["test_visualizer.TestVisualizer._random_data", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.overlay_instances().get_image", "test_visualizer.TestVisualizer.assertEqual", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.overlay_instances().get_image", "test_visualizer.TestVisualizer.assertEqual", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.overlay_instances().get_image", "test_visualizer.TestVisualizer.assertEqual", "detectron2.utils.visualizer.Visualizer.overlay_instances", "detectron2.utils.visualizer.Visualizer.overlay_instances", "detectron2.utils.visualizer.Visualizer.overlay_instances"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer._random_data", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances"], ["", "def", "test_overlay_instances", "(", "self", ")", ":", "\n", "        ", "img", ",", "boxes", ",", "labels", ",", "polygons", ",", "masks", "=", "self", ".", "_random_data", "(", ")", "\n", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ")", "\n", "output", "=", "v", ".", "overlay_instances", "(", "masks", "=", "polygons", ",", "boxes", "=", "boxes", ",", "labels", "=", "labels", ")", ".", "get_image", "(", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "shape", ",", "img", ".", "shape", ")", "\n", "\n", "# Test 2x scaling", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ",", "scale", "=", "2.0", ")", "\n", "output", "=", "v", ".", "overlay_instances", "(", "masks", "=", "polygons", ",", "boxes", "=", "boxes", ",", "labels", "=", "labels", ")", ".", "get_image", "(", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "shape", "[", "0", "]", ",", "img", ".", "shape", "[", "0", "]", "*", "2", ")", "\n", "\n", "# Test overlay masks", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ")", "\n", "output", "=", "v", ".", "overlay_instances", "(", "masks", "=", "masks", ",", "boxes", "=", "boxes", ",", "labels", "=", "labels", ")", ".", "get_image", "(", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "shape", ",", "img", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_overlay_instances_no_boxes": [[110, 114], ["test_visualizer.TestVisualizer._random_data", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.overlay_instances().get_image", "detectron2.utils.visualizer.Visualizer.overlay_instances"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer._random_data", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances"], ["", "def", "test_overlay_instances_no_boxes", "(", "self", ")", ":", "\n", "        ", "img", ",", "boxes", ",", "labels", ",", "polygons", ",", "_", "=", "self", ".", "_random_data", "(", ")", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ")", "\n", "v", ".", "overlay_instances", "(", "masks", "=", "polygons", ",", "boxes", "=", "None", ",", "labels", "=", "labels", ")", ".", "get_image", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_draw_instance_predictions": [[115, 129], ["test_visualizer.TestVisualizer._random_data", "len", "detectron2.structures.Instances", "torch.randint", "torch.rand", "torch.from_numpy", "torch.from_numpy", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer._random_data", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_instance_predictions", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_instance_predictions"], ["", "def", "test_draw_instance_predictions", "(", "self", ")", ":", "\n", "        ", "img", ",", "boxes", ",", "_", ",", "_", ",", "masks", "=", "self", ".", "_random_data", "(", ")", "\n", "num_inst", "=", "len", "(", "boxes", ")", "\n", "inst", "=", "Instances", "(", "(", "img", ".", "shape", "[", "0", "]", ",", "img", ".", "shape", "[", "1", "]", ")", ")", "\n", "inst", ".", "pred_classes", "=", "torch", ".", "randint", "(", "0", ",", "80", ",", "size", "=", "(", "num_inst", ",", ")", ")", "\n", "inst", ".", "scores", "=", "torch", ".", "rand", "(", "num_inst", ")", "\n", "inst", ".", "pred_boxes", "=", "torch", ".", "from_numpy", "(", "boxes", ")", "\n", "inst", ".", "pred_masks", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "masks", ")", ")", "\n", "\n", "v", "=", "Visualizer", "(", "img", ")", "\n", "v", ".", "draw_instance_predictions", "(", "inst", ")", "\n", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ")", "\n", "v", ".", "draw_instance_predictions", "(", "inst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_BWmode_nomask": [[130, 140], ["test_visualizer.TestVisualizer._random_data", "len", "detectron2.structures.Instances", "torch.randint", "torch.rand", "torch.from_numpy", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_instance_predictions"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer._random_data", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_instance_predictions"], ["", "def", "test_BWmode_nomask", "(", "self", ")", ":", "\n", "        ", "img", ",", "boxes", ",", "_", ",", "_", ",", "masks", "=", "self", ".", "_random_data", "(", ")", "\n", "num_inst", "=", "len", "(", "boxes", ")", "\n", "inst", "=", "Instances", "(", "(", "img", ".", "shape", "[", "0", "]", ",", "img", ".", "shape", "[", "1", "]", ")", ")", "\n", "inst", ".", "pred_classes", "=", "torch", ".", "randint", "(", "0", ",", "80", ",", "size", "=", "(", "num_inst", ",", ")", ")", "\n", "inst", ".", "scores", "=", "torch", ".", "rand", "(", "num_inst", ")", "\n", "inst", ".", "pred_boxes", "=", "torch", ".", "from_numpy", "(", "boxes", ")", "\n", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ",", "instance_mode", "=", "ColorMode", ".", "IMAGE_BW", ")", "\n", "v", ".", "draw_instance_predictions", "(", "inst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_draw_empty_mask_predictions": [[141, 152], ["test_visualizer.TestVisualizer._random_data", "len", "detectron2.structures.Instances", "torch.randint", "torch.rand", "torch.from_numpy", "torch.from_numpy", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "numpy.zeros_like", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer._random_data", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_instance_predictions"], ["", "def", "test_draw_empty_mask_predictions", "(", "self", ")", ":", "\n", "        ", "img", ",", "boxes", ",", "_", ",", "_", ",", "masks", "=", "self", ".", "_random_data", "(", ")", "\n", "num_inst", "=", "len", "(", "boxes", ")", "\n", "inst", "=", "Instances", "(", "(", "img", ".", "shape", "[", "0", "]", ",", "img", ".", "shape", "[", "1", "]", ")", ")", "\n", "inst", ".", "pred_classes", "=", "torch", ".", "randint", "(", "0", ",", "80", ",", "size", "=", "(", "num_inst", ",", ")", ")", "\n", "inst", ".", "scores", "=", "torch", ".", "rand", "(", "num_inst", ")", "\n", "inst", ".", "pred_boxes", "=", "torch", ".", "from_numpy", "(", "boxes", ")", "\n", "inst", ".", "pred_masks", "=", "torch", ".", "from_numpy", "(", "np", ".", "zeros_like", "(", "np", ".", "asarray", "(", "masks", ")", ")", ")", "\n", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ")", "\n", "v", ".", "draw_instance_predictions", "(", "inst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_correct_output_shape": [[153, 158], ["detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.output.get_image", "test_visualizer.TestVisualizer.assertEqual", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image"], ["", "def", "test_correct_output_shape", "(", "self", ")", ":", "\n", "        ", "img", "=", "np", ".", "random", ".", "rand", "(", "928", ",", "928", ",", "3", ")", "*", "255", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ")", "\n", "out", "=", "v", ".", "output", ".", "get_image", "(", ")", "\n", "self", ".", "assertEqual", "(", "out", ".", "shape", ",", "img", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_overlay_rotated_instances": [[159, 175], ["torch.zeros", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "torch.FloatTensor().uniform_", "detectron2.structures.RotatedBoxes", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.overlay_instances().get_image", "test_visualizer.TestVisualizer.assertEqual", "numpy.random.rand", "max", "max", "str", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "range", "detectron2.utils.visualizer.Visualizer.overlay_instances"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.overlay_instances"], ["", "def", "test_overlay_rotated_instances", "(", "self", ")", ":", "\n", "        ", "H", ",", "W", "=", "100", ",", "150", "\n", "img", "=", "np", ".", "random", ".", "rand", "(", "H", ",", "W", ",", "3", ")", "*", "255", "\n", "num_boxes", "=", "50", "\n", "boxes_5d", "=", "torch", ".", "zeros", "(", "num_boxes", ",", "5", ")", "\n", "boxes_5d", "[", ":", ",", "0", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "0.1", "*", "W", ",", "1.1", "*", "W", ")", "\n", "boxes_5d", "[", ":", ",", "1", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "0.1", "*", "H", ",", "1.1", "*", "H", ")", "\n", "boxes_5d", "[", ":", ",", "2", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "0", ",", "max", "(", "W", ",", "H", ")", ")", "\n", "boxes_5d", "[", ":", ",", "3", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "0", ",", "max", "(", "W", ",", "H", ")", ")", "\n", "boxes_5d", "[", ":", ",", "4", "]", "=", "torch", ".", "FloatTensor", "(", "num_boxes", ")", ".", "uniform_", "(", "-", "1800", ",", "1800", ")", "\n", "rotated_boxes", "=", "RotatedBoxes", "(", "boxes_5d", ")", "\n", "labels", "=", "[", "str", "(", "i", ")", "for", "i", "in", "range", "(", "num_boxes", ")", "]", "\n", "\n", "v", "=", "Visualizer", "(", "img", ",", "self", ".", "metadata", ")", "\n", "output", "=", "v", ".", "overlay_instances", "(", "boxes", "=", "rotated_boxes", ",", "labels", "=", "labels", ")", ".", "get_image", "(", ")", "\n", "self", ".", "assertEqual", "(", "output", ".", "shape", ",", "img", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_draw_no_metadata": [[176, 187], ["test_visualizer.TestVisualizer._random_data", "len", "detectron2.structures.Instances", "torch.randint", "torch.rand", "torch.from_numpy", "torch.from_numpy", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "numpy.asarray", "detectron2.data.MetadataCatalog.get"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer._random_data", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.video_visualizer.VideoVisualizer.draw_instance_predictions", "home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.get"], ["", "def", "test_draw_no_metadata", "(", "self", ")", ":", "\n", "        ", "img", ",", "boxes", ",", "_", ",", "_", ",", "masks", "=", "self", ".", "_random_data", "(", ")", "\n", "num_inst", "=", "len", "(", "boxes", ")", "\n", "inst", "=", "Instances", "(", "(", "img", ".", "shape", "[", "0", "]", ",", "img", ".", "shape", "[", "1", "]", ")", ")", "\n", "inst", ".", "pred_classes", "=", "torch", ".", "randint", "(", "0", ",", "80", ",", "size", "=", "(", "num_inst", ",", ")", ")", "\n", "inst", ".", "scores", "=", "torch", ".", "rand", "(", "num_inst", ")", "\n", "inst", ".", "pred_boxes", "=", "torch", ".", "from_numpy", "(", "boxes", ")", "\n", "inst", ".", "pred_masks", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "masks", ")", ")", "\n", "\n", "v", "=", "Visualizer", "(", "img", ",", "MetadataCatalog", ".", "get", "(", "\"asdfasdf\"", ")", ")", "\n", "v", ".", "draw_instance_predictions", "(", "inst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_draw_binary_mask": [[188, 208], ["test_visualizer.TestVisualizer._random_data", "numpy.zeros_like().astype", "cv2.rectangle", "numpy.zeros_like", "detectron2.utils.visualizer.Visualizer", "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "test_visualizer.TestVisualizer.assertTrue", "o.get_image().astype.get_image().astype.get_image().astype", "tempfile.TemporaryDirectory", "os.path.join", "o.get_image().astype.get_image().astype.save", "o[].sum", "cv2.imread", "o.get_image().astype.get_image().astype.get_image"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer._random_data", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_binary_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.config.lazy.LazyConfig.save", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image"], ["", "def", "test_draw_binary_mask", "(", "self", ")", ":", "\n", "        ", "img", ",", "boxes", ",", "_", ",", "_", ",", "masks", "=", "self", ".", "_random_data", "(", ")", "\n", "img", "[", ":", ",", ":", ",", "0", "]", "=", "0", "# remove red color", "\n", "mask", "=", "masks", "[", "0", "]", "\n", "mask_with_hole", "=", "np", ".", "zeros_like", "(", "mask", ")", ".", "astype", "(", "\"uint8\"", ")", "\n", "mask_with_hole", "=", "cv2", ".", "rectangle", "(", "mask_with_hole", ",", "(", "10", ",", "10", ")", ",", "(", "50", ",", "50", ")", ",", "1", ",", "5", ")", "\n", "\n", "for", "m", "in", "[", "mask", ",", "mask_with_hole", "]", ":", "\n", "            ", "for", "save", "in", "[", "True", ",", "False", "]", ":", "\n", "                ", "v", "=", "Visualizer", "(", "img", ")", "\n", "o", "=", "v", ".", "draw_binary_mask", "(", "m", ",", "color", "=", "\"red\"", ",", "text", "=", "\"test\"", ")", "\n", "if", "save", ":", "\n", "                    ", "with", "tempfile", ".", "TemporaryDirectory", "(", "prefix", "=", "\"detectron2_viz\"", ")", "as", "d", ":", "\n", "                        ", "path", "=", "os", ".", "path", ".", "join", "(", "d", ",", "\"output.png\"", ")", "\n", "o", ".", "save", "(", "path", ")", "\n", "o", "=", "cv2", ".", "imread", "(", "path", ")", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "", "", "else", ":", "\n", "                    ", "o", "=", "o", ".", "get_image", "(", ")", ".", "astype", "(", "\"float32\"", ")", "\n", "# red color is drawn on the image", "\n", "", "self", ".", "assertTrue", "(", "o", "[", ":", ",", ":", ",", "0", "]", ".", "sum", "(", ")", ">", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_border_mask_with_holes": [[209, 229], ["numpy.zeros", "detectron2.utils.visualizer.Visualizer", "numpy.zeros", "cv2.rectangle", "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "test_visualizer.TestVisualizer.assertEqual", "test_visualizer.TestVisualizer.assertEqual", "test_visualizer.TestVisualizer.assertIn", "test_visualizer.TestVisualizer.assertIn", "detectron2.utils.visualizer.Visualizer.draw_binary_mask.get_image", "tuple", "tuple", "len", "len", "x.tolist", "x.tolist"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_binary_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image"], ["", "", "", "def", "test_border_mask_with_holes", "(", "self", ")", ":", "\n", "        ", "H", ",", "W", "=", "200", ",", "200", "\n", "img", "=", "np", ".", "zeros", "(", "(", "H", ",", "W", ",", "3", ")", ")", "\n", "img", "[", ":", ",", ":", ",", "0", "]", "=", "255.0", "\n", "v", "=", "Visualizer", "(", "img", ",", "scale", "=", "3", ")", "\n", "\n", "mask", "=", "np", ".", "zeros", "(", "(", "H", ",", "W", ")", ")", "\n", "mask", "[", ":", ",", "100", ":", "150", "]", "=", "1", "\n", "# create a hole, to trigger imshow", "\n", "mask", "=", "cv2", ".", "rectangle", "(", "mask", ",", "(", "110", ",", "110", ")", ",", "(", "130", ",", "130", ")", ",", "0", ",", "thickness", "=", "-", "1", ")", "\n", "output", "=", "v", ".", "draw_binary_mask", "(", "mask", ",", "color", "=", "\"blue\"", ")", "\n", "output", "=", "output", ".", "get_image", "(", ")", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "\n", "first_row", "=", "{", "tuple", "(", "x", ".", "tolist", "(", ")", ")", "for", "x", "in", "output", "[", "0", "]", "}", "\n", "last_row", "=", "{", "tuple", "(", "x", ".", "tolist", "(", ")", ")", "for", "x", "in", "output", "[", "-", "1", "]", "}", "\n", "# Check quantization / off-by-1 error: the first and last row must have two colors", "\n", "self", ".", "assertEqual", "(", "len", "(", "last_row", ")", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "len", "(", "first_row", ")", ",", "2", ")", "\n", "self", ".", "assertIn", "(", "(", "0", ",", "0", ",", "255", ")", ",", "last_row", ")", "\n", "self", ".", "assertIn", "(", "(", "0", ",", "0", ",", "255", ")", ",", "first_row", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.tests.test_visualizer.TestVisualizer.test_border_polygons": [[230, 250], ["numpy.zeros", "detectron2.utils.visualizer.Visualizer", "numpy.zeros", "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "test_visualizer.TestVisualizer.assertGreaterEqual", "test_visualizer.TestVisualizer.assertGreaterEqual", "test_visualizer.TestVisualizer.assertIn", "test_visualizer.TestVisualizer.assertIn", "detectron2.utils.visualizer.Visualizer.draw_binary_mask.get_image", "tuple", "tuple", "len", "len", "x.tolist", "x.tolist"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.Visualizer.draw_binary_mask", "home.repos.pwc.inspect_result.microsoft_regionclip.utils.visualizer.VisImage.get_image"], ["", "def", "test_border_polygons", "(", "self", ")", ":", "\n", "        ", "H", ",", "W", "=", "200", ",", "200", "\n", "img", "=", "np", ".", "zeros", "(", "(", "H", ",", "W", ",", "3", ")", ")", "\n", "img", "[", ":", ",", ":", ",", "0", "]", "=", "255.0", "\n", "v", "=", "Visualizer", "(", "img", ",", "scale", "=", "3", ")", "\n", "mask", "=", "np", ".", "zeros", "(", "(", "H", ",", "W", ")", ")", "\n", "mask", "[", ":", ",", "100", ":", "150", "]", "=", "1", "\n", "\n", "output", "=", "v", ".", "draw_binary_mask", "(", "mask", ",", "color", "=", "\"blue\"", ")", "\n", "output", "=", "output", ".", "get_image", "(", ")", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "\n", "first_row", "=", "{", "tuple", "(", "x", ".", "tolist", "(", ")", ")", "for", "x", "in", "output", "[", "0", "]", "}", "\n", "last_row", "=", "{", "tuple", "(", "x", ".", "tolist", "(", ")", ")", "for", "x", "in", "output", "[", "-", "1", "]", "}", "\n", "# Check quantization / off-by-1 error:", "\n", "# the first and last row must have >=2 colors, because the polygon", "\n", "# touches both rows", "\n", "self", ".", "assertGreaterEqual", "(", "len", "(", "last_row", ")", ",", "2", ")", "\n", "self", ".", "assertGreaterEqual", "(", "len", "(", "first_row", ")", ",", "2", ")", "\n", "self", ".", "assertIn", "(", "(", "0", ",", "0", ",", "255", ")", ",", "last_row", ")", "\n", "self", ".", "assertIn", "(", "(", "0", ",", "0", ",", "255", ")", ",", "first_row", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__": [[41, 44], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model", ":", "nn", ".", "Module", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.device": [[45, 48], ["list", "torchvision_imagenet_R_50.ClassificationNet.model.parameters"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.data.catalog._MetadataCatalog.list"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "model", ".", "parameters", "(", ")", ")", "[", "0", "]", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationNet.forward": [[49, 57], ["torchvision_imagenet_R_50.ClassificationNet.model", "image.to", "label.to.to.to", "torch.nn.functional.cross_entropy"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.structures.boxes.Boxes.to", "home.repos.pwc.inspect_result.microsoft_regionclip.layers.wrappers.cross_entropy"], ["", "def", "forward", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "image", ",", "label", "=", "inputs", "\n", "pred", "=", "self", ".", "model", "(", "image", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "label", "=", "label", ".", "to", "(", "self", ".", "device", ")", "\n", "return", "F", ".", "cross_entropy", "(", "pred", ",", "label", ")", "\n", "", "else", ":", "\n", "            ", "return", "pred", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.reset": [[60, 62], ["None"], "methods", ["None"], ["    ", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "corr", "=", "self", ".", "total", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.process": [[63, 67], ["len", "outputs.argmax().cpu", "label.cpu", "outputs.argmax"], "methods", ["None"], ["", "def", "process", "(", "self", ",", "inputs", ",", "outputs", ")", ":", "\n", "        ", "image", ",", "label", "=", "inputs", "\n", "self", ".", "corr", "+=", "(", "outputs", ".", "argmax", "(", "dim", "=", "1", ")", ".", "cpu", "(", ")", "==", "label", ".", "cpu", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "total", "+=", "len", "(", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.ClassificationAcc.evaluate": [[68, 73], ["detectron2.utils.comm.all_gather", "sum", "sum"], "methods", ["home.repos.pwc.inspect_result.microsoft_regionclip.utils.comm.all_gather"], ["", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "all_corr_total", "=", "comm", ".", "all_gather", "(", "[", "self", ".", "corr", ",", "self", ".", "total", "]", ")", "\n", "corr", "=", "sum", "(", "x", "[", "0", "]", "for", "x", "in", "all_corr_total", ")", "\n", "total", "=", "sum", "(", "x", "[", "1", "]", "for", "x", "in", "all_corr_total", ")", "\n", "return", "{", "\"accuracy\"", ":", "corr", "/", "total", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.Misc.torchvision_imagenet_R_50.build_data_loader": [[30, 37], ["torch.utils.data.DataLoader", "len"], "function", ["None"], ["def", "build_data_loader", "(", "dataset", ",", "batch_size", ",", "num_workers", ",", "training", "=", "True", ")", ":", "\n", "    ", "return", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "sampler", "=", "(", "TrainingSampler", "if", "training", "else", "InferenceSampler", ")", "(", "len", "(", "dataset", ")", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "pin_memory", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.microsoft_regionclip.common.coco_schedule.default_X_scheduler": [[7, 40], ["detectron2.config.LazyCall", "detectron2.config.LazyCall", "detectron2.config.LazyCall"], "function", ["None"], ["def", "default_X_scheduler", "(", "num_X", ")", ":", "\n", "    ", "\"\"\"\n    Returns the config for a default multi-step LR scheduler such as \"1x\", \"3x\",\n    commonly referred to in papers, where every 1x has the total length of 1440k\n    training images (~12 COCO epochs). LR is decayed twice at the end of training\n    following the strategy defined in \"Rethinking ImageNet Pretraining\", Sec 4.\n\n    Args:\n        num_X: a positive real number\n\n    Returns:\n        DictConfig: configs that define the multiplier for LR during training\n    \"\"\"", "\n", "# total number of iterations assuming 16 batch size, using 1440000/16=90000", "\n", "total_steps_16bs", "=", "num_X", "*", "90000", "\n", "\n", "if", "num_X", "<=", "2", ":", "\n", "        ", "scheduler", "=", "L", "(", "MultiStepParamScheduler", ")", "(", "\n", "values", "=", "[", "1.0", ",", "0.1", ",", "0.01", "]", ",", "\n", "# note that scheduler is scale-invariant. This is equivalent to", "\n", "# milestones=[6, 8, 9]", "\n", "milestones", "=", "[", "60000", ",", "80000", ",", "90000", "]", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "scheduler", "=", "L", "(", "MultiStepParamScheduler", ")", "(", "\n", "values", "=", "[", "1.0", ",", "0.1", ",", "0.01", "]", ",", "\n", "milestones", "=", "[", "total_steps_16bs", "-", "60000", ",", "total_steps_16bs", "-", "20000", ",", "total_steps_16bs", "]", ",", "\n", ")", "\n", "", "return", "L", "(", "WarmupParamScheduler", ")", "(", "\n", "scheduler", "=", "scheduler", ",", "\n", "warmup_length", "=", "1000", "/", "total_steps_16bs", ",", "\n", "warmup_method", "=", "\"linear\"", ",", "\n", "warmup_factor", "=", "0.001", ",", "\n", ")", "\n"]]}