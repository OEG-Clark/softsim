{"home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.__init__": [[152, 201], ["transformers.modeling_utils.PreTrainedModel.__init__", "noisy_model.NoisyEncoderDecoderModel.tie_weights", "transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig.from_encoder_decoder_configs", "isinstance", "AutoModel.from_config", "AutoModelForCausalLM.from_config", "noisy_model.NoisyEncoderDecoderModel.encoder.config.to_dict", "noisy_model.NoisyEncoderDecoderModel.config.encoder.to_dict", "logger.warning", "noisy_model.NoisyEncoderDecoderModel.decoder.config.to_dict", "noisy_model.NoisyEncoderDecoderModel.config.decoder.to_dict", "logger.warning", "noisy_model.NoisyEncoderDecoderModel.encoder.get_output_embeddings"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.__init__", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.tie_weights", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.get_output_embeddings"], ["def", "__init__", "(", "\n", "self", ",", "\n", "config", ":", "Optional", "[", "PretrainedConfig", "]", "=", "None", ",", "\n", "encoder", ":", "Optional", "[", "PreTrainedModel", "]", "=", "None", ",", "\n", "decoder", ":", "Optional", "[", "PreTrainedModel", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "assert", "config", "is", "not", "None", "or", "(", "\n", "encoder", "is", "not", "None", "and", "decoder", "is", "not", "None", "\n", ")", ",", "\"Either a configuration or an Encoder and a decoder has to be provided\"", "\n", "if", "config", "is", "None", ":", "\n", "            ", "config", "=", "EncoderDecoderConfig", ".", "from_encoder_decoder_configs", "(", "encoder", ".", "config", ",", "decoder", ".", "config", ")", "\n", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "config", ",", "self", ".", "config_class", ")", ",", "f\"config: {config} has to be of type {self.config_class}\"", "\n", "# initialize with config", "\n", "", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "if", "encoder", "is", "None", ":", "\n", "            ", "from", "transformers", ".", "models", ".", "auto", ".", "modeling_auto", "import", "AutoModel", "\n", "\n", "encoder", "=", "AutoModel", ".", "from_config", "(", "config", ".", "encoder", ")", "\n", "\n", "", "if", "decoder", "is", "None", ":", "\n", "            ", "from", "transformers", ".", "models", ".", "auto", ".", "modeling_auto", "import", "AutoModelForCausalLM", "\n", "\n", "decoder", "=", "AutoModelForCausalLM", ".", "from_config", "(", "config", ".", "decoder", ")", "\n", "\n", "", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "\n", "if", "self", ".", "encoder", ".", "config", ".", "to_dict", "(", ")", "!=", "self", ".", "config", ".", "encoder", ".", "to_dict", "(", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Config of the encoder: {self.encoder.__class__} is overwritten by shared encoder config: {self.config.encoder}\"", "\n", ")", "\n", "", "if", "self", ".", "decoder", ".", "config", ".", "to_dict", "(", ")", "!=", "self", ".", "config", ".", "decoder", ".", "to_dict", "(", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Config of the decoder: {self.decoder.__class__} is overwritten by shared decoder config: {self.config.decoder}\"", "\n", ")", "\n", "\n", "# make sure that the individual model's config refers to the shared config", "\n", "# so that the updates to the config will be synced", "\n", "", "self", ".", "encoder", ".", "config", "=", "self", ".", "config", ".", "encoder", "\n", "self", ".", "decoder", ".", "config", "=", "self", ".", "config", ".", "decoder", "\n", "\n", "assert", "(", "\n", "self", ".", "encoder", ".", "get_output_embeddings", "(", ")", "is", "None", "\n", ")", ",", "\"The encoder {} should not have a LM Head. Please use a model without LM Head\"", "\n", "\n", "# tie encoder, decoder weights if config set accordingly", "\n", "self", ".", "tie_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.tie_weights": [[202, 209], ["noisy_model.NoisyEncoderDecoderModel._tie_encoder_decoder_weights"], "methods", ["None"], ["", "def", "tie_weights", "(", "self", ")", ":", "\n", "# tie encoder & decoder if needed", "\n", "        ", "if", "self", ".", "config", ".", "tie_encoder_decoder", ":", "\n", "# tie encoder and decoder base model", "\n", "            ", "decoder_base_model_prefix", "=", "self", ".", "decoder", ".", "base_model_prefix", "\n", "self", ".", "_tie_encoder_decoder_weights", "(", "\n", "self", ".", "encoder", ",", "self", ".", "decoder", ".", "_modules", "[", "decoder_base_model_prefix", "]", ",", "self", ".", "decoder", ".", "base_model_prefix", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.get_encoder": [[211, 213], ["None"], "methods", ["None"], ["", "", "def", "get_encoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.get_decoder": [[214, 216], ["None"], "methods", ["None"], ["", "def", "get_decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.get_input_embeddings": [[217, 219], ["noisy_model.NoisyEncoderDecoderModel.encoder.get_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.get_input_embeddings"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "encoder", ".", "get_input_embeddings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.get_output_embeddings": [[220, 222], ["noisy_model.NoisyEncoderDecoderModel.decoder.get_output_embeddings"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.get_output_embeddings"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "decoder", ".", "get_output_embeddings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.set_output_embeddings": [[223, 225], ["noisy_model.NoisyEncoderDecoderModel.decoder.set_output_embeddings"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.set_output_embeddings"], ["", "def", "set_output_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "return", "self", ".", "decoder", ".", "set_output_embeddings", "(", "new_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained": [[226, 232], ["super().from_pretrained"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# At the moment fast initialization is not supported", "\n", "# for composite models", "\n", "        ", "kwargs", "[", "\"_fast_init\"", "]", "=", "False", "\n", "return", "super", "(", ")", ".", "from_pretrained", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_encoder_decoder_pretrained": [[233, 374], ["kwargs_encoder.keys", "kwargs_decoder.keys", "kwargs_encoder.pop", "kwargs_decoder.pop", "transformers.models.encoder_decoder.configuration_encoder_decoder.EncoderDecoderConfig.from_encoder_decoder_configs", "cls", "AutoModel.from_pretrained", "AutoModelForCausalLM.from_pretrained", "kwargs.items", "argument.startswith", "kwargs.items", "argument.startswith", "AutoConfig.from_pretrained", "AutoConfig.from_pretrained", "logger.warning", "len", "len", "logger.info", "logger.info"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained"], ["", "@", "classmethod", "\n", "def", "from_encoder_decoder_pretrained", "(", "\n", "cls", ",", "\n", "encoder_pretrained_model_name_or_path", ":", "str", "=", "None", ",", "\n", "decoder_pretrained_model_name_or_path", ":", "str", "=", "None", ",", "\n", "*", "model_args", ",", "\n", "**", "kwargs", "\n", ")", "->", "PreTrainedModel", ":", "\n", "        ", "r\"\"\"\n        Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model\n        checkpoints.\n\n\n        The model is set in evaluation mode by default using :obj:`model.eval()` (Dropout modules are deactivated). To\n        train the model, you need to first set it back in training mode with :obj:`model.train()`.\n\n        Params:\n            encoder_pretrained_model_name_or_path (:obj: `str`, `optional`):\n                Information necessary to initiate the encoder. Can be either:\n\n                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n                      Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n                      a user or organization name, like ``dbmdz/bert-base-german-cased``.\n                    - A path to a `directory` containing model weights saved using\n                      :func:`~transformers.PreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n                    - A path or url to a `tensorflow index checkpoint file` (e.g, ``./tf_model/model.ckpt.index``). In\n                      this case, ``from_tf`` should be set to :obj:`True` and a configuration object should be provided\n                      as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in\n                      a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n\n            decoder_pretrained_model_name_or_path (:obj: `str`, `optional`, defaults to `None`):\n                Information necessary to initiate the decoder. Can be either:\n\n                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n                      Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n                      a user or organization name, like ``dbmdz/bert-base-german-cased``.\n                    - A path to a `directory` containing model weights saved using\n                      :func:`~transformers.PreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n                    - A path or url to a `tensorflow index checkpoint file` (e.g, ``./tf_model/model.ckpt.index``). In\n                      this case, ``from_tf`` should be set to :obj:`True` and a configuration object should be provided\n                      as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in\n                      a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n\n            model_args (remaining positional arguments, `optional`):\n                All remaning positional arguments will be passed to the underlying model's ``__init__`` method.\n\n            kwargs (remaining dictionary of keyword arguments, `optional`):\n                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n                :obj:`output_attentions=True`).\n\n                - To update the encoder configuration, use the prefix `encoder_` for each configuration parameter.\n                - To update the decoder configuration, use the prefix `decoder_` for each configuration parameter.\n                - To update the parent model configuration, do not use a prefix for each configuration parameter.\n\n                Behaves differently depending on whether a :obj:`config` is provided or automatically loaded.\n\n        Example::\n\n            >>> from transformers import EncoderDecoderModel\n            >>> # initialize a bert2bert from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized\n            >>> model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')\n            >>> # saving model after fine-tuning\n            >>> model.save_pretrained(\"./bert2bert\")\n            >>> # load fine-tuned model\n            >>> model = EncoderDecoderModel.from_pretrained(\"./bert2bert\")\n\n        \"\"\"", "\n", "\n", "cls", ".", "gaussian", "=", "kwargs", "[", "\"gaussian\"", "]", "\n", "cls", ".", "impulsive", "=", "kwargs", "[", "\"impulsive\"", "]", "\n", "\n", "kwargs_encoder", "=", "{", "\n", "argument", "[", "len", "(", "\"encoder_\"", ")", ":", "]", ":", "value", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "if", "argument", ".", "startswith", "(", "\"encoder_\"", ")", "\n", "}", "\n", "\n", "kwargs_decoder", "=", "{", "\n", "argument", "[", "len", "(", "\"decoder_\"", ")", ":", "]", ":", "value", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "if", "argument", ".", "startswith", "(", "\"decoder_\"", ")", "\n", "}", "\n", "\n", "# remove encoder, decoder kwargs from kwargs", "\n", "for", "key", "in", "kwargs_encoder", ".", "keys", "(", ")", ":", "\n", "            ", "del", "kwargs", "[", "\"encoder_\"", "+", "key", "]", "\n", "", "for", "key", "in", "kwargs_decoder", ".", "keys", "(", ")", ":", "\n", "            ", "del", "kwargs", "[", "\"decoder_\"", "+", "key", "]", "\n", "\n", "# Load and initialize the encoder and decoder", "\n", "# The distinction between encoder and decoder at the model level is made", "\n", "# by the value of the flag `is_decoder` that we need to set correctly.", "\n", "", "encoder", "=", "kwargs_encoder", ".", "pop", "(", "\"model\"", ",", "None", ")", "\n", "if", "encoder", "is", "None", ":", "\n", "            ", "assert", "(", "\n", "encoder_pretrained_model_name_or_path", "is", "not", "None", "\n", ")", ",", "\"If `model` is not defined as an argument, a `encoder_pretrained_model_name_or_path` has to be defined\"", "\n", "from", "transformers", ".", "models", ".", "auto", ".", "modeling_auto", "import", "AutoModel", "\n", "\n", "if", "\"config\"", "not", "in", "kwargs_encoder", ":", "\n", "                ", "from", "transformers", ".", "models", ".", "auto", ".", "configuration_auto", "import", "AutoConfig", "\n", "\n", "encoder_config", "=", "AutoConfig", ".", "from_pretrained", "(", "encoder_pretrained_model_name_or_path", ")", "\n", "if", "encoder_config", ".", "is_decoder", "is", "True", "or", "encoder_config", ".", "add_cross_attention", "is", "True", ":", "\n", "\n", "                    ", "logger", ".", "info", "(", "\n", "f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model from a decoder model. Cross-attention and casual mask are disabled.\"", "\n", ")", "\n", "encoder_config", ".", "is_decoder", "=", "False", "\n", "encoder_config", ".", "add_cross_attention", "=", "False", "\n", "\n", "", "kwargs_encoder", "[", "\"config\"", "]", "=", "encoder_config", "\n", "\n", "", "encoder", "=", "AutoModel", ".", "from_pretrained", "(", "encoder_pretrained_model_name_or_path", ",", "*", "model_args", ",", "**", "kwargs_encoder", ")", "\n", "\n", "", "decoder", "=", "kwargs_decoder", ".", "pop", "(", "\"model\"", ",", "None", ")", "\n", "if", "decoder", "is", "None", ":", "\n", "            ", "assert", "(", "\n", "decoder_pretrained_model_name_or_path", "is", "not", "None", "\n", ")", ",", "\"If `decoder_model` is not defined as an argument, a `decoder_pretrained_model_name_or_path` has to be defined\"", "\n", "from", "transformers", ".", "models", ".", "auto", ".", "modeling_auto", "import", "AutoModelForCausalLM", "\n", "\n", "if", "\"config\"", "not", "in", "kwargs_decoder", ":", "\n", "                ", "from", "transformers", ".", "models", ".", "auto", ".", "configuration_auto", "import", "AutoConfig", "\n", "\n", "decoder_config", "=", "AutoConfig", ".", "from_pretrained", "(", "decoder_pretrained_model_name_or_path", ")", "\n", "if", "decoder_config", ".", "is_decoder", "is", "False", "or", "decoder_config", ".", "add_cross_attention", "is", "False", ":", "\n", "                    ", "logger", ".", "info", "(", "\n", "f\"Initializing {decoder_pretrained_model_name_or_path} as a decoder model. Cross attention layers are added to {decoder_pretrained_model_name_or_path} and randomly initialized if {decoder_pretrained_model_name_or_path}'s architecture allows for cross attention layers.\"", "\n", ")", "\n", "decoder_config", ".", "is_decoder", "=", "True", "\n", "decoder_config", ".", "add_cross_attention", "=", "True", "\n", "\n", "", "kwargs_decoder", "[", "\"config\"", "]", "=", "decoder_config", "\n", "\n", "", "if", "kwargs_decoder", "[", "\"config\"", "]", ".", "is_decoder", "is", "False", "or", "kwargs_decoder", "[", "\"config\"", "]", ".", "add_cross_attention", "is", "False", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "f\"Decoder model {decoder_pretrained_model_name_or_path} is not initialized as a decoder. In order to initialize {decoder_pretrained_model_name_or_path} as a decoder, make sure that the attributes `is_decoder` and `add_cross_attention` of `decoder_config` passed to `.from_encoder_decoder_pretrained(...)` are set to `True` or do not pass a `decoder_config` to `.from_encoder_decoder_pretrained(...)`\"", "\n", ")", "\n", "\n", "", "decoder", "=", "AutoModelForCausalLM", ".", "from_pretrained", "(", "decoder_pretrained_model_name_or_path", ",", "**", "kwargs_decoder", ")", "\n", "\n", "# instantiate config with corresponding kwargs", "\n", "", "config", "=", "EncoderDecoderConfig", ".", "from_encoder_decoder_configs", "(", "encoder", ".", "config", ",", "decoder", ".", "config", ",", "**", "kwargs", ")", "\n", "return", "cls", "(", "encoder", "=", "encoder", ",", "decoder", "=", "decoder", ",", "config", "=", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.forward": [[375, 489], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.replace_return_docstrings", "noisy_model.NoisyEncoderDecoderModel.decoder", "transformers.modeling_outputs.Seq2SeqLMOutput", "noisy_model.NoisyEncoderDecoderModel.encoder", "print", "torch.randn_like", "torch.cuda.empty_cache", "print", "torch.max", "torch.min", "torch.rand_like", "encoder_hidden_states.detach", "torch.cuda.empty_cache", "kwargs.items", "kwargs.items", "argument.startswith", "argument.startswith", "len"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "ENCODER_DECODER_INPUTS_DOCSTRING", ")", "\n", "@", "replace_return_docstrings", "(", "output_type", "=", "Seq2SeqLMOutput", ",", "config_class", "=", "_CONFIG_FOR_DOC", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "decoder_input_ids", "=", "None", ",", "\n", "decoder_attention_mask", "=", "None", ",", "\n", "encoder_outputs", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "decoder_inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        Returns:\n\n        Examples::\n\n            >>> from transformers import EncoderDecoderModel, BertTokenizer\n            >>> import torch\n\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints\n\n            >>> # forward\n            >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n            >>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n\n            >>> # training\n            >>> outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)\n            >>> loss, logits = outputs.loss, outputs.logits\n\n            >>> # save and load from pretrained\n            >>> model.save_pretrained(\"bert2bert\")\n            >>> model = EncoderDecoderModel.from_pretrained(\"bert2bert\")\n\n            >>> # generation\n            >>> generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.pad_token_id)\n\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "kwargs_encoder", "=", "{", "argument", ":", "value", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "if", "not", "argument", ".", "startswith", "(", "\"decoder_\"", ")", "}", "\n", "\n", "kwargs_decoder", "=", "{", "\n", "argument", "[", "len", "(", "\"decoder_\"", ")", ":", "]", ":", "value", "for", "argument", ",", "value", "in", "kwargs", ".", "items", "(", ")", "if", "argument", ".", "startswith", "(", "\"decoder_\"", ")", "\n", "}", "\n", "\n", "if", "encoder_outputs", "is", "None", ":", "\n", "            ", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "**", "kwargs_encoder", ",", "\n", ")", "\n", "\n", "", "encoder_hidden_states", "=", "encoder_outputs", "[", "0", "]", "\n", "\n", "if", "self", ".", "training", "and", "self", ".", "gaussian", ">", "1E-6", ":", "\n", "            ", "print", "(", "f'Gaussian noise {self.gaussian}'", ")", "\n", "normal_noise", "=", "torch", ".", "randn_like", "(", "encoder_hidden_states", ")", "\n", "encoder_hidden_states", "+=", "self", ".", "gaussian", "*", "normal_noise", "\n", "del", "normal_noise", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "", "if", "self", ".", "training", "and", "self", ".", "impulsive", ">", "1E-6", ":", "\n", "            ", "print", "(", "f'Impulsive noise {self.impulsive}'", ")", "\n", "salt", "=", "torch", ".", "max", "(", "encoder_hidden_states", ")", "\n", "pepper", "=", "torch", ".", "min", "(", "encoder_hidden_states", ")", "\n", "uniform_noise", "=", "torch", ".", "rand_like", "(", "encoder_hidden_states", ")", "\n", "encoder_hidden_states_detached", "=", "encoder_hidden_states", ".", "detach", "(", ")", "\n", "encoder_hidden_states_detached", "[", "uniform_noise", "+", "self", ".", "impulsive", "/", "2", ">", "1", "]", "=", "salt", "\n", "encoder_hidden_states_detached", "[", "uniform_noise", "-", "self", ".", "impulsive", "/", "2", "<", "0", "]", "=", "pepper", "\n", "del", "uniform_noise", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "# Decode", "\n", "", "decoder_outputs", "=", "self", ".", "decoder", "(", "\n", "input_ids", "=", "decoder_input_ids", ",", "\n", "attention_mask", "=", "decoder_attention_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "attention_mask", ",", "\n", "inputs_embeds", "=", "decoder_inputs_embeds", ",", "\n", "labels", "=", "labels", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", "**", "kwargs_decoder", ",", "\n", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "decoder_outputs", "+", "encoder_outputs", "\n", "\n", "", "return", "Seq2SeqLMOutput", "(", "\n", "loss", "=", "decoder_outputs", ".", "loss", ",", "\n", "logits", "=", "decoder_outputs", ".", "logits", ",", "\n", "past_key_values", "=", "decoder_outputs", ".", "past_key_values", ",", "\n", "decoder_hidden_states", "=", "decoder_outputs", ".", "hidden_states", ",", "\n", "decoder_attentions", "=", "decoder_outputs", ".", "attentions", ",", "\n", "cross_attentions", "=", "decoder_outputs", ".", "cross_attentions", ",", "\n", "encoder_last_hidden_state", "=", "encoder_outputs", ".", "last_hidden_state", ",", "\n", "encoder_hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "encoder_attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.prepare_inputs_for_generation": [[491, 505], ["noisy_model.NoisyEncoderDecoderModel.decoder.prepare_inputs_for_generation"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.prepare_inputs_for_generation"], ["", "def", "prepare_inputs_for_generation", "(", "\n", "self", ",", "input_ids", ",", "past", "=", "None", ",", "attention_mask", "=", "None", ",", "use_cache", "=", "None", ",", "encoder_outputs", "=", "None", ",", "**", "kwargs", "\n", ")", ":", "\n", "        ", "decoder_inputs", "=", "self", ".", "decoder", ".", "prepare_inputs_for_generation", "(", "input_ids", ",", "past", "=", "past", ")", "\n", "decoder_attention_mask", "=", "decoder_inputs", "[", "\"attention_mask\"", "]", "if", "\"attention_mask\"", "in", "decoder_inputs", "else", "None", "\n", "input_dict", "=", "{", "\n", "\"attention_mask\"", ":", "attention_mask", ",", "\n", "\"decoder_attention_mask\"", ":", "decoder_attention_mask", ",", "\n", "\"decoder_input_ids\"", ":", "decoder_inputs", "[", "\"input_ids\"", "]", ",", "\n", "\"encoder_outputs\"", ":", "encoder_outputs", ",", "\n", "\"past_key_values\"", ":", "decoder_inputs", "[", "\"past_key_values\"", "]", ",", "\n", "\"use_cache\"", ":", "use_cache", ",", "\n", "}", "\n", "return", "input_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.resize_token_embeddings": [[506, 509], ["NotImplementedError"], "methods", ["None"], ["", "def", "resize_token_embeddings", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"Resizing the embedding layers via the EncoderDecoderModel directly is not supported.\"", "\n", "\"Please use the respective methods of the wrapped objects (model.encoder.resize_token_embeddings(...) or model.decoder.resize_token_embeddings(...))\"", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel._reorder_cache": [[512, 515], ["noisy_model.NoisyEncoderDecoderModel.decoder._reorder_cache"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel._reorder_cache"], ["", "def", "_reorder_cache", "(", "self", ",", "past", ",", "beam_idx", ")", ":", "\n", "# apply decoder cache reordering here", "\n", "        ", "return", "self", ".", "decoder", ".", "_reorder_cache", "(", "past", ",", "beam_idx", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_basts_data": [[72, 82], ["datasets.Dataset.from_dict", "open", "open", "line.strip().lower", "line.strip().lower", "pathlib.Path.cwd", "line.strip", "line.strip"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "_load_basts_data", "(", "lang", ",", "split", ")", ":", "\n", "        ", "assert", "lang", "in", "(", "'java'", ",", "'python'", ")", "\n", "assert", "split", "in", "(", "'train'", ",", "'valid'", ",", "'test'", ")", "\n", "data_dir", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'data'", "/", "'basts'", "/", "lang", "\n", "with", "open", "(", "data_dir", "/", "split", "/", "f'{split}.token.code'", ")", "as", "file", ":", "\n", "            ", "sources", "=", "[", "line", ".", "strip", "(", ")", ".", "lower", "(", ")", "for", "line", "in", "file", "]", "\n", "", "with", "open", "(", "data_dir", "/", "split", "/", "f'{split}.token.nl'", ")", "as", "file", ":", "\n", "            ", "targets", "=", "[", "line", ".", "strip", "(", ")", ".", "lower", "(", ")", "for", "line", "in", "file", "]", "\n", "", "return", "Dataset", ".", "from_dict", "(", "{", "'snippets'", ":", "sources", ",", "'comments'", ":", "targets", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_sit_data": [[83, 93], ["datasets.Dataset.from_dict", "open", "open", "line.strip().lower", "line.strip().lower", "pathlib.Path.cwd", "line.strip", "line.strip"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_load_sit_data", "(", "lang", ",", "split", ")", ":", "\n", "        ", "assert", "lang", "in", "(", "'java'", ",", "'python'", ")", "\n", "assert", "split", "in", "(", "'train'", ",", "'valid'", ",", "'test'", ")", "\n", "data_dir", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'data'", "/", "'sit'", "/", "lang", "\n", "with", "open", "(", "data_dir", "/", "f'{split}.token.code'", ")", "as", "file", ":", "\n", "            ", "sources", "=", "[", "line", ".", "strip", "(", ")", ".", "lower", "(", ")", "for", "line", "in", "file", "]", "\n", "", "with", "open", "(", "data_dir", "/", "f'{split}.token.nl'", ")", "as", "file", ":", "\n", "            ", "targets", "=", "[", "line", ".", "strip", "(", ")", ".", "lower", "(", ")", "for", "line", "in", "file", "]", "\n", "", "return", "Dataset", ".", "from_dict", "(", "{", "'snippets'", ":", "sources", ",", "'comments'", ":", "targets", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_gh_data": [[94, 134], ["datasets.Dataset.from_dict.map", "train_dataset.map.map.map", "valid_dataset.map.map.map", "datasets.Dataset.from_dict", "datasets.load_dataset", "datasets.load_dataset", "datasets.load_dataset", "str().lower", "str().lower", "str().lower", "str().lower", "str().lower", "str().lower", "str", "str", "str", "str", "str", "str"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_load_gh_data", "(", "lang", ",", "split", ")", ":", "\n", "        ", "assert", "lang", "in", "(", "'java'", ",", "'python'", ")", "\n", "# assert split in ('train', 'validation', 'test')", "\n", "assert", "split", "in", "(", "'train'", ",", "'valid'", ")", "\n", "# we merge <train + validation> as <new train> and see <test> as <new test>", "\n", "if", "split", "==", "'valid'", ":", "\n", "            ", "split_dataset", "=", "load_dataset", "(", "\"code_x_glue_ct_code_to_text\"", ",", "lang", ")", "[", "'test'", "]", "\n", "split_dataset", "=", "split_dataset", ".", "map", "(", "\n", "lambda", "sample", ":", "{", "\n", "# 'code_tokens' is much better than 'code'", "\n", "'snippets'", ":", "str", "(", "' '", ".", "join", "(", "sample", "[", "'code_tokens'", "]", ")", ")", ".", "lower", "(", ")", ",", "\n", "# 'docstring_tokens' is much better than 'docstring'", "\n", "'comments'", ":", "str", "(", "' '", ".", "join", "(", "sample", "[", "'docstring_tokens'", "]", ")", ")", ".", "lower", "(", ")", ",", "\n", "}", "\n", ")", "\n", "", "else", ":", "\n", "            ", "train_dataset", "=", "load_dataset", "(", "\"code_x_glue_ct_code_to_text\"", ",", "lang", ")", "[", "'train'", "]", "\n", "train_dataset", "=", "train_dataset", ".", "map", "(", "\n", "lambda", "sample", ":", "{", "\n", "# 'code_tokens' is much better than 'code'", "\n", "'snippets'", ":", "str", "(", "' '", ".", "join", "(", "sample", "[", "'code_tokens'", "]", ")", ")", ".", "lower", "(", ")", ",", "\n", "# 'docstring_tokens' is much better than 'docstring'", "\n", "'comments'", ":", "str", "(", "' '", ".", "join", "(", "sample", "[", "'docstring_tokens'", "]", ")", ")", ".", "lower", "(", ")", ",", "\n", "}", "\n", ")", "\n", "valid_dataset", "=", "load_dataset", "(", "\"code_x_glue_ct_code_to_text\"", ",", "lang", ")", "[", "'validation'", "]", "\n", "valid_dataset", "=", "valid_dataset", ".", "map", "(", "\n", "lambda", "sample", ":", "{", "\n", "# 'code_tokens' is much better than 'code'", "\n", "'snippets'", ":", "str", "(", "' '", ".", "join", "(", "sample", "[", "'code_tokens'", "]", ")", ")", ".", "lower", "(", ")", ",", "\n", "# 'docstring_tokens' is much better than 'docstring'", "\n", "'comments'", ":", "str", "(", "' '", ".", "join", "(", "sample", "[", "'docstring_tokens'", "]", ")", ")", ".", "lower", "(", ")", ",", "\n", "}", "\n", ")", "\n", "split_dataset", "=", "Dataset", ".", "from_dict", "(", "{", "\n", "'snippets'", ":", "train_dataset", "[", "'snippets'", "]", "+", "valid_dataset", "[", "'snippets'", "]", ",", "\n", "'comments'", ":", "train_dataset", "[", "'comments'", "]", "+", "valid_dataset", "[", "'comments'", "]", ",", "\n", "}", ")", "\n", "", "return", "split_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_so_data": [[135, 147], ["datasets.Dataset.from_dict", "open", "open", "line.strip().lower", "line.strip().lower", "line.strip", "line.strip", "pathlib.Path.cwd"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_load_so_data", "(", "lang", ",", "split", ")", ":", "\n", "        ", "assert", "lang", "in", "(", "'java'", ",", "'python'", ")", "\n", "assert", "split", "in", "(", "'train'", ",", "'valid'", ")", "\n", "split", "=", "'val'", "if", "split", "==", "'valid'", "else", "split", "\n", "# split: train / val", "\n", "data_dir", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'data'", "/", "'so'", "/", "'pair'", "/", "lang", "\n", "with", "open", "(", "data_dir", "/", "f'{split}.src'", ")", "as", "file", ":", "\n", "            ", "sources", "=", "[", "line", ".", "strip", "(", ")", ".", "lower", "(", ")", "for", "line", "in", "file", "]", "\n", "", "with", "open", "(", "data_dir", "/", "f'{split}.tgt'", ")", "as", "file", ":", "\n", "            ", "targets", "=", "[", "line", ".", "strip", "(", ")", ".", "lower", "(", ")", "for", "line", "in", "file", "]", "\n", "", "return", "Dataset", ".", "from_dict", "(", "{", "'snippets'", ":", "sources", ",", "'comments'", ":", "targets", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.load_data": [[148, 170], ["t2t.DataLoader._load_basts_data", "t2t.DataLoader._load_sit_data", "t2t.DataLoader._load_so_data", "t2t.DataLoader._load_so_data", "t2t.DataLoader._load_gh_data", "range", "range", "t2t.DataLoader._load_so_data"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_basts_data", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_sit_data", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_so_data", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_so_data", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_gh_data", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader._load_so_data"], ["", "@", "staticmethod", "\n", "def", "load_data", "(", "label", ",", "lang", ",", "split", ",", "dryrun", "=", "False", ")", ":", "\n", "        ", "assert", "label", "in", "(", "'basts'", ",", "'sit'", ",", "'gh'", ",", "'so'", ")", "\n", "assert", "lang", "in", "(", "'java'", ",", "'python'", ")", "\n", "assert", "split", "in", "(", "'train'", ",", "'valid'", ",", "'test'", ")", "\n", "# split: train / valid / test", "\n", "# dataset: basts sit gh so", "\n", "if", "label", "==", "'basts'", ":", "\n", "            ", "dataset", "=", "DataLoader", ".", "_load_basts_data", "(", "lang", ",", "split", ")", "\n", "", "elif", "label", "==", "'sit'", ":", "\n", "            ", "dataset", "=", "DataLoader", ".", "_load_sit_data", "(", "lang", ",", "split", ")", "\n", "", "elif", "label", "==", "'gh'", ":", "\n", "            ", "dataset", "=", "DataLoader", ".", "_load_gh_data", "(", "lang", ",", "split", ")", "\n", "", "elif", "label", "==", "'so'", ":", "\n", "            ", "dataset", "=", "DataLoader", ".", "_load_so_data", "(", "lang", ",", "split", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "# print(dataset['snippets'][:3])", "\n", "# print(dataset['comments'][:3])", "\n", "", "if", "dryrun", ":", "\n", "            ", "dataset", "=", "dataset", ".", "select", "(", "range", "(", "800", ")", ")", "if", "split", "==", "'train'", "else", "dataset", ".", "select", "(", "range", "(", "100", ")", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.clean_dataset": [[171, 190], ["dataset.map", "code_data.split", "list", "text_data.split", "list", "filter", "filter", "t2t.DataLoader.clean_dataset._clean_code"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "clean_dataset", "(", "dataset", ")", ":", "\n", "        ", "def", "_clean_code", "(", "code_data", ")", ":", "\n", "            ", "code_tokens", "=", "code_data", ".", "split", "(", ")", "\n", "code_tokens", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", ".", "isalnum", "(", ")", ",", "code_tokens", ")", ")", "\n", "return", "' '", ".", "join", "(", "code_tokens", ")", "\n", "\n", "", "def", "_clean_text", "(", "text_data", ")", ":", "\n", "            ", "text_tokens", "=", "text_data", ".", "split", "(", ")", "\n", "text_tokens", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", ".", "isalnum", "(", ")", ",", "text_tokens", ")", ")", "\n", "return", "' '", ".", "join", "(", "text_tokens", ")", "\n", "\n", "", "cleaned_dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "sample", ":", "{", "\n", "'snippets'", ":", "_clean_code", "(", "sample", "[", "'snippets'", "]", ")", ",", "\n", "'comments'", ":", "_clean_text", "(", "sample", "[", "'comments'", "]", ")", ",", "\n", "}", "\n", ")", "\n", "return", "cleaned_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.refine_dataset": [[191, 215], ["dataset.map", "src.split", "tgt.split", "set", "set", "t2t.DataLoader.refine_dataset._refine"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "refine_dataset", "(", "dataset", ",", "task", ")", ":", "\n", "        ", "assert", "task", "in", "(", "'ca'", ",", "'ce'", ",", "'ci'", ")", "\n", "\n", "def", "_refine", "(", "src", ",", "tgt", ")", ":", "\n", "            ", "src_tokens", "=", "src", ".", "split", "(", ")", "\n", "tgt_tokens", "=", "tgt", ".", "split", "(", ")", "\n", "common_tokens", "=", "set", "(", "src_tokens", ")", "&", "set", "(", "tgt_tokens", ")", "\n", "if", "task", "==", "'ca'", ":", "\n", "                ", "return", "' '", ".", "join", "(", "[", "'@+@'", "if", "token", "in", "common_tokens", "else", "'@-@'", "for", "token", "in", "tgt_tokens", "]", ")", "\n", "", "elif", "task", "==", "'ce'", ":", "\n", "                ", "return", "' '", ".", "join", "(", "[", "'@+@'", "if", "token", "in", "common_tokens", "else", "token", "for", "token", "in", "tgt_tokens", "]", ")", "\n", "", "elif", "task", "==", "'ci'", ":", "\n", "                ", "return", "' '", ".", "join", "(", "[", "'@-@'", "if", "token", "not", "in", "common_tokens", "else", "token", "for", "token", "in", "tgt_tokens", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "", "refined_dataset", "=", "dataset", ".", "map", "(", "\n", "lambda", "sample", ":", "{", "\n", "'snippets'", ":", "sample", "[", "'snippets'", "]", ",", "\n", "'comments'", ":", "_refine", "(", "sample", "[", "'snippets'", "]", ",", "sample", "[", "'comments'", "]", ")", ",", "\n", "}", "\n", ")", "\n", "return", "refined_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.define_model": [[218, 292], ["transformers.AutoTokenizer.from_pretrained", "model_constructor.from_pretrained", "model_constructor.from_encoder_decoder_pretrained.encoder.base_model.named_parameters", "model_constructor.from_encoder_decoder_pretrained.decoder.base_model.named_parameters", "model_constructor.from_encoder_decoder_pretrained", "print", "print", "print", "print", "range", "print", "range", "model_constructor.from_encoder_decoder_pretrained", "print", "logger.info", "parameter.size", "parameter.size", "print", "print", "model_constructor.from_encoder_decoder_pretrained"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_encoder_decoder_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_encoder_decoder_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_encoder_decoder_pretrained"], ["    ", "@", "staticmethod", "\n", "def", "define_model", "(", "load_path", "=", "None", ")", ":", "\n", "        ", "encoder_tag", "=", "args", ".", "encoder", "\n", "decoder_tag", "=", "args", ".", "decoder", "\n", "encoder_url", "=", "tag_table", "[", "encoder_tag", "]", "\n", "decoder_url", "=", "tag_table", "[", "decoder_tag", "]", "\n", "\n", "if", "args", ".", "orz", "==", "'adamo'", ":", "\n", "            ", "model_constructor", "=", "EncoderDecoderModel", "\n", "", "elif", "args", ".", "orz", "==", "'noisy'", ":", "\n", "            ", "from", "noisy_model", "import", "NoisyEncoderDecoderModel", "\n", "model_constructor", "=", "NoisyEncoderDecoderModel", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "if", "load_path", "is", "not", "None", ":", "\n", "            ", "model", "=", "model_constructor", ".", "from_pretrained", "(", "load_path", ")", "\n", "", "elif", "job", ".", "af_epoch", "==", "0", ":", "\n", "            ", "model", "=", "model_constructor", ".", "from_encoder_decoder_pretrained", "(", "\n", "encoder_url", ",", "decoder_url", ",", "gaussian", "=", "args", ".", "gaussian", ",", "impulsive", "=", "args", ".", "impulsive", ")", "\n", "", "else", ":", "\n", "            ", "exp_prefix", "=", "(", "'_'", "if", "args", ".", "dryrun", "else", "''", ")", "+", "args", ".", "mark", "\n", "encoder_label", "=", "f'{exp_prefix}{args.lang}_{encoder_tag}_af+{job.af_data}'", "\n", "decoder_label", "=", "f'{exp_prefix}{args.lang}_{decoder_tag}_af+{job.af_data}'", "\n", "# use 'relative path' instead of 'absolute path'", "\n", "encoder_path", "=", "encoder_url", "if", "job", ".", "af_mode", "==", "'clm'", "else", "f'../models/pretrained{args.power}/{encoder_label}/'", "\n", "decoder_path", "=", "decoder_url", "if", "job", ".", "af_mode", "==", "'mlm'", "else", "f'../models/pretrained{args.power}/{decoder_label}/'", "\n", "# encoder_path = Path.cwd().parent / 'models' / 'pretrained' / encoder_label", "\n", "# decoder_path = Path.cwd().parent / 'models' / 'pretrained' / decoder_label", "\n", "print", "(", "encoder_path", ")", "\n", "print", "(", "decoder_path", ")", "\n", "print", "(", "'+'", "*", "16", ")", "\n", "try", ":", "\n", "                ", "model", "=", "model_constructor", ".", "from_encoder_decoder_pretrained", "(", "encoder_path", ",", "decoder_path", ")", "\n", "print", "(", "'load checkpoints smoothly'", ")", "\n", "logger", ".", "info", "(", "'load checkpoints smoothly'", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "print", "(", "'!'", "*", "9", ")", "\n", "print", "(", "e", ")", "\n", "model", "=", "model_constructor", ".", "from_encoder_decoder_pretrained", "(", "encoder_url", ",", "decoder_url", ")", "\n", "\n", "", "", "if", "args", ".", "orz", "==", "'freeze'", ":", "# seems not so good", "\n", "            ", "for", "name", ",", "parameter", "in", "model", ".", "encoder", ".", "base_model", ".", "named_parameters", "(", ")", ":", "\n", "                ", "print", "(", "name", ",", "parameter", ".", "size", "(", ")", ")", "\n", "if", "'intermediate.'", "in", "name", ":", "# FeedForward", "\n", "                    ", "continue", "\n", "", "if", "'LayerNorm.'", "in", "name", ":", "# LayerNorm", "\n", "                    ", "continue", "\n", "", "for", "layer_id", "in", "range", "(", "12", ")", ":", "# or 11 or 10", "\n", "                    ", "if", "f'layer.{layer_id}.'", "in", "name", ":", "\n", "                        ", "parameter", ".", "requires_grad", "=", "False", "\n", "break", "\n", "", "", "", "for", "name", ",", "parameter", "in", "model", ".", "decoder", ".", "base_model", ".", "named_parameters", "(", ")", ":", "\n", "                ", "print", "(", "name", ",", "parameter", ".", "size", "(", ")", ")", "\n", "if", "'mlp.'", "in", "name", ":", "# FeedForward", "\n", "                    ", "continue", "\n", "", "if", "'ln_1.'", "in", "name", "or", "'ln_2.'", "in", "name", ":", "# LayerNorm", "\n", "                    ", "continue", "\n", "", "for", "layer_id", "in", "range", "(", "12", ")", ":", "# or 11 or 10", "\n", "                    ", "if", "f'h.{layer_id}.'", "in", "name", ":", "\n", "                        ", "parameter", ".", "requires_grad", "=", "False", "\n", "break", "\n", "\n", "", "", "", "", "model", ".", "config", ".", "min_length", "=", "4", "\n", "model", ".", "config", ".", "max_length", "=", "64", "\n", "model", ".", "config", ".", "vocab_size", "=", "model", ".", "config", ".", "encoder", ".", "vocab_size", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "encoder_url", ",", "model_max_length", "=", "input_max_length", ")", "\n", "if", "tokenizer", ".", "pad_token", "is", "None", ":", "\n", "            ", "tokenizer", ".", "pad_token", "=", "tokenizer", ".", "eos_token", "\n", "model", ".", "config", ".", "pad_token_id", "=", "model", ".", "config", ".", "eos_token_id", "\n", "", "model", ".", "config", ".", "eos_token_id", "=", "tokenizer", ".", "sep_token_id", "\n", "model", ".", "config", ".", "pad_token_id", "=", "tokenizer", ".", "pad_token_id", "\n", "model", ".", "config", ".", "decoder_start_token_id", "=", "tokenizer", ".", "cls_token_id", "\n", "return", "model", ",", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.train_model": [[294, 369], ["model.to", "model.train", "transformers.DataCollatorForLanguageModeling", "t2t.DataLoader.load_data", "t2t.DataLoader.load_data", "train_dataset.map.map.map", "train_dataset.map.map.set_format", "valid_dataset.map.map.map", "valid_dataset.map.map.set_format", "transformers.TrainingArguments", "transformers.Trainer", "print", "logger.info", "tokenizer", "tokenizer.input_ids.copy", "all", "transformers.Trainer.train", "str", "print", "print", "transformers.Trainer.train", "len", "pathlib.Path.cwd"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.load_data", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.load_data"], ["", "@", "staticmethod", "\n", "def", "train_model", "(", "option", ",", "tokenizer", ",", "model", ",", "data_type", ",", "objective", ")", ":", "\n", "        ", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "train", "(", ")", "\n", "\n", "mlm", "=", "True", "if", "objective", "==", "'mlm'", "else", "False", "\n", "data_collator", "=", "DataCollatorForLanguageModeling", "(", "tokenizer", "=", "tokenizer", ",", "mlm", "=", "mlm", ",", "mlm_probability", "=", "0.15", ")", "\n", "train_dataset", "=", "DataLoader", ".", "load_data", "(", "option", ".", "dataset_label", ",", "args", ".", "lang", ",", "'train'", ",", "args", ".", "dryrun", ")", "\n", "valid_dataset", "=", "DataLoader", ".", "load_data", "(", "option", ".", "dataset_label", ",", "args", ".", "lang", ",", "'valid'", ",", "args", ".", "dryrun", ")", "\n", "# train_dataset = DataLoader.clean_dataset(train_dataset)", "\n", "# valid_dataset = DataLoader.clean_dataset(valid_dataset)", "\n", "\n", "def", "_map_to_trainer_inputs", "(", "batch", ")", ":", "\n", "# Tokenizer will automatically set [BOS] <text> [EOS]", "\n", "            ", "max_length", "=", "{", "'snippets'", ":", "input_max_length", ",", "'comments'", ":", "64", "}", "[", "data_type", "]", "\n", "inputs", "=", "tokenizer", "(", "batch", "[", "data_type", "]", ",", "padding", "=", "'max_length'", ",", "truncation", "=", "True", ",", "max_length", "=", "max_length", ")", "\n", "batch", "[", "'input_ids'", "]", "=", "inputs", ".", "input_ids", "\n", "batch", "[", "'labels'", "]", "=", "inputs", ".", "input_ids", ".", "copy", "(", ")", "\n", "batch", "[", "'labels'", "]", "=", "[", "\n", "[", "-", "100", "if", "token", "==", "tokenizer", ".", "pad_token_id", "else", "token", "for", "token", "in", "labels", "]", "for", "labels", "in", "batch", "[", "'labels'", "]", "\n", "]", "\n", "\n", "assert", "all", "(", "[", "len", "(", "x", ")", "==", "max_length", "for", "x", "in", "inputs", ".", "input_ids", "]", ")", "\n", "return", "batch", "\n", "\n", "", "train_dataset", "=", "train_dataset", ".", "map", "(", "\n", "_map_to_trainer_inputs", ",", "\n", "batched", "=", "True", ",", "\n", "batch_size", "=", "128", ",", "\n", "remove_columns", "=", "[", "'snippets'", ",", "'comments'", "]", ",", "\n", ")", "\n", "train_dataset", ".", "set_format", "(", "\n", "type", "=", "'torch'", ",", "\n", "columns", "=", "[", "'input_ids'", ",", "'labels'", "]", ",", "\n", ")", "\n", "valid_dataset", "=", "valid_dataset", ".", "map", "(", "\n", "_map_to_trainer_inputs", ",", "\n", "batched", "=", "True", ",", "\n", "batch_size", "=", "128", ",", "\n", "remove_columns", "=", "[", "'snippets'", ",", "'comments'", "]", ",", "\n", ")", "\n", "valid_dataset", ".", "set_format", "(", "\n", "type", "=", "'torch'", ",", "\n", "columns", "=", "[", "'input_ids'", ",", "'labels'", "]", ",", "\n", ")", "\n", "\n", "# instantiate trainer", "\n", "dump_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "f'checkpoints{args.power}'", "/", "option", ".", "dump_label", "\n", "training_args", "=", "TrainingArguments", "(", "\n", "seed", "=", "42", ",", "\n", "report_to", "=", "'none'", ",", "\n", "# save_total_limit=1,", "\n", "save_strategy", "=", "'epoch'", ",", "\n", "output_dir", "=", "str", "(", "dump_path", ")", ",", "\n", "# load_best_model_at_end=True,", "\n", "per_device_train_batch_size", "=", "32", "if", "mlm", "else", "64", ",", "\n", "per_device_eval_batch_size", "=", "64", "if", "mlm", "else", "128", ",", "\n", "num_train_epochs", "=", "option", ".", "epoch_num", ",", "\n", ")", "\n", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "train_dataset", "=", "train_dataset", ",", "\n", "eval_dataset", "=", "valid_dataset", ",", "\n", ")", "\n", "print", "(", "'+++++++++ model training +++++++++'", ")", "\n", "logger", ".", "info", "(", "'+++++++++ model training +++++++++'", ")", "\n", "try", ":", "\n", "            ", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "True", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "'!'", "*", "9", ")", "\n", "print", "(", "e", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.tune_model": [[371, 454], ["model.to", "model.train", "t2t.DataLoader.load_data", "t2t.DataLoader.load_data", "t2t.DataLoader.refine_dataset", "t2t.DataLoader.refine_dataset", "t2t.DataLoader.refine_dataset", "t2t.DataLoader.refine_dataset", "transformers.Seq2SeqTrainingArguments", "transformers.Seq2SeqTrainer", "print", "print", "logger.info", "logger.info", "tokenizer", "tokenizer", "tokenizer.input_ids.copy", "all", "all", "t2t.DataLoader.refine_dataset", "t2t.DataLoader.refine_dataset", "time.strftime", "time.strftime", "transformers.Seq2SeqTrainer.train", "str", "time.localtime", "time.localtime", "print", "print", "transformers.Seq2SeqTrainer.train", "time.time", "time.time", "len", "len", "pathlib.Path.cwd", "pathlib.Path.cwd"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.load_data", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.load_data", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.refine_dataset", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.refine_dataset", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.refine_dataset", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.refine_dataset", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.refine_dataset", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.refine_dataset"], ["", "", "@", "staticmethod", "\n", "def", "tune_model", "(", "option", ",", "tokenizer", ",", "model", ")", ":", "\n", "        ", "def", "_map_to_encoder_decoder_inputs", "(", "batch", ")", ":", "\n", "# Tokenizer will automatically set [BOS] <text> [EOS]", "\n", "            ", "inputs", "=", "tokenizer", "(", "batch", "[", "'snippets'", "]", ",", "padding", "=", "'max_length'", ",", "truncation", "=", "True", ",", "max_length", "=", "input_max_length", ")", "\n", "outputs", "=", "tokenizer", "(", "batch", "[", "'comments'", "]", ",", "padding", "=", "'max_length'", ",", "truncation", "=", "True", ",", "max_length", "=", "64", ")", "\n", "batch", "[", "'input_ids'", "]", "=", "inputs", ".", "input_ids", "\n", "batch", "[", "'attention_mask'", "]", "=", "inputs", ".", "attention_mask", "\n", "batch", "[", "'decoder_input_ids'", "]", "=", "outputs", ".", "input_ids", "\n", "batch", "[", "'labels'", "]", "=", "outputs", ".", "input_ids", ".", "copy", "(", ")", "\n", "batch", "[", "'labels'", "]", "=", "[", "\n", "[", "-", "100", "if", "token", "==", "tokenizer", ".", "pad_token_id", "else", "token", "for", "token", "in", "labels", "]", "for", "labels", "in", "batch", "[", "'labels'", "]", "\n", "]", "\n", "batch", "[", "'decoder_attention_mask'", "]", "=", "outputs", ".", "attention_mask", "\n", "\n", "assert", "all", "(", "[", "len", "(", "x", ")", "==", "input_max_length", "for", "x", "in", "inputs", ".", "input_ids", "]", ")", "\n", "assert", "all", "(", "[", "len", "(", "x", ")", "==", "64", "for", "x", "in", "outputs", ".", "input_ids", "]", ")", "\n", "return", "batch", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "train", "(", ")", "\n", "# load dataset but distinguish bf and cf", "\n", "train_dataset", "=", "DataLoader", ".", "load_data", "(", "option", ".", "dataset_label", ",", "args", ".", "lang", ",", "'train'", ",", "args", ".", "dryrun", ")", "\n", "valid_dataset", "=", "DataLoader", ".", "load_data", "(", "option", ".", "dataset_label", ",", "args", ".", "lang", ",", "'valid'", ",", "args", ".", "dryrun", ")", "\n", "if", "'_bf'", "in", "option", ".", "dump_label", "and", "'_cf'", "not", "in", "option", ".", "dump_label", ":", "\n", "            ", "train_dataset", "=", "DataLoader", ".", "refine_dataset", "(", "train_dataset", ",", "job", ".", "bf_task", ")", "\n", "valid_dataset", "=", "DataLoader", ".", "refine_dataset", "(", "valid_dataset", ",", "job", ".", "bf_task", ")", "\n", "# map the training dataset", "\n", "", "train_dataset", "=", "train_dataset", ".", "map", "(", "\n", "_map_to_encoder_decoder_inputs", ",", "\n", "batched", "=", "True", ",", "\n", "batch_size", "=", "256", ",", "\n", "remove_columns", "=", "[", "'snippets'", ",", "'comments'", "]", ",", "\n", ")", "\n", "train_dataset", ".", "set_format", "(", "\n", "type", "=", "'torch'", ",", "\n", "columns", "=", "[", "'input_ids'", ",", "'attention_mask'", ",", "'decoder_input_ids'", ",", "'decoder_attention_mask'", ",", "'labels'", "]", ",", "\n", ")", "\n", "# map the validation dataset", "\n", "valid_dataset", "=", "valid_dataset", ".", "map", "(", "\n", "_map_to_encoder_decoder_inputs", ",", "\n", "batched", "=", "True", ",", "\n", "batch_size", "=", "256", ",", "\n", "remove_columns", "=", "[", "'snippets'", ",", "'comments'", "]", ",", "\n", ")", "\n", "valid_dataset", ".", "set_format", "(", "\n", "type", "=", "'torch'", ",", "\n", "columns", "=", "[", "'input_ids'", ",", "'attention_mask'", ",", "'decoder_input_ids'", ",", "'decoder_attention_mask'", ",", "'labels'", "]", ",", "\n", ")", "\n", "# instantiate trainer", "\n", "if", "job", ".", "af_epoch", ">", "0", ":", "\n", "            ", "dump_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "f'checkpoints{args.power}'", "/", "option", ".", "dump_label", "\n", "", "else", ":", "\n", "            ", "dump_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "'checkpoints'", "/", "option", ".", "dump_label", "\n", "", "training_args", "=", "Seq2SeqTrainingArguments", "(", "\n", "seed", "=", "42", ",", "\n", "report_to", "=", "'none'", ",", "\n", "# save_total_limit=10,", "\n", "save_strategy", "=", "'epoch'", ",", "\n", "output_dir", "=", "str", "(", "dump_path", ")", ",", "\n", "predict_with_generate", "=", "True", ",", "\n", "# load_best_model_at_end=True,", "\n", "per_device_train_batch_size", "=", "32", ",", "\n", "per_device_eval_batch_size", "=", "64", ",", "\n", "num_train_epochs", "=", "option", ".", "epoch_num", ",", "\n", ")", "\n", "trainer", "=", "Seq2SeqTrainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "train_dataset", "=", "train_dataset", ",", "\n", "eval_dataset", "=", "valid_dataset", ",", "\n", ")", "\n", "print", "(", "'+++++++++ model tuning +++++++++'", ")", "\n", "print", "(", "time", ".", "strftime", "(", "'%Y-%m-%d %H:%M:%S'", ",", "time", ".", "localtime", "(", "time", ".", "time", "(", ")", ")", ")", ")", "\n", "logger", ".", "info", "(", "'+++++++++ model tuning +++++++++'", ")", "\n", "logger", ".", "info", "(", "time", ".", "strftime", "(", "'%Y-%m-%d %H:%M:%S'", ",", "time", ".", "localtime", "(", "time", ".", "time", "(", ")", ")", ")", ")", "\n", "try", ":", "\n", "            ", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "True", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "'!'", "*", "9", ")", "\n", "print", "(", "e", ")", "\n", "trainer", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.infer_model": [[455, 498], ["model.to", "model.eval", "t2t.DataLoader.load_data", "list", "torch.utils.data.BatchSampler", "print", "logger.info", "print", "print", "metric.Metric.report", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "tokenizer", "model.generate", "tokenizer.batch_decode", "list.extend", "open", "chunked_input_dict[].to", "chunked_input_dict[].to", "file.write"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.load_data", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.metric.Metric.report"], ["", "", "@", "staticmethod", "\n", "def", "infer_model", "(", "tokenizer", ",", "model", ",", "file_path", "=", "None", ")", ":", "\n", "        ", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "test_dataset", "=", "DataLoader", ".", "load_data", "(", "job", ".", "cf_data", ",", "args", ".", "lang", ",", "'test'", ",", "args", ".", "dryrun", ")", "\n", "predictions", "=", "list", "(", ")", "\n", "if", "args", ".", "dryrun", ":", "\n", "            ", "sampler", "=", "SequentialSampler", "(", "test_dataset", "[", "'snippets'", "]", "[", ":", "100", "]", ")", "\n", "", "else", ":", "\n", "            ", "sampler", "=", "SequentialSampler", "(", "test_dataset", "[", "'snippets'", "]", ")", "\n", "", "chunked_indexes", "=", "BatchSampler", "(", "sampler", ",", "batch_size", "=", "64", ",", "drop_last", "=", "False", ")", "\n", "print", "(", "'+++++++++ model inferring +++++++++'", ")", "\n", "logger", ".", "info", "(", "'+++++++++ model inferring +++++++++'", ")", "\n", "for", "indexes", "in", "chunked_indexes", ":", "\n", "            ", "chunked_data", "=", "[", "test_dataset", "[", "'snippets'", "]", "[", "index", "]", "for", "index", "in", "indexes", "]", "\n", "chunked_input_dict", "=", "tokenizer", "(", "\n", "chunked_data", ",", "\n", "max_length", "=", "input_max_length", ",", "truncation", "=", "True", ",", "\n", "padding", "=", "'max_length'", ",", "return_tensors", "=", "'pt'", ",", "\n", ")", "\n", "chunked_output_ids", "=", "model", ".", "generate", "(", "\n", "input_ids", "=", "chunked_input_dict", "[", "'input_ids'", "]", ".", "to", "(", "device", ")", ",", "\n", "attention_mask", "=", "chunked_input_dict", "[", "'attention_mask'", "]", ".", "to", "(", "device", ")", ",", "\n", "num_beams", "=", "5", ",", "\n", "min_length", "=", "model", ".", "config", ".", "min_length", ",", "\n", "max_length", "=", "model", ".", "config", ".", "max_length", ",", "\n", ")", "\n", "chunked_predictions", "=", "tokenizer", ".", "batch_decode", "(", "chunked_output_ids", ",", "\n", "skip_special_tokens", "=", "True", ",", "\n", "clean_up_tokenization_spaces", "=", "False", ")", "\n", "predictions", ".", "extend", "(", "chunked_predictions", ")", "\n", "\n", "", "references", "=", "test_dataset", "[", "'comments'", "]", "\n", "if", "args", ".", "dryrun", ":", "\n", "            ", "references", "=", "references", "[", ":", "100", "]", "\n", "", "print", "(", "predictions", "[", ":", "5", "]", ")", "\n", "print", "(", "references", "[", ":", "5", "]", ")", "\n", "if", "file_path", "is", "not", "None", "and", "not", "(", "args", ".", "dryrun", "or", "args", ".", "fastrun", ")", ":", "\n", "            ", "with", "open", "(", "file_path", ",", "'w'", ")", "as", "file", ":", "\n", "                ", "for", "datum", "in", "predictions", ":", "\n", "                    ", "file", ".", "write", "(", "f'{datum}\\n'", ")", "\n", "", "", "", "Metric", ".", "report", "(", "predictions", ",", "references", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.run_pretrain": [[499, 530], ["model_path.exists", "transformers.AutoTokenizer.from_pretrained", "t2t.T2T.train_model", "print", "logger.info", "transformers.RobertaForMaskedLM.from_pretrained", "transformers.GPT2LMHeadModel.from_pretrained.base_model.save_pretrained", "transformers.GPT2LMHeadModel.from_pretrained", "print", "print"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.train_model", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained"], ["", "@", "staticmethod", "\n", "def", "run_pretrain", "(", "option", ",", "model_url", ",", "model_path", ",", "data_type", ",", "objective", ")", ":", "\n", "        ", "if", "model_path", ".", "exists", "(", ")", ":", "\n", "            ", "print", "(", "'return directly at run_pretrain'", ")", "\n", "logger", ".", "info", "(", "'return directly at run_pretrain'", ")", "\n", "return", "\n", "", "if", "objective", "==", "'mlm'", ":", "\n", "            ", "assert", "'bert'", "in", "args", ".", "encoder", "\n", "# with pretrained weights", "\n", "model", "=", "RobertaForMaskedLM", ".", "from_pretrained", "(", "model_url", ")", "\n", "# without pretrained weights", "\n", "# model = AutoModelForMaskedLM.from_pretrained(model_url)", "\n", "", "elif", "objective", "==", "'clm'", ":", "\n", "            ", "assert", "'gpt2'", "in", "args", ".", "decoder", "\n", "# with pretrained weights", "\n", "model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "model_url", ")", "\n", "# without pretrained weights", "\n", "# model = AutoModelForCausalLM.from_pretrained(model_url)", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_url", ",", "model_max_length", "=", "input_max_length", ")", "\n", "if", "tokenizer", ".", "pad_token", "is", "None", ":", "\n", "            ", "tokenizer", ".", "pad_token", "=", "tokenizer", ".", "eos_token", "\n", "model", ".", "config", ".", "pad_token_id", "=", "model", ".", "config", ".", "eos_token_id", "\n", "", "T2T", ".", "train_model", "(", "option", ",", "tokenizer", ",", "model", ",", "data_type", ",", "objective", ")", "\n", "try", ":", "\n", "            ", "model", ".", "base_model", ".", "save_pretrained", "(", "model_path", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "'!'", "*", "9", ")", "\n", "print", "(", "e", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.run_pipeline": [[531, 552], ["print", "logger.info", "print", "t2t.T2T.tune_model", "print", "logger.info", "transformers.EncoderDecoderModel.from_pretrained.save_pretrained", "tokenizer.save_pretrained", "t2t.T2T.define_model", "transformers.EncoderDecoderModel.from_pretrained", "print", "t2t.T2T.define_model"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.tune_model", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.define_model", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.noisy_model.NoisyEncoderDecoderModel.from_pretrained", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.define_model"], ["", "", "@", "staticmethod", "\n", "def", "run_pipeline", "(", "option", ",", "load_path", ",", "dump_path", ")", ":", "\n", "# load model", "\n", "        ", "try", ":", "\n", "            ", "model", ",", "tokenizer", "=", "T2T", ".", "define_model", "(", ")", "\n", "model", "=", "EncoderDecoderModel", ".", "from_pretrained", "(", "load_path", ")", "\n", "# tokenizer = AutoTokenizer.from_pretrained(load_path, model_max_length=input_max_length)", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "e", ")", "\n", "model", ",", "tokenizer", "=", "T2T", ".", "define_model", "(", ")", "\n", "# model, tokenizer = T2T.define_model(load_path) if load_path.exists() else T2T.define_model()", "\n", "", "print", "(", "'load the encoder-decoder model'", ")", "\n", "logger", ".", "info", "(", "'load the encoder-decoder model'", ")", "\n", "# train the model", "\n", "print", "(", "'tune the encoder-decoder model'", ")", "\n", "T2T", ".", "tune_model", "(", "option", ",", "tokenizer", ",", "model", ")", "\n", "# save model", "\n", "print", "(", "'save the encoder-decoder model'", ")", "\n", "logger", ".", "info", "(", "'save the encoder-decoder model'", ")", "\n", "model", ".", "save_pretrained", "(", "dump_path", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "dump_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.Mission.train_t2t": [[555, 604], ["t2t.Option", "t2t.T2T.run_pipeline", "t2t.Option", "t2t.T2T.run_pipeline", "t2t.Option", "t2t.T2T.run_pretrain", "t2t.Option", "t2t.T2T.run_pretrain", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.run_pipeline", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.run_pipeline", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.run_pretrain", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.run_pretrain"], ["    ", "@", "staticmethod", "\n", "def", "train_t2t", "(", ")", ":", "\n", "        ", "exp_prefix", "=", "(", "'_'", "if", "args", ".", "dryrun", "else", "''", ")", "+", "args", ".", "mark", "\n", "dump_label", "=", "f'{exp_prefix}{args.orz}_{args.lang}'", "\n", "if", "args", ".", "orz", "==", "'noisy'", ":", "\n", "            ", "dump_label", "+=", "f'_g{args.gaussian}_i{args.impulsive}'", "\n", "", "if", "job", ".", "af_epoch", "==", "job", ".", "bf_epoch", "==", "job", ".", "cf_epoch", "==", "0", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "if", "job", ".", "af_epoch", ">", "0", ":", "\n", "                ", "dump_label", "=", "f'{dump_label}_af+{job.af_data}+{job.af_mode}'", "\n", "encoder_tag", "=", "args", ".", "encoder", "\n", "decoder_tag", "=", "args", ".", "decoder", "\n", "encoder_url", "=", "tag_table", "[", "encoder_tag", "]", "\n", "decoder_url", "=", "tag_table", "[", "decoder_tag", "]", "\n", "encoder_label", "=", "f'{exp_prefix}{args.lang}_{encoder_tag}_af+{job.af_data}'", "\n", "decoder_label", "=", "f'{exp_prefix}{args.lang}_{decoder_tag}_af+{job.af_data}'", "\n", "encoder_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "f'pretrained{args.power}'", "/", "encoder_label", "\n", "decoder_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "f'pretrained{args.power}'", "/", "decoder_label", "\n", "if", "job", ".", "af_mode", "in", "[", "'mlm'", ",", "'both'", "]", ":", "\n", "# pretrain the encoder model on snippets with the mlm objective", "\n", "                    ", "option", "=", "Option", "(", "job", ".", "af_epoch", ",", "job", ".", "af_data", ",", "''", ",", "encoder_label", ")", "\n", "T2T", ".", "run_pretrain", "(", "option", ",", "encoder_url", ",", "encoder_path", ",", "'snippets'", ",", "'mlm'", ")", "\n", "", "if", "job", ".", "af_mode", "in", "[", "'clm'", ",", "'both'", "]", ":", "\n", "# pretrain the decoder model on comments with the clm objective", "\n", "                    ", "option", "=", "Option", "(", "job", ".", "af_epoch", ",", "job", ".", "af_data", ",", "''", ",", "decoder_label", ")", "\n", "T2T", ".", "run_pretrain", "(", "option", ",", "decoder_url", ",", "decoder_path", ",", "'comments'", ",", "'clm'", ")", "\n", "", "", "if", "job", ".", "bf_epoch", ">", "0", ":", "\n", "                ", "load_label", "=", "f'{dump_label}'", "\n", "dump_label", "=", "f'{dump_label}_bf+{job.bf_data}+{job.bf_task}'", "\n", "option", "=", "Option", "(", "job", ".", "bf_epoch", ",", "job", ".", "bf_data", ",", "load_label", ",", "dump_label", ")", "\n", "if", "job", ".", "af_epoch", ">", "0", ":", "\n", "                    ", "load_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "f'pretrained{args.power}'", "/", "option", ".", "load_label", "\n", "dump_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "f'pretrained{args.power}'", "/", "option", ".", "dump_label", "\n", "", "else", ":", "\n", "                    ", "load_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "'pretrained'", "/", "option", ".", "load_label", "\n", "dump_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "'pretrained'", "/", "option", ".", "dump_label", "\n", "", "T2T", ".", "run_pipeline", "(", "option", ",", "load_path", ",", "dump_path", ")", "\n", "", "if", "job", ".", "cf_epoch", ">", "0", ":", "\n", "                ", "load_label", "=", "f'{dump_label}'", "\n", "dump_label", "=", "f'{dump_label}_cf+{job.cf_data}'", "\n", "option", "=", "Option", "(", "job", ".", "cf_epoch", ",", "job", ".", "cf_data", ",", "load_label", ",", "dump_label", ")", "\n", "if", "job", ".", "af_epoch", ">", "0", ":", "\n", "                    ", "load_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "f'pretrained{args.power}'", "/", "option", ".", "load_label", "\n", "dump_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "f'pretrained{args.power}'", "/", "option", ".", "dump_label", "\n", "", "else", ":", "\n", "                    ", "load_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "'pretrained'", "/", "option", ".", "load_label", "\n", "dump_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "'pretrained'", "/", "option", ".", "dump_label", "\n", "", "T2T", ".", "run_pipeline", "(", "option", ",", "load_path", ",", "dump_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.Mission.evaluate_t2t": [[605, 637], ["file_dir.mkdir", "t2t.T2T.define_model", "t2t.T2T.infer_model", "print", "print", "t2t.T2T.define_model", "t2t.T2T.infer_model", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd", "pathlib.Path.cwd"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.define_model", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.infer_model", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.define_model", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.T2T.infer_model"], ["", "", "", "@", "staticmethod", "\n", "def", "evaluate_t2t", "(", ")", ":", "\n", "        ", "exp_prefix", "=", "(", "'_'", "if", "args", ".", "dryrun", "else", "''", ")", "+", "args", ".", "mark", "\n", "dump_label", "=", "f'{exp_prefix}{args.orz}_{args.lang}'", "\n", "if", "args", ".", "orz", "==", "'noisy'", ":", "\n", "            ", "dump_label", "+=", "f'_g{args.gaussian}_i{args.impulsive}'", "\n", "", "if", "job", ".", "af_epoch", ">", "0", ":", "\n", "            ", "file_dir", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'results'", "/", "f'{args.orz}{args.power}'", "\n", "", "else", ":", "\n", "            ", "file_dir", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'results'", "/", "args", ".", "orz", "\n", "", "file_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "if", "job", ".", "af_epoch", "==", "job", ".", "bf_epoch", "==", "job", ".", "cf_epoch", "==", "0", ":", "\n", "            ", "file_path", "=", "file_dir", "/", "f'{dump_label}_{job.cf_data}_0shot.hyp'", "\n", "model", ",", "tokenizer", "=", "T2T", ".", "define_model", "(", ")", "\n", "T2T", ".", "infer_model", "(", "tokenizer", ",", "model", ",", "file_path", ")", "\n", "", "else", ":", "\n", "            ", "if", "job", ".", "af_epoch", ">", "0", ":", "# af", "\n", "                ", "dump_label", "=", "f'{dump_label}_af+{job.af_data}+{job.af_mode}'", "\n", "", "if", "job", ".", "bf_epoch", ">", "0", ":", "# bf", "\n", "                ", "dump_label", "=", "f'{dump_label}_bf+{job.bf_data}+{job.bf_task}'", "\n", "", "if", "job", ".", "cf_epoch", ">", "0", ":", "# cf", "\n", "                ", "dump_label", "=", "f'{dump_label}_cf+{job.cf_data}'", "\n", "\n", "", "print", "(", "'*'", "*", "16", ")", "\n", "print", "(", "'# run evaluation on the dumped model'", ")", "\n", "file_path", "=", "file_dir", "/", "f'{dump_label}.hyp'", "\n", "if", "job", ".", "af_epoch", ">", "0", ":", "\n", "                ", "load_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "f'pretrained{args.power}'", "/", "dump_label", "\n", "", "else", ":", "\n", "                ", "load_path", "=", "Path", ".", "cwd", "(", ")", ".", "parent", "/", "'models'", "/", "'pretrained'", "/", "dump_label", "\n", "", "model", ",", "tokenizer", "=", "T2T", ".", "define_model", "(", "load_path", ")", "\n", "T2T", ".", "infer_model", "(", "tokenizer", ",", "model", ",", "file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.Mission.review_t2t": [[667, 709], ["dict", "list", "print", "prediction_dir.iterdir", "print", "sorted", "str", "prediction_folder.iterdir", "open", "csv.DictWriter", "csv.DictWriter.writeheader", "dict.update", "pathlib.Path().cwd", "prediction_folder.is_dir", "str", "metric.Metric.report", "sorted.append", "print", "csv.DictWriter.writerow", "t2t.DataLoader.load_data", "prediction_file.is_file", "open", "file.readlines", "line.strip", "len", "len", "list", "dict", "pathlib.Path", "zip"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.metric.Metric.report", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.load_data"], ["", "", "@", "staticmethod", "\n", "def", "review_t2t", "(", ")", ":", "\n", "        ", "references_manager", "=", "dict", "(", ")", "\n", "for", "data_value", "in", "[", "'basts'", ",", "'sit'", "]", ":", "\n", "            ", "for", "lang_value", "in", "[", "'java'", ",", "'python'", "]", ":", "\n", "                ", "identifier", "=", "data_value", "+", "':'", "+", "lang_value", "\n", "references", "=", "DataLoader", ".", "load_data", "(", "data_value", ",", "lang_value", ",", "'test'", ")", "[", "'comments'", "]", "\n", "references_manager", ".", "update", "(", "{", "identifier", ":", "references", "}", ")", "\n", "\n", "", "", "evaluations", "=", "list", "(", ")", "\n", "prediction_dir", "=", "Path", "(", ")", ".", "cwd", "(", ")", ".", "parent", "/", "'results'", "\n", "print", "(", "'ready to compute the scores'", ")", "\n", "for", "prediction_folder", "in", "prediction_dir", ".", "iterdir", "(", ")", ":", "\n", "            ", "if", "not", "prediction_folder", ".", "is_dir", "(", ")", ":", "\n", "                ", "continue", "\n", "", "group_value", "=", "str", "(", "prediction_folder", ".", "name", ")", "\n", "for", "prediction_file", "in", "prediction_folder", ".", "iterdir", "(", ")", ":", "\n", "                ", "if", "not", "prediction_file", ".", "is_file", "(", ")", ":", "\n", "                    ", "continue", "\n", "", "tag_value", "=", "str", "(", "prediction_file", ".", "stem", ")", "\n", "data_value", "=", "'basts'", "if", "'basts'", "in", "tag_value", "else", "'sit'", "\n", "lang_value", "=", "'java'", "if", "'java'", "in", "tag_value", "else", "'python'", "\n", "information", "=", "[", "group_value", ",", "data_value", ",", "lang_value", ",", "tag_value", "]", "\n", "with", "open", "(", "prediction_file", ")", "as", "file", ":", "\n", "                    ", "lines", "=", "file", ".", "readlines", "(", ")", "\n", "", "predictions", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "lines", "]", "\n", "references", "=", "references_manager", "[", "data_value", "+", "':'", "+", "lang_value", "]", "\n", "assert", "len", "(", "predictions", ")", "==", "len", "(", "references", ")", "\n", "scores", "=", "Metric", ".", "report", "(", "predictions", ",", "references", ")", "\n", "evaluation", "=", "information", "+", "list", "(", "scores", ")", "\n", "evaluations", ".", "append", "(", "evaluation", ")", "\n", "print", "(", "f'completed {group_value}:{tag_value}'", ")", "\n", "\n", "", "", "print", "(", "'ready to collect the scores'", ")", "\n", "csv_file", "=", "prediction_dir", "/", "'scores.csv'", "\n", "evaluations", "=", "sorted", "(", "evaluations", ",", "key", "=", "lambda", "x", ":", "x", "[", ":", "4", "]", ")", "\n", "header", "=", "[", "'GROUP'", ",", "'DATA'", ",", "'LANG'", ",", "'TAG'", ",", "'C-BLEU'", ",", "'S-BLEU'", ",", "'METEOR'", ",", "'ROUGE'", "]", "\n", "with", "open", "(", "csv_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "file", ":", "\n", "            ", "writer", "=", "csv", ".", "DictWriter", "(", "file", ",", "header", ")", "\n", "writer", ".", "writeheader", "(", ")", "\n", "for", "evaluation", "in", "evaluations", ":", "\n", "                ", "writer", ".", "writerow", "(", "dict", "(", "zip", "(", "header", ",", "evaluation", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.stats": [[711, 723], ["t2t.DataLoader.load_data", "len", "len", "print"], "function", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.t2t.DataLoader.load_data"], ["", "", "", "", "def", "stats", "(", ")", ":", "\n", "    ", "for", "label", "in", "(", "'basts'", ",", "'sit'", ",", "'gh'", ")", ":", "\n", "        ", "for", "lang", "in", "(", "'java'", ",", "'python'", ")", ":", "\n", "            ", "for", "split", "in", "(", "'train'", ",", "'valid'", ",", "'test'", ")", ":", "\n", "                ", "if", "label", "==", "'gh'", "and", "split", "==", "'test'", ":", "\n", "                    ", "continue", "\n", "", "dataset", "=", "DataLoader", ".", "load_data", "(", "label", ",", "lang", ",", "split", ")", "\n", "snippets_length", "=", "len", "(", "dataset", "[", "'snippets'", "]", ")", "\n", "comments_length", "=", "len", "(", "dataset", "[", "'comments'", "]", ")", "\n", "assert", "snippets_length", "==", "comments_length", "\n", "length", "=", "snippets_length", "\n", "print", "(", "f'{label}_{lang}_{split}: {length}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.metric.Metric.report": [[21, 34], ["metric.Metric.report_bleu", "metric.Metric.report_meteor", "metric.Metric.report_rouge", "print", "logger.info"], "methods", ["home.repos.pwc.inspect_result.jianguda_afm4acs.src.metric.Metric.report_bleu", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.metric.Metric.report_meteor", "home.repos.pwc.inspect_result.jianguda_afm4acs.src.metric.Metric.report_rouge"], ["    ", "@", "staticmethod", "\n", "def", "report", "(", "predictions", ",", "references", ")", ":", "\n", "        ", "corpus_bleu_score", ",", "sentence_bleu_score", "=", "Metric", ".", "report_bleu", "(", "predictions", ",", "references", ")", "\n", "meteor_score", "=", "Metric", ".", "report_meteor", "(", "predictions", ",", "references", ")", "\n", "rouge_score", "=", "Metric", ".", "report_rouge", "(", "predictions", ",", "references", ")", "\n", "# print(f'corpus bleu score: {corpus_bleu_score}')", "\n", "# print(f'sentence bleu score: {sentence_bleu_score}')", "\n", "# print(f'meteor score: {meteor_score}')", "\n", "# print(f'rouge score: {rouge_score}')", "\n", "x", "=", "f'c_bleu = {corpus_bleu_score} | s_bleu = {sentence_bleu_score} | meteor = {meteor_score} | rouge = {rouge_score}'", "\n", "print", "(", "x", ")", "\n", "logger", ".", "info", "(", "x", ")", "\n", "return", "corpus_bleu_score", ",", "sentence_bleu_score", ",", "meteor_score", ",", "rouge_score", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.metric.Metric.report_bleu": [[35, 47], ["datasets.load_metric", "zip", "prediction.split", "datasets.load_metric.compute", "len", "round", "round", "reference.split", "datasets.load_metric.compute"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "report_bleu", "(", "predictions", ",", "references", ")", ":", "\n", "        ", "predictions", "=", "[", "prediction", ".", "split", "(", ")", "for", "prediction", "in", "predictions", "]", "\n", "references", "=", "[", "[", "reference", ".", "split", "(", ")", "]", "for", "reference", "in", "references", "]", "\n", "metric", "=", "load_metric", "(", "'bleu'", ")", "\n", "# print(metric.inputs_description)", "\n", "corpus_score", "=", "metric", ".", "compute", "(", "predictions", "=", "predictions", ",", "references", "=", "references", ")", "[", "'bleu'", "]", "\n", "sum_sentence_bleu", "=", "0.0", "\n", "for", "reference", ",", "prediction", "in", "zip", "(", "references", ",", "predictions", ")", ":", "\n", "            ", "sum_sentence_bleu", "+=", "metric", ".", "compute", "(", "predictions", "=", "[", "prediction", "]", ",", "references", "=", "[", "reference", "]", ",", "smooth", "=", "True", ")", "[", "'bleu'", "]", "\n", "", "sentence_score", "=", "sum_sentence_bleu", "/", "len", "(", "predictions", ")", "\n", "return", "round", "(", "corpus_score", "*", "100", ",", "2", ")", ",", "round", "(", "sentence_score", "*", "100", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.metric.Metric.report_meteor": [[48, 63], ["nlgeval.NLGEval().compute_metrics", "round", "datasets.load_metric", "datasets.load_metric.compute", "round", "nlgeval.NLGEval"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "report_meteor", "(", "predictions", ",", "references", ",", "version", "=", "1.5", ")", ":", "\n", "        ", "if", "version", "==", "1.5", ":", "\n", "# version 1.5 https://aclanthology.org/W14-3348.pdf", "\n", "            ", "metrics_dict", "=", "NLGEval", "(", ")", ".", "compute_metrics", "(", "[", "references", "]", ",", "predictions", ")", "\n", "return", "round", "(", "metrics_dict", "[", "'METEOR'", "]", "*", "100", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "assert", "version", "==", "1.0", "\n", "# version 1.0 https://aclanthology.org/W07-0734.pdf", "\n", "predictions", "=", "[", "prediction", "for", "prediction", "in", "predictions", "]", "\n", "references", "=", "[", "reference", "for", "reference", "in", "references", "]", "\n", "metric", "=", "load_metric", "(", "'meteor'", ")", "\n", "# print(metric.inputs_description)", "\n", "scores", "=", "metric", ".", "compute", "(", "predictions", "=", "predictions", ",", "references", "=", "references", ")", "\n", "return", "round", "(", "scores", "[", "'meteor'", "]", "*", "100", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jianguda_afm4acs.src.metric.Metric.report_rouge": [[64, 72], ["datasets.load_metric", "datasets.load_metric.compute", "round"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "report_rouge", "(", "predictions", ",", "references", ")", ":", "\n", "        ", "predictions", "=", "[", "prediction", "for", "prediction", "in", "predictions", "]", "\n", "references", "=", "[", "reference", "for", "reference", "in", "references", "]", "\n", "metric", "=", "load_metric", "(", "'rouge'", ")", "\n", "# print(metric.inputs_description)", "\n", "scores", "=", "metric", ".", "compute", "(", "predictions", "=", "predictions", ",", "references", "=", "references", ")", "\n", "return", "round", "(", "scores", "[", "'rougeL'", "]", ".", "mid", ".", "fmeasure", "*", "100", ",", "2", ")", "\n", "\n"]]}