{"home.repos.pwc.inspect_result.facebookresearch_anli.utils.save_tool.ScoreLogger.__init__": [[16, 21], ["object.__init__", "dict", "save_tool.ScoreLogger.score_tracker.update"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "init_tracking_dict", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "logging_item_list", "=", "[", "]", "\n", "self", ".", "score_tracker", "=", "dict", "(", ")", "\n", "self", ".", "score_tracker", ".", "update", "(", "init_tracking_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.save_tool.ScoreLogger.incorporate_results": [[22, 36], ["score_dict.keys", "score_dict.items", "save_tool.ScoreLogger.logging_item_list.append", "len", "len", "score_dict.keys", "save_tool.ScoreLogger.score_tracker.keys"], "methods", ["None"], ["", "def", "incorporate_results", "(", "self", ",", "score_dict", ",", "save_key", ",", "item", "=", "None", ")", "->", "bool", ":", "\n", "        ", "assert", "len", "(", "score_dict", ".", "keys", "(", ")", ")", "==", "len", "(", "self", ".", "score_tracker", ".", "keys", "(", ")", ")", "\n", "for", "fieldname", "in", "score_dict", ".", "keys", "(", ")", ":", "\n", "            ", "assert", "fieldname", "in", "self", ".", "score_tracker", "\n", "\n", "", "valid_improvement", "=", "False", "\n", "for", "fieldname", ",", "value", "in", "score_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "score_dict", "[", "fieldname", "]", ">=", "self", ".", "score_tracker", "[", "fieldname", "]", ":", "\n", "                ", "self", ".", "score_tracker", "[", "fieldname", "]", "=", "score_dict", "[", "fieldname", "]", "\n", "valid_improvement", "=", "True", "\n", "\n", "", "", "self", ".", "logging_item_list", ".", "append", "(", "{", "'k'", ":", "save_key", ",", "'v'", ":", "item", "}", ")", "\n", "\n", "return", "valid_improvement", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.save_tool.ScoreLogger.logging_to_file": [[37, 50], ["pathlib.Path().is_file", "utils.common.save_json", "utils.common.load_json", "set", "pathlib.Path", "set.add", "ValueError"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_json", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_json"], ["", "def", "logging_to_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "if", "Path", "(", "filename", ")", ".", "is_file", "(", ")", ":", "\n", "            ", "old_logging_list", "=", "common", ".", "load_json", "(", "filename", ")", "\n", "current_saved_key", "=", "set", "(", ")", "\n", "\n", "for", "item", "in", "self", ".", "logging_item_list", ":", "\n", "                ", "current_saved_key", ".", "add", "(", "item", "[", "'k'", "]", ")", "\n", "\n", "", "for", "item", "in", "old_logging_list", ":", "\n", "                ", "if", "item", "[", "'k'", "]", "not", "in", "current_saved_key", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Previous logged item can not be found!\"", ")", "\n", "\n", "", "", "", "common", ".", "save_json", "(", "self", ".", "logging_item_list", ",", "filename", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.save_tool.gen_file_prefix": [[52, 58], ["os.path.join", "datetime.datetime.now().strftime", "os.path.exists", "os.makedirs", "datetime.datetime.now"], "function", ["None"], ["", "", "def", "gen_file_prefix", "(", "model_name", ",", "directory_name", "=", "'saved_models'", ",", "date", "=", "None", ")", ":", "\n", "    ", "date_now", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d-%H:%M:%S\"", ")", "if", "not", "date", "else", "date", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "config", ".", "PRO_ROOT", "/", "directory_name", "/", "'_'", ".", "join", "(", "(", "date_now", ",", "model_name", ")", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "file_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "file_path", ")", "\n", "", "return", "file_path", ",", "date_now", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.save_tool.get_cur_time_str": [[60, 63], ["datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["None"], ["", "def", "get_cur_time_str", "(", ")", ":", "\n", "    ", "date_now", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d[%H:%M:%S]\"", ")", "\n", "return", "date_now", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.JsonableObjectEncoder.default": [[28, 35], ["isinstance", "d.update", "super().default", "vars", "type"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.JsonableObjectEncoder.default"], ["    ", "def", "default", "(", "self", ",", "o", ")", ":", "\n", "        ", "if", "isinstance", "(", "o", ",", "JsonableObj", ")", ":", "\n", "            ", "d", "=", "{", "'_jcls_'", ":", "type", "(", "o", ")", ".", "__name__", "}", "\n", "d", ".", "update", "(", "vars", "(", "o", ")", ")", "\n", "return", "d", "\n", "", "else", ":", "\n", "            ", "return", "super", "(", ")", ".", "default", "(", "o", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.register_class": [[17, 21], ["registered_jsonabl_classes.update"], "function", ["None"], ["def", "register_class", "(", "cls", ")", ":", "\n", "    ", "global", "registered_jsonabl_classes", "\n", "if", "cls", "not", "in", "registered_jsonabl_classes", ":", "\n", "        ", "registered_jsonabl_classes", ".", "update", "(", "{", "cls", ".", "__name__", ":", "cls", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.unserialize_JsonableObject": [[37, 48], ["d.pop", "cls.__new__", "d.items", "setattr"], "function", ["None"], ["", "", "", "def", "unserialize_JsonableObject", "(", "d", ")", ":", "\n", "    ", "global", "registered_jsonabl_classes", "\n", "classname", "=", "d", ".", "pop", "(", "'_jcls_'", ",", "None", ")", "\n", "if", "classname", ":", "\n", "        ", "cls", "=", "registered_jsonabl_classes", "[", "classname", "]", "\n", "obj", "=", "cls", ".", "__new__", "(", "cls", ")", "# Make instance without calling __init__", "\n", "for", "key", ",", "value", "in", "d", ".", "items", "(", ")", ":", "\n", "            ", "setattr", "(", "obj", ",", "key", ",", "value", ")", "\n", "", "return", "obj", "\n", "", "else", ":", "\n", "        ", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.json_dumps": [[50, 52], ["json.dumps"], "function", ["None"], ["", "", "def", "json_dumps", "(", "item", ")", ":", "\n", "    ", "return", "json", ".", "dumps", "(", "item", ",", "cls", "=", "JsonableObjectEncoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.json_loads": [[54, 56], ["json.loads"], "function", ["None"], ["", "def", "json_loads", "(", "item_str", ")", ":", "\n", "    ", "return", "json", ".", "loads", "(", "item_str", ",", "object_hook", "=", "unserialize_JsonableObject", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl": [[60, 65], ["print", "open", "out_f.write", "json.dumps"], "function", ["None"], ["", "def", "save_jsonl", "(", "d_list", ",", "filename", ")", ":", "\n", "    ", "print", "(", "\"Save to Jsonl:\"", ",", "filename", ")", "\n", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "for", "item", "in", "d_list", ":", "\n", "            ", "out_f", ".", "write", "(", "json", ".", "dumps", "(", "item", ",", "cls", "=", "JsonableObjectEncoder", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl": [[67, 78], ["open", "print", "tqdm.tqdm", "json.loads", "d_list.append", "line.strip", "len"], "function", ["None"], ["", "", "", "def", "load_jsonl", "(", "filename", ",", "debug_num", "=", "None", ")", ":", "\n", "    ", "d_list", "=", "[", "]", "\n", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ":", "\n", "        ", "print", "(", "\"Load Jsonl:\"", ",", "filename", ")", "\n", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ",", "object_hook", "=", "unserialize_JsonableObject", ")", "\n", "d_list", ".", "append", "(", "item", ")", "\n", "if", "debug_num", "is", "not", "None", "and", "0", "<", "debug_num", "==", "len", "(", "d_list", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_json": [[80, 83], ["open", "json.load"], "function", ["None"], ["", "def", "load_json", "(", "filename", ",", "**", "kwargs", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ":", "\n", "        ", "return", "json", ".", "load", "(", "in_f", ",", "object_hook", "=", "unserialize_JsonableObject", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_json": [[85, 89], ["open", "json.dump", "out_f.close"], "function", ["None"], ["", "", "def", "save_json", "(", "obj", ",", "filename", ",", "**", "kwargs", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "json", ".", "dump", "(", "obj", ",", "out_f", ",", "cls", "=", "JsonableObjectEncoder", ",", "**", "kwargs", ")", "\n", "out_f", ".", "close", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.list_dict_data_tool.list_to_dict": [[9, 15], ["dict"], "function", ["None"], ["def", "list_to_dict", "(", "d_list", ",", "key_fields", ")", ":", "#   '_id' or 'pid'", "\n", "    ", "d_dict", "=", "dict", "(", ")", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "assert", "key_fields", "in", "item", "\n", "d_dict", "[", "item", "[", "key_fields", "]", "]", "=", "item", "\n", "", "return", "d_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.list_dict_data_tool.dict_to_list": [[17, 22], ["d_dict.items", "d_list.append"], "function", ["None"], ["", "def", "dict_to_list", "(", "d_dict", ")", ":", "\n", "    ", "d_list", "=", "[", "]", "\n", "for", "key", ",", "value", "in", "d_dict", ".", "items", "(", ")", ":", "\n", "        ", "d_list", ".", "append", "(", "value", ")", "\n", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.list_dict_data_tool.append_item_from_dict_to_list": [[24, 37], ["isinstance", "print"], "function", ["None"], ["", "def", "append_item_from_dict_to_list", "(", "d_list", ",", "d_dict", ",", "key_fieldname", ",", "append_fieldnames", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "append_fieldnames", ",", "list", ")", ":", "\n", "        ", "append_fieldnames", "=", "[", "append_fieldnames", "]", "\n", "", "for", "item", "in", "d_list", ":", "\n", "        ", "key", "=", "item", "[", "key_fieldname", "]", "\n", "if", "key", "in", "d_dict", ":", "\n", "            ", "for", "append_fieldname", "in", "append_fieldnames", ":", "\n", "                ", "item", "[", "append_fieldname", "]", "=", "d_dict", "[", "key", "]", "[", "append_fieldname", "]", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "f\"Potential Error: {key} not in scored_dict. Maybe bc all forward items are empty.\"", ")", "\n", "for", "append_fieldname", "in", "append_fieldnames", ":", "\n", "                ", "item", "[", "append_fieldname", "]", "=", "[", "]", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style": [[39, 52], ["isinstance", "print"], "function", ["None"], ["", "def", "append_item_from_dict_to_list_hotpot_style", "(", "d_list", ",", "d_dict", ",", "key_fieldname", ",", "append_fieldnames", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "append_fieldnames", ",", "list", ")", ":", "\n", "        ", "append_fieldnames", "=", "[", "append_fieldnames", "]", "\n", "", "for", "item", "in", "d_list", ":", "\n", "        ", "key", "=", "item", "[", "key_fieldname", "]", "\n", "for", "append_fieldname", "in", "append_fieldnames", ":", "\n", "            ", "if", "key", "in", "d_dict", "[", "append_fieldname", "]", ":", "\n", "                ", "item", "[", "append_fieldname", "]", "=", "d_dict", "[", "append_fieldname", "]", "[", "key", "]", "\n", "", "else", ":", "\n", "                ", "print", "(", "f\"Potential Error: {key} not in scored_dict. Maybe bc all forward items are empty.\"", ")", "\n", "# for append_fieldname in append_fieldnames:", "\n", "item", "[", "append_fieldname", "]", "=", "[", "]", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.utils.list_dict_data_tool.append_subfield_from_list_to_dict": [[54, 91], ["d_dict.keys", "dict", "print"], "function", ["None"], ["", "def", "append_subfield_from_list_to_dict", "(", "subf_list", ",", "d_dict", ",", "o_key_field_name", ",", "subfield_key_name", ",", "\n", "subfield_name", "=", "'merged_field'", ",", "check", "=", "False", ")", ":", "\n", "# Often times, we will need to split the one data point to multiple items to be feeded into neural networks", "\n", "# and after we obtain the results we will need to map the results back to original data point with some keys.", "\n", "\n", "# This method is used for this purpose.", "\n", "# The method can be invoke multiple times, (in practice usually one batch per time.)", "\n", "    ", "\"\"\"\n    :param subf_list:               The forward list.\n    :param d_dict:                  The dict that contain keys mapping to original data point.\n    :param o_key_field_name:        The fieldname of original data point key. 'pid'\n    :param subfield_key_name:       The fieldname of the sub item. 'fid'\n    :param subfield_name:           The merge field name.       'merged_field'\n    :param check:\n    :return:\n    \"\"\"", "\n", "for", "key", "in", "d_dict", ".", "keys", "(", ")", ":", "\n", "        ", "d_dict", "[", "key", "]", "[", "subfield_name", "]", "=", "dict", "(", ")", "\n", "\n", "", "for", "item", "in", "subf_list", ":", "\n", "        ", "assert", "o_key_field_name", "in", "item", "\n", "assert", "subfield_key_name", "in", "item", "\n", "map_id", "=", "item", "[", "o_key_field_name", "]", "\n", "sub_filed_id", "=", "item", "[", "subfield_key_name", "]", "\n", "assert", "map_id", "in", "d_dict", "\n", "\n", "# if subfield_name not in d_dict[map_id]:", "\n", "#     d_dict[map_id][subfield_name] = dict()", "\n", "\n", "if", "sub_filed_id", "not", "in", "d_dict", "[", "map_id", "]", "[", "subfield_name", "]", ":", "\n", "            ", "if", "check", ":", "\n", "                ", "assert", "item", "[", "o_key_field_name", "]", "==", "map_id", "\n", "", "d_dict", "[", "map_id", "]", "[", "subfield_name", "]", "[", "sub_filed_id", "]", "=", "item", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Duplicate forward item with key:\"", ",", "sub_filed_id", ")", "\n", "\n", "", "", "return", "d_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inspection_tools.summarize_attributions": [[14, 21], ["attributions.sum().squeeze.sum().squeeze", "torch.norm", "attributions.sum().squeeze.sum"], "function", ["None"], ["def", "summarize_attributions", "(", "attributions", ")", ":", "\n", "    ", "\"\"\"\n    Summarises the attribution across multiple runs\n    \"\"\"", "\n", "attributions", "=", "attributions", ".", "sum", "(", "dim", "=", "-", "1", ")", ".", "squeeze", "(", "0", ")", "\n", "attributions", "=", "attributions", "/", "torch", ".", "norm", "(", "attributions", ")", "\n", "return", "attributions", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inspection_tools.get_model_prediction": [[23, 49], ["model.eval", "torch.no_grad", "model", "model", "model", "model"], "function", ["None"], ["", "def", "get_model_prediction", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "model", ",", "model_class_item", ",", "with_gradient", "=", "False", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "if", "not", "with_gradient", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "model_class_item", "[", "'model_class_name'", "]", "in", "[", "\"distilbert\"", ",", "\"bart-large\"", "]", ":", "\n", "                ", "outputs", "=", "model", "(", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "labels", "=", "None", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "labels", "=", "None", ")", "\n", "", "", "", "else", ":", "\n", "        ", "if", "model_class_item", "[", "'model_class_name'", "]", "in", "[", "\"distilbert\"", ",", "\"bart-large\"", "]", ":", "\n", "            ", "outputs", "=", "model", "(", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "labels", "=", "None", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "model", "(", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "labels", "=", "None", ")", "\n", "\n", "", "", "return", "outputs", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inspection_tools.get_lig_object": [[51, 69], ["isinstance", "logger.warning", "captum.attr.LayerIntegratedGradients", "captum.attr.LayerIntegratedGradients", "current_layer.__getattr__.__getattr__", "model.__getattr__"], "function", ["None"], ["", "def", "get_lig_object", "(", "model", ",", "model_class_item", ")", ":", "\n", "    ", "insight_supported", "=", "model_class_item", "[", "'insight_supported'", "]", "if", "'insight_supported'", "in", "model_class_item", "else", "False", "\n", "internal_model_name", "=", "model_class_item", "[", "'internal_model_name'", "]", "\n", "lig", "=", "None", "# default is None.", "\n", "if", "not", "insight_supported", ":", "\n", "        ", "logger", ".", "warning", "(", "f\"Inspection for model '{model_class_item['model_class_name']}' is not supported.\"", ")", "\n", "return", "lig", "\n", "\n", "", "if", "isinstance", "(", "internal_model_name", ",", "list", ")", ":", "\n", "        ", "current_layer", "=", "model", "\n", "for", "layer_n", "in", "internal_model_name", ":", "\n", "            ", "current_layer", "=", "current_layer", ".", "__getattr__", "(", "layer_n", ")", "\n", "# print(current_layer)", "\n", "", "lig", "=", "LayerIntegratedGradients", "(", "get_model_prediction", ",", "current_layer", ")", "\n", "", "else", ":", "\n", "        ", "lig", "=", "LayerIntegratedGradients", "(", "get_model_prediction", ",", "\n", "model", ".", "__getattr__", "(", "internal_model_name", ")", ".", "embeddings", ".", "word_embeddings", ")", "\n", "", "return", "lig", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inspection_tools.get_tokenized_input_tokens": [[71, 79], ["tokenizer.convert_ids_to_tokens", "tokenizer.convert_tokens_to_string", "output_tokens.append", "t.replace"], "function", ["None"], ["", "def", "get_tokenized_input_tokens", "(", "tokenizer", ",", "token_ids", ")", ":", "\n", "    ", "raw_words_list", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "token_ids", ")", "\n", "string_tokens", "=", "[", "tokenizer", ".", "convert_tokens_to_string", "(", "word", ")", "for", "word", "in", "raw_words_list", "]", "\n", "# still need some cleanup, remove space within tokens", "\n", "output_tokens", "=", "[", "]", "\n", "for", "t", "in", "string_tokens", ":", "\n", "        ", "output_tokens", ".", "append", "(", "t", ".", "replace", "(", "\" \"", ",", "\"\"", ")", ")", "\n", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inspection_tools.cleanup_tokenization_special_tokens": [[81, 91], ["zip", "filtered_tokens.append", "filtered_importance.append"], "function", ["None"], ["", "def", "cleanup_tokenization_special_tokens", "(", "tokens", ",", "importance", ",", "tokenizer", ")", ":", "\n", "    ", "filtered_tokens", "=", "[", "]", "\n", "filtered_importance", "=", "[", "]", "\n", "for", "t", ",", "i", "in", "zip", "(", "tokens", ",", "importance", ")", ":", "\n", "        ", "if", "t", "in", "tokenizer", ".", "all_special_tokens", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "filtered_tokens", ".", "append", "(", "t", ")", "\n", "filtered_importance", ".", "append", "(", "i", ")", "\n", "", "", "return", "filtered_tokens", ",", "filtered_importance", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.softmax": [[33, 37], ["numpy.exp", "np.exp.sum", "numpy.asarray", "numpy.max"], "function", ["None"], ["def", "softmax", "(", "x", ")", ":", "\n", "    ", "\"\"\"Compute softmax values for each sets of scores in x.\"\"\"", "\n", "e_x", "=", "np", ".", "exp", "(", "np", ".", "asarray", "(", "x", ")", "-", "np", ".", "max", "(", "x", ")", ")", "\n", "return", "e_x", "/", "e_x", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.eval_model": [[39, 83], ["model.eval", "range", "torch.no_grad", "enumerate", "len", "len", "len", "len", "len", "dict", "inference_debug.softmax", "result_items_list.append", "flint.data_utils.batchbuilder.move_to_device", "uid_list.extend", "y_list.extend", "pred_list.extend", "logits_list.extend", "model", "model", "list", "batch[].tolist", "[].view().tolist", "logits.tolist", "[].view", "logits.size", "torch.max"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.softmax", "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device"], ["", "def", "eval_model", "(", "model", ",", "dev_dataloader", ",", "device_num", ",", "args", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "uid_list", "=", "[", "]", "\n", "y_list", "=", "[", "]", "\n", "pred_list", "=", "[", "]", "\n", "logits_list", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "dev_dataloader", ",", "0", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "if", "args", ".", "model_class_name", "in", "[", "\"distilbert\"", ",", "\"bart-large\"", "]", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "labels", "=", "None", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "token_type_ids", "=", "batch", "[", "'token_type_ids'", "]", ",", "\n", "labels", "=", "None", ")", "\n", "\n", "# print(outputs)", "\n", "", "logits", "=", "outputs", "[", "0", "]", "\n", "\n", "uid_list", ".", "extend", "(", "list", "(", "batch", "[", "'uid'", "]", ")", ")", "\n", "y_list", ".", "extend", "(", "batch", "[", "'y'", "]", ".", "tolist", "(", ")", ")", "\n", "pred_list", ".", "extend", "(", "torch", ".", "max", "(", "logits", ",", "1", ")", "[", "1", "]", ".", "view", "(", "logits", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "logits_list", ".", "extend", "(", "logits", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "logits_list", ")", "\n", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "logits_list", ")", "\n", "\n", "result_items_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "uid_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'uid'", "]", "=", "uid_list", "[", "i", "]", "\n", "r_item", "[", "'logits'", "]", "=", "logits_list", "[", "i", "]", "\n", "r_item", "[", "'probability'", "]", "=", "softmax", "(", "r_item", "[", "'logits'", "]", ")", "\n", "r_item", "[", "'predicted_label'", "]", "=", "id2label", "[", "pred_list", "[", "i", "]", "]", "\n", "\n", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.inference": [[85, 153], ["argparse.ArgumentParser", "argparse.ArgumentParser.parse_args", "model_class_item[].from_pretrained", "model_class_item[].from_pretrained", "model_class_item[].from_pretrained.load_state_dict", "nli.training.NLITransform", "nli.training.build_eval_dataset_loader_and_sampler", "nli.training.eval_model", "torch.load", "model_class_item[].from_pretrained.convert_tokens_to_ids", "flint.data_utils.fields.RawFlintField", "flint.data_utils.fields.LabelFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "torch.cuda.set_device", "model_class_item[].from_pretrained.cuda", "str", "str", "str", "uuid.uuid4"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.build_eval_dataset_loader_and_sampler", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.eval_model"], ["", "def", "inference", "(", "model_class_name", ",", "model_checkpoint_path", ",", "max_length", ",", "premise", ",", "hypothesis", ",", "cpu", "=", "True", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# CPU for now", "\n", "if", "cpu", ":", "\n", "        ", "args", ".", "global_rank", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "args", ".", "global_rank", "=", "0", "\n", "\n", "", "model_checkpoint_path", "=", "model_checkpoint_path", "\n", "args", ".", "model_class_name", "=", "model_class_name", "\n", "num_labels", "=", "3", "\n", "# we are doing NLI so we set num_labels = 3, for other task we can change this value.", "\n", "\n", "max_length", "=", "max_length", "\n", "\n", "model_class_item", "=", "MODEL_CLASSES", "[", "model_class_name", "]", "\n", "model_name", "=", "model_class_item", "[", "'model_name'", "]", "\n", "do_lower_case", "=", "model_class_item", "[", "'do_lower_case'", "]", "if", "'do_lower_case'", "in", "model_class_item", "else", "False", "\n", "\n", "tokenizer", "=", "model_class_item", "[", "'tokenizer'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "model", "=", "model_class_item", "[", "'sequence_classification'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_checkpoint_path", ")", ")", "\n", "\n", "padding_token_value", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "tokenizer", ".", "pad_token", "]", ")", "[", "0", "]", "\n", "padding_segement_value", "=", "model_class_item", "[", "\"padding_segement_value\"", "]", "\n", "padding_att_value", "=", "model_class_item", "[", "\"padding_att_value\"", "]", "\n", "left_pad", "=", "model_class_item", "[", "'left_pad'", "]", "if", "'left_pad'", "in", "model_class_item", "else", "False", "\n", "\n", "batch_size_per_gpu_eval", "=", "16", "\n", "\n", "eval_data_list", "=", "[", "{", "\n", "'uid'", ":", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", ",", "\n", "'premise'", ":", "premise", ",", "\n", "'hypothesis'", ":", "hypothesis", ",", "\n", "'label'", ":", "'h'", "# hidden", "\n", "}", "]", "\n", "\n", "batching_schema", "=", "{", "\n", "'uid'", ":", "RawFlintField", "(", ")", ",", "\n", "'y'", ":", "LabelFlintField", "(", ")", ",", "\n", "'input_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_token_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'token_type_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_segement_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'attention_mask'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_att_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "}", "\n", "\n", "data_transformer", "=", "NLITransform", "(", "model_name", ",", "tokenizer", ",", "max_length", ")", "\n", "\n", "d_dataset", ",", "d_sampler", ",", "d_dataloader", "=", "build_eval_dataset_loader_and_sampler", "(", "eval_data_list", ",", "data_transformer", ",", "\n", "batching_schema", ",", "\n", "batch_size_per_gpu_eval", ")", "\n", "\n", "if", "not", "cpu", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "0", ")", "\n", "model", ".", "cuda", "(", "0", ")", "\n", "\n", "", "pred_output_list", "=", "eval_model", "(", "model", ",", "d_dataloader", ",", "args", ".", "global_rank", ",", "args", ")", "\n", "# r_dict = dict()", "\n", "# Eval loop:", "\n", "# print(pred_output_list)", "\n", "return", "pred_output_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.NLIDataset.__init__": [[220, 225], ["torch.utils.data.Dataset.__init__", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_list", ",", "transform", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_list", "=", "data_list", "\n", "self", ".", "len", "=", "len", "(", "self", ".", "d_list", ")", "\n", "self", ".", "transform", "=", "transform", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.NLIDataset.__getitem__": [[226, 228], ["training_extra.NLIDataset.transform"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "return", "self", ".", "transform", "(", "self", ".", "d_list", "[", "index", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.NLIDataset.__len__": [[231, 233], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "len", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.NLITransform.__init__": [[236, 240], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "tokenizer", ",", "max_length", "=", "None", ")", ":", "\n", "        ", "self", ".", "model_name", "=", "model_name", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "max_length", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.NLITransform.__call__": [[241, 264], ["dict", "training_extra.NLITransform.tokenizer.encode_plus", "dict.update", "premise.strip", "hypothesis.strip"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "sample", ")", ":", "\n", "        ", "processed_sample", "=", "dict", "(", ")", "\n", "processed_sample", "[", "'uid'", "]", "=", "sample", "[", "'uid'", "]", "\n", "processed_sample", "[", "'gold_label'", "]", "=", "sample", "[", "'label'", "]", "\n", "processed_sample", "[", "'y'", "]", "=", "nli_label2index", "[", "sample", "[", "'label'", "]", "]", "\n", "\n", "# premise: str = sample['premise']", "\n", "premise", ":", "str", "=", "sample", "[", "'context'", "]", "if", "'context'", "in", "sample", "else", "sample", "[", "'premise'", "]", "\n", "hypothesis", ":", "str", "=", "sample", "[", "'hypothesis'", "]", "\n", "\n", "if", "premise", ".", "strip", "(", ")", "==", "''", ":", "\n", "            ", "premise", "=", "'empty'", "\n", "\n", "", "if", "hypothesis", ".", "strip", "(", ")", "==", "''", ":", "\n", "            ", "hypothesis", "=", "'empty'", "\n", "\n", "", "tokenized_input_seq_pair", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "premise", ",", "hypothesis", ",", "\n", "max_length", "=", "self", ".", "max_length", ",", "\n", "return_token_type_ids", "=", "True", ",", "truncation", "=", "True", ")", "\n", "\n", "processed_sample", ".", "update", "(", "tokenized_input_seq_pair", ")", "\n", "\n", "return", "processed_sample", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.set_seed": [[213, 217], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed"], "function", ["None"], ["def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.build_eval_dataset_loader_and_sampler": [[266, 277], ["training_extra.NLIDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "flint.data_utils.batchbuilder.BaseBatchBuilder"], "function", ["None"], ["", "", "def", "build_eval_dataset_loader_and_sampler", "(", "d_list", ",", "data_transformer", ",", "batching_schema", ",", "batch_size_per_gpu_eval", ")", ":", "\n", "    ", "d_dataset", "=", "NLIDataset", "(", "d_list", ",", "data_transformer", ")", "\n", "d_sampler", "=", "SequentialSampler", "(", "d_dataset", ")", "\n", "d_dataloader", "=", "DataLoader", "(", "dataset", "=", "d_dataset", ",", "\n", "batch_size", "=", "batch_size_per_gpu_eval", ",", "\n", "shuffle", "=", "False", ",", "#", "\n", "num_workers", "=", "0", ",", "\n", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "d_sampler", ",", "\n", "collate_fn", "=", "BaseBatchBuilder", "(", "batching_schema", ")", ")", "#", "\n", "return", "d_dataset", ",", "d_sampler", ",", "d_dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.sample_data_list": [[279, 295], ["int", "ValueError", "math.ceil", "range", "numpy.isclose", "sampled_d_list.extend", "int", "random.shuffle", "copy.deepcopy", "len"], "function", ["None"], ["", "def", "sample_data_list", "(", "d_list", ",", "ratio", ")", ":", "\n", "    ", "if", "ratio", "<=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid training weight ratio. Please change --train_weights.\"", ")", "\n", "", "upper_int", "=", "int", "(", "math", ".", "ceil", "(", "ratio", ")", ")", "\n", "if", "upper_int", "==", "1", ":", "\n", "        ", "return", "d_list", "# if ratio is 1 then we just return the data list", "\n", "", "else", ":", "\n", "        ", "sampled_d_list", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "upper_int", ")", ":", "\n", "            ", "sampled_d_list", ".", "extend", "(", "copy", ".", "deepcopy", "(", "d_list", ")", ")", "\n", "", "if", "np", ".", "isclose", "(", "ratio", ",", "upper_int", ")", ":", "\n", "            ", "return", "sampled_d_list", "\n", "", "else", ":", "\n", "            ", "sampled_length", "=", "int", "(", "ratio", "*", "len", "(", "d_list", ")", ")", "\n", "random", ".", "shuffle", "(", "sampled_d_list", ")", "\n", "return", "sampled_d_list", "[", ":", "sampled_length", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.main": [[297, 399], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "training_extra.train", "training_extra.train", "torch.spawn"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.train", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.train"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--cpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we only use CPU.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--single_gpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we only use single GPU.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp16\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we will use fp16.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fp16_opt_level\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"O1\"", ",", "\n", "help", "=", "\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"", "\n", "\"See details at https://nvidia.github.io/apex/amp.html\"", ",", "\n", ")", "\n", "\n", "# environment arguments", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--seed'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'manual random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--num_nodes'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of nodes'", ")", "\n", "parser", ".", "add_argument", "(", "'-g'", ",", "'--gpus_per_node'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of gpus per node'", ")", "\n", "parser", ".", "add_argument", "(", "'-nr'", ",", "'--node_rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'ranking within the nodes'", ")", "\n", "\n", "# experiments specific arguments", "\n", "parser", ".", "add_argument", "(", "'--debug_mode'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "dest", "=", "'debug_mode'", ",", "\n", "help", "=", "'weather this is debug mode or normal'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_class_name\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Set the model class of the experiment.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--experiment_name\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Set the name of the experiment. [model_name]/[data]/[task]/[other]\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--save_prediction\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "dest", "=", "'save_prediction'", ",", "\n", "help", "=", "'Do we want to save prediction'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "default", "=", "2", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of total epochs to run'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_train_batch_size\"", ",", "default", "=", "16", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for training.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--gradient_accumulation_steps\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "64", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for evaluation.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "default", "=", "160", ",", "type", "=", "int", ",", "help", "=", "\"Max length of the sequences.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "\"Max gradient norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "1e-5", ",", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "help", "=", "\"Weight decay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--eval_frequency\"", ",", "default", "=", "1000", ",", "type", "=", "int", ",", "help", "=", "\"set the evaluation frequency, evaluate every X global step.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--train_data\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data used in the experiments.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--train_weights\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data weights used in the experiments.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--eval_data\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data used in the experiments.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "cpu", ":", "\n", "        ", "args", ".", "world_size", "=", "1", "\n", "train", "(", "-", "1", ",", "args", ")", "\n", "", "elif", "args", ".", "single_gpu", ":", "\n", "        ", "args", ".", "world_size", "=", "1", "\n", "train", "(", "0", ",", "args", ")", "\n", "", "else", ":", "# distributed multiGPU training", "\n", "#########################################################", "\n", "        ", "args", ".", "world_size", "=", "args", ".", "gpus_per_node", "*", "args", ".", "num_nodes", "#", "\n", "# os.environ['MASTER_ADDR'] = '152.2.142.184'  # This is the IP address for nlp5", "\n", "# maybe we will automatically retrieve the IP later.", "\n", "os", ".", "environ", "[", "'MASTER_PORT'", "]", "=", "'88888'", "#", "\n", "mp", ".", "spawn", "(", "train", ",", "nprocs", "=", "args", ".", "gpus_per_node", ",", "args", "=", "(", "args", ",", ")", ")", "# spawn how many process in this node", "\n", "# remember train is called as train(i, args).", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.train": [[403, 802], ["training_extra.set_seed", "model_class_item[].from_pretrained", "train_data_str.split", "eval_data_str.split", "training_extra.NLITransform", "range", "len", "print", "dict", "tqdm.tqdm", "model_class_item[].from_pretrained", "modeling.res_encoder.ResEncoder", "model_class_item[].from_pretrained.convert_tokens_to_ids", "torch.init_process_group", "train_data_weights_str.split", "named_path.find", "train_data_name.append", "train_data_path.append", "train_data_list.append", "range", "named_path.find", "eval_data_name.append", "eval_data_path.append", "eval_data_list.append", "len", "len", "flint.data_utils.fields.RawFlintField", "flint.data_utils.fields.LabelFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "training_extra.build_eval_dataset_loader_and_sampler", "eval_data_loaders.append", "len", "print", "training_extra.sample_data_list", "print", "training_list.extend", "int", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "model_class_item[].from_pretrained.cuda", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "torch.optim.Adam", "modeling.res_encoder.EmptyScheduler", "amp.initialize", "torch.parallel.DistributedDataParallel", "vars", "print", "print", "print", "print", "range", "range", "random.shuffle", "training_extra.NLIDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "print", "enumerate", "str", "model_class_item[].from_pretrained", "modeling.res_encoder.BagOfWords", "model_class_item[].from_pretrained", "utils.common.load_jsonl", "utils.common.load_jsonl", "train_data_weights.append", "len", "train_data_weights.append", "utils.common.load_jsonl", "utils.common.load_jsonl", "pp.pprint", "utils.save_tool.gen_file_prefix", "os.path.basename", "utils.common.save_json", "len", "print", "training_extra.sample_data_list", "print", "training_list.extend", "print", "torch.utils.data.DistributedSampler", "training_extra.debug_node_info", "torch.utils.data.DistributedSampler.set_epoch", "tqdm.tqdm", "model_class_item[].from_pretrained.train", "flint.data_utils.batchbuilder.move_to_device", "dict", "range", "range", "str", "embedding.weight.size", "embedding.weight.size", "float", "ImportError", "open", "open", "out_f.write", "out_f.flush", "os.path.join", "pathlib.Path", "checkpoints_path.exists", "checkpoints_path.mkdir", "pathlib.Path", "prediction_path.exists", "prediction_path.mkdir", "flint.data_utils.batchbuilder.BaseBatchBuilder", "model_class_item[].from_pretrained.", "model_class_item[].from_pretrained.", "loss.backward", "torch.optim.Adam.step", "modeling.res_encoder.EmptyScheduler.step", "model_class_item[].from_pretrained.zero_grad", "len", "training_extra.evaluation_dataset", "len", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "dict.items", "dict.items", "utils.common.save_json", "str", "embedding.weight.size", "embedding.weight.size", "str", "len", "len", "model_class_item[].from_pretrained.named_parameters", "model_class_item[].from_pretrained.named_parameters", "any", "os.path.join", "it.read", "amp.scale_loss", "scaled_loss.backward", "dict", "range", "range", "model_output_dir.exists", "model_output_dir.mkdir", "hasattr", "model_to_save.state_dict", "str", "torch.optim.Adam.state_dict", "str", "modeling.res_encoder.EmptyScheduler.state_dict", "str", "cur_results_path.exists", "cur_results_path.mkdir", "utils.common.save_jsonl", "any", "len", "len", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "len", "training_extra.evaluation_dataset", "len", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "dict.items", "dict.items", "utils.common.save_json", "round", "amp.master_params", "model_class_item[].from_pretrained.parameters", "model_output_dir.exists", "model_output_dir.mkdir", "hasattr", "model_to_save.state_dict", "str", "torch.optim.Adam.state_dict", "str", "modeling.res_encoder.EmptyScheduler.state_dict", "str", "cur_results_path.exists", "cur_results_path.mkdir", "utils.common.save_jsonl", "round"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.set_seed", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.build_eval_dataset_loader_and_sampler", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.sample_data_list", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_json", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.sample_data_list", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.debug_node_info", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.train", "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.step", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.step", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.evaluation_dataset", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_json", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.evaluation_dataset", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_json", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl"], ["", "", "def", "train", "(", "local_rank", ",", "args", ")", ":", "\n", "# debug = False", "\n", "# print(\"GPU:\", gpu)", "\n", "# world_size = args.world_size", "\n", "    ", "args", ".", "global_rank", "=", "args", ".", "node_rank", "*", "args", ".", "gpus_per_node", "+", "local_rank", "\n", "args", ".", "local_rank", "=", "local_rank", "\n", "# args.warmup_steps = 20", "\n", "debug_count", "=", "1000", "\n", "num_epoch", "=", "args", ".", "epochs", "\n", "\n", "actual_train_batch_size", "=", "args", ".", "world_size", "*", "args", ".", "per_gpu_train_batch_size", "*", "args", ".", "gradient_accumulation_steps", "\n", "args", ".", "actual_train_batch_size", "=", "actual_train_batch_size", "\n", "\n", "set_seed", "(", "args", ".", "seed", ")", "\n", "num_labels", "=", "3", "# we are doing NLI so we set num_labels = 3, for other task we can change this value.", "\n", "\n", "max_length", "=", "args", ".", "max_length", "\n", "\n", "model_class_item", "=", "MODEL_CLASSES", "[", "args", ".", "model_class_name", "]", "\n", "model_class_name", "=", "args", ".", "model_class_name", "\n", "model_name", "=", "model_class_item", "[", "'model_name'", "]", "\n", "do_lower_case", "=", "model_class_item", "[", "'do_lower_case'", "]", "if", "'do_lower_case'", "in", "model_class_item", "else", "False", "\n", "\n", "tokenizer", "=", "model_class_item", "[", "'tokenizer'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "if", "model_class_name", "in", "[", "'lstm-resencoder'", "]", ":", "\n", "        ", "hg_model", "=", "model_class_item", "[", "'sequence_classification'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "embedding", "=", "hg_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", "\n", "model", "=", "ResEncoder", "(", "v_size", "=", "embedding", ".", "weight", ".", "size", "(", "0", ")", ",", "embd_dim", "=", "embedding", ".", "weight", ".", "size", "(", "1", ")", ")", "\n", "model", ".", "Embd", ".", "weight", "=", "embedding", ".", "weight", "\n", "\n", "", "elif", "model_class_name", "in", "[", "'bag-of-words'", "]", ":", "\n", "        ", "hg_model", "=", "model_class_item", "[", "'sequence_classification'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "embedding", "=", "hg_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", "\n", "model", "=", "BagOfWords", "(", "v_size", "=", "embedding", ".", "weight", ".", "size", "(", "0", ")", ",", "embd_dim", "=", "embedding", ".", "weight", ".", "size", "(", "1", ")", ")", "\n", "model", ".", "Embd", ".", "weight", "=", "embedding", ".", "weight", "\n", "\n", "", "else", ":", "\n", "        ", "model", "=", "model_class_item", "[", "'sequence_classification'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "\n", "", "padding_token_value", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "tokenizer", ".", "pad_token", "]", ")", "[", "0", "]", "\n", "padding_segement_value", "=", "model_class_item", "[", "\"padding_segement_value\"", "]", "\n", "padding_att_value", "=", "model_class_item", "[", "\"padding_att_value\"", "]", "\n", "left_pad", "=", "model_class_item", "[", "'left_pad'", "]", "if", "'left_pad'", "in", "model_class_item", "else", "False", "\n", "\n", "batch_size_per_gpu_train", "=", "args", ".", "per_gpu_train_batch_size", "\n", "batch_size_per_gpu_eval", "=", "args", ".", "per_gpu_eval_batch_size", "\n", "\n", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "        ", "dist", ".", "init_process_group", "(", "\n", "backend", "=", "'nccl'", ",", "\n", "init_method", "=", "'env://'", ",", "\n", "world_size", "=", "args", ".", "world_size", ",", "\n", "rank", "=", "args", ".", "global_rank", "\n", ")", "\n", "\n", "", "train_data_str", "=", "args", ".", "train_data", "\n", "train_data_weights_str", "=", "args", ".", "train_weights", "\n", "eval_data_str", "=", "args", ".", "eval_data", "\n", "\n", "train_data_name", "=", "[", "]", "\n", "train_data_path", "=", "[", "]", "\n", "train_data_list", "=", "[", "]", "\n", "train_data_weights", "=", "[", "]", "\n", "\n", "eval_data_name", "=", "[", "]", "\n", "eval_data_path", "=", "[", "]", "\n", "eval_data_list", "=", "[", "]", "\n", "\n", "train_data_named_path", "=", "train_data_str", ".", "split", "(", "','", ")", "\n", "weights_str", "=", "train_data_weights_str", ".", "split", "(", "','", ")", "if", "train_data_weights_str", "is", "not", "None", "else", "None", "\n", "\n", "eval_data_named_path", "=", "eval_data_str", ".", "split", "(", "','", ")", "\n", "\n", "for", "named_path", "in", "train_data_named_path", ":", "\n", "        ", "ind", "=", "named_path", ".", "find", "(", "':'", ")", "\n", "name", "=", "named_path", "[", ":", "ind", "]", "\n", "path", "=", "name", "[", "ind", "+", "1", ":", "]", "\n", "if", "name", "in", "registered_path", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "registered_path", "[", "name", "]", ")", "\n", "", "else", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "path", ")", "\n", "\n", "", "train_data_name", ".", "append", "(", "name", ")", "\n", "train_data_path", ".", "append", "(", "path", ")", "\n", "\n", "train_data_list", ".", "append", "(", "d_list", ")", "\n", "\n", "", "if", "weights_str", "is", "not", "None", ":", "\n", "        ", "for", "weights", "in", "weights_str", ":", "\n", "            ", "train_data_weights", ".", "append", "(", "float", "(", "weights", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "train_data_list", ")", ")", ":", "\n", "            ", "train_data_weights", ".", "append", "(", "1", ")", "\n", "\n", "", "", "for", "named_path", "in", "eval_data_named_path", ":", "\n", "        ", "ind", "=", "named_path", ".", "find", "(", "':'", ")", "\n", "name", "=", "named_path", "[", ":", "ind", "]", "\n", "path", "=", "name", "[", "ind", "+", "1", ":", "]", "\n", "if", "name", "in", "registered_path", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "registered_path", "[", "name", "]", ")", "\n", "", "else", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "path", ")", "\n", "", "eval_data_name", ".", "append", "(", "name", ")", "\n", "eval_data_path", ".", "append", "(", "path", ")", "\n", "\n", "eval_data_list", ".", "append", "(", "d_list", ")", "\n", "\n", "", "assert", "len", "(", "train_data_weights", ")", "==", "len", "(", "train_data_list", ")", "\n", "\n", "batching_schema", "=", "{", "\n", "'uid'", ":", "RawFlintField", "(", ")", ",", "\n", "'y'", ":", "LabelFlintField", "(", ")", ",", "\n", "'input_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_token_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'token_type_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_segement_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'attention_mask'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_att_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "}", "\n", "\n", "data_transformer", "=", "NLITransform", "(", "model_name", ",", "tokenizer", ",", "max_length", ")", "\n", "# data_transformer = NLITransform(model_name, tokenizer, max_length, with_element=True)", "\n", "\n", "eval_data_loaders", "=", "[", "]", "\n", "for", "eval_d_list", "in", "eval_data_list", ":", "\n", "        ", "d_dataset", ",", "d_sampler", ",", "d_dataloader", "=", "build_eval_dataset_loader_and_sampler", "(", "eval_d_list", ",", "data_transformer", ",", "\n", "batching_schema", ",", "\n", "batch_size_per_gpu_eval", ")", "\n", "eval_data_loaders", ".", "append", "(", "d_dataloader", ")", "\n", "\n", "# Estimate the training size:", "\n", "", "training_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_data_list", ")", ")", ":", "\n", "        ", "print", "(", "\"Build Training Data ...\"", ")", "\n", "train_d_list", "=", "train_data_list", "[", "i", "]", "\n", "train_d_name", "=", "train_data_name", "[", "i", "]", "\n", "train_d_weight", "=", "train_data_weights", "[", "i", "]", "\n", "cur_train_list", "=", "sample_data_list", "(", "train_d_list", ",", "train_d_weight", ")", "# change later  # we can apply different sample strategy here.", "\n", "print", "(", "f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"", "\n", "f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\"", ")", "\n", "training_list", ".", "extend", "(", "cur_train_list", ")", "\n", "", "estimated_training_size", "=", "len", "(", "training_list", ")", "\n", "print", "(", "\"Estimated training size:\"", ",", "estimated_training_size", ")", "\n", "# Estimate the training size ends:", "\n", "\n", "# t_total = estimated_training_size // args.gradient_accumulation_steps * num_epoch", "\n", "t_total", "=", "estimated_training_size", "*", "num_epoch", "//", "args", ".", "actual_train_batch_size", "\n", "if", "args", ".", "warmup_steps", "<=", "0", ":", "# set the warmup steps to 0.1 * total step if the given warmup step is -1.", "\n", "        ", "args", ".", "warmup_steps", "=", "int", "(", "t_total", "*", "0.1", ")", "\n", "\n", "", "if", "not", "args", ".", "cpu", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "model", ".", "cuda", "(", "args", ".", "local_rank", ")", "\n", "\n", "", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay\"", ":", "0.0", "}", ",", "\n", "]", "\n", "\n", "if", "model_class_name", "not", "in", "[", "'lstm-resencoder'", "]", ":", "\n", "        ", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "t_total", "\n", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "Adam", "(", "optimizer_grouped_parameters", ")", "\n", "scheduler", "=", "EmptyScheduler", "(", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "args", ".", "fp16_opt_level", ")", "\n", "\n", "", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "        ", "model", "=", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "local_rank", "]", ",", "\n", "output_device", "=", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "\n", "", "args_dict", "=", "dict", "(", "vars", "(", "args", ")", ")", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "print", "(", "\"Total Steps:\"", ",", "t_total", ")", "\n", "args", ".", "total_step", "=", "t_total", "\n", "print", "(", "\"Warmup Steps:\"", ",", "args", ".", "warmup_steps", ")", "\n", "print", "(", "\"Actual Training Batch Size:\"", ",", "actual_train_batch_size", ")", "\n", "print", "(", "\"Arguments\"", ",", "pp", ".", "pprint", "(", "args", ")", ")", "\n", "\n", "# Let build the logger and log everything before the start of the first training epoch.", "\n", "", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", ":", "# only do logging if we use cpu or global_rank=0", "\n", "        ", "if", "not", "args", ".", "debug_mode", ":", "\n", "            ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{args.experiment_name}\"", ")", "\n", "# # # Create Log File", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "                ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "\n", "# Save option file", "\n", "", "common", ".", "save_json", "(", "args_dict", ",", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "\"args.json\"", ")", ")", "\n", "checkpoints_path", "=", "Path", "(", "file_path_prefix", ")", "/", "\"checkpoints\"", "\n", "if", "not", "checkpoints_path", ".", "exists", "(", ")", ":", "\n", "                ", "checkpoints_path", ".", "mkdir", "(", ")", "\n", "", "prediction_path", "=", "Path", "(", "file_path_prefix", ")", "/", "\"predictions\"", "\n", "if", "not", "prediction_path", ".", "exists", "(", ")", ":", "\n", "                ", "prediction_path", ".", "mkdir", "(", ")", "\n", "\n", "", "", "", "global_step", "=", "0", "\n", "\n", "# print(f\"Global Rank:{args.global_rank} ### \", 'Init!')", "\n", "\n", "for", "epoch", "in", "tqdm", "(", "range", "(", "num_epoch", ")", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "global_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", ":", "\n", "# Let's build up training dataset for this epoch", "\n", "        ", "training_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_data_list", ")", ")", ":", "\n", "            ", "print", "(", "\"Build Training Data ...\"", ")", "\n", "train_d_list", "=", "train_data_list", "[", "i", "]", "\n", "train_d_name", "=", "train_data_name", "[", "i", "]", "\n", "train_d_weight", "=", "train_data_weights", "[", "i", "]", "\n", "cur_train_list", "=", "sample_data_list", "(", "train_d_list", ",", "train_d_weight", ")", "# change later  # we can apply different sample strategy here.", "\n", "print", "(", "f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"", "\n", "f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\"", ")", "\n", "training_list", ".", "extend", "(", "cur_train_list", ")", "\n", "\n", "", "random", ".", "shuffle", "(", "training_list", ")", "\n", "train_dataset", "=", "NLIDataset", "(", "training_list", ",", "data_transformer", ")", "\n", "\n", "train_sampler", "=", "SequentialSampler", "(", "train_dataset", ")", "\n", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "            ", "print", "(", "\"Use distributed sampler.\"", ")", "\n", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ",", "args", ".", "world_size", ",", "args", ".", "global_rank", ",", "\n", "shuffle", "=", "True", ")", "\n", "\n", "", "train_dataloader", "=", "DataLoader", "(", "dataset", "=", "train_dataset", ",", "\n", "batch_size", "=", "batch_size_per_gpu_train", ",", "\n", "shuffle", "=", "False", ",", "#", "\n", "num_workers", "=", "0", ",", "\n", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "train_sampler", ",", "\n", "collate_fn", "=", "BaseBatchBuilder", "(", "batching_schema", ")", ")", "#", "\n", "# training build finished.", "\n", "\n", "print", "(", "debug_node_info", "(", "args", ")", ",", "\"epoch: \"", ",", "epoch", ")", "\n", "\n", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "            ", "train_sampler", ".", "set_epoch", "(", "epoch", ")", "# setup the epoch to ensure random sampling at each epoch", "\n", "\n", "", "for", "forward_step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ",", "\n", "disable", "=", "args", ".", "global_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", ",", "0", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "local_rank", ")", "\n", "# print(batch['input_ids'], batch['y'])", "\n", "if", "args", ".", "model_class_name", "in", "[", "\"distilbert\"", ",", "\"bart-large\"", ",", "\"lstm-resencoder\"", ",", "\"bag-of-words\"", "]", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "token_type_ids", "=", "batch", "[", "'token_type_ids'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "", "loss", ",", "logits", "=", "outputs", "[", ":", "2", "]", "\n", "# print(debug_node_info(args), loss, logits, batch['uid'])", "\n", "# print(debug_node_info(args), loss, batch['uid'])", "\n", "\n", "# Accumulated loss", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "# if this forward step need model updates", "\n", "# handle fp16", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient clip: if max_grad_norm < 0", "\n", "", "if", "(", "forward_step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "if", "args", ".", "max_grad_norm", ">", "0", ":", "\n", "                    ", "if", "args", ".", "fp16", ":", "\n", "                        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "optimizer", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "else", ":", "\n", "                        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "global_step", "+=", "1", "\n", "\n", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", "and", "args", ".", "eval_frequency", ">", "0", "and", "global_step", "%", "args", ".", "eval_frequency", "==", "0", ":", "\n", "                    ", "r_dict", "=", "dict", "(", ")", "\n", "# Eval loop:", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                        ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "cur_eval_data_list", "=", "eval_data_list", "[", "i", "]", "\n", "cur_eval_dataloader", "=", "eval_data_loaders", "[", "i", "]", "\n", "# cur_eval_raw_data_list = eval_raw_data_list[i]", "\n", "\n", "evaluation_dataset", "(", "args", ",", "cur_eval_dataloader", ",", "cur_eval_data_list", ",", "model", ",", "r_dict", ",", "\n", "eval_name", "=", "cur_eval_data_name", ")", "\n", "\n", "# saving checkpoints", "\n", "", "current_checkpoint_filename", "=", "f'e({epoch})|i({global_step})'", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                        ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "current_checkpoint_filename", "+=", "f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'", "\n", "\n", "", "if", "not", "args", ".", "debug_mode", ":", "\n", "# save model:", "\n", "                        ", "model_output_dir", "=", "checkpoints_path", "/", "current_checkpoint_filename", "\n", "if", "not", "model_output_dir", ".", "exists", "(", ")", ":", "\n", "                            ", "model_output_dir", ".", "mkdir", "(", ")", "\n", "", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"model.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"scheduler.pt\"", ")", ")", "\n", "\n", "# save prediction:", "\n", "", "if", "not", "args", ".", "debug_mode", "and", "args", ".", "save_prediction", ":", "\n", "                        ", "cur_results_path", "=", "prediction_path", "/", "current_checkpoint_filename", "\n", "if", "not", "cur_results_path", ".", "exists", "(", ")", ":", "\n", "                            ", "cur_results_path", ".", "mkdir", "(", "parents", "=", "True", ")", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                            ", "common", ".", "save_jsonl", "(", "item", "[", "'predictions'", "]", ",", "cur_results_path", "/", "f\"{key}.jsonl\"", ")", "\n", "\n", "# avoid saving too many things", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                            ", "del", "r_dict", "[", "key", "]", "[", "'predictions'", "]", "\n", "", "common", ".", "save_json", "(", "r_dict", ",", "cur_results_path", "/", "\"results_dict.json\"", ",", "indent", "=", "2", ")", "\n", "\n", "# End of epoch evaluation.", "\n", "", "", "", "", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "            ", "r_dict", "=", "dict", "(", ")", "\n", "# Eval loop:", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "cur_eval_data_list", "=", "eval_data_list", "[", "i", "]", "\n", "cur_eval_dataloader", "=", "eval_data_loaders", "[", "i", "]", "\n", "# cur_eval_raw_data_list = eval_raw_data_list[i]", "\n", "\n", "evaluation_dataset", "(", "args", ",", "cur_eval_dataloader", ",", "cur_eval_data_list", ",", "model", ",", "r_dict", ",", "\n", "eval_name", "=", "cur_eval_data_name", ")", "\n", "\n", "# saving checkpoints", "\n", "", "current_checkpoint_filename", "=", "f'e({epoch})|i({global_step})'", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "current_checkpoint_filename", "+=", "f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'", "\n", "\n", "", "if", "not", "args", ".", "debug_mode", ":", "\n", "# save model:", "\n", "                ", "model_output_dir", "=", "checkpoints_path", "/", "current_checkpoint_filename", "\n", "if", "not", "model_output_dir", ".", "exists", "(", ")", ":", "\n", "                    ", "model_output_dir", ".", "mkdir", "(", ")", "\n", "", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"model.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"scheduler.pt\"", ")", ")", "\n", "\n", "# save prediction:", "\n", "", "if", "not", "args", ".", "debug_mode", "and", "args", ".", "save_prediction", ":", "\n", "                ", "cur_results_path", "=", "prediction_path", "/", "current_checkpoint_filename", "\n", "if", "not", "cur_results_path", ".", "exists", "(", ")", ":", "\n", "                    ", "cur_results_path", ".", "mkdir", "(", "parents", "=", "True", ")", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                    ", "common", ".", "save_jsonl", "(", "item", "[", "'predictions'", "]", ",", "cur_results_path", "/", "f\"{key}.jsonl\"", ")", "\n", "\n", "# avoid saving too many things", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                    ", "del", "r_dict", "[", "key", "]", "[", "'predictions'", "]", "\n", "", "common", ".", "save_json", "(", "r_dict", ",", "cur_results_path", "/", "\"results_dict.json\"", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.count_acc": [[812, 823], ["utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "list_dict_data_tool.list_to_dict.items", "len", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.list_dict_data_tool.list_to_dict"], ["def", "count_acc", "(", "gt_list", ",", "pred_list", ")", ":", "\n", "    ", "assert", "len", "(", "gt_list", ")", "==", "len", "(", "pred_list", ")", "\n", "gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "gt_list", ",", "'uid'", ")", "\n", "pred_list", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "pred_list", ",", "'uid'", ")", "\n", "total_count", "=", "0", "\n", "hit", "=", "0", "\n", "for", "key", ",", "value", "in", "pred_list", ".", "items", "(", ")", ":", "\n", "        ", "if", "gt_dict", "[", "key", "]", "[", "'label'", "]", "==", "value", "[", "'predicted_label'", "]", ":", "\n", "            ", "hit", "+=", "1", "\n", "", "total_count", "+=", "1", "\n", "", "return", "hit", ",", "total_count", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.evaluation_dataset": [[825, 838], ["training_extra.eval_model", "training_extra.count_acc", "print", "training_extra.debug_node_info"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.eval_model", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.count_acc", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.debug_node_info"], ["", "def", "evaluation_dataset", "(", "args", ",", "eval_dataloader", ",", "eval_list", ",", "model", ",", "r_dict", ",", "eval_name", ")", ":", "\n", "# r_dict = dict()", "\n", "    ", "pred_output_list", "=", "eval_model", "(", "model", ",", "eval_dataloader", ",", "args", ".", "global_rank", ",", "args", ")", "\n", "predictions", "=", "pred_output_list", "\n", "hit", ",", "total", "=", "count_acc", "(", "eval_list", ",", "pred_output_list", ")", "\n", "\n", "print", "(", "debug_node_info", "(", "args", ")", ",", "f\"{eval_name} Acc:\"", ",", "hit", ",", "total", ",", "hit", "/", "total", ")", "\n", "\n", "r_dict", "[", "f'{eval_name}'", "]", "=", "{", "\n", "'acc'", ":", "hit", "/", "total", ",", "\n", "'correct_count'", ":", "hit", ",", "\n", "'total_count'", ":", "total", ",", "\n", "'predictions'", ":", "predictions", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.eval_model": [[841, 883], ["model.eval", "range", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "flint.data_utils.batchbuilder.move_to_device", "uid_list.extend", "y_list.extend", "pred_list.extend", "logits_list.extend", "model", "model", "list", "batch[].tolist", "[].view().tolist", "logits.tolist", "[].view", "logits.size", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device"], ["", "def", "eval_model", "(", "model", ",", "dev_dataloader", ",", "device_num", ",", "args", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "uid_list", "=", "[", "]", "\n", "y_list", "=", "[", "]", "\n", "pred_list", "=", "[", "]", "\n", "logits_list", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "dev_dataloader", ",", "0", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "if", "args", ".", "model_class_name", "in", "[", "\"distilbert\"", ",", "\"bart-large\"", ",", "'lstm-resencoder'", ",", "\"bag-of-words\"", "]", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "token_type_ids", "=", "batch", "[", "'token_type_ids'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "\n", "", "loss", ",", "logits", "=", "outputs", "[", ":", "2", "]", "\n", "\n", "uid_list", ".", "extend", "(", "list", "(", "batch", "[", "'uid'", "]", ")", ")", "\n", "y_list", ".", "extend", "(", "batch", "[", "'y'", "]", ".", "tolist", "(", ")", ")", "\n", "pred_list", ".", "extend", "(", "torch", ".", "max", "(", "logits", ",", "1", ")", "[", "1", "]", ".", "view", "(", "logits", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "logits_list", ".", "extend", "(", "logits", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "logits_list", ")", "\n", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "logits_list", ")", "\n", "\n", "result_items_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "uid_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'uid'", "]", "=", "uid_list", "[", "i", "]", "\n", "r_item", "[", "'logits'", "]", "=", "logits_list", "[", "i", "]", "\n", "r_item", "[", "'predicted_label'", "]", "=", "id2label", "[", "pred_list", "[", "i", "]", "]", "\n", "\n", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training_extra.debug_node_info": [[885, 896], ["values.append", "getattr", "zip"], "function", ["None"], ["", "def", "debug_node_info", "(", "args", ")", ":", "\n", "    ", "names", "=", "[", "'global_rank'", ",", "'local_rank'", ",", "'node_rank'", "]", "\n", "values", "=", "[", "]", "\n", "\n", "for", "name", "in", "names", ":", "\n", "        ", "if", "name", "in", "args", ":", "\n", "            ", "values", ".", "append", "(", "getattr", "(", "args", ",", "name", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "\"Pro:No node info \"", "\n", "\n", "", "", "return", "\"Pro:\"", "+", "'|'", ".", "join", "(", "[", "f\"{name}:{value}\"", "for", "name", ",", "value", "in", "zip", "(", "names", ",", "values", ")", "]", ")", "+", "\"||Print:\"", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.NLIDataset.__init__": [[197, 202], ["torch.utils.data.Dataset.__init__", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data_list", ",", "transform", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "d_list", "=", "data_list", "\n", "self", ".", "len", "=", "len", "(", "self", ".", "d_list", ")", "\n", "self", ".", "transform", "=", "transform", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.NLIDataset.__getitem__": [[203, 205], ["training.NLIDataset.transform"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "return", "self", ".", "transform", "(", "self", ".", "d_list", "[", "index", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.NLIDataset.__len__": [[208, 210], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "len", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.NLITransform.__init__": [[213, 217], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "tokenizer", ",", "max_length", "=", "None", ")", ":", "\n", "        ", "self", ".", "model_name", "=", "model_name", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "max_length", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.NLITransform.__call__": [[218, 241], ["dict", "training.NLITransform.tokenizer.encode_plus", "dict.update", "premise.strip", "hypothesis.strip"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "sample", ")", ":", "\n", "        ", "processed_sample", "=", "dict", "(", ")", "\n", "processed_sample", "[", "'uid'", "]", "=", "sample", "[", "'uid'", "]", "\n", "processed_sample", "[", "'gold_label'", "]", "=", "sample", "[", "'label'", "]", "\n", "processed_sample", "[", "'y'", "]", "=", "nli_label2index", "[", "sample", "[", "'label'", "]", "]", "\n", "\n", "# premise: str = sample['premise']", "\n", "premise", ":", "str", "=", "sample", "[", "'context'", "]", "if", "'context'", "in", "sample", "else", "sample", "[", "'premise'", "]", "\n", "hypothesis", ":", "str", "=", "sample", "[", "'hypothesis'", "]", "\n", "\n", "if", "premise", ".", "strip", "(", ")", "==", "''", ":", "\n", "            ", "premise", "=", "'empty'", "\n", "\n", "", "if", "hypothesis", ".", "strip", "(", ")", "==", "''", ":", "\n", "            ", "hypothesis", "=", "'empty'", "\n", "\n", "", "tokenized_input_seq_pair", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "premise", ",", "hypothesis", ",", "\n", "max_length", "=", "self", ".", "max_length", ",", "\n", "return_token_type_ids", "=", "True", ",", "truncation", "=", "True", ")", "\n", "\n", "processed_sample", ".", "update", "(", "tokenized_input_seq_pair", ")", "\n", "\n", "return", "processed_sample", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.set_seed": [[190, 194], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed"], "function", ["None"], ["def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.build_eval_dataset_loader_and_sampler": [[243, 254], ["training.NLIDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "flint.data_utils.batchbuilder.BaseBatchBuilder"], "function", ["None"], ["", "", "def", "build_eval_dataset_loader_and_sampler", "(", "d_list", ",", "data_transformer", ",", "batching_schema", ",", "batch_size_per_gpu_eval", ")", ":", "\n", "    ", "d_dataset", "=", "NLIDataset", "(", "d_list", ",", "data_transformer", ")", "\n", "d_sampler", "=", "SequentialSampler", "(", "d_dataset", ")", "\n", "d_dataloader", "=", "DataLoader", "(", "dataset", "=", "d_dataset", ",", "\n", "batch_size", "=", "batch_size_per_gpu_eval", ",", "\n", "shuffle", "=", "False", ",", "#", "\n", "num_workers", "=", "0", ",", "\n", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "d_sampler", ",", "\n", "collate_fn", "=", "BaseBatchBuilder", "(", "batching_schema", ")", ")", "#", "\n", "return", "d_dataset", ",", "d_sampler", ",", "d_dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.sample_data_list": [[256, 272], ["int", "ValueError", "math.ceil", "range", "numpy.isclose", "sampled_d_list.extend", "int", "random.shuffle", "copy.deepcopy", "len"], "function", ["None"], ["", "def", "sample_data_list", "(", "d_list", ",", "ratio", ")", ":", "\n", "    ", "if", "ratio", "<=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid training weight ratio. Please change --train_weights.\"", ")", "\n", "", "upper_int", "=", "int", "(", "math", ".", "ceil", "(", "ratio", ")", ")", "\n", "if", "upper_int", "==", "1", ":", "\n", "        ", "return", "d_list", "# if ratio is 1 then we just return the data list", "\n", "", "else", ":", "\n", "        ", "sampled_d_list", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "upper_int", ")", ":", "\n", "            ", "sampled_d_list", ".", "extend", "(", "copy", ".", "deepcopy", "(", "d_list", ")", ")", "\n", "", "if", "np", ".", "isclose", "(", "ratio", ",", "upper_int", ")", ":", "\n", "            ", "return", "sampled_d_list", "\n", "", "else", ":", "\n", "            ", "sampled_length", "=", "int", "(", "ratio", "*", "len", "(", "d_list", ")", ")", "\n", "random", ".", "shuffle", "(", "sampled_d_list", ")", "\n", "return", "sampled_d_list", "[", ":", "sampled_length", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.main": [[274, 396], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "training.train", "training.train", "torch.spawn"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.train", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.train"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--cpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we only use CPU.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--single_gpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we only use single GPU.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp16\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we will use fp16.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fp16_opt_level\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"O1\"", ",", "\n", "help", "=", "\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"", "\n", "\"See details at https://nvidia.github.io/apex/amp.html\"", ",", "\n", ")", "\n", "\n", "# environment arguments", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--seed'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'manual random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--num_nodes'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of nodes'", ")", "\n", "parser", ".", "add_argument", "(", "'-g'", ",", "'--gpus_per_node'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of gpus per node'", ")", "\n", "parser", ".", "add_argument", "(", "'-nr'", ",", "'--node_rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'ranking within the nodes'", ")", "\n", "\n", "# experiments specific arguments", "\n", "parser", ".", "add_argument", "(", "'--debug_mode'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "dest", "=", "'debug_mode'", ",", "\n", "help", "=", "'weather this is debug mode or normal'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_class_name\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Set the model class of the experiment.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--experiment_name\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Set the name of the experiment. [model_name]/[data]/[task]/[other]\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--save_prediction\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "dest", "=", "'save_prediction'", ",", "\n", "help", "=", "'Do we want to save prediction'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--resume_path\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"If we want to resume model training, we need to set the resume path to restore state dicts.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--global_iteration\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "0", ",", "\n", "help", "=", "\"This argument is only used if we resume model training.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "default", "=", "2", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of total epochs to run'", ")", "\n", "parser", ".", "add_argument", "(", "'--total_step'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of step to update, default calculate with total data size.'", "\n", "'if we set this step, then epochs will be 100 to run forever.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--sampler_seed'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'The seed the controls the data sampling order.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_train_batch_size\"", ",", "default", "=", "16", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for training.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--gradient_accumulation_steps\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "64", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for evaluation.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "default", "=", "160", ",", "type", "=", "int", ",", "help", "=", "\"Max length of the sequences.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "\"Max gradient norm.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "1e-5", ",", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "help", "=", "\"Weight decay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--eval_frequency\"", ",", "default", "=", "1000", ",", "type", "=", "int", ",", "help", "=", "\"set the evaluation frequency, evaluate every X global step.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--train_data\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data used in the experiments.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--train_weights\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data weights used in the experiments.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--eval_data\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data used in the experiments.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "cpu", ":", "\n", "        ", "args", ".", "world_size", "=", "1", "\n", "train", "(", "-", "1", ",", "args", ")", "\n", "", "elif", "args", ".", "single_gpu", ":", "\n", "        ", "args", ".", "world_size", "=", "1", "\n", "train", "(", "0", ",", "args", ")", "\n", "", "else", ":", "# distributed multiGPU training", "\n", "#########################################################", "\n", "        ", "args", ".", "world_size", "=", "args", ".", "gpus_per_node", "*", "args", ".", "num_nodes", "#", "\n", "# os.environ['MASTER_ADDR'] = '152.2.142.184'  # This is the IP address for nlp5", "\n", "# maybe we will automatically retrieve the IP later.", "\n", "os", ".", "environ", "[", "'MASTER_PORT'", "]", "=", "'88888'", "#", "\n", "mp", ".", "spawn", "(", "train", ",", "nprocs", "=", "args", ".", "gpus_per_node", ",", "args", "=", "(", "args", ",", ")", ")", "# spawn how many process in this node", "\n", "# remember train is called as train(i, args).", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.train": [[400, 816], ["training.set_seed", "model_class_item[].from_pretrained", "model_class_item[].from_pretrained", "train_data_str.split", "eval_data_str.split", "training.NLITransform", "range", "len", "print", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "dict", "tqdm.tqdm", "model_class_item[].from_pretrained.convert_tokens_to_ids", "torch.init_process_group", "train_data_weights_str.split", "named_path.find", "train_data_name.append", "train_data_path.append", "train_data_list.append", "range", "named_path.find", "eval_data_name.append", "eval_data_path.append", "eval_data_list.append", "len", "len", "flint.data_utils.fields.RawFlintField", "flint.data_utils.fields.LabelFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "training.build_eval_dataset_loader_and_sampler", "eval_data_loaders.append", "len", "print", "training.sample_data_list", "print", "training_list.extend", "int", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "nn.parallel.DistributedDataParallel.cuda", "print", "print", "nn.parallel.DistributedDataParallel.load_state_dict", "transformers.AdamW.load_state_dict", "transformers.get_linear_schedule_with_warmup.load_state_dict", "print", "amp.initialize", "torch.parallel.DistributedDataParallel", "vars", "print", "print", "print", "print", "range", "range", "random.shuffle", "training.NLIDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "print", "enumerate", "str", "str", "utils.common.load_jsonl", "utils.common.load_jsonl", "train_data_weights.append", "len", "train_data_weights.append", "utils.common.load_jsonl", "utils.common.load_jsonl", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "pp.pprint", "utils.save_tool.gen_file_prefix", "os.path.basename", "utils.common.save_json", "len", "print", "training.sample_data_list", "print", "training_list.extend", "print", "torch.utils.data.DistributedSampler", "training.debug_node_info", "tqdm.tqdm", "nn.parallel.DistributedDataParallel.train", "flint.data_utils.batchbuilder.move_to_device", "dict", "range", "range", "float", "str", "str", "str", "ImportError", "open", "open", "out_f.write", "out_f.flush", "os.path.join", "pathlib.Path", "checkpoints_path.exists", "checkpoints_path.mkdir", "pathlib.Path", "prediction_path.exists", "prediction_path.mkdir", "flint.data_utils.batchbuilder.BaseBatchBuilder", "torch.utils.data.DistributedSampler.set_epoch", "torch.utils.data.DistributedSampler.set_epoch", "nn.parallel.DistributedDataParallel.", "nn.parallel.DistributedDataParallel.", "loss.backward", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "nn.parallel.DistributedDataParallel.zero_grad", "len", "training.evaluation_dataset", "len", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "dict.items", "dict.items", "utils.common.save_json", "len", "len", "nn.parallel.DistributedDataParallel.named_parameters", "nn.parallel.DistributedDataParallel.named_parameters", "any", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "os.path.join", "it.read", "open", "out_f.write", "out_f.flush", "amp.scale_loss", "scaled_loss.backward", "dict", "range", "range", "model_output_dir.exists", "model_output_dir.mkdir", "hasattr", "model_to_save.state_dict", "str", "transformers.AdamW.state_dict", "str", "transformers.get_linear_schedule_with_warmup.state_dict", "str", "cur_results_path.exists", "cur_results_path.mkdir", "utils.common.save_jsonl", "any", "pathlib.Path", "pathlib.Path", "pathlib.Path", "os.path.join", "str", "len", "len", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "len", "training.evaluation_dataset", "len", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "dict.items", "dict.items", "utils.common.save_json", "round", "amp.master_params", "nn.parallel.DistributedDataParallel.parameters", "model_output_dir.exists", "model_output_dir.mkdir", "hasattr", "model_to_save.state_dict", "str", "transformers.AdamW.state_dict", "str", "transformers.get_linear_schedule_with_warmup.state_dict", "str", "cur_results_path.exists", "cur_results_path.mkdir", "utils.common.save_jsonl", "round"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.set_seed", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.build_eval_dataset_loader_and_sampler", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.sample_data_list", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_json", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.sample_data_list", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.debug_node_info", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.train", "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.step", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.step", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.evaluation_dataset", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_json", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.evaluation_dataset", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_json", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl"], ["", "", "def", "train", "(", "local_rank", ",", "args", ")", ":", "\n", "# debug = False", "\n", "# print(\"GPU:\", gpu)", "\n", "# world_size = args.world_size", "\n", "    ", "args", ".", "global_rank", "=", "args", ".", "node_rank", "*", "args", ".", "gpus_per_node", "+", "local_rank", "\n", "args", ".", "local_rank", "=", "local_rank", "\n", "# args.warmup_steps = 20", "\n", "debug_count", "=", "1000", "\n", "\n", "if", "args", ".", "total_step", ">", "0", ":", "\n", "        ", "num_epoch", "=", "10000", "# if we set total step, num_epoch will be forever.", "\n", "", "else", ":", "\n", "        ", "num_epoch", "=", "args", ".", "epochs", "\n", "\n", "", "actual_train_batch_size", "=", "args", ".", "world_size", "*", "args", ".", "per_gpu_train_batch_size", "*", "args", ".", "gradient_accumulation_steps", "\n", "args", ".", "actual_train_batch_size", "=", "actual_train_batch_size", "\n", "\n", "set_seed", "(", "args", ".", "seed", ")", "\n", "num_labels", "=", "3", "# we are doing NLI so we set num_labels = 3, for other task we can change this value.", "\n", "\n", "max_length", "=", "args", ".", "max_length", "\n", "\n", "model_class_item", "=", "MODEL_CLASSES", "[", "args", ".", "model_class_name", "]", "\n", "model_name", "=", "model_class_item", "[", "'model_name'", "]", "\n", "do_lower_case", "=", "model_class_item", "[", "'do_lower_case'", "]", "if", "'do_lower_case'", "in", "model_class_item", "else", "False", "\n", "\n", "tokenizer", "=", "model_class_item", "[", "'tokenizer'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "model", "=", "model_class_item", "[", "'sequence_classification'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "\n", "padding_token_value", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "tokenizer", ".", "pad_token", "]", ")", "[", "0", "]", "\n", "padding_segement_value", "=", "model_class_item", "[", "\"padding_segement_value\"", "]", "\n", "padding_att_value", "=", "model_class_item", "[", "\"padding_att_value\"", "]", "\n", "left_pad", "=", "model_class_item", "[", "'left_pad'", "]", "if", "'left_pad'", "in", "model_class_item", "else", "False", "\n", "\n", "batch_size_per_gpu_train", "=", "args", ".", "per_gpu_train_batch_size", "\n", "batch_size_per_gpu_eval", "=", "args", ".", "per_gpu_eval_batch_size", "\n", "\n", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "        ", "dist", ".", "init_process_group", "(", "\n", "backend", "=", "'nccl'", ",", "\n", "init_method", "=", "'env://'", ",", "\n", "world_size", "=", "args", ".", "world_size", ",", "\n", "rank", "=", "args", ".", "global_rank", "\n", ")", "\n", "\n", "", "train_data_str", "=", "args", ".", "train_data", "\n", "train_data_weights_str", "=", "args", ".", "train_weights", "\n", "eval_data_str", "=", "args", ".", "eval_data", "\n", "\n", "train_data_name", "=", "[", "]", "\n", "train_data_path", "=", "[", "]", "\n", "train_data_list", "=", "[", "]", "\n", "train_data_weights", "=", "[", "]", "\n", "\n", "eval_data_name", "=", "[", "]", "\n", "eval_data_path", "=", "[", "]", "\n", "eval_data_list", "=", "[", "]", "\n", "\n", "train_data_named_path", "=", "train_data_str", ".", "split", "(", "','", ")", "\n", "weights_str", "=", "train_data_weights_str", ".", "split", "(", "','", ")", "if", "train_data_weights_str", "is", "not", "None", "else", "None", "\n", "\n", "eval_data_named_path", "=", "eval_data_str", ".", "split", "(", "','", ")", "\n", "\n", "for", "named_path", "in", "train_data_named_path", ":", "\n", "        ", "ind", "=", "named_path", ".", "find", "(", "':'", ")", "\n", "name", "=", "named_path", "[", ":", "ind", "]", "\n", "path", "=", "named_path", "[", "ind", "+", "1", ":", "]", "\n", "if", "name", "in", "registered_path", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "registered_path", "[", "name", "]", ")", "\n", "", "else", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "path", ")", "\n", "\n", "", "train_data_name", ".", "append", "(", "name", ")", "\n", "train_data_path", ".", "append", "(", "path", ")", "\n", "\n", "train_data_list", ".", "append", "(", "d_list", ")", "\n", "\n", "", "if", "weights_str", "is", "not", "None", ":", "\n", "        ", "for", "weights", "in", "weights_str", ":", "\n", "            ", "train_data_weights", ".", "append", "(", "float", "(", "weights", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "train_data_list", ")", ")", ":", "\n", "            ", "train_data_weights", ".", "append", "(", "1", ")", "\n", "\n", "", "", "for", "named_path", "in", "eval_data_named_path", ":", "\n", "        ", "ind", "=", "named_path", ".", "find", "(", "':'", ")", "\n", "name", "=", "named_path", "[", ":", "ind", "]", "\n", "path", "=", "named_path", "[", "ind", "+", "1", ":", "]", "\n", "if", "name", "in", "registered_path", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "registered_path", "[", "name", "]", ")", "\n", "", "else", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "path", ")", "\n", "", "eval_data_name", ".", "append", "(", "name", ")", "\n", "eval_data_path", ".", "append", "(", "path", ")", "\n", "\n", "eval_data_list", ".", "append", "(", "d_list", ")", "\n", "\n", "", "assert", "len", "(", "train_data_weights", ")", "==", "len", "(", "train_data_list", ")", "\n", "\n", "batching_schema", "=", "{", "\n", "'uid'", ":", "RawFlintField", "(", ")", ",", "\n", "'y'", ":", "LabelFlintField", "(", ")", ",", "\n", "'input_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_token_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'token_type_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_segement_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'attention_mask'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_att_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "}", "\n", "\n", "data_transformer", "=", "NLITransform", "(", "model_name", ",", "tokenizer", ",", "max_length", ")", "\n", "# data_transformer = NLITransform(model_name, tokenizer, max_length, with_element=True)", "\n", "\n", "eval_data_loaders", "=", "[", "]", "\n", "for", "eval_d_list", "in", "eval_data_list", ":", "\n", "        ", "d_dataset", ",", "d_sampler", ",", "d_dataloader", "=", "build_eval_dataset_loader_and_sampler", "(", "eval_d_list", ",", "data_transformer", ",", "\n", "batching_schema", ",", "\n", "batch_size_per_gpu_eval", ")", "\n", "eval_data_loaders", ".", "append", "(", "d_dataloader", ")", "\n", "\n", "# Estimate the training size:", "\n", "", "training_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_data_list", ")", ")", ":", "\n", "        ", "print", "(", "\"Build Training Data ...\"", ")", "\n", "train_d_list", "=", "train_data_list", "[", "i", "]", "\n", "train_d_name", "=", "train_data_name", "[", "i", "]", "\n", "train_d_weight", "=", "train_data_weights", "[", "i", "]", "\n", "cur_train_list", "=", "sample_data_list", "(", "train_d_list", ",", "train_d_weight", ")", "# change later  # we can apply different sample strategy here.", "\n", "print", "(", "f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"", "\n", "f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\"", ")", "\n", "training_list", ".", "extend", "(", "cur_train_list", ")", "\n", "", "estimated_training_size", "=", "len", "(", "training_list", ")", "\n", "print", "(", "\"Estimated training size:\"", ",", "estimated_training_size", ")", "\n", "# Estimate the training size ends:", "\n", "\n", "# t_total = estimated_training_size // args.gradient_accumulation_steps * num_epoch", "\n", "# t_total = estimated_training_size * num_epoch // args.actual_train_batch_size", "\n", "if", "args", ".", "total_step", "<=", "0", ":", "\n", "        ", "t_total", "=", "estimated_training_size", "*", "num_epoch", "//", "args", ".", "actual_train_batch_size", "\n", "", "else", ":", "\n", "        ", "t_total", "=", "args", ".", "total_step", "\n", "\n", "", "if", "args", ".", "warmup_steps", "<=", "0", ":", "# set the warmup steps to 0.1 * total step if the given warmup step is -1.", "\n", "        ", "args", ".", "warmup_steps", "=", "int", "(", "t_total", "*", "0.1", ")", "\n", "\n", "", "if", "not", "args", ".", "cpu", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "model", ".", "cuda", "(", "args", ".", "local_rank", ")", "\n", "\n", "", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay\"", ":", "0.0", "}", ",", "\n", "]", "\n", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "t_total", "\n", ")", "\n", "\n", "global_step", "=", "0", "\n", "\n", "if", "args", ".", "resume_path", ":", "\n", "        ", "print", "(", "\"Resume Training\"", ")", "\n", "global_step", "=", "args", ".", "global_iteration", "\n", "print", "(", "\"Resume Global Step: \"", ",", "global_step", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "str", "(", "Path", "(", "args", ".", "resume_path", ")", "/", "\"model.pt\"", ")", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ")", "\n", "optimizer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "str", "(", "Path", "(", "args", ".", "resume_path", ")", "/", "\"optimizer.pt\"", ")", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ")", "\n", "scheduler", ".", "load_state_dict", "(", "torch", ".", "load", "(", "str", "(", "Path", "(", "args", ".", "resume_path", ")", "/", "\"scheduler.pt\"", ")", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ")", "\n", "print", "(", "\"State Resumed\"", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", "import", "amp", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"", ")", "\n", "", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "args", ".", "fp16_opt_level", ")", "\n", "\n", "", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "        ", "model", "=", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "local_rank", "]", ",", "\n", "output_device", "=", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "\n", "", "args_dict", "=", "dict", "(", "vars", "(", "args", ")", ")", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", ":", "\n", "        ", "print", "(", "\"Total Steps:\"", ",", "t_total", ")", "\n", "args", ".", "total_step", "=", "t_total", "\n", "print", "(", "\"Warmup Steps:\"", ",", "args", ".", "warmup_steps", ")", "\n", "print", "(", "\"Actual Training Batch Size:\"", ",", "actual_train_batch_size", ")", "\n", "print", "(", "\"Arguments\"", ",", "pp", ".", "pprint", "(", "args", ")", ")", "\n", "\n", "", "is_finished", "=", "False", "\n", "\n", "# Let build the logger and log everything before the start of the first training epoch.", "\n", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", ":", "# only do logging if we use cpu or global_rank=0", "\n", "        ", "resume_prefix", "=", "\"\"", "\n", "# if args.resume_path:", "\n", "#     resume_prefix = \"resumed_\"", "\n", "\n", "if", "not", "args", ".", "debug_mode", ":", "\n", "            ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{args.experiment_name}\"", ")", "\n", "# # # Create Log File", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "                ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "\n", "# Save option file", "\n", "", "common", ".", "save_json", "(", "args_dict", ",", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "\"args.json\"", ")", ")", "\n", "checkpoints_path", "=", "Path", "(", "file_path_prefix", ")", "/", "\"checkpoints\"", "\n", "if", "not", "checkpoints_path", ".", "exists", "(", ")", ":", "\n", "                ", "checkpoints_path", ".", "mkdir", "(", ")", "\n", "", "prediction_path", "=", "Path", "(", "file_path_prefix", ")", "/", "\"predictions\"", "\n", "if", "not", "prediction_path", ".", "exists", "(", ")", ":", "\n", "                ", "prediction_path", ".", "mkdir", "(", ")", "\n", "\n", "# if this is a resumed, then we save the resumed path.", "\n", "", "if", "args", ".", "resume_path", ":", "\n", "                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "\"resume_log.txt\"", ")", ",", "'w'", ")", "as", "out_f", ":", "\n", "                    ", "out_f", ".", "write", "(", "str", "(", "args", ".", "resume_path", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "\n", "# print(f\"Global Rank:{args.global_rank} ### \", 'Init!')", "\n", "\n", "", "", "", "", "for", "epoch", "in", "tqdm", "(", "range", "(", "num_epoch", ")", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "global_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", ":", "\n", "# Let's build up training dataset for this epoch", "\n", "        ", "training_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_data_list", ")", ")", ":", "\n", "            ", "print", "(", "\"Build Training Data ...\"", ")", "\n", "train_d_list", "=", "train_data_list", "[", "i", "]", "\n", "train_d_name", "=", "train_data_name", "[", "i", "]", "\n", "train_d_weight", "=", "train_data_weights", "[", "i", "]", "\n", "cur_train_list", "=", "sample_data_list", "(", "train_d_list", ",", "train_d_weight", ")", "# change later  # we can apply different sample strategy here.", "\n", "print", "(", "f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"", "\n", "f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\"", ")", "\n", "training_list", ".", "extend", "(", "cur_train_list", ")", "\n", "\n", "", "random", ".", "shuffle", "(", "training_list", ")", "\n", "train_dataset", "=", "NLIDataset", "(", "training_list", ",", "data_transformer", ")", "\n", "\n", "train_sampler", "=", "SequentialSampler", "(", "train_dataset", ")", "\n", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "            ", "print", "(", "\"Use distributed sampler.\"", ")", "\n", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ",", "args", ".", "world_size", ",", "args", ".", "global_rank", ",", "\n", "shuffle", "=", "True", ")", "\n", "\n", "", "train_dataloader", "=", "DataLoader", "(", "dataset", "=", "train_dataset", ",", "\n", "batch_size", "=", "batch_size_per_gpu_train", ",", "\n", "shuffle", "=", "False", ",", "#", "\n", "num_workers", "=", "0", ",", "\n", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "train_sampler", ",", "\n", "collate_fn", "=", "BaseBatchBuilder", "(", "batching_schema", ")", ")", "#", "\n", "# training build finished.", "\n", "\n", "print", "(", "debug_node_info", "(", "args", ")", ",", "\"epoch: \"", ",", "epoch", ")", "\n", "\n", "if", "not", "args", ".", "cpu", "and", "not", "args", ".", "single_gpu", ":", "\n", "            ", "if", "args", ".", "sampler_seed", "==", "-", "1", ":", "\n", "                ", "train_sampler", ".", "set_epoch", "(", "epoch", ")", "# setup the epoch to ensure random sampling at each epoch", "\n", "", "else", ":", "\n", "                ", "train_sampler", ".", "set_epoch", "(", "epoch", "+", "args", ".", "sampler_seed", ")", "\n", "\n", "", "", "for", "forward_step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ",", "\n", "disable", "=", "args", ".", "global_rank", "not", "in", "[", "-", "1", ",", "0", "]", ")", ",", "0", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "local_rank", ")", "\n", "# print(batch['input_ids'], batch['y'])", "\n", "if", "args", ".", "model_class_name", "in", "[", "\"distilbert\"", ",", "\"bart-large\"", "]", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "token_type_ids", "=", "batch", "[", "'token_type_ids'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "", "loss", ",", "logits", "=", "outputs", "[", ":", "2", "]", "\n", "# print(debug_node_info(args), loss, logits, batch['uid'])", "\n", "# print(debug_node_info(args), loss, batch['uid'])", "\n", "\n", "# Accumulated loss", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "# if this forward step need model updates", "\n", "# handle fp16", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient clip: if max_grad_norm < 0", "\n", "", "if", "(", "forward_step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "if", "args", ".", "max_grad_norm", ">", "0", ":", "\n", "                    ", "if", "args", ".", "fp16", ":", "\n", "                        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "optimizer", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "else", ":", "\n", "                        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "# Update learning rate schedule", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "global_step", "+=", "1", "\n", "\n", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", "and", "args", ".", "eval_frequency", ">", "0", "and", "global_step", "%", "args", ".", "eval_frequency", "==", "0", ":", "\n", "                    ", "r_dict", "=", "dict", "(", ")", "\n", "# Eval loop:", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                        ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "cur_eval_data_list", "=", "eval_data_list", "[", "i", "]", "\n", "cur_eval_dataloader", "=", "eval_data_loaders", "[", "i", "]", "\n", "# cur_eval_raw_data_list = eval_raw_data_list[i]", "\n", "\n", "evaluation_dataset", "(", "args", ",", "cur_eval_dataloader", ",", "cur_eval_data_list", ",", "model", ",", "r_dict", ",", "\n", "eval_name", "=", "cur_eval_data_name", ")", "\n", "\n", "# saving checkpoints", "\n", "", "current_checkpoint_filename", "=", "f'e({epoch})|i({global_step})'", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                        ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "current_checkpoint_filename", "+=", "f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'", "\n", "\n", "", "if", "not", "args", ".", "debug_mode", ":", "\n", "# save model:", "\n", "                        ", "model_output_dir", "=", "checkpoints_path", "/", "current_checkpoint_filename", "\n", "if", "not", "model_output_dir", ".", "exists", "(", ")", ":", "\n", "                            ", "model_output_dir", ".", "mkdir", "(", ")", "\n", "", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"model.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"scheduler.pt\"", ")", ")", "\n", "\n", "# save prediction:", "\n", "", "if", "not", "args", ".", "debug_mode", "and", "args", ".", "save_prediction", ":", "\n", "                        ", "cur_results_path", "=", "prediction_path", "/", "current_checkpoint_filename", "\n", "if", "not", "cur_results_path", ".", "exists", "(", ")", ":", "\n", "                            ", "cur_results_path", ".", "mkdir", "(", "parents", "=", "True", ")", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                            ", "common", ".", "save_jsonl", "(", "item", "[", "'predictions'", "]", ",", "cur_results_path", "/", "f\"{key}.jsonl\"", ")", "\n", "\n", "# avoid saving too many things", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                            ", "del", "r_dict", "[", "key", "]", "[", "'predictions'", "]", "\n", "", "common", ".", "save_json", "(", "r_dict", ",", "cur_results_path", "/", "\"results_dict.json\"", ",", "indent", "=", "2", ")", "\n", "\n", "", "", "if", "args", ".", "total_step", ">", "0", "and", "global_step", "==", "t_total", ":", "\n", "# if we set total step and global step s t_total.", "\n", "                    ", "is_finished", "=", "True", "\n", "break", "\n", "\n", "# End of epoch evaluation.", "\n", "", "", "", "if", "args", ".", "global_rank", "in", "[", "-", "1", ",", "0", "]", "and", "args", ".", "total_step", "<=", "0", ":", "\n", "            ", "r_dict", "=", "dict", "(", ")", "\n", "# Eval loop:", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "cur_eval_data_list", "=", "eval_data_list", "[", "i", "]", "\n", "cur_eval_dataloader", "=", "eval_data_loaders", "[", "i", "]", "\n", "# cur_eval_raw_data_list = eval_raw_data_list[i]", "\n", "\n", "evaluation_dataset", "(", "args", ",", "cur_eval_dataloader", ",", "cur_eval_data_list", ",", "model", ",", "r_dict", ",", "\n", "eval_name", "=", "cur_eval_data_name", ")", "\n", "\n", "# saving checkpoints", "\n", "", "current_checkpoint_filename", "=", "f'e({epoch})|i({global_step})'", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "                ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "current_checkpoint_filename", "+=", "f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'", "\n", "\n", "", "if", "not", "args", ".", "debug_mode", ":", "\n", "# save model:", "\n", "                ", "model_output_dir", "=", "checkpoints_path", "/", "current_checkpoint_filename", "\n", "if", "not", "model_output_dir", ".", "exists", "(", ")", ":", "\n", "                    ", "model_output_dir", ".", "mkdir", "(", ")", "\n", "", "model_to_save", "=", "(", "\n", "model", ".", "module", "if", "hasattr", "(", "model", ",", "\"module\"", ")", "else", "model", "\n", ")", "# Take care of distributed/parallel training", "\n", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"model.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "str", "(", "model_output_dir", "/", "\"scheduler.pt\"", ")", ")", "\n", "\n", "# save prediction:", "\n", "", "if", "not", "args", ".", "debug_mode", "and", "args", ".", "save_prediction", ":", "\n", "                ", "cur_results_path", "=", "prediction_path", "/", "current_checkpoint_filename", "\n", "if", "not", "cur_results_path", ".", "exists", "(", ")", ":", "\n", "                    ", "cur_results_path", ".", "mkdir", "(", "parents", "=", "True", ")", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                    ", "common", ".", "save_jsonl", "(", "item", "[", "'predictions'", "]", ",", "cur_results_path", "/", "f\"{key}.jsonl\"", ")", "\n", "\n", "# avoid saving too many things", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "                    ", "del", "r_dict", "[", "key", "]", "[", "'predictions'", "]", "\n", "", "common", ".", "save_json", "(", "r_dict", ",", "cur_results_path", "/", "\"results_dict.json\"", ",", "indent", "=", "2", ")", "\n", "\n", "", "", "if", "is_finished", ":", "\n", "            ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.count_acc": [[826, 837], ["utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "list_dict_data_tool.list_to_dict.items", "len", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.list_dict_data_tool.list_to_dict"], ["def", "count_acc", "(", "gt_list", ",", "pred_list", ")", ":", "\n", "    ", "assert", "len", "(", "gt_list", ")", "==", "len", "(", "pred_list", ")", "\n", "gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "gt_list", ",", "'uid'", ")", "\n", "pred_list", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "pred_list", ",", "'uid'", ")", "\n", "total_count", "=", "0", "\n", "hit", "=", "0", "\n", "for", "key", ",", "value", "in", "pred_list", ".", "items", "(", ")", ":", "\n", "        ", "if", "gt_dict", "[", "key", "]", "[", "'label'", "]", "==", "value", "[", "'predicted_label'", "]", ":", "\n", "            ", "hit", "+=", "1", "\n", "", "total_count", "+=", "1", "\n", "", "return", "hit", ",", "total_count", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.evaluation_dataset": [[839, 852], ["training.eval_model", "training.count_acc", "print", "training.debug_node_info"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.eval_model", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.count_acc", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.debug_node_info"], ["", "def", "evaluation_dataset", "(", "args", ",", "eval_dataloader", ",", "eval_list", ",", "model", ",", "r_dict", ",", "eval_name", ")", ":", "\n", "# r_dict = dict()", "\n", "    ", "pred_output_list", "=", "eval_model", "(", "model", ",", "eval_dataloader", ",", "args", ".", "global_rank", ",", "args", ")", "\n", "predictions", "=", "pred_output_list", "\n", "hit", ",", "total", "=", "count_acc", "(", "eval_list", ",", "pred_output_list", ")", "\n", "\n", "print", "(", "debug_node_info", "(", "args", ")", ",", "f\"{eval_name} Acc:\"", ",", "hit", ",", "total", ",", "hit", "/", "total", ")", "\n", "\n", "r_dict", "[", "f'{eval_name}'", "]", "=", "{", "\n", "'acc'", ":", "hit", "/", "total", ",", "\n", "'correct_count'", ":", "hit", ",", "\n", "'total_count'", ":", "total", ",", "\n", "'predictions'", ":", "predictions", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.eval_model": [[855, 897], ["model.eval", "range", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "flint.data_utils.batchbuilder.move_to_device", "uid_list.extend", "y_list.extend", "pred_list.extend", "logits_list.extend", "model", "model", "list", "batch[].tolist", "[].view().tolist", "logits.tolist", "[].view", "logits.size", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device"], ["", "def", "eval_model", "(", "model", ",", "dev_dataloader", ",", "device_num", ",", "args", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "uid_list", "=", "[", "]", "\n", "y_list", "=", "[", "]", "\n", "pred_list", "=", "[", "]", "\n", "logits_list", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "dev_dataloader", ",", "0", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "if", "args", ".", "model_class_name", "in", "[", "\"distilbert\"", ",", "\"bart-large\"", "]", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "batch", "[", "'input_ids'", "]", ",", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ",", "\n", "token_type_ids", "=", "batch", "[", "'token_type_ids'", "]", ",", "\n", "labels", "=", "batch", "[", "'y'", "]", ")", "\n", "\n", "", "loss", ",", "logits", "=", "outputs", "[", ":", "2", "]", "\n", "\n", "uid_list", ".", "extend", "(", "list", "(", "batch", "[", "'uid'", "]", ")", ")", "\n", "y_list", ".", "extend", "(", "batch", "[", "'y'", "]", ".", "tolist", "(", ")", ")", "\n", "pred_list", ".", "extend", "(", "torch", ".", "max", "(", "logits", ",", "1", ")", "[", "1", "]", ".", "view", "(", "logits", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "logits_list", ".", "extend", "(", "logits", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "logits_list", ")", "\n", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "logits_list", ")", "\n", "\n", "result_items_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "uid_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'uid'", "]", "=", "uid_list", "[", "i", "]", "\n", "r_item", "[", "'logits'", "]", "=", "logits_list", "[", "i", "]", "\n", "r_item", "[", "'predicted_label'", "]", "=", "id2label", "[", "pred_list", "[", "i", "]", "]", "\n", "\n", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.debug_node_info": [[899, 910], ["values.append", "getattr", "zip"], "function", ["None"], ["", "def", "debug_node_info", "(", "args", ")", ":", "\n", "    ", "names", "=", "[", "'global_rank'", ",", "'local_rank'", ",", "'node_rank'", "]", "\n", "values", "=", "[", "]", "\n", "\n", "for", "name", "in", "names", ":", "\n", "        ", "if", "name", "in", "args", ":", "\n", "            ", "values", ".", "append", "(", "getattr", "(", "args", ",", "name", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "\"Pro:No node info \"", "\n", "\n", "", "", "return", "\"Pro:\"", "+", "'|'", ".", "join", "(", "[", "f\"{name}:{value}\"", "for", "name", ",", "value", "in", "zip", "(", "names", ",", "values", ")", "]", ")", "+", "\"||Print:\"", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.nli.evaluation.evaluation": [[22, 150], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "model_class_item[].from_pretrained", "model_class_item[].from_pretrained", "model_class_item[].from_pretrained.load_state_dict", "eval_data_str.split", "nli.training.NLITransform", "dict", "range", "torch.load", "model_class_item[].from_pretrained.convert_tokens_to_ids", "named_path.find", "eval_data_name.append", "eval_data_path.append", "eval_data_list.append", "flint.data_utils.fields.RawFlintField", "flint.data_utils.fields.LabelFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "flint.data_utils.fields.ArrayIndexFlintField", "nli.training.build_eval_dataset_loader_and_sampler", "eval_data_loaders.append", "torch.cuda.set_device", "model_class_item[].from_pretrained.cuda", "len", "nli.training.evaluation_dataset", "pathlib.Path", "dict.items", "dict.items", "utils.common.save_json", "str", "str", "utils.common.load_jsonl", "utils.common.load_jsonl", "pathlib.Path.exists", "pathlib.Path.mkdir", "utils.common.save_jsonl"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.build_eval_dataset_loader_and_sampler", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.training.evaluation_dataset", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_json", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl"], ["def", "evaluation", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--cpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"If set, we only use CPU.\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_class_name\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Set the model class of the experiment.\"", ",", "\n", "required", "=", "True", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model_checkpoint_path\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "'Set the path to save the prediction.'", ",", "required", "=", "True", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--output_prediction_path\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "'Set the path to save the prediction.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "16", ",", "type", "=", "int", ",", "help", "=", "\"Batch size per GPU/CPU for evaluation.\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "default", "=", "156", ",", "type", "=", "int", ",", "help", "=", "\"Max length of the sequences.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--eval_data\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The training data used in the experiments.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "cpu", ":", "\n", "        ", "args", ".", "global_rank", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "args", ".", "global_rank", "=", "0", "\n", "\n", "", "model_checkpoint_path", "=", "args", ".", "model_checkpoint_path", "\n", "num_labels", "=", "3", "\n", "# we are doing NLI so we set num_labels = 3, for other task we can change this value.", "\n", "\n", "max_length", "=", "args", ".", "max_length", "\n", "\n", "model_class_item", "=", "MODEL_CLASSES", "[", "args", ".", "model_class_name", "]", "\n", "model_name", "=", "model_class_item", "[", "'model_name'", "]", "\n", "do_lower_case", "=", "model_class_item", "[", "'do_lower_case'", "]", "if", "'do_lower_case'", "in", "model_class_item", "else", "False", "\n", "\n", "tokenizer", "=", "model_class_item", "[", "'tokenizer'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "model", "=", "model_class_item", "[", "'sequence_classification'", "]", ".", "from_pretrained", "(", "model_name", ",", "\n", "cache_dir", "=", "str", "(", "config", ".", "PRO_ROOT", "/", "\"trans_cache\"", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_checkpoint_path", ")", ")", "\n", "\n", "padding_token_value", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "tokenizer", ".", "pad_token", "]", ")", "[", "0", "]", "\n", "padding_segement_value", "=", "model_class_item", "[", "\"padding_segement_value\"", "]", "\n", "padding_att_value", "=", "model_class_item", "[", "\"padding_att_value\"", "]", "\n", "left_pad", "=", "model_class_item", "[", "'left_pad'", "]", "if", "'left_pad'", "in", "model_class_item", "else", "False", "\n", "\n", "batch_size_per_gpu_eval", "=", "args", ".", "per_gpu_eval_batch_size", "\n", "\n", "eval_data_str", "=", "args", ".", "eval_data", "\n", "eval_data_name", "=", "[", "]", "\n", "eval_data_path", "=", "[", "]", "\n", "eval_data_list", "=", "[", "]", "\n", "\n", "eval_data_named_path", "=", "eval_data_str", ".", "split", "(", "','", ")", "\n", "\n", "for", "named_path", "in", "eval_data_named_path", ":", "\n", "        ", "ind", "=", "named_path", ".", "find", "(", "':'", ")", "\n", "name", "=", "named_path", "[", ":", "ind", "]", "\n", "path", "=", "name", "[", "ind", "+", "1", ":", "]", "\n", "if", "name", "in", "registered_path", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "registered_path", "[", "name", "]", ")", "\n", "", "else", ":", "\n", "            ", "d_list", "=", "common", ".", "load_jsonl", "(", "path", ")", "\n", "", "eval_data_name", ".", "append", "(", "name", ")", "\n", "eval_data_path", ".", "append", "(", "path", ")", "\n", "\n", "eval_data_list", ".", "append", "(", "d_list", ")", "\n", "\n", "", "batching_schema", "=", "{", "\n", "'uid'", ":", "RawFlintField", "(", ")", ",", "\n", "'y'", ":", "LabelFlintField", "(", ")", ",", "\n", "'input_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_token_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'token_type_ids'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_segement_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "'attention_mask'", ":", "ArrayIndexFlintField", "(", "pad_idx", "=", "padding_att_value", ",", "left_pad", "=", "left_pad", ")", ",", "\n", "}", "\n", "\n", "data_transformer", "=", "NLITransform", "(", "model_name", ",", "tokenizer", ",", "max_length", ")", "\n", "eval_data_loaders", "=", "[", "]", "\n", "for", "eval_d_list", "in", "eval_data_list", ":", "\n", "        ", "d_dataset", ",", "d_sampler", ",", "d_dataloader", "=", "build_eval_dataset_loader_and_sampler", "(", "eval_d_list", ",", "data_transformer", ",", "\n", "batching_schema", ",", "\n", "batch_size_per_gpu_eval", ")", "\n", "eval_data_loaders", ".", "append", "(", "d_dataloader", ")", "\n", "\n", "", "if", "not", "args", ".", "cpu", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "0", ")", "\n", "model", ".", "cuda", "(", "0", ")", "\n", "\n", "", "r_dict", "=", "dict", "(", ")", "\n", "# Eval loop:", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_data_name", ")", ")", ":", "\n", "        ", "cur_eval_data_name", "=", "eval_data_name", "[", "i", "]", "\n", "cur_eval_data_list", "=", "eval_data_list", "[", "i", "]", "\n", "cur_eval_dataloader", "=", "eval_data_loaders", "[", "i", "]", "\n", "# cur_eval_raw_data_list = eval_raw_data_list[i]", "\n", "\n", "evaluation_dataset", "(", "args", ",", "cur_eval_dataloader", ",", "cur_eval_data_list", ",", "model", ",", "r_dict", ",", "\n", "eval_name", "=", "cur_eval_data_name", ")", "\n", "\n", "# save prediction:", "\n", "", "if", "args", ".", "output_prediction_path", "is", "not", "None", ":", "\n", "        ", "cur_results_path", "=", "Path", "(", "args", ".", "output_prediction_path", ")", "\n", "if", "not", "cur_results_path", ".", "exists", "(", ")", ":", "\n", "            ", "cur_results_path", ".", "mkdir", "(", "parents", "=", "True", ")", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "            ", "common", ".", "save_jsonl", "(", "item", "[", "'predictions'", "]", ",", "cur_results_path", "/", "f\"{key}.jsonl\"", ")", "\n", "\n", "# avoid saving too many things", "\n", "", "for", "key", ",", "item", "in", "r_dict", ".", "items", "(", ")", ":", "\n", "            ", "del", "r_dict", "[", "key", "]", "[", "'predictions'", "]", "\n", "", "common", ".", "save_json", "(", "r_dict", ",", "cur_results_path", "/", "\"results_dict.json\"", ",", "indent", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.__init__": [[18, 20], ["dict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_state_dict", "=", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.step": [[21, 23], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.EmptyScheduler.state_dict": [[24, 26], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.ResEncoder.__init__": [[29, 61], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "print", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "h_size", "=", "[", "1024", ",", "1024", ",", "1024", "]", ",", "v_size", "=", "10", ",", "embd_dim", "=", "300", ",", "mlp_d", "=", "1024", ",", "\n", "dropout_r", "=", "0.1", ",", "k", "=", "3", ",", "n_layers", "=", "1", ",", "num_labels", "=", "3", ")", ":", "\n", "        ", "super", "(", "ResEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "Embd", "=", "nn", ".", "Embedding", "(", "v_size", ",", "embd_dim", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "embd_dim", ",", "hidden_size", "=", "h_size", "[", "0", "]", ",", "\n", "num_layers", "=", "1", ",", "bidirectional", "=", "True", ")", "\n", "\n", "self", ".", "lstm_1", "=", "nn", ".", "LSTM", "(", "input_size", "=", "(", "embd_dim", "+", "h_size", "[", "0", "]", "*", "2", ")", ",", "hidden_size", "=", "h_size", "[", "1", "]", ",", "\n", "num_layers", "=", "1", ",", "bidirectional", "=", "True", ")", "\n", "\n", "self", ".", "lstm_2", "=", "nn", ".", "LSTM", "(", "input_size", "=", "(", "embd_dim", "+", "h_size", "[", "0", "]", "*", "2", ")", ",", "hidden_size", "=", "h_size", "[", "2", "]", ",", "\n", "num_layers", "=", "1", ",", "bidirectional", "=", "True", ")", "\n", "\n", "self", ".", "h_size", "=", "h_size", "\n", "self", ".", "k", "=", "k", "\n", "\n", "# self.mlp_1 = nn.Linear(h_size[2] * 2 * 4, mlp_d)", "\n", "self", ".", "mlp_1", "=", "nn", ".", "Linear", "(", "h_size", "[", "2", "]", "*", "2", ",", "mlp_d", ")", "\n", "self", ".", "mlp_2", "=", "nn", ".", "Linear", "(", "mlp_d", ",", "mlp_d", ")", "\n", "self", ".", "sm", "=", "nn", ".", "Linear", "(", "mlp_d", ",", "self", ".", "num_labels", ")", "\n", "\n", "if", "n_layers", "==", "1", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "*", "[", "self", ".", "mlp_1", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Dropout", "(", "dropout_r", ")", ",", "\n", "self", ".", "sm", "]", ")", "\n", "", "elif", "n_layers", "==", "2", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "*", "[", "self", ".", "mlp_1", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Dropout", "(", "dropout_r", ")", ",", "\n", "self", ".", "mlp_2", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Dropout", "(", "dropout_r", ")", ",", "\n", "self", ".", "sm", "]", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Error num layers\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.ResEncoder.init_embedding": [[62, 64], ["None"], "methods", ["None"], ["", "", "def", "init_embedding", "(", "self", ",", "embedding", ")", ":", "\n", "        ", "self", ".", "Embd", ".", "weight", "=", "embedding", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.ResEncoder.forward": [[65, 125], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "res_encoder.ResEncoder.Embd", "flint.auto_rnn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "flint.auto_rnn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "flint.auto_rnn", "flint.max_along_time", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "res_encoder.ResEncoder.classifier", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "res_encoder.ResEncoder.view", "labels.view", "res_encoder.ResEncoder.view", "labels.view"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.auto_rnn", "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.auto_rnn", "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.auto_rnn", "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.max_along_time"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "labels", "=", "None", ")", ":", "\n", "# if self.max_l:", "\n", "#     l1 = l1.clamp(max=self.max_l)", "\n", "#     l2 = l2.clamp(max=self.max_l)", "\n", "#     if s1.size(0) > self.max_l:", "\n", "#         s1 = s1[:self.max_l, :]", "\n", "#     if s2.size(0) > self.max_l:", "\n", "#         s2 = s2[:self.max_l, :]", "\n", "        ", "batch_l_1", "=", "torch", ".", "sum", "(", "attention_mask", ",", "dim", "=", "1", ")", "\n", "\n", "# p_s1 = self.Embd(s1)", "\n", "embedding_1", "=", "self", ".", "Embd", "(", "input_ids", ")", "\n", "\n", "s1_layer1_out", "=", "torch_util", ".", "auto_rnn", "(", "self", ".", "lstm", ",", "embedding_1", ",", "batch_l_1", ")", "\n", "# s2_layer1_out = torch_util.auto_rnn_bilstm(self.lstm, p_s2, l2)", "\n", "\n", "# Length truncate", "\n", "# len1 = s1_layer1_out.size(0)", "\n", "# len2 = s2_layer1_out.size(0)", "\n", "# p_s1 = p_s1[:len1, :, :]", "\n", "# p_s2 = p_s2[:len2, :, :]", "\n", "\n", "# Using high way", "\n", "s1_layer2_in", "=", "torch", ".", "cat", "(", "[", "embedding_1", ",", "s1_layer1_out", "]", ",", "dim", "=", "2", ")", "\n", "# s2_layer2_in = torch.cat([p_s2, s2_layer1_out], dim=2)", "\n", "\n", "s1_layer2_out", "=", "torch_util", ".", "auto_rnn", "(", "self", ".", "lstm_1", ",", "s1_layer2_in", ",", "batch_l_1", ")", "\n", "# s2_layer2_out = torch_util.auto_rnn_bilstm(self.lstm_1, s2_layer2_in, l2)", "\n", "\n", "s1_layer3_in", "=", "torch", ".", "cat", "(", "[", "embedding_1", ",", "s1_layer1_out", "+", "s1_layer2_out", "]", ",", "dim", "=", "2", ")", "\n", "# s2_layer3_in = torch.cat([p_s2, s2_layer1_out + s2_layer2_out], dim=2)", "\n", "\n", "s1_layer3_out", "=", "torch_util", ".", "auto_rnn", "(", "self", ".", "lstm_2", ",", "s1_layer3_in", ",", "batch_l_1", ")", "\n", "# s2_layer3_out = torch_util.auto_rnn_bilstm(self.lstm_2, s2_layer3_in, l2)", "\n", "\n", "s1_layer3_maxout", "=", "torch_util", ".", "max_along_time", "(", "s1_layer3_out", ",", "batch_l_1", ")", "\n", "# s2_layer3_maxout = torch_util.max_along_time(s2_layer3_out, l2)", "\n", "\n", "# Only use the last layer", "\n", "# features = torch.cat([s1_layer3_maxout, s2_layer3_maxout,", "\n", "#                       torch.abs(s1_layer3_maxout - s2_layer3_maxout),", "\n", "#                       s1_layer3_maxout * s2_layer3_maxout],", "\n", "#                      dim=1)", "\n", "\n", "features", "=", "torch", ".", "cat", "(", "[", "s1_layer3_maxout", "]", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "features", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "#  We are doing regression", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "return", "(", "loss", ",", "logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.BagOfWords.__init__": [[128, 148], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "print", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "v_size", "=", "10", ",", "embd_dim", "=", "300", ",", "mlp_d", "=", "1024", ",", "\n", "dropout_r", "=", "0.1", ",", "n_layers", "=", "1", ",", "num_labels", "=", "3", ")", ":", "\n", "        ", "super", "(", "BagOfWords", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "Embd", "=", "nn", ".", "Embedding", "(", "v_size", ",", "embd_dim", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "\n", "# self.mlp_1 = nn.Linear(h_size[2] * 2 * 4, mlp_d)", "\n", "self", ".", "mlp_1", "=", "nn", ".", "Linear", "(", "embd_dim", ",", "mlp_d", ")", "\n", "self", ".", "mlp_2", "=", "nn", ".", "Linear", "(", "mlp_d", ",", "mlp_d", ")", "\n", "self", ".", "sm", "=", "nn", ".", "Linear", "(", "mlp_d", ",", "self", ".", "num_labels", ")", "\n", "\n", "if", "n_layers", "==", "1", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "*", "[", "self", ".", "mlp_1", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Dropout", "(", "dropout_r", ")", ",", "\n", "self", ".", "sm", "]", ")", "\n", "", "elif", "n_layers", "==", "2", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "*", "[", "self", ".", "mlp_1", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Dropout", "(", "dropout_r", ")", ",", "\n", "self", ".", "mlp_2", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Dropout", "(", "dropout_r", ")", ",", "\n", "self", ".", "sm", "]", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Error num layers\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.BagOfWords.init_embedding": [[149, 151], ["None"], "methods", ["None"], ["", "", "def", "init_embedding", "(", "self", ",", "embedding", ")", ":", "\n", "        ", "self", ".", "Embd", ".", "weight", "=", "embedding", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.res_encoder.BagOfWords.forward": [[152, 190], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "res_encoder.BagOfWords.Embd", "flint.avg_along_time", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "res_encoder.BagOfWords.classifier", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "res_encoder.BagOfWords.view", "labels.view", "res_encoder.BagOfWords.view", "labels.view"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.avg_along_time"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "labels", "=", "None", ")", ":", "\n", "# if self.max_l:", "\n", "#     l1 = l1.clamp(max=self.max_l)", "\n", "#     l2 = l2.clamp(max=self.max_l)", "\n", "#     if s1.size(0) > self.max_l:", "\n", "#         s1 = s1[:self.max_l, :]", "\n", "#     if s2.size(0) > self.max_l:", "\n", "#         s2 = s2[:self.max_l, :]", "\n", "        ", "batch_l_1", "=", "torch", ".", "sum", "(", "attention_mask", ",", "dim", "=", "1", ")", "\n", "\n", "# p_s1 = self.Embd(s1)", "\n", "embedding_1", "=", "self", ".", "Embd", "(", "input_ids", ")", "\n", "\n", "s1_layer3_maxout", "=", "torch_util", ".", "avg_along_time", "(", "embedding_1", ",", "batch_l_1", ")", "\n", "# s2_layer3_maxout = torch_util.max_along_time(s2_layer3_out, l2)", "\n", "\n", "# Only use the last layer", "\n", "# features = torch.cat([s1_layer3_maxout, s2_layer3_maxout,", "\n", "#                       torch.abs(s1_layer3_maxout - s2_layer3_maxout),", "\n", "#                       s1_layer3_maxout * s2_layer3_maxout],", "\n", "#                      dim=1)", "\n", "\n", "features", "=", "torch", ".", "cat", "(", "[", "s1_layer3_maxout", "]", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "features", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "#  We are doing regression", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "return", "(", "loss", ",", "logits", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.__init__": [[185, 212], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "XLNetLayerNorm", "torch.nn.Dropout", "ValueError", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "config", ".", "d_model", "%", "config", ".", "n_head", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "d_model", ",", "config", ".", "n_head", ")", "\n", ")", "\n", "\n", "", "self", ".", "n_head", "=", "config", ".", "n_head", "\n", "self", ".", "d_head", "=", "config", ".", "d_head", "\n", "self", ".", "d_model", "=", "config", ".", "d_model", "\n", "self", ".", "scale", "=", "1", "/", "(", "config", ".", "d_head", "**", "0.5", ")", "\n", "\n", "self", ".", "q", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "config", ".", "d_model", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "k", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "config", ".", "d_model", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "v", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "config", ".", "d_model", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "o", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "config", ".", "d_model", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "r", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "config", ".", "d_model", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "\n", "self", ".", "r_r_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "r_s_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "r_w_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "self", ".", "seg_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "2", ",", "self", ".", "n_head", ",", "self", ".", "d_head", ")", ")", "\n", "\n", "self", ".", "layer_norm", "=", "XLNetLayerNorm", "(", "config", ".", "d_model", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.prune_heads": [[213, 215], ["None"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.rel_shift": [[216, 228], ["torch.index_select.reshape", "torch.index_select.reshape", "torch.index_select", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "rel_shift", "(", "x", ",", "klen", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"perform relative shift to form the relative attention score.\"\"\"", "\n", "x_size", "=", "x", ".", "shape", "\n", "\n", "x", "=", "x", ".", "reshape", "(", "x_size", "[", "1", "]", ",", "x_size", "[", "0", "]", ",", "x_size", "[", "2", "]", ",", "x_size", "[", "3", "]", ")", "\n", "x", "=", "x", "[", "1", ":", ",", "...", "]", "\n", "x", "=", "x", ".", "reshape", "(", "x_size", "[", "0", "]", ",", "x_size", "[", "1", "]", "-", "1", ",", "x_size", "[", "2", "]", ",", "x_size", "[", "3", "]", ")", "\n", "# x = x[:, 0:klen, :, :]", "\n", "x", "=", "torch", ".", "index_select", "(", "x", ",", "1", ",", "torch", ".", "arange", "(", "klen", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.rel_shift_bnij": [[229, 243], ["torch.index_select.reshape", "torch.index_select.reshape", "torch.index_select", "torch.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "rel_shift_bnij", "(", "x", ",", "klen", "=", "-", "1", ")", ":", "\n", "        ", "x_size", "=", "x", ".", "shape", "\n", "\n", "x", "=", "x", ".", "reshape", "(", "x_size", "[", "0", "]", ",", "x_size", "[", "1", "]", ",", "x_size", "[", "3", "]", ",", "x_size", "[", "2", "]", ")", "\n", "x", "=", "x", "[", ":", ",", ":", ",", "1", ":", ",", ":", "]", "\n", "x", "=", "x", ".", "reshape", "(", "x_size", "[", "0", "]", ",", "x_size", "[", "1", "]", ",", "x_size", "[", "2", "]", ",", "x_size", "[", "3", "]", "-", "1", ")", "\n", "# Note: the tensor-slice form was faster in my testing than torch.index_select", "\n", "#       However, tracing doesn't like the nature of the slice, and if klen changes", "\n", "#       during the run then it'll fail, whereas index_select will be fine.", "\n", "x", "=", "torch", ".", "index_select", "(", "x", ",", "3", ",", "torch", ".", "arange", "(", "klen", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "# x = x[:, :, :, :klen]", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.rel_attn_core": [[244, 295], ["torch.einsum", "torch.einsum", "dummy_modeling_xlnet.XLNetRelativeAttention.rel_shift_bnij", "torch.nn.functional.softmax", "dummy_modeling_xlnet.XLNetRelativeAttention.dropout", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.rel_shift_bnij", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.softmax"], ["", "def", "rel_attn_core", "(", "\n", "self", ",", "\n", "q_head", ",", "\n", "k_head_h", ",", "\n", "v_head_h", ",", "\n", "k_head_r", ",", "\n", "seg_mat", "=", "None", ",", "\n", "attn_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Core relative positional attention operations.\"\"\"", "\n", "\n", "# content based attention score", "\n", "ac", "=", "torch", ".", "einsum", "(", "\"ibnd,jbnd->bnij\"", ",", "q_head", "+", "self", ".", "r_w_bias", ",", "k_head_h", ")", "\n", "\n", "# position based attention score", "\n", "bd", "=", "torch", ".", "einsum", "(", "\"ibnd,jbnd->bnij\"", ",", "q_head", "+", "self", ".", "r_r_bias", ",", "k_head_r", ")", "\n", "bd", "=", "self", ".", "rel_shift_bnij", "(", "bd", ",", "klen", "=", "ac", ".", "shape", "[", "3", "]", ")", "\n", "\n", "# segment based attention score", "\n", "if", "seg_mat", "is", "None", ":", "\n", "            ", "ef", "=", "0", "\n", "", "else", ":", "\n", "            ", "ef", "=", "torch", ".", "einsum", "(", "\"ibnd,snd->ibns\"", ",", "q_head", "+", "self", ".", "r_s_bias", ",", "self", ".", "seg_embed", ")", "\n", "ef", "=", "torch", ".", "einsum", "(", "\"ijbs,ibns->bnij\"", ",", "seg_mat", ",", "ef", ")", "\n", "\n", "# merge attention scores and perform masking", "\n", "", "attn_score", "=", "(", "ac", "+", "bd", "+", "ef", ")", "*", "self", ".", "scale", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "# attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask", "\n", "            ", "if", "attn_mask", ".", "dtype", "==", "torch", ".", "float16", ":", "\n", "                ", "attn_score", "=", "attn_score", "-", "65500", "*", "torch", ".", "einsum", "(", "\"ijbn->bnij\"", ",", "attn_mask", ")", "\n", "", "else", ":", "\n", "                ", "attn_score", "=", "attn_score", "-", "1e30", "*", "torch", ".", "einsum", "(", "\"ijbn->bnij\"", ",", "attn_mask", ")", "\n", "\n", "# attention probability", "\n", "", "", "attn_prob", "=", "F", ".", "softmax", "(", "attn_score", ",", "dim", "=", "3", ")", "\n", "attn_prob", "=", "self", ".", "dropout", "(", "attn_prob", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attn_prob", "=", "attn_prob", "*", "torch", ".", "einsum", "(", "\"ijbn->bnij\"", ",", "head_mask", ")", "\n", "\n", "# attention output", "\n", "", "attn_vec", "=", "torch", ".", "einsum", "(", "\"bnij,jbnd->ibnd\"", ",", "attn_prob", ",", "v_head_h", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "            ", "return", "attn_vec", ",", "torch", ".", "einsum", "(", "\"bnij->ijbn\"", ",", "attn_prob", ")", "\n", "\n", "", "return", "attn_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.post_attention": [[296, 307], ["torch.einsum", "dummy_modeling_xlnet.XLNetRelativeAttention.dropout", "dummy_modeling_xlnet.XLNetRelativeAttention.layer_norm"], "methods", ["None"], ["", "def", "post_attention", "(", "self", ",", "h", ",", "attn_vec", ",", "residual", "=", "True", ")", ":", "\n", "        ", "\"\"\"Post-attention processing.\"\"\"", "\n", "# post-attention projection (back to `d_model`)", "\n", "attn_out", "=", "torch", ".", "einsum", "(", "\"ibnd,hnd->ibh\"", ",", "attn_vec", ",", "self", ".", "o", ")", "\n", "\n", "attn_out", "=", "self", ".", "dropout", "(", "attn_out", ")", "\n", "if", "residual", ":", "\n", "            ", "attn_out", "=", "attn_out", "+", "h", "\n", "", "output", "=", "self", ".", "layer_norm", "(", "attn_out", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.forward": [[308, 441], ["torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "dummy_modeling_xlnet.XLNetRelativeAttention.rel_attn_core", "dummy_modeling_xlnet.XLNetRelativeAttention.post_attention", "torch.einsum", "dummy_modeling_xlnet.XLNetRelativeAttention.post_attention", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "dummy_modeling_xlnet.XLNetRelativeAttention.rel_attn_core", "dummy_modeling_xlnet.XLNetRelativeAttention.post_attention", "torch.cat", "torch.einsum", "dummy_modeling_xlnet.XLNetRelativeAttention.rel_attn_core", "torch.einsum", "dummy_modeling_xlnet.XLNetRelativeAttention.rel_attn_core", "torch.cat", "mems.dim", "mems.dim"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.rel_attn_core", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.post_attention", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.post_attention", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.rel_attn_core", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.post_attention", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.rel_attn_core", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetRelativeAttention.rel_attn_core"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "h", ",", "\n", "g", ",", "\n", "attn_mask_h", ",", "\n", "attn_mask_g", ",", "\n", "r", ",", "\n", "seg_mat", ",", "\n", "mems", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "g", "is", "not", "None", ":", "\n", "# Two-stream attention with relative positional encoding.", "\n", "# content based attention score", "\n", "            ", "if", "mems", "is", "not", "None", "and", "mems", ".", "dim", "(", ")", ">", "1", ":", "\n", "                ", "cat", "=", "torch", ".", "cat", "(", "[", "mems", ",", "h", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "cat", "=", "h", "\n", "\n", "# content-based key head", "\n", "", "k_head_h", "=", "torch", ".", "einsum", "(", "\"ibh,hnd->ibnd\"", ",", "cat", ",", "self", ".", "k", ")", "\n", "\n", "# content-based value head", "\n", "v_head_h", "=", "torch", ".", "einsum", "(", "\"ibh,hnd->ibnd\"", ",", "cat", ",", "self", ".", "v", ")", "\n", "\n", "# position-based key head", "\n", "k_head_r", "=", "torch", ".", "einsum", "(", "\"ibh,hnd->ibnd\"", ",", "r", ",", "self", ".", "r", ")", "\n", "\n", "# h-stream", "\n", "# content-stream query head", "\n", "q_head_h", "=", "torch", ".", "einsum", "(", "\"ibh,hnd->ibnd\"", ",", "h", ",", "self", ".", "q", ")", "\n", "\n", "# core attention ops", "\n", "attn_vec_h", "=", "self", ".", "rel_attn_core", "(", "\n", "q_head_h", ",", "\n", "k_head_h", ",", "\n", "v_head_h", ",", "\n", "k_head_r", ",", "\n", "seg_mat", "=", "seg_mat", ",", "\n", "attn_mask", "=", "attn_mask_h", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "                ", "attn_vec_h", ",", "attn_prob_h", "=", "attn_vec_h", "\n", "\n", "# post processing", "\n", "", "output_h", "=", "self", ".", "post_attention", "(", "h", ",", "attn_vec_h", ")", "\n", "\n", "# g-stream", "\n", "# query-stream query head", "\n", "q_head_g", "=", "torch", ".", "einsum", "(", "\"ibh,hnd->ibnd\"", ",", "g", ",", "self", ".", "q", ")", "\n", "\n", "# core attention ops", "\n", "if", "target_mapping", "is", "not", "None", ":", "\n", "                ", "q_head_g", "=", "torch", ".", "einsum", "(", "\"mbnd,mlb->lbnd\"", ",", "q_head_g", ",", "target_mapping", ")", "\n", "attn_vec_g", "=", "self", ".", "rel_attn_core", "(", "\n", "q_head_g", ",", "\n", "k_head_h", ",", "\n", "v_head_h", ",", "\n", "k_head_r", ",", "\n", "seg_mat", "=", "seg_mat", ",", "\n", "attn_mask", "=", "attn_mask_g", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "                    ", "attn_vec_g", ",", "attn_prob_g", "=", "attn_vec_g", "\n", "\n", "", "attn_vec_g", "=", "torch", ".", "einsum", "(", "\"lbnd,mlb->mbnd\"", ",", "attn_vec_g", ",", "target_mapping", ")", "\n", "", "else", ":", "\n", "                ", "attn_vec_g", "=", "self", ".", "rel_attn_core", "(", "\n", "q_head_g", ",", "\n", "k_head_h", ",", "\n", "v_head_h", ",", "\n", "k_head_r", ",", "\n", "seg_mat", "=", "seg_mat", ",", "\n", "attn_mask", "=", "attn_mask_g", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "                    ", "attn_vec_g", ",", "attn_prob_g", "=", "attn_vec_g", "\n", "\n", "# post processing", "\n", "", "", "output_g", "=", "self", ".", "post_attention", "(", "g", ",", "attn_vec_g", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "                ", "attn_prob", "=", "attn_prob_h", ",", "attn_prob_g", "\n", "\n", "", "", "else", ":", "\n", "# Multi-head attention with relative positional encoding", "\n", "            ", "if", "mems", "is", "not", "None", "and", "mems", ".", "dim", "(", ")", ">", "1", ":", "\n", "                ", "cat", "=", "torch", ".", "cat", "(", "[", "mems", ",", "h", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "cat", "=", "h", "\n", "\n", "# content heads", "\n", "", "q_head_h", "=", "torch", ".", "einsum", "(", "\"ibh,hnd->ibnd\"", ",", "h", ",", "self", ".", "q", ")", "\n", "k_head_h", "=", "torch", ".", "einsum", "(", "\"ibh,hnd->ibnd\"", ",", "cat", ",", "self", ".", "k", ")", "\n", "v_head_h", "=", "torch", ".", "einsum", "(", "\"ibh,hnd->ibnd\"", ",", "cat", ",", "self", ".", "v", ")", "\n", "\n", "# positional heads", "\n", "k_head_r", "=", "torch", ".", "einsum", "(", "\"ibh,hnd->ibnd\"", ",", "r", ",", "self", ".", "r", ")", "\n", "\n", "# core attention ops", "\n", "attn_vec", "=", "self", ".", "rel_attn_core", "(", "\n", "q_head_h", ",", "\n", "k_head_h", ",", "\n", "v_head_h", ",", "\n", "k_head_r", ",", "\n", "seg_mat", "=", "seg_mat", ",", "\n", "attn_mask", "=", "attn_mask_h", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "\n", "if", "output_attentions", ":", "\n", "                ", "attn_vec", ",", "attn_prob", "=", "attn_vec", "\n", "\n", "# post processing", "\n", "", "output_h", "=", "self", ".", "post_attention", "(", "h", ",", "attn_vec", ")", "\n", "output_g", "=", "None", "\n", "\n", "", "outputs", "=", "(", "output_h", ",", "output_g", ")", "\n", "if", "output_attentions", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "attn_prob", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetFeedForward.__init__": [[444, 454], ["torch.nn.Module.__init__", "XLNetLayerNorm", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "isinstance"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layer_norm", "=", "XLNetLayerNorm", "(", "config", ".", "d_model", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "layer_1", "=", "nn", ".", "Linear", "(", "config", ".", "d_model", ",", "config", ".", "d_inner", ")", "\n", "self", ".", "layer_2", "=", "nn", ".", "Linear", "(", "config", ".", "d_inner", ",", "config", ".", "d_model", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "dropout", ")", "\n", "if", "isinstance", "(", "config", ".", "ff_activation", ",", "str", ")", ":", "\n", "            ", "self", ".", "activation_function", "=", "ACT2FN", "[", "config", ".", "ff_activation", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "activation_function", "=", "config", ".", "ff_activation", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetFeedForward.forward": [[455, 464], ["dummy_modeling_xlnet.XLNetFeedForward.layer_1", "dummy_modeling_xlnet.XLNetFeedForward.activation_function", "dummy_modeling_xlnet.XLNetFeedForward.dropout", "dummy_modeling_xlnet.XLNetFeedForward.layer_2", "dummy_modeling_xlnet.XLNetFeedForward.dropout", "dummy_modeling_xlnet.XLNetFeedForward.layer_norm"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "inp", ")", ":", "\n", "        ", "output", "=", "inp", "\n", "output", "=", "self", ".", "layer_1", "(", "output", ")", "\n", "output", "=", "self", ".", "activation_function", "(", "output", ")", "\n", "output", "=", "self", ".", "dropout", "(", "output", ")", "\n", "output", "=", "self", ".", "layer_2", "(", "output", ")", "\n", "output", "=", "self", ".", "dropout", "(", "output", ")", "\n", "output", "=", "self", ".", "layer_norm", "(", "output", "+", "inp", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetLayer.__init__": [[467, 472], ["torch.nn.Module.__init__", "dummy_modeling_xlnet.XLNetRelativeAttention", "dummy_modeling_xlnet.XLNetFeedForward", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "rel_attn", "=", "XLNetRelativeAttention", "(", "config", ")", "\n", "self", ".", "ff", "=", "XLNetFeedForward", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetLayer.forward": [[473, 506], ["dummy_modeling_xlnet.XLNetLayer.rel_attn", "dummy_modeling_xlnet.XLNetLayer.ff", "dummy_modeling_xlnet.XLNetLayer.ff"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "output_h", ",", "\n", "output_g", ",", "\n", "attn_mask_h", ",", "\n", "attn_mask_g", ",", "\n", "r", ",", "\n", "seg_mat", ",", "\n", "mems", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "rel_attn", "(", "\n", "output_h", ",", "\n", "output_g", ",", "\n", "attn_mask_h", ",", "\n", "attn_mask_g", ",", "\n", "r", ",", "\n", "seg_mat", ",", "\n", "mems", "=", "mems", ",", "\n", "target_mapping", "=", "target_mapping", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "output_h", ",", "output_g", "=", "outputs", "[", ":", "2", "]", "\n", "\n", "if", "output_g", "is", "not", "None", ":", "\n", "            ", "output_g", "=", "self", ".", "ff", "(", "output_g", ")", "\n", "", "output_h", "=", "self", ".", "ff", "(", "output_h", ")", "\n", "\n", "outputs", "=", "(", "output_h", ",", "output_g", ")", "+", "outputs", "[", "2", ":", "]", "# Add again attentions if there are there", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetPreTrainedModel._init_weights": [[517, 544], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_", "isinstance", "isinstance", "param.data.normal_", "module.mask_emb.data.normal_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "XLNetLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "XLNetRelativeAttention", ")", ":", "\n", "            ", "for", "param", "in", "[", "\n", "module", ".", "q", ",", "\n", "module", ".", "k", ",", "\n", "module", ".", "v", ",", "\n", "module", ".", "o", ",", "\n", "module", ".", "r", ",", "\n", "module", ".", "r_r_bias", ",", "\n", "module", ".", "r_s_bias", ",", "\n", "module", ".", "r_w_bias", ",", "\n", "module", ".", "seg_embed", ",", "\n", "]", ":", "\n", "                ", "param", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "XLNetModel", ")", ":", "\n", "            ", "module", ".", "mask_emb", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.__init__": [[622, 640], ["transformers.modeling_utils.PreTrainedModel.__init__", "torch.nn.Embedding", "torch.nn.Parameter", "torch.nn.ModuleList", "torch.nn.Dropout", "dummy_modeling_xlnet.XLNetModel.init_weights", "torch.FloatTensor", "dummy_modeling_xlnet.XLNetLayer", "range"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "mem_len", "=", "config", ".", "mem_len", "\n", "self", ".", "reuse_len", "=", "config", ".", "reuse_len", "\n", "self", ".", "d_model", "=", "config", ".", "d_model", "\n", "self", ".", "same_length", "=", "config", ".", "same_length", "\n", "self", ".", "attn_type", "=", "config", ".", "attn_type", "\n", "self", ".", "bi_data", "=", "config", ".", "bi_data", "\n", "self", ".", "clamp_len", "=", "config", ".", "clamp_len", "\n", "self", ".", "n_layer", "=", "config", ".", "n_layer", "\n", "\n", "self", ".", "word_embedding", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "d_model", ")", "\n", "self", ".", "mask_emb", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "1", ",", "1", ",", "config", ".", "d_model", ")", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "XLNetLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "n_layer", ")", "]", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "dropout", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.get_input_embeddings": [[641, 643], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "word_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.set_input_embeddings": [[644, 646], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "new_embeddings", ")", ":", "\n", "        ", "self", ".", "word_embedding", "=", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel._prune_heads": [[647, 649], ["None"], "methods", ["None"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.create_mask": [[650, 679], ["torch.ones", "torch.triu", "torch.zeros", "torch.cat", "torch.cat.to", "torch.tril", "torch.cat"], "methods", ["None"], ["", "def", "create_mask", "(", "self", ",", "qlen", ",", "mlen", ")", ":", "\n", "        ", "\"\"\"\n        Creates causal attention mask. Float mask where 1.0 indicates masked, 0.0 indicates not-masked.\n\n        Args:\n            qlen: Sequence length\n            mlen: Mask length\n\n        ::\n\n                  same_length=False:      same_length=True:\n                  <mlen > <  qlen >       <mlen > <  qlen >\n               ^ [0 0 0 0 0 1 1 1 1]     [0 0 0 0 0 1 1 1 1]\n                 [0 0 0 0 0 0 1 1 1]     [1 0 0 0 0 0 1 1 1]\n            qlen [0 0 0 0 0 0 0 1 1]     [1 1 0 0 0 0 0 1 1]\n                 [0 0 0 0 0 0 0 0 1]     [1 1 1 0 0 0 0 0 1]\n               v [0 0 0 0 0 0 0 0 0]     [1 1 1 1 0 0 0 0 0]\n\n        \"\"\"", "\n", "attn_mask", "=", "torch", ".", "ones", "(", "[", "qlen", ",", "qlen", "]", ")", "\n", "mask_up", "=", "torch", ".", "triu", "(", "attn_mask", ",", "diagonal", "=", "1", ")", "\n", "attn_mask_pad", "=", "torch", ".", "zeros", "(", "[", "qlen", ",", "mlen", "]", ")", "\n", "ret", "=", "torch", ".", "cat", "(", "[", "attn_mask_pad", ",", "mask_up", "]", ",", "dim", "=", "1", ")", "\n", "if", "self", ".", "same_length", ":", "\n", "            ", "mask_lo", "=", "torch", ".", "tril", "(", "attn_mask", ",", "diagonal", "=", "-", "1", ")", "\n", "ret", "=", "torch", ".", "cat", "(", "[", "ret", "[", ":", ",", ":", "qlen", "]", "+", "mask_lo", ",", "ret", "[", ":", ",", "qlen", ":", "]", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "ret", "=", "ret", ".", "to", "(", "self", ".", "device", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.cache_mem": [[680, 691], ["new_mem.detach", "torch.cat"], "methods", ["None"], ["", "def", "cache_mem", "(", "self", ",", "curr_out", ",", "prev_mem", ")", ":", "\n", "# cache hidden states into memory.", "\n", "        ", "if", "self", ".", "reuse_len", "is", "not", "None", "and", "self", ".", "reuse_len", ">", "0", ":", "\n", "            ", "curr_out", "=", "curr_out", "[", ":", "self", ".", "reuse_len", "]", "\n", "\n", "", "if", "prev_mem", "is", "None", ":", "\n", "            ", "new_mem", "=", "curr_out", "[", "-", "self", ".", "mem_len", ":", "]", "\n", "", "else", ":", "\n", "            ", "new_mem", "=", "torch", ".", "cat", "(", "[", "prev_mem", ",", "curr_out", "]", ",", "dim", "=", "0", ")", "[", "-", "self", ".", "mem_len", ":", "]", "\n", "\n", "", "return", "new_mem", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.positional_embedding": [[692, 702], ["torch.einsum", "torch.cat", "pos_emb.expand.expand.expand", "torch.sin", "torch.cos"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "positional_embedding", "(", "pos_seq", ",", "inv_freq", ",", "bsz", "=", "None", ")", ":", "\n", "        ", "sinusoid_inp", "=", "torch", ".", "einsum", "(", "\"i,d->id\"", ",", "pos_seq", ",", "inv_freq", ")", "\n", "pos_emb", "=", "torch", ".", "cat", "(", "[", "torch", ".", "sin", "(", "sinusoid_inp", ")", ",", "torch", ".", "cos", "(", "sinusoid_inp", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "pos_emb", "=", "pos_emb", "[", ":", ",", "None", ",", ":", "]", "\n", "\n", "if", "bsz", "is", "not", "None", ":", "\n", "            ", "pos_emb", "=", "pos_emb", ".", "expand", "(", "-", "1", ",", "bsz", ",", "-", "1", ")", "\n", "\n", "", "return", "pos_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.relative_positional_encoding": [[703, 741], ["torch.arange", "dummy_modeling_xlnet.XLNetModel.to", "torch.pow", "torch.arange", "torch.arange", "torch.cat", "torch.arange", "dummy_modeling_xlnet.XLNetModel.positional_embedding", "ValueError", "fwd_pos_seq.clamp.clamp.clamp", "bwd_pos_seq.clamp.clamp.clamp", "dummy_modeling_xlnet.XLNetModel.positional_embedding", "dummy_modeling_xlnet.XLNetModel.positional_embedding", "dummy_modeling_xlnet.XLNetModel.positional_embedding", "dummy_modeling_xlnet.XLNetModel.positional_embedding", "fwd_pos_seq.clamp.clamp.clamp"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.positional_embedding", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.positional_embedding", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.positional_embedding", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.positional_embedding", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.positional_embedding"], ["", "def", "relative_positional_encoding", "(", "self", ",", "qlen", ",", "klen", ",", "bsz", "=", "None", ")", ":", "\n", "# create relative positional encoding.", "\n", "        ", "freq_seq", "=", "torch", ".", "arange", "(", "0", ",", "self", ".", "d_model", ",", "2.0", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "inv_freq", "=", "1", "/", "torch", ".", "pow", "(", "10000", ",", "(", "freq_seq", "/", "self", ".", "d_model", ")", ")", "\n", "\n", "if", "self", ".", "attn_type", "==", "\"bi\"", ":", "\n", "# beg, end = klen - 1, -qlen", "\n", "            ", "beg", ",", "end", "=", "klen", ",", "-", "qlen", "\n", "", "elif", "self", ".", "attn_type", "==", "\"uni\"", ":", "\n", "# beg, end = klen - 1, -1", "\n", "            ", "beg", ",", "end", "=", "klen", ",", "-", "1", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown `attn_type` {}.\"", ".", "format", "(", "self", ".", "attn_type", ")", ")", "\n", "\n", "", "if", "self", ".", "bi_data", ":", "\n", "            ", "fwd_pos_seq", "=", "torch", ".", "arange", "(", "beg", ",", "end", ",", "-", "1.0", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "bwd_pos_seq", "=", "torch", ".", "arange", "(", "-", "beg", ",", "-", "end", ",", "1.0", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n", "if", "self", ".", "clamp_len", ">", "0", ":", "\n", "                ", "fwd_pos_seq", "=", "fwd_pos_seq", ".", "clamp", "(", "-", "self", ".", "clamp_len", ",", "self", ".", "clamp_len", ")", "\n", "bwd_pos_seq", "=", "bwd_pos_seq", ".", "clamp", "(", "-", "self", ".", "clamp_len", ",", "self", ".", "clamp_len", ")", "\n", "\n", "", "if", "bsz", "is", "not", "None", ":", "\n", "                ", "fwd_pos_emb", "=", "self", ".", "positional_embedding", "(", "fwd_pos_seq", ",", "inv_freq", ",", "bsz", "//", "2", ")", "\n", "bwd_pos_emb", "=", "self", ".", "positional_embedding", "(", "bwd_pos_seq", ",", "inv_freq", ",", "bsz", "//", "2", ")", "\n", "", "else", ":", "\n", "                ", "fwd_pos_emb", "=", "self", ".", "positional_embedding", "(", "fwd_pos_seq", ",", "inv_freq", ")", "\n", "bwd_pos_emb", "=", "self", ".", "positional_embedding", "(", "bwd_pos_seq", ",", "inv_freq", ")", "\n", "\n", "", "pos_emb", "=", "torch", ".", "cat", "(", "[", "fwd_pos_emb", ",", "bwd_pos_emb", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "fwd_pos_seq", "=", "torch", ".", "arange", "(", "beg", ",", "end", ",", "-", "1.0", ")", "\n", "if", "self", ".", "clamp_len", ">", "0", ":", "\n", "                ", "fwd_pos_seq", "=", "fwd_pos_seq", ".", "clamp", "(", "-", "self", ".", "clamp_len", ",", "self", ".", "clamp_len", ")", "\n", "", "pos_emb", "=", "self", ".", "positional_embedding", "(", "fwd_pos_seq", ",", "inv_freq", ",", "bsz", ")", "\n", "\n", "", "pos_emb", "=", "pos_emb", ".", "to", "(", "self", ".", "device", ")", "\n", "return", "pos_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.forward": [[742, 972], ["transformers.file_utils.add_start_docstrings_to_callable", "transformers.file_utils.add_code_sample_docstrings", "dummy_modeling_xlnet.XLNetModel.dropout", "dummy_modeling_xlnet.XLNetModel.relative_positional_encoding", "dummy_modeling_xlnet.XLNetModel.dropout", "enumerate", "dummy_modeling_xlnet.XLNetModel.dropout", "XLNET_INPUTS_DOCSTRING.format", "ValueError", "token_type_ids.transpose().contiguous", "input_mask.transpose().contiguous", "attention_mask.transpose().contiguous", "perm_mask.permute().contiguous", "target_mapping.permute().contiguous", "dummy_modeling_xlnet.XLNetModel.create_mask", "input_ids.transpose().contiguous.transpose().contiguous.transpose", "dummy_modeling_xlnet.XLNetModel.word_embedding", "word_emb_k.transpose.transpose.transpose", "dummy_modeling_xlnet.XLNetModel.mask_emb.expand", "dummy_modeling_xlnet.XLNetModel.dropout", "torch.nn.functional.one_hot().to", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.to", "layer_module", "tuple.append", "dummy_modeling_xlnet.XLNetModel.permute().contiguous", "input_ids.transpose().contiguous.transpose().contiguous.transpose().contiguous", "ValueError", "torch.zeros().to", "torch.cat", "torch.eye().to", "torch.cat", "torch.zeros", "torch.cat", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.expand", "len", "tuple.append", "tuple.append", "tuple", "tuple", "tuple", "tuple", "inputs_embeds.transpose().contiguous.transpose().contiguous.transpose().contiguous", "ValueError", "token_type_ids.transpose", "input_mask.transpose", "attention_mask.transpose", "perm_mask.permute", "target_mapping.permute", "torch.nn.functional.one_hot", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "dummy_modeling_xlnet.XLNetModel.permute", "input_ids.transpose().contiguous.transpose().contiguous.transpose", "torch.zeros", "torch.eye", "torch.zeros().to", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "next", "dummy_modeling_xlnet.XLNetModel.cache_mem", "h.permute().contiguous", "hs.permute().contiguous", "tuple", "t.permute().contiguous", "inputs_embeds.transpose().contiguous.transpose().contiguous.transpose", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "dummy_modeling_xlnet.XLNetModel.parameters", "torch.zeros", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "h.permute", "hs.permute", "att_stream.permute().contiguous", "t.permute", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze", "att_stream.permute"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.relative_positional_encoding", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.create_mask", "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetModel.cache_mem"], ["", "@", "add_start_docstrings_to_callable", "(", "XLNET_INPUTS_DOCSTRING", ".", "format", "(", "\"(batch_size, sequence_length)\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "checkpoint", "=", "\"xlnet-base-cased\"", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "mems", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "input_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "use_cache", "=", "True", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n    Return:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.XLNetConfig`) and inputs:\n        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_predict, hidden_size)`):\n            Sequence of hidden-states at the last layer of the model.\n            `num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping` is `None`, then `num_predict` corresponds to `sequence_length`.\n        mems (:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers`):\n            Contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `mems` input) to speed up sequential decoding. The token ids which have their past given to this model\n            should not be passed as input ids as they have already been computed.\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "\n", "# the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end", "\n", "# but we want a unified interface in the library with the batch size on the first dimension", "\n", "# so we move here the first dimension (batch) to the end", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_ids", "=", "input_ids", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "qlen", ",", "bsz", "=", "input_ids", ".", "shape", "[", "0", "]", ",", "input_ids", ".", "shape", "[", "1", "]", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "inputs_embeds", "=", "inputs_embeds", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "qlen", ",", "bsz", "=", "inputs_embeds", ".", "shape", "[", "0", "]", ",", "inputs_embeds", ".", "shape", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "token_type_ids", "=", "token_type_ids", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "input_mask", "=", "input_mask", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "if", "input_mask", "is", "not", "None", "else", "None", "\n", "attention_mask", "=", "attention_mask", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "perm_mask", "=", "perm_mask", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "contiguous", "(", ")", "if", "perm_mask", "is", "not", "None", "else", "None", "\n", "target_mapping", "=", "target_mapping", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "contiguous", "(", ")", "if", "target_mapping", "is", "not", "None", "else", "None", "\n", "\n", "mlen", "=", "mems", "[", "0", "]", ".", "shape", "[", "0", "]", "if", "mems", "is", "not", "None", "and", "mems", "[", "0", "]", "is", "not", "None", "else", "0", "\n", "klen", "=", "mlen", "+", "qlen", "\n", "\n", "dtype_float", "=", "self", ".", "dtype", "\n", "device", "=", "self", ".", "device", "\n", "\n", "# Attention mask", "\n", "# causal attention mask", "\n", "if", "self", ".", "attn_type", "==", "\"uni\"", ":", "\n", "            ", "attn_mask", "=", "self", ".", "create_mask", "(", "qlen", ",", "mlen", ")", "\n", "attn_mask", "=", "attn_mask", "[", ":", ",", ":", ",", "None", ",", "None", "]", "\n", "", "elif", "self", ".", "attn_type", "==", "\"bi\"", ":", "\n", "            ", "attn_mask", "=", "None", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported attention type: {}\"", ".", "format", "(", "self", ".", "attn_type", ")", ")", "\n", "\n", "# data mask: input mask & perm mask", "\n", "", "assert", "input_mask", "is", "None", "or", "attention_mask", "is", "None", ",", "\"You can only use one of input_mask (uses 1 for padding) \"", "\n", "\"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"", "\n", "if", "input_mask", "is", "None", "and", "attention_mask", "is", "not", "None", ":", "\n", "            ", "input_mask", "=", "1.0", "-", "attention_mask", "\n", "", "if", "input_mask", "is", "not", "None", "and", "perm_mask", "is", "not", "None", ":", "\n", "            ", "data_mask", "=", "input_mask", "[", "None", "]", "+", "perm_mask", "\n", "", "elif", "input_mask", "is", "not", "None", "and", "perm_mask", "is", "None", ":", "\n", "            ", "data_mask", "=", "input_mask", "[", "None", "]", "\n", "", "elif", "input_mask", "is", "None", "and", "perm_mask", "is", "not", "None", ":", "\n", "            ", "data_mask", "=", "perm_mask", "\n", "", "else", ":", "\n", "            ", "data_mask", "=", "None", "\n", "\n", "", "if", "data_mask", "is", "not", "None", ":", "\n", "# all mems can be attended to", "\n", "            ", "if", "mlen", ">", "0", ":", "\n", "                ", "mems_mask", "=", "torch", ".", "zeros", "(", "[", "data_mask", ".", "shape", "[", "0", "]", ",", "mlen", ",", "bsz", "]", ")", ".", "to", "(", "data_mask", ")", "\n", "data_mask", "=", "torch", ".", "cat", "(", "[", "mems_mask", ",", "data_mask", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "attn_mask", "is", "None", ":", "\n", "                ", "attn_mask", "=", "data_mask", "[", ":", ",", ":", ",", ":", ",", "None", "]", "\n", "", "else", ":", "\n", "                ", "attn_mask", "+=", "data_mask", "[", ":", ",", ":", ",", ":", ",", "None", "]", "\n", "\n", "", "", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "(", "attn_mask", ">", "0", ")", ".", "to", "(", "dtype_float", ")", "\n", "\n", "", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "non_tgt_mask", "=", "-", "torch", ".", "eye", "(", "qlen", ")", ".", "to", "(", "attn_mask", ")", "\n", "if", "mlen", ">", "0", ":", "\n", "                ", "non_tgt_mask", "=", "torch", ".", "cat", "(", "[", "torch", ".", "zeros", "(", "[", "qlen", ",", "mlen", "]", ")", ".", "to", "(", "attn_mask", ")", ",", "non_tgt_mask", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "non_tgt_mask", "=", "(", "(", "attn_mask", "+", "non_tgt_mask", "[", ":", ",", ":", ",", "None", ",", "None", "]", ")", ">", "0", ")", ".", "to", "(", "attn_mask", ")", "\n", "", "else", ":", "\n", "            ", "non_tgt_mask", "=", "None", "\n", "\n", "# Word embeddings and prepare h & g hidden states", "\n", "", "if", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "word_emb_k", "=", "inputs_embeds", "\n", "", "else", ":", "\n", "# Important", "\n", "# input_id is input_ids.transpose(0, 1).contiguous()", "\n", "            ", "input_ids", "=", "input_ids", ".", "transpose", "(", "0", ",", "1", ")", "\n", "word_emb_k", "=", "self", ".", "word_embedding", "(", "input_ids", ")", "\n", "# reverse the function", "\n", "word_emb_k", "=", "word_emb_k", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "output_h", "=", "self", ".", "dropout", "(", "word_emb_k", ")", "\n", "if", "target_mapping", "is", "not", "None", ":", "\n", "            ", "word_emb_q", "=", "self", ".", "mask_emb", ".", "expand", "(", "target_mapping", ".", "shape", "[", "0", "]", ",", "bsz", ",", "-", "1", ")", "\n", "# else:  # We removed the inp_q input which was same as target mapping", "\n", "#     inp_q_ext = inp_q[:, :, None]", "\n", "#     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k", "\n", "output_g", "=", "self", ".", "dropout", "(", "word_emb_q", ")", "\n", "", "else", ":", "\n", "            ", "output_g", "=", "None", "\n", "\n", "# Segment embedding", "\n", "", "if", "token_type_ids", "is", "not", "None", ":", "\n", "# Convert `token_type_ids` to one-hot `seg_mat`", "\n", "            ", "if", "mlen", ">", "0", ":", "\n", "                ", "mem_pad", "=", "torch", ".", "zeros", "(", "[", "mlen", ",", "bsz", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "cat_ids", "=", "torch", ".", "cat", "(", "[", "mem_pad", ",", "token_type_ids", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "cat_ids", "=", "token_type_ids", "\n", "\n", "# `1` indicates not in the same segment [qlen x klen x bsz]", "\n", "", "seg_mat", "=", "(", "token_type_ids", "[", ":", ",", "None", "]", "!=", "cat_ids", "[", "None", ",", ":", "]", ")", ".", "long", "(", ")", "\n", "seg_mat", "=", "F", ".", "one_hot", "(", "seg_mat", ",", "num_classes", "=", "2", ")", ".", "to", "(", "dtype_float", ")", "\n", "", "else", ":", "\n", "            ", "seg_mat", "=", "None", "\n", "\n", "# Positional encoding", "\n", "", "pos_emb", "=", "self", ".", "relative_positional_encoding", "(", "qlen", ",", "klen", ",", "bsz", "=", "bsz", ")", "\n", "pos_emb", "=", "self", ".", "dropout", "(", "pos_emb", ")", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)", "\n", "# and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "if", "head_mask", ".", "dim", "(", ")", "==", "1", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "head_mask", "=", "head_mask", ".", "expand", "(", "self", ".", "n_layer", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "", "elif", "head_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "head_mask", "=", "head_mask", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", "\n", ")", "# switch to fload if need + fp16 compatibility", "\n", "", "else", ":", "\n", "            ", "head_mask", "=", "[", "None", "]", "*", "self", ".", "n_layer", "\n", "\n", "", "new_mems", "=", "(", ")", "\n", "if", "mems", "is", "None", ":", "\n", "            ", "mems", "=", "[", "None", "]", "*", "len", "(", "self", ".", "layer", ")", "\n", "\n", "", "attentions", "=", "[", "]", "\n", "hidden_states", "=", "[", "]", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "self", ".", "mem_len", "is", "not", "None", "and", "self", ".", "mem_len", ">", "0", "and", "use_cache", "is", "True", ":", "\n", "# cache new mems", "\n", "                ", "new_mems", "=", "new_mems", "+", "(", "self", ".", "cache_mem", "(", "output_h", ",", "mems", "[", "i", "]", ")", ",", ")", "\n", "", "if", "output_hidden_states", ":", "\n", "                ", "hidden_states", ".", "append", "(", "(", "output_h", ",", "output_g", ")", "if", "output_g", "is", "not", "None", "else", "output_h", ")", "\n", "\n", "", "outputs", "=", "layer_module", "(", "\n", "output_h", ",", "\n", "output_g", ",", "\n", "attn_mask_h", "=", "non_tgt_mask", ",", "\n", "attn_mask_g", "=", "attn_mask", ",", "\n", "r", "=", "pos_emb", ",", "\n", "seg_mat", "=", "seg_mat", ",", "\n", "mems", "=", "mems", "[", "i", "]", ",", "\n", "target_mapping", "=", "target_mapping", ",", "\n", "head_mask", "=", "head_mask", "[", "i", "]", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "output_h", ",", "output_g", "=", "outputs", "[", ":", "2", "]", "\n", "if", "output_attentions", ":", "\n", "                ", "attentions", ".", "append", "(", "outputs", "[", "2", "]", ")", "\n", "\n", "# Add last hidden state", "\n", "", "", "if", "output_hidden_states", ":", "\n", "            ", "hidden_states", ".", "append", "(", "(", "output_h", ",", "output_g", ")", "if", "output_g", "is", "not", "None", "else", "output_h", ")", "\n", "\n", "", "output", "=", "self", ".", "dropout", "(", "output_g", "if", "output_g", "is", "not", "None", "else", "output_h", ")", "\n", "\n", "# Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)", "\n", "outputs", "=", "(", "output", ".", "permute", "(", "1", ",", "0", ",", "2", ")", ".", "contiguous", "(", ")", ",", ")", "\n", "\n", "if", "self", ".", "mem_len", "is", "not", "None", "and", "self", ".", "mem_len", ">", "0", "and", "use_cache", "is", "True", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "new_mems", ",", ")", "\n", "\n", "", "if", "output_hidden_states", ":", "\n", "            ", "if", "output_g", "is", "not", "None", ":", "\n", "                ", "hidden_states", "=", "tuple", "(", "h", ".", "permute", "(", "1", ",", "0", ",", "2", ")", ".", "contiguous", "(", ")", "for", "hs", "in", "hidden_states", "for", "h", "in", "hs", ")", "\n", "", "else", ":", "\n", "                ", "hidden_states", "=", "tuple", "(", "hs", ".", "permute", "(", "1", ",", "0", ",", "2", ")", ".", "contiguous", "(", ")", "for", "hs", "in", "hidden_states", ")", "\n", "", "outputs", "=", "outputs", "+", "(", "hidden_states", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "            ", "if", "target_mapping", "is", "not", "None", ":", "\n", "# when target_mapping is provided, there are 2-tuple of attentions", "\n", "                ", "attentions", "=", "tuple", "(", "\n", "tuple", "(", "att_stream", ".", "permute", "(", "2", ",", "3", ",", "0", ",", "1", ")", ".", "contiguous", "(", ")", "for", "att_stream", "in", "t", ")", "for", "t", "in", "attentions", "\n", ")", "\n", "", "else", ":", "\n", "                ", "attentions", "=", "tuple", "(", "t", ".", "permute", "(", "2", ",", "3", ",", "0", ",", "1", ")", ".", "contiguous", "(", ")", "for", "t", "in", "attentions", ")", "\n", "", "outputs", "=", "outputs", "+", "(", "attentions", ",", ")", "\n", "\n", "", "return", "outputs", "# outputs, (new_mems), (hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetLMHeadModel.__init__": [[980, 989], ["transformers.modeling_utils.PreTrainedModel.__init__", "dummy_modeling_xlnet.XLNetModel", "torch.nn.Linear", "dummy_modeling_xlnet.XLNetLMHeadModel.init_weights"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "attn_type", "=", "config", ".", "attn_type", "\n", "self", ".", "same_length", "=", "config", ".", "same_length", "\n", "\n", "self", ".", "transformer", "=", "XLNetModel", "(", "config", ")", "\n", "self", ".", "lm_loss", "=", "nn", ".", "Linear", "(", "config", ".", "d_model", ",", "config", ".", "vocab_size", ",", "bias", "=", "True", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetLMHeadModel.get_output_embeddings": [[990, 992], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lm_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetLMHeadModel.prepare_inputs_for_generation": [[993, 1025], ["torch.zeros", "torch.cat", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "self", ",", "input_ids", ",", "past", ",", "**", "kwargs", ")", ":", "\n", "# Add dummy token at the end (no attention on this one)", "\n", "\n", "        ", "effective_batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "\n", "dummy_token", "=", "torch", ".", "zeros", "(", "(", "effective_batch_size", ",", "1", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "dummy_token", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# Build permutation mask so that previous tokens don't see last token", "\n", "sequence_length", "=", "input_ids", ".", "shape", "[", "1", "]", "\n", "perm_mask", "=", "torch", ".", "zeros", "(", "\n", "(", "effective_batch_size", ",", "sequence_length", ",", "sequence_length", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "input_ids", ".", "device", "\n", ")", "\n", "perm_mask", "[", ":", ",", ":", ",", "-", "1", "]", "=", "1.0", "\n", "\n", "# We'll only predict the last token", "\n", "target_mapping", "=", "torch", ".", "zeros", "(", "\n", "(", "effective_batch_size", ",", "1", ",", "sequence_length", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "input_ids", ".", "device", "\n", ")", "\n", "target_mapping", "[", "0", ",", "0", ",", "-", "1", "]", "=", "1.0", "\n", "\n", "inputs", "=", "{", "\n", "\"input_ids\"", ":", "input_ids", ",", "\n", "\"perm_mask\"", ":", "perm_mask", ",", "\n", "\"target_mapping\"", ":", "target_mapping", ",", "\n", "\"use_cache\"", ":", "kwargs", "[", "\"use_cache\"", "]", ",", "\n", "}", "\n", "\n", "# if past is defined in model kwargs then use it for faster decoding", "\n", "if", "past", ":", "\n", "            ", "inputs", "[", "\"mems\"", "]", "=", "past", "\n", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetLMHeadModel.forward": [[1026, 1132], ["transformers.file_utils.add_start_docstrings_to_callable", "dummy_modeling_xlnet.XLNetLMHeadModel.transformer", "dummy_modeling_xlnet.XLNetLMHeadModel.lm_loss", "XLNET_INPUTS_DOCSTRING.format", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "dummy_modeling_xlnet.XLNetLMHeadModel.view", "labels.view", "dummy_modeling_xlnet.XLNetLMHeadModel.size"], "methods", ["None"], ["", "@", "add_start_docstrings_to_callable", "(", "XLNET_INPUTS_DOCSTRING", ".", "format", "(", "\"(batch_size, sequence_length)\"", ")", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "mems", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "input_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "use_cache", "=", "True", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, num_predict)`, `optional`, defaults to :obj:`None`):\n            Labels for masked language modeling.\n            `num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping` is `None`, then `num_predict` corresponds to `sequence_length`.\n            The labels should correspond to the masked input words that should be predicted and depends on `target_mapping`. Note in order to perform standard auto-regressive language modeling a `<mask>` token has to be added to the `input_ids` (see `prepare_inputs_for_generation` fn and examples below)\n            Indices are selected in ``[-100, 0, ..., config.vocab_size]``\n            All labels set to ``-100`` are ignored, the loss is only\n            computed for labels in ``[0, ..., config.vocab_size]``\n\n    Return:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.XLNetConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor` of shape `(1,)`, `optional`, returned when ``labels`` is provided)\n            Language modeling loss.\n        prediction_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_predict, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n            `num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping` is `None`, then `num_predict` corresponds to `sequence_length`.\n        mems (:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers`):\n            Contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding. The token ids which have their past given to this model\n            should not be passed as input ids as they have already been computed.\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n\n    Examples::\n\n        from transformers import XLNetTokenizer, XLNetLMHeadModel\n        import torch\n\n        tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n        model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')\n\n        # We show how to setup inputs to predict a next token using a bi-directional context.\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is very <mask>\", add_special_tokens=False)).unsqueeze(0)  # We will predict the masked token\n        perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n        perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n        target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  # Shape [1, 1, seq_length] => let's predict one token\n        target_mapping[0, 0, -1] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n\n        outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n        next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n\n        # The same way can the XLNetLMHeadModel be used to be trained by standard auto-regressive language modeling.\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is very <mask>\", add_special_tokens=False)).unsqueeze(0)  # We will predict the masked token\n        labels = torch.tensor(tokenizer.encode(\"cute\", add_special_tokens=False)).unsqueeze(0)\n        assert labels.shape[0] == 1, 'only one word will be predicted'\n        perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n        perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token as is done in standard auto-regressive lm training\n        target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  # Shape [1, 1, seq_length] => let's predict one token\n        target_mapping[0, 0, -1] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n\n        outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping, labels=labels)\n        loss, next_token_logits = outputs[:2]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n\n        \"\"\"", "\n", "transformer_outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "mems", "=", "mems", ",", "\n", "perm_mask", "=", "perm_mask", ",", "\n", "target_mapping", "=", "target_mapping", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "\n", "logits", "=", "self", ".", "lm_loss", "(", "transformer_outputs", "[", "0", "]", ")", "\n", "\n", "outputs", "=", "(", "logits", ",", ")", "+", "transformer_outputs", "[", "1", ":", "]", "# Keep mems, hidden states, attentions if there are in it", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "# Flatten the tokens", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "logits", ".", "size", "(", "-", "1", ")", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# return (loss), logits, (mems), (hidden states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForSequenceClassification.__init__": [[1140, 1149], ["transformers.modeling_utils.PreTrainedModel.__init__", "dummy_modeling_xlnet.XLNetModel", "transformers.modeling_utils.SequenceSummary", "torch.nn.Linear", "dummy_modeling_xlnet.XLNetForSequenceClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "transformer", "=", "XLNetModel", "(", "config", ")", "\n", "self", ".", "sequence_summary", "=", "SequenceSummary", "(", "config", ")", "\n", "self", ".", "logits_proj", "=", "nn", ".", "Linear", "(", "config", ".", "d_model", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForSequenceClassification.forward": [[1150, 1229], ["transformers.file_utils.add_start_docstrings_to_callable", "transformers.file_utils.add_code_sample_docstrings", "dummy_modeling_xlnet.XLNetForSequenceClassification.transformer", "dummy_modeling_xlnet.XLNetForSequenceClassification.sequence_summary", "dummy_modeling_xlnet.XLNetForSequenceClassification.logits_proj", "XLNET_INPUTS_DOCSTRING.format", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "dummy_modeling_xlnet.XLNetForSequenceClassification.view", "labels.view", "dummy_modeling_xlnet.XLNetForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "@", "add_start_docstrings_to_callable", "(", "XLNET_INPUTS_DOCSTRING", ".", "format", "(", "\"(batch_size, sequence_length)\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "checkpoint", "=", "\"xlnet-base-cased\"", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "mems", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "input_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "use_cache", "=", "True", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`)\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Return:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.XLNetConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n            Classification (or regression if config.num_labels==1) loss.\n        logits (:obj:`torch.FloatTensor` of shape :obj:(batch_size, config.num_labels)`):\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        mems (:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers`):\n            Contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding. The token ids which have their past given to this model\n            should not be passed as input ids as they have already been computed.\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        \"\"\"", "\n", "transformer_outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "mems", "=", "mems", ",", "\n", "perm_mask", "=", "perm_mask", ",", "\n", "target_mapping", "=", "target_mapping", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "output", "=", "transformer_outputs", "[", "0", "]", "\n", "\n", "output", "=", "self", ".", "sequence_summary", "(", "output", ")", "\n", "logits", "=", "self", ".", "logits_proj", "(", "output", ")", "\n", "\n", "outputs", "=", "(", "logits", ",", ")", "+", "transformer_outputs", "[", "1", ":", "]", "# Keep mems, hidden states, attentions if there are in it", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "#  We are doing regression", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# return (loss), logits, (mems), (hidden states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForTokenClassification.__init__": [[1237, 1245], ["transformers.modeling_utils.PreTrainedModel.__init__", "dummy_modeling_xlnet.XLNetModel", "torch.nn.Linear", "dummy_modeling_xlnet.XLNetForTokenClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "transformer", "=", "XLNetModel", "(", "config", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForTokenClassification.forward": [[1246, 1328], ["transformers.file_utils.add_start_docstrings_to_callable", "transformers.file_utils.add_code_sample_docstrings", "dummy_modeling_xlnet.XLNetForTokenClassification.transformer", "dummy_modeling_xlnet.XLNetForTokenClassification.classifier", "XLNET_INPUTS_DOCSTRING.format", "torch.nn.CrossEntropyLoss", "dummy_modeling_xlnet.XLNetForTokenClassification.view", "torch.where", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "labels.view", "torch.tensor().type_as", "dummy_modeling_xlnet.XLNetForTokenClassification.view", "labels.view", "torch.tensor"], "methods", ["None"], ["", "@", "add_start_docstrings_to_callable", "(", "XLNET_INPUTS_DOCSTRING", ".", "format", "(", "\"(batch_size, sequence_length)\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "checkpoint", "=", "\"xlnet-base-cased\"", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "mems", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "input_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "use_cache", "=", "True", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n\n    Return:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.XLNetConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n            Classification loss.\n        logits (:obj:`torch.FloatTensor` of shape :obj:(batch_size, config.num_labels)`):\n            Classification scores (before SoftMax).\n        mems (:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers`):\n            Contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding. The token ids which have their past given to this model\n            should not be passed as input ids as they have already been computed.\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        \"\"\"", "\n", "\n", "outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "mems", "=", "mems", ",", "\n", "perm_mask", "=", "perm_mask", ",", "\n", "target_mapping", "=", "target_mapping", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "outputs", "=", "(", "logits", ",", ")", "+", "outputs", "[", "1", ":", "]", "# Keep mems, hidden states, attentions if there are in it", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "\n", "active_labels", "=", "torch", ".", "where", "(", "\n", "active_loss", ",", "labels", ".", "view", "(", "-", "1", ")", ",", "torch", ".", "tensor", "(", "loss_fct", ".", "ignore_index", ")", ".", "type_as", "(", "labels", ")", "\n", ")", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# return (loss), logits, (mems), (hidden states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForMultipleChoice.__init__": [[1336, 1344], ["transformers.modeling_utils.PreTrainedModel.__init__", "dummy_modeling_xlnet.XLNetModel", "transformers.modeling_utils.SequenceSummary", "torch.nn.Linear", "dummy_modeling_xlnet.XLNetForMultipleChoice.init_weights"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "transformer", "=", "XLNetModel", "(", "config", ")", "\n", "self", ".", "sequence_summary", "=", "SequenceSummary", "(", "config", ")", "\n", "self", ".", "logits_proj", "=", "nn", ".", "Linear", "(", "config", ".", "d_model", ",", "1", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForMultipleChoice.forward": [[1345, 1435], ["transformers.file_utils.add_start_docstrings_to_callable", "transformers.file_utils.add_code_sample_docstrings", "dummy_modeling_xlnet.XLNetForMultipleChoice.transformer", "dummy_modeling_xlnet.XLNetForMultipleChoice.sequence_summary", "dummy_modeling_xlnet.XLNetForMultipleChoice.logits_proj", "dummy_modeling_xlnet.XLNetForMultipleChoice.view", "XLNET_INPUTS_DOCSTRING.format", "input_ids.view", "token_type_ids.view", "attention_mask.view", "input_mask.view", "inputs_embeds.view", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "input_ids.size", "token_type_ids.size", "attention_mask.size", "input_mask.size", "inputs_embeds.size", "inputs_embeds.size", "labels.view"], "methods", ["None"], ["", "@", "add_start_docstrings_to_callable", "(", "XLNET_INPUTS_DOCSTRING", ".", "format", "(", "\"(batch_size, num_choices, sequence_length)\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "checkpoint", "=", "\"xlnet-base-cased\"", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "input_mask", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "mems", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "use_cache", "=", "True", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n\n    Returns:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.XLNetConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor`` of shape `(1,)`, `optional`, returned when :obj:`labels` is provided):\n            Classification loss.\n        classification_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_choices)`):\n            `num_choices` is the second dimension of the input tensors. (see `input_ids` above).\n\n            Classification scores (before SoftMax).\n        mems (:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers`):\n            Contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding. The token ids which have their past given to this model\n            should not be passed as input ids as they have already been computed.\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        \"\"\"", "\n", "num_choices", "=", "input_ids", ".", "shape", "[", "1", "]", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "shape", "[", "1", "]", "\n", "\n", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "if", "input_ids", "is", "not", "None", "else", "None", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "flat_input_mask", "=", "input_mask", ".", "view", "(", "-", "1", ",", "input_mask", ".", "size", "(", "-", "1", ")", ")", "if", "input_mask", "is", "not", "None", "else", "None", "\n", "flat_inputs_embeds", "=", "(", "\n", "inputs_embeds", ".", "view", "(", "-", "1", ",", "inputs_embeds", ".", "size", "(", "-", "2", ")", ",", "inputs_embeds", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "inputs_embeds", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "\n", "transformer_outputs", "=", "self", ".", "transformer", "(", "\n", "flat_input_ids", ",", "\n", "token_type_ids", "=", "flat_token_type_ids", ",", "\n", "input_mask", "=", "flat_input_mask", ",", "\n", "attention_mask", "=", "flat_attention_mask", ",", "\n", "mems", "=", "mems", ",", "\n", "perm_mask", "=", "perm_mask", ",", "\n", "target_mapping", "=", "target_mapping", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "flat_inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "\n", "output", "=", "transformer_outputs", "[", "0", "]", "\n", "\n", "output", "=", "self", ".", "sequence_summary", "(", "output", ")", "\n", "logits", "=", "self", ".", "logits_proj", "(", "output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "num_choices", ")", "\n", "outputs", "=", "(", "reshaped_logits", ",", ")", "+", "transformer_outputs", "[", "\n", "1", ":", "\n", "]", "# Keep mems, hidden states, attentions if there are in it", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# return (loss), logits, (mems), (hidden states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForQuestionAnsweringSimple.__init__": [[1443, 1451], ["transformers.modeling_utils.PreTrainedModel.__init__", "dummy_modeling_xlnet.XLNetModel", "torch.nn.Linear", "dummy_modeling_xlnet.XLNetForQuestionAnsweringSimple.init_weights"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "transformer", "=", "XLNetModel", "(", "config", ")", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForQuestionAnsweringSimple.forward": [[1452, 1547], ["transformers.file_utils.add_start_docstrings_to_callable", "transformers.file_utils.add_code_sample_docstrings", "dummy_modeling_xlnet.XLNetForQuestionAnsweringSimple.transformer", "dummy_modeling_xlnet.XLNetForQuestionAnsweringSimple.qa_outputs", "dummy_modeling_xlnet.XLNetForQuestionAnsweringSimple.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "XLNET_INPUTS_DOCSTRING.format", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["None"], ["", "@", "add_start_docstrings_to_callable", "(", "XLNET_INPUTS_DOCSTRING", ".", "format", "(", "\"(batch_size, sequence_length)\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "checkpoint", "=", "\"xlnet-base-cased\"", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "mems", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "input_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "use_cache", "=", "True", ",", "\n", "start_positions", "=", "None", ",", "\n", "end_positions", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n\n    Returns:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.XLNetConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n        start_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length,)`):\n            Span-start scores (before SoftMax).\n        end_scores (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length,)`):\n            Span-end scores (before SoftMax).\n        mems (:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers`):\n            Contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding. The token ids which have their past given to this model\n            should not be passed as input ids as they have already been computed.\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        \"\"\"", "\n", "\n", "outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "mems", "=", "mems", ",", "\n", "perm_mask", "=", "perm_mask", ",", "\n", "target_mapping", "=", "target_mapping", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "outputs", "=", "(", "start_logits", ",", "end_logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "outputs", "=", "(", "total_loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# (loss), start_logits, end_logits, (mems), (hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForQuestionAnswering.__init__": [[1555, 1566], ["transformers.modeling_utils.PreTrainedModel.__init__", "dummy_modeling_xlnet.XLNetModel", "transformers.modeling_utils.PoolerStartLogits", "transformers.modeling_utils.PoolerEndLogits", "transformers.modeling_utils.PoolerAnswerClass", "dummy_modeling_xlnet.XLNetForQuestionAnswering.init_weights"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "start_n_top", "=", "config", ".", "start_n_top", "\n", "self", ".", "end_n_top", "=", "config", ".", "end_n_top", "\n", "\n", "self", ".", "transformer", "=", "XLNetModel", "(", "config", ")", "\n", "self", ".", "start_logits", "=", "PoolerStartLogits", "(", "config", ")", "\n", "self", ".", "end_logits", "=", "PoolerEndLogits", "(", "config", ")", "\n", "self", ".", "answer_class", "=", "PoolerAnswerClass", "(", "config", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.XLNetForQuestionAnswering.forward": [[1567, 1731], ["transformers.file_utils.add_start_docstrings_to_callable", "dummy_modeling_xlnet.XLNetForQuestionAnswering.transformer", "dummy_modeling_xlnet.XLNetForQuestionAnswering.start_logits", "XLNET_INPUTS_DOCSTRING.format", "dummy_modeling_xlnet.XLNetForQuestionAnswering.end_logits", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "hidden_states.size", "torch.nn.functional.softmax", "torch.topk", "start_top_index.unsqueeze().expand", "torch.gather", "torch.einsum.unsqueeze().expand", "hidden_states.unsqueeze().expand_as", "dummy_modeling_xlnet.XLNetForQuestionAnswering.end_logits", "torch.nn.functional.softmax", "torch.topk", "end_top_log_probs.view.view.view", "end_top_index.view.view.view", "torch.einsum", "dummy_modeling_xlnet.XLNetForQuestionAnswering.answer_class", "dummy_modeling_xlnet.XLNetForQuestionAnswering.answer_class", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "p_mask.unsqueeze", "x.squeeze_", "start_top_index.unsqueeze", "torch.einsum.unsqueeze", "hidden_states.unsqueeze", "x.dim"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.softmax", "home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.softmax"], ["", "@", "add_start_docstrings_to_callable", "(", "XLNET_INPUTS_DOCSTRING", ".", "format", "(", "\"(batch_size, sequence_length)\"", ")", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "mems", "=", "None", ",", "\n", "perm_mask", "=", "None", ",", "\n", "target_mapping", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "input_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "use_cache", "=", "True", ",", "\n", "start_positions", "=", "None", ",", "\n", "end_positions", "=", "None", ",", "\n", "is_impossible", "=", "None", ",", "\n", "cls_index", "=", "None", ",", "\n", "p_mask", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        is_impossible (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`, defaults to :obj:`None`):\n            Labels whether a question has an answer or no answer (SQuAD 2.0)\n        cls_index (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`, defaults to :obj:`None`):\n            Labels for position (index) of the classification token to use as input for computing plausibility of the answer.\n        p_mask (``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``, `optional`, defaults to :obj:`None`):\n            Optional mask of tokens which can't be in answers (e.g. [CLS], [PAD], ...).\n            1.0 means token should be masked. 0.0 mean token is not masked.\n\n    Returns:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.XLNetConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned if both :obj:`start_positions` and :obj:`end_positions` are provided):\n            Classification loss as the sum of start token, end token (and is_impossible if provided) classification losses.\n        start_top_log_probs (``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n        start_top_index (``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n            Indices for the top config.start_n_top start token possibilities (beam-search).\n        end_top_log_probs (``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        end_top_index (``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        cls_logits (``torch.FloatTensor`` of shape ``(batch_size,)``, `optional`, returned if ``start_positions`` or ``end_positions`` is not provided):\n            Log probabilities for the ``is_impossible`` label of the answers.\n        mems (:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers`):\n            Contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding. The token ids which have their past given to this model\n            should not be passed as input ids as they have already been computed.\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n\n        Example::\n\n            >>> from transformers import XLNetTokenizer, XLNetForQuestionAnswering\n            >>> import torch\n\n            >>> tokenizer =  XLNetTokenizer.from_pretrained('xlnet-base-cased')\n            >>> model = XLNetForQuestionAnswering.from_pretrained('xlnet-base-cased')\n\n            >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n            >>> start_positions = torch.tensor([1])\n            >>> end_positions = torch.tensor([3])\n            >>> outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n\n            >>> loss = outputs[0]\n        \"\"\"", "\n", "transformer_outputs", "=", "self", ".", "transformer", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "mems", "=", "mems", ",", "\n", "perm_mask", "=", "perm_mask", ",", "\n", "target_mapping", "=", "target_mapping", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "hidden_states", "=", "transformer_outputs", "[", "0", "]", "\n", "start_logits", "=", "self", ".", "start_logits", "(", "hidden_states", ",", "p_mask", "=", "p_mask", ")", "\n", "\n", "outputs", "=", "transformer_outputs", "[", "1", ":", "]", "# Keep mems, hidden states, attentions if there are in it", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, let's remove the dimension added by batch splitting", "\n", "            ", "for", "x", "in", "(", "start_positions", ",", "end_positions", ",", "cls_index", ",", "is_impossible", ")", ":", "\n", "                ", "if", "x", "is", "not", "None", "and", "x", ".", "dim", "(", ")", ">", "1", ":", "\n", "                    ", "x", ".", "squeeze_", "(", "-", "1", ")", "\n", "\n", "# during training, compute the end logits based on the ground truth of the start position", "\n", "", "", "end_logits", "=", "self", ".", "end_logits", "(", "hidden_states", ",", "start_positions", "=", "start_positions", ",", "p_mask", "=", "p_mask", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "\n", "if", "cls_index", "is", "not", "None", "and", "is_impossible", "is", "not", "None", ":", "\n", "# Predict answerability from the representation of CLS and START", "\n", "                ", "cls_logits", "=", "self", ".", "answer_class", "(", "hidden_states", ",", "start_positions", "=", "start_positions", ",", "cls_index", "=", "cls_index", ")", "\n", "loss_fct_cls", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "cls_loss", "=", "loss_fct_cls", "(", "cls_logits", ",", "is_impossible", ")", "\n", "\n", "# note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss", "\n", "total_loss", "+=", "cls_loss", "*", "0.5", "\n", "\n", "", "outputs", "=", "(", "total_loss", ",", ")", "+", "outputs", "\n", "\n", "", "else", ":", "\n", "# during inference, compute the end logits based on beam search", "\n", "            ", "bsz", ",", "slen", ",", "hsz", "=", "hidden_states", ".", "size", "(", ")", "\n", "start_log_probs", "=", "F", ".", "softmax", "(", "start_logits", ",", "dim", "=", "-", "1", ")", "# shape (bsz, slen)", "\n", "\n", "start_top_log_probs", ",", "start_top_index", "=", "torch", ".", "topk", "(", "\n", "start_log_probs", ",", "self", ".", "start_n_top", ",", "dim", "=", "-", "1", "\n", ")", "# shape (bsz, start_n_top)", "\n", "start_top_index_exp", "=", "start_top_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, start_n_top, hsz)", "\n", "start_states", "=", "torch", ".", "gather", "(", "hidden_states", ",", "-", "2", ",", "start_top_index_exp", ")", "# shape (bsz, start_n_top, hsz)", "\n", "start_states", "=", "start_states", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "slen", ",", "-", "1", ",", "-", "1", ")", "# shape (bsz, slen, start_n_top, hsz)", "\n", "\n", "hidden_states_expanded", "=", "hidden_states", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "\n", "start_states", "\n", ")", "# shape (bsz, slen, start_n_top, hsz)", "\n", "p_mask", "=", "p_mask", ".", "unsqueeze", "(", "-", "1", ")", "if", "p_mask", "is", "not", "None", "else", "None", "\n", "end_logits", "=", "self", ".", "end_logits", "(", "hidden_states_expanded", ",", "start_states", "=", "start_states", ",", "p_mask", "=", "p_mask", ")", "\n", "end_log_probs", "=", "F", ".", "softmax", "(", "end_logits", ",", "dim", "=", "1", ")", "# shape (bsz, slen, start_n_top)", "\n", "\n", "end_top_log_probs", ",", "end_top_index", "=", "torch", ".", "topk", "(", "\n", "end_log_probs", ",", "self", ".", "end_n_top", ",", "dim", "=", "1", "\n", ")", "# shape (bsz, end_n_top, start_n_top)", "\n", "end_top_log_probs", "=", "end_top_log_probs", ".", "view", "(", "-", "1", ",", "self", ".", "start_n_top", "*", "self", ".", "end_n_top", ")", "\n", "end_top_index", "=", "end_top_index", ".", "view", "(", "-", "1", ",", "self", ".", "start_n_top", "*", "self", ".", "end_n_top", ")", "\n", "\n", "start_states", "=", "torch", ".", "einsum", "(", "\n", "\"blh,bl->bh\"", ",", "hidden_states", ",", "start_log_probs", "\n", ")", "# get the representation of START as weighted sum of hidden states", "\n", "cls_logits", "=", "self", ".", "answer_class", "(", "\n", "hidden_states", ",", "start_states", "=", "start_states", ",", "cls_index", "=", "cls_index", "\n", ")", "# Shape (batch size,): one single `cls_logits` for each sample", "\n", "\n", "outputs", "=", "(", "start_top_log_probs", ",", "start_top_index", ",", "end_top_log_probs", ",", "end_top_index", ",", "cls_logits", ")", "+", "outputs", "\n", "\n", "# return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits", "\n", "# or (if labels are provided) (total_loss,)", "\n", "", "return", "outputs", "", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.build_tf_xlnet_to_pytorch_map": [[33, 114], ["hasattr", "tf_to_pt_map.update", "enumerate", "tf_to_pt_map.update", "hasattr", "tf_to_pt_map.update", "hasattr", "hasattr", "r_r_list.append", "r_w_list.append", "r_s_list.append", "seg_embed_list.append"], "function", ["None"], ["def", "build_tf_xlnet_to_pytorch_map", "(", "model", ",", "config", ",", "tf_weights", "=", "None", ")", ":", "\n", "    ", "\"\"\" A map of modules from TF to PyTorch.\n        I use a map to keep the PyTorch model as\n        identical to the original PyTorch model as possible.\n    \"\"\"", "\n", "\n", "tf_to_pt_map", "=", "{", "}", "\n", "\n", "if", "hasattr", "(", "model", ",", "\"transformer\"", ")", ":", "\n", "        ", "if", "hasattr", "(", "model", ",", "\"lm_loss\"", ")", ":", "\n", "# We will load also the output bias", "\n", "            ", "tf_to_pt_map", "[", "\"model/lm_loss/bias\"", "]", "=", "model", ".", "lm_loss", ".", "bias", "\n", "", "if", "hasattr", "(", "model", ",", "\"sequence_summary\"", ")", "and", "\"model/sequnece_summary/summary/kernel\"", "in", "tf_weights", ":", "\n", "# We will load also the sequence summary", "\n", "            ", "tf_to_pt_map", "[", "\"model/sequnece_summary/summary/kernel\"", "]", "=", "model", ".", "sequence_summary", ".", "summary", ".", "weight", "\n", "tf_to_pt_map", "[", "\"model/sequnece_summary/summary/bias\"", "]", "=", "model", ".", "sequence_summary", ".", "summary", ".", "bias", "\n", "", "if", "(", "\n", "hasattr", "(", "model", ",", "\"logits_proj\"", ")", "\n", "and", "config", ".", "finetuning_task", "is", "not", "None", "\n", "and", "\"model/regression_{}/logit/kernel\"", ".", "format", "(", "config", ".", "finetuning_task", ")", "in", "tf_weights", "\n", ")", ":", "\n", "            ", "tf_to_pt_map", "[", "\"model/regression_{}/logit/kernel\"", ".", "format", "(", "config", ".", "finetuning_task", ")", "]", "=", "model", ".", "logits_proj", ".", "weight", "\n", "tf_to_pt_map", "[", "\"model/regression_{}/logit/bias\"", ".", "format", "(", "config", ".", "finetuning_task", ")", "]", "=", "model", ".", "logits_proj", ".", "bias", "\n", "\n", "# Now load the rest of the transformer", "\n", "", "model", "=", "model", ".", "transformer", "\n", "\n", "# Embeddings and output", "\n", "", "tf_to_pt_map", ".", "update", "(", "\n", "{", "\n", "\"model/transformer/word_embedding/lookup_table\"", ":", "model", ".", "word_embedding", ".", "weight", ",", "\n", "\"model/transformer/mask_emb/mask_emb\"", ":", "model", ".", "mask_emb", ",", "\n", "}", "\n", ")", "\n", "\n", "# Transformer blocks", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "model", ".", "layer", ")", ":", "\n", "        ", "layer_str", "=", "\"model/transformer/layer_%d/\"", "%", "i", "\n", "tf_to_pt_map", ".", "update", "(", "\n", "{", "\n", "layer_str", "+", "\"rel_attn/LayerNorm/gamma\"", ":", "b", ".", "rel_attn", ".", "layer_norm", ".", "weight", ",", "\n", "layer_str", "+", "\"rel_attn/LayerNorm/beta\"", ":", "b", ".", "rel_attn", ".", "layer_norm", ".", "bias", ",", "\n", "layer_str", "+", "\"rel_attn/o/kernel\"", ":", "b", ".", "rel_attn", ".", "o", ",", "\n", "layer_str", "+", "\"rel_attn/q/kernel\"", ":", "b", ".", "rel_attn", ".", "q", ",", "\n", "layer_str", "+", "\"rel_attn/k/kernel\"", ":", "b", ".", "rel_attn", ".", "k", ",", "\n", "layer_str", "+", "\"rel_attn/r/kernel\"", ":", "b", ".", "rel_attn", ".", "r", ",", "\n", "layer_str", "+", "\"rel_attn/v/kernel\"", ":", "b", ".", "rel_attn", ".", "v", ",", "\n", "layer_str", "+", "\"ff/LayerNorm/gamma\"", ":", "b", ".", "ff", ".", "layer_norm", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/LayerNorm/beta\"", ":", "b", ".", "ff", ".", "layer_norm", ".", "bias", ",", "\n", "layer_str", "+", "\"ff/layer_1/kernel\"", ":", "b", ".", "ff", ".", "layer_1", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/layer_1/bias\"", ":", "b", ".", "ff", ".", "layer_1", ".", "bias", ",", "\n", "layer_str", "+", "\"ff/layer_2/kernel\"", ":", "b", ".", "ff", ".", "layer_2", ".", "weight", ",", "\n", "layer_str", "+", "\"ff/layer_2/bias\"", ":", "b", ".", "ff", ".", "layer_2", ".", "bias", ",", "\n", "}", "\n", ")", "\n", "\n", "# Relative positioning biases", "\n", "", "if", "config", ".", "untie_r", ":", "\n", "        ", "r_r_list", "=", "[", "]", "\n", "r_w_list", "=", "[", "]", "\n", "r_s_list", "=", "[", "]", "\n", "seg_embed_list", "=", "[", "]", "\n", "for", "b", "in", "model", ".", "layer", ":", "\n", "            ", "r_r_list", ".", "append", "(", "b", ".", "rel_attn", ".", "r_r_bias", ")", "\n", "r_w_list", ".", "append", "(", "b", ".", "rel_attn", ".", "r_w_bias", ")", "\n", "r_s_list", ".", "append", "(", "b", ".", "rel_attn", ".", "r_s_bias", ")", "\n", "seg_embed_list", ".", "append", "(", "b", ".", "rel_attn", ".", "seg_embed", ")", "\n", "", "", "else", ":", "\n", "        ", "r_r_list", "=", "[", "model", ".", "r_r_bias", "]", "\n", "r_w_list", "=", "[", "model", ".", "r_w_bias", "]", "\n", "r_s_list", "=", "[", "model", ".", "r_s_bias", "]", "\n", "seg_embed_list", "=", "[", "model", ".", "seg_embed", "]", "\n", "", "tf_to_pt_map", ".", "update", "(", "\n", "{", "\n", "\"model/transformer/r_r_bias\"", ":", "r_r_list", ",", "\n", "\"model/transformer/r_w_bias\"", ":", "r_w_list", ",", "\n", "\"model/transformer/r_s_bias\"", ":", "r_s_list", ",", "\n", "\"model/transformer/seg_embed\"", ":", "seg_embed_list", ",", "\n", "}", "\n", ")", "\n", "return", "tf_to_pt_map", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.load_tf_weights_in_xlnet": [[116, 176], ["tf.train.list_variables", "dummy_modeling_xlnet.build_tf_xlnet_to_pytorch_map", "build_tf_xlnet_to_pytorch_map.items", "logger.info", "logger.info", "tf.train.load_variable", "logger.info", "isinstance", "tf_weights.pop", "tf_weights.pop", "tf_weights.pop", "logger.error", "logger.info", "logger.info", "np.transpose", "enumerate", "logger.info", "torch.from_numpy", "len", "logger.info", "torch.from_numpy", "tf_weights.keys"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.modeling.dummy_modeling_xlnet.build_tf_xlnet_to_pytorch_map"], ["", "def", "load_tf_weights_in_xlnet", "(", "model", ",", "config", ",", "tf_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "logger", ".", "error", "(", "\n", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", "\n", ")", "\n", "raise", "\n", "# Load weights from TF model", "\n", "", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "tf_weights", "=", "{", "}", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "tf_weights", "[", "name", "]", "=", "array", "\n", "\n", "# Build TF to PyTorch weights loading map", "\n", "", "tf_to_pt_map", "=", "build_tf_xlnet_to_pytorch_map", "(", "model", ",", "config", ",", "tf_weights", ")", "\n", "\n", "for", "name", ",", "pointer", "in", "tf_to_pt_map", ".", "items", "(", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Importing {}\"", ".", "format", "(", "name", ")", ")", "\n", "if", "name", "not", "in", "tf_weights", ":", "\n", "            ", "logger", ".", "info", "(", "\"{} not in tf pre-trained weights, skipping\"", ".", "format", "(", "name", ")", ")", "\n", "continue", "\n", "", "array", "=", "tf_weights", "[", "name", "]", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "\"kernel\"", "in", "name", "and", "(", "\"ff\"", "in", "name", "or", "\"summary\"", "in", "name", "or", "\"logit\"", "in", "name", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Transposing\"", ")", "\n", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "if", "isinstance", "(", "pointer", ",", "list", ")", ":", "\n", "# Here we will split the TF weights", "\n", "            ", "assert", "len", "(", "pointer", ")", "==", "array", ".", "shape", "[", "0", "]", "\n", "for", "i", ",", "p_i", "in", "enumerate", "(", "pointer", ")", ":", "\n", "                ", "arr_i", "=", "array", "[", "i", ",", "...", "]", "\n", "try", ":", "\n", "                    ", "assert", "p_i", ".", "shape", "==", "arr_i", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "                    ", "e", ".", "args", "+=", "(", "p_i", ".", "shape", ",", "arr_i", ".", "shape", ")", "\n", "raise", "\n", "", "logger", ".", "info", "(", "\"Initialize PyTorch weight {} for layer {}\"", ".", "format", "(", "name", ",", "i", ")", ")", "\n", "p_i", ".", "data", "=", "torch", ".", "from_numpy", "(", "arr_i", ")", "\n", "", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "                ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "logger", ".", "info", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "tf_weights", ".", "pop", "(", "name", ",", "None", ")", "\n", "tf_weights", ".", "pop", "(", "name", "+", "\"/Adam\"", ",", "None", ")", "\n", "tf_weights", ".", "pop", "(", "name", "+", "\"/Adam_1\"", ",", "None", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Weights not copied to PyTorch model: {}\"", ".", "format", "(", "\", \"", ".", "join", "(", "tf_weights", ".", "keys", "(", ")", ")", ")", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.hg_api.interactive_eval.get_prediction": [[11, 30], ["tokenizer.encode_plus", "torch.Tensor().long().unsqueeze", "torch.Tensor().long().unsqueeze", "torch.Tensor().long().unsqueeze", "model", "torch.argmax", "predicted_probability.tolist.tolist", "torch.softmax", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.softmax"], ["def", "get_prediction", "(", "tokenizer", ",", "model", ",", "premise", ",", "hypothesis", ",", "max_length", "=", "256", ")", ":", "\n", "    ", "tokenized_input_seq_pair", "=", "tokenizer", ".", "encode_plus", "(", "premise", ",", "hypothesis", ",", "\n", "max_length", "=", "max_length", ",", "\n", "return_token_type_ids", "=", "True", ",", "truncation", "=", "True", ")", "\n", "\n", "input_ids", "=", "torch", ".", "Tensor", "(", "tokenized_input_seq_pair", "[", "'input_ids'", "]", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "token_type_ids", "=", "torch", ".", "Tensor", "(", "tokenized_input_seq_pair", "[", "'token_type_ids'", "]", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "attention_mask", "=", "torch", ".", "Tensor", "(", "tokenized_input_seq_pair", "[", "'attention_mask'", "]", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "outputs", "=", "model", "(", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "labels", "=", "None", ")", "\n", "\n", "predicted_probability", "=", "torch", ".", "softmax", "(", "outputs", "[", "0", "]", ",", "dim", "=", "1", ")", "[", "0", "]", "# batch_size only one", "\n", "predicted_index", "=", "torch", ".", "argmax", "(", "predicted_probability", ")", "\n", "predicted_probability", "=", "predicted_probability", ".", "tolist", "(", ")", "\n", "\n", "return", "predicted_probability", ",", "predicted_index", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.hg_api.interactive.evaluate": [[9, 42], ["tokenizer.encode_plus", "torch.Tensor().long().unsqueeze", "torch.Tensor().long().unsqueeze", "torch.Tensor().long().unsqueeze", "model", "[].tolist", "print", "print", "print", "print", "print", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.softmax", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.softmax"], ["def", "evaluate", "(", "tokenizer", ",", "model", ",", "premise", ",", "hypothesis", ")", ":", "\n", "    ", "max_length", "=", "256", "\n", "\n", "tokenized_input_seq_pair", "=", "tokenizer", ".", "encode_plus", "(", "premise", ",", "hypothesis", ",", "\n", "max_length", "=", "max_length", ",", "\n", "return_token_type_ids", "=", "True", ",", "truncation", "=", "True", ")", "\n", "\n", "input_ids", "=", "torch", ".", "Tensor", "(", "tokenized_input_seq_pair", "[", "'input_ids'", "]", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "# remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.", "\n", "token_type_ids", "=", "torch", ".", "Tensor", "(", "tokenized_input_seq_pair", "[", "'token_type_ids'", "]", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "attention_mask", "=", "torch", ".", "Tensor", "(", "tokenized_input_seq_pair", "[", "'attention_mask'", "]", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "outputs", "=", "model", "(", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "labels", "=", "None", ")", "\n", "# Note:", "\n", "# \"id2label\": {", "\n", "#     \"0\": \"entailment\",", "\n", "#     \"1\": \"neutral\",", "\n", "#     \"2\": \"contradiction\"", "\n", "# },", "\n", "\n", "predicted_probability", "=", "torch", ".", "softmax", "(", "outputs", "[", "0", "]", ",", "dim", "=", "1", ")", "[", "0", "]", ".", "tolist", "(", ")", "# batch_size only one", "\n", "\n", "#print(\"Premise:\", premise)", "\n", "#print(\"Hypothesis:\", hypothesis)", "\n", "print", "(", "\"Prediction:\"", ")", "\n", "print", "(", "\"Entailment:\"", ",", "predicted_probability", "[", "0", "]", ")", "\n", "print", "(", "\"Neutral:\"", ",", "predicted_probability", "[", "1", "]", ")", "\n", "print", "(", "\"Contradiction:\"", ",", "predicted_probability", "[", "2", "]", ")", "\n", "\n", "print", "(", "\"=\"", "*", "20", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.length_truncate": [[19, 35], ["torch_util.length_truncate._truncate"], "function", ["None"], ["def", "length_truncate", "(", "seq", ",", "max_l", ",", "is_elmo", "=", "False", ")", ":", "\n", "    ", "def", "_truncate", "(", "seq", ")", ":", "\n", "        ", "if", "seq", ".", "size", "(", "1", ")", ">", "max_l", ":", "\n", "            ", "return", "seq", "[", ":", ",", ":", "max_l", ",", "...", "]", "\n", "", "else", ":", "\n", "            ", "return", "seq", "\n", "\n", "", "", "if", "not", "is_elmo", ":", "\n", "        ", "return", "_truncate", "(", "seq", ")", "\n", "", "else", ":", "\n", "        ", "s1_elmo_embd", "=", "dict", "(", ")", "\n", "s1_elmo_embd", "[", "'mask'", "]", "=", "_truncate", "(", "seq", "[", "'mask'", "]", ")", "\n", "s1_elmo_embd", "[", "'elmo_representations'", "]", "=", "[", "]", "\n", "for", "e_rep", "in", "seq", "[", "'elmo_representations'", "]", ":", "\n", "            ", "s1_elmo_embd", "[", "'elmo_representations'", "]", ".", "append", "(", "_truncate", "(", "e_rep", ")", ")", "\n", "", "return", "s1_elmo_embd", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.pad_1d": [[37, 51], ["seq.size", "torch.autograd.Variable", "torch.cat", "torch.cat", "torch.cat", "seq.data.new().zero_", "seq.data.new", "seq.size"], "function", ["None"], ["", "", "def", "pad_1d", "(", "seq", ",", "pad_l", ")", ":", "\n", "    ", "\"\"\"\n    The seq is a sequence having shape [T, ..]. Note: The seq contains only one instance. This is not batched.\n\n    :param seq:  Input sequence with shape [T, ...]\n    :param pad_l: The required pad_length.\n    :return:  Output sequence will have shape [Pad_L, ...]\n    \"\"\"", "\n", "l", "=", "seq", ".", "size", "(", "0", ")", "\n", "if", "l", ">=", "pad_l", ":", "\n", "        ", "return", "seq", "[", ":", "pad_l", ",", "]", "# Truncate the length if the length is bigger than required padded_length.", "\n", "", "else", ":", "\n", "        ", "pad_seq", "=", "Variable", "(", "seq", ".", "data", ".", "new", "(", "pad_l", "-", "l", ",", "*", "seq", ".", "size", "(", ")", "[", "1", ":", "]", ")", ".", "zero_", "(", ")", ")", "# Requires_grad is False", "\n", "return", "torch", ".", "cat", "(", "[", "seq", ",", "pad_seq", "]", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.get_state_shape": [[53, 69], ["None"], "function", ["None"], ["", "", "def", "get_state_shape", "(", "rnn", ":", "nn", ".", "RNN", ",", "batch_size", ",", "bidirectional", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Return the state shape of a given RNN. This is helpful when you want to create a init state for RNN.\n\n    Example:\n    c0 = h0 = Variable(src_seq_p.data.new(*get_state_shape([your rnn], 3, bidirectional)).zero_())\n\n    :param rnn: nn.LSTM, nn.GRU or subclass of nn.RNN\n    :param batch_size:\n    :param bidirectional:\n    :return:\n    \"\"\"", "\n", "if", "bidirectional", ":", "\n", "        ", "return", "rnn", ".", "num_layers", "*", "2", ",", "batch_size", ",", "rnn", ".", "hidden_size", "\n", "", "else", ":", "\n", "        ", "return", "rnn", ".", "num_layers", ",", "batch_size", ",", "rnn", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.pack_list_sequence": [[71, 89], ["len", "range", "max", "batch_list.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "list", "torch_util.pad_1d"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.pad_1d"], ["", "", "def", "pack_list_sequence", "(", "inputs", ",", "l", ",", "max_l", "=", "None", ",", "batch_first", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Pack a batch of Tensor into one Tensor with max_length.\n    :param inputs:\n    :param l:\n    :param max_l: The max_length of the packed sequence.\n    :param batch_first:\n    :return:\n    \"\"\"", "\n", "batch_list", "=", "[", "]", "\n", "max_l", "=", "max", "(", "list", "(", "l", ")", ")", "if", "not", "max_l", "else", "max_l", "\n", "batch_size", "=", "len", "(", "inputs", ")", "\n", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "batch_list", ".", "append", "(", "pad_1d", "(", "inputs", "[", "b_i", "]", ",", "max_l", ")", ")", "\n", "", "pack_batch_list", "=", "torch", ".", "stack", "(", "batch_list", ",", "dim", "=", "1", ")", "if", "not", "batch_first", "else", "torch", ".", "stack", "(", "batch_list", ",", "dim", "=", "0", ")", "\n", "return", "pack_batch_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.pack_for_rnn_seq": [[91, 158], ["lengths.sort", "reversed", "numpy.zeros", "enumerate", "list", "torch.cat", "torch.cat", "torch.cat", "torch.utils.rnn.pack_padded_sequence", "lengths.sort", "reversed", "numpy.zeros", "tuple", "enumerate", "list", "torch.stack", "torch.stack", "torch.stack", "torch.utils.rnn.pack_padded_sequence", "tuple", "list", "lengths.size", "s_inputs_list.append", "lengths_list.append", "list", "lengths.size", "s_inputs_list.append", "lengths_list.append", "zip", "len", "inputs[].unsqueeze", "isinstance", "state_list.append", "torch.cat", "torch.cat", "torch.cat", "state[].unsqueeze"], "function", ["None"], ["", "def", "pack_for_rnn_seq", "(", "inputs", ",", "lengths", ",", "batch_first", "=", "True", ",", "states", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param states: [rnn.num_layers, batch_size, rnn.hidden_size]\n    :param inputs: Shape of the input should be [B, T, D] if batch_first else [T, B, D].\n    :param lengths:  [B]\n    :param batch_first:\n    :return:\n    \"\"\"", "\n", "if", "not", "batch_first", ":", "\n", "        ", "_", ",", "sorted_indices", "=", "lengths", ".", "sort", "(", ")", "\n", "'''\n            Reverse to decreasing order\n        '''", "\n", "r_index", "=", "reversed", "(", "list", "(", "sorted_indices", ")", ")", "\n", "\n", "s_inputs_list", "=", "[", "]", "\n", "lengths_list", "=", "[", "]", "\n", "reverse_indices", "=", "np", ".", "zeros", "(", "lengths", ".", "size", "(", "0", ")", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "r_index", ")", ":", "\n", "            ", "s_inputs_list", ".", "append", "(", "inputs", "[", ":", ",", "i", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "lengths_list", ".", "append", "(", "lengths", "[", "i", "]", ")", "\n", "reverse_indices", "[", "i", "]", "=", "j", "\n", "\n", "", "reverse_indices", "=", "list", "(", "reverse_indices", ")", "\n", "\n", "s_inputs", "=", "torch", ".", "cat", "(", "s_inputs_list", ",", "1", ")", "\n", "packed_seq", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "s_inputs", ",", "lengths_list", ")", "\n", "\n", "return", "packed_seq", ",", "reverse_indices", "\n", "\n", "", "else", ":", "\n", "        ", "_", ",", "sorted_indices", "=", "lengths", ".", "sort", "(", ")", "\n", "'''\n            Reverse to decreasing order\n        '''", "\n", "r_index", "=", "reversed", "(", "list", "(", "sorted_indices", ")", ")", "\n", "\n", "s_inputs_list", "=", "[", "]", "\n", "lengths_list", "=", "[", "]", "\n", "reverse_indices", "=", "np", ".", "zeros", "(", "lengths", ".", "size", "(", "0", ")", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "if", "states", "is", "None", ":", "\n", "            ", "states", "=", "(", ")", "\n", "", "elif", "not", "isinstance", "(", "states", ",", "tuple", ")", ":", "\n", "            ", "states", "=", "(", "states", ",", ")", "# rnn.num_layers, batch_size, rnn.hidden_size", "\n", "\n", "", "states_lists", "=", "tuple", "(", "[", "]", "for", "_", "in", "states", ")", "\n", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "r_index", ")", ":", "\n", "            ", "s_inputs_list", ".", "append", "(", "inputs", "[", "i", ",", ":", ",", ":", "]", ")", "\n", "lengths_list", ".", "append", "(", "lengths", "[", "i", "]", ")", "\n", "reverse_indices", "[", "i", "]", "=", "j", "\n", "\n", "for", "state_list", ",", "state", "in", "zip", "(", "states_lists", ",", "states", ")", ":", "\n", "                ", "state_list", ".", "append", "(", "state", "[", ":", ",", "i", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "", "", "reverse_indices", "=", "list", "(", "reverse_indices", ")", "\n", "\n", "s_inputs", "=", "torch", ".", "stack", "(", "s_inputs_list", ",", "dim", "=", "0", ")", "\n", "packed_seq", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "s_inputs", ",", "lengths_list", ",", "batch_first", "=", "batch_first", ")", "\n", "\n", "r_states", "=", "tuple", "(", "torch", ".", "cat", "(", "state_list", ",", "dim", "=", "1", ")", "for", "state_list", "in", "states_lists", ")", "\n", "if", "len", "(", "r_states", ")", "==", "1", ":", "\n", "            ", "r_states", "=", "r_states", "[", "0", "]", "\n", "\n", "", "return", "packed_seq", ",", "reverse_indices", ",", "r_states", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.unpack_from_rnn_seq": [[160, 172], ["torch.utils.rnn.pad_packed_sequence", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "s_inputs_list.append", "s_inputs_list.append", "unpacked_seq[].unsqueeze", "unpacked_seq[].unsqueeze"], "function", ["None"], ["", "", "def", "unpack_from_rnn_seq", "(", "packed_seq", ",", "reverse_indices", ",", "batch_first", "=", "True", ")", ":", "\n", "    ", "unpacked_seq", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_seq", ",", "batch_first", "=", "batch_first", ")", "\n", "s_inputs_list", "=", "[", "]", "\n", "\n", "if", "not", "batch_first", ":", "\n", "        ", "for", "i", "in", "reverse_indices", ":", "\n", "            ", "s_inputs_list", ".", "append", "(", "unpacked_seq", "[", ":", ",", "i", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "", "return", "torch", ".", "cat", "(", "s_inputs_list", ",", "1", ")", "\n", "", "else", ":", "\n", "        ", "for", "i", "in", "reverse_indices", ":", "\n", "            ", "s_inputs_list", ".", "append", "(", "unpacked_seq", "[", "i", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "return", "torch", ".", "cat", "(", "s_inputs_list", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.reverse_indice_for_state": [[174, 194], ["tuple", "tuple", "zip", "len", "isinstance", "state_list.append", "torch.cat", "torch.cat", "torch.cat", "state[].unsqueeze"], "function", ["None"], ["", "", "def", "reverse_indice_for_state", "(", "states", ",", "reverse_indices", ")", ":", "\n", "    ", "\"\"\"\n    :param states: [rnn.num_layers, batch_size, rnn.hidden_size]\n    :param reverse_indices: [batch_size]\n    :return:\n    \"\"\"", "\n", "if", "states", "is", "None", ":", "\n", "        ", "states", "=", "(", ")", "\n", "", "elif", "not", "isinstance", "(", "states", ",", "tuple", ")", ":", "\n", "        ", "states", "=", "(", "states", ",", ")", "# rnn.num_layers, batch_size, rnn.hidden_size", "\n", "\n", "", "states_lists", "=", "tuple", "(", "[", "]", "for", "_", "in", "states", ")", "\n", "for", "i", "in", "reverse_indices", ":", "\n", "        ", "for", "state_list", ",", "state", "in", "zip", "(", "states_lists", ",", "states", ")", ":", "\n", "            ", "state_list", ".", "append", "(", "state", "[", ":", ",", "i", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "", "", "r_states", "=", "tuple", "(", "torch", ".", "cat", "(", "state_list", ",", "dim", "=", "1", ")", "for", "state_list", "in", "states_lists", ")", "\n", "if", "len", "(", "r_states", ")", "==", "1", ":", "\n", "        ", "r_states", "=", "r_states", "[", "0", "]", "\n", "", "return", "r_states", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.auto_rnn": [[196, 220], ["torch_util.get_state_shape", "torch_util.pack_for_rnn_seq", "rnn", "torch_util.unpack_from_rnn_seq", "seqs.size", "seqs.size", "len", "torch.autograd.Variable", "torch_util.reverse_indice_for_state", "seqs.data.new().zero_", "seqs.data.new"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.get_state_shape", "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.pack_for_rnn_seq", "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.unpack_from_rnn_seq", "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.reverse_indice_for_state"], ["", "def", "auto_rnn", "(", "rnn", ":", "nn", ".", "RNN", ",", "seqs", ",", "lengths", ",", "batch_first", "=", "True", ",", "init_state", "=", "None", ",", "output_last_states", "=", "False", ")", ":", "\n", "    ", "batch_size", "=", "seqs", ".", "size", "(", "0", ")", "if", "batch_first", "else", "seqs", ".", "size", "(", "1", ")", "\n", "state_shape", "=", "get_state_shape", "(", "rnn", ",", "batch_size", ",", "rnn", ".", "bidirectional", ")", "\n", "\n", "# if init_state is None:", "\n", "#     h0 = c0 = Variable(seqs.data.new(*state_shape).zero_())", "\n", "# else:", "\n", "#     h0 = init_state[0] # rnn.num_layers, batch_size, rnn.hidden_size", "\n", "#     c0 = init_state[1]", "\n", "\n", "packed_pinputs", ",", "r_index", ",", "init_state", "=", "pack_for_rnn_seq", "(", "seqs", ",", "lengths", ",", "batch_first", ",", "init_state", ")", "\n", "\n", "if", "len", "(", "init_state", ")", "==", "0", ":", "\n", "        ", "h0", "=", "c0", "=", "Variable", "(", "seqs", ".", "data", ".", "new", "(", "*", "state_shape", ")", ".", "zero_", "(", ")", ")", "\n", "init_state", "=", "(", "h0", ",", "c0", ")", "\n", "\n", "", "output", ",", "last_state", "=", "rnn", "(", "packed_pinputs", ",", "init_state", ")", "\n", "output", "=", "unpack_from_rnn_seq", "(", "output", ",", "r_index", ",", "batch_first", ")", "\n", "\n", "if", "not", "output_last_states", ":", "\n", "        ", "return", "output", "\n", "", "else", ":", "\n", "        ", "last_state", "=", "reverse_indice_for_state", "(", "last_state", ",", "r_index", ")", "\n", "return", "output", ",", "last_state", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.pack_sequence_for_linear": [[222, 241], ["enumerate", "torch.cat", "torch.cat", "torch.cat", "NotImplemented", "batch_list.append"], "function", ["None"], ["", "", "def", "pack_sequence_for_linear", "(", "inputs", ",", "lengths", ",", "batch_first", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T, D] if batch_first\n    :param lengths:  [B]\n    :param batch_first:\n    :return:\n    \"\"\"", "\n", "batch_list", "=", "[", "]", "\n", "if", "batch_first", ":", "\n", "        ", "for", "i", ",", "l", "in", "enumerate", "(", "lengths", ")", ":", "\n", "# print(inputs[i, :l].size())", "\n", "            ", "batch_list", ".", "append", "(", "inputs", "[", "i", ",", ":", "l", "]", ")", "\n", "", "packed_sequence", "=", "torch", ".", "cat", "(", "batch_list", ",", "0", ")", "\n", "# if chuck:", "\n", "#     return list(torch.chunk(packed_sequence, chuck, dim=0))", "\n", "# else:", "\n", "return", "packed_sequence", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplemented", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.chucked_forward": [[243, 249], ["net", "torch.cat", "torch.cat", "torch.cat", "net", "torch.chunk", "torch.chunk", "torch.chunk"], "function", ["None"], ["", "", "def", "chucked_forward", "(", "inputs", ",", "net", ",", "chuck", "=", "None", ")", ":", "\n", "    ", "if", "not", "chuck", ":", "\n", "        ", "return", "net", "(", "inputs", ")", "\n", "", "else", ":", "\n", "        ", "output_list", "=", "[", "net", "(", "chuck", ")", "for", "chuck", "in", "torch", ".", "chunk", "(", "inputs", ",", "chuck", ",", "dim", "=", "0", ")", "]", "\n", "return", "torch", ".", "cat", "(", "output_list", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.unpack_sequence_for_linear": [[251, 268], ["max", "torch.cat", "torch.cat", "torch.cat", "isinstance", "torch.stack", "torch.stack", "torch.stack", "NotImplemented", "batch_list.append", "torch_util.pad_1d"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.pad_1d"], ["", "", "def", "unpack_sequence_for_linear", "(", "inputs", ",", "lengths", ",", "batch_first", "=", "True", ")", ":", "\n", "    ", "batch_list", "=", "[", "]", "\n", "max_l", "=", "max", "(", "lengths", ")", "\n", "\n", "if", "not", "isinstance", "(", "inputs", ",", "list", ")", ":", "\n", "        ", "inputs", "=", "[", "inputs", "]", "\n", "", "inputs", "=", "torch", ".", "cat", "(", "inputs", ")", "\n", "\n", "if", "batch_first", ":", "\n", "        ", "start", "=", "0", "\n", "for", "l", "in", "lengths", ":", "\n", "            ", "end", "=", "start", "+", "l", "\n", "batch_list", ".", "append", "(", "pad_1d", "(", "inputs", "[", "start", ":", "end", "]", ",", "max_l", ")", ")", "\n", "start", "=", "end", "\n", "", "return", "torch", ".", "stack", "(", "batch_list", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplemented", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.seq2seq_cross_entropy": [[270, 292], ["torch_util.pack_sequence_for_linear", "functools.partial", "sum", "zip", "logits.size", "pack_sequence_for_linear.size", "logits.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "functools.partial.", "functools.partial."], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.pack_sequence_for_linear"], ["", "", "def", "seq2seq_cross_entropy", "(", "logits", ",", "label", ",", "l", ",", "chuck", "=", "None", ",", "sos_truncate", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    :param logits: [exB, V] : exB = sum(l)\n    :param label: [B] : a batch of Label\n    :param l: [B] : a batch of LongTensor indicating the lengths of each inputs\n    :param chuck: Number of chuck to process\n    :return: A loss value\n    \"\"\"", "\n", "packed_label", "=", "pack_sequence_for_linear", "(", "label", ",", "l", ")", "\n", "cross_entropy_loss", "=", "functools", ".", "partial", "(", "F", ".", "cross_entropy", ",", "size_average", "=", "False", ")", "\n", "total", "=", "sum", "(", "l", ")", "\n", "\n", "assert", "total", "==", "logits", ".", "size", "(", "0", ")", "or", "packed_label", ".", "size", "(", "0", ")", "==", "logits", ".", "size", "(", "0", ")", ",", "\"logits length mismatch with label length.\"", "\n", "\n", "if", "chuck", ":", "\n", "        ", "logits_losses", "=", "0", "\n", "for", "x", ",", "y", "in", "zip", "(", "torch", ".", "chunk", "(", "logits", ",", "chuck", ",", "dim", "=", "0", ")", ",", "torch", ".", "chunk", "(", "packed_label", ",", "chuck", ",", "dim", "=", "0", ")", ")", ":", "\n", "            ", "logits_losses", "+=", "cross_entropy_loss", "(", "x", ",", "y", ")", "\n", "", "return", "logits_losses", "*", "(", "1", "/", "total", ")", "\n", "", "else", ":", "\n", "        ", "return", "cross_entropy_loss", "(", "logits", ",", "packed_label", ")", "*", "(", "1", "/", "total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.max_along_time": [[294, 321], ["list", "enumerate", "torch.stack", "torch.stack", "torch.stack", "enumerate", "torch.stack", "torch.stack", "torch.stack", "seq_i.max", "seq_i_max.squeeze.squeeze", "b_seq_max_list.append", "seq_i.max", "seq_i_max.squeeze.squeeze", "b_seq_max_list.append"], "function", ["None"], ["", "", "def", "max_along_time", "(", "inputs", ",", "lengths", ",", "list_in", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T, D]\n    :param lengths:  [B]\n    :return: [B * D] max_along_time\n    :param list_in:\n    \"\"\"", "\n", "ls", "=", "list", "(", "lengths", ")", "\n", "\n", "if", "not", "list_in", ":", "\n", "        ", "b_seq_max_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "ls", ")", ":", "\n", "            ", "seq_i", "=", "inputs", "[", "i", ",", ":", "l", ",", ":", "]", "\n", "seq_i_max", ",", "_", "=", "seq_i", ".", "max", "(", "dim", "=", "0", ")", "\n", "seq_i_max", "=", "seq_i_max", ".", "squeeze", "(", ")", "\n", "b_seq_max_list", ".", "append", "(", "seq_i_max", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "b_seq_max_list", ")", "\n", "", "else", ":", "\n", "        ", "b_seq_max_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "ls", ")", ":", "\n", "            ", "seq_i", "=", "inputs", "[", "i", "]", "\n", "seq_i_max", ",", "_", "=", "seq_i", ".", "max", "(", "dim", "=", "0", ")", "\n", "seq_i_max", "=", "seq_i_max", ".", "squeeze", "(", ")", "\n", "b_seq_max_list", ".", "append", "(", "seq_i_max", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "b_seq_max_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.avg_along_time": [[323, 350], ["list", "enumerate", "torch.stack", "torch.stack", "torch.stack", "enumerate", "torch.stack", "torch.stack", "torch.stack", "seq_i.mean", "seq_i_avg.squeeze.squeeze", "b_seq_avg_list.append", "seq_i.mean", "seq_i_avg.squeeze.squeeze", "b_seq_avg_list.append"], "function", ["None"], ["", "", "def", "avg_along_time", "(", "inputs", ",", "lengths", ",", "list_in", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T, D]\n    :param lengths:  [B]\n    :return: [B * D] max_along_time\n    :param list_in:\n    \"\"\"", "\n", "ls", "=", "list", "(", "lengths", ")", "\n", "\n", "if", "not", "list_in", ":", "\n", "        ", "b_seq_avg_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "ls", ")", ":", "\n", "            ", "seq_i", "=", "inputs", "[", "i", ",", ":", "l", ",", ":", "]", "\n", "seq_i_avg", "=", "seq_i", ".", "mean", "(", "dim", "=", "0", ")", "\n", "seq_i_avg", "=", "seq_i_avg", ".", "squeeze", "(", ")", "\n", "b_seq_avg_list", ".", "append", "(", "seq_i_avg", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "b_seq_avg_list", ")", "\n", "", "else", ":", "\n", "        ", "b_seq_avg_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "ls", ")", ":", "\n", "            ", "seq_i", "=", "inputs", "[", "i", "]", "\n", "seq_i_avg", ",", "_", "=", "seq_i", ".", "mean", "(", "dim", "=", "0", ")", "\n", "seq_i_avg", "=", "seq_i_avg", ".", "squeeze", "(", ")", "\n", "b_seq_avg_list", ".", "append", "(", "seq_i_avg", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "b_seq_avg_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.get_reverse_indices": [[368, 379], ["indices.data.new().fill_", "indices.size", "range", "int", "enumerate", "indices.data.new", "indices.size", "int"], "function", ["None"], ["", "", "def", "get_reverse_indices", "(", "indices", ",", "lengths", ")", ":", "\n", "    ", "r_indices", "=", "indices", ".", "data", ".", "new", "(", "indices", ".", "size", "(", ")", ")", ".", "fill_", "(", "0", ")", "\n", "batch_size", "=", "indices", ".", "size", "(", "0", ")", "\n", "for", "i", "in", "range", "(", "int", "(", "batch_size", ")", ")", ":", "\n", "        ", "b_ind", "=", "indices", "[", "i", "]", "\n", "b_l", "=", "lengths", "[", "i", "]", "\n", "for", "k", ",", "ind", "in", "enumerate", "(", "b_ind", ")", ":", "\n", "            ", "if", "k", ">=", "b_l", ":", "\n", "                ", "break", "\n", "", "r_indices", "[", "i", ",", "int", "(", "ind", ")", "]", "=", "k", "\n", "", "", "return", "r_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.index_ordering": [[381, 401], ["inputs.size", "range", "torch.stack", "torch.stack", "torch.stack", "int", "ordered_out_list.append", "b_out.size"], "function", ["None"], ["", "def", "index_ordering", "(", "inputs", ",", "lengths", ",", "indices", ",", "pad_value", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T, ~]\n    :param lengths: [B]\n    :param indices: [B, T]\n    :return:\n    \"\"\"", "\n", "batch_size", "=", "inputs", ".", "size", "(", "0", ")", "\n", "ordered_out_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "int", "(", "batch_size", ")", ")", ":", "\n", "        ", "b_input", "=", "inputs", "[", "i", "]", "\n", "b_l", "=", "lengths", "[", "i", "]", "\n", "b_ind", "=", "indices", "[", "i", "]", "\n", "b_out", "=", "b_input", "[", "b_ind", "]", "\n", "if", "b_out", ".", "size", "(", "0", ")", ">", "b_l", ":", "\n", "            ", "b_out", "[", "b_l", ":", "]", "=", "pad_value", "\n", "", "ordered_out_list", ".", "append", "(", "b_out", ")", "\n", "\n", "", "outs", "=", "torch", ".", "stack", "(", "ordered_out_list", ",", "dim", "=", "0", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.start_and_end_token_handling": [[403, 433], ["torch.cat.size", "torch.cat", "torch.cat", "torch.cat", "range", "torch.autograd.Variable", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat.data.new().zero_", "torch.autograd.Variable", "torch.cat.data.new", "torch.cat.data.new().zero_", "torch.cat.data.new"], "function", ["None"], ["", "def", "start_and_end_token_handling", "(", "inputs", ",", "lengths", ",", "sos_index", "=", "1", ",", "eos_index", "=", "2", ",", "pad_index", "=", "0", ",", "\n", "op", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T]\n    :param lengths: [B]\n    :param sos_index:\n    :param eos_index:\n    :param pad_index:\n    :return:\n    \"\"\"", "\n", "batch_size", "=", "inputs", ".", "size", "(", "0", ")", "\n", "\n", "if", "not", "op", ":", "\n", "        ", "return", "inputs", ",", "lengths", "\n", "", "elif", "op", "==", "'rm_start'", ":", "\n", "        ", "inputs", "=", "torch", ".", "cat", "(", "[", "inputs", "[", ":", ",", "1", ":", "]", ",", "Variable", "(", "inputs", ".", "data", ".", "new", "(", "batch_size", ",", "1", ")", ".", "zero_", "(", ")", ")", "]", ",", "dim", "=", "1", ")", "\n", "return", "inputs", ",", "lengths", "-", "1", "\n", "", "elif", "op", "==", "'rm_end'", ":", "\n", "        ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "pass", "\n", "# Potential problems!?", "\n", "# inputs[i, lengths[i] - 1] = pad_index", "\n", "", "return", "inputs", ",", "lengths", "-", "1", "\n", "", "elif", "op", "==", "'rm_both'", ":", "\n", "        ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "pass", "\n", "# Potential problems!?", "\n", "# inputs[i, lengths[i] - 1] = pad_index", "\n", "", "inputs", "=", "torch", ".", "cat", "(", "[", "inputs", "[", ":", ",", "1", ":", "]", ",", "Variable", "(", "inputs", ".", "data", ".", "new", "(", "batch_size", ",", "1", ")", ".", "zero_", "(", ")", ")", "]", ",", "dim", "=", "1", ")", "\n", "return", "inputs", ",", "lengths", "-", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.flint.torch_util.seq2seq_att": [[435, 499], ["state.size", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "att_net", "enumerate", "torch.stack", "torch.stack", "torch.stack", "batch_list_mems.append", "state[].expand", "batch_list_state.append", "torch.softmax().transpose", "torch.sum", "torch.sum", "torch.sum", "result_list.append", "b_mems.size", "torch.softmax", "b_score.transpose"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.nli.inference_debug.softmax"], ["", "", "def", "seq2seq_att", "(", "mems", ",", "lengths", ",", "state", ",", "att_net", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param mems: [B, T, D_mem] This are the memories.\n                    I call memory for this variable because I think attention is just like read something and then\n                    make alignments with your memories.\n                    This memory here is usually the input hidden state of the encoder.\n\n    :param lengths: [B]\n    :param state: [B, D_state]\n                    I call state for this variable because it's the state I percepts at this time step.\n\n    :param att_net: This is the attention network that will be used to calculate the alignment score between\n                    state and memories.\n                    input of the att_net is mems and state with shape:\n                        mems: [exB, D_mem]\n                        state: [exB, D_state]\n                    return of the att_net is [exB, 1]\n\n                    So any function that map a vector to a scalar could work.\n\n    :return: [B, D_result]\n    \"\"\"", "\n", "\n", "d_state", "=", "state", ".", "size", "(", "1", ")", "\n", "\n", "if", "not", "att_net", ":", "\n", "        ", "return", "state", "\n", "", "else", ":", "\n", "        ", "batch_list_mems", "=", "[", "]", "\n", "batch_list_state", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "lengths", ")", ":", "\n", "            ", "b_mems", "=", "mems", "[", "i", ",", ":", "l", "]", "# [T, D_mem]", "\n", "batch_list_mems", ".", "append", "(", "b_mems", ")", "\n", "\n", "b_state", "=", "state", "[", "i", "]", ".", "expand", "(", "b_mems", ".", "size", "(", "0", ")", ",", "d_state", ")", "# [T, D_state]", "\n", "batch_list_state", ".", "append", "(", "b_state", ")", "\n", "\n", "", "packed_sequence_mems", "=", "torch", ".", "cat", "(", "batch_list_mems", ",", "0", ")", "# [sum(l), D_mem]", "\n", "packed_sequence_state", "=", "torch", ".", "cat", "(", "batch_list_state", ",", "0", ")", "# [sum(l), D_state]", "\n", "\n", "align_score", "=", "att_net", "(", "packed_sequence_mems", ",", "packed_sequence_state", ")", "# [sum(l), 1]", "\n", "\n", "# The score grouped as [(a1, a2, a3), (a1, a2), (a1, a2, a3, a4)].", "\n", "# aligned_seq = packed_sequence_mems * align_score", "\n", "\n", "start", "=", "0", "\n", "result_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "lengths", ")", ":", "\n", "            ", "end", "=", "start", "+", "l", "\n", "\n", "b_mems", "=", "packed_sequence_mems", "[", "start", ":", "end", ",", ":", "]", "# [l, D_mems]", "\n", "b_score", "=", "align_score", "[", "start", ":", "end", ",", ":", "]", "# [l, 1]", "\n", "\n", "softed_b_score", "=", "F", ".", "softmax", "(", "b_score", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "# [l, 1]", "\n", "\n", "weighted_sum", "=", "torch", ".", "sum", "(", "b_mems", "*", "softed_b_score", ",", "dim", "=", "0", ",", "keepdim", "=", "False", ")", "# [D_mems]", "\n", "\n", "result_list", ".", "append", "(", "weighted_sum", ")", "\n", "\n", "start", "=", "end", "\n", "\n", "", "result", "=", "torch", ".", "stack", "(", "result_list", ",", "dim", "=", "0", ")", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.BaseBatchBuilder.__init__": [[13, 16], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "batching_schema", ":", "Dict", "[", "str", ",", "FlintField", "]", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "batching_schema", ":", "Dict", "[", "str", ",", "FlintField", "]", "=", "batching_schema", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.BaseBatchBuilder.__call__": [[17, 30], ["batch[].keys", "dict", "flint.data_utils.fields.RawFlintField.batching", "batchbuilder.BaseBatchBuilder.batching_schema[].batching"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.batching", "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.batching"], ["", "def", "__call__", "(", "self", ",", "batch", ")", ":", "\n", "        ", "field_names", "=", "batch", "[", "0", "]", ".", "keys", "(", ")", "\n", "batched_data", "=", "dict", "(", ")", "\n", "\n", "for", "field_name", "in", "field_names", ":", "\n", "            ", "if", "field_name", "not", "in", "self", ".", "batching_schema", ":", "\n", "# default is RawFlintField", "\n", "                ", "batched_data", "[", "field_name", "]", "=", "RawFlintField", ".", "batching", "(", "[", "item", "[", "field_name", "]", "for", "item", "in", "batch", "]", ")", "\n", "\n", "", "else", ":", "\n", "                ", "batched_data", "[", "field_name", "]", "=", "self", ".", "batching_schema", "[", "field_name", "]", ".", "batching", "(", "[", "item", "[", "field_name", "]", "for", "item", "in", "batch", "]", ")", "\n", "\n", "", "", "return", "batched_data", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.has_tensor": [[32, 45], ["isinstance", "isinstance", "any", "isinstance", "any", "batchbuilder.has_tensor", "obj.values", "batchbuilder.has_tensor"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.has_tensor", "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.has_tensor"], ["", "", "def", "has_tensor", "(", "obj", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    Given a possibly complex data structure,\n    check if it has any torch.Tensors in it.\n    \"\"\"", "\n", "if", "isinstance", "(", "obj", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "isinstance", "(", "obj", ",", "dict", ")", ":", "\n", "        ", "return", "any", "(", "has_tensor", "(", "value", ")", "for", "value", "in", "obj", ".", "values", "(", ")", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "return", "any", "(", "has_tensor", "(", "item", ")", "for", "item", "in", "obj", ")", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device": [[47, 68], ["isinstance", "batchbuilder.has_tensor", "obj.cuda", "isinstance", "isinstance", "batchbuilder.move_to_device", "obj.items", "batchbuilder.move_to_device", "isinstance", "hasattr", "obj.__class__", "isinstance", "tuple", "batchbuilder.move_to_device", "batchbuilder.move_to_device"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.has_tensor", "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device", "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device", "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device", "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.batchbuilder.move_to_device"], ["", "", "def", "move_to_device", "(", "obj", ",", "cuda_device", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Given a structure (possibly) containing Tensors on the CPU,\n    move all the Tensors to the specified GPU (or do nothing, if they should be on the CPU).\n    \"\"\"", "\n", "\n", "if", "cuda_device", "<", "0", "or", "not", "has_tensor", "(", "obj", ")", ":", "\n", "        ", "return", "obj", "\n", "", "elif", "isinstance", "(", "obj", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "obj", ".", "cuda", "(", "cuda_device", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "dict", ")", ":", "\n", "        ", "return", "{", "key", ":", "move_to_device", "(", "value", ",", "cuda_device", ")", "for", "key", ",", "value", "in", "obj", ".", "items", "(", ")", "}", "\n", "", "elif", "isinstance", "(", "obj", ",", "list", ")", ":", "\n", "        ", "return", "[", "move_to_device", "(", "item", ",", "cuda_device", ")", "for", "item", "in", "obj", "]", "\n", "", "elif", "isinstance", "(", "obj", ",", "tuple", ")", "and", "hasattr", "(", "obj", ",", "\"_fields\"", ")", ":", "\n", "# This is the best way to detect a NamedTuple, it turns out.", "\n", "        ", "return", "obj", ".", "__class__", "(", "*", "(", "move_to_device", "(", "item", ",", "cuda_device", ")", "for", "item", "in", "obj", ")", ")", "\n", "", "elif", "isinstance", "(", "obj", ",", "tuple", ")", ":", "\n", "        ", "return", "tuple", "(", "move_to_device", "(", "item", ",", "cuda_device", ")", "for", "item", "in", "obj", ")", "\n", "", "else", ":", "\n", "        ", "return", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.FlintField.batching": [[10, 13], ["NotImplemented"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "batching", "(", "cls", ",", "batched_data", ")", ":", "\n", "        ", "raise", "NotImplemented", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.RawFlintField.batching": [[16, 19], ["None"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "batching", "(", "cls", ",", "batched_data", ")", ":", "\n", "        ", "return", "batched_data", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.LabelFlintField.batching": [[22, 24], ["torch.tensor"], "methods", ["None"], ["    ", "def", "batching", "(", "self", ",", "batched_data", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "batched_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__": [[27, 33], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pad_idx", ",", "eos_idx", "=", "None", ",", "left_pad", "=", "False", ",", "move_eos_to_beginning", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "self", ".", "eos_idx", "=", "eos_idx", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "self", ".", "move_eos_to_beginning", "=", "move_eos_to_beginning", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.collate_tokens": [[34, 56], ["max", "values[].new().fill_", "enumerate", "torch.is_tensor", "fields.ArrayIndexFlintField.collate_tokens.copy_tensor"], "methods", ["None"], ["", "def", "collate_tokens", "(", "self", ",", "values", ",", "pad_idx", ",", "eos_idx", "=", "None", ",", "left_pad", "=", "False", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Convert a list of 1d tensors into a padded 2d tensor.\n        \"\"\"", "\n", "if", "not", "torch", ".", "is_tensor", "(", "values", "[", "0", "]", ")", ":", "\n", "            ", "values", "=", "[", "torch", ".", "tensor", "(", "v", ")", "for", "v", "in", "values", "]", "\n", "\n", "", "size", "=", "max", "(", "v", ".", "size", "(", "0", ")", "for", "v", "in", "values", ")", "\n", "res", "=", "values", "[", "0", "]", ".", "new", "(", "len", "(", "values", ")", ",", "size", ")", ".", "fill_", "(", "pad_idx", ")", "\n", "\n", "def", "copy_tensor", "(", "src", ",", "dst", ")", ":", "\n", "            ", "assert", "dst", ".", "numel", "(", ")", "==", "src", ".", "numel", "(", ")", "\n", "if", "move_eos_to_beginning", ":", "\n", "                ", "assert", "src", "[", "-", "1", "]", "==", "eos_idx", "\n", "dst", "[", "0", "]", "=", "eos_idx", "\n", "dst", "[", "1", ":", "]", "=", "src", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "dst", ".", "copy_", "(", "src", ")", "\n", "\n", "", "", "for", "i", ",", "v", "in", "enumerate", "(", "values", ")", ":", "\n", "            ", "copy_tensor", "(", "v", ",", "res", "[", "i", "]", "[", "size", "-", "len", "(", "v", ")", ":", "]", "if", "left_pad", "else", "res", "[", "i", "]", "[", ":", "len", "(", "v", ")", "]", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.batching": [[57, 63], ["fields.ArrayIndexFlintField.collate_tokens"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_anli.data_utils.fields.ArrayIndexFlintField.collate_tokens"], ["", "def", "batching", "(", "self", ",", "batched_data", ")", ":", "\n", "        ", "return", "self", ".", "collate_tokens", "(", "batched_data", ",", "\n", "self", ".", "pad_idx", ",", "\n", "self", ".", "eos_idx", ",", "\n", "self", ".", "left_pad", ",", "\n", "self", ".", "move_eos_to_beginning", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.sm_nli2std_format": [[41, 54], ["dict", "p_list.append"], "function", ["None"], ["def", "sm_nli2std_format", "(", "d_list", ",", "filter_invalid", "=", "True", ")", ":", "\n", "    ", "p_list", ":", "List", "[", "Dict", "]", "=", "[", "]", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "formatted_item", ":", "Dict", "=", "dict", "(", ")", "\n", "formatted_item", "[", "'uid'", "]", ":", "str", "=", "item", "[", "\"pairID\"", "]", "\n", "formatted_item", "[", "'premise'", "]", ":", "str", "=", "item", "[", "\"sentence1\"", "]", "\n", "formatted_item", "[", "'hypothesis'", "]", ":", "str", "=", "item", "[", "\"sentence2\"", "]", "\n", "formatted_item", "[", "'label'", "]", ":", "str", "=", "smnli_label2std_label", "[", "item", "[", "\"gold_label\"", "]", "]", "\n", "if", "filter_invalid", "and", "formatted_item", "[", "'label'", "]", "==", "'o'", ":", "\n", "            ", "continue", "# Skip example with invalid label.", "\n", "\n", "", "p_list", ".", "append", "(", "formatted_item", ")", "\n", "", "return", "p_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.fever_nli2std_format": [[56, 69], ["dict", "p_list.append"], "function", ["None"], ["", "def", "fever_nli2std_format", "(", "d_list", ",", "filter_invalid", "=", "True", ")", ":", "\n", "    ", "p_list", ":", "List", "[", "Dict", "]", "=", "[", "]", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "formatted_item", ":", "Dict", "=", "dict", "(", ")", "\n", "formatted_item", "[", "'uid'", "]", ":", "str", "=", "item", "[", "\"fid\"", "]", "\n", "formatted_item", "[", "'premise'", "]", ":", "str", "=", "item", "[", "\"context\"", "]", "\n", "formatted_item", "[", "'hypothesis'", "]", ":", "str", "=", "item", "[", "\"query\"", "]", "\n", "formatted_item", "[", "'label'", "]", ":", "str", "=", "fever_label2std_label", "[", "item", "[", "\"label\"", "]", "]", "\n", "if", "filter_invalid", "and", "formatted_item", "[", "'label'", "]", "==", "'o'", ":", "\n", "            ", "continue", "# Skip example with invalid label.", "\n", "\n", "", "p_list", ".", "append", "(", "formatted_item", ")", "\n", "", "return", "p_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.a_nli2std_format": [[71, 85], ["dict", "p_list.append"], "function", ["None"], ["", "def", "a_nli2std_format", "(", "d_list", ",", "filter_invalid", "=", "True", ")", ":", "\n", "    ", "p_list", ":", "List", "[", "Dict", "]", "=", "[", "]", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "formatted_item", ":", "Dict", "=", "dict", "(", ")", "\n", "formatted_item", "[", "'uid'", "]", ":", "str", "=", "item", "[", "\"uid\"", "]", "\n", "formatted_item", "[", "'premise'", "]", ":", "str", "=", "item", "[", "\"context\"", "]", "\n", "formatted_item", "[", "'hypothesis'", "]", ":", "str", "=", "item", "[", "\"hypothesis\"", "]", "\n", "formatted_item", "[", "'label'", "]", ":", "str", "=", "anli_label2std_label", "[", "item", "[", "\"label\"", "]", "]", "\n", "formatted_item", "[", "'reason'", "]", ":", "str", "=", "item", "[", "\"reason\"", "]", "\n", "if", "filter_invalid", "and", "formatted_item", "[", "'label'", "]", "==", "'o'", ":", "\n", "            ", "continue", "# Skip example with invalid label.", "\n", "\n", "", "p_list", ".", "append", "(", "formatted_item", ")", "\n", "", "return", "p_list", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.build_data.build_snli": [[15, 35], ["utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "dataset_tools.format_convert.sm_nli2std_format", "dataset_tools.format_convert.sm_nli2std_format", "dataset_tools.format_convert.sm_nli2std_format", "print", "print", "print", "print", "utils.common.save_jsonl", "utils.common.save_jsonl", "utils.common.save_jsonl", "snli_data_root_path.exists", "snli_data_root_path.mkdir", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.sm_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.sm_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.sm_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl"], ["def", "build_snli", "(", "path", ":", "Path", ")", ":", "\n", "    ", "snli_data_root_path", "=", "(", "path", "/", "\"snli\"", ")", "\n", "if", "not", "snli_data_root_path", ".", "exists", "(", ")", ":", "\n", "        ", "snli_data_root_path", ".", "mkdir", "(", ")", "\n", "", "o_train", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "\"data/snli_1.0/snli_1.0_train.jsonl\"", ")", "\n", "o_dev", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "\"data/snli_1.0/snli_1.0_dev.jsonl\"", ")", "\n", "o_test", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "\"data/snli_1.0/snli_1.0_test.jsonl\"", ")", "\n", "\n", "d_trian", "=", "sm_nli2std_format", "(", "o_train", ")", "\n", "d_dev", "=", "sm_nli2std_format", "(", "o_dev", ")", "\n", "d_test", "=", "sm_nli2std_format", "(", "o_test", ")", "\n", "\n", "print", "(", "\"SNLI examples without gold label have been filtered.\"", ")", "\n", "print", "(", "\"SNLI Train size:\"", ",", "len", "(", "d_trian", ")", ")", "\n", "print", "(", "\"SNLI Dev size:\"", ",", "len", "(", "d_dev", ")", ")", "\n", "print", "(", "\"SNLI Test size:\"", ",", "len", "(", "d_test", ")", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "d_trian", ",", "snli_data_root_path", "/", "'train.jsonl'", ")", "\n", "common", ".", "save_jsonl", "(", "d_dev", ",", "snli_data_root_path", "/", "'dev.jsonl'", ")", "\n", "common", ".", "save_jsonl", "(", "d_test", ",", "snli_data_root_path", "/", "'test.jsonl'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.build_data.build_mnli": [[37, 57], ["utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "dataset_tools.format_convert.sm_nli2std_format", "dataset_tools.format_convert.sm_nli2std_format", "dataset_tools.format_convert.sm_nli2std_format", "print", "print", "print", "print", "utils.common.save_jsonl", "utils.common.save_jsonl", "utils.common.save_jsonl", "data_root_path.exists", "data_root_path.mkdir", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.sm_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.sm_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.sm_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl"], ["", "def", "build_mnli", "(", "path", ":", "Path", ")", ":", "\n", "    ", "data_root_path", "=", "(", "path", "/", "\"mnli\"", ")", "\n", "if", "not", "data_root_path", ".", "exists", "(", ")", ":", "\n", "        ", "data_root_path", ".", "mkdir", "(", ")", "\n", "", "o_train", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "\"data/multinli_1.0/multinli_1.0_train.jsonl\"", ")", "\n", "o_mm_dev", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "\"data/multinli_1.0/multinli_1.0_dev_mismatched.jsonl\"", ")", "\n", "o_m_dev", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "\"data/multinli_1.0/multinli_1.0_dev_matched.jsonl\"", ")", "\n", "\n", "d_trian", "=", "sm_nli2std_format", "(", "o_train", ")", "\n", "d_mm_dev", "=", "sm_nli2std_format", "(", "o_mm_dev", ")", "\n", "d_m_test", "=", "sm_nli2std_format", "(", "o_m_dev", ")", "\n", "\n", "print", "(", "\"MNLI examples without gold label have been filtered.\"", ")", "\n", "print", "(", "\"MNLI Train size:\"", ",", "len", "(", "d_trian", ")", ")", "\n", "print", "(", "\"MNLI MisMatched Dev size:\"", ",", "len", "(", "d_mm_dev", ")", ")", "\n", "print", "(", "\"MNLI Matched dev size:\"", ",", "len", "(", "d_m_test", ")", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "d_trian", ",", "data_root_path", "/", "'train.jsonl'", ")", "\n", "common", ".", "save_jsonl", "(", "d_mm_dev", ",", "data_root_path", "/", "'mm_dev.jsonl'", ")", "\n", "common", ".", "save_jsonl", "(", "d_m_test", ",", "data_root_path", "/", "'m_dev.jsonl'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.build_data.build_fever_nli": [[59, 79], ["utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "dataset_tools.format_convert.fever_nli2std_format", "dataset_tools.format_convert.fever_nli2std_format", "dataset_tools.format_convert.fever_nli2std_format", "print", "print", "print", "utils.common.save_jsonl", "utils.common.save_jsonl", "utils.common.save_jsonl", "data_root_path.exists", "data_root_path.mkdir", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.fever_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.fever_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.fever_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl"], ["", "def", "build_fever_nli", "(", "path", ":", "Path", ")", ":", "\n", "    ", "data_root_path", "=", "(", "path", "/", "\"fever_nli\"", ")", "\n", "if", "not", "data_root_path", ".", "exists", "(", ")", ":", "\n", "        ", "data_root_path", ".", "mkdir", "(", ")", "\n", "\n", "", "o_train", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "\"data/nli_fever/train_fitems.jsonl\"", ")", "\n", "o_dev", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "\"data/nli_fever/dev_fitems.jsonl\"", ")", "\n", "o_test", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "\"data/nli_fever/test_fitems.jsonl\"", ")", "\n", "\n", "d_trian", "=", "fever_nli2std_format", "(", "o_train", ")", "\n", "d_dev", "=", "fever_nli2std_format", "(", "o_dev", ")", "\n", "d_test", "=", "fever_nli2std_format", "(", "o_test", ")", "\n", "\n", "print", "(", "\"FEVER-NLI Train size:\"", ",", "len", "(", "d_trian", ")", ")", "\n", "print", "(", "\"FEVER-NLI Dev size:\"", ",", "len", "(", "d_dev", ")", ")", "\n", "print", "(", "\"FEVER-NLI Test size:\"", ",", "len", "(", "d_test", ")", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "d_trian", ",", "data_root_path", "/", "'train.jsonl'", ")", "\n", "common", ".", "save_jsonl", "(", "d_dev", ",", "data_root_path", "/", "'dev.jsonl'", ")", "\n", "common", ".", "save_jsonl", "(", "d_test", ",", "data_root_path", "/", "'test.jsonl'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.build_data.build_anli": [[81, 106], ["str", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "dataset_tools.format_convert.a_nli2std_format", "dataset_tools.format_convert.a_nli2std_format", "dataset_tools.format_convert.a_nli2std_format", "print", "print", "print", "utils.common.save_jsonl", "utils.common.save_jsonl", "utils.common.save_jsonl", "data_root_path.exists", "data_root_path.mkdir", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.load_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.a_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.a_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.format_convert.a_nli2std_format", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl", "home.repos.pwc.inspect_result.facebookresearch_anli.utils.common.save_jsonl"], ["", "def", "build_anli", "(", "path", ":", "Path", ",", "round", "=", "1", ",", "version", "=", "'1.0'", ")", ":", "\n", "    ", "data_root_path", "=", "(", "path", "/", "\"anli\"", ")", "\n", "if", "not", "data_root_path", ".", "exists", "(", ")", ":", "\n", "        ", "data_root_path", ".", "mkdir", "(", ")", "\n", "\n", "", "round_tag", "=", "str", "(", "round", ")", "\n", "\n", "o_train", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "f\"data/anli_v{version}/R{round_tag}/train.jsonl\"", ")", "\n", "o_dev", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "f\"data/anli_v{version}/R{round_tag}/dev.jsonl\"", ")", "\n", "o_test", "=", "common", ".", "load_jsonl", "(", "config", ".", "PRO_ROOT", "/", "f\"data/anli_v{version}/R{round_tag}/test.jsonl\"", ")", "\n", "\n", "d_trian", "=", "a_nli2std_format", "(", "o_train", ")", "\n", "d_dev", "=", "a_nli2std_format", "(", "o_dev", ")", "\n", "d_test", "=", "a_nli2std_format", "(", "o_test", ")", "\n", "\n", "print", "(", "f\"ANLI (R{round_tag}) Train size:\"", ",", "len", "(", "d_trian", ")", ")", "\n", "print", "(", "f\"ANLI (R{round_tag}) Dev size:\"", ",", "len", "(", "d_dev", ")", ")", "\n", "print", "(", "f\"ANLI (R{round_tag}) Test size:\"", ",", "len", "(", "d_test", ")", ")", "\n", "\n", "if", "not", "(", "data_root_path", "/", "f\"r{round_tag}\"", ")", ".", "exists", "(", ")", ":", "\n", "        ", "(", "data_root_path", "/", "f\"r{round_tag}\"", ")", ".", "mkdir", "(", ")", "\n", "\n", "", "common", ".", "save_jsonl", "(", "d_trian", ",", "data_root_path", "/", "f\"r{round_tag}\"", "/", "'train.jsonl'", ")", "\n", "common", ".", "save_jsonl", "(", "d_dev", ",", "data_root_path", "/", "f\"r{round_tag}\"", "/", "'dev.jsonl'", ")", "\n", "common", ".", "save_jsonl", "(", "d_test", ",", "data_root_path", "/", "f\"r{round_tag}\"", "/", "'test.jsonl'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.build_data.build_data": [[108, 119], ["build_data.build_snli", "build_data.build_mnli", "build_data.build_fever_nli", "print", "processed_data_root.exists", "processed_data_root.mkdir", "build_data.build_anli"], "function", ["home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.build_data.build_snli", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.build_data.build_mnli", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.build_data.build_fever_nli", "home.repos.pwc.inspect_result.facebookresearch_anli.dataset_tools.build_data.build_anli"], ["", "def", "build_data", "(", ")", ":", "\n", "    ", "processed_data_root", "=", "config", ".", "PRO_ROOT", "/", "\"data\"", "/", "\"build\"", "\n", "if", "not", "processed_data_root", ".", "exists", "(", ")", ":", "\n", "        ", "processed_data_root", ".", "mkdir", "(", ")", "\n", "", "build_snli", "(", "processed_data_root", ")", "\n", "build_mnli", "(", "processed_data_root", ")", "\n", "build_fever_nli", "(", "processed_data_root", ")", "\n", "for", "round", "in", "[", "1", ",", "2", ",", "3", "]", ":", "\n", "        ", "build_anli", "(", "processed_data_root", ",", "round", ")", "\n", "\n", "", "print", "(", "\"NLI data built!\"", ")", "\n", "\n"]]}