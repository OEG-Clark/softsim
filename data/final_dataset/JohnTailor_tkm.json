{"home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.__init__": [[17, 40], ["algTools.to_np", "len", "TKMCore.TKMCore._initialise"], "methods", ["home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.to_np", "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore._initialise"], ["    ", "def", "__init__", "(", "self", ",", "m_docs", ",", "n_words", ",", "n_topics", ",", "winwid", ",", "alpha", ",", "beta", ")", ":", "\n", "        ", "\"\"\"\n        :param m_docs: Document corpus transformed so that integer IDs\n                       represent each word in the vocabulary.\n                       Type: List[List[int]]\n        :param n_words: Number of unique words in the corpus' vocabulary\n        :param n_topics: Maximum number of topics to be assigned to the corpus\n        :param winwid: ???\n        :param alpha: Topic concentration prior: the larger \u03b1, the more\n                      concentrated the topics - i.e. fewer topics per document\n        :param beta: Prior with states that each word is assumed to occur at\n                     least \u03b2 times in each topic\n        \"\"\"", "\n", "self", ".", "m_docs", "=", "m_docs", "\n", "self", ".", "np_docs", "=", "algTools", ".", "to_np", "(", "m_docs", ")", "\n", "self", ".", "n_docs", "=", "len", "(", "m_docs", ")", "\n", "self", ".", "n_topics", "=", "n_topics", "\n", "self", ".", "n_words", "=", "n_words", "\n", "self", ".", "beta", "=", "beta", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "winwid", "=", "winwid", "\n", "\n", "self", ".", "_initialise", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore._initialise": [[41, 52], ["numpy.ones", "numpy.full", "numpy.sum", "range", "numpy.random.random"], "methods", ["None"], ["", "def", "_initialise", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initialise the keyword score and document topic probability matrices\n        \"\"\"", "\n", "self", ".", "f_w_t", "=", "np", ".", "ones", "(", "(", "self", ".", "n_words", ",", "self", ".", "n_topics", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "p_d_t", "=", "np", ".", "full", "(", "(", "self", ".", "n_docs", ",", "self", ".", "n_topics", ")", ",", "1.0", "/", "self", ".", "n_topics", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "f_w_t", "+=", "np", ".", "random", ".", "random", "(", "(", "self", ".", "n_words", ",", "self", ".", "n_topics", ")", ")", "/", "75", "# ~1% randomness", "\n", "normw", "=", "np", ".", "sum", "(", "self", ".", "f_w_t", ",", "axis", "=", "0", ")", "\n", "for", "t", "in", "range", "(", "self", ".", "n_topics", ")", ":", "\n", "            ", "self", ".", "f_w_t", "[", ":", ",", "t", "]", "=", "self", ".", "f_w_t", "[", ":", ",", "t", "]", "/", "normw", "[", "t", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.compute_keyword_score": [[53, 82], ["numpy.sum", "numpy.apply_along_axis", "numpy.log2", "range", "numpy.clip", "numpy.sum", "concentration.reshape", "numpy.log2", "concentration.reshape"], "methods", ["None"], ["", "", "def", "compute_keyword_score", "(", "self", ",", "nwt", ",", "human", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param nwt: Vector representing a word's assignment count to the\n                    different topics\n        :param human: Set to True for a more human readable interpretation by\n                      setting a stronger focus on frequency\n        :return: The updated keyword score matrix\n        \"\"\"", "\n", "nw", "=", "np", ".", "sum", "(", "nwt", ",", "axis", "=", "1", ")", "# Number of times word occurs overall", "\n", "# Entropy of topic-word distribution.", "\n", "# Is a small constant if word has 0 occurrences in all topics.", "\n", "entropy_p_t_w", "=", "np", ".", "apply_along_axis", "(", "entropy", ",", "1", ",", "nwt", "+", "1e-30", ")", "\n", "# For concentration normalization:", "\n", "#   min (log #topics, log #times word occurs)", "\n", "max_entropy", "=", "np", ".", "log2", "(", "np", ".", "clip", "(", "nw", ",", "2", ",", "self", ".", "n_topics", ")", ")", "\n", "concentration", "=", "(", "max_entropy", "/", "(", "1", "+", "entropy_p_t_w", ")", ")", "**", "1.5", "\n", "\n", "if", "human", ":", "# Human readable: stronger focus on frequency", "\n", "            ", "new_f_w_t", "=", "nwt", "*", "(", "concentration", ".", "reshape", "(", "concentration", ".", "shape", "[", "0", "]", ",", "1", ")", ")", "\n", "", "else", ":", "# For inference/extracting keywords", "\n", "            ", "new_f_w_t", "=", "np", ".", "log2", "(", "1", "+", "nwt", "+", "self", ".", "beta", ")", "*", "(", "concentration", ".", "reshape", "(", "concentration", ".", "shape", "[", "0", "]", ",", "1", ")", ")", "\n", "\n", "# Sum across words for a topic # sum_z p(w|d,z)*p(z|d)", "\n", "", "norm_const", "=", "1e-30", "+", "np", ".", "sum", "(", "new_f_w_t", ",", "axis", "=", "0", ")", "\n", "for", "t", "in", "range", "(", "norm_const", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "new_f_w_t", "[", ":", ",", "t", "]", "/=", "norm_const", "[", "t", "]", "\n", "\n", "", "return", "new_f_w_t", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.one_step": [[83, 94], ["numpy.zeros", "numpy.zeros", "_topicAssign._getTopicAssignments", "TKMCore.TKMCore.compute_keyword_score"], "methods", ["home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.compute_keyword_score"], ["", "def", "one_step", "(", "self", ",", "pdt", ")", ":", "\n", "        ", "\"\"\"Performs one E-step and one M-step\"\"\"", "\n", "# Calculate new probability distribution", "\n", "new_p_d_t", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_docs", ",", "self", ".", "n_topics", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "nwt", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_words", ",", "self", ".", "n_topics", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "_topicAssign", ".", "_getTopicAssignments", "(", "\n", "self", ".", "np_docs", ",", "self", ".", "n_docs", ",", "self", ".", "n_topics", ",", "self", ".", "winwid", ",", "self", ".", "f_w_t", ",", "\n", "nwt", ",", "pdt", ",", "new_p_d_t", ",", "self", ".", "alpha", ")", "\n", "new_f_w_t", "=", "self", ".", "compute_keyword_score", "(", "nwt", ")", "\n", "\n", "return", "new_f_w_t", ",", "new_p_d_t", ",", "nwt", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.run": [[95, 130], ["int", "numpy.random.seed", "random.seed", "range", "algTools.get_unique_topics", "len", "numpy.transpose().copy", "TKMCore.TKMCore.get_topics", "numpy.random.random", "TKMCore.TKMCore.one_step", "time.time", "algTools.compute_perplexity", "numpy.transpose", "print", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.get_unique_topics", "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.get_topics", "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.one_step", "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.compute_perplexity"], ["", "def", "run", "(", "self", ",", "convergence_constant", "=", "0.05", ",", "max_iter", "=", "500", ",", "kl_threshold", "=", "0.25", ",", "\n", "mseed", "=", "int", "(", "time", ".", "time", "(", ")", "%", "10000", ")", ")", ":", "\n", "        ", "\"\"\"\n        Run the TKM algorithm until convergence, and update the number of\n        topics, topic keyword score matrix and the document-topic probability\n        distribution.\n\n        :param convergence_constant: Convergence constant\n        :param max_iter: Maximum number of iterations\n        :param kl_threshold: Maximum Kullback-Leibler divergence\n        :param mseed: Random seed\n        :return: None\n        \"\"\"", "\n", "np", ".", "random", ".", "seed", "(", "mseed", ")", "\n", "random", ".", "seed", "(", "np", ".", "random", ".", "random", "(", ")", ")", "\n", "\n", "# Iterate until convergence", "\n", "prev_perplexity", "=", "1e20", "\n", "for", "i", "in", "range", "(", "1", ",", "max_iter", "+", "1", ")", ":", "\n", "            ", "self", ".", "f_w_t", ",", "self", ".", "p_d_t", ",", "_", "=", "self", ".", "one_step", "(", "self", ".", "p_d_t", ")", "\n", "# Only check for convergence every few iterations for increased", "\n", "# stability", "\n", "if", "i", "%", "7", "==", "0", ":", "\n", "                ", "perplexity", "=", "algTools", ".", "compute_perplexity", "(", "self", ".", "f_w_t", ",", "\n", "self", ".", "p_d_t", ",", "\n", "self", ".", "m_docs", ")", "\n", "if", "(", "prev_perplexity", "-", "perplexity", ")", "/", "prev_perplexity", "<", "convergence_constant", ":", "\n", "                    ", "print", "(", "\"Number of iters: %s\"", "%", "i", ")", "\n", "break", "\n", "", "prev_perplexity", "=", "perplexity", "\n", "\n", "", "", "unique_topics", "=", "algTools", ".", "get_unique_topics", "(", "self", ".", "f_w_t", ",", "kl_threshold", ")", "\n", "self", ".", "n_topics", "=", "len", "(", "unique_topics", ")", "\n", "self", ".", "f_w_t", "=", "np", ".", "transpose", "(", "np", ".", "array", "(", "unique_topics", ")", ")", ".", "copy", "(", "order", "=", "'C'", ")", "\n", "self", ".", "p_d_t", ",", "self", ".", "nwt_latest_iteration", "=", "self", ".", "get_topics", "(", "self", ".", "m_docs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.get_topics": [[131, 147], ["algTools.to_np", "len", "numpy.full", "TKMCore.TKMCore.one_step"], "methods", ["home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.to_np", "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.one_step"], ["", "def", "get_topics", "(", "self", ",", "m_docs", ")", ":", "\n", "        ", "\"\"\"\n        Get the topics of new, unseen documents without adjusting the current\n        distribution\n        :param m_docs: Unseen documents transformed so that words are replaced\n                       by integer representations using the same dictionary\n                       (ID-to-word mapping) as used when training\n        :return: Updated matrices for document topic probabilities and number\n                 of assignments to each topic for each word\n        \"\"\"", "\n", "self", ".", "np_docs", "=", "algTools", ".", "to_np", "(", "m_docs", ")", "\n", "self", ".", "n_docs", "=", "len", "(", "m_docs", ")", "\n", "p_d_t", "=", "np", ".", "full", "(", "(", "self", ".", "n_docs", ",", "self", ".", "n_topics", ")", ",", "1.0", "/", "self", ".", "n_topics", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "_", ",", "p_d_t", ",", "nwt_updated", "=", "self", ".", "one_step", "(", "p_d_t", ")", "\n", "return", "p_d_t", ",", "nwt_updated", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.get_p_d_t": [[149, 150], ["None"], "methods", ["None"], ["", "def", "get_p_d_t", "(", "self", ")", ":", "return", "self", ".", "p_d_t", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.get_f_w_t": [[151, 152], ["None"], "methods", ["None"], ["def", "get_f_w_t", "(", "self", ")", ":", "return", "self", ".", "f_w_t", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.get_f_w_t_hu": [[153, 155], ["TKMCore.TKMCore.compute_keyword_score"], "methods", ["home.repos.pwc.inspect_result.JohnTailor_tkm.None.TKMCore.TKMCore.compute_keyword_score"], ["def", "get_f_w_t_hu", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "compute_keyword_score", "(", "self", ".", "nwt_latest_iteration", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.get_unique_topics": [[45, 70], ["print", "print", "range", "numpy.apply_along_axis", "numpy.transpose", "numpy.min", "unique_topics.append", "scipy.stats.entropy", "scipy.stats.entropy", "numpy.array", "numpy.transpose", "numpy.transpose"], "function", ["None"], ["def", "get_unique_topics", "(", "p_w_t", ",", "kl_threshold", ")", ":", "\n", "    ", "\"\"\"\n    Get the remaining unique topics, after filtering out those that are not\n    significantly different, as set by the Kulback-Leibler distance threshold\n\n    :param p_w_t: Word-topic probabilities matrix\n    :param kl_threshold: Kulback-Leibler distance threshold for significantly\n                         differing topics\n    :returns: Filtered list of topics\n    \"\"\"", "\n", "print", "(", "\"p_w_t:\"", ")", "\n", "print", "(", "p_w_t", "[", ":", "5", "]", ")", "\n", "unique_topics", "=", "[", "p_w_t", "[", ":", ",", "0", "]", "]", "\n", "n_topics", "=", "p_w_t", ".", "shape", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "n_topics", ")", ":", "\n", "        ", "def", "kl_distance", "(", "topics_arr", ")", ":", "\n", "            ", "return", "(", "scipy", ".", "stats", ".", "entropy", "(", "topics_arr", "+", "1e-8", ",", "\n", "np", ".", "transpose", "(", "p_w_t", "[", ":", ",", "i", "]", "+", "1e-8", ")", ")", "\n", "+", "scipy", ".", "stats", ".", "entropy", "(", "np", ".", "transpose", "(", "p_w_t", "[", ":", ",", "i", "]", "+", "1e-8", ")", ",", "\n", "topics_arr", "+", "1e-8", ")", ")", "\n", "", "topic_entropy", "=", "np", ".", "apply_along_axis", "(", "kl_distance", ",", "0", ",", "\n", "np", ".", "transpose", "(", "np", ".", "array", "(", "unique_topics", ")", ")", ")", "\n", "if", "np", ".", "min", "(", "topic_entropy", ")", ">=", "kl_threshold", ":", "\n", "            ", "unique_topics", ".", "append", "(", "p_w_t", "[", ":", ",", "i", "]", ")", "\n", "", "", "return", "unique_topics", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.compute_perplexity": [[72, 83], ["enumerate", "sum", "numpy.log", "enumerate", "numpy.exp", "numpy.fabs", "numpy.log", "len", "numpy.dot"], "function", ["None"], ["", "def", "compute_perplexity", "(", "p_w_t", ",", "p_d_t", ",", "m_docs", ")", ":", "\n", "    ", "perplexity", "=", "0", "\n", "for", "_id", ",", "doc", "in", "enumerate", "(", "m_docs", ")", ":", "\n", "        ", "for", "i", ",", "word_id", "in", "enumerate", "(", "doc", ")", ":", "\n", "            ", "pwidt", "=", "p_w_t", "[", "word_id", ",", ":", "]", "\n", "p", "=", "np", ".", "fabs", "(", "np", ".", "dot", "(", "pwidt", ",", "p_d_t", "[", "_id", ",", ":", "]", ")", ")", "\n", "perplexity", "+=", "np", ".", "log", "(", "p", ")", "\n", "", "", "normalisation_const", "=", "sum", "(", "[", "len", "(", "d", ")", "for", "d", "in", "m_docs", "]", ")", "\n", "perplexity", "/=", "normalisation_const", "\n", "perplexity", "=", "np", ".", "log", "(", "np", ".", "exp", "(", "-", "perplexity", ")", ")", "# correct: np.exp(-perplexity/lenNew)", "\n", "return", "perplexity", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.to_np": [[85, 97], ["sum", "numpy.zeros", "numpy.frombuffer", "len", "len", "len"], "function", ["None"], ["", "def", "to_np", "(", "modocs", ")", ":", "\n", "    ", "mdocs", "=", "[", "np", ".", "frombuffer", "(", "d", ",", "dtype", "=", "np", ".", "int32", ")", "for", "d", "in", "modocs", "]", "\n", "toLen", "=", "sum", "(", "[", "len", "(", "d", ")", "for", "d", "in", "mdocs", "]", ")", "\n", "npdocs", "=", "np", ".", "zeros", "(", "toLen", "+", "len", "(", "mdocs", ")", ",", "dtype", "=", "np", ".", "intc", ")", "\n", "i", "=", "0", "\n", "for", "d", "in", "mdocs", ":", "\n", "        ", "npdocs", "[", "i", "]", "=", "len", "(", "d", ")", "\n", "i", "+=", "1", "\n", "for", "w", "in", "d", ":", "\n", "            ", "npdocs", "[", "i", "]", "=", "w", "\n", "i", "+=", "1", "\n", "", "", "return", "npdocs", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.words_to_index": [[99, 120], ["collections.defaultdict", "enumerate", "sorted", "enumerate", "enumerate", "array.array", "enumerate", "collections.defaultdict.items", "enumerate", "operator.itemgetter", "len"], "function", ["None"], ["", "def", "words_to_index", "(", "docs", ")", ":", "#use 1 for true", "\n", "    ", "mdocs", "=", "[", "array", "(", "'I'", ",", "[", "0", "]", "*", "len", "(", "doc", ")", ")", "for", "doc", "in", "docs", "]", "\n", "word_counts", "=", "defaultdict", "(", "int", ")", "\n", "for", "doc_id", ",", "doc", "in", "enumerate", "(", "docs", ")", ":", "\n", "        ", "for", "word_i", ",", "word", "in", "enumerate", "(", "doc", ")", ":", "\n", "            ", "word_counts", "[", "word", "]", "+=", "1", "\n", "\n", "# Sort words by frequency, since this is accessed more. This setup reduces", "\n", "# cache misses going forwards.", "\n", "", "", "sorted_counts", "=", "sorted", "(", "word_counts", ".", "items", "(", ")", ",", "key", "=", "operator", ".", "itemgetter", "(", "1", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "word2id", "=", "{", "}", "\n", "id2word", "=", "{", "}", "\n", "for", "i", ",", "(", "word", ",", "_", ")", "in", "enumerate", "(", "sorted_counts", ")", ":", "\n", "        ", "word2id", "[", "word", "]", "=", "i", "\n", "id2word", "[", "i", "]", "=", "word", "\n", "", "for", "doc_id", ",", "doc", "in", "enumerate", "(", "docs", ")", ":", "\n", "        ", "for", "word_i", ",", "word", "in", "enumerate", "(", "doc", ")", ":", "\n", "            ", "mdocs", "[", "doc_id", "]", "[", "word_i", "]", "=", "word2id", "[", "word", "]", "\n", "\n", "", "", "return", "mdocs", ",", "id2word", ",", "word2id", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.process_corpus": [[122, 161], ["algTools.words_to_index", "collections.defaultdict", "enumerate", "algTools.words_to_index", "algTools.tokenize_and_update_mapping", "set", "set", "len", "range", "len", "len", "collections.defaultdict.items"], "function", ["home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.words_to_index", "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.words_to_index", "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.tokenize_and_update_mapping"], ["", "def", "process_corpus", "(", "corpus", ",", "min_doc_length", "=", "10", ")", ":", "\n", "    ", "\"\"\"\n    Clean corpus, and then transform words into integer representations.\n    Cleaning entails removing stop words, normalising and stemming words, and\n    filtering out documents shorter than specified length, as well as filtering\n    out unique words.\n\n    :param corpus: List strings, where each string is document of words\n    :param min_doc_length: Minimum number of tokens a document needs to contain\n\n    :return: Tuple of the transformed corpus and an id2word dictionary\n    \"\"\"", "\n", "word_map", "=", "{", "}", "\n", "tokenized_corpus", "=", "[", "tokenize_and_update_mapping", "(", "doc", ",", "word_map", ")", "\n", "for", "doc", "in", "corpus", "]", "\n", "transformed_corpus", ",", "id2word", ",", "word2id", "=", "words_to_index", "(", "tokenized_corpus", ")", "\n", "\n", "# Remove short docs and words that only occur once, and update word counts", "\n", "word_counts", "=", "defaultdict", "(", "int", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "transformed_corpus", ")", ":", "\n", "# Filter out short docs, they only cause trouble later on", "\n", "        ", "if", "len", "(", "doc", ")", "<", "min_doc_length", ":", "\n", "            ", "continue", "\n", "", "for", "word_id", "in", "doc", ":", "\n", "            ", "if", "len", "(", "id2word", "[", "word_id", "]", ")", ">", "1", ":", "# Filter out words of length 1", "\n", "                ", "word_counts", "[", "word_id", "]", "+=", "1", "\n", "\n", "# Gather list of IDs for words that occur more than once", "\n", "", "", "", "non_unique_word_ids", "=", "set", "(", "range", "(", "len", "(", "id2word", ")", ")", ")", "-", "set", "(", "[", "w", "for", "w", ",", "c", "in", "word_counts", ".", "items", "(", ")", "if", "c", "<", "2", "]", ")", "\n", "\n", "# Filter out unique words", "\n", "filtered_docs", "=", "[", "[", "id2word", "[", "word_id", "]", "for", "word_id", "in", "doc", "if", "word_id", "in", "non_unique_word_ids", "]", "\n", "for", "doc", "in", "transformed_corpus", "]", "\n", "\n", "# Return the final corpus with words transformed into their integer", "\n", "# representations, as well as the id2word dictionary", "\n", "transformed_corpus", ",", "id2word", ",", "word2id", "=", "words_to_index", "(", "filtered_docs", ")", "\n", "return", "transformed_corpus", ",", "id2word", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.tokenize": [[163, 176], ["pattern.sub().replace().split", "t.lower", "stemmer.stem", "pattern.sub().replace", "t.isalpha", "pattern.sub", "len", "re.escape", "m.group"], "function", ["None"], ["", "def", "tokenize", "(", "doc", ")", ":", "\n", "    ", "tokens", "=", "pattern", ".", "sub", "(", "lambda", "m", ":", "replacements", "[", "re", ".", "escape", "(", "m", ".", "group", "(", "0", ")", ")", "]", ",", "doc", ")", ".", "replace", "(", "\"  \"", ",", "\" \"", ")", ".", "split", "(", "\" \"", ")", "\n", "# lower case, remove non-letters and words of length 1", "\n", "matches", "=", "[", "t", ".", "lower", "(", ")", "for", "t", "in", "tokens", "if", "len", "(", "t", ")", ">", "1", "and", "t", ".", "isalpha", "(", ")", "]", "\n", "# remove stop words", "\n", "matches", "=", "[", "t", "for", "t", "in", "matches", "if", "t", "not", "in", "stopwords_set", "]", "\n", "# stem words", "\n", "matches", "=", "[", "stemmer", ".", "stem", "(", "t", ")", "for", "t", "in", "matches", "]", "\n", "# remove stop words again, due to stemming this seems to happen...", "\n", "matches", "=", "[", "t", "for", "t", "in", "matches", "if", "t", "not", "in", "stopwords_set", "]", "\n", "return", "matches", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.tokenize_and_update_mapping": [[178, 188], ["algTools.tokenize", "words.append", "words.append"], "function", ["home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.tokenize"], ["", "def", "tokenize_and_update_mapping", "(", "doc", ",", "word_map", ")", ":", "\n", "    ", "words", "=", "[", "]", "\n", "tokens", "=", "tokenize", "(", "doc", ")", "\n", "for", "word", "in", "tokens", ":", "\n", "        ", "if", "word", "not", "in", "word_map", ":", "\n", "            ", "word_map", "[", "word", "]", "=", "word", "\n", "words", ".", "append", "(", "word", ")", "\n", "", "else", ":", "\n", "            ", "words", ".", "append", "(", "word_map", "[", "word", "]", ")", "\n", "", "", "return", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.algTools.print_topics": [[190, 217], ["print", "len", "len", "range", "min", "word_topic_probabilities.sort", "print", "str", "len", "range"], "function", ["None"], ["", "def", "print_topics", "(", "p_w_t", ",", "id2word", ",", "max_topics", "=", "50", ",", "max_words", "=", "15", ")", ":", "\n", "    ", "\"\"\"\n    Print the most probable words for each topic, as limited by `max_word` and\n    `max_topics`\n    :param p_w_t: Word-topic probability matrix\n    :param id2word: Dictionary mapping IDs to words\n    :param max_topics: Maximum number of topics to print\n    :param max_words: Maximum number of words to print per topic\n    \"\"\"", "\n", "display_str", "=", "\"Printing up to {} topics with up to {} words each...\"", ".", "format", "(", "max_topics", ",", "max_words", ")", "\n", "print", "(", "\"\\n{}\\n{}\\n\"", ".", "format", "(", "display_str", ",", "\"-\"", "*", "len", "(", "display_str", ")", ")", ")", "\n", "\n", "n_topics", "=", "len", "(", "p_w_t", "[", "0", "]", ")", "\n", "num_words", "=", "len", "(", "p_w_t", ")", "\n", "for", "topic_i", "in", "range", "(", "min", "(", "n_topics", ",", "max_topics", ")", ")", ":", "\n", "        ", "word_topic_probabilities", "=", "[", "(", "p_w_t", "[", "word_id", "]", "[", "topic_i", "]", ",", "word_id", ")", "\n", "for", "word_id", "in", "range", "(", "num_words", ")", "]", "\n", "word_topic_probabilities", ".", "sort", "(", "reverse", "=", "True", ")", "\n", "\n", "topic_str", "=", "str", "(", "topic_i", ")", "+", "\":\\t\"", "\n", "for", "p", ",", "word_id", "in", "word_topic_probabilities", "[", "0", ":", "max_words", "]", ":", "\n", "            ", "if", "p", ">", "0.001", ":", "\n", "                ", "topic_str", "+=", "'%s %.3f, '", "%", "(", "id2word", "[", "word_id", "]", ",", "p", "*", "10", ")", "\n", "", "else", ":", "\n", "                ", "topic_str", "+=", "'%s %.5f, '", "%", "(", "id2word", "[", "word_id", "]", ",", "p", "*", "10", ")", "\n", "", "", "print", "(", "topic_str", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.JohnTailor_tkm.None.test.readBrownDataset": [[14, 24], ["nltk.download", "nltk.corpus.brown.fileids", "len", "nltk.corpus.brown.raw().replace", "re.sub", "docs.append", "nltk.corpus.brown.categories", "nltk.corpus.brown.raw"], "function", ["None"], ["def", "readBrownDataset", "(", ")", ":", "\n", "    ", "nltk", ".", "download", "(", "\"brown\"", ")", "\n", "documents", "=", "brown", ".", "fileids", "(", ")", "\n", "docs", "=", "[", "]", "\n", "for", "doc", "in", "documents", ":", "\n", "        ", "if", "len", "(", "brown", ".", "categories", "(", "doc", ")", ")", "==", "1", ":", "\n", "            ", "d", "=", "brown", ".", "raw", "(", "doc", ")", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "d", "=", "re", ".", "sub", "(", "r\"/[A-Za-z0-9_-]+ \"", ",", "\" \"", ",", "d", ")", "#The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn\") #.replace(\"/at\",\"\").replace(\"/nn-tl\",\"\").replace(\"/nn-hp\",\"\").replace(\"/np-hl\",\"\").replace(\"/nn\",\"\").replace(\"/vbd\",\"\").replace(\"/in\",\"\").replace(\"/jj\",\"\").replace(\"/hvz\",\"\").replace(\"/cs\",\"\").replace(\"/nps\",\"\").replace(\"/nr\",\"\").replace(\"/np-tl\",\"\").replace(\"/md\",\"\").replace(\"/np\",\"\").replace(\"/cd-hl\",\"\").replace(\"/vbn\",\"\").replace(\"/np-tl\",\"\").replace(\"/dti\",\"\").replace(\"--/--\",\"\")", "\n", "docs", ".", "append", "(", "d", ")", "\n", "", "", "return", "docs", "\n", "\n"]]}