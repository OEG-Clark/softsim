{"home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.BertForMultitask.__init__": [[28, 39], ["pytorch_pretrained_bert.modeling.PreTrainedBertModel.__init__", "pytorch_pretrained_bert.modeling.BertModel", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "model.BertForMultitask.apply"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "cls_num_labels", "=", "2", ",", "tok_num_labels", "=", "2", ",", "tok2id", "=", "None", ")", ":", "\n", "        ", "super", "(", "BertForMultitask", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "\n", "self", ".", "cls_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "cls_classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "cls_num_labels", ")", "\n", "\n", "self", ".", "tok_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "tok_classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "tok_num_labels", ")", "\n", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.BertForMultitask.forward": [[41, 55], ["model.BertForMultitask.bert", "model.BertForMultitask.cls_classifier", "model.BertForMultitask.cls_dropout", "model.BertForMultitask.tok_classifier", "model.BertForMultitask.tok_dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "rel_ids", "=", "None", ",", "pos_ids", "=", "None", ",", "categories", "=", "None", ",", "pre_len", "=", "None", ")", ":", "\n", "        ", "global", "ARGS", "\n", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "\n", "cls_logits", "=", "self", ".", "cls_classifier", "(", "pooled_output", ")", "\n", "cls_logits", "=", "self", ".", "cls_dropout", "(", "cls_logits", ")", "\n", "\n", "# NOTE -- dropout is after proj, which is non-standard", "\n", "tok_logits", "=", "self", ".", "tok_classifier", "(", "sequence_output", ")", "\n", "tok_logits", "=", "self", ".", "tok_dropout", "(", "tok_logits", ")", "\n", "\n", "return", "cls_logits", ",", "tok_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.ConcatCombine.__init__": [[60, 107], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "model.ConcatCombine.out.cuda", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "model.ConcatCombine.enricher.cuda", "min", "max", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "feature_size", ",", "out_size", ",", "layers", ",", "\n", "dropout_prob", ",", "small", "=", "False", ",", "pre_enrich", "=", "False", ",", "activation", "=", "False", ",", "\n", "include_categories", "=", "False", ",", "category_emb", "=", "False", ",", "\n", "add_category_emb", "=", "False", ")", ":", "\n", "        ", "super", "(", "ConcatCombine", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "include_categories", "=", "include_categories", "\n", "self", ".", "add_category_emb", "=", "add_category_emb", "\n", "if", "include_categories", ":", "\n", "            ", "if", "category_emb", "and", "not", "add_category_emb", ":", "\n", "                ", "feature_size", "*=", "2", "\n", "", "elif", "not", "category_emb", ":", "\n", "                ", "feature_size", "+=", "43", "\n", "\n", "", "", "if", "layers", "==", "1", ":", "\n", "            ", "self", ".", "out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "+", "feature_size", ",", "out_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "elif", "layers", "==", "2", ":", "\n", "            ", "waist_size", "=", "min", "(", "hidden_size", ",", "feature_size", ")", "if", "small", "else", "max", "(", "hidden_size", ",", "feature_size", ")", "\n", "if", "activation", ":", "\n", "                ", "self", ".", "out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "+", "feature_size", ",", "waist_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "waist_size", ",", "out_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "+", "feature_size", ",", "waist_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "waist_size", ",", "out_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "", "if", "pre_enrich", ":", "\n", "            ", "if", "activation", ":", "\n", "                ", "self", ".", "enricher", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "feature_size", ",", "feature_size", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "enricher", "=", "nn", ".", "Linear", "(", "feature_size", ",", "feature_size", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "enricher", "=", "None", "\n", "# manually set cuda because module doesn't see these combiners for bottom ", "\n", "", "if", "CUDA", ":", "\n", "            ", "self", ".", "out", "=", "self", ".", "out", ".", "cuda", "(", ")", "\n", "if", "self", ".", "enricher", ":", "\n", "                ", "self", ".", "enricher", "=", "self", ".", "enricher", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.ConcatCombine.forward": [[108, 121], ["model.ConcatCombine.out", "categories.repeat.repeat.unsqueeze", "categories.repeat.repeat.repeat", "model.ConcatCombine.enricher", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "hidden", ",", "features", ",", "categories", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "include_categories", ":", "\n", "            ", "categories", "=", "categories", ".", "unsqueeze", "(", "1", ")", "\n", "categories", "=", "categories", ".", "repeat", "(", "1", ",", "features", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "if", "self", ".", "add_category_emb", ":", "\n", "                ", "features", "=", "features", "+", "categories", "\n", "", "else", ":", "\n", "                ", "features", "=", "torch", ".", "cat", "(", "(", "features", ",", "categories", ")", ",", "-", "1", ")", "\n", "\n", "", "", "if", "self", ".", "enricher", "is", "not", "None", ":", "\n", "            ", "features", "=", "self", ".", "enricher", "(", "features", ")", "\n", "\n", "", "return", "self", ".", "out", "(", "torch", ".", "cat", "(", "(", "hidden", ",", "features", ")", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.AddCombine.__init__": [[124, 162], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "model.AddCombine.expand.cuda", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "min", "max", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "model.AddCombine.out.cuda", "model.AddCombine.enricher.cuda"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_dim", ",", "feat_dim", ",", "layers", ",", "dropout_prob", ",", "small", "=", "False", ",", "\n", "out_dim", "=", "-", "1", ",", "pre_enrich", "=", "False", ",", "include_categories", "=", "False", ",", "\n", "category_emb", "=", "False", ",", "add_category_emb", "=", "False", ")", ":", "\n", "        ", "super", "(", "AddCombine", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "include_categories", "=", "include_categories", "\n", "if", "include_categories", ":", "\n", "            ", "feat_dim", "+=", "43", "\n", "\n", "", "if", "layers", "==", "1", ":", "\n", "            ", "self", ".", "expand", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "feat_dim", ",", "hidden_dim", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "else", ":", "\n", "            ", "waist_size", "=", "min", "(", "feat_dim", ",", "hidden_dim", ")", "if", "small", "else", "max", "(", "feat_dim", ",", "hidden_dim", ")", "\n", "self", ".", "expand", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "feat_dim", ",", "waist_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "waist_size", ",", "hidden_dim", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "\n", "", "if", "out_dim", ">", "0", ":", "\n", "            ", "self", ".", "out", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "out_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "out", "=", "None", "\n", "\n", "", "if", "pre_enrich", ":", "\n", "            ", "self", ".", "enricher", "=", "nn", ".", "Linear", "(", "feature_size", ",", "feature_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "enricher", "=", "None", "\n", "\n", "# manually set cuda because module doesn't see these combiners for bottom         ", "\n", "", "if", "CUDA", ":", "\n", "            ", "self", ".", "expand", "=", "self", ".", "expand", ".", "cuda", "(", ")", "\n", "if", "out_dim", ">", "0", ":", "\n", "                ", "self", ".", "out", "=", "self", ".", "out", ".", "cuda", "(", ")", "\n", "", "if", "self", ".", "enricher", "is", "not", "None", ":", "\n", "                ", "self", ".", "enricher", "=", "self", ".", "enricher", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.AddCombine.forward": [[163, 181], ["categories.repeat.repeat.unsqueeze", "categories.repeat.repeat.repeat", "model.AddCombine.enricher", "model.AddCombine.expand", "model.AddCombine.out", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "hidden", ",", "feat", ",", "categories", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "include_categories", ":", "\n", "            ", "categories", "=", "categories", ".", "unsqueeze", "(", "1", ")", "\n", "categories", "=", "categories", ".", "repeat", "(", "1", ",", "features", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "if", "self", ".", "add_category_emb", ":", "\n", "                ", "features", "=", "features", "+", "categories", "\n", "", "else", ":", "\n", "                ", "features", "=", "torch", ".", "cat", "(", "(", "features", ",", "categories", ")", ",", "-", "1", ")", "\n", "\n", "", "", "if", "self", ".", "enricher", "is", "not", "None", ":", "\n", "            ", "feat", "=", "self", ".", "enricher", "(", "feat", ")", "\n", "\n", "", "combined", "=", "self", ".", "expand", "(", "feat", ")", "+", "hidden", "\n", "\n", "if", "self", ".", "out", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "out", "(", "combined", ")", "\n", "\n", "", "return", "combined", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.BertForMultitaskWithFeaturesOnTop.__init__": [[185, 222], ["pytorch_pretrained_bert.modeling.PreTrainedBertModel.__init__", "pytorch_pretrained_bert.modeling.BertModel", "features.Featurizer", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "model.BertForMultitaskWithFeaturesOnTop.apply", "model.ConcatCombine", "model.AddCombine", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "cls_num_labels", "=", "2", ",", "tok_num_labels", "=", "2", ",", "tok2id", "=", "None", ")", ":", "\n", "        ", "super", "(", "BertForMultitaskWithFeaturesOnTop", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "global", "ARGS", "\n", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "\n", "self", ".", "featurizer", "=", "features", ".", "Featurizer", "(", "\n", "tok2id", ",", "lexicon_feature_bits", "=", "ARGS", ".", "lexicon_feature_bits", ")", "\n", "# TODO -- don't hardcode this...", "\n", "nfeats", "=", "90", "if", "ARGS", ".", "lexicon_feature_bits", "==", "1", "else", "118", "\n", "\n", "if", "ARGS", ".", "extra_features_method", "==", "'concat'", ":", "\n", "            ", "self", ".", "tok_classifier", "=", "ConcatCombine", "(", "\n", "config", ".", "hidden_size", ",", "nfeats", ",", "tok_num_labels", ",", "\n", "ARGS", ".", "combiner_layers", ",", "config", ".", "hidden_dropout_prob", ",", "\n", "ARGS", ".", "small_waist", ",", "pre_enrich", "=", "ARGS", ".", "pre_enrich", ",", "\n", "activation", "=", "ARGS", ".", "activation_hidden", ",", "\n", "include_categories", "=", "ARGS", ".", "concat_categories", ",", "\n", "category_emb", "=", "ARGS", ".", "category_emb", ",", "\n", "add_category_emb", "=", "ARGS", ".", "add_category_emb", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "tok_classifier", "=", "AddCombine", "(", "\n", "config", ".", "hidden_size", ",", "nfeats", ",", "ARGS", ".", "combiner_layers", ",", "\n", "config", ".", "hidden_dropout_prob", ",", "ARGS", ".", "small_waist", ",", "\n", "out_dim", "=", "tok_num_labels", ",", "pre_enrich", "=", "ARGS", ".", "pre_enrich", ",", "\n", "include_categories", "=", "ARGS", ".", "concat_categories", ",", "\n", "category_emb", "=", "ARGS", ".", "category_emb", ",", "\n", "add_category_emb", "=", "ARGS", ".", "add_category_emb", ")", "\n", "\n", "", "self", ".", "cls_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "cls_classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "cls_num_labels", ")", "\n", "\n", "self", ".", "category_emb", "=", "ARGS", ".", "category_emb", "\n", "if", "ARGS", ".", "category_emb", ":", "\n", "            ", "self", ".", "category_embeddings", "=", "nn", ".", "Embedding", "(", "43", ",", "nfeats", ")", "\n", "\n", "", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.BertForMultitaskWithFeaturesOnTop.forward": [[224, 252], ["model.BertForMultitaskWithFeaturesOnTop.featurizer.featurize_batch", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "model.BertForMultitaskWithFeaturesOnTop.bert", "model.BertForMultitaskWithFeaturesOnTop.cls_dropout", "model.BertForMultitaskWithFeaturesOnTop.cls_classifier", "model.BertForMultitaskWithFeaturesOnTop.tok_classifier", "input_ids.detach().cpu().numpy", "rel_ids.detach().cpu().numpy", "pos_ids.detach().cpu().numpy", "features.cuda.cuda.cuda", "model.BertForMultitaskWithFeaturesOnTop.category_embeddings", "[].type", "input_ids.detach().cpu", "rel_ids.detach().cpu", "pos_ids.detach().cpu", "input_ids.detach", "rel_ids.detach", "pos_ids.detach", "model.BertForMultitaskWithFeaturesOnTop.max"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.featurize_batch"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "rel_ids", "=", "None", ",", "pos_ids", "=", "None", ",", "categories", "=", "None", ",", "pre_len", "=", "None", ")", ":", "\n", "        ", "global", "ARGS", "\n", "global", "CUDA", "\n", "\n", "features", "=", "self", ".", "featurizer", ".", "featurize_batch", "(", "\n", "input_ids", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "rel_ids", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "pos_ids", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "padded_len", "=", "input_ids", ".", "shape", "[", "1", "]", ")", "\n", "features", "=", "torch", ".", "tensor", "(", "features", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "if", "CUDA", ":", "\n", "            ", "features", "=", "features", ".", "cuda", "(", ")", "\n", "\n", "", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "\n", "pooled_output", "=", "self", ".", "cls_dropout", "(", "pooled_output", ")", "\n", "cls_logits", "=", "self", ".", "cls_classifier", "(", "pooled_output", ")", "\n", "\n", "if", "ARGS", ".", "category_emb", ":", "\n", "            ", "categories", "=", "self", ".", "category_embeddings", "(", "\n", "categories", ".", "max", "(", "-", "1", ")", "[", "1", "]", ".", "type", "(", "\n", "'torch.cuda.LongTensor'", "if", "CUDA", "else", "'torch.LongTensor'", ")", ")", "\n", "\n", "", "tok_logits", "=", "self", ".", "tok_classifier", "(", "sequence_output", ",", "features", ",", "categories", ")", "\n", "\n", "return", "cls_logits", ",", "tok_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.TaggerFromDebiaser.__init__": [[255, 288], ["torch.Module.__init__", "print", "model.TaggerFromDebiaser.debias_model.load_state_dict", "print", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "seq2seq.PointerSeq2Seq", "seq2seq.Seq2Seq", "torch.load", "torch.load", "torch.load", "torch.load", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load"], ["    ", "def", "__init__", "(", "self", ",", "cls_num_labels", "=", "2", ",", "tok_num_labels", "=", "2", ",", "tok2id", "=", "None", ")", ":", "\n", "        ", "super", "(", "TaggerFromDebiaser", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "global", "ARGS", "\n", "global", "CUDA", "\n", "\n", "if", "ARGS", ".", "pointer_generator", ":", "\n", "            ", "self", ".", "debias_model", "=", "seq2seq_model", ".", "PointerSeq2Seq", "(", "\n", "vocab_size", "=", "len", "(", "tok2id", ")", ",", "hidden_size", "=", "ARGS", ".", "hidden_size", ",", "\n", "emb_dim", "=", "768", ",", "dropout", "=", "0.2", ",", "tok2id", "=", "tok2id", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "debias_model", "=", "seq2seq_model", ".", "Seq2Seq", "(", "\n", "vocab_size", "=", "len", "(", "tok2id", ")", ",", "hidden_size", "=", "ARGS", ".", "hidden_size", ",", "\n", "emb_dim", "=", "768", ",", "dropout", "=", "0.2", ",", "tok2id", "=", "tok2id", ")", "\n", "\n", "", "assert", "ARGS", ".", "debias_checkpoint", "\n", "print", "(", "'LOADING DEBIASER FROM '", "+", "ARGS", ".", "debias_checkpoint", ")", "\n", "self", ".", "debias_model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "ARGS", ".", "debias_checkpoint", ")", ")", "\n", "print", "(", "'...DONE'", ")", "\n", "\n", "self", ".", "cls_classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "ARGS", ".", "hidden_size", ",", "ARGS", ".", "hidden_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "0.1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "ARGS", ".", "hidden_size", ",", "cls_num_labels", ")", ",", "\n", "nn", ".", "Dropout", "(", "0.1", ")", ")", "\n", "\n", "self", ".", "tok_classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "ARGS", ".", "hidden_size", ",", "ARGS", ".", "hidden_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "0.1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "ARGS", ".", "hidden_size", ",", "tok_num_labels", ")", ",", "\n", "nn", ".", "Dropout", "(", "0.1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.TaggerFromDebiaser.forward": [[290, 303], ["model.TaggerFromDebiaser.debias_model.run_encoder", "model.TaggerFromDebiaser.cls_classifier", "model.TaggerFromDebiaser.tok_classifier"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.run_encoder"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "rel_ids", "=", "None", ",", "pos_ids", "=", "None", ",", "categories", "=", "None", ",", "pre_len", "=", "None", ")", ":", "\n", "\n", "        ", "pre_mask", "=", "1.0", "-", "attention_mask", "\n", "\n", "# src_outputs is [batch_size, sequence_length, hidden_size].", "\n", "src_outputs", ",", "h_t", ",", "_", "=", "self", ".", "debias_model", ".", "run_encoder", "(", "\n", "input_ids", ",", "pre_len", ",", "pre_mask", ")", "\n", "\n", "cls_logits", "=", "self", ".", "cls_classifier", "(", "h_t", ")", "\n", "tok_logits", "=", "self", ".", "tok_classifier", "(", "src_outputs", ")", "\n", "\n", "return", "cls_logits", ",", "tok_logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.gelu": [[16, 22], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.model.identity": [[23, 25], ["None"], "function", ["None"], ["", "def", "identity", "(", "x", ")", ":", "\n", "    ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.baseline.data_for_scipy": [[49, 89], ["tqdm.tqdm", "pre_id.numpy.numpy", "pre_len.numpy.numpy", "rel_ids.numpy.numpy", "pos_ids.numpy.numpy", "tok_label_id.numpy.numpy", "featurizer.featurize_batch", "zip", "scipy.sparse.vstack", "range", "numpy.zeros", "seqX.append", "seqY.append", "outX.append", "outY.append", "len", "scipy.sparse.csr_matrix", "scipy.sparse.vstack"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.featurize_batch"], ["def", "data_for_scipy", "(", "dataloader", ",", "by_seq", "=", "False", ")", ":", "\n", "    ", "outX", "=", "[", "]", "\n", "outY", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "dataloader", ")", ":", "\n", "        ", "(", "\n", "pre_id", ",", "pre_mask", ",", "pre_len", ",", "\n", "post_in_id", ",", "post_out_id", ",", "\n", "tok_label_id", ",", "_", ",", "tok_dist", ",", "\n", "replace_id", ",", "rel_ids", ",", "pos_ids", ",", "type_ids", ",", "categories", "\n", ")", "=", "batch", "\n", "pre_id", "=", "pre_id", ".", "numpy", "(", ")", "\n", "pre_len", "=", "pre_len", ".", "numpy", "(", ")", "\n", "rel_ids", "=", "rel_ids", ".", "numpy", "(", ")", "\n", "pos_ids", "=", "pos_ids", ".", "numpy", "(", ")", "\n", "tok_label_id", "=", "tok_label_id", ".", "numpy", "(", ")", "\n", "\n", "features", "=", "featurizer", ".", "featurize_batch", "(", "pre_id", ",", "rel_ids", ",", "pos_ids", ")", "\n", "for", "id_seq", ",", "seq_feats", ",", "seq_len", ",", "label_seq", "in", "zip", "(", "pre_id", ",", "features", ",", "pre_len", ",", "tok_label_id", ")", ":", "\n", "            ", "seqX", "=", "[", "]", "\n", "seqY", "=", "[", "]", "\n", "for", "ti", "in", "range", "(", "seq_len", ")", ":", "\n", "                ", "word_features", "=", "np", ".", "zeros", "(", "len", "(", "tok2id", ")", ")", "\n", "word_features", "[", "id_seq", "[", "ti", "]", "]", "=", "1.0", "\n", "\n", "timestep_vec", "=", "seq_feats", "[", "ti", "]", "\n", "#timestep_vec = np.concatenate((word_features, seq_feats[ti]))", "\n", "\n", "seqX", ".", "append", "(", "csr_matrix", "(", "timestep_vec", ")", ")", "\n", "seqY", ".", "append", "(", "label_seq", "[", "ti", "]", ")", "\n", "\n", "", "if", "by_seq", ":", "\n", "                ", "outX", ".", "append", "(", "scipy", ".", "sparse", ".", "vstack", "(", "seqX", ")", ")", "\n", "outY", ".", "append", "(", "seqY", ")", "\n", "", "else", ":", "\n", "                ", "outX", "+=", "seqX", "\n", "outY", "+=", "seqY", "\n", "", "", "", "if", "by_seq", ":", "\n", "        ", "return", "outX", ",", "outY", "\n", "\n", "", "return", "scipy", ".", "sparse", ".", "vstack", "(", "outX", ")", ",", "outY", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.build_optimizer": [[13, 33], ["list", "torch.Adam", "list", "list", "pytorch_pretrained_bert.optimization.BertAdam", "list", "list", "filter", "model.named_parameters", "filter", "model.cls_classifier.parameters", "model.tok_classifier.parameters", "any", "any"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters"], ["def", "build_optimizer", "(", "model", ",", "num_train_steps", ",", "learning_rate", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "if", "ARGS", ".", "tagger_from_debiaser", ":", "\n", "        ", "parameters", "=", "list", "(", "model", ".", "cls_classifier", ".", "parameters", "(", ")", ")", "+", "list", "(", "\n", "model", ".", "tok_classifier", ".", "parameters", "(", ")", ")", "\n", "parameters", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "parameters", ")", ")", "\n", "return", "optim", ".", "Adam", "(", "parameters", ",", "lr", "=", "ARGS", ".", "learning_rate", ")", "\n", "", "else", ":", "\n", "        ", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "param_optimizer", "=", "list", "(", "filter", "(", "lambda", "name_param", ":", "name_param", "[", "1", "]", ".", "requires_grad", ",", "param_optimizer", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "return", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "0.1", ",", "\n", "t_total", "=", "num_train_steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.build_loss_fn": [[35, 78], ["torch.ones", "torch.ones", "weight_mask.cuda.cuda", "torch.nn.CrossEntropyLoss().cuda", "torch.nn.CrossEntropyLoss().cuda", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "apply_mask.contiguous().view", "torch.nn.CrossEntropyLoss.", "torch.mean", "torch.mean", "logits.contiguous().view", "labels.contiguous().view().type", "logits.contiguous().view", "labels.contiguous().view().type", "per_tok_losses[].squeeze", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "apply_mask.contiguous", "logits.contiguous", "labels.contiguous().view", "logits.contiguous", "labels.contiguous().view", "labels.contiguous", "labels.contiguous", "torch.nonzero", "torch.nonzero"], "function", ["None"], ["", "", "def", "build_loss_fn", "(", "debias_weight", "=", "None", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "if", "debias_weight", "is", "None", ":", "\n", "        ", "debias_weight", "=", "ARGS", ".", "debias_weight", "\n", "\n", "", "weight_mask", "=", "torch", ".", "ones", "(", "ARGS", ".", "num_tok_labels", ")", "\n", "weight_mask", "[", "-", "1", "]", "=", "0", "\n", "\n", "if", "CUDA", ":", "\n", "        ", "weight_mask", "=", "weight_mask", ".", "cuda", "(", ")", "\n", "criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ")", ".", "cuda", "(", ")", "\n", "per_tok_criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ",", "reduction", "=", "'none'", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ")", "\n", "per_tok_criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ",", "reduction", "=", "'none'", ")", "\n", "\n", "\n", "", "def", "cross_entropy_loss", "(", "logits", ",", "labels", ",", "apply_mask", "=", "None", ")", ":", "\n", "        ", "return", "criterion", "(", "\n", "logits", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "ARGS", ".", "num_tok_labels", ")", ",", "\n", "labels", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ".", "type", "(", "'torch.cuda.LongTensor'", "if", "CUDA", "else", "'torch.LongTensor'", ")", ")", "\n", "\n", "", "def", "weighted_cross_entropy_loss", "(", "logits", ",", "labels", ",", "apply_mask", "=", "None", ")", ":", "\n", "# weight mask = where to apply weight (post_tok_label_id from the batch)", "\n", "        ", "weights", "=", "apply_mask", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "weights", "=", "(", "(", "debias_weight", "-", "1", ")", "*", "weights", ")", "+", "1.0", "\n", "\n", "per_tok_losses", "=", "per_tok_criterion", "(", "\n", "logits", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "ARGS", ".", "num_tok_labels", ")", ",", "\n", "labels", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ".", "type", "(", "'torch.cuda.LongTensor'", "if", "CUDA", "else", "'torch.LongTensor'", ")", ")", "\n", "per_tok_losses", "=", "per_tok_losses", "*", "weights", "\n", "\n", "loss", "=", "torch", ".", "mean", "(", "per_tok_losses", "[", "torch", ".", "nonzero", "(", "per_tok_losses", ")", "]", ".", "squeeze", "(", ")", ")", "\n", "\n", "return", "loss", "\n", "\n", "", "if", "debias_weight", "==", "1.0", ":", "\n", "        ", "loss_fn", "=", "cross_entropy_loss", "\n", "", "else", ":", "\n", "        ", "loss_fn", "=", "weighted_cross_entropy_loss", "\n", "\n", "", "return", "loss_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.softmax": [[80, 84], ["numpy.exp", "x.max", "np.exp.sum"], "function", ["None"], ["", "def", "softmax", "(", "x", ",", "axis", "=", "None", ")", ":", "\n", "    ", "x", "=", "x", "-", "x", ".", "max", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "y", "=", "np", ".", "exp", "(", "x", ")", "\n", "return", "y", "/", "y", ".", "sum", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.run_inference": [[86, 131], ["enumerate", "tqdm.tqdm", "out[].append", "tok_logits.detach().cpu().numpy", "tok_label_id.cpu().numpy", "tok_logits.detach().cpu().numpy.tolist", "tok_label_id.cpu().numpy.tolist", "utils.to_probs", "utils.tag_hits", "tuple", "torch.no_grad", "torch.no_grad", "model", "loss_fn", "tokenizer.convert_ids_to_tokens", "tokenizer.convert_ids_to_tokens", "float", "pre_id.cpu().numpy", "post_in_id.cpu().numpy", "loss_fn.cpu().numpy", "tok_logits.detach().cpu", "tok_label_id.cpu", "x.cuda", "pre_id.cpu", "post_in_id.cpu", "loss_fn.cpu", "tok_logits.detach"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.to_probs", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.tag_hits"], ["", "def", "run_inference", "(", "model", ",", "eval_dataloader", ",", "loss_fn", ",", "tokenizer", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "out", "=", "{", "\n", "'input_toks'", ":", "[", "]", ",", "\n", "'post_toks'", ":", "[", "]", ",", "\n", "\n", "'tok_loss'", ":", "[", "]", ",", "\n", "'tok_logits'", ":", "[", "]", ",", "\n", "'tok_probs'", ":", "[", "]", ",", "\n", "'tok_labels'", ":", "[", "]", ",", "\n", "\n", "'labeling_hits'", ":", "[", "]", "\n", "}", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "eval_dataloader", ")", ")", ":", "\n", "        ", "if", "ARGS", ".", "debug_skip", "and", "step", ">", "2", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "CUDA", ":", "\n", "            ", "batch", "=", "tuple", "(", "x", ".", "cuda", "(", ")", "for", "x", "in", "batch", ")", "\n", "\n", "", "(", "\n", "pre_id", ",", "pre_mask", ",", "pre_len", ",", "\n", "post_in_id", ",", "post_out_id", ",", "\n", "tok_label_id", ",", "_", ",", "\n", "rel_ids", ",", "pos_ids", ",", "categories", "\n", ")", "=", "batch", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", ",", "tok_logits", "=", "model", "(", "pre_id", ",", "attention_mask", "=", "1.0", "-", "pre_mask", ",", "\n", "rel_ids", "=", "rel_ids", ",", "pos_ids", "=", "pos_ids", ",", "categories", "=", "categories", ",", "\n", "pre_len", "=", "pre_len", ")", "\n", "tok_loss", "=", "loss_fn", "(", "tok_logits", ",", "tok_label_id", ",", "apply_mask", "=", "tok_label_id", ")", "\n", "", "out", "[", "'input_toks'", "]", "+=", "[", "tokenizer", ".", "convert_ids_to_tokens", "(", "seq", ")", "for", "seq", "in", "pre_id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", "\n", "out", "[", "'post_toks'", "]", "+=", "[", "tokenizer", ".", "convert_ids_to_tokens", "(", "seq", ")", "for", "seq", "in", "post_in_id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", "\n", "out", "[", "'tok_loss'", "]", ".", "append", "(", "float", "(", "tok_loss", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ")", "\n", "logits", "=", "tok_logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "labels", "=", "tok_label_id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "out", "[", "'tok_logits'", "]", "+=", "logits", ".", "tolist", "(", ")", "\n", "out", "[", "'tok_labels'", "]", "+=", "labels", ".", "tolist", "(", ")", "\n", "out", "[", "'tok_probs'", "]", "+=", "to_probs", "(", "logits", ",", "pre_len", ")", "\n", "out", "[", "'labeling_hits'", "]", "+=", "tag_hits", "(", "logits", ",", "labels", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.train_for_epoch": [[132, 160], ["enumerate", "tqdm.tqdm", "model", "loss_fn", "loss_fn.backward", "optimizer.step", "model.zero_grad", "losses.append", "tuple", "loss_fn.detach().cpu().numpy", "x.cuda", "loss_fn.detach().cpu", "loss_fn.detach"], "function", ["None"], ["", "def", "train_for_epoch", "(", "model", ",", "train_dataloader", ",", "loss_fn", ",", "optimizer", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "losses", "=", "[", "]", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ")", ")", ":", "\n", "        ", "if", "ARGS", ".", "debug_skip", "and", "step", ">", "2", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "CUDA", ":", "\n", "            ", "batch", "=", "tuple", "(", "x", ".", "cuda", "(", ")", "for", "x", "in", "batch", ")", "\n", "", "(", "\n", "pre_id", ",", "pre_mask", ",", "pre_len", ",", "\n", "post_in_id", ",", "post_out_id", ",", "\n", "tok_label_id", ",", "_", ",", "\n", "rel_ids", ",", "pos_ids", ",", "categories", "\n", ")", "=", "batch", "\n", "_", ",", "tok_logits", "=", "model", "(", "pre_id", ",", "attention_mask", "=", "1.0", "-", "pre_mask", ",", "\n", "rel_ids", "=", "rel_ids", ",", "pos_ids", "=", "pos_ids", ",", "categories", "=", "categories", ",", "\n", "pre_len", "=", "pre_len", ")", "\n", "loss", "=", "loss_fn", "(", "tok_logits", ",", "tok_label_id", ",", "apply_mask", "=", "tok_label_id", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "losses", ".", "append", "(", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.to_probs": [[161, 169], ["utils.softmax", "zip", "out.append", "numpy.array", "score_seq[].tolist"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.softmax"], ["", "def", "to_probs", "(", "logits", ",", "lens", ")", ":", "\n", "    ", "per_tok_probs", "=", "softmax", "(", "np", ".", "array", "(", "logits", ")", "[", ":", ",", ":", ",", ":", "2", "]", ",", "axis", "=", "2", ")", "\n", "pos_scores", "=", "per_tok_probs", "[", ":", ",", ":", ",", "-", "1", "]", "\n", "\n", "out", "=", "[", "]", "\n", "for", "score_seq", ",", "l", "in", "zip", "(", "pos_scores", ",", "lens", ")", ":", "\n", "        ", "out", ".", "append", "(", "score_seq", "[", ":", "l", "]", ".", "tolist", "(", ")", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.is_ranking_hit": [[170, 181], ["list", "list", "list", "zip", "zip", "zip", "sum", "range", "numpy.array", "len", "sorted", "zip"], "function", ["None"], ["", "def", "is_ranking_hit", "(", "probs", ",", "labels", ",", "top", "=", "1", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "# get rid of padding idx", "\n", "[", "probs", ",", "labels", "]", "=", "list", "(", "zip", "(", "*", "[", "(", "p", ",", "l", ")", "for", "p", ",", "l", "in", "zip", "(", "probs", ",", "labels", ")", "if", "l", "!=", "ARGS", ".", "num_tok_labels", "-", "1", "]", ")", ")", "\n", "probs_indices", "=", "list", "(", "zip", "(", "np", ".", "array", "(", "probs", ")", "[", ":", ",", "1", "]", ",", "range", "(", "len", "(", "labels", ")", ")", ")", ")", "\n", "[", "_", ",", "top_indices", "]", "=", "list", "(", "zip", "(", "*", "sorted", "(", "probs_indices", ",", "reverse", "=", "True", ")", "[", ":", "top", "]", ")", ")", "\n", "if", "sum", "(", "[", "labels", "[", "i", "]", "for", "i", "in", "top_indices", "]", ")", ">", "0", ":", "\n", "        ", "return", "1", "\n", "", "else", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.tag_hits": [[182, 192], ["utils.softmax", "utils.is_ranking_hit", "numpy.array", "zip"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.softmax", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.utils.is_ranking_hit"], ["", "", "def", "tag_hits", "(", "logits", ",", "tok_labels", ",", "top", "=", "1", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "probs", "=", "softmax", "(", "np", ".", "array", "(", "logits", ")", "[", ":", ",", ":", ",", ":", "ARGS", ".", "num_tok_labels", "-", "1", "]", ",", "axis", "=", "2", ")", "\n", "\n", "hits", "=", "[", "\n", "is_ranking_hit", "(", "prob_dist", ",", "tok_label", ",", "top", "=", "top", ")", "\n", "for", "prob_dist", ",", "tok_label", "in", "zip", "(", "probs", ",", "tok_labels", ")", "\n", "]", "\n", "return", "hits", "\n", "", ""]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.__init__": [[16, 41], ["features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "features.Featurizer.read_lexicon", "tok2id.items"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon"], ["    ", "def", "__init__", "(", "self", ",", "tok2id", "=", "{", "}", ",", "pad_id", "=", "0", ",", "lexicon_feature_bits", "=", "1", ")", ":", "\n", "        ", "self", ".", "tok2id", "=", "tok2id", "\n", "self", ".", "id2tok", "=", "{", "x", ":", "tok", "for", "tok", ",", "x", "in", "tok2id", ".", "items", "(", ")", "}", "\n", "self", ".", "pad_id", "=", "pad_id", "\n", "\n", "self", ".", "pos2id", "=", "POS2ID", "\n", "self", ".", "rel2id", "=", "REL2ID", "\n", "\n", "self", ".", "lexicons", "=", "{", "\n", "'assertives'", ":", "self", ".", "read_lexicon", "(", "'lexicons/assertives_hooper1975.txt'", ")", ",", "\n", "'entailed_arg'", ":", "self", ".", "read_lexicon", "(", "'lexicons/entailed_arg_berant2012.txt'", ")", ",", "\n", "'entailed'", ":", "self", ".", "read_lexicon", "(", "'lexicons/entailed_berant2012.txt'", ")", ",", "\n", "'entailing_arg'", ":", "self", ".", "read_lexicon", "(", "'lexicons/entailing_arg_berant2012.txt'", ")", ",", "\n", "'entailing'", ":", "self", ".", "read_lexicon", "(", "'lexicons/entailing_berant2012.txt'", ")", ",", "\n", "'factives'", ":", "self", ".", "read_lexicon", "(", "'lexicons/factives_hooper1975.txt'", ")", ",", "\n", "'hedges'", ":", "self", ".", "read_lexicon", "(", "'lexicons/hedges_hyland2005.txt'", ")", ",", "\n", "'implicatives'", ":", "self", ".", "read_lexicon", "(", "'lexicons/implicatives_karttunen1971.txt'", ")", ",", "\n", "'negatives'", ":", "self", ".", "read_lexicon", "(", "'lexicons/negative_liu2005.txt'", ")", ",", "\n", "'positives'", ":", "self", ".", "read_lexicon", "(", "'lexicons/positive_liu2005.txt'", ")", ",", "\n", "'npov'", ":", "self", ".", "read_lexicon", "(", "'lexicons/npov_lexicon.txt'", ")", ",", "\n", "'reports'", ":", "self", ".", "read_lexicon", "(", "'lexicons/report_verbs.txt'", ")", ",", "\n", "'strong_subjectives'", ":", "self", ".", "read_lexicon", "(", "'lexicons/strong_subjectives_riloff2003.txt'", ")", ",", "\n", "'weak_subjectives'", ":", "self", ".", "read_lexicon", "(", "'lexicons/weak_subjectives_riloff2003.txt'", ")", "\n", "}", "\n", "self", ".", "lexicon_feature_bits", "=", "lexicon_feature_bits", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.get_feature_names": [[43, 51], ["list", "list", "list", "features.Featurizer.lexicons.keys", "list", "list", "zip", "zip", "sorted", "sorted", "features.Featurizer.pos2id.items", "features.Featurizer.rel2id.items"], "methods", ["None"], ["", "def", "get_feature_names", "(", "self", ")", ":", "\n", "\n", "        ", "lexicon_feature_names", "=", "list", "(", "self", ".", "lexicons", ".", "keys", "(", ")", ")", "\n", "context_feature_names", "=", "[", "x", "+", "'_context'", "for", "x", "in", "lexicon_feature_names", "]", "\n", "pos_names", "=", "list", "(", "list", "(", "zip", "(", "*", "sorted", "(", "self", ".", "pos2id", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", ")", ")", "[", "0", "]", ")", "\n", "rel_names", "=", "list", "(", "list", "(", "zip", "(", "*", "sorted", "(", "self", ".", "rel2id", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", ")", ")", "[", "0", "]", ")", "\n", "\n", "return", "lexicon_feature_names", "+", "context_feature_names", "+", "pos_names", "+", "rel_names", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.read_lexicon": [[52, 59], ["set", "l.strip", "open", "l.startswith", "l.startswith", "len", "l.strip().split", "l.strip"], "methods", ["None"], ["", "def", "read_lexicon", "(", "self", ",", "fp", ")", ":", "\n", "        ", "out", "=", "set", "(", "[", "\n", "l", ".", "strip", "(", ")", "for", "l", "in", "open", "(", "fp", ",", "errors", "=", "'ignore'", ")", "\n", "if", "not", "l", ".", "startswith", "(", "'#'", ")", "and", "not", "l", ".", "startswith", "(", "';'", ")", "\n", "and", "len", "(", "l", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "==", "1", "\n", "]", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.lexicon_features": [[61, 82], ["numpy.array", "numpy.array", "out.reshape.reshape.append", "out.reshape.reshape.reshape", "len", "features.Featurizer.lexicons.items"], "methods", ["None"], ["", "def", "lexicon_features", "(", "self", ",", "words", ",", "bits", "=", "2", ")", ":", "\n", "        ", "assert", "bits", "in", "[", "1", ",", "2", "]", "\n", "if", "bits", "==", "1", ":", "\n", "            ", "true", "=", "1", "\n", "false", "=", "0", "\n", "", "else", ":", "\n", "            ", "true", "=", "[", "1", ",", "0", "]", "\n", "false", "=", "[", "0", ",", "1", "]", "\n", "\n", "", "out", "=", "[", "]", "\n", "for", "word", "in", "words", ":", "\n", "            ", "out", ".", "append", "(", "[", "\n", "true", "if", "word", "in", "lexicon", "else", "false", "\n", "for", "_", ",", "lexicon", "in", "self", ".", "lexicons", ".", "items", "(", ")", "\n", "]", ")", "\n", "", "out", "=", "np", ".", "array", "(", "out", ")", "\n", "\n", "if", "bits", "==", "2", ":", "\n", "            ", "out", "=", "out", ".", "reshape", "(", "len", "(", "words", ")", ",", "-", "1", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.context_features": [[84, 98], ["range", "numpy.array", "numpy.array", "max", "min", "out.append", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.sum", "numpy.sum"], "methods", ["None"], ["", "def", "context_features", "(", "self", ",", "lex_feats", ",", "window_size", "=", "2", ")", ":", "\n", "        ", "out", "=", "[", "]", "\n", "nwords", "=", "lex_feats", ".", "shape", "[", "0", "]", "\n", "nfeats", "=", "lex_feats", ".", "shape", "[", "1", "]", "\n", "for", "wi", "in", "range", "(", "lex_feats", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "window_start", "=", "max", "(", "wi", "-", "window_size", ",", "0", ")", "\n", "window_end", "=", "min", "(", "wi", "+", "window_size", "+", "1", ",", "nwords", ")", "\n", "\n", "left", "=", "lex_feats", "[", "window_start", ":", "wi", ",", ":", "]", "if", "wi", ">", "0", "else", "np", ".", "zeros", "(", "(", "1", ",", "nfeats", ")", ")", "\n", "right", "=", "lex_feats", "[", "wi", "+", "1", ":", "window_end", ",", ":", "]", "if", "wi", "<", "nwords", "-", "1", "else", "np", ".", "zeros", "(", "(", "1", ",", "nfeats", ")", ")", "\n", "\n", "out", ".", "append", "(", "(", "np", ".", "sum", "(", "left", "+", "right", ",", "axis", "=", "0", ")", ">", "0", ")", ".", "astype", "(", "int", ")", ")", "\n", "\n", "", "return", "np", ".", "array", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.features": [[100, 144], ["enumerate", "features.Featurizer.lexicon_features", "features.Featurizer.context_features", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "id_seq.index", "len", "tok.startswith", "tok.replace", "word_indices[].append", "words.append", "word_indices.append", "numpy.repeat", "numpy.repeat", "len", "len", "len", "len", "numpy.zeros", "numpy.zeros", "numpy.expand_dims", "numpy.expand_dims", "len", "zip", "range", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.lexicon_features", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.context_features"], ["", "def", "features", "(", "self", ",", "id_seq", ",", "rel_ids", ",", "pos_ids", ")", ":", "\n", "        ", "if", "self", ".", "pad_id", "in", "id_seq", ":", "\n", "            ", "pad_idx", "=", "id_seq", ".", "index", "(", "self", ".", "pad_id", ")", "\n", "pad_len", "=", "len", "(", "id_seq", "[", "pad_idx", ":", "]", ")", "\n", "id_seq", "=", "id_seq", "[", ":", "pad_idx", "]", "\n", "rel_ids", "=", "rel_ids", "[", ":", "pad_idx", "]", "\n", "pos_ids", "=", "pos_ids", "[", ":", "pad_idx", "]", "\n", "", "else", ":", "\n", "            ", "pad_len", "=", "0", "\n", "\n", "", "toks", "=", "[", "self", ".", "id2tok", "[", "x", "]", "for", "x", "in", "id_seq", "]", "\n", "# build list of [word, [tok indices the word came from]]", "\n", "words", "=", "[", "]", "\n", "word_indices", "=", "[", "]", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "toks", ")", ":", "\n", "            ", "if", "tok", ".", "startswith", "(", "'##'", ")", ":", "\n", "                ", "words", "[", "-", "1", "]", "+=", "tok", ".", "replace", "(", "'##'", ",", "''", ")", "\n", "word_indices", "[", "-", "1", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                ", "words", ".", "append", "(", "tok", ")", "\n", "word_indices", ".", "append", "(", "[", "i", "]", ")", "\n", "\n", "# get expert features", "\n", "", "", "lex_feats", "=", "self", ".", "lexicon_features", "(", "words", ",", "bits", "=", "self", ".", "lexicon_feature_bits", ")", "\n", "context_feats", "=", "self", ".", "context_features", "(", "lex_feats", ")", "\n", "expert_feats", "=", "np", ".", "concatenate", "(", "(", "lex_feats", ",", "context_feats", ")", ",", "axis", "=", "1", ")", "\n", "# break word-features into tokens", "\n", "feats", "=", "np", ".", "concatenate", "(", "[", "\n", "np", ".", "repeat", "(", "np", ".", "expand_dims", "(", "word_vec", ",", "axis", "=", "0", ")", ",", "len", "(", "indices", ")", ",", "axis", "=", "0", ")", "\n", "for", "(", "word_vec", ",", "indices", ")", "in", "zip", "(", "expert_feats", ",", "word_indices", ")", "\n", "]", ",", "axis", "=", "0", ")", "\n", "\n", "# add in the pos and relational features", "\n", "pos_feats", "=", "np", ".", "zeros", "(", "(", "len", "(", "pos_ids", ")", ",", "len", "(", "POS2ID", ")", ")", ")", "\n", "pos_feats", "[", "range", "(", "len", "(", "pos_ids", ")", ")", ",", "pos_ids", "]", "=", "1", "\n", "rel_feats", "=", "np", ".", "zeros", "(", "(", "len", "(", "rel_ids", ")", ",", "len", "(", "REL2ID", ")", ")", ")", "\n", "rel_feats", "[", "range", "(", "len", "(", "rel_ids", ")", ")", ",", "rel_ids", "]", "=", "1", "\n", "\n", "feats", "=", "np", ".", "concatenate", "(", "(", "feats", ",", "pos_feats", ",", "rel_feats", ")", ",", "axis", "=", "1", ")", "\n", "\n", "# add pad back in                ", "\n", "feats", "=", "np", ".", "concatenate", "(", "(", "feats", ",", "np", ".", "zeros", "(", "(", "pad_len", ",", "feats", ".", "shape", "[", "1", "]", ")", ")", ")", ")", "\n", "\n", "return", "feats", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.featurize_batch": [[146, 154], ["numpy.array", "numpy.array", "features.Featurizer.features", "list", "list", "list", "zip"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.features"], ["", "def", "featurize_batch", "(", "self", ",", "batch_ids", ",", "rel_ids", ",", "pos_ids", ",", "padded_len", "=", "0", ")", ":", "\n", "        ", "\"\"\" takes [batch, len] returns [batch, len, features] \"\"\"", "\n", "\n", "batch_feats", "=", "[", "\n", "self", ".", "features", "(", "list", "(", "id_seq", ")", ",", "list", "(", "rel_ids", ")", ",", "list", "(", "pos_ids", ")", ")", "\n", "for", "id_seq", ",", "rel_ids", ",", "pos_ids", "in", "zip", "(", "batch_ids", ",", "rel_ids", ",", "pos_ids", ")", "]", "\n", "batch_feats", "=", "np", ".", "array", "(", "batch_feats", ")", "\n", "return", "batch_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.model.JointModel.__init__": [[16, 28], ["torch.Module.__init__", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Threshold", "torch.Threshold"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n", "", "def", "identity", "(", "x", ")", ":", "\n", "    ", "return", "x", "\n", "\n", "", "class", "BertForMultitask", "(", "PreTrainedBertModel", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "config", ",", "cls_num_labels", "=", "2", ",", "tok_num_labels", "=", "2", ",", "tok2id", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.model.JointModel.run_tagger": [[30, 49], ["model.JointModel.tagging_model", "model.JointModel.masked_fill", "model.JointModel.token_sm", "model.JointModel.tok_threshold", "model.JointModel.time_sm"], "methods", ["None"], ["self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "\n", "self", ".", "cls_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "cls_classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "cls_num_labels", ")", "\n", "\n", "self", ".", "tok_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "tok_classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "tok_num_labels", ")", "\n", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "rel_ids", "=", "None", ",", "pos_ids", "=", "None", ",", "categories", "=", "None", ",", "pre_len", "=", "None", ")", ":", "\n", "        ", "global", "ARGS", "\n", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "\n", "cls_logits", "=", "self", ".", "cls_classifier", "(", "pooled_output", ")", "\n", "cls_logits", "=", "self", ".", "cls_dropout", "(", "cls_logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.model.JointModel.forward": [[51, 69], ["model.JointModel.debias_model", "model.JointModel.run_tagger"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.model.JointModel.run_tagger"], ["tok_logits", "=", "self", ".", "tok_classifier", "(", "sequence_output", ")", "\n", "tok_logits", "=", "self", ".", "tok_dropout", "(", "tok_logits", ")", "\n", "\n", "return", "cls_logits", ",", "tok_logits", "\n", "\n", "\n", "\n", "\n", "", "", "class", "ConcatCombine", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "feature_size", ",", "out_size", ",", "layers", ",", "\n", "dropout_prob", ",", "small", "=", "False", ",", "pre_enrich", "=", "False", ",", "activation", "=", "False", ",", "\n", "include_categories", "=", "False", ",", "category_emb", "=", "False", ",", "\n", "add_category_emb", "=", "False", ")", ":", "\n", "        ", "super", "(", "ConcatCombine", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "include_categories", "=", "include_categories", "\n", "self", ".", "add_category_emb", "=", "add_category_emb", "\n", "if", "include_categories", ":", "\n", "            ", "if", "category_emb", "and", "not", "add_category_emb", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.model.JointModel.inference_forward": [[70, 143], ["model.JointModel.debias_model.run_encoder", "model.JointModel.run_tagger", "src_outputs.repeat.repeat.repeat", "pre_mask.repeat.repeat.repeat", "pre_len.repeat.repeat.repeat", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "range", "model.JointModel.inference_forward_greedy", "h_t.repeat", "c_t.repeat", "is_bias_probs.repeat.repeat.repeat", "shared.beam.Beam", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "word_probs[].squeeze().view().transpose", "range", "get_top_hyp().contiguous().view", "[].detach().cpu().numpy", "is_bias_probs.repeat.repeat.detach().cpu().numpy", "range", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "b.sort_best", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack().transpose.append", "torch.stack().transpose.append", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.JointModel.debias_model.run_decoder", "beams[].advance", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "word_probs[].squeeze().view", "get_top_hyp().contiguous", "[].detach().cpu", "is_bias_probs.repeat.repeat.detach().cpu", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "b.get_hyp", "word_probs[].squeeze", "model.JointModel.inference_forward.get_top_hyp"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.run_encoder", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.model.JointModel.run_tagger", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.inference_forward_greedy", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.sort_best", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.PointerSeq2Seq.run_decoder", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.advance", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.get_hyp"], ["                ", "feature_size", "*=", "2", "\n", "", "elif", "not", "category_emb", ":", "\n", "                ", "feature_size", "+=", "43", "\n", "\n", "", "", "if", "layers", "==", "1", ":", "\n", "            ", "self", ".", "out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "+", "feature_size", ",", "out_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "elif", "layers", "==", "2", ":", "\n", "            ", "waist_size", "=", "min", "(", "hidden_size", ",", "feature_size", ")", "if", "small", "else", "max", "(", "hidden_size", ",", "feature_size", ")", "\n", "if", "activation", ":", "\n", "                ", "self", ".", "out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "+", "feature_size", ",", "waist_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "waist_size", ",", "out_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "+", "feature_size", ",", "waist_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "waist_size", ",", "out_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "", "if", "pre_enrich", ":", "\n", "            ", "if", "activation", ":", "\n", "                ", "self", ".", "enricher", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "feature_size", ",", "feature_size", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "enricher", "=", "nn", ".", "Linear", "(", "feature_size", ",", "feature_size", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "enricher", "=", "None", "\n", "# manually set cuda because module doesn't see these combiners for bottom ", "\n", "", "if", "CUDA", ":", "\n", "            ", "self", ".", "out", "=", "self", ".", "out", ".", "cuda", "(", ")", "\n", "if", "self", ".", "enricher", ":", "\n", "                ", "self", ".", "enricher", "=", "self", ".", "enricher", ".", "cuda", "(", ")", "\n", "\n", "", "", "", "def", "forward", "(", "self", ",", "hidden", ",", "features", ",", "categories", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "include_categories", ":", "\n", "            ", "categories", "=", "categories", ".", "unsqueeze", "(", "1", ")", "\n", "categories", "=", "categories", ".", "repeat", "(", "1", ",", "features", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "if", "self", ".", "add_category_emb", ":", "\n", "                ", "features", "=", "features", "+", "categories", "\n", "", "else", ":", "\n", "                ", "features", "=", "torch", ".", "cat", "(", "(", "features", ",", "categories", ")", ",", "-", "1", ")", "\n", "\n", "", "", "if", "self", ".", "enricher", "is", "not", "None", ":", "\n", "            ", "features", "=", "self", ".", "enricher", "(", "features", ")", "\n", "\n", "", "return", "self", ".", "out", "(", "torch", ".", "cat", "(", "(", "hidden", ",", "features", ")", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "\n", "", "", "class", "AddCombine", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "hidden_dim", ",", "feat_dim", ",", "layers", ",", "dropout_prob", ",", "small", "=", "False", ",", "\n", "out_dim", "=", "-", "1", ",", "pre_enrich", "=", "False", ",", "include_categories", "=", "False", ",", "\n", "category_emb", "=", "False", ",", "add_category_emb", "=", "False", ")", ":", "\n", "        ", "super", "(", "AddCombine", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "include_categories", "=", "include_categories", "\n", "if", "include_categories", ":", "\n", "            ", "feat_dim", "+=", "43", "\n", "\n", "", "if", "layers", "==", "1", ":", "\n", "            ", "self", ".", "expand", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "feat_dim", ",", "hidden_dim", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "else", ":", "\n", "            ", "waist_size", "=", "min", "(", "feat_dim", ",", "hidden_dim", ")", "if", "small", "else", "max", "(", "feat_dim", ",", "hidden_dim", ")", "\n", "self", ".", "expand", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "feat_dim", ",", "waist_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "waist_size", ",", "hidden_dim", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.model.JointModel.inference_forward_greedy": [[144, 170], ["torch.autograd.Variable", "torch.autograd.Variable", "range", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.cat.cuda", "torch.cat.cuda", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.detach().cpu().numpy", "torch.cat.detach().cpu().numpy", "is_bias_probs.detach().cpu().numpy", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.JointModel.forward", "torch.max", "torch.max", "torch.max", "torch.max", "next_preds.unsqueeze", "torch.cat.detach().cpu", "torch.cat.detach().cpu", "is_bias_probs.detach().cpu", "range", "pre_id.size", "torch.cat.detach", "torch.cat.detach", "is_bias_probs.detach"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.decoders.StackedAttentionLSTM.forward"], ["\n", "", "if", "out_dim", ">", "0", ":", "\n", "            ", "self", ".", "out", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "out_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "out", "=", "None", "\n", "\n", "", "if", "pre_enrich", ":", "\n", "            ", "self", ".", "enricher", "=", "nn", ".", "Linear", "(", "feature_size", ",", "feature_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "enricher", "=", "None", "\n", "\n", "# manually set cuda because module doesn't see these combiners for bottom         ", "\n", "", "if", "CUDA", ":", "\n", "            ", "self", ".", "expand", "=", "self", ".", "expand", ".", "cuda", "(", ")", "\n", "if", "out_dim", ">", "0", ":", "\n", "                ", "self", ".", "out", "=", "self", ".", "out", ".", "cuda", "(", ")", "\n", "", "if", "self", ".", "enricher", "is", "not", "None", ":", "\n", "                ", "self", ".", "enricher", "=", "self", ".", "enricher", ".", "cuda", "(", ")", "\n", "\n", "", "", "", "def", "forward", "(", "self", ",", "hidden", ",", "feat", ",", "categories", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "include_categories", ":", "\n", "            ", "categories", "=", "categories", ".", "unsqueeze", "(", "1", ")", "\n", "categories", "=", "categories", ".", "repeat", "(", "1", ",", "features", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "if", "self", ".", "add_category_emb", ":", "\n", "                ", "features", "=", "features", "+", "categories", "\n", "", "else", ":", "\n", "                ", "features", "=", "torch", ".", "cat", "(", "(", "features", ",", "categories", ")", ",", "-", "1", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.model.JointModel.save": [[171, 173], ["torch.save", "torch.save", "torch.save", "torch.save", "model.JointModel.state_dict"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save"], ["\n", "", "", "if", "self", ".", "enricher", "is", "not", "None", ":", "\n", "            ", "feat", "=", "self", ".", "enricher", "(", "feat", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.model.JointModel.load": [[174, 176], ["model.JointModel.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load"], ["\n", "", "combined", "=", "self", ".", "expand", "(", "feat", ")", "+", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.utils.train_for_epoch": [[15, 53], ["model.train", "enumerate", "tqdm.tqdm", "model", "debias_loss_fn", "debias_loss_fn.backward", "torch.utils.clip_grad_norm_", "optimizer.step", "model.zero_grad", "losses.append", "tuple", "tagging_loss_fn", "seq2seq.coverage_loss", "model.parameters", "debias_loss_fn.detach().cpu().numpy", "x.cuda", "debias_loss_fn.detach().cpu", "debias_loss_fn.detach"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.coverage_loss", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters"], ["\n", "if", "ARGS", ".", "tagger_from_debiaser", ":", "\n", "        ", "parameters", "=", "list", "(", "model", ".", "cls_classifier", ".", "parameters", "(", ")", ")", "+", "list", "(", "\n", "model", ".", "tok_classifier", ".", "parameters", "(", ")", ")", "\n", "parameters", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "parameters", ")", ")", "\n", "return", "optim", ".", "Adam", "(", "parameters", ",", "lr", "=", "ARGS", ".", "learning_rate", ")", "\n", "", "else", ":", "\n", "        ", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "param_optimizer", "=", "list", "(", "filter", "(", "lambda", "name_param", ":", "name_param", "[", "1", "]", ".", "requires_grad", ",", "param_optimizer", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "return", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "0.1", ",", "\n", "t_total", "=", "num_train_steps", ")", "\n", "\n", "\n", "", "", "def", "build_loss_fn", "(", "debias_weight", "=", "None", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "if", "debias_weight", "is", "None", ":", "\n", "        ", "debias_weight", "=", "ARGS", ".", "debias_weight", "\n", "\n", "", "weight_mask", "=", "torch", ".", "ones", "(", "ARGS", ".", "num_tok_labels", ")", "\n", "weight_mask", "[", "-", "1", "]", "=", "0", "\n", "\n", "if", "CUDA", ":", "\n", "        ", "weight_mask", "=", "weight_mask", ".", "cuda", "(", ")", "\n", "criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ")", ".", "cuda", "(", ")", "\n", "per_tok_criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ",", "reduction", "=", "'none'", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ")", "\n", "per_tok_criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ",", "reduction", "=", "'none'", ")", "\n", "\n", "\n", "", "def", "cross_entropy_loss", "(", "logits", ",", "labels", ",", "apply_mask", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.joint.utils.run_eval": [[55, 99], ["open", "enumerate", "open.close", "tqdm.tqdm", "min", "seq2seq.dump_outputs", "tok2id.items", "tuple", "torch.no_grad", "torch.no_grad", "model.inference_forward", "pre_id.detach().cpu().numpy", "post_out_id.detach().cpu().numpy", "pre_tok_label_id.detach().cpu().numpy", "pre_len[].detach().cpu().numpy", "x.cuda", "pre_id.detach().cpu", "post_out_id.detach().cpu", "pre_tok_label_id.detach().cpu", "pre_len[].detach().cpu", "pre_id.detach", "post_out_id.detach", "pre_tok_label_id.detach", "pre_len[].detach"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.dump_outputs", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.inference_forward"], ["logits", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "ARGS", ".", "num_tok_labels", ")", ",", "\n", "labels", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ".", "type", "(", "'torch.cuda.LongTensor'", "if", "CUDA", "else", "'torch.LongTensor'", ")", ")", "\n", "\n", "", "def", "weighted_cross_entropy_loss", "(", "logits", ",", "labels", ",", "apply_mask", "=", "None", ")", ":", "\n", "# weight mask = where to apply weight (post_tok_label_id from the batch)", "\n", "        ", "weights", "=", "apply_mask", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "weights", "=", "(", "(", "debias_weight", "-", "1", ")", "*", "weights", ")", "+", "1.0", "\n", "\n", "per_tok_losses", "=", "per_tok_criterion", "(", "\n", "logits", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "ARGS", ".", "num_tok_labels", ")", ",", "\n", "labels", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ".", "type", "(", "'torch.cuda.LongTensor'", "if", "CUDA", "else", "'torch.LongTensor'", ")", ")", "\n", "per_tok_losses", "=", "per_tok_losses", "*", "weights", "\n", "\n", "loss", "=", "torch", ".", "mean", "(", "per_tok_losses", "[", "torch", ".", "nonzero", "(", "per_tok_losses", ")", "]", ".", "squeeze", "(", ")", ")", "\n", "\n", "return", "loss", "\n", "\n", "", "if", "debias_weight", "==", "1.0", ":", "\n", "        ", "loss_fn", "=", "cross_entropy_loss", "\n", "", "else", ":", "\n", "        ", "loss_fn", "=", "weighted_cross_entropy_loss", "\n", "\n", "", "return", "loss_fn", "\n", "\n", "\n", "", "def", "softmax", "(", "x", ",", "axis", "=", "None", ")", ":", "\n", "    ", "x", "=", "x", "-", "x", ".", "max", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "y", "=", "np", ".", "exp", "(", "x", ")", "\n", "return", "y", "/", "y", ".", "sum", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n", "\n", "", "def", "run_inference", "(", "model", ",", "eval_dataloader", ",", "loss_fn", ",", "tokenizer", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "out", "=", "{", "\n", "'input_toks'", ":", "[", "]", ",", "\n", "'post_toks'", ":", "[", "]", ",", "\n", "\n", "'tok_loss'", ":", "[", "]", ",", "\n", "'tok_logits'", ":", "[", "]", ",", "\n", "'tok_probs'", ":", "[", "]", ",", "\n", "'tok_labels'", ":", "[", "]", ",", "\n", "\n", "'labeling_hits'", ":", "[", "]", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.PositionwiseFeedForward.__init__": [[25, 33], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "d_ff", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "PositionwiseFeedForward", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "w_1", "=", "nn", ".", "Linear", "(", "d_model", ",", "d_ff", ")", "\n", "self", ".", "w_2", "=", "nn", ".", "Linear", "(", "d_ff", ",", "d_model", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "1e-6", ")", "\n", "self", ".", "dropout_1", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "dropout_2", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.PositionwiseFeedForward.forward": [[34, 47], ["transformer_decoder.PositionwiseFeedForward.dropout_1", "transformer_decoder.PositionwiseFeedForward.dropout_2", "transformer_decoder.PositionwiseFeedForward.relu", "transformer_decoder.PositionwiseFeedForward.w_2", "transformer_decoder.PositionwiseFeedForward.w_1", "transformer_decoder.PositionwiseFeedForward.layer_norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Layer definition.\n\n        Args:\n            x: ``(batch_size, input_len, model_dim)``\n\n        Returns:\n            (FloatTensor): Output ``(batch_size, input_len, model_dim)``.\n        \"\"\"", "\n", "\n", "inter", "=", "self", ".", "dropout_1", "(", "self", ".", "relu", "(", "self", ".", "w_1", "(", "self", ".", "layer_norm", "(", "x", ")", ")", ")", ")", "\n", "output", "=", "self", ".", "dropout_2", "(", "self", ".", "w_2", "(", "inter", ")", ")", "\n", "return", "output", "+", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.MultiHeadedAttention.__init__": [[90, 115], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Embedding", "torch.Embedding"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "head_count", ",", "model_dim", ",", "dropout", "=", "0.1", ",", "\n", "max_relative_positions", "=", "0", ")", ":", "\n", "        ", "assert", "model_dim", "%", "head_count", "==", "0", "\n", "self", ".", "dim_per_head", "=", "model_dim", "//", "head_count", "\n", "self", ".", "model_dim", "=", "model_dim", "\n", "\n", "super", "(", "MultiHeadedAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "head_count", "=", "head_count", "\n", "\n", "self", ".", "linear_keys", "=", "nn", ".", "Linear", "(", "model_dim", ",", "\n", "head_count", "*", "self", ".", "dim_per_head", ")", "\n", "self", ".", "linear_values", "=", "nn", ".", "Linear", "(", "model_dim", ",", "\n", "head_count", "*", "self", ".", "dim_per_head", ")", "\n", "self", ".", "linear_query", "=", "nn", ".", "Linear", "(", "model_dim", ",", "\n", "head_count", "*", "self", ".", "dim_per_head", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "final_linear", "=", "nn", ".", "Linear", "(", "model_dim", ",", "model_dim", ")", "\n", "\n", "self", ".", "max_relative_positions", "=", "max_relative_positions", "\n", "\n", "if", "max_relative_positions", ">", "0", ":", "\n", "            ", "vocab_size", "=", "max_relative_positions", "*", "2", "+", "1", "\n", "self", ".", "relative_positions_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "vocab_size", ",", "self", ".", "dim_per_head", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.MultiHeadedAttention.forward": [[116, 268], ["shape.size", "shape.size", "transformer_decoder.MultiHeadedAttention.size", "transformer_decoder.MultiHeadedAttention.forward.shape"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "key", ",", "value", ",", "query", ",", "mask", "=", "None", ",", "\n", "layer_cache", "=", "None", ",", "type", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Compute the context vector and the attention vectors.\n\n        Args:\n           key (FloatTensor): set of `key_len`\n               key vectors ``(batch, key_len, dim)``\n           value (FloatTensor): set of `key_len`\n               value vectors ``(batch, key_len, dim)``\n           query (FloatTensor): set of `query_len`\n               query vectors  ``(batch, query_len, dim)``\n           mask: binary mask indicating which keys have\n               non-zero attention ``(batch, query_len, key_len)``\n        Returns:\n           (FloatTensor, FloatTensor):\n\n           * output context vectors ``(batch, query_len, dim)``\n           * one of the attention vectors ``(batch, query_len, key_len)``\n        \"\"\"", "\n", "# CHECKS", "\n", "# batch, k_len, d = key.size()", "\n", "# batch_, k_len_, d_ = value.size()", "\n", "# aeq(batch, batch_)", "\n", "# aeq(k_len, k_len_)", "\n", "# aeq(d, d_)", "\n", "# batch_, q_len, d_ = query.size()", "\n", "# aeq(batch, batch_)", "\n", "# aeq(d, d_)", "\n", "# aeq(self.model_dim % 8, 0)", "\n", "# if mask is not None:", "\n", "#    batch_, q_len_, k_len_ = mask.size()", "\n", "#    aeq(batch_, batch)", "\n", "#    aeq(k_len_, k_len)", "\n", "#    aeq(q_len_ == q_len)", "\n", "# END CHECKS", "\n", "\n", "batch_size", "=", "key", ".", "size", "(", "0", ")", "\n", "dim_per_head", "=", "self", ".", "dim_per_head", "\n", "head_count", "=", "self", ".", "head_count", "\n", "key_len", "=", "key", ".", "size", "(", "1", ")", "\n", "query_len", "=", "query", ".", "size", "(", "1", ")", "\n", "device", "=", "key", ".", "device", "\n", "\n", "def", "shape", "(", "x", ")", ":", "\n", "            ", "\"\"\"Projection.\"\"\"", "\n", "return", "x", ".", "view", "(", "batch_size", ",", "-", "1", ",", "head_count", ",", "dim_per_head", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "", "def", "unshape", "(", "x", ")", ":", "\n", "            ", "\"\"\"Compute context.\"\"\"", "\n", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "-", "1", ",", "head_count", "*", "dim_per_head", ")", "\n", "\n", "# 1) Project key, value, and query.", "\n", "", "if", "layer_cache", "is", "not", "None", ":", "\n", "            ", "if", "type", "==", "\"self\"", ":", "\n", "                ", "query", ",", "key", ",", "value", "=", "self", ".", "linear_query", "(", "query", ")", ",", "self", ".", "linear_keys", "(", "query", ")", ",", "self", ".", "linear_values", "(", "query", ")", "\n", "key", "=", "shape", "(", "key", ")", "\n", "value", "=", "shape", "(", "value", ")", "\n", "if", "layer_cache", "[", "\"self_keys\"", "]", "is", "not", "None", ":", "\n", "                    ", "key", "=", "torch", ".", "cat", "(", "\n", "(", "layer_cache", "[", "\"self_keys\"", "]", ".", "to", "(", "device", ")", ",", "key", ")", ",", "\n", "dim", "=", "2", ")", "\n", "", "if", "layer_cache", "[", "\"self_values\"", "]", "is", "not", "None", ":", "\n", "                    ", "value", "=", "torch", ".", "cat", "(", "\n", "(", "layer_cache", "[", "\"self_values\"", "]", ".", "to", "(", "device", ")", ",", "value", ")", ",", "\n", "dim", "=", "2", ")", "\n", "", "layer_cache", "[", "\"self_keys\"", "]", "=", "key", "\n", "layer_cache", "[", "\"self_values\"", "]", "=", "value", "\n", "", "elif", "type", "==", "\"context\"", ":", "\n", "                ", "query", "=", "self", ".", "linear_query", "(", "query", ")", "\n", "if", "layer_cache", "[", "\"memory_keys\"", "]", "is", "None", ":", "\n", "                    ", "key", ",", "value", "=", "self", ".", "linear_keys", "(", "key", ")", ",", "self", ".", "linear_values", "(", "value", ")", "\n", "key", "=", "shape", "(", "key", ")", "\n", "value", "=", "shape", "(", "value", ")", "\n", "", "else", ":", "\n", "                    ", "key", ",", "value", "=", "layer_cache", "[", "\"memory_keys\"", "]", ",", "layer_cache", "[", "\"memory_values\"", "]", "\n", "", "layer_cache", "[", "\"memory_keys\"", "]", "=", "key", "\n", "layer_cache", "[", "\"memory_values\"", "]", "=", "value", "\n", "", "", "else", ":", "\n", "            ", "key", "=", "self", ".", "linear_keys", "(", "key", ")", "\n", "value", "=", "self", ".", "linear_values", "(", "value", ")", "\n", "query", "=", "self", ".", "linear_query", "(", "query", ")", "\n", "key", "=", "shape", "(", "key", ")", "\n", "value", "=", "shape", "(", "value", ")", "\n", "\n", "", "if", "self", ".", "max_relative_positions", ">", "0", "and", "type", "==", "\"self\"", ":", "\n", "            ", "key_len", "=", "key", ".", "size", "(", "2", ")", "\n", "# 1 or key_len x key_len", "\n", "relative_positions_matrix", "=", "generate_relative_positions_matrix", "(", "\n", "key_len", ",", "self", ".", "max_relative_positions", ",", "\n", "cache", "=", "True", "if", "layer_cache", "is", "not", "None", "else", "False", ")", "\n", "#  1 or key_len x key_len x dim_per_head", "\n", "relations_keys", "=", "self", ".", "relative_positions_embeddings", "(", "\n", "relative_positions_matrix", ".", "to", "(", "device", ")", ")", "\n", "#  1 or key_len x key_len x dim_per_head", "\n", "relations_values", "=", "self", ".", "relative_positions_embeddings", "(", "\n", "relative_positions_matrix", ".", "to", "(", "device", ")", ")", "\n", "\n", "", "query", "=", "shape", "(", "query", ")", "\n", "\n", "key_len", "=", "key", ".", "size", "(", "2", ")", "\n", "query_len", "=", "query", ".", "size", "(", "2", ")", "\n", "\n", "# 2) Calculate and scale scores.", "\n", "query", "=", "query", "/", "math", ".", "sqrt", "(", "dim_per_head", ")", "\n", "# batch x num_heads x query_len x key_len", "\n", "query_key", "=", "torch", ".", "matmul", "(", "query", ",", "key", ".", "transpose", "(", "2", ",", "3", ")", ")", "\n", "\n", "if", "self", ".", "max_relative_positions", ">", "0", "and", "type", "==", "\"self\"", ":", "\n", "            ", "scores", "=", "query_key", "+", "relative_matmul", "(", "query", ",", "relations_keys", ",", "True", ")", "\n", "", "else", ":", "\n", "            ", "scores", "=", "query_key", "\n", "", "scores", "=", "scores", ".", "float", "(", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", "# [B, 1, 1, T_values]", "\n", "scores", "=", "scores", ".", "masked_fill", "(", "mask", ",", "-", "1e18", ")", "\n", "\n", "# 3) Apply attention dropout and compute context vectors.", "\n", "", "attn", "=", "self", ".", "softmax", "(", "scores", ")", ".", "to", "(", "query", ".", "dtype", ")", "\n", "drop_attn", "=", "self", ".", "dropout", "(", "attn", ")", "\n", "\n", "context_original", "=", "torch", ".", "matmul", "(", "drop_attn", ",", "value", ")", "\n", "\n", "if", "self", ".", "max_relative_positions", ">", "0", "and", "type", "==", "\"self\"", ":", "\n", "            ", "context", "=", "unshape", "(", "context_original", "\n", "+", "relative_matmul", "(", "drop_attn", ",", "\n", "relations_values", ",", "\n", "False", ")", ")", "\n", "", "else", ":", "\n", "            ", "context", "=", "unshape", "(", "context_original", ")", "\n", "\n", "", "output", "=", "self", ".", "final_linear", "(", "context", ")", "\n", "# CHECK", "\n", "# batch_, q_len_, d_ = output.size()", "\n", "# aeq(q_len, q_len_)", "\n", "# aeq(batch, batch_)", "\n", "# aeq(d, d_)", "\n", "\n", "# Return one attn", "\n", "top_attn", "=", "attn", ".", "view", "(", "batch_size", ",", "head_count", ",", "\n", "query_len", ",", "key_len", ")", "[", ":", ",", "0", ",", ":", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "\n", "return", "output", ",", "top_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.AverageAttention.__init__": [[319, 327], ["torch.Module.__init__", "transformer_decoder.PositionwiseFeedForward", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "model_dim", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "self", ".", "model_dim", "=", "model_dim", "\n", "\n", "super", "(", "AverageAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "average_layer", "=", "PositionwiseFeedForward", "(", "model_dim", ",", "model_dim", ",", "\n", "dropout", ")", "\n", "self", ".", "gating_layer", "=", "nn", ".", "Linear", "(", "model_dim", "*", "2", ",", "model_dim", "*", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.AverageAttention.cumulative_average_mask": [[328, 349], ["torch.tril", "torch.tril", "torch.tril", "torch.tril", "mask.unsqueeze().expand", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "weights.transpose", "mask.unsqueeze"], "methods", ["None"], ["", "def", "cumulative_average_mask", "(", "self", ",", "batch_size", ",", "inputs_len", ")", ":", "\n", "        ", "\"\"\"\n        Builds the mask to compute the cumulative average as described in\n        :cite:`DBLP:journals/corr/abs-1805-00631` -- Figure 3\n\n        Args:\n            batch_size (int): batch size\n            inputs_len (int): length of the inputs\n\n        Returns:\n            (FloatTensor):\n\n            * A Tensor of shape ``(batch_size, input_len, input_len)``\n        \"\"\"", "\n", "\n", "triangle", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "inputs_len", ",", "inputs_len", ")", ")", "\n", "weights", "=", "torch", ".", "ones", "(", "1", ",", "inputs_len", ")", "/", "torch", ".", "arange", "(", "\n", "1", ",", "inputs_len", "+", "1", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "mask", "=", "triangle", "*", "weights", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "return", "mask", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "inputs_len", ",", "inputs_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.AverageAttention.cumulative_average": [[350, 380], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "layer_cache[].to"], "methods", ["None"], ["", "def", "cumulative_average", "(", "self", ",", "inputs", ",", "mask_or_step", ",", "\n", "layer_cache", "=", "None", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Computes the cumulative average as described in\n        :cite:`DBLP:journals/corr/abs-1805-00631` -- Equations (1) (5) (6)\n\n        Args:\n            inputs (FloatTensor): sequence to average\n                ``(batch_size, input_len, dimension)``\n            mask_or_step: if cache is set, this is assumed\n                to be the current step of the\n                dynamic decoding. Otherwise, it is the mask matrix\n                used to compute the cumulative average.\n            layer_cache: a dictionary containing the cumulative average\n                of the previous step.\n\n        Returns:\n            a tensor of the same shape and type as ``inputs``.\n        \"\"\"", "\n", "\n", "if", "layer_cache", "is", "not", "None", ":", "\n", "            ", "step", "=", "mask_or_step", "\n", "device", "=", "inputs", ".", "device", "\n", "average_attention", "=", "(", "inputs", "+", "step", "*", "\n", "layer_cache", "[", "\"prev_g\"", "]", ".", "to", "(", "device", ")", ")", "/", "(", "step", "+", "1", ")", "\n", "layer_cache", "[", "\"prev_g\"", "]", "=", "average_attention", "\n", "return", "average_attention", "\n", "", "else", ":", "\n", "            ", "mask", "=", "mask_or_step", "\n", "return", "torch", ".", "matmul", "(", "mask", ",", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.AverageAttention.forward": [[381, 410], ["inputs.size", "inputs.size", "transformer_decoder.AverageAttention.cumulative_average", "transformer_decoder.AverageAttention.average_layer", "transformer_decoder.AverageAttention.gating_layer", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "transformer_decoder.AverageAttention.cumulative_average_mask().to().float", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "transformer_decoder.AverageAttention.cumulative_average_mask().to", "transformer_decoder.AverageAttention.cumulative_average_mask"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.AverageAttention.cumulative_average", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.AverageAttention.cumulative_average_mask"], ["", "", "def", "forward", "(", "self", ",", "inputs", ",", "mask", "=", "None", ",", "layer_cache", "=", "None", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            inputs (FloatTensor): ``(batch_size, input_len, model_dim)``\n\n        Returns:\n            (FloatTensor, FloatTensor):\n\n            * gating_outputs ``(batch_size, input_len, model_dim)``\n            * average_outputs average attention\n                ``(batch_size, input_len, model_dim)``\n        \"\"\"", "\n", "\n", "batch_size", "=", "inputs", ".", "size", "(", "0", ")", "\n", "inputs_len", "=", "inputs", ".", "size", "(", "1", ")", "\n", "\n", "device", "=", "inputs", ".", "device", "\n", "average_outputs", "=", "self", ".", "cumulative_average", "(", "\n", "inputs", ",", "self", ".", "cumulative_average_mask", "(", "batch_size", ",", "\n", "inputs_len", ")", ".", "to", "(", "device", ")", ".", "float", "(", ")", "\n", "if", "layer_cache", "is", "None", "else", "step", ",", "layer_cache", "=", "layer_cache", ")", "\n", "average_outputs", "=", "self", ".", "average_layer", "(", "average_outputs", ")", "\n", "gating_outputs", "=", "self", ".", "gating_layer", "(", "torch", ".", "cat", "(", "(", "inputs", ",", "\n", "average_outputs", ")", ",", "-", "1", ")", ")", "\n", "input_gate", ",", "forget_gate", "=", "torch", ".", "chunk", "(", "gating_outputs", ",", "2", ",", "dim", "=", "2", ")", "\n", "gating_outputs", "=", "torch", ".", "sigmoid", "(", "input_gate", ")", "*", "inputs", "+", "torch", ".", "sigmoid", "(", "forget_gate", ")", "*", "average_outputs", "\n", "\n", "return", "gating_outputs", ",", "average_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.TransformerDecoderLayer.__init__": [[425, 442], ["torch.Module.__init__", "transformer_decoder.MultiHeadedAttention", "transformer_decoder.PositionwiseFeedForward", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "transformer_decoder.MultiHeadedAttention", "transformer_decoder.AverageAttention"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "heads", ",", "d_ff", ",", "dropout", ",", "\n", "self_attn_type", "=", "\"scaled-dot\"", ",", "max_relative_positions", "=", "0", ")", ":", "\n", "        ", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "self_attn_type", "==", "\"scaled-dot\"", ":", "\n", "            ", "self", ".", "self_attn", "=", "MultiHeadedAttention", "(", "\n", "heads", ",", "d_model", ",", "dropout", "=", "dropout", ",", "\n", "max_relative_positions", "=", "max_relative_positions", ")", "\n", "", "elif", "self_attn_type", "==", "\"average\"", ":", "\n", "            ", "self", ".", "self_attn", "=", "AverageAttention", "(", "d_model", ",", "dropout", "=", "dropout", ")", "\n", "\n", "", "self", ".", "context_attn", "=", "MultiHeadedAttention", "(", "\n", "heads", ",", "d_model", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "feed_forward", "=", "PositionwiseFeedForward", "(", "d_model", ",", "d_ff", ",", "dropout", ")", "\n", "self", ".", "layer_norm_1", "=", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "1e-6", ")", "\n", "self", ".", "layer_norm_2", "=", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "1e-6", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.TransformerDecoderLayer.forward": [[443, 489], ["transformer_decoder.TransformerDecoderLayer.layer_norm_1", "isinstance", "transformer_decoder.TransformerDecoderLayer.layer_norm_2", "transformer_decoder.TransformerDecoderLayer.context_attn", "transformer_decoder.TransformerDecoderLayer.feed_forward", "tgt_pad_mask.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "future_mask.triu_().view.triu_().view.triu_().view", "torch.gt", "torch.gt", "torch.gt", "torch.gt", "transformer_decoder.TransformerDecoderLayer.self_attn", "isinstance", "transformer_decoder.TransformerDecoderLayer.drop", "transformer_decoder.TransformerDecoderLayer.self_attn", "transformer_decoder.TransformerDecoderLayer.drop", "future_mask.triu_().view.triu_().view.triu_"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "memory_bank", ",", "src_pad_mask", ",", "tgt_pad_mask", ",", "\n", "layer_cache", "=", "None", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            inputs (FloatTensor): ``(batch_size, 1, model_dim)``\n            memory_bank (FloatTensor): ``(batch_size, src_len, model_dim)``\n            src_pad_mask (LongTensor): ``(batch_size, 1, src_len)``\n            tgt_pad_mask (LongTensor): ``(batch_size, 1, 1)``\n\n        Returns:\n            (FloatTensor, FloatTensor):\n\n            * output ``(batch_size, 1, model_dim)``\n            * attn ``(batch_size, 1, src_len)``\n\n        \"\"\"", "\n", "dec_mask", "=", "None", "\n", "if", "step", "is", "None", ":", "\n", "            ", "tgt_len", "=", "tgt_pad_mask", ".", "size", "(", "-", "1", ")", "\n", "future_mask", "=", "torch", ".", "ones", "(", "\n", "[", "tgt_len", ",", "tgt_len", "]", ",", "\n", "device", "=", "tgt_pad_mask", ".", "device", ",", "\n", "dtype", "=", "torch", ".", "uint8", ")", "\n", "future_mask", "=", "future_mask", ".", "triu_", "(", "1", ")", ".", "view", "(", "1", ",", "tgt_len", ",", "tgt_len", ")", "\n", "dec_mask", "=", "torch", ".", "gt", "(", "tgt_pad_mask", "+", "future_mask", ",", "0", ")", "\n", "\n", "", "input_norm", "=", "self", ".", "layer_norm_1", "(", "inputs", ")", "\n", "if", "isinstance", "(", "self", ".", "self_attn", ",", "MultiHeadedAttention", ")", ":", "\n", "            ", "query", ",", "attn", "=", "self", ".", "self_attn", "(", "input_norm", ",", "input_norm", ",", "input_norm", ",", "\n", "mask", "=", "dec_mask", ",", "\n", "layer_cache", "=", "layer_cache", ",", "\n", "type", "=", "\"self\"", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "self_attn", ",", "AverageAttention", ")", ":", "\n", "            ", "query", ",", "attn", "=", "self", ".", "self_attn", "(", "input_norm", ",", "mask", "=", "dec_mask", ",", "\n", "layer_cache", "=", "layer_cache", ",", "step", "=", "step", ")", "\n", "\n", "", "query", "=", "self", ".", "drop", "(", "query", ")", "+", "inputs", "\n", "\n", "query_norm", "=", "self", ".", "layer_norm_2", "(", "query", ")", "\n", "mid", ",", "attn", "=", "self", ".", "context_attn", "(", "memory_bank", ",", "memory_bank", ",", "query_norm", ",", "\n", "mask", "=", "src_pad_mask", ",", "\n", "layer_cache", "=", "layer_cache", ",", "\n", "type", "=", "\"context\"", ")", "\n", "output", "=", "self", ".", "feed_forward", "(", "self", ".", "drop", "(", "mid", ")", "+", "query", ")", "\n", "\n", "return", "output", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.TransformerDecoder.__init__": [[521, 544], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "transformer_decoder.TransformerDecoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "num_layers", ",", "d_model", ",", "heads", ",", "d_ff", ",", "\n", "copy_attn", ",", "self_attn_type", ",", "dropout", ",", "embeddings", ",", "\n", "max_relative_positions", ")", ":", "\n", "        ", "super", "(", "TransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "\n", "# Decoder State", "\n", "self", ".", "state", "=", "{", "}", "\n", "\n", "self", ".", "transformer_layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "TransformerDecoderLayer", "(", "d_model", ",", "heads", ",", "d_ff", ",", "dropout", ",", "\n", "self_attn_type", "=", "self_attn_type", ",", "\n", "max_relative_positions", "=", "max_relative_positions", ")", "\n", "for", "i", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "\n", "# previously, there was a GlobalAttention module here for copy", "\n", "# attention. But it was never actually used -- the \"copy\" attention", "\n", "# just reuses the context attention.", "\n", "self", ".", "_copy", "=", "copy_attn", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "d_model", ",", "eps", "=", "1e-6", ")", "\n", "\n", "self", ".", "decoder_bridge", "=", "nn", ".", "Linear", "(", "768", ",", "d_model", ")", "# TODO", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.TransformerDecoder.from_opt": [[546, 559], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_opt", "(", "cls", ",", "opt", ",", "embeddings", ")", ":", "\n", "        ", "\"\"\"Alternate constructor.\"\"\"", "\n", "return", "cls", "(", "\n", "opt", ".", "dec_layers", ",", "\n", "opt", ".", "dec_rnn_size", ",", "\n", "opt", ".", "heads", ",", "\n", "opt", ".", "transformer_ff", ",", "\n", "opt", ".", "copy_attn", ",", "\n", "opt", ".", "self_attn_type", ",", "\n", "opt", ".", "dropout", ",", "\n", "embeddings", ",", "\n", "opt", ".", "max_relative_positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.TransformerDecoder.init_state": [[560, 564], ["None"], "methods", ["None"], ["", "def", "init_state", "(", "self", ",", "src", ",", "memory_bank", ",", "enc_hidden", ")", ":", "\n", "        ", "\"\"\"Initialize decoder state.\"\"\"", "\n", "self", ".", "state", "[", "\"src\"", "]", "=", "src", "\n", "self", ".", "state", "[", "\"cache\"", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.TransformerDecoder.map_state": [[565, 577], ["fn", "struct.items", "transformer_decoder.TransformerDecoder.map_state._recursive_map"], "methods", ["None"], ["", "def", "map_state", "(", "self", ",", "fn", ")", ":", "\n", "        ", "def", "_recursive_map", "(", "struct", ",", "batch_dim", "=", "0", ")", ":", "\n", "            ", "for", "k", ",", "v", "in", "struct", ".", "items", "(", ")", ":", "\n", "                ", "if", "v", "is", "not", "None", ":", "\n", "                    ", "if", "isinstance", "(", "v", ",", "dict", ")", ":", "\n", "                        ", "_recursive_map", "(", "v", ")", "\n", "", "else", ":", "\n", "                        ", "struct", "[", "k", "]", "=", "fn", "(", "v", ",", "batch_dim", ")", "\n", "\n", "", "", "", "", "self", ".", "state", "[", "\"src\"", "]", "=", "fn", "(", "self", ".", "state", "[", "\"src\"", "]", ",", "1", ")", "\n", "if", "self", ".", "state", "[", "\"cache\"", "]", "is", "not", "None", ":", "\n", "            ", "_recursive_map", "(", "self", ".", "state", "[", "\"cache\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.TransformerDecoder.detach_state": [[578, 580], ["transformer_decoder.TransformerDecoder.state[].detach"], "methods", ["None"], ["", "", "def", "detach_state", "(", "self", ")", ":", "\n", "        ", "self", ".", "state", "[", "\"src\"", "]", "=", "self", ".", "state", "[", "\"src\"", "]", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.TransformerDecoder.forward": [[581, 616], ["transformer_decoder.TransformerDecoder.decoder_bridge", "mask.unsqueeze", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "transformer_decoder.TransformerDecoder.layer_norm", "transformer_decoder.TransformerDecoder.transpose().contiguous", "attn.transpose().contiguous.transpose().contiguous.transpose().contiguous", "transformer_decoder.TransformerDecoder.transpose().contiguous.transpose", "tgt_pad_mask.cuda.cuda.cuda", "layer", "transformer_decoder.TransformerDecoder.transpose().contiguous.transpose", "transformer_decoder.TransformerDecoder.transpose", "attn.transpose().contiguous.transpose().contiguous.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt_emb", ",", "initial_state", ",", "memory_bank", ",", "mask", ",", "step", "=", "None", ")", ":", "#, step=None, **kwargs):", "\n", "        ", "\"\"\"Decode, possibly stepwise.\"\"\"", "\n", "global", "CUDA", "\n", "\n", "output", "=", "self", ".", "decoder_bridge", "(", "tgt_emb", ")", "# [B, L, D]", "\n", "src_memory_bank", "=", "memory_bank", "# [B, L, D]", "\n", "\n", "src_pad_mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", "# [B, 1, L_src]", "\n", "tgt_pad_mask", "=", "torch", ".", "zeros", "(", "output", ".", "shape", "[", "0", "]", ",", "1", ",", "output", ".", "shape", "[", "1", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "if", "CUDA", ":", "\n", "            ", "tgt_pad_mask", "=", "tgt_pad_mask", ".", "cuda", "(", ")", "\n", "\n", "", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "transformer_layers", ")", ":", "\n", "            ", "layer_cache", "=", "self", ".", "state", "[", "\"cache\"", "]", "[", "\"layer_{}\"", ".", "format", "(", "i", ")", "]", "if", "step", "is", "not", "None", "else", "None", "\n", "output", ",", "attn", "=", "layer", "(", "\n", "output", ",", "\n", "src_memory_bank", ",", "\n", "src_pad_mask", ",", "\n", "tgt_pad_mask", ",", "\n", "layer_cache", "=", "layer_cache", ",", "\n", "step", "=", "step", ")", "\n", "\n", "", "output", "=", "self", ".", "layer_norm", "(", "output", ")", "\n", "dec_outs", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "attns", "=", "{", "\"std\"", ":", "attn", "}", "\n", "if", "self", ".", "_copy", ":", "\n", "            ", "attns", "[", "\"copy\"", "]", "=", "attn", "\n", "\n", "# TODO change the way attns is returned dict => list or tuple (onnx)", "\n", "", "outputs", "=", "dec_outs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "return", "dec_outs", ".", "transpose", "(", "0", ",", "1", ")", ",", "(", "dec_outs", ",", "dec_outs", ")", ",", "attn", ",", "dec_outs", ",", "dec_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.TransformerDecoder._init_cache": [[617, 630], ["memory_bank.size", "memory_bank.size", "enumerate", "isinstance", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "_init_cache", "(", "self", ",", "memory_bank", ")", ":", "\n", "        ", "self", ".", "state", "[", "\"cache\"", "]", "=", "{", "}", "\n", "batch_size", "=", "memory_bank", ".", "size", "(", "1", ")", "\n", "depth", "=", "memory_bank", ".", "size", "(", "-", "1", ")", "\n", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "transformer_layers", ")", ":", "\n", "            ", "layer_cache", "=", "{", "\"memory_keys\"", ":", "None", ",", "\"memory_values\"", ":", "None", "}", "\n", "if", "isinstance", "(", "layer", ".", "self_attn", ",", "AverageAttention", ")", ":", "\n", "                ", "layer_cache", "[", "\"prev_g\"", "]", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "1", ",", "depth", ")", ")", "\n", "", "else", ":", "\n", "                ", "layer_cache", "[", "\"self_keys\"", "]", "=", "None", "\n", "layer_cache", "[", "\"self_values\"", "]", "=", "None", "\n", "", "self", ".", "state", "[", "\"cache\"", "]", "[", "\"layer_{}\"", ".", "format", "(", "i", ")", "]", "=", "layer_cache", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.generate_relative_positions_matrix": [[270, 286], ["torch.clamp", "torch.clamp", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange.unsqueeze().expand().transpose", "range_vec.unsqueeze().expand().transpose.transpose", "torch.arange", "torch.arange", "torch.arange.unsqueeze().expand", "torch.arange.unsqueeze"], "function", ["None"], ["", "", "def", "generate_relative_positions_matrix", "(", "length", ",", "max_relative_positions", ",", "\n", "cache", "=", "False", ")", ":", "\n", "    ", "\"\"\"Generate the clipped relative positions matrix\n       for a given length and maximum relative positions\"\"\"", "\n", "if", "cache", ":", "\n", "        ", "distance_mat", "=", "torch", ".", "arange", "(", "-", "length", "+", "1", ",", "1", ",", "1", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "        ", "range_vec", "=", "torch", ".", "arange", "(", "length", ")", "\n", "range_mat", "=", "range_vec", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "length", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "distance_mat", "=", "range_mat", "-", "range_mat", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "distance_mat_clipped", "=", "torch", ".", "clamp", "(", "distance_mat", ",", "\n", "min", "=", "-", "max_relative_positions", ",", "\n", "max", "=", "max_relative_positions", ")", "\n", "# Shift values to be >= 0", "\n", "final_mat", "=", "distance_mat_clipped", "+", "max_relative_positions", "\n", "return", "final_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.transformer_decoder.relative_matmul": [[290, 305], ["x.permute", "x.permute.reshape", "torch.matmul.reshape", "x_tz_matmul.reshape.permute", "z.transpose", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul"], "function", ["None"], ["", "def", "relative_matmul", "(", "x", ",", "z", ",", "transpose", ")", ":", "\n", "    ", "\"\"\"Helper function for relative positions attention.\"\"\"", "\n", "batch_size", "=", "x", ".", "shape", "[", "0", "]", "\n", "heads", "=", "x", ".", "shape", "[", "1", "]", "\n", "length", "=", "x", ".", "shape", "[", "2", "]", "\n", "x_t", "=", "x", ".", "permute", "(", "2", ",", "0", ",", "1", ",", "3", ")", "\n", "x_t_r", "=", "x_t", ".", "reshape", "(", "length", ",", "heads", "*", "batch_size", ",", "-", "1", ")", "\n", "if", "transpose", ":", "\n", "        ", "z_t", "=", "z", ".", "transpose", "(", "1", ",", "2", ")", "\n", "x_tz_matmul", "=", "torch", ".", "matmul", "(", "x_t_r", ",", "z_t", ")", "\n", "", "else", ":", "\n", "        ", "x_tz_matmul", "=", "torch", ".", "matmul", "(", "x_t_r", ",", "z", ")", "\n", "", "x_tz_matmul_r", "=", "x_tz_matmul", ".", "reshape", "(", "length", ",", "batch_size", ",", "heads", ",", "-", "1", ")", "\n", "x_tz_matmul_r_t", "=", "x_tz_matmul_r", ".", "permute", "(", "1", ",", "2", ",", "0", ",", "3", ")", "\n", "return", "x_tz_matmul_r_t", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.BilinearAttention.__init__": [[29, 51], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["        ", "super", "(", "BertForMultitask", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "\n", "self", ".", "cls_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "cls_classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "cls_num_labels", ")", "\n", "\n", "self", ".", "tok_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "tok_classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "tok_num_labels", ")", "\n", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "rel_ids", "=", "None", ",", "pos_ids", "=", "None", ",", "categories", "=", "None", ",", "pre_len", "=", "None", ")", ":", "\n", "        ", "global", "ARGS", "\n", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "\n", "cls_logits", "=", "self", ".", "cls_classifier", "(", "pooled_output", ")", "\n", "cls_logits", "=", "self", ".", "cls_dropout", "(", "cls_logits", ")", "\n", "\n", "# NOTE -- dropout is after proj, which is non-standard", "\n", "tok_logits", "=", "self", ".", "tok_classifier", "(", "sequence_output", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.BilinearAttention.forward": [[52, 97], ["model.BilinearAttention.query_in_projection", "model.BilinearAttention.score_fn", "model.BilinearAttention.softmax", "model.BilinearAttention.unsqueeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.BilinearAttention.tanh", "model.BilinearAttention.key_in_projection", "attn_scores.masked_fill.masked_fill.masked_fill", "model.BilinearAttention.out_projection", "model.BilinearAttention.key_in_projection", "model.BilinearAttention.cov_projection", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "float"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.softmax"], ["tok_logits", "=", "self", ".", "tok_dropout", "(", "tok_logits", ")", "\n", "\n", "return", "cls_logits", ",", "tok_logits", "\n", "\n", "\n", "\n", "\n", "", "", "class", "ConcatCombine", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "feature_size", ",", "out_size", ",", "layers", ",", "\n", "dropout_prob", ",", "small", "=", "False", ",", "pre_enrich", "=", "False", ",", "activation", "=", "False", ",", "\n", "include_categories", "=", "False", ",", "category_emb", "=", "False", ",", "\n", "add_category_emb", "=", "False", ")", ":", "\n", "        ", "super", "(", "ConcatCombine", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "include_categories", "=", "include_categories", "\n", "self", ".", "add_category_emb", "=", "add_category_emb", "\n", "if", "include_categories", ":", "\n", "            ", "if", "category_emb", "and", "not", "add_category_emb", ":", "\n", "                ", "feature_size", "*=", "2", "\n", "", "elif", "not", "category_emb", ":", "\n", "                ", "feature_size", "+=", "43", "\n", "\n", "", "", "if", "layers", "==", "1", ":", "\n", "            ", "self", ".", "out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "+", "feature_size", ",", "out_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "elif", "layers", "==", "2", ":", "\n", "            ", "waist_size", "=", "min", "(", "hidden_size", ",", "feature_size", ")", "if", "small", "else", "max", "(", "hidden_size", ",", "feature_size", ")", "\n", "if", "activation", ":", "\n", "                ", "self", ".", "out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "+", "feature_size", ",", "waist_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "waist_size", ",", "out_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "out", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_size", "+", "feature_size", ",", "waist_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "waist_size", ",", "out_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "", "if", "pre_enrich", ":", "\n", "            ", "if", "activation", ":", "\n", "                ", "self", ".", "enricher", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "feature_size", ",", "feature_size", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.BilinearAttention.dot": [[99, 105], ["torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "query.unsqueeze"], "methods", ["None"], ["                ", "self", ".", "enricher", "=", "nn", ".", "Linear", "(", "feature_size", ",", "feature_size", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "enricher", "=", "None", "\n", "# manually set cuda because module doesn't see these combiners for bottom ", "\n", "", "if", "CUDA", ":", "\n", "            ", "self", ".", "out", "=", "self", ".", "out", ".", "cuda", "(", ")", "\n", "if", "self", ".", "enricher", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.BilinearAttention.bahdanau": [[107, 113], ["model.BilinearAttention.v_att().squeeze", "model.BilinearAttention.v_att", "model.BilinearAttention.score_tanh", "query.unsqueeze"], "methods", ["None"], ["\n", "", "", "", "def", "forward", "(", "self", ",", "hidden", ",", "features", ",", "categories", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "include_categories", ":", "\n", "            ", "categories", "=", "categories", ".", "unsqueeze", "(", "1", ")", "\n", "categories", "=", "categories", ".", "repeat", "(", "1", ",", "features", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "if", "self", ".", "add_category_emb", ":", "\n", "                ", "features", "=", "features", "+", "categories", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.LSTMEncoder.__init__": [[117, 131], ["torch.Module.__init__", "torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["", "", "if", "self", ".", "enricher", "is", "not", "None", ":", "\n", "            ", "features", "=", "self", ".", "enricher", "(", "features", ")", "\n", "\n", "", "return", "self", ".", "out", "(", "torch", ".", "cat", "(", "(", "hidden", ",", "features", ")", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "\n", "", "", "class", "AddCombine", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "hidden_dim", ",", "feat_dim", ",", "layers", ",", "dropout_prob", ",", "small", "=", "False", ",", "\n", "out_dim", "=", "-", "1", ",", "pre_enrich", "=", "False", ",", "include_categories", "=", "False", ",", "\n", "category_emb", "=", "False", ",", "add_category_emb", "=", "False", ")", ":", "\n", "        ", "super", "(", "AddCombine", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "include_categories", "=", "include_categories", "\n", "if", "include_categories", ":", "\n", "            ", "feat_dim", "+=", "43", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.LSTMEncoder.init_state": [[132, 150], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda"], "methods", ["None"], ["\n", "", "if", "layers", "==", "1", ":", "\n", "            ", "self", ".", "expand", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "feat_dim", ",", "hidden_dim", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "", "else", ":", "\n", "            ", "waist_size", "=", "min", "(", "feat_dim", ",", "hidden_dim", ")", "if", "small", "else", "max", "(", "feat_dim", ",", "hidden_dim", ")", "\n", "self", ".", "expand", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "feat_dim", ",", "waist_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ",", "\n", "nn", ".", "Linear", "(", "waist_size", ",", "hidden_dim", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout_prob", ")", ")", "\n", "\n", "", "if", "out_dim", ">", "0", ":", "\n", "            ", "self", ".", "out", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "out_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "out", "=", "None", "\n", "\n", "", "if", "pre_enrich", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.LSTMEncoder.forward": [[152, 167], ["model.LSTMEncoder.init_state", "model.LSTMEncoder.lstm", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "src_embedding.size"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.encoders.LSTMEncoder.init_state"], ["", "else", ":", "\n", "            ", "self", ".", "enricher", "=", "None", "\n", "\n", "# manually set cuda because module doesn't see these combiners for bottom         ", "\n", "", "if", "CUDA", ":", "\n", "            ", "self", ".", "expand", "=", "self", ".", "expand", ".", "cuda", "(", ")", "\n", "if", "out_dim", ">", "0", ":", "\n", "                ", "self", ".", "out", "=", "self", ".", "out", ".", "cuda", "(", ")", "\n", "", "if", "self", ".", "enricher", "is", "not", "None", ":", "\n", "                ", "self", ".", "enricher", "=", "self", ".", "enricher", ".", "cuda", "(", ")", "\n", "\n", "", "", "", "def", "forward", "(", "self", ",", "hidden", ",", "feat", ",", "categories", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "include_categories", ":", "\n", "            ", "categories", "=", "categories", ".", "unsqueeze", "(", "1", ")", "\n", "categories", "=", "categories", ".", "repeat", "(", "1", ",", "features", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "if", "self", ".", "add_category_emb", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.AttentionalLSTM.__init__": [[173, 184], ["torch.Module.__init__", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "model.BilinearAttention"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["            ", "feat", "=", "self", ".", "enricher", "(", "feat", ")", "\n", "\n", "", "combined", "=", "self", ".", "expand", "(", "feat", ")", "+", "hidden", "\n", "\n", "if", "self", ".", "out", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "out", "(", "combined", ")", "\n", "\n", "", "return", "combined", "\n", "\n", "\n", "", "", "class", "BertForMultitaskWithFeaturesOnTop", "(", "PreTrainedBertModel", ")", ":", "\n", "    ", "\"\"\" stick the features on top of the model \"\"\"", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.AttentionalLSTM.forward": [[186, 220], ["input.transpose.transpose.transpose", "range", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "output.transpose.transpose.transpose", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "input.transpose.transpose.size", "model.AttentionalLSTM.cell", "input.transpose.transpose.size", "model.AttentionalLSTM.attention_layer", "output.transpose.transpose.append", "torch.stack().squeeze.append", "torch.stack().squeeze.append", "torch.stack().squeeze.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "output.transpose.transpose.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "output[].size", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["        ", "super", "(", "BertForMultitaskWithFeaturesOnTop", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "global", "ARGS", "\n", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "\n", "self", ".", "featurizer", "=", "features", ".", "Featurizer", "(", "\n", "tok2id", ",", "lexicon_feature_bits", "=", "ARGS", ".", "lexicon_feature_bits", ")", "\n", "# TODO -- don't hardcode this...", "\n", "nfeats", "=", "90", "if", "ARGS", ".", "lexicon_feature_bits", "==", "1", "else", "118", "\n", "\n", "if", "ARGS", ".", "extra_features_method", "==", "'concat'", ":", "\n", "            ", "self", ".", "tok_classifier", "=", "ConcatCombine", "(", "\n", "config", ".", "hidden_size", ",", "nfeats", ",", "tok_num_labels", ",", "\n", "ARGS", ".", "combiner_layers", ",", "config", ".", "hidden_dropout_prob", ",", "\n", "ARGS", ".", "small_waist", ",", "pre_enrich", "=", "ARGS", ".", "pre_enrich", ",", "\n", "activation", "=", "ARGS", ".", "activation_hidden", ",", "\n", "include_categories", "=", "ARGS", ".", "concat_categories", ",", "\n", "category_emb", "=", "ARGS", ".", "category_emb", ",", "\n", "add_category_emb", "=", "ARGS", ".", "add_category_emb", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "tok_classifier", "=", "AddCombine", "(", "\n", "config", ".", "hidden_size", ",", "nfeats", ",", "ARGS", ".", "combiner_layers", ",", "\n", "config", ".", "hidden_dropout_prob", ",", "ARGS", ".", "small_waist", ",", "\n", "out_dim", "=", "tok_num_labels", ",", "pre_enrich", "=", "ARGS", ".", "pre_enrich", ",", "\n", "include_categories", "=", "ARGS", ".", "concat_categories", ",", "\n", "category_emb", "=", "ARGS", ".", "category_emb", ",", "\n", "add_category_emb", "=", "ARGS", ".", "add_category_emb", ")", "\n", "\n", "", "self", ".", "cls_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "cls_classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "cls_num_labels", ")", "\n", "\n", "self", ".", "category_emb", "=", "ARGS", ".", "category_emb", "\n", "if", "ARGS", ".", "category_emb", ":", "\n", "            ", "self", ".", "category_embeddings", "=", "nn", ".", "Embedding", "(", "43", ",", "nfeats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.StackedAttentionLSTM.__init__": [[225, 235], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "range", "cell_class", "model.StackedAttentionLSTM.add_module", "model.StackedAttentionLSTM.layers.append"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["labels", "=", "None", ",", "rel_ids", "=", "None", ",", "pos_ids", "=", "None", ",", "categories", "=", "None", ",", "pre_len", "=", "None", ")", ":", "\n", "        ", "global", "ARGS", "\n", "global", "CUDA", "\n", "\n", "features", "=", "self", ".", "featurizer", ".", "featurize_batch", "(", "\n", "input_ids", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "rel_ids", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "pos_ids", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "padded_len", "=", "input_ids", ".", "shape", "[", "1", "]", ")", "\n", "features", "=", "torch", ".", "tensor", "(", "features", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "if", "CUDA", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.StackedAttentionLSTM.forward": [[237, 257], ["enumerate", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "layer", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "len", "model.StackedAttentionLSTM.dropout"], "methods", ["None"], ["\n", "", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "\n", "pooled_output", "=", "self", ".", "cls_dropout", "(", "pooled_output", ")", "\n", "cls_logits", "=", "self", ".", "cls_classifier", "(", "pooled_output", ")", "\n", "\n", "if", "ARGS", ".", "category_emb", ":", "\n", "            ", "categories", "=", "self", ".", "category_embeddings", "(", "\n", "categories", ".", "max", "(", "-", "1", ")", "[", "1", "]", ".", "type", "(", "\n", "'torch.cuda.LongTensor'", "if", "CUDA", "else", "'torch.LongTensor'", ")", ")", "\n", "\n", "", "tok_logits", "=", "self", ".", "tok_classifier", "(", "sequence_output", ",", "features", ",", "categories", ")", "\n", "\n", "return", "cls_logits", ",", "tok_logits", "\n", "\n", "\n", "", "", "class", "TaggerFromDebiaser", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "cls_num_labels", "=", "2", ",", "tok_num_labels", "=", "2", ",", "tok2id", "=", "None", ")", ":", "\n", "        ", "super", "(", "TaggerFromDebiaser", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.__init__": [[261, 335], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "BertModel.from_pretrained.LSTMEncoder", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.LogSoftmax", "torch.LogSoftmax", "torch.LogSoftmax", "BertModel.from_pretrained.Seq2Seq.init_weights", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.Linear", "torch.Linear", "torch.Linear", "seq2seq.TransformerDecoder", "BertModel.from_pretrained.StackedAttentionLSTM", "pytorch_pretrained_bert.modeling.BertModel.from_pretrained", "BertModel.from_pretrained.Seq2Seq.embeddings.parameters", "BertModel.from_pretrained.Seq2Seq.enrich_input.cuda"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.SeqModel.init_weights", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters"], ["if", "ARGS", ".", "pointer_generator", ":", "\n", "            ", "self", ".", "debias_model", "=", "seq2seq_model", ".", "PointerSeq2Seq", "(", "\n", "vocab_size", "=", "len", "(", "tok2id", ")", ",", "hidden_size", "=", "ARGS", ".", "hidden_size", ",", "\n", "emb_dim", "=", "768", ",", "dropout", "=", "0.2", ",", "tok2id", "=", "tok2id", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "debias_model", "=", "seq2seq_model", ".", "Seq2Seq", "(", "\n", "vocab_size", "=", "len", "(", "tok2id", ")", ",", "hidden_size", "=", "ARGS", ".", "hidden_size", ",", "\n", "emb_dim", "=", "768", ",", "dropout", "=", "0.2", ",", "tok2id", "=", "tok2id", ")", "\n", "\n", "", "assert", "ARGS", ".", "debias_checkpoint", "\n", "print", "(", "'LOADING DEBIASER FROM '", "+", "ARGS", ".", "debias_checkpoint", ")", "\n", "self", ".", "debias_model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "ARGS", ".", "debias_checkpoint", ")", ")", "\n", "print", "(", "'...DONE'", ")", "\n", "\n", "self", ".", "cls_classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "ARGS", ".", "hidden_size", ",", "ARGS", ".", "hidden_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "0.1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "ARGS", ".", "hidden_size", ",", "cls_num_labels", ")", ",", "\n", "nn", ".", "Dropout", "(", "0.1", ")", ")", "\n", "\n", "self", ".", "tok_classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "ARGS", ".", "hidden_size", ",", "ARGS", ".", "hidden_size", ")", ",", "\n", "nn", ".", "Dropout", "(", "0.1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "ARGS", ".", "hidden_size", ",", "tok_num_labels", ")", ",", "\n", "nn", ".", "Dropout", "(", "0.1", ")", ")", "\n", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "labels", "=", "None", ",", "rel_ids", "=", "None", ",", "pos_ids", "=", "None", ",", "categories", "=", "None", ",", "pre_len", "=", "None", ")", ":", "\n", "\n", "        ", "pre_mask", "=", "1.0", "-", "attention_mask", "\n", "\n", "# src_outputs is [batch_size, sequence_length, hidden_size].", "\n", "src_outputs", ",", "h_t", ",", "_", "=", "self", ".", "debias_model", ".", "run_encoder", "(", "\n", "input_ids", ",", "pre_len", ",", "pre_mask", ")", "\n", "\n", "cls_logits", "=", "self", ".", "cls_classifier", "(", "h_t", ")", "\n", "tok_logits", "=", "self", ".", "tok_classifier", "(", "src_outputs", ")", "\n", "\n", "return", "cls_logits", ",", "tok_logits", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.init_weights": [[336, 341], ["model.Seq2Seq.parameters", "param.data.uniform_"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.run_encoder": [[342, 376], ["model.Seq2Seq.embeddings", "model.Seq2Seq.encoder", "model.Seq2Seq.bridge", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "model.Seq2Seq.h_t_projection", "model.Seq2Seq.c_t_projection", "model.Seq2Seq.encoder", "model.Seq2Seq.bridge", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "final_hidden_states.size", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.run_decoder": [[377, 403], ["model.Seq2Seq.embeddings", "model.Seq2Seq.decoder", "tgt_outputs.contiguous().view", "model.Seq2Seq.output_projection", "logits.view.view.view", "model.Seq2Seq.softmax", "model.Seq2Seq.log_softmax", "model.Seq2Seq.enricher().repeat", "tok_dist.unsqueeze", "tgt_outputs.contiguous", "tgt_outputs.size", "tgt_outputs.size", "tgt_outputs.size", "logits.view.view.size", "model.Seq2Seq.enricher", "tgt_outputs.size", "tgt_outputs.size"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.softmax"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.forward": [[404, 409], ["model.Seq2Seq.run_encoder", "model.Seq2Seq.run_decoder"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.run_encoder", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.PointerSeq2Seq.run_decoder"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.inference_forward": [[411, 465], ["model.Seq2Seq.run_encoder", "src_outputs.repeat.repeat.repeat", "pre_mask.repeat.repeat.repeat", "pre_len.repeat.repeat.repeat", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "torch.stack().t().contiguous().view", "range", "[].detach().cpu().numpy", "model.Seq2Seq.inference_forward_greedy", "h_t.repeat", "c_t.repeat", "tok_dist.repeat.repeat.repeat", "shared.beam.Beam", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "torch.stack().transpose", "word_probs[].squeeze().view().transpose", "range", "get_top_hyp().contiguous().view", "range", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "torch.stack().t().contiguous", "b.sort_best", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack().transpose.append", "torch.stack().transpose.append", "torch.stack().transpose.append", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.Seq2Seq.run_decoder", "beams[].advance", "[].detach().cpu", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "word_probs[].squeeze().view", "get_top_hyp().contiguous", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "[].detach", "b.get_hyp", "word_probs[].squeeze", "model.Seq2Seq.inference_forward.get_top_hyp"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.run_encoder", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.inference_forward_greedy", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.sort_best", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.PointerSeq2Seq.run_decoder", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.advance", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.get_hyp"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.inference_forward_greedy": [[467, 487], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "range", "torch.cat.detach().cpu().numpy", "torch.cat.detach().cpu().numpy", "torch.cat.detach().cpu().numpy", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.cat.cuda", "torch.cat.cuda", "torch.cat.cuda", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.Seq2Seq.forward", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.cat.detach().cpu", "torch.cat.detach().cpu", "torch.cat.detach().cpu", "next_preds.unsqueeze", "range", "torch.cat.detach", "torch.cat.detach", "torch.cat.detach", "pre_id.size"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.decoders.StackedAttentionLSTM.forward"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.save": [[488, 490], ["torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "model.Seq2Seq.state_dict"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.load": [[491, 493], ["model.Seq2Seq.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.PointerSeq2Seq.__init__": [[497, 508], ["model.Seq2Seq.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.PointerSeq2Seq.run_decoder": [[509, 583], ["model.PointerSeq2Seq.embeddings", "range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "probs.permute.permute.permute", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "model.PointerSeq2Seq.enricher().repeat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "tgt_emb[].unsqueeze", "model.PointerSeq2Seq.decoder", "output_i.squeeze.squeeze.squeeze", "attn.squeeze.squeeze.squeeze", "raw_hidden.squeeze", "model.PointerSeq2Seq.p_gen_W", "model.PointerSeq2Seq.p_gen_sigmoid", "gen_probs.scatter_add_", "tgt_output_probs.append", "attns.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "tok_dist.unsqueeze", "coverage.cuda.cuda.cuda", "coverage_vecs.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.PointerSeq2Seq.softmax", "h_tilde_i.squeeze", "ci.squeeze", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "model.PointerSeq2Seq.enricher", "coverage.cuda.cuda.unsqueeze().clone", "coverage.cuda.cuda.clone", "model.PointerSeq2Seq.output_projection", "attn_ctx.squeeze", "ci.squeeze", "tgt_emb[].unsqueeze.squeeze", "coverage.cuda.cuda.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.softmax"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.bleu_stats": [[18, 34], ["stats.append", "stats.append", "range", "len", "len", "collections.Counter", "collections.Counter", "stats.append", "stats.append", "max", "max", "tuple", "tuple", "range", "range", "sum", "len", "len", "len"], "function", ["None"], ["model", ".", "tok_classifier", ".", "parameters", "(", ")", ")", "\n", "parameters", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "parameters", ")", ")", "\n", "return", "optim", ".", "Adam", "(", "parameters", ",", "lr", "=", "ARGS", ".", "learning_rate", ")", "\n", "", "else", ":", "\n", "        ", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "param_optimizer", "=", "list", "(", "filter", "(", "lambda", "name_param", ":", "name_param", "[", "1", "]", ".", "requires_grad", ",", "param_optimizer", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "return", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "0.1", ",", "\n", "t_total", "=", "num_train_steps", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.bleu": [[36, 45], ["math.exp", "len", "sum", "list", "min", "filter", "math.log", "zip", "float", "float"], "function", ["None"], ["    ", "global", "ARGS", "\n", "\n", "if", "debias_weight", "is", "None", ":", "\n", "        ", "debias_weight", "=", "ARGS", ".", "debias_weight", "\n", "\n", "", "weight_mask", "=", "torch", ".", "ones", "(", "ARGS", ".", "num_tok_labels", ")", "\n", "weight_mask", "[", "-", "1", "]", "=", "0", "\n", "\n", "if", "CUDA", ":", "\n", "        ", "weight_mask", "=", "weight_mask", ".", "cuda", "(", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.get_bleu": [[47, 53], ["numpy.array", "zip", "numpy.array", "utils.bleu", "utils.bleu_stats"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.bleu", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.bleu_stats"], ["per_tok_criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ",", "reduction", "=", "'none'", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ")", "\n", "per_tok_criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ",", "reduction", "=", "'none'", ")", "\n", "\n", "\n", "", "def", "cross_entropy_loss", "(", "logits", ",", "labels", ",", "apply_mask", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.build_loss_fn": [[56, 96], ["torch.ones", "torch.ones", "torch.ones", "torch.NLLLoss", "torch.NLLLoss", "weight_mask.cuda.cuda", "criterion.cuda.cuda", "per_tok_criterion.cuda.cuda", "criterion.cuda.", "apply_mask.contiguous().view", "per_tok_criterion.cuda.", "torch.mean", "torch.mean", "torch.mean", "log_probs.contiguous().view", "labels.contiguous().view", "log_probs.contiguous().view", "labels.contiguous().view", "per_tok_losses[].squeeze", "apply_mask.contiguous", "log_probs.contiguous", "labels.contiguous", "log_probs.contiguous", "labels.contiguous", "torch.nonzero", "torch.nonzero", "torch.nonzero"], "function", ["None"], ["labels", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ".", "type", "(", "'torch.cuda.LongTensor'", "if", "CUDA", "else", "'torch.LongTensor'", ")", ")", "\n", "\n", "", "def", "weighted_cross_entropy_loss", "(", "logits", ",", "labels", ",", "apply_mask", "=", "None", ")", ":", "\n", "# weight mask = where to apply weight (post_tok_label_id from the batch)", "\n", "        ", "weights", "=", "apply_mask", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "weights", "=", "(", "(", "debias_weight", "-", "1", ")", "*", "weights", ")", "+", "1.0", "\n", "\n", "per_tok_losses", "=", "per_tok_criterion", "(", "\n", "logits", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "ARGS", ".", "num_tok_labels", ")", ",", "\n", "labels", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ".", "type", "(", "'torch.cuda.LongTensor'", "if", "CUDA", "else", "'torch.LongTensor'", ")", ")", "\n", "per_tok_losses", "=", "per_tok_losses", "*", "weights", "\n", "\n", "loss", "=", "torch", ".", "mean", "(", "per_tok_losses", "[", "torch", ".", "nonzero", "(", "per_tok_losses", ")", "]", ".", "squeeze", "(", ")", ")", "\n", "\n", "return", "loss", "\n", "\n", "", "if", "debias_weight", "==", "1.0", ":", "\n", "        ", "loss_fn", "=", "cross_entropy_loss", "\n", "", "else", ":", "\n", "        ", "loss_fn", "=", "weighted_cross_entropy_loss", "\n", "\n", "", "return", "loss_fn", "\n", "\n", "\n", "", "def", "softmax", "(", "x", ",", "axis", "=", "None", ")", ":", "\n", "    ", "x", "=", "x", "-", "x", ".", "max", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "y", "=", "np", ".", "exp", "(", "x", ")", "\n", "return", "y", "/", "y", ".", "sum", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n", "\n", "", "def", "run_inference", "(", "model", ",", "eval_dataloader", ",", "loss_fn", ",", "tokenizer", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "out", "=", "{", "\n", "'input_toks'", ":", "[", "]", ",", "\n", "'post_toks'", ":", "[", "]", ",", "\n", "\n", "'tok_loss'", ":", "[", "]", ",", "\n", "'tok_logits'", ":", "[", "]", ",", "\n", "'tok_probs'", ":", "[", "]", ",", "\n", "'tok_labels'", ":", "[", "]", ",", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.build_optimizer": [[98, 124], ["list", "list", "pytorch_pretrained_bert.optimization.BertAdam", "list", "list", "torch.Adam", "model.named_parameters", "filter", "model.parameters", "filter", "any", "any"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters"], ["'labeling_hits'", ":", "[", "]", "\n", "}", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "eval_dataloader", ")", ")", ":", "\n", "        ", "if", "ARGS", ".", "debug_skip", "and", "step", ">", "2", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "CUDA", ":", "\n", "            ", "batch", "=", "tuple", "(", "x", ".", "cuda", "(", ")", "for", "x", "in", "batch", ")", "\n", "\n", "", "(", "\n", "pre_id", ",", "pre_mask", ",", "pre_len", ",", "\n", "post_in_id", ",", "post_out_id", ",", "\n", "tok_label_id", ",", "_", ",", "\n", "rel_ids", ",", "pos_ids", ",", "categories", "\n", ")", "=", "batch", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", ",", "tok_logits", "=", "model", "(", "pre_id", ",", "attention_mask", "=", "1.0", "-", "pre_mask", ",", "\n", "rel_ids", "=", "rel_ids", ",", "pos_ids", "=", "pos_ids", ",", "categories", "=", "categories", ",", "\n", "pre_len", "=", "pre_len", ")", "\n", "tok_loss", "=", "loss_fn", "(", "tok_logits", ",", "tok_label_id", ",", "apply_mask", "=", "tok_label_id", ")", "\n", "", "out", "[", "'input_toks'", "]", "+=", "[", "tokenizer", ".", "convert_ids_to_tokens", "(", "seq", ")", "for", "seq", "in", "pre_id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", "\n", "out", "[", "'post_toks'", "]", "+=", "[", "tokenizer", ".", "convert_ids_to_tokens", "(", "seq", ")", "for", "seq", "in", "post_in_id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", "\n", "out", "[", "'tok_loss'", "]", ".", "append", "(", "float", "(", "tok_loss", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", ")", "\n", "logits", "=", "tok_logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "labels", "=", "tok_label_id", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.coverage_loss": [[126, 132], ["zip", "torch.sum", "torch.sum", "torch.sum", "torch.min", "torch.min", "torch.min"], "function", ["None"], ["out", "[", "'tok_labels'", "]", "+=", "labels", ".", "tolist", "(", ")", "\n", "out", "[", "'tok_probs'", "]", "+=", "to_probs", "(", "logits", ",", "pre_len", ")", "\n", "out", "[", "'labeling_hits'", "]", "+=", "tag_hits", "(", "logits", ",", "labels", ")", "\n", "\n", "", "return", "out", "\n", "\n", "", "def", "train_for_epoch", "(", "model", ",", "train_dataloader", ",", "loss_fn", ",", "optimizer", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.train_for_epoch": [[134, 169], ["enumerate", "tqdm.tqdm", "model", "loss_fn", "loss_fn.backward", "torch.utils.clip_grad_norm_", "optimizer.step", "model.zero_grad", "losses.append", "tuple", "utils.coverage_loss", "model.parameters", "loss_fn.detach().cpu().numpy", "x.cuda", "loss_fn.detach().cpu", "loss_fn.detach"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.coverage_loss", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters"], ["\n", "losses", "=", "[", "]", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ")", ")", ":", "\n", "        ", "if", "ARGS", ".", "debug_skip", "and", "step", ">", "2", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "CUDA", ":", "\n", "            ", "batch", "=", "tuple", "(", "x", ".", "cuda", "(", ")", "for", "x", "in", "batch", ")", "\n", "", "(", "\n", "pre_id", ",", "pre_mask", ",", "pre_len", ",", "\n", "post_in_id", ",", "post_out_id", ",", "\n", "tok_label_id", ",", "_", ",", "\n", "rel_ids", ",", "pos_ids", ",", "categories", "\n", ")", "=", "batch", "\n", "_", ",", "tok_logits", "=", "model", "(", "pre_id", ",", "attention_mask", "=", "1.0", "-", "pre_mask", ",", "\n", "rel_ids", "=", "rel_ids", ",", "pos_ids", "=", "pos_ids", ",", "categories", "=", "categories", ",", "\n", "pre_len", "=", "pre_len", ")", "\n", "loss", "=", "loss_fn", "(", "tok_logits", ",", "tok_label_id", ",", "apply_mask", "=", "tok_label_id", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "losses", ".", "append", "(", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "return", "losses", "\n", "\n", "", "def", "to_probs", "(", "logits", ",", "lens", ")", ":", "\n", "    ", "per_tok_probs", "=", "softmax", "(", "np", ".", "array", "(", "logits", ")", "[", ":", ",", ":", ",", ":", "2", "]", ",", "axis", "=", "2", ")", "\n", "pos_scores", "=", "per_tok_probs", "[", ":", ",", ":", ",", "-", "1", "]", "\n", "\n", "out", "=", "[", "]", "\n", "for", "score_seq", ",", "l", "in", "zip", "(", "pos_scores", ",", "lens", ")", ":", "\n", "        ", "out", ".", "append", "(", "score_seq", "[", ":", "l", "]", ".", "tolist", "(", ")", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.dump_outputs": [[171, 220], ["zip", "print", "print", "print", "print", "print", "print", "print", "print", "preds_for_bleu.append", "golds_for_bleu.append", "srcs_for_bleu.append", "len", "src_seq.encode", "gold_seq.encode", "pred_seq.encode", "list", "list", "list", "list", "out_hits.append", "out_hits.append", "pred_seq.split", "gold_seq.split", "src_seq.split", "simplediff.diff", "simplediff.diff", "gold_seq.index", "pred_seq.index"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff"], ["    ", "global", "ARGS", "\n", "\n", "# get rid of padding idx", "\n", "[", "probs", ",", "labels", "]", "=", "list", "(", "zip", "(", "*", "[", "(", "p", ",", "l", ")", "for", "p", ",", "l", "in", "zip", "(", "probs", ",", "labels", ")", "if", "l", "!=", "ARGS", ".", "num_tok_labels", "-", "1", "]", ")", ")", "\n", "probs_indices", "=", "list", "(", "zip", "(", "np", ".", "array", "(", "probs", ")", "[", ":", ",", "1", "]", ",", "range", "(", "len", "(", "labels", ")", ")", ")", ")", "\n", "[", "_", ",", "top_indices", "]", "=", "list", "(", "zip", "(", "*", "sorted", "(", "probs_indices", ",", "reverse", "=", "True", ")", "[", ":", "top", "]", ")", ")", "\n", "if", "sum", "(", "[", "labels", "[", "i", "]", "for", "i", "in", "top_indices", "]", ")", ">", "0", ":", "\n", "        ", "return", "1", "\n", "", "else", ":", "\n", "        ", "return", "0", "\n", "\n", "", "", "def", "tag_hits", "(", "logits", ",", "tok_labels", ",", "top", "=", "1", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "probs", "=", "softmax", "(", "np", ".", "array", "(", "logits", ")", "[", ":", ",", ":", ",", ":", "ARGS", ".", "num_tok_labels", "-", "1", "]", ",", "axis", "=", "2", ")", "\n", "\n", "hits", "=", "[", "\n", "is_ranking_hit", "(", "prob_dist", ",", "tok_label", ",", "top", "=", "top", ")", "\n", "for", "prob_dist", ",", "tok_label", "in", "zip", "(", "probs", ",", "tok_labels", ")", "\n", "]", "\n", "return", "hits", "\n", "", ""]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.run_eval": [[222, 270], ["torch.ones", "torch.ones", "torch.ones", "torch.CrossEntropyLoss", "open", "enumerate", "open.close", "len", "tqdm.tqdm", "min", "utils.dump_outputs", "tok2id.items", "tuple", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.inference_forward", "pre_id.detach().cpu().numpy", "post_out_id.detach().cpu().numpy", "pre_tok_label_id.detach().cpu().numpy", "pre_len[].detach().cpu().numpy", "x.cuda", "pre_id.detach().cpu", "post_out_id.detach().cpu", "pre_tok_label_id.detach().cpu", "pre_len[].detach().cpu", "pre_id.detach", "post_out_id.detach", "pre_tok_label_id.detach", "pre_len[].detach"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.utils.dump_outputs", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.seq2seq.model.Seq2Seq.inference_forward"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.__init__": [[9, 30], ["beam.Beam.tt.FloatTensor().zero_", "beam.Beam.tt.LongTensor().fill_", "beam.Beam.tt.FloatTensor", "beam.Beam.tt.LongTensor"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "size", ",", "tok2id", ",", "cuda", "=", "False", ")", ":", "\n", "        ", "\"\"\"Initialize params.\"\"\"", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "done", "=", "False", "\n", "self", ".", "pad", "=", "tok2id", "[", "'[PAD]'", "]", "\n", "self", ".", "bos", "=", "tok2id", "[", "'\u884c'", "]", "\n", "self", ".", "eos", "=", "tok2id", "[", "'\u6b62'", "]", "\n", "self", ".", "tt", "=", "torch", ".", "cuda", "if", "cuda", "else", "torch", "\n", "\n", "# The score for each translation on the beam.", "\n", "self", ".", "scores", "=", "self", ".", "tt", ".", "FloatTensor", "(", "size", ")", ".", "zero_", "(", ")", "\n", "\n", "# The backpointers at each time-step.", "\n", "self", ".", "prevKs", "=", "[", "]", "\n", "\n", "# The outputs at each time-step. [time, beam]", "\n", "self", ".", "nextYs", "=", "[", "self", ".", "tt", ".", "LongTensor", "(", "size", ")", ".", "fill_", "(", "self", ".", "pad", ")", "]", "\n", "self", ".", "nextYs", "[", "0", "]", "[", "0", "]", "=", "self", ".", "bos", "# TODO CHANGED THIS", "\n", "\n", "# The attentions (matrix) for each time.", "\n", "self", ".", "attn", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.get_current_state": [[32, 35], ["None"], "methods", ["None"], ["", "def", "get_current_state", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get state of beam.\"\"\"", "\n", "return", "self", ".", "nextYs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.get_current_origin": [[37, 40], ["None"], "methods", ["None"], ["", "def", "get_current_origin", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the backpointer to the beam at this step.\"\"\"", "\n", "return", "self", ".", "prevKs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.advance": [[51, 77], ["workd_lk.size", "beam_lk.view", "beam_lk.view.topk", "beam.Beam.prevKs.append", "beam.Beam.nextYs.append", "len", "beam.Beam.scores.unsqueeze().expand_as", "beam.Beam.scores.unsqueeze"], "methods", ["None"], ["", "def", "advance", "(", "self", ",", "workd_lk", ")", ":", "\n", "        ", "\"\"\"Advance the beam.\"\"\"", "\n", "num_words", "=", "workd_lk", ".", "size", "(", "1", ")", "\n", "\n", "# Sum the previous scores.", "\n", "if", "len", "(", "self", ".", "prevKs", ")", ">", "0", ":", "\n", "            ", "beam_lk", "=", "workd_lk", "+", "self", ".", "scores", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "workd_lk", ")", "\n", "", "else", ":", "\n", "            ", "beam_lk", "=", "workd_lk", "[", "0", "]", "\n", "\n", "", "flat_beam_lk", "=", "beam_lk", ".", "view", "(", "-", "1", ")", "\n", "\n", "bestScores", ",", "bestScoresId", "=", "flat_beam_lk", ".", "topk", "(", "self", ".", "size", ",", "0", ",", "True", ",", "True", ")", "\n", "self", ".", "scores", "=", "bestScores", "\n", "\n", "# bestScoresId is flattened beam x word array, so calculate which", "\n", "# word and beam each score came from", "\n", "prev_k", "=", "bestScoresId", "/", "num_words", "\n", "self", ".", "prevKs", ".", "append", "(", "prev_k", ")", "\n", "self", ".", "nextYs", ".", "append", "(", "bestScoresId", "-", "prev_k", "*", "num_words", ")", "\n", "\n", "# End condition is when top-of-beam is EOS.", "\n", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "0", "]", "==", "self", ".", "eos", ":", "\n", "            ", "self", ".", "done", "=", "True", "\n", "\n", "", "return", "self", ".", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.sort_best": [[78, 81], ["torch.sort"], "methods", ["None"], ["", "def", "sort_best", "(", "self", ")", ":", "\n", "        ", "\"\"\"Sort the beam.\"\"\"", "\n", "return", "torch", ".", "sort", "(", "self", ".", "scores", ",", "0", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.get_best": [[83, 87], ["beam.Beam.sort_best"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.sort_best"], ["", "def", "get_best", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the most likely candidate.\"\"\"", "\n", "scores", ",", "ids", "=", "self", ".", "sort_best", "(", ")", "\n", "return", "scores", "[", "1", "]", ",", "ids", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.beam.Beam.get_hyp": [[98, 107], ["range", "hyp.append", "len"], "methods", ["None"], ["", "def", "get_hyp", "(", "self", ",", "k", ")", ":", "\n", "        ", "\"\"\"Get hypotheses.\"\"\"", "\n", "hyp", "=", "[", "]", "\n", "# -2 to include start tok", "\n", "for", "j", "in", "range", "(", "len", "(", "self", ".", "prevKs", ")", "-", "1", ",", "-", "2", ",", "-", "1", ")", ":", "\n", "            ", "hyp", ".", "append", "(", "self", ".", "nextYs", "[", "j", "+", "1", "]", "[", "k", "]", ")", "\n", "k", "=", "self", ".", "prevKs", "[", "j", "]", "[", "k", "]", "\n", "\n", "", "return", "hyp", "[", ":", ":", "-", "1", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.softmax": [[41, 45], ["numpy.exp", "x.max", "np.exp.sum"], "function", ["None"], ["def", "softmax", "(", "x", ",", "axis", "=", "None", ")", ":", "\n", "    ", "x", "=", "x", "-", "x", ".", "max", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "y", "=", "np", ".", "exp", "(", "x", ")", "\n", "return", "y", "/", "y", ".", "sum", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.get_tok_labels": [[47, 62], ["len", "len", "len", "len"], "function", ["None"], ["", "def", "get_tok_labels", "(", "s_diff", ")", ":", "\n", "    ", "pre_tok_labels", "=", "[", "]", "\n", "post_tok_labels", "=", "[", "]", "\n", "for", "tag", ",", "chunk", "in", "s_diff", ":", "\n", "        ", "if", "tag", "==", "'='", ":", "\n", "            ", "pre_tok_labels", "+=", "[", "0", "]", "*", "len", "(", "chunk", ")", "\n", "post_tok_labels", "+=", "[", "0", "]", "*", "len", "(", "chunk", ")", "\n", "", "elif", "tag", "==", "'-'", ":", "\n", "            ", "pre_tok_labels", "+=", "[", "1", "]", "*", "len", "(", "chunk", ")", "\n", "", "elif", "tag", "==", "'+'", ":", "\n", "            ", "post_tok_labels", "+=", "[", "1", "]", "*", "len", "(", "chunk", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "return", "pre_tok_labels", ",", "post_tok_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.noise_seq": [[64, 100], ["random.shuffle", "numpy.random.random", "len", "tmp.append", "tmp.append", "numpy.random.random", "sorted", "enumerate", "numpy.random.random"], "function", ["None"], ["", "def", "noise_seq", "(", "seq", ",", "drop_prob", "=", "0.25", ",", "shuf_dist", "=", "3", ",", "drop_set", "=", "None", ",", "keep_bigrams", "=", "False", ")", ":", "\n", "# from https://arxiv.org/pdf/1711.00043.pdf", "\n", "    ", "def", "perm", "(", "i", ")", ":", "\n", "        ", "return", "i", "[", "0", "]", "+", "(", "shuf_dist", "+", "1", ")", "*", "np", ".", "random", ".", "random", "(", ")", "\n", "\n", "", "if", "drop_set", "==", "None", ":", "\n", "        ", "dropped_seq", "=", "[", "x", "for", "x", "in", "seq", "if", "np", ".", "random", ".", "random", "(", ")", ">", "drop_prob", "]", "\n", "", "else", ":", "\n", "        ", "dropped_seq", "=", "[", "x", "for", "x", "in", "seq", "if", "not", "(", "x", "in", "drop_set", "and", "np", ".", "random", ".", "random", "(", ")", "<", "drop_prob", ")", "]", "\n", "\n", "", "if", "keep_bigrams", ":", "\n", "        ", "i", "=", "0", "\n", "original", "=", "' '", ".", "join", "(", "seq", ")", "\n", "tmp", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "dropped_seq", ")", "-", "1", ":", "\n", "            ", "if", "' '", ".", "join", "(", "dropped_seq", "[", "i", ":", "i", "+", "2", "]", ")", "in", "original", ":", "\n", "                ", "tmp", ".", "append", "(", "dropped_seq", "[", "i", ":", "i", "+", "2", "]", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                ", "tmp", ".", "append", "(", "[", "dropped_seq", "[", "i", "]", "]", ")", "\n", "i", "+=", "1", "\n", "\n", "", "", "dropped_seq", "=", "tmp", "\n", "\n", "# global shuffle", "\n", "", "if", "shuf_dist", "==", "-", "1", ":", "\n", "        ", "shuffle", "(", "dropped_seq", ")", "\n", "# local shuffle", "\n", "", "elif", "shuf_dist", ">", "0", ":", "\n", "        ", "dropped_seq", "=", "[", "x", "for", "_", ",", "x", "in", "sorted", "(", "enumerate", "(", "dropped_seq", ")", ",", "key", "=", "perm", ")", "]", "\n", "# shuf_dist of 0 = no shuffle", "\n", "\n", "", "if", "keep_bigrams", ":", "\n", "        ", "dropped_seq", "=", "[", "z", "for", "y", "in", "dropped_seq", "for", "z", "in", "y", "]", "\n", "\n", "", "return", "dropped_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.get_examples": [[102, 224], ["collections.defaultdict", "enumerate", "print", "set", "open", "next", "tqdm.tqdm", "line.strip().split", "pre.strip().split", "post.strip().split", "rels.strip().split.strip().split", "pos.strip().split.strip().split", "simplediff.diff", "data.get_tok_labels", "data.get_examples.pad"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.get_tok_labels"], ["", "def", "get_examples", "(", "data_path", ",", "tok2id", ",", "max_seq_len", ",", "\n", "noise", "=", "False", ",", "add_del_tok", "=", "False", ",", "\n", "categories_path", "=", "None", ")", ":", "\n", "    ", "global", "REL2ID", "\n", "global", "POS2ID", "\n", "global", "EDIT_TYPE2ID", "\n", "global", "ARGS", "\n", "\n", "if", "ARGS", ".", "drop_words", "is", "not", "None", ":", "\n", "        ", "drop_set", "=", "set", "(", "[", "l", ".", "strip", "(", ")", "for", "l", "in", "open", "(", "ARGS", ".", "drop_words", ")", "]", ")", "\n", "", "else", ":", "\n", "        ", "drop_set", "=", "None", "\n", "\n", "", "def", "pad", "(", "id_arr", ",", "pad_idx", ")", ":", "\n", "        ", "return", "id_arr", "+", "(", "[", "pad_idx", "]", "*", "(", "max_seq_len", "-", "len", "(", "id_arr", ")", ")", ")", "\n", "\n", "", "skipped", "=", "0", "\n", "out", "=", "defaultdict", "(", "list", ")", "\n", "if", "categories_path", "is", "not", "None", ":", "\n", "        ", "category_fp", "=", "open", "(", "categories_path", ")", "\n", "next", "(", "category_fp", ")", "# ignore header", "\n", "revid2topic", "=", "{", "\n", "l", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "[", "0", "]", ":", "[", "float", "(", "x", ")", "for", "x", "in", "l", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "[", "1", ":", "]", "]", "\n", "for", "l", "in", "category_fp", "\n", "}", "\n", "", "for", "i", ",", "(", "line", ")", "in", "enumerate", "(", "tqdm", "(", "open", "(", "data_path", ")", ")", ")", ":", "\n", "        ", "parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "\n", "# if there pos/rel info", "\n", "if", "len", "(", "parts", ")", "==", "7", ":", "\n", "            ", "[", "revid", ",", "pre", ",", "post", ",", "_", ",", "_", ",", "pos", ",", "rels", "]", "=", "parts", "\n", "# no pos/rel info", "\n", "", "elif", "len", "(", "parts", ")", "==", "5", ":", "\n", "            ", "[", "revid", ",", "pre", ",", "post", ",", "_", ",", "_", "]", "=", "parts", "\n", "pos", "=", "' '", ".", "join", "(", "[", "'<UNK>'", "]", "*", "len", "(", "pre", ".", "strip", "(", ")", ".", "split", "(", ")", ")", ")", "\n", "rels", "=", "' '", ".", "join", "(", "[", "'<UNK>'", "]", "*", "len", "(", "pre", ".", "strip", "(", ")", ".", "split", "(", ")", ")", ")", "\n", "# broken line", "\n", "", "else", ":", "\n", "            ", "skipped", "+=", "1", "\n", "continue", "\n", "\n", "# break up tokens", "\n", "", "tokens", "=", "pre", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "post_tokens", "=", "post", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "rels", "=", "rels", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "pos", "=", "pos", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "\n", "# get diff + binary diff masks", "\n", "tok_diff", "=", "diff", "(", "tokens", ",", "post_tokens", ")", "\n", "pre_tok_labels", ",", "post_tok_labels", "=", "get_tok_labels", "(", "tok_diff", ")", "\n", "\n", "# make sure everything lines up    ", "\n", "if", "len", "(", "tokens", ")", "!=", "len", "(", "pre_tok_labels", ")", "or", "len", "(", "tokens", ")", "!=", "len", "(", "rels", ")", "or", "len", "(", "tokens", ")", "!=", "len", "(", "pos", ")", "or", "len", "(", "post_tokens", ")", "!=", "len", "(", "post_tok_labels", ")", ":", "\n", "            ", "skipped", "+=", "1", "\n", "continue", "\n", "\n", "# leave room in the post for start/stop and possible category/class token", "\n", "", "if", "len", "(", "tokens", ")", ">", "max_seq_len", "-", "1", "or", "len", "(", "post_tokens", ")", ">", "max_seq_len", "-", "1", ":", "\n", "            ", "skipped", "+=", "1", "\n", "continue", "\n", "\n", "# category info if provided", "\n", "# TODO -- if provided but not in diyi's data, we fill with random...is that ok?", "\n", "", "if", "categories_path", "is", "not", "None", "and", "revid", "in", "revid2topic", ":", "\n", "            ", "categories", "=", "revid2topic", "[", "revid", "]", "\n", "", "else", ":", "\n", "            ", "categories", "=", "np", ".", "random", ".", "uniform", "(", "size", "=", "43", ")", "# 43 = number of categories", "\n", "categories", "=", "categories", "/", "sum", "(", "categories", ")", "# normalize", "\n", "\n", "", "if", "ARGS", ".", "category_input", ":", "\n", "            ", "category_id", "=", "np", ".", "argmax", "(", "categories", ")", "\n", "tokens", "=", "[", "'[unused%d]'", "%", "category_id", "]", "+", "tokens", "\n", "pre_tok_labels", "=", "[", "EDIT_TYPE2ID", "[", "'mask'", "]", "]", "+", "pre_tok_labels", "\n", "post_tok_labels", "=", "[", "EDIT_TYPE2ID", "[", "'mask'", "]", "]", "+", "post_tok_labels", "\n", "\n", "# add start + end symbols to post in/out", "\n", "", "post_input_tokens", "=", "[", "'\u884c'", "]", "+", "post_tokens", "\n", "post_output_tokens", "=", "post_tokens", "+", "[", "'\u6b62'", "]", "\n", "\n", "# shuffle + convert to ids + pad", "\n", "try", ":", "\n", "            ", "if", "noise", ":", "\n", "                ", "pre_toks", "=", "noise_seq", "(", "\n", "tokens", "[", ":", "]", ",", "\n", "drop_prob", "=", "ARGS", ".", "noise_prob", ",", "\n", "shuf_dist", "=", "ARGS", ".", "shuf_dist", ",", "\n", "drop_set", "=", "drop_set", ",", "\n", "keep_bigrams", "=", "ARGS", ".", "keep_bigrams", ")", "\n", "", "else", ":", "\n", "                ", "pre_toks", "=", "tokens", "\n", "\n", "", "pre_ids", "=", "pad", "(", "[", "tok2id", "[", "x", "]", "for", "x", "in", "pre_toks", "]", ",", "0", ")", "\n", "post_in_ids", "=", "pad", "(", "[", "tok2id", "[", "x", "]", "for", "x", "in", "post_input_tokens", "]", ",", "0", ")", "\n", "post_out_ids", "=", "pad", "(", "[", "tok2id", "[", "x", "]", "for", "x", "in", "post_output_tokens", "]", ",", "0", ")", "\n", "pre_tok_label_ids", "=", "pad", "(", "pre_tok_labels", ",", "EDIT_TYPE2ID", "[", "'mask'", "]", ")", "\n", "post_tok_label_ids", "=", "pad", "(", "post_tok_labels", ",", "EDIT_TYPE2ID", "[", "'mask'", "]", ")", "\n", "rel_ids", "=", "pad", "(", "[", "REL2ID", ".", "get", "(", "x", ",", "REL2ID", "[", "'<UNK>'", "]", ")", "for", "x", "in", "rels", "]", ",", "0", ")", "\n", "pos_ids", "=", "pad", "(", "[", "POS2ID", ".", "get", "(", "x", ",", "POS2ID", "[", "'<UNK>'", "]", ")", "for", "x", "in", "pos", "]", ",", "0", ")", "\n", "", "except", "KeyError", ":", "\n", "# TODO FUCK THIS ENCODING BUG!!!", "\n", "            ", "skipped", "+=", "1", "\n", "continue", "\n", "\n", "", "input_mask", "=", "pad", "(", "[", "0", "]", "*", "len", "(", "tokens", ")", ",", "1", ")", "\n", "pre_len", "=", "len", "(", "tokens", ")", "\n", "\n", "out", "[", "'pre_ids'", "]", ".", "append", "(", "pre_ids", ")", "\n", "out", "[", "'pre_masks'", "]", ".", "append", "(", "input_mask", ")", "\n", "out", "[", "'pre_lens'", "]", ".", "append", "(", "pre_len", ")", "\n", "out", "[", "'post_in_ids'", "]", ".", "append", "(", "post_in_ids", ")", "\n", "out", "[", "'post_out_ids'", "]", ".", "append", "(", "post_out_ids", ")", "\n", "out", "[", "'pre_tok_label_ids'", "]", ".", "append", "(", "pre_tok_label_ids", ")", "\n", "out", "[", "'post_tok_label_ids'", "]", ".", "append", "(", "post_tok_label_ids", ")", "\n", "out", "[", "'rel_ids'", "]", ".", "append", "(", "rel_ids", ")", "\n", "out", "[", "'pos_ids'", "]", ".", "append", "(", "pos_ids", ")", "\n", "out", "[", "'categories'", "]", ".", "append", "(", "categories", ")", "\n", "\n", "", "print", "(", "'SKIPPED '", ",", "skipped", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.get_dataloader": [[227, 287], ["torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "max", "os.path.exists", "pickle.load", "data.get_examples", "pickle.dump", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "torch.utils.data.TensorDataset.sort", "torch.stack", "torch.stack", "open", "open", "zip", "torch.utils.data.SequentialSampler", "torch.utils.data.RandomSampler"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.get_examples"], ["", "def", "get_dataloader", "(", "data_path", ",", "tok2id", ",", "batch_size", ",", "\n", "pickle_path", "=", "None", ",", "test", "=", "False", ",", "noise", "=", "False", ",", "add_del_tok", "=", "False", ",", "\n", "categories_path", "=", "None", ",", "sort_batch", "=", "True", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "def", "collate", "(", "data", ")", ":", "\n", "        ", "if", "sort_batch", ":", "\n", "# sort by length for packing/padding", "\n", "            ", "data", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "2", "]", ",", "reverse", "=", "True", ")", "\n", "# group by datatype", "\n", "", "[", "\n", "src_id", ",", "src_mask", ",", "src_len", ",", "\n", "post_in_id", ",", "post_out_id", ",", "\n", "pre_tok_label", ",", "post_tok_label", ",", "\n", "rel_ids", ",", "pos_ids", ",", "categories", "\n", "]", "=", "[", "torch", ".", "stack", "(", "x", ")", "for", "x", "in", "zip", "(", "*", "data", ")", "]", "\n", "\n", "# cut off at max len of this batch for unpacking/repadding", "\n", "max_len", "=", "max", "(", "src_len", ")", "\n", "data", "=", "[", "\n", "src_id", "[", ":", ",", ":", "max_len", "]", ",", "src_mask", "[", ":", ",", ":", "max_len", "]", ",", "src_len", ",", "\n", "post_in_id", "[", ":", ",", ":", "max_len", "+", "10", "]", ",", "post_out_id", "[", ":", ",", ":", "max_len", "+", "10", "]", ",", "# +10 for wiggle room", "\n", "pre_tok_label", "[", ":", ",", ":", "max_len", "]", ",", "post_tok_label", "[", ":", ",", ":", "max_len", "+", "10", "]", ",", "# +10 for post_toks_labels too (it's just gonna be matched up with post ids)", "\n", "rel_ids", "[", ":", ",", ":", "max_len", "]", ",", "pos_ids", "[", ":", ",", ":", "max_len", "]", ",", "categories", "\n", "]", "\n", "\n", "return", "data", "\n", "\n", "", "if", "pickle_path", "is", "not", "None", "and", "os", ".", "path", ".", "exists", "(", "pickle_path", ")", ":", "\n", "        ", "examples", "=", "pickle", ".", "load", "(", "open", "(", "pickle_path", ",", "'rb'", ")", ")", "\n", "", "else", ":", "\n", "        ", "examples", "=", "get_examples", "(", "\n", "data_path", "=", "data_path", ",", "\n", "tok2id", "=", "tok2id", ",", "\n", "max_seq_len", "=", "ARGS", ".", "max_seq_len", ",", "\n", "noise", "=", "noise", ",", "\n", "add_del_tok", "=", "add_del_tok", ",", "\n", "categories_path", "=", "categories_path", ")", "\n", "\n", "pickle", ".", "dump", "(", "examples", ",", "open", "(", "pickle_path", ",", "'wb'", ")", ")", "\n", "\n", "", "data", "=", "TensorDataset", "(", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pre_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pre_masks'", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", ",", "# byte for masked_fill()", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pre_lens'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'post_in_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'post_out_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pre_tok_label_ids'", "]", ",", "dtype", "=", "torch", ".", "float", ")", ",", "# for compartin to enrichment stuff", "\n", "torch", ".", "tensor", "(", "examples", "[", "'post_tok_label_ids'", "]", ",", "dtype", "=", "torch", ".", "float", ")", ",", "# for loss multiplying", "\n", "torch", ".", "tensor", "(", "examples", "[", "'rel_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pos_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'categories'", "]", ",", "dtype", "=", "torch", ".", "float", ")", ")", "\n", "\n", "dataloader", "=", "DataLoader", "(", "\n", "data", ",", "\n", "sampler", "=", "(", "SequentialSampler", "(", "data", ")", "if", "test", "else", "RandomSampler", "(", "data", ")", ")", ",", "\n", "collate_fn", "=", "collate", ",", "\n", "batch_size", "=", "batch_size", ")", "\n", "\n", "return", "dataloader", ",", "len", "(", "examples", "[", "'pre_ids'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.SeqModel.__init__": [[54, 173], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "decoders.StackedAttentionLSTM", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "models.SeqModel.init_weights", "torch.Embedding", "torch.Embedding", "encoders.LSTMEncoder", "torch.Linear", "torch.Linear", "NotImplementedError", "torch.Embedding", "torch.Embedding", "ops.FFNN", "encoders.LSTMEncoder", "ops.FeedForwardAttention", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "NotImplementedError", "ops.BilinearAttention", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "ops.BilinearAttention"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.SeqModel.init_weights"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "src_vocab_size", ",", "\n", "tgt_vocab_size", ",", "\n", "pad_id_src", ",", "\n", "pad_id_tgt", ",", "\n", "config", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Initialize model.\"\"\"", "\n", "super", "(", "SeqModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "src_vocab_size", "=", "src_vocab_size", "\n", "self", ".", "tgt_vocab_size", "=", "tgt_vocab_size", "\n", "self", ".", "pad_id_src", "=", "pad_id_src", "\n", "self", ".", "pad_id_tgt", "=", "pad_id_tgt", "\n", "self", ".", "batch_size", "=", "config", "[", "'data'", "]", "[", "'batch_size'", "]", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "options", "=", "config", "[", "'model'", "]", "\n", "self", ".", "model_type", "=", "config", "[", "'model'", "]", "[", "'model_type'", "]", "\n", "\n", "self", ".", "src_embedding", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "src_vocab_size", ",", "\n", "self", ".", "options", "[", "'emb_dim'", "]", ",", "\n", "self", ".", "pad_id_src", ")", "\n", "\n", "if", "self", ".", "config", "[", "'data'", "]", "[", "'share_vocab'", "]", ":", "\n", "            ", "self", ".", "tgt_embedding", "=", "self", ".", "src_embedding", "\n", "", "else", ":", "\n", "            ", "self", ".", "tgt_embedding", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "tgt_vocab_size", ",", "\n", "self", ".", "options", "[", "'emb_dim'", "]", ",", "\n", "self", ".", "pad_id_tgt", ")", "\n", "\n", "", "if", "self", ".", "options", "[", "'encoder'", "]", "==", "'lstm'", ":", "\n", "            ", "self", ".", "encoder", "=", "encoders", ".", "LSTMEncoder", "(", "\n", "self", ".", "options", "[", "'emb_dim'", "]", ",", "\n", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "self", ".", "options", "[", "'src_layers'", "]", ",", "\n", "self", ".", "options", "[", "'bidirectional'", "]", ",", "\n", "self", ".", "options", "[", "'dropout'", "]", ")", "\n", "self", ".", "ctx_bridge", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "self", ".", "options", "[", "'tgt_hidden_dim'", "]", ")", "\n", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'unknown encoder type'", ")", "\n", "\n", "# # # # # #  # # # # # #  # # # # #  NEW STUFF FROM STD SEQ2SEQ", "\n", "\n", "", "if", "self", ".", "model_type", "==", "'delete'", ":", "\n", "            ", "self", ".", "attribute_embedding", "=", "nn", ".", "Embedding", "(", "\n", "num_embeddings", "=", "2", ",", "\n", "embedding_dim", "=", "self", ".", "options", "[", "'emb_dim'", "]", ")", "\n", "attr_size", "=", "self", ".", "options", "[", "'emb_dim'", "]", "\n", "\n", "", "elif", "self", ".", "model_type", "in", "'delete_retrieve'", ":", "\n", "            ", "self", ".", "attribute_encoder", "=", "encoders", ".", "LSTMEncoder", "(", "\n", "self", ".", "options", "[", "'emb_dim'", "]", ",", "\n", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "self", ".", "options", "[", "'src_layers'", "]", ",", "\n", "self", ".", "options", "[", "'bidirectional'", "]", ",", "\n", "self", ".", "options", "[", "'dropout'", "]", ",", "\n", "pack", "=", "False", ")", "\n", "attr_size", "=", "self", ".", "options", "[", "'src_hidden_dim'", "]", "\n", "\n", "", "elif", "self", ".", "model_type", "==", "'seq2seq'", ":", "\n", "            ", "attr_size", "=", "0", "\n", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'unknown model type'", ")", "\n", "\n", "", "self", ".", "c_bridge", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "self", ".", "options", "[", "'tgt_hidden_dim'", "]", ")", "\n", "self", ".", "h_bridge", "=", "nn", ".", "Linear", "(", "\n", "attr_size", "+", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "self", ".", "options", "[", "'tgt_hidden_dim'", "]", ")", "\n", "\n", "if", "self", ".", "config", "[", "'experimental'", "]", "[", "'predict_sides'", "]", ":", "\n", "            ", "if", "self", ".", "config", "[", "'experimental'", "]", "[", "'side_attn_type'", "]", "==", "'feedforward'", ":", "\n", "                ", "self", ".", "side_attn", "=", "ops", ".", "FeedForwardAttention", "(", "\n", "input_dim", "=", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "hidden_dim", "=", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "layers", "=", "2", ",", "\n", "dropout", "=", "self", ".", "options", "[", "'dropout'", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "'experimental'", "]", "[", "'side_attn_type'", "]", "==", "'dot'", ":", "\n", "                ", "self", ".", "side_attn", "=", "ops", ".", "BilinearAttention", "(", "\n", "hidden", "=", "self", ".", "options", "[", "'src_hidden_dim'", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "'experimental'", "]", "[", "'side_attn_type'", "]", "==", "'bahdanau'", ":", "\n", "                ", "self", ".", "side_attn", "=", "ops", ".", "BilinearAttention", "(", "\n", "hidden", "=", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "score_fn", "=", "'bahdanau'", ")", "\n", "\n", "", "self", ".", "side_predictor", "=", "ops", ".", "FFNN", "(", "\n", "input_dim", "=", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "hidden_dim", "=", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "output_dim", "=", "self", ".", "config", "[", "'experimental'", "]", "[", "'n_side_outputs'", "]", ",", "# TODO -- SET SOMEWHERE", "\n", "nlayers", "=", "2", ",", "\n", "dropout", "=", "self", ".", "options", "[", "'dropout'", "]", ")", "\n", "\n", "if", "self", ".", "config", "[", "'experimental'", "]", "[", "'add_side_embeddings'", "]", ":", "\n", "                ", "self", ".", "side_embeddings", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros", "(", "self", ".", "config", "[", "'experimental'", "]", "[", "'n_side_outputs'", "]", ",", "self", ".", "options", "[", "'emb_dim'", "]", ")", ",", "\n", "requires_grad", "=", "True", ")", "\n", "self", ".", "h_compression", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "options", "[", "'emb_dim'", "]", "+", "self", ".", "options", "[", "'src_hidden_dim'", "]", ",", "\n", "self", ".", "options", "[", "'tgt_hidden_dim'", "]", ")", "\n", "self", ".", "side_softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "# # # # # #  # # # # # #  # # # # # END NEW STUFF", "\n", "\n", "", "", "self", ".", "decoder", "=", "decoders", ".", "StackedAttentionLSTM", "(", "config", "=", "config", ")", "\n", "\n", "self", ".", "output_projection", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "options", "[", "'tgt_hidden_dim'", "]", ",", "\n", "tgt_vocab_size", ")", "\n", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.SeqModel.init_weights": [[174, 179], ["models.SeqModel.parameters", "param.data.uniform_"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize weights.\"\"\"", "\n", "initrange", "=", "0.1", "\n", "for", "param", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "data", ".", "uniform_", "(", "-", "initrange", ",", "initrange", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.SeqModel.forward": [[180, 265], ["models.SeqModel.src_embedding", "models.SeqModel.encoder", "models.SeqModel.ctx_bridge", "models.SeqModel.c_bridge", "models.SeqModel.h_bridge", "models.SeqModel.tgt_embedding", "models.SeqModel.decoder", "tgt_outputs.contiguous().view", "models.SeqModel.output_projection", "decoder_logit.view.view.view", "models.SeqModel.softmax", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "side_info[].squeeze", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "models.SeqModel.side_attn", "models.SeqModel.side_predictor", "models.SeqModel.attribute_embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "query.cuda.cuda.cuda", "models.SeqModel.side_softmax", "models.SeqModel.side_embeddings.repeat", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.SeqModel.h_compression", "models.SeqModel.src_embedding", "models.SeqModel.attribute_encoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "tgt_outputs.contiguous", "tgt_outputs.size", "tgt_outputs.size", "tgt_outputs.size", "decoder_logit.view.view.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "probs.cuda.cuda.scatter_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "tgt_outputs.size", "tgt_outputs.size", "probs.cuda.cuda.cuda", "side_info[].squeeze.unsqueeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "probs.cuda.cuda.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.softmax"], ["", "", "def", "forward", "(", "self", ",", "input_src", ",", "input_tgt", ",", "srcmask", ",", "srclens", ",", "input_attr", ",", "attrlens", ",", "attrmask", ",", "side_info", ")", ":", "\n", "        ", "src_emb", "=", "self", ".", "src_embedding", "(", "input_src", ")", "\n", "\n", "srcmask", "=", "(", "1", "-", "srcmask", ")", ".", "byte", "(", ")", "\n", "\n", "src_outputs", ",", "(", "src_h_t", ",", "src_c_t", ")", "=", "self", ".", "encoder", "(", "src_emb", ",", "srclens", ",", "srcmask", ")", "\n", "\n", "if", "self", ".", "options", "[", "'bidirectional'", "]", ":", "\n", "            ", "h_t", "=", "torch", ".", "cat", "(", "(", "src_h_t", "[", "-", "1", "]", ",", "src_h_t", "[", "-", "2", "]", ")", ",", "1", ")", "\n", "c_t", "=", "torch", ".", "cat", "(", "(", "src_c_t", "[", "-", "1", "]", ",", "src_c_t", "[", "-", "2", "]", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "h_t", "=", "src_h_t", "[", "-", "1", "]", "\n", "c_t", "=", "src_c_t", "[", "-", "1", "]", "\n", "\n", "", "src_outputs", "=", "self", ".", "ctx_bridge", "(", "src_outputs", ")", "\n", "\n", "# # # #  # # # #  # #  # # # # # # #  # # seq2seq diff", "\n", "if", "self", ".", "config", "[", "'experimental'", "]", "[", "'predict_sides'", "]", ":", "\n", "            ", "side_info", "=", "side_info", "[", ":", ",", "1", ":", "]", ".", "squeeze", "(", "1", ")", "# ignore the \"start token\" from data.get_minibatch", "\n", "query", "=", "torch", ".", "zeros", "(", "src_outputs", "[", ":", ",", "0", ",", ":", "]", ".", "shape", ")", "\n", "if", "CUDA", ":", "\n", "                ", "query", "=", "query", ".", "cuda", "(", ")", "\n", "", "src_summary", ",", "_", ",", "probs", "=", "self", ".", "side_attn", "(", "\n", "query", "=", "query", ",", "\n", "keys", "=", "src_outputs", ",", "\n", "values", "=", "src_outputs", ",", "\n", "mask", "=", "srcmask", ")", "\n", "\n", "side_logit", ",", "side_loss", "=", "self", ".", "side_predictor", "(", "\n", "src_summary", ",", "side_info", ")", "\n", "if", "self", ".", "config", "[", "'experimental'", "]", "[", "'add_side_embeddings'", "]", ":", "\n", "# use probs to do weighted sum of embeddings, join those with h_T", "\n", "                ", "probs", "=", "self", ".", "side_softmax", "(", "side_logit", ")", "\n", "if", "self", ".", "config", "[", "'experimental'", "]", "[", "'side_embedding_teacher_force'", "]", ":", "\n", "                    ", "probs", "=", "torch", ".", "zeros", "(", "probs", ".", "shape", ")", "\n", "if", "CUDA", ":", "\n", "                        ", "probs", "=", "probs", ".", "cuda", "(", ")", "\n", "", "probs", "=", "probs", ".", "scatter_", "(", "1", ",", "side_info", ".", "unsqueeze", "(", "1", ")", ",", "1.0", ")", "\n", "", "embs", "=", "self", ".", "side_embeddings", ".", "repeat", "(", "side_logit", ".", "shape", "[", "0", "]", ",", "1", ",", "1", ")", "\n", "weighted_emb", "=", "torch", ".", "bmm", "(", "probs", ".", "unsqueeze", "(", "1", ")", ",", "embs", ")", ".", "squeeze", "(", "1", ")", "\n", "h_t", "=", "torch", ".", "cat", "(", "(", "h_t", ",", "weighted_emb", ")", ",", "-", "1", ")", "\n", "h_t", "=", "self", ".", "h_compression", "(", "h_t", ")", "\n", "", "", "else", ":", "\n", "            ", "side_logit", ",", "side_loss", "=", "None", ",", "0.0", "\n", "\n", "# join attribute with h/c then bridge 'em", "\n", "# TODO -- put this stuff in a method, overlaps w/above?", "\n", "", "if", "self", ".", "model_type", "==", "'delete'", ":", "\n", "# just do h i guess?", "\n", "            ", "a_ht", "=", "self", ".", "attribute_embedding", "(", "input_attr", ")", "\n", "h_t", "=", "torch", ".", "cat", "(", "(", "h_t", ",", "a_ht", ")", ",", "-", "1", ")", "\n", "\n", "", "elif", "self", ".", "model_type", "==", "'delete_retrieve'", ":", "\n", "            ", "attr_emb", "=", "self", ".", "src_embedding", "(", "input_attr", ")", "\n", "_", ",", "(", "a_ht", ",", "a_ct", ")", "=", "self", ".", "attribute_encoder", "(", "attr_emb", ",", "attrlens", ",", "attrmask", ")", "\n", "if", "self", ".", "options", "[", "'bidirectional'", "]", ":", "\n", "                ", "a_ht", "=", "torch", ".", "cat", "(", "(", "a_ht", "[", "-", "1", "]", ",", "a_ht", "[", "-", "2", "]", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "                ", "a_ht", "=", "a_ht", "[", "-", "1", "]", "\n", "\n", "", "h_t", "=", "torch", ".", "cat", "(", "(", "h_t", ",", "a_ht", ")", ",", "-", "1", ")", "\n", "\n", "", "c_t", "=", "self", ".", "c_bridge", "(", "c_t", ")", "\n", "h_t", "=", "self", ".", "h_bridge", "(", "h_t", ")", "\n", "# # # #  # # # #  # #  # # # # # # #  # # end diff", "\n", "\n", "tgt_emb", "=", "self", ".", "tgt_embedding", "(", "input_tgt", ")", "\n", "tgt_outputs", ",", "(", "_", ",", "_", ")", "=", "self", ".", "decoder", "(", "\n", "tgt_emb", ",", "\n", "(", "h_t", ",", "c_t", ")", ",", "\n", "src_outputs", ",", "\n", "srcmask", ")", "\n", "\n", "tgt_outputs_reshape", "=", "tgt_outputs", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "tgt_outputs", ".", "size", "(", ")", "[", "0", "]", "*", "tgt_outputs", ".", "size", "(", ")", "[", "1", "]", ",", "\n", "tgt_outputs", ".", "size", "(", ")", "[", "2", "]", ")", "\n", "decoder_logit", "=", "self", ".", "output_projection", "(", "tgt_outputs_reshape", ")", "\n", "decoder_logit", "=", "decoder_logit", ".", "view", "(", "\n", "tgt_outputs", ".", "size", "(", ")", "[", "0", "]", ",", "\n", "tgt_outputs", ".", "size", "(", ")", "[", "1", "]", ",", "\n", "decoder_logit", ".", "size", "(", ")", "[", "1", "]", ")", "\n", "\n", "probs", "=", "self", ".", "softmax", "(", "decoder_logit", ")", "\n", "\n", "return", "decoder_logit", ",", "probs", ",", "side_logit", ",", "side_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.SeqModel.count_params": [[266, 271], ["models.SeqModel.parameters", "numpy.prod", "param.data.cpu().numpy", "param.data.cpu"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters"], ["", "def", "count_params", "(", "self", ")", ":", "\n", "        ", "n_params", "=", "0", "\n", "for", "param", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "n_params", "+=", "np", ".", "prod", "(", "param", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "shape", ")", "\n", "", "return", "n_params", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.__init__": [[279, 287], ["sklearn.svm.LinearSVC", "sklearn.feature_extraction.text.CountVectorizer"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab", "=", "None", ")", ":", "\n", "# vocab: {tok: idx}", "\n", "        ", "if", "vocab", ":", "\n", "            ", "self", ".", "vectorizer", "=", "CountVectorizer", "(", "vocabulary", "=", "vocab", ",", "binary", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "vectorizer", "=", "None", "\n", "\n", "", "self", ".", "predictor", "=", "svm", ".", "LinearSVC", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.parameters": [[288, 292], ["models.TextClassifier.vectorizer.vocabulary_.items", "enumerate"], "methods", ["None"], ["", "def", "parameters", "(", "self", ")", ":", "\n", "        ", "id2tok", "=", "{", "i", ":", "x", "for", "x", ",", "i", "in", "self", ".", "vectorizer", ".", "vocabulary_", ".", "items", "(", ")", "}", "\n", "out", "=", "{", "id2tok", "[", "i", "]", ":", "coef", "for", "i", ",", "coef", "in", "enumerate", "(", "self", ".", "predictor", ".", "coef_", "[", "0", "]", ")", "}", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.fit": [[293, 301], ["models.TextClassifier.vectorizer.fit_transform", "sklearn.utils.shuffle", "models.TextClassifier.predictor.fit", "x.strip", "x.strip", "open", "open", "open", "open"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.fit"], ["", "def", "fit", "(", "self", ",", "corpus1_path", ",", "corpus2_path", ",", "seed", "=", "0", ")", ":", "\n", "        ", "sents", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "open", "(", "corpus1_path", ")", "]", "+", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "open", "(", "corpus2_path", ")", "]", "\n", "\n", "X", "=", "self", ".", "vectorizer", ".", "fit_transform", "(", "sents", ")", "\n", "Y", "=", "[", "0", "for", "_", "in", "open", "(", "corpus1_path", ")", "]", "+", "[", "1", "for", "_", "in", "open", "(", "corpus2_path", ")", "]", "\n", "X", ",", "Y", "=", "shuffle", "(", "X", ",", "Y", ",", "random_state", "=", "seed", ")", "\n", "\n", "self", ".", "predictor", ".", "fit", "(", "X", ",", "Y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.predict": [[302, 306], ["models.TextClassifier.vectorizer.transform", "models.TextClassifier.predictor.predict"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.predict"], ["", "def", "predict", "(", "self", ",", "seqs", ")", ":", "\n", "        ", "X", "=", "self", ".", "vectorizer", ".", "transform", "(", "seqs", ")", "\n", "Y_hat", "=", "self", ".", "predictor", ".", "predict", "(", "X", ")", "\n", "return", "Y_hat", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.error_rate": [[307, 311], ["models.TextClassifier.predict", "len", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.predict"], ["", "def", "error_rate", "(", "self", ",", "seqs", ",", "Y", ")", ":", "\n", "        ", "Y_hat", "=", "self", ".", "predict", "(", "seqs", ")", "\n", "error", "=", "len", "(", "[", "yhat", "for", "yhat", ",", "y", "in", "zip", "(", "Y_hat", ",", "Y", ")", "if", "yhat", "!=", "y", "]", ")", "*", "1.0", "/", "len", "(", "Y", ")", "\n", "return", "error", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.save": [[312, 317], ["open", "pickle.dump", "open", "pickle.dump"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "path_prefix", ")", ":", "\n", "        ", "with", "open", "(", "path_prefix", "+", "'.vectorizer.pkl'", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "vectorizer", ",", "f", ")", "\n", "", "with", "open", "(", "path_prefix", "+", "'.predictor.pkl'", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "predictor", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load": [[318, 323], ["open", "pickle.load", "open", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load"], ["", "", "def", "load", "(", "self", ",", "path_prefix", ")", ":", "\n", "        ", "with", "open", "(", "path_prefix", "+", "'.vectorizer.pkl'", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "self", ".", "vectorizer", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "path_prefix", "+", "'.predictor.pkl'", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "self", ".", "predictor", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.from_pickle": [[324, 329], ["models.TextClassifier", "models.TextClassifier.load"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load"], ["", "", "@", "staticmethod", "\n", "def", "from_pickle", "(", "path_prefex", ")", ":", "\n", "        ", "out", "=", "TextClassifier", "(", ")", "\n", "out", ".", "load", "(", "path_prefex", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.get_latest_ckpt": [[23, 34], ["glob.glob", "map", "os.path.join", "len", "sorted", "int", "ckpt.split"], "function", ["None"], ["def", "get_latest_ckpt", "(", "ckpt_dir", ")", ":", "\n", "    ", "ckpts", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "ckpt_dir", ",", "'*.ckpt'", ")", ")", "\n", "# nothing to load, continue with fresh params", "\n", "if", "len", "(", "ckpts", ")", "==", "0", ":", "\n", "        ", "return", "-", "1", ",", "None", "\n", "", "ckpts", "=", "map", "(", "lambda", "ckpt", ":", "(", "\n", "int", "(", "ckpt", ".", "split", "(", "'.'", ")", "[", "1", "]", ")", ",", "\n", "ckpt", ")", ",", "ckpts", ")", "\n", "# get most recent checkpoint", "\n", "epoch", ",", "ckpt_path", "=", "sorted", "(", "ckpts", ")", "[", "-", "1", "]", "\n", "return", "epoch", ",", "ckpt_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.attempt_load_model": [[36, 50], ["models.get_latest_ckpt", "int", "model.load_state_dict", "print", "torch.load", "torch.load", "checkpoint_path.split"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.get_latest_ckpt", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load"], ["", "def", "attempt_load_model", "(", "model", ",", "checkpoint_dir", "=", "None", ",", "checkpoint_path", "=", "None", ")", ":", "\n", "    ", "assert", "checkpoint_dir", "or", "checkpoint_path", "\n", "\n", "if", "checkpoint_dir", ":", "\n", "        ", "epoch", ",", "checkpoint_path", "=", "get_latest_ckpt", "(", "checkpoint_dir", ")", "\n", "", "else", ":", "\n", "        ", "epoch", "=", "int", "(", "checkpoint_path", ".", "split", "(", "'.'", ")", "[", "-", "2", "]", ")", "\n", "\n", "", "if", "checkpoint_path", ":", "\n", "        ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "checkpoint_path", ")", ")", "\n", "print", "(", "'Load from %s sucessful!'", "%", "checkpoint_path", ")", "\n", "return", "model", ",", "epoch", "+", "1", "\n", "", "else", ":", "\n", "        ", "return", "model", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.FcTube.__init__": [[10, 38], ["torch.Module.__init__", "range", "torch.Sequential", "torch.Sequential", "ops.FcTube.layers.append", "ops.FcTube.layers.append", "range", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "len", "ops.FcTube.layers[].cuda", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "output_dim", ",", "nlayers", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "FcTube", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "nlayers", "==", "1", ":", "\n", "            ", "hidden_dim", "=", "output_dim", "\n", "\n", "", "self", ".", "layers", "=", "[", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "hidden_dim", ")", ",", "\n", "# nn.ReLU(), ", "\n", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", ")", "]", "\n", "for", "_", "in", "range", "(", "nlayers", "-", "2", ")", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "\n", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "hidden_dim", ")", ",", "\n", "# nn.ReLU(), ", "\n", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", ")", ")", "\n", "", "if", "nlayers", ">", "1", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "\n", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hidden_dim", ",", "output_dim", ")", ",", "\n", "# nn.ReLU(), ", "\n", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", ")", ")", "\n", "# TODO -- get this into model.parameters so that we can just do model.cuda()", "\n", "", "if", "CUDA", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "                ", "self", ".", "layers", "[", "i", "]", "=", "self", ".", "layers", "[", "i", "]", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.FcTube.forward": [[40, 44], ["layer"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "X", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "X", "=", "layer", "(", "X", ")", "\n", "", "return", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.FFNN.__init__": [[48, 52], ["torch.Module.__init__", "ops.FcTube", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "output_dim", ",", "nlayers", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "FFNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tube", "=", "FcTube", "(", "input_dim", ",", "hidden_dim", ",", "output_dim", ",", "nlayers", ",", "dropout", ")", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.FFNN.forward": [[53, 58], ["ops.FFNN.tube", "ops.FFNN.criterion"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "X", ",", "Y", ")", ":", "\n", "        ", "X", "=", "self", ".", "tube", "(", "X", ")", "\n", "\n", "loss", "=", "self", ".", "criterion", "(", "X", ",", "Y", ")", "\n", "return", "X", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.FeedForwardAttention.__init__": [[64, 70], ["torch.Module.__init__", "ops.FcTube", "torch.Softmax", "torch.Softmax", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "layers", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "FeedForwardAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "scorer", "=", "FcTube", "(", "input_dim", ",", "hidden_dim", ",", "1", ",", "layers", ",", "dropout", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", ")", "\n", "self", ".", "out_projection", "=", "nn", ".", "Linear", "(", "hidden_dim", "*", "2", ",", "hidden_dim", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.FeedForwardAttention.forward": [[71, 79], ["ops.FeedForwardAttention.scorer().squeeze", "scores.masked_fill.masked_fill.masked_fill", "ops.FeedForwardAttention.softmax", "probs.unsqueeze.unsqueeze.unsqueeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "ops.FeedForwardAttention.scorer", "float", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.softmax"], ["", "def", "forward", "(", "self", ",", "query", ",", "keys", ",", "mask", "=", "None", ",", "values", "=", "None", ")", ":", "\n", "        ", "scores", "=", "self", ".", "scorer", "(", "keys", ")", ".", "squeeze", "(", "2", ")", "\n", "scores", "=", "scores", ".", "masked_fill", "(", "mask", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "probs", "=", "self", ".", "softmax", "(", "scores", ")", "\n", "probs", "=", "probs", ".", "unsqueeze", "(", "1", ")", "\n", "weighted_context", "=", "torch", ".", "bmm", "(", "probs", ",", "keys", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "return", "weighted_context", ",", "None", ",", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.BilinearAttention.__init__": [[85, 98], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "hidden", ",", "score_fn", "=", "'dot'", ")", ":", "\n", "        ", "super", "(", "BilinearAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "query_in_projection", "=", "nn", ".", "Linear", "(", "hidden", ",", "hidden", ")", "\n", "self", ".", "key_in_projection", "=", "nn", ".", "Linear", "(", "hidden", ",", "hidden", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", ")", "\n", "self", ".", "out_projection", "=", "nn", ".", "Linear", "(", "hidden", "*", "2", ",", "hidden", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "score_fn", "=", "self", ".", "dot", "\n", "\n", "if", "score_fn", "==", "'bahdanau'", ":", "\n", "            ", "self", ".", "v_att", "=", "nn", ".", "Linear", "(", "hidden", ",", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "score_tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "score_fn", "=", "self", ".", "bahdanau", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.BilinearAttention.forward": [[99, 133], ["ops.BilinearAttention.key_in_projection", "ops.BilinearAttention.query_in_projection", "ops.BilinearAttention.score_fn", "ops.BilinearAttention.softmax", "ops.BilinearAttention.unsqueeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ops.BilinearAttention.tanh", "attn_scores.masked_fill.masked_fill.masked_fill", "ops.BilinearAttention.out_projection", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "float"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.shared.data.softmax"], ["", "", "def", "forward", "(", "self", ",", "query", ",", "keys", ",", "mask", "=", "None", ",", "values", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n            query: [batch, hidden]\n            keys: [batch, len, hidden]\n            values: [batch, len, hidden] (optional, if none will = keys)\n            mask: [batch, len] mask key-scores\n\n            compare query to keys, use the scores to do weighted sum of values\n            if no value is specified, then values = keys\n        \"\"\"", "\n", "att_keys", "=", "self", ".", "key_in_projection", "(", "keys", ")", "\n", "\n", "if", "values", "is", "None", ":", "\n", "            ", "values", "=", "att_keys", "\n", "\n", "# [Batch, Hidden, 1]", "\n", "", "att_query", "=", "self", ".", "query_in_projection", "(", "query", ")", "\n", "\n", "# [Batch, Source length]", "\n", "attn_scores", "=", "self", ".", "score_fn", "(", "att_keys", ",", "att_query", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "attn_scores", "=", "attn_scores", ".", "masked_fill", "(", "mask", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "\n", "", "attn_probs", "=", "self", ".", "softmax", "(", "attn_scores", ")", "\n", "# [Batch, 1, source length]", "\n", "attn_probs_transposed", "=", "attn_probs", ".", "unsqueeze", "(", "1", ")", "\n", "# [Batch, hidden]", "\n", "weighted_context", "=", "torch", ".", "bmm", "(", "attn_probs_transposed", ",", "values", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "context_query_mixed", "=", "torch", ".", "cat", "(", "(", "weighted_context", ",", "query", ")", ",", "1", ")", "\n", "context_query_mixed", "=", "self", ".", "tanh", "(", "self", ".", "out_projection", "(", "context_query_mixed", ")", ")", "\n", "\n", "return", "weighted_context", ",", "context_query_mixed", ",", "attn_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.BilinearAttention.dot": [[135, 141], ["torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "query.unsqueeze"], "methods", ["None"], ["", "def", "dot", "(", "self", ",", "keys", ",", "query", ")", ":", "\n", "        ", "\"\"\"\n        keys: [B, T, H]\n        query: [B, H]\n        \"\"\"", "\n", "return", "torch", ".", "bmm", "(", "keys", ",", "query", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.BilinearAttention.bahdanau": [[143, 149], ["ops.BilinearAttention.v_att().squeeze", "ops.BilinearAttention.v_att", "ops.BilinearAttention.score_tanh", "query.unsqueeze"], "methods", ["None"], ["", "def", "bahdanau", "(", "self", ",", "keys", ",", "query", ")", ":", "\n", "        ", "\"\"\"\n        keys: [B, T, H]\n        query: [B, H]\n        \"\"\"", "\n", "return", "self", ".", "v_att", "(", "self", ".", "score_tanh", "(", "keys", "+", "query", ".", "unsqueeze", "(", "1", ")", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.encoders.LSTMEncoder.__init__": [[11, 25], ["torch.Module.__init__", "torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "emb_dim", ",", "hidden_dim", ",", "layers", ",", "bidirectional", ",", "dropout", ",", "pack", "=", "True", ")", ":", "\n", "        ", "super", "(", "LSTMEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_directions", "=", "2", "if", "bidirectional", "else", "1", "\n", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "\n", "emb_dim", ",", "\n", "hidden_dim", "//", "self", ".", "num_directions", ",", "\n", "layers", ",", "\n", "bidirectional", "=", "bidirectional", ",", "\n", "batch_first", "=", "True", ",", "\n", "dropout", "=", "dropout", ")", "\n", "\n", "self", ".", "pack", "=", "pack", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.encoders.LSTMEncoder.init_state": [[26, 42], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda"], "methods", ["None"], ["", "def", "init_state", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "\n", "self", ".", "lstm", ".", "num_layers", "*", "self", ".", "num_directions", ",", "\n", "batch_size", ",", "\n", "self", ".", "lstm", ".", "hidden_size", "\n", ")", ",", "requires_grad", "=", "False", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "\n", "self", ".", "lstm", ".", "num_layers", "*", "self", ".", "num_directions", ",", "\n", "batch_size", ",", "\n", "self", ".", "lstm", ".", "hidden_size", "\n", ")", ",", "requires_grad", "=", "False", ")", "\n", "\n", "if", "CUDA", ":", "\n", "            ", "return", "h0", ".", "cuda", "(", ")", ",", "c0", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "h0", ",", "c0", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.encoders.LSTMEncoder.forward": [[44, 59], ["encoders.LSTMEncoder.init_state", "encoders.LSTMEncoder.lstm", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "src_embedding.size"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.encoders.LSTMEncoder.init_state"], ["", "", "def", "forward", "(", "self", ",", "src_embedding", ",", "srclens", ",", "srcmask", ")", ":", "\n", "# retrieve batch size dynamically for decoding", "\n", "        ", "h0", ",", "c0", "=", "self", ".", "init_state", "(", "batch_size", "=", "src_embedding", ".", "size", "(", "0", ")", ")", "\n", "\n", "if", "self", ".", "pack", ":", "\n", "            ", "inputs", "=", "pack_padded_sequence", "(", "src_embedding", ",", "srclens", ",", "batch_first", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "inputs", "=", "src_embedding", "\n", "\n", "", "outputs", ",", "(", "h_final", ",", "c_final", ")", "=", "self", ".", "lstm", "(", "inputs", ",", "(", "h0", ",", "c0", ")", ")", "\n", "\n", "if", "self", ".", "pack", ":", "\n", "            ", "outputs", ",", "_", "=", "pad_packed_sequence", "(", "outputs", ",", "batch_first", "=", "True", ")", "\n", "\n", "", "return", "outputs", ",", "(", "h_final", ",", "c_final", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.CorpusSearcher.__init__": [[36, 46], ["data.CorpusSearcher.vectorizer.fit", "data.CorpusSearcher.vectorizer.transform"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.fit"], ["\n", "# 0: shared 1: edited", "\n", "EDIT_TYPE2ID", "=", "{", "'0'", ":", "0", ",", "'1'", ":", "1", ",", "'mask'", ":", "2", "}", "\n", "\n", "\n", "def", "softmax", "(", "x", ",", "axis", "=", "None", ")", ":", "\n", "    ", "x", "=", "x", "-", "x", ".", "max", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "y", "=", "np", ".", "exp", "(", "x", ")", "\n", "return", "y", "/", "y", ".", "sum", "(", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.CorpusSearcher.most_similar": [[48, 63], ["data.CorpusSearcher.vectorizer.transform", "numpy.dot", "numpy.squeeze", "zip", "numpy.squeeze.toarray", "range", "sorted", "len"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.BilinearAttention.dot"], ["    ", "pre_tok_labels", "=", "[", "]", "\n", "post_tok_labels", "=", "[", "]", "\n", "for", "tag", ",", "chunk", "in", "s_diff", ":", "\n", "        ", "if", "tag", "==", "'='", ":", "\n", "            ", "pre_tok_labels", "+=", "[", "0", "]", "*", "len", "(", "chunk", ")", "\n", "post_tok_labels", "+=", "[", "0", "]", "*", "len", "(", "chunk", ")", "\n", "", "elif", "tag", "==", "'-'", ":", "\n", "            ", "pre_tok_labels", "+=", "[", "1", "]", "*", "len", "(", "chunk", ")", "\n", "", "elif", "tag", "==", "'+'", ":", "\n", "            ", "post_tok_labels", "+=", "[", "1", "]", "*", "len", "(", "chunk", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "return", "pre_tok_labels", ",", "post_tok_labels", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.build_vocab_maps": [[65, 89], ["os.path.exists", "enumerate", "len", "x.strip", "open"], "function", ["None"], ["# from https://arxiv.org/pdf/1711.00043.pdf", "\n", "    ", "def", "perm", "(", "i", ")", ":", "\n", "        ", "return", "i", "[", "0", "]", "+", "(", "shuf_dist", "+", "1", ")", "*", "np", ".", "random", ".", "random", "(", ")", "\n", "\n", "", "if", "drop_set", "==", "None", ":", "\n", "        ", "dropped_seq", "=", "[", "x", "for", "x", "in", "seq", "if", "np", ".", "random", ".", "random", "(", ")", ">", "drop_prob", "]", "\n", "", "else", ":", "\n", "        ", "dropped_seq", "=", "[", "x", "for", "x", "in", "seq", "if", "not", "(", "x", "in", "drop_set", "and", "np", ".", "random", ".", "random", "(", ")", "<", "drop_prob", ")", "]", "\n", "\n", "", "if", "keep_bigrams", ":", "\n", "        ", "i", "=", "0", "\n", "original", "=", "' '", ".", "join", "(", "seq", ")", "\n", "tmp", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "dropped_seq", ")", "-", "1", ":", "\n", "            ", "if", "' '", ".", "join", "(", "dropped_seq", "[", "i", ":", "i", "+", "2", "]", ")", "in", "original", ":", "\n", "                ", "tmp", ".", "append", "(", "dropped_seq", "[", "i", ":", "i", "+", "2", "]", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                ", "tmp", ".", "append", "(", "[", "dropped_seq", "[", "i", "]", "]", ")", "\n", "i", "+=", "1", "\n", "\n", "", "", "dropped_seq", "=", "tmp", "\n", "\n", "# global shuffle", "\n", "", "if", "shuf_dist", "==", "-", "1", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.extract_attributes": [[91, 100], ["attribute.append", "content.append"], "function", ["None"], ["# local shuffle", "\n", "", "elif", "shuf_dist", ">", "0", ":", "\n", "        ", "dropped_seq", "=", "[", "x", "for", "_", ",", "x", "in", "sorted", "(", "enumerate", "(", "dropped_seq", ")", ",", "key", "=", "perm", ")", "]", "\n", "# shuf_dist of 0 = no shuffle", "\n", "\n", "", "if", "keep_bigrams", ":", "\n", "        ", "dropped_seq", "=", "[", "z", "for", "y", "in", "dropped_seq", "for", "z", "in", "y", "]", "\n", "\n", "", "return", "dropped_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.split_with_diff": [[101, 117], ["zip", "simplediff.diff", "collections.defaultdict", "content.append", "src_attr.append", "tgt_attr.append"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff"], ["\n", "", "def", "get_examples", "(", "data_path", ",", "tok2id", ",", "max_seq_len", ",", "\n", "noise", "=", "False", ",", "add_del_tok", "=", "False", ",", "\n", "categories_path", "=", "None", ")", ":", "\n", "    ", "global", "REL2ID", "\n", "global", "POS2ID", "\n", "global", "EDIT_TYPE2ID", "\n", "global", "ARGS", "\n", "\n", "if", "ARGS", ".", "drop_words", "is", "not", "None", ":", "\n", "        ", "drop_set", "=", "set", "(", "[", "l", ".", "strip", "(", ")", "for", "l", "in", "open", "(", "ARGS", ".", "drop_words", ")", "]", ")", "\n", "", "else", ":", "\n", "        ", "drop_set", "=", "None", "\n", "\n", "", "def", "pad", "(", "id_arr", ",", "pad_idx", ")", ":", "\n", "        ", "return", "id_arr", "+", "(", "[", "pad_idx", "]", "*", "(", "max_seq_len", "-", "len", "(", "id_arr", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_side_info": [[119, 127], ["zip", "out.append", "out.append"], "function", ["None"], ["out", "=", "defaultdict", "(", "list", ")", "\n", "if", "categories_path", "is", "not", "None", ":", "\n", "        ", "category_fp", "=", "open", "(", "categories_path", ")", "\n", "next", "(", "category_fp", ")", "# ignore header", "\n", "revid2topic", "=", "{", "\n", "l", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "[", "0", "]", ":", "[", "float", "(", "x", ")", "for", "x", "in", "l", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "[", "1", ":", "]", "]", "\n", "for", "l", "in", "category_fp", "\n", "}", "\n", "", "for", "i", ",", "(", "line", ")", "in", "enumerate", "(", "tqdm", "(", "open", "(", "data_path", ")", ")", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.read_nmt_data": [[141, 219], ["data.build_vocab_maps", "data.build_vocab_maps", "data.CorpusSearcher", "data.get_side_info", "l.strip().split", "l.strip().split", "data.split_with_diff", "set", "list", "list", "data.CorpusSearcher", "data.CorpusSearcher", "open", "open", "zip", "zip", "sklearn.feature_extraction.text.CountVectorizer", "l.strip", "l.strip", "x.strip", "sklearn.feature_extraction.text.CountVectorizer", "len", "len", "sklearn.feature_extraction.text.TfidfVectorizer", "open", "data.extract_attributes", "data.extract_attributes"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.build_vocab_maps", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.build_vocab_maps", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_side_info", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.split_with_diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.extract_attributes", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.extract_attributes"], ["continue", "\n", "\n", "# break up tokens", "\n", "", "tokens", "=", "pre", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "post_tokens", "=", "post", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "rels", "=", "rels", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "pos", "=", "pos", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "\n", "# get diff + binary diff masks", "\n", "tok_diff", "=", "diff", "(", "tokens", ",", "post_tokens", ")", "\n", "pre_tok_labels", ",", "post_tok_labels", "=", "get_tok_labels", "(", "tok_diff", ")", "\n", "\n", "# make sure everything lines up    ", "\n", "if", "len", "(", "tokens", ")", "!=", "len", "(", "pre_tok_labels", ")", "or", "len", "(", "tokens", ")", "!=", "len", "(", "rels", ")", "or", "len", "(", "tokens", ")", "!=", "len", "(", "pos", ")", "or", "len", "(", "post_tokens", ")", "!=", "len", "(", "post_tok_labels", ")", ":", "\n", "            ", "skipped", "+=", "1", "\n", "continue", "\n", "\n", "# leave room in the post for start/stop and possible category/class token", "\n", "", "if", "len", "(", "tokens", ")", ">", "max_seq_len", "-", "1", "or", "len", "(", "post_tokens", ")", ">", "max_seq_len", "-", "1", ":", "\n", "            ", "skipped", "+=", "1", "\n", "continue", "\n", "\n", "# category info if provided", "\n", "# TODO -- if provided but not in diyi's data, we fill with random...is that ok?", "\n", "", "if", "categories_path", "is", "not", "None", "and", "revid", "in", "revid2topic", ":", "\n", "            ", "categories", "=", "revid2topic", "[", "revid", "]", "\n", "", "else", ":", "\n", "            ", "categories", "=", "np", ".", "random", ".", "uniform", "(", "size", "=", "43", ")", "# 43 = number of categories", "\n", "categories", "=", "categories", "/", "sum", "(", "categories", ")", "# normalize", "\n", "\n", "", "if", "ARGS", ".", "category_input", ":", "\n", "            ", "category_id", "=", "np", ".", "argmax", "(", "categories", ")", "\n", "tokens", "=", "[", "'[unused%d]'", "%", "category_id", "]", "+", "tokens", "\n", "pre_tok_labels", "=", "[", "EDIT_TYPE2ID", "[", "'mask'", "]", "]", "+", "pre_tok_labels", "\n", "post_tok_labels", "=", "[", "EDIT_TYPE2ID", "[", "'mask'", "]", "]", "+", "post_tok_labels", "\n", "\n", "# add start + end symbols to post in/out", "\n", "", "post_input_tokens", "=", "[", "'\u884c'", "]", "+", "post_tokens", "\n", "post_output_tokens", "=", "post_tokens", "+", "[", "'\u6b62'", "]", "\n", "\n", "# shuffle + convert to ids + pad", "\n", "try", ":", "\n", "            ", "if", "noise", ":", "\n", "                ", "pre_toks", "=", "noise_seq", "(", "\n", "tokens", "[", ":", "]", ",", "\n", "drop_prob", "=", "ARGS", ".", "noise_prob", ",", "\n", "shuf_dist", "=", "ARGS", ".", "shuf_dist", ",", "\n", "drop_set", "=", "drop_set", ",", "\n", "keep_bigrams", "=", "ARGS", ".", "keep_bigrams", ")", "\n", "", "else", ":", "\n", "                ", "pre_toks", "=", "tokens", "\n", "\n", "", "pre_ids", "=", "pad", "(", "[", "tok2id", "[", "x", "]", "for", "x", "in", "pre_toks", "]", ",", "0", ")", "\n", "post_in_ids", "=", "pad", "(", "[", "tok2id", "[", "x", "]", "for", "x", "in", "post_input_tokens", "]", ",", "0", ")", "\n", "post_out_ids", "=", "pad", "(", "[", "tok2id", "[", "x", "]", "for", "x", "in", "post_output_tokens", "]", ",", "0", ")", "\n", "pre_tok_label_ids", "=", "pad", "(", "pre_tok_labels", ",", "EDIT_TYPE2ID", "[", "'mask'", "]", ")", "\n", "post_tok_label_ids", "=", "pad", "(", "post_tok_labels", ",", "EDIT_TYPE2ID", "[", "'mask'", "]", ")", "\n", "rel_ids", "=", "pad", "(", "[", "REL2ID", ".", "get", "(", "x", ",", "REL2ID", "[", "'<UNK>'", "]", ")", "for", "x", "in", "rels", "]", ",", "0", ")", "\n", "pos_ids", "=", "pad", "(", "[", "POS2ID", ".", "get", "(", "x", ",", "POS2ID", "[", "'<UNK>'", "]", ")", "for", "x", "in", "pos", "]", ",", "0", ")", "\n", "", "except", "KeyError", ":", "\n", "# TODO FUCK THIS ENCODING BUG!!!", "\n", "            ", "skipped", "+=", "1", "\n", "continue", "\n", "\n", "", "input_mask", "=", "pad", "(", "[", "0", "]", "*", "len", "(", "tokens", ")", ",", "1", ")", "\n", "pre_len", "=", "len", "(", "tokens", ")", "\n", "\n", "out", "[", "'pre_ids'", "]", ".", "append", "(", "pre_ids", ")", "\n", "out", "[", "'pre_masks'", "]", ".", "append", "(", "input_mask", ")", "\n", "out", "[", "'pre_lens'", "]", ".", "append", "(", "pre_len", ")", "\n", "out", "[", "'post_in_ids'", "]", ".", "append", "(", "post_in_ids", ")", "\n", "out", "[", "'post_out_ids'", "]", ".", "append", "(", "post_out_ids", ")", "\n", "out", "[", "'pre_tok_label_ids'", "]", ".", "append", "(", "pre_tok_label_ids", ")", "\n", "out", "[", "'post_tok_label_ids'", "]", ".", "append", "(", "post_tok_label_ids", ")", "\n", "out", "[", "'rel_ids'", "]", ".", "append", "(", "rel_ids", ")", "\n", "out", "[", "'pos_ids'", "]", ".", "append", "(", "pos_ids", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.sample_replace": [[221, 247], ["enumerate", "range", "random.random", "len", "next.insert", "len", "dist_measurer.most_similar", "next", "tgt_attr.split"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.most_similar"], ["\n", "", "print", "(", "'SKIPPED '", ",", "skipped", ")", "\n", "return", "out", "\n", "\n", "\n", "\n", "", "def", "get_dataloader", "(", "data_path", ",", "tok2id", ",", "batch_size", ",", "\n", "pickle_path", "=", "None", ",", "test", "=", "False", ",", "noise", "=", "False", ",", "add_del_tok", "=", "False", ",", "\n", "categories_path", "=", "None", ",", "sort_batch", "=", "True", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "def", "collate", "(", "data", ")", ":", "\n", "        ", "if", "sort_batch", ":", "\n", "# sort by length for packing/padding", "\n", "            ", "data", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "2", "]", ",", "reverse", "=", "True", ")", "\n", "# group by datatype", "\n", "", "[", "\n", "src_id", ",", "src_mask", ",", "src_len", ",", "\n", "post_in_id", ",", "post_out_id", ",", "\n", "pre_tok_label", ",", "post_tok_label", ",", "\n", "rel_ids", ",", "pos_ids", ",", "categories", "\n", "]", "=", "[", "torch", ".", "stack", "(", "x", ")", "for", "x", "in", "zip", "(", "*", "data", ")", "]", "\n", "\n", "# cut off at max len of this batch for unpacking/repadding", "\n", "max_len", "=", "max", "(", "src_len", ")", "\n", "data", "=", "[", "\n", "src_id", "[", ":", ",", ":", "max_len", "]", ",", "src_mask", "[", ":", ",", ":", "max_len", "]", ",", "src_len", ",", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch": [[249, 303], ["max", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "data.sample_replace", "torch.LongTensor", "torch.LongTensor", "torch.FloatTensor", "input_lines.cuda.cuda", "output_lines.cuda.cuda", "mask.cuda.cuda", "len", "tok2id.get", "tok2id.get", "sorted", "enumerate", "len", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.sample_replace"], ["pre_tok_label", "[", ":", ",", ":", "max_len", "]", ",", "post_tok_label", "[", ":", ",", ":", "max_len", "+", "10", "]", ",", "# +10 for post_toks_labels too (it's just gonna be matched up with post ids)", "\n", "rel_ids", "[", ":", ",", ":", "max_len", "]", ",", "pos_ids", "[", ":", ",", ":", "max_len", "]", ",", "categories", "\n", "]", "\n", "\n", "return", "data", "\n", "\n", "", "if", "pickle_path", "is", "not", "None", "and", "os", ".", "path", ".", "exists", "(", "pickle_path", ")", ":", "\n", "        ", "examples", "=", "pickle", ".", "load", "(", "open", "(", "pickle_path", ",", "'rb'", ")", ")", "\n", "", "else", ":", "\n", "        ", "examples", "=", "get_examples", "(", "\n", "data_path", "=", "data_path", ",", "\n", "tok2id", "=", "tok2id", ",", "\n", "max_seq_len", "=", "ARGS", ".", "max_seq_len", ",", "\n", "noise", "=", "noise", ",", "\n", "add_del_tok", "=", "add_del_tok", ",", "\n", "categories_path", "=", "categories_path", ")", "\n", "\n", "pickle", ".", "dump", "(", "examples", ",", "open", "(", "pickle_path", ",", "'wb'", ")", ")", "\n", "\n", "", "data", "=", "TensorDataset", "(", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pre_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pre_masks'", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", ",", "# byte for masked_fill()", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pre_lens'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'post_in_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'post_out_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pre_tok_label_ids'", "]", ",", "dtype", "=", "torch", ".", "float", ")", ",", "# for compartin to enrichment stuff", "\n", "torch", ".", "tensor", "(", "examples", "[", "'post_tok_label_ids'", "]", ",", "dtype", "=", "torch", ".", "float", ")", ",", "# for loss multiplying", "\n", "torch", ".", "tensor", "(", "examples", "[", "'rel_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'pos_ids'", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "examples", "[", "'categories'", "]", ",", "dtype", "=", "torch", ".", "float", ")", ")", "\n", "\n", "dataloader", "=", "DataLoader", "(", "\n", "data", ",", "\n", "sampler", "=", "(", "SequentialSampler", "(", "data", ")", "if", "test", "else", "RandomSampler", "(", "data", ")", ")", ",", "\n", "collate_fn", "=", "collate", ",", "\n", "batch_size", "=", "batch_size", ")", "\n", "\n", "return", "dataloader", ",", "len", "(", "examples", "[", "'pre_ids'", "]", ")", "\n", "\n", "\n", "", ""]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.minibatch": [[305, 372], ["data.get_minibatch", "data.get_minibatch", "data.get_minibatch", "data.get_minibatch", "len", "torch.autograd.Variable", "random.random", "torch.LongTensor", "attribute_ids.cuda.cuda", "data.get_minibatch", "data.get_minibatch", "data.get_minibatch", "data.get_minibatch", "range", "data.get_minibatch", "data.get_minibatch", "Exception"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.get_minibatch"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.unsort": [[374, 381], ["enumerate"], "function", ["None"], []], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.decoders.AttentionalLSTM.__init__": [[16, 28], ["torch.Module.__init__", "torch.LSTMCell", "torch.LSTMCell", "ops.BilinearAttention"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "config", ",", "attention", ")", ":", "\n", "        ", "\"\"\"Initialize params.\"\"\"", "\n", "super", "(", "AttentionalLSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "num_layers", "=", "1", "\n", "self", ".", "use_attention", "=", "attention", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "cell", "=", "nn", ".", "LSTMCell", "(", "input_dim", ",", "hidden_dim", ")", "\n", "\n", "if", "self", ".", "use_attention", ":", "\n", "            ", "self", ".", "attention_layer", "=", "ops", ".", "BilinearAttention", "(", "hidden_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.decoders.AttentionalLSTM.forward": [[30, 52], ["input.transpose.transpose.transpose", "range", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "output.transpose.transpose.transpose", "input.transpose.transpose.size", "decoders.AttentionalLSTM.cell", "input.transpose.transpose.size", "decoders.AttentionalLSTM.attention_layer", "output.transpose.transpose.append", "output.transpose.transpose.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "output[].size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "hidden", ",", "ctx", ",", "srcmask", ",", "kb", "=", "None", ")", ":", "\n", "        ", "input", "=", "input", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "output", "=", "[", "]", "\n", "timesteps", "=", "range", "(", "input", ".", "size", "(", "0", ")", ")", "\n", "for", "i", "in", "timesteps", ":", "\n", "            ", "hy", ",", "cy", "=", "self", ".", "cell", "(", "input", "[", "i", "]", ",", "hidden", ")", "\n", "if", "self", ".", "use_attention", ":", "\n", "                ", "_", ",", "h_tilde", ",", "alpha", "=", "self", ".", "attention_layer", "(", "hy", ",", "ctx", ",", "srcmask", ")", "\n", "hidden", "=", "h_tilde", ",", "cy", "\n", "output", ".", "append", "(", "h_tilde", ")", "\n", "", "else", ":", "\n", "                ", "hidden", "=", "hy", ",", "cy", "\n", "output", ".", "append", "(", "hy", ")", "\n", "\n", "# combine outputs, and get into [time, batch, dim]", "\n", "", "", "output", "=", "torch", ".", "cat", "(", "output", ",", "0", ")", ".", "view", "(", "\n", "input", ".", "size", "(", "0", ")", ",", "*", "output", "[", "0", "]", ".", "size", "(", ")", ")", "\n", "\n", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "return", "output", ",", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.decoders.StackedAttentionLSTM.__init__": [[57, 72], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "range", "cell_class", "decoders.StackedAttentionLSTM.add_module", "decoders.StackedAttentionLSTM.layers.append"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__"], ["def", "__init__", "(", "self", ",", "cell_class", "=", "AttentionalLSTM", ",", "config", "=", "None", ")", ":", "\n", "        ", "super", "(", "StackedAttentionLSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "options", "=", "config", "[", "'model'", "]", "\n", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "options", "[", "'dropout'", "]", ")", "\n", "\n", "self", ".", "layers", "=", "[", "]", "\n", "input_dim", "=", "self", ".", "options", "[", "'emb_dim'", "]", "\n", "hidden_dim", "=", "self", ".", "options", "[", "'tgt_hidden_dim'", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "options", "[", "'tgt_layers'", "]", ")", ":", "\n", "            ", "layer", "=", "cell_class", "(", "input_dim", ",", "hidden_dim", ",", "config", ",", "config", "[", "'model'", "]", "[", "'attention'", "]", ")", "\n", "self", ".", "add_module", "(", "'layer_%d'", "%", "i", ",", "layer", ")", "\n", "self", ".", "layers", ".", "append", "(", "layer", ")", "\n", "input_dim", "=", "hidden_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.decoders.StackedAttentionLSTM.forward": [[74, 91], ["enumerate", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "layer", "torch.stack.append", "torch.stack.append", "torch.stack.append", "torch.stack.append", "len", "decoders.StackedAttentionLSTM.dropout"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input", ",", "hidden", ",", "ctx", ",", "srcmask", ",", "kb", "=", "None", ")", ":", "\n", "        ", "h_final", ",", "c_final", "=", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "output", ",", "(", "h_final_i", ",", "c_final_i", ")", "=", "layer", "(", "input", ",", "hidden", ",", "ctx", ",", "srcmask", ",", "kb", ")", "\n", "\n", "input", "=", "output", "\n", "\n", "if", "i", "!=", "len", "(", "self", ".", "layers", ")", ":", "\n", "                ", "input", "=", "self", ".", "dropout", "(", "input", ")", "\n", "\n", "", "h_final", ".", "append", "(", "h_final_i", ")", "\n", "c_final", ".", "append", "(", "c_final_i", ")", "\n", "\n", "", "h_final", "=", "torch", ".", "stack", "(", "h_final", ")", "\n", "c_final", "=", "torch", ".", "stack", "(", "c_final", ")", "\n", "\n", "return", "input", ",", "(", "h_final", ",", "c_final", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.utils.nested_iter": [[4, 11], ["sorted", "d.items", "isinstance", "utils.nested_iter"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.utils.nested_iter"], ["import", "torch", ".", "optim", "as", "optim", "\n", "from", "torch", ".", "nn", "import", "CrossEntropyLoss", "\n", "from", "pytorch_pretrained_bert", ".", "optimization", "import", "BertAdam", "\n", "\n", "import", "sys", ";", "sys", ".", "path", ".", "append", "(", "'.'", ")", "\n", "from", "shared", ".", "args", "import", "ARGS", "\n", "from", "shared", ".", "constants", "import", "CUDA", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.utils.config_val_string": [[12, 16], ["map", "utils.nested_iter", "str"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.utils.nested_iter"], ["\n", "def", "build_optimizer", "(", "model", ",", "num_train_steps", ",", "learning_rate", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "if", "ARGS", ".", "tagger_from_debiaser", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.utils.config_key_string": [[17, 21], ["map", "utils.nested_iter", "str"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.utils.nested_iter"], ["        ", "parameters", "=", "list", "(", "model", ".", "cls_classifier", ".", "parameters", "(", ")", ")", "+", "list", "(", "\n", "model", ".", "tok_classifier", ".", "parameters", "(", ")", ")", "\n", "parameters", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "parameters", ")", ")", "\n", "return", "optim", ".", "Adam", "(", "parameters", ",", "lr", "=", "ARGS", ".", "learning_rate", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_precisions_recalls_DEPRECIATED": [[20, 49], ["list", "len", "len", "len", "zip", "set", "set", "set", "set", "set", "set", "set", "set", "set", "evaluation.get_precisions_recalls_DEPRECIATED.precision_recall"], "function", ["None"], ["def", "get_precisions_recalls_DEPRECIATED", "(", "inputs", ",", "preds", ",", "ground_truths", ")", ":", "\n", "    ", "\"\"\" v1 of precision/recall based on some dumb logic \"\"\"", "\n", "def", "precision_recall", "(", "src", ",", "tgt", ",", "pred", ")", ":", "\n", "        ", "\"\"\"\n        src: [string tokens], the input to the model\n        tgt: [string tokens], the gold targets\n        pred: [string tokens], the model outputs\n        \"\"\"", "\n", "tgt_unique", "=", "set", "(", "tgt", ")", "-", "set", "(", "src", ")", "\n", "src_unique", "=", "set", "(", "src", ")", "-", "set", "(", "tgt", ")", "\n", "\n", "# new words the model correctly introduced", "\n", "true_positives", "=", "len", "(", "set", "(", "pred", ")", "&", "tgt_unique", ")", "\n", "# new words the model incorrectly introduced", "\n", "false_positives", "=", "len", "(", "set", "(", "pred", ")", "-", "set", "(", "src", ")", "-", "set", "(", "tgt", ")", ")", "\n", "# old words the model incorrectly retained", "\n", "false_negatives", "=", "len", "(", "set", "(", "pred", ")", "&", "src_unique", ")", "\n", "\n", "precision", "=", "true_positives", "*", "1.0", "/", "(", "true_positives", "+", "false_positives", "+", "0.001", ")", "\n", "recall", "=", "true_postitives", "*", "1.0", "/", "(", "true_positives", "+", "false_negatives", "+", "0.001", ")", "\n", "\n", "return", "precision", ",", "recall", "\n", "\n", "", "[", "precisions", ",", "recalls", "]", "=", "list", "(", "zip", "(", "*", "[", "\n", "precision_recall", "(", "src", ",", "tgt", ",", "pred", ")", "\n", "for", "src", ",", "tgt", ",", "pred", "in", "zip", "(", "inputs", ",", "ground_truths", ",", "preds", ")", "\n", "]", ")", ")", "\n", "\n", "return", "precisions", ",", "recalls", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.bleu_stats": [[58, 82], ["stats.append", "stats.append", "range", "len", "len", "collections.Counter", "collections.Counter", "stats.append", "stats.append", "max", "max", "len", "tuple", "tuple", "range", "evaluation.bleu_stats.is_valid_ngram"], "function", ["None"], ["", "def", "bleu_stats", "(", "hypothesis", ",", "reference", ",", "word_list", "=", "None", ")", ":", "\n", "    ", "\"\"\"Compute statistics for BLEU.\"\"\"", "\n", "\n", "def", "is_valid_ngram", "(", "ngram", ")", ":", "\n", "        ", "if", "word_list", "is", "None", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "len", "(", "set", "(", "word_list", ")", "&", "set", "(", "ngram", ")", ")", ">", "0", "\n", "\n", "", "", "stats", "=", "[", "]", "\n", "stats", ".", "append", "(", "len", "(", "hypothesis", ")", ")", "\n", "stats", ".", "append", "(", "len", "(", "reference", ")", ")", "\n", "for", "n", "in", "range", "(", "1", ",", "5", ")", ":", "\n", "        ", "s_ngrams", "=", "Counter", "(", "[", "\n", "tuple", "(", "hypothesis", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "hypothesis", ")", "+", "1", "-", "n", ")", "\n", "if", "is_valid_ngram", "(", "hypothesis", "[", "i", ":", "i", "+", "n", "]", ")", "\n", "]", ")", "\n", "r_ngrams", "=", "Counter", "(", "[", "\n", "tuple", "(", "reference", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "reference", ")", "+", "1", "-", "n", ")", "\n", "if", "is_valid_ngram", "(", "reference", "[", "i", ":", "i", "+", "n", "]", ")", "\n", "]", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "sum", "(", "(", "s_ngrams", "&", "r_ngrams", ")", ".", "values", "(", ")", ")", ",", "0", "]", ")", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "len", "(", "hypothesis", ")", "+", "1", "-", "n", ",", "0", "]", ")", ")", "\n", "", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.bleu": [[83, 92], ["math.exp", "len", "sum", "list", "min", "filter", "math.log", "zip", "float", "float"], "function", ["None"], ["", "def", "bleu", "(", "stats", ")", ":", "\n", "    ", "\"\"\"Compute BLEU given n-gram statistics.\"\"\"", "\n", "if", "len", "(", "list", "(", "filter", "(", "lambda", "x", ":", "x", "==", "0", ",", "stats", ")", ")", ")", ">", "0", ":", "\n", "        ", "return", "0", "\n", "", "(", "c", ",", "r", ")", "=", "stats", "[", ":", "2", "]", "\n", "log_bleu_prec", "=", "sum", "(", "\n", "[", "math", ".", "log", "(", "float", "(", "x", ")", "/", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "stats", "[", "2", ":", ":", "2", "]", ",", "stats", "[", "3", ":", ":", "2", "]", ")", "]", "\n", ")", "/", "4.", "\n", "return", "math", ".", "exp", "(", "min", "(", "[", "0", ",", "1", "-", "float", "(", "r", ")", "/", "c", "]", ")", "+", "log_bleu_prec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_bleu": [[93, 106], ["numpy.array", "zip", "numpy.array", "evaluation.bleu", "evaluation.bleu_stats", "range", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.bleu", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.bleu_stats"], ["", "def", "get_bleu", "(", "hypotheses", ",", "reference", ",", "word_lists", "=", "None", ")", ":", "\n", "    ", "\"\"\"Get validation BLEU score for dev set.\n        If provided with a list of word lists, then we'll only consider\n            ngrams with words from that list.\n    \"\"\"", "\n", "stats", "=", "np", ".", "array", "(", "[", "0.", ",", "0.", ",", "0.", ",", "0.", ",", "0.", ",", "0.", ",", "0.", ",", "0.", ",", "0.", ",", "0.", "]", ")", "\n", "\n", "if", "word_lists", "is", "None", ":", "\n", "        ", "word_lists", "=", "[", "None", "for", "_", "in", "range", "(", "len", "(", "hypotheses", ")", ")", "]", "\n", "\n", "", "for", "hyp", ",", "ref", ",", "wlist", "in", "zip", "(", "hypotheses", ",", "reference", ",", "word_lists", ")", ":", "\n", "        ", "stats", "+=", "np", ".", "array", "(", "bleu_stats", "(", "hyp", ",", "ref", ",", "word_list", "=", "wlist", ")", ")", "\n", "", "return", "100", "*", "bleu", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_precision_recall": [[108, 132], ["list", "len", "zip", "numpy.average", "numpy.average", "set", "set", "len", "len", "len", "len", "evaluation.get_precisions_recalls_DEPRECIATED.precision_recall"], "function", ["None"], ["", "def", "get_precision_recall", "(", "inputs", ",", "top_k_preds", ",", "ground_truths", ",", "k", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Precision@k = (# of generated candidates @k that are relevant to targets) / (# of generated candidates @k)\n    Recall@k = (# of generated candidates @k that are relevant to targets) / (total # of relevant targets)\n    \n    top_k_preds: [Batch, length, k]\n    \"\"\"", "\n", "if", "not", "k", ":", "\n", "        ", "k", "=", "len", "(", "top_k_preds", "[", "0", "]", "[", "0", "]", ")", "\n", "\n", "", "def", "precision_recall", "(", "src", ",", "tgt", ",", "top_k", ")", ":", "\n", "        ", "tgt_unique", "=", "set", "(", "tgt", ")", "-", "set", "(", "src", ")", "\n", "pred_toks", "=", "[", "tok", "for", "klist", "in", "top_k", "for", "tok", "in", "klist", "[", ":", "k", "]", "]", "\n", "precision", "=", "len", "(", "tgt_unique", "&", "set", "(", "pred_toks", ")", ")", "*", "1.0", "/", "(", "len", "(", "pred_toks", ")", "+", "0.0001", ")", "\n", "recall", "=", "len", "(", "tgt_unique", "&", "set", "(", "pred_toks", ")", ")", "*", "1.0", "/", "(", "len", "(", "tgt_unique", ")", "+", "0.0001", ")", "\n", "\n", "return", "precision", ",", "recall", "\n", "\n", "", "[", "precisions", ",", "recalls", "]", "=", "list", "(", "zip", "(", "*", "[", "\n", "precision_recall", "(", "src", ",", "tgt", ",", "pred", ")", "\n", "for", "src", ",", "tgt", ",", "pred", "in", "zip", "(", "inputs", ",", "ground_truths", ",", "top_k_preds", ")", "\n", "]", ")", ")", "\n", "\n", "return", "np", ".", "average", "(", "precisions", ")", ",", "np", ".", "average", "(", "recalls", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_edit_distance": [[135, 141], ["zip", "editdistance.eval", "len"], "function", ["None"], ["", "def", "get_edit_distance", "(", "hypotheses", ",", "reference", ")", ":", "\n", "    ", "ed", "=", "0", "\n", "for", "hyp", ",", "ref", "in", "zip", "(", "hypotheses", ",", "reference", ")", ":", "\n", "        ", "ed", "+=", "editdistance", ".", "eval", "(", "hyp", ",", "ref", ")", "\n", "\n", "", "return", "ed", "*", "1.0", "/", "len", "(", "hypotheses", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.decode_minibatch": [[143, 181], ["torch.autograd.Variable", "range", "numpy.array", "numpy.transpose", "numpy.array_equal", "torch.LongTensor", "torch.LongTensor", "torch.cat.cuda", "model", "torch.autograd.Variable", "torch.cat", "torch.cat", "np.transpose.append", "tgt_input[].data.cpu().numpy", "word_probs.data.cpu().numpy", "numpy.argsort", "torch.from_numpy", "torch.from_numpy", "next_preds.cuda.cuda", "next_preds.cuda.unsqueeze", "tgt_input[].data.cpu", "range", "word_probs.data.cpu", "src_input.size"], "function", ["None"], ["", "def", "decode_minibatch", "(", "max_len", ",", "start_id", ",", "model", ",", "src_input", ",", "srclens", ",", "srcmask", ",", "\n", "aux_input", ",", "auxlens", ",", "auxmask", ",", "side_info", ",", "k", ")", ":", "\n", "    ", "\"\"\" argmax decoding \"\"\"", "\n", "# Initialize target with <s> for every sentence", "\n", "tgt_input", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "[", "\n", "[", "start_id", "]", "for", "i", "in", "range", "(", "src_input", ".", "size", "(", "0", ")", ")", "\n", "]", ")", ")", "\n", "if", "CUDA", ":", "\n", "        ", "tgt_input", "=", "tgt_input", ".", "cuda", "(", ")", "\n", "\n", "", "top_k_toks", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "max_len", ")", ":", "\n", "# run input through the model", "\n", "        ", "decoder_logit", ",", "word_probs", ",", "_", ",", "_", "=", "model", "(", "src_input", ",", "tgt_input", ",", "srcmask", ",", "srclens", ",", "\n", "aux_input", ",", "auxmask", ",", "auxlens", ",", "side_info", ")", "\n", "\n", "# logits for the latest timestep", "\n", "word_probs", "=", "word_probs", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "# sorted indices (descending)", "\n", "sorted_indices", "=", "np", ".", "argsort", "(", "(", "word_probs", ")", ")", "[", ":", ",", ":", ":", "-", "1", "]", "\n", "# select the predicted \"next\" tokens, attach to target-side inputs", "\n", "next_preds", "=", "Variable", "(", "torch", ".", "from_numpy", "(", "sorted_indices", "[", ":", ",", "0", "]", ")", ")", "\n", "if", "CUDA", ":", "\n", "            ", "next_preds", "=", "next_preds", ".", "cuda", "(", ")", "\n", "", "tgt_input", "=", "torch", ".", "cat", "(", "(", "tgt_input", ",", "next_preds", ".", "unsqueeze", "(", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "# remember the top k indices at this step for evaluation", "\n", "top_k_toks", ".", "append", "(", "sorted_indices", "[", ":", ",", ":", "k", "]", ")", "\n", "\n", "# make top_k_toks into [Batch, Length, k] tensor", "\n", "", "top_k_toks", "=", "np", ".", "array", "(", "top_k_toks", ")", "\n", "top_k_toks", "=", "np", ".", "transpose", "(", "top_k_toks", ",", "(", "1", ",", "0", ",", "2", ")", ")", "\n", "\n", "# make sure the top k=1 tokens is equal to the true model predictions (argmax)", "\n", "assert", "np", ".", "array_equal", "(", "\n", "top_k_toks", "[", ":", ",", ":", ",", "0", "]", ",", "\n", "tgt_input", "[", ":", ",", "1", ":", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "# ignore <s> kickstart", "\n", "\n", "return", "top_k_toks", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.ids_to_toks": [[183, 207], ["isinstance", "enumerate", "data.unsort", "tok_seqs.cpu().numpy.cpu().numpy", "data.unsort.append", "tok_seqs.cpu().numpy.cpu", "toks.index", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.unsort"], ["", "def", "ids_to_toks", "(", "tok_seqs", ",", "id2tok", ",", "sort_indices", ",", "cuts", "=", "None", ",", "save_cuts", "=", "False", ")", ":", "\n", "    ", "out", "=", "[", "]", "\n", "cut_indices", "=", "[", "]", "\n", "# take off the gpu", "\n", "if", "isinstance", "(", "tok_seqs", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "tok_seqs", "=", "tok_seqs", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# convert to toks, cut off at </s>", "\n", "", "for", "i", ",", "line", "in", "enumerate", "(", "tok_seqs", ")", ":", "\n", "        ", "toks", "=", "[", "id2tok", "[", "x", "]", "for", "x", "in", "line", "]", "\n", "if", "cuts", "is", "not", "None", ":", "\n", "            ", "cut_idx", "=", "cuts", "[", "i", "]", "\n", "", "elif", "'</s>'", "in", "toks", ":", "\n", "            ", "cut_idx", "=", "toks", ".", "index", "(", "'</s>'", ")", "\n", "", "else", ":", "\n", "            ", "cut_idx", "=", "len", "(", "toks", ")", "\n", "", "out", ".", "append", "(", "toks", "[", ":", "cut_idx", "]", ")", "\n", "cut_indices", "+=", "[", "cut_idx", "]", "\n", "# unsort", "\n", "", "out", "=", "data", ".", "unsort", "(", "out", ",", "sort_indices", ")", "\n", "\n", "if", "save_cuts", ":", "\n", "        ", "return", "out", ",", "cut_indices", "\n", "", "else", ":", "\n", "        ", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.decode_dataset": [[208, 275], ["range", "len", "sys.stdout.write", "sys.stdout.flush", "data.minibatch", "evaluation.decode_minibatch", "evaluation.ids_to_toks", "evaluation.ids_to_toks", "evaluation.ids_to_toks", "evaluation.ids_to_toks", "range", "len", "data.unsort", "range", "top_k_pred.append", "range", "evaluation.ids_to_toks", "range", "evaluation.ids_to_toks", "len", "str", "input_ids_aux.data.cpu().numpy", "range", "input_ids_aux.data.cpu", "range"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.decode_minibatch", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.ids_to_toks", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.ids_to_toks", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.ids_to_toks", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.ids_to_toks", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.unsort", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.ids_to_toks", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.ids_to_toks"], ["", "", "def", "decode_dataset", "(", "model", ",", "src", ",", "tgt", ",", "config", ",", "k", "=", "20", ")", ":", "\n", "    ", "\"\"\"Evaluate model.\"\"\"", "\n", "inputs", "=", "[", "]", "\n", "preds", "=", "[", "]", "\n", "top_k_preds", "=", "[", "]", "\n", "auxs", "=", "[", "]", "\n", "ground_truths", "=", "[", "]", "\n", "raw_srcs", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "0", ",", "len", "(", "src", "[", "'data'", "]", ")", ",", "config", "[", "'data'", "]", "[", "'batch_size'", "]", ")", ":", "\n", "        ", "sys", ".", "stdout", ".", "write", "(", "\"\\r%s/%s...\"", "%", "(", "j", ",", "len", "(", "src", "[", "'data'", "]", ")", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "# get batch", "\n", "input_content", ",", "input_aux", ",", "output", ",", "side_info", ",", "raw_src", "=", "data", ".", "minibatch", "(", "\n", "src", ",", "tgt", ",", "j", ",", "\n", "config", "[", "'data'", "]", "[", "'batch_size'", "]", ",", "\n", "config", "[", "'data'", "]", "[", "'max_len'", "]", ",", "\n", "config", ",", "\n", "is_test", "=", "True", ")", "\n", "input_lines_src", ",", "output_lines_src", ",", "srclens", ",", "srcmask", ",", "indices", "=", "input_content", "\n", "input_ids_aux", ",", "_", ",", "auxlens", ",", "auxmask", ",", "_", "=", "input_aux", "\n", "input_lines_tgt", ",", "output_lines_tgt", ",", "_", ",", "_", ",", "_", "=", "output", "\n", "_", ",", "raw_src", ",", "_", ",", "_", ",", "_", "=", "raw_src", "\n", "side_info", ",", "_", ",", "_", ",", "_", ",", "_", "=", "side_info", "\n", "\n", "# TODO -- beam search", "\n", "tgt_pred_top_k", "=", "decode_minibatch", "(", "\n", "config", "[", "'data'", "]", "[", "'max_len'", "]", ",", "tgt", "[", "'tok2id'", "]", "[", "'<s>'", "]", ",", "\n", "model", ",", "input_lines_src", ",", "srclens", ",", "srcmask", ",", "\n", "input_ids_aux", ",", "auxlens", ",", "auxmask", ",", "side_info", ",", "k", "=", "k", ")", "\n", "\n", "# convert inputs/preds/targets/aux to human-readable form", "\n", "inputs", "+=", "ids_to_toks", "(", "output_lines_src", ",", "src", "[", "'id2tok'", "]", ",", "indices", ")", "\n", "ground_truths", "+=", "ids_to_toks", "(", "output_lines_tgt", ",", "tgt", "[", "'id2tok'", "]", ",", "indices", ")", "\n", "raw_srcs", "+=", "ids_to_toks", "(", "raw_src", ",", "src", "[", "'id2tok'", "]", ",", "indices", ")", "\n", "\n", "# TODO -- refactor this stuff!! it's shitty", "\n", "# get the \"offical\" predictions from the model", "\n", "pred_toks", ",", "pred_lens", "=", "ids_to_toks", "(", "\n", "tgt_pred_top_k", "[", ":", ",", ":", ",", "0", "]", ",", "tgt", "[", "'id2tok'", "]", ",", "indices", ",", "save_cuts", "=", "True", ")", "\n", "preds", "+=", "pred_toks", "\n", "# now get all the other top-k prediction levels", "\n", "top_k_pred", "=", "[", "pred_toks", "]", "\n", "for", "i", "in", "range", "(", "k", "-", "1", ")", ":", "\n", "            ", "top_k_pred", ".", "append", "(", "ids_to_toks", "(", "\n", "tgt_pred_top_k", "[", ":", ",", ":", ",", "i", "+", "1", "]", ",", "tgt", "[", "'id2tok'", "]", ",", "indices", ",", "cuts", "=", "pred_lens", ")", "\n", ")", "\n", "# top_k_pred is [k, batch, length] where length is ragged", "\n", "# but we want it in [batch, length, k]. Manual transpose b/c ragged :( ", "\n", "", "batch_size", "=", "len", "(", "top_k_pred", "[", "0", "]", ")", "# could be variable at test time", "\n", "pred_lens", "=", "data", ".", "unsort", "(", "pred_lens", ",", "indices", ")", "\n", "top_k_pred_transposed", "=", "[", "[", "]", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "for", "bi", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "ti", "in", "range", "(", "pred_lens", "[", "bi", "]", ")", ":", "\n", "                ", "top_k_pred_transposed", "[", "bi", "]", "+=", "[", "[", "\n", "top_k_pred", "[", "ki", "]", "[", "bi", "]", "[", "ti", "]", "for", "ki", "in", "range", "(", "k", ")", "\n", "]", "]", "\n", "", "", "top_k_preds", "+=", "top_k_pred_transposed", "\n", "\n", "if", "config", "[", "'model'", "]", "[", "'model_type'", "]", "==", "'delete'", ":", "\n", "            ", "auxs", "+=", "[", "[", "str", "(", "x", ")", "]", "for", "x", "in", "input_ids_aux", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", "# because of list comp in inference_metrics()", "\n", "", "elif", "config", "[", "'model'", "]", "[", "'model_type'", "]", "==", "'delete_retrieve'", ":", "\n", "            ", "auxs", "+=", "ids_to_toks", "(", "input_ids_aux", ",", "tgt", "[", "'id2tok'", "]", ",", "indices", ")", "\n", "", "elif", "config", "[", "'model'", "]", "[", "'model_type'", "]", "==", "'seq2seq'", ":", "\n", "            ", "auxs", "+=", "[", "'None'", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "", "", "return", "inputs", ",", "preds", ",", "top_k_preds", ",", "ground_truths", ",", "auxs", ",", "raw_srcs", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_metrics": [[277, 316], ["evaluation.get_bleu", "evaluation.get_bleu", "evaluation.get_bleu", "evaluation.get_edit_distance", "evaluation.get_precision_recall", "evaluation.get_precision_recall", "classifier.error_rate", "set", "set", "zip", "set", "set", "zip", "range", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_bleu", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_bleu", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_bleu", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_edit_distance", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_precision_recall", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_precision_recall", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.error_rate"], ["", "def", "get_metrics", "(", "inputs", ",", "preds", ",", "ground_truths", ",", "top_k_preds", "=", "None", ",", "classifier", "=", "None", ")", ":", "\n", "    ", "bleu", "=", "get_bleu", "(", "preds", ",", "ground_truths", ")", "\n", "src_bleu", "=", "get_bleu", "(", "\n", "preds", ",", "inputs", ",", "\n", "word_lists", "=", "[", "\n", "set", "(", "src", ")", "-", "set", "(", "tgt", ")", "for", "src", ",", "tgt", "in", "zip", "(", "inputs", ",", "ground_truths", ")", "\n", "]", "\n", ")", "\n", "tgt_bleu", "=", "get_bleu", "(", "\n", "preds", ",", "ground_truths", ",", "\n", "word_lists", "=", "[", "\n", "set", "(", "tgt", ")", "-", "set", "(", "src", ")", "for", "src", ",", "tgt", "in", "zip", "(", "inputs", ",", "ground_truths", ")", "\n", "]", "\n", ")", "\n", "\n", "edit_distance", "=", "get_edit_distance", "(", "preds", ",", "ground_truths", ")", "\n", "\n", "if", "top_k_preds", "is", "None", ":", "\n", "        ", "top_k_preds", "=", "[", "[", "[", "x", "]", "for", "x", "in", "seq", "]", "for", "seq", "in", "preds", "]", "\n", "", "tgt_precision", ",", "tgt_recall", "=", "get_precision_recall", "(", "inputs", ",", "top_k_preds", ",", "ground_truths", ")", "\n", "src_precision", ",", "src_recall", "=", "get_precision_recall", "(", "ground_truths", ",", "top_k_preds", ",", "inputs", ")", "\n", "\n", "if", "classifier", "is", "not", "None", ":", "\n", "        ", "classifier_error", "=", "classifier", ".", "error_rate", "(", "\n", "seqs", "=", "[", "' '", ".", "join", "(", "seq", ")", "for", "seq", "in", "preds", "]", ",", "\n", "Y", "=", "[", "1", "for", "_", "in", "range", "(", "len", "(", "preds", ")", ")", "]", ")", "# we're trying to create \"target\" seqs", "\n", "", "else", ":", "\n", "        ", "classifier_error", "=", "-", "1", "\n", "\n", "", "return", "{", "\n", "'bleu'", ":", "bleu", ",", "\n", "'src_bleu'", ":", "src_bleu", ",", "\n", "'tgt_bleu'", ":", "tgt_bleu", ",", "\n", "'edit_distance'", ":", "edit_distance", ",", "\n", "'tgt_precision'", ":", "tgt_precision", ",", "\n", "'src_precision'", ":", "src_precision", ",", "\n", "'tgt_recall'", ":", "tgt_recall", ",", "\n", "'src_recall'", ":", "src_recall", ",", "\n", "'classifier_error'", ":", "classifier_error", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.inference_metrics": [[319, 337], ["evaluation.decode_dataset", "models.TextClassifier.from_pickle", "evaluation.get_metrics"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.decode_dataset", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.from_pickle", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.get_metrics"], ["", "def", "inference_metrics", "(", "model", ",", "src", ",", "tgt", ",", "config", ")", ":", "\n", "    ", "\"\"\" decode and evaluate bleu \"\"\"", "\n", "inputs", ",", "preds", ",", "top_k_preds", ",", "ground_truths", ",", "auxs", ",", "raw_srcs", "=", "decode_dataset", "(", "\n", "model", ",", "src", ",", "tgt", ",", "config", ",", "k", "=", "config", "[", "'eval'", "]", "[", "'precision_recall_k'", "]", ")", "\n", "\n", "eval_classifier", "=", "models", ".", "TextClassifier", ".", "from_pickle", "(", "\n", "config", "[", "'eval'", "]", "[", "'classifier_path'", "]", ")", "\n", "\n", "metrics", "=", "get_metrics", "(", "\n", "raw_srcs", ",", "preds", ",", "ground_truths", ",", "\n", "top_k_preds", "=", "top_k_preds", ",", "classifier", "=", "eval_classifier", ")", "\n", "\n", "inputs", "=", "[", "' '", ".", "join", "(", "seq", ")", "for", "seq", "in", "inputs", "]", "\n", "preds", "=", "[", "' '", ".", "join", "(", "seq", ")", "for", "seq", "in", "preds", "]", "\n", "ground_truths", "=", "[", "' '", ".", "join", "(", "seq", ")", "for", "seq", "in", "ground_truths", "]", "\n", "auxs", "=", "[", "' '", ".", "join", "(", "seq", ")", "for", "seq", "in", "auxs", "]", "\n", "\n", "return", "metrics", ",", "inputs", ",", "preds", ",", "ground_truths", ",", "auxs", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.evaluation.evaluate_lpp": [[339, 380], ["torch.ones", "torch.ones", "torch.CrossEntropyLoss", "range", "numpy.mean", "len", "weight_mask.cuda.cuda", "loss_criterion.cuda.cuda", "len", "sys.stdout.write", "sys.stdout.flush", "data.minibatch", "model", "loss_criterion.cuda.", "losses.append", "decoder_logit.contiguous().view", "output_lines_tgt.view", "len", "len", "decoder_logit.contiguous"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.data.minibatch"], ["", "def", "evaluate_lpp", "(", "model", ",", "src", ",", "tgt", ",", "config", ")", ":", "\n", "    ", "\"\"\" evaluate log perplexity WITHOUT decoding\n        (i.e., with teacher forcing)\n    \"\"\"", "\n", "weight_mask", "=", "torch", ".", "ones", "(", "len", "(", "tgt", "[", "'tok2id'", "]", ")", ")", "\n", "if", "CUDA", ":", "\n", "        ", "weight_mask", "=", "weight_mask", ".", "cuda", "(", ")", "\n", "", "weight_mask", "[", "tgt", "[", "'tok2id'", "]", "[", "'<pad>'", "]", "]", "=", "0", "\n", "loss_criterion", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ")", "\n", "if", "CUDA", ":", "\n", "        ", "loss_criterion", "=", "loss_criterion", ".", "cuda", "(", ")", "\n", "\n", "", "losses", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "0", ",", "len", "(", "src", "[", "'data'", "]", ")", ",", "config", "[", "'data'", "]", "[", "'batch_size'", "]", ")", ":", "\n", "        ", "sys", ".", "stdout", ".", "write", "(", "\"\\r%s/%s...\"", "%", "(", "j", ",", "len", "(", "src", "[", "'data'", "]", ")", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "# get batch", "\n", "input_content", ",", "input_aux", ",", "output", ",", "side_info", ",", "_", "=", "data", ".", "minibatch", "(", "\n", "src", ",", "tgt", ",", "j", ",", "\n", "config", "[", "'data'", "]", "[", "'batch_size'", "]", ",", "\n", "config", "[", "'data'", "]", "[", "'max_len'", "]", ",", "\n", "config", ",", "\n", "is_test", "=", "True", ")", "\n", "input_lines_src", ",", "_", ",", "srclens", ",", "srcmask", ",", "_", "=", "input_content", "\n", "input_ids_aux", ",", "_", ",", "auxlens", ",", "auxmask", ",", "_", "=", "input_aux", "\n", "input_lines_tgt", ",", "output_lines_tgt", ",", "_", ",", "_", ",", "_", "=", "output", "\n", "side_info", ",", "_", ",", "_", ",", "_", ",", "_", "=", "side_info", "\n", "\n", "decoder_logit", ",", "decoder_probs", ",", "_", ",", "_", "=", "model", "(", "\n", "input_lines_src", ",", "input_lines_tgt", ",", "srcmask", ",", "srclens", ",", "\n", "input_ids_aux", ",", "auxlens", ",", "auxmask", ",", "\n", "side_info", ")", "\n", "\n", "loss", "=", "loss_criterion", "(", "\n", "decoder_logit", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "len", "(", "tgt", "[", "'tok2id'", "]", ")", ")", ",", "\n", "output_lines_tgt", ".", "view", "(", "-", "1", ")", "\n", ")", "\n", "losses", ".", "append", "(", "loss", ".", "data", "[", "0", "]", ")", "\n", "\n", "", "return", "np", ".", "mean", "(", "losses", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.add_tags.get_pos_dep": [[12, 39], ["add_tags.get_pos_dep.words_from_toks"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.tag_corpusfile.words_from_toks"], ["def", "get_pos_dep", "(", "toks", ")", ":", "\n", "    ", "def", "words_from_toks", "(", "toks", ")", ":", "\n", "        ", "words", "=", "[", "]", "\n", "word_indices", "=", "[", "]", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "toks", ")", ":", "\n", "            ", "if", "tok", ".", "startswith", "(", "'##'", ")", ":", "\n", "                ", "words", "[", "-", "1", "]", "+=", "tok", ".", "replace", "(", "'##'", ",", "''", ")", "\n", "word_indices", "[", "-", "1", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                ", "words", ".", "append", "(", "tok", ")", "\n", "word_indices", ".", "append", "(", "[", "i", "]", ")", "\n", "", "", "return", "words", ",", "word_indices", "\n", "\n", "", "out_pos", ",", "out_dep", "=", "[", "]", ",", "[", "]", "\n", "words", ",", "word_indices", "=", "words_from_toks", "(", "toks", ")", "\n", "analysis", "=", "NLP", "(", "' '", ".", "join", "(", "words", ")", ")", "\n", "\n", "if", "len", "(", "analysis", ")", "!=", "len", "(", "words", ")", ":", "\n", "        ", "return", "None", ",", "None", "\n", "\n", "", "for", "analysis_tok", ",", "idx", "in", "zip", "(", "analysis", ",", "word_indices", ")", ":", "\n", "        ", "out_pos", "+=", "[", "analysis_tok", ".", "pos_", "]", "*", "len", "(", "idx", ")", "\n", "out_dep", "+=", "[", "analysis_tok", ".", "dep_", "]", "*", "len", "(", "idx", ")", "\n", "\n", "", "assert", "len", "(", "out_pos", ")", "==", "len", "(", "out_dep", ")", "==", "len", "(", "toks", ")", "\n", "\n", "return", "' '", ".", "join", "(", "out_pos", ")", ",", "' '", ".", "join", "(", "out_dep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.add_tags.main": [[41, 51], ["tqdm.tqdm", "open", "line.strip().split", "add_tags.get_pos_dep", "sum", "len", "parts[].split", "print", "line.strip", "open"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.add_tags.get_pos_dep"], ["", "def", "main", "(", "in_file", ")", ":", "\n", "    ", "for", "line", "in", "tqdm", "(", "open", "(", "in_file", ")", ",", "total", "=", "sum", "(", "1", "for", "_", "in", "open", "(", "in_file", ")", ")", ")", ":", "\n", "    \t", "parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "parts", ")", "!=", "5", ":", "\n", "    \t\t", "continue", "\n", "\n", "", "pre_pos", ",", "pre_dep", "=", "get_pos_dep", "(", "parts", "[", "1", "]", ".", "split", "(", ")", ")", "\n", "\n", "if", "pre_pos", "is", "not", "None", "and", "pre_dep", "is", "not", "None", ":", "\n", "    \t\t", "print", "(", "'\\t'", ".", "join", "(", "parts", "+", "[", "pre_pos", ",", "pre_dep", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.crawl_revision_text.print_withcolor": [[54, 74], ["l.replace.replace", "re.compile", "re.compile", "enumerate", "print", "re.finditer", "m.end", "m.group", "str", "m.start"], "function", ["None"], ["", "def", "print_withcolor", "(", "idx", ",", "l", ")", ":", "\n", "    ", "l", "=", "l", ".", "replace", "(", "'\\n'", ",", "''", ")", "\n", "ins_p", "=", "re", ".", "compile", "(", "r'<ins.*?>(.*?)</ins>'", ",", "re", ".", "DOTALL", ")", "\n", "del_p", "=", "re", ".", "compile", "(", "r'<del.*?>(.*?)</del>'", ",", "re", ".", "DOTALL", ")", "\n", "patterns", "=", "[", "ins_p", ",", "del_p", "]", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "patterns", ")", ":", "\n", "        ", "match", "=", "re", ".", "finditer", "(", "p", ",", "l", ")", "\n", "if", "match", ":", "\n", "            ", "new_l", "=", "\"\"", "\n", "last", "=", "0", "\n", "for", "m", "in", "match", ":", "\n", "                ", "if", "i", "==", "1", ":", "\n", "                    ", "color", "=", "bcolors", ".", "OKBLUE", "\n", "", "else", ":", "\n", "                    ", "color", "=", "bcolors", ".", "OKGREEN", "\n", "", "new_l", "=", "new_l", "+", "l", "[", "last", ":", "m", ".", "start", "(", "1", ")", "]", "+", "color", "+", "m", ".", "group", "(", "1", ")", "+", "bcolors", ".", "ENDC", "\n", "last", "=", "m", ".", "end", "(", "1", ")", "\n", "", "new_l", "=", "new_l", "+", "l", "[", "last", ":", "]", "\n", "l", "=", "new_l", "\n", "", "", "print", "(", "bcolors", ".", "HEADER", "+", "'line '", "+", "str", "(", "idx", "+", "1", ")", "+", "':'", "+", "bcolors", ".", "ENDC", "+", "l", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.crawl_revision_text.html2diff": [[79, 115], ["bs4.BeautifulSoup", "bs4.BeautifulSoup.find_all", "re.compile", "range", "len", "re.compile", "len", "re.match", "node_next.div.prettify", "next_added.append", "re.match", "re.match", "re.match", "re.match.group().strip", "node_prev.div.prettify", "prev_deleted.append", "node_prev.div.prettify", "node_next.div.prettify", "prev_changed.append", "next_changed.append", "re.match.group().strip", "re.match.group().strip", "re.match.group().strip", "re.match.group", "re.match.group", "re.match.group", "re.match.group"], "function", ["None"], ["", "def", "html2diff", "(", "html", ")", ":", "\n", "    ", "prev_changed", ",", "next_changed", "=", "[", "]", ",", "[", "]", "\n", "prev_deleted", ",", "next_added", "=", "[", "]", ",", "[", "]", "\n", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "'html'", ")", "\n", "nodes", "=", "soup", ".", "find_all", "(", "class_", "=", "re", ".", "compile", "(", "r'(diff-deletedline)|(diff-addedline)|(diff-empty)'", ")", ")", "\n", "div_p", "=", "re", ".", "compile", "(", "r'<div.*?>(.*)</div>'", ",", "re", ".", "DOTALL", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "nodes", ")", ",", "2", ")", ":", "\n", "# skip straddeling cases", "\n", "        ", "if", "i", "+", "1", ">=", "len", "(", "nodes", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "node_prev", "=", "nodes", "[", "i", "]", "\n", "node_next", "=", "nodes", "[", "i", "+", "1", "]", "\n", "\n", "# seperate  revisions into chunks that were modified,", "\n", "# chunks that were purely deleted and chunks that were purely added", "\n", "if", "not", "node_prev", ".", "div", "and", "not", "node_next", ".", "div", ":", "\n", "            ", "continue", "\n", "", "elif", "not", "node_prev", ".", "div", ":", "\n", "            ", "next_match", "=", "re", ".", "match", "(", "div_p", ",", "node_next", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "if", "next_match", ":", "\n", "                ", "next_added", ".", "append", "(", "next_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "", "", "elif", "not", "node_next", ".", "div", ":", "\n", "            ", "prev_match", "=", "re", ".", "match", "(", "div_p", ",", "node_prev", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "if", "prev_match", ":", "\n", "                ", "prev_deleted", ".", "append", "(", "prev_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "prev_match", "=", "re", ".", "match", "(", "div_p", ",", "node_prev", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "next_match", "=", "re", ".", "match", "(", "div_p", ",", "node_next", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "if", "prev_match", "and", "next_match", ":", "\n", "                ", "prev_changed", ".", "append", "(", "prev_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "next_changed", ".", "append", "(", "next_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "\n", "", "", "", "return", "prev_changed", ",", "next_changed", ",", "prev_deleted", ",", "next_added", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.crawl_revision_text.url2diff": [[117, 125], ["urllib.request.urlopen", "urllib.request.urlopen.read", "crawl_revision_text.html2diff", "print"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.util.html2diff"], ["", "def", "url2diff", "(", "url", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "response", "=", "urlopen", "(", "url", ")", "\n", "html", "=", "response", ".", "read", "(", ")", "\n", "return", "html2diff", "(", "html", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "print", "(", "e", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "return", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.crawl_revision_text.wiki_text_clean": [[127, 131], ["text.replace().replace.replace().replace", "text.replace().replace.replace"], "function", ["None"], ["", "", "def", "wiki_text_clean", "(", "text", ")", ":", "\n", "    ", "text", "=", "''", ".", "join", "(", "[", "x", "for", "x", "in", "text", "if", "x", "in", "string", ".", "printable", "]", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.crawl_revision_text.gen_revisions": [[132, 164], ["len", "tqdm.tqdm", "print", "print", "crawl_revision_text.url2diff", "zip", "str", "len", "len", "print", "prevs.append", "nexts.append", "crawl_revision_text.wiki_text_clean", "crawl_revision_text.wiki_text_clean", "print", "str", "crawl_revision_text.wiki_text_clean", "crawl_revision_text.wiki_text_clean", "len", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.util.url2diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.wiki_text_clean", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.wiki_text_clean", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.wiki_text_clean", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.wiki_text_clean"], ["", "def", "gen_revisions", "(", "rev_ids", ")", ":", "\n", "    ", "rev_size", "=", "len", "(", "rev_ids", ")", "\n", "success", "=", "0", "\n", "out", "=", "{", "}", "\n", "\n", "for", "rev_id", "in", "tqdm", "(", "rev_ids", ")", ":", "\n", "        ", "print", "(", "'processing revision id = '", "+", "str", "(", "rev_id", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "url", "=", "'https://en.wikipedia.org/wiki/?diff='", "+", "str", "(", "rev_id", ")", "\n", "prevs_", ",", "nexts_", ",", "prev_deleted", ",", "next_added", "=", "url2diff", "(", "url", ")", "\n", "\n", "if", "len", "(", "prevs_", ")", "!=", "len", "(", "nexts_", ")", ":", "\n", "            ", "print", "(", "'ERROR: corpus sizes not equal!'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "continue", "\n", "\n", "", "prevs", ",", "nexts", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "pre", ",", "post", "in", "zip", "(", "prevs_", ",", "nexts_", ")", ":", "\n", "            ", "prevs", ".", "append", "(", "wiki_text_clean", "(", "pre", ")", ")", "\n", "nexts", ".", "append", "(", "wiki_text_clean", "(", "post", ")", ")", "\n", "", "prevs_deleted", "=", "[", "wiki_text_clean", "(", "pre", ")", "for", "pre", "in", "(", "prev_deleted", "or", "[", "'no_deleted_chunks'", "]", ")", "]", "\n", "nexts_added", "=", "[", "wiki_text_clean", "(", "nxt", ")", "for", "nxt", "in", "(", "next_added", "or", "[", "'no_added_chunks'", "]", ")", "]", "\n", "\n", "\n", "if", "len", "(", "prevs", ")", ">", "0", "and", "len", "(", "nexts", ")", ">", "0", ":", "\n", "            ", "print", "(", "'...success!'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "success", "+=", "1", "\n", "yield", "rev_id", ",", "prevs", ",", "nexts", ",", "prevs_deleted", ",", "nexts_added", "\n", "\n", "", "", "print", "(", "'failures: '", ",", "rev_size", "-", "success", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.crawl_revision_text.go": [[166, 177], ["crawl_revision_text.gen_revisions", "open", "print", "l.split"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.gen_revisions"], ["", "def", "go", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "rev_ids", "=", "[", "l", ".", "split", "(", "'\\t'", ")", "[", "0", "]", "for", "l", "in", "f", "]", "\n", "\n", "", "for", "rev_id", ",", "prevs", ",", "nexts", ",", "prev_deleted", ",", "next_added", "in", "gen_revisions", "(", "rev_ids", ")", ":", "\n", "        ", "print", "(", "'\\t'", ".", "join", "(", "[", "\n", "rev_id", ",", "\n", "'<EDIT-DELIM>'", ".", "join", "(", "prevs", ")", ",", "\n", "'<EDIT-DELIM>'", ".", "join", "(", "nexts", ")", ",", "\n", "'<EDIT-DELIM>'", ".", "join", "(", "prev_deleted", ")", ",", "\n", "'<EDIT-DELIM>'", ".", "join", "(", "next_added", ")", "\n", "]", ")", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.get_revision_ids.Revision.__init__": [[14, 24], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "revid", "=", "None", "\n", "self", ".", "comment", "=", "None", "\n", "self", ".", "timestamp", "=", "None", "\n", "# negative filter on revisions", "\n", "self", ".", "INVALID_REV_RE", "=", "'revert|undo|undid|robot'", "\n", "# NPOV detector. Essentially looks for common pov-related words", "\n", "#     pov, depov, npov, yespov, attributepov, rmpov, wpov, vpov, neutral", "\n", "# with certain leading punctuation allowed", "\n", "self", ".", "NPOV_RE", "=", "'([- wnv\\/\\\\\\:\\{\\(\\[\\\"\\+\\'\\.\\|\\_\\)\\#\\=\\;\\~](rm)?(attribute)?(yes)?(de)?n?pov)|([- n\\/\\\\\\:\\{\\(\\[\\\"\\+\\'\\.\\|\\_\\)\\#\\;\\~]neutral)'", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.get_revision_ids.Revision.incomplete": [[25, 27], ["None"], "methods", ["None"], ["", "def", "incomplete", "(", "self", ")", ":", "\n", "        ", "return", "not", "self", ".", "revid", "or", "not", "self", ".", "comment", "or", "not", "self", ".", "timestamp", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.get_revision_ids.Revision.is_admissible": [[28, 39], ["get_revision_ids.Revision.comment.lower", "re.search", "re.search"], "methods", ["None"], ["", "def", "is_admissible", "(", "self", ")", ":", "\n", "        ", "c_lower", "=", "self", ".", "comment", ".", "lower", "(", ")", "\n", "\n", "\n", "if", "re", ".", "search", "(", "self", ".", "INVALID_REV_RE", ",", "c_lower", ")", ":", "\n", "            ", "return", "False", "\n", "", "if", "re", ".", "search", "(", "self", ".", "NPOV_RE", ",", "c_lower", ")", ":", "\n", "            ", "if", "'pover'", "in", "c_lower", ":", "# special case: \"poverty\", \"impovershiment\", etc", "\n", "                ", "return", "False", "\n", "", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.get_revision_ids.Revision.print_out": [[40, 42], ["print"], "methods", ["None"], ["", "def", "print_out", "(", "self", ")", ":", "\n", "        ", "print", "(", "'\\t'", ".", "join", "(", "[", "self", ".", "revid", ",", "self", ".", "comment", ",", "self", ".", "timestamp", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.gen_data_from_crawl.rm_refs": [[70, 80], ["re.sub", "re.sub", "re.sub"], "function", ["None"], ["def", "rm_refs", "(", "x", ")", ":", "\n", "    ", "REF_RE", "=", "'<ref([-\\w=\" <>]+)?>.*?<([ ]+)?\\/([ ]+)?ref>'", "\n", "x", "=", "re", ".", "sub", "(", "REF_RE", ",", "' '", ",", "x", ")", "\n", "# leading </ref>", "\n", "if", "'</ref>'", "in", "x", ":", "\n", "        ", "x", "=", "re", ".", "sub", "(", "REF_RE", ",", "' '", ",", "'<ref>'", "+", "x", ")", "\n", "# trailing <ref>", "\n", "", "if", "'<ref'", "in", "x", ":", "\n", "        ", "x", "=", "re", ".", "sub", "(", "REF_RE", ",", "' '", ",", "x", "+", "'</ref>'", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.gen_data_from_crawl.clean_wikitext": [[81, 141], ["re.sub.strip", "gen_data_from_crawl.rm_refs", "re.sub", "mwparserfromhell.parse", "mwparserfromhell.parse.strip_code", "gen_data_from_crawl.rm_refs", "re.sub", "mwparserfromhell.parse", "mwparserfromhell.parse.strip_code", "re.sub().strip.replace().replace", "mwparserfromhell.parse", "mwparserfromhell.parse.strip_code", "re.sub().strip.replace().replace", "re.sub", "re.sub", "re.sub", "re.sub().strip.replace", "re.sub", "re.sub().strip.replace", "re.sub().strip.replace", "re.sub().strip.replace", "re.sub().strip.replace", "re.sub().strip", "filter", "re.sub().strip.startswith", "re.sub().strip.startswith", "re.findall", "re.sub().strip.lower", "re.sub().strip.replace", "re.sub().strip.replace", "re.sub"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.rm_refs", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.rm_refs"], ["", "def", "clean_wikitext", "(", "token_list", ")", ":", "\n", "    ", "x", "=", "' '", ".", "join", "(", "token_list", ")", "\n", "\n", "# ascii only", "\n", "x", "=", "''", ".", "join", "(", "filter", "(", "lambda", "x", ":", "x", "in", "string", ".", "printable", ",", "x", ")", ")", "\n", "\n", "# preemptively remove <ref>'s (including uncompleted)", "\n", "x", "=", "x", ".", "strip", "(", ")", "\n", "x", "=", "rm_refs", "(", "x", ")", "\n", "# collapse multispaces", "\n", "x", "=", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "x", ")", "\n", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "x", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "plaintext", "=", "rm_refs", "(", "plaintext", ")", "# get refs again? some things missed", "\n", "# collapse multispaces", "\n", "plaintext", "=", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "plaintext", ")", "\n", "# parse again to hit complicatd nested wikicode like 21055249", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "plaintext", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "\n", "# ignore lines starting with ! or | (likely table artifacts)", "\n", "if", "plaintext", ".", "startswith", "(", "'?'", ")", "or", "plaintext", ".", "startswith", "(", "'|'", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "\n", "# ignore lines without text, e.g. ( , , , , ) or ]]", "\n", "", "if", "not", "re", ".", "findall", "(", "'\\w'", ",", "plaintext", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "\n", "# parse AGAIN again to hit remaining links e.g. 377258469", "\n", "", "plaintext", "=", "plaintext", ".", "replace", "(", "'[ '", ",", "'['", ")", ".", "replace", "(", "' ]'", ",", "']'", ")", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "plaintext", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "\n", "# at this point just rm all brackets", "\n", "plaintext", "=", "plaintext", ".", "replace", "(", "']'", ",", "''", ")", ".", "replace", "(", "'['", ",", "''", ")", "\n", "# rm html", "\n", "plaintext", "=", "re", ".", "sub", "(", "'http\\S+'", ",", "''", ",", "plaintext", ")", "\n", "# rm parents with nothing in them, e.g. (; )", "\n", "plaintext", "=", "re", ".", "sub", "(", "'\\([^\\w]*\\)'", ",", "''", ",", "plaintext", ")", "\n", "# rm remining <del>, <ins> (valid tags should already have been taken parsed)", "\n", "plaintext", "=", "re", ".", "sub", "(", "'<\\/?(del|ins)([-\\w=\" <>]+)?>'", ",", "''", ",", "plaintext", ")", "\n", "# fuck stars", "\n", "plaintext", "=", "plaintext", ".", "replace", "(", "'*'", ",", "''", ")", "\n", "# rm table fragments", "\n", "plaintext", "=", "re", ".", "sub", "(", "'(right[ ]?\\||left[ ]?\\||thumb[ ]?\\||frame[ ]?\\||\\d+px[ ]?\\|)'", ",", "''", ",", "plaintext", ")", "\n", "# ignore timestamp sentences", "\n", "if", "'retrieved on'", "in", "plaintext", ".", "lower", "(", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "# msc html missed", "\n", "", "plaintext", "=", "plaintext", ".", "replace", "(", "'<blockquote>'", ",", "''", ")", "\n", "\n", "# remove tabs and newlines (those is our deliminators beeyotch)", "\n", "plaintext", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "plaintext", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "plaintext", ".", "replace", "(", "'\\r'", ",", "''", ")", "\n", "# collapse multispaces (again again)", "\n", "plaintext", "=", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "plaintext", ")", ".", "strip", "(", ")", "\n", "\n", "return", "plaintext", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.gen_data_from_crawl.find_matches": [[143, 182], ["range", "stats.append", "stats.append", "range", "math.exp", "len", "max", "len", "len", "collections.Counter", "collections.Counter", "stats.append", "stats.append", "len", "sum", "max", "max", "list", "min", "gen_data_from_crawl.find_matches.BLEU"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.BLEU"], ["", "def", "find_matches", "(", "a_list", ",", "b_list", ",", "delta", "=", "3", ")", ":", "\n", "    ", "def", "BLEU", "(", "hyp", ",", "ref", ")", ":", "\n", "# get ngram stats", "\n", "        ", "stats", "=", "[", "]", "\n", "stats", ".", "append", "(", "len", "(", "hyp", ")", ")", "\n", "stats", ".", "append", "(", "len", "(", "ref", ")", ")", "\n", "for", "n", "in", "range", "(", "1", ",", "5", ")", ":", "\n", "            ", "s_ngrams", "=", "Counter", "(", "\n", "[", "tuple", "(", "hyp", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "hyp", ")", "+", "1", "-", "n", ")", "]", "\n", ")", "\n", "r_ngrams", "=", "Counter", "(", "\n", "[", "tuple", "(", "ref", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "ref", ")", "+", "1", "-", "n", ")", "]", "\n", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "sum", "(", "(", "s_ngrams", "&", "r_ngrams", ")", ".", "values", "(", ")", ")", ",", "0", "]", ")", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "len", "(", "hyp", ")", "+", "1", "-", "n", ",", "0", "]", ")", ")", "\n", "\n", "# get bleu from stats", "\n", "", "if", "len", "(", "list", "(", "filter", "(", "lambda", "x", ":", "x", "==", "0", ",", "stats", ")", ")", ")", ">", "0", ":", "\n", "            ", "return", "0", "\n", "", "(", "c", ",", "r", ")", "=", "stats", "[", ":", "2", "]", "\n", "log_bleu_prec", "=", "sum", "(", "\n", "[", "math", ".", "log", "(", "float", "(", "x", ")", "/", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "stats", "[", "2", ":", ":", "2", "]", ",", "stats", "[", "3", ":", ":", "2", "]", ")", "]", "\n", ")", "/", "4.", "\n", "bleu", "=", "math", ".", "exp", "(", "min", "(", "[", "0", ",", "1", "-", "float", "(", "r", ")", "/", "c", "]", ")", "+", "log_bleu_prec", ")", "\n", "\n", "return", "100", "*", "bleu", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "a_list", ")", ")", ":", "\n", "        ", "neighborhood_bleus", "=", "[", "\n", "(", "BLEU", "(", "a_list", "[", "i", "]", ".", "split", "(", ")", ",", "b_list", "[", "j", "]", ".", "split", "(", ")", ")", ",", "j", ")", "\n", "for", "j", "in", "range", "(", "max", "(", "i", "-", "delta", ",", "0", ")", ",", "min", "(", "i", "+", "delta", ",", "len", "(", "b_list", ")", ")", ")", "\n", "]", "\n", "# corner case: len(a_list) >> len(b_list)", "\n", "if", "not", "neighborhood_bleus", ":", "\n", "            ", "continue", "\n", "\n", "", "max_bleu", ",", "match_idx", "=", "max", "(", "neighborhood_bleus", ")", "\n", "\n", "yield", "i", ",", "match_idx", ",", "max_bleu", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.gen_data_from_crawl.tokenize": [[184, 188], ["TOKENIZER.tokenize", "s.strip"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize"], ["", "", "def", "tokenize", "(", "s", ")", ":", "\n", "    ", "global", "TOKENIZER", "\n", "tok_list", "=", "TOKENIZER", ".", "tokenize", "(", "s", ".", "strip", "(", ")", ")", "\n", "return", "' '", ".", "join", "(", "tok_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.gen_data_from_crawl.is_spelling_diff": [[191, 207], ["enumerate", "sum", "autocorrect.spell", "len", "len", "len"], "function", ["None"], ["", "def", "is_spelling_diff", "(", "d", ")", ":", "\n", "    ", "\"\"\"takes a word diff as arg\"\"\"", "\n", "global", "SPELLCHECKER", "\n", "\n", "# only look at the one-word diffs", "\n", "if", "sum", "(", "[", "len", "(", "chunk", ")", "for", "tag", ",", "chunk", "in", "d", "if", "tag", "==", "'-'", "]", ")", ">", "1", ":", "\n", "        ", "return", "False", "\n", "\n", "", "for", "i", ",", "(", "tag", ",", "words", ")", "in", "enumerate", "(", "d", ")", ":", "\n", "        ", "if", "tag", "==", "'-'", "and", "i", "+", "1", "<", "len", "(", "d", ")", "-", "1", "and", "len", "(", "words", ")", "==", "1", "and", "d", "[", "i", "+", "1", "]", "[", "0", "]", "==", "'+'", ":", "\n", "# is one-word spelling replacement (post correction)", "\n", "            ", "correction", "=", "spell", "(", "words", "[", "0", "]", ")", "\n", "if", "not", "correction", "==", "words", "[", "0", "]", "and", "correction", "in", "' '", ".", "join", "(", "d", "[", "i", "+", "1", "]", "[", "1", "]", ")", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.gen_data_from_crawl.get_tok_labels": [[209, 220], ["len", "len"], "function", ["None"], ["", "def", "get_tok_labels", "(", "s_diff", ")", ":", "\n", "    ", "tok_labels", "=", "[", "]", "\n", "for", "tag", ",", "chunk", "in", "s_diff", ":", "\n", "        ", "if", "tag", "==", "'='", ":", "\n", "            ", "tok_labels", "+=", "[", "0", "]", "*", "len", "(", "chunk", ")", "\n", "", "elif", "tag", "==", "'-'", ":", "\n", "            ", "tok_labels", "+=", "[", "1", "]", "*", "len", "(", "chunk", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "return", "tok_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.gen_data_from_crawl.should_keep": [[222, 297], ["simplediff.diff", "gen_data_from_crawl.get_tok_labels", "simplediff.diff", "gen_data_from_crawl.is_spelling_diff", "gen_data_from_crawl.should_keep.is_single_word_edit"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.get_tok_labels", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.is_spelling_diff"], ["", "def", "should_keep", "(", "prev_raw", ",", "prev_tok", ",", "post_raw", ",", "post_tok", ",", "bleu", ",", "rev_id", ")", ":", "\n", "    ", "global", "CTR_LOW_BLEU", "\n", "global", "CTR_LOW_LEVEN", "\n", "global", "CTR_TOO_MANY_1_TOKS", "\n", "global", "CTR_SPELLING", "\n", "global", "CTR_CHEMISTRY", "\n", "global", "CTR_ONLY_PUNC_CHANGED", "\n", "\n", "# KEEP -- exact match", "\n", "if", "bleu", "==", "100", "or", "prev_raw", "==", "post_raw", ":", "\n", "        ", "return", "True", ",", "None", ",", "[", "0", "for", "_", "in", "range", "(", "len", "(", "prev_tok", ".", "split", "(", ")", ")", ")", "]", "\n", "\n", "# clearly not a match", "\n", "", "if", "bleu", "<", "15.0", ":", "\n", "        ", "CTR_LOW_BLEU", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "# too close", "\n", "", "if", "Levenshtein", ".", "distance", "(", "prev_tok", ",", "post_tok", ")", "<", "4", ":", "\n", "        ", "CTR_LOW_LEVEN", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "", "tok_diff", "=", "diff", "(", "prev_tok", ".", "split", "(", ")", ",", "post_tok", ".", "split", "(", ")", ")", "\n", "tok_labels", "=", "get_tok_labels", "(", "tok_diff", ")", "\n", "assert", "len", "(", "tok_labels", ")", "==", "len", "(", "prev_tok", ".", "split", "(", ")", ")", "\n", "\n", "changed_text", "=", "''", ".", "join", "(", "[", "''", ".", "join", "(", "chunk", ")", "for", "tag", ",", "chunk", "in", "tok_diff", "if", "tag", "!=", "'='", "]", ")", "\n", "if", "not", "re", ".", "search", "(", "'[a-z]'", ",", "changed_text", ")", ":", "\n", "        ", "CTR_ONLY_PUNC_CHANGED", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# too dissimilar -- less than half of toks shared", "\n", "", "tok_nums", "=", "[", "int", "(", "x", ")", "for", "x", "in", "tok_labels", "]", "\n", "if", "(", "sum", "(", "tok_nums", ")", "*", "1.0", "/", "len", "(", "tok_nums", ")", ")", ">", "0.5", ":", "\n", "        ", "CTR_TOO_MANY_1_TOKS", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# edit was just fixing a spelling error", "\n", "", "word_diff", "=", "diff", "(", "word_tokenize", "(", "prev_raw", ")", ",", "word_tokenize", "(", "post_raw", ")", ")", "\n", "if", "is_spelling_diff", "(", "word_diff", ")", ":", "\n", "        ", "CTR_SPELLING", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# some simple filtering to get out the chemistry \"neutral\" edits", "\n", "", "if", "' molecules'", "in", "prev_raw", "or", "' ions'", "in", "prev_raw", "or", "' ionic'", "in", "prev_raw", "or", "' atoms'", "in", "prev_raw", ":", "\n", "        ", "CTR_CHEMISTRY", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# # use enchant to make sure example has enough normal words", "\n", "# prev_words = prev_words.translate(str.maketrans('', '', string.punctuation)).split()", "\n", "# n_words = sum(1 if d.check(w) else 0 for w in pre_words)", "\n", "# if len(prev_words) == 0 or (float(n_words) / len(prev_words)) < 0.5:", "\n", "#     return False, None, None", "\n", "\n", "\n", "# see if this is a \"single word\" edit, where a single word was replaced with 0+ words", "\n", "", "def", "is_single_word_edit", "(", "d", ")", ":", "\n", "        ", "\"\"\" is this diff good for the final generation dataset \"\"\"", "\n", "pre_chunks", "=", "[", "chunk", "for", "tag", ",", "chunk", "in", "d", "if", "tag", "==", "'-'", "]", "\n", "post_chunks", "=", "[", "chunk", "for", "tag", ",", "chunk", "in", "d", "if", "tag", "==", "'+'", "]", "\n", "# a single word on the pre side", "\n", "if", "sum", "(", "[", "len", "(", "chunk", ")", "for", "chunk", "in", "pre_chunks", "]", ")", "!=", "1", ":", "\n", "            ", "return", "False", "\n", "# 0 words in the post", "\n", "", "if", "len", "(", "post_chunks", ")", "==", "0", ":", "\n", "            ", "return", "True", "\n", "# ensure 1 post chunk", "\n", "", "if", "len", "(", "post_chunks", ")", ">", "1", ":", "\n", "            ", "return", "False", "\n", "# post language chunk is directly after the pre chunk", "\n", "", "prei", "=", "next", "(", "(", "i", "for", "i", ",", "x", "in", "enumerate", "(", "d", ")", "if", "x", "[", "0", "]", "==", "'-'", ")", ")", "\n", "if", "prei", "<", "len", "(", "d", ")", "-", "1", "and", "d", "[", "prei", "+", "1", "]", "[", "0", "]", "==", "'+'", ":", "\n", "            ", "return", "True", "\n", "", "", "single_word_edit", "=", "is_single_word_edit", "(", "word_diff", ")", "\n", "\n", "return", "True", ",", "single_word_edit", ",", "tok_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.harvest.gen_data_from_crawl.sent_generator": [[301, 375], ["tqdm.tqdm", "isinstance", "isinstance", "clean_wikitext().lower", "clean_wikitext().lower", "nltk.sent_tokenize", "nltk.sent_tokenize", "gen_data_from_crawl.find_matches", "len", "len", "gen_data_from_crawl.tokenize", "gen_data_from_crawl.tokenize", "gen_data_from_crawl.should_keep", "len", "x.decode", "x.decode", "len", "len", "gen_data_from_crawl.clean_wikitext", "gen_data_from_crawl.clean_wikitext", "rev_examples.append", "len", "len", "len", "len", "set", "set", "list", "sum"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.find_matches", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.should_keep", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.clean_wikitext", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.clean_wikitext"], ["", "def", "sent_generator", "(", "revisions", ")", ":", "\n", "    ", "global", "CTR_EMPTY_REV", "\n", "global", "CTR_MULTIPLE_EDITS", "\n", "global", "CTR_FAILED_CLEANING", "\n", "global", "CTR_DUPS", "\n", "global", "CTR_INVALID_NUM_CHANGED_SENTS", "\n", "global", "CTR_NON_EDIT_CHUNKS", "\n", "global", "CTR_EDIT_CHANGED_NUM_SENTS", "\n", "global", "CTR_FAILED_TAGGING", "\n", "\n", "for", "rev_id", "in", "tqdm", "(", "revisions", ")", ":", "\n", "        ", "prevs", ",", "posts", ",", "prev_deleted", ",", "posts_added", "=", "revisions", "[", "rev_id", "]", "\n", "\n", "# empty revision", "\n", "if", "not", "prevs", "or", "not", "posts", ":", "\n", "            ", "CTR_EMPTY_REV", "+=", "1", "\n", "continue", "\n", "\n", "", "if", "prev_deleted", "!=", "[", "'no_deleted_chunks'", "]", "or", "posts_added", "!=", "[", "'no_added_chunks'", "]", ":", "\n", "            ", "CTR_NON_EDIT_CHUNKS", "+=", "1", "\n", "continue", "\n", "\n", "# unicode dat shit", "\n", "", "if", "isinstance", "(", "prevs", "[", "0", "]", ",", "bytes", ")", ":", "\n", "            ", "prevs", "=", "[", "x", ".", "decode", "(", ")", "for", "x", "in", "prevs", "]", "\n", "", "if", "isinstance", "(", "posts", "[", "0", "]", ",", "bytes", ")", ":", "\n", "            ", "posts", "=", "[", "x", ".", "decode", "(", ")", "for", "x", "in", "posts", "]", "\n", "\n", "# multiple edits", "\n", "", "if", "len", "(", "prevs", ")", ">", "1", "or", "len", "(", "posts", ")", ">", "1", ":", "\n", "            ", "CTR_MULTIPLE_EDITS", "+=", "1", "\n", "continue", "\n", "\n", "", "prev_text", "=", "clean_wikitext", "(", "prevs", ")", ".", "lower", "(", ")", "\n", "post_text", "=", "clean_wikitext", "(", "posts", ")", ".", "lower", "(", ")", "\n", "\n", "# failed cleaning", "\n", "if", "not", "prev_text", "or", "not", "post_text", ":", "\n", "            ", "CTR_FAILED_CLEANING", "+=", "1", "\n", "continue", "\n", "\n", "", "prev_sents_raw", "=", "sent_tokenize", "(", "prev_text", ")", "\n", "post_sents_raw", "=", "sent_tokenize", "(", "post_text", ")", "\n", "\n", "if", "len", "(", "prev_sents_raw", ")", "!=", "len", "(", "post_sents_raw", ")", ":", "\n", "            ", "CTR_EDIT_CHANGED_NUM_SENTS", "+=", "1", "\n", "continue", "\n", "\n", "", "prev_sents_tok", "=", "[", "tokenize", "(", "s", ")", "for", "s", "in", "prev_sents_raw", "]", "\n", "post_sents_tok", "=", "[", "tokenize", "(", "s", ")", "for", "s", "in", "post_sents_raw", "]", "\n", "\n", "rev_examples", "=", "[", "]", "\n", "for", "i", ",", "j", ",", "score", "in", "find_matches", "(", "prev_sents_tok", ",", "post_sents_tok", ")", ":", "\n", "            ", "ex", "=", "prev_sents_raw", "[", "i", "]", ",", "prev_sents_tok", "[", "i", "]", ",", "post_sents_raw", "[", "j", "]", ",", "post_sents_tok", "[", "j", "]", ",", "score", ",", "rev_id", "\n", "keep", ",", "is_word_edit", ",", "tok_labels", "=", "should_keep", "(", "*", "ex", ")", "\n", "\n", "if", "keep", ":", "\n", "                ", "rev_examples", ".", "append", "(", "list", "(", "ex", ")", "+", "[", "is_word_edit", ",", "tok_labels", "]", ")", "\n", "\n", "# only take revisions where a single sentence was changed", "\n", "# if sum([sum(x[-1]) > 0 for x in rev_examples]) != 1:", "\n", "#     CTR_INVALID_NUM_CHANGED_SENTS += len([x for x in rev_examples if sum(x[-1]) > 0])", "\n", "#     continue", "\n", "\n", "# ignore the revision if dups got in the mix somehow", "\n", "", "", "rev_prevs", "=", "[", "x", "[", "0", "]", "for", "x", "in", "rev_examples", "]", "\n", "rev_posts", "=", "[", "x", "[", "2", "]", "for", "x", "in", "rev_examples", "]", "\n", "if", "len", "(", "rev_prevs", ")", "!=", "len", "(", "set", "(", "rev_prevs", ")", ")", "or", "len", "(", "rev_posts", ")", "!=", "len", "(", "set", "(", "rev_posts", ")", ")", ":", "\n", "            ", "CTR_DUPS", "+=", "len", "(", "[", "x", "for", "x", "in", "rev_examples", "if", "sum", "(", "x", "[", "-", "1", "]", ")", ">", "0", "]", ")", "\n", "continue", "\n", "\n", "# WHAT REMAINS IS OK!!", "\n", "", "for", "x", "in", "rev_examples", ":", "\n", "            ", "yield", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.eval.score_results.detokenize": [[21, 29], ["s.split", "w.startswith", "out.append", "len"], "function", ["None"], ["def", "detokenize", "(", "s", ")", ":", "\n", "    ", "out", "=", "[", "]", "\n", "for", "w", "in", "s", ".", "split", "(", ")", ":", "\n", "        ", "if", "w", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "out", ")", ">", "0", ":", "\n", "            ", "out", "[", "-", "1", "]", "+=", "w", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "out", ".", "append", "(", "w", ")", "\n", "", "", "return", "' '", ".", "join", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.eval.score_results.main": [[31, 106], ["os.listdir", "len", "print", "utils.parse_results_file", "random.choice", "range", "print", "print", "print", "input", "set.add", "os.system", "open", "print", "json.dump", "open", "json.load", "collections.defaultdict", "set", "print", "set", "collections.defaultdict", "os.path.join", "list", "random.choice", "input", "results_dict.keys", "results_dict[].keys", "results_dict.keys", "list", "collections.defaultdict", "collections.defaultdict", "results_dict[].keys", "score_results.detokenize", "score_results.detokenize", "collections.defaultdict.values", "file_dict.keys", "len", "str", "str"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.eval.utils.parse_results_file", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.resultsfile_to_mturk.detokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.resultsfile_to_mturk.detokenize"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "# Load save file", "\n", "  ", "try", ":", "\n", "    ", "with", "open", "(", "args", ".", "scores_file", ",", "'r'", ")", "as", "f", ":", "\n", "# Load hashes of results that have been labeled already.", "\n", "      ", "tmp", "=", "json", ".", "load", "(", "f", ")", "\n", "scores_dict", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "int", ")", ")", "\n", "for", "filename", "in", "tmp", ":", "\n", "        ", "for", "src_hash", "in", "tmp", "[", "filename", "]", ":", "\n", "          ", "scores_dict", "[", "filename", "]", "[", "src_hash", "]", "=", "tmp", "[", "filename", "]", "[", "src_hash", "]", "\n", "\n", "", "", "labeled_hashes", "=", "set", "(", "[", "\n", "src_hash", "for", "file_dict", "in", "scores_dict", ".", "values", "(", ")", "for", "src_hash", "in", "file_dict", ".", "keys", "(", ")", "\n", "]", ")", "\n", "print", "(", "f'Found {len(labeled_hashes)} previously labeled examples.'", ")", "\n", "", "", "except", ":", "\n", "    ", "labeled_hashes", "=", "set", "(", ")", "\n", "scores_dict", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "int", ")", ")", "\n", "\n", "# Load results files", "\n", "", "results_files", "=", "os", ".", "listdir", "(", "args", ".", "results_dir", ")", "\n", "results_dict", "=", "{", "\n", "results_path", ":", "utils", ".", "parse_results_file", "(", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "results_dir", ",", "results_path", ")", ",", "ignore_unchanged", "=", "True", ")", "\n", "for", "results_path", "in", "results_files", "\n", "}", "\n", "\n", "# Get examples that haven't been labeled yet", "\n", "num_unlabeled_outputs", "=", "len", "(", "[", "\n", "(", "filename", ",", "src_hash", ")", "\n", "for", "filename", "in", "results_dict", ".", "keys", "(", ")", "\n", "for", "src_hash", "in", "results_dict", "[", "filename", "]", ".", "keys", "(", ")", "\n", "if", "src_hash", "not", "in", "labeled_hashes", "\n", "]", ")", "\n", "print", "(", "f'Found {num_unlabeled_outputs} examples to label.'", ")", "\n", "\n", "# label examples (random uniform across files)", "\n", "i", "=", "0", "\n", "while", "True", ":", "\n", "    ", "filename", "=", "choice", "(", "list", "(", "results_dict", ".", "keys", "(", ")", ")", ")", "\n", "for", "_", "in", "range", "(", "100", ")", ":", "#hacky: go until you get a cache miss", "\n", "      ", "src_hash", "=", "choice", "(", "list", "(", "results_dict", "[", "filename", "]", ".", "keys", "(", ")", ")", ")", "\n", "if", "src_hash", "not", "in", "labeled_hashes", ":", "\n", "        ", "break", "\n", "", "", "if", "src_hash", "in", "labeled_hashes", ":", "\n", "      ", "continue", "\n", "\n", "", "unlabeled_output", "=", "results_dict", "[", "filename", "]", "[", "src_hash", "]", "\n", "print", "(", "'%d / %d'", "%", "(", "i", ",", "num_unlabeled_outputs", ")", ")", "\n", "print", "(", "f'Source:\\t\\t{detokenize(str(unlabeled_output[\"src\"]))}'", ")", "\n", "print", "(", "f'Prediction:\\t{detokenize(str(unlabeled_output[\"pred\"]))}'", ")", "\n", "\n", "# Get a score from the user.", "\n", "score", "=", "input", "(", "\n", "\"\"\"Rate the quality of the output on a scale of %s-%s.\n         1: Unsuccessful\n         2: Successful\n         Enter -1 to exit.\\n\"\"\"", "%", "(", "MIN_SCORE", ",", "MAX_SCORE", ")", ")", "\n", "while", "(", "score", "<", "MIN_SCORE", "or", "MAX_SCORE", "<", "score", ")", "and", "score", "!=", "'-1'", ":", "\n", "      ", "score", "=", "input", "(", "f'Invalid score. Rate the quality of the output on a '", "\n", "f'scale of {MIN_SCORE}-{MAX_SCORE}. Enter -1 to exit. '", ")", "\n", "", "if", "score", "==", "'-1'", ":", "\n", "      ", "break", "\n", "\n", "# Update the data structures that keep track of scores.", "\n", "", "labeled_hashes", ".", "add", "(", "src_hash", ")", "\n", "scores_dict", "[", "filename", "]", "[", "src_hash", "]", "=", "score", "\n", "os", ".", "system", "(", "'clear'", ")", "\n", "i", "+=", "1", "\n", "\n", "# Save data structures.", "\n", "", "with", "open", "(", "args", ".", "scores_file", ",", "'w'", ")", "as", "f", ":", "\n", "    ", "print", "(", "\"Saving scores.\"", ")", "\n", "json", ".", "dump", "(", "scores_dict", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.eval.utils.parse_results_file": [[5, 59], ["open", "simplediff.diff", "l.strip.strip", "a.split", "b.split", "re.search", "utils.parse_results_file.is_complete"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff"], ["from", "torch", ".", "nn", "import", "CrossEntropyLoss", "\n", "from", "pytorch_pretrained_bert", ".", "optimization", "import", "BertAdam", "\n", "\n", "import", "sys", ";", "sys", ".", "path", ".", "append", "(", "'.'", ")", "\n", "from", "shared", ".", "args", "import", "ARGS", "\n", "from", "shared", ".", "constants", "import", "CUDA", "\n", "\n", "\n", "def", "build_optimizer", "(", "model", ",", "num_train_steps", ",", "learning_rate", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "if", "ARGS", ".", "tagger_from_debiaser", ":", "\n", "        ", "parameters", "=", "list", "(", "model", ".", "cls_classifier", ".", "parameters", "(", ")", ")", "+", "list", "(", "\n", "model", ".", "tok_classifier", ".", "parameters", "(", ")", ")", "\n", "parameters", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "parameters", ")", ")", "\n", "return", "optim", ".", "Adam", "(", "parameters", ",", "lr", "=", "ARGS", ".", "learning_rate", ")", "\n", "", "else", ":", "\n", "        ", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "param_optimizer", "=", "list", "(", "filter", "(", "lambda", "name_param", ":", "name_param", "[", "1", "]", ".", "requires_grad", ",", "param_optimizer", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'gamma'", ",", "'beta'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay_rate'", ":", "0.0", "}", "\n", "]", "\n", "return", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "0.1", ",", "\n", "t_total", "=", "num_train_steps", ")", "\n", "\n", "\n", "", "", "def", "build_loss_fn", "(", "debias_weight", "=", "None", ")", ":", "\n", "    ", "global", "ARGS", "\n", "\n", "if", "debias_weight", "is", "None", ":", "\n", "        ", "debias_weight", "=", "ARGS", ".", "debias_weight", "\n", "\n", "", "weight_mask", "=", "torch", ".", "ones", "(", "ARGS", ".", "num_tok_labels", ")", "\n", "weight_mask", "[", "-", "1", "]", "=", "0", "\n", "\n", "if", "CUDA", ":", "\n", "        ", "weight_mask", "=", "weight_mask", ".", "cuda", "(", ")", "\n", "criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ")", ".", "cuda", "(", ")", "\n", "per_tok_criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ",", "reduction", "=", "'none'", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ")", "\n", "per_tok_criterion", "=", "CrossEntropyLoss", "(", "weight", "=", "weight_mask", ",", "reduction", "=", "'none'", ")", "\n", "\n", "\n", "", "def", "cross_entropy_loss", "(", "logits", ",", "labels", ",", "apply_mask", "=", "None", ")", ":", "\n", "        ", "return", "criterion", "(", "\n", "logits", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "ARGS", ".", "num_tok_labels", ")", ",", "\n", "labels", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ".", "type", "(", "'torch.cuda.LongTensor'", "if", "CUDA", "else", "'torch.LongTensor'", ")", ")", "\n", "\n", "", "def", "weighted_cross_entropy_loss", "(", "logits", ",", "labels", ",", "apply_mask", "=", "None", ")", ":", "\n", "# weight mask = where to apply weight (post_tok_label_id from the batch)", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.sandbox.words_distribution.histogram": [[16, 22], ["matplotlib.figure", "plt.figure.add_subplot", "fig.add_subplot.hist", "fig.add_subplot.set_title", "plt.figure.show"], "function", ["None"], ["def", "histogram", "(", "x", ",", "title", ")", ":", "\n", "    ", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", "1", ",", "1", ",", "1", ")", "\n", "ax", ".", "hist", "(", "x", ")", "\n", "ax", ".", "set_title", "(", "title", ")", "\n", "fig", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.sandbox.words_distribution.src_unique": [[24, 26], ["set", "set", "src.strip().split", "tgt.strip().split", "src.strip", "tgt.strip"], "function", ["None"], ["", "def", "src_unique", "(", "src", ",", "tgt", ")", ":", "\n", "    ", "return", "set", "(", "src", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "-", "set", "(", "tgt", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.sandbox.words_distribution.tgt_unique": [[27, 29], ["set", "set", "tgt.strip().split", "src.strip().split", "tgt.strip", "src.strip"], "function", ["None"], ["", "def", "tgt_unique", "(", "src", ",", "tgt", ")", ":", "\n", "    ", "return", "set", "(", "tgt", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "-", "set", "(", "src", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.make_attribute_vocab.SalienceCalculator.__init__": [[12, 24], ["sklearn.feature_extraction.text.CountVectorizer", "make_attribute_vocab.SalienceCalculator.vectorizer.fit_transform", "numpy.sum", "numpy.squeeze", "make_attribute_vocab.SalienceCalculator.vectorizer.fit_transform", "numpy.sum", "numpy.squeeze", "numpy.asarray", "numpy.asarray"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "pre_corpus", ",", "post_corpus", ")", ":", "\n", "        ", "self", ".", "vectorizer", "=", "CountVectorizer", "(", ")", "\n", "\n", "pre_count_matrix", "=", "self", ".", "vectorizer", ".", "fit_transform", "(", "pre_corpus", ")", "\n", "self", ".", "pre_vocab", "=", "self", ".", "vectorizer", ".", "vocabulary_", "\n", "self", ".", "pre_counts", "=", "np", ".", "sum", "(", "pre_count_matrix", ",", "axis", "=", "0", ")", "\n", "self", ".", "pre_counts", "=", "np", ".", "squeeze", "(", "np", ".", "asarray", "(", "self", ".", "pre_counts", ")", ")", "\n", "\n", "post_count_matrix", "=", "self", ".", "vectorizer", ".", "fit_transform", "(", "post_corpus", ")", "\n", "self", ".", "post_vocab", "=", "self", ".", "vectorizer", ".", "vocabulary_", "\n", "self", ".", "post_counts", "=", "np", ".", "sum", "(", "post_count_matrix", ",", "axis", "=", "0", ")", "\n", "self", ".", "post_counts", "=", "np", ".", "squeeze", "(", "np", ".", "asarray", "(", "self", ".", "post_counts", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.make_attribute_vocab.SalienceCalculator.salience": [[26, 43], ["None"], "methods", ["None"], ["", "def", "salience", "(", "self", ",", "feature", ",", "attribute", "=", "'pre'", ",", "lmbda", "=", "0.5", ")", ":", "\n", "        ", "assert", "attribute", "in", "[", "'pre'", ",", "'post'", "]", "\n", "\n", "if", "feature", "not", "in", "self", ".", "pre_vocab", ":", "\n", "            ", "pre_count", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "pre_count", "=", "self", ".", "pre_counts", "[", "self", ".", "pre_vocab", "[", "feature", "]", "]", "\n", "\n", "", "if", "feature", "not", "in", "self", ".", "post_vocab", ":", "\n", "            ", "post_count", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "post_count", "=", "self", ".", "post_counts", "[", "self", ".", "post_vocab", "[", "feature", "]", "]", "\n", "\n", "", "if", "attribute", "==", "'pre'", ":", "\n", "            ", "return", "(", "pre_count", "+", "lmbda", ")", "/", "(", "post_count", "+", "lmbda", ")", "\n", "", "else", ":", "\n", "            ", "return", "(", "post_count", "+", "lmbda", ")", "/", "(", "pre_count", "+", "lmbda", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.tag_corpusfile.timeout": [[64, 81], ["os.strerror", "os.strerror", "tag_corpusfile.TimeoutError", "signal.signal", "signal.alarm", "functools.wraps", "func", "signal.alarm"], "function", ["None"], ["", "def", "timeout", "(", "seconds", "=", "10", ",", "error_message", "=", "os", ".", "strerror", "(", "errno", ".", "ETIME", ")", ")", ":", "\n", "    ", "def", "decorator", "(", "func", ")", ":", "\n", "        ", "def", "_handle_timeout", "(", "signum", ",", "frame", ")", ":", "\n", "            ", "raise", "TimeoutError", "(", "error_message", ")", "\n", "\n", "", "def", "wrapper", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "signal", ".", "signal", "(", "signal", ".", "SIGALRM", ",", "_handle_timeout", ")", "\n", "signal", ".", "alarm", "(", "seconds", ")", "\n", "try", ":", "\n", "                ", "result", "=", "func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "finally", ":", "\n", "                ", "signal", ".", "alarm", "(", "0", ")", "\n", "", "return", "result", "\n", "\n", "", "return", "wraps", "(", "func", ")", "(", "wrapper", ")", "\n", "\n", "", "return", "decorator", "\n", "############################## TIMOUT", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.tag_corpusfile.words_from_toks": [[85, 96], ["enumerate", "tok.startswith", "tok.replace", "word_indices[].append", "words.append", "word_indices.append"], "function", ["None"], ["", "def", "words_from_toks", "(", "toks", ")", ":", "\n", "    ", "words", "=", "[", "]", "\n", "word_indices", "=", "[", "]", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "toks", ")", ":", "\n", "        ", "if", "tok", ".", "startswith", "(", "'##'", ")", ":", "\n", "            ", "words", "[", "-", "1", "]", "+=", "tok", ".", "replace", "(", "'##'", ",", "''", ")", "\n", "word_indices", "[", "-", "1", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "            ", "words", ".", "append", "(", "tok", ")", "\n", "word_indices", ".", "append", "(", "[", "i", "]", ")", "\n", "", "", "return", "words", ",", "word_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.tag_corpusfile.pos_rel_from_words": [[97, 134], ["tag_corpusfile.timeout", "zip", "parser.raw_parse", "tree.to_conll", "enumerate", "print", "l.split", "len", "range", "range", "tree.to_conll.strip().split", "len", "out_pos.append", "out_rels.append", "len", "out_pos.append", "out_rels.append", "tree.to_conll.strip"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.tag_corpusfile.timeout"], ["", "@", "timeout", "(", "10", ")", "\n", "def", "pos_rel_from_words", "(", "words", ",", "word_indices", ")", ":", "\n", "    ", "words_tags_rels", "=", "[", "]", "\n", "# TODO MOVE THIS OUTSIDE!! CLEAN WORDS OF BROKEN ENCODINGS!!", "\n", "try", ":", "\n", "        ", "trees", "=", "parser", ".", "raw_parse", "(", "' '", ".", "join", "(", "words", ")", ")", "\n", "", "except", ":", "\n", "        ", "print", "(", "'SKIPPING...'", ")", "\n", "return", "[", "'SKIPPED'", "]", ",", "[", "'SKIPPED'", "]", "\n", "\n", "", "for", "tree", "in", "trees", ":", "\n", "        ", "conll", "=", "tree", ".", "to_conll", "(", "4", ")", "\n", "conll", "=", "[", "l", ".", "split", "(", "'\\t'", ")", "for", "l", "in", "conll", ".", "strip", "(", ")", ".", "split", "(", "'\\n'", ")", "]", "\n", "words_tags_rels", "+=", "[", "(", "word", ",", "tag", ",", "rel", ")", "for", "[", "word", ",", "tag", ",", "_", ",", "rel", "]", "in", "conll", "]", "\n", "\n", "# +1 for missing tags", "\n", "", "out_pos", "=", "[", "]", "\n", "out_rels", "=", "[", "]", "\n", "\n", "tagi", "=", "0", "\n", "for", "(", "wi", ",", "word", ")", ",", "indices", "in", "zip", "(", "enumerate", "(", "words", ")", ",", "word_indices", ")", ":", "\n", "        ", "if", "tagi", "<", "len", "(", "words_tags_rels", ")", ":", "\n", "            ", "tagged_word", ",", "pos", ",", "rel", "=", "words_tags_rels", "[", "tagi", "]", "\n", "", "else", ":", "\n", "            ", "tagged_word", "=", "' skip me '", "\n", "\n", "", "if", "tagged_word", "==", "word", ":", "\n", "            ", "for", "_", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "                    ", "out_pos", ".", "append", "(", "pos", ")", "\n", "out_rels", ".", "append", "(", "rel", ")", "\n", "", "tagi", "+=", "1", "\n", "", "else", ":", "\n", "            ", "for", "_", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "                    ", "out_pos", ".", "append", "(", "'<SKIP>'", ")", "\n", "out_rels", ".", "append", "(", "'<SKIP>'", ")", "\n", "\n", "", "", "", "return", "out_pos", ",", "out_rels", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.get_revision_ids.Revision.__init__": [[16, 23], ["None"], "methods", ["None"], ["self", ".", "comment", "=", "None", "\n", "self", ".", "timestamp", "=", "None", "\n", "# negative filter on revisions", "\n", "self", ".", "INVALID_REV_RE", "=", "'revert|undo|undid|robot'", "\n", "# NPOV detector. Essentially looks for common pov-related words", "\n", "#     pov, depov, npov, yespov, attributepov, rmpov, wpov, vpov, neutral", "\n", "# with certain leading punctuation allowed", "\n", "self", ".", "NPOV_RE", "=", "'([- wnv\\/\\\\\\:\\{\\(\\[\\\"\\+\\'\\.\\|\\_\\)\\#\\=\\;\\~](rm)?(attribute)?(yes)?(de)?n?pov)|([- n\\/\\\\\\:\\{\\(\\[\\\"\\+\\'\\.\\|\\_\\)\\#\\;\\~]neutral)'", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.get_revision_ids.Revision.incomplete": [[24, 26], ["None"], "methods", ["None"], ["\n", "", "def", "incomplete", "(", "self", ")", ":", "\n", "        ", "return", "not", "self", ".", "revid", "or", "not", "self", ".", "comment", "or", "not", "self", ".", "timestamp", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.get_revision_ids.Revision.is_admissible": [[27, 38], ["get_revision_ids.Revision.comment.lower", "re.search", "re.search"], "methods", ["None"], ["\n", "", "def", "is_admissible", "(", "self", ")", ":", "\n", "        ", "c_lower", "=", "self", ".", "comment", ".", "lower", "(", ")", "\n", "\n", "\n", "if", "re", ".", "search", "(", "self", ".", "INVALID_REV_RE", ",", "c_lower", ")", ":", "\n", "            ", "return", "False", "\n", "", "if", "re", ".", "search", "(", "self", ".", "NPOV_RE", ",", "c_lower", ")", ":", "\n", "            ", "if", "'pover'", "in", "c_lower", ":", "# special case: \"poverty\", \"impovershiment\", etc", "\n", "                ", "return", "False", "\n", "", "return", "True", "\n", "", "return", "False", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.get_revision_ids.Revision.print_out": [[39, 41], ["print"], "methods", ["None"], ["\n", "", "def", "print_out", "(", "self", ")", ":", "\n", "        ", "print", "(", "'\\t'", ".", "join", "(", "[", "self", ".", "revid", ",", "self", ".", "comment", ",", "self", ".", "timestamp", "]", ")", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.rm_refs": [[69, 79], ["re.sub", "re.sub", "re.sub"], "function", ["None"], ["\n", "def", "rm_refs", "(", "x", ")", ":", "\n", "    ", "REF_RE", "=", "'<ref([-\\w=\" <>]+)?>.*?<([ ]+)?\\/([ ]+)?ref>'", "\n", "x", "=", "re", ".", "sub", "(", "REF_RE", ",", "' '", ",", "x", ")", "\n", "# leading </ref>", "\n", "if", "'</ref>'", "in", "x", ":", "\n", "        ", "x", "=", "re", ".", "sub", "(", "REF_RE", ",", "' '", ",", "'<ref>'", "+", "x", ")", "\n", "# trailing <ref>", "\n", "", "if", "'<ref'", "in", "x", ":", "\n", "        ", "x", "=", "re", ".", "sub", "(", "REF_RE", ",", "' '", ",", "x", "+", "'</ref>'", ")", "\n", "", "return", "x", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.clean_wikitext": [[80, 137], ["re.sub.strip", "gen_data_from_crawl.rm_refs", "re.sub", "mwparserfromhell.parse", "mwparserfromhell.parse.strip_code", "gen_data_from_crawl.rm_refs", "re.sub", "mwparserfromhell.parse", "mwparserfromhell.parse.strip_code", "re.sub().strip.replace().replace", "mwparserfromhell.parse", "mwparserfromhell.parse.strip_code", "re.sub().strip.replace().replace", "re.sub", "re.sub", "re.sub", "re.sub().strip.replace", "re.sub", "re.sub().strip.replace", "re.sub().strip.replace", "re.sub().strip.replace", "re.sub().strip.replace", "re.sub().strip", "re.sub().strip.startswith", "re.sub().strip.startswith", "re.findall", "re.sub().strip.lower", "re.sub().strip.replace", "re.sub().strip.replace", "re.sub"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.rm_refs", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.rm_refs"], ["\n", "", "def", "clean_wikitext", "(", "token_list", ")", ":", "\n", "    ", "x", "=", "' '", ".", "join", "(", "token_list", ")", "\n", "\n", "# ascii only", "\n", "x", "=", "''", ".", "join", "(", "filter", "(", "lambda", "x", ":", "x", "in", "string", ".", "printable", ",", "x", ")", ")", "\n", "\n", "# preemptively remove <ref>'s (including uncompleted)", "\n", "x", "=", "x", ".", "strip", "(", ")", "\n", "x", "=", "rm_refs", "(", "x", ")", "\n", "# collapse multispaces", "\n", "x", "=", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "x", ")", "\n", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "x", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "plaintext", "=", "rm_refs", "(", "plaintext", ")", "# get refs again? some things missed", "\n", "# collapse multispaces", "\n", "plaintext", "=", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "plaintext", ")", "\n", "# parse again to hit complicatd nested wikicode like 21055249", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "plaintext", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "\n", "# ignore lines starting with ! or | (likely table artifacts)", "\n", "if", "plaintext", ".", "startswith", "(", "'?'", ")", "or", "plaintext", ".", "startswith", "(", "'|'", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "\n", "# ignore lines without text, e.g. ( , , , , ) or ]]", "\n", "", "if", "not", "re", ".", "findall", "(", "'\\w'", ",", "plaintext", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "\n", "# parse AGAIN again to hit remaining links e.g. 377258469", "\n", "", "plaintext", "=", "plaintext", ".", "replace", "(", "'[ '", ",", "'['", ")", ".", "replace", "(", "' ]'", ",", "']'", ")", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "plaintext", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "\n", "# at this point just rm all brackets", "\n", "plaintext", "=", "plaintext", ".", "replace", "(", "']'", ",", "''", ")", ".", "replace", "(", "'['", ",", "''", ")", "\n", "# rm html", "\n", "plaintext", "=", "re", ".", "sub", "(", "'http\\S+'", ",", "''", ",", "plaintext", ")", "\n", "# rm parents with nothing in them, e.g. (; )", "\n", "plaintext", "=", "re", ".", "sub", "(", "'\\([^\\w]*\\)'", ",", "''", ",", "plaintext", ")", "\n", "# rm remining <del>, <ins> (valid tags should already have been taken parsed)", "\n", "plaintext", "=", "re", ".", "sub", "(", "'<\\/?(del|ins)([-\\w=\" <>]+)?>'", ",", "''", ",", "plaintext", ")", "\n", "# fuck stars", "\n", "plaintext", "=", "plaintext", ".", "replace", "(", "'*'", ",", "''", ")", "\n", "# rm table fragments", "\n", "plaintext", "=", "re", ".", "sub", "(", "'(right[ ]?\\||left[ ]?\\||thumb[ ]?\\||frame[ ]?\\||\\d+px[ ]?\\|)'", ",", "''", ",", "plaintext", ")", "\n", "# ignore timestamp sentences", "\n", "if", "'retrieved on'", "in", "plaintext", ".", "lower", "(", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "# msc html missed", "\n", "", "plaintext", "=", "plaintext", ".", "replace", "(", "'<blockquote>'", ",", "''", ")", "\n", "\n", "# remove tabs and newlines (those is our deliminators beeyotch)", "\n", "plaintext", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "plaintext", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "plaintext", ".", "replace", "(", "'\\r'", ",", "''", ")", "\n", "# collapse multispaces (again again)", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.find_matches": [[139, 178], ["range", "stats.append", "stats.append", "range", "math.exp", "len", "max", "len", "len", "collections.Counter", "collections.Counter", "stats.append", "stats.append", "len", "sum", "max", "max", "list", "min", "gen_data_from_crawl.find_matches.BLEU"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.BLEU"], ["\n", "return", "plaintext", "\n", "\n", "\n", "", "def", "find_matches", "(", "a_list", ",", "b_list", ",", "delta", "=", "3", ")", ":", "\n", "    ", "def", "BLEU", "(", "hyp", ",", "ref", ")", ":", "\n", "# get ngram stats", "\n", "        ", "stats", "=", "[", "]", "\n", "stats", ".", "append", "(", "len", "(", "hyp", ")", ")", "\n", "stats", ".", "append", "(", "len", "(", "ref", ")", ")", "\n", "for", "n", "in", "range", "(", "1", ",", "5", ")", ":", "\n", "            ", "s_ngrams", "=", "Counter", "(", "\n", "[", "tuple", "(", "hyp", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "hyp", ")", "+", "1", "-", "n", ")", "]", "\n", ")", "\n", "r_ngrams", "=", "Counter", "(", "\n", "[", "tuple", "(", "ref", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "ref", ")", "+", "1", "-", "n", ")", "]", "\n", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "sum", "(", "(", "s_ngrams", "&", "r_ngrams", ")", ".", "values", "(", ")", ")", ",", "0", "]", ")", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "len", "(", "hyp", ")", "+", "1", "-", "n", ",", "0", "]", ")", ")", "\n", "\n", "# get bleu from stats", "\n", "", "if", "len", "(", "list", "(", "filter", "(", "lambda", "x", ":", "x", "==", "0", ",", "stats", ")", ")", ")", ">", "0", ":", "\n", "            ", "return", "0", "\n", "", "(", "c", ",", "r", ")", "=", "stats", "[", ":", "2", "]", "\n", "log_bleu_prec", "=", "sum", "(", "\n", "[", "math", ".", "log", "(", "float", "(", "x", ")", "/", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "stats", "[", "2", ":", ":", "2", "]", ",", "stats", "[", "3", ":", ":", "2", "]", ")", "]", "\n", ")", "/", "4.", "\n", "bleu", "=", "math", ".", "exp", "(", "min", "(", "[", "0", ",", "1", "-", "float", "(", "r", ")", "/", "c", "]", ")", "+", "log_bleu_prec", ")", "\n", "\n", "return", "100", "*", "bleu", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "a_list", ")", ")", ":", "\n", "        ", "neighborhood_bleus", "=", "[", "\n", "(", "BLEU", "(", "a_list", "[", "i", "]", ".", "split", "(", ")", ",", "b_list", "[", "j", "]", ".", "split", "(", ")", ")", ",", "j", ")", "\n", "for", "j", "in", "range", "(", "max", "(", "i", "-", "delta", ",", "0", ")", ",", "min", "(", "i", "+", "delta", ",", "len", "(", "b_list", ")", ")", ")", "\n", "]", "\n", "# corner case: len(a_list) >> len(b_list)", "\n", "if", "not", "neighborhood_bleus", ":", "\n", "            ", "continue", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.tokenize": [[180, 184], ["TOKENIZER.tokenize", "s.strip"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize"], ["\n", "yield", "i", ",", "match_idx", ",", "max_bleu", "\n", "\n", "\n", "", "", "def", "tokenize", "(", "s", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.is_spelling_diff": [[187, 203], ["enumerate", "sum", "autocorrect.spell", "len", "len", "len"], "function", ["None"], ["return", "' '", ".", "join", "(", "tok_list", ")", "\n", "\n", "\n", "\n", "", "def", "is_spelling_diff", "(", "d", ")", ":", "\n", "    ", "\"\"\"takes a word diff as arg\"\"\"", "\n", "global", "SPELLCHECKER", "\n", "\n", "# only look at the one-word diffs", "\n", "if", "sum", "(", "[", "len", "(", "chunk", ")", "for", "tag", ",", "chunk", "in", "d", "if", "tag", "==", "'-'", "]", ")", ">", "1", ":", "\n", "        ", "return", "False", "\n", "\n", "", "for", "i", ",", "(", "tag", ",", "words", ")", "in", "enumerate", "(", "d", ")", ":", "\n", "        ", "if", "tag", "==", "'-'", "and", "i", "+", "1", "<", "len", "(", "d", ")", "-", "1", "and", "len", "(", "words", ")", "==", "1", "and", "d", "[", "i", "+", "1", "]", "[", "0", "]", "==", "'+'", ":", "\n", "# is one-word spelling replacement (post correction)", "\n", "            ", "correction", "=", "spell", "(", "words", "[", "0", "]", ")", "\n", "if", "not", "correction", "==", "words", "[", "0", "]", "and", "correction", "in", "' '", ".", "join", "(", "d", "[", "i", "+", "1", "]", "[", "1", "]", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.get_tok_labels": [[205, 216], ["len", "len"], "function", ["None"], ["\n", "", "", "", "return", "False", "\n", "\n", "\n", "", "def", "get_tok_labels", "(", "s_diff", ")", ":", "\n", "    ", "tok_labels", "=", "[", "]", "\n", "for", "tag", ",", "chunk", "in", "s_diff", ":", "\n", "        ", "if", "tag", "==", "'='", ":", "\n", "            ", "tok_labels", "+=", "[", "0", "]", "*", "len", "(", "chunk", ")", "\n", "", "elif", "tag", "==", "'-'", ":", "\n", "            ", "tok_labels", "+=", "[", "1", "]", "*", "len", "(", "chunk", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.should_keep": [[218, 268], ["simplediff.diff", "gen_data_from_crawl.get_tok_labels", "simplediff.diff", "gen_data_from_crawl.is_spelling_diff", "Levenshtein.distance", "prev_tok.split", "post_tok.split", "len", "len", "re.search", "int", "nltk.word_tokenize", "nltk.word_tokenize", "sum", "prev_tok.split", "len", "sum", "len", "range", "len", "prev_tok.split"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.get_tok_labels", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.is_spelling_diff"], ["\n", "", "", "return", "tok_labels", "\n", "\n", "\n", "", "def", "should_keep", "(", "prev_raw", ",", "prev_tok", ",", "post_raw", ",", "post_tok", ",", "bleu", ",", "rev_id", ")", ":", "\n", "    ", "global", "CTR_LOW_BLEU", "\n", "global", "CTR_LOW_LEVEN", "\n", "global", "CTR_TOO_MANY_1_TOKS", "\n", "global", "CTR_SPELLING", "\n", "global", "CTR_CHEMISTRY", "\n", "global", "CTR_ONLY_PUNC_CHANGED", "\n", "\n", "# KEEP -- exact match", "\n", "if", "bleu", "==", "100", "or", "prev_raw", "==", "post_raw", ":", "\n", "        ", "return", "True", ",", "None", ",", "[", "0", "for", "_", "in", "range", "(", "len", "(", "prev_tok", ".", "split", "(", ")", ")", ")", "]", "\n", "\n", "# clearly not a match", "\n", "", "if", "bleu", "<", "15.0", ":", "\n", "        ", "CTR_LOW_BLEU", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "# too close", "\n", "", "if", "Levenshtein", ".", "distance", "(", "prev_tok", ",", "post_tok", ")", "<", "4", ":", "\n", "        ", "CTR_LOW_LEVEN", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "", "tok_diff", "=", "diff", "(", "prev_tok", ".", "split", "(", ")", ",", "post_tok", ".", "split", "(", ")", ")", "\n", "tok_labels", "=", "get_tok_labels", "(", "tok_diff", ")", "\n", "assert", "len", "(", "tok_labels", ")", "==", "len", "(", "prev_tok", ".", "split", "(", ")", ")", "\n", "\n", "changed_text", "=", "''", ".", "join", "(", "[", "''", ".", "join", "(", "chunk", ")", "for", "tag", ",", "chunk", "in", "tok_diff", "if", "tag", "!=", "'='", "]", ")", "\n", "if", "not", "re", ".", "search", "(", "'[a-z]'", ",", "changed_text", ")", ":", "\n", "        ", "CTR_ONLY_PUNC_CHANGED", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# too dissimilar -- less than half of toks shared", "\n", "", "tok_nums", "=", "[", "int", "(", "x", ")", "for", "x", "in", "tok_labels", "]", "\n", "if", "(", "sum", "(", "tok_nums", ")", "*", "1.0", "/", "len", "(", "tok_nums", ")", ")", ">", "0.5", ":", "\n", "        ", "CTR_TOO_MANY_1_TOKS", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# edit was just fixing a spelling error", "\n", "", "word_diff", "=", "diff", "(", "word_tokenize", "(", "prev_raw", ")", ",", "word_tokenize", "(", "post_raw", ")", ")", "\n", "if", "is_spelling_diff", "(", "word_diff", ")", ":", "\n", "        ", "CTR_SPELLING", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# some simple filtering to get out the chemistry \"neutral\" edits", "\n", "", "if", "' molecules'", "in", "prev_raw", "or", "' ions'", "in", "prev_raw", "or", "' ionic'", "in", "prev_raw", "or", "' atoms'", "in", "prev_raw", ":", "\n", "        ", "CTR_CHEMISTRY", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.sent_generator": [[271, 340], ["tqdm.tqdm", "isinstance", "isinstance", "clean_wikitext().lower", "clean_wikitext().lower", "nltk.sent_tokenize", "nltk.sent_tokenize", "gen_data_from_crawl.find_matches", "gen_data_from_crawl.tokenize", "gen_data_from_crawl.tokenize", "gen_data_from_crawl.should_keep", "sum", "len", "x.decode", "x.decode", "len", "len", "gen_data_from_crawl.clean_wikitext", "gen_data_from_crawl.clean_wikitext", "rev_examples.append", "len", "len", "len", "len", "len", "list", "set", "set"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.find_matches", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.data.gen_data_from_crawl.should_keep", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.clean_wikitext", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.clean_wikitext"], ["# n_words = sum(1 if d.check(w) else 0 for w in pre_words)", "\n", "# if len(prev_words) == 0 or (float(n_words) / len(prev_words)) < 0.5:", "\n", "#     return False, None, None", "\n", "\n", "\n", "# see if this is a \"single word\" edit, where a single word was replaced with 0+ words", "\n", "", "def", "is_single_word_edit", "(", "d", ")", ":", "\n", "        ", "\"\"\" is this diff good for the final generation dataset \"\"\"", "\n", "pre_chunks", "=", "[", "chunk", "for", "tag", ",", "chunk", "in", "d", "if", "tag", "==", "'-'", "]", "\n", "post_chunks", "=", "[", "chunk", "for", "tag", ",", "chunk", "in", "d", "if", "tag", "==", "'+'", "]", "\n", "# a single word on the pre side", "\n", "if", "sum", "(", "[", "len", "(", "chunk", ")", "for", "chunk", "in", "pre_chunks", "]", ")", "!=", "1", ":", "\n", "            ", "return", "False", "\n", "# 0 words in the post", "\n", "", "if", "len", "(", "post_chunks", ")", "==", "0", ":", "\n", "            ", "return", "True", "\n", "# ensure 1 post chunk", "\n", "", "if", "len", "(", "post_chunks", ")", ">", "1", ":", "\n", "            ", "return", "False", "\n", "# post language chunk is directly after the pre chunk", "\n", "", "prei", "=", "next", "(", "(", "i", "for", "i", ",", "x", "in", "enumerate", "(", "d", ")", "if", "x", "[", "0", "]", "==", "'-'", ")", ")", "\n", "if", "prei", "<", "len", "(", "d", ")", "-", "1", "and", "d", "[", "prei", "+", "1", "]", "[", "0", "]", "==", "'+'", ":", "\n", "            ", "return", "True", "\n", "", "", "single_word_edit", "=", "is_single_word_edit", "(", "word_diff", ")", "\n", "\n", "return", "True", ",", "single_word_edit", ",", "tok_labels", "\n", "\n", "\n", "\n", "\n", "", "def", "sent_generator", "(", "revisions", ")", ":", "\n", "    ", "global", "CTR_EMPTY_REV", "\n", "global", "CTR_MULTIPLE_EDITS", "\n", "global", "CTR_FAILED_CLEANING", "\n", "global", "CTR_DUPS", "\n", "global", "CTR_INVALID_NUM_CHANGED_SENTS", "\n", "global", "CTR_NON_EDIT_CHUNKS", "\n", "global", "CTR_EDIT_CHANGED_NUM_SENTS", "\n", "global", "CTR_FAILED_TAGGING", "\n", "\n", "for", "rev_id", "in", "tqdm", "(", "revisions", ")", ":", "\n", "        ", "prevs", ",", "posts", ",", "prev_deleted", ",", "posts_added", "=", "revisions", "[", "rev_id", "]", "\n", "\n", "# empty revision", "\n", "if", "not", "prevs", "or", "not", "posts", ":", "\n", "            ", "CTR_EMPTY_REV", "+=", "1", "\n", "continue", "\n", "\n", "", "if", "prev_deleted", "!=", "[", "'no_deleted_chunks'", "]", "or", "posts_added", "!=", "[", "'no_added_chunks'", "]", ":", "\n", "            ", "CTR_NON_EDIT_CHUNKS", "+=", "1", "\n", "continue", "\n", "\n", "# unicode dat shit", "\n", "", "if", "isinstance", "(", "prevs", "[", "0", "]", ",", "bytes", ")", ":", "\n", "            ", "prevs", "=", "[", "x", ".", "decode", "(", ")", "for", "x", "in", "prevs", "]", "\n", "", "if", "isinstance", "(", "posts", "[", "0", "]", ",", "bytes", ")", ":", "\n", "            ", "posts", "=", "[", "x", ".", "decode", "(", ")", "for", "x", "in", "posts", "]", "\n", "\n", "# multiple edits", "\n", "", "if", "len", "(", "prevs", ")", ">", "1", "or", "len", "(", "posts", ")", ">", "1", ":", "\n", "            ", "CTR_MULTIPLE_EDITS", "+=", "1", "\n", "continue", "\n", "\n", "", "prev_text", "=", "clean_wikitext", "(", "prevs", ")", ".", "lower", "(", ")", "\n", "post_text", "=", "clean_wikitext", "(", "posts", ")", ".", "lower", "(", ")", "\n", "\n", "# failed cleaning", "\n", "if", "not", "prev_text", "or", "not", "post_text", ":", "\n", "            ", "CTR_FAILED_CLEANING", "+=", "1", "\n", "continue", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.diff_vec_clustering_attempt.diff_explore.read_vecs": [[9, 14], ["open", "numpy.array", "out.append", "float", "l.strip().split", "l.strip"], "function", ["None"], ["def", "read_vecs", "(", "path", ")", ":", "\n", "\t", "out", "=", "[", "]", "\n", "for", "l", "in", "open", "(", "path", ")", ":", "\n", "\t\t", "out", ".", "append", "(", "[", "float", "(", "x", ")", "for", "x", "in", "l", ".", "strip", "(", ")", ".", "split", "(", ")", "]", ")", "\n", "", "return", "np", ".", "array", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.diff_vec_clustering_attempt.make_corpus_vectors.diff": [[21, 26], ["diff_match_patch.diff_match_patch", "dmp_module.diff_match_patch.diff_main", "dmp_module.diff_match_patch.diff_cleanupSemantic"], "function", ["None"], ["def", "diff", "(", "s1", ",", "s2", ")", ":", "\n", "    ", "dmp", "=", "dmp_module", ".", "diff_match_patch", "(", ")", "\n", "d", "=", "dmp", ".", "diff_main", "(", "s1", ",", "s2", ")", "\n", "dmp", ".", "diff_cleanupSemantic", "(", "d", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.diff_vec_clustering_attempt.make_corpus_vectors.set_diff": [[27, 31], ["set", "set", "set.strip().split", "set.strip().split", "set.strip", "set.strip"], "function", ["None"], ["", "def", "set_diff", "(", "s1", ",", "s2", ")", ":", "\n", "\t", "s1", "=", "set", "(", "s1", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "s2", "=", "set", "(", "s2", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "return", "' '", ".", "join", "(", "s1", "-", "s2", ")", ",", "' '", ".", "join", "(", "s2", "-", "s1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.diff_vec_clustering_attempt.vector_diff.read_vecs": [[9, 14], ["open", "numpy.array", "out.append", "float", "l.strip().split", "l.strip"], "function", ["None"], ["def", "read_vecs", "(", "path", ")", ":", "\n", "\t", "out", "=", "[", "]", "\n", "for", "l", "in", "open", "(", "path", ")", ":", "\n", "\t\t", "out", ".", "append", "(", "[", "float", "(", "x", ")", "for", "x", "in", "l", ".", "strip", "(", ")", ".", "split", "(", ")", "]", ")", "\n", "", "return", "np", ".", "array", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff": [[40, 45], ["diff_match_patch.diff_match_patch", "dmp_module.diff_match_patch.diff_main", "dmp_module.diff_match_patch.diff_cleanupSemantic"], "function", ["None"], ["def", "diff", "(", "s1", ",", "s2", ")", ":", "\n", "    ", "dmp", "=", "dmp_module", ".", "diff_match_patch", "(", ")", "\n", "d", "=", "dmp", ".", "diff_main", "(", "s1", ",", "s2", ")", "\n", "dmp", ".", "diff_cleanupSemantic", "(", "d", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.load_metadata": [[47, 59], ["open", "l.strip().split", "l.strip"], "function", ["None"], ["", "def", "load_metadata", "(", "path", ")", ":", "\n", "    ", "out", "=", "{", "}", "\n", "for", "l", "in", "open", "(", "path", ")", ":", "\n", "        ", "parts", "=", "l", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "out", "[", "parts", "[", "0", "]", "]", "=", "{", "\n", "'rev_comment'", ":", "parts", "[", "1", "]", ",", "\n", "'rev_user'", ":", "parts", "[", "2", "]", ",", "\n", "'rev_user_text'", ":", "parts", "[", "3", "]", ",", "\n", "'rev_timestamp'", ":", "parts", "[", "4", "]", ",", "\n", "'rev_minor_edit'", ":", "parts", "[", "5", "]", "\n", "}", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.ratio": [[60, 62], ["len", "len"], "function", ["None"], ["", "def", "ratio", "(", "s1", ",", "s2", ")", ":", "\n", "    ", "return", "len", "(", "s1", ")", "*", "1.0", "/", "len", "(", "s2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.tokenize": [[64, 68], ["nltk.word_tokenize", "re.sub", "s.lower"], "function", ["None"], ["", "def", "tokenize", "(", "s", ")", ":", "\n", "    ", "tok_list", "=", "word_tokenize", "(", "s", ".", "lower", "(", ")", ")", "\n", "s_tok", "=", "' '", ".", "join", "(", "tok_list", ")", "\n", "return", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "s_tok", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.BLEU": [[70, 95], ["stats.append", "stats.append", "range", "math.exp", "len", "len", "collections.Counter", "collections.Counter", "stats.append", "stats.append", "len", "sum", "max", "max", "list", "min", "tuple", "tuple", "filter", "math.log", "range", "range", "sum", "zip", "float", "len", "float", "len", "len"], "function", ["None"], ["", "def", "BLEU", "(", "hyp", ",", "ref", ")", ":", "\n", "# get ngram stats", "\n", "    ", "stats", "=", "[", "]", "\n", "stats", ".", "append", "(", "len", "(", "hyp", ")", ")", "\n", "stats", ".", "append", "(", "len", "(", "ref", ")", ")", "\n", "for", "n", "in", "range", "(", "1", ",", "5", ")", ":", "\n", "        ", "s_ngrams", "=", "Counter", "(", "\n", "[", "tuple", "(", "hyp", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "hyp", ")", "+", "1", "-", "n", ")", "]", "\n", ")", "\n", "r_ngrams", "=", "Counter", "(", "\n", "[", "tuple", "(", "ref", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "ref", ")", "+", "1", "-", "n", ")", "]", "\n", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "sum", "(", "(", "s_ngrams", "&", "r_ngrams", ")", ".", "values", "(", ")", ")", ",", "0", "]", ")", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "len", "(", "hyp", ")", "+", "1", "-", "n", ",", "0", "]", ")", ")", "\n", "\n", "# get bleu from stats", "\n", "", "if", "len", "(", "list", "(", "filter", "(", "lambda", "x", ":", "x", "==", "0", ",", "stats", ")", ")", ")", ">", "0", ":", "\n", "        ", "return", "0", "\n", "", "(", "c", ",", "r", ")", "=", "stats", "[", ":", "2", "]", "\n", "log_bleu_prec", "=", "sum", "(", "\n", "[", "math", ".", "log", "(", "float", "(", "x", ")", "/", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "stats", "[", "2", ":", ":", "2", "]", ",", "stats", "[", "3", ":", ":", "2", "]", ")", "]", "\n", ")", "/", "4.", "\n", "bleu", "=", "math", ".", "exp", "(", "min", "(", "[", "0", ",", "1", "-", "float", "(", "r", ")", "/", "c", "]", ")", "+", "log_bleu_prec", ")", "\n", "\n", "return", "100", "*", "bleu", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.get_sents": [[97, 148], ["nltk.sent_tokenize", "nltk.sent_tokenize", "enumerate", "max", "gen_parallel_corpus.diff", "len", "Levenshtein.distance", "len", "len", "re.findall", "gen_parallel_corpus.BLEU", "range", "prev_sent.strip", "next_sent.strip", "max", "min", "prev_sent.strip", "next_sent.strip", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.BLEU"], ["", "def", "get_sents", "(", "prev_edit_str", ",", "next_edit_str", ")", ":", "\n", "    ", "global", "CTR_SENT_MISMATCH", "\n", "\n", "prev_sents", "=", "sent_tokenize", "(", "prev_edit_str", ")", "\n", "next_sents", "=", "sent_tokenize", "(", "next_edit_str", ")", "\n", "\n", "for", "i", ",", "prev_sent", "in", "enumerate", "(", "prev_sents", ")", ":", "\n", "        ", "bleus", "=", "[", "\n", "(", "BLEU", "(", "prev_sents", "[", "i", "]", ",", "next_sents", "[", "j", "]", ")", ",", "j", ")", "\n", "for", "j", "in", "range", "(", "max", "(", "i", "-", "5", ",", "0", ")", ",", "min", "(", "i", "+", "5", ",", "len", "(", "next_sents", ")", "-", "1", ")", ")", "\n", "]", "\n", "# corner case: way more prev's than next's", "\n", "if", "not", "bleus", ":", "\n", "            ", "continue", "\n", "", "match_bleu", ",", "match_idx", "=", "max", "(", "bleus", ")", "\n", "next_sent", "=", "next_sents", "[", "match_idx", "]", "\n", "# skip perfect matches", "\n", "if", "match_bleu", "==", "100", ":", "\n", "            ", "yield", "prev_sent", ".", "strip", "(", ")", ",", "next_sent", ".", "strip", "(", ")", ",", "'0'", "\n", "continue", "\n", "\n", "# skip near-perfect matches", "\n", "", "if", "Levenshtein", ".", "distance", "(", "prev_sent", ",", "next_sent", ")", "<", "4", ":", "\n", "            ", "continue", "\n", "\n", "", "sent_diff", "=", "diff", "(", "prev_sent", ",", "next_sent", ")", "\n", "shared_regions", "=", "[", "x", "for", "x", "in", "sent_diff", "if", "x", "[", "0", "]", "==", "0", "]", "\n", "dif_regions", "=", "[", "x", "for", "x", "in", "sent_diff", "if", "x", "[", "0", "]", "!=", "0", "]", "\n", "\n", "# skip completely different matches (or, again, completely identical)", "\n", "if", "not", "shared_regions", "or", "not", "dif_regions", ":", "\n", "            ", "continue", "\n", "\n", "# skip matches that are too different", "\n", "", "shared_len", "=", "len", "(", "' '", ".", "join", "(", "[", "s", "for", "_", ",", "s", "in", "shared_regions", "]", ")", ")", "\n", "prev_ratio", "=", "shared_len", "*", "1.0", "/", "len", "(", "prev_sent", ")", "\n", "post_ratio", "=", "shared_len", "*", "1.0", "/", "len", "(", "next_sent", ")", "\n", "ratio", "=", "(", "prev_ratio", "+", "post_ratio", ")", "/", "2.0", "\n", "if", "ratio", "<", "0.5", ":", "\n", "            ", "continue", "\n", "\n", "# skip matches where only punctuation is shared", "\n", "", "if", "not", "re", ".", "findall", "(", "'\\w'", ",", "''", ".", "join", "(", "[", "s", "for", "_", ",", "s", "in", "shared_regions", "]", ")", ")", ":", "\n", "            ", "continue", "\n", "\n", "# TODO -- skip matches where diff occurs AFTER length threshold", "\n", "# ALSO MAX LENGTH THRESHOLD, THROW OUT EXAMPLES WHOSE", "\n", "# DIF IS AFTER MY MAX LENGTH (MAYBE 60 OR SO)", "\n", "\n", "\n", "", "yield", "prev_sent", ".", "strip", "(", ")", ",", "next_sent", ".", "strip", "(", ")", ",", "'1'", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.prep_wikitext": [[150, 198], ["x.replace.replace", "x.replace.replace", "x.replace.replace", "x.replace.replace", "mwparserfromhell.parse", "mwparserfromhell.parse.strip_code", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "plaintext.strip.replace", "plaintext.strip.replace", "plaintext.strip.replace", "plaintext.strip.strip", "plaintext.strip.lower", "plaintext.strip.startswith", "plaintext.strip.startswith"], "function", ["None"], ["", "", "def", "prep_wikitext", "(", "token_list", ")", ":", "\n", "    ", "x", "=", "' '", ".", "join", "(", "token_list", ")", "\n", "# fix tags", "\n", "x", "=", "x", ".", "replace", "(", "'< '", ",", "'<'", ")", "\n", "x", "=", "x", ".", "replace", "(", "'</ '", ",", "'</'", ")", "\n", "x", "=", "x", ".", "replace", "(", "' >'", ",", "'>'", ")", "\n", "x", "=", "x", ".", "replace", "(", "' />'", ",", "'/>'", ")", "\n", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "x", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "\n", "# rm [[text]] and [text]", "\n", "plaintext", "=", "re", ".", "sub", "(", "'\\[?\\[.*?\\]\\]?'", ",", "''", ",", "plaintext", ")", "\n", "# rm {{text}} and {text}", "\n", "plaintext", "=", "re", ".", "sub", "(", "'\\{?\\{.*?\\}\\}?'", ",", "''", ",", "plaintext", ")", "\n", "# collapse multispaces ", "\n", "plaintext", "=", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "plaintext", ")", "\n", "# remove urls", "\n", "plaintext", "=", "re", ".", "sub", "(", "'(?P<url>https?://[^\\\\s]+)'", ",", "''", ",", "plaintext", ")", "\n", "# remove wiki headings (sometimes mwparserfromhell misses these)", "\n", "plaintext", "=", "re", ".", "sub", "(", "'==(.*)?=='", ",", "''", ",", "plaintext", ")", "\n", "# remove leftover table bits", "\n", "plaintext", "=", "re", ".", "sub", "(", "'\\|?thumb( )?\\|(.*)?(right|left)( )?(\\|?)'", ",", "''", ",", "plaintext", ")", "\n", "plaintext", "=", "plaintext", ".", "replace", "(", "'thumb|'", ",", "''", ")", "\n", "# empty parens", "\n", "plaintext", "=", "plaintext", ".", "replace", "(", "'()'", ",", "''", ")", "\n", "# ignore timestamp sentences", "\n", "if", "'retrieved on'", "in", "plaintext", ".", "lower", "(", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "# fuck stars", "\n", "", "plaintext", "=", "plaintext", ".", "replace", "(", "'*'", ",", "''", ")", "\n", "\n", "# TODO?", "\n", "# 200px|", "\n", "# ( , , , , )", "\n", "# | year = 2002 |accessdate = may 31 , 2006 |url=", "\n", "# name= '' hrw '' / >", "\n", "# ( come-and-hear .com/yebamoth/yebamoth_47.html # partb babylonian talmud yevamot 47b . )", "\n", "#    JUST RM ALL PARENS??", "\n", "\n", "\n", "\n", "# rm examples starting with ! or | (will be thrown out in downstream filtering)", "\n", "plaintext", "=", "plaintext", ".", "strip", "(", ")", "\n", "if", "plaintext", ".", "startswith", "(", "'?'", ")", "or", "plaintext", ".", "startswith", "(", "'|'", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "\n", "", "return", "plaintext", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.extract_examples": [[201, 243], ["tqdm.tqdm", "enumerate", "gen_parallel_corpus.prep_wikitext", "gen_parallel_corpus.prep_wikitext", "gen_parallel_corpus.get_sents", "iter", "len", "metadata.items", "len", "len", "gen_parallel_corpus.tokenize", "gen_parallel_corpus.tokenize", "gen_parallel_corpus.tokenize", "gen_parallel_corpus.ratio", "gen_parallel_corpus.tokenize", "gen_parallel_corpus.tokenize"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.prep_wikitext", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.prep_wikitext", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.get_sents", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.ratio", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize"], ["", "def", "extract_examples", "(", "metadata", ",", "revisions", ")", ":", "\n", "    ", "global", "CTR_ID_MISMATCH", "\n", "global", "CTR_MULTIPLE_EDITS", "\n", "global", "CTR_NO_TEXT", "\n", "global", "CTR_NO_EDITS", "\n", "\n", "\n", "for", "i", ",", "(", "rev_id", ",", "metadata_dict", ")", "in", "tqdm", "(", "enumerate", "(", "iter", "(", "metadata", ".", "items", "(", ")", ")", ")", ",", "total", "=", "len", "(", "metadata", ")", ")", ":", "\n", "# ignore headers...     ", "\n", "        ", "if", "i", "==", "0", ":", "continue", "\n", "\n", "if", "rev_id", "not", "in", "revisions", ":", "\n", "            ", "CTR_ID_MISMATCH", "+=", "1", "\n", "continue", "\n", "\n", "", "prevs", ",", "nexts", "=", "revisions", "[", "rev_id", "]", "\n", "\n", "if", "len", "(", "prevs", ")", ">", "1", "or", "len", "(", "nexts", ")", ">", "1", ":", "\n", "            ", "CTR_MULTIPLE_EDITS", "+=", "1", "\n", "continue", "\n", "\n", "", "prev_text", "=", "prep_wikitext", "(", "prevs", ")", "\n", "next_text", "=", "prep_wikitext", "(", "nexts", ")", "\n", "\n", "if", "not", "prev_text", "or", "not", "next_text", ":", "\n", "            ", "CTR_NO_TEXT", "+=", "1", "\n", "continue", "\n", "\n", "", "i", "=", "0", "\n", "for", "prev_sent", ",", "next_sent", ",", "bias_status", "in", "get_sents", "(", "prev_text", ",", "next_text", ")", ":", "\n", "            ", "i", "+=", "1", "\n", "#            print(prev_sent)", "\n", "#            print(next_sent)", "\n", "#            print()", "\n", "yield", "(", "\n", "rev_id", ",", "tokenize", "(", "metadata_dict", "[", "'rev_comment'", "]", ")", ",", "\n", "tokenize", "(", "prev_sent", ")", ",", "tokenize", "(", "next_sent", ")", ",", "bias_status", ",", "\n", "ratio", "(", "tokenize", "(", "prev_sent", ")", ",", "tokenize", "(", "next_sent", ")", ")", "\n", ")", "\n", "\n", "", "if", "i", "==", "0", ":", "\n", "            ", "CTR_NO_EDITS", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.diff_dmp": [[54, 59], ["diff_match_patch.diff_match_patch", "dmp_module.diff_match_patch.diff_main", "dmp_module.diff_match_patch.diff_cleanupSemantic"], "function", ["None"], ["CTR_LENGTH_RATIO", "=", "0", "\n", "CTR_CHEMISTRY", "=", "0", "\n", "CTR_DUPS", "=", "0", "\n", "CTR_ONLY_PUNC_CHANGED", "=", "0", "\n", "CTR_INVALID_NUM_CHANGED_SENTS", "=", "0", "\n", "CTR_EDIT_CHANGED_NUM_SENTS", "=", "0", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.load_metadata": [[61, 73], ["open", "l.strip().split", "l.strip"], "function", ["None"], ["CTR_FAILED_TAGGING", "=", "0", "\n", "\n", "\n", "BERT_MODEL", "=", "\"bert-base-uncased\"", "\n", "TOKENIZER", "=", "BertTokenizer", ".", "from_pretrained", "(", "BERT_MODEL", ",", "cache_dir", "=", "cache_path", ")", "\n", "\n", "# ENCHANT_DICT = enchant.Dict(\"en_US\")", "\n", "\n", "\n", "def", "rm_refs", "(", "x", ")", ":", "\n", "    ", "REF_RE", "=", "'<ref([-\\w=\" <>]+)?>.*?<([ ]+)?\\/([ ]+)?ref>'", "\n", "x", "=", "re", ".", "sub", "(", "REF_RE", ",", "' '", ",", "x", ")", "\n", "# leading </ref>", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.ratio": [[74, 76], ["len", "len"], "function", ["None"], ["if", "'</ref>'", "in", "x", ":", "\n", "        ", "x", "=", "re", ".", "sub", "(", "REF_RE", ",", "' '", ",", "'<ref>'", "+", "x", ")", "\n", "# trailing <ref>", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.clean_wikitext": [[78, 124], ["x.replace.replace", "x.replace.replace", "x.replace.replace", "x.replace.replace", "mwparserfromhell.parse", "mwparserfromhell.parse.strip_code", "plaintext.strip.replace", "plaintext.strip.replace", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "plaintext.strip.replace", "plaintext.strip.replace", "plaintext.strip.replace", "plaintext.strip.strip", "plaintext.strip.lower", "re.findall", "plaintext.strip.startswith", "plaintext.strip.startswith"], "function", ["None"], ["        ", "x", "=", "re", ".", "sub", "(", "REF_RE", ",", "' '", ",", "x", "+", "'</ref>'", ")", "\n", "", "return", "x", "\n", "\n", "", "def", "clean_wikitext", "(", "token_list", ")", ":", "\n", "    ", "x", "=", "' '", ".", "join", "(", "token_list", ")", "\n", "\n", "# ascii only", "\n", "x", "=", "''", ".", "join", "(", "filter", "(", "lambda", "x", ":", "x", "in", "string", ".", "printable", ",", "x", ")", ")", "\n", "\n", "# preemptively remove <ref>'s (including uncompleted)", "\n", "x", "=", "x", ".", "strip", "(", ")", "\n", "x", "=", "rm_refs", "(", "x", ")", "\n", "# collapse multispaces", "\n", "x", "=", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "x", ")", "\n", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "x", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "plaintext", "=", "rm_refs", "(", "plaintext", ")", "# get refs again? some things missed", "\n", "# collapse multispaces", "\n", "plaintext", "=", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "plaintext", ")", "\n", "# parse again to hit complicatd nested wikicode like 21055249", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "plaintext", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "\n", "# ignore lines starting with ! or | (likely table artifacts)", "\n", "if", "plaintext", ".", "startswith", "(", "'?'", ")", "or", "plaintext", ".", "startswith", "(", "'|'", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "\n", "# ignore lines without text, e.g. ( , , , , ) or ]]", "\n", "", "if", "not", "re", ".", "findall", "(", "'\\w'", ",", "plaintext", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "\n", "# parse AGAIN again to hit remaining links e.g. 377258469", "\n", "", "plaintext", "=", "plaintext", ".", "replace", "(", "'[ '", ",", "'['", ")", ".", "replace", "(", "' ]'", ",", "']'", ")", "\n", "parse", "=", "mwparserfromhell", ".", "parse", "(", "plaintext", ")", "\n", "plaintext", "=", "parse", ".", "strip_code", "(", ")", "\n", "\n", "# at this point just rm all brackets", "\n", "plaintext", "=", "plaintext", ".", "replace", "(", "']'", ",", "''", ")", ".", "replace", "(", "'['", ",", "''", ")", "\n", "# rm html", "\n", "plaintext", "=", "re", ".", "sub", "(", "'http\\S+'", ",", "''", ",", "plaintext", ")", "\n", "# rm parents with nothing in them, e.g. (; )", "\n", "plaintext", "=", "re", ".", "sub", "(", "'\\([^\\w]*\\)'", ",", "''", ",", "plaintext", ")", "\n", "# rm remining <del>, <ins> (valid tags should already have been taken parsed)", "\n", "plaintext", "=", "re", ".", "sub", "(", "'<\\/?(del|ins)([-\\w=\" <>]+)?>'", ",", "''", ",", "plaintext", ")", "\n", "# fuck stars", "\n", "plaintext", "=", "plaintext", ".", "replace", "(", "'*'", ",", "''", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.BLEU": [[127, 152], ["stats.append", "stats.append", "range", "math.exp", "len", "len", "collections.Counter", "collections.Counter", "stats.append", "stats.append", "len", "sum", "max", "max", "list", "min", "tuple", "tuple", "filter", "math.log", "range", "range", "sum", "zip", "float", "len", "float", "len", "len"], "function", ["None"], ["# ignore timestamp sentences", "\n", "if", "'retrieved on'", "in", "plaintext", ".", "lower", "(", ")", ":", "\n", "        ", "plaintext", "=", "''", "\n", "# msc html missed", "\n", "", "plaintext", "=", "plaintext", ".", "replace", "(", "'<blockquote>'", ",", "''", ")", "\n", "\n", "# remove tabs and newlines (those is our deliminators beeyotch)", "\n", "plaintext", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "plaintext", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "plaintext", ".", "replace", "(", "'\\r'", ",", "''", ")", "\n", "# collapse multispaces (again again)", "\n", "plaintext", "=", "re", ".", "sub", "(", "'[ ]+'", ",", "' '", ",", "plaintext", ")", ".", "strip", "(", ")", "\n", "\n", "return", "plaintext", "\n", "\n", "\n", "", "def", "find_matches", "(", "a_list", ",", "b_list", ",", "delta", "=", "3", ")", ":", "\n", "    ", "def", "BLEU", "(", "hyp", ",", "ref", ")", ":", "\n", "# get ngram stats", "\n", "        ", "stats", "=", "[", "]", "\n", "stats", ".", "append", "(", "len", "(", "hyp", ")", ")", "\n", "stats", ".", "append", "(", "len", "(", "ref", ")", ")", "\n", "for", "n", "in", "range", "(", "1", ",", "5", ")", ":", "\n", "            ", "s_ngrams", "=", "Counter", "(", "\n", "[", "tuple", "(", "hyp", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "hyp", ")", "+", "1", "-", "n", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.find_matches": [[154, 167], ["range", "len", "max", "gen_data_from_crawl.BLEU", "range", "a_list[].split", "b_list[].split", "max", "min", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.BLEU"], ["[", "tuple", "(", "ref", "[", "i", ":", "i", "+", "n", "]", ")", "for", "i", "in", "range", "(", "len", "(", "ref", ")", "+", "1", "-", "n", ")", "]", "\n", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "sum", "(", "(", "s_ngrams", "&", "r_ngrams", ")", ".", "values", "(", ")", ")", ",", "0", "]", ")", ")", "\n", "stats", ".", "append", "(", "max", "(", "[", "len", "(", "hyp", ")", "+", "1", "-", "n", ",", "0", "]", ")", ")", "\n", "\n", "# get bleu from stats", "\n", "", "if", "len", "(", "list", "(", "filter", "(", "lambda", "x", ":", "x", "==", "0", ",", "stats", ")", ")", ")", ">", "0", ":", "\n", "            ", "return", "0", "\n", "", "(", "c", ",", "r", ")", "=", "stats", "[", ":", "2", "]", "\n", "log_bleu_prec", "=", "sum", "(", "\n", "[", "math", ".", "log", "(", "float", "(", "x", ")", "/", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "stats", "[", "2", ":", ":", "2", "]", ",", "stats", "[", "3", ":", ":", "2", "]", ")", "]", "\n", ")", "/", "4.", "\n", "bleu", "=", "math", ".", "exp", "(", "min", "(", "[", "0", ",", "1", "-", "float", "(", "r", ")", "/", "c", "]", ")", "+", "log_bleu_prec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.is_spelling_diff": [[170, 190], ["simplediff.diff", "spellchecker.SpellChecker", "enumerate", "nltk.word_tokenize", "nltk.word_tokenize", "sum", "len", "len", "spellchecker.SpellChecker.correction", "len", "spellchecker.SpellChecker.correction"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff"], ["", "for", "i", "in", "range", "(", "len", "(", "a_list", ")", ")", ":", "\n", "        ", "neighborhood_bleus", "=", "[", "\n", "(", "BLEU", "(", "a_list", "[", "i", "]", ".", "split", "(", ")", ",", "b_list", "[", "j", "]", ".", "split", "(", ")", ")", ",", "j", ")", "\n", "for", "j", "in", "range", "(", "max", "(", "i", "-", "delta", ",", "0", ")", ",", "min", "(", "i", "+", "delta", ",", "len", "(", "b_list", ")", ")", ")", "\n", "]", "\n", "# corner case: len(a_list) >> len(b_list)", "\n", "if", "not", "neighborhood_bleus", ":", "\n", "            ", "continue", "\n", "\n", "", "max_bleu", ",", "match_idx", "=", "max", "(", "neighborhood_bleus", ")", "\n", "\n", "yield", "i", ",", "match_idx", ",", "max_bleu", "\n", "\n", "\n", "", "", "def", "tokenize", "(", "s", ")", ":", "\n", "    ", "global", "TOKENIZER", "\n", "tok_list", "=", "TOKENIZER", ".", "tokenize", "(", "s", ".", "strip", "(", ")", ")", "\n", "return", "' '", ".", "join", "(", "tok_list", ")", "\n", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.should_filter": [[193, 232], ["gen_data_from_crawl.diff_dmp", "gen_data_from_crawl.is_spelling_diff", "Levenshtein.distance", "len", "len", "int", "re.findall", "prev_tok.split", "len", "sum"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.diff_dmp", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.is_spelling_diff"], ["global", "SPELLCHECKER", "\n", "\n", "# only look at the one-word diffs", "\n", "if", "sum", "(", "[", "len", "(", "chunk", ")", "for", "tag", ",", "chunk", "in", "d", "if", "tag", "==", "'-'", "]", ")", ">", "1", ":", "\n", "        ", "return", "False", "\n", "\n", "", "for", "i", ",", "(", "tag", ",", "words", ")", "in", "enumerate", "(", "d", ")", ":", "\n", "        ", "if", "tag", "==", "'-'", "and", "i", "+", "1", "<", "len", "(", "d", ")", "-", "1", "and", "len", "(", "words", ")", "==", "1", "and", "d", "[", "i", "+", "1", "]", "[", "0", "]", "==", "'+'", ":", "\n", "# is one-word spelling replacement (post correction)", "\n", "            ", "correction", "=", "spell", "(", "words", "[", "0", "]", ")", "\n", "if", "not", "correction", "==", "words", "[", "0", "]", "and", "correction", "in", "' '", ".", "join", "(", "d", "[", "i", "+", "1", "]", "[", "1", "]", ")", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "", "return", "False", "\n", "\n", "\n", "", "def", "get_tok_labels", "(", "s_diff", ")", ":", "\n", "    ", "tok_labels", "=", "[", "]", "\n", "for", "tag", ",", "chunk", "in", "s_diff", ":", "\n", "        ", "if", "tag", "==", "'='", ":", "\n", "            ", "tok_labels", "+=", "[", "0", "]", "*", "len", "(", "chunk", ")", "\n", "", "elif", "tag", "==", "'-'", ":", "\n", "            ", "tok_labels", "+=", "[", "1", "]", "*", "len", "(", "chunk", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "return", "tok_labels", "\n", "\n", "\n", "", "def", "should_keep", "(", "prev_raw", ",", "prev_tok", ",", "post_raw", ",", "post_tok", ",", "bleu", ",", "rev_id", ")", ":", "\n", "    ", "global", "CTR_LOW_BLEU", "\n", "global", "CTR_LOW_LEVEN", "\n", "global", "CTR_TOO_MANY_1_TOKS", "\n", "global", "CTR_SPELLING", "\n", "global", "CTR_CHEMISTRY", "\n", "global", "CTR_ONLY_PUNC_CHANGED", "\n", "\n", "# KEEP -- exact match", "\n", "if", "bleu", "==", "100", "or", "prev_raw", "==", "post_raw", ":", "\n", "        ", "return", "True", ",", "None", ",", "[", "0", "for", "_", "in", "range", "(", "len", "(", "prev_tok", ".", "split", "(", ")", ")", ")", "]", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.get_tok_labels": [[235, 248], ["simplediff.diff", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff"], ["", "if", "bleu", "<", "15.0", ":", "\n", "        ", "CTR_LOW_BLEU", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "# too close", "\n", "", "if", "Levenshtein", ".", "distance", "(", "prev_tok", ",", "post_tok", ")", "<", "4", ":", "\n", "        ", "CTR_LOW_LEVEN", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "", "tok_diff", "=", "diff", "(", "prev_tok", ".", "split", "(", ")", ",", "post_tok", ".", "split", "(", ")", ")", "\n", "tok_labels", "=", "get_tok_labels", "(", "tok_diff", ")", "\n", "assert", "len", "(", "tok_labels", ")", "==", "len", "(", "prev_tok", ".", "split", "(", ")", ")", "\n", "\n", "changed_text", "=", "''", ".", "join", "(", "[", "''", ".", "join", "(", "chunk", ")", "for", "tag", ",", "chunk", "in", "tok_diff", "if", "tag", "!=", "'='", "]", ")", "\n", "if", "not", "re", ".", "search", "(", "'[a-z]'", ",", "changed_text", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize": [[249, 253], ["TOKENIZER.tokenize", "s.strip"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize"], ["        ", "CTR_ONLY_PUNC_CHANGED", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# too dissimilar -- less than half of toks shared", "\n", "", "tok_nums", "=", "[", "int", "(", "x", ")", "for", "x", "in", "tok_labels", "]", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.extract_sents": [[254, 288], ["nltk.sent_tokenize", "nltk.sent_tokenize", "gen_data_from_crawl.find_matches", "gen_data_from_crawl.tokenize", "gen_data_from_crawl.tokenize", "gen_data_from_crawl.get_tok_labels", "gen_data_from_crawl.should_filter", "s.lower", "s.lower", "cur_prev.strip().split", "cur_post.strip().split", "cur_prev.strip", "cur_post.strip", "range", "len", "cur_prev.split"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.find_matches", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.tokenize", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.get_tok_labels", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.should_filter"], ["if", "(", "sum", "(", "tok_nums", ")", "*", "1.0", "/", "len", "(", "tok_nums", ")", ")", ">", "0.5", ":", "\n", "        ", "CTR_TOO_MANY_1_TOKS", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# edit was just fixing a spelling error", "\n", "", "word_diff", "=", "diff", "(", "word_tokenize", "(", "prev_raw", ")", ",", "word_tokenize", "(", "post_raw", ")", ")", "\n", "if", "is_spelling_diff", "(", "word_diff", ")", ":", "\n", "        ", "CTR_SPELLING", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# some simple filtering to get out the chemistry \"neutral\" edits", "\n", "", "if", "' molecules'", "in", "prev_raw", "or", "' ions'", "in", "prev_raw", "or", "' ionic'", "in", "prev_raw", "or", "' atoms'", "in", "prev_raw", ":", "\n", "        ", "CTR_CHEMISTRY", "+=", "1", "\n", "return", "False", ",", "None", ",", "None", "\n", "\n", "# # use enchant to make sure example has enough normal words", "\n", "# prev_words = prev_words.translate(str.maketrans('', '', string.punctuation)).split()", "\n", "# n_words = sum(1 if d.check(w) else 0 for w in pre_words)", "\n", "# if len(prev_words) == 0 or (float(n_words) / len(prev_words)) < 0.5:", "\n", "#     return False, None, None", "\n", "\n", "\n", "# see if this is a \"single word\" edit, where a single word was replaced with 0+ words", "\n", "", "def", "is_single_word_edit", "(", "d", ")", ":", "\n", "        ", "\"\"\" is this diff good for the final generation dataset \"\"\"", "\n", "pre_chunks", "=", "[", "chunk", "for", "tag", ",", "chunk", "in", "d", "if", "tag", "==", "'-'", "]", "\n", "post_chunks", "=", "[", "chunk", "for", "tag", ",", "chunk", "in", "d", "if", "tag", "==", "'+'", "]", "\n", "# a single word on the pre side", "\n", "if", "sum", "(", "[", "len", "(", "chunk", ")", "for", "chunk", "in", "pre_chunks", "]", ")", "!=", "1", ":", "\n", "            ", "return", "False", "\n", "# 0 words in the post", "\n", "", "if", "len", "(", "post_chunks", ")", "==", "0", ":", "\n", "            ", "return", "True", "\n", "# ensure 1 post chunk", "\n", "", "if", "len", "(", "post_chunks", ")", ">", "1", ":", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.extract_examples": [[291, 328], ["tqdm.tqdm", "isinstance", "isinstance", "gen_data_from_crawl.clean_wikitext", "gen_data_from_crawl.clean_wikitext", "gen_data_from_crawl.extract_sents", "x.decode", "x.decode", "len", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.clean_wikitext", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.clean_wikitext", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.extract_sents"], ["", "prei", "=", "next", "(", "(", "i", "for", "i", ",", "x", "in", "enumerate", "(", "d", ")", "if", "x", "[", "0", "]", "==", "'-'", ")", ")", "\n", "if", "prei", "<", "len", "(", "d", ")", "-", "1", "and", "d", "[", "prei", "+", "1", "]", "[", "0", "]", "==", "'+'", ":", "\n", "            ", "return", "True", "\n", "", "", "single_word_edit", "=", "is_single_word_edit", "(", "word_diff", ")", "\n", "\n", "return", "True", ",", "single_word_edit", ",", "tok_labels", "\n", "\n", "\n", "\n", "\n", "", "def", "sent_generator", "(", "revisions", ")", ":", "\n", "    ", "global", "CTR_EMPTY_REV", "\n", "global", "CTR_MULTIPLE_EDITS", "\n", "global", "CTR_FAILED_CLEANING", "\n", "global", "CTR_DUPS", "\n", "global", "CTR_INVALID_NUM_CHANGED_SENTS", "\n", "global", "CTR_NON_EDIT_CHUNKS", "\n", "global", "CTR_EDIT_CHANGED_NUM_SENTS", "\n", "global", "CTR_FAILED_TAGGING", "\n", "\n", "for", "rev_id", "in", "tqdm", "(", "revisions", ")", ":", "\n", "        ", "prevs", ",", "posts", ",", "prev_deleted", ",", "posts_added", "=", "revisions", "[", "rev_id", "]", "\n", "\n", "# empty revision", "\n", "if", "not", "prevs", "or", "not", "posts", ":", "\n", "            ", "CTR_EMPTY_REV", "+=", "1", "\n", "continue", "\n", "\n", "", "if", "prev_deleted", "!=", "[", "'no_deleted_chunks'", "]", "or", "posts_added", "!=", "[", "'no_added_chunks'", "]", ":", "\n", "            ", "CTR_NON_EDIT_CHUNKS", "+=", "1", "\n", "continue", "\n", "\n", "# unicode dat shit", "\n", "", "if", "isinstance", "(", "prevs", "[", "0", "]", ",", "bytes", ")", ":", "\n", "            ", "prevs", "=", "[", "x", ".", "decode", "(", ")", "for", "x", "in", "prevs", "]", "\n", "", "if", "isinstance", "(", "posts", "[", "0", "]", ",", "bytes", ")", ":", "\n", "            ", "posts", "=", "[", "x", ".", "decode", "(", ")", "for", "x", "in", "posts", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_data_from_crawl.is_single_word_diff": [[330, 336], ["nltk.word_tokenize", "nltk.word_tokenize", "simplediff.diff", "nltk.word_tokenize.lower().strip", "nltk.word_tokenize.lower().strip", "sum", "nltk.word_tokenize.lower", "nltk.word_tokenize.lower", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.DEPRECIATED.gen_parallel_corpus.diff"], ["", "if", "len", "(", "prevs", ")", ">", "1", "or", "len", "(", "posts", ")", ">", "1", ":", "\n", "            ", "CTR_MULTIPLE_EDITS", "+=", "1", "\n", "continue", "\n", "\n", "", "prev_text", "=", "clean_wikitext", "(", "prevs", ")", ".", "lower", "(", ")", "\n", "post_text", "=", "clean_wikitext", "(", "posts", ")", ".", "lower", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.print_withcolor": [[33, 53], ["l.replace.replace", "re.compile", "re.compile", "enumerate", "print", "re.finditer", "m.end", "m.group", "str", "m.start"], "function", ["None"], ["", "def", "print_withcolor", "(", "idx", ",", "l", ")", ":", "\n", "    ", "l", "=", "l", ".", "replace", "(", "'\\n'", ",", "''", ")", "\n", "ins_p", "=", "re", ".", "compile", "(", "r'<ins.*?>(.*?)</ins>'", ",", "re", ".", "DOTALL", ")", "\n", "del_p", "=", "re", ".", "compile", "(", "r'<del.*?>(.*?)</del>'", ",", "re", ".", "DOTALL", ")", "\n", "patterns", "=", "[", "ins_p", ",", "del_p", "]", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "patterns", ")", ":", "\n", "        ", "match", "=", "re", ".", "finditer", "(", "p", ",", "l", ")", "\n", "if", "match", ":", "\n", "            ", "new_l", "=", "\"\"", "\n", "last", "=", "0", "\n", "for", "m", "in", "match", ":", "\n", "                ", "if", "i", "==", "1", ":", "\n", "                    ", "color", "=", "bcolors", ".", "OKBLUE", "\n", "", "else", ":", "\n", "                    ", "color", "=", "bcolors", ".", "OKGREEN", "\n", "", "new_l", "=", "new_l", "+", "l", "[", "last", ":", "m", ".", "start", "(", "1", ")", "]", "+", "color", "+", "m", ".", "group", "(", "1", ")", "+", "bcolors", ".", "ENDC", "\n", "last", "=", "m", ".", "end", "(", "1", ")", "\n", "", "new_l", "=", "new_l", "+", "l", "[", "last", ":", "]", "\n", "l", "=", "new_l", "\n", "", "", "print", "(", "bcolors", ".", "HEADER", "+", "'line '", "+", "str", "(", "idx", "+", "1", ")", "+", "':'", "+", "bcolors", ".", "ENDC", "+", "l", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.html2diff": [[58, 94], ["BeautifulSoup", "BeautifulSoup.find_all", "re.compile", "range", "len", "re.compile", "len", "re.match", "node_next.div.prettify", "next_added.append", "re.match", "re.match", "re.match", "re.match.group().strip", "node_prev.div.prettify", "prev_deleted.append", "node_prev.div.prettify", "node_next.div.prettify", "prev_changed.append", "next_changed.append", "re.match.group().strip", "re.match.group().strip", "re.match.group().strip", "re.match.group", "re.match.group", "re.match.group", "re.match.group"], "function", ["None"], ["", "def", "html2diff", "(", "html", ")", ":", "\n", "    ", "prev_changed", ",", "next_changed", "=", "[", "]", ",", "[", "]", "\n", "prev_deleted", ",", "next_added", "=", "[", "]", ",", "[", "]", "\n", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "'html'", ")", "\n", "nodes", "=", "soup", ".", "find_all", "(", "class_", "=", "re", ".", "compile", "(", "r'(diff-deletedline)|(diff-addedline)|(diff-empty)'", ")", ")", "\n", "div_p", "=", "re", ".", "compile", "(", "r'<div.*?>(.*)</div>'", ",", "re", ".", "DOTALL", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "nodes", ")", ",", "2", ")", ":", "\n", "# skip straddeling cases", "\n", "        ", "if", "i", "+", "1", ">=", "len", "(", "nodes", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "node_prev", "=", "nodes", "[", "i", "]", "\n", "node_next", "=", "nodes", "[", "i", "+", "1", "]", "\n", "\n", "# seperate  revisions into chunks that were modified,", "\n", "# chunks that were purely deleted and chunks that were purely added", "\n", "if", "not", "node_prev", ".", "div", "and", "not", "node_next", ".", "div", ":", "\n", "            ", "continue", "\n", "", "elif", "not", "node_prev", ".", "div", ":", "\n", "            ", "next_match", "=", "re", ".", "match", "(", "div_p", ",", "node_next", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "if", "next_match", ":", "\n", "                ", "next_added", ".", "append", "(", "next_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "", "", "elif", "not", "node_next", ".", "div", ":", "\n", "            ", "prev_match", "=", "re", ".", "match", "(", "div_p", ",", "node_prev", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "if", "prev_match", ":", "\n", "                ", "prev_deleted", ".", "append", "(", "prev_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "prev_match", "=", "re", ".", "match", "(", "div_p", ",", "node_prev", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "next_match", "=", "re", ".", "match", "(", "div_p", ",", "node_next", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "if", "prev_match", "and", "next_match", ":", "\n", "                ", "prev_changed", ".", "append", "(", "prev_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "next_changed", ".", "append", "(", "next_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "\n", "", "", "", "return", "prev_changed", ",", "next_changed", ",", "prev_deleted", ",", "next_added", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.url2diff": [[96, 104], ["urlopen", "urlopen.read", "util.html2diff", "print"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.util.html2diff"], ["", "def", "url2diff", "(", "url", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "response", "=", "urlopen", "(", "url", ")", "\n", "html", "=", "response", ".", "read", "(", ")", "\n", "return", "html2diff", "(", "html", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "print", "(", "e", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "return", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.wiki_text_clean": [[106, 110], ["text.replace().replace.replace().replace", "text.replace().replace.replace"], "function", ["None"], ["", "", "def", "wiki_text_clean", "(", "text", ")", ":", "\n", "    ", "text", "=", "''", ".", "join", "(", "[", "x", "for", "x", "in", "text", "if", "x", "in", "string", ".", "printable", "]", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.gen_revisions": [[111, 143], ["len", "tqdm.tqdm", "print", "print", "util.url2diff", "zip", "str", "len", "len", "print", "prevs.append", "nexts.append", "gain_wiki_revision_text.wiki_text_clean", "gain_wiki_revision_text.wiki_text_clean", "print", "str", "gain_wiki_revision_text.wiki_text_clean", "gain_wiki_revision_text.wiki_text_clean", "len", "len"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.util.url2diff", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.wiki_text_clean", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.wiki_text_clean", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.wiki_text_clean", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.wiki_text_clean"], ["", "def", "gen_revisions", "(", "rev_ids", ")", ":", "\n", "    ", "rev_size", "=", "len", "(", "rev_ids", ")", "\n", "success", "=", "0", "\n", "out", "=", "{", "}", "\n", "\n", "for", "rev_id", "in", "tqdm", "(", "rev_ids", ")", ":", "\n", "        ", "print", "(", "'processing revision id = '", "+", "str", "(", "rev_id", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "url", "=", "'https://en.wikipedia.org/wiki/?diff='", "+", "str", "(", "rev_id", ")", "\n", "prevs_", ",", "nexts_", ",", "prev_deleted", ",", "next_added", "=", "url2diff", "(", "url", ")", "\n", "\n", "if", "len", "(", "prevs_", ")", "!=", "len", "(", "nexts_", ")", ":", "\n", "            ", "print", "(", "'ERROR: corpus sizes not equal!'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "continue", "\n", "\n", "", "prevs", ",", "nexts", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "pre", ",", "post", "in", "zip", "(", "prevs_", ",", "nexts_", ")", ":", "\n", "            ", "prevs", ".", "append", "(", "wiki_text_clean", "(", "pre", ")", ")", "\n", "nexts", ".", "append", "(", "wiki_text_clean", "(", "post", ")", ")", "\n", "", "prevs_deleted", "=", "[", "wiki_text_clean", "(", "pre", ")", "for", "pre", "in", "(", "prev_deleted", "or", "[", "'no_deleted_chunks'", "]", ")", "]", "\n", "nexts_added", "=", "[", "wiki_text_clean", "(", "nxt", ")", "for", "nxt", "in", "(", "next_added", "or", "[", "'no_added_chunks'", "]", ")", "]", "\n", "\n", "\n", "if", "len", "(", "prevs", ")", ">", "0", "and", "len", "(", "nexts", ")", ">", "0", ":", "\n", "            ", "print", "(", "'...success!'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "success", "+=", "1", "\n", "yield", "rev_id", ",", "prevs", ",", "nexts", ",", "prevs_deleted", ",", "nexts_added", "\n", "\n", "", "", "print", "(", "'failures: '", ",", "rev_size", "-", "success", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.go": [[145, 156], ["gain_wiki_revision_text.gen_revisions", "open", "print", "l.split"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.gain_wiki_revision_text.gen_revisions"], ["", "def", "go", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "rev_ids", "=", "[", "l", ".", "split", "(", "'\\t'", ")", "[", "0", "]", "for", "l", "in", "f", "]", "\n", "\n", "", "for", "rev_id", ",", "prevs", ",", "nexts", ",", "prev_deleted", ",", "next_added", "in", "gen_revisions", "(", "rev_ids", ")", ":", "\n", "        ", "print", "(", "'\\t'", ".", "join", "(", "[", "\n", "rev_id", ",", "\n", "'<EDIT-DELIM>'", ".", "join", "(", "prevs", ")", ",", "\n", "'<EDIT-DELIM>'", ".", "join", "(", "nexts", ")", ",", "\n", "'<EDIT-DELIM>'", ".", "join", "(", "prev_deleted", ")", ",", "\n", "'<EDIT-DELIM>'", ".", "join", "(", "next_added", ")", "\n", "]", ")", ")", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.util.html2diff": [[17, 42], ["bs4.BeautifulSoup", "bs4.BeautifulSoup.find_all", "re.compile", "range", "len", "re.match", "re.match", "re.compile", "len", "node_prev.div.prettify", "node_next.div.prettify", "prev_doc.append", "next_doc.append", "re.match.group().strip", "re.match.group().strip", "re.match.group", "re.match.group"], "function", ["None"], ["", "def", "html2diff", "(", "html", ")", ":", "\n", "    ", "prev_doc", ",", "next_doc", "=", "[", "]", ",", "[", "]", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "'html'", ")", "\n", "nodes", "=", "soup", ".", "find_all", "(", "class_", "=", "re", ".", "compile", "(", "r'(diff-deletedline)|(diff-addedline)|(diff-empty)'", ")", ")", "\n", "div_p", "=", "re", ".", "compile", "(", "r'<div.*?>(.*)</div>'", ",", "re", ".", "DOTALL", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "nodes", ")", ",", "2", ")", ":", "\n", "# skip straddeling cases", "\n", "        ", "if", "i", "+", "1", ">=", "len", "(", "nodes", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "node_prev", "=", "nodes", "[", "i", "]", "\n", "node_next", "=", "nodes", "[", "i", "+", "1", "]", "\n", "\n", "if", "not", "node_prev", ".", "div", "or", "not", "node_next", ".", "div", ":", "\n", "            ", "continue", "\n", "\n", "", "prev_match", "=", "re", ".", "match", "(", "div_p", ",", "node_prev", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "next_match", "=", "re", ".", "match", "(", "div_p", ",", "node_next", ".", "div", ".", "prettify", "(", "formatter", "=", "None", ")", ")", "\n", "\n", "if", "prev_match", "and", "next_match", ":", "\n", "            ", "prev_doc", ".", "append", "(", "prev_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "next_doc", ".", "append", "(", "next_match", ".", "group", "(", "1", ")", ".", "strip", "(", ")", ")", "\n", "\n", "", "", "return", "prev_doc", ",", "next_doc", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.util.url2diff": [[44, 52], ["urllib.request.urlopen", "urllib.request.urlopen.read", "util.html2diff", "print"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.util.html2diff"], ["", "def", "url2diff", "(", "url", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "response", "=", "urlopen", "(", "url", ")", "\n", "html", "=", "response", ".", "read", "(", ")", "\n", "return", "html2diff", "(", "html", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "print", "(", "e", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "return", "[", "]", ",", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.wiki_crawl.util.print_withcolor": [[53, 73], ["l.replace.replace", "re.compile", "re.compile", "enumerate", "print", "re.finditer", "m.end", "m.group", "str", "m.start"], "function", ["None"], ["", "", "def", "print_withcolor", "(", "idx", ",", "l", ")", ":", "\n", "    ", "l", "=", "l", ".", "replace", "(", "'\\n'", ",", "''", ")", "\n", "ins_p", "=", "re", ".", "compile", "(", "r'<ins.*?>(.*?)</ins>'", ",", "re", ".", "DOTALL", ")", "\n", "del_p", "=", "re", ".", "compile", "(", "r'<del.*?>(.*?)</del>'", ",", "re", ".", "DOTALL", ")", "\n", "patterns", "=", "[", "ins_p", ",", "del_p", "]", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "patterns", ")", ":", "\n", "        ", "match", "=", "re", ".", "finditer", "(", "p", ",", "l", ")", "\n", "if", "match", ":", "\n", "            ", "new_l", "=", "\"\"", "\n", "last", "=", "0", "\n", "for", "m", "in", "match", ":", "\n", "                ", "if", "i", "==", "1", ":", "\n", "                    ", "color", "=", "bcolors", ".", "OKBLUE", "\n", "", "else", ":", "\n", "                    ", "color", "=", "bcolors", ".", "OKGREEN", "\n", "", "new_l", "=", "new_l", "+", "l", "[", "last", ":", "m", ".", "start", "(", "1", ")", "]", "+", "color", "+", "m", ".", "group", "(", "1", ")", "+", "bcolors", ".", "ENDC", "\n", "last", "=", "m", ".", "end", "(", "1", ")", "\n", "", "new_l", "=", "new_l", "+", "l", "[", "last", ":", "]", "\n", "l", "=", "new_l", "\n", "", "", "print", "(", "bcolors", ".", "HEADER", "+", "'line '", "+", "str", "(", "idx", "+", "1", ")", "+", "':'", "+", "bcolors", ".", "ENDC", "+", "l", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.interpretation.mutual_information.mi": [[69, 81], ["math.log", "math.log", "math.log", "math.log"], "function", ["None"], ["", "", "", "", "def", "mi", "(", "n00", ",", "n01", ",", "n10", ",", "n11", ")", ":", "\n", "    ", "n0_", "=", "n00", "+", "n01", "# docs without term", "\n", "n1_", "=", "n11", "+", "n10", "# docs with term", "\n", "n_0", "=", "n10", "+", "n00", "# docs with 0 label", "\n", "n_1", "=", "n11", "+", "n01", "# docs with 1 label", "\n", "n", "=", "n00", "+", "n01", "+", "n11", "+", "n10", "# total n    ", "\n", "\n", "mutual_info", "=", "(", "n11", "/", "n", ")", "*", "math", ".", "log", "(", "(", "n", "*", "n11", ")", "/", "(", "n1_", "*", "n_1", ")", ")", "+", "(", "n01", "/", "n", ")", "*", "math", ".", "log", "(", "(", "n", "*", "n01", ")", "/", "(", "n0_", "*", "n_1", ")", ")", "+", "(", "n10", "/", "n", ")", "*", "math", ".", "log", "(", "(", "n", "*", "n10", ")", "/", "(", "n1_", "*", "n_0", ")", ")", "+", "(", "n00", "/", "n", ")", "*", "math", ".", "log", "(", "(", "n", "*", "n00", ")", "/", "(", "n0_", "*", "n_0", ")", ")", "\n", "return", "mutual_info", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.interpretation.feature_importance.importance_scores": [[21, 62], ["torch.load", "torch.load.items", "print", "print", "print", "tagging.features.Featurizer", "tagging.features.Featurizer.get_feature_names", "len", "state_dict[].numpy", "state_dict[].numpy", "out_matrix[].transpose", "collections.defaultdict", "enumerate", "list", "numpy.mean", "numpy.std", "range", "matrix.numpy().flatten", "collections.defaultdict", "range", "matrix.numpy", "feature_importance.importance_scores.relu"], "function", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.load", "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.tagging.features.Featurizer.get_feature_names"], ["def", "importance_scores", "(", "ckpt_path", ")", ":", "\n", "\n", "\t", "state_dict", "=", "torch", ".", "load", "(", "ckpt_path", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "l", "=", "[", "]", "\n", "for", "key", ",", "matrix", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "\t\t", "l", "+=", "list", "(", "matrix", ".", "numpy", "(", ")", ".", "flatten", "(", ")", ")", "\n", "\n", "", "print", "(", "'mean: '", ",", "np", ".", "mean", "(", "l", ")", ")", "\n", "print", "(", "'std: '", ",", "np", ".", "std", "(", "l", ")", ")", "\n", "print", "(", ")", "\n", "\n", "featurizer", "=", "Featurizer", "(", ")", "\n", "feature_names", "=", "featurizer", ".", "get_feature_names", "(", ")", "\n", "num_feats", "=", "len", "(", "feature_names", ")", "\n", "\n", "in_matrix", "=", "state_dict", "[", "'tagging_model.tok_classifier.enricher.0.weight'", "]", ".", "numpy", "(", ")", "\n", "\n", "out_matrix", "=", "state_dict", "[", "'tagging_model.tok_classifier.out.0.weight'", "]", ".", "numpy", "(", ")", "\n", "# only look at rows the features are multiplied by", "\n", "out_matrix", "=", "out_matrix", "[", ":", ",", "-", "num_feats", ":", "]", ".", "transpose", "(", ")", "\n", "\n", "def", "relu", "(", "x", ")", ":", "\n", "\t    ", "return", "x", "if", "x", ">=", "0", "else", "0", "\n", "\n", "", "scores", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "int", ")", ")", "\n", "\n", "\n", "for", "feature_i", ",", "feature", "in", "enumerate", "(", "feature_names", ")", ":", "\n", "\t    ", "for", "hidden_j", "in", "range", "(", "90", ")", ":", "\n", "\t        ", "in_weight", "=", "in_matrix", "[", "feature_i", "]", "[", "hidden_j", "]", "\n", "\n", "for", "out_k", "in", "range", "(", "2", ")", ":", "\n", "\t            ", "out_weight", "=", "out_matrix", "[", "hidden_j", "]", "[", "out_k", "]", "\n", "\n", "scores", "[", "feature", "]", "[", "out_k", "]", "+=", "relu", "(", "in_weight", ")", "*", "out_weight", "\n", "\n", "\n", "", "", "", "positive_scores", "=", "{", "feature", ":", "scores", "[", "feature", "]", "[", "1", "]", "for", "feature", "in", "feature_names", "}", "\n", "\n", "return", "positive_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.corpusfile_to_mturk.detokenize": [[34, 42], ["s.split", "w.startswith", "out.append", "len"], "function", ["None"], ["def", "detokenize", "(", "s", ")", ":", "\n", "    ", "out", "=", "[", "]", "\n", "for", "w", "in", "s", ".", "split", "(", ")", ":", "\n", "        ", "if", "w", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "out", ")", ">", "0", ":", "\n", "            ", "out", "[", "-", "1", "]", "+=", "w", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "out", ".", "append", "(", "w", ")", "\n", "", "", "return", "' '", ".", "join", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.__init__": [[16, 25], ["mturk_to_datafile.CorpusSearcher.vectorizer.fit", "mturk_to_datafile.CorpusSearcher.vectorizer.transform"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.models.TextClassifier.fit"], ["    ", "def", "__init__", "(", "self", ",", "key_corpus", ",", "value_corpus", ",", "vectorizer", ")", ":", "\n", "        ", "self", ".", "vectorizer", "=", "vectorizer", "\n", "self", ".", "vectorizer", ".", "fit", "(", "key_corpus", ")", "\n", "\n", "self", ".", "key_corpus", "=", "key_corpus", "\n", "self", ".", "value_corpus", "=", "value_corpus", "\n", "\n", "# rows = docs, cols = features", "\n", "self", ".", "key_corpus_matrix", "=", "self", ".", "vectorizer", ".", "transform", "(", "key_corpus", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.mturk_to_datafile.CorpusSearcher.most_similar": [[27, 41], ["mturk_to_datafile.CorpusSearcher.vectorizer.transform", "numpy.dot", "numpy.squeeze", "zip", "numpy.squeeze.toarray", "range", "sorted", "len"], "methods", ["home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.baselines.ops.BilinearAttention.dot"], ["", "def", "most_similar", "(", "self", ",", "query", ",", "n", "=", "10", ")", ":", "\n", "        ", "query_vec", "=", "self", ".", "vectorizer", ".", "transform", "(", "[", "query", "]", ")", "\n", "\n", "scores", "=", "np", ".", "dot", "(", "self", ".", "key_corpus_matrix", ",", "query_vec", ".", "T", ")", "\n", "scores", "=", "np", ".", "squeeze", "(", "scores", ".", "toarray", "(", ")", ")", "\n", "scores_indices", "=", "zip", "(", "scores", ",", "range", "(", "len", "(", "scores", ")", ")", ")", "\n", "selected", "=", "sorted", "(", "scores_indices", ",", "reverse", "=", "True", ")", "[", ":", "n", "]", "\n", "# use the retrieved i to pick examples from the VALUE corpus", "\n", "selected", "=", "[", "\n", "(", "query", ",", "self", ".", "key_corpus", "[", "i", "]", ",", "self", ".", "value_corpus", "[", "i", "]", ",", "i", ",", "score", ")", "\n", "for", "(", "score", ",", "i", ")", "in", "selected", "\n", "]", "\n", "\n", "return", "selected", "\n", "\n"]], "home.repos.pwc.inspect_result.rpryzant_neutralizing-bias.statement_quality_scripts.resultsfile_to_mturk.detokenize": [[20, 30], ["str", "str.split", "w.startswith", "out.append", "len"], "function", ["None"], ["def", "detokenize", "(", "s", ")", ":", "\n", "    ", "s", "=", "str", "(", "s", ")", "\n", "out", "=", "[", "]", "\n", "for", "w", "in", "s", ".", "split", "(", ")", ":", "\n", "        ", "if", "w", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "out", ")", ">", "0", ":", "\n", "            ", "out", "[", "-", "1", "]", "+=", "w", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "out", ".", "append", "(", "w", ")", "\n", "\n", "", "", "return", "' '", ".", "join", "(", "out", ")", "\n", "\n"]]}