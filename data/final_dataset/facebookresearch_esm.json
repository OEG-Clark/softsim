{"home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.read_msa": [[19, 28], ["str", "itertools.islice", "seq.upper", "Bio.SeqIO.parse"], "function", ["None"], ["def", "read_msa", "(", "filename", ":", "str", ",", "nseq", ":", "int", ")", "->", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", ":", "\n", "    ", "\"\"\" Reads the first nseq sequences from an MSA file, automatically removes insertions.\"\"\"", "\n", "\n", "msa", "=", "[", "\n", "(", "record", ".", "description", ",", "str", "(", "record", ".", "seq", ")", ")", "\n", "for", "record", "in", "itertools", ".", "islice", "(", "SeqIO", ".", "parse", "(", "filename", ",", "\"fasta\"", ")", ",", "nseq", ")", "\n", "]", "\n", "msa", "=", "[", "(", "desc", ",", "seq", ".", "upper", "(", ")", ")", "for", "desc", ",", "seq", "in", "msa", "]", "\n", "return", "msa", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.create_parser": [[30, 90], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["", "def", "create_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Label a deep mutational scan with predictions from an ensemble of ESM-1v models.\"", "# noqa", "\n", ")", "\n", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "\n", "\"--model-location\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"PyTorch model file OR name of pretrained model to download (see README for models)\"", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--sequence\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Base sequence to which mutations were applied\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dms-input\"", ",", "\n", "type", "=", "pathlib", ".", "Path", ",", "\n", "help", "=", "\"CSV file containing the deep mutational scan\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--mutation-col\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"mutant\"", ",", "\n", "help", "=", "\"column in the deep mutational scan labeling the mutation as 'AiB'\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dms-output\"", ",", "\n", "type", "=", "pathlib", ".", "Path", ",", "\n", "help", "=", "\"Output file containing the deep mutational scan along with predictions\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--offset-idx\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "0", ",", "\n", "help", "=", "\"Offset of the mutation positions in `--mutation-col`\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--scoring-strategy\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"wt-marginals\"", ",", "\n", "choices", "=", "[", "\"wt-marginals\"", ",", "\"pseudo-ppl\"", ",", "\"masked-marginals\"", "]", ",", "\n", "help", "=", "\"\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--msa-path\"", ",", "\n", "type", "=", "pathlib", ".", "Path", ",", "\n", "help", "=", "\"path to MSA (required for MSA Transformer)\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--msa-samples\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "400", ",", "\n", "help", "=", "\"number of sequences to randomly sample from the MSA\"", "\n", ")", "\n", "# fmt: on", "\n", "parser", ".", "add_argument", "(", "\"--nogpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Do not use GPU even if available\"", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.label_row": [[92, 101], ["score.item", "alphabet.get_idx", "alphabet.get_idx", "int"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx"], ["", "def", "label_row", "(", "row", ",", "sequence", ",", "token_probs", ",", "alphabet", ",", "offset_idx", ")", ":", "\n", "    ", "wt", ",", "idx", ",", "mt", "=", "row", "[", "0", "]", ",", "int", "(", "row", "[", "1", ":", "-", "1", "]", ")", "-", "offset_idx", ",", "row", "[", "-", "1", "]", "\n", "assert", "sequence", "[", "idx", "]", "==", "wt", ",", "\"The listed wildtype does not match the provided sequence\"", "\n", "\n", "wt_encoded", ",", "mt_encoded", "=", "alphabet", ".", "get_idx", "(", "wt", ")", ",", "alphabet", ".", "get_idx", "(", "mt", ")", "\n", "\n", "# add 1 for BOS", "\n", "score", "=", "token_probs", "[", "0", ",", "1", "+", "idx", ",", "mt_encoded", "]", "-", "token_probs", "[", "0", ",", "1", "+", "idx", ",", "wt_encoded", "]", "\n", "return", "score", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.compute_pppl": [[103, 130], ["alphabet.get_batch_converter", "alphabet.get_batch_converter.", "range", "sum", "alphabet.get_idx", "alphabet.get_idx", "batch_tokens.clone", "log_probs.append", "int", "len", "torch.no_grad", "torch.log_softmax", "token_probs[].item", "model", "batch_tokens.clone.cuda", "alphabet.get_idx"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_batch_converter", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx"], ["", "def", "compute_pppl", "(", "row", ",", "sequence", ",", "model", ",", "alphabet", ",", "offset_idx", ")", ":", "\n", "    ", "wt", ",", "idx", ",", "mt", "=", "row", "[", "0", "]", ",", "int", "(", "row", "[", "1", ":", "-", "1", "]", ")", "-", "offset_idx", ",", "row", "[", "-", "1", "]", "\n", "assert", "sequence", "[", "idx", "]", "==", "wt", ",", "\"The listed wildtype does not match the provided sequence\"", "\n", "\n", "# modify the sequence", "\n", "sequence", "=", "sequence", "[", ":", "idx", "]", "+", "mt", "+", "sequence", "[", "(", "idx", "+", "1", ")", ":", "]", "\n", "\n", "# encode the sequence", "\n", "data", "=", "[", "\n", "(", "\"protein1\"", ",", "sequence", ")", ",", "\n", "]", "\n", "\n", "batch_converter", "=", "alphabet", ".", "get_batch_converter", "(", ")", "\n", "\n", "batch_labels", ",", "batch_strs", ",", "batch_tokens", "=", "batch_converter", "(", "data", ")", "\n", "\n", "wt_encoded", ",", "mt_encoded", "=", "alphabet", ".", "get_idx", "(", "wt", ")", ",", "alphabet", ".", "get_idx", "(", "mt", ")", "\n", "\n", "# compute probabilities at each position", "\n", "log_probs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "sequence", ")", "-", "1", ")", ":", "\n", "        ", "batch_tokens_masked", "=", "batch_tokens", ".", "clone", "(", ")", "\n", "batch_tokens_masked", "[", "0", ",", "i", "]", "=", "alphabet", ".", "mask_idx", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "token_probs", "=", "torch", ".", "log_softmax", "(", "model", "(", "batch_tokens_masked", ".", "cuda", "(", ")", ")", "[", "\"logits\"", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "log_probs", ".", "append", "(", "token_probs", "[", "0", ",", "i", ",", "alphabet", ".", "get_idx", "(", "sequence", "[", "i", "]", ")", "]", ".", "item", "(", ")", ")", "# vocab size", "\n", "", "return", "sum", "(", "log_probs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.main": [[132, 221], ["pandas.read_csv", "pd.read_csv.to_csv", "esm.pretrained.load_model_and_alphabet", "model.cuda.eval", "alphabet.get_batch_converter", "isinstance", "torch.cuda.is_available", "model.cuda.cuda", "print", "alphabet.get_batch_converter.", "tqdm.tqdm", "torch.cat().unsqueeze", "pd.read_csv.apply", "alphabet.get_batch_converter.", "predict.read_msa", "range", "batch_tokens.clone", "all_token_probs.append", "pd.read_csv.apply", "batch_tokens.size", "torch.no_grad", "torch.log_softmax", "torch.cat", "predict.label_row", "torch.no_grad", "torch.log_softmax", "tqdm.tqdm", "torch.cat().unsqueeze", "pd.read_csv.apply", "predict.label_row", "range", "batch_tokens.clone", "all_token_probs.append", "tqdm.tqdm.pandas", "pd.read_csv.progress_apply", "model.cuda.", "model.cuda.", "batch_tokens.size", "torch.no_grad", "torch.log_softmax", "torch.cat", "predict.label_row", "batch_tokens.clone.cuda", "batch_tokens.cuda", "predict.compute_pppl", "model.cuda.", "batch_tokens.clone.cuda"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_batch_converter", "home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.read_msa", "home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.label_row", "home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.label_row", "home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.label_row", "home.repos.pwc.inspect_result.facebookresearch_esm.variant-prediction.predict.compute_pppl"], ["", "def", "main", "(", "args", ")", ":", "\n", "# Load the deep mutational scan", "\n", "    ", "df", "=", "pd", ".", "read_csv", "(", "args", ".", "dms_input", ")", "\n", "\n", "# inference for each model", "\n", "for", "model_location", "in", "args", ".", "model_location", ":", "\n", "        ", "model", ",", "alphabet", "=", "pretrained", ".", "load_model_and_alphabet", "(", "model_location", ")", "\n", "model", ".", "eval", "(", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "nogpu", ":", "\n", "            ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "print", "(", "\"Transferred model to GPU\"", ")", "\n", "\n", "", "batch_converter", "=", "alphabet", ".", "get_batch_converter", "(", ")", "\n", "\n", "if", "isinstance", "(", "model", ",", "MSATransformer", ")", ":", "\n", "            ", "data", "=", "[", "read_msa", "(", "args", ".", "msa_path", ",", "args", ".", "msa_samples", ")", "]", "\n", "assert", "(", "\n", "args", ".", "scoring_strategy", "==", "\"masked-marginals\"", "\n", ")", ",", "\"MSA Transformer only supports masked marginal strategy\"", "\n", "\n", "batch_labels", ",", "batch_strs", ",", "batch_tokens", "=", "batch_converter", "(", "data", ")", "\n", "\n", "all_token_probs", "=", "[", "]", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "batch_tokens", ".", "size", "(", "2", ")", ")", ")", ":", "\n", "                ", "batch_tokens_masked", "=", "batch_tokens", ".", "clone", "(", ")", "\n", "batch_tokens_masked", "[", "0", ",", "0", ",", "i", "]", "=", "alphabet", ".", "mask_idx", "# mask out first sequence", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "token_probs", "=", "torch", ".", "log_softmax", "(", "\n", "model", "(", "batch_tokens_masked", ".", "cuda", "(", ")", ")", "[", "\"logits\"", "]", ",", "dim", "=", "-", "1", "\n", ")", "\n", "", "all_token_probs", ".", "append", "(", "token_probs", "[", ":", ",", "0", ",", "i", "]", ")", "# vocab size", "\n", "", "token_probs", "=", "torch", ".", "cat", "(", "all_token_probs", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "df", "[", "model_location", "]", "=", "df", ".", "apply", "(", "\n", "lambda", "row", ":", "label_row", "(", "\n", "row", "[", "args", ".", "mutation_col", "]", ",", "args", ".", "sequence", ",", "token_probs", ",", "alphabet", ",", "args", ".", "offset_idx", "\n", ")", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "", "else", ":", "\n", "            ", "data", "=", "[", "\n", "(", "\"protein1\"", ",", "args", ".", "sequence", ")", ",", "\n", "]", "\n", "batch_labels", ",", "batch_strs", ",", "batch_tokens", "=", "batch_converter", "(", "data", ")", "\n", "\n", "if", "args", ".", "scoring_strategy", "==", "\"wt-marginals\"", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "token_probs", "=", "torch", ".", "log_softmax", "(", "model", "(", "batch_tokens", ".", "cuda", "(", ")", ")", "[", "\"logits\"", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "df", "[", "model_location", "]", "=", "df", ".", "apply", "(", "\n", "lambda", "row", ":", "label_row", "(", "\n", "row", "[", "args", ".", "mutation_col", "]", ",", "\n", "args", ".", "sequence", ",", "\n", "token_probs", ",", "\n", "alphabet", ",", "\n", "args", ".", "offset_idx", ",", "\n", ")", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "", "elif", "args", ".", "scoring_strategy", "==", "\"masked-marginals\"", ":", "\n", "                ", "all_token_probs", "=", "[", "]", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "batch_tokens", ".", "size", "(", "1", ")", ")", ")", ":", "\n", "                    ", "batch_tokens_masked", "=", "batch_tokens", ".", "clone", "(", ")", "\n", "batch_tokens_masked", "[", "0", ",", "i", "]", "=", "alphabet", ".", "mask_idx", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                        ", "token_probs", "=", "torch", ".", "log_softmax", "(", "\n", "model", "(", "batch_tokens_masked", ".", "cuda", "(", ")", ")", "[", "\"logits\"", "]", ",", "dim", "=", "-", "1", "\n", ")", "\n", "", "all_token_probs", ".", "append", "(", "token_probs", "[", ":", ",", "i", "]", ")", "# vocab size", "\n", "", "token_probs", "=", "torch", ".", "cat", "(", "all_token_probs", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "df", "[", "model_location", "]", "=", "df", ".", "apply", "(", "\n", "lambda", "row", ":", "label_row", "(", "\n", "row", "[", "args", ".", "mutation_col", "]", ",", "\n", "args", ".", "sequence", ",", "\n", "token_probs", ",", "\n", "alphabet", ",", "\n", "args", ".", "offset_idx", ",", "\n", ")", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "", "elif", "args", ".", "scoring_strategy", "==", "\"pseudo-ppl\"", ":", "\n", "                ", "tqdm", ".", "pandas", "(", ")", "\n", "df", "[", "model_location", "]", "=", "df", ".", "progress_apply", "(", "\n", "lambda", "row", ":", "compute_pppl", "(", "\n", "row", "[", "args", ".", "mutation_col", "]", ",", "args", ".", "sequence", ",", "model", ",", "alphabet", ",", "args", ".", "offset_idx", "\n", ")", ",", "\n", "axis", "=", "1", ",", "\n", ")", "\n", "\n", "", "", "", "df", ".", "to_csv", "(", "args", ".", "dms_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.sample_sequences.main": [[19, 68], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "model.eval.eval", "esm.inverse_folding.util.load_coords", "esm.inverse_folding.util.load_coords", "print", "print", "print", "pathlib.Path().parent.mkdir", "open", "range", "print", "model.eval.sample", "print", "print", "f.write", "f.write", "numpy.mean", "print", "pathlib.Path", "zip"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.load_coords", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.load_coords", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.sample"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Sample sequences based on a given structure.'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'pdbfile'", ",", "type", "=", "str", ",", "\n", "help", "=", "'input filepath, either .pdb or .cif'", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--chain'", ",", "type", "=", "str", ",", "\n", "help", "=", "'chain id for the chain of interest'", ",", "default", "=", "None", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--temperature'", ",", "type", "=", "float", ",", "\n", "help", "=", "'temperature for sampling, higher for more diversity'", ",", "\n", "default", "=", "1.", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--outpath'", ",", "type", "=", "str", ",", "\n", "help", "=", "'output filepath for saving sampled sequences'", ",", "\n", "default", "=", "'output/sampled_seqs.fasta'", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--num-samples'", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of sequences to sample'", ",", "\n", "default", "=", "1", ",", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "model", ",", "alphabet", "=", "esm", ".", "pretrained", ".", "esm_if1_gvp4_t16_142M_UR50", "(", ")", "\n", "model", "=", "model", ".", "eval", "(", ")", "\n", "coords", ",", "seq", "=", "esm", ".", "inverse_folding", ".", "util", ".", "load_coords", "(", "args", ".", "pdbfile", ",", "args", ".", "chain", ")", "\n", "print", "(", "'Sequence loaded from file:'", ")", "\n", "print", "(", "seq", ")", "\n", "\n", "print", "(", "f'Saving sampled sequences to {args.outpath}.'", ")", "\n", "\n", "Path", "(", "args", ".", "outpath", ")", ".", "parent", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "args", ".", "outpath", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "i", "in", "range", "(", "args", ".", "num_samples", ")", ":", "\n", "            ", "print", "(", "f'\\nSampling.. ({i+1} of {args.num_samples})'", ")", "\n", "sampled_seq", "=", "model", ".", "sample", "(", "coords", ",", "temperature", "=", "args", ".", "temperature", ")", "\n", "print", "(", "'Sampled sequence:'", ")", "\n", "print", "(", "sampled_seq", ")", "\n", "f", ".", "write", "(", "f'>sampled_seq_{i+1}\\n'", ")", "\n", "f", ".", "write", "(", "sampled_seq", "+", "'\\n'", ")", "\n", "\n", "recovery", "=", "np", ".", "mean", "(", "[", "(", "a", "==", "b", ")", "for", "a", ",", "b", "in", "zip", "(", "seq", ",", "sampled_seq", ")", "]", ")", "\n", "print", "(", "'Sequence recovery:'", ",", "recovery", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.score_log_likelihoods.main": [[23, 71], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "model.eval.eval", "esm.inverse_folding.util.load_coords", "esm.inverse_folding.util.load_coords", "print", "print", "print", "esm.inverse_folding.util.score_sequence", "esm.inverse_folding.util.score_sequence", "print", "print", "print", "print", "biotite.sequence.io.fasta.FastaFile", "biotite.sequence.io.fasta.FastaFile.read", "biotite.sequence.io.fasta.get_sequences", "pathlib.Path().parent.mkdir", "print", "open", "fout.write", "tqdm.tqdm", "biotite.sequence.io.fasta.get_sequences.items", "esm.inverse_folding.util.score_sequence", "esm.inverse_folding.util.score_sequence", "fout.write", "numpy.exp", "pathlib.Path", "str", "str"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.load_coords", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.load_coords", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.score_sequence", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.score_sequence", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.score_sequence", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.score_sequence"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Score sequences based on a given structure.'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'pdbfile'", ",", "type", "=", "str", ",", "\n", "help", "=", "'input filepath, either .pdb or .cif'", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'seqfile'", ",", "type", "=", "str", ",", "\n", "help", "=", "'input filepath for variant sequences in a .fasta file'", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--outpath'", ",", "type", "=", "str", ",", "\n", "help", "=", "'output filepath for scores of variant sequences'", ",", "\n", "default", "=", "'output/sequence_scores.csv'", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--chain'", ",", "type", "=", "str", ",", "\n", "help", "=", "'chain id for the chain of interest'", ",", "default", "=", "'A'", ",", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "model", ",", "alphabet", "=", "esm", ".", "pretrained", ".", "esm_if1_gvp4_t16_142M_UR50", "(", ")", "\n", "model", "=", "model", ".", "eval", "(", ")", "\n", "coords", ",", "seq", "=", "esm", ".", "inverse_folding", ".", "util", ".", "load_coords", "(", "args", ".", "pdbfile", ",", "args", ".", "chain", ")", "\n", "print", "(", "'Native sequence loaded from structure file:'", ")", "\n", "print", "(", "seq", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n", "ll", ",", "_", "=", "esm", ".", "inverse_folding", ".", "util", ".", "score_sequence", "(", "\n", "model", ",", "alphabet", ",", "coords", ",", "seq", ")", "\n", "print", "(", "'Native sequence'", ")", "\n", "print", "(", "f'Log likelihood: {ll:.2f}'", ")", "\n", "print", "(", "f'Perplexity: {np.exp(-ll):.2f}'", ")", "\n", "\n", "print", "(", "'\\nScoring variant sequences from sequence file..\\n'", ")", "\n", "infile", "=", "FastaFile", "(", ")", "\n", "infile", ".", "read", "(", "args", ".", "seqfile", ")", "\n", "seqs", "=", "get_sequences", "(", "infile", ")", "\n", "Path", "(", "args", ".", "outpath", ")", ".", "parent", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "args", ".", "outpath", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "fout", ".", "write", "(", "'seqid,log_likelihood\\n'", ")", "\n", "for", "header", ",", "seq", "in", "tqdm", "(", "seqs", ".", "items", "(", ")", ")", ":", "\n", "            ", "ll", ",", "_", "=", "esm", ".", "inverse_folding", ".", "util", ".", "score_sequence", "(", "\n", "model", ",", "alphabet", ",", "coords", ",", "str", "(", "seq", ")", ")", "\n", "fout", ".", "write", "(", "header", "+", "','", "+", "str", "(", "ll", ")", "+", "'\\n'", ")", "\n", "", "", "print", "(", "f'Results saved to {args.outpath}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_utils.flatten_graph": [[9, 48], ["torch.any", "edge_index.permute().flatten.permute().flatten", "edge_mask.flatten.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "edge_index.permute().flatten.permute", "torch.arange"], "function", ["None"], ["def", "flatten_graph", "(", "node_embeddings", ",", "edge_embeddings", ",", "edge_index", ")", ":", "\n", "    ", "\"\"\"\n    Flattens the graph into a batch size one (with disconnected subgraphs for\n    each example) to be compatible with pytorch-geometric package.\n    Args:\n        node_embeddings: node embeddings in tuple form (scalar, vector)\n                - scalar: shape batch size x nodes x node_embed_dim\n                - vector: shape batch size x nodes x node_embed_dim x 3\n        edge_embeddings: edge embeddings of in tuple form (scalar, vector)\n                - scalar: shape batch size x edges x edge_embed_dim\n                - vector: shape batch size x edges x edge_embed_dim x 3\n        edge_index: shape batch_size x 2 (source node and target node) x edges\n    Returns:\n        node_embeddings: node embeddings in tuple form (scalar, vector)\n                - scalar: shape batch total_nodes x node_embed_dim\n                - vector: shape batch total_nodes x node_embed_dim x 3\n        edge_embeddings: edge embeddings of in tuple form (scalar, vector)\n                - scalar: shape batch total_edges x edge_embed_dim\n                - vector: shape batch total_edges x edge_embed_dim x 3\n        edge_index: shape 2 x total_edges\n    \"\"\"", "\n", "x_s", ",", "x_v", "=", "node_embeddings", "\n", "e_s", ",", "e_v", "=", "edge_embeddings", "\n", "batch_size", ",", "N", "=", "x_s", ".", "shape", "[", "0", "]", ",", "x_s", ".", "shape", "[", "1", "]", "\n", "node_embeddings", "=", "(", "torch", ".", "flatten", "(", "x_s", ",", "0", ",", "1", ")", ",", "torch", ".", "flatten", "(", "x_v", ",", "0", ",", "1", ")", ")", "\n", "edge_embeddings", "=", "(", "torch", ".", "flatten", "(", "e_s", ",", "0", ",", "1", ")", ",", "torch", ".", "flatten", "(", "e_v", ",", "0", ",", "1", ")", ")", "\n", "\n", "edge_mask", "=", "torch", ".", "any", "(", "edge_index", "!=", "-", "1", ",", "dim", "=", "1", ")", "\n", "# Re-number the nodes by adding batch_idx * N to each batch", "\n", "edge_index", "=", "edge_index", "+", "(", "torch", ".", "arange", "(", "batch_size", ",", "device", "=", "edge_index", ".", "device", ")", "*", "\n", "N", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "edge_index", "=", "edge_index", ".", "permute", "(", "1", ",", "0", ",", "2", ")", ".", "flatten", "(", "1", ",", "2", ")", "\n", "edge_mask", "=", "edge_mask", ".", "flatten", "(", ")", "\n", "edge_index", "=", "edge_index", "[", ":", ",", "edge_mask", "]", "\n", "edge_embeddings", "=", "(", "\n", "edge_embeddings", "[", "0", "]", "[", "edge_mask", ",", ":", "]", ",", "\n", "edge_embeddings", "[", "1", "]", "[", "edge_mask", ",", ":", "]", "\n", ")", "\n", "return", "node_embeddings", ",", "edge_embeddings", ",", "edge_index", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_utils.unflatten_graph": [[50, 67], ["x_s.reshape.reshape", "x_v.reshape.reshape"], "function", ["None"], ["", "def", "unflatten_graph", "(", "node_embeddings", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"\n    Unflattens node embeddings.\n    Args:\n        node_embeddings: node embeddings in tuple form (scalar, vector)\n                - scalar: shape batch total_nodes x node_embed_dim\n                - vector: shape batch total_nodes x node_embed_dim x 3\n        batch_size: int\n    Returns:\n        node_embeddings: node embeddings in tuple form (scalar, vector)\n                - scalar: shape batch size x nodes x node_embed_dim\n                - vector: shape batch size x nodes x node_embed_dim x 3\n    \"\"\"", "\n", "x_s", ",", "x_v", "=", "node_embeddings", "\n", "x_s", "=", "x_s", ".", "reshape", "(", "batch_size", ",", "-", "1", ",", "x_s", ".", "shape", "[", "1", "]", ")", "\n", "x_v", "=", "x_v", ".", "reshape", "(", "batch_size", ",", "-", "1", ",", "x_v", ".", "shape", "[", "1", "]", ",", "x_v", ".", "shape", "[", "2", "]", ")", "\n", "return", "(", "x_s", ",", "x_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer_encoder.GVPTransformerEncoder.__init__": [[34, 69], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "math.sqrt", "esm.modules.SinusoidalPositionalEmbedding", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "features.DihedralFeatures", "argparse.Namespace", "vars().items", "gvp_encoder.GVPEncoder", "torch.Linear", "torch.Linear", "torch.ModuleList", "torch.ModuleList", "gvp_transformer_encoder.GVPTransformerEncoder.layers.extend", "len", "torch.LayerNorm", "torch.LayerNorm", "k.startswith", "vars", "setattr", "gvp_transformer_encoder.GVPTransformerEncoder.build_encoder_layer", "range"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer_encoder.GVPTransformerEncoder.build_encoder_layer"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "\n", "self", ".", "dropout_module", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n", "embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "\n", "self", ".", "embed_positions", "=", "SinusoidalPositionalEmbedding", "(", "\n", "embed_dim", ",", "\n", "self", ".", "padding_idx", ",", "\n", ")", "\n", "self", ".", "embed_gvp_input_features", "=", "nn", ".", "Linear", "(", "15", ",", "embed_dim", ")", "\n", "self", ".", "embed_confidence", "=", "nn", ".", "Linear", "(", "16", ",", "embed_dim", ")", "\n", "self", ".", "embed_dihedrals", "=", "DihedralFeatures", "(", "embed_dim", ")", "\n", "\n", "gvp_args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "for", "k", ",", "v", "in", "vars", "(", "args", ")", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", ".", "startswith", "(", "\"gvp_\"", ")", ":", "\n", "                ", "setattr", "(", "gvp_args", ",", "k", "[", "4", ":", "]", ",", "v", ")", "\n", "", "", "self", ".", "gvp_encoder", "=", "GVPEncoder", "(", "gvp_args", ")", "\n", "gvp_out_dim", "=", "gvp_args", ".", "node_hidden_dim_scalar", "+", "(", "3", "*", "\n", "gvp_args", ".", "node_hidden_dim_vector", ")", "\n", "self", ".", "embed_gvp_output", "=", "nn", ".", "Linear", "(", "gvp_out_dim", ",", "embed_dim", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "\n", "[", "self", ".", "build_encoder_layer", "(", "args", ")", "for", "i", "in", "range", "(", "args", ".", "encoder_layers", ")", "]", "\n", ")", "\n", "self", ".", "num_layers", "=", "len", "(", "self", ".", "layers", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer_encoder.GVPTransformerEncoder.build_encoder_layer": [[70, 72], ["transformer_layer.TransformerEncoderLayer"], "methods", ["None"], ["", "def", "build_encoder_layer", "(", "self", ",", "args", ")", ":", "\n", "        ", "return", "TransformerEncoderLayer", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer_encoder.GVPTransformerEncoder.forward_embedding": [[73, 122], ["dict", "torch.all", "torch.all", "torch.all", "torch.all", "util.nan_to_num", "gvp_transformer_encoder.GVPTransformerEncoder.embed_dihedrals", "gvp_transformer_encoder.GVPTransformerEncoder.gvp_encoder", "util.get_rotation_frames", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "gvp_transformer_encoder.GVPTransformerEncoder.embed_gvp_output", "gvp_transformer_encoder.GVPTransformerEncoder.embed_confidence", "torch.cat.GVPInputFeaturizer.get_node_features", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "gvp_transformer_encoder.GVPTransformerEncoder.embed_gvp_input_features", "sum", "gvp_transformer_encoder.GVPTransformerEncoder.dropout_module", "torch.all", "torch.all", "torch.all", "torch.all", "gvp_transformer_encoder.GVPTransformerEncoder.embed_tokens", "util.rbf", "dict.values", "gvp_transformer_encoder.GVPTransformerEncoder.embed_positions", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "gvp_transformer_encoder.GVPTransformerEncoder.dictionary.get_idx", "util.rotate().flatten", "util.rotate().flatten", "util.rotate", "util.rotate", "util.get_rotation_frames.transpose", "util.get_rotation_frames.transpose"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.nan_to_num", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.get_rotation_frames", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer.get_node_features", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.rbf", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.rotate", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.rotate"], ["", "def", "forward_embedding", "(", "self", ",", "coords", ",", "padding_mask", ",", "confidence", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            coords: N, CA, C backbone coordinates in shape length x 3 (atoms) x 3 \n            padding_mask: boolean Tensor (true for padding) of shape length\n            confidence: confidence scores between 0 and 1 of shape length\n        \"\"\"", "\n", "components", "=", "dict", "(", ")", "\n", "coord_mask", "=", "torch", ".", "all", "(", "torch", ".", "all", "(", "torch", ".", "isfinite", "(", "coords", ")", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "coords", "=", "nan_to_num", "(", "coords", ")", "\n", "mask_tokens", "=", "(", "\n", "padding_mask", "*", "self", ".", "dictionary", ".", "padding_idx", "+", "\n", "~", "padding_mask", "*", "self", ".", "dictionary", ".", "get_idx", "(", "\"<mask>\"", ")", "\n", ")", "\n", "components", "[", "\"tokens\"", "]", "=", "self", ".", "embed_tokens", "(", "mask_tokens", ")", "*", "self", ".", "embed_scale", "\n", "components", "[", "\"diherals\"", "]", "=", "self", ".", "embed_dihedrals", "(", "coords", ")", "\n", "\n", "# GVP encoder", "\n", "gvp_out_scalars", ",", "gvp_out_vectors", "=", "self", ".", "gvp_encoder", "(", "coords", ",", "\n", "coord_mask", ",", "padding_mask", ",", "confidence", ")", "\n", "R", "=", "get_rotation_frames", "(", "coords", ")", "\n", "# Rotate to local rotation frame for rotation-invariance", "\n", "gvp_out_features", "=", "torch", ".", "cat", "(", "[", "\n", "gvp_out_scalars", ",", "\n", "rotate", "(", "gvp_out_vectors", ",", "R", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", ".", "flatten", "(", "-", "2", ",", "-", "1", ")", ",", "\n", "]", ",", "dim", "=", "-", "1", ")", "\n", "components", "[", "\"gvp_out\"", "]", "=", "self", ".", "embed_gvp_output", "(", "gvp_out_features", ")", "\n", "\n", "components", "[", "\"confidence\"", "]", "=", "self", ".", "embed_confidence", "(", "\n", "rbf", "(", "confidence", ",", "0.", ",", "1.", ")", ")", "\n", "\n", "# In addition to GVP encoder outputs, also directly embed GVP input node", "\n", "# features to the Transformer", "\n", "scalar_features", ",", "vector_features", "=", "GVPInputFeaturizer", ".", "get_node_features", "(", "\n", "coords", ",", "coord_mask", ",", "with_coord_mask", "=", "False", ")", "\n", "features", "=", "torch", ".", "cat", "(", "[", "\n", "scalar_features", ",", "\n", "rotate", "(", "vector_features", ",", "R", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", ".", "flatten", "(", "-", "2", ",", "-", "1", ")", ",", "\n", "]", ",", "dim", "=", "-", "1", ")", "\n", "components", "[", "\"gvp_input_features\"", "]", "=", "self", ".", "embed_gvp_input_features", "(", "features", ")", "\n", "\n", "embed", "=", "sum", "(", "components", ".", "values", "(", ")", ")", "\n", "# for k, v in components.items():", "\n", "#     print(k, torch.mean(v, dim=(0,1)), torch.std(v, dim=(0,1)))", "\n", "\n", "x", "=", "embed", "\n", "x", "=", "x", "+", "self", ".", "embed_positions", "(", "mask_tokens", ")", "\n", "x", "=", "self", ".", "dropout_module", "(", "x", ")", "\n", "return", "x", ",", "components", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer_encoder.GVPTransformerEncoder.forward": [[123, 184], ["gvp_transformer_encoder.GVPTransformerEncoder.forward_embedding", "gvp_transformer_encoder.GVPTransformerEncoder.transpose", "encoder_states.append", "layer", "gvp_transformer_encoder.GVPTransformerEncoder.layer_norm", "encoder_padding_mask.unsqueeze().type_as", "encoder_states.append", "encoder_padding_mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer_encoder.GVPTransformerEncoder.forward_embedding"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "coords", ",", "\n", "encoder_padding_mask", ",", "\n", "confidence", ",", "\n", "return_all_hiddens", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            coords (Tensor): backbone coordinates\n                shape batch_size x num_residues x num_atoms (3 for N, CA, C) x 3\n            encoder_padding_mask (ByteTensor): the positions of\n                  padding elements of shape `(batch_size x num_residues)`\n            confidence (Tensor): the confidence score of shape (batch_size x\n                num_residues). The value is between 0. and 1. for each residue\n                coordinate, or -1. if no coordinate is given\n            return_all_hiddens (bool, optional): also return all of the\n                intermediate hidden states (default: False).\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer's output of\n                  shape `(num_residues, batch_size, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch_size, num_residues)`\n                - **encoder_embedding** (Tensor): the (scaled) embedding lookup\n                  of shape `(batch_size, num_residues, embed_dim)`\n                - **encoder_states** (List[Tensor]): all intermediate\n                  hidden states of shape `(num_residues, batch_size, embed_dim)`.\n                  Only populated if *return_all_hiddens* is True.\n        \"\"\"", "\n", "x", ",", "encoder_embedding", "=", "self", ".", "forward_embedding", "(", "coords", ",", "\n", "encoder_padding_mask", ",", "confidence", ")", "\n", "# account for padding while computing the representation", "\n", "x", "=", "x", "*", "(", "1", "-", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "type_as", "(", "x", ")", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "encoder_states", "=", "[", "]", "\n", "\n", "if", "return_all_hiddens", ":", "\n", "            ", "encoder_states", ".", "append", "(", "x", ")", "\n", "\n", "# encoder layers", "\n", "", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "layer", "(", "\n", "x", ",", "encoder_padding_mask", "=", "encoder_padding_mask", "\n", ")", "\n", "if", "return_all_hiddens", ":", "\n", "                ", "assert", "encoder_states", "is", "not", "None", "\n", "encoder_states", ".", "append", "(", "x", ")", "\n", "\n", "", "", "if", "self", ".", "layer_norm", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "", "return", "{", "\n", "\"encoder_out\"", ":", "[", "x", "]", ",", "# T x B x C", "\n", "\"encoder_padding_mask\"", ":", "[", "encoder_padding_mask", "]", ",", "# B x T", "\n", "\"encoder_embedding\"", ":", "[", "encoder_embedding", "]", ",", "# dictionary", "\n", "\"encoder_states\"", ":", "encoder_states", ",", "# List[T x B x C]", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.__init__": [[32, 45], ["torch.nn.Module.__init__", "gvp_transformer.GVPTransformerModel.build_embedding", "gvp_transformer.GVPTransformerModel.build_embedding", "gvp_transformer.GVPTransformerModel.build_encoder", "gvp_transformer.GVPTransformerModel.build_decoder"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.build_embedding", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.build_embedding", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.build_encoder", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.build_decoder"], ["def", "__init__", "(", "self", ",", "args", ",", "alphabet", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "encoder_embed_tokens", "=", "self", ".", "build_embedding", "(", "\n", "args", ",", "alphabet", ",", "args", ".", "encoder_embed_dim", ",", "\n", ")", "\n", "decoder_embed_tokens", "=", "self", ".", "build_embedding", "(", "\n", "args", ",", "alphabet", ",", "args", ".", "decoder_embed_dim", ",", "\n", ")", "\n", "encoder", "=", "self", ".", "build_encoder", "(", "args", ",", "alphabet", ",", "encoder_embed_tokens", ")", "\n", "decoder", "=", "self", ".", "build_decoder", "(", "args", ",", "alphabet", ",", "decoder_embed_tokens", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.build_encoder": [[46, 50], ["gvp_transformer_encoder.GVPTransformerEncoder"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_encoder", "(", "cls", ",", "args", ",", "src_dict", ",", "embed_tokens", ")", ":", "\n", "        ", "encoder", "=", "GVPTransformerEncoder", "(", "args", ",", "src_dict", ",", "embed_tokens", ")", "\n", "return", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.build_decoder": [[51, 59], ["transformer_decoder.TransformerDecoder"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_decoder", "(", "cls", ",", "args", ",", "tgt_dict", ",", "embed_tokens", ")", ":", "\n", "        ", "decoder", "=", "TransformerDecoder", "(", "\n", "args", ",", "\n", "tgt_dict", ",", "\n", "embed_tokens", ",", "\n", ")", "\n", "return", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.build_embedding": [[60, 68], ["len", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.constant_", "torch.nn.init.constant_"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_embedding", "(", "cls", ",", "args", ",", "dictionary", ",", "embed_dim", ")", ":", "\n", "        ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "padding_idx", "\n", "emb", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "emb", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embed_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "emb", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.forward": [[69, 87], ["gvp_transformer.GVPTransformerModel.encoder", "gvp_transformer.GVPTransformerModel.decoder"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "coords", ",", "\n", "padding_mask", ",", "\n", "confidence", ",", "\n", "prev_output_tokens", ",", "\n", "return_all_hiddens", ":", "bool", "=", "False", ",", "\n", "features_only", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "encoder_out", "=", "self", ".", "encoder", "(", "coords", ",", "padding_mask", ",", "confidence", ",", "\n", "return_all_hiddens", "=", "return_all_hiddens", ")", "\n", "logits", ",", "extra", "=", "self", ".", "decoder", "(", "\n", "prev_output_tokens", ",", "\n", "encoder_out", "=", "encoder_out", ",", "\n", "features_only", "=", "features_only", ",", "\n", "return_all_hiddens", "=", "return_all_hiddens", ",", "\n", ")", "\n", "return", "logits", ",", "extra", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_transformer.GVPTransformerModel.sample": [[88, 130], ["len", "util.CoordBatchConverter", "util.CoordBatchConverter.", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "gvp_transformer.GVPTransformerModel.decoder.dictionary.get_idx", "dict", "gvp_transformer.GVPTransformerModel.encoder", "range", "gvp_transformer.GVPTransformerModel.decoder", "logits[].transpose", "torch.softmax", "torch.softmax", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "torch.multinomial().squeeze", "gvp_transformer.GVPTransformerModel.decoder.dictionary.get_tok", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_tok"], ["", "def", "sample", "(", "self", ",", "coords", ",", "temperature", "=", "1.0", ",", "confidence", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Samples sequences based on multinomial sampling (no beam search).\n\n        Args:\n            coords: L x 3 x 3 list representing one backbone\n            temperature: sampling temperature, use low temperature for higher\n                sequence recovery and high temperature for higher diversity\n            confidence: optional length L list of confidence scores for coordinates\n        \"\"\"", "\n", "L", "=", "len", "(", "coords", ")", "\n", "# Convert to batch format", "\n", "batch_converter", "=", "CoordBatchConverter", "(", "self", ".", "decoder", ".", "dictionary", ")", "\n", "batch_coords", ",", "confidence", ",", "_", ",", "_", ",", "padding_mask", "=", "(", "\n", "batch_converter", "(", "[", "(", "coords", ",", "confidence", ",", "None", ")", "]", ")", "\n", ")", "\n", "\n", "# Start with prepend token", "\n", "sampled_tokens", "=", "torch", ".", "zeros", "(", "1", ",", "1", "+", "L", ",", "dtype", "=", "int", ")", "\n", "sampled_tokens", "[", "0", ",", "0", "]", "=", "self", ".", "decoder", ".", "dictionary", ".", "get_idx", "(", "'<cath>'", ")", "\n", "\n", "# Save incremental states for faster sampling", "\n", "incremental_state", "=", "dict", "(", ")", "\n", "\n", "# Run encoder only once", "\n", "encoder_out", "=", "self", ".", "encoder", "(", "batch_coords", ",", "padding_mask", ",", "confidence", ")", "\n", "\n", "# Decode one token at a time", "\n", "for", "i", "in", "range", "(", "1", ",", "L", "+", "1", ")", ":", "\n", "            ", "logits", ",", "_", "=", "self", ".", "decoder", "(", "\n", "sampled_tokens", "[", ":", ",", ":", "i", "]", ",", "\n", "encoder_out", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "\n", "logits", "=", "logits", "[", "0", "]", ".", "transpose", "(", "0", ",", "1", ")", "\n", "logits", "/=", "temperature", "\n", "probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "sampled_tokens", "[", ":", ",", "i", "]", "=", "torch", ".", "multinomial", "(", "probs", ",", "1", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "", "sampled_seq", "=", "sampled_tokens", "[", "0", ",", "1", ":", "]", "\n", "\n", "# Convert back to string via lookup", "\n", "return", "''", ".", "join", "(", "[", "self", ".", "decoder", ".", "dictionary", ".", "get_tok", "(", "a", ")", "for", "a", "in", "sampled_seq", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.GVP.__init__": [[110, 131], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "max", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "self", ",", "in_dims", ",", "out_dims", ",", "h_dim", "=", "None", ",", "vector_gate", "=", "False", ",", "\n", "activations", "=", "(", "F", ".", "relu", ",", "torch", ".", "sigmoid", ")", ",", "tuple_io", "=", "True", ",", "\n", "eps", "=", "1e-8", ")", ":", "\n", "        ", "super", "(", "GVP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "si", ",", "self", ".", "vi", "=", "in_dims", "\n", "self", ".", "so", ",", "self", ".", "vo", "=", "out_dims", "\n", "self", ".", "tuple_io", "=", "tuple_io", "\n", "if", "self", ".", "vi", ":", "\n", "            ", "self", ".", "h_dim", "=", "h_dim", "or", "max", "(", "self", ".", "vi", ",", "self", ".", "vo", ")", "\n", "self", ".", "wh", "=", "nn", ".", "Linear", "(", "self", ".", "vi", ",", "self", ".", "h_dim", ",", "bias", "=", "False", ")", "\n", "self", ".", "ws", "=", "nn", ".", "Linear", "(", "self", ".", "h_dim", "+", "self", ".", "si", ",", "self", ".", "so", ")", "\n", "if", "self", ".", "vo", ":", "\n", "                ", "self", ".", "wv", "=", "nn", ".", "Linear", "(", "self", ".", "h_dim", ",", "self", ".", "vo", ",", "bias", "=", "False", ")", "\n", "if", "vector_gate", ":", "\n", "                    ", "self", ".", "wg", "=", "nn", ".", "Linear", "(", "self", ".", "so", ",", "self", ".", "vo", ")", "\n", "", "", "", "else", ":", "\n", "            ", "self", ".", "ws", "=", "nn", ".", "Linear", "(", "self", ".", "si", ",", "self", ".", "so", ")", "\n", "\n", "", "self", ".", "vector_gate", "=", "vector_gate", "\n", "self", ".", "scalar_act", ",", "self", ".", "vector_act", "=", "activations", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.GVP.forward": [[132, 174], ["torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "gvp_modules.GVP.wh", "gvp_modules._norm_no_nan", "gvp_modules.GVP.ws", "gvp_modules.GVP.ws", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "gvp_modules.GVP.scalar_act", "gvp_modules.GVP.wv", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "gvp_modules.GVP.scalar_act", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "gvp_modules.GVP.wg().unsqueeze", "gvp_modules._norm_no_nan", "gvp_modules.GVP.vector_act", "gvp_modules.GVP.wg", "list"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._norm_no_nan", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._norm_no_nan"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "'''\n        :param x: tuple (s, V) of `torch.Tensor`, \n                  or (if vectors_in is 0), a single `torch.Tensor`\n        :return: tuple (s, V) of `torch.Tensor`,\n                 or (if vectors_out is 0), a single `torch.Tensor`\n        '''", "\n", "if", "self", ".", "vi", ":", "\n", "            ", "s", ",", "v", "=", "x", "\n", "v", "=", "torch", ".", "transpose", "(", "v", ",", "-", "1", ",", "-", "2", ")", "\n", "vh", "=", "self", ".", "wh", "(", "v", ")", "\n", "vn", "=", "_norm_no_nan", "(", "vh", ",", "axis", "=", "-", "2", ",", "eps", "=", "self", ".", "eps", ")", "\n", "s", "=", "self", ".", "ws", "(", "torch", ".", "cat", "(", "[", "s", ",", "vn", "]", ",", "-", "1", ")", ")", "\n", "if", "self", ".", "scalar_act", ":", "\n", "                ", "s", "=", "self", ".", "scalar_act", "(", "s", ")", "\n", "", "if", "self", ".", "vo", ":", "\n", "                ", "v", "=", "self", ".", "wv", "(", "vh", ")", "\n", "v", "=", "torch", ".", "transpose", "(", "v", ",", "-", "1", ",", "-", "2", ")", "\n", "if", "self", ".", "vector_gate", ":", "\n", "                    ", "g", "=", "self", ".", "wg", "(", "s", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "                    ", "g", "=", "_norm_no_nan", "(", "v", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ",", "eps", "=", "self", ".", "eps", ")", "\n", "", "if", "self", ".", "vector_act", ":", "\n", "                    ", "g", "=", "self", ".", "vector_act", "(", "g", ")", "\n", "v", "=", "v", "*", "g", "\n", "", "", "", "else", ":", "\n", "            ", "if", "self", ".", "tuple_io", ":", "\n", "                ", "assert", "x", "[", "1", "]", "is", "None", "\n", "x", "=", "x", "[", "0", "]", "\n", "", "s", "=", "self", ".", "ws", "(", "x", ")", "\n", "if", "self", ".", "scalar_act", ":", "\n", "                ", "s", "=", "self", ".", "scalar_act", "(", "s", ")", "\n", "", "if", "self", ".", "vo", ":", "\n", "                ", "v", "=", "torch", ".", "zeros", "(", "list", "(", "s", ".", "shape", ")", "[", ":", "-", "1", "]", "+", "[", "self", ".", "vo", ",", "3", "]", ",", "\n", "device", "=", "s", ".", "device", ")", "\n", "\n", "", "", "if", "self", ".", "vo", ":", "\n", "            ", "return", "(", "s", ",", "v", ")", "\n", "", "elif", "self", ".", "tuple_io", ":", "\n", "            ", "return", "(", "s", ",", "None", ")", "\n", "", "else", ":", "\n", "            ", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._VDropout.__init__": [[181, 184], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "self", ",", "drop_rate", ")", ":", "\n", "        ", "super", "(", "_VDropout", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._VDropout.forward": [[185, 199], ["torch.bernoulli().unsqueeze", "torch.bernoulli().unsqueeze", "torch.bernoulli().unsqueeze", "torch.bernoulli().unsqueeze", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "'''\n        :param x: `torch.Tensor` corresponding to vector channels\n        '''", "\n", "if", "x", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "device", "=", "x", ".", "device", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "return", "x", "\n", "", "mask", "=", "torch", ".", "bernoulli", "(", "\n", "(", "1", "-", "self", ".", "drop_rate", ")", "*", "torch", ".", "ones", "(", "x", ".", "shape", "[", ":", "-", "1", "]", ",", "device", "=", "device", ")", "\n", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "x", "=", "mask", "*", "x", "/", "(", "1", "-", "self", ".", "drop_rate", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.Dropout.__init__": [[205, 209], ["torch.nn.Module.__init__", "torch.nn.Dropout", "torch.nn.Dropout", "gvp_modules._VDropout"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "self", ",", "drop_rate", ")", ":", "\n", "        ", "super", "(", "Dropout", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sdropout", "=", "nn", ".", "Dropout", "(", "drop_rate", ")", "\n", "self", ".", "vdropout", "=", "_VDropout", "(", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.Dropout.forward": [[210, 220], ["type", "gvp_modules.Dropout.sdropout", "gvp_modules.Dropout.sdropout", "gvp_modules.Dropout.vdropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "'''\n        :param x: tuple (s, V) of `torch.Tensor`,\n                  or single `torch.Tensor` \n                  (will be assumed to be scalar channels)\n        '''", "\n", "if", "type", "(", "x", ")", "is", "torch", ".", "Tensor", ":", "\n", "            ", "return", "self", ".", "sdropout", "(", "x", ")", "\n", "", "s", ",", "v", "=", "x", "\n", "return", "self", ".", "sdropout", "(", "s", ")", ",", "self", ".", "vdropout", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.LayerNorm.__init__": [[226, 232], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "self", ",", "dims", ",", "tuple_io", "=", "True", ",", "eps", "=", "1e-8", ")", ":", "\n", "        ", "super", "(", "LayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tuple_io", "=", "tuple_io", "\n", "self", ".", "s", ",", "self", ".", "v", "=", "dims", "\n", "self", ".", "scalar_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "s", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.LayerNorm.forward": [[233, 251], ["gvp_modules._norm_no_nan", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "gvp_modules.LayerNorm.scalar_norm", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "gvp_modules.LayerNorm.scalar_norm", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "gvp_modules.LayerNorm.scalar_norm"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._norm_no_nan"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "'''\n        :param x: tuple (s, V) of `torch.Tensor`,\n                  or single `torch.Tensor` \n                  (will be assumed to be scalar channels)\n        '''", "\n", "if", "not", "self", ".", "v", ":", "\n", "            ", "if", "self", ".", "tuple_io", ":", "\n", "                ", "return", "self", ".", "scalar_norm", "(", "x", "[", "0", "]", ")", ",", "None", "\n", "", "return", "self", ".", "scalar_norm", "(", "x", ")", "\n", "", "s", ",", "v", "=", "x", "\n", "vn", "=", "_norm_no_nan", "(", "v", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ",", "sqrt", "=", "False", ",", "eps", "=", "self", ".", "eps", ")", "\n", "nonzero_mask", "=", "(", "vn", ">", "2", "*", "self", ".", "eps", ")", "\n", "vn", "=", "torch", ".", "sum", "(", "vn", "*", "nonzero_mask", ",", "dim", "=", "-", "2", ",", "keepdim", "=", "True", "\n", ")", "/", "(", "self", ".", "eps", "+", "torch", ".", "sum", "(", "nonzero_mask", ",", "dim", "=", "-", "2", ",", "keepdim", "=", "True", ")", ")", "\n", "vn", "=", "torch", ".", "sqrt", "(", "vn", "+", "self", ".", "eps", ")", "\n", "v", "=", "nonzero_mask", "*", "(", "v", "/", "vn", ")", "\n", "return", "self", ".", "scalar_norm", "(", "s", ")", ",", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.GVPConv.__init__": [[269, 295], ["torch_geometric.nn.MessagePassing.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "module_list.append", "module_list.append", "range", "module_list.append", "gvp_modules.GVP", "gvp_modules.GVP", "module_list.append", "gvp_modules.GVP", "gvp_modules.GVP"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "self", ",", "in_dims", ",", "out_dims", ",", "edge_dims", ",", "n_layers", "=", "3", ",", "\n", "vector_gate", "=", "False", ",", "module_list", "=", "None", ",", "aggr", "=", "\"mean\"", ",", "eps", "=", "1e-8", ",", "\n", "activations", "=", "(", "F", ".", "relu", ",", "torch", ".", "sigmoid", ")", ")", ":", "\n", "        ", "super", "(", "GVPConv", ",", "self", ")", ".", "__init__", "(", "aggr", "=", "aggr", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "si", ",", "self", ".", "vi", "=", "in_dims", "\n", "self", ".", "so", ",", "self", ".", "vo", "=", "out_dims", "\n", "self", ".", "se", ",", "self", ".", "ve", "=", "edge_dims", "\n", "\n", "module_list", "=", "module_list", "or", "[", "]", "\n", "if", "not", "module_list", ":", "\n", "            ", "if", "n_layers", "==", "1", ":", "\n", "                ", "module_list", ".", "append", "(", "\n", "GVP", "(", "(", "2", "*", "self", ".", "si", "+", "self", ".", "se", ",", "2", "*", "self", ".", "vi", "+", "self", ".", "ve", ")", ",", "\n", "(", "self", ".", "so", ",", "self", ".", "vo", ")", ",", "activations", "=", "(", "None", ",", "None", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "module_list", ".", "append", "(", "\n", "GVP", "(", "(", "2", "*", "self", ".", "si", "+", "self", ".", "se", ",", "2", "*", "self", ".", "vi", "+", "self", ".", "ve", ")", ",", "out_dims", ",", "\n", "vector_gate", "=", "vector_gate", ",", "activations", "=", "activations", ")", "\n", ")", "\n", "for", "i", "in", "range", "(", "n_layers", "-", "2", ")", ":", "\n", "                    ", "module_list", ".", "append", "(", "GVP", "(", "out_dims", ",", "out_dims", ",", "\n", "vector_gate", "=", "vector_gate", ")", ")", "\n", "", "module_list", ".", "append", "(", "GVP", "(", "out_dims", ",", "out_dims", ",", "\n", "activations", "=", "(", "None", ",", "None", ")", ")", ")", "\n", "", "", "self", ".", "message_func", "=", "nn", ".", "Sequential", "(", "*", "module_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.GVPConv.forward": [[296, 307], ["gvp_modules.GVPConv.propagate", "gvp_modules._split", "x_v.reshape"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._split"], ["", "def", "forward", "(", "self", ",", "x", ",", "edge_index", ",", "edge_attr", ")", ":", "\n", "        ", "'''\n        :param x: tuple (s, V) of `torch.Tensor`\n        :param edge_index: array of shape [2, n_edges]\n        :param edge_attr: tuple (s, V) of `torch.Tensor`\n        '''", "\n", "x_s", ",", "x_v", "=", "x", "\n", "message", "=", "self", ".", "propagate", "(", "edge_index", ",", "\n", "s", "=", "x_s", ",", "v", "=", "x_v", ".", "reshape", "(", "x_v", ".", "shape", "[", "0", "]", ",", "3", "*", "x_v", ".", "shape", "[", "1", "]", ")", ",", "\n", "edge_attr", "=", "edge_attr", ")", "\n", "return", "_split", "(", "message", ",", "self", ".", "vo", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.GVPConv.message": [[308, 314], ["v_j.view.view.view", "v_i.view.view.view", "gvp_modules.tuple_cat", "gvp_modules.GVPConv.message_func", "gvp_modules._merge"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_cat", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._merge"], ["", "def", "message", "(", "self", ",", "s_i", ",", "v_i", ",", "s_j", ",", "v_j", ",", "edge_attr", ")", ":", "\n", "        ", "v_j", "=", "v_j", ".", "view", "(", "v_j", ".", "shape", "[", "0", "]", ",", "v_j", ".", "shape", "[", "1", "]", "//", "3", ",", "3", ")", "\n", "v_i", "=", "v_i", ".", "view", "(", "v_i", ".", "shape", "[", "0", "]", ",", "v_i", ".", "shape", "[", "1", "]", "//", "3", ",", "3", ")", "\n", "message", "=", "tuple_cat", "(", "(", "s_j", ",", "v_j", ")", ",", "edge_attr", ",", "(", "s_i", ",", "v_i", ")", ")", "\n", "message", "=", "self", ".", "message_func", "(", "message", ")", "\n", "return", "_merge", "(", "*", "message", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.GVPConvLayer.__init__": [[334, 387], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Sequential", "torch.nn.Sequential", "gvp_modules.GVPConv", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "ff_func.append", "ff_func.append", "range", "ff_func.append", "range", "torch.nn.Sequential", "torch.nn.Sequential", "gvp_modules.Dropout", "gvp_modules.Dropout", "gvp_modules.GVP", "gvp_modules.GVP", "ff_func.append", "gvp_modules.GVP", "gvp_modules.GVP", "module_list.append", "module_list.append", "gvp_modules.LayerNorm", "torch.nn.Identity", "torch.nn.Identity", "gvp_modules.LayerNorm", "torch.nn.Identity", "torch.nn.Identity", "range", "gvp_modules.GVP", "gvp_modules.GVP", "gvp_modules.GVP", "range", "range"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "self", ",", "node_dims", ",", "edge_dims", ",", "vector_gate", "=", "False", ",", "\n", "n_message", "=", "3", ",", "n_feedforward", "=", "2", ",", "drop_rate", "=", ".1", ",", "\n", "autoregressive", "=", "False", ",", "attention_heads", "=", "0", ",", "\n", "conv_activations", "=", "(", "F", ".", "relu", ",", "torch", ".", "sigmoid", ")", ",", "\n", "n_edge_gvps", "=", "0", ",", "layernorm", "=", "True", ",", "eps", "=", "1e-8", ")", ":", "\n", "\n", "        ", "super", "(", "GVPConvLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "attention_heads", "==", "0", ":", "\n", "            ", "self", ".", "conv", "=", "GVPConv", "(", "\n", "node_dims", ",", "node_dims", ",", "edge_dims", ",", "n_layers", "=", "n_message", ",", "\n", "vector_gate", "=", "vector_gate", ",", "\n", "aggr", "=", "\"add\"", "if", "autoregressive", "else", "\"mean\"", ",", "\n", "activations", "=", "conv_activations", ",", "\n", "eps", "=", "eps", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "if", "layernorm", ":", "\n", "            ", "self", ".", "norm", "=", "nn", ".", "ModuleList", "(", "[", "LayerNorm", "(", "node_dims", ",", "eps", "=", "eps", ")", "for", "_", "in", "range", "(", "2", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "norm", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Identity", "(", ")", "for", "_", "in", "range", "(", "2", ")", "]", ")", "\n", "", "self", ".", "dropout", "=", "nn", ".", "ModuleList", "(", "[", "Dropout", "(", "drop_rate", ")", "for", "_", "in", "range", "(", "2", ")", "]", ")", "\n", "\n", "ff_func", "=", "[", "]", "\n", "if", "n_feedforward", "==", "1", ":", "\n", "            ", "ff_func", ".", "append", "(", "GVP", "(", "node_dims", ",", "node_dims", ",", "activations", "=", "(", "None", ",", "None", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "hid_dims", "=", "4", "*", "node_dims", "[", "0", "]", ",", "2", "*", "node_dims", "[", "1", "]", "\n", "ff_func", ".", "append", "(", "GVP", "(", "node_dims", ",", "hid_dims", ",", "vector_gate", "=", "vector_gate", ")", ")", "\n", "for", "i", "in", "range", "(", "n_feedforward", "-", "2", ")", ":", "\n", "                ", "ff_func", ".", "append", "(", "GVP", "(", "hid_dims", ",", "hid_dims", ",", "vector_gate", "=", "vector_gate", ")", ")", "\n", "", "ff_func", ".", "append", "(", "GVP", "(", "hid_dims", ",", "node_dims", ",", "activations", "=", "(", "None", ",", "None", ")", ")", ")", "\n", "", "self", ".", "ff_func", "=", "nn", ".", "Sequential", "(", "*", "ff_func", ")", "\n", "\n", "self", ".", "edge_message_func", "=", "None", "\n", "if", "n_edge_gvps", ">", "0", ":", "\n", "            ", "si", ",", "vi", "=", "node_dims", "\n", "se", ",", "ve", "=", "edge_dims", "\n", "module_list", "=", "[", "\n", "GVP", "(", "(", "2", "*", "si", "+", "se", ",", "2", "*", "vi", "+", "ve", ")", ",", "edge_dims", ",", "vector_gate", "=", "vector_gate", ")", "\n", "]", "\n", "for", "i", "in", "range", "(", "n_edge_gvps", "-", "2", ")", ":", "\n", "                ", "module_list", ".", "append", "(", "GVP", "(", "edge_dims", ",", "edge_dims", ",", "\n", "vector_gate", "=", "vector_gate", ")", ")", "\n", "", "if", "n_edge_gvps", ">", "1", ":", "\n", "                ", "module_list", ".", "append", "(", "GVP", "(", "edge_dims", ",", "edge_dims", ",", "\n", "activations", "=", "(", "None", ",", "None", ")", ")", ")", "\n", "", "self", ".", "edge_message_func", "=", "nn", ".", "Sequential", "(", "*", "module_list", ")", "\n", "if", "layernorm", ":", "\n", "                ", "self", ".", "edge_norm", "=", "LayerNorm", "(", "edge_dims", ",", "eps", "=", "eps", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "edge_norm", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "edge_dropout", "=", "Dropout", "(", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.GVPConvLayer.forward": [[388, 458], ["gvp_modules.GVPConvLayer.ff_func", "gvp_modules.GVPConvLayer.edge_message_func", "gvp_modules.GVPConvLayer.edge_norm", "gvp_modules.tuple_index", "gvp_modules.tuple_index", "gvp_modules.tuple_sum", "torch_scatter.scatter_add().clamp().unsqueeze", "gvp_modules.GVPConvLayer.conv", "gvp_modules.tuple_sum", "gvp_modules.tuple_sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "gvp_modules.tuple_sum", "gvp_modules.GVPConvLayer.conv", "gvp_modules.GVPConvLayer.conv", "gvp_modules.tuple_index", "gvp_modules.tuple_index", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "gvp_modules.GVPConvLayer.edge_dropout", "torch_scatter.scatter_add().clamp", "torch_scatter.scatter_add().clamp().unsqueeze.unsqueeze", "mask.unsqueeze", "torch_scatter.scatter_add", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "dh[].size"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_index", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_index", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_sum", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_sum", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_sum", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_sum", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_index", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_index"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "edge_index", ",", "edge_attr", ",", "\n", "autoregressive_x", "=", "None", ",", "node_mask", "=", "None", ")", ":", "\n", "        ", "'''\n        :param x: tuple (s, V) of `torch.Tensor`\n        :param edge_index: array of shape [2, n_edges]\n        :param edge_attr: tuple (s, V) of `torch.Tensor`\n        :param autoregressive_x: tuple (s, V) of `torch.Tensor`. \n                If not `None`, will be used as srcqq node embeddings\n                for forming messages where src >= dst. The corrent node \n                embeddings `x` will still be the base of the update and the \n                pointwise feedforward.\n        :param node_mask: array of type `bool` to index into the first\n                dim of node embeddings (s, V). If not `None`, only\n                these nodes will be updated.\n        '''", "\n", "if", "self", ".", "edge_message_func", ":", "\n", "            ", "src", ",", "dst", "=", "edge_index", "\n", "if", "autoregressive_x", "is", "None", ":", "\n", "                ", "x_src", "=", "x", "[", "0", "]", "[", "src", "]", ",", "x", "[", "1", "]", "[", "src", "]", "\n", "", "else", ":", "\n", "                ", "mask", "=", "(", "src", "<", "dst", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "x_src", "=", "(", "\n", "torch", ".", "where", "(", "mask", ",", "x", "[", "0", "]", "[", "src", "]", ",", "autoregressive_x", "[", "0", "]", "[", "src", "]", ")", ",", "\n", "torch", ".", "where", "(", "mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "x", "[", "1", "]", "[", "src", "]", ",", "\n", "autoregressive_x", "[", "1", "]", "[", "src", "]", ")", "\n", ")", "\n", "", "x_dst", "=", "x", "[", "0", "]", "[", "dst", "]", ",", "x", "[", "1", "]", "[", "dst", "]", "\n", "x_edge", "=", "(", "\n", "torch", ".", "cat", "(", "[", "x_src", "[", "0", "]", ",", "edge_attr", "[", "0", "]", ",", "x_dst", "[", "0", "]", "]", ",", "dim", "=", "-", "1", ")", ",", "\n", "torch", ".", "cat", "(", "[", "x_src", "[", "1", "]", ",", "edge_attr", "[", "1", "]", ",", "x_dst", "[", "1", "]", "]", ",", "dim", "=", "-", "2", ")", "\n", ")", "\n", "edge_attr_dh", "=", "self", ".", "edge_message_func", "(", "x_edge", ")", "\n", "edge_attr", "=", "self", ".", "edge_norm", "(", "tuple_sum", "(", "edge_attr", ",", "\n", "self", ".", "edge_dropout", "(", "edge_attr_dh", ")", ")", ")", "\n", "\n", "", "if", "autoregressive_x", "is", "not", "None", ":", "\n", "            ", "src", ",", "dst", "=", "edge_index", "\n", "mask", "=", "src", "<", "dst", "\n", "edge_index_forward", "=", "edge_index", "[", ":", ",", "mask", "]", "\n", "edge_index_backward", "=", "edge_index", "[", ":", ",", "~", "mask", "]", "\n", "edge_attr_forward", "=", "tuple_index", "(", "edge_attr", ",", "mask", ")", "\n", "edge_attr_backward", "=", "tuple_index", "(", "edge_attr", ",", "~", "mask", ")", "\n", "\n", "dh", "=", "tuple_sum", "(", "\n", "self", ".", "conv", "(", "x", ",", "edge_index_forward", ",", "edge_attr_forward", ")", ",", "\n", "self", ".", "conv", "(", "autoregressive_x", ",", "edge_index_backward", ",", "edge_attr_backward", ")", "\n", ")", "\n", "\n", "count", "=", "scatter_add", "(", "torch", ".", "ones_like", "(", "dst", ")", ",", "dst", ",", "\n", "dim_size", "=", "dh", "[", "0", "]", ".", "size", "(", "0", ")", ")", ".", "clamp", "(", "min", "=", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "dh", "=", "dh", "[", "0", "]", "/", "count", ",", "dh", "[", "1", "]", "/", "count", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "", "else", ":", "\n", "            ", "dh", "=", "self", ".", "conv", "(", "x", ",", "edge_index", ",", "edge_attr", ")", "\n", "\n", "", "if", "node_mask", "is", "not", "None", ":", "\n", "            ", "x_", "=", "x", "\n", "x", ",", "dh", "=", "tuple_index", "(", "x", ",", "node_mask", ")", ",", "tuple_index", "(", "dh", ",", "node_mask", ")", "\n", "\n", "", "x", "=", "self", ".", "norm", "[", "0", "]", "(", "tuple_sum", "(", "x", ",", "self", ".", "dropout", "[", "0", "]", "(", "dh", ")", ")", ")", "\n", "\n", "dh", "=", "self", ".", "ff_func", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "[", "1", "]", "(", "tuple_sum", "(", "x", ",", "self", ".", "dropout", "[", "1", "]", "(", "dh", ")", ")", ")", "\n", "\n", "if", "node_mask", "is", "not", "None", ":", "\n", "            ", "x_", "[", "0", "]", "[", "node_mask", "]", ",", "x_", "[", "1", "]", "[", "node_mask", "]", "=", "x", "[", "0", "]", ",", "x", "[", "1", "]", "\n", "x", "=", "x_", "\n", "\n", "", "return", "x", ",", "edge_attr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_size": [[20, 22], ["tuple", "a.size"], "function", ["None"], ["def", "tuple_size", "(", "tp", ")", ":", "\n", "    ", "return", "tuple", "(", "[", "0", "if", "a", "is", "None", "else", "a", ".", "size", "(", ")", "for", "a", "in", "tp", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_sum": [[23, 29], ["None"], "function", ["None"], ["", "def", "tuple_sum", "(", "tp1", ",", "tp2", ")", ":", "\n", "    ", "s1", ",", "v1", "=", "tp1", "\n", "s2", ",", "v2", "=", "tp2", "\n", "if", "v2", "is", "None", "and", "v2", "is", "None", ":", "\n", "        ", "return", "(", "s1", "+", "s2", ",", "None", ")", "\n", "", "return", "(", "s1", "+", "s2", ",", "v1", "+", "v2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_cat": [[30, 42], ["len", "list", "zip", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "tuple_cat", "(", "*", "args", ",", "dim", "=", "-", "1", ")", ":", "\n", "    ", "'''\n    Concatenates any number of tuples (s, V) elementwise.\n    \n    :param dim: dimension along which to concatenate when viewed\n                as the `dim` index for the scalar-channel tensors.\n                This means that `dim=-1` will be applied as\n                `dim=-2` for the vector-channel tensors.\n    '''", "\n", "dim", "%=", "len", "(", "args", "[", "0", "]", "[", "0", "]", ".", "shape", ")", "\n", "s_args", ",", "v_args", "=", "list", "(", "zip", "(", "*", "args", ")", ")", "\n", "return", "torch", ".", "cat", "(", "s_args", ",", "dim", "=", "dim", ")", ",", "torch", ".", "cat", "(", "v_args", ",", "dim", "=", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.tuple_index": [[43, 50], ["None"], "function", ["None"], ["", "def", "tuple_index", "(", "x", ",", "idx", ")", ":", "\n", "    ", "'''\n    Indexes into a tuple (s, V) along the first dimension.\n    \n    :param idx: any object which can be used to index into a `torch.Tensor`\n    '''", "\n", "return", "x", "[", "0", "]", "[", "idx", "]", ",", "x", "[", "1", "]", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn": [[51, 63], ["torch.randn", "torch.randn", "torch.randn", "torch.randn"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn"], ["", "def", "randn", "(", "n", ",", "dims", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "    ", "'''\n    Returns random tuples (s, V) drawn elementwise from a normal distribution.\n    \n    :param n: number of data points\n    :param dims: tuple of dimensions (n_scalar, n_vector)\n    \n    :return: (s, V) with s.shape = (n, n_scalar) and\n             V.shape = (n, n_vector, 3)\n    '''", "\n", "return", "torch", ".", "randn", "(", "n", ",", "dims", "[", "0", "]", ",", "device", "=", "device", ")", ",", "torch", ".", "randn", "(", "n", ",", "dims", "[", "1", "]", ",", "3", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._norm_no_nan": [[64, 74], ["torch.sum", "torch.sum", "torch.sqrt", "torch.sqrt", "torch.square", "torch.square"], "function", ["None"], ["", "def", "_norm_no_nan", "(", "x", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "False", ",", "eps", "=", "1e-8", ",", "sqrt", "=", "True", ")", ":", "\n", "    ", "'''\n    L2 norm of tensor clamped above a minimum value `eps`.\n    \n    :param sqrt: if `False`, returns the square of the L2 norm\n    '''", "\n", "# clamp is slow", "\n", "# out = torch.clamp(torch.sum(torch.square(x), axis, keepdims), min=eps)", "\n", "out", "=", "torch", ".", "sum", "(", "torch", ".", "square", "(", "x", ")", ",", "axis", ",", "keepdims", ")", "+", "eps", "\n", "return", "torch", ".", "sqrt", "(", "out", ")", "if", "sqrt", "else", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._split": [[75, 87], ["torch.reshape", "torch.reshape"], "function", ["None"], ["", "def", "_split", "(", "x", ",", "nv", ")", ":", "\n", "    ", "'''\n    Splits a merged representation of (s, V) back into a tuple. \n    Should be used only with `_merge(s, V)` and only if the tuple \n    representation cannot be used.\n    \n    :param x: the `torch.Tensor` returned from `_merge`\n    :param nv: the number of vector channels in the input to `_merge`\n    '''", "\n", "v", "=", "torch", ".", "reshape", "(", "x", "[", "...", ",", "-", "3", "*", "nv", ":", "]", ",", "x", ".", "shape", "[", ":", "-", "1", "]", "+", "(", "nv", ",", "3", ")", ")", "\n", "s", "=", "x", "[", "...", ",", ":", "-", "3", "*", "nv", "]", "\n", "return", "s", ",", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules._merge": [[88, 97], ["torch.reshape", "torch.reshape", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "_merge", "(", "s", ",", "v", ")", ":", "\n", "    ", "'''\n    Merges a tuple (s, V) into a single `torch.Tensor`, where the\n    vector channels are flattened and appended to the scalar channels.\n    Should be used only if the tuple representation cannot be used.\n    Use `_split(x, nv)` to reverse.\n    '''", "\n", "v", "=", "torch", ".", "reshape", "(", "v", ",", "v", ".", "shape", "[", ":", "-", "2", "]", "+", "(", "3", "*", "v", ".", "shape", "[", "-", "2", "]", ",", ")", ")", "\n", "return", "torch", ".", "cat", "(", "[", "s", ",", "v", "]", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_encoder.GVPEncoder.__init__": [[20, 45], ["torch.Module.__init__", "features.GVPGraphEmbedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "gvp_modules.GVPConvLayer", "range"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "embed_graph", "=", "GVPGraphEmbedding", "(", "args", ")", "\n", "\n", "node_hidden_dim", "=", "(", "args", ".", "node_hidden_dim_scalar", ",", "\n", "args", ".", "node_hidden_dim_vector", ")", "\n", "edge_hidden_dim", "=", "(", "args", ".", "edge_hidden_dim_scalar", ",", "\n", "args", ".", "edge_hidden_dim_vector", ")", "\n", "\n", "conv_activations", "=", "(", "F", ".", "relu", ",", "torch", ".", "sigmoid", ")", "\n", "self", ".", "encoder_layers", "=", "nn", ".", "ModuleList", "(", "\n", "GVPConvLayer", "(", "\n", "node_hidden_dim", ",", "\n", "edge_hidden_dim", ",", "\n", "drop_rate", "=", "args", ".", "dropout", ",", "\n", "vector_gate", "=", "True", ",", "\n", "attention_heads", "=", "0", ",", "\n", "n_message", "=", "3", ",", "\n", "conv_activations", "=", "conv_activations", ",", "\n", "n_edge_gvps", "=", "0", ",", "\n", "eps", "=", "1e-4", ",", "\n", "layernorm", "=", "True", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "num_encoder_layers", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_encoder.GVPEncoder.forward": [[47, 57], ["gvp_encoder.GVPEncoder.embed_graph", "enumerate", "gvp_utils.unflatten_graph", "layer"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_utils.unflatten_graph"], ["", "def", "forward", "(", "self", ",", "coords", ",", "coord_mask", ",", "padding_mask", ",", "confidence", ")", ":", "\n", "        ", "node_embeddings", ",", "edge_embeddings", ",", "edge_index", "=", "self", ".", "embed_graph", "(", "\n", "coords", ",", "coord_mask", ",", "padding_mask", ",", "confidence", ")", "\n", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "encoder_layers", ")", ":", "\n", "            ", "node_embeddings", ",", "edge_embeddings", "=", "layer", "(", "node_embeddings", ",", "\n", "edge_index", ",", "edge_embeddings", ")", "\n", "\n", "", "node_embeddings", "=", "unflatten_graph", "(", "node_embeddings", ",", "coords", ".", "shape", "[", "0", "]", ")", "\n", "return", "node_embeddings", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.__init__": [[37, 80], ["torch.Module.__init__", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.Dropout", "torch.Dropout", "math.sqrt", "esm.modules.SinusoidalPositionalEmbedding", "torch.ModuleList", "torch.ModuleList", "transformer_decoder.TransformerDecoder.layers.extend", "len", "torch.LayerNorm", "torch.LayerNorm", "transformer_decoder.TransformerDecoder.build_output_projection", "torch.Linear", "torch.Linear", "transformer_decoder.TransformerDecoder.build_decoder_layer", "range"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.build_output_projection", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.build_decoder_layer"], ["def", "__init__", "(", "\n", "self", ",", "\n", "args", ",", "\n", "dictionary", ",", "\n", "embed_tokens", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "_future_mask", "=", "torch", ".", "empty", "(", "0", ")", "\n", "\n", "self", ".", "dropout_module", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n", "input_embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "\n", "\n", "self", ".", "project_in_dim", "=", "(", "\n", "nn", ".", "Linear", "(", "input_embed_dim", ",", "embed_dim", ",", "bias", "=", "False", ")", "\n", "if", "embed_dim", "!=", "input_embed_dim", "\n", "else", "None", "\n", ")", "\n", "self", ".", "embed_positions", "=", "SinusoidalPositionalEmbedding", "(", "\n", "embed_dim", ",", "\n", "self", ".", "padding_idx", ",", "\n", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "\n", "[", "\n", "self", ".", "build_decoder_layer", "(", "args", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "decoder_layers", ")", "\n", "]", "\n", ")", "\n", "self", ".", "num_layers", "=", "len", "(", "self", ".", "layers", ")", "\n", "self", ".", "layer_norm", "=", "nn", ".", "LayerNorm", "(", "embed_dim", ")", "\n", "\n", "self", ".", "build_output_projection", "(", "args", ",", "dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.build_output_projection": [[81, 87], ["torch.Linear", "torch.Linear", "torch.init.normal_", "torch.init.normal_", "len"], "methods", ["None"], ["", "def", "build_output_projection", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "self", ".", "output_projection", "=", "nn", ".", "Linear", "(", "\n", "args", ".", "decoder_embed_dim", ",", "len", "(", "dictionary", ")", ",", "bias", "=", "False", "\n", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "\n", "self", ".", "output_projection", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "args", ".", "decoder_embed_dim", "**", "-", "0.5", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.build_decoder_layer": [[89, 91], ["transformer_layer.TransformerDecoderLayer"], "methods", ["None"], ["", "def", "build_decoder_layer", "(", "self", ",", "args", ")", ":", "\n", "        ", "return", "TransformerDecoderLayer", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.forward": [[92, 127], ["transformer_decoder.TransformerDecoder.extract_features", "transformer_decoder.TransformerDecoder.transpose", "transformer_decoder.TransformerDecoder.output_layer"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.extract_features", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.output_layer"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "prev_output_tokens", ",", "\n", "encoder_out", ":", "Optional", "[", "Dict", "[", "str", ",", "List", "[", "Tensor", "]", "]", "]", "=", "None", ",", "\n", "incremental_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", "]", "=", "None", ",", "\n", "features_only", ":", "bool", "=", "False", ",", "\n", "return_all_hiddens", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (optional): output from the encoder, used for\n                encoder-side attention, should be of size T x B x C\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n            features_only (bool, optional): only return features without\n                applying output layer (default: False).\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "\n", "x", ",", "extra", "=", "self", ".", "extract_features", "(", "\n", "prev_output_tokens", ",", "\n", "encoder_out", "=", "encoder_out", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "\n", "\n", "if", "not", "features_only", ":", "\n", "            ", "x", "=", "self", ".", "output_layer", "(", "x", ")", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "# B x T x C -> B x C x T", "\n", "return", "x", ",", "extra", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.extract_features": [[128, 211], ["prev_output_tokens.size", "transformer_decoder.TransformerDecoder.embed_positions", "transformer_decoder.TransformerDecoder.dropout_module", "transformer_decoder.TransformerDecoder.transpose", "prev_output_tokens.eq().any", "enumerate", "transformer_decoder.TransformerDecoder.transpose", "transformer_decoder.TransformerDecoder.embed_tokens", "transformer_decoder.TransformerDecoder.project_in_dim", "prev_output_tokens.eq", "layer", "inner_states.append", "transformer_decoder.TransformerDecoder.layer_norm", "len", "len", "prev_output_tokens.eq", "transformer_decoder.TransformerDecoder.buffered_future_mask", "enc.size"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.buffered_future_mask"], ["", "def", "extract_features", "(", "\n", "self", ",", "\n", "prev_output_tokens", ",", "\n", "encoder_out", ":", "Optional", "[", "Dict", "[", "str", ",", "List", "[", "Tensor", "]", "]", "]", ",", "\n", "incremental_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Similar to *forward* but only return features.\n\n        Includes several features from \"Jointly Learning to Align and\n        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "bs", ",", "slen", "=", "prev_output_tokens", ".", "size", "(", ")", "\n", "\n", "enc", ":", "Optional", "[", "Tensor", "]", "=", "None", "\n", "padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", "\n", "if", "encoder_out", "is", "not", "None", "and", "len", "(", "encoder_out", "[", "\"encoder_out\"", "]", ")", ">", "0", ":", "\n", "            ", "enc", "=", "encoder_out", "[", "\"encoder_out\"", "]", "[", "0", "]", "\n", "assert", "(", "\n", "enc", ".", "size", "(", ")", "[", "1", "]", "==", "bs", "\n", ")", ",", "f\"Expected enc.shape == (t, {bs}, c) got {enc.shape}\"", "\n", "", "if", "encoder_out", "is", "not", "None", "and", "len", "(", "encoder_out", "[", "\"encoder_padding_mask\"", "]", ")", ">", "0", ":", "\n", "            ", "padding_mask", "=", "encoder_out", "[", "\"encoder_padding_mask\"", "]", "[", "0", "]", "\n", "\n", "# embed positions", "\n", "", "positions", "=", "self", ".", "embed_positions", "(", "\n", "prev_output_tokens", "\n", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "positions", "=", "positions", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# embed tokens and positions", "\n", "", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "\n", "if", "self", ".", "project_in_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_in_dim", "(", "x", ")", "\n", "\n", "", "x", "+=", "positions", "\n", "\n", "x", "=", "self", ".", "dropout_module", "(", "x", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "self_attn_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", "\n", "if", "prev_output_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", ".", "any", "(", ")", ":", "\n", "            ", "self_attn_padding_mask", "=", "prev_output_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "\n", "# decoder layers", "\n", "", "attn", ":", "Optional", "[", "Tensor", "]", "=", "None", "\n", "inner_states", ":", "List", "[", "Optional", "[", "Tensor", "]", "]", "=", "[", "x", "]", "\n", "for", "idx", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "self_attn_mask", "=", "self", ".", "buffered_future_mask", "(", "x", ")", "\n", "", "else", ":", "\n", "                ", "self_attn_mask", "=", "None", "\n", "\n", "", "x", ",", "layer_attn", ",", "_", "=", "layer", "(", "\n", "x", ",", "\n", "enc", ",", "\n", "padding_mask", ",", "\n", "incremental_state", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "self_attn_padding_mask", "=", "self_attn_padding_mask", ",", "\n", "need_attn", "=", "False", ",", "\n", "need_head_weights", "=", "False", ",", "\n", ")", "\n", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "layer_norm", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# T x B x C -> B x C x T", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "return", "x", ",", "{", "\"inner_states\"", ":", "inner_states", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.output_layer": [[212, 215], ["transformer_decoder.TransformerDecoder.output_projection"], "methods", ["None"], ["", "def", "output_layer", "(", "self", ",", "features", ")", ":", "\n", "        ", "\"\"\"Project features to the vocabulary size.\"\"\"", "\n", "return", "self", ".", "output_projection", "(", "features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.TransformerDecoder.buffered_future_mask": [[216, 229], ["tensor.size", "transformer_decoder.TransformerDecoder._future_mask.to", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "transformer_decoder.TransformerDecoder._future_mask.size", "transformer_decoder.TransformerDecoder._future_mask.size", "transformer_decoder.fill_with_neg_inf", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.fill_with_neg_inf"], ["", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "# self._future_mask.device != tensor.device is not working in TorchScript. This is a workaround.", "\n", "if", "(", "\n", "self", ".", "_future_mask", ".", "size", "(", "0", ")", "==", "0", "\n", "or", "(", "not", "self", ".", "_future_mask", ".", "device", "==", "tensor", ".", "device", ")", "\n", "or", "self", ".", "_future_mask", ".", "size", "(", "0", ")", "<", "dim", "\n", ")", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "\n", "fill_with_neg_inf", "(", "torch", ".", "zeros", "(", "[", "dim", ",", "dim", "]", ")", ")", ",", "1", "\n", ")", "\n", "", "self", ".", "_future_mask", "=", "self", ".", "_future_mask", ".", "to", "(", "tensor", ")", "\n", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_decoder.fill_with_neg_inf": [[19, 22], ["t.float().fill_().type_as", "t.float().fill_", "float", "t.float"], "function", ["None"], ["def", "fill_with_neg_inf", "(", "t", ")", ":", "\n", "    ", "\"\"\"FP16-compatible function that fills a tensor with -inf.\"\"\"", "\n", "return", "t", ".", "float", "(", ")", ".", "fill_", "(", "float", "(", "\"-inf\"", ")", ")", ".", "type_as", "(", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.CoordBatchConverter.__call__": [[212, 259], ["util.CoordBatchConverter.alphabet.get_idx", "super().__call__", "util.CoordBatchConverter.collate_dense_tensors", "util.CoordBatchConverter.collate_dense_tensors", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "batch.append", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "coords.to.to.to", "confidence.to.to.to", "tokens.to.to.to", "coords.to.to.sum().sum", "isinstance", "isinstance", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len", "coords.to.to.sum", "float"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.MSABatchConverter.__call__", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.CoordBatchConverter.collate_dense_tensors", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.CoordBatchConverter.collate_dense_tensors"], ["    ", "def", "__call__", "(", "self", ",", "raw_batch", ":", "Sequence", "[", "Tuple", "[", "Sequence", ",", "str", "]", "]", ",", "device", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            raw_batch: List of tuples (coords, confidence, seq)\n            In each tuple,\n                coords: list of floats, shape L x 3 x 3\n                confidence: list of floats, shape L; or scalar float; or None\n                seq: string of length L\n        Returns:\n            coords: Tensor of shape batch_size x L x 3 x 3\n            confidence: Tensor of shape batch_size x L\n            strs: list of strings\n            tokens: LongTensor of shape batch_size x L\n            padding_mask: ByteTensor of shape batch_size x L\n        \"\"\"", "\n", "self", ".", "alphabet", ".", "cls_idx", "=", "self", ".", "alphabet", ".", "get_idx", "(", "\"<cath>\"", ")", "\n", "batch", "=", "[", "]", "\n", "for", "coords", ",", "confidence", ",", "seq", "in", "raw_batch", ":", "\n", "            ", "if", "confidence", "is", "None", ":", "\n", "                ", "confidence", "=", "1.", "\n", "", "if", "isinstance", "(", "confidence", ",", "float", ")", "or", "isinstance", "(", "confidence", ",", "int", ")", ":", "\n", "                ", "confidence", "=", "[", "float", "(", "confidence", ")", "]", "*", "len", "(", "coords", ")", "\n", "", "if", "seq", "is", "None", ":", "\n", "                ", "seq", "=", "'X'", "*", "len", "(", "coords", ")", "\n", "", "batch", ".", "append", "(", "(", "(", "coords", ",", "confidence", ")", ",", "seq", ")", ")", "\n", "\n", "", "coords_and_confidence", ",", "strs", ",", "tokens", "=", "super", "(", ")", ".", "__call__", "(", "batch", ")", "\n", "\n", "# pad beginning and end of each protein due to legacy reasons", "\n", "coords", "=", "[", "\n", "F", ".", "pad", "(", "torch", ".", "tensor", "(", "cd", ")", ",", "(", "0", ",", "0", ",", "0", ",", "0", ",", "1", ",", "1", ")", ",", "value", "=", "np", ".", "inf", ")", "\n", "for", "cd", ",", "_", "in", "coords_and_confidence", "\n", "]", "\n", "confidence", "=", "[", "\n", "F", ".", "pad", "(", "torch", ".", "tensor", "(", "cf", ")", ",", "(", "1", ",", "1", ")", ",", "value", "=", "-", "1.", ")", "\n", "for", "_", ",", "cf", "in", "coords_and_confidence", "\n", "]", "\n", "coords", "=", "self", ".", "collate_dense_tensors", "(", "coords", ",", "pad_v", "=", "np", ".", "nan", ")", "\n", "confidence", "=", "self", ".", "collate_dense_tensors", "(", "confidence", ",", "pad_v", "=", "-", "1.", ")", "\n", "if", "device", "is", "not", "None", ":", "\n", "            ", "coords", "=", "coords", ".", "to", "(", "device", ")", "\n", "confidence", "=", "confidence", ".", "to", "(", "device", ")", "\n", "tokens", "=", "tokens", ".", "to", "(", "device", ")", "\n", "", "padding_mask", "=", "torch", ".", "isnan", "(", "coords", "[", ":", ",", ":", ",", "0", ",", "0", "]", ")", "\n", "coord_mask", "=", "torch", ".", "isfinite", "(", "coords", ".", "sum", "(", "-", "2", ")", ".", "sum", "(", "-", "1", ")", ")", "\n", "confidence", "=", "confidence", "*", "coord_mask", "+", "(", "-", "1.", ")", "*", "padding_mask", "\n", "return", "coords", ",", "confidence", ",", "strs", ",", "tokens", ",", "padding_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.CoordBatchConverter.from_lists": [[260, 286], ["len", "zip", "util.CoordBatchConverter.__call__"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.MSABatchConverter.__call__"], ["", "def", "from_lists", "(", "self", ",", "coords_list", ",", "confidence_list", "=", "None", ",", "seq_list", "=", "None", ",", "device", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            coords_list: list of length batch_size, each item is a list of\n            floats in shape L x 3 x 3 to describe a backbone\n            confidence_list: one of\n                - None, default to highest confidence\n                - list of length batch_size, each item is a scalar\n                - list of length batch_size, each item is a list of floats of\n                    length L to describe the confidence scores for the backbone\n                    with values between 0. and 1.\n            seq_list: either None or a list of strings\n        Returns:\n            coords: Tensor of shape batch_size x L x 3 x 3\n            confidence: Tensor of shape batch_size x L\n            strs: list of strings\n            tokens: LongTensor of shape batch_size x L\n            padding_mask: ByteTensor of shape batch_size x L\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "coords_list", ")", "\n", "if", "confidence_list", "is", "None", ":", "\n", "            ", "confidence_list", "=", "[", "None", "]", "*", "batch_size", "\n", "", "if", "seq_list", "is", "None", ":", "\n", "            ", "seq_list", "=", "[", "None", "]", "*", "batch_size", "\n", "", "raw_batch", "=", "zip", "(", "coords_list", ",", "confidence_list", ",", "seq_list", ")", "\n", "return", "self", ".", "__call__", "(", "raw_batch", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.CoordBatchConverter.collate_dense_tensors": [[287, 315], ["tuple", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty.fill_", "torch.empty.fill_", "torch.empty.fill_", "torch.empty.fill_", "range", "len", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "len", "RuntimeError", "set", "max", "len", "len", "set", "zip", "tuple", "x.dim", "x.dim", "slice"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collate_dense_tensors", "(", "samples", ",", "pad_v", ")", ":", "\n", "        ", "\"\"\"\n        Takes a list of tensors with the following dimensions:\n            [(d_11,       ...,           d_1K),\n             (d_21,       ...,           d_2K),\n             ...,\n             (d_N1,       ...,           d_NK)]\n        and stack + pads them into a single tensor of:\n        (N, max_i=1,N { d_i1 }, ..., max_i=1,N {diK})\n        \"\"\"", "\n", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "            ", "return", "torch", ".", "Tensor", "(", ")", "\n", "", "if", "len", "(", "set", "(", "x", ".", "dim", "(", ")", "for", "x", "in", "samples", ")", ")", "!=", "1", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "f\"Samples has varying dimensions: {[x.dim() for x in samples]}\"", "\n", ")", "\n", "", "(", "device", ",", ")", "=", "tuple", "(", "set", "(", "x", ".", "device", "for", "x", "in", "samples", ")", ")", "# assumes all on same device", "\n", "max_shape", "=", "[", "max", "(", "lst", ")", "for", "lst", "in", "zip", "(", "*", "[", "x", ".", "shape", "for", "x", "in", "samples", "]", ")", "]", "\n", "result", "=", "torch", ".", "empty", "(", "\n", "len", "(", "samples", ")", ",", "*", "max_shape", ",", "dtype", "=", "samples", "[", "0", "]", ".", "dtype", ",", "device", "=", "device", "\n", ")", "\n", "result", ".", "fill_", "(", "pad_v", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "samples", ")", ")", ":", "\n", "            ", "result_i", "=", "result", "[", "i", "]", "\n", "t", "=", "samples", "[", "i", "]", "\n", "result_i", "[", "tuple", "(", "slice", "(", "0", ",", "k", ")", "for", "k", "in", "t", ".", "shape", ")", "]", "=", "t", "\n", "", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.load_structure": [[27, 56], ["fpath.endswith", "biotite.structure.filter_backbone", "biotite.structure.get_chains", "print", "print", "biotite.structure.io.pdbx.get_structure", "fpath.endswith", "len", "ValueError", "ValueError", "open", "biotite.structure.io.pdbx.PDBxFile.read", "biotite.structure.io.pdb.get_structure", "open", "biotite.structure.io.pdb.PDBFile.read", "len"], "function", ["None"], ["def", "load_structure", "(", "fpath", ",", "chain", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        fpath: filepath to either pdb or cif file\n        chain: the chain id\n    Returns:\n        biotite.structure.AtomArray\n    \"\"\"", "\n", "if", "fpath", ".", "endswith", "(", "'cif'", ")", ":", "\n", "        ", "with", "open", "(", "fpath", ")", "as", "fin", ":", "\n", "            ", "pdbxf", "=", "pdbx", ".", "PDBxFile", ".", "read", "(", "fin", ")", "\n", "", "structure", "=", "pdbx", ".", "get_structure", "(", "pdbxf", ",", "model", "=", "1", ")", "\n", "", "elif", "fpath", ".", "endswith", "(", "'pdb'", ")", ":", "\n", "        ", "with", "open", "(", "fpath", ")", "as", "fin", ":", "\n", "            ", "pdbf", "=", "pdb", ".", "PDBFile", ".", "read", "(", "fin", ")", "\n", "", "structure", "=", "pdb", ".", "get_structure", "(", "pdbf", ",", "model", "=", "1", ")", "\n", "", "bbmask", "=", "filter_backbone", "(", "structure", ")", "\n", "structure", "=", "structure", "[", "bbmask", "]", "\n", "chains", "=", "get_chains", "(", "structure", ")", "\n", "print", "(", "f'Found {len(chains)} chains:'", ",", "chains", ",", "'\\n'", ")", "\n", "if", "len", "(", "chains", ")", "==", "0", ":", "\n", "        ", "raise", "ValueError", "(", "'No chains found in the input file.'", ")", "\n", "", "if", "chain", "is", "None", ":", "\n", "        ", "chain", "=", "chains", "[", "0", "]", "\n", "", "if", "chain", "not", "in", "chains", ":", "\n", "        ", "raise", "ValueError", "(", "f'Chain {chain} not found in input file'", ")", "\n", "", "structure", "=", "structure", "[", "structure", ".", "chain_id", "==", "chain", "]", "\n", "print", "(", "f'Loaded chain {chain}\\n'", ")", "\n", "return", "structure", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.extract_coords_from_structure": [[58, 71], ["util.get_atom_coords_residuewise", "biotite.structure.residues.get_residues", "biotite.sequence.ProteinSequence.convert_letter_3to1"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.get_atom_coords_residuewise"], ["", "def", "extract_coords_from_structure", "(", "structure", ":", "biotite", ".", "structure", ".", "AtomArray", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        structure: An instance of biotite AtomArray\n    Returns:\n        Tuple (coords, seq)\n            - coords is an L x 3 x 3 array for N, CA, C coordinates\n            - seq is the extracted sequence\n    \"\"\"", "\n", "coords", "=", "get_atom_coords_residuewise", "(", "[", "\"N\"", ",", "\"CA\"", ",", "\"C\"", "]", ",", "structure", ")", "\n", "residue_identities", "=", "get_residues", "(", "structure", ")", "[", "1", "]", "\n", "seq", "=", "''", ".", "join", "(", "[", "ProteinSequence", ".", "convert_letter_3to1", "(", "r", ")", "for", "r", "in", "residue_identities", "]", ")", "\n", "return", "coords", ",", "seq", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.load_coords": [[73, 85], ["util.load_structure", "util.extract_coords_from_structure"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.load_structure", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.extract_coords_from_structure"], ["", "def", "load_coords", "(", "fpath", ",", "chain", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        fpath: filepath to either pdb or cif file\n        chain: the chain id\n    Returns:\n        Tuple (coords, seq)\n            - coords is an L x 3 x 3 array for N, CA, C coordinates\n            - seq is the extracted sequence\n    \"\"\"", "\n", "structure", "=", "load_structure", "(", "fpath", ",", "chain", ")", "\n", "return", "extract_coords_from_structure", "(", "structure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.get_atom_coords_residuewise": [[87, 102], ["biotite.structure.apply_residue_wise", "numpy.stack", "np.stack.sum", "np.stack.argmax", "float", "numpy.all", "RuntimeError", "numpy.ones"], "function", ["None"], ["", "def", "get_atom_coords_residuewise", "(", "atoms", ":", "List", "[", "str", "]", ",", "struct", ":", "biotite", ".", "structure", ".", "AtomArray", ")", ":", "\n", "    ", "\"\"\"\n    Example for atoms argument: [\"N\", \"CA\", \"C\"]\n    \"\"\"", "\n", "def", "filterfn", "(", "s", ",", "axis", "=", "None", ")", ":", "\n", "        ", "filters", "=", "np", ".", "stack", "(", "[", "s", ".", "atom_name", "==", "name", "for", "name", "in", "atoms", "]", ",", "axis", "=", "1", ")", "\n", "sum", "=", "filters", ".", "sum", "(", "0", ")", "\n", "if", "not", "np", ".", "all", "(", "sum", "<=", "np", ".", "ones", "(", "filters", ".", "shape", "[", "1", "]", ")", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"structure has multiple atoms with same name\"", ")", "\n", "", "index", "=", "filters", ".", "argmax", "(", "0", ")", "\n", "coords", "=", "s", "[", "index", "]", ".", "coord", "\n", "coords", "[", "sum", "==", "0", "]", "=", "float", "(", "\"nan\"", ")", "\n", "return", "coords", "\n", "\n", "", "return", "biotite", ".", "structure", ".", "apply_residue_wise", "(", "struct", ",", "struct", ",", "filterfn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.score_sequence": [[104, 124], ["util.CoordBatchConverter", "CoordBatchConverter.", "model.forward", "torch.cross_entropy", "torch.all", "torch.all", "torch.all", "torch.all", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "avgloss.detach().numpy().item", "torch.all", "torch.all", "torch.all", "torch.all", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "avgloss.detach().numpy().item", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite", "avgloss.detach().numpy", "avgloss.detach().numpy", "avgloss.detach", "avgloss.detach"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.forward"], ["", "def", "score_sequence", "(", "model", ",", "alphabet", ",", "coords", ",", "seq", ")", ":", "\n", "    ", "batch_converter", "=", "CoordBatchConverter", "(", "alphabet", ")", "\n", "batch", "=", "[", "(", "coords", ",", "None", ",", "seq", ")", "]", "\n", "coords", ",", "confidence", ",", "strs", ",", "tokens", ",", "padding_mask", "=", "batch_converter", "(", "batch", ")", "\n", "\n", "prev_output_tokens", "=", "tokens", "[", ":", ",", ":", "-", "1", "]", "\n", "target", "=", "tokens", "[", ":", ",", "1", ":", "]", "\n", "target_padding_mask", "=", "(", "target", "==", "alphabet", ".", "padding_idx", ")", "\n", "logits", ",", "_", "=", "model", ".", "forward", "(", "coords", ",", "padding_mask", ",", "confidence", ",", "prev_output_tokens", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "target", ",", "reduction", "=", "'none'", ")", "\n", "\n", "avgloss", "=", "torch", ".", "sum", "(", "loss", "*", "~", "target_padding_mask", ",", "dim", "=", "-", "1", ")", "/", "torch", ".", "sum", "(", "~", "target_padding_mask", ",", "dim", "=", "-", "1", ")", "\n", "ll_fullseq", "=", "-", "avgloss", ".", "detach", "(", ")", ".", "numpy", "(", ")", ".", "item", "(", ")", "\n", "\n", "coord_mask", "=", "torch", ".", "all", "(", "torch", ".", "all", "(", "torch", ".", "isfinite", "(", "coords", ")", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "coord_mask", "=", "coord_mask", "[", ":", ",", "1", ":", "-", "1", "]", "\n", "avgloss", "=", "torch", ".", "sum", "(", "loss", "*", "coord_mask", ",", "dim", "=", "-", "1", ")", "/", "torch", ".", "sum", "(", "coord_mask", ",", "dim", "=", "-", "1", ")", "\n", "ll_withcoord", "=", "-", "avgloss", ".", "detach", "(", ")", ".", "numpy", "(", ")", ".", "item", "(", ")", "\n", "\n", "return", "ll_fullseq", ",", "ll_withcoord", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.get_encoder_output": [[126, 135], ["util.CoordBatchConverter", "CoordBatchConverter.", "model.encoder.forward"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.forward"], ["", "def", "get_encoder_output", "(", "model", ",", "alphabet", ",", "coords", ")", ":", "\n", "    ", "batch_converter", "=", "CoordBatchConverter", "(", "alphabet", ")", "\n", "# the batch_converter is essential for forming the correct input format", "\n", "batch", "=", "[", "(", "coords", ",", "None", ",", "None", ")", "]", "\n", "coords", ",", "confidence", ",", "_", ",", "_", ",", "padding_mask", "=", "batch_converter", "(", "batch", ")", "\n", "encoder_out", "=", "model", ".", "encoder", ".", "forward", "(", "coords", ",", "padding_mask", ",", "confidence", ",", "\n", "return_all_hiddens", "=", "False", ")", "\n", "# remove beginning and end (bos and eos tokens)", "\n", "return", "encoder_out", "[", "'encoder_out'", "]", "[", "0", "]", "[", "1", ":", "-", "1", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.rotate": [[137, 151], ["R.unsqueeze.unsqueeze", "v.unsqueeze.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["None"], ["", "def", "rotate", "(", "v", ",", "R", ")", ":", "\n", "    ", "\"\"\"\n    Rotates a vector by a rotation matrix.\n    \n    Args:\n        v: 3D vector, tensor of shape (length x batch_size x channels x 3)\n        R: rotation matrix, tensor of shape (length x batch_size x 3 x 3)\n\n    Returns:\n        Rotated version of v by rotation matrix R.\n    \"\"\"", "\n", "R", "=", "R", ".", "unsqueeze", "(", "-", "3", ")", "\n", "v", "=", "v", ".", "unsqueeze", "(", "-", "1", ")", "\n", "return", "torch", ".", "sum", "(", "v", "*", "R", ",", "dim", "=", "-", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.get_rotation_frames": [[153, 172], ["util.normalize", "util.normalize", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize"], ["", "def", "get_rotation_frames", "(", "coords", ")", ":", "\n", "    ", "\"\"\"\n    Returns a local rotation frame defined by N, CA, C positions.\n\n    Args:\n        coords: coordinates, tensor of shape (batch_size x length x 3 x 3)\n        where the third dimension is in order of N, CA, C\n\n    Returns:\n        Local relative rotation frames in shape (batch_size x length x 3 x 3)\n    \"\"\"", "\n", "v1", "=", "coords", "[", ":", ",", ":", ",", "2", "]", "-", "coords", "[", ":", ",", ":", ",", "1", "]", "\n", "v2", "=", "coords", "[", ":", ",", ":", ",", "0", "]", "-", "coords", "[", ":", ",", ":", ",", "1", "]", "\n", "e1", "=", "normalize", "(", "v1", ",", "dim", "=", "-", "1", ")", "\n", "u2", "=", "v2", "-", "e1", "*", "torch", ".", "sum", "(", "e1", "*", "v2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "e2", "=", "normalize", "(", "u2", ",", "dim", "=", "-", "1", ")", "\n", "e3", "=", "torch", ".", "cross", "(", "e1", ",", "e2", ",", "dim", "=", "-", "1", ")", "\n", "R", "=", "torch", ".", "stack", "(", "[", "e1", ",", "e2", ",", "e3", "]", ",", "dim", "=", "-", "2", ")", "\n", "return", "R", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.nan_to_num": [[174, 180], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.where", "torch.where", "torch.where", "torch.where", "torch.isfinite", "torch.isfinite", "torch.isfinite", "torch.isfinite"], "function", ["None"], ["", "def", "nan_to_num", "(", "ts", ",", "val", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"\n    Replaces nans in tensor with a fixed value.    \n    \"\"\"", "\n", "val", "=", "torch", ".", "tensor", "(", "val", ",", "dtype", "=", "ts", ".", "dtype", ",", "device", "=", "ts", ".", "device", ")", "\n", "return", "torch", ".", "where", "(", "~", "torch", ".", "isfinite", "(", "ts", ")", ",", "val", ",", "ts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.rbf": [[182, 192], ["torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "rbf_centers.view.view", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "values.unsqueeze", "len"], "function", ["None"], ["", "def", "rbf", "(", "values", ",", "v_min", ",", "v_max", ",", "n_bins", "=", "16", ")", ":", "\n", "    ", "\"\"\"\n    Returns RBF encodings in a new dimension at the end.\n    \"\"\"", "\n", "rbf_centers", "=", "torch", ".", "linspace", "(", "v_min", ",", "v_max", ",", "n_bins", ",", "device", "=", "values", ".", "device", ")", "\n", "rbf_centers", "=", "rbf_centers", ".", "view", "(", "[", "1", "]", "*", "len", "(", "values", ".", "shape", ")", "+", "[", "-", "1", "]", ")", "\n", "rbf_std", "=", "(", "v_max", "-", "v_min", ")", "/", "n_bins", "\n", "v_expand", "=", "torch", ".", "unsqueeze", "(", "values", ",", "-", "1", ")", "\n", "z", "=", "(", "values", ".", "unsqueeze", "(", "-", "1", ")", "-", "rbf_centers", ")", "/", "rbf_std", "\n", "return", "torch", ".", "exp", "(", "-", "z", "**", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.norm": [[194, 200], ["torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.square", "torch.square", "torch.square", "torch.square"], "function", ["None"], ["", "def", "norm", "(", "tensor", ",", "dim", ",", "eps", "=", "1e-8", ",", "keepdim", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Returns L2 norm along a dimension.\n    \"\"\"", "\n", "return", "torch", ".", "sqrt", "(", "\n", "torch", ".", "sum", "(", "torch", ".", "square", "(", "tensor", ")", ",", "dim", "=", "dim", ",", "keepdim", "=", "keepdim", ")", "+", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize": [[202, 208], ["util.nan_to_num", "torch.div", "torch.div", "torch.div", "torch.div", "util.norm"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.nan_to_num", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.norm"], ["", "def", "normalize", "(", "tensor", ",", "dim", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"\n    Normalizes a tensor along a dimension after removing nans.\n    \"\"\"", "\n", "return", "nan_to_num", "(", "\n", "torch", ".", "div", "(", "tensor", ",", "norm", "(", "tensor", ",", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerEncoderLayer.__init__": [[25, 43], ["torch.Module.__init__", "transformer_layer.TransformerEncoderLayer.build_self_attention", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout", "transformer_layer.TransformerEncoderLayer.build_fc1", "transformer_layer.TransformerEncoderLayer.build_fc2", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_self_attention", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_fc1", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_fc2"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "embed_dim", "=", "args", ".", "encoder_embed_dim", "\n", "self", ".", "self_attn", "=", "self", ".", "build_self_attention", "(", "self", ".", "embed_dim", ",", "args", ")", "\n", "self", ".", "self_attn_layer_norm", "=", "torch", ".", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "dropout_module", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "self", ".", "activation_fn", "=", "F", ".", "relu", "\n", "self", ".", "fc1", "=", "self", ".", "build_fc1", "(", "\n", "self", ".", "embed_dim", ",", "\n", "args", ".", "encoder_ffn_embed_dim", ",", "\n", ")", "\n", "self", ".", "fc2", "=", "self", ".", "build_fc2", "(", "\n", "args", ".", "encoder_ffn_embed_dim", ",", "\n", "self", ".", "embed_dim", ",", "\n", ")", "\n", "\n", "self", ".", "final_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerEncoderLayer.build_fc1": [[44, 46], ["torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["None"], ["", "def", "build_fc1", "(", "self", ",", "input_dim", ",", "output_dim", ")", ":", "\n", "        ", "return", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerEncoderLayer.build_fc2": [[47, 49], ["torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["None"], ["", "def", "build_fc2", "(", "self", ",", "input_dim", ",", "output_dim", ")", ":", "\n", "        ", "return", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerEncoderLayer.build_self_attention": [[50, 56], ["esm.multihead_attention.MultiheadAttention"], "methods", ["None"], ["", "def", "build_self_attention", "(", "self", ",", "embed_dim", ",", "args", ")", ":", "\n", "        ", "return", "MultiheadAttention", "(", "\n", "embed_dim", ",", "\n", "args", ".", "encoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", "self_attention", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerEncoderLayer.residual_connection": [[58, 60], ["None"], "methods", ["None"], ["", "def", "residual_connection", "(", "self", ",", "x", ",", "residual", ")", ":", "\n", "        ", "return", "residual", "+", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerEncoderLayer.forward": [[61, 112], ["transformer_layer.TransformerEncoderLayer.self_attn_layer_norm", "transformer_layer.TransformerEncoderLayer.self_attn", "transformer_layer.TransformerEncoderLayer.dropout_module", "transformer_layer.TransformerEncoderLayer.residual_connection", "transformer_layer.TransformerEncoderLayer.final_layer_norm", "transformer_layer.TransformerEncoderLayer.activation_fn", "transformer_layer.TransformerEncoderLayer.fc2", "transformer_layer.TransformerEncoderLayer.dropout_module", "transformer_layer.TransformerEncoderLayer.residual_connection", "attn_mask.masked_fill.masked_fill.masked_fill", "transformer_layer.TransformerEncoderLayer.fc1", "attn_mask.masked_fill.masked_fill.to"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.residual_connection", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.residual_connection"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ",", "\n", "encoder_padding_mask", ":", "Optional", "[", "Tensor", "]", ",", "\n", "attn_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, seq_len)` where padding elements are indicated by ``1``.\n            attn_mask (ByteTensor): binary tensor of shape `(tgt_len, src_len)`,\n                where `tgt_len` is the length of output and `src_len` is the\n                length of input, though here both are equal to `seq_len`.\n                `attn_mask[tgt_i, src_j] = 1` means that when calculating the\n                embedding for `tgt_i`, we exclude (mask out) `src_j`. This is\n                useful for strided self-attention.\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"", "\n", "# anything in original attn_mask = 1, becomes -1e8", "\n", "# anything in original attn_mask = 0, becomes 0", "\n", "# Note that we cannot use -inf here, because at some edge cases,", "\n", "# the attention weight (before softmax) for some padded element in query", "\n", "# will become -inf, which results in NaN in model parameters", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "attn_mask", ".", "masked_fill", "(", "\n", "attn_mask", ".", "to", "(", "torch", ".", "bool", ")", ",", "-", "1e8", "if", "x", ".", "dtype", "==", "torch", ".", "float32", "else", "-", "1e4", "\n", ")", "\n", "\n", "", "residual", "=", "x", "\n", "x", "=", "self", ".", "self_attn_layer_norm", "(", "x", ")", "\n", "x", ",", "_", "=", "self", ".", "self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "key_padding_mask", "=", "encoder_padding_mask", ",", "\n", "need_weights", "=", "False", ",", "\n", "attn_mask", "=", "attn_mask", ",", "\n", ")", "\n", "x", "=", "self", ".", "dropout_module", "(", "x", ")", "\n", "x", "=", "self", ".", "residual_connection", "(", "x", ",", "residual", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "final_layer_norm", "(", "x", ")", "\n", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "dropout_module", "(", "x", ")", "\n", "x", "=", "self", ".", "residual_connection", "(", "x", ",", "residual", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.__init__": [[124, 178], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "transformer_layer.TransformerDecoderLayer.build_self_attention", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "transformer_layer.TransformerDecoderLayer.build_fc1", "transformer_layer.TransformerDecoderLayer.build_fc2", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "transformer_layer.TransformerDecoderLayer.build_encoder_attention", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "getattr", "LayerNorm", "getattr", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_self_attention", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_fc1", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_fc2", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_encoder_attention"], ["def", "__init__", "(", "\n", "self", ",", "args", ",", "no_encoder_attn", "=", "False", ",", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "self", ".", "dropout_module", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "\n", "self", ".", "self_attn", "=", "self", ".", "build_self_attention", "(", "\n", "self", ".", "embed_dim", ",", "\n", "args", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", ")", "\n", "self", ".", "nh", "=", "self", ".", "self_attn", ".", "num_heads", "\n", "self", ".", "head_dim", "=", "self", ".", "self_attn", ".", "head_dim", "\n", "\n", "self", ".", "activation_fn", "=", "F", ".", "relu", "\n", "\n", "self", ".", "self_attn_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "if", "no_encoder_attn", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "None", "\n", "self", ".", "encoder_attn_layer_norm", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "self", ".", "build_encoder_attention", "(", "self", ".", "embed_dim", ",", "args", ")", "\n", "self", ".", "encoder_attn_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "", "self", ".", "ffn_layernorm", "=", "(", "\n", "LayerNorm", "(", "args", ".", "decoder_ffn_embed_dim", ")", "\n", "if", "getattr", "(", "args", ",", "\"scale_fc\"", ",", "False", ")", "\n", "else", "None", "\n", ")", "\n", "self", ".", "w_resid", "=", "(", "\n", "nn", ".", "Parameter", "(", "\n", "torch", ".", "ones", "(", "\n", "self", ".", "embed_dim", ",", "\n", ")", ",", "\n", "requires_grad", "=", "True", ",", "\n", ")", "\n", "if", "getattr", "(", "args", ",", "\"scale_resids\"", ",", "False", ")", "\n", "else", "None", "\n", ")", "\n", "\n", "self", ".", "fc1", "=", "self", ".", "build_fc1", "(", "\n", "self", ".", "embed_dim", ",", "\n", "args", ".", "decoder_ffn_embed_dim", ",", "\n", ")", "\n", "self", ".", "fc2", "=", "self", ".", "build_fc2", "(", "\n", "args", ".", "decoder_ffn_embed_dim", ",", "\n", "self", ".", "embed_dim", ",", "\n", ")", "\n", "\n", "self", ".", "final_layer_norm", "=", "nn", ".", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_fc1": [[179, 181], ["torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["None"], ["", "def", "build_fc1", "(", "self", ",", "input_dim", ",", "output_dim", ")", ":", "\n", "        ", "return", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_fc2": [[182, 184], ["torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["None"], ["", "def", "build_fc2", "(", "self", ",", "input_dim", ",", "output_dim", ")", ":", "\n", "        ", "return", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_self_attention": [[185, 195], ["esm.multihead_attention.MultiheadAttention"], "methods", ["None"], ["", "def", "build_self_attention", "(", "\n", "self", ",", "embed_dim", ",", "args", ",", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", "\n", ")", ":", "\n", "        ", "return", "MultiheadAttention", "(", "\n", "embed_dim", ",", "\n", "args", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "self_attention", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.build_encoder_attention": [[197, 205], ["esm.multihead_attention.MultiheadAttention"], "methods", ["None"], ["", "def", "build_encoder_attention", "(", "self", ",", "embed_dim", ",", "args", ")", ":", "\n", "        ", "return", "MultiheadAttention", "(", "\n", "embed_dim", ",", "\n", "args", ".", "decoder_attention_heads", ",", "\n", "kdim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "vdim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", "encoder_decoder_attention", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.residual_connection": [[207, 209], ["None"], "methods", ["None"], ["", "def", "residual_connection", "(", "self", ",", "x", ",", "residual", ")", ":", "\n", "        ", "return", "residual", "+", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.forward": [[210, 305], ["transformer_layer.TransformerDecoderLayer.self_attn_layer_norm", "transformer_layer.TransformerDecoderLayer.self_attn._get_input_buffer", "transformer_layer.TransformerDecoderLayer.self_attn", "transformer_layer.TransformerDecoderLayer.dropout_module", "transformer_layer.TransformerDecoderLayer.residual_connection", "transformer_layer.TransformerDecoderLayer.final_layer_norm", "transformer_layer.TransformerDecoderLayer.activation_fn", "transformer_layer.TransformerDecoderLayer.fc2", "transformer_layer.TransformerDecoderLayer.dropout_module", "transformer_layer.TransformerDecoderLayer.residual_connection", "transformer_layer.TransformerDecoderLayer.self_attn._set_input_buffer", "transformer_layer.TransformerDecoderLayer.encoder_attn_layer_norm", "transformer_layer.TransformerDecoderLayer.encoder_attn", "transformer_layer.TransformerDecoderLayer.dropout_module", "transformer_layer.TransformerDecoderLayer.residual_connection", "transformer_layer.TransformerDecoderLayer.fc1", "transformer_layer.TransformerDecoderLayer.ffn_layernorm", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "len", "transformer_layer.TransformerDecoderLayer.encoder_attn._set_input_buffer", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.residual_connection", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.residual_connection", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._set_input_buffer", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.transformer_layer.TransformerDecoderLayer.residual_connection", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._set_input_buffer"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ",", "\n", "encoder_out", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "encoder_padding_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "incremental_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", "]", "=", "None", ",", "\n", "prev_self_attn_state", ":", "Optional", "[", "List", "[", "torch", ".", "Tensor", "]", "]", "=", "None", ",", "\n", "prev_attn_state", ":", "Optional", "[", "List", "[", "torch", ".", "Tensor", "]", "]", "=", "None", ",", "\n", "self_attn_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "self_attn_padding_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "need_attn", ":", "bool", "=", "False", ",", "\n", "need_head_weights", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor, optional): binary\n                ByteTensor of shape `(batch, src_len)` where padding\n                elements are indicated by ``1``.\n            need_attn (bool, optional): return attention weights\n            need_head_weights (bool, optional): return attention weights\n                for each head (default: return average over heads).\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"", "\n", "if", "need_head_weights", ":", "\n", "            ", "need_attn", "=", "True", "\n", "\n", "", "residual", "=", "x", "\n", "x", "=", "self", ".", "self_attn_layer_norm", "(", "x", ")", "\n", "if", "prev_self_attn_state", "is", "not", "None", ":", "\n", "            ", "prev_key", ",", "prev_value", "=", "prev_self_attn_state", "[", ":", "2", "]", "\n", "saved_state", ":", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "=", "{", "\n", "\"prev_key\"", ":", "prev_key", ",", "\n", "\"prev_value\"", ":", "prev_value", ",", "\n", "}", "\n", "if", "len", "(", "prev_self_attn_state", ")", ">=", "3", ":", "\n", "                ", "saved_state", "[", "\"prev_key_padding_mask\"", "]", "=", "prev_self_attn_state", "[", "2", "]", "\n", "", "assert", "incremental_state", "is", "not", "None", "\n", "self", ".", "self_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "_self_attn_input_buffer", "=", "self", ".", "self_attn", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "y", "=", "x", "\n", "\n", "x", ",", "attn", "=", "self", ".", "self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "y", ",", "\n", "value", "=", "y", ",", "\n", "key_padding_mask", "=", "self_attn_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "need_weights", "=", "False", ",", "\n", "attn_mask", "=", "self_attn_mask", ",", "\n", ")", "\n", "x", "=", "self", ".", "dropout_module", "(", "x", ")", "\n", "x", "=", "self", ".", "residual_connection", "(", "x", ",", "residual", ")", "\n", "\n", "if", "self", ".", "encoder_attn", "is", "not", "None", "and", "encoder_out", "is", "not", "None", ":", "\n", "            ", "residual", "=", "x", "\n", "x", "=", "self", ".", "encoder_attn_layer_norm", "(", "x", ")", "\n", "if", "prev_attn_state", "is", "not", "None", ":", "\n", "                ", "prev_key", ",", "prev_value", "=", "prev_attn_state", "[", ":", "2", "]", "\n", "saved_state", ":", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "=", "{", "\n", "\"prev_key\"", ":", "prev_key", ",", "\n", "\"prev_value\"", ":", "prev_value", ",", "\n", "}", "\n", "if", "len", "(", "prev_attn_state", ")", ">=", "3", ":", "\n", "                    ", "saved_state", "[", "\"prev_key_padding_mask\"", "]", "=", "prev_attn_state", "[", "2", "]", "\n", "", "assert", "incremental_state", "is", "not", "None", "\n", "self", ".", "encoder_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "\n", "", "x", ",", "attn", "=", "self", ".", "encoder_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "encoder_out", ",", "\n", "value", "=", "encoder_out", ",", "\n", "key_padding_mask", "=", "encoder_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "static_kv", "=", "True", ",", "\n", "need_weights", "=", "need_attn", "or", "(", "not", "self", ".", "training", "and", "self", ".", "need_attn", ")", ",", "\n", "need_head_weights", "=", "need_head_weights", ",", "\n", ")", "\n", "x", "=", "self", ".", "dropout_module", "(", "x", ")", "\n", "x", "=", "self", ".", "residual_connection", "(", "x", ",", "residual", ")", "\n", "\n", "", "residual", "=", "x", "\n", "x", "=", "self", ".", "final_layer_norm", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "if", "self", ".", "ffn_layernorm", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "ffn_layernorm", "(", "x", ")", "\n", "", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "dropout_module", "(", "x", ")", "\n", "if", "self", ".", "w_resid", "is", "not", "None", ":", "\n", "            ", "residual", "=", "torch", ".", "mul", "(", "self", ".", "w_resid", ",", "residual", ")", "\n", "", "x", "=", "self", ".", "residual_connection", "(", "x", ",", "residual", ")", "\n", "return", "x", ",", "attn", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer.get_node_features": [[30, 45], ["features.GVPInputFeaturizer._dihedrals", "features.GVPInputFeaturizer._orientations", "features.GVPInputFeaturizer._sidechains", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "features.GVPInputFeaturizer._sidechains", "coord_mask.float().unsqueeze", "coord_mask.float"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.DihedralFeatures._dihedrals", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._orientations", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._sidechains", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._sidechains"], ["    ", "@", "staticmethod", "\n", "def", "get_node_features", "(", "coords", ",", "coord_mask", ",", "with_coord_mask", "=", "True", ")", ":", "\n", "# scalar features", "\n", "        ", "node_scalar_features", "=", "GVPInputFeaturizer", ".", "_dihedrals", "(", "coords", ")", "\n", "if", "with_coord_mask", ":", "\n", "            ", "node_scalar_features", "=", "torch", ".", "cat", "(", "[", "\n", "node_scalar_features", ",", "\n", "coord_mask", ".", "float", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "]", ",", "dim", "=", "-", "1", ")", "\n", "# vector features", "\n", "", "X_ca", "=", "coords", "[", ":", ",", ":", ",", "1", "]", "\n", "orientations", "=", "GVPInputFeaturizer", ".", "_orientations", "(", "X_ca", ")", "\n", "sidechains", "=", "GVPInputFeaturizer", ".", "_sidechains", "(", "coords", ")", "\n", "node_vector_features", "=", "torch", ".", "cat", "(", "[", "orientations", ",", "sidechains", ".", "unsqueeze", "(", "-", "2", ")", "]", ",", "dim", "=", "-", "2", ")", "\n", "return", "node_scalar_features", ",", "node_vector_features", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._orientations": [[46, 53], ["util.normalize", "util.normalize", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.pad.unsqueeze", "torch.pad.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize"], ["", "@", "staticmethod", "\n", "def", "_orientations", "(", "X", ")", ":", "\n", "        ", "forward", "=", "normalize", "(", "X", "[", ":", ",", "1", ":", "]", "-", "X", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "backward", "=", "normalize", "(", "X", "[", ":", ",", ":", "-", "1", "]", "-", "X", "[", ":", ",", "1", ":", "]", ")", "\n", "forward", "=", "F", ".", "pad", "(", "forward", ",", "[", "0", ",", "0", ",", "0", ",", "1", "]", ")", "\n", "backward", "=", "F", ".", "pad", "(", "backward", ",", "[", "0", ",", "0", ",", "1", ",", "0", "]", ")", "\n", "return", "torch", ".", "cat", "(", "[", "forward", ".", "unsqueeze", "(", "-", "2", ")", ",", "backward", ".", "unsqueeze", "(", "-", "2", ")", "]", ",", "-", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._sidechains": [[54, 62], ["util.normalize", "util.normalize", "util.normalize", "util.normalize", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "math.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize"], ["", "@", "staticmethod", "\n", "def", "_sidechains", "(", "X", ")", ":", "\n", "        ", "n", ",", "origin", ",", "c", "=", "X", "[", ":", ",", ":", ",", "0", "]", ",", "X", "[", ":", ",", ":", ",", "1", "]", ",", "X", "[", ":", ",", ":", ",", "2", "]", "\n", "c", ",", "n", "=", "normalize", "(", "c", "-", "origin", ")", ",", "normalize", "(", "n", "-", "origin", ")", "\n", "bisector", "=", "normalize", "(", "c", "+", "n", ")", "\n", "perp", "=", "normalize", "(", "torch", ".", "cross", "(", "c", ",", "n", ",", "dim", "=", "-", "1", ")", ")", "\n", "vec", "=", "-", "bisector", "*", "math", ".", "sqrt", "(", "1", "/", "3", ")", "-", "perp", "*", "math", ".", "sqrt", "(", "2", "/", "3", ")", "\n", "return", "vec", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._dihedrals": [[63, 88], ["torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "util.normalize", "util.normalize", "util.normalize", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.pad", "torch.pad", "torch.pad", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize"], ["", "@", "staticmethod", "\n", "def", "_dihedrals", "(", "X", ",", "eps", "=", "1e-7", ")", ":", "\n", "        ", "X", "=", "torch", ".", "flatten", "(", "X", "[", ":", ",", ":", ",", ":", "3", "]", ",", "1", ",", "2", ")", "\n", "bsz", "=", "X", ".", "shape", "[", "0", "]", "\n", "dX", "=", "X", "[", ":", ",", "1", ":", "]", "-", "X", "[", ":", ",", ":", "-", "1", "]", "\n", "U", "=", "normalize", "(", "dX", ",", "dim", "=", "-", "1", ")", "\n", "u_2", "=", "U", "[", ":", ",", ":", "-", "2", "]", "\n", "u_1", "=", "U", "[", ":", ",", "1", ":", "-", "1", "]", "\n", "u_0", "=", "U", "[", ":", ",", "2", ":", "]", "\n", "\n", "# Backbone normals", "\n", "n_2", "=", "normalize", "(", "torch", ".", "cross", "(", "u_2", ",", "u_1", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "n_1", "=", "normalize", "(", "torch", ".", "cross", "(", "u_1", ",", "u_0", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Angle between normals", "\n", "cosD", "=", "torch", ".", "sum", "(", "n_2", "*", "n_1", ",", "-", "1", ")", "\n", "cosD", "=", "torch", ".", "clamp", "(", "cosD", ",", "-", "1", "+", "eps", ",", "1", "-", "eps", ")", "\n", "D", "=", "torch", ".", "sign", "(", "torch", ".", "sum", "(", "u_2", "*", "n_1", ",", "-", "1", ")", ")", "*", "torch", ".", "acos", "(", "cosD", ")", "\n", "\n", "# This scheme will remove phi[0], psi[-1], omega[-1]", "\n", "D", "=", "F", ".", "pad", "(", "D", ",", "[", "1", ",", "2", "]", ")", "\n", "D", "=", "torch", ".", "reshape", "(", "D", ",", "[", "bsz", ",", "-", "1", ",", "3", "]", ")", "\n", "# Lift angle representations to the circle", "\n", "D_features", "=", "torch", ".", "cat", "(", "[", "torch", ".", "cos", "(", "D", ")", ",", "torch", ".", "sin", "(", "D", ")", "]", ",", "-", "1", ")", "\n", "return", "D_features", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._positional_embeddings": [[89, 106], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "d.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "numpy.log"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_positional_embeddings", "(", "edge_index", ",", "\n", "num_embeddings", "=", "None", ",", "\n", "num_positional_embeddings", "=", "16", ",", "\n", "period_range", "=", "[", "2", ",", "1000", "]", ")", ":", "\n", "# From https://github.com/jingraham/neurips19-graph-protein-design", "\n", "        ", "num_embeddings", "=", "num_embeddings", "or", "num_positional_embeddings", "\n", "d", "=", "edge_index", "[", "0", "]", "-", "edge_index", "[", "1", "]", "\n", "\n", "frequency", "=", "torch", ".", "exp", "(", "\n", "torch", ".", "arange", "(", "0", ",", "num_embeddings", ",", "2", ",", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "edge_index", ".", "device", ")", "\n", "*", "-", "(", "np", ".", "log", "(", "10000.0", ")", "/", "num_embeddings", ")", "\n", ")", "\n", "angles", "=", "d", ".", "unsqueeze", "(", "-", "1", ")", "*", "frequency", "\n", "E", "=", "torch", ".", "cat", "(", "(", "torch", ".", "cos", "(", "angles", ")", ",", "torch", ".", "sin", "(", "angles", ")", ")", ",", "-", "1", ")", "\n", "return", "E", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._dist": [[107, 137], ["torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.abs().repeat", "torch.abs().repeat", "torch.abs().repeat", "torch.abs().repeat", "torch.abs().repeat", "torch.abs().repeat", "torch.abs().repeat", "torch.abs().repeat", "torch.abs().repeat", "X.size", "X.size", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "torch.unsqueeze", "util.norm", "torch.arange.repeat", "torch.arange.repeat", "torch.arange.repeat", "min", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "util.nan_to_num", "X.size", "torch.arange.unsqueeze", "torch.arange.unsqueeze", "torch.arange.unsqueeze", "torch.arange.unsqueeze", "torch.arange.unsqueeze", "torch.arange.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.norm", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.nan_to_num"], ["", "@", "staticmethod", "\n", "def", "_dist", "(", "X", ",", "coord_mask", ",", "padding_mask", ",", "top_k_neighbors", ",", "eps", "=", "1e-8", ")", ":", "\n", "        ", "\"\"\" Pairwise euclidean distances \"\"\"", "\n", "bsz", ",", "maxlen", "=", "X", ".", "size", "(", "0", ")", ",", "X", ".", "size", "(", "1", ")", "\n", "coord_mask_2D", "=", "torch", ".", "unsqueeze", "(", "coord_mask", ",", "1", ")", "*", "torch", ".", "unsqueeze", "(", "coord_mask", ",", "2", ")", "\n", "residue_mask", "=", "~", "padding_mask", "\n", "residue_mask_2D", "=", "torch", ".", "unsqueeze", "(", "residue_mask", ",", "1", ")", "*", "torch", ".", "unsqueeze", "(", "residue_mask", ",", "2", ")", "\n", "dX", "=", "torch", ".", "unsqueeze", "(", "X", ",", "1", ")", "-", "torch", ".", "unsqueeze", "(", "X", ",", "2", ")", "\n", "D", "=", "coord_mask_2D", "*", "norm", "(", "dX", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# sorting preference: first those with coords, then among the residues that", "\n", "# exist but are masked use distance in sequence as tie breaker, and then the", "\n", "# residues that came from padding are last", "\n", "seqpos", "=", "torch", ".", "arange", "(", "maxlen", ",", "device", "=", "X", ".", "device", ")", "\n", "Dseq", "=", "torch", ".", "abs", "(", "seqpos", ".", "unsqueeze", "(", "1", ")", "-", "seqpos", ".", "unsqueeze", "(", "0", ")", ")", ".", "repeat", "(", "bsz", ",", "1", ",", "1", ")", "\n", "D_adjust", "=", "nan_to_num", "(", "D", ")", "+", "(", "~", "coord_mask_2D", ")", "*", "(", "1e8", "+", "Dseq", "*", "1e6", ")", "+", "(", "\n", "~", "residue_mask_2D", ")", "*", "(", "1e10", ")", "\n", "\n", "if", "top_k_neighbors", "==", "-", "1", ":", "\n", "            ", "D_neighbors", "=", "D_adjust", "\n", "E_idx", "=", "seqpos", ".", "repeat", "(", "\n", "*", "D_neighbors", ".", "shape", "[", ":", "-", "1", "]", ",", "1", ")", "\n", "", "else", ":", "\n", "# Identify k nearest neighbors (including self)", "\n", "            ", "k", "=", "min", "(", "top_k_neighbors", ",", "X", ".", "size", "(", "1", ")", ")", "\n", "D_neighbors", ",", "E_idx", "=", "torch", ".", "topk", "(", "D_adjust", ",", "k", ",", "dim", "=", "-", "1", ",", "largest", "=", "False", ")", "\n", "\n", "", "coord_mask_neighbors", "=", "(", "D_neighbors", "<", "5e7", ")", "\n", "residue_mask_neighbors", "=", "(", "D_neighbors", "<", "5e9", ")", "\n", "return", "D_neighbors", ",", "E_idx", ",", "coord_mask_neighbors", ",", "residue_mask_neighbors", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.Normalize.__init__": [[140, 145], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "features", ",", "epsilon", "=", "1e-6", ")", ":", "\n", "        ", "super", "(", "Normalize", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gain", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "features", ")", ")", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.Normalize.forward": [[146, 158], ["x.mean", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "gain.view.view.view", "bias.view.view.view", "x.var", "len", "features.Normalize.gain.size", "x.mean.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "mu", "=", "x", ".", "mean", "(", "dim", ",", "keepdim", "=", "True", ")", "\n", "sigma", "=", "torch", ".", "sqrt", "(", "x", ".", "var", "(", "dim", ",", "keepdim", "=", "True", ")", "+", "self", ".", "epsilon", ")", "\n", "gain", "=", "self", ".", "gain", "\n", "bias", "=", "self", ".", "bias", "\n", "# Reshape", "\n", "if", "dim", "!=", "-", "1", ":", "\n", "            ", "shape", "=", "[", "1", "]", "*", "len", "(", "mu", ".", "size", "(", ")", ")", "\n", "shape", "[", "dim", "]", "=", "self", ".", "gain", ".", "size", "(", ")", "[", "0", "]", "\n", "gain", "=", "gain", ".", "view", "(", "shape", ")", "\n", "bias", "=", "bias", ".", "view", "(", "shape", ")", "\n", "", "return", "gain", "*", "(", "x", "-", "mu", ")", "/", "(", "sigma", "+", "self", ".", "epsilon", ")", "+", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.DihedralFeatures.__init__": [[161, 169], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "features.Normalize"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "node_embed_dim", ")", ":", "\n", "        ", "\"\"\" Embed dihedral angle features. \"\"\"", "\n", "super", "(", "DihedralFeatures", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# 3 dihedral angles; sin and cos of each angle", "\n", "node_in", "=", "6", "\n", "# Normalization and embedding", "\n", "self", ".", "node_embedding", "=", "nn", ".", "Linear", "(", "node_in", ",", "node_embed_dim", ",", "bias", "=", "True", ")", "\n", "self", ".", "norm_nodes", "=", "Normalize", "(", "node_embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.DihedralFeatures.forward": [[170, 176], ["features.DihedralFeatures._dihedrals", "features.DihedralFeatures.node_embedding", "features.DihedralFeatures.norm_nodes"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.DihedralFeatures._dihedrals"], ["", "def", "forward", "(", "self", ",", "X", ")", ":", "\n", "        ", "\"\"\" Featurize coordinates as an attributed graph \"\"\"", "\n", "V", "=", "self", ".", "_dihedrals", "(", "X", ")", "\n", "V", "=", "self", ".", "node_embedding", "(", "V", ")", "\n", "V", "=", "self", ".", "norm_nodes", "(", "V", ")", "\n", "return", "V", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.DihedralFeatures._dihedrals": [[177, 208], ["X[].reshape", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.normalize", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.pad", "torch.pad", "torch.pad", "D.view.view.view", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.cross", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "torch.acos", "D.view.view.size", "int", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "D.view.view.size"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize"], ["", "@", "staticmethod", "\n", "def", "_dihedrals", "(", "X", ",", "eps", "=", "1e-7", ",", "return_angles", "=", "False", ")", ":", "\n", "# First 3 coordinates are N, CA, C", "\n", "        ", "X", "=", "X", "[", ":", ",", ":", ",", ":", "3", ",", ":", "]", ".", "reshape", "(", "X", ".", "shape", "[", "0", "]", ",", "3", "*", "X", ".", "shape", "[", "1", "]", ",", "3", ")", "\n", "\n", "# Shifted slices of unit vectors", "\n", "dX", "=", "X", "[", ":", ",", "1", ":", ",", ":", "]", "-", "X", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "U", "=", "F", ".", "normalize", "(", "dX", ",", "dim", "=", "-", "1", ")", "\n", "u_2", "=", "U", "[", ":", ",", ":", "-", "2", ",", ":", "]", "\n", "u_1", "=", "U", "[", ":", ",", "1", ":", "-", "1", ",", ":", "]", "\n", "u_0", "=", "U", "[", ":", ",", "2", ":", ",", ":", "]", "\n", "# Backbone normals", "\n", "n_2", "=", "F", ".", "normalize", "(", "torch", ".", "cross", "(", "u_2", ",", "u_1", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "n_1", "=", "F", ".", "normalize", "(", "torch", ".", "cross", "(", "u_1", ",", "u_0", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Angle between normals", "\n", "cosD", "=", "(", "n_2", "*", "n_1", ")", ".", "sum", "(", "-", "1", ")", "\n", "cosD", "=", "torch", ".", "clamp", "(", "cosD", ",", "-", "1", "+", "eps", ",", "1", "-", "eps", ")", "\n", "D", "=", "torch", ".", "sign", "(", "(", "u_2", "*", "n_1", ")", ".", "sum", "(", "-", "1", ")", ")", "*", "torch", ".", "acos", "(", "cosD", ")", "\n", "\n", "# This scheme will remove phi[0], psi[-1], omega[-1]", "\n", "D", "=", "F", ".", "pad", "(", "D", ",", "(", "1", ",", "2", ")", ",", "'constant'", ",", "0", ")", "\n", "D", "=", "D", ".", "view", "(", "(", "D", ".", "size", "(", "0", ")", ",", "int", "(", "D", ".", "size", "(", "1", ")", "/", "3", ")", ",", "3", ")", ")", "\n", "phi", ",", "psi", ",", "omega", "=", "torch", ".", "unbind", "(", "D", ",", "-", "1", ")", "\n", "\n", "if", "return_angles", ":", "\n", "            ", "return", "phi", ",", "psi", ",", "omega", "\n", "\n", "# Lift angle representations to the circle", "\n", "", "D_features", "=", "torch", ".", "cat", "(", "(", "torch", ".", "cos", "(", "D", ")", ",", "torch", ".", "sin", "(", "D", ")", ")", ",", "2", ")", "\n", "return", "D_features", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPGraphEmbedding.__init__": [[212, 232], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "gvp_modules.GVP", "gvp_modules.LayerNorm", "gvp_modules.GVP", "gvp_modules.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "top_k_neighbors", "=", "args", ".", "top_k_neighbors", "\n", "self", ".", "num_positional_embeddings", "=", "16", "\n", "self", ".", "remove_edges_without_coords", "=", "True", "\n", "node_input_dim", "=", "(", "7", ",", "3", ")", "\n", "edge_input_dim", "=", "(", "34", ",", "1", ")", "\n", "node_hidden_dim", "=", "(", "args", ".", "node_hidden_dim_scalar", ",", "\n", "args", ".", "node_hidden_dim_vector", ")", "\n", "edge_hidden_dim", "=", "(", "args", ".", "edge_hidden_dim_scalar", ",", "\n", "args", ".", "edge_hidden_dim_vector", ")", "\n", "self", ".", "embed_node", "=", "nn", ".", "Sequential", "(", "\n", "GVP", "(", "node_input_dim", ",", "node_hidden_dim", ",", "activations", "=", "(", "None", ",", "None", ")", ")", ",", "\n", "LayerNorm", "(", "node_hidden_dim", ",", "eps", "=", "1e-4", ")", "\n", ")", "\n", "self", ".", "embed_edge", "=", "nn", ".", "Sequential", "(", "\n", "GVP", "(", "edge_input_dim", ",", "edge_hidden_dim", ",", "activations", "=", "(", "None", ",", "None", ")", ")", ",", "\n", "LayerNorm", "(", "edge_hidden_dim", ",", "eps", "=", "1e-4", ")", "\n", ")", "\n", "self", ".", "embed_confidence", "=", "nn", ".", "Linear", "(", "16", ",", "args", ".", "node_hidden_dim_scalar", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPGraphEmbedding.forward": [[233, 250], ["features.GVPGraphEmbedding.embed_node", "features.GVPGraphEmbedding.embed_edge", "util.rbf", "gvp_utils.flatten_graph", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "features.GVPGraphEmbedding.get_node_features", "features.GVPGraphEmbedding.get_edge_features", "features.GVPGraphEmbedding.embed_confidence"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.rbf", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_utils.flatten_graph", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer.get_node_features", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPGraphEmbedding.get_edge_features"], ["", "def", "forward", "(", "self", ",", "coords", ",", "coord_mask", ",", "padding_mask", ",", "confidence", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "node_features", "=", "self", ".", "get_node_features", "(", "coords", ",", "coord_mask", ")", "\n", "edge_features", ",", "edge_index", "=", "self", ".", "get_edge_features", "(", "\n", "coords", ",", "coord_mask", ",", "padding_mask", ")", "\n", "", "node_embeddings_scalar", ",", "node_embeddings_vector", "=", "self", ".", "embed_node", "(", "node_features", ")", "\n", "edge_embeddings", "=", "self", ".", "embed_edge", "(", "edge_features", ")", "\n", "\n", "rbf_rep", "=", "rbf", "(", "confidence", ",", "0.", ",", "1.", ")", "\n", "node_embeddings", "=", "(", "\n", "node_embeddings_scalar", "+", "self", ".", "embed_confidence", "(", "rbf_rep", ")", ",", "\n", "node_embeddings_vector", "\n", ")", "\n", "\n", "node_embeddings", ",", "edge_embeddings", ",", "edge_index", "=", "flatten_graph", "(", "\n", "node_embeddings", ",", "edge_embeddings", ",", "edge_index", ")", "\n", "return", "node_embeddings", ",", "edge_embeddings", ",", "edge_index", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPGraphEmbedding.get_edge_features": [[251, 304], ["features.GVPInputFeaturizer._dist", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.arange().view().expand", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "torch.stack().flatten", "E_dist.flatten.flatten.flatten", "E_coord_mask.flatten().unsqueeze.flatten().unsqueeze.flatten().unsqueeze", "E_residue_mask.flatten.flatten.flatten", "features.GVPInputFeaturizer._positional_embeddings", "util.rbf", "X_ca.unsqueeze().expand().flatten", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "coord_mask.unsqueeze().expand().flatten", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "util.normalize().unsqueeze", "map", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "edge_index[].unsqueeze().expand", "edge_index[].expand", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.stack().flatten.transpose", "torch.stack().flatten.transpose", "torch.stack().flatten.transpose", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "E_coord_mask.flatten().unsqueeze.flatten().unsqueeze.flatten", "X_ca.unsqueeze().expand", "coord_mask.unsqueeze().expand", "util.normalize", "edge_index[].unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "X_ca.unsqueeze", "coord_mask.unsqueeze", "E_coord_mask.flatten().unsqueeze.flatten().unsqueeze.squeeze"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._dist", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.features.GVPInputFeaturizer._positional_embeddings", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.rbf", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.util.normalize"], ["", "def", "get_edge_features", "(", "self", ",", "coords", ",", "coord_mask", ",", "padding_mask", ")", ":", "\n", "        ", "X_ca", "=", "coords", "[", ":", ",", ":", ",", "1", "]", "\n", "# Get distances to the top k neighbors", "\n", "E_dist", ",", "E_idx", ",", "E_coord_mask", ",", "E_residue_mask", "=", "GVPInputFeaturizer", ".", "_dist", "(", "\n", "X_ca", ",", "coord_mask", ",", "padding_mask", ",", "self", ".", "top_k_neighbors", ")", "\n", "# Flatten the graph to be batch size 1 for torch_geometric package ", "\n", "dest", "=", "E_idx", "\n", "B", ",", "L", ",", "k", "=", "E_idx", ".", "shape", "[", ":", "3", "]", "\n", "src", "=", "torch", ".", "arange", "(", "L", ",", "device", "=", "E_idx", ".", "device", ")", ".", "view", "(", "[", "1", ",", "L", ",", "1", "]", ")", ".", "expand", "(", "B", ",", "L", ",", "k", ")", "\n", "# After flattening, [2, B, E]", "\n", "edge_index", "=", "torch", ".", "stack", "(", "[", "src", ",", "dest", "]", ",", "dim", "=", "0", ")", ".", "flatten", "(", "2", ",", "3", ")", "\n", "# After flattening, [B, E]", "\n", "E_dist", "=", "E_dist", ".", "flatten", "(", "1", ",", "2", ")", "\n", "E_coord_mask", "=", "E_coord_mask", ".", "flatten", "(", "1", ",", "2", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "E_residue_mask", "=", "E_residue_mask", ".", "flatten", "(", "1", ",", "2", ")", "\n", "# Calculate relative positional embeddings and distance RBF ", "\n", "pos_embeddings", "=", "GVPInputFeaturizer", ".", "_positional_embeddings", "(", "\n", "edge_index", ",", "\n", "num_positional_embeddings", "=", "self", ".", "num_positional_embeddings", ",", "\n", ")", "\n", "D_rbf", "=", "rbf", "(", "E_dist", ",", "0.", ",", "20.", ")", "\n", "# Calculate relative orientation ", "\n", "X_src", "=", "X_ca", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "k", ",", "-", "1", ")", ".", "flatten", "(", "1", ",", "2", ")", "\n", "X_dest", "=", "torch", ".", "gather", "(", "\n", "X_ca", ",", "\n", "1", ",", "\n", "edge_index", "[", "1", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "[", "B", ",", "L", "*", "k", ",", "3", "]", ")", "\n", ")", "\n", "coord_mask_src", "=", "coord_mask", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "k", ")", ".", "flatten", "(", "1", ",", "2", ")", "\n", "coord_mask_dest", "=", "torch", ".", "gather", "(", "\n", "coord_mask", ",", "\n", "1", ",", "\n", "edge_index", "[", "1", ",", ":", ",", ":", "]", ".", "expand", "(", "[", "B", ",", "L", "*", "k", "]", ")", "\n", ")", "\n", "E_vectors", "=", "X_src", "-", "X_dest", "\n", "# For the ones without coordinates, substitute in the average vector", "\n", "E_vector_mean", "=", "torch", ".", "sum", "(", "E_vectors", "*", "E_coord_mask", ",", "dim", "=", "1", ",", "\n", "keepdims", "=", "True", ")", "/", "torch", ".", "sum", "(", "E_coord_mask", ",", "dim", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "E_vectors", "=", "E_vectors", "*", "E_coord_mask", "+", "E_vector_mean", "*", "~", "(", "E_coord_mask", ")", "\n", "# Normalize and remove nans ", "\n", "edge_s", "=", "torch", ".", "cat", "(", "[", "D_rbf", ",", "pos_embeddings", "]", ",", "dim", "=", "-", "1", ")", "\n", "edge_v", "=", "normalize", "(", "E_vectors", ")", ".", "unsqueeze", "(", "-", "2", ")", "\n", "edge_s", ",", "edge_v", "=", "map", "(", "nan_to_num", ",", "(", "edge_s", ",", "edge_v", ")", ")", "\n", "# Also add indications of whether the coordinates are present ", "\n", "edge_s", "=", "torch", ".", "cat", "(", "[", "\n", "edge_s", ",", "\n", "(", "~", "coord_mask_src", ")", ".", "float", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "(", "~", "coord_mask_dest", ")", ".", "float", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "]", ",", "dim", "=", "-", "1", ")", "\n", "edge_index", "[", ":", ",", "~", "E_residue_mask", "]", "=", "-", "1", "\n", "if", "self", ".", "remove_edges_without_coords", ":", "\n", "            ", "edge_index", "[", ":", ",", "~", "E_coord_mask", ".", "squeeze", "(", "-", "1", ")", "]", "=", "-", "1", "\n", "", "return", "(", "edge_s", ",", "edge_v", ")", ",", "edge_index", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.scripts.extract.create_parser": [[15, 60], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Extract per-token representations and model outputs for sequences in a FASTA file\"", "# noqa", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"model_location\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"PyTorch model file OR name of pretrained model to download (see README for models)\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"fasta_file\"", ",", "\n", "type", "=", "pathlib", ".", "Path", ",", "\n", "help", "=", "\"FASTA file on which to extract representations\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"output_dir\"", ",", "\n", "type", "=", "pathlib", ".", "Path", ",", "\n", "help", "=", "\"output directory for extracted representations\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--toks_per_batch\"", ",", "type", "=", "int", ",", "default", "=", "4096", ",", "help", "=", "\"maximum batch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--repr_layers\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "[", "-", "1", "]", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "\"layers indices from which to extract representations (0 to num_layers, inclusive)\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--include\"", ",", "\n", "type", "=", "str", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", "choices", "=", "[", "\"mean\"", ",", "\"per_tok\"", ",", "\"bos\"", ",", "\"contacts\"", "]", ",", "\n", "help", "=", "\"specify which representations to return\"", ",", "\n", "required", "=", "True", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--truncate\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Truncate sequences longer than 1024 to match the training setup\"", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--nogpu\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Do not use GPU even if available\"", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.scripts.extract.main": [[62, 134], ["esm.pretrained.load_model_and_alphabet", "model.cuda.eval", "isinstance", "esm.FastaBatchedDataset.from_file", "FastaBatchedDataset.from_file.get_batch_indices", "torch.utils.data.DataLoader", "print", "args.output_dir.mkdir", "all", "ValueError", "torch.cuda.is_available", "model.cuda.cuda", "print", "torch.no_grad", "enumerate", "alphabet.get_batch_converter", "print", "model.cuda.", "out[].to", "enumerate", "len", "torch.cuda.is_available", "toks.to.to", "t.to", "out[].to", "args.output_file.parent.mkdir", "torch.save", "out[].items", "contacts[].clone", "len", "toks.to.size", "t[].clone", "t[].mean().clone", "t[].clone", "representations.items", "representations.items", "representations.items", "t[].mean", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.FastaBatchedDataset.from_file", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.FastaBatchedDataset.get_batch_indices", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_batch_converter"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "model", ",", "alphabet", "=", "pretrained", ".", "load_model_and_alphabet", "(", "args", ".", "model_location", ")", "\n", "model", ".", "eval", "(", ")", "\n", "if", "isinstance", "(", "model", ",", "MSATransformer", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"This script currently does not handle models with MSA input (MSA Transformer).\"", "\n", ")", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "nogpu", ":", "\n", "        ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "print", "(", "\"Transferred model to GPU\"", ")", "\n", "\n", "", "dataset", "=", "FastaBatchedDataset", ".", "from_file", "(", "args", ".", "fasta_file", ")", "\n", "batches", "=", "dataset", ".", "get_batch_indices", "(", "args", ".", "toks_per_batch", ",", "extra_toks_per_seq", "=", "1", ")", "\n", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "collate_fn", "=", "alphabet", ".", "get_batch_converter", "(", ")", ",", "batch_sampler", "=", "batches", "\n", ")", "\n", "print", "(", "f\"Read {args.fasta_file} with {len(dataset)} sequences\"", ")", "\n", "\n", "args", ".", "output_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "return_contacts", "=", "\"contacts\"", "in", "args", ".", "include", "\n", "\n", "assert", "all", "(", "-", "(", "model", ".", "num_layers", "+", "1", ")", "<=", "i", "<=", "model", ".", "num_layers", "for", "i", "in", "args", ".", "repr_layers", ")", "\n", "repr_layers", "=", "[", "(", "i", "+", "model", ".", "num_layers", "+", "1", ")", "%", "(", "model", ".", "num_layers", "+", "1", ")", "for", "i", "in", "args", ".", "repr_layers", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_idx", ",", "(", "labels", ",", "strs", ",", "toks", ")", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "            ", "print", "(", "\n", "f\"Processing {batch_idx + 1} of {len(batches)} batches ({toks.size(0)} sequences)\"", "\n", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "nogpu", ":", "\n", "                ", "toks", "=", "toks", ".", "to", "(", "device", "=", "\"cuda\"", ",", "non_blocking", "=", "True", ")", "\n", "\n", "# The model is trained on truncated sequences and passing longer ones in at", "\n", "# infernce will cause an error. See https://github.com/facebookresearch/esm/issues/21", "\n", "", "if", "args", ".", "truncate", ":", "\n", "                ", "toks", "=", "toks", "[", ":", ",", ":", "1022", "]", "\n", "\n", "", "out", "=", "model", "(", "toks", ",", "repr_layers", "=", "repr_layers", ",", "return_contacts", "=", "return_contacts", ")", "\n", "\n", "logits", "=", "out", "[", "\"logits\"", "]", ".", "to", "(", "device", "=", "\"cpu\"", ")", "\n", "representations", "=", "{", "\n", "layer", ":", "t", ".", "to", "(", "device", "=", "\"cpu\"", ")", "for", "layer", ",", "t", "in", "out", "[", "\"representations\"", "]", ".", "items", "(", ")", "\n", "}", "\n", "if", "return_contacts", ":", "\n", "                ", "contacts", "=", "out", "[", "\"contacts\"", "]", ".", "to", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "", "for", "i", ",", "label", "in", "enumerate", "(", "labels", ")", ":", "\n", "                ", "args", ".", "output_file", "=", "args", ".", "output_dir", "/", "f\"{label}.pt\"", "\n", "args", ".", "output_file", ".", "parent", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "result", "=", "{", "\"label\"", ":", "label", "}", "\n", "# Call clone on tensors to ensure tensors are not views into a larger representation", "\n", "# See https://github.com/pytorch/pytorch/issues/1995", "\n", "if", "\"per_tok\"", "in", "args", ".", "include", ":", "\n", "                    ", "result", "[", "\"representations\"", "]", "=", "{", "\n", "layer", ":", "t", "[", "i", ",", "1", ":", "len", "(", "strs", "[", "i", "]", ")", "+", "1", "]", ".", "clone", "(", ")", "\n", "for", "layer", ",", "t", "in", "representations", ".", "items", "(", ")", "\n", "}", "\n", "", "if", "\"mean\"", "in", "args", ".", "include", ":", "\n", "                    ", "result", "[", "\"mean_representations\"", "]", "=", "{", "\n", "layer", ":", "t", "[", "i", ",", "1", ":", "len", "(", "strs", "[", "i", "]", ")", "+", "1", "]", ".", "mean", "(", "0", ")", ".", "clone", "(", ")", "\n", "for", "layer", ",", "t", "in", "representations", ".", "items", "(", ")", "\n", "}", "\n", "", "if", "\"bos\"", "in", "args", ".", "include", ":", "\n", "                    ", "result", "[", "\"bos_representations\"", "]", "=", "{", "\n", "layer", ":", "t", "[", "i", ",", "0", "]", ".", "clone", "(", ")", "for", "layer", ",", "t", "in", "representations", ".", "items", "(", ")", "\n", "}", "\n", "", "if", "return_contacts", ":", "\n", "                    ", "result", "[", "\"contacts\"", "]", "=", "contacts", "[", "i", ",", ":", "len", "(", "strs", "[", "i", "]", ")", ",", ":", "len", "(", "strs", "[", "i", "]", ")", "]", ".", "clone", "(", ")", "\n", "\n", "", "torch", ".", "save", "(", "\n", "result", ",", "\n", "args", ".", "output_file", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_notebooks.convert_notebook_to_py": [[15, 33], ["PythonExporter", "PythonExporter.from_notebook_node", "source.replace", "open", "nbformat.reads", "open", "fh.writelines", "fh.read"], "function", ["None"], ["def", "convert_notebook_to_py", "(", "nb_fn", ":", "Path", ",", "py_fn", ":", "Path", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    From https://stackoverflow.com/questions/17077494/how-do-i-convert-a-ipython-notebook-into-a-python-file-via-commandline\n    \"\"\"", "\n", "import", "nbformat", "\n", "from", "nbconvert", "import", "PythonExporter", "\n", "\n", "with", "open", "(", "nb_fn", ")", "as", "fh", ":", "\n", "        ", "nb", "=", "nbformat", ".", "reads", "(", "fh", ".", "read", "(", ")", ",", "nbformat", ".", "NO_CONVERT", ")", "\n", "\n", "", "exporter", "=", "PythonExporter", "(", ")", "\n", "source", ",", "meta", "=", "exporter", ".", "from_notebook_node", "(", "nb", ")", "\n", "\n", "# Skip the magic, which gets converted to `get_ipython()`", "\n", "source", ".", "replace", "(", "\"get_ipython\"", ",", "\"# get_ipython\"", ")", "\n", "\n", "with", "open", "(", "py_fn", ",", "\"w+\"", ")", "as", "fh", ":", "\n", "        ", "fh", ".", "writelines", "(", "source", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_notebooks.run_multiple": [[35, 39], ["cmds.strip().split", "print", "subprocess.run", "cmds.strip", "cmd.strip"], "function", ["None"], ["", "", "def", "run_multiple", "(", "cmds", ")", ":", "\n", "    ", "for", "cmd", "in", "cmds", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", ":", "\n", "        ", "print", "(", "cmd", ")", "\n", "subprocess", ".", "run", "(", "cmd", ".", "strip", "(", ")", ",", "shell", "=", "True", ",", "check", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_notebooks.do_setup": [[41, 52], ["test_notebooks.run_multiple", "print"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_notebooks.run_multiple"], ["", "", "def", "do_setup", "(", "nb_name", ")", ":", "\n", "    ", "\"\"\" Do any setup work; see intro of the notebook \"\"\"", "\n", "if", "nb_name", "==", "\"sup_variant_prediction\"", ":", "\n", "        ", "cmds", "=", "\"\"\"\n        curl -O https://dl.fbaipublicfiles.com/fair-esm/examples/P62593_reprs.tar.gz\n        tar -xzf P62593_reprs.tar.gz\n        curl -O https://dl.fbaipublicfiles.com/fair-esm/examples/P62593.fasta\n        \"\"\"", "\n", "run_multiple", "(", "cmds", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "f\"No setup work for {nb_name}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_notebooks.test_run_notebook": [[54, 64], ["pytest.mark.parametrize", "print", "test_notebooks.convert_notebook_to_py", "os.chdir", "test_notebooks.do_setup", "exec", "list", "py_fn.read_text"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_notebooks.convert_notebook_to_py", "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_notebooks.do_setup"], ["", "", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\"nb_fn\"", ",", "list", "(", "notebook_fns", ")", ")", "\n", "def", "test_run_notebook", "(", "nb_fn", ":", "Path", ",", "tmp_path", ":", "Path", ")", ":", "\n", "    ", "\"\"\" Simply make sure the notebooks run from a-z \"\"\"", "\n", "py_fn", "=", "tmp_path", "/", "(", "nb_fn", ".", "stem", "+", "\".py\"", ")", "\n", "print", "(", "py_fn", ")", "\n", "convert_notebook_to_py", "(", "nb_fn", ",", "py_fn", ")", "\n", "os", ".", "chdir", "(", "notebook_dir", ")", "\n", "do_setup", "(", "nb_fn", ".", "stem", ")", "\n", "_globals", "=", "{", "}", "\n", "exec", "(", "py_fn", ".", "read_text", "(", ")", ",", "_globals", ")", "\n", "# No asserts, just running is enough for now", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_load_all.test_load_hub_fwd_model": [[32, 42], ["pytest.mark.parametrize", "torch.tensor", "model", "output[].squeeze", "getattr", "dummy_inp.unsqueeze.unsqueeze", "len"], "function", ["None"], ["@", "pytest", ".", "mark", ".", "parametrize", "(", "\"model_name\"", ",", "model_names", ")", "\n", "def", "test_load_hub_fwd_model", "(", "model_name", ":", "str", ")", "->", "None", ":", "\n", "    ", "model", ",", "alphabet", "=", "getattr", "(", "esm", ".", "pretrained", ",", "model_name", ")", "(", ")", "\n", "# batch_size = 2, seq_len = 3, tokens within vocab", "\n", "dummy_inp", "=", "torch", ".", "tensor", "(", "[", "[", "0", ",", "1", ",", "2", "]", ",", "[", "3", ",", "4", ",", "5", "]", "]", ")", "\n", "if", "\"esm_msa\"", "in", "model_name", ":", "\n", "        ", "dummy_inp", "=", "dummy_inp", ".", "unsqueeze", "(", "0", ")", "\n", "", "output", "=", "model", "(", "dummy_inp", ")", "# dict", "\n", "logits", "=", "output", "[", "\"logits\"", "]", ".", "squeeze", "(", "0", ")", "\n", "assert", "logits", ".", "shape", "==", "(", "2", ",", "3", ",", "len", "(", "alphabet", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_load_all.test_load_local": [[44, 51], ["pytest.mark.parametrize", "model_name.endswith", "esm.pretrained.load_model_and_alphabet_local", "pathlib.Path.home"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_local"], ["", "@", "pytest", ".", "mark", ".", "parametrize", "(", "\"model_name\"", ",", "model_names", ")", "\n", "def", "test_load_local", "(", "model_name", ":", "str", ")", "->", "None", ":", "\n", "# Assumes everything has already been loaded & cached.", "\n", "    ", "local_path", "=", "Path", ".", "home", "(", ")", "/", "\".cache/torch/hub/checkpoints\"", "/", "(", "model_name", "+", "\".pt\"", ")", "\n", "if", "model_name", ".", "endswith", "(", "\"esm1v_t33_650M_UR90S\"", ")", ":", "\n", "        ", "return", "# skip; needs to get rerouted to specific instance", "\n", "", "model", ",", "alphabet", "=", "esm", ".", "pretrained", ".", "load_model_and_alphabet_local", "(", "local_path", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_alphabet._test_esm1b": [[6, 25], ["alphabet.get_batch_converter", "alphabet.get_batch_converter.", "torch.tensor", "torch.allclose"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_batch_converter"], ["def", "_test_esm1b", "(", "alphabet", ")", ":", "\n", "    ", "import", "torch", "\n", "\n", "batch_converter", "=", "alphabet", ".", "get_batch_converter", "(", ")", "\n", "\n", "data", "=", "[", "\n", "(", "\"protein1\"", ",", "\"MKTVRQG\"", ")", ",", "\n", "(", "\"protein2 with mask\"", ",", "\"KALTA<mask>ISQP\"", ")", ",", "\n", "(", "\"protein3\"", ",", "\"K A <mask> I S Q\"", ")", ",", "\n", "]", "\n", "_", ",", "_", ",", "batch_tokens", "=", "batch_converter", "(", "data", ")", "\n", "expected_tokens", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "0", ",", "20", ",", "15", ",", "11", ",", "7", ",", "10", ",", "16", ",", "6", ",", "2", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "[", "0", ",", "15", ",", "5", ",", "4", ",", "11", ",", "5", ",", "32", ",", "12", ",", "8", ",", "16", ",", "14", ",", "2", "]", ",", "\n", "[", "0", ",", "15", ",", "5", ",", "32", ",", "12", ",", "8", ",", "16", ",", "2", ",", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "]", "\n", ")", "\n", "assert", "torch", ".", "allclose", "(", "batch_tokens", ",", "expected_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_alphabet.test_esm1b_alphabet": [[27, 32], ["esm.pretrained.esm1b_t33_650M_UR50S", "test_alphabet._test_esm1b"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1b_t33_650M_UR50S", "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_alphabet._test_esm1b"], ["", "def", "test_esm1b_alphabet", "(", ")", ":", "\n", "    ", "import", "esm", "\n", "\n", "_", ",", "alphabet", "=", "esm", ".", "pretrained", ".", "esm1b_t33_650M_UR50S", "(", ")", "\n", "_test_esm1b", "(", "alphabet", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_alphabet.test_esm1v_alphabet": [[34, 39], ["esm.pretrained.esm1v_t33_650M_UR90S_1", "test_alphabet._test_esm1b"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1v_t33_650M_UR90S_1", "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_alphabet._test_esm1b"], ["", "def", "test_esm1v_alphabet", "(", ")", ":", "\n", "    ", "import", "esm", "\n", "\n", "_", ",", "alphabet", "=", "esm", ".", "pretrained", ".", "esm1v_t33_650M_UR90S_1", "(", ")", "\n", "_test_esm1b", "(", "alphabet", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_alphabet.test_esm1_msa1b_alphabet": [[41, 65], ["esm.pretrained.esm_msa1b_t12_100M_UR50S", "alphabet.get_batch_converter", "alphabet.get_batch_converter.", "torch.tensor", "torch.allclose"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_msa1b_t12_100M_UR50S", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_batch_converter"], ["", "def", "test_esm1_msa1b_alphabet", "(", ")", ":", "\n", "    ", "import", "torch", "\n", "import", "esm", "\n", "\n", "# Load ESM-1b model", "\n", "_", ",", "alphabet", "=", "esm", ".", "pretrained", ".", "esm_msa1b_t12_100M_UR50S", "(", ")", "\n", "batch_converter", "=", "alphabet", ".", "get_batch_converter", "(", ")", "\n", "\n", "data", "=", "[", "\n", "(", "\"protein1\"", ",", "\"MKTVRQG\"", ")", ",", "\n", "(", "\"protein2\"", ",", "\"KALTRAI\"", ")", ",", "\n", "(", "\"protein3\"", ",", "\"KAAISQQ\"", ")", ",", "\n", "]", "\n", "_", ",", "_", ",", "batch_tokens", "=", "batch_converter", "(", "data", ")", "\n", "expected_tokens", "=", "torch", ".", "tensor", "(", "\n", "[", "\n", "[", "\n", "[", "0", ",", "20", ",", "15", ",", "11", ",", "7", ",", "10", ",", "16", ",", "6", "]", ",", "\n", "[", "0", ",", "15", ",", "5", ",", "4", ",", "11", ",", "10", ",", "5", ",", "12", "]", ",", "\n", "[", "0", ",", "15", ",", "5", ",", "5", ",", "12", ",", "8", ",", "16", ",", "16", "]", ",", "\n", "]", "\n", "]", "\n", ")", "\n", "assert", "torch", ".", "allclose", "(", "batch_tokens", ",", "expected_tokens", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.test_readme_1": [[16, 20], ["torch.hub.load"], "function", ["None"], ["def", "test_readme_1", "(", ")", ":", "\n", "    ", "import", "torch", "\n", "\n", "model", ",", "alphabet", "=", "torch", ".", "hub", ".", "load", "(", "\"facebookresearch/esm:main\"", ",", "\"esm1b_t33_650M_UR50S\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.test_readme_2": [[22, 64], ["esm.pretrained.esm1b_t33_650M_UR50S", "alphabet.get_batch_converter", "model.eval", "alphabet.get_batch_converter.", "enumerate", "zip", "torch.no_grad", "model", "sequence_representations.append", "plt.matshow", "plt.title", "plt.show", "token_representations[].mean", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1b_t33_650M_UR50S", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_batch_converter"], ["", "def", "test_readme_2", "(", ")", ":", "\n", "    ", "import", "torch", "\n", "import", "esm", "\n", "\n", "# Load ESM-1b model", "\n", "model", ",", "alphabet", "=", "esm", ".", "pretrained", ".", "esm1b_t33_650M_UR50S", "(", ")", "\n", "batch_converter", "=", "alphabet", ".", "get_batch_converter", "(", ")", "\n", "model", ".", "eval", "(", ")", "# disables dropout for deterministic results", "\n", "\n", "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)", "\n", "data", "=", "[", "\n", "(", "\"protein1\"", ",", "\"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"", ")", ",", "\n", "(", "\"protein2\"", ",", "\"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"", ")", ",", "\n", "(", "\n", "\"protein2 with mask\"", ",", "\n", "\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"", "\n", ")", ",", "\n", "(", "\n", "\"protein3\"", ",", "\n", "\"K A <mask> I S Q\"", "\n", ")", ",", "\n", "]", "\n", "batch_labels", ",", "batch_strs", ",", "batch_tokens", "=", "batch_converter", "(", "data", ")", "\n", "\n", "# Extract per-residue representations (on CPU)", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "results", "=", "model", "(", "batch_tokens", ",", "repr_layers", "=", "[", "33", "]", ",", "return_contacts", "=", "True", ")", "\n", "", "token_representations", "=", "results", "[", "\"representations\"", "]", "[", "33", "]", "\n", "\n", "# Generate per-sequence representations via averaging", "\n", "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.", "\n", "sequence_representations", "=", "[", "]", "\n", "for", "i", ",", "(", "_", ",", "seq", ")", "in", "enumerate", "(", "data", ")", ":", "\n", "        ", "sequence_representations", ".", "append", "(", "token_representations", "[", "i", ",", "1", ":", "len", "(", "seq", ")", "+", "1", "]", ".", "mean", "(", "0", ")", ")", "\n", "\n", "# Look at the unsupervised self-attention map contact predictions", "\n", "", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "\n", "for", "(", "_", ",", "seq", ")", ",", "attention_contacts", "in", "zip", "(", "data", ",", "results", "[", "\"contacts\"", "]", ")", ":", "\n", "        ", "plt", ".", "matshow", "(", "attention_contacts", "[", ":", "len", "(", "seq", ")", ",", ":", "len", "(", "seq", ")", "]", ")", "\n", "plt", ".", "title", "(", "seq", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme._run_py_cmd": [[66, 70], ["cmd.replace", "subprocess.run"], "function", ["None"], ["", "", "def", "_run_py_cmd", "(", "cmd", ",", "**", "kwargs", ")", ":", "\n", "    ", "this_python", "=", "sys", ".", "executable", "\n", "cmd", ".", "replace", "(", "\"python\"", ",", "this_python", ")", "\n", "subprocess", ".", "run", "(", "cmd", ",", "shell", "=", "True", ",", "check", "=", "True", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.test_readme_3": [[72, 84], ["test_readme._run_py_cmd", "test_readme.confirm_all_tensors_equal"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme._run_py_cmd", "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.confirm_all_tensors_equal"], ["", "def", "test_readme_3", "(", ")", ":", "\n", "# NOTE modification on copy paste from README for speed:", "\n", "# * some_proteins -> few_proteins (subset)", "\n", "# * I computed reference values a while ago for: esm1b -> esm1 and layers 33 -> 34", "\n", "    ", "cmd", "=", "\"\"\"\npython scripts/extract.py esm1_t34_670M_UR50S examples/data/few_proteins.fasta examples/data/few_proteins_emb_esm1/ \\\n    --repr_layers 0 33 34 --include mean per_tok\n\"\"\"", "\n", "_run_py_cmd", "(", "cmd", ")", "\n", "confirm_all_tensors_equal", "(", "\n", "\"examples/few_proteins_emb_esm1/\"", ",", "\n", "\"https://dl.fbaipublicfiles.com/fair-esm/tests/some_proteins_emb_esm1_t34_670M_UR50S_ref\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.assert_pt_file_equal": [[87, 95], ["torch.load", "torch.load", "a[].keys", "b[].keys", "torch.allclose"], "function", ["None"], ["", "def", "assert_pt_file_equal", "(", "f", ",", "fref", ")", ":", "\n", "    ", "a", "=", "torch", ".", "load", "(", "f", ")", "\n", "b", "=", "torch", ".", "load", "(", "fref", ")", "\n", "# set intersection of dict keys:", "\n", "which_layers", "=", "a", "[", "\"representations\"", "]", ".", "keys", "(", ")", "&", "b", "[", "\"representations\"", "]", ".", "keys", "(", ")", "\n", "assert", "which_layers", ",", "\"Expected at least one layer appearing in both dumps\"", "\n", "for", "layer", "in", "which_layers", ":", "\n", "        ", "assert", "torch", ".", "allclose", "(", "a", "[", "\"representations\"", "]", "[", "layer", "]", ",", "b", "[", "\"representations\"", "]", "[", "layer", "]", ",", "atol", "=", "1e-3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.confirm_all_tensors_equal": [[97, 106], ["pathlib.Path().glob", "pathlib.Path", "tempfile.NamedTemporaryFile", "f.seek", "test_readme.assert_pt_file_equal", "requests.get", "shutil.copyfileobj"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.assert_pt_file_equal"], ["", "", "def", "confirm_all_tensors_equal", "(", "local_dir", ":", "str", ",", "ref_dir", ":", "str", ")", "->", "None", ":", "\n", "# TODO use pytest built-in fixtures for tmp_path https://docs.pytest.org/en/6.2.x/fixture.html#fixtures", "\n", "    ", "for", "fn", "in", "Path", "(", "local_dir", ")", ".", "glob", "(", "\"*.pt\"", ")", ":", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", "mode", "=", "\"w+b\"", ",", "prefix", "=", "fn", ".", "name", ")", "as", "f", ":", "\n", "            ", "ref_url", "=", "f\"{ref_dir}/{fn.name}\"", "\n", "with", "requests", ".", "get", "(", "ref_url", ",", "stream", "=", "True", ")", "as", "r", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "r", ".", "raw", ",", "f", ")", "\n", "", "f", ".", "seek", "(", "0", ")", "\n", "assert_pt_file_equal", "(", "fn", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.test_msa_transformers": [[108, 111], ["test_readme._test_msa_transformer", "test_readme._test_msa_transformer", "esm.pretrained.esm_msa1_t12_100M_UR50S", "esm.pretrained.esm_msa1b_t12_100M_UR50S"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme._test_msa_transformer", "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme._test_msa_transformer", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_msa1_t12_100M_UR50S", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_msa1b_t12_100M_UR50S"], ["", "", "", "def", "test_msa_transformers", "(", ")", ":", "\n", "    ", "_test_msa_transformer", "(", "*", "esm", ".", "pretrained", ".", "esm_msa1_t12_100M_UR50S", "(", ")", ")", "\n", "_test_msa_transformer", "(", "*", "esm", ".", "pretrained", ".", "esm_msa1b_t12_100M_UR50S", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme._test_msa_transformer": [[113, 127], ["alphabet.get_batch_converter", "alphabet.get_batch_converter.", "torch.no_grad", "model"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_batch_converter"], ["", "def", "_test_msa_transformer", "(", "model", ",", "alphabet", ")", ":", "\n", "    ", "batch_converter", "=", "alphabet", ".", "get_batch_converter", "(", ")", "\n", "# Make an \"MSA\" of size 3", "\n", "data", "=", "[", "\n", "(", "\"protein1\"", ",", "\"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"", ")", ",", "\n", "(", "\"protein2\"", ",", "\"MHTVRQSRLKSIVRILEMSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"", ")", ",", "\n", "(", "\"protein3\"", ",", "\"MHTVRQSRLKSIVRILEMSKEPVSGAQL---LSVSRQVIVQDIAYLRSLGYNIVAT----VLAGG\"", ")", ",", "\n", "]", "\n", "batch_labels", ",", "batch_strs", ",", "batch_tokens", "=", "batch_converter", "(", "data", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "results", "=", "model", "(", "batch_tokens", ",", "repr_layers", "=", "[", "12", "]", ",", "return_contacts", "=", "True", ")", "\n", "", "token_representations", "=", "results", "[", "\"representations\"", "]", "[", "12", "]", "\n", "assert", "token_representations", ".", "shape", "==", "(", "1", ",", "3", ",", "66", ",", "768", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.test_variant_readme_1": [[129, 141], ["test_readme._run_py_cmd"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme._run_py_cmd"], ["", "def", "test_variant_readme_1", "(", ")", ":", "\n", "    ", "cmd", "=", "\"\"\"\npython predict.py \\\n    --model-location esm1v_t33_650M_UR90S_1 esm1v_t33_650M_UR90S_2 esm1v_t33_650M_UR90S_3 esm1v_t33_650M_UR90S_4 esm1v_t33_650M_UR90S_5 \\\n    --sequence HPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW \\\n    --dms-input ./data/BLAT_ECOLX_Ranganathan2015.csv \\\n    --mutation-col mutant \\\n    --dms-output ./data/BLAT_ECOLX_Ranganathan2015_labeled.csv \\\n    --offset-idx 24 \\\n    --scoring-strategy wt-marginals\n    \"\"\"", "\n", "_run_py_cmd", "(", "cmd", ",", "cwd", "=", "\"examples/variant-prediction/\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme.test_variant_readme_2": [[143, 156], ["test_readme._run_py_cmd"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_readme._run_py_cmd"], ["", "def", "test_variant_readme_2", "(", ")", ":", "\n", "    ", "cmd", "=", "\"\"\"\npython predict.py \\\n    --model-location esm_msa1b_t12_100M_UR50S \\\n    --sequence HPETLVKVKDAEDQLGARVGYIELDLNSGKILESFRPEERFPMMSTFKVLLCGAVLSRVDAGQEQLGRRIHYSQNDLVEYSPVTEKHLTDGMTVRELCSAAITMSDNTAANLLLTTIGGPKELTAFLHNMGDHVTRLDRWEPELNEAIPNDERDTTMPAAMATTLRKLLTGELLTLASRQQLIDWMEADKVAGPLLRSALPAGWFIADKSGAGERGSRGIIAALGPDGKPSRIVVIYTTGSQATMDERNRQIAEIGASLIKHW \\\n    --dms-input ./data/BLAT_ECOLX_Ranganathan2015.csv \\\n    --mutation-col mutant \\\n    --dms-output ./data/BLAT_ECOLX_Ranganathan2015_labeled.csv \\\n    --offset-idx 24 \\\n    --scoring-strategy masked-marginals \\\n    --msa-path ./data/BLAT_ECOLX_1_b0.5.a3m\n    \"\"\"", "\n", "_run_py_cmd", "(", "cmd", ",", "cwd", "=", "\"examples/variant-prediction/\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.tests.test_inverse_folding.test_esm_if1": [[1, 66], ["esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "model.eval.eval", "esm.inverse_folding.util.CoordBatchConverter", "open", "json.load", "torch.no_grad", "print", "esm.inverse_folding.util.CoordBatchConverter.", "model.eval.forward", "torch.nn.functional.cross_entropy", "torch.all", "np.testing.assert_allclose", "print", "tqdm", "Path().absolute", "torch.all", "torch.sum", "torch.sum", "torch.exp().item", "esm.inverse_folding.util.CoordBatchConverter.", "model.eval.forward", "special_ortho_group.rvs", "torch.tensor", "torch.matmul", "model.eval.forward", "np.testing.assert_allclose", "torch.isfinite", "torch.any", "logits.detach().numpy", "logits_rotated.detach().numpy", "Path", "torch.exp", "torch.isnan", "logits.detach", "logits_rotated.detach"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_if1_gvp4_t16_142M_UR50", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.forward", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.forward", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.forward"], ["def", "test_esm_if1", "(", ")", ":", "\n", "\n", "    ", "import", "json", "\n", "import", "numpy", "as", "np", "\n", "from", "pathlib", "import", "Path", "\n", "from", "scipy", ".", "stats", "import", "special_ortho_group", "\n", "from", "tqdm", "import", "tqdm", "\n", "import", "torch", "\n", "\n", "import", "esm", "\n", "import", "esm", ".", "inverse_folding", "\n", "\n", "example_file", "=", "Path", "(", "__file__", ")", ".", "absolute", "(", ")", ".", "parent", "/", "\"inverse_folding_test_example.json\"", "\n", "with", "open", "(", "example_file", ")", "as", "f", ":", "\n", "        ", "examples", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "model", ",", "alphabet", "=", "esm", ".", "pretrained", ".", "esm_if1_gvp4_t16_142M_UR50", "(", ")", "\n", "model", "=", "model", ".", "eval", "(", ")", "\n", "batch_converter", "=", "esm", ".", "inverse_folding", ".", "util", ".", "CoordBatchConverter", "(", "alphabet", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "print", "(", "'Testing batch inference on 3 examples...'", ")", "\n", "# Test batch with multiple examples", "\n", "batch", "=", "[", "(", "e", "[", "\"coords\"", "]", ",", "None", ",", "e", "[", "\"seq\"", "]", ")", "for", "e", "in", "examples", "[", ":", "3", "]", "]", "\n", "coords", ",", "confidence", ",", "strs", ",", "tokens", ",", "padding_mask", "=", "(", "\n", "batch_converter", "(", "batch", ")", "\n", ")", "\n", "prev_output_tokens", "=", "tokens", "[", ":", ",", ":", "-", "1", "]", "\n", "target", "=", "tokens", "[", ":", ",", "1", ":", "]", "\n", "logits", ",", "_", "=", "model", ".", "forward", "(", "coords", ",", "padding_mask", ",", "confidence", ",", "\n", "prev_output_tokens", ")", "\n", "loss", "=", "torch", ".", "nn", ".", "functional", ".", "cross_entropy", "(", "logits", ",", "target", ",", "reduction", "=", "'none'", ")", "\n", "coord_mask", "=", "torch", ".", "all", "(", "torch", ".", "all", "(", "torch", ".", "isfinite", "(", "coords", ")", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "coord_mask", "=", "coord_mask", "[", ":", ",", "1", ":", "-", "1", "]", "\n", "avgloss", "=", "torch", ".", "sum", "(", "loss", "*", "coord_mask", ")", "/", "torch", ".", "sum", "(", "coord_mask", ")", "\n", "expected_ppl", "=", "4.40", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "\n", "expected_ppl", ",", "\n", "torch", ".", "exp", "(", "avgloss", ")", ".", "item", "(", ")", ",", "\n", "atol", "=", "1e-02", ",", "\n", ")", "\n", "\n", "print", "(", "'Testing on 10 examples from validation set...'", ")", "\n", "# Test batch with single example", "\n", "for", "example", "in", "tqdm", "(", "examples", ")", ":", "\n", "            ", "batch", "=", "[", "(", "example", "[", "\"coords\"", "]", ",", "None", ",", "example", "[", "\"seq\"", "]", ")", "]", "\n", "coords", ",", "confidence", ",", "strs", ",", "tokens", ",", "padding_mask", "=", "(", "\n", "batch_converter", "(", "batch", ")", "\n", ")", "\n", "prev_output_tokens", "=", "tokens", "[", ":", ",", ":", "-", "1", "]", "\n", "target", "=", "tokens", "[", ":", ",", "1", ":", "]", "\n", "logits", ",", "_", "=", "model", ".", "forward", "(", "coords", ",", "padding_mask", ",", "confidence", ",", "\n", "prev_output_tokens", ")", "\n", "assert", "torch", ".", "any", "(", "torch", ".", "isnan", "(", "logits", ")", ")", "==", "False", "\n", "\n", "# Test equivariance", "\n", "R", "=", "special_ortho_group", ".", "rvs", "(", "3", ")", "\n", "R", "=", "torch", ".", "tensor", "(", "R", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "coords", "=", "torch", ".", "matmul", "(", "coords", ",", "R", ")", "\n", "logits_rotated", ",", "_", "=", "model", ".", "forward", "(", "coords", ",", "padding_mask", ",", "\n", "confidence", ",", "prev_output_tokens", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "\n", "logits", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "logits_rotated", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "atol", "=", "1e-01", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState.__init__": [[25, 28], ["object.__init__", "multihead_attention.FairseqIncrementalState.init_incremental_state"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState.init_incremental_state"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "init_incremental_state", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState.init_incremental_state": [[29, 31], ["str", "uuid.uuid4"], "methods", ["None"], ["", "def", "init_incremental_state", "(", "self", ")", ":", "\n", "        ", "self", ".", "_incremental_state_id", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState._get_full_incremental_state_key": [[32, 34], ["None"], "methods", ["None"], ["", "def", "_get_full_incremental_state_key", "(", "self", ",", "key", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "\"{}.{}\"", ".", "format", "(", "self", ".", "_incremental_state_id", ",", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState.get_incremental_state": [[35, 45], ["multihead_attention.FairseqIncrementalState._get_full_incremental_state_key"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState._get_full_incremental_state_key"], ["", "def", "get_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", "]", ",", "\n", "key", ":", "str", ",", "\n", ")", "->", "Optional", "[", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", ":", "\n", "        ", "\"\"\"Helper for getting incremental state for an nn.Module.\"\"\"", "\n", "full_key", "=", "self", ".", "_get_full_incremental_state_key", "(", "key", ")", "\n", "if", "incremental_state", "is", "None", "or", "full_key", "not", "in", "incremental_state", ":", "\n", "            ", "return", "None", "\n", "", "return", "incremental_state", "[", "full_key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState.set_incremental_state": [[46, 57], ["multihead_attention.FairseqIncrementalState._get_full_incremental_state_key"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState._get_full_incremental_state_key"], ["", "def", "set_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", "]", ",", "\n", "key", ":", "str", ",", "\n", "value", ":", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", ",", "\n", ")", "->", "Optional", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", "]", ":", "\n", "        ", "\"\"\"Helper for setting incremental state for an nn.Module.\"\"\"", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "full_key", "=", "self", ".", "_get_full_incremental_state_key", "(", "key", ")", "\n", "incremental_state", "[", "full_key", "]", "=", "value", "\n", "", "return", "incremental_state", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention.__init__": [[73, 130], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "multihead_attention.MultiheadAttention.reset_parameters", "hasattr", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention.reset_parameters"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embed_dim", ",", "\n", "num_heads", ",", "\n", "kdim", "=", "None", ",", "\n", "vdim", "=", "None", ",", "\n", "dropout", "=", "0.0", ",", "\n", "bias", "=", "True", ",", "\n", "add_bias_kv", "=", "False", ",", "\n", "add_zero_attn", "=", "False", ",", "\n", "self_attention", "=", "False", ",", "\n", "encoder_decoder_attention", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "kdim", "=", "kdim", "if", "kdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "vdim", "=", "vdim", "if", "vdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "qkv_same_dim", "=", "self", ".", "kdim", "==", "embed_dim", "and", "self", ".", "vdim", "==", "embed_dim", "\n", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "assert", "(", "\n", "self", ".", "head_dim", "*", "num_heads", "==", "self", ".", "embed_dim", "\n", ")", ",", "\"embed_dim must be divisible by num_heads\"", "\n", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "self_attention", "=", "self_attention", "\n", "self", ".", "encoder_decoder_attention", "=", "encoder_decoder_attention", "\n", "\n", "assert", "not", "self", ".", "self_attention", "or", "self", ".", "qkv_same_dim", ",", "(", "\n", "\"Self-attention requires query, key and \"", "\"value to be of the same size\"", "\n", ")", "\n", "\n", "self", ".", "k_proj", "=", "nn", ".", "Linear", "(", "self", ".", "kdim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "self", ".", "v_proj", "=", "nn", ".", "Linear", "(", "self", ".", "vdim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "self", ".", "q_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "add_bias_kv", ":", "\n", "            ", "self", ".", "bias_k", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "bias_v", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias_k", "=", "self", ".", "bias_v", "=", "None", "\n", "\n", "", "self", ".", "add_zero_attn", "=", "add_zero_attn", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n", "self", ".", "enable_torch_version", "=", "False", "\n", "if", "hasattr", "(", "F", ",", "\"multi_head_attention_forward\"", ")", ":", "\n", "            ", "self", ".", "enable_torch_version", "=", "True", "\n", "", "else", ":", "\n", "            ", "self", ".", "enable_torch_version", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention.prepare_for_onnx_export_": [[131, 133], ["None"], "methods", ["None"], ["", "", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention.reset_parameters": [[134, 153], ["torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "math.sqrt", "math.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "# Empirically observed the convergence to be much better with", "\n", "# the scaled initialization", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "k_proj", ".", "weight", ",", "gain", "=", "1", "/", "math", ".", "sqrt", "(", "2", ")", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "v_proj", ".", "weight", ",", "gain", "=", "1", "/", "math", ".", "sqrt", "(", "2", ")", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "q_proj", ".", "weight", ",", "gain", "=", "1", "/", "math", ".", "sqrt", "(", "2", ")", ")", "\n", "", "else", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "k_proj", ".", "weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "v_proj", ".", "weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "q_proj", ".", "weight", ")", "\n", "\n", "", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "out_proj", ".", "weight", ")", "\n", "if", "self", ".", "out_proj", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "out_proj", ".", "bias", ",", "0.0", ")", "\n", "", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_k", ")", "\n", "", "if", "self", ".", "bias_v", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention.forward": [[154, 398], ["query.size", "multihead_attention.MultiheadAttention.contiguous().view().transpose", "torch.cat.size", "torch.cat.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "multihead_attention.MultiheadAttention.apply_sparse_mask", "multihead_attention.utils_softmax", "utils_softmax.type_as", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "multihead_attention.MultiheadAttention.out_proj", "list", "torch.multi_head_attention_forward", "torch.multi_head_attention_forward", "multihead_attention.MultiheadAttention._get_input_buffer", "multihead_attention.MultiheadAttention.q_proj", "multihead_attention.MultiheadAttention.k_proj", "multihead_attention.MultiheadAttention.v_proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "multihead_attention.MultiheadAttention._append_prev_key_padding_mask", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "multihead_attention.MultiheadAttention._set_input_buffer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.transpose", "torch.cat.transpose", "list", "attn_mask.repeat.repeat.unsqueeze", "attn_weights.mean.mean.view", "attn_weights.mean.mean.masked_fill", "attn_weights.mean.mean.view", "utils_softmax.type_as", "list", "attn.transpose().contiguous().view.transpose().contiguous().view.contiguous().view", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "utils_softmax.view().transpose", "query.size", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "multihead_attention.MultiheadAttention.q_proj", "multihead_attention.MultiheadAttention.q_proj", "multihead_attention.MultiheadAttention.k_proj", "multihead_attention.MultiheadAttention.v_proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "multihead_attention.MultiheadAttention.contiguous().view", "_prev_key.view", "_prev_value.view", "torch.cat.dim", "torch.cat.dim", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attn_weights.mean.mean.size", "attn_mask.repeat.repeat.repeat", "torch.cat.unsqueeze().unsqueeze().to", "torch.cat.unsqueeze().unsqueeze().to", "float", "attn.transpose().contiguous().view.transpose().contiguous().view.size", "attn.transpose().contiguous().view.transpose().contiguous().view.size", "attn_weights.mean.mean.mean", "multihead_attention.MultiheadAttention.k_proj", "multihead_attention.MultiheadAttention.v_proj", "multihead_attention.MultiheadAttention.bias_k.repeat", "multihead_attention.MultiheadAttention.bias_v.repeat", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.size", "torch.cat.size", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "attn_weights.mean.mean.size", "attn.transpose().contiguous().view.transpose().contiguous().view.contiguous", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "utils_softmax.view", "attn_mask.repeat.repeat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "multihead_attention.MultiheadAttention.contiguous", "attn_mask.repeat.repeat.new_zeros", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.cat.unsqueeze().unsqueeze", "torch.cat.unsqueeze().unsqueeze", "attn_mask.repeat.repeat.size", "torch.cat.size", "torch.cat.size", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "attn_mask.repeat.repeat.size", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.size", "torch.cat.size"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention.apply_sparse_mask", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.utils_softmax", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._append_prev_key_padding_mask", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._set_input_buffer"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "query", ",", "\n", "key", ":", "Optional", "[", "Tensor", "]", ",", "\n", "value", ":", "Optional", "[", "Tensor", "]", ",", "\n", "key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "incremental_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", "]", "=", "None", ",", "\n", "need_weights", ":", "bool", "=", "True", ",", "\n", "static_kv", ":", "bool", "=", "False", ",", "\n", "attn_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "before_softmax", ":", "bool", "=", "False", ",", "\n", "need_head_weights", ":", "bool", "=", "False", ",", "\n", ")", "->", "Tuple", "[", "Tensor", ",", "Optional", "[", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_softmax (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        \"\"\"", "\n", "if", "need_head_weights", ":", "\n", "            ", "need_weights", "=", "True", "\n", "\n", "", "tgt_len", ",", "bsz", ",", "embed_dim", "=", "query", ".", "size", "(", ")", "\n", "assert", "embed_dim", "==", "self", ".", "embed_dim", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "embed_dim", "]", "\n", "\n", "if", "(", "\n", "self", ".", "enable_torch_version", "\n", "and", "not", "self", ".", "onnx_trace", "\n", "and", "incremental_state", "is", "None", "\n", "and", "not", "static_kv", "\n", "# A workaround for quantization to work. Otherwise JIT compilation", "\n", "# treats bias in linear module as method.", "\n", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", "\n", "and", "not", "need_head_weights", "\n", ")", ":", "\n", "            ", "assert", "key", "is", "not", "None", "and", "value", "is", "not", "None", "\n", "return", "F", ".", "multi_head_attention_forward", "(", "\n", "query", ",", "\n", "key", ",", "\n", "value", ",", "\n", "self", ".", "embed_dim", ",", "\n", "self", ".", "num_heads", ",", "\n", "torch", ".", "empty", "(", "[", "0", "]", ")", ",", "\n", "torch", ".", "cat", "(", "(", "self", ".", "q_proj", ".", "bias", ",", "self", ".", "k_proj", ".", "bias", ",", "self", ".", "v_proj", ".", "bias", ")", ")", ",", "\n", "self", ".", "bias_k", ",", "\n", "self", ".", "bias_v", ",", "\n", "self", ".", "add_zero_attn", ",", "\n", "self", ".", "dropout", ",", "\n", "self", ".", "out_proj", ".", "weight", ",", "\n", "self", ".", "out_proj", ".", "bias", ",", "\n", "self", ".", "training", ",", "\n", "key_padding_mask", ",", "\n", "need_weights", ",", "\n", "attn_mask", ",", "\n", "use_separate_proj_weight", "=", "True", ",", "\n", "q_proj_weight", "=", "self", ".", "q_proj", ".", "weight", ",", "\n", "k_proj_weight", "=", "self", ".", "k_proj", ".", "weight", ",", "\n", "v_proj_weight", "=", "self", ".", "v_proj", ".", "weight", ",", "\n", ")", "\n", "\n", "", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "saved_state", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "saved_state", "is", "not", "None", "and", "\"prev_key\"", "in", "saved_state", ":", "\n", "# previous time steps are cached - no need to recompute", "\n", "# key and value if they are static", "\n", "                ", "if", "static_kv", ":", "\n", "                    ", "assert", "self", ".", "encoder_decoder_attention", "and", "not", "self", ".", "self_attention", "\n", "key", "=", "value", "=", "None", "\n", "", "", "", "else", ":", "\n", "            ", "saved_state", "=", "None", "\n", "\n", "", "if", "self", ".", "self_attention", ":", "\n", "            ", "q", "=", "self", ".", "q_proj", "(", "query", ")", "\n", "k", "=", "self", ".", "k_proj", "(", "query", ")", "\n", "v", "=", "self", ".", "v_proj", "(", "query", ")", "\n", "", "elif", "self", ".", "encoder_decoder_attention", ":", "\n", "# encoder-decoder attention", "\n", "            ", "q", "=", "self", ".", "q_proj", "(", "query", ")", "\n", "if", "key", "is", "None", ":", "\n", "                ", "assert", "value", "is", "None", "\n", "k", "=", "v", "=", "None", "\n", "", "else", ":", "\n", "                ", "k", "=", "self", ".", "k_proj", "(", "key", ")", "\n", "v", "=", "self", ".", "v_proj", "(", "key", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "assert", "key", "is", "not", "None", "and", "value", "is", "not", "None", "\n", "q", "=", "self", ".", "q_proj", "(", "query", ")", "\n", "k", "=", "self", ".", "k_proj", "(", "key", ")", "\n", "v", "=", "self", ".", "v_proj", "(", "value", ")", "\n", "", "q", "*=", "self", ".", "scaling", "\n", "\n", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "bias_v", "is", "not", "None", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "self", ".", "bias_k", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "self", ".", "bias_v", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "attn_mask", "=", "torch", ".", "cat", "(", "\n", "[", "attn_mask", ",", "attn_mask", ".", "new_zeros", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", "\n", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "                ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "key_padding_mask", ",", "\n", "key_padding_mask", ".", "new_zeros", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", ",", "\n", "]", ",", "\n", "dim", "=", "1", ",", "\n", ")", "\n", "\n", "", "", "q", "=", "q", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "k", "is", "not", "None", ":", "\n", "            ", "k", "=", "k", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "v", "is", "not", "None", ":", "\n", "            ", "v", "=", "v", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "if", "saved_state", "is", "not", "None", ":", "\n", "# saved states are stored with shape (bsz, num_heads, seq_len, head_dim)", "\n", "            ", "if", "\"prev_key\"", "in", "saved_state", ":", "\n", "                ", "_prev_key", "=", "saved_state", "[", "\"prev_key\"", "]", "\n", "assert", "_prev_key", "is", "not", "None", "\n", "prev_key", "=", "_prev_key", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "k", "=", "prev_key", "\n", "", "else", ":", "\n", "                    ", "assert", "k", "is", "not", "None", "\n", "k", "=", "torch", ".", "cat", "(", "[", "prev_key", ",", "k", "]", ",", "dim", "=", "1", ")", "\n", "", "", "if", "\"prev_value\"", "in", "saved_state", ":", "\n", "                ", "_prev_value", "=", "saved_state", "[", "\"prev_value\"", "]", "\n", "assert", "_prev_value", "is", "not", "None", "\n", "prev_value", "=", "_prev_value", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "v", "=", "prev_value", "\n", "", "else", ":", "\n", "                    ", "assert", "v", "is", "not", "None", "\n", "v", "=", "torch", ".", "cat", "(", "[", "prev_value", ",", "v", "]", ",", "dim", "=", "1", ")", "\n", "", "", "prev_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", "\n", "if", "\"prev_key_padding_mask\"", "in", "saved_state", ":", "\n", "                ", "prev_key_padding_mask", "=", "saved_state", "[", "\"prev_key_padding_mask\"", "]", "\n", "", "assert", "k", "is", "not", "None", "and", "v", "is", "not", "None", "\n", "key_padding_mask", "=", "MultiheadAttention", ".", "_append_prev_key_padding_mask", "(", "\n", "key_padding_mask", "=", "key_padding_mask", ",", "\n", "prev_key_padding_mask", "=", "prev_key_padding_mask", ",", "\n", "batch_size", "=", "bsz", ",", "\n", "src_len", "=", "k", ".", "size", "(", "1", ")", ",", "\n", "static_kv", "=", "static_kv", ",", "\n", ")", "\n", "\n", "saved_state", "[", "\"prev_key\"", "]", "=", "k", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "saved_state", "[", "\"prev_value\"", "]", "=", "v", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "saved_state", "[", "\"prev_key_padding_mask\"", "]", "=", "key_padding_mask", "\n", "# In this branch incremental_state is never None", "\n", "assert", "incremental_state", "is", "not", "None", "\n", "incremental_state", "=", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "assert", "k", "is", "not", "None", "\n", "src_len", "=", "k", ".", "size", "(", "1", ")", "\n", "\n", "# This is part of a workaround to get around fork/join parallelism", "\n", "# not supporting Optional types.", "\n", "if", "key_padding_mask", "is", "not", "None", "and", "key_padding_mask", ".", "dim", "(", ")", "==", "0", ":", "\n", "            ", "key_padding_mask", "=", "None", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "assert", "key_padding_mask", ".", "size", "(", "0", ")", "==", "bsz", "\n", "assert", "key_padding_mask", ".", "size", "(", "1", ")", "==", "src_len", "\n", "\n", "", "if", "self", ".", "add_zero_attn", ":", "\n", "            ", "assert", "v", "is", "not", "None", "\n", "src_len", "+=", "1", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "k", ".", "new_zeros", "(", "(", "k", ".", "size", "(", "0", ")", ",", "1", ")", "+", "k", ".", "size", "(", ")", "[", "2", ":", "]", ")", "]", ",", "dim", "=", "1", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "v", ".", "new_zeros", "(", "(", "v", ".", "size", "(", "0", ")", ",", "1", ")", "+", "v", ".", "size", "(", ")", "[", "2", ":", "]", ")", "]", ",", "dim", "=", "1", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "attn_mask", "=", "torch", ".", "cat", "(", "\n", "[", "attn_mask", ",", "attn_mask", ".", "new_zeros", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", "\n", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "                ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "key_padding_mask", ",", "\n", "torch", ".", "zeros", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", ".", "type_as", "(", "key_padding_mask", ")", ",", "\n", "]", ",", "\n", "dim", "=", "1", ",", "\n", ")", "\n", "\n", "", "", "attn_weights", "=", "torch", ".", "bmm", "(", "q", ",", "k", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "attn_weights", "=", "MultiheadAttention", ".", "apply_sparse_mask", "(", "attn_weights", ",", "tgt_len", ",", "src_len", ",", "bsz", ")", "\n", "\n", "assert", "list", "(", "attn_weights", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", "]", "\n", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "attn_mask", ".", "unsqueeze", "(", "0", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "                ", "attn_mask", "=", "attn_mask", ".", "repeat", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "", "attn_weights", "+=", "attn_mask", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "# don't attend to padding symbols", "\n", "            ", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "attn_weights", "=", "attn_weights", ".", "masked_fill", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ".", "to", "(", "torch", ".", "bool", ")", ",", "float", "(", "\"-inf\"", ")", "\n", ")", "\n", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "", "if", "before_softmax", ":", "\n", "            ", "return", "attn_weights", ",", "v", "\n", "\n", "", "attn_weights_float", "=", "utils_softmax", "(", "attn_weights", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", "\n", "attn_weights", "=", "attn_weights_float", ".", "type_as", "(", "attn_weights", ")", "\n", "attn_probs", "=", "F", ".", "dropout", "(", "\n", "attn_weights_float", ".", "type_as", "(", "attn_weights", ")", ",", "\n", "p", "=", "self", ".", "dropout", ",", "\n", "training", "=", "self", ".", "training", ",", "\n", ")", "\n", "assert", "v", "is", "not", "None", "\n", "attn", "=", "torch", ".", "bmm", "(", "attn_probs", ",", "v", ")", "\n", "assert", "list", "(", "attn", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "self", ".", "head_dim", "]", "\n", "if", "self", ".", "onnx_trace", "and", "attn", ".", "size", "(", "1", ")", "==", "1", ":", "\n", "# when ONNX tracing a single decoder step (sequence length == 1)", "\n", "# the transpose is a no-op copy before view, thus unnecessary", "\n", "            ", "attn", "=", "attn", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "", "attn", "=", "self", ".", "out_proj", "(", "attn", ")", "\n", "attn_weights", ":", "Optional", "[", "Tensor", "]", "=", "None", "\n", "if", "need_weights", ":", "\n", "            ", "attn_weights", "=", "attn_weights_float", ".", "view", "(", "\n", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", "\n", ")", ".", "transpose", "(", "1", ",", "0", ")", "\n", "if", "not", "need_head_weights", ":", "\n", "# average attention weights over heads", "\n", "                ", "attn_weights", "=", "attn_weights", ".", "mean", "(", "dim", "=", "0", ")", "\n", "\n", "", "", "return", "attn", ",", "attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._append_prev_key_padding_mask": [[399, 434], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prev_key_padding_mask.float", "key_padding_mask.float", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prev_key_padding_mask.float", "torch.zeros.float", "torch.zeros.float", "prev_key_padding_mask.size", "torch.zeros.float", "torch.zeros.float", "key_padding_mask.float", "key_padding_mask.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_append_prev_key_padding_mask", "(", "\n", "key_padding_mask", ":", "Optional", "[", "Tensor", "]", ",", "\n", "prev_key_padding_mask", ":", "Optional", "[", "Tensor", "]", ",", "\n", "batch_size", ":", "int", ",", "\n", "src_len", ":", "int", ",", "\n", "static_kv", ":", "bool", ",", "\n", ")", "->", "Optional", "[", "Tensor", "]", ":", "\n", "# saved key padding masks have shape (bsz, seq_len)", "\n", "        ", "if", "prev_key_padding_mask", "is", "not", "None", "and", "static_kv", ":", "\n", "            ", "new_key_padding_mask", "=", "prev_key_padding_mask", "\n", "", "elif", "prev_key_padding_mask", "is", "not", "None", "and", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "new_key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "prev_key_padding_mask", ".", "float", "(", ")", ",", "key_padding_mask", ".", "float", "(", ")", "]", ",", "dim", "=", "1", "\n", ")", "\n", "# During incremental decoding, as the padding token enters and", "\n", "# leaves the frame, there will be a time when prev or current", "\n", "# is None", "\n", "", "elif", "prev_key_padding_mask", "is", "not", "None", ":", "\n", "            ", "filler", "=", "torch", ".", "zeros", "(", "\n", "(", "batch_size", ",", "src_len", "-", "prev_key_padding_mask", ".", "size", "(", "1", ")", ")", ",", "\n", "device", "=", "prev_key_padding_mask", ".", "device", ",", "\n", ")", "\n", "new_key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "prev_key_padding_mask", ".", "float", "(", ")", ",", "filler", ".", "float", "(", ")", "]", ",", "dim", "=", "1", "\n", ")", "\n", "", "elif", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "filler", "=", "torch", ".", "zeros", "(", "\n", "(", "batch_size", ",", "src_len", "-", "key_padding_mask", ".", "size", "(", "1", ")", ")", ",", "\n", "device", "=", "key_padding_mask", ".", "device", ",", "\n", ")", "\n", "new_key_padding_mask", "=", "torch", ".", "cat", "(", "[", "filler", ".", "float", "(", ")", ",", "key_padding_mask", ".", "float", "(", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "new_key_padding_mask", "=", "prev_key_padding_mask", "\n", "", "return", "new_key_padding_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention.reorder_incremental_state": [[435, 452], ["multihead_attention.MultiheadAttention._get_input_buffer", "multihead_attention.MultiheadAttention.keys", "multihead_attention.MultiheadAttention._set_input_buffer", "input_buffer_k.index_select", "input_buffer_k.size", "new_order.size"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._set_input_buffer"], ["", "@", "torch", ".", "jit", ".", "export", "\n", "def", "reorder_incremental_state", "(", "\n", "self", ",", "incremental_state", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", ",", "new_order", ":", "Tensor", "\n", ")", ":", "\n", "        ", "\"\"\"Reorder buffered internal state (for incremental generation).\"\"\"", "\n", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "for", "k", "in", "input_buffer", ".", "keys", "(", ")", ":", "\n", "                ", "input_buffer_k", "=", "input_buffer", "[", "k", "]", "\n", "if", "input_buffer_k", "is", "not", "None", ":", "\n", "                    ", "if", "self", ".", "encoder_decoder_attention", "and", "input_buffer_k", ".", "size", "(", "0", ")", "==", "new_order", ".", "size", "(", "\n", "0", "\n", ")", ":", "\n", "                        ", "break", "\n", "", "input_buffer", "[", "k", "]", "=", "input_buffer_k", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "", "incremental_state", "=", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "", "return", "incremental_state", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._get_input_buffer": [[453, 462], ["multihead_attention.MultiheadAttention.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState.get_incremental_state"], ["", "def", "_get_input_buffer", "(", "\n", "self", ",", "incremental_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", "]", "\n", ")", "->", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", ":", "\n", "        ", "result", "=", "self", ".", "get_incremental_state", "(", "incremental_state", ",", "\"attn_state\"", ")", "\n", "if", "result", "is", "not", "None", ":", "\n", "            ", "return", "result", "\n", "", "else", ":", "\n", "            ", "empty_result", ":", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "=", "{", "}", "\n", "return", "empty_result", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention._set_input_buffer": [[463, 469], ["multihead_attention.MultiheadAttention.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.FairseqIncrementalState.set_incremental_state"], ["", "", "def", "_set_input_buffer", "(", "\n", "self", ",", "\n", "incremental_state", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", "]", ",", "\n", "buffer", ":", "Dict", "[", "str", ",", "Optional", "[", "Tensor", "]", "]", ",", "\n", ")", ":", "\n", "        ", "return", "self", ".", "set_incremental_state", "(", "incremental_state", ",", "\"attn_state\"", ",", "buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention.apply_sparse_mask": [[470, 472], ["None"], "methods", ["None"], ["", "def", "apply_sparse_mask", "(", "attn_weights", ",", "tgt_len", ":", "int", ",", "src_len", ":", "int", ",", "bsz", ":", "int", ")", ":", "\n", "        ", "return", "attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.MultiheadAttention.upgrade_state_dict_named": [[473, 501], ["state_dict.keys", "items_to_add.items", "k.endswith", "int", "keys_to_remove.append", "state_dict.keys", "int", "keys_to_remove.append"], "methods", ["None"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "prefix", "=", "name", "+", "\".\"", "if", "name", "!=", "\"\"", "else", "\"\"", "\n", "items_to_add", "=", "{", "}", "\n", "keys_to_remove", "=", "[", "]", "\n", "for", "k", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "if", "k", ".", "endswith", "(", "prefix", "+", "\"in_proj_weight\"", ")", ":", "\n", "# in_proj_weight used to be q + k + v with same dimensions", "\n", "                ", "dim", "=", "int", "(", "state_dict", "[", "k", "]", ".", "shape", "[", "0", "]", "/", "3", ")", "\n", "items_to_add", "[", "prefix", "+", "\"q_proj.weight\"", "]", "=", "state_dict", "[", "k", "]", "[", ":", "dim", "]", "\n", "items_to_add", "[", "prefix", "+", "\"k_proj.weight\"", "]", "=", "state_dict", "[", "k", "]", "[", "dim", ":", "2", "*", "dim", "]", "\n", "items_to_add", "[", "prefix", "+", "\"v_proj.weight\"", "]", "=", "state_dict", "[", "k", "]", "[", "2", "*", "dim", ":", "]", "\n", "\n", "keys_to_remove", ".", "append", "(", "k", ")", "\n", "\n", "k_bias", "=", "prefix", "+", "\"in_proj_bias\"", "\n", "if", "k_bias", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "dim", "=", "int", "(", "state_dict", "[", "k", "]", ".", "shape", "[", "0", "]", "/", "3", ")", "\n", "items_to_add", "[", "prefix", "+", "\"q_proj.bias\"", "]", "=", "state_dict", "[", "k_bias", "]", "[", ":", "dim", "]", "\n", "items_to_add", "[", "prefix", "+", "\"k_proj.bias\"", "]", "=", "state_dict", "[", "k_bias", "]", "[", "dim", ":", "2", "*", "dim", "]", "\n", "items_to_add", "[", "prefix", "+", "\"v_proj.bias\"", "]", "=", "state_dict", "[", "k_bias", "]", "[", "2", "*", "dim", ":", "]", "\n", "\n", "keys_to_remove", ".", "append", "(", "prefix", "+", "\"in_proj_bias\"", ")", "\n", "\n", "", "", "", "for", "k", "in", "keys_to_remove", ":", "\n", "            ", "del", "state_dict", "[", "k", "]", "\n", "\n", "", "for", "key", ",", "value", "in", "items_to_add", ".", "items", "(", ")", ":", "\n", "            ", "state_dict", "[", "key", "]", "=", "value", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.utils_softmax": [[17, 22], ["torch.softmax", "torch.softmax", "x.float"], "function", ["None"], ["def", "utils_softmax", "(", "x", ",", "dim", ":", "int", ",", "onnx_trace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "if", "onnx_trace", ":", "\n", "        ", "return", "F", ".", "softmax", "(", "x", ".", "float", "(", ")", ",", "dim", "=", "dim", ")", "\n", "", "else", ":", "\n", "        ", "return", "F", ".", "softmax", "(", "x", ",", "dim", "=", "dim", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.multihead_attention.with_incremental_state": [[59, 64], ["tuple"], "function", ["None"], ["", "", "def", "with_incremental_state", "(", "cls", ")", ":", "\n", "    ", "cls", ".", "__bases__", "=", "(", "FairseqIncrementalState", ",", ")", "+", "tuple", "(", "\n", "b", "for", "b", "in", "cls", ".", "__bases__", "if", "b", "!=", "FairseqIncrementalState", "\n", ")", "\n", "return", "cls", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained._has_regression_weights": [[14, 18], ["None"], "function", ["None"], ["def", "_has_regression_weights", "(", "model_name", ")", ":", "\n", "    ", "\"\"\"Return whether we expect / require regression weights;\n    Right now that is all models except ESM-1v and ESM-IF\"\"\"", "\n", "return", "not", "(", "\"esm1v\"", "in", "model_name", "or", "\"esm_if\"", "in", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet": [[20, 25], ["model_name.endswith", "pretrained.load_model_and_alphabet_local", "pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_local", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "load_model_and_alphabet", "(", "model_name", ")", ":", "\n", "    ", "if", "model_name", ".", "endswith", "(", "\".pt\"", ")", ":", "# treat as filepath", "\n", "        ", "return", "load_model_and_alphabet_local", "(", "model_name", ")", "\n", "", "else", ":", "\n", "        ", "return", "load_model_and_alphabet_hub", "(", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_hub_workaround": [[27, 40], ["torch.hub.load_state_dict_from_url", "torch.load", "Exception", "pathlib.Path", "torch.hub.get_dir"], "function", ["None"], ["", "", "def", "load_hub_workaround", "(", "url", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "data", "=", "torch", ".", "hub", ".", "load_state_dict_from_url", "(", "url", ",", "progress", "=", "False", ",", "map_location", "=", "\"cpu\"", ")", "\n", "", "except", "RuntimeError", ":", "\n", "# Pytorch version issue - see https://github.com/pytorch/pytorch/issues/43106", "\n", "        ", "fn", "=", "Path", "(", "url", ")", ".", "name", "\n", "data", "=", "torch", ".", "load", "(", "\n", "f\"{torch.hub.get_dir()}/checkpoints/{fn}\"", ",", "\n", "map_location", "=", "\"cpu\"", ",", "\n", ")", "\n", "", "except", "urllib", ".", "error", ".", "HTTPError", "as", "e", ":", "\n", "        ", "raise", "Exception", "(", "f\"Could not load {url}, check if you specified a correct model name?\"", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_regression_hub": [[42, 46], ["pretrained.load_hub_workaround"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_hub_workaround"], ["", "def", "load_regression_hub", "(", "model_name", ")", ":", "\n", "    ", "url", "=", "f\"https://dl.fbaipublicfiles.com/fair-esm/regression/{model_name}-contact-regression.pt\"", "\n", "regression_data", "=", "load_hub_workaround", "(", "url", ")", "\n", "return", "regression_data", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub": [[48, 56], ["pretrained.load_hub_workaround", "pretrained._has_regression_weights", "pretrained.load_model_and_alphabet_core", "pretrained.load_regression_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_hub_workaround", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained._has_regression_weights", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_core", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_regression_hub"], ["", "def", "load_model_and_alphabet_hub", "(", "model_name", ")", ":", "\n", "    ", "url", "=", "f\"https://dl.fbaipublicfiles.com/fair-esm/models/{model_name}.pt\"", "\n", "model_data", "=", "load_hub_workaround", "(", "url", ")", "\n", "if", "_has_regression_weights", "(", "model_name", ")", ":", "\n", "        ", "regression_data", "=", "load_regression_hub", "(", "model_name", ")", "\n", "", "else", ":", "\n", "        ", "regression_data", "=", "None", "\n", "", "return", "load_model_and_alphabet_core", "(", "model_data", ",", "regression_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_local": [[58, 69], ["pathlib.Path", "torch.load", "pretrained._has_regression_weights", "pretrained.load_model_and_alphabet_core", "str", "torch.load", "str", "pathlib.Path.with_suffix"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained._has_regression_weights", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_core"], ["", "def", "load_model_and_alphabet_local", "(", "model_location", ")", ":", "\n", "    ", "\"\"\" Load from local path. The regression weights need to be co-located \"\"\"", "\n", "model_location", "=", "Path", "(", "model_location", ")", "\n", "model_data", "=", "torch", ".", "load", "(", "str", "(", "model_location", ")", ",", "map_location", "=", "\"cpu\"", ")", "\n", "model_name", "=", "model_location", ".", "stem", "\n", "if", "_has_regression_weights", "(", "model_name", ")", ":", "\n", "        ", "regression_location", "=", "str", "(", "model_location", ".", "with_suffix", "(", "\"\"", ")", ")", "+", "\"-contact-regression.pt\"", "\n", "regression_data", "=", "torch", ".", "load", "(", "regression_location", ",", "map_location", "=", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "        ", "regression_data", "=", "None", "\n", "", "return", "load_model_and_alphabet_core", "(", "model_data", ",", "regression_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.has_emb_layer_norm_before": [[71, 74], ["any", "k.startswith", "model_state.items"], "function", ["None"], ["", "def", "has_emb_layer_norm_before", "(", "model_state", ")", ":", "\n", "    ", "\"\"\" Determine whether layer norm needs to be applied before the encoder \"\"\"", "\n", "return", "any", "(", "k", ".", "startswith", "(", "\"emb_layer_norm_before\"", ")", "for", "k", ",", "param", "in", "model_state", ".", "items", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_core": [[76, 181], ["esm.Alphabet.from_architecture", "model_type", "set", "set", "model_type.load_state_dict", "model_data[].update", "[].zero_", "pretrained.has_emb_layer_norm_before", "argparse.Namespace", "model_type.state_dict().keys", "model_state.keys", "pra", "prs1", "error_msgs.append", "error_msgs.append", "RuntimeError", "warnings.warn", "vars().items", "prs2", "model_data[].items", "pra", "prs", "vars.get", "model_type.state_dict", "vars().items", "model_data[].items", "pra", "prs1", "model_state[].size", "vars", "ValueError", "s.replace.split", "s.replace.split", "s.replace.split", "vars", "s.replace.replace", "s.replace.replace", "vars().items", "prs2", "model_data[].items", "s.replace.replace", "s.replace.replace", "s.replace.replace", "s.replace.replace", "s.replace.replace", "s.replace.replace", "s.replace.replace", "s.replace.replace", "pretrained.load_model_and_alphabet_core.update_name"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.from_architecture", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.has_emb_layer_norm_before"], ["", "def", "load_model_and_alphabet_core", "(", "model_data", ",", "regression_data", "=", "None", ")", ":", "\n", "    ", "import", "esm", "# conditional esm.inverse_folding below", "\n", "if", "regression_data", "is", "not", "None", ":", "\n", "        ", "model_data", "[", "\"model\"", "]", ".", "update", "(", "regression_data", "[", "\"model\"", "]", ")", "\n", "\n", "", "alphabet", "=", "esm", ".", "Alphabet", ".", "from_architecture", "(", "model_data", "[", "\"args\"", "]", ".", "arch", ")", "\n", "\n", "if", "model_data", "[", "\"args\"", "]", ".", "arch", "==", "\"roberta_large\"", ":", "\n", "# upgrade state dict", "\n", "        ", "pra", "=", "lambda", "s", ":", "\"\"", ".", "join", "(", "s", ".", "split", "(", "\"encoder_\"", ")", "[", "1", ":", "]", "if", "\"encoder\"", "in", "s", "else", "s", ")", "\n", "prs1", "=", "lambda", "s", ":", "\"\"", ".", "join", "(", "s", ".", "split", "(", "\"encoder.\"", ")", "[", "1", ":", "]", "if", "\"encoder\"", "in", "s", "else", "s", ")", "\n", "prs2", "=", "lambda", "s", ":", "\"\"", ".", "join", "(", "\n", "s", ".", "split", "(", "\"sentence_encoder.\"", ")", "[", "1", ":", "]", "if", "\"sentence_encoder\"", "in", "s", "else", "s", "\n", ")", "\n", "model_args", "=", "{", "pra", "(", "arg", "[", "0", "]", ")", ":", "arg", "[", "1", "]", "for", "arg", "in", "vars", "(", "model_data", "[", "\"args\"", "]", ")", ".", "items", "(", ")", "}", "\n", "model_state", "=", "{", "prs1", "(", "prs2", "(", "arg", "[", "0", "]", ")", ")", ":", "arg", "[", "1", "]", "for", "arg", "in", "model_data", "[", "\"model\"", "]", ".", "items", "(", ")", "}", "\n", "model_state", "[", "\"embed_tokens.weight\"", "]", "[", "alphabet", ".", "mask_idx", "]", ".", "zero_", "(", ")", "# For token drop", "\n", "model_args", "[", "\"emb_layer_norm_before\"", "]", "=", "has_emb_layer_norm_before", "(", "model_state", ")", "\n", "model_type", "=", "esm", ".", "ProteinBertModel", "\n", "\n", "", "elif", "model_data", "[", "\"args\"", "]", ".", "arch", "==", "\"protein_bert_base\"", ":", "\n", "\n", "# upgrade state dict", "\n", "        ", "pra", "=", "lambda", "s", ":", "\"\"", ".", "join", "(", "s", ".", "split", "(", "\"decoder_\"", ")", "[", "1", ":", "]", "if", "\"decoder\"", "in", "s", "else", "s", ")", "\n", "prs", "=", "lambda", "s", ":", "\"\"", ".", "join", "(", "s", ".", "split", "(", "\"decoder.\"", ")", "[", "1", ":", "]", "if", "\"decoder\"", "in", "s", "else", "s", ")", "\n", "model_args", "=", "{", "pra", "(", "arg", "[", "0", "]", ")", ":", "arg", "[", "1", "]", "for", "arg", "in", "vars", "(", "model_data", "[", "\"args\"", "]", ")", ".", "items", "(", ")", "}", "\n", "model_state", "=", "{", "prs", "(", "arg", "[", "0", "]", ")", ":", "arg", "[", "1", "]", "for", "arg", "in", "model_data", "[", "\"model\"", "]", ".", "items", "(", ")", "}", "\n", "model_type", "=", "esm", ".", "ProteinBertModel", "\n", "", "elif", "model_data", "[", "\"args\"", "]", ".", "arch", "==", "\"msa_transformer\"", ":", "\n", "\n", "# upgrade state dict", "\n", "        ", "pra", "=", "lambda", "s", ":", "\"\"", ".", "join", "(", "s", ".", "split", "(", "\"encoder_\"", ")", "[", "1", ":", "]", "if", "\"encoder\"", "in", "s", "else", "s", ")", "\n", "prs1", "=", "lambda", "s", ":", "\"\"", ".", "join", "(", "s", ".", "split", "(", "\"encoder.\"", ")", "[", "1", ":", "]", "if", "\"encoder\"", "in", "s", "else", "s", ")", "\n", "prs2", "=", "lambda", "s", ":", "\"\"", ".", "join", "(", "\n", "s", ".", "split", "(", "\"sentence_encoder.\"", ")", "[", "1", ":", "]", "if", "\"sentence_encoder\"", "in", "s", "else", "s", "\n", ")", "\n", "prs3", "=", "lambda", "s", ":", "s", ".", "replace", "(", "\"row\"", ",", "\"column\"", ")", "if", "\"row\"", "in", "s", "else", "s", ".", "replace", "(", "\"column\"", ",", "\"row\"", ")", "\n", "model_args", "=", "{", "pra", "(", "arg", "[", "0", "]", ")", ":", "arg", "[", "1", "]", "for", "arg", "in", "vars", "(", "model_data", "[", "\"args\"", "]", ")", ".", "items", "(", ")", "}", "\n", "model_state", "=", "{", "prs1", "(", "prs2", "(", "prs3", "(", "arg", "[", "0", "]", ")", ")", ")", ":", "arg", "[", "1", "]", "for", "arg", "in", "model_data", "[", "\"model\"", "]", ".", "items", "(", ")", "}", "\n", "if", "model_args", ".", "get", "(", "\"embed_positions_msa\"", ",", "False", ")", ":", "\n", "            ", "emb_dim", "=", "model_state", "[", "\"msa_position_embedding\"", "]", ".", "size", "(", "-", "1", ")", "\n", "model_args", "[", "\"embed_positions_msa_dim\"", "]", "=", "emb_dim", "# initial release, bug: emb_dim==1", "\n", "\n", "", "model_type", "=", "esm", ".", "MSATransformer", "\n", "\n", "", "elif", "\"invariant_gvp\"", "in", "model_data", "[", "\"args\"", "]", ".", "arch", ":", "\n", "        ", "import", "esm", ".", "inverse_folding", "\n", "model_type", "=", "esm", ".", "inverse_folding", ".", "gvp_transformer", ".", "GVPTransformerModel", "\n", "model_args", "=", "vars", "(", "model_data", "[", "\"args\"", "]", ")", "# convert Namespace -> dict", "\n", "\n", "def", "update_name", "(", "s", ")", ":", "\n", "# Map the module names in checkpoints trained with internal code to", "\n", "# the updated module names in open source code", "\n", "            ", "s", "=", "s", ".", "replace", "(", "\"W_v\"", ",", "\"embed_graph.embed_node\"", ")", "\n", "s", "=", "s", ".", "replace", "(", "\"W_e\"", ",", "\"embed_graph.embed_edge\"", ")", "\n", "s", "=", "s", ".", "replace", "(", "\"embed_scores.0\"", ",", "\"embed_confidence\"", ")", "\n", "s", "=", "s", ".", "replace", "(", "\"embed_score.\"", ",", "\"embed_graph.embed_confidence.\"", ")", "\n", "s", "=", "s", ".", "replace", "(", "\"seq_logits_projection.\"", ",", "\"\"", ")", "\n", "s", "=", "s", ".", "replace", "(", "\"embed_ingraham_features\"", ",", "\"embed_dihedrals\"", ")", "\n", "s", "=", "s", ".", "replace", "(", "\"embed_gvp_in_local_frame.0\"", ",", "\"embed_gvp_output\"", ")", "\n", "s", "=", "s", ".", "replace", "(", "\"embed_features_in_local_frame.0\"", ",", "\n", "\"embed_gvp_input_features\"", ")", "\n", "return", "s", "\n", "\n", "", "model_state", "=", "{", "\n", "update_name", "(", "sname", ")", ":", "svalue", "for", "sname", ",", "svalue", "in", "\n", "model_data", "[", "\"model\"", "]", ".", "items", "(", ")", "\n", "if", "\"version\"", "not", "in", "sname", "\n", "}", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown architecture selected\"", ")", "\n", "\n", "", "model", "=", "model_type", "(", "\n", "Namespace", "(", "**", "model_args", ")", ",", "\n", "alphabet", ",", "\n", ")", "\n", "\n", "expected_keys", "=", "set", "(", "model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ")", "\n", "found_keys", "=", "set", "(", "model_state", ".", "keys", "(", ")", ")", "\n", "\n", "if", "regression_data", "is", "None", ":", "\n", "        ", "expected_missing", "=", "{", "\"contact_head.regression.weight\"", ",", "\"contact_head.regression.bias\"", "}", "\n", "error_msgs", "=", "[", "]", "\n", "missing", "=", "(", "expected_keys", "-", "found_keys", ")", "-", "expected_missing", "\n", "if", "missing", ":", "\n", "            ", "error_msgs", ".", "append", "(", "f\"Missing key(s) in state_dict: {missing}.\"", ")", "\n", "", "unexpected", "=", "found_keys", "-", "expected_keys", "\n", "if", "unexpected", ":", "\n", "            ", "error_msgs", ".", "append", "(", "f\"Unexpected key(s) in state_dict: {unexpected}.\"", ")", "\n", "\n", "", "if", "error_msgs", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "\"Error(s) in loading state_dict for {}:\\n\\t{}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", "\n", ")", "\n", ")", "\n", "", "if", "expected_missing", "-", "found_keys", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Regression weights not found, predicting contacts will not produce correct results.\"", "\n", ")", "\n", "\n", "", "", "model", ".", "load_state_dict", "(", "model_state", ",", "strict", "=", "regression_data", "is", "not", "None", ")", "\n", "\n", "return", "model", ",", "alphabet", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1_t34_670M_UR50S": [[183, 189], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1_t34_670M_UR50S", "(", ")", ":", "\n", "    ", "\"\"\"34 layer transformer model with 670M params, trained on Uniref50 Sparse.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1_t34_670M_UR50S\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1_t34_670M_UR50D": [[191, 197], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1_t34_670M_UR50D", "(", ")", ":", "\n", "    ", "\"\"\"34 layer transformer model with 670M params, trained on Uniref50 Dense.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1_t34_670M_UR50D\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1_t34_670M_UR100": [[199, 205], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1_t34_670M_UR100", "(", ")", ":", "\n", "    ", "\"\"\"34 layer transformer model with 670M params, trained on Uniref100.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1_t34_670M_UR100\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1_t12_85M_UR50S": [[207, 213], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1_t12_85M_UR50S", "(", ")", ":", "\n", "    ", "\"\"\"12 layer transformer model with 85M params, trained on Uniref50 Sparse.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1_t12_85M_UR50S\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1_t6_43M_UR50S": [[215, 221], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1_t6_43M_UR50S", "(", ")", ":", "\n", "    ", "\"\"\"6 layer transformer model with 43M params, trained on Uniref50 Sparse.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1_t6_43M_UR50S\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1b_t33_650M_UR50S": [[223, 230], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1b_t33_650M_UR50S", "(", ")", ":", "\n", "    ", "\"\"\"33 layer transformer model with 650M params, trained on Uniref50 Sparse.\n    This is our best performing model, which will be described in a future publication.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1b_t33_650M_UR50S\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_msa1_t12_100M_UR50S": [[232, 238], ["warnings.warn", "pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm_msa1_t12_100M_UR50S", "(", ")", ":", "\n", "    ", "warnings", ".", "warn", "(", "\n", "\"This model had a minor bug in the positional embeddings, \"", "\n", "\"please use ESM-MSA-1b: esm.pretrained.esm_msa1b_t12_100M_UR50S()\"", ",", "\n", ")", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm_msa1_t12_100M_UR50S\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_msa1b_t12_100M_UR50S": [[240, 242], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm_msa1b_t12_100M_UR50S", "(", ")", ":", "\n", "    ", "return", "load_model_and_alphabet_hub", "(", "\"esm_msa1b_t12_100M_UR50S\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1v_t33_650M_UR90S": [[244, 251], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1v_t33_650M_UR90S", "(", ")", ":", "\n", "    ", "\"\"\"33 layer transformer model with 650M params, trained on Uniref90.\n    This is model 1 of a 5 model ensemble.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1v_t33_650M_UR90S_1\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1v_t33_650M_UR90S_1": [[253, 260], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1v_t33_650M_UR90S_1", "(", ")", ":", "\n", "    ", "\"\"\"33 layer transformer model with 650M params, trained on Uniref90.\n    This is model 1 of a 5 model ensemble.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1v_t33_650M_UR90S_1\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1v_t33_650M_UR90S_2": [[262, 269], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1v_t33_650M_UR90S_2", "(", ")", ":", "\n", "    ", "\"\"\"33 layer transformer model with 650M params, trained on Uniref90.\n    This is model 2 of a 5 model ensemble.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1v_t33_650M_UR90S_2\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1v_t33_650M_UR90S_3": [[271, 278], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1v_t33_650M_UR90S_3", "(", ")", ":", "\n", "    ", "\"\"\"33 layer transformer model with 650M params, trained on Uniref90.\n    This is model 3 of a 5 model ensemble.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1v_t33_650M_UR90S_3\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1v_t33_650M_UR90S_4": [[280, 287], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1v_t33_650M_UR90S_4", "(", ")", ":", "\n", "    ", "\"\"\"33 layer transformer model with 650M params, trained on Uniref90.\n    This is model 4 of a 5 model ensemble.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1v_t33_650M_UR90S_4\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm1v_t33_650M_UR90S_5": [[289, 296], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm1v_t33_650M_UR90S_5", "(", ")", ":", "\n", "    ", "\"\"\"33 layer transformer model with 650M params, trained on Uniref90.\n    This is model 5 of a 5 model ensemble.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm1v_t33_650M_UR90S_5\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.esm_if1_gvp4_t16_142M_UR50": [[298, 307], ["pretrained.load_model_and_alphabet_hub"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.pretrained.load_model_and_alphabet_hub"], ["", "def", "esm_if1_gvp4_t16_142M_UR50", "(", ")", ":", "\n", "    ", "\"\"\"Inverse folding model with 142M params, with 4 GVP-GNN layers, 8\n    Transformer encoder layers, and 8 Transformer decoder layers, trained on\n    CATH structures and 12 million alphafold2 predicted structures from UniRef50\n    sequences.\n\n    Returns a tuple of (Model, Alphabet).\n    \"\"\"", "\n", "return", "load_model_and_alphabet_hub", "(", "\"esm_if1_gvp4_t16_142M_UR50\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention.__init__": [[14, 35], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embed_dim", ",", "\n", "num_heads", ",", "\n", "dropout", "=", "0.0", ",", "\n", "max_tokens_per_msa", ":", "int", "=", "2", "**", "16", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "self", ".", "max_tokens_per_msa", "=", "max_tokens_per_msa", "\n", "self", ".", "attn_shape", "=", "\"hnij\"", "\n", "\n", "self", ".", "k_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "v_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "q_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "dropout_module", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention.align_scaling": [[36, 39], ["q.size", "math.sqrt"], "methods", ["None"], ["", "def", "align_scaling", "(", "self", ",", "q", ")", ":", "\n", "        ", "num_rows", "=", "q", ".", "size", "(", "0", ")", "\n", "return", "self", ".", "scaling", "/", "math", ".", "sqrt", "(", "num_rows", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention._batched_forward": [[40, 70], ["x.size", "max", "axial_attention.RowSelfAttention.align_scaling", "range", "attns.softmax", "axial_attention.RowSelfAttention.dropout_module", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "axial_attention.RowSelfAttention.compute_attention_weights", "axial_attention.RowSelfAttention.compute_attention_update", "outputs.append"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention.align_scaling", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention.compute_attention_weights", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.ColumnSelfAttention.compute_attention_update"], ["", "def", "_batched_forward", "(", "\n", "self", ",", "\n", "x", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "self_attn_padding_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "num_rows", ",", "num_cols", ",", "batch_size", ",", "embed_dim", "=", "x", ".", "size", "(", ")", "\n", "max_rows", "=", "max", "(", "1", ",", "self", ".", "max_tokens_per_msa", "//", "num_cols", ")", "\n", "attns", "=", "0", "\n", "scaling", "=", "self", ".", "align_scaling", "(", "x", ")", "\n", "for", "start", "in", "range", "(", "0", ",", "num_rows", ",", "max_rows", ")", ":", "\n", "            ", "attn_weights", "=", "self", ".", "compute_attention_weights", "(", "\n", "x", "[", "start", ":", "start", "+", "max_rows", "]", ",", "\n", "scaling", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "self_attn_padding_mask", "=", "self_attn_padding_mask", "[", ":", ",", "start", ":", "start", "+", "max_rows", "]", "\n", "if", "self_attn_padding_mask", "is", "not", "None", "\n", "else", "None", ",", "\n", ")", "\n", "attns", "+=", "attn_weights", "\n", "", "attn_probs", "=", "attns", ".", "softmax", "(", "-", "1", ")", "\n", "attn_probs", "=", "self", ".", "dropout_module", "(", "attn_probs", ")", "\n", "\n", "outputs", "=", "[", "]", "\n", "for", "start", "in", "range", "(", "0", ",", "num_rows", ",", "max_rows", ")", ":", "\n", "            ", "output", "=", "self", ".", "compute_attention_update", "(", "x", "[", "start", ":", "start", "+", "max_rows", "]", ",", "attn_probs", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "\n", "", "output", "=", "torch", ".", "cat", "(", "outputs", ",", "0", ")", "\n", "return", "output", ",", "attn_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention.compute_attention_weights": [[71, 100], ["x.size", "axial_attention.RowSelfAttention.q_proj().view", "axial_attention.RowSelfAttention.k_proj().view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "attn_weights.masked_fill.masked_fill.masked_fill", "axial_attention.RowSelfAttention.q_proj", "axial_attention.RowSelfAttention.k_proj", "axial_attention.RowSelfAttention.permute().unsqueeze().unsqueeze().to", "axial_attention.RowSelfAttention.unsqueeze().unsqueeze", "axial_attention.RowSelfAttention.permute().unsqueeze().unsqueeze", "axial_attention.RowSelfAttention.unsqueeze", "axial_attention.RowSelfAttention.permute().unsqueeze", "axial_attention.RowSelfAttention.permute"], "methods", ["None"], ["", "def", "compute_attention_weights", "(", "\n", "self", ",", "\n", "x", ",", "\n", "scaling", ":", "float", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "self_attn_padding_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "num_rows", ",", "num_cols", ",", "batch_size", ",", "embed_dim", "=", "x", ".", "size", "(", ")", "\n", "q", "=", "self", ".", "q_proj", "(", "x", ")", ".", "view", "(", "num_rows", ",", "num_cols", ",", "batch_size", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", "\n", "k", "=", "self", ".", "k_proj", "(", "x", ")", ".", "view", "(", "num_rows", ",", "num_cols", ",", "batch_size", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", "\n", "q", "*=", "scaling", "\n", "if", "self_attn_padding_mask", "is", "not", "None", ":", "\n", "# Zero out any padded aligned positions - this is important since", "\n", "# we take a sum across the alignment axis.", "\n", "            ", "q", "*=", "1", "-", "self_attn_padding_mask", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ".", "unsqueeze", "(", "3", ")", ".", "unsqueeze", "(", "4", ")", ".", "to", "(", "q", ")", "\n", "\n", "", "attn_weights", "=", "torch", ".", "einsum", "(", "f\"rinhd,rjnhd->{self.attn_shape}\"", ",", "q", ",", "k", ")", "\n", "\n", "if", "self_attn_mask", "is", "not", "None", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "# Mask Size: [B x R x C], Weights Size: [H x B x C x C]", "\n", "\n", "", "if", "self_attn_padding_mask", "is", "not", "None", ":", "\n", "            ", "attn_weights", "=", "attn_weights", ".", "masked_fill", "(", "\n", "self_attn_padding_mask", "[", ":", ",", "0", "]", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "-", "10000", ",", "\n", ")", "\n", "\n", "", "return", "attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention.compute_attention_update": [[101, 112], ["x.size", "axial_attention.RowSelfAttention.v_proj().view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "context.contiguous().view.contiguous().view.contiguous().view", "axial_attention.RowSelfAttention.out_proj", "axial_attention.RowSelfAttention.v_proj", "context.contiguous().view.contiguous().view.contiguous"], "methods", ["None"], ["", "def", "compute_attention_update", "(", "\n", "self", ",", "\n", "x", ",", "\n", "attn_probs", ",", "\n", ")", ":", "\n", "        ", "num_rows", ",", "num_cols", ",", "batch_size", ",", "embed_dim", "=", "x", ".", "size", "(", ")", "\n", "v", "=", "self", ".", "v_proj", "(", "x", ")", ".", "view", "(", "num_rows", ",", "num_cols", ",", "batch_size", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", "\n", "context", "=", "torch", ".", "einsum", "(", "f\"{self.attn_shape},rjnhd->rinhd\"", ",", "attn_probs", ",", "v", ")", "\n", "context", "=", "context", ".", "contiguous", "(", ")", ".", "view", "(", "num_rows", ",", "num_cols", ",", "batch_size", ",", "embed_dim", ")", "\n", "output", "=", "self", ".", "out_proj", "(", "context", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention.forward": [[113, 131], ["x.size", "axial_attention.RowSelfAttention._batched_forward", "axial_attention.RowSelfAttention.align_scaling", "axial_attention.RowSelfAttention.compute_attention_weights", "axial_attention.RowSelfAttention.softmax", "axial_attention.RowSelfAttention.dropout_module", "axial_attention.RowSelfAttention.compute_attention_update", "torch.is_grad_enabled", "torch.is_grad_enabled", "torch.is_grad_enabled", "torch.is_grad_enabled"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.ColumnSelfAttention._batched_forward", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention.align_scaling", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.RowSelfAttention.compute_attention_weights", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.ColumnSelfAttention.compute_attention_update"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "self_attn_padding_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "num_rows", ",", "num_cols", ",", "batch_size", ",", "embed_dim", "=", "x", ".", "size", "(", ")", "\n", "if", "(", "num_rows", "*", "num_cols", ">", "self", ".", "max_tokens_per_msa", ")", "and", "not", "torch", ".", "is_grad_enabled", "(", ")", ":", "\n", "            ", "return", "self", ".", "_batched_forward", "(", "x", ",", "self_attn_mask", ",", "self_attn_padding_mask", ")", "\n", "", "else", ":", "\n", "            ", "scaling", "=", "self", ".", "align_scaling", "(", "x", ")", "\n", "attn_weights", "=", "self", ".", "compute_attention_weights", "(", "\n", "x", ",", "scaling", ",", "self_attn_mask", ",", "self_attn_padding_mask", "\n", ")", "\n", "attn_probs", "=", "attn_weights", ".", "softmax", "(", "-", "1", ")", "\n", "attn_probs", "=", "self", ".", "dropout_module", "(", "attn_probs", ")", "\n", "output", "=", "self", ".", "compute_attention_update", "(", "x", ",", "attn_probs", ")", "\n", "return", "output", ",", "attn_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.ColumnSelfAttention.__init__": [[136, 157], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embed_dim", ",", "\n", "num_heads", ",", "\n", "dropout", "=", "0.0", ",", "\n", "max_tokens_per_msa", ":", "int", "=", "2", "**", "16", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "self", ".", "max_tokens_per_msa", "=", "max_tokens_per_msa", "\n", "\n", "self", ".", "k_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "v_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "q_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "dropout_module", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.ColumnSelfAttention._batched_forward": [[158, 181], ["x.size", "max", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "axial_attention.ColumnSelfAttention.", "outputs.append", "torch.cat.append", "torch.cat.append"], "methods", ["None"], ["", "def", "_batched_forward", "(", "\n", "self", ",", "\n", "x", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "self_attn_padding_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "num_rows", ",", "num_cols", ",", "batch_size", ",", "embed_dim", "=", "x", ".", "size", "(", ")", "\n", "max_cols", "=", "max", "(", "1", ",", "self", ".", "max_tokens_per_msa", "//", "num_rows", ")", "\n", "outputs", "=", "[", "]", "\n", "attns", "=", "[", "]", "\n", "for", "start", "in", "range", "(", "0", ",", "num_cols", ",", "max_cols", ")", ":", "\n", "            ", "output", ",", "attn", "=", "self", "(", "\n", "x", "[", ":", ",", "start", ":", "start", "+", "max_cols", "]", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "self_attn_padding_mask", "=", "self_attn_padding_mask", "[", ":", ",", ":", ",", "start", ":", "start", "+", "max_cols", "]", "\n", "if", "self_attn_padding_mask", "is", "not", "None", "\n", "else", "None", ",", "\n", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "attns", ".", "append", "(", "attn", ")", "\n", "", "output", "=", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "\n", "attns", "=", "torch", ".", "cat", "(", "attns", ",", "1", ")", "\n", "return", "output", ",", "attns", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.ColumnSelfAttention.compute_attention_update": [[182, 223], ["x.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "axial_attention.ColumnSelfAttention.out_proj", "axial_attention.ColumnSelfAttention.q_proj().view", "axial_attention.ColumnSelfAttention.k_proj().view", "axial_attention.ColumnSelfAttention.v_proj().view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "attn_weights.masked_fill.masked_fill.softmax", "axial_attention.ColumnSelfAttention.dropout_module", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "context.contiguous().view.contiguous().view.contiguous().view", "axial_attention.ColumnSelfAttention.out_proj", "axial_attention.ColumnSelfAttention.v_proj", "attn_weights.masked_fill.masked_fill.masked_fill", "axial_attention.ColumnSelfAttention.q_proj", "axial_attention.ColumnSelfAttention.k_proj", "axial_attention.ColumnSelfAttention.v_proj", "axial_attention.ColumnSelfAttention.permute().unsqueeze().unsqueeze", "context.contiguous().view.contiguous().view.contiguous", "axial_attention.ColumnSelfAttention.permute().unsqueeze", "axial_attention.ColumnSelfAttention.permute"], "methods", ["None"], ["", "def", "compute_attention_update", "(", "\n", "self", ",", "\n", "x", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "self_attn_padding_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "num_rows", ",", "num_cols", ",", "batch_size", ",", "embed_dim", "=", "x", ".", "size", "(", ")", "\n", "if", "num_rows", "==", "1", ":", "\n", "# if there is only 1 position, this is equivalent and doesn't break with padding", "\n", "            ", "attn_probs", "=", "torch", ".", "ones", "(", "\n", "self", ".", "num_heads", ",", "\n", "num_cols", ",", "\n", "batch_size", ",", "\n", "num_rows", ",", "\n", "num_rows", ",", "\n", "device", "=", "x", ".", "device", ",", "\n", "dtype", "=", "x", ".", "dtype", ",", "\n", ")", "\n", "output", "=", "self", ".", "out_proj", "(", "self", ".", "v_proj", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "            ", "q", "=", "self", ".", "q_proj", "(", "x", ")", ".", "view", "(", "num_rows", ",", "num_cols", ",", "batch_size", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", "\n", "k", "=", "self", ".", "k_proj", "(", "x", ")", ".", "view", "(", "num_rows", ",", "num_cols", ",", "batch_size", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", "\n", "v", "=", "self", ".", "v_proj", "(", "x", ")", ".", "view", "(", "num_rows", ",", "num_cols", ",", "batch_size", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", "\n", "q", "*=", "self", ".", "scaling", "\n", "\n", "attn_weights", "=", "torch", ".", "einsum", "(", "\"icnhd,jcnhd->hcnij\"", ",", "q", ",", "k", ")", "\n", "\n", "if", "self_attn_mask", "is", "not", "None", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "", "if", "self_attn_padding_mask", "is", "not", "None", ":", "\n", "                ", "attn_weights", "=", "attn_weights", ".", "masked_fill", "(", "\n", "self_attn_padding_mask", ".", "permute", "(", "2", ",", "0", ",", "1", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "3", ")", ",", "\n", "-", "10000", ",", "\n", ")", "\n", "\n", "", "attn_probs", "=", "attn_weights", ".", "softmax", "(", "-", "1", ")", "\n", "attn_probs", "=", "self", ".", "dropout_module", "(", "attn_probs", ")", "\n", "context", "=", "torch", ".", "einsum", "(", "\"hcnij,jcnhd->icnhd\"", ",", "attn_probs", ",", "v", ")", "\n", "context", "=", "context", ".", "contiguous", "(", ")", ".", "view", "(", "num_rows", ",", "num_cols", ",", "batch_size", ",", "embed_dim", ")", "\n", "output", "=", "self", ".", "out_proj", "(", "context", ")", "\n", "", "return", "output", ",", "attn_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.ColumnSelfAttention.forward": [[224, 240], ["x.size", "axial_attention.ColumnSelfAttention._batched_forward", "axial_attention.ColumnSelfAttention.compute_attention_update", "torch.is_grad_enabled", "torch.is_grad_enabled", "torch.is_grad_enabled", "torch.is_grad_enabled"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.ColumnSelfAttention._batched_forward", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.axial_attention.ColumnSelfAttention.compute_attention_update"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "self_attn_padding_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "num_rows", ",", "num_cols", ",", "batch_size", ",", "embed_dim", "=", "x", ".", "size", "(", ")", "\n", "# if False and num_rows * num_cols > 2 ** 14 and not torch.is_grad_enabled():", "\n", "if", "(", "num_rows", "*", "num_cols", ")", ">", "self", ".", "max_tokens_per_msa", "and", "not", "torch", ".", "is_grad_enabled", "(", ")", ":", "\n", "            ", "return", "self", ".", "_batched_forward", "(", "\n", "x", ",", "\n", "self_attn_mask", ",", "\n", "self_attn_padding_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "compute_attention_update", "(", "x", ",", "self_attn_mask", ",", "self_attn_padding_mask", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.ESM1LayerNorm.__init__": [[45, 56], ["torch.Module.__init__", "bool", "isinstance", "tuple", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "eps", "=", "1e-12", ",", "affine", "=", "True", ")", ":", "\n", "        ", "\"\"\"Construct a layernorm layer in the TF style (eps inside the sqrt).\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_size", "=", "(", "hidden_size", ",", ")", "if", "isinstance", "(", "hidden_size", ",", "int", ")", "else", "tuple", "(", "hidden_size", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "affine", "=", "bool", "(", "affine", ")", "\n", "if", "self", ".", "affine", ":", "\n", "            ", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "hidden_size", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "hidden_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "weight", ",", "self", ".", "bias", "=", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.ESM1LayerNorm.forward": [[57, 66], ["tuple", "x.mean", "x_zeromean.pow().mean", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "x_zeromean.pow", "range", "len"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "dims", "=", "tuple", "(", "-", "(", "i", "+", "1", ")", "for", "i", "in", "range", "(", "len", "(", "self", ".", "hidden_size", ")", ")", ")", "\n", "means", "=", "x", ".", "mean", "(", "dims", ",", "keepdim", "=", "True", ")", "\n", "x_zeromean", "=", "x", "-", "means", "\n", "variances", "=", "x_zeromean", ".", "pow", "(", "2", ")", ".", "mean", "(", "dims", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "x_zeromean", "/", "torch", ".", "sqrt", "(", "variances", "+", "self", ".", "eps", ")", "\n", "if", "self", ".", "affine", ":", "\n", "            ", "x", "=", "(", "self", ".", "weight", "*", "x", ")", "+", "self", ".", "bias", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.TransformerLayer.__init__": [[88, 101], ["torch.Module.__init__", "modules.TransformerLayer._init_submodules"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.TransformerLayer._init_submodules"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embed_dim", ",", "\n", "ffn_embed_dim", ",", "\n", "attention_heads", ",", "\n", "add_bias_kv", "=", "True", ",", "\n", "use_esm1b_layer_norm", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "ffn_embed_dim", "=", "ffn_embed_dim", "\n", "self", ".", "attention_heads", "=", "attention_heads", "\n", "self", ".", "_init_submodules", "(", "add_bias_kv", ",", "use_esm1b_layer_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.TransformerLayer._init_submodules": [[102, 117], ["multihead_attention.MultiheadAttention", "BertLayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "BertLayerNorm"], "methods", ["None"], ["", "def", "_init_submodules", "(", "self", ",", "add_bias_kv", ",", "use_esm1b_layer_norm", ")", ":", "\n", "        ", "BertLayerNorm", "=", "ESM1bLayerNorm", "if", "use_esm1b_layer_norm", "else", "ESM1LayerNorm", "\n", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "\n", "self", ".", "attention_heads", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "False", ",", "\n", ")", "\n", "self", ".", "self_attn_layer_norm", "=", "BertLayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "self", ".", "ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "final_layer_norm", "=", "BertLayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.TransformerLayer.forward": [[118, 141], ["modules.TransformerLayer.self_attn_layer_norm", "modules.TransformerLayer.self_attn", "modules.TransformerLayer.final_layer_norm", "modules.gelu", "modules.TransformerLayer.fc2", "modules.TransformerLayer.fc1"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.gelu"], ["", "def", "forward", "(", "\n", "self", ",", "x", ",", "self_attn_mask", "=", "None", ",", "self_attn_padding_mask", "=", "None", ",", "need_head_weights", "=", "False", "\n", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "x", "=", "self", ".", "self_attn_layer_norm", "(", "x", ")", "\n", "x", ",", "attn", "=", "self", ".", "self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "key_padding_mask", "=", "self_attn_padding_mask", ",", "\n", "need_weights", "=", "True", ",", "\n", "need_head_weights", "=", "need_head_weights", ",", "\n", "attn_mask", "=", "self_attn_mask", ",", "\n", ")", "\n", "x", "=", "residual", "+", "x", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "final_layer_norm", "(", "x", ")", "\n", "x", "=", "gelu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "residual", "+", "x", "\n", "\n", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.AxialTransformerLayer.__init__": [[146, 186], ["torch.Module.__init__", "axial_attention.RowSelfAttention", "axial_attention.ColumnSelfAttention", "modules.FeedForwardNetwork", "modules.AxialTransformerLayer.build_residual", "modules.AxialTransformerLayer.build_residual", "modules.AxialTransformerLayer.build_residual"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.AxialTransformerLayer.build_residual", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.AxialTransformerLayer.build_residual", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.AxialTransformerLayer.build_residual"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embedding_dim", ":", "int", "=", "768", ",", "\n", "ffn_embedding_dim", ":", "int", "=", "3072", ",", "\n", "num_attention_heads", ":", "int", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "attention_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "max_tokens_per_msa", ":", "int", "=", "2", "**", "14", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialize parameters", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "dropout_prob", "=", "dropout", "\n", "\n", "row_self_attention", "=", "RowSelfAttention", "(", "\n", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "dropout", "=", "dropout", ",", "\n", "max_tokens_per_msa", "=", "max_tokens_per_msa", ",", "\n", ")", "\n", "\n", "column_self_attention", "=", "ColumnSelfAttention", "(", "\n", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "dropout", "=", "dropout", ",", "\n", "max_tokens_per_msa", "=", "max_tokens_per_msa", ",", "\n", ")", "\n", "\n", "feed_forward_layer", "=", "FeedForwardNetwork", "(", "\n", "embedding_dim", ",", "\n", "ffn_embedding_dim", ",", "\n", "activation_dropout", "=", "activation_dropout", ",", "\n", "max_tokens_per_msa", "=", "max_tokens_per_msa", ",", "\n", ")", "\n", "\n", "self", ".", "row_self_attention", "=", "self", ".", "build_residual", "(", "row_self_attention", ")", "\n", "self", ".", "column_self_attention", "=", "self", ".", "build_residual", "(", "column_self_attention", ")", "\n", "self", ".", "feed_forward_layer", "=", "self", ".", "build_residual", "(", "feed_forward_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.AxialTransformerLayer.build_residual": [[187, 192], ["modules.NormalizedResidualBlock"], "methods", ["None"], ["", "def", "build_residual", "(", "self", ",", "layer", ":", "nn", ".", "Module", ")", ":", "\n", "        ", "return", "NormalizedResidualBlock", "(", "\n", "layer", ",", "\n", "self", ".", "embedding_dim", ",", "\n", "self", ".", "dropout_prob", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.AxialTransformerLayer.forward": [[194, 220], ["modules.AxialTransformerLayer.row_self_attention", "modules.AxialTransformerLayer.column_self_attention", "modules.AxialTransformerLayer.feed_forward_layer"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "self_attn_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "self_attn_padding_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "need_head_weights", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        LayerNorm is applied either before or after the self-attention/ffn\n        modules similar to the original Transformer implementation.\n        \"\"\"", "\n", "x", ",", "row_attn", "=", "self", ".", "row_self_attention", "(", "\n", "x", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "self_attn_padding_mask", "=", "self_attn_padding_mask", ",", "\n", ")", "\n", "x", ",", "column_attn", "=", "self", ".", "column_self_attention", "(", "\n", "x", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "self_attn_padding_mask", "=", "self_attn_padding_mask", ",", "\n", ")", "\n", "x", "=", "self", ".", "feed_forward_layer", "(", "x", ")", "\n", "if", "need_head_weights", ":", "\n", "            ", "return", "x", ",", "column_attn", ",", "row_attn", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.LearnedPositionalEmbedding.__init__": [[230, 237], ["torch.Embedding.__init__"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "self", ",", "num_embeddings", ":", "int", ",", "embedding_dim", ":", "int", ",", "padding_idx", ":", "int", ")", ":", "\n", "        ", "if", "padding_idx", "is", "not", "None", ":", "\n", "            ", "num_embeddings_", "=", "num_embeddings", "+", "padding_idx", "+", "1", "\n", "", "else", ":", "\n", "            ", "num_embeddings_", "=", "num_embeddings", "\n", "", "super", "(", ")", ".", "__init__", "(", "num_embeddings_", ",", "embedding_dim", ",", "padding_idx", ")", "\n", "self", ".", "max_positions", "=", "num_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.LearnedPositionalEmbedding.forward": [[238, 255], ["input.ne().int", "torch.embedding", "torch.embedding", "torch.embedding", "input.size", "ValueError", "input.ne", "input.size", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"", "\n", "if", "input", ".", "size", "(", "1", ")", ">", "self", ".", "max_positions", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Sequence length {input.size(1)} above maximum \"", "\n", "f\" sequence length of {self.max_positions}\"", "\n", ")", "\n", "", "mask", "=", "input", ".", "ne", "(", "self", ".", "padding_idx", ")", ".", "int", "(", ")", "\n", "positions", "=", "(", "torch", ".", "cumsum", "(", "mask", ",", "dim", "=", "1", ")", ".", "type_as", "(", "mask", ")", "*", "mask", ")", ".", "long", "(", ")", "+", "self", ".", "padding_idx", "\n", "return", "F", ".", "embedding", "(", "\n", "positions", ",", "\n", "self", ".", "weight", ",", "\n", "self", ".", "padding_idx", ",", "\n", "self", ".", "max_norm", ",", "\n", "self", ".", "norm_type", ",", "\n", "self", ".", "scale_grad_by_freq", ",", "\n", "self", ".", "sparse", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.SinusoidalPositionalEmbedding.__init__": [[259, 265], ["torch.Module.__init__", "modules.SinusoidalPositionalEmbedding.register_buffer", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embed_dim", ",", "padding_idx", ",", "learned", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "self", ".", "register_buffer", "(", "\"_float_tensor\"", ",", "torch", ".", "FloatTensor", "(", "1", ")", ")", "\n", "self", ".", "weights", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.SinusoidalPositionalEmbedding.forward": [[266, 275], ["modules.SinusoidalPositionalEmbedding.weights.type_as", "modules.SinusoidalPositionalEmbedding.make_positions", "modules.SinusoidalPositionalEmbedding.weights.index_select().view().detach", "modules.SinusoidalPositionalEmbedding.get_embedding", "modules.SinusoidalPositionalEmbedding.weights.size", "modules.SinusoidalPositionalEmbedding.weights.index_select().view", "modules.SinusoidalPositionalEmbedding.weights.index_select", "modules.SinusoidalPositionalEmbedding.view"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.SinusoidalPositionalEmbedding.make_positions", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.SinusoidalPositionalEmbedding.get_embedding"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "bsz", ",", "seq_len", "=", "x", ".", "shape", "\n", "max_pos", "=", "self", ".", "padding_idx", "+", "1", "+", "seq_len", "\n", "if", "self", ".", "weights", "is", "None", "or", "max_pos", ">", "self", ".", "weights", ".", "size", "(", "0", ")", ":", "\n", "            ", "self", ".", "weights", "=", "self", ".", "get_embedding", "(", "max_pos", ")", "\n", "", "self", ".", "weights", "=", "self", ".", "weights", ".", "type_as", "(", "self", ".", "_float_tensor", ")", "\n", "\n", "positions", "=", "self", ".", "make_positions", "(", "x", ")", "\n", "return", "self", ".", "weights", ".", "index_select", "(", "0", ",", "positions", ".", "view", "(", "-", "1", ")", ")", ".", "view", "(", "bsz", ",", "seq_len", ",", "-", "1", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.SinusoidalPositionalEmbedding.make_positions": [[276, 281], ["x.ne", "range_buf.expand_as", "torch.arange().expand_as", "torch.arange().expand_as", "torch.arange().expand_as", "torch.arange().expand_as", "torch.arange().expand_as", "torch.arange().expand_as", "torch.arange().expand_as", "torch.arange().expand_as", "torch.arange().expand_as", "x.ne.long", "x.ne.long", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "x.size"], "methods", ["None"], ["", "def", "make_positions", "(", "self", ",", "x", ")", ":", "\n", "        ", "mask", "=", "x", ".", "ne", "(", "self", ".", "padding_idx", ")", "\n", "range_buf", "=", "torch", ".", "arange", "(", "x", ".", "size", "(", "1", ")", ",", "device", "=", "x", ".", "device", ")", ".", "expand_as", "(", "x", ")", "+", "self", ".", "padding_idx", "+", "1", "\n", "positions", "=", "range_buf", ".", "expand_as", "(", "x", ")", "\n", "return", "positions", "*", "mask", ".", "long", "(", ")", "+", "self", ".", "padding_idx", "*", "(", "1", "-", "mask", ".", "long", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.SinusoidalPositionalEmbedding.get_embedding": [[282, 294], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "math.log", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos"], "methods", ["None"], ["", "def", "get_embedding", "(", "self", ",", "num_embeddings", ")", ":", "\n", "        ", "half_dim", "=", "self", ".", "embed_dim", "//", "2", "\n", "emb", "=", "math", ".", "log", "(", "10000", ")", "/", "(", "half_dim", "-", "1", ")", "\n", "emb", "=", "torch", ".", "exp", "(", "torch", ".", "arange", "(", "half_dim", ",", "dtype", "=", "torch", ".", "float", ")", "*", "-", "emb", ")", "\n", "emb", "=", "torch", ".", "arange", "(", "num_embeddings", ",", "dtype", "=", "torch", ".", "float", ")", ".", "unsqueeze", "(", "1", ")", "*", "emb", ".", "unsqueeze", "(", "0", ")", "\n", "emb", "=", "torch", ".", "cat", "(", "[", "torch", ".", "sin", "(", "emb", ")", ",", "torch", ".", "cos", "(", "emb", ")", "]", ",", "dim", "=", "1", ")", ".", "view", "(", "num_embeddings", ",", "-", "1", ")", "\n", "if", "self", ".", "embed_dim", "%", "2", "==", "1", ":", "\n", "# zero pad", "\n", "            ", "emb", "=", "torch", ".", "cat", "(", "[", "emb", ",", "torch", ".", "zeros", "(", "num_embeddings", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "self", ".", "padding_idx", "is", "not", "None", ":", "\n", "            ", "emb", "[", "self", ".", "padding_idx", ",", ":", "]", "=", "0", "\n", "", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.RobertaLMHead.__init__": [[299, 305], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "ESM1bLayerNorm", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "output_dim", ",", "weight", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "layer_norm", "=", "ESM1bLayerNorm", "(", "embed_dim", ")", "\n", "self", ".", "weight", "=", "weight", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "output_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.RobertaLMHead.forward": [[306, 313], ["modules.RobertaLMHead.dense", "modules.gelu", "modules.RobertaLMHead.layer_norm", "torch.linear", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.gelu"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "        ", "x", "=", "self", ".", "dense", "(", "features", ")", "\n", "x", "=", "gelu", "(", "x", ")", "\n", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "# project back to size of vocabulary with bias", "\n", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "weight", ")", "+", "self", ".", "bias", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.ContactPredictionHead.__init__": [[318, 335], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "ValueError"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_features", ":", "int", ",", "\n", "prepend_bos", ":", "bool", ",", "\n", "append_eos", ":", "bool", ",", "\n", "bias", "=", "True", ",", "\n", "eos_idx", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_features", "=", "in_features", "\n", "self", ".", "prepend_bos", "=", "prepend_bos", "\n", "self", ".", "append_eos", "=", "append_eos", "\n", "if", "append_eos", "and", "eos_idx", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Using an alphabet with eos token, but no eos token was passed in.\"", ")", "\n", "", "self", ".", "eos_idx", "=", "eos_idx", "\n", "self", ".", "regression", "=", "nn", ".", "Linear", "(", "in_features", ",", "1", ",", "bias", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.ContactPredictionHead.forward": [[336, 356], ["attentions.permute.permute.size", "attentions.permute.permute.view", "attentions.permute.permute.to", "modules.apc", "attentions.permute.permute.permute", "modules.ContactPredictionHead.activation", "tokens.ne().to", "next", "modules.symmetrize", "modules.ContactPredictionHead.regression().squeeze", "tokens.ne().to.unsqueeze", "tokens.ne().to.unsqueeze", "modules.ContactPredictionHead.parameters", "tokens.ne", "modules.ContactPredictionHead.regression"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.apc", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.symmetrize"], ["", "def", "forward", "(", "self", ",", "tokens", ",", "attentions", ")", ":", "\n", "# remove eos token attentions", "\n", "        ", "if", "self", ".", "append_eos", ":", "\n", "            ", "eos_mask", "=", "tokens", ".", "ne", "(", "self", ".", "eos_idx", ")", ".", "to", "(", "attentions", ")", "\n", "eos_mask", "=", "eos_mask", ".", "unsqueeze", "(", "1", ")", "*", "eos_mask", ".", "unsqueeze", "(", "2", ")", "\n", "attentions", "=", "attentions", "*", "eos_mask", "[", ":", ",", "None", ",", "None", ",", ":", ",", ":", "]", "\n", "attentions", "=", "attentions", "[", "...", ",", ":", "-", "1", ",", ":", "-", "1", "]", "\n", "# remove cls token attentions", "\n", "", "if", "self", ".", "prepend_bos", ":", "\n", "            ", "attentions", "=", "attentions", "[", "...", ",", "1", ":", ",", "1", ":", "]", "\n", "", "batch_size", ",", "layers", ",", "heads", ",", "seqlen", ",", "_", "=", "attentions", ".", "size", "(", ")", "\n", "attentions", "=", "attentions", ".", "view", "(", "batch_size", ",", "layers", "*", "heads", ",", "seqlen", ",", "seqlen", ")", "\n", "\n", "# features: B x C x T x T", "\n", "attentions", "=", "attentions", ".", "to", "(", "\n", "next", "(", "self", ".", "parameters", "(", ")", ")", "\n", ")", "# attentions always float32, may need to convert to float16", "\n", "attentions", "=", "apc", "(", "symmetrize", "(", "attentions", ")", ")", "\n", "attentions", "=", "attentions", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "return", "self", ".", "activation", "(", "self", ".", "regression", "(", "attentions", ")", ".", "squeeze", "(", "3", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.NormalizedResidualBlock.__init__": [[359, 373], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "ESM1bLayerNorm"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "layer", ":", "nn", ".", "Module", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "\n", "self", ".", "layer", "=", "layer", "\n", "self", ".", "dropout_module", "=", "nn", ".", "Dropout", "(", "\n", "dropout", ",", "\n", ")", "\n", "self", ".", "layer_norm", "=", "ESM1bLayerNorm", "(", "self", ".", "embedding_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.NormalizedResidualBlock.forward": [[374, 391], ["modules.NormalizedResidualBlock.layer_norm", "modules.NormalizedResidualBlock.layer", "isinstance", "modules.NormalizedResidualBlock.dropout_module", "tuple"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "outputs", "=", "self", ".", "layer", "(", "x", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "if", "isinstance", "(", "outputs", ",", "tuple", ")", ":", "\n", "            ", "x", ",", "*", "out", "=", "outputs", "\n", "", "else", ":", "\n", "            ", "x", "=", "outputs", "\n", "out", "=", "None", "\n", "\n", "", "x", "=", "self", ".", "dropout_module", "(", "x", ")", "\n", "x", "=", "residual", "+", "x", "\n", "\n", "if", "out", "is", "not", "None", ":", "\n", "            ", "return", "(", "x", ",", ")", "+", "tuple", "(", "out", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.FeedForwardNetwork.__init__": [[394, 411], ["torch.Module.__init__", "torch.GELU", "torch.GELU", "torch.GELU", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "ffn_embedding_dim", ":", "int", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "max_tokens_per_msa", ":", "int", "=", "2", "**", "14", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "ffn_embedding_dim", "=", "ffn_embedding_dim", "\n", "self", ".", "max_tokens_per_msa", "=", "max_tokens_per_msa", "\n", "self", ".", "activation_fn", "=", "nn", ".", "GELU", "(", ")", "\n", "self", ".", "activation_dropout_module", "=", "nn", ".", "Dropout", "(", "\n", "activation_dropout", ",", "\n", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "embedding_dim", ",", "ffn_embedding_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "ffn_embedding_dim", ",", "embedding_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.FeedForwardNetwork.forward": [[412, 417], ["modules.FeedForwardNetwork.activation_fn", "modules.FeedForwardNetwork.activation_dropout_module", "modules.FeedForwardNetwork.fc2", "modules.FeedForwardNetwork.fc1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "activation_dropout_module", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.gelu": [[17, 25], ["torch.erf", "torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n\n    For information: OpenAI GPT's gelu is slightly different\n    (and gives slightly different results):\n    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.symmetrize": [[27, 30], ["x.transpose"], "function", ["None"], ["", "def", "symmetrize", "(", "x", ")", ":", "\n", "    ", "\"Make layer symmetric in final two dimensions, used for contact prediction.\"", "\n", "return", "x", "+", "x", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.modules.apc": [[32, 42], ["x.sum", "x.sum", "x.sum", "avg.div_"], "function", ["None"], ["", "def", "apc", "(", "x", ")", ":", "\n", "    ", "\"Perform average product correct, used for contact prediction.\"", "\n", "a1", "=", "x", ".", "sum", "(", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "a2", "=", "x", ".", "sum", "(", "-", "2", ",", "keepdims", "=", "True", ")", "\n", "a12", "=", "x", ".", "sum", "(", "(", "-", "1", ",", "-", "2", ")", ",", "keepdims", "=", "True", ")", "\n", "\n", "avg", "=", "a1", "*", "a2", "\n", "avg", ".", "div_", "(", "a12", ")", "# in-place to reduce memory", "\n", "normalized", "=", "x", "-", "avg", "\n", "return", "normalized", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.FastaBatchedDataset.__init__": [[20, 23], ["list", "list"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sequence_labels", ",", "sequence_strs", ")", ":", "\n", "        ", "self", ".", "sequence_labels", "=", "list", "(", "sequence_labels", ")", "\n", "self", ".", "sequence_strs", "=", "list", "(", "sequence_strs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.FastaBatchedDataset.from_file": [[24, 58], ["data.FastaBatchedDataset.from_file._flush_current_seq"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_file", "(", "cls", ",", "fasta_file", ")", ":", "\n", "        ", "sequence_labels", ",", "sequence_strs", "=", "[", "]", ",", "[", "]", "\n", "cur_seq_label", "=", "None", "\n", "buf", "=", "[", "]", "\n", "\n", "def", "_flush_current_seq", "(", ")", ":", "\n", "            ", "nonlocal", "cur_seq_label", ",", "buf", "\n", "if", "cur_seq_label", "is", "None", ":", "\n", "                ", "return", "\n", "", "sequence_labels", ".", "append", "(", "cur_seq_label", ")", "\n", "sequence_strs", ".", "append", "(", "\"\"", ".", "join", "(", "buf", ")", ")", "\n", "cur_seq_label", "=", "None", "\n", "buf", "=", "[", "]", "\n", "\n", "", "with", "open", "(", "fasta_file", ",", "\"r\"", ")", "as", "infile", ":", "\n", "            ", "for", "line_idx", ",", "line", "in", "enumerate", "(", "infile", ")", ":", "\n", "                ", "if", "line", ".", "startswith", "(", "\">\"", ")", ":", "# label line", "\n", "                    ", "_flush_current_seq", "(", ")", "\n", "line", "=", "line", "[", "1", ":", "]", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", ">", "0", ":", "\n", "                        ", "cur_seq_label", "=", "line", "\n", "", "else", ":", "\n", "                        ", "cur_seq_label", "=", "f\"seqnum{line_idx:09d}\"", "\n", "", "", "else", ":", "# sequence line", "\n", "                    ", "buf", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "", "", "", "_flush_current_seq", "(", ")", "\n", "\n", "assert", "len", "(", "set", "(", "sequence_labels", ")", ")", "==", "len", "(", "\n", "sequence_labels", "\n", ")", ",", "\"Found duplicate sequence labels\"", "\n", "\n", "return", "cls", "(", "sequence_labels", ",", "sequence_strs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.FastaBatchedDataset.__len__": [[59, 61], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "sequence_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.FastaBatchedDataset.__getitem__": [[62, 64], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "sequence_labels", "[", "idx", "]", ",", "self", ".", "sequence_strs", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.FastaBatchedDataset.get_batch_indices": [[65, 89], ["sizes.sort", "data.FastaBatchedDataset.get_batch_indices._flush_current_buf"], "methods", ["None"], ["", "def", "get_batch_indices", "(", "self", ",", "toks_per_batch", ",", "extra_toks_per_seq", "=", "0", ")", ":", "\n", "        ", "sizes", "=", "[", "(", "len", "(", "s", ")", ",", "i", ")", "for", "i", ",", "s", "in", "enumerate", "(", "self", ".", "sequence_strs", ")", "]", "\n", "sizes", ".", "sort", "(", ")", "\n", "batches", "=", "[", "]", "\n", "buf", "=", "[", "]", "\n", "max_len", "=", "0", "\n", "\n", "def", "_flush_current_buf", "(", ")", ":", "\n", "            ", "nonlocal", "max_len", ",", "buf", "\n", "if", "len", "(", "buf", ")", "==", "0", ":", "\n", "                ", "return", "\n", "", "batches", ".", "append", "(", "buf", ")", "\n", "buf", "=", "[", "]", "\n", "max_len", "=", "0", "\n", "\n", "", "for", "sz", ",", "i", "in", "sizes", ":", "\n", "            ", "sz", "+=", "extra_toks_per_seq", "\n", "if", "max", "(", "sz", ",", "max_len", ")", "*", "(", "len", "(", "buf", ")", "+", "1", ")", ">", "toks_per_batch", ":", "\n", "                ", "_flush_current_buf", "(", ")", "\n", "", "max_len", "=", "max", "(", "max_len", ",", "sz", ")", "\n", "buf", ".", "append", "(", "i", ")", "\n", "\n", "", "_flush_current_buf", "(", ")", "\n", "return", "batches", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.__init__": [[92, 123], ["list", "list", "list", "list", "data.Alphabet.all_toks.extend", "range", "data.Alphabet.all_toks.extend", "data.Alphabet.get_idx", "data.Alphabet.get_idx", "data.Alphabet.get_idx", "data.Alphabet.get_idx", "data.Alphabet.all_toks.append", "enumerate", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "standard_toks", ":", "Sequence", "[", "str", "]", ",", "\n", "prepend_toks", ":", "Sequence", "[", "str", "]", "=", "(", "\"<null_0>\"", ",", "\"<pad>\"", ",", "\"<eos>\"", ",", "\"<unk>\"", ")", ",", "\n", "append_toks", ":", "Sequence", "[", "str", "]", "=", "(", "\"<cls>\"", ",", "\"<mask>\"", ",", "\"<sep>\"", ")", ",", "\n", "prepend_bos", ":", "bool", "=", "True", ",", "\n", "append_eos", ":", "bool", "=", "False", ",", "\n", "use_msa", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self", ".", "standard_toks", "=", "list", "(", "standard_toks", ")", "\n", "self", ".", "prepend_toks", "=", "list", "(", "prepend_toks", ")", "\n", "self", ".", "append_toks", "=", "list", "(", "append_toks", ")", "\n", "self", ".", "prepend_bos", "=", "prepend_bos", "\n", "self", ".", "append_eos", "=", "append_eos", "\n", "self", ".", "use_msa", "=", "use_msa", "\n", "\n", "self", ".", "all_toks", "=", "list", "(", "self", ".", "prepend_toks", ")", "\n", "self", ".", "all_toks", ".", "extend", "(", "self", ".", "standard_toks", ")", "\n", "for", "i", "in", "range", "(", "(", "8", "-", "(", "len", "(", "self", ".", "all_toks", ")", "%", "8", ")", ")", "%", "8", ")", ":", "\n", "            ", "self", ".", "all_toks", ".", "append", "(", "f\"<null_{i  + 1}>\"", ")", "\n", "", "self", ".", "all_toks", ".", "extend", "(", "self", ".", "append_toks", ")", "\n", "\n", "self", ".", "tok_to_idx", "=", "{", "tok", ":", "i", "for", "i", ",", "tok", "in", "enumerate", "(", "self", ".", "all_toks", ")", "}", "\n", "\n", "self", ".", "unk_idx", "=", "self", ".", "tok_to_idx", "[", "\"<unk>\"", "]", "\n", "self", ".", "padding_idx", "=", "self", ".", "get_idx", "(", "\"<pad>\"", ")", "\n", "self", ".", "cls_idx", "=", "self", ".", "get_idx", "(", "\"<cls>\"", ")", "\n", "self", ".", "mask_idx", "=", "self", ".", "get_idx", "(", "\"<mask>\"", ")", "\n", "self", ".", "eos_idx", "=", "self", ".", "get_idx", "(", "\"<eos>\"", ")", "\n", "self", ".", "all_special_tokens", "=", "[", "'<eos>'", ",", "'<unk>'", ",", "'<pad>'", ",", "'<cls>'", ",", "'<mask>'", "]", "\n", "self", ".", "unique_no_split_tokens", "=", "self", ".", "all_toks", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.__len__": [[124, 126], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "all_toks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_idx": [[127, 129], ["data.Alphabet.tok_to_idx.get"], "methods", ["None"], ["", "def", "get_idx", "(", "self", ",", "tok", ")", ":", "\n", "        ", "return", "self", ".", "tok_to_idx", ".", "get", "(", "tok", ",", "self", ".", "unk_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_tok": [[130, 132], ["None"], "methods", ["None"], ["", "def", "get_tok", "(", "self", ",", "ind", ")", ":", "\n", "        ", "return", "self", ".", "all_toks", "[", "ind", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.to_dict": [[133, 135], ["data.Alphabet.tok_to_idx.copy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tok_to_idx", ".", "copy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.get_batch_converter": [[136, 141], ["data.MSABatchConverter", "data.BatchConverter"], "methods", ["None"], ["", "def", "get_batch_converter", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "use_msa", ":", "\n", "            ", "return", "MSABatchConverter", "(", "self", ")", "\n", "", "else", ":", "\n", "            ", "return", "BatchConverter", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.from_architecture": [[142, 175], ["cls", "name.lower", "ValueError"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_architecture", "(", "cls", ",", "name", ":", "str", ")", "->", "\"Alphabet\"", ":", "\n", "        ", "if", "name", "in", "(", "\"ESM-1\"", ",", "\"protein_bert_base\"", ")", ":", "\n", "            ", "standard_toks", "=", "proteinseq_toks", "[", "\"toks\"", "]", "\n", "prepend_toks", ":", "Tuple", "[", "str", ",", "...", "]", "=", "(", "\"<null_0>\"", ",", "\"<pad>\"", ",", "\"<eos>\"", ",", "\"<unk>\"", ")", "\n", "append_toks", ":", "Tuple", "[", "str", ",", "...", "]", "=", "(", "\"<cls>\"", ",", "\"<mask>\"", ",", "\"<sep>\"", ")", "\n", "prepend_bos", "=", "True", "\n", "append_eos", "=", "False", "\n", "use_msa", "=", "False", "\n", "", "elif", "name", "in", "(", "\"ESM-1b\"", ",", "\"roberta_large\"", ")", ":", "\n", "            ", "standard_toks", "=", "proteinseq_toks", "[", "\"toks\"", "]", "\n", "prepend_toks", "=", "(", "\"<cls>\"", ",", "\"<pad>\"", ",", "\"<eos>\"", ",", "\"<unk>\"", ")", "\n", "append_toks", "=", "(", "\"<mask>\"", ",", ")", "\n", "prepend_bos", "=", "True", "\n", "append_eos", "=", "True", "\n", "use_msa", "=", "False", "\n", "", "elif", "name", "in", "(", "\"MSA Transformer\"", ",", "\"msa_transformer\"", ")", ":", "\n", "            ", "standard_toks", "=", "proteinseq_toks", "[", "\"toks\"", "]", "\n", "prepend_toks", "=", "(", "\"<cls>\"", ",", "\"<pad>\"", ",", "\"<eos>\"", ",", "\"<unk>\"", ")", "\n", "append_toks", "=", "(", "\"<mask>\"", ",", ")", "\n", "prepend_bos", "=", "True", "\n", "append_eos", "=", "False", "\n", "use_msa", "=", "True", "\n", "", "elif", "\"invariant_gvp\"", "in", "name", ".", "lower", "(", ")", ":", "\n", "            ", "standard_toks", "=", "proteinseq_toks", "[", "\"toks\"", "]", "\n", "prepend_toks", "=", "(", "\"<null_0>\"", ",", "\"<pad>\"", ",", "\"<eos>\"", ",", "\"<unk>\"", ")", "\n", "append_toks", "=", "(", "\"<mask>\"", ",", "\"<cath>\"", ",", "\"<af2>\"", ")", "\n", "prepend_bos", "=", "True", "\n", "append_eos", "=", "False", "\n", "use_msa", "=", "False", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown architecture selected\"", ")", "\n", "", "return", "cls", "(", "standard_toks", ",", "prepend_toks", ",", "append_toks", ",", "prepend_bos", ",", "append_eos", ",", "use_msa", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet._tokenize": [[176, 178], ["text.split"], "methods", ["None"], ["", "def", "_tokenize", "(", "self", ",", "text", ")", "->", "str", ":", "\n", "        ", "return", "text", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.tokenize": [[179, 248], ["data.Alphabet.tokenize.split_on_tokens"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "**", "kwargs", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "\"\"\"\n        Inspired by https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils.py\n        Converts a string in a sequence of tokens, using the tokenizer.\n\n        Args:\n            text (:obj:`str`):\n                The sequence to be encoded.\n\n        Returns:\n            :obj:`List[str]`: The list of tokens.\n        \"\"\"", "\n", "\n", "def", "split_on_token", "(", "tok", ",", "text", ")", ":", "\n", "            ", "result", "=", "[", "]", "\n", "split_text", "=", "text", ".", "split", "(", "tok", ")", "\n", "for", "i", ",", "sub_text", "in", "enumerate", "(", "split_text", ")", ":", "\n", "# AddedToken can control whitespace stripping around them.", "\n", "# We use them for GPT2 and Roberta to have different behavior depending on the special token", "\n", "# Cf. https://github.com/huggingface/transformers/pull/2778", "\n", "# and https://github.com/huggingface/transformers/issues/3788", "\n", "# We strip left and right by default", "\n", "                ", "if", "i", "<", "len", "(", "split_text", ")", "-", "1", ":", "\n", "                    ", "sub_text", "=", "sub_text", ".", "rstrip", "(", ")", "\n", "", "if", "i", ">", "0", ":", "\n", "                    ", "sub_text", "=", "sub_text", ".", "lstrip", "(", ")", "\n", "\n", "", "if", "i", "==", "0", "and", "not", "sub_text", ":", "\n", "                    ", "result", ".", "append", "(", "tok", ")", "\n", "", "elif", "i", "==", "len", "(", "split_text", ")", "-", "1", ":", "\n", "                    ", "if", "sub_text", ":", "\n", "                        ", "result", ".", "append", "(", "sub_text", ")", "\n", "", "else", ":", "\n", "                        ", "pass", "\n", "", "", "else", ":", "\n", "                    ", "if", "sub_text", ":", "\n", "                        ", "result", ".", "append", "(", "sub_text", ")", "\n", "", "result", ".", "append", "(", "tok", ")", "\n", "", "", "return", "result", "\n", "\n", "", "def", "split_on_tokens", "(", "tok_list", ",", "text", ")", ":", "\n", "            ", "if", "not", "text", ".", "strip", "(", ")", ":", "\n", "                ", "return", "[", "]", "\n", "\n", "", "tokenized_text", "=", "[", "]", "\n", "text_list", "=", "[", "text", "]", "\n", "for", "tok", "in", "tok_list", ":", "\n", "                ", "tokenized_text", "=", "[", "]", "\n", "for", "sub_text", "in", "text_list", ":", "\n", "                    ", "if", "sub_text", "not", "in", "self", ".", "unique_no_split_tokens", ":", "\n", "                        ", "tokenized_text", ".", "extend", "(", "split_on_token", "(", "tok", ",", "sub_text", ")", ")", "\n", "", "else", ":", "\n", "                        ", "tokenized_text", ".", "append", "(", "sub_text", ")", "\n", "", "", "text_list", "=", "tokenized_text", "\n", "\n", "", "return", "list", "(", "\n", "itertools", ".", "chain", ".", "from_iterable", "(", "\n", "(", "\n", "self", ".", "_tokenize", "(", "token", ")", "\n", "if", "token", "not", "in", "self", ".", "unique_no_split_tokens", "\n", "else", "[", "token", "]", "\n", "for", "token", "in", "tokenized_text", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "", "no_split_token", "=", "self", ".", "unique_no_split_tokens", "\n", "tokenized_text", "=", "split_on_tokens", "(", "no_split_token", ",", "text", ")", "\n", "return", "tokenized_text", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.encode": [[249, 251], ["data.Alphabet.tokenize"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.tokenize"], ["", "def", "encode", "(", "self", ",", "text", ")", ":", "\n", "        ", "return", "[", "self", ".", "tok_to_idx", "[", "tok", "]", "for", "tok", "in", "self", ".", "tokenize", "(", "text", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.BatchConverter.__init__": [[258, 260], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "alphabet", ")", ":", "\n", "        ", "self", ".", "alphabet", "=", "alphabet", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.BatchConverter.__call__": [[261, 295], ["len", "zip", "max", "torch.empty", "torch.empty.fill_", "enumerate", "data.BatchConverter.alphabet.encode", "zip", "labels.append", "strs.append", "torch.tensor", "len", "int", "int", "int", "len", "int", "len", "int"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.Alphabet.encode"], ["", "def", "__call__", "(", "self", ",", "raw_batch", ":", "Sequence", "[", "Tuple", "[", "str", ",", "str", "]", "]", ")", ":", "\n", "# RoBERTa uses an eos token, while ESM-1 does not.", "\n", "        ", "batch_size", "=", "len", "(", "raw_batch", ")", "\n", "batch_labels", ",", "seq_str_list", "=", "zip", "(", "*", "raw_batch", ")", "\n", "seq_encoded_list", "=", "[", "self", ".", "alphabet", ".", "encode", "(", "seq_str", ")", "for", "seq_str", "in", "seq_str_list", "]", "\n", "max_len", "=", "max", "(", "len", "(", "seq_encoded", ")", "for", "seq_encoded", "in", "seq_encoded_list", ")", "\n", "tokens", "=", "torch", ".", "empty", "(", "\n", "(", "\n", "batch_size", ",", "\n", "max_len", "+", "int", "(", "self", ".", "alphabet", ".", "prepend_bos", ")", "+", "int", "(", "self", ".", "alphabet", ".", "append_eos", ")", ",", "\n", ")", ",", "\n", "dtype", "=", "torch", ".", "int64", ",", "\n", ")", "\n", "tokens", ".", "fill_", "(", "self", ".", "alphabet", ".", "padding_idx", ")", "\n", "labels", "=", "[", "]", "\n", "strs", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "label", ",", "seq_str", ",", "seq_encoded", ")", "in", "enumerate", "(", "\n", "zip", "(", "batch_labels", ",", "seq_str_list", ",", "seq_encoded_list", ")", "\n", ")", ":", "\n", "            ", "labels", ".", "append", "(", "label", ")", "\n", "strs", ".", "append", "(", "seq_str", ")", "\n", "if", "self", ".", "alphabet", ".", "prepend_bos", ":", "\n", "                ", "tokens", "[", "i", ",", "0", "]", "=", "self", ".", "alphabet", ".", "cls_idx", "\n", "", "seq", "=", "torch", ".", "tensor", "(", "seq_encoded", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "tokens", "[", "\n", "i", ",", "\n", "int", "(", "self", ".", "alphabet", ".", "prepend_bos", ")", ":", "len", "(", "seq_encoded", ")", "\n", "+", "int", "(", "self", ".", "alphabet", ".", "prepend_bos", ")", ",", "\n", "]", "=", "seq", "\n", "if", "self", ".", "alphabet", ".", "append_eos", ":", "\n", "                ", "tokens", "[", "i", ",", "len", "(", "seq_encoded", ")", "+", "int", "(", "self", ".", "alphabet", ".", "prepend_bos", ")", "]", "=", "self", ".", "alphabet", ".", "eos_idx", "\n", "\n", "", "", "return", "labels", ",", "strs", ",", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.MSABatchConverter.__call__": [[298, 334], ["isinstance", "len", "max", "max", "torch.empty", "torch.empty.fill_", "enumerate", "set", "data.BatchConverter.__call__", "labels.append", "strs.append", "len", "len", "RuntimeError", "int", "len", "len", "int", "msa_tokens.size", "msa_tokens.size"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.MSABatchConverter.__call__"], ["    ", "def", "__call__", "(", "self", ",", "inputs", ":", "Union", "[", "Sequence", "[", "RawMSA", "]", ",", "RawMSA", "]", ")", ":", "\n", "        ", "if", "isinstance", "(", "inputs", "[", "0", "]", "[", "0", "]", ",", "str", ")", ":", "\n", "# Input is a single MSA", "\n", "            ", "raw_batch", ":", "Sequence", "[", "RawMSA", "]", "=", "[", "inputs", "]", "# type: ignore", "\n", "", "else", ":", "\n", "            ", "raw_batch", "=", "inputs", "# type: ignore", "\n", "\n", "", "batch_size", "=", "len", "(", "raw_batch", ")", "\n", "max_alignments", "=", "max", "(", "len", "(", "msa", ")", "for", "msa", "in", "raw_batch", ")", "\n", "max_seqlen", "=", "max", "(", "len", "(", "msa", "[", "0", "]", "[", "1", "]", ")", "for", "msa", "in", "raw_batch", ")", "\n", "\n", "tokens", "=", "torch", ".", "empty", "(", "\n", "(", "\n", "batch_size", ",", "\n", "max_alignments", ",", "\n", "max_seqlen", "+", "int", "(", "self", ".", "alphabet", ".", "prepend_bos", ")", "+", "int", "(", "self", ".", "alphabet", ".", "append_eos", ")", ",", "\n", ")", ",", "\n", "dtype", "=", "torch", ".", "int64", ",", "\n", ")", "\n", "tokens", ".", "fill_", "(", "self", ".", "alphabet", ".", "padding_idx", ")", "\n", "labels", "=", "[", "]", "\n", "strs", "=", "[", "]", "\n", "\n", "for", "i", ",", "msa", "in", "enumerate", "(", "raw_batch", ")", ":", "\n", "            ", "msa_seqlens", "=", "set", "(", "len", "(", "seq", ")", "for", "_", ",", "seq", "in", "msa", ")", "\n", "if", "not", "len", "(", "msa_seqlens", ")", "==", "1", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Received unaligned sequences for input to MSA, all sequence \"", "\n", "\"lengths must be equal.\"", "\n", ")", "\n", "", "msa_labels", ",", "msa_strs", ",", "msa_tokens", "=", "super", "(", ")", ".", "__call__", "(", "msa", ")", "\n", "labels", ".", "append", "(", "msa_labels", ")", "\n", "strs", ".", "append", "(", "msa_strs", ")", "\n", "tokens", "[", "i", ",", ":", "msa_tokens", ".", "size", "(", "0", ")", ",", ":", "msa_tokens", ".", "size", "(", "1", ")", "]", "=", "msa_tokens", "\n", "\n", "", "return", "labels", ",", "strs", ",", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.ESMStructuralSplitDataset.__init__": [[427, 454], ["os.path.expanduser", "super().__init__", "os.path.join", "os.path.join", "os.path.join", "data.ESMStructuralSplitDataset.download", "open", "f.read().splitlines", "f.read"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.ESMStructuralSplitDataset.download"], ["def", "__init__", "(", "\n", "self", ",", "\n", "split_level", ",", "\n", "cv_partition", ",", "\n", "split", ",", "\n", "root_path", "=", "os", ".", "path", ".", "expanduser", "(", "\"~/.cache/torch/data/esm\"", ")", ",", "\n", "download", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "split", "in", "[", "\n", "\"train\"", ",", "\n", "\"valid\"", ",", "\n", "]", ",", "\"train_valid must be 'train' or 'valid'\"", "\n", "self", ".", "root_path", "=", "root_path", "\n", "self", ".", "base_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_path", ",", "self", ".", "base_folder", ")", "\n", "\n", "# check if root path has what you need or else download it", "\n", "if", "download", ":", "\n", "            ", "self", ".", "download", "(", ")", "\n", "\n", "", "self", ".", "split_file", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "base_path", ",", "\"splits\"", ",", "split_level", ",", "cv_partition", ",", "f\"{split}.txt\"", "\n", ")", "\n", "self", ".", "pkl_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "\"pkl\"", ")", "\n", "self", ".", "names", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "split_file", ")", "as", "f", ":", "\n", "            ", "self", ".", "names", "=", "f", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.ESMStructuralSplitDataset.__len__": [[455, 457], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.ESMStructuralSplitDataset._check_exists": [[458, 464], ["os.path.join", "os.path.exists", "os.path.isdir"], "methods", ["None"], ["", "def", "_check_exists", "(", "self", ")", "->", "bool", ":", "\n", "        ", "for", "(", "_", ",", "_", ",", "filename", ",", "_", ")", "in", "self", ".", "file_list", ":", "\n", "            ", "fpath", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "fpath", ")", "or", "not", "os", ".", "path", ".", "isdir", "(", "fpath", ")", ":", "\n", "                ", "return", "False", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.ESMStructuralSplitDataset.download": [[465, 477], ["data.ESMStructuralSplitDataset._check_exists", "print", "os.path.join", "download_url", "shutil.unpack_archive"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.ESMStructuralSplitDataset._check_exists"], ["", "def", "download", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "_check_exists", "(", ")", ":", "\n", "            ", "print", "(", "\"Files already downloaded and verified\"", ")", "\n", "return", "\n", "\n", "", "from", "torchvision", ".", "datasets", ".", "utils", "import", "download_url", "\n", "\n", "for", "url", ",", "tar_filename", ",", "filename", ",", "md5_hash", "in", "self", ".", "file_list", ":", "\n", "            ", "download_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "tar_filename", ")", "\n", "download_url", "(", "url", "=", "url", ",", "root", "=", "self", ".", "base_path", ",", "filename", "=", "tar_filename", ",", "md5", "=", "md5_hash", ")", "\n", "shutil", ".", "unpack_archive", "(", "download_path", ",", "self", ".", "base_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.ESMStructuralSplitDataset.__getitem__": [[478, 491], ["os.path.join", "open", "pickle.load"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dict with the following entires\n         - seq : Str (domain sequence)\n         - ssp : Str (SSP labels)\n         - dist : np.array (distance map)\n         - coords : np.array (3D coordinates)\n        \"\"\"", "\n", "name", "=", "self", ".", "names", "[", "idx", "]", "\n", "pkl_fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "pkl_dir", ",", "name", "[", "1", ":", "3", "]", ",", "f\"{name}.pkl\"", ")", "\n", "with", "open", "(", "pkl_fname", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "obj", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "obj", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.read_fasta": [[336, 347], ["open", "data.read_alignment_lines"], "function", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.read_alignment_lines"], ["", "", "def", "read_fasta", "(", "\n", "path", ",", "\n", "keep_gaps", "=", "True", ",", "\n", "keep_insertions", "=", "True", ",", "\n", "to_upper", "=", "False", ",", "\n", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "result", "in", "read_alignment_lines", "(", "\n", "f", ",", "keep_gaps", "=", "keep_gaps", ",", "keep_insertions", "=", "keep_insertions", ",", "to_upper", "=", "to_upper", "\n", ")", ":", "\n", "            ", "yield", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.data.read_alignment_lines": [[349, 376], ["isinstance", "isinstance", "re.sub", "re.sub", "re.sub.upper", "line.strip", "isinstance", "line.strip", "data.read_alignment_lines.parse"], "function", ["None"], ["", "", "", "def", "read_alignment_lines", "(", "\n", "lines", ",", "\n", "keep_gaps", "=", "True", ",", "\n", "keep_insertions", "=", "True", ",", "\n", "to_upper", "=", "False", ",", "\n", ")", ":", "\n", "    ", "seq", "=", "desc", "=", "None", "\n", "\n", "def", "parse", "(", "s", ")", ":", "\n", "        ", "if", "not", "keep_gaps", ":", "\n", "            ", "s", "=", "re", ".", "sub", "(", "\"-\"", ",", "\"\"", ",", "s", ")", "\n", "", "if", "not", "keep_insertions", ":", "\n", "            ", "s", "=", "re", ".", "sub", "(", "\"[a-z]\"", ",", "\"\"", ",", "s", ")", "\n", "", "return", "s", ".", "upper", "(", ")", "if", "to_upper", "else", "s", "\n", "\n", "", "for", "line", "in", "lines", ":", "\n", "# Line may be empty if seq % file_line_width == 0", "\n", "        ", "if", "len", "(", "line", ")", ">", "0", "and", "line", "[", "0", "]", "==", "\">\"", ":", "\n", "            ", "if", "seq", "is", "not", "None", ":", "\n", "                ", "yield", "desc", ",", "parse", "(", "seq", ")", "\n", "", "desc", "=", "line", ".", "strip", "(", ")", "\n", "seq", "=", "\"\"", "\n", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "seq", ",", "str", ")", "\n", "seq", "+=", "line", ".", "strip", "(", ")", "\n", "", "", "assert", "isinstance", "(", "seq", ",", "str", ")", "and", "isinstance", "(", "desc", ",", "str", ")", "\n", "yield", "desc", ",", "parse", "(", "seq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel.add_args": [[26, 50], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "add_args", "(", "cls", ",", "parser", ")", ":", "\n", "        ", "parser", ".", "add_argument", "(", "\n", "\"--num_layers\"", ",", "default", "=", "36", ",", "type", "=", "int", ",", "metavar", "=", "\"N\"", ",", "help", "=", "\"number of layers\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--embed_dim\"", ",", "default", "=", "1280", ",", "type", "=", "int", ",", "metavar", "=", "\"N\"", ",", "help", "=", "\"embedding dimension\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--logit_bias\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"whether to apply bias to logits\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--ffn_embed_dim\"", ",", "\n", "default", "=", "5120", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"embedding dimension for FFN\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--attention_heads\"", ",", "\n", "default", "=", "20", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"number of attention heads\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel.__init__": [[52, 69], ["torch.Module.__init__", "len", "getattr", "model.ProteinBertModel._init_submodules_esm1b", "model.ProteinBertModel._init_submodules_esm1"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel._init_submodules_esm1b", "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel._init_submodules_esm1"], ["", "def", "__init__", "(", "self", ",", "args", ",", "alphabet", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "alphabet_size", "=", "len", "(", "alphabet", ")", "\n", "self", ".", "padding_idx", "=", "alphabet", ".", "padding_idx", "\n", "self", ".", "mask_idx", "=", "alphabet", ".", "mask_idx", "\n", "self", ".", "cls_idx", "=", "alphabet", ".", "cls_idx", "\n", "self", ".", "eos_idx", "=", "alphabet", ".", "eos_idx", "\n", "self", ".", "prepend_bos", "=", "alphabet", ".", "prepend_bos", "\n", "self", ".", "append_eos", "=", "alphabet", ".", "append_eos", "\n", "self", ".", "emb_layer_norm_before", "=", "getattr", "(", "self", ".", "args", ",", "\"emb_layer_norm_before\"", ",", "False", ")", "\n", "if", "self", ".", "args", ".", "arch", "==", "\"roberta_large\"", ":", "\n", "            ", "self", ".", "model_version", "=", "\"ESM-1b\"", "\n", "self", ".", "_init_submodules_esm1b", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "model_version", "=", "\"ESM-1\"", "\n", "self", ".", "_init_submodules_esm1", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel._init_submodules_common": [[70, 92], ["torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "modules.ContactPredictionHead", "modules.TransformerLayer", "range"], "methods", ["None"], ["", "", "def", "_init_submodules_common", "(", "self", ")", ":", "\n", "        ", "self", ".", "embed_tokens", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "alphabet_size", ",", "self", ".", "args", ".", "embed_dim", ",", "padding_idx", "=", "self", ".", "padding_idx", "\n", ")", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "\n", "TransformerLayer", "(", "\n", "self", ".", "args", ".", "embed_dim", ",", "\n", "self", ".", "args", ".", "ffn_embed_dim", ",", "\n", "self", ".", "args", ".", "attention_heads", ",", "\n", "add_bias_kv", "=", "(", "self", ".", "model_version", "!=", "\"ESM-1b\"", ")", ",", "\n", "use_esm1b_layer_norm", "=", "(", "self", ".", "model_version", "==", "\"ESM-1b\"", ")", ",", "\n", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "args", ".", "layers", ")", "\n", "]", "\n", ")", "\n", "\n", "self", ".", "contact_head", "=", "ContactPredictionHead", "(", "\n", "self", ".", "args", ".", "layers", "*", "self", ".", "args", ".", "attention_heads", ",", "\n", "self", ".", "prepend_bos", ",", "\n", "self", ".", "append_eos", ",", "\n", "eos_idx", "=", "self", ".", "eos_idx", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel._init_submodules_esm1b": [[94, 108], ["model.ProteinBertModel._init_submodules_common", "modules.LearnedPositionalEmbedding", "modules.ESM1bLayerNorm", "modules.RobertaLMHead", "modules.ESM1bLayerNorm"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel._init_submodules_common"], ["", "def", "_init_submodules_esm1b", "(", "self", ")", ":", "\n", "        ", "self", ".", "_init_submodules_common", "(", ")", "\n", "self", ".", "embed_scale", "=", "1", "\n", "self", ".", "embed_positions", "=", "LearnedPositionalEmbedding", "(", "\n", "self", ".", "args", ".", "max_positions", ",", "self", ".", "args", ".", "embed_dim", ",", "self", ".", "padding_idx", "\n", ")", "\n", "self", ".", "emb_layer_norm_before", "=", "(", "\n", "ESM1bLayerNorm", "(", "self", ".", "args", ".", "embed_dim", ")", "if", "self", ".", "emb_layer_norm_before", "else", "None", "\n", ")", "\n", "self", ".", "emb_layer_norm_after", "=", "ESM1bLayerNorm", "(", "self", ".", "args", ".", "embed_dim", ")", "\n", "self", ".", "lm_head", "=", "RobertaLMHead", "(", "\n", "embed_dim", "=", "self", ".", "args", ".", "embed_dim", ",", "\n", "output_dim", "=", "self", ".", "alphabet_size", ",", "\n", "weight", "=", "self", ".", "embed_tokens", ".", "weight", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel._init_submodules_esm1": [[110, 118], ["model.ProteinBertModel._init_submodules_common", "math.sqrt", "modules.SinusoidalPositionalEmbedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel._init_submodules_common"], ["", "def", "_init_submodules_esm1", "(", "self", ")", ":", "\n", "        ", "self", ".", "_init_submodules_common", "(", ")", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "self", ".", "args", ".", "embed_dim", ")", "\n", "self", ".", "embed_positions", "=", "SinusoidalPositionalEmbedding", "(", "self", ".", "args", ".", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "self", ".", "embed_out", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "(", "self", ".", "alphabet_size", ",", "self", ".", "args", ".", "embed_dim", ")", ")", ")", "\n", "self", ".", "embed_out_bias", "=", "None", "\n", "if", "self", ".", "args", ".", "final_bias", ":", "\n", "            ", "self", ".", "embed_out_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "alphabet_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel.forward": [[119, 197], ["tokens.eq", "getattr", "set", "model.ProteinBertModel.transpose", "enumerate", "model.ProteinBertModel.embed_tokens", "model.ProteinBertModel.masked_fill_", "model.ProteinBertModel.embed_positions", "tokens.eq.any", "layer", "model.ProteinBertModel.emb_layer_norm_after", "model.ProteinBertModel.transpose", "model.ProteinBertModel.lm_head", "torch.linear", "torch.linear", "torch.linear", "model.ProteinBertModel.transpose", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "model.ProteinBertModel.emb_layer_norm_before", "model.ProteinBertModel.transpose", "attn_weights.append", "model.ProteinBertModel.contact_head", "attn.transpose", "tokens.eq.type_as", "attention_mask.unsqueeze", "attention_mask.unsqueeze", "tokens.eq.unsqueeze().type_as", "tokens.eq.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "tokens", ",", "repr_layers", "=", "[", "]", ",", "need_head_weights", "=", "False", ",", "return_contacts", "=", "False", ")", ":", "\n", "        ", "if", "return_contacts", ":", "\n", "            ", "need_head_weights", "=", "True", "\n", "\n", "", "assert", "tokens", ".", "ndim", "==", "2", "\n", "padding_mask", "=", "tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "# B, T", "\n", "\n", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "tokens", ")", "\n", "\n", "if", "getattr", "(", "self", ".", "args", ",", "\"token_dropout\"", ",", "False", ")", ":", "\n", "            ", "x", ".", "masked_fill_", "(", "(", "tokens", "==", "self", ".", "mask_idx", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "0.0", ")", "\n", "# x: B x T x C", "\n", "mask_ratio_train", "=", "0.15", "*", "0.8", "\n", "src_lengths", "=", "(", "~", "padding_mask", ")", ".", "sum", "(", "-", "1", ")", "\n", "mask_ratio_observed", "=", "(", "tokens", "==", "self", ".", "mask_idx", ")", ".", "sum", "(", "-", "1", ")", ".", "float", "(", ")", "/", "src_lengths", "\n", "x", "=", "x", "*", "(", "1", "-", "mask_ratio_train", ")", "/", "(", "1", "-", "mask_ratio_observed", ")", "[", ":", ",", "None", ",", "None", "]", "\n", "\n", "", "x", "=", "x", "+", "self", ".", "embed_positions", "(", "tokens", ")", "\n", "\n", "if", "self", ".", "model_version", "==", "\"ESM-1b\"", ":", "\n", "            ", "if", "self", ".", "emb_layer_norm_before", ":", "\n", "                ", "x", "=", "self", ".", "emb_layer_norm_before", "(", "x", ")", "\n", "", "if", "padding_mask", "is", "not", "None", ":", "\n", "                ", "x", "=", "x", "*", "(", "1", "-", "padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "type_as", "(", "x", ")", ")", "\n", "\n", "", "", "repr_layers", "=", "set", "(", "repr_layers", ")", "\n", "hidden_representations", "=", "{", "}", "\n", "if", "0", "in", "repr_layers", ":", "\n", "            ", "hidden_representations", "[", "0", "]", "=", "x", "\n", "\n", "", "if", "need_head_weights", ":", "\n", "            ", "attn_weights", "=", "[", "]", "\n", "\n", "# (B, T, E) => (T, B, E)", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "if", "not", "padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "padding_mask", "=", "None", "\n", "\n", "", "for", "layer_idx", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "x", ",", "attn", "=", "layer", "(", "\n", "x", ",", "self_attn_padding_mask", "=", "padding_mask", ",", "need_head_weights", "=", "need_head_weights", "\n", ")", "\n", "if", "(", "layer_idx", "+", "1", ")", "in", "repr_layers", ":", "\n", "                ", "hidden_representations", "[", "layer_idx", "+", "1", "]", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "need_head_weights", ":", "\n", "# (H, B, T, T) => (B, H, T, T)", "\n", "                ", "attn_weights", ".", "append", "(", "attn", ".", "transpose", "(", "1", ",", "0", ")", ")", "\n", "\n", "", "", "if", "self", ".", "model_version", "==", "\"ESM-1b\"", ":", "\n", "            ", "x", "=", "self", ".", "emb_layer_norm_after", "(", "x", ")", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "# (T, B, E) => (B, T, E)", "\n", "\n", "# last hidden representation should have layer norm applied", "\n", "if", "(", "layer_idx", "+", "1", ")", "in", "repr_layers", ":", "\n", "                ", "hidden_representations", "[", "layer_idx", "+", "1", "]", "=", "x", "\n", "", "x", "=", "self", ".", "lm_head", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_out", ",", "bias", "=", "self", ".", "embed_out_bias", ")", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "# (T, B, E) => (B, T, E)", "\n", "\n", "", "result", "=", "{", "\"logits\"", ":", "x", ",", "\"representations\"", ":", "hidden_representations", "}", "\n", "if", "need_head_weights", ":", "\n", "# attentions: B x L x H x T x T", "\n", "            ", "attentions", "=", "torch", ".", "stack", "(", "attn_weights", ",", "1", ")", "\n", "if", "self", ".", "model_version", "==", "\"ESM-1\"", ":", "\n", "# ESM-1 models have an additional null-token for attention, which we remove", "\n", "                ", "attentions", "=", "attentions", "[", "...", ",", ":", "-", "1", "]", "\n", "", "if", "padding_mask", "is", "not", "None", ":", "\n", "                ", "attention_mask", "=", "1", "-", "padding_mask", ".", "type_as", "(", "attentions", ")", "\n", "attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", "*", "attention_mask", ".", "unsqueeze", "(", "2", ")", "\n", "attentions", "=", "attentions", "*", "attention_mask", "[", ":", ",", "None", ",", "None", ",", ":", ",", ":", "]", "\n", "", "result", "[", "\"attentions\"", "]", "=", "attentions", "\n", "if", "return_contacts", ":", "\n", "                ", "contacts", "=", "self", ".", "contact_head", "(", "tokens", ",", "attentions", ")", "\n", "result", "[", "\"contacts\"", "]", "=", "contacts", "\n", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel.predict_contacts": [[198, 200], ["model.ProteinBertModel."], "methods", ["None"], ["", "def", "predict_contacts", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "self", "(", "tokens", ",", "return_contacts", "=", "True", ")", "[", "\"contacts\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.ProteinBertModel.num_layers": [[201, 204], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_layers", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "args", ".", "layers", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.add_args": [[207, 267], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "add_args", "(", "cls", ",", "parser", ")", ":", "\n", "# fmt: off", "\n", "        ", "parser", ".", "add_argument", "(", "\n", "\"--num_layers\"", ",", "\n", "default", "=", "12", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"number of layers\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--embed_dim\"", ",", "\n", "default", "=", "768", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"embedding dimension\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--logit_bias\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"whether to apply bias to logits\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--ffn_embed_dim\"", ",", "\n", "default", "=", "3072", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"embedding dimension for FFN\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--attention_heads\"", ",", "\n", "default", "=", "12", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"number of attention heads\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dropout\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout to apply.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--attention_dropout\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout to apply.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--activation_dropout\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout to apply.\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--max_tokens_per_msa\"", ",", "\n", "default", "=", "2", "**", "14", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "(", "\n", "\"Used during inference to batch attention computations in a single \"", "\n", "\"forward pass. This allows increased input sizes with less memory.\"", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__": [[272, 329], ["torch.Module.__init__", "len", "torch.Embedding", "torch.Embedding", "torch.Embedding", "getattr", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "modules.ContactPredictionHead", "modules.LearnedPositionalEmbedding", "modules.ESM1bLayerNorm", "modules.ESM1bLayerNorm", "modules.RobertaLMHead", "getattr", "torch.Parameter", "torch.Parameter", "torch.Parameter", "model.MSATransformer.register_parameter", "modules.AxialTransformerLayer", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "getattr", "range"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.__init__", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn", "home.repos.pwc.inspect_result.facebookresearch_esm.inverse_folding.gvp_modules.randn"], ["", "def", "__init__", "(", "self", ",", "args", ",", "alphabet", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "alphabet_size", "=", "len", "(", "alphabet", ")", "\n", "self", ".", "padding_idx", "=", "alphabet", ".", "padding_idx", "\n", "self", ".", "mask_idx", "=", "alphabet", ".", "mask_idx", "\n", "self", ".", "cls_idx", "=", "alphabet", ".", "cls_idx", "\n", "self", ".", "eos_idx", "=", "alphabet", ".", "eos_idx", "\n", "self", ".", "prepend_bos", "=", "alphabet", ".", "prepend_bos", "\n", "self", ".", "append_eos", "=", "alphabet", ".", "append_eos", "\n", "\n", "self", ".", "embed_tokens", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "alphabet_size", ",", "self", ".", "args", ".", "embed_dim", ",", "padding_idx", "=", "self", ".", "padding_idx", "\n", ")", "\n", "\n", "if", "getattr", "(", "self", ".", "args", ",", "\"embed_positions_msa\"", ",", "False", ")", ":", "\n", "            ", "emb_dim", "=", "getattr", "(", "self", ".", "args", ",", "\"embed_positions_msa_dim\"", ",", "self", ".", "args", ".", "embed_dim", ")", "\n", "self", ".", "msa_position_embedding", "=", "nn", ".", "Parameter", "(", "\n", "0.01", "*", "torch", ".", "randn", "(", "1", ",", "1024", ",", "1", ",", "emb_dim", ")", ",", "\n", "requires_grad", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "\"msa_position_embedding\"", ",", "None", ")", "\n", "\n", "", "self", ".", "dropout_module", "=", "nn", ".", "Dropout", "(", "self", ".", "args", ".", "dropout", ")", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "\n", "AxialTransformerLayer", "(", "\n", "self", ".", "args", ".", "embed_dim", ",", "\n", "self", ".", "args", ".", "ffn_embed_dim", ",", "\n", "self", ".", "args", ".", "attention_heads", ",", "\n", "self", ".", "args", ".", "dropout", ",", "\n", "self", ".", "args", ".", "attention_dropout", ",", "\n", "self", ".", "args", ".", "activation_dropout", ",", "\n", "getattr", "(", "self", ".", "args", ",", "\"max_tokens_per_msa\"", ",", "self", ".", "args", ".", "max_tokens", ")", ",", "\n", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "args", ".", "layers", ")", "\n", "]", "\n", ")", "\n", "\n", "self", ".", "contact_head", "=", "ContactPredictionHead", "(", "\n", "self", ".", "args", ".", "layers", "*", "self", ".", "args", ".", "attention_heads", ",", "\n", "self", ".", "prepend_bos", ",", "\n", "self", ".", "append_eos", ",", "\n", "eos_idx", "=", "self", ".", "eos_idx", ",", "\n", ")", "\n", "self", ".", "embed_positions", "=", "LearnedPositionalEmbedding", "(", "\n", "self", ".", "args", ".", "max_positions", ",", "\n", "self", ".", "args", ".", "embed_dim", ",", "\n", "self", ".", "padding_idx", ",", "\n", ")", "\n", "self", ".", "emb_layer_norm_before", "=", "ESM1bLayerNorm", "(", "self", ".", "args", ".", "embed_dim", ")", "\n", "self", ".", "emb_layer_norm_after", "=", "ESM1bLayerNorm", "(", "self", ".", "args", ".", "embed_dim", ")", "\n", "self", ".", "lm_head", "=", "RobertaLMHead", "(", "\n", "embed_dim", "=", "self", ".", "args", ".", "embed_dim", ",", "\n", "output_dim", "=", "self", ".", "alphabet_size", ",", "\n", "weight", "=", "self", ".", "embed_tokens", ".", "weight", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.forward": [[331, 406], ["tokens.size", "tokens.eq", "model.MSATransformer.embed_tokens", "model.MSATransformer.embed_positions().view", "model.MSATransformer.emb_layer_norm_before", "model.MSATransformer.dropout_module", "set", "layer.permute", "enumerate", "model.MSATransformer.emb_layer_norm_after", "layer.permute", "model.MSATransformer.lm_head", "tokens.eq.any", "layer.size", "layer", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "model.MSATransformer.embed_positions", "layer.size", "RuntimeError", "col_attn_weights.append", "row_attn_weights.append", "layer.permute", "model.MSATransformer.contact_head", "tokens.view", "tokens.eq.unsqueeze().type_as", "col_attn.permute", "row_attn.permute", "layer.size", "tokens.eq.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tokens", ",", "repr_layers", "=", "[", "]", ",", "need_head_weights", "=", "False", ",", "return_contacts", "=", "False", ")", ":", "\n", "        ", "if", "return_contacts", ":", "\n", "            ", "need_head_weights", "=", "True", "\n", "\n", "", "assert", "tokens", ".", "ndim", "==", "3", "\n", "batch_size", ",", "num_alignments", ",", "seqlen", "=", "tokens", ".", "size", "(", ")", "\n", "padding_mask", "=", "tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "# B, R, C", "\n", "if", "not", "padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "padding_mask", "=", "None", "\n", "\n", "", "x", "=", "self", ".", "embed_tokens", "(", "tokens", ")", "\n", "x", "+=", "self", ".", "embed_positions", "(", "tokens", ".", "view", "(", "batch_size", "*", "num_alignments", ",", "seqlen", ")", ")", ".", "view", "(", "x", ".", "size", "(", ")", ")", "\n", "if", "self", ".", "msa_position_embedding", "is", "not", "None", ":", "\n", "            ", "if", "x", ".", "size", "(", "1", ")", ">", "1024", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Using model with MSA position embedding trained on maximum MSA \"", "\n", "f\"depth of 1024, but received {x.size(1)} alignments.\"", "\n", ")", "\n", "", "x", "+=", "self", ".", "msa_position_embedding", "[", ":", ",", ":", "num_alignments", "]", "\n", "\n", "", "x", "=", "self", ".", "emb_layer_norm_before", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "dropout_module", "(", "x", ")", "\n", "\n", "if", "padding_mask", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "*", "(", "1", "-", "padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "type_as", "(", "x", ")", ")", "\n", "\n", "", "repr_layers", "=", "set", "(", "repr_layers", ")", "\n", "hidden_representations", "=", "{", "}", "\n", "if", "0", "in", "repr_layers", ":", "\n", "            ", "hidden_representations", "[", "0", "]", "=", "x", "\n", "\n", "", "if", "need_head_weights", ":", "\n", "            ", "row_attn_weights", "=", "[", "]", "\n", "col_attn_weights", "=", "[", "]", "\n", "\n", "# B x R x C x D -> R x C x B x D", "\n", "", "x", "=", "x", ".", "permute", "(", "1", ",", "2", ",", "0", ",", "3", ")", "\n", "\n", "for", "layer_idx", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "x", "=", "layer", "(", "\n", "x", ",", "\n", "self_attn_padding_mask", "=", "padding_mask", ",", "\n", "need_head_weights", "=", "need_head_weights", ",", "\n", ")", "\n", "if", "need_head_weights", ":", "\n", "                ", "x", ",", "col_attn", ",", "row_attn", "=", "x", "\n", "# H x C x B x R x R -> B x H x C x R x R", "\n", "col_attn_weights", ".", "append", "(", "col_attn", ".", "permute", "(", "2", ",", "0", ",", "1", ",", "3", ",", "4", ")", ")", "\n", "# H x B x C x C -> B x H x C x C", "\n", "row_attn_weights", ".", "append", "(", "row_attn", ".", "permute", "(", "1", ",", "0", ",", "2", ",", "3", ")", ")", "\n", "", "if", "(", "layer_idx", "+", "1", ")", "in", "repr_layers", ":", "\n", "                ", "hidden_representations", "[", "layer_idx", "+", "1", "]", "=", "x", ".", "permute", "(", "2", ",", "0", ",", "1", ",", "3", ")", "\n", "\n", "", "", "x", "=", "self", ".", "emb_layer_norm_after", "(", "x", ")", "\n", "x", "=", "x", ".", "permute", "(", "2", ",", "0", ",", "1", ",", "3", ")", "# R x C x B x D -> B x R x C x D", "\n", "\n", "# last hidden representation should have layer norm applied", "\n", "if", "(", "layer_idx", "+", "1", ")", "in", "repr_layers", ":", "\n", "            ", "hidden_representations", "[", "layer_idx", "+", "1", "]", "=", "x", "\n", "", "x", "=", "self", ".", "lm_head", "(", "x", ")", "\n", "\n", "result", "=", "{", "\"logits\"", ":", "x", ",", "\"representations\"", ":", "hidden_representations", "}", "\n", "if", "need_head_weights", ":", "\n", "# col_attentions: B x L x H x C x R x R", "\n", "            ", "col_attentions", "=", "torch", ".", "stack", "(", "col_attn_weights", ",", "1", ")", "\n", "# row_attentions: B x L x H x C x C", "\n", "row_attentions", "=", "torch", ".", "stack", "(", "row_attn_weights", ",", "1", ")", "\n", "result", "[", "\"col_attentions\"", "]", "=", "col_attentions", "\n", "result", "[", "\"row_attentions\"", "]", "=", "row_attentions", "\n", "if", "return_contacts", ":", "\n", "                ", "contacts", "=", "self", ".", "contact_head", "(", "tokens", ",", "row_attentions", ")", "\n", "result", "[", "\"contacts\"", "]", "=", "contacts", "\n", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.predict_contacts": [[407, 409], ["model.MSATransformer."], "methods", ["None"], ["", "def", "predict_contacts", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "self", "(", "tokens", ",", "return_contacts", "=", "True", ")", "[", "\"contacts\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.num_layers": [[410, 413], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_layers", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "args", ".", "layers", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_esm.esm.model.MSATransformer.max_tokens_per_msa_": [[414, 424], ["model.MSATransformer.modules", "isinstance"], "methods", ["None"], ["", "def", "max_tokens_per_msa_", "(", "self", ",", "value", ":", "int", ")", "->", "None", ":", "\n", "        ", "\"\"\"The MSA Transformer automatically batches attention computations when\n        gradients are disabled to allow you to pass in larger MSAs at test time than\n        you can fit in GPU memory. By default this occurs when more than 2^14 tokens\n        are passed in the input MSA. You can set this value to infinity to disable\n        this behavior.\n        \"\"\"", "\n", "for", "module", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "(", "RowSelfAttention", ",", "ColumnSelfAttention", ")", ")", ":", "\n", "                ", "module", ".", "max_tokens_per_msa", "=", "value", "\n", "", "", "", "", ""]]}