{"home.repos.pwc.inspect_result.jarfo_joint.None.score.get_parser": [[20, 33], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Command-line script for BLEU scoring.'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--sys'", ",", "default", "=", "'-'", ",", "help", "=", "'system output'", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--ref'", ",", "required", "=", "True", ",", "help", "=", "'references'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--order'", ",", "default", "=", "4", ",", "metavar", "=", "'N'", ",", "\n", "type", "=", "int", ",", "help", "=", "'consider ngrams up to this order'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore-case'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'case-insensitive scoring'", ")", "\n", "parser", ".", "add_argument", "(", "'--sacrebleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'score with sacrebleu'", ")", "\n", "# fmt: on", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.None.score.main": [[35, 75], ["score.get_parser", "get_parser.parse_args", "print", "os.path.exists", "fairseq.data.dictionary.Dictionary", "os.path.exists", "fd.readlines", "score.main.score"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.None.score.get_parser"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "assert", "args", ".", "sys", "==", "'-'", "or", "os", ".", "path", ".", "exists", "(", "args", ".", "sys", ")", ",", "\"System output file {} does not exist\"", ".", "format", "(", "args", ".", "sys", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "args", ".", "ref", ")", ",", "\"Reference file {} does not exist\"", ".", "format", "(", "args", ".", "ref", ")", "\n", "\n", "dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n", "\n", "def", "readlines", "(", "fd", ")", ":", "\n", "        ", "for", "line", "in", "fd", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "args", ".", "ignore_case", ":", "\n", "                ", "yield", "line", ".", "lower", "(", ")", "\n", "", "else", ":", "\n", "                ", "yield", "line", "\n", "\n", "", "", "", "if", "args", ".", "sacrebleu", ":", "\n", "        ", "import", "sacrebleu", "\n", "\n", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "print", "(", "sacrebleu", ".", "corpus_bleu", "(", "fdsys", ",", "[", "fdref", "]", ")", ")", "\n", "", "", "", "else", ":", "\n", "        ", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "scorer", "=", "bleu", ".", "Scorer", "(", "dict", ".", "pad", "(", ")", ",", "dict", ".", "eos", "(", ")", ",", "dict", ".", "unk", "(", ")", ")", "\n", "for", "sys_tok", ",", "ref_tok", "in", "zip", "(", "readlines", "(", "fdsys", ")", ",", "readlines", "(", "fdref", ")", ")", ":", "\n", "                    ", "sys_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "sys_tok", ",", "dict", ")", "\n", "ref_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "ref_tok", ",", "dict", ")", "\n", "scorer", ".", "add", "(", "ref_tok", ",", "sys_tok", ")", "\n", "", "print", "(", "scorer", ".", "result_string", "(", "args", ".", "order", ")", ")", "\n", "\n", "", "", "", "if", "args", ".", "sys", "==", "'-'", ":", "\n", "        ", "score", "(", "sys", ".", "stdin", ")", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "args", ".", "sys", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "score", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.__init__": [[26, 53], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Linear", "torch.nn.Linear", "protected_multihead_attention.ProtectedMultiheadAttention.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.Parameter", "torch.nn.Parameter", "protected_multihead_attention.ProtectedMultiheadAttention.register_parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.__init__", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.Linear", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.Linear", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.reset_parameters"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "num_heads", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "assert", "self", ".", "head_dim", "*", "num_heads", "==", "self", ".", "embed_dim", ",", "\"embed_dim must be divisible by num_heads\"", "\n", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "in_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ",", "embed_dim", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "in_proj_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'in_proj_bias'", ",", "None", ")", "\n", "", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "add_bias_kv", ":", "\n", "            ", "self", ".", "bias_k", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "bias_v", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias_k", "=", "self", ".", "bias_v", "=", "None", "\n", "\n", "", "self", ".", "add_zero_attn", "=", "add_zero_attn", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.prepare_for_onnx_export_": [[54, 56], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.reset_parameters": [[57, 67], ["torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "in_proj_weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "out_proj", ".", "weight", ")", "\n", "if", "self", ".", "in_proj_bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "in_proj_bias", ",", "0.", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "out_proj", ".", "bias", ",", "0.", ")", "\n", "", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_k", ")", "\n", "", "if", "self", ".", "bias_v", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.forward": [[68, 219], ["query.size", "protected_multihead_attention.ProtectedMultiheadAttention.contiguous().view().transpose", "torch.cat.size", "torch.cat.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.softmax().type_as", "torch.softmax().type_as", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "protected_multihead_attention.ProtectedMultiheadAttention.out_proj", "query.data_ptr", "key.data_ptr", "value.data_ptr", "key.data_ptr", "value.data_ptr", "list", "key.size", "value.size", "protected_multihead_attention.ProtectedMultiheadAttention._get_input_buffer", "protected_multihead_attention.ProtectedMultiheadAttention.in_proj_qkv", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "protected_multihead_attention.ProtectedMultiheadAttention._set_input_buffer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.transpose", "torch.cat.transpose", "list", "attn_mask.repeat.repeat.unsqueeze", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.view", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.view", "torch.isinf().all", "torch.isinf().all", "torch.isinf().all", "torch.isinf().all", "torch.isinf().all.any", "torch.isinf().all.any", "list", "attn.transpose().contiguous().view.transpose().contiguous().view.contiguous().view", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.view", "query.size", "protected_multihead_attention.ProtectedMultiheadAttention.in_proj_q", "protected_multihead_attention.ProtectedMultiheadAttention.in_proj_q", "protected_multihead_attention.ProtectedMultiheadAttention.in_proj_k", "protected_multihead_attention.ProtectedMultiheadAttention.in_proj_v", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "protected_multihead_attention.ProtectedMultiheadAttention.contiguous().view", "saved_state[].view", "saved_state[].view", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.size", "attn_mask.repeat.repeat.repeat", "torch.where().type_as", "torch.where().type_as", "torch.where().type_as", "torch.where().type_as", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill().type_as", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill().type_as", "torch.softmax", "torch.softmax", "attn.transpose().contiguous().view.transpose().contiguous().view.size", "attn.transpose().contiguous().view.transpose().contiguous().view.size", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.sum", "protected_multihead_attention.ProtectedMultiheadAttention.in_proj_kv", "protected_multihead_attention.ProtectedMultiheadAttention.bias_k.repeat", "protected_multihead_attention.ProtectedMultiheadAttention.bias_v.repeat", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.size", "torch.isinf", "torch.isinf", "torch.isinf", "torch.isinf", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float", "attn.transpose().contiguous().view.transpose().contiguous().view.contiguous", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "attn_mask.repeat.repeat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "protected_multihead_attention.ProtectedMultiheadAttention.contiguous", "attn_mask.repeat.repeat.new_zeros", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.where", "torch.where", "torch.where", "torch.where", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill", "attn_mask.repeat.repeat.size", "torch.cat.size", "torch.cat.size", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "attn_mask.repeat.repeat.size", "torch.cat.unsqueeze().unsqueeze", "torch.cat.unsqueeze().unsqueeze", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float", "torch.cat.unsqueeze().unsqueeze", "torch.cat.unsqueeze().unsqueeze", "float", "torch.isinf().all.unsqueeze", "torch.isinf().all.unsqueeze", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float", "torch.cat.size", "torch.cat.size", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "float", "torch.cat.unsqueeze", "torch.cat.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_qkv", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._set_input_buffer", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_k", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_v", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_kv"], ["", "", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "key_padding_mask", "=", "None", ",", "incremental_state", "=", "None", ",", "\n", "need_weights", "=", "True", ",", "static_kv", "=", "False", ",", "attn_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input shape: Time x Batch x Channel\n\n        Self-attention can be implemented by passing in the same arguments for\n        query, key and value. Timesteps can be masked by supplying a T x T mask in the\n        `attn_mask` argument. Padding elements can be excluded from\n        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n        batch x src_len, where padding elements are indicated by 1s.\n        \"\"\"", "\n", "\n", "qkv_same", "=", "query", ".", "data_ptr", "(", ")", "==", "key", ".", "data_ptr", "(", ")", "==", "value", ".", "data_ptr", "(", ")", "\n", "kv_same", "=", "key", ".", "data_ptr", "(", ")", "==", "value", ".", "data_ptr", "(", ")", "\n", "\n", "tgt_len", ",", "bsz", ",", "embed_dim", "=", "query", ".", "size", "(", ")", "\n", "assert", "embed_dim", "==", "self", ".", "embed_dim", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "embed_dim", "]", "\n", "assert", "key", ".", "size", "(", ")", "==", "value", ".", "size", "(", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "saved_state", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "'prev_key'", "in", "saved_state", ":", "\n", "# previous time steps are cached - no need to recompute", "\n", "# key and value if they are static", "\n", "                ", "if", "static_kv", ":", "\n", "                    ", "assert", "kv_same", "and", "not", "qkv_same", "\n", "key", "=", "value", "=", "None", "\n", "", "", "", "else", ":", "\n", "            ", "saved_state", "=", "None", "\n", "\n", "", "if", "qkv_same", ":", "\n", "# self-attention", "\n", "            ", "q", ",", "k", ",", "v", "=", "self", ".", "in_proj_qkv", "(", "query", ")", "\n", "", "elif", "kv_same", ":", "\n", "# encoder-decoder attention", "\n", "            ", "q", "=", "self", ".", "in_proj_q", "(", "query", ")", "\n", "if", "key", "is", "None", ":", "\n", "                ", "assert", "value", "is", "None", "\n", "k", "=", "v", "=", "None", "\n", "", "else", ":", "\n", "                ", "k", ",", "v", "=", "self", ".", "in_proj_kv", "(", "key", ")", "\n", "", "", "else", ":", "\n", "            ", "q", "=", "self", ".", "in_proj_q", "(", "query", ")", "\n", "k", "=", "self", ".", "in_proj_k", "(", "key", ")", "\n", "v", "=", "self", ".", "in_proj_v", "(", "value", ")", "\n", "", "q", "*=", "self", ".", "scaling", "\n", "\n", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "bias_v", "is", "not", "None", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "self", ".", "bias_k", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "self", ".", "bias_v", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "attn_mask", "=", "torch", ".", "cat", "(", "[", "attn_mask", ",", "attn_mask", ".", "new_zeros", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "                ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "key_padding_mask", ",", "key_padding_mask", ".", "new_zeros", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "", "q", "=", "q", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "k", "is", "not", "None", ":", "\n", "            ", "k", "=", "k", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "v", "is", "not", "None", ":", "\n", "            ", "v", "=", "v", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "if", "saved_state", "is", "not", "None", ":", "\n", "# saved states are stored with shape (bsz, num_heads, seq_len, head_dim)", "\n", "            ", "if", "'prev_key'", "in", "saved_state", ":", "\n", "                ", "prev_key", "=", "saved_state", "[", "'prev_key'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "k", "=", "prev_key", "\n", "", "else", ":", "\n", "                    ", "k", "=", "torch", ".", "cat", "(", "(", "prev_key", ",", "k", ")", ",", "dim", "=", "1", ")", "\n", "", "", "if", "'prev_value'", "in", "saved_state", ":", "\n", "                ", "prev_value", "=", "saved_state", "[", "'prev_value'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "v", "=", "prev_value", "\n", "", "else", ":", "\n", "                    ", "v", "=", "torch", ".", "cat", "(", "(", "prev_value", ",", "v", ")", ",", "dim", "=", "1", ")", "\n", "", "", "saved_state", "[", "'prev_key'", "]", "=", "k", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "saved_state", "[", "'prev_value'", "]", "=", "v", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "\n", "", "src_len", "=", "k", ".", "size", "(", "1", ")", "\n", "\n", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "assert", "key_padding_mask", ".", "size", "(", "0", ")", "==", "bsz", "\n", "assert", "key_padding_mask", ".", "size", "(", "1", ")", "==", "src_len", "\n", "\n", "", "if", "self", ".", "add_zero_attn", ":", "\n", "            ", "src_len", "+=", "1", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "k", ".", "new_zeros", "(", "(", "k", ".", "size", "(", "0", ")", ",", "1", ")", "+", "k", ".", "size", "(", ")", "[", "2", ":", "]", ")", "]", ",", "dim", "=", "1", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "v", ".", "new_zeros", "(", "(", "v", ".", "size", "(", "0", ")", ",", "1", ")", "+", "v", ".", "size", "(", ")", "[", "2", ":", "]", ")", "]", ",", "dim", "=", "1", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "attn_mask", "=", "torch", ".", "cat", "(", "[", "attn_mask", ",", "attn_mask", ".", "new_zeros", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "                ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "key_padding_mask", ",", "torch", ".", "zeros", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", ".", "type_as", "(", "key_padding_mask", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "", "attn_weights", "=", "torch", ".", "bmm", "(", "q", ",", "k", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "assert", "list", "(", "attn_weights", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", "]", "\n", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "attn_mask", ".", "unsqueeze", "(", "0", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "                ", "attn_mask", "=", "attn_mask", ".", "repeat", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "", "attn_weights", "+=", "attn_mask", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "# don't attend to padding symbols", "\n", "            ", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "                ", "attn_weights", "=", "torch", ".", "where", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "torch", ".", "Tensor", "(", "[", "float", "(", "\"-Inf\"", ")", "]", ")", ",", "\n", "attn_weights", ".", "float", "(", ")", "\n", ")", ".", "type_as", "(", "attn_weights", ")", "\n", "", "else", ":", "\n", "                ", "attn_weights", "=", "attn_weights", ".", "float", "(", ")", ".", "masked_fill", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "float", "(", "'-inf'", ")", ",", "\n", ")", ".", "type_as", "(", "attn_weights", ")", "# FP16 support: cast to float and back", "\n", "", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "all_inf", "=", "torch", ".", "isinf", "(", "attn_weights", ")", ".", "all", "(", "dim", "=", "-", "1", ")", "\n", "if", "all_inf", ".", "any", "(", ")", ":", "\n", "                ", "attn_weights", "=", "attn_weights", ".", "float", "(", ")", ".", "masked_fill", "(", "\n", "all_inf", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "0", ",", "\n", ")", ".", "type_as", "(", "attn_weights", ")", "# FP16 support: cast to float and back", "\n", "\n", "\n", "", "", "attn_weights", "=", "F", ".", "softmax", "(", "attn_weights", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ".", "type_as", "(", "attn_weights", ")", "\n", "attn_weights", "=", "F", ".", "dropout", "(", "attn_weights", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "attn", "=", "torch", ".", "bmm", "(", "attn_weights", ",", "v", ")", "\n", "assert", "list", "(", "attn", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "self", ".", "head_dim", "]", "\n", "if", "(", "self", ".", "onnx_trace", "and", "attn", ".", "size", "(", "1", ")", "==", "1", ")", ":", "\n", "# when ONNX tracing a single decoder step (sequence length == 1)", "\n", "# the transpose is a no-op copy before view, thus unnecessary", "\n", "            ", "attn", "=", "attn", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "", "attn", "=", "self", ".", "out_proj", "(", "attn", ")", "\n", "\n", "if", "need_weights", ":", "\n", "# average attention weights over heads", "\n", "            ", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "attn_weights", "=", "attn_weights", ".", "sum", "(", "dim", "=", "1", ")", "/", "self", ".", "num_heads", "\n", "", "else", ":", "\n", "            ", "attn_weights", "=", "None", "\n", "\n", "", "return", "attn", ",", "attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_qkv": [[220, 222], ["protected_multihead_attention.ProtectedMultiheadAttention._in_proj().chunk", "protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], ["", "def", "in_proj_qkv", "(", "self", ",", "query", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "query", ")", ".", "chunk", "(", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_kv": [[223, 225], ["protected_multihead_attention.ProtectedMultiheadAttention._in_proj().chunk", "protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], ["", "def", "in_proj_kv", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "key", ",", "start", "=", "self", ".", "embed_dim", ")", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_q": [[226, 228], ["protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], ["", "def", "in_proj_q", "(", "self", ",", "query", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "query", ",", "end", "=", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_k": [[229, 231], ["protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], ["", "def", "in_proj_k", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "key", ",", "start", "=", "self", ".", "embed_dim", ",", "end", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.in_proj_v": [[232, 234], ["protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._in_proj"], ["", "def", "in_proj_v", "(", "self", ",", "value", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "value", ",", "start", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._in_proj": [[235, 242], ["torch.linear", "torch.linear"], "methods", ["None"], ["", "def", "_in_proj", "(", "self", ",", "input", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "\n", "        ", "weight", "=", "self", ".", "in_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "weight", "=", "weight", "[", "start", ":", "end", ",", ":", "]", "\n", "if", "bias", "is", "not", "None", ":", "\n", "            ", "bias", "=", "bias", "[", "start", ":", "end", "]", "\n", "", "return", "F", ".", "linear", "(", "input", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention.reorder_incremental_state": [[243, 250], ["protected_multihead_attention.ProtectedMultiheadAttention._get_input_buffer", "protected_multihead_attention.ProtectedMultiheadAttention.keys", "protected_multihead_attention.ProtectedMultiheadAttention._set_input_buffer", "input_buffer[].index_select"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder buffered internal state (for incremental generation).\"\"\"", "\n", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "for", "k", "in", "input_buffer", ".", "keys", "(", ")", ":", "\n", "                ", "input_buffer", "[", "k", "]", "=", "input_buffer", "[", "k", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._get_input_buffer": [[251, 256], ["protected_multihead_attention.ProtectedMultiheadAttention.get_incremental_state"], "methods", ["None"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "self", ".", "get_incremental_state", "(", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", ")", "or", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._set_input_buffer": [[257, 262], ["protected_multihead_attention.ProtectedMultiheadAttention.set_incremental_state"], "methods", ["None"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "buffer", ")", ":", "\n", "        ", "return", "self", ".", "set_incremental_state", "(", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", "buffer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionModel.__init__": [[42, 44], ["fairseq.models.FairseqEncoderDecoderModel.__init__"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionModel.add_args": [[45, 83], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.options.eval_str_list"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the encoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--relu-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after ReLU in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--kernel-size-list'", ",", "type", "=", "lambda", "x", ":", "options", ".", "eval_str_list", "(", "x", ",", "int", ")", ",", "\n", "help", "=", "'list of kernel size (default: None)'", ")", "\n", "parser", ".", "add_argument", "(", "'--language-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use language embeddings'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionModel.build_model": [[84, 136], ["joint.base_architecture", "joint.JointAttentionEncoder", "joint.JointAttentionDecoder", "joint.JointAttentionModel", "hasattr", "hasattr", "len", "dictionary.pad", "joint.Embedding", "joint.JointAttentionModel.build_model.build_embedding"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.base_architecture", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.Embedding"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "1024", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "1024", "\n", "\n", "", "src_dict", ",", "tgt_dict", "=", "task", ".", "source_dictionary", ",", "task", ".", "target_dictionary", "\n", "\n", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ",", "path", "=", "None", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "# if provided, load from preloaded dictionaries", "\n", "if", "path", ":", "\n", "                ", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "path", ")", "\n", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "emb", ")", "\n", "", "return", "emb", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "src_dict", "!=", "tgt_dict", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings requires a joined dictionary'", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings not compatible with --decoder-embed-path'", ")", "\n", "", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'The joint_attention model requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "tgt_dict", ",", "args", ".", "decoder_embed_dim", ",", "args", ".", "decoder_embed_path", "\n", ")", "\n", "\n", "", "encoder", "=", "JointAttentionEncoder", "(", "args", ",", "src_dict", ",", "encoder_embed_tokens", ",", "left_pad", "=", "args", ".", "left_pad_source", ")", "\n", "decoder", "=", "JointAttentionDecoder", "(", "args", ",", "tgt_dict", ",", "decoder_embed_tokens", ",", "left_pad", "=", "args", ".", "left_pad_target", ")", "\n", "return", "JointAttentionModel", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionEncoder.__init__": [[149, 166], ["fairseq.models.FairseqEncoder.__init__", "math.sqrt", "joint.JointAttentionEncoder.register_buffer", "fairseq.modules.PositionalEmbedding", "joint.LanguageEmbedding", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.__init__", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.LanguageEmbedding"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "left_pad", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "\n", "embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_source_positions", "=", "args", ".", "max_source_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_source_positions", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", "learned", "=", "args", ".", "encoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "self", ".", "embed_language", "=", "LanguageEmbedding", "(", "embed_dim", ")", "if", "args", ".", "language_embeddings", "else", "None", "\n", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionEncoder.forward": [[167, 203], ["torch.dropout", "torch.dropout", "torch.dropout", "x.transpose.transpose.transpose", "src_tokens.eq", "joint.JointAttentionEncoder.embed_tokens", "joint.JointAttentionEncoder.embed_positions", "src_tokens.eq.any", "joint.JointAttentionEncoder.embed_language.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (torch.LongTensor): lengths of each source sentence of\n                shape `(batch)`\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): embedding output of shape\n                  `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        \"\"\"", "\n", "# embed tokens and positions", "\n", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "# language embedding", "\n", "", "if", "self", ".", "embed_language", "is", "not", "None", ":", "\n", "            ", "lang_emb", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_language", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", "\n", "x", "+=", "lang_emb", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# compute padding mask", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "", "return", "{", "\n", "'encoder_out'", ":", "x", ",", "# T x B x C", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionEncoder.reorder_encoder_out": [[205, 223], ["encoder_out[].index_select", "encoder_out[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"", "\n", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "=", "encoder_out", "[", "'encoder_out'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionEncoder.max_positions": [[224, 229], ["min"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_source_positions", "\n", "", "return", "min", "(", "self", ".", "max_source_positions", ",", "self", ".", "embed_positions", ".", "max_positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionDecoder.__init__": [[244, 285], ["fairseq.models.FairseqIncrementalDecoder.__init__", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "joint.JointAttentionDecoder.layers.extend", "joint.JointAttentionDecoder.register_buffer", "joint.Linear", "fairseq.modules.PositionalEmbedding", "joint.LanguageEmbedding", "joint.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "joint.LayerNorm", "joint.ProtectedTransformerDecoderLayer", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "range", "len"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.__init__", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.Linear", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.LanguageEmbedding", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.Linear", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "left_pad", "=", "False", ",", "final_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", "\n", "self", ".", "kernel_size_list", "=", "args", ".", "kernel_size_list", "\n", "\n", "input_embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "output_embed_dim", "=", "args", ".", "decoder_output_dim", "\n", "\n", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_target_positions", "=", "args", ".", "max_target_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "\n", "\n", "self", ".", "project_in_dim", "=", "Linear", "(", "input_embed_dim", ",", "embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "input_embed_dim", "else", "None", "\n", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_target_positions", ",", "embed_dim", ",", "padding_idx", ",", "\n", "learned", "=", "args", ".", "decoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "embed_language", "=", "LanguageEmbedding", "(", "embed_dim", ")", "if", "args", ".", "language_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "ProtectedTransformerDecoderLayer", "(", "args", ",", "no_encoder_attn", "=", "True", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "decoder_layers", ")", "\n", "]", ")", "\n", "\n", "self", ".", "project_out_dim", "=", "Linear", "(", "embed_dim", ",", "output_embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "output_embed_dim", "and", "not", "args", ".", "tie_adaptive_weights", "else", "None", "\n", "\n", "if", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "self", ".", "embed_out", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "len", "(", "dictionary", ")", ",", "output_embed_dim", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "embed_out", ",", "mean", "=", "0", ",", "std", "=", "output_embed_dim", "**", "-", "0.5", ")", "\n", "", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "normalize", "=", "args", ".", "decoder_normalize_before", "and", "final_norm", "\n", "if", "self", ".", "normalize", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionDecoder.forward": [[286, 411], ["prev_output_tokens.size", "torch.dropout", "torch.dropout", "torch.dropout", "torch.linear.transpose", "enumerate", "torch.linear.transpose", "joint.JointAttentionDecoder.embed_positions", "joint.JointAttentionDecoder.embed_tokens", "joint.JointAttentionDecoder.project_in_dim", "source_padding_mask.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "layer", "inner_states.append", "joint.JointAttentionDecoder.layer_norm", "joint.JointAttentionDecoder.project_out_dim", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "joint.JointAttentionDecoder.embed_language.view", "len", "joint.JointAttentionDecoder.local_mask", "joint.JointAttentionDecoder.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "layer", "inner_states.append", "source_padding_mask.size", "joint.JointAttentionDecoder.buffered_future_mask", "joint.JointAttentionDecoder.local_mask", "joint.JointAttentionDecoder.size", "source.size"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionDecoder.local_mask", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionDecoder.buffered_future_mask", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionDecoder.local_mask"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input (dict): with\n                prev_output_tokens (LongTensor): previous decoder outputs of shape\n                    `(batch, tgt_len)`, for input feeding/teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the last decoder layer's output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer's attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        \"\"\"", "\n", "tgt_len", "=", "prev_output_tokens", ".", "size", "(", "1", ")", "\n", "\n", "# embed positions", "\n", "positions", "=", "self", ".", "embed_positions", "(", "\n", "prev_output_tokens", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "None", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "if", "positions", "is", "not", "None", ":", "\n", "                ", "positions", "=", "positions", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# embed tokens and positions", "\n", "", "", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "\n", "if", "self", ".", "project_in_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_in_dim", "(", "x", ")", "\n", "\n", "", "if", "positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "positions", "\n", "\n", "# language embedding", "\n", "", "if", "self", ".", "embed_language", "is", "not", "None", ":", "\n", "            ", "lang_emb", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_language", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", "\n", "x", "+=", "lang_emb", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "attn", "=", "None", "\n", "inner_states", "=", "[", "x", "]", "\n", "source", "=", "encoder_out", "[", "'encoder_out'", "]", "\n", "process_source", "=", "incremental_state", "is", "None", "or", "len", "(", "incremental_state", ")", "==", "0", "\n", "\n", "# extended padding mask", "\n", "source_padding_mask", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", "\n", "if", "source_padding_mask", "is", "not", "None", ":", "\n", "            ", "target_padding_mask", "=", "source_padding_mask", ".", "new_zeros", "(", "(", "source_padding_mask", ".", "size", "(", "0", ")", ",", "tgt_len", ")", ")", "\n", "self_attn_padding_mask", "=", "torch", ".", "cat", "(", "(", "source_padding_mask", ",", "target_padding_mask", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "self_attn_padding_mask", "=", "None", "\n", "\n", "# transformer layers", "\n", "", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "\n", "            ", "if", "self", ".", "kernel_size_list", "is", "not", "None", ":", "\n", "                ", "target_mask", "=", "self", ".", "local_mask", "(", "x", ",", "self", ".", "kernel_size_list", "[", "i", "]", ",", "causal", "=", "True", ",", "tgt_len", "=", "tgt_len", ")", "\n", "", "elif", "incremental_state", "is", "None", ":", "\n", "                ", "target_mask", "=", "self", ".", "buffered_future_mask", "(", "x", ")", "\n", "", "else", ":", "\n", "                ", "target_mask", "=", "None", "\n", "\n", "", "if", "target_mask", "is", "not", "None", ":", "\n", "                ", "zero_mask", "=", "target_mask", ".", "new_zeros", "(", "(", "target_mask", ".", "size", "(", "0", ")", ",", "source", ".", "size", "(", "0", ")", ")", ")", "\n", "self_attn_mask", "=", "torch", ".", "cat", "(", "(", "zero_mask", ",", "target_mask", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "self_attn_mask", "=", "None", "\n", "\n", "", "state", "=", "incremental_state", "\n", "if", "process_source", ":", "\n", "                ", "if", "state", "is", "None", ":", "\n", "                    ", "state", "=", "{", "}", "\n", "", "if", "self", ".", "kernel_size_list", "is", "not", "None", ":", "\n", "                    ", "source_mask", "=", "self", ".", "local_mask", "(", "source", ",", "self", ".", "kernel_size_list", "[", "i", "]", ",", "causal", "=", "False", ")", "\n", "", "else", ":", "\n", "                    ", "source_mask", "=", "None", "\n", "", "source", ",", "attn", "=", "layer", "(", "\n", "source", ",", "\n", "None", ",", "\n", "None", ",", "\n", "state", ",", "\n", "self_attn_mask", "=", "source_mask", ",", "\n", "self_attn_padding_mask", "=", "source_padding_mask", "\n", ")", "\n", "inner_states", ".", "append", "(", "source", ")", "\n", "\n", "", "x", ",", "attn", "=", "layer", "(", "\n", "x", ",", "\n", "None", ",", "\n", "None", ",", "\n", "state", ",", "\n", "self_attn_mask", "=", "self_attn_mask", ",", "\n", "self_attn_padding_mask", "=", "self_attn_padding_mask", "\n", ")", "\n", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "normalize", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "if", "self", ".", "project_out_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_out_dim", "(", "x", ")", "\n", "\n", "# project back to size of vocabulary", "\n", "", "if", "self", ".", "share_input_output_embed", ":", "\n", "            ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_tokens", ".", "weight", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_out", ")", "\n", "\n", "", "pred", "=", "x", "\n", "info", "=", "{", "'attn'", ":", "attn", ",", "'inner_states'", ":", "inner_states", "}", "\n", "\n", "return", "pred", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionDecoder.max_positions": [[412, 417], ["min"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_target_positions", "\n", "", "return", "min", "(", "self", ".", "max_target_positions", ",", "self", ".", "embed_positions", ".", "max_positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionDecoder.buffered_future_mask": [[418, 427], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "joint.JointAttentionDecoder._future_mask.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "hasattr", "fairseq.utils.fill_with_neg_inf", "fairseq.utils.fill_with_neg_inf", "tensor.new", "joint.JointAttentionDecoder._future_mask.resize_"], "methods", ["None"], ["", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "\"\"\"Cached future mask.\"\"\"", "\n", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "#pylint: disable=access-member-before-definition, attribute-defined-outside-init", "\n", "if", "not", "hasattr", "(", "self", ",", "'_future_mask'", ")", "or", "self", ".", "_future_mask", "is", "None", "or", "self", ".", "_future_mask", ".", "device", "!=", "tensor", ".", "device", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "if", "self", ".", "_future_mask", ".", "size", "(", "0", ")", "<", "dim", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "self", ".", "_future_mask", ".", "resize_", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.JointAttentionDecoder.local_mask": [[428, 446], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "tensor.size", "fairseq.utils.fill_with_neg_inf", "fairseq.utils.fill_with_neg_inf", "fairseq.utils.fill_with_neg_inf", "tensor.new", "tensor.new", "tensor.new"], "methods", ["None"], ["", "def", "local_mask", "(", "self", ",", "tensor", ",", "kernel_size", ",", "causal", ",", "tgt_len", "=", "None", ")", ":", "\n", "        ", "\"\"\"Locality constraint mask.\"\"\"", "\n", "rows", "=", "tensor", ".", "size", "(", "0", ")", "\n", "cols", "=", "tensor", ".", "size", "(", "0", ")", "if", "tgt_len", "is", "None", "else", "tgt_len", "\n", "if", "causal", ":", "\n", "            ", "if", "rows", "==", "1", ":", "\n", "                ", "mask", "=", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "1", ",", "cols", ")", ")", "\n", "mask", "[", "0", ",", "-", "kernel_size", ":", "]", "=", "0", "\n", "return", "mask", "\n", "", "else", ":", "\n", "                ", "diag_u", ",", "diag_l", "=", "1", ",", "kernel_size", "\n", "", "", "else", ":", "\n", "            ", "diag_u", ",", "diag_l", "=", "(", "(", "kernel_size", "+", "1", ")", "//", "2", ",", "(", "kernel_size", "+", "1", ")", "//", "2", ")", "if", "kernel_size", "%", "2", "==", "1", "else", "(", "kernel_size", "//", "2", ",", "kernel_size", "//", "2", "+", "1", ")", "\n", "", "mask1", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "rows", ",", "cols", ")", ")", ",", "diag_u", ")", "\n", "mask2", "=", "torch", ".", "tril", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "rows", ",", "cols", ")", ")", ",", "-", "diag_l", ")", "\n", "\n", "return", "mask1", "+", "mask2", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.__init__": [[466, 496], ["torch.Module.__init__", "protected_multihead_attention.ProtectedMultiheadAttention", "joint.LayerNorm", "joint.Linear", "joint.Linear", "joint.LayerNorm", "protected_multihead_attention.ProtectedMultiheadAttention", "joint.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.__init__", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.LayerNorm", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.Linear", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.Linear", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.LayerNorm", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ",", "no_encoder_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "self", ".", "self_attn", "=", "ProtectedMultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "args", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "relu_dropout", "=", "args", ".", "relu_dropout", "\n", "self", ".", "normalize_before", "=", "args", ".", "decoder_normalize_before", "\n", "\n", "self", ".", "self_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "if", "no_encoder_attn", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "None", "\n", "self", ".", "encoder_attn_layer_norm", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "ProtectedMultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "args", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", ")", "\n", "self", ".", "encoder_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "", "self", ".", "fc1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "args", ".", "decoder_ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "Linear", "(", "args", ".", "decoder_ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.prepare_for_onnx_export_": [[497, 499], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.forward": [[500, 569], ["joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "joint.ProtectedTransformerDecoderLayer.self_attn", "torch.dropout", "torch.dropout", "torch.dropout", "joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "torch.relu", "torch.relu", "torch.relu", "torch.dropout", "torch.dropout", "torch.dropout", "joint.ProtectedTransformerDecoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "joint.ProtectedTransformerDecoderLayer.self_attn._set_input_buffer", "joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "joint.ProtectedTransformerDecoderLayer.encoder_attn", "torch.dropout", "torch.dropout", "torch.dropout", "joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "joint.ProtectedTransformerDecoderLayer.fc1", "joint.ProtectedTransformerDecoderLayer.self_attn._get_input_buffer", "joint.ProtectedTransformerDecoderLayer.encoder_attn._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._set_input_buffer", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._get_input_buffer", "home.repos.pwc.inspect_result.jarfo_joint.models.protected_multihead_attention.ProtectedMultiheadAttention._set_input_buffer"], ["", "def", "forward", "(", "self", ",", "x", ",", "encoder_out", ",", "encoder_padding_mask", ",", "incremental_state", ",", "\n", "prev_self_attn_state", "=", "None", ",", "prev_attn_state", "=", "None", ",", "self_attn_mask", "=", "None", ",", "\n", "self_attn_padding_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "self_attn_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_self_attn_state", "is", "not", "None", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_self_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "self_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "_", "=", "self", ".", "self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "key_padding_mask", "=", "self_attn_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "need_weights", "=", "False", ",", "\n", "attn_mask", "=", "self_attn_mask", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "self_attn_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "attn", "=", "None", "\n", "if", "self", ".", "encoder_attn", "is", "not", "None", ":", "\n", "            ", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_attn_state", "is", "not", "None", ":", "\n", "                ", "if", "incremental_state", "is", "None", ":", "\n", "                    ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "encoder_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "attn", "=", "self", ".", "encoder_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "encoder_out", ",", "\n", "value", "=", "encoder_out", ",", "\n", "key_padding_mask", "=", "encoder_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "static_kv", "=", "True", ",", "\n", "need_weights", "=", "(", "not", "self", ".", "training", "and", "self", ".", "need_attn", ")", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "relu_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "            ", "saved_state", "=", "self", ".", "self_attn", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "self_attn_state", "=", "saved_state", "[", "\"prev_key\"", "]", ",", "saved_state", "[", "\"prev_value\"", "]", "\n", "return", "x", ",", "attn", ",", "self_attn_state", "\n", "", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.maybe_layer_norm": [[570, 576], ["layer_norm"], "methods", ["None"], ["", "def", "maybe_layer_norm", "(", "self", ",", "layer_norm", ",", "x", ",", "before", "=", "False", ",", "after", "=", "False", ")", ":", "\n", "        ", "assert", "before", "^", "after", "\n", "if", "after", "^", "self", ".", "normalize_before", ":", "\n", "            ", "return", "layer_norm", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.ProtectedTransformerDecoderLayer.make_generation_fast_": [[577, 579], ["None"], "methods", ["None"], ["", "", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.Embedding": [[581, 586], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.LanguageEmbedding": [[588, 592], ["torch.Parameter", "torch.init.normal_", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "function", ["None"], ["", "def", "LanguageEmbedding", "(", "embedding_dim", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "embedding_dim", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.LayerNorm": [[594, 597], ["torch.LayerNorm"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.LayerNorm"], ["", "def", "LayerNorm", "(", "embedding_dim", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LayerNorm", "(", "embedding_dim", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.Linear": [[599, 605], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.base_architecture": [[607, 633], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "len"], "function", ["None"], ["", "@", "register_model_architecture", "(", "'joint_attention'", ",", "'joint_attention'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_learned_pos", "=", "getattr", "(", "args", ",", "'encoder_learned_pos'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "14", ")", "\n", "\n", "args", ".", "decoder_normalize_before", "=", "getattr", "(", "args", ",", "'decoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.", ")", "\n", "args", ".", "relu_dropout", "=", "getattr", "(", "args", ",", "'relu_dropout'", ",", "0.", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "True", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "False", ")", "\n", "args", ".", "no_token_positional_embeddings", "=", "getattr", "(", "args", ",", "'no_token_positional_embeddings'", ",", "False", ")", "\n", "args", ".", "kernel_size_list", "=", "getattr", "(", "args", ",", "'kernel_size_list'", ",", "None", ")", "\n", "assert", "args", ".", "kernel_size_list", "is", "None", "or", "len", "(", "args", ".", "kernel_size_list", ")", "==", "args", ".", "decoder_layers", ",", "\"kernel_size_list doesn't match decoder_layers\"", "\n", "args", ".", "language_embeddings", "=", "getattr", "(", "args", ",", "'language_embeddings'", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.joint_attention_iwslt_de_en": [[635, 644], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "joint.base_architecture"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.base_architecture"], ["", "@", "register_model_architecture", "(", "'joint_attention'", ",", "'joint_attention_iwslt_de_en'", ")", "\n", "def", "joint_attention_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.3", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.local_joint_attention_iwslt_de_en": [[646, 650], ["fairseq.models.register_model_architecture", "getattr", "joint.joint_attention_iwslt_de_en"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.joint_attention_iwslt_de_en"], ["", "@", "register_model_architecture", "(", "'joint_attention'", ",", "'local_joint_attention_iwslt_de_en'", ")", "\n", "def", "local_joint_attention_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "kernel_size_list", "=", "getattr", "(", "args", ",", "'kernel_size_list'", ",", "[", "3", ",", "5", ",", "7", ",", "9", ",", "11", ",", "13", ",", "15", ",", "17", ",", "21", ",", "25", ",", "29", ",", "33", ",", "37", ",", "41", "]", ")", "\n", "joint_attention_iwslt_de_en", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.joint_attention_wmt_en_de": [[652, 655], ["fairseq.models.register_model_architecture", "joint.base_architecture"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.base_architecture"], ["", "@", "register_model_architecture", "(", "'joint_attention'", ",", "'joint_attention_wmt_en_de'", ")", "\n", "def", "joint_attention_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.joint_attention_wmt_en_de_big": [[657, 666], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "joint.base_architecture"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.base_architecture"], ["", "@", "register_model_architecture", "(", "'joint_attention'", ",", "'joint_attention_wmt_en_de_big'", ")", "\n", "def", "joint_attention_wmt_en_de_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.3", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.local_joint_attention_wmt_en_de_big": [[668, 672], ["fairseq.models.register_model_architecture", "getattr", "joint.joint_attention_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.joint_attention_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'joint_attention'", ",", "'local_joint_attention_wmt_en_de_big'", ")", "\n", "def", "local_joint_attention_wmt_en_de_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "kernel_size_list", "=", "getattr", "(", "args", ",", "'kernel_size_list'", ",", "[", "7", ",", "15", ",", "31", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", "]", ")", "\n", "joint_attention_wmt_en_de_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.joint_attention_wmt_en_fr_big": [[674, 678], ["fairseq.models.register_model_architecture", "getattr", "joint.joint_attention_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.joint_attention_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'joint_attention'", ",", "'joint_attention_wmt_en_fr_big'", ")", "\n", "def", "joint_attention_wmt_en_fr_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "joint_attention_wmt_en_de_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jarfo_joint.models.joint.local_joint_attention_wmt_en_fr_big": [[680, 684], ["fairseq.models.register_model_architecture", "getattr", "joint.joint_attention_wmt_en_fr_big"], "function", ["home.repos.pwc.inspect_result.jarfo_joint.models.joint.joint_attention_wmt_en_fr_big"], ["", "@", "register_model_architecture", "(", "'joint_attention'", ",", "'local_joint_attention_wmt_en_fr_big'", ")", "\n", "def", "local_joint_attention_wmt_en_fr_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "kernel_size_list", "=", "getattr", "(", "args", ",", "'kernel_size_list'", ",", "[", "7", ",", "15", ",", "31", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", ",", "63", "]", ")", "\n", "joint_attention_wmt_en_fr_big", "(", "args", ")", "\n", "", ""]]}