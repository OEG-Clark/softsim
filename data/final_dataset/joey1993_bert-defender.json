{"home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.BertAdam.__init__": [[59, 78], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "\n", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "\n", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay", "=", "weight_decay", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BertAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.BertAdam.get_lr": [[79, 93], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.BertAdam.step": [[94, 163], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "", "", "", ""]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.warmup_cosine": [[23, 27], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "0.5", "*", "(", "1.0", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.warmup_constant": [[28, 32], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.warmup_linear": [[33, 37], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_classifier.main": [[41, 406], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "parser.parse_args.task_name.lower", "processor.get_labels", "tokenization.BertTokenizer.from_pretrained", "bert_model.BertForClassifier.from_pretrained", "torch.nn.DataParallel.to", "list", "os.path.join", "os.path.join", "bert_model.BertConfig", "bert_model.BertForClassifier", "torch.nn.DataParallel.load_state_dict", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "os.path.exists", "os.listdir", "ValueError", "os.path.exists", "os.makedirs", "ValueError", "processor.get_train_examples", "os.path.join", "torch.nn.DataParallel.half", "DDP", "torch.nn.DataParallel.named_parameters", "FusedAdam", "optimization.BertAdam", "bert_utils.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.nn.DataParallel.train", "tqdm.trange", "torch.load", "processor.get_dev_examples", "bert_utils.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.nn.DataParallel.eval", "tqdm.tqdm", "os.path.join", "bool", "int", "torch.nn.DataParallel", "FP16_Optimizer", "FP16_Optimizer", "len", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "int", "enumerate", "os.path.join", "os.path.join", "torch.save", "os.path.join", "len", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "label_ids.to().numpy.to", "model.detach().cpu().numpy", "label_ids.to().numpy.to().numpy", "bert_utils.accuracy", "tmp_eval_loss.mean().item", "input_ids.to.size", "open", "logger.info", "sorted", "torch.distributed.get_world_size", "ImportError", "ImportError", "tqdm.tqdm", "tuple", "torch.nn.DataParallel.", "loss.mean.item", "input_ids.to.size", "label_ids.to().numpy.to().numpy", "model.detach().cpu().numpy", "bert_utils.accuracy", "input_ids.to.size", "open", "writer.write", "sorted", "writer.write", "hasattr", "model_to_save.state_dict", "open", "f.write", "torch.distributed.get_rank", "torch.no_grad", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "result.keys", "logger.info", "writer.write", "torch.cuda.is_available", "any", "loss.mean.mean", "FP16_Optimizer.backward", "loss.mean.backward", "FP16_Optimizer.step", "FP16_Optimizer.zero_grad", "result.keys", "logger.info", "writer.write", "model_to_save.config.to_json_string", "str", "model.detach().cpu", "label_ids.to().numpy.to", "tmp_eval_loss.mean", "str", "len", "any", "t.to", "label_ids.to().numpy.to", "model.detach().cpu", "str", "optimization.warmup_linear", "str", "str", "model.detach", "str", "model.detach", "str"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_labels", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_train_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_dev_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.accuracy", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.accuracy", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.BertAdam.step", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.to_json_string", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.warmup_linear"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"", "\n", "\"bert-base-multilingual-cased, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--cache_dir\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_ip'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_port'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "server_ip", "and", "args", ".", "server_port", ":", "\n", "# Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script", "\n", "        ", "import", "ptvsd", "\n", "print", "(", "\"Waiting for debugger attach\"", ")", "\n", "ptvsd", ".", "enable_attach", "(", "address", "=", "(", "args", ".", "server_ip", ",", "args", ".", "server_port", ")", ",", "redirect_output", "=", "True", ")", "\n", "ptvsd", ".", "wait_for_attach", "(", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "train_batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "and", "args", ".", "do_train", ":", "\n", "        ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "num_labels", "=", "num_labels_task", "[", "task_name", "]", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_optimization_steps", "=", "None", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "num_train_epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "# Prepare model", "\n", "", "", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "os", ".", "path", ".", "join", "(", "PYTORCH_PRETRAINED_BERT_CACHE", ",", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "model", "=", "BertForClassifier", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "cache_dir", "=", "cache_dir", ",", "num_labels", "=", "num_labels", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "", "global_step", "=", "0", "\n", "nb_tr_steps", "=", "0", "\n", "tr_loss", "=", "0", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_features", "=", "convert_examples_to_features", "(", "\n", "train_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_optimization_steps", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "for", "ind", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "\n", "nb_tr_examples", ",", "nb_tr_steps", "=", "0", ",", "0", "\n", "eval_loss", ",", "eval_accuracy", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", ")", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", "=", "batch", "\n", "loss", ",", "logits", "=", "model", "(", "input_ids", ",", "input_mask", ",", "label_ids", ",", "segment_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "if", "args", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "# if args.fp16 is False, BertAdam is used that handles this automatically", "\n", "                        ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "num_train_optimization_steps", ",", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                            ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "# eval during training", "\n", "", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "tmp_eval_accuracy", "=", "accuracy", "(", "logits", ",", "label_ids", ")", "\n", "eval_accuracy", "+=", "tmp_eval_accuracy", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_accuracy", "=", "eval_accuracy", "/", "nb_eval_examples", "\n", "loss", "=", "tr_loss", "/", "nb_tr_steps", "if", "args", ".", "do_train", "else", "None", "\n", "result", "=", "{", "\n", "'eval_accuracy'", ":", "eval_accuracy", ",", "\n", "'c2_loss'", ":", "loss", "\n", "}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"train_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"a\"", ")", "as", "writer", ":", "\n", "#logger.info(\"***** Training results *****\")", "\n", "                ", "writer", ".", "write", "(", "\"epoch\"", "+", "str", "(", "ind", ")", "+", "'\\n'", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "", "writer", ".", "write", "(", "'\\n'", ")", "\n", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"epoch\"", "+", "str", "(", "ind", ")", "+", "\"_\"", "+", "WEIGHTS_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "with", "open", "(", "output_config_file", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "model_to_save", ".", "config", ".", "to_json_string", "(", ")", ")", "\n", "\n", "\n", "# Save a trained model and the associated configuration", "\n", "#model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self", "\n", "#output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)", "\n", "#torch.save(model_to_save.state_dict(), output_model_file)", "\n", "\n", "\n", "# Load a trained model and config that you have fine-tuned", "\n", "", "", "", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"epoch\"", "+", "str", "(", "ind", ")", "+", "\"_\"", "+", "WEIGHTS_NAME", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", "(", "output_config_file", ")", "\n", "model", "=", "BertForClassifier", "(", "config", ",", "num_labels", "=", "num_labels", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "output_model_file", ")", ")", "\n", "\n", "\n", "if", "args", ".", "do_eval", "and", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "        ", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ")", "\n", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", ",", "eval_accuracy", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "\n", "            ", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "tmp_eval_loss", ",", "_", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "label_ids", ")", "\n", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "tmp_eval_accuracy", "=", "accuracy", "(", "logits", ",", "label_ids", ")", "\n", "\n", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "eval_accuracy", "+=", "tmp_eval_accuracy", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "eval_accuracy", "=", "eval_accuracy", "/", "nb_eval_examples", "\n", "loss", "=", "tr_loss", "/", "nb_tr_steps", "if", "args", ".", "do_train", "else", "None", "\n", "result", "=", "{", "'eval_loss'", ":", "eval_loss", ",", "\n", "'eval_accuracy'", ":", "eval_accuracy", ",", "\n", "'global_step'", ":", "global_step", ",", "\n", "'loss'", ":", "loss", "}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_eval.main": [[39, 200], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "parser.parse_args.task_name.lower", "processor.get_labels", "tokenization.BertTokenizer.from_pretrained", "os.path.join", "bert_model.BertConfig", "bert_model.BertForSequenceClassification", "bert_model.BertForSequenceClassification.load_state_dict", "bert_model.BertForSequenceClassification.to", "processor.get_dev_examples", "bert_utils.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "bert_model.BertForSequenceClassification.eval", "tqdm.tqdm", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "torch.cuda.manual_seed_all", "ValueError", "os.path.join", "os.path.join", "torch.load", "len", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "label_ids.to().numpy.to", "model.detach().cpu().numpy", "label_ids.to().numpy.to().numpy", "bert_utils.accuracy", "model.mean().item", "input_ids.to.size", "open", "logger.info", "sorted", "bool", "torch.no_grad", "bert_model.BertForSequenceClassification.", "bert_model.BertForSequenceClassification.", "result.keys", "logger.info", "writer.write", "model.detach().cpu", "label_ids.to().numpy.to", "model.mean", "str", "torch.cuda.is_available", "str", "model.detach", "str"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_labels", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_dev_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.accuracy"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_file\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"", "\n", "\"bert-base-multilingual-cased, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_file\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output results will be written.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--weights\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The output results will be written.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "num_labels", "=", "num_labels_task", "[", "task_name", "]", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "if", "args", ".", "weights", "!=", "''", ":", "\n", "        ", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "\"epoch\"", "+", "str", "(", "args", ".", "weights", ")", "+", "\"_\"", "+", "WEIGHTS_NAME", ")", "\n", "", "else", ":", "\n", "        ", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "WEIGHTS_NAME", ")", "\n", "\n", "", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", "(", "output_config_file", ")", "\n", "model", "=", "BertForSequenceClassification", "(", "config", ",", "num_labels", "=", "num_labels", ")", "\n", "##model = BertForLMClassification(config, num_labels=num_labels)", "\n", "##model = BertForDClassifier(config, num_labels=num_labels)", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "output_model_file", ")", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_file", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ")", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", ",", "eval_accuracy", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "        ", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "tmp_eval_loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "label_ids", ")", "\n", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "tmp_eval_accuracy", "=", "accuracy", "(", "logits", ",", "label_ids", ")", "\n", "\n", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "eval_accuracy", "+=", "tmp_eval_accuracy", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "eval_accuracy", "=", "eval_accuracy", "/", "nb_eval_examples", "\n", "result", "=", "{", "'eval_loss'", ":", "eval_loss", ",", "\n", "'eval_accuracy'", ":", "eval_accuracy", "}", "\n", "\n", "output_eval_file", "=", "args", ".", "output_file", "\n", "with", "open", "(", "output_eval_file", ",", "\"a\"", ")", "as", "writer", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.url_to_filename": [[39, 55], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.filename_to_url": [[57, 81], ["os.path.join", "isinstance", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "io.open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.cached_path": [[83, 111], ["urlparse", "isinstance", "str", "isinstance", "str", "file_utils.get_from_cache", "os.path.exists", "EnvironmentError", "ValueError"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.split_s3_path": [[113, 124], ["urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.s3_request": [[126, 143], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.s3_etag": [[145, 152], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.s3_get": [[154, 160], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.http_get": [[162, 172], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["None"], ["", "def", "http_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.get_from_cache": [[174, 232], ["url.startswith", "file_utils.url_to_filename", "os.path.join", "isinstance", "str", "os.path.exists", "os.makedirs", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "io.open", "shutil.copyfileobj", "io.open", "json.dump"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.url_to_filename", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.s3_etag", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.s3_get", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.http_get"], ["", "def", "get_from_cache", "(", "url", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.read_set_from_file": [[234, 244], ["set", "io.open", "set.add", "line.rstrip"], "function", ["None"], ["", "def", "read_set_from_file", "(", "filename", ")", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.get_file_extension": [[246, 250], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ",", "dot", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "", "", ""]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_generator.main": [[24, 398], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "parser.parse_args.task_name.lower", "processor.get_labels", "tokenization.BertTokenizer.from_pretrained", "logger.info", "logger.info", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "os.path.exists", "os.makedirs", "ValueError", "bert_utils.load_vectors", "bert_utils.write_vocab_info", "bert_utils.load_vocab_info", "os.path.exists", "bert_utils.load_embeddings_and_save_index", "bert_utils.load_embedding_index", "processor.get_train_examples", "bert_utils.convert_examples_to_features_gnrt_train", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "bert_model.BertForNgramClassification.from_pretrained", "torch.nn.DataParallel.to", "list", "optimization.BertAdam", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.nn.DataParallel.train", "tqdm.trange", "processor.get_gnrt_dev_examples", "bert_utils.convert_examples_to_features_gnrt_eval", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "bool", "range", "int", "len", "os.path.join", "DDP", "torch.nn.DataParallel.named_parameters", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "int", "enumerate", "os.path.join", "os.path.join", "torch.save", "os.path.join", "len", "tqdm.trange", "tqdm.trange", "os.path.join", "os.path.join", "os.path.join", "bert_model.BertConfig", "bert_model.BertForNgramClassification", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel.to", "torch.nn.DataParallel.eval", "tqdm.tqdm", "torch.distributed.get_world_size", "torch.nn.DataParallel", "tqdm.tqdm", "tuple", "torch.nn.DataParallel.", "loss.mean.backward", "loss.mean.item", "open", "csv.writer.write", "sorted", "csv.writer.write", "hasattr", "model_to_save.state_dict", "open", "f.write", "torch.distributed.get_rank", "int", "int", "int", "open", "csv.writer", "csv.writer.writerow", "torch.load", "ngram_ids.to.to", "ngram_mask.to.to", "model.detach().cpu().numpy", "flaw_labels.to().numpy.to().numpy", "label_id.to().numpy.to().numpy", "token_ids.to().numpy.to().numpy", "ngram_mask.to.to().numpy", "torch.cuda.is_available", "ImportError", "loss.mean.mean", "optimization.BertAdam.step", "optimization.BertAdam.zero_grad", "result.keys", "logger.info", "csv.writer.write", "model_to_save.config.to_json_string", "torch.no_grad", "torch.nn.DataParallel.", "open", "range", "len", "any", "t.to", "str", "str", "str", "str", "model.detach().cpu", "flaw_labels.to().numpy.to", "label_id.to().numpy.to", "token_ids.to().numpy.to", "ngram_mask.to.to", "len", "bert_utils.look_up_words", "bert_utils.replace_token", "str", "csv.writer", "csv.writer.writerow", "any", "str", "str", "model.detach"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_labels", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_vectors", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.write_vocab_info", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_vocab_info", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_embeddings_and_save_index", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_embedding_index", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_train_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_gnrt_train", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_gnrt_dev_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_gnrt_eval", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.BertAdam.step", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.to_json_string", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.look_up_words", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.replace_token"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"", "\n", "\"bert-base-multilingual-cased, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_embedding_file\"", ",", "\n", "default", "=", "'emb/crawl-300d-2M.vec'", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input directory of word embeddings.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--index_path\"", ",", "\n", "default", "=", "'emb/p_index.bin'", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input directory of word embedding index.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_embedding_info\"", ",", "\n", "default", "=", "'emb/vocab_info.txt'", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input directory of word embedding info.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_file\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input directory of input data file.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--cache_dir\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_ngram_length\"", ",", "\n", "default", "=", "16", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total ngram sequence\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--embedding_size\"", ",", "\n", "default", "=", "300", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for embeddings.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_eval_epochs'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "0", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--single'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_ip'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_port'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "server_ip", "and", "args", ".", "server_port", ":", "\n", "# Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script", "\n", "        ", "import", "ptvsd", "\n", "print", "(", "\"Waiting for debugger attach\"", ")", "\n", "ptvsd", ".", "enable_attach", "(", "address", "=", "(", "args", ".", "server_ip", ",", "args", ".", "server_port", ")", ",", "redirect_output", "=", "True", ")", "\n", "ptvsd", ".", "wait_for_attach", "(", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "train_batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "num_labels", "=", "num_labels_task", "[", "task_name", "]", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "logger", ".", "info", "(", "\"loading embeddings ... \"", ")", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "emb_dict", ",", "emb_vec", ",", "vocab_list", ",", "emb_vocab_size", "=", "load_vectors", "(", "args", ".", "word_embedding_file", ")", "\n", "write_vocab_info", "(", "args", ".", "word_embedding_info", ",", "emb_vocab_size", ",", "vocab_list", ")", "\n", "", "if", "args", ".", "do_eval", ":", "\n", "        ", "emb_vocab_size", ",", "vocab_list", "=", "load_vocab_info", "(", "args", ".", "word_embedding_info", ")", "\n", "#emb_dict, emb_vec, vocab_list, emb_vocab_size = load_vectors(args.word_embedding_file)", "\n", "#write_vocab_info(args.word_embedding_info, emb_vocab_size, vocab_list)", "\n", "", "logger", ".", "info", "(", "\"loading p index ...\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "index_path", ")", ":", "\n", "        ", "p", "=", "load_embeddings_and_save_index", "(", "range", "(", "emb_vocab_size", ")", ",", "emb_vec", ",", "args", ".", "index_path", ")", "\n", "", "else", ":", "\n", "        ", "p", "=", "load_embedding_index", "(", "args", ".", "index_path", ",", "emb_vocab_size", ",", "num_dim", "=", "args", ".", "embedding_size", ")", "\n", "\n", "", "train_examples", "=", "None", "\n", "num_train_optimization_steps", "=", "None", "\n", "w2i", ",", "i2w", ",", "vocab_size", "=", "{", "}", ",", "{", "}", ",", "1", "\n", "if", "args", ".", "do_train", ":", "\n", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "num_train_epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "train_features", ",", "w2i", ",", "i2w", ",", "vocab_size", "=", "convert_examples_to_features_gnrt_train", "(", "train_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "args", ".", "max_ngram_length", ",", "tokenizer", ",", "emb_dict", ")", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num token vocab = %d\"", ",", "vocab_size", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_optimization_steps", ")", "\n", "all_ngram_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ngram_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ngram_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ngram_labels", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ngram_masks", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ngram_masks", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ngram_embeddings", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ngram_embeddings", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n", "# Prepare model", "\n", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "os", ".", "path", ".", "join", "(", "PYTORCH_PRETRAINED_BERT_CACHE", ",", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "model", "=", "BertForNgramClassification", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "embedding_size", "=", "args", ".", "embedding_size", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_length", ",", "\n", "max_ngram_length", "=", "args", ".", "max_ngram_length", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "try", ":", "\n", "                ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "                ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "global_step", "=", "0", "\n", "nb_tr_steps", "=", "0", "\n", "tr_loss", "=", "0", "\n", "\n", "#if args.do_train:", "\n", "\n", "train_data", "=", "TensorDataset", "(", "all_ngram_ids", ",", "all_ngram_labels", ",", "all_ngram_masks", ",", "all_ngram_embeddings", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "for", "ind", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "\n", "nb_tr_steps", "=", "0", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", ")", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "ngram_ids", ",", "ngram_labels", ",", "ngram_masks", ",", "ngram_embeddings", "=", "batch", "\n", "loss", "=", "model", "(", "ngram_ids", ",", "ngram_masks", ",", "ngram_embeddings", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "\n", "                    ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "", "", "loss", "=", "tr_loss", "/", "nb_tr_steps", "if", "args", ".", "do_train", "else", "None", "\n", "result", "=", "{", "\n", "'loss'", ":", "loss", ",", "\n", "}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"train_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"a\"", ")", "as", "writer", ":", "\n", "#logger.info(\"***** Training results *****\")", "\n", "                ", "writer", ".", "write", "(", "\"epoch\"", "+", "str", "(", "ind", ")", "+", "'\\n'", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "", "writer", ".", "write", "(", "'\\n'", ")", "\n", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"epoch\"", "+", "str", "(", "ind", ")", "+", "WEIGHTS_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "with", "open", "(", "output_config_file", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "model_to_save", ".", "config", ".", "to_json_string", "(", ")", ")", "\n", "\n", "# Load a trained model and config that you have fine-tuned", "\n", "", "", "", "if", "args", ".", "do_eval", "and", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "\n", "        ", "eval_examples", "=", "processor", ".", "get_gnrt_dev_examples", "(", "args", ".", "data_file", ")", "\n", "eval_features", ",", "w2i", ",", "i2w", ",", "vocab_size", "=", "convert_examples_to_features_gnrt_eval", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "args", ".", "max_ngram_length", ",", "tokenizer", ",", "w2i", ",", "i2w", ",", "vocab_size", ")", "\n", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num token vocab = %d\"", ",", "vocab_size", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "\n", "all_token_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "token_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "# all_flaw_labels: indexes of wrong words predicted by disc", "\n", "all_flaw_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "flaw_labels", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ngram_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ngram_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ngram_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ngram_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ngram_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ngram_labels", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_id", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "all_token_ids", ",", "all_ngram_ids", ",", "all_ngram_mask", ",", "all_ngram_labels", ",", "all_label_id", ",", "all_flaw_labels", ")", "\n", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "if", "args", ".", "single", ":", "\n", "            ", "eval_range", "=", "trange", "(", "int", "(", "args", ".", "num_eval_epochs", ")", ",", "int", "(", "args", ".", "num_eval_epochs", "+", "1", ")", ",", "desc", "=", "\"Epoch\"", ")", "\n", "", "else", ":", "\n", "            ", "eval_range", "=", "trange", "(", "int", "(", "args", ".", "num_eval_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", "\n", "\n", "", "for", "epoch", "in", "eval_range", ":", "\n", "\n", "            ", "output_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"epoch\"", "+", "str", "(", "epoch", ")", "+", "\"gnrt_outputs.tsv\"", ")", "\n", "with", "open", "(", "output_file", ",", "\"w\"", ")", "as", "csv_file", ":", "\n", "                ", "writer", "=", "csv", ".", "writer", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "writer", ".", "writerow", "(", "[", "\"sentence\"", ",", "\"label\"", "]", ")", "\n", "\n", "", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"epoch\"", "+", "str", "(", "epoch", ")", "+", "WEIGHTS_NAME", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", "(", "output_config_file", ")", "\n", "model", "=", "BertForNgramClassification", "(", "config", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "embedding_size", "=", "args", ".", "embedding_size", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_length", ",", "\n", "max_ngram_length", "=", "args", ".", "max_ngram_length", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "output_model_file", ")", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "for", "token_ids", ",", "ngram_ids", ",", "ngram_mask", ",", "ngram_labels", ",", "label_id", ",", "flaw_labels", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "\n", "                ", "ngram_ids", "=", "ngram_ids", ".", "to", "(", "device", ")", "\n", "ngram_mask", "=", "ngram_mask", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "logits", "=", "model", "(", "ngram_ids", ",", "ngram_mask", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "flaw_labels", "=", "flaw_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "label_id", "=", "label_id", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "token_ids", "=", "token_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "masks", "=", "ngram_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "\n", "with", "open", "(", "output_file", ",", "\"a\"", ")", "as", "csv_file", ":", "\n", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "label_id", ")", ")", ":", "\n", "\n", "                        ", "correct_tokens", "=", "look_up_words", "(", "logits", "[", "i", "]", ",", "masks", "[", "i", "]", ",", "vocab_list", ",", "p", ")", "\n", "token_new", "=", "replace_token", "(", "token_ids", "[", "i", "]", ",", "flaw_labels", "[", "i", "]", ",", "correct_tokens", ",", "i2w", ")", "\n", "token_new", "=", "' '", ".", "join", "(", "token_new", ")", "\n", "label", "=", "str", "(", "label_id", "[", "i", "]", ")", "\n", "writer", "=", "csv", ".", "writer", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "writer", ".", "writerow", "(", "[", "token_new", ",", "label", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_discriminator.main": [[25, 472], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "parser.parse_args.task_name.lower", "processor.get_labels", "tokenization.BertTokenizer.from_pretrained", "bert_model.BertForDiscriminator.from_pretrained", "bert_model.BertForDiscriminator.to", "list", "optimization.BertAdam", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "os.path.exists", "os.makedirs", "ValueError", "processor.get_train_examples", "bert_utils.convert_examples_to_features_disc_train", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "logger.info", "bert_utils.load_vectors", "os.path.join", "DDP", "bert_model.BertForDiscriminator.named_parameters", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "bert_model.BertForDiscriminator.train", "tqdm.trange", "processor.get_disc_dev_examples", "bert_utils.convert_examples_to_features_disc_eval", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "bool", "int", "len", "os.path.exists", "bert_utils.write_vocab_info", "bert_utils.load_embeddings_and_save_index", "bert_utils.load_embedding_index", "torch.nn.DataParallel", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "int", "enumerate", "os.path.join", "os.path.join", "torch.save", "os.path.join", "len", "tqdm.trange", "tqdm.trange", "os.path.join", "os.path.join", "os.path.join", "bert_model.BertConfig", "bert_model.BertForDiscriminator", "bert_model.BertForDiscriminator.load_state_dict", "bert_model.BertForDiscriminator.to", "bert_model.BertForDiscriminator.eval", "tqdm.tqdm", "bert_utils.f1_2d", "os.path.join", "torch.distributed.get_world_size", "range", "ImportError", "tqdm.tqdm", "tuple", "tokens.to().numpy.to().numpy", "bert_utils.convert_examples_to_features_flaw", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "bert_model.BertForDiscriminator.", "model.detach().cpu().numpy", "loss.mean.backward", "loss.mean.item", "flaw_ids.to().numpy.size", "flaw_labels.to.to().numpy", "bert_utils.f1_3d", "flaw_eval_f1.append", "flaw_eval_recall.append", "flaw_eval_precision.append", "flaw_ids.to().numpy.size", "sum", "len", "sum", "len", "sum", "len", "open", "csv.writer.write", "sorted", "csv.writer.write", "hasattr", "model_to_save.state_dict", "open", "f.write", "torch.distributed.get_rank", "int", "int", "int", "open", "csv.writer", "csv.writer.writerow", "torch.load", "token_ids.to().numpy.to", "input_ids.to.to", "input_mask.to.to", "flaw_labels.to.to", "flaw_ids.to().numpy.to", "model.detach().cpu().numpy", "torch.argmax.detach().cpu().numpy", "flaw_ids.to().numpy.to().numpy", "label_id.to().numpy.to().numpy", "chunks.to().numpy.to().numpy", "token_ids.to().numpy.to().numpy", "bert_utils.logit_converter", "range", "bert_utils.accuracy_2d", "tmp_eval_loss.mean().item", "input_ids.to.size", "open", "logger.info", "sorted", "torch.cuda.is_available", "any", "loss.mean.mean", "optimization.BertAdam.step", "optimization.BertAdam.zero_grad", "result.keys", "logger.info", "csv.writer.write", "model_to_save.config.to_json_string", "torch.no_grad", "bert_model.BertForDiscriminator.", "bert_model.BertForDiscriminator.", "torch.argmax", "len", "range", "true_logits.append", "open", "range", "result.keys", "logger.info", "csv.writer.write", "len", "any", "t.to", "tokens.to().numpy.to", "torch.tensor", "torch.tensor", "torch.tensor", "model.detach().cpu", "flaw_labels.to.to", "str", "str", "str", "str", "model.detach().cpu", "torch.argmax.detach().cpu", "flaw_ids.to().numpy.to", "label_id.to().numpy.to", "chunks.to().numpy.to", "token_ids.to().numpy.to", "len", "len", "tmp_eval_loss.mean", "len", "str", "csv.writer", "csv.writer.writerow", "str", "str", "len", "model.detach", "str", "model.detach", "torch.argmax.detach", "str", "str", "enumerate"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_labels", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_train_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_disc_train", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_vectors", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_disc_dev_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_disc_eval", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.write_vocab_info", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_embeddings_and_save_index", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_embedding_index", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.f1_2d", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_flaw", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.f1_3d", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.logit_converter", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.accuracy_2d", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.optimization.BertAdam.step", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.to_json_string"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"", "\n", "\"bert-base-multilingual-cased, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_embedding_file\"", ",", "\n", "default", "=", "'./emb/crawl-300d-2M.vec'", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input directory of word embeddings.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--index_path\"", ",", "\n", "default", "=", "'./emb/p_index.bin'", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input directory of word embedding index.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_embedding_info\"", ",", "\n", "default", "=", "'./emb/vocab_info.txt'", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input directory of word embedding info.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_file\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input directory of input data file.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--cache_dir\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_ngram_length\"", ",", "\n", "default", "=", "16", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total ngram sequence\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--embedding_size\"", ",", "\n", "default", "=", "300", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for embeddings.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_eval_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of eval epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--single'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether only evaluate a single epoch\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_ip'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_port'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "server_ip", "and", "args", ".", "server_port", ":", "\n", "# Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script", "\n", "        ", "import", "ptvsd", "\n", "print", "(", "\"Waiting for debugger attach\"", ")", "\n", "ptvsd", ".", "enable_attach", "(", "address", "=", "(", "args", ".", "server_ip", ",", "args", ".", "server_port", ")", ",", "redirect_output", "=", "True", ")", "\n", "ptvsd", ".", "wait_for_attach", "(", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "train_batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "num_labels", "=", "num_labels_task", "[", "task_name", "]", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_optimization_steps", "=", "None", "\n", "w2i", ",", "i2w", ",", "vocab_size", "=", "{", "}", ",", "{", "}", ",", "1", "\n", "if", "args", ".", "do_train", ":", "\n", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "num_train_epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "train_features", ",", "w2i", ",", "i2w", ",", "vocab_size", "=", "convert_examples_to_features_disc_train", "(", "train_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num token vocab = %d\"", ",", "vocab_size", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_optimization_steps", ")", "\n", "all_tokens", "=", "torch", ".", "tensor", "(", "[", "f", ".", "token_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_id", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "# load embeddings", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading word embeddings ... \"", ")", "\n", "emb_dict", ",", "emb_vec", ",", "vocab_list", ",", "emb_vocab_size", "=", "load_vectors", "(", "args", ".", "word_embedding_file", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "index_path", ")", ":", "\n", "\n", "            ", "write_vocab_info", "(", "args", ".", "word_embedding_info", ",", "emb_vocab_size", ",", "vocab_list", ")", "\n", "p", "=", "load_embeddings_and_save_index", "(", "range", "(", "emb_vocab_size", ")", ",", "emb_vec", ",", "args", ".", "index_path", ")", "\n", "", "else", ":", "\n", "#emb_vocab_size, vocab_list = load_vocab_info(args.word_embedding_info)", "\n", "            ", "p", "=", "load_embedding_index", "(", "args", ".", "index_path", ",", "emb_vocab_size", ",", "num_dim", "=", "args", ".", "embedding_size", ")", "\n", "#emb_dict, emb_vec, vocab_list, emb_vocab_size, p = None, None, None, None, None", "\n", "\n", "# Prepare model", "\n", "", "", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "os", ".", "path", ".", "join", "(", "PYTORCH_PRETRAINED_BERT_CACHE", ",", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "model", "=", "BertForDiscriminator", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "cache_dir", "=", "cache_dir", ",", "num_labels", "=", "num_labels", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "global_step", "=", "0", "\n", "nb_tr_steps", "=", "1", "\n", "tr_loss", "=", "0", "\n", "if", "args", ".", "do_train", ":", "\n", "\n", "        ", "train_data", "=", "TensorDataset", "(", "all_tokens", ",", "all_label_id", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "for", "ind", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "\n", "nb_tr_examples", ",", "nb_tr_steps", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "flaw_eval_f1", "=", "[", "]", "\n", "flaw_eval_recall", "=", "[", "]", "\n", "flaw_eval_precision", "=", "[", "]", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", ")", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "tokens", ",", "_", "=", "batch", "#, label_id, ngram_ids, ngram_labels, ngram_masks", "\n", "\n", "# module1: learn a discriminator", "\n", "tokens", "=", "tokens", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "train_features", "=", "convert_examples_to_features_flaw", "(", "tokens", ",", "\n", "args", ".", "max_seq_length", ",", "\n", "args", ".", "max_ngram_length", ",", "\n", "tokenizer", ",", "\n", "i2w", ",", "\n", "emb_dict", ",", "\n", "p", ",", "\n", "vocab_list", ")", "\n", "\n", "flaw_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "flaw_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "device", ")", "# [1, 1, 1, 1, 0,0,0,0]", "\n", "flaw_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "flaw_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "device", ")", "# [12,25,37,54,0,0,0,0]", "\n", "flaw_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "flaw_labels", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "device", ")", "# [0, 1, 1, 1, 0,0,0,0]", "\n", "\n", "loss", ",", "logits", "=", "model", "(", "flaw_ids", ",", "flaw_mask", ",", "flaw_labels", ")", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "\n", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "nb_tr_examples", "+=", "flaw_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "\n", "                    ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "# eval during training", "\n", "", "flaw_labels", "=", "flaw_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "flaw_tmp_eval_f1", ",", "flaw_tmp_eval_recall", ",", "flaw_tmp_eval_precision", "=", "f1_3d", "(", "logits", ",", "flaw_labels", ")", "\n", "flaw_eval_f1", ".", "append", "(", "flaw_tmp_eval_f1", ")", "\n", "flaw_eval_recall", ".", "append", "(", "flaw_tmp_eval_recall", ")", "\n", "flaw_eval_precision", ".", "append", "(", "flaw_tmp_eval_precision", ")", "\n", "\n", "nb_eval_examples", "+=", "flaw_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "flaw_f1", "=", "sum", "(", "flaw_eval_f1", ")", "/", "len", "(", "flaw_eval_f1", ")", "\n", "flaw_recall", "=", "sum", "(", "flaw_eval_recall", ")", "/", "len", "(", "flaw_eval_recall", ")", "\n", "flaw_precision", "=", "sum", "(", "flaw_eval_precision", ")", "/", "len", "(", "flaw_eval_precision", ")", "\n", "loss", "=", "tr_loss", "/", "nb_tr_steps", "if", "args", ".", "do_train", "else", "None", "\n", "result", "=", "{", "\n", "'flaw_f1'", ":", "flaw_f1", ",", "\n", "\"flaw_recall\"", ":", "flaw_recall", ",", "\n", "\"flaw_precision\"", ":", "flaw_precision", ",", "\n", "'loss'", ":", "loss", ",", "\n", "}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"train_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"a\"", ")", "as", "writer", ":", "\n", "#logger.info(\"***** Training results *****\")", "\n", "                ", "writer", ".", "write", "(", "\"epoch\"", "+", "str", "(", "ind", ")", "+", "'\\n'", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "", "writer", ".", "write", "(", "'\\n'", ")", "\n", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"epoch\"", "+", "str", "(", "ind", ")", "+", "WEIGHTS_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "with", "open", "(", "output_config_file", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "model_to_save", ".", "config", ".", "to_json_string", "(", ")", ")", "\n", "\n", "", "", "", "if", "args", ".", "do_eval", "and", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "\n", "        ", "eval_examples", "=", "processor", ".", "get_disc_dev_examples", "(", "args", ".", "data_file", ")", "\n", "eval_features", ",", "w2i", ",", "i2w", ",", "vocab_size", "=", "convert_examples_to_features_disc_eval", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ",", "w2i", ",", "i2w", ",", "vocab_size", ")", "\n", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num token vocab = %d\"", ",", "vocab_size", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "\n", "all_token_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "token_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_flaw_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "flaw_labels", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_flaw_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "flaw_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_id", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_chunks", "=", "torch", ".", "tensor", "(", "[", "f", ".", "chunks", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "all_token_ids", ",", "all_input_ids", ",", "all_input_mask", ",", "all_flaw_ids", ",", "all_flaw_labels", ",", "all_label_id", ",", "all_chunks", ")", "\n", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "# Load a trained model and config that you have fine-tuned", "\n", "if", "args", ".", "single", ":", "\n", "            ", "eval_range", "=", "trange", "(", "int", "(", "args", ".", "num_eval_epochs", ")", ",", "int", "(", "args", ".", "num_eval_epochs", "+", "1", ")", ",", "desc", "=", "\"Epoch\"", ")", "\n", "", "else", ":", "\n", "            ", "eval_range", "=", "trange", "(", "int", "(", "args", ".", "num_eval_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", "\n", "\n", "", "for", "epoch", "in", "eval_range", ":", "\n", "\n", "            ", "output_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"epoch\"", "+", "str", "(", "epoch", ")", "+", "\"disc_outputs.tsv\"", ")", "\n", "with", "open", "(", "output_file", ",", "\"w\"", ")", "as", "csv_file", ":", "\n", "                ", "writer", "=", "csv", ".", "writer", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "writer", ".", "writerow", "(", "[", "\"sentence\"", ",", "\"label\"", ",", "\"ids\"", "]", ")", "\n", "\n", "", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"epoch\"", "+", "str", "(", "epoch", ")", "+", "WEIGHTS_NAME", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "\n", "config", "=", "BertConfig", "(", "output_config_file", ")", "\n", "model", "=", "BertForDiscriminator", "(", "config", ",", "num_labels", "=", "num_labels", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "output_model_file", ")", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "predictions", ",", "truths", "=", "[", "]", ",", "[", "]", "\n", "eval_loss", ",", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", ",", "0", "\n", "eval_accuracy", "=", "0", "\n", "\n", "for", "token_ids", ",", "input_ids", ",", "input_mask", ",", "flaw_ids", ",", "flaw_labels", ",", "label_id", ",", "chunks", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "\n", "                ", "token_ids", "=", "token_ids", ".", "to", "(", "device", ")", "\n", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "flaw_labels", "=", "flaw_labels", ".", "to", "(", "device", ")", "\n", "flaw_ids", "=", "flaw_ids", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "tmp_eval_loss", ",", "_", "=", "model", "(", "input_ids", ",", "input_mask", ",", "flaw_labels", ")", "\n", "logits", "=", "model", "(", "input_ids", ",", "input_mask", ")", "\n", "flaw_logits", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "2", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "flaw_logits", "=", "flaw_logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "flaw_ids", "=", "flaw_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "label_id", "=", "label_id", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "chunks", "=", "chunks", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "token_ids", "=", "token_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "flaw_logits", "=", "logit_converter", "(", "flaw_logits", ",", "chunks", ")", "# each word only has one '1'", "\n", "true_logits", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "flaw_ids", ")", ")", ":", "\n", "                    ", "tmp", "=", "[", "0", "]", "*", "len", "(", "flaw_logits", "[", "i", "]", ")", "\n", "for", "j", "in", "range", "(", "len", "(", "flaw_ids", "[", "0", "]", ")", ")", ":", "\n", "                        ", "if", "flaw_ids", "[", "i", "]", "[", "j", "]", "==", "0", ":", "break", "\n", "if", "flaw_ids", "[", "i", "]", "[", "j", "]", ">=", "len", "(", "tmp", ")", ":", "continue", "\n", "tmp", "[", "flaw_ids", "[", "i", "]", "[", "j", "]", "]", "=", "1", "\n", "\n", "", "true_logits", ".", "append", "(", "tmp", ")", "\n", "\n", "", "tmp_eval_accuracy", "=", "accuracy_2d", "(", "flaw_logits", ",", "true_logits", ")", "\n", "eval_accuracy", "+=", "tmp_eval_accuracy", "\n", "\n", "predictions", "+=", "true_logits", "\n", "truths", "+=", "flaw_logits", "\n", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "with", "open", "(", "output_file", ",", "\"a\"", ")", "as", "csv_file", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "label_id", ")", ")", ":", "\n", "                        ", "token", "=", "' '", ".", "join", "(", "[", "i2w", "[", "x", "]", "for", "x", "in", "token_ids", "[", "i", "]", "if", "x", "!=", "0", "]", ")", "\n", "flaw_logit", "=", "flaw_logits", "[", "i", "]", "\n", "label", "=", "str", "(", "label_id", "[", "i", "]", ")", "\n", "logit", "=", "','", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", ",", "x", "in", "enumerate", "(", "flaw_logit", ")", "if", "x", "==", "1", "]", ")", "\n", "logit", "=", "'-1'", "if", "logit", "==", "''", "else", "logit", "\n", "writer", "=", "csv", ".", "writer", "(", "csv_file", ",", "delimiter", "=", "'\\t'", ")", "\n", "writer", ".", "writerow", "(", "[", "token", ",", "label", ",", "logit", "]", ")", "\n", "\n", "", "", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "eval_accuracy", "=", "eval_accuracy", "/", "nb_eval_steps", "\n", "eval_f1_score", ",", "eval_recall_score", ",", "eval_precision_score", "=", "f1_2d", "(", "truths", ",", "predictions", ")", "\n", "loss", "=", "tr_loss", "/", "nb_tr_steps", "if", "args", ".", "do_train", "else", "None", "\n", "result", "=", "{", "'eval_loss'", ":", "eval_loss", ",", "\n", "'eval_f1'", ":", "eval_f1_score", ",", "\n", "'eval_recall'", ":", "eval_recall_score", ",", "\n", "'eval_precision'", ":", "eval_precision_score", ",", "\n", "'eval_acc'", ":", "eval_accuracy", "}", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"a\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.__init__": [[77, 90], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ",", "\n", "never_split", "=", "never_split", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.tokenize": [[91, 97], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.convert_tokens_to_ids": [[98, 113], ["len", "ValueError", "ids.append", "ids.append", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "try", ":", "\n", "                ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "except", ":", "\n", "                ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "\"[UNK]\"", "]", ")", "\n", "", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.convert_ids_to_tokens": [[114, 120], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.from_pretrained": [[121, 158], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.file_utils.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name_or_path", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer.__init__": [[163, 173], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "do_lower_case", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer.tokenize": [[174, 194], ["tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._run_strip_accents": [[195, 205], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._run_split_on_punc": [[206, 227], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._tokenize_chinese_chars": [[228, 240], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._is_chinese_char": [[241, 262], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BasicTokenizer._clean_text": [[263, 275], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization._is_whitespace", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.WordpieceTokenizer.__init__": [[280, 284], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.WordpieceTokenizer.tokenize": [[285, 335], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.load_vocab": [[50, 63], ["collections.OrderedDict", "io.open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.whitespace_tokenize": [[65, 72], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization._is_whitespace": [[337, 347], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization._is_control": [[349, 359], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization._is_punctuation": [[361, 375], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "", "", ""]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.InputExample.__init__": [[48, 55], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ",", "flaw_labels", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "self", ".", "flaw_labels", "=", "flaw_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.InputFeatures.__init__": [[58, 63], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_id", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.InputFeatures_disc_train.__init__": [[66, 70], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "token_ids", ",", "label_id", ")", ":", "\n", "\n", "        ", "self", ".", "token_ids", "=", "token_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.InputFeatures_disc_eval.__init__": [[73, 82], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "token_ids", ",", "input_ids", ",", "input_mask", ",", "flaw_labels", ",", "flaw_ids", ",", "label_id", ",", "chunks", ")", ":", "\n", "\n", "        ", "self", ".", "token_ids", "=", "token_ids", "\n", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "flaw_labels", "=", "flaw_labels", "\n", "self", ".", "flaw_ids", "=", "flaw_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "chunks", "=", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.InputFeatures_ngram.__init__": [[92, 99], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokens", ",", "label_id", ",", "ngram_ids", ",", "ngram_labels", ",", "ngram_masks", ")", ":", "\n", "\n", "        ", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "ngram_ids", "=", "ngram_ids", "\n", "self", ".", "ngram_labels", "=", "ngram_labels", "\n", "self", ".", "ngram_masks", "=", "ngram_masks", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.InputFeatures_gnrt_train.__init__": [[102, 108], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "ngram_ids", ",", "ngram_labels", ",", "ngram_masks", ",", "ngram_embeddings", ")", ":", "\n", "\n", "        ", "self", ".", "ngram_ids", "=", "ngram_ids", "\n", "self", ".", "ngram_labels", "=", "ngram_labels", "\n", "self", ".", "ngram_masks", "=", "ngram_masks", "\n", "self", ".", "ngram_embeddings", "=", "ngram_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.InputFeatures_gnrt_eval.__init__": [[111, 118], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "token_ids", ",", "ngram_ids", ",", "ngram_labels", ",", "ngram_masks", ",", "flaw_labels", ",", "label_id", ")", ":", "\n", "        ", "self", ".", "token_ids", "=", "token_ids", "\n", "self", ".", "ngram_ids", "=", "ngram_ids", "\n", "self", ".", "ngram_labels", "=", "ngram_labels", "\n", "self", ".", "ngram_mask", "=", "ngram_masks", "\n", "self", ".", "flaw_labels", "=", "flaw_labels", "\n", "self", ".", "label_id", "=", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.InputFeatures_flaw.__init__": [[122, 127], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "flaw_ids", ",", "flaw_mask", ",", "flaw_labels", ")", ":", "\n", "\n", "        ", "self", ".", "flaw_mask", "=", "flaw_mask", "\n", "self", ".", "flaw_ids", "=", "flaw_ids", "\n", "self", ".", "flaw_labels", "=", "flaw_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor.get_train_examples": [[548, 551], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor.get_dev_examples": [[552, 555], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor.get_labels": [[556, 559], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv": [[560, 571], ["open", "csv.reader", "lines.append", "list", "unicode"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_read_tsv", "(", "cls", ",", "input_file", ",", "quotechar", "=", "None", ")", ":", "\n", "        ", "\"\"\"Reads a tab separated value file.\"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quotechar", "=", "quotechar", ")", "\n", "lines", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "                ", "if", "sys", ".", "version_info", "[", "0", "]", "==", "2", ":", "\n", "                    ", "line", "=", "list", "(", "unicode", "(", "cell", ",", "'utf-8'", ")", "for", "cell", "in", "line", ")", "\n", "", "lines", ".", "append", "(", "line", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv": [[572, 583], ["open", "csv.reader", "lines.append", "list"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "_read_csv", "(", "cls", ",", "input_file", ")", ":", "\n", "        ", "\"\"\"Reads a tab separated value file.\"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\",\"", ")", "\n", "lines", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "                ", "if", "sys", ".", "version_info", "[", "0", "]", "==", "2", ":", "\n", "                    ", "line", "=", "list", "(", "line", ")", "#list(unicode(cell, 'utf-8') for cell in line)", "\n", "", "lines", ".", "append", "(", "line", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.IMDBProcessor.get_train_examples": [[587, 595], ["bert_utils.IMDBProcessor._create_examples", "bert_utils.IMDBProcessor._create_examples", "bert_utils.IMDBProcessor._read_csv", "bert_utils.IMDBProcessor._read_csv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "if", "'csv'", "in", "data_dir", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "data_dir", ")", ",", "\"train\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.csv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.IMDBProcessor.get_dev_examples": [[596, 604], ["bert_utils.IMDBProcessor._create_examples", "bert_utils.IMDBProcessor._create_examples", "bert_utils.IMDBProcessor._read_csv", "bert_utils.IMDBProcessor._read_csv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv"], ["", "", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "if", "'csv'", "in", "data_dir", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "data_dir", ")", ",", "\"dev\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test.csv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.IMDBProcessor.get_disc_dev_examples": [[605, 613], ["bert_utils.IMDBProcessor._create_examples", "bert_utils.IMDBProcessor._create_examples", "bert_utils.IMDBProcessor._read_csv", "bert_utils.IMDBProcessor._read_csv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv"], ["", "", "def", "get_disc_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "if", "'csv'", "in", "data_dir", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "data_dir", ")", ",", "\"dev\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"disc_test.csv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.IMDBProcessor.get_gnrt_dev_examples": [[614, 622], ["bert_utils.IMDBProcessor._create_examples", "bert_utils.IMDBProcessor._create_examples", "bert_utils.IMDBProcessor._read_csv", "bert_utils.IMDBProcessor._read_csv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv"], ["", "", "def", "get_gnrt_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "if", "'csv'", "in", "data_dir", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "data_dir", ")", ",", "\"dev\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"disc_outputs.csv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.IMDBProcessor.get_clf_dev_examples": [[623, 627], ["bert_utils.IMDBProcessor._create_examples", "bert_utils.IMDBProcessor._read_csv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_csv"], ["", "", "def", "get_clf_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"gnrt_outputs.csv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.IMDBProcessor.get_labels": [[628, 631], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.IMDBProcessor._create_examples": [[632, 644], ["enumerate", "examples.append", "len", "bert_utils.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "flaw_labels", "=", "None", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "label", "=", "line", "[", "0", "]", "\n", "if", "len", "(", "line", ")", "==", "3", ":", "flaw_labels", "=", "line", "[", "2", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ",", "flaw_labels", "=", "flaw_labels", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_train_examples": [[650, 658], ["bert_utils.SST2Processor._create_examples", "bert_utils.SST2Processor._create_examples", "bert_utils.SST2Processor._read_tsv", "bert_utils.SST2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "if", "'tsv'", "in", "data_dir", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "data_dir", ")", ",", "\"train\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_dev_examples": [[659, 667], ["bert_utils.SST2Processor._create_examples", "bert_utils.SST2Processor._create_examples", "bert_utils.SST2Processor._read_tsv", "bert_utils.SST2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv"], ["", "", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "if", "'tsv'", "in", "data_dir", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "data_dir", ")", ",", "\"dev\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_disc_dev_examples": [[668, 676], ["bert_utils.SST2Processor._create_examples", "bert_utils.SST2Processor._create_examples", "bert_utils.SST2Processor._read_tsv", "bert_utils.SST2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv"], ["", "", "def", "get_disc_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "if", "'tsv'", "in", "data_dir", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "data_dir", ")", ",", "\"dev\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"disc_dev.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_gnrt_dev_examples": [[677, 685], ["bert_utils.SST2Processor._create_examples", "bert_utils.SST2Processor._create_examples", "bert_utils.SST2Processor._read_tsv", "bert_utils.SST2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv"], ["", "", "def", "get_gnrt_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "if", "'tsv'", "in", "data_dir", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "data_dir", ")", ",", "\"dev\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"disc_outputs.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_clf_dev_examples": [[686, 690], ["bert_utils.SST2Processor._create_examples", "bert_utils.SST2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.DataProcessor._read_tsv"], ["", "", "def", "get_clf_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"gnrt_outputs.tsv\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_labels": [[691, 694], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor._create_examples": [[695, 709], ["enumerate", "examples.append", "len", "bert_utils.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "flaw_labels", "=", "None", "\n", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "0", "]", "\n", "label", "=", "line", "[", "1", "]", "\n", "if", "len", "(", "line", ")", "==", "3", ":", "flaw_labels", "=", "line", "[", "2", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ",", "flaw_labels", "=", "flaw_labels", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_vectors": [[24, 34], ["io.open", "list", "list", "line.rstrip().split", "list", "emb_dict.keys", "emb_dict.values", "len", "len", "map", "line.rstrip"], "function", ["None"], ["def", "load_vectors", "(", "fname", ")", ":", "\n", "    ", "fin", "=", "io", ".", "open", "(", "fname", ",", "'r'", ",", "encoding", "=", "'utf-8'", ",", "newline", "=", "'\\n'", ",", "errors", "=", "'ignore'", ")", "\n", "emb_dict", "=", "{", "}", "\n", "for", "line", "in", "fin", ":", "\n", "        ", "tokens", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "len", "(", "tokens", ")", "==", "2", ":", "continue", "\n", "emb_dict", "[", "tokens", "[", "0", "]", "]", "=", "list", "(", "map", "(", "float", ",", "tokens", "[", "1", ":", "]", ")", ")", "\n", "", "vocab_list", "=", "list", "(", "emb_dict", ".", "keys", "(", ")", ")", "\n", "emb_vec", "=", "list", "(", "emb_dict", ".", "values", "(", ")", ")", "\n", "return", "emb_dict", ",", "emb_vec", ",", "vocab_list", ",", "len", "(", "vocab_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.write_vocab_info": [[35, 40], ["open", "g.write", "g.write", "str"], "function", ["None"], ["", "def", "write_vocab_info", "(", "fname", ",", "vocab_size", ",", "vocab_list", ")", ":", "\n", "    ", "with", "open", "(", "fname", ",", "'w'", ")", "as", "g", ":", "\n", "        ", "g", ".", "write", "(", "str", "(", "vocab_size", ")", "+", "'\\n'", ")", "\n", "for", "vocab", "in", "vocab_list", ":", "\n", "            ", "g", ".", "write", "(", "vocab", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_vocab_info": [[41, 45], ["open", "open.readlines", "int", "contents[].replace", "x.replace"], "function", ["None"], ["", "", "", "def", "load_vocab_info", "(", "fname", ")", ":", "\n", "    ", "f", "=", "open", "(", "fname", ",", "'r'", ")", "\n", "contents", "=", "f", ".", "readlines", "(", ")", "\n", "return", "int", "(", "contents", "[", "0", "]", ".", "replace", "(", "'\\n'", ",", "''", ")", ")", ",", "[", "x", ".", "replace", "(", "'\\n'", ",", "''", ")", "for", "x", "in", "contents", "[", "1", ":", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features": [[128, 167], ["enumerate", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "bert_utils.InputFeatures", "len", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "\n", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "            ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "if", "ex_index", "<", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_disc_train": [[168, 196], ["enumerate", "nltk.tokenize.word_tokenize", "features.append", "enumerate", "len", "token_ids.append", "logger.info", "logger.info", "logger.info", "bert_utils.InputFeatures_disc_train", "len", "str", "str"], "function", ["None"], ["", "def", "convert_examples_to_features_disc_train", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ",", "w2i", "=", "{", "}", ",", "i2w", "=", "{", "}", ",", "index", "=", "1", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "token_ids", "=", "[", "]", "\n", "tokens", "=", "word_tokenize", "(", "example", ".", "text_a", ")", "\n", "if", "len", "(", "tokens", ")", ">", "max_seq_length", ":", "\n", "            ", "tokens", "=", "tokens", "[", ":", "max_seq_length", "]", "\n", "", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", "not", "in", "w2i", ":", "\n", "                ", "w2i", "[", "token", "]", "=", "index", "\n", "i2w", "[", "index", "]", "=", "token", "\n", "index", "+=", "1", "\n", "", "token_ids", ".", "append", "(", "w2i", "[", "token", "]", ")", "\n", "", "token_ids", "+=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "token_ids", ")", ")", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "\n", "if", "ex_index", "<", "2", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "token_ids", "]", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures_disc_train", "(", "token_ids", "=", "token_ids", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", ",", "w2i", ",", "i2w", ",", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_disc_eval": [[197, 286], ["enumerate", "nltk.tokenize.word_tokenize", "enumerate", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "len", "token_ids.append", "tokenizer.tokenize", "chunks.append", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "bert_utils.InputFeatures_disc_eval", "len", "flaw_ids_cut.append", "len", "len", "len", "len", "len", "len", "int", "len", "len", "sum", "example.flaw_labels.split", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "convert_examples_to_features_disc_eval", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ",", "w2i", "=", "{", "}", ",", "i2w", "=", "{", "}", ",", "index", "=", "1", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "\n", "        ", "flaw_ids", "=", "[", "]", "\n", "tokens", "=", "word_tokenize", "(", "example", ".", "text_a", ")", "\n", "if", "len", "(", "tokens", ")", ">", "max_seq_length", ":", "\n", "            ", "tokens", "=", "tokens", "[", ":", "max_seq_length", "]", "\n", "", "if", "example", ".", "flaw_labels", "is", "not", "None", ":", "\n", "            ", "if", "example", ".", "flaw_labels", "==", "''", ":", "flaw_ids", "=", "[", "-", "1", "]", "\n", "else", ":", "\n", "                ", "flaw_ids", "=", "[", "int", "(", "x", ")", "for", "x", "in", "(", "example", ".", "flaw_labels", ")", ".", "split", "(", "','", ")", "]", "\n", "\n", "# flaw_ids: the index of flaw words on word-level", "\n", "# flaw_labels: the index of flaw words on wordpiece-level", "\n", "# usually, flaw_ids will have more flaw words than flaw_labels, since their max length both equals max_seq_length but flaw_labels need more space", "\n", "\n", "", "", "token_ids", "=", "[", "]", "\n", "input_pieces", ",", "chunks", ",", "flaw_labels", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "flaw_ids_cut", "=", "[", "]", "\n", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "tokens", ")", ":", "\n", "\n", "            ", "if", "tok", "not", "in", "w2i", ":", "\n", "                ", "w2i", "[", "tok", "]", "=", "index", "\n", "i2w", "[", "index", "]", "=", "tok", "\n", "index", "+=", "1", "\n", "", "token_ids", ".", "append", "(", "w2i", "[", "tok", "]", ")", "\n", "\n", "word_pieces", "=", "tokenizer", ".", "tokenize", "(", "tok", ")", "\n", "input_pieces", "+=", "word_pieces", "\n", "chunks", ".", "append", "(", "len", "(", "word_pieces", ")", ")", "\n", "\n", "if", "i", "in", "flaw_ids", ":", "\n", "                ", "flaw_ids_cut", ".", "append", "(", "i", ")", "\n", "flaw_labels", "+=", "[", "1", "]", "*", "len", "(", "word_pieces", ")", "\n", "", "else", ":", "\n", "                ", "flaw_labels", "+=", "[", "0", "]", "*", "len", "(", "word_pieces", ")", "\n", "\n", "", "if", "len", "(", "input_pieces", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "input_pieces", "=", "input_pieces", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "flaw_labels", "=", "flaw_labels", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "chunks", "[", "-", "1", "]", "=", "max_seq_length", "-", "2", "-", "sum", "(", "chunks", "[", ":", "-", "1", "]", ")", "\n", "break", "\n", "\n", "", "", "if", "len", "(", "chunks", ")", ">", "max_seq_length", "-", "2", ":", "\n", "            ", "chunks", "=", "chunks", "[", ":", "max_seq_length", "-", "2", "]", "\n", "\n", "", "input_pieces", "=", "[", "\"[CLS]\"", "]", "+", "input_pieces", "+", "[", "\"[SEP]\"", "]", "\n", "flaw_labels", "=", "[", "0", "]", "+", "flaw_labels", "+", "[", "0", "]", "\n", "chunks", "=", "[", "1", "]", "+", "chunks", "+", "[", "1", "]", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "input_pieces", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "flaw_labels", "+=", "padding", "\n", "chunks", "+=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "chunks", ")", ")", "\n", "token_ids", "+=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "token_ids", ")", ")", "\n", "flaw_ids", "+=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "flaw_ids", ")", ")", "\n", "flaw_ids_cut", "+=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "flaw_ids_cut", ")", ")", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "\n", "if", "ex_index", "<", "3", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "example", ".", "text_a", ")", "\n", "logger", ".", "info", "(", "\"token_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "token_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_labels: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_labels", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_ids_cut: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_ids_cut", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"chunks: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "chunks", "]", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures_disc_eval", "(", "token_ids", "=", "token_ids", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "flaw_labels", "=", "flaw_labels", ",", "\n", "flaw_ids", "=", "flaw_ids_cut", ",", "\n", "label_id", "=", "label_id", ",", "\n", "chunks", "=", "chunks", ")", ")", "\n", "", "return", "features", ",", "w2i", ",", "i2w", ",", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_tokens_to_ngram": [[287, 335], ["range", "len", "len", "len", "len", "labels.append", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "masks.append", "len", "len", "len", "random.random", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "def", "convert_tokens_to_ngram", "(", "words", ",", "max_seq_length", ",", "max_ngram_length", ",", "tokenizer", ",", "w2i", ",", "flaw_labels", "=", "None", ",", "N", "=", "2", ",", "train", "=", "False", ")", ":", "\n", "\n", "\n", "# padding for collecting n-grams", "\n", "    ", "words_pad", "=", "[", "\"[CLS]\"", "]", "*", "N", "+", "words", "+", "[", "\"[SEP]\"", "]", "*", "N", "\n", "# features: ngram_input_ids (batch_size, sequence_length, ngram_length)", "\n", "# labels: the word-level ids of the token to predict (batch_size, sequence_length)", "\n", "# masks: ngram_input_mask (batch_size, sequence_length, ngram_length)", "\n", "features", ",", "labels", ",", "masks", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "words", ")", ")", ":", "\n", "        ", "if", "len", "(", "words", ")", ">", "max_seq_length", "and", "train", "and", "random", ".", "random", "(", ")", ">", "0.25", ":", "\n", "            ", "continue", "\n", "# two situations the ngram would be created:", "\n", "# 1. no flaw labels are given, should generate ngrams for all the tokens to train ", "\n", "# 2. flaw labels are given, should generate ngrams for those flaw tokens to test", "\n", "", "if", "(", "flaw_labels", "is", "not", "None", "and", "i", "in", "flaw_labels", ")", "or", "(", "flaw_labels", "is", "None", ")", ":", "\n", "            ", "tokens", "=", "words_pad", "[", "i", ":", "(", "i", "+", "1", "+", "2", "*", "N", ")", "]", "\n", "labels", ".", "append", "(", "w2i", "[", "tokens", "[", "N", "]", "]", ")", "\n", "# mask the middle for prediction", "\n", "tokens", "[", "N", "]", "=", "\"[MASK]\"", "\n", "tokens", "=", "' '", ".", "join", "(", "tokens", ")", "\n", "word_pieces", "=", "tokenizer", ".", "tokenize", "(", "tokens", ")", "\n", "word_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "word_pieces", ")", "\n", "\n", "if", "len", "(", "word_ids", ")", ">", "max_ngram_length", ":", "\n", "                ", "word_ids", "=", "word_ids", "[", ":", "max_ngram_length", "]", "\n", "\n", "", "mask_ids", "=", "[", "1", "]", "*", "len", "(", "word_ids", ")", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_ngram_length", "-", "len", "(", "word_ids", ")", ")", "\n", "word_ids", "+=", "padding", "\n", "mask_ids", "+=", "padding", "\n", "features", ".", "append", "(", "word_ids", ")", "\n", "masks", ".", "append", "(", "mask_ids", ")", "\n", "\n", "", "", "if", "len", "(", "features", ")", ">", "max_seq_length", ":", "\n", "        ", "features", "=", "features", "[", ":", "max_seq_length", "]", "\n", "", "if", "len", "(", "masks", ")", ">", "max_seq_length", ":", "\n", "        ", "masks", "=", "masks", "[", ":", "max_seq_length", "]", "\n", "", "if", "len", "(", "labels", ")", ">", "max_seq_length", ":", "\n", "        ", "labels", "=", "labels", "[", ":", "max_seq_length", "]", "\n", "\n", "", "padding", "=", "[", "[", "0", "]", "*", "max_ngram_length", "]", "*", "(", "max_seq_length", "-", "len", "(", "features", ")", ")", "\n", "features", "+=", "padding", "\n", "masks", "+=", "padding", "\n", "labels", "+=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "labels", ")", ")", "\n", "\n", "return", "features", ",", "labels", ",", "masks", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_gnrt_train": [[336, 379], ["enumerate", "nltk.tokenize.word_tokenize", "bert_utils.convert_tokens_to_ngram", "bert_utils.look_up_embeddings", "features.append", "token_ids.append", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "bert_utils.InputFeatures_gnrt_train", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_tokens_to_ngram", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.look_up_embeddings"], ["", "def", "convert_examples_to_features_gnrt_train", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "max_ngram_length", ",", "tokenizer", ",", "embeddings", ",", "w2i", "=", "{", "}", ",", "i2w", "=", "{", "}", ",", "index", "=", "1", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "\n", "        ", "tokens", "=", "word_tokenize", "(", "example", ".", "text_a", ")", "\n", "# if len(tokens) > max_seq_length:", "\n", "#     tokens = tokens[:max_ngram_length]", "\n", "\n", "token_ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", "not", "in", "w2i", ":", "\n", "                ", "w2i", "[", "token", "]", "=", "index", "\n", "i2w", "[", "index", "]", "=", "token", "\n", "index", "+=", "1", "\n", "", "token_ids", ".", "append", "(", "w2i", "[", "token", "]", ")", "\n", "\n", "", "ngram_ids", ",", "ngram_labels", ",", "ngram_masks", "=", "convert_tokens_to_ngram", "(", "tokens", ",", "\n", "max_seq_length", ",", "\n", "max_ngram_length", ",", "\n", "tokenizer", ",", "\n", "w2i", ",", "\n", "train", "=", "True", ")", "\n", "\n", "ngram_embeddings", "=", "look_up_embeddings", "(", "ngram_labels", ",", "embeddings", ",", "i2w", ")", "\n", "\n", "\n", "if", "ex_index", "<", "3", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "token_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"ngram_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "ngram_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"ngram_labels: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "ngram_labels", "]", ")", ")", "\n", "#logger.info(\"ngram_masks: %s\" % \" \".join([str(x) for x in ngram_masks]))", "\n", "#logger.info(\"ngram_embeddings: %s\" % \" \".join([str(x) for x in ngram_embeddings]))", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures_gnrt_train", "(", "ngram_ids", "=", "ngram_ids", ",", "\n", "ngram_labels", "=", "ngram_labels", ",", "\n", "ngram_masks", "=", "ngram_masks", ",", "\n", "ngram_embeddings", "=", "ngram_embeddings", ")", ")", "\n", "", "return", "features", ",", "w2i", ",", "i2w", ",", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_gnrt_eval": [[380, 433], ["enumerate", "nltk.tokenize.word_tokenize", "bert_utils.convert_tokens_to_ngram", "features.append", "enumerate", "len", "token_ids.append", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "bert_utils.InputFeatures_gnrt_eval", "int", "len", "len", "example.flaw_labels.split", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_tokens_to_ngram"], ["", "def", "convert_examples_to_features_gnrt_eval", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "max_ngram_length", ",", "tokenizer", ",", "w2i", "=", "{", "}", ",", "i2w", "=", "{", "}", ",", "index", "=", "1", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "\n", "        ", "tokens", "=", "word_tokenize", "(", "example", ".", "text_a", ")", "\n", "if", "len", "(", "tokens", ")", ">", "max_seq_length", ":", "\n", "            ", "tokens", "=", "tokens", "[", ":", "max_seq_length", "]", "\n", "\n", "", "if", "example", ".", "flaw_labels", "is", "not", "None", ":", "\n", "            ", "flaw_labels", "=", "[", "int", "(", "x", ")", "for", "x", "in", "(", "example", ".", "flaw_labels", ")", ".", "split", "(", "','", ")", "]", "\n", "# else:", "\n", "#     flaw_labels = list(range(len(tokens)))   ", "\n", "\n", "", "token_ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", "not", "in", "w2i", ":", "\n", "                ", "w2i", "[", "token", "]", "=", "index", "\n", "i2w", "[", "index", "]", "=", "token", "\n", "index", "+=", "1", "\n", "", "token_ids", ".", "append", "(", "w2i", "[", "token", "]", ")", "\n", "\n", "", "ngram_ids", ",", "ngram_labels", ",", "ngram_masks", "=", "convert_tokens_to_ngram", "(", "tokens", ",", "\n", "max_seq_length", ",", "\n", "max_ngram_length", ",", "\n", "tokenizer", ",", "\n", "w2i", ",", "\n", "flaw_labels", ")", "\n", "flaw_labels", "+=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "flaw_labels", ")", ")", "\n", "token_ids", "+=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "token_ids", ")", ")", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "\n", "if", "ex_index", "<", "3", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"token_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "token_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_labels: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_labels", "]", ")", ")", "\n", "#logger.info(\"ngram_ids: %s\" % \" \".join([str(x) for x in ngram_ids]))", "\n", "logger", ".", "info", "(", "\"ngram_labels: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "ngram_labels", "]", ")", ")", "\n", "#logger.info(\"ngram_masks: %s\" % \" \".join([str(x) for x in ngram_masks]))", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures_gnrt_eval", "(", "token_ids", "=", "token_ids", ",", "\n", "ngram_ids", "=", "ngram_ids", ",", "\n", "ngram_labels", "=", "ngram_labels", ",", "\n", "ngram_masks", "=", "ngram_masks", ",", "\n", "flaw_labels", "=", "flaw_labels", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "\n", "", "return", "features", ",", "w2i", ",", "i2w", ",", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_adv": [[434, 483], ["enumerate", "tokenizer.convert_tokens_to_ids", "features.append", "bert_utils.random_attack", "tokenizer.tokenize", "flaw_tokens.append", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "bert_utils.InputFeatures_flaw", "len", "len", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.random_attack", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "convert_examples_to_features_adv", "(", "examples", ",", "max_seq_length", ",", "tokenizer", ",", "i2w", ",", "embeddings", "=", "None", ",", "emb_index", "=", "None", ",", "words", "=", "None", ")", ":", "\n", "\n", "    ", "features", "=", "[", "]", "\n", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "\n", "        ", "tokens", "=", "example", "\n", "flaw_tokens", ",", "flaw_pieces", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "tok_id", "in", "tokens", ":", "\n", "\n", "            ", "if", "tok_id", "==", "0", ":", "break", "\n", "\n", "tok", "=", "i2w", "[", "tok_id", "]", "\n", "\n", "_", ",", "tok_flaw", "=", "random_attack", "(", "tok", ",", "embeddings", ",", "emb_index", ",", "words", ")", "\n", "word_pieces", "=", "tokenizer", ".", "tokenize", "(", "tok_flaw", ")", "\n", "\n", "flaw_pieces", "+=", "word_pieces", "\n", "flaw_tokens", ".", "append", "(", "tok_flaw", ")", "\n", "\n", "if", "len", "(", "flaw_pieces", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "flaw_pieces", "=", "flaw_pieces", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "break", "\n", "\n", "", "", "flaw_pieces", "=", "[", "\"[CLS]\"", "]", "+", "flaw_pieces", "+", "[", "\"[SEP]\"", "]", "\n", "flaw_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "flaw_pieces", ")", "\n", "flaw_mask", "=", "[", "1", "]", "*", "len", "(", "flaw_ids", ")", "\n", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "flaw_ids", ")", ")", "\n", "flaw_ids", "+=", "padding", "\n", "flaw_mask", "+=", "padding", "\n", "\n", "assert", "len", "(", "flaw_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "flaw_mask", ")", "==", "max_seq_length", "\n", "\n", "if", "ex_index", "<", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_mask", "]", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures_flaw", "(", "flaw_ids", "=", "flaw_ids", ",", "\n", "flaw_mask", "=", "flaw_mask", ",", "\n", "flaw_labels", "=", "None", ")", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features_flaw": [[485, 544], ["enumerate", "tokenizer.convert_tokens_to_ids", "features.append", "bert_utils.random_attack", "tokenizer.tokenize", "flaw_tokens.append", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "bert_utils.InputFeatures_flaw", "len", "len", "len", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.random_attack", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "convert_examples_to_features_flaw", "(", "examples", ",", "max_seq_length", ",", "max_ngram_length", ",", "tokenizer", ",", "i2w", ",", "embeddings", "=", "None", ",", "emb_index", "=", "None", ",", "words", "=", "None", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "\n", "        ", "tokens", "=", "example", "\n", "flaw_labels", "=", "[", "]", "\n", "flaw_tokens", ",", "flaw_pieces", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "tok_id", "in", "tokens", ":", "\n", "\n", "            ", "if", "tok_id", "==", "0", ":", "break", "\n", "\n", "tok", "=", "i2w", "[", "tok_id", "]", "\n", "\n", "label", ",", "tok_flaw", "=", "random_attack", "(", "tok", ",", "embeddings", ",", "emb_index", ",", "words", ")", "#embeddings", "\n", "word_pieces", "=", "tokenizer", ".", "tokenize", "(", "tok_flaw", ")", "\n", "\n", "flaw_labels", "+=", "[", "label", "]", "*", "len", "(", "word_pieces", ")", "\n", "flaw_pieces", "+=", "word_pieces", "\n", "\n", "flaw_tokens", ".", "append", "(", "tok_flaw", ")", "\n", "\n", "if", "len", "(", "flaw_pieces", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "flaw_pieces", "=", "flaw_pieces", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "flaw_labels", "=", "flaw_labels", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "break", "\n", "\n", "", "", "flaw_pieces", "=", "[", "\"[CLS]\"", "]", "+", "flaw_pieces", "+", "[", "\"[SEP]\"", "]", "\n", "flaw_labels", "=", "[", "0", "]", "+", "flaw_labels", "+", "[", "0", "]", "\n", "\n", "flaw_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "flaw_pieces", ")", "\n", "flaw_mask", "=", "[", "1", "]", "*", "len", "(", "flaw_ids", ")", "\n", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "flaw_ids", ")", ")", "\n", "flaw_ids", "+=", "padding", "\n", "flaw_mask", "+=", "padding", "\n", "flaw_labels", "+=", "padding", "\n", "\n", "assert", "len", "(", "flaw_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "flaw_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "flaw_labels", ")", "==", "max_seq_length", "\n", "\n", "if", "ex_index", "<", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"flaw_labels: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "flaw_labels", "]", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures_flaw", "(", "flaw_ids", "=", "flaw_ids", ",", "\n", "flaw_mask", "=", "flaw_mask", ",", "\n", "flaw_labels", "=", "flaw_labels", ")", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.attack_char": [[711, 724], ["random.random", "len", "random.choice", "random.choice", "range"], "function", ["None"], ["", "", "def", "attack_char", "(", "token", ")", ":", "\n", "\n", "    ", "sign", "=", "random", ".", "random", "(", ")", "\n", "length", "=", "len", "(", "token", ")", "\n", "index", "=", "random", ".", "choice", "(", "range", "(", "length", ")", ")", "\n", "letter", "=", "random", ".", "choice", "(", "string", ".", "ascii_letters", ")", "\n", "if", "sign", "<", "0.33", ":", "# swap", "\n", "        ", "token", "=", "token", "[", ":", "index", "]", "+", "letter", "+", "token", "[", "index", "+", "1", ":", "]", "\n", "", "elif", "sign", "<", "0.66", ":", "# add", "\n", "        ", "token", "=", "token", "[", ":", "index", "]", "+", "letter", "+", "token", "[", "index", ":", "]", "\n", "", "else", ":", "# drop", "\n", "        ", "token", "=", "token", "[", ":", "index", "]", "+", "token", "[", "index", "+", "1", ":", "]", "\n", "", "return", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_embeddings_and_save_index": [[726, 744], ["list", "numpy.asarray", "len", "len", "hnswlib.Index", "hnswlib.Index.init_index", "hnswlib.Index.set_num_threads", "hnswlib.Index.add_items", "hnswlib.Index.save_index", "len", "len"], "function", ["None"], ["", "def", "load_embeddings_and_save_index", "(", "labels", ",", "emb_vec", ",", "index_path", ")", ":", "\n", "    ", "'''\n        labels: size-N numpy array of integer word IDs for embeddings.\n        embeddings: size N*d numpy array of vectors.\n        index_path: path to store the index.\n    '''", "\n", "\n", "labels", "=", "list", "(", "labels", ")", "\n", "emb_vec", "=", "np", ".", "asarray", "(", "emb_vec", ")", "\n", "num_words", "=", "len", "(", "labels", ")", "\n", "num_dim", "=", "len", "(", "emb_vec", "[", "0", "]", ")", "\n", "assert", "(", "len", "(", "labels", ")", "==", "len", "(", "emb_vec", ")", ")", "\n", "p", "=", "hnswlib", ".", "Index", "(", "space", "=", "'l2'", ",", "dim", "=", "num_dim", ")", "\n", "p", ".", "init_index", "(", "max_elements", "=", "num_words", ",", "ef_construction", "=", "200", ",", "M", "=", "16", ")", "\n", "p", ".", "set_num_threads", "(", "16", ")", "\n", "p", ".", "add_items", "(", "emb_vec", ",", "labels", ")", "\n", "p", ".", "save_index", "(", "index_path", ")", "\n", "return", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.load_embedding_index": [[746, 750], ["hnswlib.Index", "hnswlib.Index.load_index"], "function", ["None"], ["", "def", "load_embedding_index", "(", "index_path", ",", "vocab_size", ",", "num_dim", "=", "300", ")", ":", "\n", "    ", "p", "=", "hnswlib", ".", "Index", "(", "space", "=", "'l2'", ",", "dim", "=", "num_dim", ")", "\n", "p", ".", "load_index", "(", "index_path", ",", "max_elements", "=", "vocab_size", ")", "\n", "return", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.query_most_similar_word_id_from_embedding": [[751, 754], ["p.knn_query"], "function", ["None"], ["", "def", "query_most_similar_word_id_from_embedding", "(", "p", ",", "emb", ",", "n", ")", ":", "\n", "    ", "finding", "=", "p", ".", "knn_query", "(", "[", "emb", "]", ",", "k", "=", "n", ")", "\n", "return", "finding", "[", "0", "]", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.attack_word": [[756, 773], ["bert_utils.query_most_similar_word_id_from_embedding", "random.choice", "range", "len"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.query_most_similar_word_id_from_embedding"], ["", "def", "attack_word", "(", "tok", ",", "p", ",", "emb_dict", ",", "vocab_list", ")", ":", "# TODO: attack tokens from the word-level: find a similar word and replace", "\n", "    ", "'''\n        tok: string to be attacked.\n        p: the object loaded by load_embedding_index or generated by load_embeddings_and_save_index.\n        emb_dict: a dict, transferring a word to the corresponding embedding vector.\n        emb_vocab: a list or a dict, transferring the word id to the corresponding word string.\n    '''", "\n", "# embeddings: dict", "\n", "# tok: string", "\n", "if", "tok", "in", "emb_dict", ":", "\n", "        ", "tok_emb", "=", "emb_dict", "[", "tok", "]", "\n", "", "else", ":", "\n", "        ", "pass", "\n", "\n", "", "most_similar_word_id", "=", "query_most_similar_word_id_from_embedding", "(", "p", ",", "tok_emb", ",", "20", ")", "\n", "index", "=", "random", ".", "choice", "(", "range", "(", "len", "(", "most_similar_word_id", ")", ")", ")", "\n", "return", "vocab_list", "[", "most_similar_word_id", "[", "index", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.random_attack": [[774, 794], ["numpy.random.random", "bert_utils.attack_char", "bert_utils.attack_word"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.attack_char", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.attack_word"], ["", "def", "random_attack", "(", "tok", ",", "emb_dict", ",", "p", ",", "vocab_list", ")", ":", "\n", "\n", "    ", "prob", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "# attack token with 15% probability", "\n", "if", "prob", "<", "0.15", ":", "\n", "        ", "prob", "/=", "0.15", "\n", "# 60% randomly attack token from character level", "\n", "if", "prob", "<", "0.8", ":", "#0.8 ", "\n", "            ", "tok_flaw", "=", "attack_char", "(", "tok", ")", "\n", "# 40% randomly attack token from word level", "\n", "", "else", ":", "\n", "            ", "if", "emb_dict", "is", "not", "None", ":", "\n", "                ", "tok_flaw", "=", "attack_word", "(", "tok", ",", "p", ",", "emb_dict", ",", "vocab_list", ")", "\n", "#print(tok+' '+tok_flaw)", "\n", "return", "1", ",", "tok_flaw", "\n", "", "else", ":", "\n", "                ", "return", "0", ",", "tok", "\n", "", "", "return", "1", ",", "tok_flaw", "\n", "", "else", ":", "\n", "        ", "return", "0", ",", "tok", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.accuracy": [[795, 798], ["numpy.argmax", "numpy.sum"], "function", ["None"], ["", "", "def", "accuracy", "(", "out", ",", "labels", ")", ":", "\n", "    ", "outputs", "=", "np", ".", "argmax", "(", "out", ",", "axis", "=", "1", ")", "\n", "return", "np", ".", "sum", "(", "outputs", "==", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.accuracy_2d": [[799, 806], ["numpy.sum", "len", "range", "len"], "function", ["None"], ["", "def", "accuracy_2d", "(", "out", ",", "labels", ")", ":", "\n", "    ", "tmp1", ",", "tmp2", "=", "[", "]", ",", "[", "]", "\n", "for", "l", "in", "out", ":", "\n", "        ", "tmp1", "+=", "l", "\n", "", "for", "l", "in", "labels", ":", "\n", "        ", "tmp2", "+=", "l", "\n", "", "return", "np", ".", "sum", "(", "[", "1", "if", "tmp1", "[", "i", "]", "==", "tmp2", "[", "i", "]", "else", "0", "for", "i", "in", "range", "(", "len", "(", "tmp2", ")", ")", "]", ")", "/", "len", "(", "tmp2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.f1_3d": [[807, 814], ["numpy.reshape", "numpy.reshape", "numpy.argmax", "sklearn.metrics.f1_score", "sklearn.metrics.recall_score", "sklearn.metrics.precision_score"], "function", ["None"], ["", "def", "f1_3d", "(", "out", ",", "labels", ")", ":", "\n", "\n", "    ", "num", "=", "out", ".", "shape", "[", "-", "1", "]", "\n", "out", "=", "np", ".", "reshape", "(", "out", ",", "[", "-", "1", ",", "num", "]", ")", "\n", "labels", "=", "np", ".", "reshape", "(", "labels", ",", "[", "-", "1", "]", ")", "\n", "outputs", "=", "np", ".", "argmax", "(", "out", ",", "axis", "=", "1", ")", "\n", "return", "f1_score", "(", "labels", ",", "outputs", ")", ",", "recall_score", "(", "labels", ",", "outputs", ")", ",", "precision_score", "(", "labels", ",", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.f1_2d": [[815, 822], ["sklearn.metrics.f1_score", "sklearn.metrics.recall_score", "sklearn.metrics.precision_score"], "function", ["None"], ["", "def", "f1_2d", "(", "labels", ",", "out", ")", ":", "\n", "    ", "tmp1", ",", "tmp2", "=", "[", "]", ",", "[", "]", "\n", "for", "l", "in", "out", ":", "\n", "        ", "tmp1", "+=", "l", "\n", "", "for", "l", "in", "labels", ":", "\n", "        ", "tmp2", "+=", "l", "\n", "", "return", "f1_score", "(", "tmp2", ",", "tmp1", ")", ",", "recall_score", "(", "tmp2", ",", "tmp1", ")", ",", "precision_score", "(", "tmp2", ",", "tmp1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.look_up_embeddings": [[823, 839], ["embs.append", "embs.append", "list"], "function", ["None"], ["", "def", "look_up_embeddings", "(", "ngram_labels", ",", "embeddings", ",", "i2w", ")", ":", "\n", "#ngram_labels: (batch_size, sequence_length); padded", "\n", "#ngram_embeddings: (batch_size, sequence_length, embedding_size)", "\n", "\n", "    ", "embs", "=", "[", "]", "\n", "for", "label", "in", "ngram_labels", ":", "\n", "        ", "if", "label", "==", "0", ":", "\n", "            ", "embs", ".", "append", "(", "[", "0", "]", "*", "300", ")", "\n", "", "else", ":", "\n", "            ", "word", "=", "i2w", "[", "label", "]", "\n", "if", "word", "in", "embeddings", ":", "\n", "                ", "embs", ".", "append", "(", "list", "(", "embeddings", "[", "word", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "", "", "return", "embs", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.look_up_words": [[840, 852], ["enumerate", "ngram_labels.append", "sum", "bert_utils.query_most_similar_word_id_from_embedding"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.query_most_similar_word_id_from_embedding"], ["", "def", "look_up_words", "(", "ngram_logits", ",", "ngram_masks", ",", "vocab_list", ",", "p", ")", ":", "#embeddings", "\n", "# ngram_logits: (sequence_size, embedding_size)", "\n", "# ngram_labels: (sequence_size)", "\n", "# ngram_masks: (sequence_size)", "\n", "    ", "ngram_labels", "=", "[", "]", "\n", "for", "i", ",", "emb", "in", "enumerate", "(", "ngram_logits", ")", ":", "# TODO: find the most similar words from the embeddings ", "\n", "        ", "if", "sum", "(", "ngram_masks", "[", "i", "]", ")", "==", "0", ":", "break", "\n", "most_similar_word_id", "=", "query_most_similar_word_id_from_embedding", "(", "p", ",", "emb", ",", "1", ")", "[", "0", "]", "\n", "\n", "ngram_labels", ".", "append", "(", "vocab_list", "[", "most_similar_word_id", "]", ")", "\n", "\n", "", "return", "ngram_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.multiplyList": [[853, 860], ["None"], "function", ["None"], ["", "def", "multiplyList", "(", "myList", ")", ":", "\n", "\n", "# Multiply elements one by one ", "\n", "    ", "result", "=", "1", "\n", "for", "x", "in", "myList", ":", "\n", "        ", "result", "=", "result", "*", "x", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.logit_converter": [[862, 881], ["len", "len", "range", "range", "flaw_logits.append", "flaw_logit.append", "bert_utils.multiplyList"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.multiplyList"], ["", "def", "logit_converter", "(", "logits", ",", "chunks", ")", ":", "\n", "# logits: (batch, sequence_length); padded", "\n", "# flaw_logits: (batch, sequence_length); padded", "\n", "\n", "    ", "max_seq_length", "=", "len", "(", "chunks", "[", "0", "]", ")", "\n", "max_batch_size", "=", "len", "(", "chunks", ")", "\n", "flaw_logits", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "max_batch_size", ")", ":", "\n", "        ", "flaw_logit", "=", "[", "]", "\n", "index", "=", "1", "\n", "for", "j", "in", "range", "(", "1", ",", "max_seq_length", "-", "1", ")", ":", "#index = 1; (1,max_seq_length-1): [CLS] and [SEP]", "\n", "            ", "com", "=", "chunks", "[", "i", "]", "[", "j", "]", "\n", "if", "com", "==", "0", ":", "break", "\n", "flaw_logit", ".", "append", "(", "multiplyList", "(", "logits", "[", "i", "]", "[", "index", ":", "(", "index", "+", "com", ")", "]", ")", ")", "\n", "index", "+=", "com", "\n", "", "flaw_logits", ".", "append", "(", "flaw_logit", ")", "\n", "\n", "", "return", "flaw_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.replace_token": [[882, 903], ["range", "len", "len"], "function", ["None"], ["", "def", "replace_token", "(", "token_ids", ",", "flaw_labels", ",", "correct_tokens", ",", "i2w", ")", ":", "\n", "\n", "    ", "tokens", "=", "[", "i2w", "[", "x", "]", "for", "x", "in", "token_ids", "if", "x", "!=", "0", "]", "\n", "if", "-", "1", "in", "flaw_labels", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "#flaw_labels = [x for x in flaw_labels if x != 0] ", "\n", "        ", "while", "len", "(", "flaw_labels", ")", ">", "1", "and", "flaw_labels", "[", "-", "1", "]", "==", "0", ":", "flaw_labels", "=", "flaw_labels", "[", ":", "-", "1", "]", "\n", "\n", "# print(\"flaw_labels:{}\".format(flaw_labels))", "\n", "# print(\"tokens:{}\".format(tokens))", "\n", "# print(\"correct_tokens:{}\".format(correct_tokens))", "\n", "\n", "try", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "flaw_labels", ")", ")", ":", "\n", "                ", "tokens", "[", "flaw_labels", "[", "i", "]", "]", "=", "correct_tokens", "[", "i", "]", "\n", "", "", "except", ":", "\n", "#print(\"############# out of bound ###############\")", "\n", "            ", "pass", "\n", "\n", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.enumerate_attacks.attack": [[32, 56], ["random.randint", "len", "random.randint", "random.choice", "random.randint", "len", "random.choice", "len", "emb_index.get_items", "emb_index.knn_query", "random.choice", "len"], "function", ["None"], ["def", "attack", "(", "text", ",", "attack_type", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "if", "attack_type", "==", "'add'", ":", "\n", "            ", "idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "text", ")", ")", "\n", "return", "text", "[", ":", "idx", "]", "+", "random", ".", "choice", "(", "available_chars", ")", "+", "text", "[", "idx", ":", "]", "\n", "", "elif", "attack_type", "==", "'drop'", ":", "\n", "            ", "idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "text", ")", "-", "1", ")", "\n", "return", "text", "[", ":", "idx", "]", "+", "text", "[", "idx", "+", "1", ":", "]", "\n", "", "elif", "attack_type", "==", "'swap'", ":", "\n", "            ", "idx", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "text", ")", "-", "2", ")", "\n", "return", "text", "[", ":", "idx", "]", "+", "text", "[", "idx", "+", "1", "]", "+", "text", "[", "idx", "]", "+", "text", "[", "idx", "+", "2", ":", "]", "\n", "", "elif", "attack_type", "==", "'rand'", ":", "\n", "            ", "return", "random", ".", "choice", "(", "wordbase", ")", "\n", "", "elif", "attack_type", "==", "'embed'", ":", "\n", "            ", "emb", "=", "emb_index", ".", "get_items", "(", "[", "emb_word_id", "[", "text", "]", "]", ")", "\n", "word_ids", ",", "_", "=", "emb_index", ".", "knn_query", "(", "emb", ",", "k", "=", "20", ")", "\n", "candidates", "=", "[", "emb_words", "[", "i", "]", "for", "i", "in", "word_ids", "[", "0", "]", "if", "emb_words", "[", "i", "]", "!=", "text", "and", "(", "emb_words", "[", "i", "]", "not", "in", "forbids", ")", "]", "\n", "if", "len", "(", "candidates", ")", "==", "0", ":", "\n", "                ", "return", "None", "\n", "", "return", "random", ".", "choice", "(", "candidates", ")", "\n", "\n", "", "", "except", ":", "\n", "        ", "return", "None", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.enumerate_attacks.is_number": [[57, 63], ["float"], "function", ["None"], ["", "def", "is_number", "(", "x", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "float", "(", "x", ")", "\n", "return", "True", "\n", "", "except", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_eval_epoches.main": [[39, 203], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "parser.parse_args.task_name.lower", "processor.get_labels", "tokenization.BertTokenizer.from_pretrained", "processor.get_dev_examples", "bert_utils.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "range", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "torch.cuda.manual_seed_all", "ValueError", "len", "print", "os.path.join", "os.path.join", "bert_model.BertConfig", "bert_model.BertForSequenceClassification", "bert_model.BertForSequenceClassification.load_state_dict", "bert_model.BertForSequenceClassification.to", "bert_model.BertForSequenceClassification.eval", "tqdm.tqdm", "bool", "torch.load", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "label_ids.to().numpy.to", "model.detach().cpu().numpy", "label_ids.to().numpy.to().numpy", "bert_utils.accuracy", "model.mean().item", "input_ids.to.size", "open", "logger.info", "sorted", "torch.no_grad", "bert_model.BertForSequenceClassification.", "bert_model.BertForSequenceClassification.", "result.keys", "logger.info", "writer.write", "torch.cuda.is_available", "model.detach().cpu", "label_ids.to().numpy.to", "model.mean", "str", "str", "model.detach", "str"], "function", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_labels", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.SST2Processor.get_dev_examples", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.convert_examples_to_features", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_utils.accuracy"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"", "\n", "\"bert-base-multilingual-cased, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_file\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output results will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_file\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The input directory of input data file.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--weights\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The output results will be written.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "num_labels", "=", "num_labels_task", "[", "task_name", "]", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_file", ")", "\n", "eval_features", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ")", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "args", ".", "weights", ")", ":", "\n", "        ", "print", "(", "\"epoch:{}\"", ".", "format", "(", "i", ")", ")", "\n", "#if args.weights != '':", "\n", "#WEIGHTS_NAME = \"epoch\"+str(args.weights)+\"_\"+WEIGHTS_NAME", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "\"epoch\"", "+", "str", "(", "i", ")", "+", "\"_\"", "+", "WEIGHTS_NAME", ")", "\n", "#else:", "\n", "#output_model_file = os.path.join(args.model_dir, WEIGHTS_NAME)", "\n", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "model_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", "(", "output_config_file", ")", "\n", "model", "=", "BertForSequenceClassification", "(", "config", ",", "num_labels", "=", "num_labels", ")", "\n", "#model = BertForLMClassification(config, num_labels=num_labels)", "\n", "#model = BertForDClassifier(config, num_labels=num_labels)", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "output_model_file", ")", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", ",", "eval_accuracy", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "            ", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "tmp_eval_loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "label_ids", ")", "\n", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "tmp_eval_accuracy", "=", "accuracy", "(", "logits", ",", "label_ids", ")", "\n", "\n", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "eval_accuracy", "+=", "tmp_eval_accuracy", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "eval_accuracy", "=", "eval_accuracy", "/", "nb_eval_examples", "\n", "result", "=", "{", "'eval_loss'", ":", "eval_loss", ",", "\n", "'eval_accuracy'", ":", "eval_accuracy", "}", "\n", "\n", "output_eval_file", "=", "args", ".", "output_file", "\n", "with", "open", "(", "output_eval_file", ",", "\"a\"", ")", "as", "writer", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.__init__": [[134, 190], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "\n", "and", "isinstance", "(", "vocab_size_or_config_json_file", ",", "unicode", ")", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.from_dict": [[192, 199], ["bert_model.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.from_json_file": [[200, 206], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.__repr__": [[207, 209], ["str", "bert_model.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.to_dict": [[210, 214], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.to_json_string": [[215, 218], ["json.dumps", "bert_model.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertEmbeddings.__init__": [[241, 251], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertEmbeddings.forward": [[252, 267], ["input_ids.size", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "bert_model.BertEmbeddings.word_embeddings", "bert_model.BertEmbeddings.position_embeddings", "bert_model.BertEmbeddings.token_type_embeddings", "bert_model.BertEmbeddings.LayerNorm", "bert_model.BertEmbeddings.dropout", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertSelfAttention.__init__": [[270, 285], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertSelfAttention.transpose_for_scores": [[286, 290], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertSelfAttention.forward": [[291, 318], ["bert_model.BertSelfAttention.query", "bert_model.BertSelfAttention.key", "bert_model.BertSelfAttention.value", "bert_model.BertSelfAttention.transpose_for_scores", "bert_model.BertSelfAttention.transpose_for_scores", "bert_model.BertSelfAttention.transpose_for_scores", "torch.matmul", "bert_model.BertSelfAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "bert_model.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertSelfOutput.__init__": [[321, 326], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertSelfOutput.forward": [[327, 332], ["bert_model.BertSelfOutput.dense", "bert_model.BertSelfOutput.dropout", "bert_model.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertAttention.__init__": [[335, 339], ["torch.nn.Module.__init__", "bert_model.BertSelfAttention", "bert_model.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertAttention.forward": [[340, 344], ["bert_model.BertAttention.self", "bert_model.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertIntermediate.__init__": [[347, 354], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertIntermediate.forward": [[355, 359], ["bert_model.BertIntermediate.dense", "bert_model.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertOutput.__init__": [[362, 367], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertOutput.forward": [[368, 373], ["bert_model.BertOutput.dense", "bert_model.BertOutput.dropout", "bert_model.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertLayer.__init__": [[376, 381], ["torch.nn.Module.__init__", "bert_model.BertAttention", "bert_model.BertIntermediate", "bert_model.BertOutput"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertLayer.forward": [[382, 387], ["bert_model.BertLayer.attention", "bert_model.BertLayer.intermediate", "bert_model.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertEncoder.__init__": [[390, 394], ["torch.nn.Module.__init__", "bert_model.BertLayer", "torch.nn.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BertLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertEncoder.forward": [[395, 404], ["layer_module", "all_encoder_layers.append", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ")", ":", "\n", "        ", "all_encoder_layers", "=", "[", "]", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPooler.__init__": [[407, 411], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPooler.forward": [[412, 419], ["bert_model.BertPooler.dense", "bert_model.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPredictionHeadTransform.__init__": [[422, 430], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "config", ".", "hidden_act", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPredictionHeadTransform.forward": [[431, 436], ["bert_model.BertPredictionHeadTransform.dense", "bert_model.BertPredictionHeadTransform.transform_act_fn", "bert_model.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertLMPredictionHead.__init__": [[439, 450], ["torch.nn.Module.__init__", "bert_model.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "bert_model_embedding_weights.size"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertLMPredictionHead.forward": [[451, 455], ["bert_model.BertLMPredictionHead.transform", "bert_model.BertLMPredictionHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertOnlyMLMHead.__init__": [[458, 461], ["torch.nn.Module.__init__", "bert_model.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertOnlyMLMHead.forward": [[462, 465], ["bert_model.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertOnlyNSPHead.__init__": [[468, 471], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertOnlyNSPHead.forward": [[472, 475], ["bert_model.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainingHeads.__init__": [[478, 482], ["torch.nn.Module.__init__", "bert_model.BertLMPredictionHead", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainingHeads.forward": [[483, 487], ["bert_model.BertPreTrainingHeads.predictions", "bert_model.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.__init__": [[493, 503], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "BertPreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.init_bert_weights": [[504, 516], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertPreTrainedModel.from_pretrained": [[517, 639], ["os.path.join", "bert_model.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "zip", "getattr", "torch.load.copy", "bert_model.BertPreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertConfig.from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "\n", "from_tf", "=", "False", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name_or_path", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", "or", "from_tf", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "# Instantiate model.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ",", "map_location", "=", "'cpu'", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "None", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "TF_WEIGHTS_NAME", ")", "\n", "return", "load_tf_weights_in_bert", "(", "model", ",", "weights_path", ")", "\n", "# Load from a PyTorch state_dict", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "start_prefix", "=", "''", "\n", "if", "not", "hasattr", "(", "model", ",", "'bert'", ")", "and", "any", "(", "s", ".", "startswith", "(", "'bert.'", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "start_prefix", "=", "'bert.'", "\n", "", "load", "(", "model", ",", "prefix", "=", "start_prefix", ")", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Error(s) in loading state_dict for {}:\\n\\t{}'", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", ")", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertModel.__init__": [[685, 691], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertEmbeddings", "bert_model.BertEncoder", "bert_model.BertPooler", "bert_model.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertModel.forward": [[692, 722], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "bert_model.BertModel.embeddings", "bert_model.BertModel.encoder", "bert_model.BertModel.pooler", "torch.ones_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "next", "bert_model.BertModel.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForPreTraining.__init__": [[774, 779], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "bert_model.BertPreTrainingHeads", "bert_model.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForPreTraining", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForPreTraining.forward": [[780, 793], ["bert_model.BertForPreTraining.bert", "bert_model.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "next_sentence_label", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "sequence_output", ",", "pooled_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForMaskedLM.__init__": [[837, 842], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "bert_model.BertOnlyMLMHead", "bert_model.BertForMaskedLM.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForMaskedLM", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForMaskedLM.forward": [[843, 854], ["bert_model.BertForMaskedLM.bert", "bert_model.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "bert_model.BertForMaskedLM.view", "masked_lm_labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "masked_lm_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForNextSentencePrediction.__init__": [[899, 904], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "bert_model.BertOnlyNSPHead", "bert_model.BertForNextSentencePrediction.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForNextSentencePrediction", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyNSPHead", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForNextSentencePrediction.forward": [[905, 916], ["bert_model.BertForNextSentencePrediction.bert", "bert_model.BertForNextSentencePrediction.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "bert_model.BertForNextSentencePrediction.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "next_sentence_label", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "seq_relationship_score", "=", "self", ".", "cls", "(", "pooled_output", ")", "\n", "\n", "if", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "next_sentence_loss", "\n", "", "else", ":", "\n", "            ", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForSequenceClassification.__init__": [[963, 970], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "bert_model.BertForSequenceClassification.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForSequenceClassification.forward": [[971, 982], ["bert_model.BertForSequenceClassification.bert", "bert_model.BertForSequenceClassification.dropout", "bert_model.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "bert_model.BertForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForMultipleChoice.__init__": [[1028, 1035], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "bert_model.BertForMultipleChoice.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_choices", ")", ":", "\n", "        ", "super", "(", "BertForMultipleChoice", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_choices", "=", "num_choices", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForMultipleChoice.forward": [[1036, 1051], ["input_ids.view", "token_type_ids.view", "attention_mask.view", "bert_model.BertForMultipleChoice.bert", "bert_model.BertForMultipleChoice.dropout", "bert_model.BertForMultipleChoice.classifier", "bert_model.BertForMultipleChoice.view", "input_ids.size", "token_type_ids.size", "attention_mask.size", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "flat_input_ids", ",", "flat_token_type_ids", ",", "flat_attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_choices", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "reshaped_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForTokenClassification.__init__": [[1098, 1105], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "bert_model.BertForTokenClassification.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForTokenClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForTokenClassification.forward": [[1106, 1124], ["bert_model.BertForTokenClassification.bert", "bert_model.BertForTokenClassification.dropout", "bert_model.BertForTokenClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "bert_model.BertForTokenClassification.view", "labels.view", "bert_model.BertForTokenClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForQuestionAnswering.__init__": [[1173, 1180], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "torch.nn.Linear", "bert_model.BertForQuestionAnswering.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "# TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForQuestionAnswering.forward": [[1181, 1206], ["bert_model.BertForQuestionAnswering.bert", "bert_model.BertForQuestionAnswering.qa_outputs", "bert_model.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForDiscriminator.__init__": [[1212, 1219], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.CrossEntropyLoss", "bert_model.BertForDiscriminator.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForDiscriminator", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "discriminator", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "self", ".", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForDiscriminator.forward": [[1220, 1241], ["bert_model.BertForDiscriminator.bert", "bert_model.BertForDiscriminator.dropout", "bert_model.BertForDiscriminator.discriminator", "bert_model.BertForDiscriminator.loss_fct", "bert_model.BertForDiscriminator.view", "label_flaw.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids_flaw", "=", "None", ",", "attention_mask", "=", "None", ",", "label_flaw", "=", "None", ")", ":", "\n", "\n", "# discriminator loss", "\n", "# sequence_output_flaw: (batch_size, sequence_length, hidden_size)", "\n", "#print(\"input_ids_flaw:{}\".format(input_ids_flaw[0]))", "\n", "\n", "        ", "sequence_output_flaw", ",", "_", "=", "self", ".", "bert", "(", "input_ids_flaw", ",", "None", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "\n", "# flaw_logits: (batch_size, sequence_length, 2)", "\n", "sequence_output_flaw", "=", "self", ".", "dropout", "(", "sequence_output_flaw", ")", "\n", "flaw_logits", "=", "self", ".", "discriminator", "(", "sequence_output_flaw", ")", "\n", "\n", "#predictions = torch.argmax(flaw_logits,dim=2)", "\n", "#print(\"predictions:{}\".format(predictions[0]))", "\n", "\n", "if", "label_flaw", "is", "not", "None", ":", "\n", "            ", "flaw_loss", "=", "self", ".", "loss_fct", "(", "flaw_logits", ".", "view", "(", "-", "1", ",", "2", ")", ",", "label_flaw", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "flaw_loss", ",", "flaw_logits", "\n", "", "else", ":", "\n", "            ", "return", "flaw_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.RMSELoss.__init__": [[1244, 1247], ["torch.nn.Module.__init__", "torch.nn.MSELoss"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mse", "=", "nn", ".", "MSELoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.RMSELoss.forward": [[1248, 1250], ["torch.sqrt", "bert_model.RMSELoss.mse"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "yhat", ",", "y", ")", ":", "\n", "        ", "return", "torch", ".", "sqrt", "(", "self", ".", "mse", "(", "yhat", ",", "y", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForNgramClassification.__init__": [[1254, 1264], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "torch.nn.Linear", "torch.nn.Dropout", "bert_model.RMSELoss", "bert_model.BertForNgramClassification.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", ",", "embedding_size", ",", "max_seq_length", ",", "max_ngram_length", ")", ":", "\n", "        ", "super", "(", "BertForNgramClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "converter", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "embedding_size", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "max_ngram_length", "=", "max_ngram_length", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "self", ".", "hidden_size", "=", "config", ".", "hidden_size", "\n", "self", ".", "criterion", "=", "RMSELoss", "(", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForNgramClassification.forward": [[1265, 1288], ["bert_model.BertForNgramClassification.bert", "bert_model.BertForNgramClassification.dropout", "pooled_output.view.view.view", "bert_model.BertForNgramClassification.converter", "torch.sum", "torch.clamp", "new_mask.expand().float.expand().float.expand().float", "ngram_ids.view", "attention_mask.view", "bert_model.BertForNgramClassification.criterion", "new_mask.expand().float.expand().float.expand"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "ngram_ids", ",", "attention_mask", "=", "None", ",", "ngram_embeddings", "=", "None", ")", ":", "\n", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "ngram_ids", ".", "view", "(", "[", "-", "1", ",", "self", ".", "max_ngram_length", "]", ")", ",", "\n", "None", ",", "\n", "attention_mask", ".", "view", "(", "[", "-", "1", ",", "self", ".", "max_ngram_length", "]", ")", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "pooled_output", "=", "pooled_output", ".", "view", "(", "[", "-", "1", ",", "self", ".", "max_seq_length", ",", "self", ".", "hidden_size", "]", ")", "\n", "\n", "# logits: (batch_size, sequence_length, embedding_size)", "\n", "logits", "=", "self", ".", "converter", "(", "pooled_output", ")", "\n", "new_mask", "=", "torch", ".", "sum", "(", "attention_mask", ",", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "new_mask", "=", "torch", ".", "clamp", "(", "new_mask", ",", "min", "=", "0", ",", "max", "=", "1", ")", "\n", "shape0", "=", "new_mask", ".", "shape", "[", "0", "]", "\n", "shape1", "=", "new_mask", ".", "shape", "[", "1", "]", "\n", "new_mask", "=", "new_mask", ".", "expand", "(", "shape0", ",", "shape1", ",", "300", ")", ".", "float", "(", ")", "\n", "logits", "=", "logits", "*", "new_mask", "\n", "#print(logits)", "\n", "\n", "if", "ngram_embeddings", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "criterion", "(", "logits", ",", "ngram_embeddings", ")", "\n", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__": [[1292, 1299], ["bert_model.BertPreTrainedModel.__init__", "bert_model.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "bert_model.BertForClassifier.apply"], "methods", ["home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForClassifier", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.BertForClassifier.forward": [[1300, 1311], ["bert_model.BertForClassifier.bert", "bert_model.BertForClassifier.dropout", "bert_model.BertForClassifier.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "bert_model.BertForClassifier.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "token_type_ids", "=", "None", ",", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", ",", "logits", "\n", "", "else", ":", "\n", "            ", "return", "logits", "", "", "", "", ""]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.load_tf_weights_in_bert": [[54, 113], ["os.path.abspath", "print", "tf.train.list_variables", "zip", "print", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "print", "torch.from_numpy", "print", "print", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr"], "function", ["None"], ["def", "load_tf_weights_in_bert", "(", "model", ",", "tf_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "print", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "print", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "any", "(", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", "]", "for", "n", "in", "name", ")", ":", "\n", "            ", "print", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+_\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'_(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'kernel'", "or", "l", "[", "0", "]", "==", "'gamma'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_bias'", "or", "l", "[", "0", "]", "==", "'beta'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_weights'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "'_embeddings'", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "m_name", "==", "'kernel'", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.gelu": [[115, 122], ["torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.joey1993_bert-defender.None.bert_model.swish": [[124, 126], ["torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]]}