{"home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.DQN.DQN.__init__": [[37, 41], ["None"], "methods", ["None"], ["", "", "def", "__init__", "(", "self", ",", "config", ",", "double", "=", "False", ",", "debug", "=", "False", ")", ":", "\n", "        ", "self", ".", "__enableDebug", "=", "debug", "\n", "self", ".", "__config", "=", "config", "\n", "self", ".", "__doubleDQN", "=", "double", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.DQN.DQN.__debug": [[42, 47], ["print", "range"], "methods", ["None"], ["", "def", "__debug", "(", "self", ",", "msg", ":", "str", ",", "newlines", ":", "int", "=", "0", ")", ":", "\n", "        ", "if", "(", "self", ".", "__enableDebug", ")", ":", "\n", "            ", "print", "(", "\"{0}DQN: {1}\"", ".", "format", "(", "\n", "\"\"", ".", "join", "(", "[", "\"\\n\"", "for", "i", "in", "range", "(", "newlines", ")", "]", ")", ",", "\n", "msg", "\n", ")", ")", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.DQN.DQN.__update": [[49, 80], ["_q.xp.asarray", "_q.xp.asarray", "_q.xp.asarray", "_q.xp.asarray", "_q.xp.asarray", "chainer.functions.select_item", "chainer.functions.mean", "_q.cleargrads", "chainer.functions.mean.backward", "optimiser.update", "_q", "chainer.no_backprop_mode", "chainer.functions.huber_loss", "chainer.functions.select_item", "chainer.functions.max", "_qTarget", "chainer.functions.argmax", "_qTarget", "_q"], "methods", ["None"], ["", "", "def", "__update", "(", "self", ",", "_q", ",", "_qTarget", ",", "optimiser", ",", "samples", ",", "gamma", "=", "0.99", ")", ":", "\n", "        ", "\"\"\"Update a Q-function with given samples and a target Q-function.\"\"\"", "\n", "# self.__debug(\"Running update...\")", "\n", "\n", "currentStates", "=", "_q", ".", "xp", ".", "asarray", "(", "samples", "[", "\"states\"", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "actions", "=", "_q", ".", "xp", ".", "asarray", "(", "samples", "[", "\"actions\"", "]", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "rewards", "=", "_q", ".", "xp", ".", "asarray", "(", "samples", "[", "\"rewards\"", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "completes", "=", "_q", ".", "xp", ".", "asarray", "(", "samples", "[", "\"completes\"", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "nextState", "=", "_q", ".", "xp", ".", "asarray", "(", "samples", "[", "\"nextStates\"", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# Predicted values: Q(s,a)", "\n", "predictions", "=", "functions", ".", "select_item", "(", "_q", "(", "currentStates", ")", ",", "actions", ")", "\n", "\n", "# Target values: r + gamma * max_b Q(s',b)", "\n", "with", "chainer", ".", "no_backprop_mode", "(", ")", ":", "\n", "            ", "if", "self", ".", "__doubleDQN", ":", "\n", "                ", "_qNext", "=", "functions", ".", "select_item", "(", "\n", "_qTarget", "(", "nextState", ")", ",", "\n", "functions", ".", "argmax", "(", "_q", "(", "nextState", ")", ",", "axis", "=", "1", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "_qNext", "=", "functions", ".", "max", "(", "_qTarget", "(", "nextState", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "target", "=", "rewards", "+", "gamma", "*", "(", "1", "-", "completes", ")", "*", "_qNext", "\n", "\n", "", "loss", "=", "functions", ".", "mean", "(", "\n", "functions", ".", "huber_loss", "(", "predictions", ",", "target", ",", "delta", "=", "1.0", ",", "reduce", "=", "'no'", ")", "\n", ")", "\n", "_q", ".", "cleargrads", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimiser", ".", "update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.DQN.DQN.__generateAction": [[81, 87], ["_q.xp.asarray", "int", "chainer.no_backprop_mode", "_qValue.argmax", "_q"], "methods", ["None"], ["", "def", "__generateAction", "(", "self", ",", "_q", ",", "state", ")", ":", "\n", "        ", "\"\"\"Get a greedy action wrt a given Q-function.\"\"\"", "\n", "state", "=", "_q", ".", "xp", ".", "asarray", "(", "state", "[", "None", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "with", "chainer", ".", "no_backprop_mode", "(", ")", ":", "\n", "            ", "_qValue", "=", "_q", "(", "state", ")", ".", "data", "[", "0", "]", "\n", "", "return", "int", "(", "_qValue", ".", "argmax", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.DQN.DQN.run": [[89, 186], ["DQN.DQN.__debug", "gym.make", "hasattr", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "helpers.ReplayBuffer.ReplayBuffer", "collections.deque", "DQN.DQN.QFunction", "copy.deepcopy", "chainer.optimizers.Adam", "chainer.optimizers.Adam.setup", "DQN.DQN.__debug", "range", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__debug", "gym.make.reset", "time.time.time", "collections.deque.append", "DQN.DQN.__config.get", "DQN.DQN.__config.get", "DQN.DQN.__debug", "gym.make.step", "helpers.ReplayBuffer.ReplayBuffer.Experience", "helpers.ReplayBuffer.ReplayBuffer.add", "DQN.DQN.__config.get", "len", "max", "gym.make.action_space.sample", "DQN.DQN.__generateAction", "len", "helpers.ReplayBuffer.ReplayBuffer.randomSample", "DQN.DQN.__update", "copy.deepcopy", "numpy.mean", "numpy.interp", "numpy.random.rand", "time.time.time"], "methods", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__debug", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__debug", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__debug", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__debug", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.add", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__generateAction", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.randomSample", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.DQN.DQN.__update"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "self", ".", "__debug", "(", "self", ".", "__config", ".", "get", "(", "\"episodes\"", ")", ")", "\n", "\n", "# Build OpenAI Gym environment", "\n", "environment", "=", "gym", ".", "make", "(", "self", ".", "__config", ".", "get", "(", "\"environment\"", ")", ")", "\n", "if", "hasattr", "(", "environment", ",", "'env'", ")", ":", "\n", "            ", "environment", "=", "environment", ".", "env", "\n", "", "obvSpace", "=", "environment", ".", "observation_space", ".", "low", ".", "size", "\n", "actSpace", "=", "environment", ".", "action_space", ".", "n", "\n", "\n", "# Get parameter configuration.", "\n", "replayStartThreshold", "=", "self", ".", "__config", ".", "get", "(", "\"replay_start_threshold\"", ")", "\n", "minimumEpsilon", "=", "self", ".", "__config", ".", "get", "(", "\"minimum_epsilon\"", ")", "\n", "epsilonDecayPeriod", "=", "self", ".", "__config", ".", "get", "(", "\"epsilon_decay_period\"", ")", "# Iterations", "\n", "rewardScaling", "=", "self", ".", "__config", ".", "get", "(", "\"reward_scaling\"", ")", "\n", "minibatchSize", "=", "self", ".", "__config", ".", "get", "(", "\"minibatch_size\"", ")", "\n", "hiddenLayers", "=", "self", ".", "__config", ".", "get", "(", "\"hidden_layers\"", ")", "\n", "gamma", "=", "self", ".", "__config", ".", "get", "(", "\"gamma\"", ")", "\n", "networkUpdateFrequency", "=", "self", ".", "__config", ".", "get", "(", "\"network_update_frequency\"", ")", "\n", "maximumNumberOfSteps", "=", "self", ".", "__config", ".", "get", "(", "\"maximum_timesteps\"", ")", "\n", "\n", "# Initialise storage queues.", "\n", "replayBuffer", "=", "ReplayBuffer", "(", "capacity", "=", "10", "**", "6", ")", "\n", "episodeTotals", "=", "deque", "(", "maxlen", "=", "self", ".", "__config", ".", "get", "(", "\"episode_history_averaging\"", ")", ")", "\n", "\n", "\n", "# Build the Q function modeller and optimiser.", "\n", "_q", "=", "self", ".", "QFunction", "(", "obvSpace", ",", "actSpace", ",", "hiddenLayers", "=", "hiddenLayers", ")", "\n", "_qTarget", "=", "copy", ".", "deepcopy", "(", "_q", ")", "\n", "optimiser", "=", "optimizers", ".", "Adam", "(", "eps", "=", "self", ".", "__config", ".", "get", "(", "\"loss_rate\"", ")", ")", "\n", "optimiser", ".", "setup", "(", "_q", ")", "\n", "\n", "# Episode loop.", "\n", "iteration", "=", "0", "\n", "self", ".", "__debug", "(", "\"Starting episode loop...\"", ")", "\n", "for", "episode", "in", "range", "(", "self", ".", "__config", ".", "get", "(", "\"episodes\"", ")", ")", ":", "\n", "            ", "self", ".", "__debug", "(", "\"\\n\\n--- EPISODE {} ---\"", ".", "format", "(", "episode", ")", ")", "\n", "currentState", "=", "environment", ".", "reset", "(", ")", "\n", "episodeRewards", "=", "0", "\n", "running", "=", "True", "\n", "timestep", "=", "0", "\n", "start", "=", "time", "(", ")", "\n", "\n", "# Run an iteration of the current episode.", "\n", "while", "running", "and", "timestep", "<", "maximumNumberOfSteps", ":", "\n", "                ", "self", ".", "__debug", "(", "\"Episode {}: Timestep {}\"", ".", "format", "(", "episode", ",", "timestep", ")", ")", "\n", "# Decay the epsilon value as the episode progresses.", "\n", "epsilon", "=", "1.0", "\n", "if", "(", "len", "(", "replayBuffer", ")", ">=", "replayStartThreshold", ")", ":", "\n", "                    ", "epsilon", "=", "max", "(", "\n", "minimumEpsilon", ",", "\n", "np", ".", "interp", "(", "\n", "iteration", ",", "\n", "[", "0", ",", "epsilonDecayPeriod", "]", ",", "\n", "[", "1.0", ",", "minimumEpsilon", "]", "\n", ")", "\n", ")", "\n", "\n", "# Select action to perform.", "\n", "# Either random or greedy depending on the current epsilon value.", "\n", "", "action", "=", "environment", ".", "action_space", ".", "sample", "(", ")", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "epsilon", "else", "self", ".", "__generateAction", "(", "_q", ",", "currentState", ")", "\n", "\n", "# Execute the chosen action.", "\n", "nextState", ",", "reward", ",", "completed", ",", "_", "=", "environment", ".", "step", "(", "action", ")", "\n", "episodeRewards", "+=", "reward", "\n", "\n", "# Save the experience.", "\n", "experience", "=", "ReplayBuffer", ".", "Experience", "(", "\n", "currentState", ",", "action", ",", "reward", "*", "rewardScaling", ",", "completed", ",", "nextState", "\n", ")", "\n", "replayBuffer", ".", "add", "(", "experience", ")", "\n", "currentState", "=", "nextState", "\n", "\n", "# Sample and replay minibatch if threshold reached.", "\n", "if", "(", "len", "(", "replayBuffer", ")", ">=", "replayStartThreshold", ")", ":", "\n", "                    ", "minibatchSamples", "=", "replayBuffer", ".", "randomSample", "(", "minibatchSize", ")", "\n", "self", ".", "__update", "(", "_q", ",", "_qTarget", ",", "optimiser", ",", "minibatchSamples", ",", "gamma", "=", "gamma", ")", "\n", "\n", "\n", "# Update the target Q network.", "\n", "", "if", "iteration", "%", "networkUpdateFrequency", "==", "0", ":", "\n", "                    ", "_qTarget", "=", "copy", ".", "deepcopy", "(", "_q", ")", "\n", "\n", "", "iteration", "+=", "1", "\n", "timestep", "+=", "1", "\n", "running", "=", "not", "completed", "\n", "\n", "# Run post episode event handler.", "\n", "", "episodeTotals", ".", "append", "(", "episodeRewards", ")", "\n", "self", ".", "__config", ".", "get", "(", "\"post_episode\"", ")", "(", "{", "\n", "\"episode\"", ":", "episode", ",", "\n", "\"iteration\"", ":", "iteration", ",", "\n", "\"reward\"", ":", "episodeRewards", ",", "\n", "\"meanPreviousRewards\"", ":", "np", ".", "mean", "(", "episodeTotals", ")", ",", "\n", "\"duration\"", ":", "time", "(", ")", "-", "start", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__init__": [[259, 263], ["None"], "methods", ["None"], ["", "", "def", "__init__", "(", "self", ",", "config", ",", "double", "=", "False", ",", "debug", "=", "False", ")", ":", "\n", "        ", "self", ".", "__enableDebug", "=", "debug", "\n", "self", ".", "__config", "=", "config", "\n", "self", ".", "__doubleVDQN", "=", "double", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__debug": [[264, 269], ["print", "range"], "methods", ["None"], ["", "def", "__debug", "(", "self", ",", "msg", ":", "str", ",", "newlines", ":", "int", "=", "0", ")", ":", "\n", "        ", "if", "(", "self", ".", "__enableDebug", ")", ":", "\n", "            ", "print", "(", "\"{0}VDQN: {1}\"", ".", "format", "(", "\n", "\"\"", ".", "join", "(", "[", "\"\\n\"", "for", "i", "in", "range", "(", "newlines", ")", "]", ")", ",", "\n", "msg", "\n", ")", ")", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__generateAction": [[271, 276], ["VDQN.VDQN.__n.sample", "_q.computeValue", "numpy.argmax", "_q.computeValue.flatten"], "methods", ["None"], ["", "", "def", "__generateAction", "(", "self", ",", "_q", ",", "state", ")", ":", "\n", "        ", "noise_W", ",", "noise_b", "=", "self", ".", "__n", ".", "sample", "(", "1", ")", "\n", "_qValue", "=", "_q", ".", "computeValue", "(", "state", "[", "None", "]", ",", "noise_W", ",", "noise_b", ")", "\n", "action", "=", "np", ".", "argmax", "(", "_qValue", ".", "flatten", "(", ")", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.run": [[278, 385], ["VDQN.VDQN.__debug", "gym.make", "hasattr", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "helpers.ReplayBuffer.ReplayBuffer", "collections.deque", "collections.deque", "collections.deque", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "tensorflow.Session", "VDQN.VDQN.VariationalQFunction", "VDQN.VDQN.VariationalQFunction", "VDQN.VDQN.NormalSampler", "session.run", "VDQN.VDQN.update", "VDQN.VDQN.__debug", "range", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "tensorflow.global_variables_initializer", "VDQN.VDQN.__config.get", "VDQN.VDQN.__debug", "gym.make.reset", "time.time.time", "collections.deque.append", "tensorflow.train.AdamOptimizer", "tensorflow.train.AdamOptimizer", "VDQN.VDQN.get_shape", "VDQN.VDQN.__debug", "VDQN.VDQN.__generateAction", "gym.make.step", "gym.make.render", "helpers.ReplayBuffer.ReplayBuffer.Experience", "helpers.ReplayBuffer.ReplayBuffer.add", "VDQN.VDQN.__config.get", "VDQN.VDQN.__config.get", "len", "helpers.ReplayBuffer.ReplayBuffer.randomSample", "VDQN.VDQN.sample", "VDQN.VDQN.computeValue", "VDQN.VDQN.train", "collections.deque.append", "VDQN.VDQN.computeValue", "numpy.mean", "collections.deque.append", "VDQN.VDQN.update", "numpy.mean", "numpy.mean", "numpy.mean", "VDQN.VDQN.computeValue", "chainer.functions.select_item", "chainer.functions.max", "time.time.time", "chainer.functions.argmax", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__debug", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.drive.run", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__debug", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__debug", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__debug", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.VDQN.__generateAction", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.add", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.randomSample"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "self", ".", "__debug", "(", "self", ".", "__config", ".", "get", "(", "\"episodes\"", ")", ")", "\n", "\n", "# Build OpenAI Gym environment", "\n", "environment", "=", "gym", ".", "make", "(", "self", ".", "__config", ".", "get", "(", "\"environment\"", ")", ")", "\n", "if", "hasattr", "(", "environment", ",", "'env'", ")", ":", "\n", "            ", "environment", "=", "environment", ".", "env", "\n", "", "obvSpace", "=", "environment", ".", "observation_space", ".", "low", ".", "size", "\n", "actSpace", "=", "environment", ".", "action_space", ".", "n", "\n", "\n", "# Get parameter configuration.", "\n", "replayStartThreshold", "=", "self", ".", "__config", ".", "get", "(", "\"replay_start_threshold\"", ")", "\n", "minimumEpsilon", "=", "self", ".", "__config", ".", "get", "(", "\"minimum_epsilon\"", ")", "\n", "epsilonDecayPeriod", "=", "self", ".", "__config", ".", "get", "(", "\"epsilon_decay_period\"", ")", "# Iterations", "\n", "rewardScaling", "=", "self", ".", "__config", ".", "get", "(", "\"reward_scaling\"", ")", "\n", "minibatchSize", "=", "self", ".", "__config", ".", "get", "(", "\"minibatch_size\"", ")", "\n", "hiddenLayers", "=", "[", "self", ".", "__config", ".", "get", "(", "\"hidden_layers\"", ")", "]", "*", "2", "\n", "gamma", "=", "self", ".", "__config", ".", "get", "(", "\"gamma\"", ")", "\n", "tau", "=", "self", ".", "__config", ".", "get", "(", "\"tau\"", ")", "\n", "sigma", "=", "self", ".", "__config", ".", "get", "(", "\"sigma\"", ")", "\n", "networkUpdateFrequency", "=", "self", ".", "__config", ".", "get", "(", "\"network_update_frequency\"", ")", "\n", "maximumNumberOfSteps", "=", "self", ".", "__config", ".", "get", "(", "\"maximum_timesteps\"", ")", "\n", "\n", "# Initialise storage queues.", "\n", "replayBuffer", "=", "ReplayBuffer", "(", "capacity", "=", "10", "**", "6", ")", "\n", "episodeTotals", "=", "deque", "(", "maxlen", "=", "self", ".", "__config", ".", "get", "(", "\"episode_history_averaging\"", ")", ")", "\n", "variationalLosses", "=", "deque", "(", "maxlen", "=", "self", ".", "__config", ".", "get", "(", "\"episode_history_averaging\"", ")", ")", "\n", "bellmanLosses", "=", "deque", "(", "maxlen", "=", "self", ".", "__config", ".", "get", "(", "\"episode_history_averaging\"", ")", ")", "\n", "\n", "with", "tf", ".", "Session", "(", ")", "as", "session", ":", "\n", "            ", "_q", "=", "self", ".", "VariationalQFunction", "(", "obvSpace", ",", "actSpace", ",", "hiddenLayers", ",", "session", ",", "optimiser", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "__config", ".", "get", "(", "\"loss_rate\"", ")", ")", ",", "scope", "=", "\"primary\"", ")", "\n", "_qTarget", "=", "self", ".", "VariationalQFunction", "(", "obvSpace", ",", "actSpace", ",", "hiddenLayers", ",", "session", ",", "optimiser", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "1e-3", ")", ",", "scope", "=", "\"target\"", ")", "\n", "_n", "=", "self", ".", "NormalSampler", "(", "*", "_q", ".", "get_shape", "(", ")", ")", "\n", "self", ".", "__n", "=", "_n", "\n", "session", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n", "_qTarget", ".", "update", "(", "_q", ")", "\n", "\n", "# Episode loop.", "\n", "iteration", "=", "0", "\n", "self", ".", "__debug", "(", "\"Starting episode loop...\"", ")", "\n", "for", "episode", "in", "range", "(", "self", ".", "__config", ".", "get", "(", "\"episodes\"", ")", ")", ":", "\n", "                ", "self", ".", "__debug", "(", "\"\\n\\n--- EPISODE {} ---\"", ".", "format", "(", "episode", ")", ")", "\n", "currentState", "=", "environment", ".", "reset", "(", ")", "\n", "episodeRewards", "=", "0", "\n", "running", "=", "True", "\n", "timestep", "=", "0", "\n", "start", "=", "time", "(", ")", "\n", "\n", "# Run an iteration of the current episode.", "\n", "while", "running", "and", "timestep", "<", "maximumNumberOfSteps", ":", "\n", "                    ", "self", ".", "__debug", "(", "\"Episode {}: Timestep {}\"", ".", "format", "(", "episode", ",", "timestep", ")", ")", "\n", "\n", "# Select and execute the next action.", "\n", "action", "=", "self", ".", "__generateAction", "(", "_q", ",", "currentState", ")", "\n", "nextState", ",", "reward", ",", "completed", ",", "_", "=", "environment", ".", "step", "(", "action", ")", "\n", "environment", ".", "render", "(", ")", "\n", "episodeRewards", "+=", "reward", "\n", "\n", "# Save the experience.", "\n", "experience", "=", "ReplayBuffer", ".", "Experience", "(", "\n", "currentState", ",", "action", ",", "reward", "*", "rewardScaling", ",", "completed", ",", "nextState", "\n", ")", "\n", "replayBuffer", ".", "add", "(", "experience", ")", "\n", "currentState", "=", "nextState", "\n", "\n", "# Sample and replay minibatch if threshold reached.", "\n", "if", "(", "len", "(", "replayBuffer", ")", ">=", "replayStartThreshold", ")", ":", "\n", "                        ", "minibatch", "=", "replayBuffer", ".", "randomSample", "(", "minibatchSize", ")", "\n", "noise_W", ",", "noise_b", "=", "_n", ".", "sample", "(", "minibatchSize", ")", "\n", "\n", "_alpha", "=", "_qTarget", ".", "computeValue", "(", "minibatch", "[", "\"nextStates\"", "]", ",", "noise_W", ",", "noise_b", ")", "\n", "if", "self", ".", "__doubleVDQN", ":", "\n", "                            ", "_beta", "=", "_q", ".", "computeValue", "(", "minibatch", "[", "\"nextStates\"", "]", ",", "noise_W", ",", "noise_b", ")", "\n", "_qNext", "=", "functions", ".", "select_item", "(", "_alpha", ",", "functions", ".", "argmax", "(", "_beta", ",", "axis", "=", "1", ")", ")", "\n", "", "else", ":", "\n", "                            ", "_qNext", "=", "functions", ".", "max", "(", "_alpha", ",", "axis", "=", "1", ")", "\n", "\n", "# _qNext = np.max(_alpha, axis=1) # .array", "\n", "", "_qTargetValue", "=", "gamma", "*", "_qNext", ".", "array", "*", "(", "1", "-", "minibatch", "[", "\"completes\"", "]", ")", "+", "minibatch", "[", "\"rewards\"", "]", "\n", "_loss", "=", "_q", ".", "train", "(", "minibatch", "[", "\"states\"", "]", ",", "minibatch", "[", "\"actions\"", "]", ",", "_qTargetValue", ")", "\n", "variationalLosses", ".", "append", "(", "_loss", "[", "\"loss\"", "]", ")", "\n", "\n", "noise_W_dup", ",", "noise_b_dup", "=", "noise_W", ",", "noise_b", "\n", "_prediction", "=", "_q", ".", "computeValue", "(", "minibatch", "[", "\"states\"", "]", ",", "noise_W_dup", ",", "noise_b_dup", ")", "\n", "_predictedAction", "=", "_prediction", "[", "np", ".", "arange", "(", "minibatchSize", ")", ",", "minibatch", "[", "\"actions\"", "]", "]", "\n", "_bellmanLoss", "=", "np", ".", "mean", "(", "(", "_predictedAction", "-", "_qTargetValue", ")", "**", "2", ")", "\n", "bellmanLosses", ".", "append", "(", "_bellmanLoss", ")", "\n", "\n", "# Update the target Q network.", "\n", "", "if", "iteration", "%", "networkUpdateFrequency", "==", "0", ":", "\n", "                        ", "_qTarget", ".", "update", "(", "_q", ")", "\n", "\n", "", "iteration", "+=", "1", "\n", "timestep", "+=", "1", "\n", "running", "=", "not", "completed", "\n", "\n", "# Run post episode event handler.", "\n", "", "episodeTotals", ".", "append", "(", "episodeRewards", ")", "\n", "self", ".", "__config", ".", "get", "(", "\"post_episode\"", ")", "(", "{", "\n", "\"episode\"", ":", "episode", ",", "\n", "\"iteration\"", ":", "iteration", ",", "\n", "\"reward\"", ":", "episodeRewards", ",", "\n", "\"meanPreviousRewards\"", ":", "np", ".", "mean", "(", "episodeTotals", ")", ",", "\n", "\"duration\"", ":", "time", "(", ")", "-", "start", ",", "\n", "\"variationalLosses\"", ":", "np", ".", "mean", "(", "variationalLosses", ")", ",", "\n", "\"bellmanLosses\"", ":", "np", ".", "mean", "(", "bellmanLosses", ")", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.VDQN.handlePostEpisode": [[392, 406], ["data.get", "data.get", "data.get", "data.get", "print", "data.get", "data.get", "data.get"], "function", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get"], ["def", "handlePostEpisode", "(", "data", ",", "output", ",", "silent", "=", "False", ",", "variational", "=", "False", ")", ":", "\n", "    ", "dataline", "=", "(", "\"Episode {0} (i: {1}, {2} seconds) --- r: {3} (avg: {4}){5}\"", ".", "format", "(", "\n", "data", ".", "get", "(", "\"episode\"", ",", "\"-1\"", ")", ",", "\n", "data", ".", "get", "(", "\"iteration\"", ",", "\"-1\"", ")", ",", "\n", "\"{:.2f}\"", ".", "format", "(", "data", ".", "get", "(", "\"duration\"", ",", "-", "1", ")", ")", ",", "\n", "data", ".", "get", "(", "\"reward\"", ",", "\"-1\"", ")", ",", "\n", "data", ".", "get", "(", "\"meanPreviousRewards\"", ",", "\"-1\"", ")", ",", "\n", "\"\"", "if", "not", "variational", "else", "\" (vi: {}, bellman: {})\"", ".", "format", "(", "\n", "data", ".", "get", "(", "\"variationalLosses\"", ",", "\"-1\"", ")", ",", "data", ".", "get", "(", "\"bellmanLosses\"", ",", "\"-1\"", ")", ",", "\n", ")", ",", "\n", ")", ")", "\n", "\n", "if", "not", "silent", ":", "\n", "        ", "print", "(", "dataline", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.drive.run": [[39, 45], ["time.time", "print", "run.execute", "print", "time.time"], "function", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.run.execute"], ["def", "run", "(", "id", ",", "algorithm", ",", "env", ",", "episodes", ",", "timesteps", ",", "update_cadence", ",", "seed", ",", "lr", ",", "epsilon", ",", "gamma", ")", ":", "\n", "    ", "start", "=", "time", "(", ")", "\n", "name", "=", "\"{}: {}-{} (L: {}, G: {})\"", ".", "format", "(", "id", ",", "algorithm", ",", "env", ",", "lr", ",", "gamma", ")", "\n", "print", "(", "\"Starting {}\"", ".", "format", "(", "name", ")", ")", "\n", "execute", "(", "algorithm", ",", "env", ",", "episodes", ",", "timesteps", ",", "update_cadence", ",", "seed", ",", "lr", ",", "epsilon", ",", "gamma", "=", "gamma", ",", "silent", "=", "True", ")", "\n", "print", "(", "\"Finished {} in {} seconds\"", ".", "format", "(", "name", ",", "time", "(", ")", "-", "start", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.drive.process_alive": [[47, 50], ["proc.join", "proc.is_alive"], "function", ["None"], ["", "def", "process_alive", "(", "proc", ")", ":", "\n", "    ", "proc", ".", "join", "(", "timeout", "=", "0", ")", "\n", "return", "proc", ".", "is_alive", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.__init__": [[3, 24], ["print", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "initDict.get", "print"], "methods", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get"], ["    ", "def", "__init__", "(", "self", ",", "initDict", ")", ":", "\n", "        ", "self", ".", "__attributes", "=", "{", "\n", "\"output_path\"", ":", "initDict", ".", "get", "(", "\"output_path\"", ",", "\"/tmp/VDQN\"", ")", ",", "\n", "\"post_episode\"", ":", "initDict", ".", "get", "(", "\"post_episode\"", ",", "lambda", "x", ":", "print", "(", "x", ")", ")", ",", "\n", "\"episodes\"", ":", "initDict", ".", "get", "(", "\"episodes\"", ",", "100", ")", ",", "\n", "\"environment\"", ":", "initDict", ".", "get", "(", "\"environment\"", ",", "\"CartPole-v1\"", ")", ",", "\n", "\"loss_rate\"", ":", "initDict", ".", "get", "(", "\"loss_rate\"", ",", "1e-2", ")", ",", "\n", "\"replay_start_threshold\"", ":", "initDict", ".", "get", "(", "\"replay_start_threshold\"", ",", "500", ")", ",", "\n", "\"minimum_epsilon\"", ":", "initDict", ".", "get", "(", "\"minimum_epsilon\"", ",", "0.01", ")", ",", "\n", "\"epsilon_decay_period\"", ":", "initDict", ".", "get", "(", "\"epsilon_decay_period\"", ",", "5000", ")", ",", "\n", "\"reward_scaling\"", ":", "initDict", ".", "get", "(", "\"reward_scaling\"", ",", "1", ")", ",", "\n", "\"minibatch_size\"", ":", "initDict", ".", "get", "(", "\"minibatch_size\"", ",", "64", ")", ",", "\n", "\"hidden_layers\"", ":", "initDict", ".", "get", "(", "\"hidden_layers\"", ",", "100", ")", ",", "\n", "\"gamma\"", ":", "initDict", ".", "get", "(", "\"gamma\"", ",", "0.99", ")", ",", "\n", "\"tau\"", ":", "initDict", ".", "get", "(", "\"tau\"", ",", "1.0", ")", ",", "\n", "\"sigma\"", ":", "initDict", ".", "get", "(", "\"sigma\"", ",", "0.01", ")", ",", "\n", "\"network_update_frequency\"", ":", "initDict", ".", "get", "(", "\"network_update_frequency\"", ",", "100", ")", ",", "\n", "\"episode_history_averaging\"", ":", "initDict", ".", "get", "(", "\"episode_history_averaging\"", ",", "20", ")", ",", "\n", "\"maximum_timesteps\"", ":", "initDict", ".", "get", "(", "\"maximum_timesteps\"", ",", "1000", ")", ",", "\n", "}", "\n", "print", "(", "self", ".", "__attributes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get": [[27, 29], ["AlgorithmConfig.AlgorithmConfig.__attributes.get"], "methods", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get"], ["", "def", "get", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "__attributes", ".", "get", "(", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.run.execute": [[15, 61], ["algorithm.upper.upper", "str", "numpy.random.seed", "tensorflow.set_random_seed", "pathlib.Path", "pathlib.Path.mkdir", "pathlib.Path.absolute().as_posix", "pathlib.Path", "pathlib.Path.exists", "pathlib.Path.absolute().as_posix", "AlgorithmConfig.AlgorithmConfig", "switcher.get", "switcher.get.", "sys.exit", "pathlib.Path", "pathlib.Path.absolute", "pathlib.Path.absolute", "DQN.DQN().run", "DQN.DQN().run", "VDQN.VDQN().run", "VDQN.VDQN().run", "sys.exit", "run.handlePostEpisode", "DQN.DQN", "DQN.DQN", "VDQN.VDQN", "VDQN.VDQN"], "function", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.drive.run", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.drive.run", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.drive.run", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.drive.run", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.run.handlePostEpisode"], ["def", "execute", "(", "algorithm", ",", "env", ",", "episodes", ",", "timesteps", ",", "update_cadence", "=", "100", ",", "seed", "=", "100", ",", "lr", "=", "1e-2", ",", "epsilon", "=", "5000", ",", "gamma", "=", "0.99", ",", "silent", "=", "False", ")", ":", "\n", "# Initialise", "\n", "    ", "algorithm", "=", "algorithm", ".", "upper", "(", ")", "\n", "if", "not", "algorithm", "in", "[", "\"DQN\"", ",", "\"DDQN\"", ",", "\"VDQN\"", ",", "\"DVDQN\"", "]", ":", "\n", "        ", "sys", ".", "exit", "(", "\"Invalid algorithm\"", ")", "\n", "\n", "", "os", ".", "environ", "[", "'CHAINER_SEED'", "]", "=", "str", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "tf", ".", "set_random_seed", "(", "seed", ")", "\n", "\n", "# Logs", "\n", "output_dir", "=", "\"logs/{}/{}/l{}_g{}\"", ".", "format", "(", "\n", "algorithm", ",", "env", ",", "lr", ",", "gamma", "\n", ")", "\n", "output_path", "=", "Path", "(", "output_dir", ")", "\n", "output_path", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "output_dir_abs", "=", "output_path", ".", "absolute", "(", ")", ".", "as_posix", "(", ")", "\n", "\n", "output_file_index", "=", "0", "\n", "output_file", "=", "Path", "(", "\"{}/{}-{}\"", ".", "format", "(", "output_dir_abs", ",", "output_file_index", ",", "episodes", ")", ")", "\n", "while", "output_file", ".", "exists", "(", ")", ":", "\n", "        ", "output_file_index", "+=", "1", "\n", "output_file", "=", "Path", "(", "\"{}/{}-{}\"", ".", "format", "(", "output_dir_abs", ",", "output_file_index", ",", "episodes", ")", ")", "\n", "", "output_file_uri", "=", "output_file", ".", "absolute", "(", ")", ".", "as_posix", "(", ")", "\n", "\n", "# Build concifg object", "\n", "config", "=", "AlgorithmConfig", "(", "{", "\n", "\"output_path\"", ":", "output_dir_abs", ",", "\n", "\"episodes\"", ":", "episodes", ",", "\n", "\"environment\"", ":", "env", ",", "\n", "\"post_episode\"", ":", "lambda", "x", ":", "handlePostEpisode", "(", "x", ",", "output_file_uri", ",", "silent", ",", "variational", "=", "(", "algorithm", "in", "[", "\"VDQN\"", ",", "\"DVDQN\"", "]", ")", ")", ",", "\n", "\"maximum_timesteps\"", ":", "timesteps", ",", "\n", "\"network_update_frequency\"", ":", "update_cadence", ",", "\n", "\"epsilon_decay_period\"", ":", "epsilon", ",", "\n", "\"loss_rate\"", ":", "lr", ",", "\n", "\"gamma\"", ":", "gamma", ",", "\n", "}", ")", "\n", "\n", "switcher", "=", "{", "\n", "\"DQN\"", ":", "lambda", ":", "DQN", "(", "config", ")", ".", "run", "(", ")", ",", "\n", "\"DDQN\"", ":", "lambda", ":", "DQN", "(", "config", ",", "double", "=", "True", ")", ".", "run", "(", ")", ",", "\n", "\"VDQN\"", ":", "lambda", ":", "VDQN", "(", "config", ")", ".", "run", "(", ")", ",", "\n", "\"DVDQN\"", ":", "lambda", ":", "VDQN", "(", "config", ",", "double", "=", "True", ")", ".", "run", "(", ")", ",", "\n", "}", "\n", "func", "=", "switcher", ".", "get", "(", "algorithm", ",", "lambda", ":", "sys", ".", "exit", "(", "\"No algorithm: {}\"", ".", "format", "(", "algorithm", ")", ")", ")", "\n", "func", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.run.handlePostEpisode": [[62, 78], ["data.get", "data.get", "data.get", "data.get", "print", "open", "log.write", "data.get", "data.get", "data.get"], "function", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get", "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.AlgorithmConfig.AlgorithmConfig.get"], ["", "def", "handlePostEpisode", "(", "data", ",", "output", ",", "silent", ",", "variational", "=", "False", ")", ":", "\n", "    ", "dataline", "=", "(", "\"Episode {0} (i: {1}, {2} seconds) --- r: {3} (avg: {4}){5}\"", ".", "format", "(", "\n", "data", ".", "get", "(", "\"episode\"", ",", "\"-1\"", ")", ",", "\n", "data", ".", "get", "(", "\"iteration\"", ",", "\"-1\"", ")", ",", "\n", "\"{:.2f}\"", ".", "format", "(", "data", ".", "get", "(", "\"duration\"", ",", "-", "1", ")", ")", ",", "\n", "data", ".", "get", "(", "\"reward\"", ",", "\"-1\"", ")", ",", "\n", "data", ".", "get", "(", "\"meanPreviousRewards\"", ",", "\"-1\"", ")", ",", "\n", "\"\"", "if", "not", "variational", "else", "\" (vi: {}, bellman: {})\"", ".", "format", "(", "\n", "data", ".", "get", "(", "\"variationalLosses\"", ",", "\"-1\"", ")", ",", "data", ".", "get", "(", "\"bellmanLosses\"", ",", "\"-1\"", ")", ",", "\n", ")", ",", "\n", ")", ")", "\n", "\n", "if", "not", "silent", ":", "\n", "        ", "print", "(", "dataline", ")", "\n", "", "with", "open", "(", "output", ",", "\"a+\"", ")", "as", "log", ":", "\n", "        ", "log", ".", "write", "(", "\"{}\\n\"", ".", "format", "(", "dataline", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.run.main": [[79, 90], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "run.execute"], "function", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.None.run.execute"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "argparser", "=", "ArgumentParser", "(", "description", "=", "'VDQN/DQN Demonstrator'", ")", "\n", "argparser", ".", "add_argument", "(", "'--algorithm'", ",", "'-a'", ",", "type", "=", "str", ",", "default", "=", "'DQN'", ",", "help", "=", "'Algorithm to run'", ")", "\n", "argparser", ".", "add_argument", "(", "'--environment'", ",", "type", "=", "str", ",", "default", "=", "'CartPole-v0'", ",", "help", "=", "'OpenAI Gym Environment'", ")", "\n", "argparser", ".", "add_argument", "(", "'--episodes'", ",", "'-e'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'Duration (episodes)'", ")", "\n", "argparser", ".", "add_argument", "(", "'--timesteps'", ",", "'-t'", ",", "type", "=", "int", ",", "default", "=", "500", ",", "help", "=", "'Max episode length (iterations)'", ")", "\n", "argparser", ".", "add_argument", "(", "'--updates'", ",", "'-u'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'Active network update cadence (iterations)'", ")", "\n", "argparser", ".", "add_argument", "(", "'--lossrate'", ",", "'-l'", ",", "type", "=", "float", ",", "default", "=", "1e-2", ",", "help", "=", "'Loss rate'", ")", "\n", "argparser", ".", "add_argument", "(", "'--decay'", ",", "'-d'", ",", "type", "=", "float", ",", "default", "=", "5000", ",", "help", "=", "'Epsilon decay period (iterations)'", ")", "\n", "args", "=", "argparser", ".", "parse_args", "(", ")", "\n", "execute", "(", "args", ".", "algorithm", ",", "args", ".", "environment", ",", "args", ".", "episodes", ",", "args", ".", "timesteps", ",", "update_cadence", "=", "args", ".", "updates", ",", "lr", "=", "args", ".", "lossrate", ",", "epsilon", "=", "args", ".", "decay", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.utils.plot-combined.extract_data": [[19, 39], ["open", "f.read", "list", "patterns.append", "patterns.append", "matches.append", "zip", "float", "re.findall"], "function", ["None"], ["def", "extract_data", "(", "filename", ",", "variational", "=", "False", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "data", "=", "f", ".", "read", "(", ")", "\n", "matches", "=", "[", "]", "\n", "# Episode 8 (i: 4434, 2.56 seconds) --- r: -433.0 (avg: -492.55555555555554)", "\n", "patterns", "=", "[", "\n", "\"((?<=Episode )\\d+)\"", ",", "\n", "\"((?<=i: )\\d+(?=,))\"", ",", "\n", "\"((?<=, )\\d+.\\d+(?= seconds))\"", ",", "\n", "\"((?<=r: )\\-?\\d+.\\d+(?= ))\"", ",", "\n", "\"((?<=\\(avg: )\\-?\\d+.\\d+(?=\\)))\"", ",", "\n", "]", "\n", "if", "variational", ":", "\n", "            ", "patterns", ".", "append", "(", "\"((?<=\\(vi: )\\-?\\d+.\\d+|nan(?=,))\"", ")", "\n", "patterns", ".", "append", "(", "\"((?<=, bellman: )\\-?\\d+.\\d+|nan(?=\\)))\"", ")", "\n", "\n", "", "for", "pattern", "in", "patterns", ":", "\n", "            ", "matches", ".", "append", "(", "[", "(", "float", "(", "x", ")", "if", "x", "!=", "\"nan\"", "else", "-", "1", ")", "for", "x", "in", "re", ".", "findall", "(", "pattern", ",", "data", ")", "]", ")", "\n", "\n", "", "return", "list", "(", "zip", "(", "*", "matches", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.utils.plot-combined.generate_spline": [[40, 56], ["numpy.array", "numpy.array", "numpy.array", "numpy.linspace", "scipy.interpolate.make_interp_spline", "scipy.interpolate.make_interp_spline", "scipy.interpolate.make_interp_spline.", "scipy.interpolate.make_interp_spline.", "np.array.min", "np.array.max", "len", "len", "len", "len"], "function", ["None"], ["", "", "def", "generate_spline", "(", "xs", ",", "ys", ",", "errs", ")", ":", "\n", "    ", "if", "(", "len", "(", "xs", ")", "==", "0", "or", "len", "(", "ys", ")", "==", "0", "or", "len", "(", "errs", ")", "==", "0", ")", ":", "\n", "        ", "return", "xs", ",", "ys", ",", "errs", "\n", "\n", "", "ys", "=", "np", ".", "array", "(", "ys", ")", "\n", "err", "=", "np", ".", "array", "(", "errs", ")", "\n", "\n", "x_vals", "=", "np", ".", "array", "(", "xs", ")", "\n", "xnew", "=", "np", ".", "linspace", "(", "x_vals", ".", "min", "(", ")", ",", "x_vals", ".", "max", "(", ")", ",", "len", "(", "xs", ")", "*", "3", ")", "\n", "# print(xnew)", "\n", "spl", "=", "make_interp_spline", "(", "x_vals", ",", "ys", ",", "k", "=", "2", ")", "# type: BSpline", "\n", "spl_err", "=", "make_interp_spline", "(", "x_vals", ",", "err", ",", "k", "=", "2", ")", "# type: BSpline", "\n", "ys_smooth", "=", "spl", "(", "xnew", ")", "\n", "err_smooth", "=", "spl_err", "(", "xnew", ")", "\n", "\n", "return", "xnew", ",", "ys_smooth", ",", "err_smooth", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.utils.plot-combined.uneven_tuple_zip": [[57, 70], ["len", "len", "results.append", "results[].append"], "function", ["None"], ["", "def", "uneven_tuple_zip", "(", "*", "lists", ",", "index", "=", "1", ")", ":", "\n", "    ", "results", "=", "[", "]", "\n", "for", "_l", "in", "lists", ":", "\n", "        ", "_index", "=", "0", "\n", "for", "_e", "in", "_l", ":", "\n", "            ", "if", "(", "len", "(", "_e", ")", ">", "index", ")", ":", "\n", "                ", "if", "(", "_index", ">=", "len", "(", "results", ")", ")", ":", "\n", "                    ", "results", ".", "append", "(", "[", "_e", "[", "index", "]", "]", ")", "\n", "", "else", ":", "\n", "                    ", "results", "[", "_index", "]", ".", "append", "(", "_e", "[", "index", "]", ")", "\n", "", "", "_index", "+=", "1", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.utils.plot-combined.apply_averaging": [[71, 76], ["range", "len", "result.append", "numpy.mean", "max"], "function", ["None"], ["", "def", "apply_averaging", "(", "l", ",", "averaging", "=", "10", ")", ":", "\n", "    ", "result", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "len", "(", "l", ")", ")", ":", "\n", "        ", "result", ".", "append", "(", "np", ".", "mean", "(", "l", "[", "max", "(", "0", ",", "index", "+", "1", "-", "averaging", ")", ":", "index", "+", "1", "]", ")", ")", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.utils.plot.extract_data": [[19, 38], ["open", "f.read", "list", "patterns.append", "patterns.append", "matches.append", "zip", "float", "re.findall"], "function", ["None"], ["def", "extract_data", "(", "filename", ",", "variational", "=", "False", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "data", "=", "f", ".", "read", "(", ")", "\n", "matches", "=", "[", "]", "\n", "# Episode 8 (i: 4434, 2.56 seconds) --- r: -433.0 (avg: -492.55555555555554)", "\n", "patterns", "=", "[", "\n", "\"((?<=Episode )\\d+)\"", ",", "\n", "\"((?<=i: )\\d+(?=,))\"", ",", "\n", "\"((?<=, )\\d+.\\d+(?= seconds))\"", ",", "\n", "\"((?<=r: )\\-?\\d+.\\d+(?= ))\"", ",", "\n", "\"((?<=\\(avg: )\\-?\\d+.\\d+(?=\\)))\"", ",", "\n", "]", "\n", "if", "variational", ":", "\n", "            ", "patterns", ".", "append", "(", "\"((?<=\\(vi: )\\-?\\d+.\\d+(?=,))\"", ")", "\n", "patterns", ".", "append", "(", "\"((?<=, bellman: )\\-?\\d+.\\d+(?=\\)))\"", ")", "\n", "\n", "", "for", "pattern", "in", "patterns", ":", "\n", "            ", "matches", ".", "append", "(", "[", "float", "(", "x", ")", "for", "x", "in", "re", ".", "findall", "(", "pattern", ",", "data", ")", "]", ")", "\n", "", "return", "list", "(", "zip", "(", "*", "matches", ")", ")", "\n", "# return [float(x) for x in output[drop_first_n:(len(output)-drop_last_n)]]", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.__init__": [[15, 19], ["collections.deque"], "methods", ["None"], ["", "", "def", "__init__", "(", "self", ",", "capacity", ":", "int", "=", "100", ")", ":", "\n", "        ", "self", ".", "__capacity", "=", "capacity", "\n", "self", ".", "__current", "=", "0", "\n", "self", ".", "__queue", "=", "deque", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.__len__": [[20, 22], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__current", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.__getitem__": [[23, 25], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "arg", ")", ":", "\n", "        ", "return", "self", ".", "__queue", "[", "arg", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.__normalise": [[26, 30], ["ReplayBuffer.ReplayBuffer.__queue.popleft"], "methods", ["None"], ["", "def", "__normalise", "(", "self", ")", ":", "\n", "        ", "while", "(", "self", ".", "__current", ">", "self", ".", "__capacity", ")", ":", "\n", "            ", "self", ".", "__queue", ".", "popleft", "(", ")", "\n", "self", ".", "__current", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.add": [[31, 35], ["ReplayBuffer.ReplayBuffer.__queue.append", "ReplayBuffer.ReplayBuffer.__normalise"], "methods", ["home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.__normalise"], ["", "", "def", "add", "(", "self", ",", "experience", ":", "Experience", ")", ":", "\n", "        ", "self", ".", "__queue", ".", "append", "(", "experience", ")", "\n", "self", ".", "__current", "+=", "1", "\n", "self", ".", "__normalise", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HarriBellThomas_VDQN.helpers.ReplayBuffer.ReplayBuffer.randomSample": [[36, 50], ["random.sample", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "randomSample", "(", "self", ",", "number", ":", "int", ")", ":", "\n", "        ", "minibatch", "=", "random", ".", "sample", "(", "self", ".", "__queue", ",", "number", ")", "\n", "states", "=", "[", "experience", ".", "state", "for", "experience", "in", "minibatch", "]", "\n", "actions", "=", "[", "experience", ".", "action", "for", "experience", "in", "minibatch", "]", "\n", "rewards", "=", "[", "experience", ".", "reward", "for", "experience", "in", "minibatch", "]", "\n", "completes", "=", "[", "experience", ".", "completed", "for", "experience", "in", "minibatch", "]", "\n", "nextStates", "=", "[", "experience", ".", "nextState", "for", "experience", "in", "minibatch", "]", "\n", "\n", "return", "{", "\n", "\"states\"", ":", "np", ".", "array", "(", "states", ")", ",", "\n", "\"actions\"", ":", "np", ".", "array", "(", "actions", ")", ",", "\n", "\"rewards\"", ":", "np", ".", "array", "(", "rewards", ")", ",", "\n", "\"completes\"", ":", "np", ".", "array", "(", "completes", ")", ",", "\n", "\"nextStates\"", ":", "np", ".", "array", "(", "nextStates", ")", "\n", "}", "\n"]]}