{"home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.get_articles_features_config": [[99, 129], ["tensorflow.logging.info"], "function", ["None"], ["def", "get_articles_features_config", "(", ")", ":", "\n", "    ", "articles_features_config", "=", "{", "\n", "#Required fields", "\n", "'article_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'created_at_ts'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "#Additional metadata fields", "\n", "#'publisher_id': {'type': 'categorical', 'dtype': 'int'},", "\n", "'category_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'cardinality'", ":", "461", "}", "\n", "}", "\n", "\n", "feature_groups", "=", "{", "\n", "'category'", ":", "[", "'category_id'", "]", ",", "\n", "}", "\n", "\n", "\n", "#Disabling optional features when required", "\n", "if", "FLAGS", ".", "enabled_articles_input_features_groups", "!=", "[", "ALL_FEATURES", "]", ":", "\n", "        ", "for", "feature_group", "in", "feature_groups", ":", "\n", "            ", "if", "feature_group", "not", "in", "FLAGS", ".", "enabled_articles_input_features_groups", ":", "\n", "                ", "for", "feature", "in", "feature_groups", "[", "feature_group", "]", ":", "\n", "                    ", "del", "articles_features_config", "[", "feature", "]", "\n", "\n", "#Adding cardinality to categorical features", "\n", "#for feature_name in articles_features_config:", "\n", "#    if feature_name in acr_label_encoders and articles_features_config[feature_name]['type'] == 'categorical':", "\n", "#        articles_features_config[feature_name]['cardinality'] = len(acr_label_encoders[feature_name].classes_)", "\n", "\n", "\n", "", "", "", "", "tf", ".", "logging", ".", "info", "(", "'Article Features: {}'", ".", "format", "(", "articles_features_config", ")", ")", "\n", "return", "articles_features_config", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.load_acr_module_resources": [[131, 140], ["utils.deserialize", "tensorflow.logging.info", "pandas.read_csv", "tensorflow.logging.info", "tensorflow.gfile.Open", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["", "def", "load_acr_module_resources", "(", "articles_metadata_csv_path", ",", "articles_content_embeddings_pickle_path", ")", ":", "\n", "\n", "    ", "content_article_embeddings", "=", "deserialize", "(", "articles_content_embeddings_pickle_path", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read ACR article content embeddings: {}\"", ".", "format", "(", "content_article_embeddings", ".", "shape", ")", ")", "\n", "\n", "articles_metadata_df", "=", "pd", ".", "read_csv", "(", "tf", ".", "gfile", ".", "Open", "(", "articles_metadata_csv_path", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read ACR articles metadata: {}\"", ".", "format", "(", "len", "(", "articles_metadata_df", ")", ")", ")", "\n", "\n", "return", "articles_metadata_df", ",", "content_article_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.process_articles_metadata": [[142, 147], ["None"], "function", ["None"], ["", "def", "process_articles_metadata", "(", "articles_metadata_df", ",", "articles_features_config", ")", ":", "\n", "    ", "articles_metadata", "=", "{", "}", "\n", "for", "feature_name", "in", "articles_features_config", ":", "\n", "        ", "articles_metadata", "[", "feature_name", "]", "=", "articles_metadata_df", "[", "feature_name", "]", ".", "values", "\n", "", "return", "articles_metadata", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.get_session_features_config": [[150, 219], ["tensorflow.logging.info"], "function", ["None"], ["", "def", "get_session_features_config", "(", ")", ":", "\n", "    ", "session_features_config", "=", "{", "\n", "'single_features'", ":", "{", "\n", "#Control features", "\n", "'user_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'cardinality'", ":", "341193", "}", ",", "\n", "#####'user_id': {'type': 'categorical', 'dtype': 'bytes'},", "\n", "#####'session_id_original': {'type': 'categorical', 'dtype': 'bytes'},", "\n", "'session_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'session_start'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'session_size'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "}", ",", "\n", "'sequence_features'", ":", "{", "\n", "#Required sequence features", "\n", "'event_timestamp'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'item_clicked'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'cardinality'", ":", "364047", "}", ",", "\n", "\n", "#Device           ", "\n", "'environment'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'cardinality'", ":", "5", "}", ",", "\n", "'deviceGroup'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'cardinality'", ":", "6", "}", ",", "\n", "'os'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'cardinality'", ":", "23", "}", ",", "\n", "\n", "#Location", "\n", "'country'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'cardinality'", ":", "12", "}", ",", "\n", "'region'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'cardinality'", ":", "29", "}", ",", "\n", "#coord_lat scale is from -2.0  to 5.0", "\n", "#'coord_lat': {'type': 'numerical', 'dtype': 'float'},", "\n", "#coord_long scale is from -5.0  to 8.0", "\n", "#'coord_long': {'type': 'numerical', 'dtype': 'float'},", "\n", "\n", "#Time", "\n", "'local_hour_sin'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'float'", "}", ",", "\n", "'local_hour_cos'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'float'", "}", ",", "\n", "'local_weekday'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'float'", "}", ",", "\n", "\n", "#Referrer type", "\n", "'referrer_type'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'cardinality'", ":", "8", "}", ",", "\n", "\n", "#Implicit feedback", "\n", "#t_clicks Scale is from -0.5 to 3.5", "\n", "#'t_clicks': {'type': 'numerical', 'dtype': 'float'},", "\n", "#'t_scroll': {'type': 'numerical', 'dtype': 'float'},", "\n", "}", "\n", "}", "\n", "\n", "feature_groups", "=", "{", "\n", "'time'", ":", "[", "'local_hour_sin'", ",", "'local_hour_cos'", ",", "'local_weekday'", "]", ",", "\n", "'device'", ":", "[", "'environment'", ",", "'deviceGroup'", ",", "'os'", "]", ",", "\n", "'location'", ":", "[", "'country'", ",", "'region'", "]", ",", "\n", "'referrer'", ":", "[", "'referrer_type'", "]", "\n", "}", "\n", "\n", "\n", "#Disabling optional features when required", "\n", "if", "FLAGS", ".", "enabled_clicks_input_features_groups", "!=", "[", "ALL_FEATURES", "]", ":", "\n", "        ", "for", "feature_group", "in", "feature_groups", ":", "\n", "            ", "if", "feature_group", "not", "in", "FLAGS", ".", "enabled_clicks_input_features_groups", ":", "\n", "                ", "for", "feature", "in", "feature_groups", "[", "feature_group", "]", ":", "\n", "                    ", "del", "session_features_config", "[", "'sequence_features'", "]", "[", "feature", "]", "\n", "\n", "##Adding cardinality to categorical features", "\n", "#for feature_groups_key in session_features_config:", "\n", "#    features_group_config = session_features_config[feature_groups_key]", "\n", "#    for feature_name in features_group_config:", "\n", "#        if feature_name in nar_label_encoders and features_group_config[feature_name]['type'] == 'categorical':", "\n", "#            features_group_config[feature_name]['cardinality'] = len(nar_label_encoders[feature_name].classes_)", "\n", "\n", "", "", "", "", "tf", ".", "logging", ".", "info", "(", "'Session Features: {}'", ".", "format", "(", "session_features_config", ")", ")", "\n", "\n", "return", "session_features_config", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.get_internal_enabled_features_config": [[220, 232], ["tensorflow.logging.info", "set", "set().intersection", "set", "set"], "function", ["None"], ["", "def", "get_internal_enabled_features_config", "(", ")", ":", "\n", "    ", "VALID_INTERNAL_FEATURES", "=", "[", "'recency'", ",", "'novelty'", ",", "'article_content_embeddings'", ",", "'item_clicked_embeddings'", "]", "\n", "internal_features_config", "=", "{", "}", "\n", "enabled_features", "=", "[", "]", "\n", "if", "FLAGS", ".", "enabled_internal_features", "==", "[", "ALL_FEATURES", "]", ":", "\n", "        ", "enabled_features", "=", "set", "(", "VALID_INTERNAL_FEATURES", ")", "\n", "", "else", ":", "\n", "        ", "enabled_features", "=", "set", "(", "FLAGS", ".", "enabled_internal_features", ")", ".", "intersection", "(", "set", "(", "VALID_INTERNAL_FEATURES", ")", ")", "\n", "", "for", "feature", "in", "VALID_INTERNAL_FEATURES", ":", "\n", "        ", "internal_features_config", "[", "feature", "]", "=", "(", "feature", "in", "enabled_features", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Enabled internal features: {}'", ".", "format", "(", "enabled_features", ")", ")", "\n", "return", "internal_features_config", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.nar_module_model_fn": [[234, 333], ["nar_trainer_gcom.get_internal_enabled_features_config", "nar_model.NARModuleModel", "nar_model.ItemsStateUpdaterHook", "tensorflow.estimator.EstimatorSpec", "tensorflow.estimator.EstimatorSpec"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.get_internal_enabled_features_config"], ["", "def", "nar_module_model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "\n", "#features_input_layer = tf.feature_column.input_layer(features, params['feature_columns'])", "\n", "\n", "    ", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "negative_samples", "=", "params", "[", "'train_total_negative_samples'", "]", "\n", "negative_sample_from_buffer", "=", "params", "[", "'train_negative_samples_from_buffer'", "]", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "        ", "negative_samples", "=", "params", "[", "'eval_total_negative_samples'", "]", "\n", "negative_sample_from_buffer", "=", "params", "[", "'eval_negative_samples_from_buffer'", "]", "\n", "\n", "\n", "", "dropout_keep_prob", "=", "params", "[", "'dropout_keep_prob'", "]", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", "else", "1.0", "\n", "\n", "internal_features_config", "=", "get_internal_enabled_features_config", "(", ")", "\n", "\n", "\n", "eval_metrics_top_n", "=", "params", "[", "'eval_metrics_top_n'", "]", "\n", "\n", "model", "=", "NARModuleModel", "(", "mode", ",", "features", ",", "labels", ",", "\n", "session_features_config", "=", "params", "[", "'session_features_config'", "]", ",", "\n", "articles_features_config", "=", "params", "[", "'articles_features_config'", "]", ",", "\n", "batch_size", "=", "params", "[", "'batch_size'", "]", ",", "\n", "lr", "=", "params", "[", "'lr'", "]", ",", "\n", "keep_prob", "=", "dropout_keep_prob", ",", "\n", "negative_samples", "=", "negative_samples", ",", "\n", "negative_sample_from_buffer", "=", "negative_sample_from_buffer", ",", "\n", "reg_weight_decay", "=", "params", "[", "'reg_weight_decay'", "]", ",", "\n", "softmax_temperature", "=", "params", "[", "'softmax_temperature'", "]", ",", "\n", "articles_metadata", "=", "params", "[", "'articles_metadata'", "]", ",", "\n", "content_article_embeddings_matrix", "=", "params", "[", "'content_article_embeddings_matrix'", "]", ",", "\n", "recent_clicks_buffer_hours", "=", "params", "[", "'recent_clicks_buffer_hours'", "]", ",", "\n", "recent_clicks_buffer_max_size", "=", "params", "[", "'recent_clicks_buffer_max_size'", "]", ",", "\n", "recent_clicks_for_normalization", "=", "params", "[", "'recent_clicks_for_normalization'", "]", ",", "\n", "CAR_embedding_size", "=", "params", "[", "'CAR_embedding_size'", "]", ",", "\n", "rnn_units", "=", "params", "[", "'rnn_units'", "]", ",", "\n", "metrics_top_n", "=", "eval_metrics_top_n", ",", "\n", "plot_histograms", "=", "params", "[", "'save_histograms'", "]", ",", "\n", "novelty_reg_factor", "=", "params", "[", "'novelty_reg_factor'", "]", ",", "\n", "diversity_reg_factor", "=", "params", "[", "'diversity_reg_factor'", "]", ",", "\n", "internal_features_config", "=", "internal_features_config", ",", "\n", "eval_cold_start", "=", "params", "[", "'eval_cold_start'", "]", "\n", ")", "\n", "\n", "#Using these variables as global so that they persist across different train and eval", "\n", "global", "clicked_items_state", ",", "eval_sessions_metrics_log", ",", "sessions_negative_items_log", "\n", "\n", "eval_benchmark_classifiers", "=", "[", "]", "\n", "if", "not", "FLAGS", ".", "disable_eval_benchmarks", ":", "\n", "        ", "eval_benchmark_classifiers", "=", "[", "{", "'recommender'", ":", "RecentlyPopularRecommender", ",", "'params'", ":", "{", "}", "}", ",", "\n", "{", "'recommender'", ":", "ItemCooccurrenceRecommender", ",", "'params'", ":", "{", "}", "}", ",", "\n", "{", "'recommender'", ":", "ItemKNNRecommender", ",", "\n", "'params'", ":", "{", "'reg_lambda'", ":", "20", ",", "#Regularization. Discounts the similarity of rare items (incidental co-occurrences). ", "\n", "'alpha'", ":", "0.75", "#Balance between normalizing with the supports of the two items. 0.5 gives cosine similarity, 1.0 gives confidence (as in association rules).", "\n", "}", "}", ",", "\n", "{", "'recommender'", ":", "SessionBasedKNNRecommender", ",", "\n", "'params'", ":", "{", "'sessions_buffer_size'", ":", "3000", ",", "#Buffer size of last processed sessions", "\n", "'candidate_sessions_sample_size'", ":", "1000", ",", "#Number of candidate near sessions to sample  #1000", "\n", "'sampling_strategy'", ":", "'recent'", ",", "#(recent,random)", "\n", "'nearest_neighbor_session_for_scoring'", ":", "500", ",", "#Nearest neighbors to compute item scores   #500    ", "\n", "'similarity'", ":", "'cosine'", ",", "#(jaccard, cosine)", "\n", "'first_session_clicks_decay'", ":", "'div'", "#Decays weight of first user clicks in active session when finding neighbor sessions (same, div, linear, log, quadradic)", "\n", "}", "}", ",", "\n", "{", "'recommender'", ":", "ContentBasedRecommender", ",", "\n", "'params'", ":", "{", "'articles_metadata'", ":", "params", "[", "'articles_metadata'", "]", ",", "\n", "'content_article_embeddings_matrix'", ":", "params", "[", "'content_article_embeddings_matrix'", "]", "}", "}", ",", "\n", "{", "'recommender'", ":", "SequentialRulesRecommender", ",", "\n", "'params'", ":", "{", "'max_clicks_dist'", ":", "10", ",", "#Max number of clicks to walk back in the session from the currently viewed item. (Default value: 10) ", "\n", "'dist_between_clicks_decay'", ":", "'div'", "#Decay function for distance between two items clicks within a session (linear, same, div, log, qudratic). (Default value: div) ", "\n", "}", "}", "\n", "]", "\n", "\n", "", "hooks", "=", "[", "ItemsStateUpdaterHook", "(", "mode", ",", "model", ",", "\n", "eval_metrics_top_n", "=", "eval_metrics_top_n", ",", "\n", "clicked_items_state", "=", "clicked_items_state", ",", "\n", "eval_sessions_metrics_log", "=", "eval_sessions_metrics_log", ",", "\n", "sessions_negative_items_log", "=", "sessions_negative_items_log", ",", "\n", "sessions_chameleon_recommendations_log", "=", "sessions_chameleon_recommendations_log", ",", "\n", "content_article_embeddings_matrix", "=", "params", "[", "'content_article_embeddings_matrix'", "]", ",", "\n", "articles_metadata", "=", "params", "[", "'articles_metadata'", "]", ",", "\n", "eval_negative_sample_relevance", "=", "params", "[", "'eval_negative_sample_relevance'", "]", ",", "\n", "eval_benchmark_classifiers", "=", "eval_benchmark_classifiers", ",", "\n", "eval_metrics_by_session_position", "=", "params", "[", "'eval_metrics_by_session_position'", "]", ",", "\n", "eval_cold_start", "=", "params", "[", "'eval_cold_start'", "]", "\n", ")", "]", "\n", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "\n", "        ", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "model", ".", "total_loss", ",", "train_op", "=", "model", ".", "train", ",", "\n", "training_chief_hooks", "=", "hooks", ")", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "\n", "        ", "eval_metrics", "=", "{", "#'hitrate_at_1': (model.next_item_accuracy_at_1, model.next_item_accuracy_at_1_update_op),", "\n", "'hitrate_at_n'", ":", "(", "model", ".", "recall_at_n", ",", "model", ".", "recall_at_n_update_op", ")", ",", "\n", "'mrr_at_n'", ":", "(", "model", ".", "mrr", ",", "model", ".", "mrr_update_op", ")", ",", "\n", "#'ndcg_at_n': (model.ndcg_at_n_mean, model.ndcg_at_n_mean_update_op),                 ", "\n", "}", "\n", "\n", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "model", ".", "total_loss", ",", "eval_metric_ops", "=", "eval_metrics", ",", "\n", "evaluation_hooks", "=", "hooks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.build_estimator": [[335, 387], ["tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator"], "function", ["None"], ["", "", "def", "build_estimator", "(", "model_dir", ",", "\n", "content_article_embeddings_matrix", ",", "\n", "articles_metadata", ",", "articles_features_config", ",", "\n", "session_features_config", ")", ":", "\n", "    ", "\"\"\"Build an estimator appropriate for the given model type.\"\"\"", "\n", "\n", "#Disabling GPU (memory issues on local machine)", "\n", "#config_proto = tf.ConfigProto(device_count={'GPU': 0})", "\n", "run_config", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "tf_random_seed", "=", "RANDOM_SEED", ",", "\n", "keep_checkpoint_max", "=", "1", ",", "\n", "save_checkpoints_secs", "=", "1200", ",", "\n", "save_summary_steps", "=", "100", ",", "\n", "log_step_count_steps", "=", "100", ",", "\n", "#session_config=config_proto", "\n", ")", "\n", "\n", "estimator", "=", "tf", ".", "estimator", ".", "Estimator", "(", "\n", "config", "=", "run_config", ",", "\n", "model_dir", "=", "model_dir", ",", "\n", "model_fn", "=", "nar_module_model_fn", ",", "\n", "params", "=", "{", "\n", "'batch_size'", ":", "FLAGS", ".", "batch_size", ",", "\n", "'lr'", ":", "FLAGS", ".", "learning_rate", ",", "\n", "'dropout_keep_prob'", ":", "FLAGS", ".", "dropout_keep_prob", ",", "\n", "'reg_weight_decay'", ":", "FLAGS", ".", "reg_l2", ",", "\n", "'recent_clicks_buffer_hours'", ":", "FLAGS", ".", "recent_clicks_buffer_hours", ",", "\n", "'recent_clicks_buffer_max_size'", ":", "FLAGS", ".", "recent_clicks_buffer_max_size", ",", "\n", "'recent_clicks_for_normalization'", ":", "FLAGS", ".", "recent_clicks_for_normalization", ",", "\n", "'eval_metrics_top_n'", ":", "FLAGS", ".", "eval_metrics_top_n", ",", "\n", "'CAR_embedding_size'", ":", "FLAGS", ".", "CAR_embedding_size", ",", "\n", "'rnn_units'", ":", "FLAGS", ".", "rnn_units", ",", "\n", "'train_total_negative_samples'", ":", "FLAGS", ".", "train_total_negative_samples", ",", "\n", "'train_negative_samples_from_buffer'", ":", "FLAGS", ".", "train_negative_samples_from_buffer", ",", "\n", "'eval_total_negative_samples'", ":", "FLAGS", ".", "eval_total_negative_samples", ",", "\n", "'eval_negative_samples_from_buffer'", ":", "FLAGS", ".", "eval_negative_samples_from_buffer", ",", "\n", "'softmax_temperature'", ":", "FLAGS", ".", "softmax_temperature", ",", "\n", "'save_histograms'", ":", "FLAGS", ".", "save_histograms", ",", "\n", "'eval_metrics_by_session_position'", ":", "FLAGS", ".", "eval_metrics_by_session_position", ",", "\n", "'novelty_reg_factor'", ":", "FLAGS", ".", "novelty_reg_factor", ",", "\n", "'diversity_reg_factor'", ":", "FLAGS", ".", "diversity_reg_factor", ",", "\n", "'eval_negative_sample_relevance'", ":", "FLAGS", ".", "eval_negative_sample_relevance", ",", "\n", "'eval_cold_start'", ":", "FLAGS", ".", "eval_cold_start", ",", "\n", "\n", "#From pre-processing", "\n", "'session_features_config'", ":", "session_features_config", ",", "\n", "'articles_features_config'", ":", "articles_features_config", ",", "\n", "'articles_metadata'", ":", "articles_metadata", ",", "\n", "#From ACR module", "\n", "'content_article_embeddings_matrix'", ":", "content_article_embeddings_matrix", "\n", "}", ")", "\n", "\n", "return", "estimator", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.save_sessions_negative_items": [[390, 395], ["utils.append_lines_to_text_file", "os.path.join", "map", "json.dumps"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.append_lines_to_text_file"], ["", "def", "save_sessions_negative_items", "(", "model_output_dir", ",", "sessions_negative_items_list", ",", "output_file", "=", "'eval_sessions_negative_samples.json'", ")", ":", "\n", "    ", "append_lines_to_text_file", "(", "os", ".", "path", ".", "join", "(", "model_output_dir", ",", "output_file", ")", ",", "\n", "map", "(", "lambda", "x", ":", "json", ".", "dumps", "(", "{", "'session_id'", ":", "x", "[", "'session_id'", "]", ",", "\n", "'negative_items'", ":", "x", "[", "'negative_items'", "]", "}", ")", ",", "\n", "sessions_negative_items_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.save_sessions_chameleon_recommendations_log": [[397, 408], ["utils.append_lines_to_text_file", "os.path.join", "map", "json.dumps"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.append_lines_to_text_file"], ["", "def", "save_sessions_chameleon_recommendations_log", "(", "model_output_dir", ",", "sessions_chameleon_recommendations_log_list", ",", "\n", "eval_hour_id", ",", "output_file", "=", "'eval_chameleon_recommendations_log.json'", ")", ":", "\n", "    ", "append_lines_to_text_file", "(", "os", ".", "path", ".", "join", "(", "model_output_dir", ",", "output_file", ")", ",", "\n", "map", "(", "lambda", "x", ":", "json", ".", "dumps", "(", "{", "'eval_hour_id'", ":", "eval_hour_id", ",", "\n", "'session_id'", ":", "x", "[", "'session_id'", "]", ",", "\n", "'next_click_labels'", ":", "x", "[", "'next_click_labels'", "]", ",", "\n", "'predicted_item_ids'", ":", "x", "[", "'predicted_item_ids'", "]", ",", "\n", "'predicted_item_probs'", ":", "x", "[", "'predicted_item_probs'", "]", ",", "\n", "'predicted_item_norm_pop'", ":", "x", "[", "'predicted_item_norm_pop'", "]", "\n", "}", ")", ",", "\n", "sessions_chameleon_recommendations_log_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_gcom.main": [[418, 587], ["json.loads", "task_data.get", "print", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "nar_trainer_gcom.load_acr_module_resources", "sklearn.preprocessing.Normalizer", "sklearn.preprocessing.Normalizer.fit_transform", "nar_trainer_gcom.get_articles_features_config", "nar_trainer_gcom.process_articles_metadata", "nar_trainer_gcom.get_session_features_config", "tensorflow.logging.info", "clicked_items_state.ClickedItemsState", "nar_trainer_gcom.build_estimator", "tensorflow.logging.info", "utils.resolve_files", "tensorflow.logging.info", "time.time", "tensorflow.logging.info", "list", "range", "tensorflow.logging.info", "nar_utils.save_eval_benchmark_metrics_csv", "tensorflow.logging.info", "utils.log_elapsed_time", "os.environ.get", "json.loads.get", "len", "logging.getLogger", "tempfile.mkdtemp", "tensorflow.logging.info", "os.path.join", "os.path.join", "tensorflow.logging.info", "tensorflow.logging.info", "nar_utils.dowload_model_output_from_gcs", "list", "tensorflow.logging.info", "Exception", "utils.chunks", "tensorflow.logging.info", "build_estimator.train", "nar_trainer_gcom.save_sessions_negative_items", "nar_trainer_gcom.save_sessions_chameleon_recommendations_log", "nar_utils.upload_model_output_to_gcs", "tensorflow.logging.error", "glob.iglob", "len", "len", "tensorflow.logging.info", "build_estimator.evaluate", "tensorflow.logging.info", "nar_utils.save_eval_benchmark_metrics_csv", "len", "nar_trainer_gcom.save_sessions_negative_items", "nar_trainer_gcom.save_sessions_chameleon_recommendations_log", "tensorflow.logging.info", "nar_utils.upload_model_output_to_gcs", "datasets.prepare_dataset_iterator", "datasets.prepare_dataset_iterator"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_acr_module_resources", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.get_articles_features_config", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.process_articles_metadata", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.get_session_features_config", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.build_estimator", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.resolve_files", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.save_eval_benchmark_metrics_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.log_elapsed_time", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.dowload_model_output_from_gcs", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.chunks", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.train", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_negative_items", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_chameleon_recommendations_log", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.upload_model_output_to_gcs", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.evaluate", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.save_eval_benchmark_metrics_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_negative_items", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_chameleon_recommendations_log", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.upload_model_output_to_gcs", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.prepare_dataset_iterator", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.prepare_dataset_iterator"], ["def", "main", "(", "unused_argv", ")", ":", "\n", "    ", "try", ":", "\n", "# Capture whether it will be a single training job or a hyper parameter tuning job.", "\n", "        ", "tf_config_env", "=", "json", ".", "loads", "(", "os", ".", "environ", ".", "get", "(", "'TF_CONFIG'", ",", "'{}'", ")", ")", "\n", "task_data", "=", "tf_config_env", ".", "get", "(", "'task'", ")", "or", "{", "'type'", ":", "'master'", ",", "'index'", ":", "0", "}", "\n", "trial", "=", "task_data", ".", "get", "(", "'trial'", ")", "\n", "\n", "running_on_mlengine", "=", "(", "len", "(", "tf_config_env", ")", ">", "0", ")", "\n", "print", "(", "'Running {}'", ".", "format", "(", "'on Google ML Engine'", "if", "running_on_mlengine", "else", "'on a server/machine'", ")", ")", "\n", "\n", "#Disabling duplicate logs on console when running locally", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "propagate", "=", "running_on_mlengine", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Starting training job'", ")", "\n", "\n", "gcs_model_output_dir", "=", "FLAGS", ".", "model_dir", "\n", "#If must persist and load model ouput in a local cache (to speedup in ML Engine)", "\n", "if", "FLAGS", ".", "use_local_cache_model_dir", ":", "\n", "            ", "model_output_dir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Created local temp folder for models output: {}'", ".", "format", "(", "model_output_dir", ")", ")", "\n", "", "else", ":", "\n", "            ", "model_output_dir", "=", "gcs_model_output_dir", "\n", "\n", "", "if", "trial", "is", "not", "None", ":", "\n", "            ", "model_output_dir", "=", "os", ".", "path", ".", "join", "(", "model_output_dir", ",", "trial", ")", "\n", "gcs_model_output_dir", "=", "os", ".", "path", ".", "join", "(", "gcs_model_output_dir", ",", "trial", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"Hyperparameter Tuning - Trial {} - model_dir = {} - gcs_model_output_dir = {} \"", ".", "format", "(", "trial", ",", "model_output_dir", ",", "gcs_model_output_dir", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Will save temporary model outputs to {}'", ".", "format", "(", "model_output_dir", ")", ")", "\n", "\n", "#If should warm start training from other previously trained model", "\n", "if", "FLAGS", ".", "warmup_model_dir", "!=", "None", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "'Copying model outputs from previous job ({}) for warm start'", ".", "format", "(", "FLAGS", ".", "warmup_model_dir", ")", ")", "\n", "dowload_model_output_from_gcs", "(", "model_output_dir", ",", "\n", "gcs_model_dir", "=", "FLAGS", ".", "warmup_model_dir", ",", "\n", "files_pattern", "=", "[", "'graph.pb'", ",", "\n", "'model.ckpt-'", ",", "\n", "'checkpoint'", "]", ")", "\n", "\n", "local_files_after_download_to_debug", "=", "list", "(", "glob", ".", "iglob", "(", "\"{}/**/*\"", ".", "format", "(", "model_output_dir", ")", ",", "recursive", "=", "True", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Files copied from GCS to warm start training: {}'", ".", "format", "(", "local_files_after_download_to_debug", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Loading ACR module assets'", ")", "\n", "articles_metadata_df", ",", "content_article_embeddings_matrix", "=", "load_acr_module_resources", "(", "FLAGS", ".", "acr_module_articles_metadata_csv_path", ",", "\n", "FLAGS", ".", "acr_module_articles_content_embeddings_pickle_path", ")", "\n", "\n", "#Min-max scaling of the ACR embedding for a compatible range with other input features for NAR module", "\n", "#######content_article_embeddings_matrix = min_max_scale(content_article_embeddings_matrix, min_max_range=(-0.1,0.1))", "\n", "\n", "#Apply l2-norm by sample", "\n", "l2_normalizer_by_sample", "=", "Normalizer", "(", "norm", "=", "'l2'", ")", "\n", "content_article_embeddings_matrix", "=", "l2_normalizer_by_sample", ".", "fit_transform", "(", "content_article_embeddings_matrix", ")", "\n", "\n", "#Rescaling content features        ", "\n", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "*", "FLAGS", ".", "content_embedding_scale_factor", "\n", "\n", "\n", "articles_features_config", "=", "get_articles_features_config", "(", ")", "\n", "articles_metadata", "=", "process_articles_metadata", "(", "articles_metadata_df", ",", "articles_features_config", ")", "\n", "\n", "session_features_config", "=", "get_session_features_config", "(", ")", "\n", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Building NAR model'", ")", "\n", "global", "eval_sessions_metrics_log", ",", "clicked_items_state", ",", "sessions_negative_items_log", ",", "sessions_chameleon_recommendations_log", ",", "global_eval_hour_id", "\n", "eval_sessions_metrics_log", "=", "[", "]", "\n", "clicked_items_state", "=", "ClickedItemsState", "(", "FLAGS", ".", "recent_clicks_buffer_hours", ",", "\n", "FLAGS", ".", "recent_clicks_buffer_max_size", ",", "\n", "FLAGS", ".", "recent_clicks_for_normalization", ",", "\n", "content_article_embeddings_matrix", ".", "shape", "[", "0", "]", ")", "\n", "\n", "model", "=", "build_estimator", "(", "model_output_dir", ",", "\n", "content_article_embeddings_matrix", ",", "articles_metadata", ",", "articles_features_config", ",", "\n", "session_features_config", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Getting training file names'", ")", "\n", "train_files", "=", "resolve_files", "(", "FLAGS", ".", "train_set_path_regex", ")", "\n", "\n", "if", "FLAGS", ".", "train_files_from", ">", "FLAGS", ".", "train_files_up_to", ":", "\n", "            ", "raise", "Exception", "(", "'Final training file cannot be lower than Starting training file'", ")", "\n", "", "train_files", "=", "train_files", "[", "FLAGS", ".", "train_files_from", ":", "FLAGS", ".", "train_files_up_to", "+", "1", "]", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'{} files where the network will be trained and evaluated on, from {} to {}'", ".", "format", "(", "len", "(", "train_files", ")", ",", "train_files", "[", "0", "]", ",", "train_files", "[", "-", "1", "]", ")", ")", "\n", "\n", "start_train", "=", "time", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Starting Training Loop\"", ")", "\n", "\n", "#training_chunks_count = int(len(train_files) / float(FLAGS.training_hours_for_each_eval))", "\n", "training_files_chunks", "=", "list", "(", "chunks", "(", "train_files", ",", "FLAGS", ".", "training_hours_for_each_eval", ")", ")", "\n", "\n", "for", "chunk_id", "in", "range", "(", "0", ",", "len", "(", "training_files_chunks", ")", "-", "1", ")", ":", "\n", "\n", "            ", "training_files_chunk", "=", "training_files_chunks", "[", "chunk_id", "]", "\n", "tf", ".", "logging", ".", "info", "(", "'Training files from {} to {}'", ".", "format", "(", "training_files_chunk", "[", "0", "]", ",", "training_files_chunk", "[", "-", "1", "]", ")", ")", "\n", "model", ".", "train", "(", "input_fn", "=", "lambda", ":", "prepare_dataset_iterator", "(", "training_files_chunk", ",", "session_features_config", ",", "\n", "batch_size", "=", "FLAGS", ".", "batch_size", ",", "\n", "truncate_session_length", "=", "FLAGS", ".", "truncate_session_length", ")", ")", "\n", "\n", "if", "chunk_id", "<", "len", "(", "training_files_chunks", ")", "-", "1", ":", "\n", "#Using the first hour of next training chunck as eval", "\n", "                ", "eval_file", "=", "training_files_chunks", "[", "chunk_id", "+", "1", "]", "[", "0", "]", "\n", "tf", ".", "logging", ".", "info", "(", "'Evaluating file {}'", ".", "format", "(", "eval_file", ")", ")", "\n", "model", ".", "evaluate", "(", "input_fn", "=", "lambda", ":", "prepare_dataset_iterator", "(", "eval_file", ",", "session_features_config", ",", "\n", "batch_size", "=", "FLAGS", ".", "batch_size", ",", "\n", "truncate_session_length", "=", "FLAGS", ".", "truncate_session_length", ")", ")", "\n", "\n", "#After each number of train/eval loops", "\n", "", "if", "chunk_id", "%", "FLAGS", ".", "save_results_each_n_evals", "==", "0", ":", "\n", "                ", "tf", ".", "logging", ".", "info", "(", "'Saving eval metrics'", ")", "\n", "save_eval_benchmark_metrics_csv", "(", "eval_sessions_metrics_log", ",", "model_output_dir", ",", "\n", "training_hours_for_each_eval", "=", "FLAGS", ".", "training_hours_for_each_eval", ")", "\n", "\n", "if", "FLAGS", ".", "save_eval_sessions_negative_samples", ":", "\n", "#Flushing to disk the negative samples used to evaluate each sessions, ", "\n", "#so that benchmarks metrics outside the framework (eg. Matrix Factorization) can be comparable", "\n", "                    ", "save_sessions_negative_items", "(", "model_output_dir", ",", "sessions_negative_items_log", ")", "\n", "sessions_negative_items_log", "=", "[", "]", "\n", "\n", "", "if", "FLAGS", ".", "save_eval_sessions_recommendations", ":", "\n", "#Flushing to disk the recommended items to test re-ranking approaches (e.g. MMR)", "\n", "                    ", "save_sessions_chameleon_recommendations_log", "(", "model_output_dir", ",", "\n", "sessions_chameleon_recommendations_log", ",", "global_eval_hour_id", ")", "\n", "sessions_chameleon_recommendations_log", "=", "[", "]", "\n", "\n", "#Incrementing the eval hour id                    ", "\n", "global_eval_hour_id", "+=", "1", "\n", "\n", "\n", "#If must persist and load model ouput in a local cache (to speedup in ML Engine)", "\n", "", "if", "FLAGS", ".", "use_local_cache_model_dir", ":", "\n", "                    ", "tf", ".", "logging", ".", "info", "(", "'Uploading cached results to GCS'", ")", "\n", "upload_model_output_to_gcs", "(", "model_output_dir", ",", "gcs_model_dir", "=", "gcs_model_output_dir", ",", "\n", "#files_pattern=None)", "\n", "files_pattern", "=", "[", "#'events.out.tfevents.', ", "\n", "'.csv'", ",", "'.json'", "]", ")", "\n", "\n", "\n", "\n", "", "", "", "tf", ".", "logging", ".", "info", "(", "'Finalized Training'", ")", "\n", "\n", "save_eval_benchmark_metrics_csv", "(", "eval_sessions_metrics_log", ",", "model_output_dir", ",", "\n", "training_hours_for_each_eval", "=", "FLAGS", ".", "training_hours_for_each_eval", ")", "\n", "\n", "if", "FLAGS", ".", "save_eval_sessions_negative_samples", ":", "\n", "#Flushing to disk the negative samples used to evaluate each sessions, ", "\n", "#so that benchmarks metrics outside the framework (eg. Matrix Factorization) can be comparable", "\n", "            ", "save_sessions_negative_items", "(", "model_output_dir", ",", "sessions_negative_items_log", ")", "\n", "\n", "", "if", "FLAGS", ".", "save_eval_sessions_recommendations", ":", "\n", "#Flushing to disk the recommended items to test re-ranking approaches (e.g. MMR)", "\n", "            ", "save_sessions_chameleon_recommendations_log", "(", "model_output_dir", ",", "sessions_chameleon_recommendations_log", ",", "global_eval_hour_id", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Saved eval metrics'", ")", "\n", "\n", "#If must persist and load model ouput in a local cache (to speedup in ML Engine)", "\n", "if", "FLAGS", ".", "use_local_cache_model_dir", ":", "\n", "#Uploads all files to GCS", "\n", "            ", "upload_model_output_to_gcs", "(", "model_output_dir", ",", "gcs_model_dir", "=", "gcs_model_output_dir", ",", "\n", "files_pattern", "=", "None", ")", "\n", "\n", "\n", "", "log_elapsed_time", "(", "start_train", ",", "'Finalized TRAINING Loop'", ")", "\n", "\n", "", "except", "Exception", "as", "ex", ":", "\n", "        ", "tf", ".", "logging", ".", "error", "(", "'ERROR: {}'", ".", "format", "(", "ex", ")", ")", "\n", "raise", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.load_acr_module_resources": [[9, 18], ["utils.deserialize", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "acr_label_encoders.keys", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["def", "load_acr_module_resources", "(", "acr_module_resources_path", ")", ":", "\n", "    ", "(", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings", ")", "=", "deserialize", "(", "acr_module_resources_path", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read ACR label encoders for: {}\"", ".", "format", "(", "acr_label_encoders", ".", "keys", "(", ")", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read ACR articles metadata: {}\"", ".", "format", "(", "len", "(", "articles_metadata_df", ")", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read ACR article content embeddings: {}\"", ".", "format", "(", "content_article_embeddings", ".", "shape", ")", ")", "\n", "\n", "return", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.load_nar_module_preprocessing_resources": [[21, 30], ["utils.deserialize", "tensorflow.logging.info", "nar_label_encoders.keys"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["", "def", "load_nar_module_preprocessing_resources", "(", "nar_module_preprocessing_resources_path", ")", ":", "\n", "#{'nar_label_encoders', 'nar_standard_scalers'}", "\n", "    ", "nar_resources", "=", "deserialize", "(", "nar_module_preprocessing_resources_path", ")", "\n", "\n", "nar_label_encoders", "=", "nar_resources", "[", "'nar_label_encoders'", "]", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read NAR label encoders for: {}\"", ".", "format", "(", "nar_label_encoders", ".", "keys", "(", ")", ")", ")", "\n", "\n", "return", "nar_label_encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.save_eval_benchmark_metrics_csv": [[31, 41], ["pandas.DataFrame", "metrics_df.reset_index.reset_index", "metrics_df[].apply", "metrics_df[].apply", "os.path.join", "metrics_df.reset_index.to_csv", "int"], "function", ["None"], ["", "def", "save_eval_benchmark_metrics_csv", "(", "eval_sessions_metrics_log", ",", "output_dir", ",", "\n", "training_hours_for_each_eval", ",", "\n", "output_csv", "=", "'eval_stats_benchmarks.csv'", ")", ":", "\n", "    ", "metrics_df", "=", "pd", ".", "DataFrame", "(", "eval_sessions_metrics_log", ")", "\n", "metrics_df", "=", "metrics_df", ".", "reset_index", "(", ")", "\n", "metrics_df", "[", "'hour'", "]", "=", "metrics_df", "[", "'index'", "]", ".", "apply", "(", "lambda", "x", ":", "(", "(", "x", "+", "1", ")", "*", "training_hours_for_each_eval", ")", "%", "24", ")", "\n", "metrics_df", "[", "'day'", "]", "=", "metrics_df", "[", "'index'", "]", ".", "apply", "(", "lambda", "x", ":", "int", "(", "(", "(", "x", "+", "1", ")", "*", "training_hours_for_each_eval", ")", "/", "24", ")", ")", "\n", "\n", "csv_output_path", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "output_csv", ")", "\n", "metrics_df", ".", "to_csv", "(", "csv_output_path", ",", "index", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.upload_model_output_to_gcs": [[42, 59], ["re.search", "tensorflow.logging.info", "gcs_utils.upload_local_dir_to_gcs", "tensorflow.logging.info", "re.search.group", "gcs_model_dir.replace", "re.search.group", "Exception"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.upload_local_dir_to_gcs"], ["", "def", "upload_model_output_to_gcs", "(", "local_dir_path", ",", "gcs_model_dir", ",", "files_pattern", "=", "None", ")", ":", "\n", "    ", "re_search", "=", "re", ".", "search", "(", "r'gs://([a-z0-9_]+)/'", ",", "gcs_model_dir", ")", "\n", "if", "re_search", ":", "\n", "#Removing bucket prefix", "\n", "        ", "bucket_prefix", "=", "re_search", ".", "group", "(", "0", ")", "\n", "gcs_relative_path", "=", "gcs_model_dir", ".", "replace", "(", "bucket_prefix", ",", "''", ")", "\n", "\n", "bucket_name", "=", "re_search", ".", "group", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Invalid model dir. Expected a GCS path: {}'", ".", "format", "(", "gcs_model_dir", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Uploading model local cached output files from {} to {}'", ".", "format", "(", "local_dir_path", ",", "gcs_model_dir", ")", ")", "\n", "upload_local_dir_to_gcs", "(", "local_folder_path", "=", "local_dir_path", ",", "\n", "gcs_bucket", "=", "bucket_name", ",", "\n", "gcs_relative_path", "=", "gcs_relative_path", ",", "\n", "files_pattern", "=", "files_pattern", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Finished uploading model output to GCS'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.dowload_model_output_from_gcs": [[61, 78], ["re.search", "tensorflow.logging.info", "gcs_utils.download_from_gcs_dir", "tensorflow.logging.info", "re.search.group", "gcs_model_dir.replace", "re.search.group", "Exception"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.download_from_gcs_dir"], ["", "def", "dowload_model_output_from_gcs", "(", "local_dir_path", ",", "gcs_model_dir", ",", "files_pattern", "=", "None", ")", ":", "\n", "    ", "re_search", "=", "re", ".", "search", "(", "r'gs://([a-z_]+)/'", ",", "gcs_model_dir", ")", "\n", "if", "re_search", ":", "\n", "#Removing bucket prefix", "\n", "        ", "bucket_prefix", "=", "re_search", ".", "group", "(", "0", ")", "\n", "gcs_relative_path", "=", "gcs_model_dir", ".", "replace", "(", "bucket_prefix", ",", "''", ")", "\n", "\n", "bucket_name", "=", "re_search", ".", "group", "(", "1", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Invalid model dir. Expected a GCS path: {}'", ".", "format", "(", "gcs_model_dir", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Dowloading previously trained model checkpoints to local cached from {} to {}'", ".", "format", "(", "gcs_model_dir", ",", "local_dir_path", ")", ")", "\n", "download_from_gcs_dir", "(", "local_folder_path", "=", "local_dir_path", ",", "\n", "gcs_bucket", "=", "bucket_name", ",", "\n", "gcs_relative_path", "=", "gcs_relative_path", ",", "\n", "files_pattern", "=", "files_pattern", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Finished dowloading model output from GCS'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.get_articles_features_config": [[106, 136], ["tensorflow.logging.info", "len"], "function", ["None"], ["def", "get_articles_features_config", "(", "acr_label_encoders", ")", ":", "\n", "    ", "articles_features_config", "=", "{", "\n", "#Required fields", "\n", "'article_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'created_at_ts'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "#Additional metadata fields", "\n", "'category0'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#'cardinality': 41},", "\n", "'category1'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#'cardinality': 128},       ", "\n", "'author'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#'cardinality': 112},       ", "\n", "}", "\n", "\n", "feature_groups", "=", "{", "\n", "'category'", ":", "[", "'category0'", ",", "'category1'", "]", ",", "\n", "'author'", ":", "[", "'author'", "]", ",", "\n", "}", "\n", "\n", "#Disabling optional features when required", "\n", "if", "FLAGS", ".", "enabled_articles_input_features_groups", "!=", "[", "ALL_FEATURES", "]", ":", "\n", "        ", "for", "feature_group", "in", "feature_groups", ":", "\n", "            ", "if", "feature_group", "not", "in", "FLAGS", ".", "enabled_articles_input_features_groups", ":", "\n", "                ", "for", "feature", "in", "feature_groups", "[", "feature_group", "]", ":", "\n", "                    ", "del", "articles_features_config", "[", "feature", "]", "\n", "\n", "#Adding cardinality to categorical features", "\n", "", "", "", "", "for", "feature_name", "in", "articles_features_config", ":", "\n", "        ", "if", "feature_name", "in", "acr_label_encoders", "and", "articles_features_config", "[", "feature_name", "]", "[", "'type'", "]", "==", "'categorical'", ":", "\n", "            ", "articles_features_config", "[", "feature_name", "]", "[", "'cardinality'", "]", "=", "len", "(", "acr_label_encoders", "[", "feature_name", "]", ")", "\n", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "'Article Features: {}'", ".", "format", "(", "articles_features_config", ")", ")", "\n", "return", "articles_features_config", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.process_articles_metadata": [[138, 146], ["numpy.hstack"], "function", ["None"], ["", "def", "process_articles_metadata", "(", "articles_metadata_df", ",", "articles_features_config", ")", ":", "\n", "    ", "articles_metadata", "=", "{", "}", "\n", "for", "feature_name", "in", "articles_features_config", ":", "\n", "        ", "articles_metadata", "[", "feature_name", "]", "=", "articles_metadata_df", "[", "feature_name", "]", ".", "values", "\n", "#Appending a row in the first position to correspond to the <PAD> article #", "\n", "# (so that it correspond to content_article_embeddings_matrix.shape[0])", "\n", "articles_metadata", "[", "feature_name", "]", "=", "np", ".", "hstack", "(", "[", "[", "0", "]", ",", "articles_metadata", "[", "feature_name", "]", "]", ")", "\n", "", "return", "articles_metadata", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.get_session_features_config": [[147, 212], ["tensorflow.logging.info", "len"], "function", ["None"], ["", "def", "get_session_features_config", "(", "nar_label_encoders_dict", ")", ":", "\n", "    ", "session_features_config", "=", "{", "\n", "'single_features'", ":", "{", "\n", "#Control features", "\n", "'user_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'bytes'", "}", ",", "\n", "'session_id'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'session_size'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'session_start'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "}", ",", "\n", "'sequence_features'", ":", "{", "\n", "#Required sequence features", "\n", "'event_timestamp'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'item_clicked'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#, 'cardinality': 72933},", "\n", "\n", "#Location        ", "\n", "'city'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#'cardinality': 1022}, ", "\n", "'region'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#'cardinality': 237}, ", "\n", "'country'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#'cardinality': 70}, ", "\n", "\n", "#Device", "\n", "'device'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#'cardinality': 5}, ", "\n", "'os'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#'cardinality': 10}, ", "\n", "\n", "#Time", "\n", "'local_hour_sin'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'float'", "}", ",", "\n", "'local_hour_cos'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'float'", "}", ",", "\n", "'weekday'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'float'", "}", ",", "\n", "\n", "#Referrer type", "\n", "'referrer_class'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#'cardinality': 7}}}", "\n", "\n", "#Implicit feedback", "\n", "#Needs to run NAR preprocessing again to update 'active_time_secs_by_word' feature based on the articles content extented with body (limited to 1000 tokens)", "\n", "#'active_time_secs_by_word': {'type': 'numerical', 'dtype': 'float'},", "\n", "#'user_elapsed_ms_since_last_click': {'type': 'numerical', 'dtype': 'float'},", "\n", "}", "\n", "}", "\n", "\n", "\n", "feature_groups", "=", "{", "\n", "'time'", ":", "[", "'local_hour_sin'", ",", "'local_hour_cos'", ",", "'weekday'", "]", ",", "\n", "'device'", ":", "[", "'device'", ",", "'os'", "]", ",", "\n", "'location'", ":", "[", "'country'", ",", "'region'", ",", "'city'", "]", ",", "\n", "'referrer'", ":", "[", "'referrer_class'", "]", "\n", "}", "\n", "\n", "\n", "#Disabling optional features when required", "\n", "if", "FLAGS", ".", "enabled_clicks_input_features_groups", "!=", "[", "ALL_FEATURES", "]", ":", "\n", "        ", "for", "feature_group", "in", "feature_groups", ":", "\n", "            ", "if", "feature_group", "not", "in", "FLAGS", ".", "enabled_clicks_input_features_groups", ":", "\n", "                ", "for", "feature", "in", "feature_groups", "[", "feature_group", "]", ":", "\n", "                    ", "del", "session_features_config", "[", "'sequence_features'", "]", "[", "feature", "]", "\n", "\n", "\n", "#Adding cardinality to categorical features", "\n", "", "", "", "", "for", "feature_groups_key", "in", "session_features_config", ":", "\n", "        ", "features_group_config", "=", "session_features_config", "[", "feature_groups_key", "]", "\n", "for", "feature_name", "in", "features_group_config", ":", "\n", "            ", "if", "feature_name", "in", "nar_label_encoders_dict", "and", "features_group_config", "[", "feature_name", "]", "[", "'type'", "]", "==", "'categorical'", ":", "\n", "                ", "features_group_config", "[", "feature_name", "]", "[", "'cardinality'", "]", "=", "len", "(", "nar_label_encoders_dict", "[", "feature_name", "]", ")", "\n", "\n", "", "", "", "tf", ".", "logging", ".", "info", "(", "'Session Features: {}'", ".", "format", "(", "session_features_config", ")", ")", "\n", "\n", "return", "session_features_config", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.get_internal_enabled_features_config": [[213, 225], ["tensorflow.logging.info", "set", "set().intersection", "set", "set"], "function", ["None"], ["", "def", "get_internal_enabled_features_config", "(", ")", ":", "\n", "    ", "VALID_INTERNAL_FEATURES", "=", "[", "'recency'", ",", "'novelty'", ",", "'article_content_embeddings'", ",", "'item_clicked_embeddings'", "]", "\n", "internal_features_config", "=", "{", "}", "\n", "enabled_features", "=", "[", "]", "\n", "if", "FLAGS", ".", "enabled_internal_features", "==", "[", "ALL_FEATURES", "]", ":", "\n", "        ", "enabled_features", "=", "set", "(", "VALID_INTERNAL_FEATURES", ")", "\n", "", "else", ":", "\n", "        ", "enabled_features", "=", "set", "(", "FLAGS", ".", "enabled_internal_features", ")", ".", "intersection", "(", "set", "(", "VALID_INTERNAL_FEATURES", ")", ")", "\n", "", "for", "feature", "in", "VALID_INTERNAL_FEATURES", ":", "\n", "        ", "internal_features_config", "[", "feature", "]", "=", "(", "feature", "in", "enabled_features", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Enabled internal features: {}'", ".", "format", "(", "enabled_features", ")", ")", "\n", "return", "internal_features_config", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.nar_module_model_fn": [[227, 324], ["nar_trainer_adressa.get_internal_enabled_features_config", "nar_model.NARModuleModel", "nar_model.ItemsStateUpdaterHook", "tensorflow.estimator.EstimatorSpec", "tensorflow.estimator.EstimatorSpec"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.get_internal_enabled_features_config"], ["", "def", "nar_module_model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "\n", "    ", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "negative_samples", "=", "params", "[", "'train_total_negative_samples'", "]", "\n", "negative_sample_from_buffer", "=", "params", "[", "'train_negative_samples_from_buffer'", "]", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "        ", "negative_samples", "=", "params", "[", "'eval_total_negative_samples'", "]", "\n", "negative_sample_from_buffer", "=", "params", "[", "'eval_negative_samples_from_buffer'", "]", "\n", "\n", "\n", "", "dropout_keep_prob", "=", "params", "[", "'dropout_keep_prob'", "]", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", "else", "1.0", "\n", "\n", "internal_features_config", "=", "get_internal_enabled_features_config", "(", ")", "\n", "\n", "eval_metrics_top_n", "=", "params", "[", "'eval_metrics_top_n'", "]", "\n", "\n", "model", "=", "NARModuleModel", "(", "mode", ",", "features", ",", "labels", ",", "\n", "session_features_config", "=", "params", "[", "'session_features_config'", "]", ",", "\n", "articles_features_config", "=", "params", "[", "'articles_features_config'", "]", ",", "\n", "batch_size", "=", "params", "[", "'batch_size'", "]", ",", "\n", "lr", "=", "params", "[", "'lr'", "]", ",", "\n", "keep_prob", "=", "dropout_keep_prob", ",", "\n", "negative_samples", "=", "negative_samples", ",", "\n", "negative_sample_from_buffer", "=", "negative_sample_from_buffer", ",", "\n", "reg_weight_decay", "=", "params", "[", "'reg_weight_decay'", "]", ",", "\n", "softmax_temperature", "=", "params", "[", "'softmax_temperature'", "]", ",", "\n", "articles_metadata", "=", "params", "[", "'articles_metadata'", "]", ",", "\n", "content_article_embeddings_matrix", "=", "params", "[", "'content_article_embeddings_matrix'", "]", ",", "\n", "recent_clicks_buffer_hours", "=", "params", "[", "'recent_clicks_buffer_hours'", "]", ",", "\n", "recent_clicks_buffer_max_size", "=", "params", "[", "'recent_clicks_buffer_max_size'", "]", ",", "\n", "recent_clicks_for_normalization", "=", "params", "[", "'recent_clicks_for_normalization'", "]", ",", "\n", "CAR_embedding_size", "=", "params", "[", "'CAR_embedding_size'", "]", ",", "\n", "rnn_units", "=", "params", "[", "'rnn_units'", "]", ",", "\n", "metrics_top_n", "=", "eval_metrics_top_n", ",", "\n", "plot_histograms", "=", "params", "[", "'save_histograms'", "]", ",", "\n", "novelty_reg_factor", "=", "params", "[", "'novelty_reg_factor'", "]", ",", "\n", "diversity_reg_factor", "=", "params", "[", "'diversity_reg_factor'", "]", ",", "\n", "internal_features_config", "=", "internal_features_config", ",", "\n", "eval_cold_start", "=", "params", "[", "'eval_cold_start'", "]", "\n", ")", "\n", "\n", "#Using these variables as global so that they persist across different train and eval", "\n", "global", "clicked_items_state", ",", "eval_sessions_metrics_log", ",", "sessions_negative_items_log", "\n", "\n", "eval_benchmark_classifiers", "=", "[", "]", "\n", "if", "not", "FLAGS", ".", "disable_eval_benchmarks", ":", "\n", "        ", "eval_benchmark_classifiers", "=", "[", "\n", "{", "'recommender'", ":", "RecentlyPopularRecommender", ",", "'params'", ":", "{", "}", "}", ",", "\n", "{", "'recommender'", ":", "ItemCooccurrenceRecommender", ",", "'params'", ":", "{", "}", "}", ",", "\n", "{", "'recommender'", ":", "ItemKNNRecommender", ",", "\n", "'params'", ":", "{", "'reg_lambda'", ":", "20", ",", "#Regularization. Discounts the similarity of rare items (incidental co-occurrences). ", "\n", "'alpha'", ":", "0.5", "#Balance between normalizing with the supports of the two items. 0.5 gives cosine similarity, 1.0 gives confidence (as in association rules).", "\n", "}", "}", ",", "\n", "{", "'recommender'", ":", "SessionBasedKNNRecommender", ",", "\n", "'params'", ":", "{", "'sessions_buffer_size'", ":", "3000", ",", "#Buffer size of last processed sessions", "\n", "'candidate_sessions_sample_size'", ":", "2000", ",", "#200, #Number of candidate near sessions to sample  ", "\n", "'sampling_strategy'", ":", "'recent'", ",", "#(recent,random)", "\n", "'nearest_neighbor_session_for_scoring'", ":", "500", ",", "#50 #Nearest neighbors to compute item scores    ", "\n", "'similarity'", ":", "'cosine'", ",", "#(jaccard, cosine)", "\n", "'first_session_clicks_decay'", ":", "'div'", "#Decays weight of first user clicks in active session when finding neighbor sessions (same, div, linear, log, quadradic)", "\n", "}", "}", ",", "\n", "{", "'recommender'", ":", "ContentBasedRecommender", ",", "\n", "'params'", ":", "{", "'articles_metadata'", ":", "params", "[", "'articles_metadata'", "]", ",", "\n", "'content_article_embeddings_matrix'", ":", "params", "[", "'content_article_embeddings_matrix'", "]", "}", "}", ",", "\n", "{", "'recommender'", ":", "SequentialRulesRecommender", ",", "\n", "'params'", ":", "{", "'max_clicks_dist'", ":", "10", ",", "#Max number of clicks to walk back in the session from the currently viewed item. (Default value: 10) ", "\n", "'dist_between_clicks_decay'", ":", "'div'", "#Decay function for distance between two items clicks within a session (linear, same, div, log, qudratic). (Default value: div) ", "\n", "}", "}", "\n", "]", "\n", "\n", "", "hooks", "=", "[", "ItemsStateUpdaterHook", "(", "mode", ",", "model", ",", "\n", "eval_metrics_top_n", "=", "eval_metrics_top_n", ",", "\n", "clicked_items_state", "=", "clicked_items_state", ",", "\n", "eval_sessions_metrics_log", "=", "eval_sessions_metrics_log", ",", "\n", "sessions_negative_items_log", "=", "sessions_negative_items_log", ",", "\n", "sessions_chameleon_recommendations_log", "=", "sessions_chameleon_recommendations_log", ",", "\n", "content_article_embeddings_matrix", "=", "params", "[", "'content_article_embeddings_matrix'", "]", ",", "\n", "articles_metadata", "=", "params", "[", "'articles_metadata'", "]", ",", "\n", "eval_negative_sample_relevance", "=", "params", "[", "'eval_negative_sample_relevance'", "]", ",", "\n", "eval_benchmark_classifiers", "=", "eval_benchmark_classifiers", ",", "\n", "eval_metrics_by_session_position", "=", "params", "[", "'eval_metrics_by_session_position'", "]", ",", "\n", "eval_cold_start", "=", "params", "[", "'eval_cold_start'", "]", "\n", ")", "]", "\n", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "model", ".", "total_loss", ",", "train_op", "=", "model", ".", "train", ",", "\n", "training_chief_hooks", "=", "hooks", ")", "\n", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "\n", "        ", "eval_metrics", "=", "{", "#'hitrate_at_1': (model.next_item_accuracy_at_1, model.next_item_accuracy_at_1_update_op),", "\n", "'hitrate_at_n'", ":", "(", "model", ".", "recall_at_n", ",", "model", ".", "recall_at_n_update_op", ")", ",", "\n", "'mrr_at_n'", ":", "(", "model", ".", "mrr", ",", "model", ".", "mrr_update_op", ")", ",", "\n", "#'ndcg_at_n': (model.ndcg_at_n_mean, model.ndcg_at_n_mean_update_op),                 ", "\n", "}", "\n", "\n", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "mode", ",", "loss", "=", "model", ".", "total_loss", ",", "eval_metric_ops", "=", "eval_metrics", ",", "\n", "evaluation_hooks", "=", "hooks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.build_estimator": [[326, 378], ["tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator"], "function", ["None"], ["", "", "def", "build_estimator", "(", "model_dir", ",", "\n", "content_article_embeddings_matrix", ",", "\n", "articles_metadata", ",", "articles_features_config", ",", "\n", "session_features_config", ")", ":", "\n", "    ", "\"\"\"Build an estimator appropriate for the given model type.\"\"\"", "\n", "\n", "#Disabling GPU (memory issues on local machine)", "\n", "#config_proto = tf.ConfigProto(device_count={'GPU': 0})    ", "\n", "run_config", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "tf_random_seed", "=", "RANDOM_SEED", ",", "\n", "keep_checkpoint_max", "=", "1", ",", "\n", "save_checkpoints_secs", "=", "1200", ",", "\n", "save_summary_steps", "=", "100", ",", "\n", "log_step_count_steps", "=", "100", ",", "\n", "#session_config=config_proto", "\n", ")", "\n", "\n", "estimator", "=", "tf", ".", "estimator", ".", "Estimator", "(", "\n", "config", "=", "run_config", ",", "\n", "model_dir", "=", "model_dir", ",", "\n", "model_fn", "=", "nar_module_model_fn", ",", "\n", "params", "=", "{", "\n", "'batch_size'", ":", "FLAGS", ".", "batch_size", ",", "\n", "'lr'", ":", "FLAGS", ".", "learning_rate", ",", "\n", "'dropout_keep_prob'", ":", "FLAGS", ".", "dropout_keep_prob", ",", "\n", "'reg_weight_decay'", ":", "FLAGS", ".", "reg_l2", ",", "\n", "'recent_clicks_buffer_hours'", ":", "FLAGS", ".", "recent_clicks_buffer_hours", ",", "\n", "'recent_clicks_buffer_max_size'", ":", "FLAGS", ".", "recent_clicks_buffer_max_size", ",", "\n", "'recent_clicks_for_normalization'", ":", "FLAGS", ".", "recent_clicks_for_normalization", ",", "\n", "'eval_metrics_top_n'", ":", "FLAGS", ".", "eval_metrics_top_n", ",", "\n", "'CAR_embedding_size'", ":", "FLAGS", ".", "CAR_embedding_size", ",", "\n", "'rnn_units'", ":", "FLAGS", ".", "rnn_units", ",", "\n", "'train_total_negative_samples'", ":", "FLAGS", ".", "train_total_negative_samples", ",", "\n", "'train_negative_samples_from_buffer'", ":", "FLAGS", ".", "train_negative_samples_from_buffer", ",", "\n", "'eval_total_negative_samples'", ":", "FLAGS", ".", "eval_total_negative_samples", ",", "\n", "'eval_negative_samples_from_buffer'", ":", "FLAGS", ".", "eval_negative_samples_from_buffer", ",", "\n", "'softmax_temperature'", ":", "FLAGS", ".", "softmax_temperature", ",", "\n", "'save_histograms'", ":", "FLAGS", ".", "save_histograms", ",", "\n", "'eval_metrics_by_session_position'", ":", "FLAGS", ".", "eval_metrics_by_session_position", ",", "\n", "'novelty_reg_factor'", ":", "FLAGS", ".", "novelty_reg_factor", ",", "\n", "'diversity_reg_factor'", ":", "FLAGS", ".", "diversity_reg_factor", ",", "\n", "'eval_negative_sample_relevance'", ":", "FLAGS", ".", "eval_negative_sample_relevance", ",", "\n", "'eval_cold_start'", ":", "FLAGS", ".", "eval_cold_start", ",", "\n", "\n", "#From pre-processing", "\n", "'session_features_config'", ":", "session_features_config", ",", "\n", "'articles_features_config'", ":", "articles_features_config", ",", "\n", "'articles_metadata'", ":", "articles_metadata", ",", "\n", "#From ACR module", "\n", "'content_article_embeddings_matrix'", ":", "content_article_embeddings_matrix", "\n", "}", ")", "\n", "\n", "return", "estimator", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_negative_items": [[381, 386], ["utils.append_lines_to_text_file", "os.path.join", "map", "json.dumps"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.append_lines_to_text_file"], ["", "def", "save_sessions_negative_items", "(", "model_output_dir", ",", "sessions_negative_items_list", ",", "output_file", "=", "'eval_sessions_negative_samples.json'", ")", ":", "\n", "    ", "append_lines_to_text_file", "(", "os", ".", "path", ".", "join", "(", "model_output_dir", ",", "output_file", ")", ",", "\n", "map", "(", "lambda", "x", ":", "json", ".", "dumps", "(", "{", "'session_id'", ":", "x", "[", "'session_id'", "]", ",", "\n", "'negative_items'", ":", "x", "[", "'negative_items'", "]", "}", ")", ",", "\n", "sessions_negative_items_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_chameleon_recommendations_log": [[388, 399], ["utils.append_lines_to_text_file", "os.path.join", "map", "json.dumps"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.append_lines_to_text_file"], ["", "def", "save_sessions_chameleon_recommendations_log", "(", "model_output_dir", ",", "sessions_chameleon_recommendations_log_list", ",", "\n", "eval_hour_id", ",", "output_file", "=", "'eval_chameleon_recommendations_log.json'", ")", ":", "\n", "    ", "append_lines_to_text_file", "(", "os", ".", "path", ".", "join", "(", "model_output_dir", ",", "output_file", ")", ",", "\n", "map", "(", "lambda", "x", ":", "json", ".", "dumps", "(", "{", "'eval_hour_id'", ":", "eval_hour_id", ",", "\n", "'session_id'", ":", "x", "[", "'session_id'", "]", ",", "\n", "'next_click_labels'", ":", "x", "[", "'next_click_labels'", "]", ",", "\n", "'predicted_item_ids'", ":", "x", "[", "'predicted_item_ids'", "]", ",", "\n", "'predicted_item_probs'", ":", "x", "[", "'predicted_item_probs'", "]", ",", "\n", "'predicted_item_norm_pop'", ":", "x", "[", "'predicted_item_norm_pop'", "]", "\n", "}", ")", ",", "\n", "sessions_chameleon_recommendations_log_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.main": [[409, 575], ["json.loads", "task_data.get", "print", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "nar_utils.load_acr_module_resources", "sklearn.preprocessing.Normalizer", "sklearn.preprocessing.Normalizer.fit_transform", "nar_trainer_adressa.get_articles_features_config", "nar_trainer_adressa.process_articles_metadata", "tensorflow.logging.info", "nar_utils.load_nar_module_preprocessing_resources", "nar_trainer_adressa.get_session_features_config", "tensorflow.logging.info", "clicked_items_state.ClickedItemsState", "nar_trainer_adressa.build_estimator", "tensorflow.logging.info", "utils.resolve_files", "tensorflow.logging.info", "time.time", "tensorflow.logging.info", "list", "range", "tensorflow.logging.info", "nar_utils.save_eval_benchmark_metrics_csv", "tensorflow.logging.info", "utils.log_elapsed_time", "os.environ.get", "json.loads.get", "len", "logging.getLogger", "tempfile.mkdtemp", "tensorflow.logging.info", "os.path.join", "os.path.join", "tensorflow.logging.info", "tensorflow.logging.info", "nar_utils.dowload_model_output_from_gcs", "list", "tensorflow.logging.info", "Exception", "utils.chunks", "tensorflow.logging.info", "build_estimator.train", "nar_trainer_adressa.save_sessions_negative_items", "nar_trainer_adressa.save_sessions_chameleon_recommendations_log", "nar_utils.upload_model_output_to_gcs", "tensorflow.logging.error", "glob.iglob", "len", "len", "tensorflow.logging.info", "build_estimator.evaluate", "tensorflow.logging.info", "nar_utils.save_eval_benchmark_metrics_csv", "len", "nar_trainer_adressa.save_sessions_negative_items", "nar_trainer_adressa.save_sessions_chameleon_recommendations_log", "tensorflow.logging.info", "nar_utils.upload_model_output_to_gcs", "datasets.prepare_dataset_iterator", "datasets.prepare_dataset_iterator"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_acr_module_resources", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.get_articles_features_config", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.process_articles_metadata", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.load_nar_module_preprocessing_resources", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.get_session_features_config", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.build_estimator", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.resolve_files", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.save_eval_benchmark_metrics_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.log_elapsed_time", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.dowload_model_output_from_gcs", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.chunks", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.train", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_negative_items", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_chameleon_recommendations_log", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.upload_model_output_to_gcs", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.evaluate", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.save_eval_benchmark_metrics_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_negative_items", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_trainer_adressa.save_sessions_chameleon_recommendations_log", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.upload_model_output_to_gcs", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.prepare_dataset_iterator", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.prepare_dataset_iterator"], ["def", "main", "(", "unused_argv", ")", ":", "\n", "    ", "try", ":", "\n", "# Capture whether it will be a single training job or a hyper parameter tuning job.", "\n", "        ", "tf_config_env", "=", "json", ".", "loads", "(", "os", ".", "environ", ".", "get", "(", "'TF_CONFIG'", ",", "'{}'", ")", ")", "\n", "task_data", "=", "tf_config_env", ".", "get", "(", "'task'", ")", "or", "{", "'type'", ":", "'master'", ",", "'index'", ":", "0", "}", "\n", "trial", "=", "task_data", ".", "get", "(", "'trial'", ")", "\n", "\n", "running_on_mlengine", "=", "(", "len", "(", "tf_config_env", ")", ">", "0", ")", "\n", "print", "(", "'Running {}'", ".", "format", "(", "'on Google ML Engine'", "if", "running_on_mlengine", "else", "'on a server/machine'", ")", ")", "\n", "\n", "#Disabling duplicate logs on console when running locally", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "propagate", "=", "running_on_mlengine", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Starting training job'", ")", "\n", "\n", "gcs_model_output_dir", "=", "FLAGS", ".", "model_dir", "\n", "#If must persist and load model ouput in a local cache (to speedup in ML Engine)", "\n", "if", "FLAGS", ".", "use_local_cache_model_dir", ":", "\n", "            ", "model_output_dir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Created local temp folder for models output: {}'", ".", "format", "(", "model_output_dir", ")", ")", "\n", "", "else", ":", "\n", "            ", "model_output_dir", "=", "gcs_model_output_dir", "\n", "\n", "", "if", "trial", "is", "not", "None", ":", "\n", "            ", "model_output_dir", "=", "os", ".", "path", ".", "join", "(", "model_output_dir", ",", "trial", ")", "\n", "gcs_model_output_dir", "=", "os", ".", "path", ".", "join", "(", "gcs_model_output_dir", ",", "trial", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"Hyperparameter Tuning - Trial {} - model_dir = {} - gcs_model_output_dir = {} \"", ".", "format", "(", "trial", ",", "model_output_dir", ",", "gcs_model_output_dir", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Will save temporary model outputs to {}'", ".", "format", "(", "model_output_dir", ")", ")", "\n", "\n", "#If should warm start training from other previously trained model", "\n", "if", "FLAGS", ".", "warmup_model_dir", "!=", "None", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "'Copying model outputs from previous job ({}) for warm start'", ".", "format", "(", "FLAGS", ".", "warmup_model_dir", ")", ")", "\n", "dowload_model_output_from_gcs", "(", "model_output_dir", ",", "\n", "gcs_model_dir", "=", "FLAGS", ".", "warmup_model_dir", ",", "\n", "files_pattern", "=", "[", "'graph.pb'", ",", "\n", "'model.ckpt-'", ",", "\n", "'checkpoint'", "]", ")", "\n", "\n", "local_files_after_download_to_debug", "=", "list", "(", "glob", ".", "iglob", "(", "\"{}/**/*\"", ".", "format", "(", "model_output_dir", ")", ",", "recursive", "=", "True", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Files copied from GCS to warm start training: {}'", ".", "format", "(", "local_files_after_download_to_debug", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Loading ACR module assets'", ")", "\n", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings_matrix", "=", "load_acr_module_resources", "(", "FLAGS", ".", "acr_module_resources_path", ")", "\n", "\n", "#Apply l2-norm by sample", "\n", "l2_normalizer_by_sample", "=", "Normalizer", "(", "norm", "=", "'l2'", ")", "\n", "content_article_embeddings_matrix", "=", "l2_normalizer_by_sample", ".", "fit_transform", "(", "content_article_embeddings_matrix", ")", "\n", "\n", "#Rescaling content features        ", "\n", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "*", "FLAGS", ".", "content_embedding_scale_factor", "\n", "\n", "articles_features_config", "=", "get_articles_features_config", "(", "acr_label_encoders", ")", "\n", "articles_metadata", "=", "process_articles_metadata", "(", "articles_metadata_df", ",", "articles_features_config", ")", "\n", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Loading NAR module preprocesing assets'", ")", "\n", "nar_label_encoders", "=", "load_nar_module_preprocessing_resources", "(", "FLAGS", ".", "nar_module_preprocessing_resources_path", ")", "\n", "\n", "session_features_config", "=", "get_session_features_config", "(", "nar_label_encoders", ")", "\n", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Building NAR model'", ")", "\n", "global", "eval_sessions_metrics_log", ",", "clicked_items_state", ",", "sessions_negative_items_log", ",", "sessions_chameleon_recommendations_log", ",", "global_eval_hour_id", "\n", "eval_sessions_metrics_log", "=", "[", "]", "\n", "clicked_items_state", "=", "ClickedItemsState", "(", "FLAGS", ".", "recent_clicks_buffer_hours", ",", "\n", "FLAGS", ".", "recent_clicks_buffer_max_size", ",", "\n", "FLAGS", ".", "recent_clicks_for_normalization", ",", "\n", "content_article_embeddings_matrix", ".", "shape", "[", "0", "]", ")", "\n", "model", "=", "build_estimator", "(", "model_output_dir", ",", "\n", "content_article_embeddings_matrix", ",", "articles_metadata", ",", "articles_features_config", ",", "\n", "session_features_config", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Getting training file names'", ")", "\n", "train_files", "=", "resolve_files", "(", "FLAGS", ".", "train_set_path_regex", ")", "\n", "\n", "if", "FLAGS", ".", "train_files_from", ">", "FLAGS", ".", "train_files_up_to", ":", "\n", "            ", "raise", "Exception", "(", "'Final training file cannot be lower than Starting training file'", ")", "\n", "", "train_files", "=", "train_files", "[", "FLAGS", ".", "train_files_from", ":", "FLAGS", ".", "train_files_up_to", "+", "1", "]", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'{} files where the network will be trained and evaluated on, from {} to {}'", ".", "format", "(", "len", "(", "train_files", ")", ",", "train_files", "[", "0", "]", ",", "train_files", "[", "-", "1", "]", ")", ")", "\n", "\n", "start_train", "=", "time", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Starting Training Loop\"", ")", "\n", "\n", "training_files_chunks", "=", "list", "(", "chunks", "(", "train_files", ",", "FLAGS", ".", "training_hours_for_each_eval", ")", ")", "\n", "\n", "for", "chunk_id", "in", "range", "(", "0", ",", "len", "(", "training_files_chunks", ")", "-", "1", ")", ":", "\n", "\n", "            ", "training_files_chunk", "=", "training_files_chunks", "[", "chunk_id", "]", "\n", "tf", ".", "logging", ".", "info", "(", "'Training files from {} to {}'", ".", "format", "(", "training_files_chunk", "[", "0", "]", ",", "training_files_chunk", "[", "-", "1", "]", ")", ")", "\n", "model", ".", "train", "(", "input_fn", "=", "lambda", ":", "prepare_dataset_iterator", "(", "training_files_chunk", ",", "session_features_config", ",", "\n", "batch_size", "=", "FLAGS", ".", "batch_size", ",", "\n", "truncate_session_length", "=", "FLAGS", ".", "truncate_session_length", ")", ")", "\n", "\n", "if", "chunk_id", "<", "len", "(", "training_files_chunks", ")", "-", "1", ":", "\n", "#Using the first hour of next training chunck as eval", "\n", "                ", "eval_file", "=", "training_files_chunks", "[", "chunk_id", "+", "1", "]", "[", "0", "]", "\n", "tf", ".", "logging", ".", "info", "(", "'Evaluating file {}'", ".", "format", "(", "eval_file", ")", ")", "\n", "model", ".", "evaluate", "(", "input_fn", "=", "lambda", ":", "prepare_dataset_iterator", "(", "eval_file", ",", "session_features_config", ",", "\n", "batch_size", "=", "FLAGS", ".", "batch_size", ",", "\n", "truncate_session_length", "=", "FLAGS", ".", "truncate_session_length", ")", ")", "\n", "\n", "#After each number of train/eval loops", "\n", "", "if", "chunk_id", "%", "FLAGS", ".", "save_results_each_n_evals", "==", "0", ":", "\n", "                ", "tf", ".", "logging", ".", "info", "(", "'Saving eval metrics'", ")", "\n", "save_eval_benchmark_metrics_csv", "(", "eval_sessions_metrics_log", ",", "model_output_dir", ",", "\n", "training_hours_for_each_eval", "=", "FLAGS", ".", "training_hours_for_each_eval", ")", "\n", "\n", "if", "FLAGS", ".", "save_eval_sessions_negative_samples", ":", "\n", "#Flushing to disk the negative samples used to evaluate each sessions, ", "\n", "#so that benchmarks metrics outside the framework (eg. Matrix Factorization) can be comparable", "\n", "                    ", "save_sessions_negative_items", "(", "model_output_dir", ",", "sessions_negative_items_log", ")", "\n", "sessions_negative_items_log", "=", "[", "]", "\n", "\n", "", "if", "FLAGS", ".", "save_eval_sessions_recommendations", ":", "\n", "#Flushing to disk the recommended items to test re-ranking approaches (e.g. MMR)", "\n", "                    ", "save_sessions_chameleon_recommendations_log", "(", "model_output_dir", ",", "\n", "sessions_chameleon_recommendations_log", ",", "global_eval_hour_id", ")", "\n", "sessions_chameleon_recommendations_log", "=", "[", "]", "\n", "\n", "#Incrementing the eval hour id                    ", "\n", "global_eval_hour_id", "+=", "1", "\n", "\n", "\n", "#If must persist and load model ouput in a local cache (to speedup in ML Engine)", "\n", "", "if", "FLAGS", ".", "use_local_cache_model_dir", ":", "\n", "                    ", "tf", ".", "logging", ".", "info", "(", "'Uploading cached results to GCS'", ")", "\n", "upload_model_output_to_gcs", "(", "model_output_dir", ",", "gcs_model_dir", "=", "gcs_model_output_dir", ",", "\n", "#files_pattern=None)", "\n", "files_pattern", "=", "[", "#'events.out.tfevents.', ", "\n", "'.csv'", ",", "'.json'", "]", ")", "\n", "\n", "\n", "\n", "", "", "", "tf", ".", "logging", ".", "info", "(", "'Finalized Training'", ")", "\n", "\n", "save_eval_benchmark_metrics_csv", "(", "eval_sessions_metrics_log", ",", "model_output_dir", ",", "\n", "training_hours_for_each_eval", "=", "FLAGS", ".", "training_hours_for_each_eval", ")", "\n", "\n", "if", "FLAGS", ".", "save_eval_sessions_negative_samples", ":", "\n", "#Flushing to disk the negative samples used to evaluate each sessions, ", "\n", "#so that benchmarks metrics outside the framework (eg. Matrix Factorization) can be comparable", "\n", "            ", "save_sessions_negative_items", "(", "model_output_dir", ",", "sessions_negative_items_log", ")", "\n", "\n", "", "if", "FLAGS", ".", "save_eval_sessions_recommendations", ":", "\n", "#Flushing to disk the recommended items to test re-ranking approaches (e.g. MMR)", "\n", "            ", "save_sessions_chameleon_recommendations_log", "(", "model_output_dir", ",", "sessions_chameleon_recommendations_log", ",", "global_eval_hour_id", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Saved eval metrics'", ")", "\n", "\n", "#If must persist and load model ouput in a local cache (to speedup in ML Engine)", "\n", "if", "FLAGS", ".", "use_local_cache_model_dir", ":", "\n", "#Uploads all files to GCS", "\n", "            ", "upload_model_output_to_gcs", "(", "model_output_dir", ",", "gcs_model_dir", "=", "gcs_model_output_dir", ",", "\n", "files_pattern", "=", "None", ")", "\n", "\n", "\n", "", "log_elapsed_time", "(", "start_train", ",", "'Finalized TRAINING Loop'", ")", "\n", "\n", "", "except", "Exception", "as", "ex", ":", "\n", "        ", "tf", ".", "logging", ".", "error", "(", "'ERROR: {}'", ".", "format", "(", "ex", ")", ")", "\n", "raise", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.get_dir_recursive_files": [[6, 10], ["glob.iglob", "list", "path.replace", "os.path.isfile"], "function", ["None"], ["def", "get_dir_recursive_files", "(", "base_dir", ")", ":", "\n", "    ", "recursive_files", "=", "glob", ".", "iglob", "(", "\"{}/**/*\"", ".", "format", "(", "base_dir", ")", ",", "recursive", "=", "True", ")", "\n", "relative_paths", "=", "[", "path", ".", "replace", "(", "base_dir", ",", "''", ")", "for", "path", "in", "recursive_files", "if", "os", ".", "path", ".", "isfile", "(", "path", ")", "]", "\n", "return", "list", "(", "relative_paths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.upload_local_file_to_gcs": [[11, 20], ["google.cloud.storage.Client", "storage.Client.get_bucket", "client.get_bucket.blob", "bucket.blob.upload_from_filename"], "function", ["None"], ["", "def", "upload_local_file_to_gcs", "(", "local_file_path", ",", "gcs_bucket", ",", "gcs_relative_path", ")", ":", "\n", "\t", "CHUNK_SIZE", "=", "10485760", "# 10MB", "\n", "client", "=", "storage", ".", "Client", "(", ")", "\n", "bucket", "=", "client", ".", "get_bucket", "(", "gcs_bucket", ")", "\n", "blob", "=", "bucket", ".", "blob", "(", "gcs_relative_path", ",", "\n", "chunk_size", "=", "CHUNK_SIZE", "\n", ")", "\n", "\n", "blob", ".", "upload_from_filename", "(", "local_file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.upload_local_dir_to_gcs": [[22, 41], ["gcs_utils.get_dir_recursive_files", "os.path.join", "print", "os.path.join", "tensorflow.logging.info", "gcs_utils.upload_local_file_to_gcs", "os.path.basename", "len", "list", "filter", "os.path.basename.find"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.get_dir_recursive_files", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.upload_local_file_to_gcs"], ["", "def", "upload_local_dir_to_gcs", "(", "local_folder_path", ",", "gcs_bucket", ",", "gcs_relative_path", ",", "files_pattern", "=", "None", ")", ":", "\n", "\t", "basedir", "=", "local_folder_path", "+", "\"/\"", "\n", "relative_paths", "=", "get_dir_recursive_files", "(", "basedir", ")", "\n", "for", "file_relative_path", "in", "relative_paths", ":", "\n", "#If there is a pattern to filter files, ignores files that do not match the pattern\t", "\n", "\t\t", "if", "files_pattern", "!=", "None", ":", "\n", "\t\t\t", "file_name", "=", "os", ".", "path", ".", "basename", "(", "file_relative_path", ")", "\n", "if", "len", "(", "list", "(", "filter", "(", "lambda", "p", ":", "file_name", ".", "find", "(", "p", ")", "!=", "-", "1", ",", "files_pattern", ")", ")", ")", "==", "0", ":", "\n", "\t\t\t\t", "continue", "\n", "\n", "\n", "", "", "local_file_path", "=", "os", ".", "path", ".", "join", "(", "basedir", ",", "file_relative_path", ")", "\n", "print", "(", "local_file_path", ")", "\n", "remote_file_path", "=", "os", ".", "path", ".", "join", "(", "gcs_relative_path", ",", "file_relative_path", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Uploading {} to gs://{}/{}'", ".", "format", "(", "file_relative_path", ",", "gcs_bucket", ",", "remote_file_path", ")", ")", "\n", "\n", "upload_local_file_to_gcs", "(", "local_file_path", "=", "local_file_path", ",", "\n", "gcs_bucket", "=", "gcs_bucket", ",", "\n", "gcs_relative_path", "=", "remote_file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.list_blobs_with_prefix": [[43, 73], ["google.cloud.storage.Client", "storage.Client.get_bucket", "storage_client.get_bucket.list_blobs", "list"], "function", ["None"], ["", "", "def", "list_blobs_with_prefix", "(", "bucket_name", ",", "prefix", ",", "delimiter", "=", "None", ")", ":", "\n", "    ", "\"\"\"Lists all the blobs in the bucket that begin with the prefix.\n\n    This can be used to list all blobs in a \"folder\", e.g. \"public/\".\n\n    The delimiter argument can be used to restrict the results to only the\n    \"files\" in the given \"folder\". Without the delimiter, the entire tree under\n    the prefix is returned. For example, given these blobs:\n\n        /a/1.txt\n        /a/b/2.txt\n\n    If you just specify prefix = '/a', you'll get back:\n\n        /a/1.txt\n        /a/b/2.txt\n\n    However, if you specify prefix='/a' and delimiter='/', you'll get back:\n\n        /a/1.txt\n\n    \"\"\"", "\n", "storage_client", "=", "storage", ".", "Client", "(", ")", "\n", "bucket", "=", "storage_client", ".", "get_bucket", "(", "bucket_name", ")", "\n", "\n", "blobs", "=", "bucket", ".", "list_blobs", "(", "prefix", "=", "prefix", ",", "delimiter", "=", "delimiter", ")", "\n", "\n", "\n", "remote_files", "=", "list", "(", "[", "blob", ".", "name", "for", "blob", "in", "blobs", "]", ")", "\n", "return", "remote_files", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.download_file_from_gcs": [[75, 81], ["google.cloud.storage.Client", "storage.Client.get_bucket", "client.get_bucket.blob", "bucket.blob.download_to_filename"], "function", ["None"], ["", "def", "download_file_from_gcs", "(", "local_file_path", ",", "gcs_bucket", ",", "gcs_relative_path", ")", ":", "\n", "\t", "client", "=", "storage", ".", "Client", "(", ")", "\n", "bucket", "=", "client", ".", "get_bucket", "(", "gcs_bucket", ")", "\n", "blob", "=", "bucket", ".", "blob", "(", "gcs_relative_path", ")", "\n", "\n", "blob", ".", "download_to_filename", "(", "local_file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.download_from_gcs_dir": [[83, 110], ["gcs_utils.list_blobs_with_prefix", "gcs_path.replace", "os.path.basename", "os.path.join", "os.path.dirname", "os.path.join", "tensorflow.logging.info", "gcs_utils.download_file_from_gcs", "os.path.exists", "os.makedirs", "len", "list", "filter", "os.path.basename.find"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.list_blobs_with_prefix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.gcs_utils.download_file_from_gcs"], ["", "def", "download_from_gcs_dir", "(", "local_folder_path", ",", "gcs_bucket", ",", "gcs_relative_path", ",", "files_pattern", "=", "None", ")", ":", "\n", "\t", "gcs_full_paths", "=", "list_blobs_with_prefix", "(", "gcs_bucket", ",", "prefix", "=", "gcs_relative_path", ")", "\n", "\n", "for", "gcs_path", "in", "gcs_full_paths", ":", "\n", "\t\t", "relative_path", "=", "gcs_path", ".", "replace", "(", "gcs_relative_path", ",", "''", ")", "\n", "if", "relative_path", "[", "0", "]", "==", "'/'", ":", "\n", "\t\t\t", "relative_path", "=", "relative_path", "[", "1", ":", "]", "\n", "\n", "", "file_name", "=", "os", ".", "path", ".", "basename", "(", "relative_path", ")", "\n", "#If there is a pattern to filter files, ignores files that do not match the pattern\t", "\n", "if", "files_pattern", "!=", "None", ":", "\n", "\t\t\t", "if", "len", "(", "list", "(", "filter", "(", "lambda", "p", ":", "file_name", ".", "find", "(", "p", ")", "!=", "-", "1", ",", "files_pattern", ")", ")", ")", "==", "0", ":", "\n", "\t\t\t\t", "continue", "\n", "\n", "\n", "", "", "local_file_path", "=", "os", ".", "path", ".", "join", "(", "local_folder_path", ",", "relative_path", ")", "\n", "local_file_dir", "=", "os", ".", "path", ".", "dirname", "(", "local_file_path", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "local_file_dir", ")", ":", "\n", "\t\t\t", "os", ".", "makedirs", "(", "local_file_dir", ")", "\n", "\n", "", "remote_file_path", "=", "os", ".", "path", ".", "join", "(", "gcs_relative_path", ",", "relative_path", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Downloading from gs://{}/{} to {}'", ".", "format", "(", "gcs_bucket", ",", "remote_file_path", ",", "local_file_path", ")", ")", "\n", "\n", "download_file_from_gcs", "(", "local_file_path", "=", "local_file_path", ",", "\n", "gcs_bucket", "=", "gcs_bucket", ",", "\n", "gcs_relative_path", "=", "remote_file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.tf_records_management.make_sequential_feature": [[12, 20], ["tensorflow.train.FeatureList", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Int64List", "tensorflow.train.Feature", "tensorflow.train.FloatList", "tensorflow.train.BytesList", "value.encode"], "function", ["None"], ["def", "make_sequential_feature", "(", "values", ",", "vtype", "=", "int", ")", ":", "\n", "    ", "if", "vtype", "==", "int", ":", "\n", "        ", "features", "=", "[", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "value", "]", ")", ")", "for", "value", "in", "values", "]", "\n", "", "elif", "vtype", "==", "float", ":", "\n", "        ", "features", "=", "[", "tf", ".", "train", ".", "Feature", "(", "float_list", "=", "tf", ".", "train", ".", "FloatList", "(", "value", "=", "[", "value", "]", ")", ")", "for", "value", "in", "values", "]", "\n", "", "elif", "vtype", "==", "str", ":", "\n", "        ", "features", "=", "[", "tf", ".", "train", ".", "Feature", "(", "bytes_list", "=", "tf", ".", "train", ".", "BytesList", "(", "value", "=", "[", "value", ".", "encode", "(", ")", "]", ")", ")", "for", "value", "in", "values", "]", "\n", "", "return", "tf", ".", "train", ".", "FeatureList", "(", "feature", "=", "features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.tf_records_management.save_rows_to_tf_record_file": [[22, 33], ["tensorflow.python.lib.io.tf_record.TFRecordOptions", "tensorflow.python.lib.io.tf_record.TFRecordWriter", "tf_record.TFRecordWriter.close", "sys.stdout.flush", "make_sequence_example_fn", "tf_record.TFRecordWriter.write", "make_sequence_example_fn.SerializeToString"], "function", ["None"], ["", "def", "save_rows_to_tf_record_file", "(", "rows", ",", "make_sequence_example_fn", ",", "export_filename", ")", ":", "\n", "    ", "tf_record_options", "=", "tf_record", ".", "TFRecordOptions", "(", "tf_record", ".", "TFRecordCompressionType", ".", "GZIP", ")", "\n", "\n", "tf_writer", "=", "tf_record", ".", "TFRecordWriter", "(", "export_filename", ",", "options", "=", "tf_record_options", ")", "\n", "try", ":", "\n", "        ", "for", "row", "in", "rows", ":", "\n", "            ", "seq_example", "=", "make_sequence_example_fn", "(", "row", ")", "\n", "tf_writer", ".", "write", "(", "seq_example", ".", "SerializeToString", "(", ")", ")", "\n", "", "", "finally", ":", "\n", "        ", "tf_writer", ".", "close", "(", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.tf_records_management.export_dataframe_to_tf_records": [[34, 44], ["output_path.replace", "enumerate", "utils.chunks", "print", "tf_records_management.save_rows_to_tf_record_file", "len", "output_path.replace.format"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.chunks", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.save_rows_to_tf_record_file"], ["", "", "def", "export_dataframe_to_tf_records", "(", "dataframe", ",", "make_sequence_example_fn", ",", "output_path", ",", "\n", "examples_by_file", "=", "1000", ")", ":", "\n", "    ", "export_file_template", "=", "output_path", ".", "replace", "(", "'*'", ",", "'{0:04d}'", ")", "\n", "\n", "#Exporting rows to TF record by chunks", "\n", "for", "chunk_index", ",", "df_chunk", "in", "enumerate", "(", "chunks", "(", "dataframe", ",", "examples_by_file", ")", ")", ":", "\n", "        ", "print", "(", "\"Exporting chunk {} (length: {})\"", ".", "format", "(", "chunk_index", ",", "len", "(", "df_chunk", ")", ")", ")", "\n", "save_rows_to_tf_record_file", "(", "df_chunk", ",", "\n", "make_sequence_example_fn", ",", "\n", "export_filename", "=", "export_file_template", ".", "format", "(", "chunk_index", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.__init__": [[12, 18], ["clicked_items_state.ClickedItemsState.reset_state"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.reset_state"], ["    ", "def", "__init__", "(", "self", ",", "recent_clicks_buffer_hours", ",", "recent_clicks_buffer_max_size", ",", "recent_clicks_for_normalization", ",", "num_items", ")", ":", "\n", "        ", "self", ".", "recent_clicks_buffer_hours", "=", "recent_clicks_buffer_hours", "\n", "self", ".", "recent_clicks_buffer_max_size", "=", "recent_clicks_buffer_max_size", "\n", "self", ".", "recent_clicks_for_normalization", "=", "recent_clicks_for_normalization", "\n", "self", ".", "num_items", "=", "num_items", "\n", "self", ".", "reset_state", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.reset_state": [[19, 46], ["numpy.zeros", "numpy.zeros", "clicked_items_state.ClickedItemsState._update_recent_pop_norm", "numpy.zeros", "scipy.sparse.csr_matrix", "dict", "dict", "dict", "dict", "evaluation.ColdStartAnalysisState"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState._update_recent_pop_norm"], ["", "def", "reset_state", "(", "self", ")", ":", "\n", "#Global state", "\n", "        ", "self", ".", "articles_pop", "=", "np", ".", "zeros", "(", "shape", "=", "[", "self", ".", "num_items", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "self", ".", "articles_recent_pop", "=", "np", ".", "zeros", "(", "shape", "=", "[", "self", ".", "num_items", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "_update_recent_pop_norm", "(", "self", ".", "articles_recent_pop", ")", "\n", "\n", "#Clicked buffer has two columns (article_id, click_timestamp)", "\n", "self", ".", "pop_recent_clicks_buffer", "=", "np", ".", "zeros", "(", "shape", "=", "[", "self", ".", "recent_clicks_buffer_max_size", ",", "2", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "pop_recent_buffer_article_id_column", "=", "0", "\n", "self", ".", "pop_recent_buffer_timestamp_column", "=", "1", "\n", "\n", "\n", "#State shared by ItemCooccurrenceRecommender and ItemKNNRecommender", "\n", "self", ".", "items_coocurrences", "=", "csr_matrix", "(", "(", "self", ".", "num_items", ",", "self", ".", "num_items", ")", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "#States specific for benchmarks", "\n", "self", ".", "benchmarks_states", "=", "dict", "(", ")", "\n", "\n", "#Stores the timestamp of the first click in the item", "\n", "self", ".", "items_first_click_ts", "=", "dict", "(", ")", "\n", "#Stores the delay (in minutes) from item's first click to item's first recommendation from CHAMELEON", "\n", "self", ".", "items_delay_for_first_recommendation", "=", "dict", "(", ")", "\n", "\n", "self", ".", "current_step", "=", "0", "\n", "self", ".", "items_first_click_step", "=", "dict", "(", ")", "\n", "self", ".", "cold_start_state", "=", "ColdStartAnalysisState", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.save_state_checkpoint": [[49, 60], ["numpy.copy", "numpy.copy", "scipy.sparse.csr_matrix.copy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy"], "methods", ["None"], ["", "def", "save_state_checkpoint", "(", "self", ")", ":", "\n", "        ", "self", ".", "articles_pop_chkp", "=", "np", ".", "copy", "(", "self", ".", "articles_pop", ")", "\n", "self", ".", "pop_recent_clicks_buffer_chkp", "=", "np", ".", "copy", "(", "self", ".", "pop_recent_clicks_buffer", ")", "\n", "self", ".", "items_coocurrences_chkp", "=", "csr_matrix", ".", "copy", "(", "self", ".", "items_coocurrences", ")", "\n", "self", ".", "benchmarks_states_chkp", "=", "deepcopy", "(", "self", ".", "benchmarks_states", ")", "\n", "self", ".", "items_first_click_ts_chkp", "=", "deepcopy", "(", "self", ".", "items_first_click_ts", ")", "\n", "self", ".", "items_delay_for_first_recommendation_chkp", "=", "deepcopy", "(", "self", ".", "items_delay_for_first_recommendation", ")", "\n", "\n", "self", ".", "items_first_click_step_chkp", "=", "deepcopy", "(", "self", ".", "items_first_click_step", ")", "\n", "self", ".", "cold_start_state_chkp", "=", "deepcopy", "(", "self", ".", "cold_start_state", ")", "\n", "self", ".", "current_step_chkp", "=", "self", ".", "current_step", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.restore_state_checkpoint": [[61, 80], ["None"], "methods", ["None"], ["", "def", "restore_state_checkpoint", "(", "self", ")", ":", "\n", "        ", "self", ".", "articles_pop", "=", "self", ".", "articles_pop_chkp", "\n", "del", "self", ".", "articles_pop_chkp", "\n", "self", ".", "pop_recent_clicks_buffer", "=", "self", ".", "pop_recent_clicks_buffer_chkp", "\n", "del", "self", ".", "pop_recent_clicks_buffer_chkp", "\n", "self", ".", "items_coocurrences", "=", "self", ".", "items_coocurrences_chkp", "\n", "del", "self", ".", "items_coocurrences_chkp", "\n", "self", ".", "items_first_click_ts", "=", "self", ".", "items_first_click_ts_chkp", "\n", "del", "self", ".", "items_first_click_ts_chkp", "\n", "self", ".", "items_delay_for_first_recommendation", "=", "self", ".", "items_delay_for_first_recommendation_chkp", "\n", "del", "self", ".", "items_delay_for_first_recommendation_chkp", "\n", "self", ".", "benchmarks_states", "=", "self", ".", "benchmarks_states_chkp", "\n", "del", "self", ".", "benchmarks_states_chkp", "\n", "\n", "self", ".", "items_first_click_step", "=", "self", ".", "items_first_click_step_chkp", "\n", "del", "self", ".", "items_first_click_step_chkp", "\n", "self", ".", "cold_start_state", "=", "self", ".", "cold_start_state_chkp", "\n", "del", "self", ".", "cold_start_state_chkp", "\n", "self", ".", "current_step", "=", "self", ".", "current_step_chkp", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_pop": [[81, 83], ["None"], "methods", ["None"], ["", "def", "get_articles_pop", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "articles_pop", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop": [[84, 86], ["None"], "methods", ["None"], ["", "def", "get_articles_recent_pop", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "articles_recent_pop", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm": [[87, 89], ["None"], "methods", ["None"], ["", "def", "get_articles_recent_pop_norm", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "articles_recent_pop_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_recent_clicks_buffer": [[90, 93], ["None"], "methods", ["None"], ["", "def", "get_recent_clicks_buffer", "(", "self", ")", ":", "\n", "#Returns only the first column (article_id)", "\n", "        ", "return", "self", ".", "pop_recent_clicks_buffer", "[", ":", ",", "self", ".", "pop_recent_buffer_article_id_column", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_items_coocurrences": [[94, 96], ["None"], "methods", ["None"], ["", "def", "get_items_coocurrences", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "items_coocurrences", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.increment_current_step": [[97, 99], ["None"], "methods", ["None"], ["", "def", "increment_current_step", "(", "self", ")", ":", "\n", "        ", "self", ".", "current_step", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_current_step": [[100, 102], ["None"], "methods", ["None"], ["", "def", "get_current_step", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "current_step", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_cold_start_state": [[103, 105], ["None"], "methods", ["None"], ["", "def", "get_cold_start_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cold_start_state", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_max_timestamp_recent_clicks": [[107, 109], ["numpy.max"], "methods", ["None"], ["", "def", "get_max_timestamp_recent_clicks", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "max", "(", "self", ".", "pop_recent_clicks_buffer", "[", ":", ",", "self", ".", "pop_recent_buffer_timestamp_column", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.update_items_first_click_ts": [[111, 123], ["batch_clicked_items.reshape", "batch_clicked_timestamps.reshape", "sorted", "zip", "tf.logging.warn"], "methods", ["None"], ["", "def", "update_items_first_click_ts", "(", "self", ",", "batch_clicked_items", ",", "batch_clicked_timestamps", ")", ":", "\n", "\n", "        ", "batch_item_ids", "=", "batch_clicked_items", ".", "reshape", "(", "-", "1", ")", "\n", "batch_clicks_timestamp", "=", "batch_clicked_timestamps", ".", "reshape", "(", "-", "1", ")", "\n", "sorted_item_clicks", "=", "sorted", "(", "zip", "(", "batch_clicks_timestamp", ",", "batch_item_ids", ")", ")", "\n", "\n", "for", "click_ts", ",", "item_id", "in", "sorted_item_clicks", ":", "\n", "            ", "if", "item_id", "!=", "0", "and", "click_ts", "==", "0", ":", "\n", "                ", "tf", ".", "logging", ".", "warn", "(", "'Item {} has timestamp {}. Original clicked_items: {}. Original timestamps: {}'", ".", "format", "(", "item_id", ",", "click_ts", ",", "batch_clicked_items", ",", "batch_clicked_timestamps", ")", ")", "\n", "#Ignoring padded items", "\n", "", "elif", "item_id", "!=", "0", "and", "(", "not", "item_id", "in", "self", ".", "items_first_click_ts", "or", "click_ts", "<", "self", ".", "items_first_click_ts", "[", "item_id", "]", ")", ":", "\n", "                ", "self", ".", "items_first_click_ts", "[", "item_id", "]", "=", "click_ts", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.update_items_state": [[187, 194], ["clicked_items_state.ClickedItemsState._update_recently_clicked_items_buffer", "clicked_items_state.ClickedItemsState._update_recent_pop_items", "clicked_items_state.ClickedItemsState._update_pop_items"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState._update_recently_clicked_items_buffer", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState._update_recent_pop_items", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState._update_pop_items"], ["def", "update_items_state", "(", "self", ",", "batch_clicked_items", ",", "batch_clicked_timestamps", ")", ":", "\n", "#batch_items_nonzero = self._get_non_zero_items_vector(batch_clicked_items)", "\n", "\n", "        ", "self", ".", "_update_recently_clicked_items_buffer", "(", "batch_clicked_items", ",", "batch_clicked_timestamps", ")", "\n", "self", ".", "_update_recent_pop_items", "(", ")", "\n", "\n", "self", ".", "_update_pop_items", "(", "batch_clicked_items", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.update_items_first_click_step": [[196, 204], ["set().difference", "set", "set", "clicked_items_state.ClickedItemsState.get_current_step"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_current_step"], ["", "def", "update_items_first_click_step", "(", "self", ",", "batch_clicked_items", ")", ":", "\n", "\n", "#Getting the unique clicked items in the batch", "\n", "        ", "batch_clicked_items_set", "=", "set", "(", "batch_clicked_items", ")", ".", "difference", "(", "set", "(", "[", "0", "]", ")", ")", "\n", "\n", "for", "item_id", "in", "batch_clicked_items_set", ":", "\n", "            ", "if", "item_id", "not", "in", "self", ".", "items_first_click_step", ":", "\n", "                ", "self", ".", "items_first_click_step", "[", "item_id", "]", "=", "self", ".", "get_current_step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState._update_recently_clicked_items_buffer": [[206, 224], ["numpy.hstack", "numpy.min", "clicked_items_state.ClickedItemsState.truncate_last_hours_recent_clicks_buffer", "numpy.vstack", "numpy.vstack", "batch_clicked_items.reshape", "batch_clicked_timestamps.reshape", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.truncate_last_hours_recent_clicks_buffer"], ["", "", "", "def", "_update_recently_clicked_items_buffer", "(", "self", ",", "batch_clicked_items", ",", "batch_clicked_timestamps", ")", ":", "\n", "\n", "#Concatenating column vectors of batch clicked items", "\n", "        ", "batch_recent_clicks_timestamps", "=", "np", ".", "hstack", "(", "[", "batch_clicked_items", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "batch_clicked_timestamps", ".", "reshape", "(", "-", "1", ",", "1", ")", "]", ")", "\n", "#Inverting the order of clicks, so that latter clicks are now the first in the vector", "\n", "batch_recent_clicks_timestamps", "=", "batch_recent_clicks_timestamps", "[", ":", ":", "-", "1", "]", "\n", "\n", "#Keeping in the buffer only clicks within the last N hours", "\n", "min_timestamp_batch", "=", "np", ".", "min", "(", "batch_clicked_timestamps", ")", "\n", "\n", "self", ".", "truncate_last_hours_recent_clicks_buffer", "(", "min_timestamp_batch", ")", "\n", "\n", "#Concatenating batch clicks with recent buffer clicks, limited by the buffer size", "\n", "self", ".", "pop_recent_clicks_buffer", "=", "np", ".", "vstack", "(", "[", "batch_recent_clicks_timestamps", ",", "self", ".", "pop_recent_clicks_buffer", "]", ")", "[", ":", "self", ".", "recent_clicks_buffer_max_size", "]", "\n", "#Complete buffer with zeroes if necessary", "\n", "if", "self", ".", "pop_recent_clicks_buffer", ".", "shape", "[", "0", "]", "<", "self", ".", "recent_clicks_buffer_max_size", ":", "\n", "            ", "self", ".", "pop_recent_clicks_buffer", "=", "np", ".", "vstack", "(", "[", "self", ".", "pop_recent_clicks_buffer", ",", "\n", "np", ".", "zeros", "(", "shape", "=", "[", "self", ".", "recent_clicks_buffer_max_size", "-", "self", ".", "pop_recent_clicks_buffer", ".", "shape", "[", "0", "]", ",", "2", "]", ",", "dtype", "=", "np", ".", "int64", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.truncate_last_hours_recent_clicks_buffer": [[225, 229], ["int"], "methods", ["None"], ["", "", "def", "truncate_last_hours_recent_clicks_buffer", "(", "self", ",", "reference_timestamp", ")", ":", "\n", "        ", "MILISECS_BY_HOUR", "=", "1000", "*", "60", "*", "60", "\n", "min_timestamp_buffer_threshold", "=", "reference_timestamp", "-", "int", "(", "self", ".", "recent_clicks_buffer_hours", "*", "MILISECS_BY_HOUR", ")", "\n", "self", ".", "pop_recent_clicks_buffer", "=", "self", ".", "pop_recent_clicks_buffer", "[", "self", ".", "pop_recent_clicks_buffer", "[", ":", ",", "self", ".", "pop_recent_buffer_timestamp_column", "]", ">=", "min_timestamp_buffer_threshold", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState._update_recent_pop_items": [[231, 241], ["collections.Counter", "numpy.zeros", "list", "clicked_items_state.ClickedItemsState._update_recent_pop_norm", "collections.Counter.values", "numpy.nonzero", "list", "collections.Counter.keys"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState._update_recent_pop_norm"], ["", "def", "_update_recent_pop_items", "(", "self", ")", ":", "\n", "#Using all the buffer to compute items popularity", "\n", "        ", "pop_recent_clicks_buffer_items", "=", "self", ".", "pop_recent_clicks_buffer", "[", ":", ",", "self", ".", "pop_recent_buffer_article_id_column", "]", "\n", "recent_clicks_buffer_nonzero", "=", "pop_recent_clicks_buffer_items", "[", "np", ".", "nonzero", "(", "pop_recent_clicks_buffer_items", ")", "]", "\n", "recent_clicks_item_counter", "=", "Counter", "(", "recent_clicks_buffer_nonzero", ")", "\n", "\n", "self", ".", "articles_recent_pop", "=", "np", ".", "zeros", "(", "shape", "=", "[", "self", ".", "num_items", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "articles_recent_pop", "[", "list", "(", "recent_clicks_item_counter", ".", "keys", "(", ")", ")", "]", "=", "list", "(", "recent_clicks_item_counter", ".", "values", "(", ")", ")", "\n", "\n", "self", ".", "_update_recent_pop_norm", "(", "self", ".", "articles_recent_pop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState._update_recent_pop_norm": [[242, 247], ["numpy.maximum", "articles_recent_pop.sum"], "methods", ["None"], ["", "def", "_update_recent_pop_norm", "(", "self", ",", "articles_recent_pop", ")", ":", "\n", "#Minimum value for norm_pop, to avoid 0", "\n", "        ", "min_norm_pop", "=", "1.0", "/", "self", ".", "recent_clicks_for_normalization", "\n", "self", ".", "articles_recent_pop_norm", "=", "np", ".", "maximum", "(", "articles_recent_pop", "/", "(", "articles_recent_pop", ".", "sum", "(", ")", "+", "1", ")", ",", "\n", "[", "min_norm_pop", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState._update_pop_items": [[248, 251], ["collections.Counter", "list", "collections.Counter.values", "list", "collections.Counter.keys"], "methods", ["None"], ["", "def", "_update_pop_items", "(", "self", ",", "batch_items_nonzero", ")", ":", "\n", "        ", "batch_item_counter", "=", "Counter", "(", "batch_items_nonzero", ")", "\n", "self", ".", "articles_pop", "[", "list", "(", "batch_item_counter", ".", "keys", "(", ")", ")", "]", "+=", "list", "(", "batch_item_counter", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.update_items_coocurrences": [[252, 257], ["itertools.permutations", "zip", "numpy.nonzero"], "methods", ["None"], ["", "def", "update_items_coocurrences", "(", "self", ",", "batch_clicked_items", ")", ":", "\n", "        ", "for", "session_items", "in", "batch_clicked_items", ":", "\n", "            ", "session_pairs", "=", "permutations", "(", "session_items", "[", "np", ".", "nonzero", "(", "session_items", ")", "]", ",", "r", "=", "2", ")", "\n", "rows", ",", "cols", "=", "zip", "(", "*", "session_pairs", ")", "\n", "self", ".", "items_coocurrences", "[", "rows", ",", "cols", "]", "+=", "1", "", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.StreamingMetric.__init__": [[26, 29], ["metrics.StreamingMetric.reset"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.reset"], ["def", "__init__", "(", "self", ",", "topn", ")", ":", "\n", "        ", "self", ".", "topn", "=", "topn", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.StreamingMetric.reset": [[30, 32], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.StreamingMetric.add": [[33, 35], ["None"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.StreamingMetric.result": [[36, 38], ["None"], "methods", ["None"], ["", "def", "result", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.MRR.__init__": [[44, 46], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.MRR.reset": [[47, 49], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "mrr_results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.MRR.add": [[50, 63], ["enumerate", "metrics.MRR.mrr_results.extend", "enumerate", "[].astype", "measures.append", "numpy.where", "len"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "measures", "=", "[", "]", "\n", "for", "row_idx", ",", "session_labels", "in", "enumerate", "(", "labels", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_label", "in", "enumerate", "(", "session_labels", ")", ":", "\n", "                    ", "if", "item_label", "!=", "0", ":", "\n", "                        ", "correct_preds", "=", "(", "item_label", "==", "predictions", "[", "row_idx", ",", "col_idx", "]", ")", "[", ":", "self", ".", "topn", "]", ".", "astype", "(", "np", ".", "int32", ")", "\n", "correct_preds_pos", "=", "np", ".", "where", "(", "correct_preds", ")", "[", "0", "]", "\n", "\n", "reciprocal_rank", "=", "0", "\n", "if", "len", "(", "correct_preds_pos", ")", ">", "0", ":", "\n", "                            ", "reciprocal_rank", "=", "1.0", "/", "(", "1", "+", "correct_preds_pos", "[", "0", "]", ")", "\n", "", "measures", ".", "append", "(", "reciprocal_rank", ")", "\n", "", "", "", "self", ".", "mrr_results", ".", "extend", "(", "measures", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.MRR.result": [[64, 67], ["numpy.mean"], "methods", ["None"], ["", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_mrr", "=", "np", ".", "mean", "(", "self", ".", "mrr_results", ")", "\n", "return", "avg_mrr", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.NDCG.__init__": [[73, 75], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.NDCG.reset": [[76, 78], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "ndcg_results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.NDCG.add": [[79, 88], ["enumerate", "metrics.NDCG.ndcg_results.extend", "enumerate", "metrics.NDCG._ndcg_at_k", "measures.append"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.NDCG._ndcg_at_k"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "measures", "=", "[", "]", "\n", "for", "row_idx", ",", "session_labels", "in", "enumerate", "(", "labels", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_label", "in", "enumerate", "(", "session_labels", ")", ":", "\n", "                    ", "if", "item_label", "!=", "0", ":", "\n", "                        ", "correct_preds", "=", "(", "item_label", "==", "predictions", "[", "row_idx", ",", "col_idx", "]", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "ndcg", "=", "NDCG", ".", "_ndcg_at_k", "(", "correct_preds", ",", "self", ".", "topn", ")", "\n", "measures", ".", "append", "(", "ndcg", ")", "\n", "", "", "", "self", ".", "ndcg_results", ".", "extend", "(", "measures", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.NDCG.result": [[89, 92], ["numpy.mean"], "methods", ["None"], ["", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_ndcg", "=", "np", ".", "mean", "(", "self", ".", "ndcg_results", ")", "\n", "return", "avg_ndcg", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.NDCG._ndcg_at_k": [[93, 107], ["metrics.NDCG._ndcg_at_k.dcg_at_k"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_ndcg_at_k", "(", "r", ",", "k", ")", ":", "\n", "#Based on https://gist.github.com/bwhite/3726239, but with alternative formulation of DCG", "\n", "#which places stronger emphasis on retrieving relevant documents (used in Kaggle)", "\n", "        ", "def", "dcg_at_k", "(", "r", ",", "k", ")", ":", "\n", "            ", "r", "=", "np", ".", "asfarray", "(", "r", ")", "[", ":", "k", "]", "\n", "if", "r", ".", "size", ":", "\n", "                ", "return", "np", ".", "sum", "(", "(", "np", ".", "power", "(", "2", ",", "r", ")", "-", "1", ")", "/", "np", ".", "log2", "(", "np", ".", "arange", "(", "2", ",", "r", ".", "size", "+", "2", ")", ")", ")", "\n", "", "return", "0.", "\n", "\n", "", "dcg_max", "=", "dcg_at_k", "(", "sorted", "(", "r", ",", "reverse", "=", "True", ")", ",", "k", ")", "\n", "if", "not", "dcg_max", ":", "\n", "            ", "return", "0.", "\n", "", "return", "dcg_at_k", "(", "r", ",", "k", ")", "/", "dcg_max", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.HitRate.__init__": [[113, 115], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.HitRate.reset": [[116, 119], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "hitrate_total", "=", "0", "\n", "self", ".", "hitrate_matches", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.HitRate.add": [[120, 131], ["enumerate", "enumerate"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "total", "=", "0", "\n", "matches", "=", "0", "\n", "for", "row_idx", ",", "session_labels", "in", "enumerate", "(", "labels", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_label", "in", "enumerate", "(", "session_labels", ")", ":", "\n", "                    ", "if", "item_label", "!=", "0", ":", "\n", "                        ", "total", "+=", "1", "\n", "if", "item_label", "in", "predictions", "[", "row_idx", ",", "col_idx", "]", "[", ":", "self", ".", "topn", "]", ":", "\n", "                            ", "matches", "+=", "1", "\n", "", "", "", "", "self", ".", "hitrate_total", "+=", "total", "\n", "self", ".", "hitrate_matches", "+=", "matches", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.HitRate.result": [[132, 135], ["float"], "methods", ["None"], ["", "def", "result", "(", "self", ")", ":", "\n", "        ", "hitrate", "=", "self", ".", "hitrate_matches", "/", "float", "(", "self", ".", "hitrate_total", ")", "\n", "return", "hitrate", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.HitRateBySessionPosition.__init__": [[140, 142], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.HitRateBySessionPosition.reset": [[143, 147], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "hitrate_matches_by_session_pos", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "hitrate_total_by_session_pos", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "norm_pop_by_pos", "=", "defaultdict", "(", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.HitRateBySessionPosition.add": [[148, 156], ["enumerate", "enumerate"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ",", "labels_norm_pop", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_labels", "in", "enumerate", "(", "labels", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_label", "in", "enumerate", "(", "session_labels", ")", ":", "\n", "                    ", "if", "item_label", "!=", "0", ":", "\n", "                        ", "self", ".", "hitrate_total_by_session_pos", "[", "col_idx", "+", "1", "]", "+=", "1", "\n", "self", ".", "norm_pop_by_pos", "[", "col_idx", "+", "1", "]", "+=", "labels_norm_pop", "[", "row_idx", ",", "col_idx", "]", "\n", "if", "item_label", "in", "predictions", "[", "row_idx", ",", "col_idx", "]", "[", ":", "self", ".", "topn", "]", ":", "\n", "                            ", "self", ".", "hitrate_matches_by_session_pos", "[", "col_idx", "+", "1", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.HitRateBySessionPosition.result": [[158, 169], ["dict", "dict", "float", "float"], "methods", ["None"], ["", "", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "hitrate_by_session_pos", "=", "dict", "(", "[", "(", "key", ",", "(", "self", ".", "hitrate_matches_by_session_pos", "[", "key", "]", "if", "key", "in", "self", ".", "hitrate_matches_by_session_pos", "else", "0", ")", "/", "float", "(", "self", ".", "hitrate_total_by_session_pos", "[", "key", "]", ")", ")", "for", "key", "in", "self", ".", "hitrate_total_by_session_pos", "]", ")", "\n", "\n", "avg_norm_pop_by_session_pos", "=", "dict", "(", "[", "(", "key", ",", "(", "self", ".", "norm_pop_by_pos", "[", "key", "]", "if", "key", "in", "self", ".", "norm_pop_by_pos", "else", "0", ")", "/", "float", "(", "self", ".", "hitrate_total_by_session_pos", "[", "key", "]", ")", ")", "for", "key", "in", "self", ".", "hitrate_total_by_session_pos", "]", ")", "\n", "\n", "\n", "return", "hitrate_by_session_pos", ",", "avg_norm_pop_by_session_pos", ",", "self", ".", "hitrate_total_by_session_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.PopularityBias.__init__": [[176, 178], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.PopularityBias.reset": [[179, 181], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.PopularityBias.add": [[182, 192], ["enumerate", "metrics.PopularityBias.results.extend", "enumerate", "measures.extend"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ",", "predictions_norm_pop", ")", ":", "\n", "        ", "measures", "=", "[", "]", "\n", "for", "row_idx", ",", "session_preds_norm_pop", "in", "enumerate", "(", "predictions_norm_pop", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_preds_norm_pop", "in", "enumerate", "(", "session_preds_norm_pop", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "                        ", "click_top_predictions_pop_norm", "=", "item_preds_norm_pop", "[", ":", "self", ".", "topn", "]", "\n", "measures", ".", "extend", "(", "click_top_predictions_pop_norm", ")", "\n", "\n", "", "", "", "self", ".", "results", ".", "extend", "(", "measures", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.PopularityBias.result": [[193, 196], ["numpy.mean"], "methods", ["None"], ["", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_pop", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_pop", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.Novelty.__init__": [[203, 205], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.Novelty.reset": [[206, 208], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.Novelty.add": [[209, 220], ["enumerate", "metrics.Novelty.results.extend", "enumerate", "measures.extend", "numpy.log2"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ",", "predictions_norm_pop", ")", ":", "\n", "        ", "measures", "=", "[", "]", "\n", "for", "row_idx", ",", "session_preds_norm_pop", "in", "enumerate", "(", "predictions_norm_pop", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_preds_norm_pop", "in", "enumerate", "(", "session_preds_norm_pop", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "#From \"Novelty and Diversity Metrics for Recommender Systems - Choice, Discovery and Relevance\" (2011)", "\n", "                        ", "novelty", "=", "-", "np", ".", "log2", "(", "item_preds_norm_pop", "[", ":", "self", ".", "topn", "]", ")", "\n", "measures", ".", "extend", "(", "novelty", ")", "\n", "\n", "", "", "", "self", ".", "results", ".", "extend", "(", "measures", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.Novelty.result": [[221, 224], ["numpy.mean"], "methods", ["None"], ["", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_novelty", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_novelty", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ExpectedRankSensitiveNovelty.__init__": [[230, 232], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ExpectedRankSensitiveNovelty.reset": [[233, 235], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ExpectedRankSensitiveNovelty.add": [[236, 262], ["enumerate", "enumerate", "range", "metrics.ExpectedRankSensitiveNovelty.results.append", "metrics.log_rank_discount", "items_novelty.append", "disc_weights.append", "sum", "float", "len", "numpy.log2", "sum"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ",", "predictions_norm_pop", ")", ":", "\n", "\n", "        ", "for", "row_idx", ",", "session_preds_norm_pop", "in", "enumerate", "(", "predictions_norm_pop", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_preds_norm_pop", "in", "enumerate", "(", "session_preds_norm_pop", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "                        ", "top_preds_norm_pop", "=", "item_preds_norm_pop", "[", ":", "self", ".", "topn", "]", "\n", "\n", "disc_weights", "=", "[", "]", "\n", "items_novelty", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "top_preds_norm_pop", ")", "-", "1", ")", ":", "\n", "#Logarithmic rank discount, to prioritize more diverse items in the top of the list", "\n", "                            ", "discount", "=", "log_rank_discount", "(", "i", ")", "\n", "\n", "#From \"Novelty and Diversity Metrics for Recommender Systems - Choice, Discovery and Relevance\" (2011)", "\n", "item_novelty", "=", "-", "np", ".", "log2", "(", "top_preds_norm_pop", "[", "i", "]", ")", "\n", "\n", "items_novelty", ".", "append", "(", "item_novelty", "*", "discount", ")", "\n", "disc_weights", ".", "append", "(", "discount", ")", "\n", "\n", "\n", "#Expected Novelty with logarithmic rank discount", "\n", "#Adapted from \"Incorporating Diversity in a Learning to Rank Recommender System\" (2016)", "\n", "", "avg_novelty", "=", "sum", "(", "items_novelty", ")", "/", "float", "(", "sum", "(", "disc_weights", ")", ")", "\n", "\n", "self", ".", "results", ".", "append", "(", "avg_novelty", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ExpectedRankSensitiveNovelty.result": [[263, 266], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_novelty", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_novelty", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ExpectedRankRelevanceSensitiveNovelty.__init__": [[273, 277], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "relevance_positive_sample", ",", "relevance_negative_samples", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "self", ".", "relevance_positive_sample", "=", "relevance_positive_sample", "\n", "self", ".", "relevance_negative_samples", "=", "relevance_negative_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ExpectedRankRelevanceSensitiveNovelty.reset": [[278, 280], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ExpectedRankRelevanceSensitiveNovelty.add": [[281, 310], ["range", "len", "range", "len", "range", "metrics.ExpectedRankRelevanceSensitiveNovelty.results.append", "metrics.log_rank_discount", "items_novelty.append", "weights.append", "sum", "float", "len", "numpy.log2", "sum"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ",", "predictions_norm_pop", ")", ":", "\n", "        ", "for", "row_idx", "in", "range", "(", "len", "(", "predictions", ")", ")", ":", "\n", "                ", "for", "col_idx", "in", "range", "(", "len", "(", "predictions", "[", "row_idx", "]", ")", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "                        ", "click_top_predictions", "=", "predictions", "[", "row_idx", ",", "col_idx", "]", "[", ":", "self", ".", "topn", "]", "\n", "click_top_preds_norm_pop", "=", "predictions_norm_pop", "[", "row_idx", ",", "col_idx", "]", "[", ":", "self", ".", "topn", "]", "\n", "\n", "items_novelty", "=", "[", "]", "\n", "weights", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "click_top_preds_norm_pop", ")", "-", "1", ")", ":", "\n", "#Logarithmic rank discount, to prioritize more diverse items in the top of the list", "\n", "                            ", "discount", "=", "log_rank_discount", "(", "i", ")", "\n", "\n", "#From \"Novelty and Diversity Metrics for Recommender Systems - Choice, Discovery and Relevance\" (2011)", "\n", "item_novelty", "=", "-", "np", ".", "log2", "(", "click_top_preds_norm_pop", "[", "i", "]", ")", "\n", "\n", "\n", "relevance", "=", "self", ".", "relevance_positive_sample", "if", "click_top_predictions", "[", "i", "]", "==", "labels", "[", "row_idx", ",", "col_idx", "]", "else", "self", ".", "relevance_negative_samples", "\n", "\n", "items_novelty", ".", "append", "(", "item_novelty", "*", "discount", "*", "relevance", ")", "\n", "weights", ".", "append", "(", "discount", ")", "\n", "\n", "\n", "#Expected Novelty with logarithmic rank discount", "\n", "#Adapted from \"Incorporating Diversity in a Learning to Rank Recommender System\" (2016)", "\n", "", "avg_novelty", "=", "sum", "(", "items_novelty", ")", "/", "float", "(", "sum", "(", "weights", ")", ")", "\n", "\n", "self", ".", "results", ".", "append", "(", "avg_novelty", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ExpectedRankRelevanceSensitiveNovelty.result": [[311, 314], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_novelty", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_novelty", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ItemCoverage.__init__": [[321, 324], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "recent_clicks_buffer", ")", ":", "\n", "        ", "self", ".", "recent_clicks_buffer", "=", "recent_clicks_buffer", "\n", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ItemCoverage.reset": [[325, 328], ["set", "set"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "clicked_items", "=", "set", "(", "self", ".", "recent_clicks_buffer", ")", "\n", "self", ".", "recommended_items", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ItemCoverage.add": [[329, 340], ["enumerate", "set", "metrics.ItemCoverage.clicked_items.update", "enumerate", "numpy.hstack", "metrics.ItemCoverage.recommended_items.update", "numpy.nonzero", "numpy.nonzero"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ",", "clicked_items", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_predictions", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_predictions", "in", "enumerate", "(", "session_predictions", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "                        ", "click_top_predictions", "=", "item_predictions", "[", ":", "self", ".", "topn", "]", "\n", "self", ".", "recommended_items", ".", "update", "(", "click_top_predictions", ")", "\n", "\n", "####Including both the clicked item and all recommended items to ensure that we have a comprehensive set of all recommendable items at a given time", "\n", "", "", "", "batch_clicked_items", "=", "set", "(", "np", ".", "hstack", "(", "[", "labels", "[", "np", ".", "nonzero", "(", "labels", ")", "]", ",", "clicked_items", "[", "np", ".", "nonzero", "(", "clicked_items", ")", "]", "]", ")", ")", "\n", "self", ".", "clicked_items", ".", "update", "(", "batch_clicked_items", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ItemCoverage.result": [[341, 344], ["len", "float", "len"], "methods", ["None"], ["", "def", "result", "(", "self", ")", ":", "\n", "        ", "item_coverage", "=", "len", "(", "self", ".", "recommended_items", ")", "/", "float", "(", "len", "(", "self", ".", "clicked_items", ")", ")", "\n", "return", "item_coverage", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentAverageIntraListDiversity.__init__": [[350, 353], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "content_article_embeddings_matrix", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "self", ".", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentAverageIntraListDiversity.reset": [[354, 356], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentAverageIntraListDiversity.add": [[357, 378], ["enumerate", "enumerate", "metrics.cosine_distance", "range", "metrics.ContentAverageIntraListDiversity.results.append", "range", "sum", "float", "len", "len", "dists.append", "len"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.cosine_distance"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_predictions", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_predictions", "in", "enumerate", "(", "session_predictions", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "\n", "                        ", "click_top_predictions", "=", "item_predictions", "[", ":", "self", ".", "topn", "]", "\n", "\n", "distances", "=", "cosine_distance", "(", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ",", "\n", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ")", "\n", "\n", "dists", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", "-", "1", ")", ":", "\n", "                            ", "for", "j", "in", "range", "(", "i", "+", "1", ",", "len", "(", "click_top_predictions", ")", ")", ":", "\n", "\n", "                                ", "cos_dist", "=", "distances", "[", "i", ",", "j", "]", "\n", "\n", "dists", ".", "append", "(", "cos_dist", ")", "\n", "\n", "", "", "avg_cos_dist", "=", "sum", "(", "dists", ")", "/", "float", "(", "len", "(", "dists", ")", ")", "\n", "self", ".", "results", ".", "append", "(", "avg_cos_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentAverageIntraListDiversity.result": [[379, 382], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_cos_dist", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_cos_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentMedianIntraListDiversity.__init__": [[389, 392], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "content_article_embeddings_matrix", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "self", ".", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentMedianIntraListDiversity.reset": [[393, 395], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentMedianIntraListDiversity.add": [[396, 416], ["enumerate", "enumerate", "metrics.cosine_distance", "range", "numpy.median", "metrics.ContentMedianIntraListDiversity.results.append", "range", "len", "len", "dists.append"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.cosine_distance"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_predictions", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_predictions", "in", "enumerate", "(", "session_predictions", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "                        ", "click_top_predictions", "=", "item_predictions", "[", ":", "self", ".", "topn", "]", "\n", "\n", "distances", "=", "cosine_distance", "(", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ",", "\n", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ")", "\n", "\n", "dists", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", "-", "1", ")", ":", "\n", "                            ", "for", "j", "in", "range", "(", "i", "+", "1", ",", "len", "(", "click_top_predictions", ")", ")", ":", "\n", "\n", "                                ", "cos_dist", "=", "distances", "[", "i", ",", "j", "]", "\n", "\n", "dists", ".", "append", "(", "cos_dist", ")", "\n", "\n", "", "", "median_cos_dist", "=", "np", ".", "median", "(", "dists", ")", "\n", "self", ".", "results", ".", "append", "(", "median_cos_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentMedianIntraListDiversity.result": [[417, 420], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_cos_dist", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_cos_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentMinIntraListDiversity.__init__": [[427, 430], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "content_article_embeddings_matrix", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "self", ".", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentMinIntraListDiversity.reset": [[431, 433], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentMinIntraListDiversity.add": [[434, 456], ["enumerate", "enumerate", "metrics.cosine_distance", "range", "numpy.min", "metrics.ContentMinIntraListDiversity.results.append", "range", "len", "len", "dists.append"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.cosine_distance"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_predictions", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_predictions", "in", "enumerate", "(", "session_predictions", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "\n", "                        ", "click_top_predictions", "=", "item_predictions", "[", ":", "self", ".", "topn", "]", "\n", "\n", "distances", "=", "cosine_distance", "(", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ",", "\n", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ")", "\n", "\n", "dists", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", "-", "1", ")", ":", "\n", "                            ", "for", "j", "in", "range", "(", "i", "+", "1", ",", "len", "(", "click_top_predictions", ")", ")", ":", "\n", "\n", "                                ", "cos_dist", "=", "distances", "[", "i", ",", "j", "]", "\n", "\n", "\n", "dists", ".", "append", "(", "cos_dist", ")", "\n", "\n", "", "", "min_cos_dist", "=", "np", ".", "min", "(", "dists", ")", "\n", "self", ".", "results", ".", "append", "(", "min_cos_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentMinIntraListDiversity.result": [[457, 460], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_cos_dist", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_cos_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankSensitiveIntraListDiversity.__init__": [[466, 469], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "content_article_embeddings_matrix", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "self", ".", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankSensitiveIntraListDiversity.reset": [[470, 472], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankSensitiveIntraListDiversity.add": [[473, 507], ["enumerate", "enumerate", "metrics.cosine_distance", "range", "metrics.ContentExpectedRankSensitiveIntraListDiversity.results.append", "disc_weights.append", "range", "avg_dists.append", "sum", "float", "len", "math.log2", "len", "dists.append", "sum", "float", "sum", "len"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.cosine_distance"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_predictions", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_predictions", "in", "enumerate", "(", "session_predictions", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "\n", "                        ", "click_top_predictions", "=", "item_predictions", "[", ":", "self", ".", "topn", "]", "\n", "\n", "distances", "=", "cosine_distance", "(", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ",", "\n", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ")", "\n", "\n", "avg_dists", "=", "[", "]", "\n", "disc_weights", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", "-", "1", ")", ":", "\n", "#Logarithmic rank discount, to prioritize more diverse items in the top of the list", "\n", "                            ", "discount", "=", "1.", "/", "math", ".", "log2", "(", "i", "+", "2", ")", "\n", "disc_weights", ".", "append", "(", "discount", ")", "\n", "\n", "dists", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "i", "+", "1", ",", "len", "(", "click_top_predictions", ")", ")", ":", "\n", "\n", "                                ", "dist", "=", "distances", "[", "i", ",", "j", "]", "\n", "\n", "dists", ".", "append", "(", "dist", ")", "\n", "\n", "", "avg_dist", "=", "sum", "(", "dists", ")", "/", "float", "(", "len", "(", "dists", ")", ")", "\n", "avg_dists", ".", "append", "(", "avg_dist", "*", "discount", ")", "\n", "\n", "\n", "#Expected Intra-List Diversity (EILD) with logarithmic rank discount", "\n", "#From \"Incorporating Diversity in a Learning to Rank Recommender System\" (2016)", "\n", "", "avg_cos_dist", "=", "sum", "(", "avg_dists", ")", "/", "float", "(", "sum", "(", "disc_weights", ")", ")", "\n", "\n", "self", ".", "results", ".", "append", "(", "avg_cos_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankSensitiveIntraListDiversity.result": [[508, 511], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_cos_dist", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_cos_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelativeSensitiveIntraListDiversity.__init__": [[517, 520], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "content_article_embeddings_matrix", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "self", ".", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelativeSensitiveIntraListDiversity.reset": [[521, 523], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelativeSensitiveIntraListDiversity.add": [[524, 570], ["enumerate", "enumerate", "metrics.cosine_distance", "range", "metrics.ContentExpectedRankRelativeSensitiveIntraListDiversity.results.append", "range", "metrics.log_rank_discount", "avg_dists.append", "disc_weights.append", "sum", "float", "len", "len", "metrics.log_rank_discount", "dists.append", "weights.append", "sum", "float", "sum", "max", "sum"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.cosine_distance", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_predictions", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_predictions", "in", "enumerate", "(", "session_predictions", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "\n", "                        ", "click_top_predictions", "=", "item_predictions", "[", ":", "self", ".", "topn", "]", "\n", "\n", "distances", "=", "cosine_distance", "(", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ",", "\n", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ")", "\n", "\n", "\n", "avg_dists", "=", "[", "]", "\n", "disc_weights", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", "-", "1", ")", ":", "\n", "\n", "                            ", "dists", "=", "[", "]", "\n", "weights", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", ")", ":", "\n", "#Ignoring self-similarity", "\n", "                                ", "if", "j", "==", "i", ":", "\n", "                                    ", "continue", "\n", "\n", "", "dist", "=", "distances", "[", "i", ",", "j", "]", "\n", "\n", "#Under the assumption that diversity of the items is more perceived by users when items are near in the ranked list", "\n", "rel_discount", "=", "log_rank_discount", "(", "max", "(", "0", ",", "j", "-", "i", "-", "1", ")", ")", "\n", "dists", ".", "append", "(", "dist", "*", "rel_discount", ")", "\n", "weights", ".", "append", "(", "rel_discount", ")", "\n", "\n", "", "weighted_avg_dists", "=", "sum", "(", "dists", ")", "/", "float", "(", "sum", "(", "weights", ")", ")", "\n", "\n", "#Logarithmic rank discount, to prioritize more diverse items in the top of the list", "\n", "discount", "=", "log_rank_discount", "(", "i", ")", "\n", "\n", "avg_dists", ".", "append", "(", "weighted_avg_dists", "*", "discount", ")", "\n", "disc_weights", ".", "append", "(", "discount", ")", "\n", "\n", "\n", "\n", "\n", "#Expected Intra-List Diversity (EILD) with logarithmic rank discount", "\n", "#From \"Incorporating Diversity in a Learning to Rank Recommender System\" (2016)", "\n", "", "avg_cos_dist", "=", "sum", "(", "avg_dists", ")", "/", "float", "(", "sum", "(", "disc_weights", ")", ")", "\n", "\n", "self", ".", "results", ".", "append", "(", "avg_cos_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelativeSensitiveIntraListDiversity.result": [[571, 574], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_cos_dist", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_cos_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity.__init__": [[581, 586], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "content_article_embeddings_matrix", ",", "relevance_positive_sample", ",", "relevance_negative_samples", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "self", ".", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "\n", "self", ".", "relevance_positive_sample", "=", "relevance_positive_sample", "\n", "self", ".", "relevance_negative_samples", "=", "relevance_negative_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity.reset": [[587, 589], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity.add": [[590, 640], ["enumerate", "enumerate", "metrics.cosine_distance", "range", "metrics.ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity.results.append", "range", "metrics.log_rank_discount", "avg_dists.append", "disc_weights.append", "sum", "float", "len", "len", "metrics.log_rank_discount", "dists.append", "weights.append", "sum", "float", "sum", "max", "sum"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.cosine_distance", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_predictions", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_predictions", "in", "enumerate", "(", "session_predictions", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "\n", "                        ", "click_top_predictions", "=", "item_predictions", "[", ":", "self", ".", "topn", "]", "\n", "\n", "distances", "=", "cosine_distance", "(", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ",", "\n", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ")", "\n", "\n", "avg_dists", "=", "[", "]", "\n", "disc_weights", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", "-", "1", ")", ":", "\n", "\n", "                            ", "dists", "=", "[", "]", "\n", "weights", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "i", "+", "1", ",", "len", "(", "click_top_predictions", ")", ")", ":", "\n", "#Ignoring self-similarity", "\n", "                                ", "if", "j", "==", "i", ":", "\n", "                                    ", "continue", "\n", "\n", "", "dist", "=", "distances", "[", "i", ",", "j", "]", "\n", "\n", "#Weights item by relevance", "\n", "relevance_j", "=", "self", ".", "relevance_positive_sample", "if", "click_top_predictions", "[", "j", "]", "==", "labels", "[", "row_idx", ",", "col_idx", "]", "else", "self", ".", "relevance_negative_samples", "\n", "\n", "#Under the assumption that diversity of the items is more perceived by users when items are near in the ranked list", "\n", "rel_discount", "=", "log_rank_discount", "(", "max", "(", "0", ",", "j", "-", "i", "-", "1", ")", ")", "\n", "\n", "dists", ".", "append", "(", "dist", "*", "rel_discount", "*", "relevance_j", ")", "\n", "weights", ".", "append", "(", "rel_discount", "*", "relevance_j", ")", "\n", "\n", "", "avg_dists_i", "=", "sum", "(", "dists", ")", "/", "float", "(", "sum", "(", "weights", ")", ")", "\n", "\n", "#Weights item by relevance", "\n", "relevance_i", "=", "self", ".", "relevance_positive_sample", "if", "click_top_predictions", "[", "i", "]", "==", "labels", "[", "row_idx", ",", "col_idx", "]", "else", "self", ".", "relevance_negative_samples", "\n", "\n", "#Logarithmic rank discount, to prioritize more diverse items in the top of the list", "\n", "rank_discount_i", "=", "log_rank_discount", "(", "i", ")", "\n", "\n", "avg_dists", ".", "append", "(", "avg_dists_i", "*", "rank_discount_i", "*", "relevance_i", ")", "\n", "disc_weights", ".", "append", "(", "rank_discount_i", ")", "\n", "\n", "\n", "#Expected Intra-List Diversity (EILD) with logarithmic rank discount", "\n", "#From \"Incorporating Diversity in a Learning to Rank Recommender System\" (2016)", "\n", "", "avg_cos_dist", "=", "sum", "(", "avg_dists", ")", "/", "float", "(", "sum", "(", "disc_weights", ")", ")", "\n", "\n", "self", ".", "results", ".", "append", "(", "avg_cos_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity.result": [[641, 644], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_cos_dist", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_cos_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelevanceSensitiveIntraListDiversity.__init__": [[650, 655], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "content_article_embeddings_matrix", ",", "relevance_positive_sample", ",", "relevance_negative_samples", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "self", ".", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "\n", "self", ".", "relevance_positive_sample", "=", "relevance_positive_sample", "\n", "self", ".", "relevance_negative_samples", "=", "relevance_negative_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelevanceSensitiveIntraListDiversity.reset": [[656, 658], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelevanceSensitiveIntraListDiversity.add": [[659, 716], ["enumerate", "enumerate", "metrics.cosine_distance", "range", "metrics.ContentExpectedRankRelevanceSensitiveIntraListDiversity.results.append", "range", "metrics.log_rank_discount", "avg_dists.append", "disc_weights.append", "sum", "float", "len", "len", "dists.append", "sum", "float", "sum", "len"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.cosine_distance", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_predictions", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_predictions", "in", "enumerate", "(", "session_predictions", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "\n", "                        ", "click_top_predictions", "=", "item_predictions", "[", ":", "self", ".", "topn", "]", "\n", "\n", "distances", "=", "cosine_distance", "(", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ",", "\n", "self", ".", "content_article_embeddings_matrix", "[", "click_top_predictions", "]", ")", "\n", "\n", "avg_dists", "=", "[", "]", "\n", "disc_weights", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", "-", "1", ")", ":", "\n", "\n", "                            ", "dists", "=", "[", "]", "\n", "#weights = []", "\n", "for", "j", "in", "range", "(", "i", "+", "1", ",", "len", "(", "click_top_predictions", ")", ")", ":", "\n", "#Ignoring self-similarity", "\n", "                                ", "if", "j", "==", "i", ":", "\n", "                                    ", "continue", "\n", "\n", "", "'''\n                                embedding_item_i = self.content_article_embeddings_matrix[click_top_predictions[i]].reshape(1,-1)\n                                embedding_item_j = self.content_article_embeddings_matrix[click_top_predictions[j]].reshape(1,-1)\n                                dist = cosine_distance(embedding_item_i, embedding_item_j)\n                                '''", "\n", "\n", "dist", "=", "distances", "[", "i", ",", "j", "]", "\n", "\n", "#Weights item by relevance", "\n", "#relevance_j = self.relevance_positive_sample if click_top_predictions[j] == labels[row_idx, col_idx] else self.relevance_negative_samples ", "\n", "\n", "#Under the assumption that diversity of the items is more perceived by users when items are near in the ranked list", "\n", "#rel_discount = log_rank_discount(max(0, j-i-1))", "\n", "\n", "dists", ".", "append", "(", "dist", ")", "\n", "#weights.append(rel_discount * relevance_j)", "\n", "\n", "#avg_dists_i = sum(dists)/float(sum(weights))", "\n", "", "avg_dists_i", "=", "sum", "(", "dists", ")", "/", "float", "(", "len", "(", "dists", ")", ")", "\n", "\n", "#Weights item by relevance", "\n", "relevance_i", "=", "self", ".", "relevance_positive_sample", "if", "click_top_predictions", "[", "i", "]", "==", "labels", "[", "row_idx", ",", "col_idx", "]", "else", "self", ".", "relevance_negative_samples", "\n", "\n", "#Logarithmic rank discount, to prioritize more diverse items in the top of the list", "\n", "rank_discount_i", "=", "log_rank_discount", "(", "i", ")", "\n", "\n", "avg_dists", ".", "append", "(", "avg_dists_i", "*", "rank_discount_i", "*", "relevance_i", ")", "\n", "disc_weights", ".", "append", "(", "rank_discount_i", ")", "\n", "\n", "\n", "#Expected Intra-List Diversity (EILD) with logarithmic rank discount", "\n", "#From \"Incorporating Diversity in a Learning to Rank Recommender System\" (2016)", "\n", "", "avg_cos_dist", "=", "sum", "(", "avg_dists", ")", "/", "float", "(", "sum", "(", "disc_weights", ")", ")", "\n", "\n", "self", ".", "results", ".", "append", "(", "avg_cos_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.ContentExpectedRankRelevanceSensitiveIntraListDiversity.result": [[717, 720], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_cos_dist", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_cos_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.__init__": [[726, 729], ["metrics.StreamingMetric.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["def", "__init__", "(", "self", ",", "topn", ",", "categories", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "topn", ")", "\n", "self", ".", "categories", "=", "categories", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.reset": [[730, 732], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "results", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add": [[733, 776], ["enumerate", "enumerate", "range", "metrics.CategoryExpectedIntraListDiversity.results.append", "range", "metrics.log_rank_discount", "avg_dists.append", "disc_weights.append", "sum", "float", "len", "len", "metrics.CategoryExpectedIntraListDiversity.categories[].reshape", "metrics.CategoryExpectedIntraListDiversity.categories[].reshape", "metrics.log_rank_discount", "dists.append", "weights.append", "sum", "float", "sum", "max", "sum"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount"], ["", "def", "add", "(", "self", ",", "predictions", ",", "labels", ")", ":", "\n", "        ", "for", "row_idx", ",", "session_predictions", "in", "enumerate", "(", "predictions", ")", ":", "\n", "                ", "for", "col_idx", ",", "item_predictions", "in", "enumerate", "(", "session_predictions", ")", ":", "\n", "#If this is not a padded item", "\n", "                    ", "if", "labels", "[", "row_idx", ",", "col_idx", "]", "!=", "0", ":", "\n", "\n", "                        ", "click_top_predictions", "=", "item_predictions", "[", ":", "self", ".", "topn", "]", "\n", "\n", "#TODO: Vectorize this inner loop (too slow)", "\n", "\n", "avg_dists", "=", "[", "]", "\n", "disc_weights", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", "-", "1", ")", ":", "\n", "\n", "                            ", "dists", "=", "[", "]", "\n", "weights", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "0", ",", "len", "(", "click_top_predictions", ")", ")", ":", "\n", "#Ignoring self-similarity", "\n", "                                ", "if", "j", "==", "i", ":", "\n", "                                    ", "continue", "\n", "\n", "", "category_item_i", "=", "self", ".", "categories", "[", "click_top_predictions", "[", "i", "]", "]", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "category_item_j", "=", "self", ".", "categories", "[", "click_top_predictions", "[", "j", "]", "]", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "dist", "=", "0.0", "if", "category_item_i", "==", "category_item_j", "else", "1.0", "\n", "\n", "#Under the assumption that diversity of the items is more perceived by users when items are near in the ranked list", "\n", "rel_discount", "=", "log_rank_discount", "(", "max", "(", "0", ",", "j", "-", "i", "-", "1", ")", ")", "\n", "dists", ".", "append", "(", "dist", "*", "rel_discount", ")", "\n", "weights", ".", "append", "(", "rel_discount", ")", "\n", "\n", "", "avg_dist", "=", "sum", "(", "dists", ")", "/", "float", "(", "sum", "(", "weights", ")", ")", "\n", "\n", "#Logarithmic rank discount, to prioritize more diverse items in the top of the list", "\n", "discount", "=", "log_rank_discount", "(", "i", ")", "\n", "\n", "avg_dists", ".", "append", "(", "avg_dist", "*", "discount", ")", "\n", "disc_weights", ".", "append", "(", "discount", ")", "\n", "\n", "\n", "#Expected Intra-List Diversity (EILD) with logarithmic rank discount", "\n", "#From \"Incorporating Diversity in a Learning to Rank Recommender System\" (2016)", "\n", "", "avg_cos_dist", "=", "sum", "(", "avg_dists", ")", "/", "float", "(", "sum", "(", "disc_weights", ")", ")", "\n", "self", ".", "results", ".", "append", "(", "avg_cos_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.result": [[777, 780], ["numpy.mean"], "methods", ["None"], ["", "", "", "", "def", "result", "(", "self", ")", ":", "\n", "        ", "avg_cos_dist", "=", "np", ".", "mean", "(", "self", ".", "results", ")", "\n", "return", "avg_cos_dist", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.cosine_distance": [[11, 15], ["sklearn.metrics.pairwise.cosine_distances"], "function", ["None"], ["def", "cosine_distance", "(", "v1", ",", "v2", ")", ":", "\n", "#As cosine similarity interval is [-1.0, 1.0], the cosine distance interval is [0.0, 2.0].", "\n", "#This normalizes the cosine distance to interval [0.0, 1.0]", "\n", "    ", "return", "pairwise", ".", "cosine_distances", "(", "v1", ",", "v2", ")", "/", "2.0", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.log_rank_discount": [[19, 21], ["math.log2"], "function", ["None"], ["", "def", "log_rank_discount", "(", "k", ")", ":", "\n", "    ", "return", "1.", "/", "math", ".", "log2", "(", "k", "+", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.__init__": [[102, 729], ["tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.placeholder", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.variable_scope", "nar_model.NARModuleModel.build_rnn", "tensorflow.device", "tensorflow.device", "tensorflow.placeholder", "tensorflow.device", "tensorflow.placeholder", "tensorflow.variable_scope", "tensorflow.sequence_mask", "tensorflow.expand_dims", "tensorflow.reduce_max", "tensorflow.concat", "tensorflow.variable_scope", "tensorflow.boolean_mask", "tensorflow.unique", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.variable_scope", "nar_model.NARModuleModel.get_sample_from_recently_clicked_items_buffer", "nar_model.NARModuleModel.get_batch_negative_samples", "tensorflow.variable_scope", "nar_model.NARModuleModel.get_features", "nar_model.NARModuleModel.get_item_features", "tensorflow.concat", "nar_model.NARModuleModel.scale_center_features", "tensorflow.layers.dropout", "nar_model.NARModuleModel.get_item_features", "tensorflow.concat", "nar_model.NARModuleModel.scale_center_features", "tensorflow.layers.dropout", "nar_model.NARModuleModel.get_item_features", "tensorflow.tile", "tensorflow.concat", "nar_model.NARModuleModel.scale_center_features", "tensorflow.layers.dropout", "tensorflow.variable_scope", "tensorflow.layers.Dense", "tensorflow.layers.Dense.", "tensorflow.layers.Dense", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.layers.Dense", "tensorflow.layers.Dense", "tensorflow.layers.Dense", "tensorflow.layers.Dense", "tensorflow.variable_scope", "tensorflow.to_float", "tensorflow.losses.get_regularization_loss", "tensorflow.summary.scalar", "tensorflow.multiply", "tensorflow.summary.scalar", "tensorflow.multiply", "tensorflow.summary.scalar", "tensorflow.placeholder", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.shape", "tensorflow.python.ops.array_ops.shape", "tensorflow.cast", "tensorflow.shape", "tensorflow.shape", "tensorflow.zeros_like", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.expand_dims", "tensorflow.summary.histogram", "tensorflow.variable_scope", "tensorflow.layers.Dense.", "tensorflow.variable_scope", "tensorflow.layers.Dense.", "tensorflow.variable_scope", "tensorflow.layers.Dense.", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.variable_scope", "tensorflow.multiply", "tensorflow.layers.Dense.", "tensorflow.variable_scope", "tensorflow.multiply", "tensorflow.layers.Dense.", "tensorflow.squeeze", "tensorflow.variable_scope", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.concat", "nar_model.NARModuleModel.rank_items_by_predicted_prob", "nar_model.NARModuleModel.define_eval_metrics", "tensorflow.variable_scope", "nar_model.NARModuleModel.get_items_norm_popularity_feature", "nar_model.NARModuleModel.get_items_norm_popularity_feature", "tensorflow.squeeze", "tensorflow.concat", "nar_model.NARModuleModel.get_items_pop_novelty_feature", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.log", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.variable_scope", "tensorflow.train.AdamOptimizer", "tensorflow.get_collection", "tensorflow.shape", "tensorflow.shape", "tensorflow.sign", "tensorflow.summary.histogram", "tensorflow.expand_dims", "tensorflow.contrib.layers.variance_scaling_initializer", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.summary.histogram", "tensorflow.layers.Dense.", "tensorflow.summary.histogram", "tensorflow.layers.Dense.", "tensorflow.summary.histogram", "tensorflow.contrib.layers.variance_scaling_initializer", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.contrib.layers.variance_scaling_initializer", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.contrib.layers.variance_scaling_initializer", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.contrib.layers.variance_scaling_initializer", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.initializers.lecun_uniform", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.summary.histogram", "tensorflow.layers.Dense.", "tensorflow.summary.histogram", "tensorflow.expand_dims", "tensorflow.summary.histogram", "tensorflow.layers.Dense.", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.reduce_sum", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.control_dependencies", "tensorflow.train.AdamOptimizer.compute_gradients", "tensorflow.train.AdamOptimizer.apply_gradients", "utils.get_tf_dtype", "tensorflow.shape", "tensorflow.layers.Dense.", "tensorflow.layers.Dense.", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.summary.histogram", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.unique", "tensorflow.unique", "tensorflow.layers.Dense.", "tensorflow.boolean_mask", "tensorflow.layers.Dense.", "tensorflow.boolean_mask", "tensorflow.boolean_mask", "tensorflow.boolean_mask", "tensorflow.multiply", "tensorflow.train.get_global_step", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.multiply", "tensorflow.expand_dims", "tensorflow.boolean_mask", "tensorflow.summary.histogram", "tensorflow.sign", "tensorflow.sign", "tensorflow.sign", "tensorflow.sign", "tensorflow.cast", "tensorflow.sign"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.build_rnn", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_sample_from_recently_clicked_items_buffer", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_batch_negative_samples", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_item_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.scale_center_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_item_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.scale_center_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_item_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.scale_center_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.softmax", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.softmax", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.rank_items_by_predicted_prob", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.define_eval_metrics", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_norm_popularity_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_norm_popularity_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_pop_novelty_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_tf_dtype"], ["    ", "def", "__init__", "(", "self", ",", "mode", ",", "inputs", ",", "labels", ",", "\n", "session_features_config", ",", "\n", "articles_features_config", ",", "\n", "batch_size", ",", "\n", "lr", ",", "keep_prob", ",", "negative_samples", ",", "negative_sample_from_buffer", ",", "\n", "content_article_embeddings_matrix", ",", "\n", "rnn_num_layers", "=", "1", ",", "\n", "softmax_temperature", "=", "1.0", ",", "\n", "reg_weight_decay", "=", "0.0", ",", "\n", "recent_clicks_buffer_hours", "=", "1.0", ",", "\n", "recent_clicks_buffer_max_size", "=", "1000", ",", "\n", "recent_clicks_for_normalization", "=", "1000", ",", "\n", "articles_metadata", "=", "None", ",", "\n", "plot_histograms", "=", "False", ",", "\n", "metrics_top_n", "=", "5", ",", "\n", "elapsed_days_smooth_log_base", "=", "1.3", ",", "\n", "popularity_smooth_log_base", "=", "2.0", ",", "\n", "CAR_embedding_size", "=", "256", ",", "\n", "rnn_units", "=", "256", ",", "\n", "max_cardinality_for_ohe", "=", "10", ",", "\n", "novelty_reg_factor", "=", "0.0", ",", "\n", "diversity_reg_factor", "=", "0.0", ",", "\n", "internal_features_config", "=", "{", "'recency'", ":", "True", ",", "\n", "'novelty'", ":", "True", ",", "\n", "'article_content_embeddings'", ":", "True", ",", "\n", "'item_clicked_embeddings'", ":", "True", "}", ",", "\n", "eval_cold_start", "=", "False", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "lr", "=", "lr", "\n", "self", ".", "keep_prob", "=", "keep_prob", "\n", "\n", "self", ".", "elapsed_days_smooth_log_base", "=", "elapsed_days_smooth_log_base", "\n", "self", ".", "popularity_smooth_log_base", "=", "popularity_smooth_log_base", "\n", "\n", "self", ".", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "self", ".", "internal_features_config", "=", "internal_features_config", "\n", "\n", "self", ".", "negative_samples", "=", "negative_samples", "\n", "self", ".", "negative_sample_from_buffer", "=", "negative_sample_from_buffer", "\n", "\n", "\n", "self", ".", "rnn_num_layers", "=", "rnn_num_layers", "\n", "self", ".", "metrics_top_n", "=", "metrics_top_n", "\n", "\n", "self", ".", "plot_histograms", "=", "plot_histograms", "\n", "\n", "self", ".", "reg_weight_decay", "=", "reg_weight_decay", "\n", "self", ".", "batch_size", "=", "tf", ".", "constant", "(", "batch_size", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "self", ".", "session_features_config", "=", "session_features_config", "\n", "self", ".", "articles_features_config", "=", "articles_features_config", "\n", "\n", "self", ".", "max_cardinality_for_ohe", "=", "max_cardinality_for_ohe", "\n", "\n", "self", ".", "novelty_reg_factor", "=", "tf", ".", "constant", "(", "novelty_reg_factor", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "diversity_reg_factor", "=", "tf", ".", "constant", "(", "diversity_reg_factor", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "self", ".", "softmax_temperature", "=", "tf", ".", "constant", "(", "softmax_temperature", ",", "dtype", "=", "tf", ".", "float32", ",", "name", "=", "'softmax_temperature'", ")", "\n", "\n", "self", ".", "recent_clicks_for_normalization", "=", "recent_clicks_for_normalization", "\n", "self", ".", "eval_cold_start", "=", "eval_cold_start", "\n", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"article_content_embeddings\"", ")", ":", "\n", "\n", "            ", "self", ".", "articles_metadata", "=", "{", "}", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "#Converting Article metadata feature vectors to constants in the graph, to avoid many copies (is saved with the graph)", "\n", "                ", "for", "feature_name", "in", "articles_metadata", ":", "\n", "                    ", "'''\n                    self.articles_metadata[feature_name] = tf.constant(articles_metadata[feature_name], \n                                                    shape=articles_metadata[feature_name].shape, \n                                                    dtype=get_tf_dtype(articles_features_config[feature_name]['dtype']))\n                    '''", "\n", "self", ".", "articles_metadata", "[", "feature_name", "]", "=", "tf", ".", "placeholder", "(", "name", "=", "\"articles_metadata\"", ",", "\n", "shape", "=", "articles_metadata", "[", "feature_name", "]", ".", "shape", ",", "\n", "dtype", "=", "get_tf_dtype", "(", "articles_features_config", "[", "feature_name", "]", "[", "'dtype'", "]", ")", ")", "\n", "\n", "\n", "", "", "self", ".", "items_vocab_size", "=", "articles_features_config", "[", "'article_id'", "]", "[", "'cardinality'", "]", "\n", "\n", "#To run on local machine (GPU card with 4 GB RAM), keep Content Article Embeddings constant in CPU memory", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "\n", "#Expects vectors within the range [-0.1, 0.1] (min-max scaled) for compatibility with other input features", "\n", "                ", "self", ".", "content_article_embeddings_matrix", "=", "tf", ".", "placeholder", "(", "name", "=", "\"content_article_embeddings_matrix\"", ",", "\n", "shape", "=", "content_article_embeddings_matrix", ".", "shape", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"articles_status\"", ")", ":", "\n", "            ", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "                ", "self", ".", "articles_recent_pop_norm", "=", "tf", ".", "placeholder", "(", "name", "=", "\"articles_recent_pop_norm\"", ",", "\n", "shape", "=", "[", "self", ".", "items_vocab_size", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "\n", "", "self", ".", "pop_recent_items_buffer", "=", "tf", ".", "placeholder", "(", "name", "=", "\"pop_recent_items_buffer\"", ",", "\n", "shape", "=", "[", "recent_clicks_buffer_max_size", "]", ",", "\n", "dtype", "=", "tf", ".", "int64", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'unique_items_clicked_recently'", ",", "family", "=", "'stats'", ",", "tensor", "=", "tf", ".", "shape", "(", "tf", ".", "unique", "(", "self", ".", "pop_recent_items_buffer", ")", "[", "0", "]", ")", "[", "0", "]", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'unique_items_clicked_recently_for_normalization'", ",", "family", "=", "'stats'", ",", "tensor", "=", "tf", ".", "shape", "(", "tf", ".", "unique", "(", "self", ".", "pop_recent_items_buffer", "[", ":", "self", ".", "recent_clicks_for_normalization", "]", ")", "[", "0", "]", ")", "[", "0", "]", ")", "\n", "\n", "\n", "#PS: variance_scaling_initializer() is recommended for RELU activations in https://arxiv.org/abs/1502.01852", "\n", "#whilst xavier_initializer is recommended for tanh activations", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"main\"", ",", "initializer", "=", "xavier_initializer", "(", ")", ")", ":", "\n", "\n", "\n", "#Initializes CAR item embeddings variable", "\n", "#self.create_item_embed_lookup_variable()", "\n", "\n", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"inputs\"", ")", ":", "\n", "\n", "                ", "item_clicked", "=", "inputs", "[", "'item_clicked'", "]", "\n", "self", ".", "item_clicked", "=", "item_clicked", "\n", "\n", "#Control features (ensuring that they keep two dims even when the batch has only one session)", "\n", "self", ".", "user_id", "=", "inputs", "[", "'user_id'", "]", "\n", "self", ".", "session_id", "=", "inputs", "[", "'session_id'", "]", "\n", "self", ".", "session_start", "=", "inputs", "[", "'session_start'", "]", "\n", "\n", "seq_lengths", "=", "inputs", "[", "'session_size'", "]", "-", "1", "#Ignoring last click only as label", "\n", "self", ".", "seq_lengths", "=", "seq_lengths", "\n", "\n", "#Creates the sessions mask and ensure that rank will be 2 (even when this batch size is 1)", "\n", "self", ".", "item_clicked_mask", "=", "tf", ".", "sequence_mask", "(", "seq_lengths", ")", "\n", "\n", "event_timestamp", "=", "tf", ".", "expand_dims", "(", "inputs", "[", "\"event_timestamp\"", "]", ",", "-", "1", ")", "\n", "self", ".", "event_timestamp", "=", "event_timestamp", "\n", "max_event_timestamp", "=", "tf", ".", "reduce_max", "(", "event_timestamp", ")", "\n", "\n", "\n", "#Retrieving last label of the sequence", "\n", "label_last_item", "=", "labels", "[", "'label_last_item'", "]", "\n", "self", ".", "label_last_item", "=", "label_last_item", "\n", "all_clicked_items", "=", "tf", ".", "concat", "(", "[", "item_clicked", ",", "label_last_item", "]", ",", "axis", "=", "1", ")", "\n", "\n", "#Labels            ", "\n", "next_item_label", "=", "labels", "[", "'label_next_item'", "]", "\n", "self", ".", "next_item_label", "=", "next_item_label", "\n", "\n", "batch_max_session_length", "=", "tf", ".", "shape", "(", "next_item_label", ")", "[", "1", "]", "\n", "batch_current_size", "=", "array_ops", ".", "shape", "(", "next_item_label", ")", "[", "0", "]", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"batch_stats\"", ")", ":", "\n", "\n", "#batch_items = self.get_masked_seq_values(inputs['item_clicked']) ", "\n", "#Known bug: The article_id 0 will not be considered as negative sample, because padding values also have value 0 ", "\n", "                ", "batch_items_nonzero", "=", "tf", ".", "boolean_mask", "(", "all_clicked_items", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "all_clicked_items", ")", ",", "tf", ".", "bool", ")", ")", "\n", "batch_items_count", "=", "tf", ".", "shape", "(", "batch_items_nonzero", ")", "[", "0", "]", "\n", "self", ".", "batch_items_count", "=", "batch_items_count", "\n", "\n", "batch_unique_items", ",", "_", "=", "tf", ".", "unique", "(", "batch_items_nonzero", ")", "\n", "batch_unique_items_count", "=", "tf", ".", "shape", "(", "batch_unique_items", ")", "[", "0", "]", "\n", "self", ".", "batch_unique_items_count", "=", "batch_unique_items_count", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'batch_items'", ",", "family", "=", "'stats'", ",", "tensor", "=", "batch_items_count", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'batch_unique_items'", ",", "family", "=", "'stats'", ",", "tensor", "=", "batch_unique_items_count", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"neg_samples\"", ")", ":", "\n", "#Samples from recent items buffer", "\n", "                ", "negative_sample_recently_clicked_ids", "=", "self", ".", "get_sample_from_recently_clicked_items_buffer", "(", "\n", "self", ".", "negative_sample_from_buffer", ")", "\n", "\n", "\n", "batch_negative_items", "=", "self", ".", "get_batch_negative_samples", "(", "all_clicked_items", ",", "\n", "additional_samples", "=", "negative_sample_recently_clicked_ids", ",", "\n", "num_negative_samples", "=", "self", ".", "negative_samples", ")", "\n", "#Ignoring last elements from second dimension, as they refer to the last labels concatenated with all_clicked_items just to ignore them in negative samples", "\n", "batch_negative_items", "=", "batch_negative_items", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "self", ".", "batch_negative_items", "=", "batch_negative_items", "\n", "\n", "\n", "", "'''\n            with tf.variable_scope(\"samples_avg_content_similarity\"):\n                sample_items_unique, _ = tf.unique(tf.concat([tf.reshape(all_clicked_items, [-1]), \n                                                               tf.reshape(batch_negative_items, [-1])], 0))\n                self.sample_items_unique = sample_items_unique\n                #tf.logging.info(\"pos_neg_items_unique TYPE: {}\".format(sample_items_unique.dtype))\n                #pos_neg_items_unique_idxs = tf.cast(tf.range(tf.shape(sample_items_unique)[0], dtype=tf.int32), tf.int64)\n                #tf.logging.info(\"pos_neg_items_unique_idxs TYPE: {}\".format(pos_neg_items_unique_idxs.dtype))\n\n                sample_items_article_embeddings = tf.gather(self.content_article_embeddings_matrix, sample_items_unique)\n\n                # Normalize each row\n                sample_items_article_embeddings_normalized = tf.nn.l2_normalize(sample_items_article_embeddings, axis = 1)\n\n                # multiply row i with row j using transpose\n                # element wise product\n                sample_items_content_cos_sims_batch = tf.matmul(sample_items_article_embeddings_normalized, \n                                                                 sample_items_article_embeddings_normalized,\n                                                                 transpose_b=True\n                                                                 ) \n                #As the original interval of cosine similarity is [-1,1], scaling to the interval [0,1]                                                                 \n                sample_items_content_cos_sims_batch = (sample_items_content_cos_sims_batch + tf.constant(1.0, tf.float32)) / tf.constant(2.0, tf.float32)\n\n                #Making the diagonal 0 (self.similarities which are always one)\n                sample_items_content_cos_sims_batch = tf.multiply(sample_items_content_cos_sims_batch, \n                                                                  tf.abs(tf.eye(tf.shape(sample_items_content_cos_sims_batch)[0]) - 1))\n\n                #Average cross-similarity for each sample item\n                sample_items_content_cos_sims_mean = tf.reduce_mean(sample_items_content_cos_sims_batch, \n                                                                    axis=1)\n                self.sample_items_content_cos_sims_mean = sample_items_content_cos_sims_mean\n            '''", "\n", "\n", "\n", "#WARNING: Must keep these variables under the same variable scope, to avoid leaking the positive item to the network (probably due to normalization)", "\n", "with", "tf", ".", "variable_scope", "(", "\"user_items_contextual_features\"", ")", ":", "\n", "                ", "user_context_features", "=", "self", ".", "get_features", "(", "inputs", ",", "\n", "features_config", "=", "self", ".", "session_features_config", "[", "'sequence_features'", "]", ",", "\n", "features_to_ignore", "=", "SESSION_REQ_SEQ_FEATURES", ")", "\n", "\n", "#If there is no user contextual features, creates a dummy variable to not break following concats", "\n", "if", "user_context_features", "!=", "None", ":", "\n", "                    ", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"user_context_features\"", ",", "user_context_features", ")", "\n", "", "", "else", ":", "\n", "#Dummy tensor with zeroed values", "\n", "                    ", "user_context_features", "=", "tf", ".", "zeros_like", "(", "tf", ".", "expand_dims", "(", "item_clicked", ",", "-", "1", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "\n", "", "input_items_features", "=", "self", ".", "get_item_features", "(", "item_clicked", ",", "event_timestamp", ",", "'clicked'", ")", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "\"input_items_features\"", ",", "input_items_features", ")", "\n", "\n", "", "input_user_items_features_concat", "=", "tf", ".", "concat", "(", "[", "user_context_features", ",", "input_items_features", "]", ",", "axis", "=", "2", ")", "\n", "input_user_items_features", "=", "self", ".", "scale_center_features", "(", "input_user_items_features_concat", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "\"input_user_items_features\"", ",", "input_user_items_features", ")", "\n", "\n", "", "input_user_items_features", "=", "tf", ".", "layers", ".", "dropout", "(", "input_user_items_features", ",", "\n", "rate", "=", "1.0", "-", "self", ".", "keep_prob", ",", "\n", "training", "=", "self", ".", "is_training", ")", "\n", "\n", "\n", "positive_items_features", "=", "self", ".", "get_item_features", "(", "next_item_label", ",", "max_event_timestamp", ",", "'positive'", ")", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "\"positive_items_features\"", ",", "positive_items_features", ")", "\n", "", "positive_user_items_features_concat", "=", "tf", ".", "concat", "(", "[", "user_context_features", ",", "positive_items_features", "]", ",", "axis", "=", "2", ")", "\n", "positive_user_items_features", "=", "self", ".", "scale_center_features", "(", "positive_user_items_features_concat", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "\"positive_user_items_features\"", ",", "input_user_items_features", ")", "\n", "\n", "", "positive_user_items_features", "=", "tf", ".", "layers", ".", "dropout", "(", "positive_user_items_features", ",", "\n", "rate", "=", "1.0", "-", "self", ".", "keep_prob", ",", "\n", "training", "=", "self", ".", "is_training", ")", "\n", "\n", "negative_items_features", "=", "self", ".", "get_item_features", "(", "batch_negative_items", ",", "max_event_timestamp", ",", "'negative'", ")", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "\"negative_items_features\"", ",", "negative_items_features", ")", "\n", "\n", "", "user_context_features_tiled", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "user_context_features", ",", "2", ")", ",", "(", "1", ",", "1", ",", "tf", ".", "shape", "(", "negative_items_features", ")", "[", "2", "]", ",", "1", ")", ")", "\n", "negative_user_items_features_concat", "=", "tf", ".", "concat", "(", "[", "user_context_features_tiled", ",", "negative_items_features", "]", ",", "axis", "=", "3", ")", "\n", "\n", "\n", "negative_user_items_features", "=", "self", ".", "scale_center_features", "(", "negative_user_items_features_concat", ",", "begin_norm_axis", "=", "3", ")", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "\"negative_user_items_features\"", ",", "negative_user_items_features", ")", "\n", "\n", "", "negative_user_items_features", "=", "tf", ".", "layers", ".", "dropout", "(", "negative_user_items_features", ",", "\n", "rate", "=", "1.0", "-", "self", ".", "keep_prob", ",", "\n", "training", "=", "self", ".", "is_training", ")", "\n", "\n", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"CAR\"", ")", ":", "\n", "                ", "PreCAR_dense", "=", "tf", ".", "layers", ".", "Dense", "(", "CAR_embedding_size", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "\n", "kernel_initializer", "=", "variance_scaling_initializer", "(", ")", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ",", "\n", "name", "=", "\"PreCAR_representation\"", "\n", ")", "\n", "\n", "input_contextual_item_embedding_pre_CAR", "=", "PreCAR_dense", "(", "input_user_items_features", ")", "\n", "\n", "CAR_dense", "=", "tf", ".", "layers", ".", "Dense", "(", "CAR_embedding_size", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "tanh", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ",", "\n", "name", "=", "\"CAR_representation\"", "\n", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"user_personalized_contextual_article_embedding\"", ")", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "\"input\"", ")", ":", "\n", "                    ", "input_contextual_item_embedding", "=", "CAR_dense", "(", "input_contextual_item_embedding_pre_CAR", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"input_contextual_item_embedding\"", ",", "input_contextual_item_embedding", ")", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"positive\"", ")", ":", "\n", "                    ", "positive_contextual_item_embedding", "=", "CAR_dense", "(", "PreCAR_dense", "(", "positive_user_items_features", ")", ")", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"positive_contextual_item_embedding\"", ",", "positive_contextual_item_embedding", ")", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"negative\"", ")", ":", "\n", "                    ", "negative_contextual_item_embedding", "=", "CAR_dense", "(", "PreCAR_dense", "(", "negative_user_items_features", ")", ")", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"negative_contextual_item_embedding\"", ",", "negative_contextual_item_embedding", ")", "\n", "\n", "#Building RNN", "\n", "", "", "", "rnn_outputs", "=", "self", ".", "build_rnn", "(", "input_contextual_item_embedding", ",", "seq_lengths", ",", "rnn_units", "=", "rnn_units", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"session_representation\"", ")", ":", "\n", "                ", "rnn_outputs_fc1", "=", "tf", ".", "layers", ".", "dense", "(", "rnn_outputs", ",", "512", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "\n", "kernel_initializer", "=", "variance_scaling_initializer", "(", ")", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ",", "\n", "name", "=", "\"FC1\"", "\n", ")", "\n", "\n", "rnn_outputs_fc1_dropout", "=", "tf", ".", "layers", ".", "dropout", "(", "inputs", "=", "rnn_outputs_fc1", ",", "\n", "rate", "=", "1.0", "-", "self", ".", "keep_prob", ",", "\n", "training", "=", "self", ".", "is_training", ")", "\n", "\n", "\n", "rnn_outputs_fc2", "=", "tf", ".", "layers", ".", "dense", "(", "rnn_outputs_fc1_dropout", ",", "CAR_embedding_size", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "tanh", ",", "\n", "name", "=", "'FC2'", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ")", "\n", "\n", "#tf.summary.scalar('rnn_outputs_fc2/fraction_of_zero_values', tf.nn.zero_fraction(rnn_outputs_fc1))", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "\"rnn_outputs_fc2\"", ",", "rnn_outputs_fc2", ")", "\n", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"predicted_contextual_item_embedding\"", ")", ":", "\n", "#Continuing with DSSM losss", "\n", "#Apply l2-norm to be able to compute cosine similarity by matrix multiplication", "\n", "#predicted_contextual_item_embedding = tf.nn.l2_normalize(rnn_outputs_fc2, axis=-1)", "\n", "                ", "predicted_contextual_item_embedding", "=", "rnn_outputs_fc2", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "\"predicted_contextual_item_embedding\"", ",", "predicted_contextual_item_embedding", ")", "\n", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"recommendations_ranking\"", ")", ":", "\n", "\n", "\n", "                ", "matching_dense_layer_1", "=", "tf", ".", "layers", ".", "Dense", "(", "128", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "\n", "kernel_initializer", "=", "variance_scaling_initializer", "(", ")", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ",", "\n", "name", "=", "\"matching_dense_layer_1\"", "\n", ")", "\n", "\n", "matching_dense_layer_2", "=", "tf", ".", "layers", ".", "Dense", "(", "64", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "\n", "kernel_initializer", "=", "variance_scaling_initializer", "(", ")", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ",", "\n", "name", "=", "\"matching_dense_layer_2\"", "\n", ")", "\n", "\n", "matching_dense_layer_3", "=", "tf", ".", "layers", ".", "Dense", "(", "32", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "\n", "kernel_initializer", "=", "variance_scaling_initializer", "(", ")", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ",", "\n", "name", "=", "\"matching_dense_layer_3\"", "\n", ")", "\n", "\n", "matching_dense_layer_4", "=", "tf", ".", "layers", ".", "Dense", "(", "1", ",", "\n", "activation", "=", "None", ",", "\n", "kernel_initializer", "=", "tf", ".", "initializers", ".", "lecun_uniform", "(", ")", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ",", "\n", "name", "=", "\"matching_dense_layer_4\"", "\n", ")", "\n", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"cos_sim_positive\"", ")", ":", "\n", "\n", "                    ", "positive_multiplied_embeddings", "=", "tf", ".", "multiply", "(", "positive_contextual_item_embedding", ",", "\n", "predicted_contextual_item_embedding", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"train/positive_multiplied_embeddings\"", ",", "positive_multiplied_embeddings", ")", "\n", "\n", "\n", "", "cos_sim_positive", "=", "matching_dense_layer_4", "(", "matching_dense_layer_3", "(", "matching_dense_layer_2", "(", "matching_dense_layer_1", "(", "positive_multiplied_embeddings", ")", ")", ")", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"train/cos_sim_positive\"", ",", "\n", "values", "=", "tf", ".", "boolean_mask", "(", "cos_sim_positive", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "next_item_label", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"cos_sim_negative\"", ")", ":", "\n", "                    ", "negative_multiplied_embeddings", "=", "tf", ".", "multiply", "(", "negative_contextual_item_embedding", ",", "\n", "tf", ".", "expand_dims", "(", "predicted_contextual_item_embedding", ",", "2", ")", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"train/negative_multiplied_embeddings\"", ",", "negative_multiplied_embeddings", ")", "\n", "\n", "", "cos_sim_negative", "=", "matching_dense_layer_4", "(", "matching_dense_layer_3", "(", "matching_dense_layer_2", "(", "matching_dense_layer_1", "(", "negative_multiplied_embeddings", ")", ")", ")", ")", "\n", "cos_sim_negative", "=", "tf", ".", "squeeze", "(", "cos_sim_negative", ",", "axis", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"train/cos_sim_negative\"", ",", "\n", "values", "=", "tf", ".", "boolean_mask", "(", "cos_sim_negative", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "next_item_label", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"softmax_function\"", ")", ":", "\n", "\n", "#Concatenating cosine similarities (positive + K sampled negative)", "\n", "                    ", "cos_sim_concat", "=", "tf", ".", "concat", "(", "[", "cos_sim_positive", ",", "cos_sim_negative", "]", ",", "axis", "=", "2", ")", "\n", "\n", "#Computing softmax over cosine similarities", "\n", "cos_sim_concat_scaled", "=", "cos_sim_concat", "/", "self", ".", "softmax_temperature", "\n", "items_prob", "=", "tf", ".", "nn", ".", "softmax", "(", "cos_sim_concat_scaled", ")", "\n", "\n", "neg_items_prob", "=", "tf", ".", "nn", ".", "softmax", "(", "cos_sim_negative", "/", "self", ".", "softmax_temperature", ")", "\n", "\n", "\n", "", "if", "self", ".", "eval_cold_start", "or", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ")", ":", "\n", "                    ", "pos_neg_items_ids", "=", "tf", ".", "concat", "(", "[", "tf", ".", "expand_dims", "(", "next_item_label", ",", "-", "1", ")", ",", "batch_negative_items", "]", ",", "2", ")", "\n", "predicted_item_ids", ",", "predicted_item_probs", "=", "self", ".", "rank_items_by_predicted_prob", "(", "pos_neg_items_ids", ",", "items_prob", ")", "\n", "self", ".", "predicted_item_ids", "=", "predicted_item_ids", "\n", "self", ".", "predicted_item_probs", "=", "predicted_item_probs", "\n", "\n", "\n", "", "if", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ")", ":", "\n", "#Computing evaluation metrics", "\n", "                    ", "self", ".", "define_eval_metrics", "(", "next_item_label", ",", "predicted_item_ids", ")", "\n", "\n", "\n", "#if mode == tf.estimator.ModeKeys.TRAIN:", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"samples_popularity\"", ")", ":", "\n", "                    ", "positive_articles_norm_pop", "=", "self", ".", "get_items_norm_popularity_feature", "(", "next_item_label", ",", "summary_suffix", "=", "'positive'", ")", "\n", "\n", "negative_articles_articles_norm_pop", "=", "self", ".", "get_items_norm_popularity_feature", "(", "batch_negative_items", ",", "summary_suffix", "=", "'negative'", ")", "\n", "negative_articles_articles_norm_pop_squeezed", "=", "tf", ".", "squeeze", "(", "negative_articles_articles_norm_pop", ",", "axis", "=", "-", "1", ")", "\n", "negative_articles_articles_norm_pop_tiled", "=", "negative_articles_articles_norm_pop_squeezed", "\n", "\n", "candidate_samples_norm_pop", "=", "tf", ".", "concat", "(", "[", "positive_articles_norm_pop", ",", "negative_articles_articles_norm_pop_tiled", "]", ",", "axis", "=", "2", ")", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"candidate_samples_norm_pop\"", ",", "values", "=", "tf", ".", "boolean_mask", "(", "candidate_samples_norm_pop", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "next_item_label", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "", "negative_samples_norm_pop_scaled", "=", "self", ".", "get_items_pop_novelty_feature", "(", "negative_articles_articles_norm_pop_tiled", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "\"negative_samples_norm_pop_scaled\"", ",", "values", "=", "tf", ".", "boolean_mask", "(", "negative_samples_norm_pop_scaled", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "next_item_label", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "\n", "\n", "", "", "'''\n                with tf.variable_scope(\"samples_diversity\"):\n                    \n                    #pos_neg_items_unique, _ = tf.unique(tf.concat([tf.reshape(next_item_label, [-1]), \n                    #                                                tf.reshape(batch_negative_items, [-1])], 0))\n                    ####tf.logging.info(\"pos_neg_items_unique TYPE: {}\".format(pos_neg_items_unique.dtype))\n                    ####pos_neg_items_unique_idxs = tf.cast(tf.range(tf.shape(pos_neg_items_unique)[0], dtype=tf.int32), tf.int64)\n                    #####tf.logging.info(\"pos_neg_items_unique_idxs TYPE: {}\".format(pos_neg_items_unique_idxs.dtype))\n\n                    #pos_neg_items_article_embeddings = tf.gather(self.content_article_embeddings_matrix, pos_neg_items_unique)\n\n                    # Normalize each row\n                    #pos_neg_items_article_embeddings_normalized = tf.nn.l2_normalize(pos_neg_items_article_embeddings, axis = 1)\n\n                    # multiply row i with row j using transpose\n                    # element wise product\n                    #pos_neg_items_article_cos_sims_batch = tf.matmul(pos_neg_items_article_embeddings_normalized, \n                    #                                            pos_neg_items_article_embeddings_normalized,\n                    #                                            transpose_b=True\n                    #                                            )     \n\n                    #Cleaning self.similarities in the diagonal (always one)\n                    #inverted_diag_matrix = tf.abs(tf.eye(tf.shape(pos_neg_items_article_cos_sims_batch)[0]) - 1)\n                    #pos_neg_items_article_cos_sims_batch = tf.multiply(pos_neg_items_article_cos_sims_batch, inverted_diag_matrix)\n                    \n\n                    neg_item_ids_tiled = tf.tile(tf.expand_dims(batch_negative_items, axis=1), \n                                                    (1,batch_max_session_length,1)) \n                    #tf.logging.info(\"neg_item_ids_tiled: {}\".format(neg_item_ids_tiled.get_shape()))\n                    #tf.logging.info(\"next_item_label_expanded: {}\".format(next_item_label_expanded.get_shape()))\n                    pos_neg_item_ids = tf.concat([tf.expand_dims(next_item_label, axis=[-1]), \n                                                    neg_item_ids_tiled], axis=2, \n                                                    name=\"pos_neg_item_ids_concat\")\n\n                    \n                    #self.unique_ids_table = tf.contrib.lookup.HashTable(\n                    #          tf.contrib.lookup.KeyValueTensorInitializer(pos_neg_items_unique, pos_neg_items_unique_idxs), -1\n                    #        )\n                    #pos_neg_item_ids_idxs = self.unique_ids_table.lookup(pos_neg_item_ids, name=\"lookup27\")\n\n                    #Looking up for the corresponding intra cosine similarities in the third-dimension\n                    #Ps. Tried to use tf.contrib.lookup.HashTable() but didn't work due to initialization dependencies\n                    pos_neg_item_ids_idxs = tf.map_fn(lambda x: \n                                                                tf.map_fn(lambda y: \n                                                                                    paired_permutations(\n                                                                                        tf.reshape(\n                                                                                            tf.map_fn(lambda z: tf.where(tf.equal(sample_items_unique, z))[0], \n                                                                                                        y)\n                                                                                            , [-1])\n                                                                                    ),\n                                                                            x),\n                                                        pos_neg_item_ids)\n\n                    #tf.logging.warn(\"pos_neg_item_ids_idxs: {}\".format(pos_neg_item_ids_idxs.get_shape()))\n                    pos_neg_item_ids_idxs_reshaped = tf.reshape(pos_neg_item_ids_idxs, tf.concat([tf.shape(pos_neg_item_ids), [-1, 2]], axis=0))\n                    #tf.logging.warn(\"pos_neg_item_ids_idxs_reshaped: {}\".format(pos_neg_item_ids_idxs_reshaped.get_shape()))\n\n\n                    pos_neg_item_sims = tf.gather_nd(sample_items_content_cos_sims_batch, \n                                                        pos_neg_item_ids_idxs_reshaped,\n                                                        name=\"pos_neg_item_sims_gather_nd_op\")\n                    #tf.summary.histogram(\"pos_neg_item_sims\", pos_neg_item_sims)\n                    #tf.logging.warn(\"pos_neg_item_sims: {}\".format(pos_neg_item_sims.get_shape()))\n\n                    #Testing the effect of squaring the cross-distances, to increase the impact of highly similar items\n                    #pos_neg_item_sims_squared = tf.multiply(pos_neg_item_sims,pos_neg_item_sims)\n\n                                                        \n                    #Multiplying columns from similarity matrix by items prob. (weighting) so that items with the largest prob have more impact on diversity loss\n                    items_prob_tiled = tf.tile(tf.expand_dims(items_prob, axis=2), \n                                                    (1,1,tf.shape(items_prob)[2],1)) \n                    pos_neg_items_article_cos_sims_weighted = tf.multiply(pos_neg_item_sims, \n                                                                            #pos_neg_item_sims_squared,\n                                                                            items_prob_tiled, \n                                                                            name='pos_neg_items_article_cos_sims_weighted_op')                  \n\n\n\n                    #Computing the weighted sum of similarity of items (by row, in similarity matrix)\n                    pos_neg_items_article_cos_sims_weighted_sum = tf.reduce_sum(pos_neg_items_article_cos_sims_weighted, axis=3, keepdims=False)\n                    #tf.summary.histogram(\"pos_neg_items_article_cos_sims_weighted_sum\", pos_neg_items_article_cos_sims_weighted_sum)\n\n                    #Computing sum of items probs (always equal to 1.0 (because of softmax))\n                    pos_neg_items_prob_sum = tf.reduce_sum(items_prob_tiled, axis=3, keepdims=False)\n                    #tf.summary.histogram(\"pos_neg_items_prob_sum\", pos_neg_items_prob_sum)\n                '''", "\n", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"loss\"", ")", ":", "\n", "#Computing batch loss", "\n", "                ", "loss_mask", "=", "tf", ".", "to_float", "(", "self", ".", "item_clicked_mask", ")", "\n", "\n", "#Computing the probability of the positive item (label)", "\n", "positive_prob", "=", "items_prob", "[", ":", ",", ":", ",", "0", "]", "\n", "negative_probs", "=", "items_prob", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "\n", "\n", "#Summary of first element of the batch sequence (because others might be masked)", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "\"positive_prob\"", ",", "positive_prob", "[", ":", ",", "0", "]", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "\"negative_probs\"", ",", "negative_probs", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "\n", "\n", "#reg_loss = self.reg_weight_decay * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name))", "\n", "", "reg_loss", "=", "tf", ".", "losses", ".", "get_regularization_loss", "(", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"reg_loss\"", ",", "family", "=", "'train'", ",", "tensor", "=", "reg_loss", ")", "\n", "\n", "\n", "#XE loss", "\n", "xe_loss", "=", "tf", ".", "multiply", "(", "tf", ".", "log", "(", "positive_prob", ")", ",", "loss_mask", ")", "\n", "\n", "\n", "#Averaging the loss by the number of masked items in the batch", "\n", "cosine_sim_loss", "=", "-", "tf", ".", "reduce_sum", "(", "xe_loss", ")", "/", "tf", ".", "reduce_sum", "(", "loss_mask", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"cosine_sim_loss\"", ",", "family", "=", "'train'", ",", "tensor", "=", "cosine_sim_loss", ")", "\n", "\n", "self", ".", "total_loss", "=", "cosine_sim_loss", "+", "reg_loss", "\n", "\n", "#if mode == tf.estimator.ModeKeys.TRAIN:", "\n", "items_prob_masked", "=", "tf", ".", "multiply", "(", "items_prob", ",", "tf", ".", "expand_dims", "(", "loss_mask", ",", "-", "1", ")", ",", "name", "=", "'items_prob_masked_op'", ")", "\n", "\n", "\n", "if", "novelty_reg_factor", ">", "0.0", ":", "\n", "                    ", "with", "tf", ".", "variable_scope", "(", "\"novelty_loss\"", ")", ":", "\n", "                        ", "masked_nov_reg", "=", "self", ".", "novelty_reg_factor", "*", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "tf", ".", "multiply", "(", "neg_items_prob", ",", "negative_samples_norm_pop_scaled", ")", ",", "tf", ".", "expand_dims", "(", "loss_mask", ",", "-", "1", ")", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                            ", "tf", ".", "summary", ".", "histogram", "(", "\"masked_nov_reg\"", ",", "values", "=", "tf", ".", "boolean_mask", "(", "masked_nov_reg", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "next_item_label", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "", "nov_reg_loss", "=", "tf", ".", "reduce_sum", "(", "masked_nov_reg", ")", "/", "tf", ".", "reduce_sum", "(", "loss_mask", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"nov_reg_loss\"", ",", "family", "=", "'train'", ",", "tensor", "=", "nov_reg_loss", ")", "\n", "self", ".", "total_loss", "=", "self", ".", "total_loss", "-", "nov_reg_loss", "\n", "\n", "", "", "'''\n                with tf.variable_scope(\"diversity_loss\"):\n                    #if mode != tf.estimator.ModeKeys.EVAL:\n                    #    tf.summary.histogram(\"pos_neg_item_avg_sim\", pos_neg_items_article_cos_sims_weighted_mean)   \n                    items_prob_x_avg_sim_masked = tf.multiply(items_prob_masked, pos_neg_items_article_cos_sims_weighted_sum, name='pos_neg_item_avg_sim_masked_op')\n                    #if mode != tf.estimator.ModeKeys.EVAL:\n                    #    tf.summary.histogram(\"items_prob_x_avg_sim\", items_prob_x_avg_sim)                               \n                    \n                    #TODO: Evaluate if the normalization shouln't be made by next-click, instead of a global weighted average\n\n                    #TODO: Test more aggressive errors on popular as similar items (e.g. the exp, like in softmax)\n                    diversity_normalization = tf.multiply(items_prob_masked, pos_neg_items_prob_sum, name='diversity_normalization_op')\n                    diversity_reg_loss = tf.reduce_sum(items_prob_x_avg_sim_masked) / tf.reduce_sum(diversity_normalization) \n                    diversity_reg_loss = self.diversity_reg_factor*diversity_reg_loss\n                    tf.summary.scalar(\"diversity_reg_loss\", family='train', tensor=diversity_reg_loss)\n                    \n                    #self.total_loss = self.total_loss + diversity_reg_loss\n                '''", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"total_loss\"", ",", "family", "=", "'train'", ",", "tensor", "=", "self", ".", "total_loss", ")", "\n", "\n", "", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'training'", ")", ":", "\n", "                    ", "opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "lr", ",", "\n", "beta1", "=", "0.9", ",", "\n", "beta2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-08", ")", "\n", "\n", "\n", "#Necessary to run update ops for batch_norm, streaming metrics", "\n", "update_ops", "=", "tf", ".", "get_collection", "(", "tf", ".", "GraphKeys", ".", "UPDATE_OPS", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "update_ops", ")", ":", "\n", "# Get the gradient pairs (Tensor, Variable)", "\n", "                        ", "grads", "=", "opt", ".", "compute_gradients", "(", "self", ".", "total_loss", ")", "\n", "# Update the weights wrt to the gradient", "\n", "self", ".", "train", "=", "opt", ".", "apply_gradients", "(", "grads", ",", "\n", "global_step", "=", "tf", ".", "train", ".", "get_global_step", "(", ")", "#self.gs", "\n", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "# Add histograms for trainable variables.", "\n", "                            ", "for", "grad", ",", "var", "in", "grads", ":", "\n", "                                ", "if", "grad", "is", "not", "None", ":", "\n", "                                    ", "tf", ".", "summary", ".", "histogram", "(", "var", ".", "op", ".", "name", "+", "'/gradients'", ",", "grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_features": [[730, 774], ["tensorflow.one_hot", "tensorflow.variable_scope", "tensorflow.variable_scope", "nar_model.get_embedding_size", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "features_list.append", "len", "tensorflow.concat", "tensorflow.summary.histogram", "tensorflow.contrib.layers.l2_regularizer", "nar_model.NARModuleModel.get_features.cat_ohe"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.get_embedding_size"], ["", "", "", "", "", "", "", "", "def", "get_features", "(", "self", ",", "inputs", ",", "features_config", ",", "\n", "features_to_ignore", ")", ":", "\n", "\n", "        ", "def", "cat_ohe", "(", "feature_name", ",", "size", ",", "inputs", ")", ":", "\n", "            ", "return", "tf", ".", "one_hot", "(", "inputs", "[", "feature_name", "]", ",", "size", ",", "name", "=", "\"{}_cat_one_hot\"", ".", "format", "(", "feature_name", ")", ")", "\n", "\n", "", "def", "cat_embed", "(", "feature_name", ",", "size", ",", "inputs", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"{}_cat_embedding\"", ".", "format", "(", "feature_name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "                ", "dim", "=", "get_embedding_size", "(", "size", ")", "\n", "embeddings", "=", "tf", ".", "get_variable", "(", "\"{}_embedding\"", ".", "format", "(", "feature_name", ")", ",", "shape", "=", "[", "size", ",", "dim", "]", ",", "\n", "regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ")", "\n", "lookup", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embeddings", ",", "ids", "=", "inputs", "[", "feature_name", "]", ")", "#, max_norm=1)", "\n", "return", "lookup", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"features\"", ")", ":", "\n", "            ", "features_list", "=", "[", "]", "\n", "for", "feature_name", "in", "features_config", ":", "\n", "#Ignores item_clicked and timestamp as user contextual features", "\n", "                ", "if", "feature_name", "in", "features_to_ignore", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "features_config", "[", "feature_name", "]", "[", "'type'", "]", "==", "'categorical'", ":", "\n", "                    ", "size", "=", "features_config", "[", "feature_name", "]", "[", "'cardinality'", "]", "\n", "if", "features_config", "[", "feature_name", "]", "[", "'cardinality'", "]", "<=", "self", ".", "max_cardinality_for_ohe", ":", "\n", "                        ", "feature_op", "=", "cat_ohe", "(", "feature_name", ",", "size", ",", "inputs", ")", "\n", "", "else", ":", "\n", "                        ", "feature_op", "=", "cat_embed", "(", "feature_name", ",", "size", ",", "inputs", ")", "\n", "", "", "elif", "features_config", "[", "feature_name", "]", "[", "'type'", "]", "==", "'numerical'", ":", "\n", "                    ", "feature_op", "=", "tf", ".", "expand_dims", "(", "inputs", "[", "feature_name", "]", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "'Invalid feature type: {}'", ".", "format", "(", "feature_name", ")", ")", "\n", "\n", "\n", "", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "feature_name", ",", "family", "=", "'stats'", ",", "\n", "values", "=", "feature_op", ")", "\n", "\n", "", "features_list", ".", "append", "(", "feature_op", ")", "\n", "\n", "", "if", "len", "(", "features_list", ")", ">", "0", ":", "\n", "                ", "features_concat", "=", "tf", ".", "concat", "(", "features_list", ",", "axis", "=", "-", "1", ")", "\n", "return", "features_concat", "\n", "", "else", ":", "\n", "                ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.rank_items_by_predicted_prob": [[777, 795], ["tensorflow.variable_scope", "tensorflow.nn.top_k", "tensorflow.gather_nd", "tensorflow.concat", "tensorflow.reshape", "tensorflow.contrib.layers.dense_to_sparse", "tensorflow.gather_nd", "tensorflow.shape", "tensorflow.expand_dims", "tensorflow.shape", "tensorflow.cast"], "methods", ["None"], ["", "", "", "def", "rank_items_by_predicted_prob", "(", "self", ",", "item_ids", ",", "items_prob", ")", ":", "\n", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"predicted_items\"", ")", ":", "\n", "\n", "#Ranking item ids by their predicted probabilities", "\n", "            ", "items_top_prob", "=", "tf", ".", "nn", ".", "top_k", "(", "items_prob", ",", "k", "=", "tf", ".", "shape", "(", "items_prob", ")", "[", "2", "]", ")", "\n", "items_top_prob_indexes", "=", "items_top_prob", ".", "indices", "\n", "predicted_item_probs", "=", "items_top_prob", ".", "values", "\n", "\n", "items_top_prob_indexes_idx", "=", "tf", ".", "contrib", ".", "layers", ".", "dense_to_sparse", "(", "items_top_prob_indexes", ",", "eos_token", "=", "-", "1", ")", ".", "indices", "\n", "\n", "items_top_prob_indexes_val", "=", "tf", ".", "gather_nd", "(", "items_top_prob_indexes", ",", "items_top_prob_indexes_idx", ")", "\n", "#Takes the first two columns of the index and use sorted indices as the last column", "\n", "items_top_prob_reordered_indexes", "=", "tf", ".", "concat", "(", "[", "items_top_prob_indexes_idx", "[", ":", ",", ":", "2", "]", ",", "\n", "tf", ".", "expand_dims", "(", "tf", ".", "cast", "(", "items_top_prob_indexes_val", ",", "tf", ".", "int64", ")", ",", "1", ")", "]", ",", "1", ")", "\n", "predicted_item_ids", "=", "tf", ".", "reshape", "(", "tf", ".", "gather_nd", "(", "item_ids", ",", "items_top_prob_reordered_indexes", ")", ",", "\n", "tf", ".", "shape", "(", "item_ids", ")", ")", "\n", "return", "predicted_item_ids", ",", "predicted_item_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.define_eval_metrics": [[797, 836], ["tensorflow.variable_scope", "tensorflow.expand_dims", "tensorflow.contrib.metrics.sparse_recall_at_top_k", "nar_model.NARModuleModel.define_mrr_metric", "tensorflow.to_float"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.define_mrr_metric"], ["", "", "def", "define_eval_metrics", "(", "self", ",", "next_item_label", ",", "predicted_item_ids", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"evaluation_metrics\"", ")", ":", "\n", "            ", "'''\n            with tf.variable_scope(\"predicted_items\"):\n\n                next_item_label_expanded = tf.expand_dims(next_item_label, -1)\n                \n                pos_neg_items_concat = tf.concat([next_item_label_expanded, batch_negative_items], 2)\n\n                #Predicting item ids from [positive + k negative samples]\n                items_top_prob = tf.nn.top_k(items_prob,  k=tf.shape(items_prob)[2])\n                items_top_prob_indexes = items_top_prob.indices\n                self.items_top_prob_values = items_top_prob.values\n\n                items_top_prob_indexes_idx = tf.contrib.layers.dense_to_sparse(items_top_prob_indexes, eos_token=-1).indices\n                \n                items_top_prob_indexes_val = tf.gather_nd(items_top_prob_indexes, items_top_prob_indexes_idx)\n                #Takes the first two columns of the index and use sorted indices as the last column\n                items_top_prob_reordered_indexes = tf.concat([items_top_prob_indexes_idx[:,:2], \n                                                              tf.expand_dims(tf.cast(items_top_prob_indexes_val, tf.int64), 1)], 1)\n                predicted_item_ids = tf.reshape(tf.gather_nd(pos_neg_items_concat, items_top_prob_reordered_indexes), \n                                                tf.shape(pos_neg_items_concat))\n                self.predicted_item_ids = predicted_item_ids\n            '''", "\n", "\n", "next_item_label_expanded", "=", "tf", ".", "expand_dims", "(", "next_item_label", ",", "-", "1", ")", "\n", "\n", "\n", "#Computing Recall@N", "\n", "self", ".", "recall_at_n", ",", "self", ".", "recall_at_n_update_op", "=", "tf", ".", "contrib", ".", "metrics", ".", "sparse_recall_at_top_k", "(", "\n", "labels", "=", "next_item_label_expanded", ",", "\n", "top_k_predictions", "=", "predicted_item_ids", "[", ":", ",", ":", ",", ":", "self", ".", "metrics_top_n", "]", ",", "\n", "weights", "=", "tf", ".", "to_float", "(", "self", ".", "item_clicked_mask", ")", ",", "\n", "name", "=", "'hitrate_at_n'", ")", "\n", "\n", "\n", "#Computing MRR@N", "\n", "self", ".", "mrr", ",", "self", ".", "mrr_update_op", "=", "self", ".", "define_mrr_metric", "(", "predicted_item_ids", ",", "next_item_label_expanded", ",", "\n", "topk", "=", "self", ".", "metrics_top_n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.define_ndcg_metric": [[842, 857], ["tensorflow.variable_scope", "tensorflow.to_int32", "nar_model.tf_ndcg_at_k", "tensorflow.to_float", "tensorflow.metrics.mean", "tensorflow.equal"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.tf_ndcg_at_k"], ["", "", "def", "define_ndcg_metric", "(", "self", ",", "predicted_item_ids", ",", "next_item_label_expanded", ",", "topk", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"ndcg\"", ")", ":", "\n", "#Computing NDCG", "\n", "            ", "predicted_correct", "=", "tf", ".", "to_int32", "(", "tf", ".", "equal", "(", "predicted_item_ids", ",", "next_item_label_expanded", ")", ")", "\n", "ndcg_predicted", "=", "tf_ndcg_at_k", "(", "predicted_correct", ",", "topk", ")", "\n", "\n", "#Combining masks of padding items and NDCG zeroed values (because the correct value is not in the top n)", "\n", "ndcg_mask", "=", "tf", ".", "to_float", "(", "self", ".", "item_clicked_mask", ")", "\n", "\n", "ndcg_mean", ",", "ndcg_mean_update_op", "=", "tf", ".", "metrics", ".", "mean", "(", "\n", "values", "=", "ndcg_predicted", ",", "\n", "weights", "=", "ndcg_mask", ",", "\n", "name", "=", "'ndcg_at_n'", ")", "\n", "\n", "return", "ndcg_mean", ",", "ndcg_mean_update_op", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.define_mrr_metric": [[859, 886], ["tensorflow.variable_scope", "tensorflow.div", "tensorflow.reduce_sum", "tensorflow.concat", "tensorflow.metrics.mean", "tensorflow.constant", "tensorflow.cast", "tensorflow.to_int32", "tensorflow.size", "tensorflow.zeros", "tensorflow.constant", "tensorflow.where", "tensorflow.logical_and", "tensorflow.equal", "tensorflow.expand_dims"], "methods", ["None"], ["", "", "def", "define_mrr_metric", "(", "self", ",", "predicted_item_ids", ",", "next_item_label_expanded", ",", "topk", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"mrr\"", ")", ":", "\n", "            ", "reciprocal_ranks", "=", "tf", ".", "div", "(", "tf", ".", "constant", "(", "1.0", ")", ",", "tf", ".", "cast", "(", "tf", ".", "constant", "(", "1", ",", "tf", ".", "int64", ")", "+", "tf", ".", "where", "(", "\n", "tf", ".", "logical_and", "(", "\n", "tf", ".", "equal", "(", "next_item_label_expanded", ",", "\n", "predicted_item_ids", "[", ":", ",", ":", ",", ":", "topk", "]", ")", ",", "\n", "tf", ".", "expand_dims", "(", "self", ".", "item_clicked_mask", ",", "-", "1", ")", "#Apply mask to sessions with padded items", "\n", ")", "\n", ")", "[", ":", ",", "2", "]", ",", "\n", "tf", ".", "float32", ")", ")", "\n", "\n", "\n", "batch_valid_labels_count", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "to_int32", "(", "self", ".", "item_clicked_mask", ")", ")", "\n", "batch_labels_not_found_in_topk", "=", "batch_valid_labels_count", "-", "tf", ".", "size", "(", "reciprocal_ranks", ")", "\n", "\n", "\n", "#Completing with items for which the label was not in the preds (because tf.where() do not return indexes in this case), ", "\n", "#so that mean is consistent", "\n", "reciprocal_ranks", "=", "tf", ".", "concat", "(", "[", "reciprocal_ranks", ",", "tf", ".", "zeros", "(", "batch_labels_not_found_in_topk", ")", "]", ",", "axis", "=", "0", ")", "\n", "\n", "\n", "mrr", ",", "mrr_update_op", "=", "tf", ".", "metrics", ".", "mean", "(", "\n", "values", "=", "reciprocal_ranks", ",", "\n", "name", "=", "'mrr_at_n'", ")", "\n", "\n", "return", "mrr", ",", "mrr_update_op", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.scale_center_features": [[887, 908], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.ones_initializer", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.zeros_initializer", "tensorflow.contrib.layers.l2_regularizer", "item_features.get_shape", "item_features.get_shape"], "methods", ["None"], ["", "", "def", "scale_center_features", "(", "self", ",", "item_features", ",", "begin_norm_axis", "=", "2", ")", ":", "\n", "\n", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"input_features_center_scale\"", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "            ", "gamma", "=", "tf", ".", "get_variable", "(", "\"gamma_scale\"", ",", "\n", "shape", "=", "[", "item_features", ".", "get_shape", "(", ")", "[", "-", "1", "]", "]", ",", "\n", "initializer", "=", "tf", ".", "ones_initializer", "(", ")", ",", "\n", "regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ")", "\n", "beta", "=", "tf", ".", "get_variable", "(", "\"beta_center\"", ",", "\n", "shape", "=", "[", "item_features", ".", "get_shape", "(", ")", "[", "-", "1", "]", "]", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ",", "\n", "regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "'input_features_gamma_scale'", ",", "family", "=", "'stats'", ",", "values", "=", "gamma", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'input_features_beta_center'", ",", "family", "=", "'stats'", ",", "values", "=", "beta", ")", "\n", "\n", "\n", "", "item_features_centered_scaled", "=", "(", "item_features", "*", "gamma", ")", "+", "beta", "\n", "\n", "", "return", "item_features_centered_scaled", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.items_cat_embed": [[911, 920], ["tensorflow.variable_scope", "nar_model.get_embedding_size", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.contrib.layers.l2_regularizer"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.get_embedding_size"], ["", "def", "items_cat_embed", "(", "self", ",", "item_ids", ")", ":", "\n", "#with tf.device('/cpu:0'):", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"item_cat_embedding\"", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "            ", "size", "=", "self", ".", "items_vocab_size", "\n", "dim", "=", "get_embedding_size", "(", "size", ")", "\n", "embeddings", "=", "tf", ".", "get_variable", "(", "\"items_embedding\"", ",", "shape", "=", "[", "size", ",", "dim", "]", ",", "\n", "regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "reg_weight_decay", ")", ")", "\n", "lookup", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embeddings", ",", "ids", "=", "item_ids", ")", "#, max_norm=1)", "\n", "return", "lookup", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_item_features": [[921, 995], ["tensorflow.variable_scope", "nar_model.NARModuleModel.get_items_dynamic_features", "tensorflow.concat", "len", "nar_model.NARModuleModel.get_features", "items_features_list.append", "tensorflow.nn.embedding_lookup", "items_features_list.append", "nar_model.NARModuleModel.items_cat_embed", "items_features_list.append", "items_features_list.append", "tensorflow.gather", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.boolean_mask", "tensorflow.boolean_mask", "tensorflow.boolean_mask", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.sign", "tensorflow.sign", "tensorflow.sign"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_dynamic_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.items_cat_embed"], ["", "", "def", "get_item_features", "(", "self", ",", "item_ids", ",", "events_timestamp", ",", "summary_suffix", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"item_features\"", ")", ":", "\n", "\n", "#Obtaining item features for specified items (e.g. clicked, negative samples)", "\n", "            ", "item_metadata_features_values", "=", "{", "}", "\n", "for", "feature_name", "in", "self", ".", "articles_features_config", ":", "\n", "                ", "if", "feature_name", "not", "in", "ARTICLE_REQ_FEATURES", ":", "\n", "                    ", "item_metadata_features_values", "[", "feature_name", "]", "=", "tf", ".", "gather", "(", "self", ".", "articles_metadata", "[", "feature_name", "]", ",", "item_ids", ")", "\n", "\n", "", "", "items_features_list", "=", "[", "]", "\n", "\n", "if", "len", "(", "item_metadata_features_values", ")", ">", "0", ":", "\n", "#Concatenating item contextual features", "\n", "                ", "item_metadata_features", "=", "self", ".", "get_features", "(", "item_metadata_features_values", ",", "\n", "features_config", "=", "self", ".", "articles_features_config", ",", "\n", "features_to_ignore", "=", "ARTICLE_REQ_FEATURES", ")", "\n", "#Adding articles metadata attributes as input for the network", "\n", "items_features_list", ".", "append", "(", "item_metadata_features", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "'item_metadata_features/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "values", "=", "tf", ".", "boolean_mask", "(", "item_metadata_features", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "\n", "#If enabled, add Article Content Embeddings trained by ACR module", "\n", "", "", "if", "self", ".", "internal_features_config", "[", "'article_content_embeddings'", "]", ":", "\n", "                ", "items_acr_embeddings_lookup", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "content_article_embeddings_matrix", ",", "ids", "=", "item_ids", ")", "\n", "\n", "items_features_list", ".", "append", "(", "items_acr_embeddings_lookup", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "'items_acr_embeddings_lookup/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "values", "=", "tf", ".", "boolean_mask", "(", "items_acr_embeddings_lookup", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "\n", "#If enabled, adds trainable item embeddings", "\n", "", "", "if", "self", ".", "internal_features_config", "[", "'item_clicked_embeddings'", "]", ":", "\n", "                ", "item_clicked_interactions_embedding", "=", "self", ".", "items_cat_embed", "(", "item_ids", ")", "\n", "items_features_list", ".", "append", "(", "item_clicked_interactions_embedding", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                    ", "tf", ".", "summary", ".", "histogram", "(", "'item_clicked_interactions_embedding/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "values", "=", "tf", ".", "boolean_mask", "(", "item_clicked_interactions_embedding", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "", "", "'''\n            #Computes cross similarity among all sampled items, to create a avg_cross_sim feature, for diversity regularization\n            item_ids_sim_idx = tf.map_fn(lambda x: \n                                            tf.map_fn(lambda y:                                                                     \n                                                           tf.where(tf.equal(self.sample_items_unique, y))[0]\n                                                      , x),\n                                     item_ids)\n            item_avg_cross_sim = tf.gather(self.sample_items_content_cos_sims_mean, item_ids_sim_idx)\n\n            items_features_list.append(item_avg_cross_sim)\n\n            if self.plot_histograms:\n                tf.summary.histogram('item_avg_cross_sim/'+summary_suffix, family='stats',\n                                  values=tf.boolean_mask(item_avg_cross_sim, tf.cast(tf.sign(item_ids), tf.bool)))\n            '''", "\n", "\n", "\n", "#Computing Item Dynamic features (RECENCY and POPULARITY)", "\n", "items_context_features", "=", "self", ".", "get_items_dynamic_features", "(", "item_ids", ",", "\n", "events_timestamp", ",", "\n", "summary_suffix", "=", "summary_suffix", ")", "\n", "\n", "#If both Recency and Novelty feature were disabled, ignore them", "\n", "if", "items_context_features", "is", "not", "None", ":", "\n", "                ", "items_features_list", ".", "append", "(", "items_context_features", ")", "\n", "\n", "", "items_features_concat", "=", "tf", ".", "concat", "(", "items_features_list", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "items_features_concat", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.min_max_normalization": [[996, 1010], ["tensorflow.constant", "tensorflow.constant", "tensorflow.constant", "tensorflow.reduce_min", "tensorflow.reduce_max", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.maximum"], "methods", ["None"], ["", "", "def", "min_max_normalization", "(", "self", ",", "tensor", ",", "tensor_to_get_stats_from", ",", "min_max_range", "=", "(", "-", "1.0", ",", "1.0", ")", ",", "epsilon", "=", "1e-24", ",", "summary_suffix", "=", "''", ")", ":", "\n", "        ", "epsilon", "=", "tf", ".", "constant", "(", "epsilon", ",", "dtype", "=", "tf", ".", "float32", ",", "name", "=", "\"epsilon_min_max\"", ")", "\n", "min_scale", "=", "tf", ".", "constant", "(", "min_max_range", "[", "0", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "name", "=", "\"scale_min_value\"", ")", "\n", "max_scale", "=", "tf", ".", "constant", "(", "min_max_range", "[", "1", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "name", "=", "\"scale_max_value\"", ")", "\n", "\n", "min_value", "=", "tf", ".", "reduce_min", "(", "tensor_to_get_stats_from", ")", "\n", "max_value", "=", "tf", ".", "reduce_max", "(", "tensor_to_get_stats_from", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'min_max_normalization/'", "+", "summary_suffix", "+", "'/min'", ",", "family", "=", "'stats'", ",", "tensor", "=", "min_value", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'min_max_normalization/'", "+", "summary_suffix", "+", "'/max'", ",", "family", "=", "'stats'", ",", "tensor", "=", "max_value", ")", "\n", "\n", "scaled", "=", "(", "tensor", "-", "min_value", "+", "epsilon", ")", "/", "tf", ".", "maximum", "(", "(", "max_value", "-", "min_value", ")", ",", "2", "*", "epsilon", ")", "\n", "centered", "=", "scaled", "*", "(", "max_scale", "-", "min_scale", ")", "+", "min_scale", "\n", "return", "centered", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.normalize_values": [[1011, 1040], ["tensorflow.variable_scope", "tensorflow.nn.moments", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.constant", "tensorflow.sqrt", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "nar_model.NARModuleModel.min_max_normalization"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.min_max_normalization"], ["", "def", "normalize_values", "(", "self", ",", "tensor_to_normalize", ",", "tensor_to_get_stats_from", ",", "summary_suffix", "=", "''", ",", "\n", "min_max_scaling_after_znorm", "=", "True", ",", "min_max_range", "=", "(", "-", "1.0", ",", "1.0", ")", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"values_normalization\"", ")", ":", "\n", "            ", "mean", ",", "variance", "=", "tf", ".", "nn", ".", "moments", "(", "tensor_to_get_stats_from", ",", "axes", "=", "[", "0", "]", ")", "\n", "\n", "#tf.logging.info('normalize_values/{}/mean={}'.format(summary_suffix, mean.get_shape()))", "\n", "#tf.logging.info('normalize_values/{}/variance={}'.format(summary_suffix, variance.get_shape()))", "\n", "\n", "#Fixing size of stats to avoid dynamic last dimension on tensor_normed", "\n", "mean", "=", "tf", ".", "reshape", "(", "mean", ",", "[", "1", "]", ")", "\n", "variance", "=", "tf", ".", "reshape", "(", "variance", ",", "[", "1", "]", ")", "\n", "\n", "#To avoid division by zero", "\n", "epsilon", "=", "tf", ".", "constant", "(", "1e-24", ")", "\n", "stddev", "=", "tf", ".", "sqrt", "(", "variance", "+", "epsilon", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'normalize_values/'", "+", "summary_suffix", "+", "'/mean'", ",", "family", "=", "'stats'", ",", "tensor", "=", "mean", "[", "0", "]", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'normalize_values/'", "+", "summary_suffix", "+", "'/stddev'", ",", "family", "=", "'stats'", ",", "tensor", "=", "stddev", "[", "0", "]", ")", "\n", "\n", "#Standardization (z-normalization)", "\n", "tensor_normed", "=", "(", "tensor_to_normalize", "-", "mean", ")", "/", "stddev", "\n", "\n", "if", "min_max_scaling_after_znorm", ":", "\n", "                ", "tensor_to_get_stats_from_normed", "=", "(", "tensor_to_get_stats_from", "-", "mean", ")", "/", "stddev", "\n", "tensor_normed", "=", "self", ".", "min_max_normalization", "(", "tensor_normed", ",", "tensor_to_get_stats_from_normed", ",", "\n", "min_max_range", "=", "min_max_range", ",", "\n", "summary_suffix", "=", "summary_suffix", ")", "\n", "\n", "", "return", "tensor_normed", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_last_items_from_recent_clicks_buffer": [[1041, 1045], ["tensorflow.variable_scope", "tensorflow.boolean_mask", "tensorflow.cast", "tensorflow.sign"], "methods", ["None"], ["", "", "def", "get_last_items_from_recent_clicks_buffer", "(", "self", ",", "last_n", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"last_items_from_recent_clicks_buffer\"", ")", ":", "\n", "            ", "non_zero_recent_items", "=", "tf", ".", "boolean_mask", "(", "self", ".", "pop_recent_items_buffer", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "self", ".", "pop_recent_items_buffer", ")", ",", "tf", ".", "bool", ")", ")", "\n", "return", "non_zero_recent_items", "[", ":", "last_n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_unique_items_from_pop_recent_buffer": [[1046, 1053], ["tensorflow.variable_scope", "tensorflow.unique", "tensorflow.boolean_mask", "tensorflow.cast", "tensorflow.sign"], "methods", ["None"], ["", "", "def", "get_unique_items_from_pop_recent_buffer", "(", "self", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"unique_items_from_pop_recent_buffer\"", ")", ":", "\n", "            ", "recent_items_unique", ",", "_", "=", "tf", ".", "unique", "(", "self", ".", "pop_recent_items_buffer", ")", "\n", "#Removing zero", "\n", "recent_items_unique", "=", "tf", ".", "boolean_mask", "(", "recent_items_unique", ",", "\n", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "recent_items_unique", ")", ",", "tf", ".", "bool", ")", ")", "\n", "return", "recent_items_unique", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.calculate_elapsed_days_since_publishing": [[1055, 1061], ["tensorflow.variable_scope", "tensorflow.nn.relu", "tensorflow.constant", "tensorflow.to_float", "tensorflow.to_float"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.relu"], ["", "", "def", "calculate_elapsed_days_since_publishing", "(", "self", ",", "creation_dates", ",", "reference_timestamps", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"elapsed_days_since_publishing\"", ")", ":", "\n", "#Timestamps and created_at_ts", "\n", "            ", "elapsed_days", "=", "tf", ".", "nn", ".", "relu", "(", "(", "tf", ".", "to_float", "(", "reference_timestamps", ")", "-", "tf", ".", "to_float", "(", "creation_dates", ")", ")", "/", "tf", ".", "constant", "(", "1000.0", "*", "60.0", "*", "60.0", "*", "24.0", ")", ")", "\n", "return", "elapsed_days", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.normalize_recency_feature": [[1062, 1090], ["tensorflow.variable_scope", "nar_model.NARModuleModel.get_last_items_from_recent_clicks_buffer", "tensorflow.gather", "nar_model.NARModuleModel.calculate_elapsed_days_since_publishing", "nar_model.log_1p", "nar_model.log_1p", "tensorflow.reshape", "tensorflow.cond", "nar_model.NARModuleModel.normalize_values", "tensorflow.reduce_max", "tensorflow.boolean_mask", "tensorflow.equal", "tensorflow.cast", "tensorflow.constant", "tensorflow.sign", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_last_items_from_recent_clicks_buffer", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.calculate_elapsed_days_since_publishing", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.log_1p", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.log_1p", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.normalize_values"], ["", "", "def", "normalize_recency_feature", "(", "self", ",", "batch_elapsed_days_since_publishing", ",", "batch_events_timestamp", ",", "item_ids", ",", "\n", "summary_suffix", "=", "''", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"normalize_recency_feature\"", ")", ":", "\n", "#Computing global recency stats from buffer", "\n", "            ", "last_clicked_items", "=", "self", ".", "get_last_items_from_recent_clicks_buffer", "(", "self", ".", "recent_clicks_for_normalization", ")", "\n", "recent_items_creation_date", "=", "tf", ".", "gather", "(", "self", ".", "articles_metadata", "[", "'created_at_ts'", "]", ",", "last_clicked_items", ")", "\n", "recent_items_elapsed_days_since_creation", "=", "self", ".", "calculate_elapsed_days_since_publishing", "(", "recent_items_creation_date", ",", "\n", "tf", ".", "reduce_max", "(", "batch_events_timestamp", ")", ")", "\n", "recent_items_elapsed_days_since_creation_smoothed", "=", "log_1p", "(", "recent_items_elapsed_days_since_creation", ",", "\n", "base", "=", "self", ".", "elapsed_days_smooth_log_base", ")", "\n", "\n", "#Normalizing batch recency feature", "\n", "batch_elapsed_days_since_publishing_smoothed", "=", "log_1p", "(", "batch_elapsed_days_since_publishing", ",", "\n", "base", "=", "self", ".", "elapsed_days_smooth_log_base", ")", "\n", "\n", "\n", "batch_elapsed_days_since_publishing_smoothed_non_zero", "=", "tf", ".", "reshape", "(", "tf", ".", "boolean_mask", "(", "batch_elapsed_days_since_publishing_smoothed", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", ",", "[", "-", "1", "]", ")", "\n", "\n", "#If there aren't recent items available in the buffer (first batch), use batch items (zeroed matrix) to compute norm stats", "\n", "#After that, do not use batch to compute mean and stddev, to avoid leak", "\n", "tensor_to_get_stats_from", "=", "tf", ".", "cond", "(", "tf", ".", "equal", "(", "tf", ".", "shape", "(", "last_clicked_items", ")", "[", "0", "]", ",", "tf", ".", "constant", "(", "0", ")", ")", ",", "\n", "lambda", ":", "batch_elapsed_days_since_publishing_smoothed_non_zero", ",", "\n", "lambda", ":", "recent_items_elapsed_days_since_creation_smoothed", ")", "\n", "\n", "batch_elapsed_days_since_publishing_normed", "=", "self", ".", "normalize_values", "(", "batch_elapsed_days_since_publishing_smoothed", ",", "\n", "tensor_to_get_stats_from", ",", "\n", "summary_suffix", "=", "'log_elapsed_days_since_publishing/'", "+", "summary_suffix", ")", "\n", "return", "batch_elapsed_days_since_publishing_normed", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_recency_feature": [[1092, 1132], ["tensorflow.variable_scope", "tensorflow.gather", "nar_model.NARModuleModel.calculate_elapsed_days_since_publishing", "tensorflow.summary.scalar", "nar_model.NARModuleModel.normalize_recency_feature", "tensorflow.summary.scalar", "tensorflow.reshape", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.boolean_mask", "tensorflow.boolean_mask", "tensorflow.boolean_mask", "tensorflow.boolean_mask", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.sign", "tensorflow.sign", "tensorflow.sign", "tensorflow.sign"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.calculate_elapsed_days_since_publishing", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.normalize_recency_feature"], ["", "", "def", "get_items_recency_feature", "(", "self", ",", "item_ids", ",", "events_timestamp", ",", "summary_suffix", "=", "''", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"items_recency_feature\"", ")", ":", "\n", "#Computing RECENCY feature", "\n", "            ", "batch_articles_creation_date", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "self", ".", "articles_metadata", "[", "'created_at_ts'", "]", ",", "\n", "[", "-", "1", ",", "1", "]", ")", ",", "item_ids", ")", "\n", "\n", "'''\n            #TO DEBUG\n            elapsed_hours = self.calculate_elapsed_hours_temp(batch_articles_creation_date, events_timestamp)\n            if self.plot_histograms:\n                tf.summary.histogram('batch_elapsed_hours/'+summary_suffix, family='stats',\n                                  values=tf.boolean_mask(elapsed_hours, tf.cast(tf.sign(item_ids), tf.bool)))\n\n                tf.summary.scalar('batch_elapsed_hours_scalar/'+summary_suffix, family='stats',\n                                    tensor=tf.reduce_mean(tf.boolean_mask(elapsed_hours, tf.cast(tf.sign(item_ids), tf.bool))))\n            '''", "\n", "\n", "elapsed_days_since_publishing", "=", "self", ".", "calculate_elapsed_days_since_publishing", "(", "batch_articles_creation_date", ",", "events_timestamp", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'batch_elapsed_days_since_publishing_scalar/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "tensor", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "boolean_mask", "(", "elapsed_days_since_publishing", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", ")", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "'batch_elapsed_days_since_publishing/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "values", "=", "tf", ".", "boolean_mask", "(", "elapsed_days_since_publishing", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "\n", "", "elapsed_days_since_publishing_norm", "=", "self", ".", "normalize_recency_feature", "(", "elapsed_days_since_publishing", ",", "\n", "events_timestamp", ",", "item_ids", ",", "\n", "summary_suffix", "=", "summary_suffix", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'batch_elapsed_days_since_publishing_norm_scalar/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "tensor", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "boolean_mask", "(", "elapsed_days_since_publishing_norm", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", ")", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "'batch_elapsed_days_since_publishing_norm/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "values", "=", "tf", ".", "boolean_mask", "(", "elapsed_days_since_publishing_norm", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "\n", "", "return", "elapsed_days_since_publishing_norm", ",", "batch_articles_creation_date", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_norm_popularity_feature": [[1134, 1146], ["tensorflow.variable_scope", "tensorflow.gather", "tensorflow.expand_dims", "tensorflow.summary.histogram", "tensorflow.boolean_mask", "tensorflow.cast", "tensorflow.sign"], "methods", ["None"], ["", "", "def", "get_items_norm_popularity_feature", "(", "self", ",", "item_ids", ",", "summary_suffix", "=", "''", ")", ":", "\n", "#Computing POPULARITY feature", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"items_norm_popularity_feature\"", ")", ":", "\n", "\n", "            ", "batch_articles_norm_pop", "=", "tf", ".", "gather", "(", "self", ".", "articles_recent_pop_norm", ",", "tf", ".", "expand_dims", "(", "item_ids", ",", "-", "1", ")", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "'batch_articles_norm_pop/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "values", "=", "tf", ".", "boolean_mask", "(", "batch_articles_norm_pop", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", ")", "\n", "\n", "\n", "", "return", "batch_articles_norm_pop", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_pop_novelty_feature": [[1147, 1149], ["nar_model.log_base"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.log_base"], ["", "", "def", "get_items_pop_novelty_feature", "(", "self", ",", "items_norm_pop", ")", ":", "\n", "        ", "return", "-", "log_base", "(", "items_norm_pop", ",", "self", ".", "popularity_smooth_log_base", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_pop_novelty_feature_standardized": [[1150, 1194], ["tensorflow.variable_scope", "nar_model.NARModuleModel.get_last_items_from_recent_clicks_buffer", "nar_model.NARModuleModel.get_items_norm_popularity_feature", "nar_model.NARModuleModel.get_items_pop_novelty_feature", "tensorflow.summary.scalar", "nar_model.NARModuleModel.get_items_norm_popularity_feature", "nar_model.NARModuleModel.get_items_pop_novelty_feature", "tensorflow.boolean_mask", "tensorflow.summary.scalar", "tensorflow.cond", "nar_model.NARModuleModel.normalize_values", "tensorflow.cast", "tensorflow.summary.histogram", "tensorflow.equal", "tensorflow.summary.histogram", "tensorflow.reduce_mean", "tensorflow.sign", "tensorflow.reduce_mean", "tensorflow.constant", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_last_items_from_recent_clicks_buffer", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_norm_popularity_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_pop_novelty_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_norm_popularity_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_pop_novelty_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.normalize_values"], ["", "def", "get_items_pop_novelty_feature_standardized", "(", "self", ",", "item_ids", ",", "summary_suffix", "=", "''", ")", ":", "\n", "#Computing POPULARITY feature", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"items_novelty_feature\"", ")", ":", "\n", "\n", "#Computing global recency stats from buffer", "\n", "#recent_items_unique = self.get_unique_items_from_pop_recent_buffer()            ", "\n", "            ", "last_clicked_items", "=", "self", ".", "get_last_items_from_recent_clicks_buffer", "(", "self", ".", "recent_clicks_for_normalization", ")", "\n", "recent_items_norm_pop", "=", "self", ".", "get_items_norm_popularity_feature", "(", "last_clicked_items", ",", "summary_suffix", "=", "summary_suffix", "+", "'_global'", ")", "\n", "recent_items_novelty", "=", "self", ".", "get_items_pop_novelty_feature", "(", "recent_items_norm_pop", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'recent_items_novelty/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "tensor", "=", "tf", ".", "reduce_mean", "(", "recent_items_novelty", ")", ")", "\n", "\n", "\n", "batch_articles_norm_pop_input", "=", "self", ".", "get_items_norm_popularity_feature", "(", "item_ids", ",", "summary_suffix", "=", "summary_suffix", ")", "\n", "\n", "batch_articles_novelty", "=", "self", ".", "get_items_pop_novelty_feature", "(", "batch_articles_norm_pop_input", ")", "\n", "\n", "batch_articles_novelty_non_zero", "=", "tf", ".", "boolean_mask", "(", "batch_articles_novelty", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "item_ids", ")", ",", "tf", ".", "bool", ")", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "'batch_articles_novelty/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "values", "=", "batch_articles_novelty_non_zero", ")", "\n", "\n", "", "tf", ".", "summary", ".", "scalar", "(", "'batch_items_novelty/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "tensor", "=", "tf", ".", "reduce_mean", "(", "batch_articles_novelty_non_zero", ")", ")", "\n", "\n", "#If there aren't recent items available in the buffer (first batch), use batch items (zeroed matrix) to compute norm stats", "\n", "#After that, do not use batch to compute mean and stddev, to avoid leak", "\n", "tensor_to_get_stats_from", "=", "tf", ".", "cond", "(", "tf", ".", "equal", "(", "tf", ".", "shape", "(", "last_clicked_items", ")", "[", "0", "]", ",", "tf", ".", "constant", "(", "0", ")", ")", ",", "\n", "lambda", ":", "batch_articles_novelty_non_zero", ",", "\n", "lambda", ":", "recent_items_novelty", ")", "\n", "\n", "#Applying standardization", "\n", "batch_items_novelty_standardized", "=", "self", ".", "normalize_values", "(", "batch_articles_novelty", ",", "\n", "tensor_to_get_stats_from", ",", "\n", "summary_suffix", "=", "'novelty/'", "+", "summary_suffix", ")", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "'batch_items_novelty_standardized/'", "+", "summary_suffix", ",", "family", "=", "'stats'", ",", "\n", "values", "=", "batch_items_novelty_standardized", ")", "\n", "\n", "\n", "", "return", "batch_items_novelty_standardized", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_dynamic_features": [[1197, 1219], ["tensorflow.variable_scope", "nar_model.NARModuleModel.get_items_recency_feature", "nar_model.NARModuleModel.get_items_pop_novelty_feature_standardized", "dynamic_features.append", "dynamic_features.append", "len", "tensorflow.concat"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_recency_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_items_pop_novelty_feature_standardized"], ["", "", "def", "get_items_dynamic_features", "(", "self", ",", "item_ids", ",", "events_timestamp", ",", "summary_suffix", "=", "''", ")", ":", "\n", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"items_dynamic_features\"", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "\n", "#Computing RECENCY feature", "\n", "            ", "elapsed_days_since_publishing_log", ",", "batch_articles_creation_date", "=", "self", ".", "get_items_recency_feature", "(", "item_ids", ",", "events_timestamp", ",", "summary_suffix", "=", "summary_suffix", ")", "\n", "\n", "batch_articles_novelty", "=", "self", ".", "get_items_pop_novelty_feature_standardized", "(", "item_ids", ",", "\n", "summary_suffix", "=", "summary_suffix", ")", "\n", "\n", "#Including dynamic item features, according to configuration", "\n", "dynamic_features", "=", "[", "]", "\n", "if", "self", ".", "internal_features_config", "[", "'recency'", "]", ":", "\n", "                ", "dynamic_features", ".", "append", "(", "elapsed_days_since_publishing_log", ")", "\n", "", "if", "self", ".", "internal_features_config", "[", "'novelty'", "]", ":", "\n", "                ", "dynamic_features", ".", "append", "(", "batch_articles_novelty", ")", "\n", "\n", "", "if", "len", "(", "dynamic_features", ")", ">", "0", ":", "\n", "                ", "return", "tf", ".", "concat", "(", "dynamic_features", ",", "axis", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_sample_from_recently_clicked_items_buffer": [[1220, 1234], ["tensorflow.variable_scope", "tensorflow.boolean_mask", "tensorflow.summary.scalar", "tensorflow.random_shuffle", "tensorflow.cast", "tensorflow.sign", "tensorflow.shape"], "methods", ["None"], ["", "", "", "def", "get_sample_from_recently_clicked_items_buffer", "(", "self", ",", "sample_size", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"neg_samples_buffer\"", ")", ":", "\n", "            ", "pop_recent_items_buffer_masked", "=", "tf", ".", "boolean_mask", "(", "self", ".", "pop_recent_items_buffer", ",", "\n", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "self", ".", "pop_recent_items_buffer", ")", ",", "tf", ".", "bool", ")", ")", "\n", "\n", "#tf.summary.scalar('unique_clicked_items_on_buffer', family='stats', tensor=tf.shape(unique_pop_recent_items_buffer_masked)[0])", "\n", "tf", ".", "summary", ".", "scalar", "(", "'clicked_items_on_buffer'", ",", "family", "=", "'stats'", ",", "tensor", "=", "tf", ".", "shape", "(", "pop_recent_items_buffer_masked", ")", "[", "0", "]", ")", "\n", "\n", "#recent_items_unique_sample, idxs = tf.unique(tf.random_shuffle(pop_recent_items_buffer_masked)[:sample_size*sample_size_factor_to_look_for_unique])", "\n", "recent_items_unique_sample", "=", "tf", ".", "random_shuffle", "(", "pop_recent_items_buffer_masked", ")", "\n", "\n", "#Samples K articles from recent articles", "\n", "sample_recently_clicked_items", "=", "recent_items_unique_sample", "[", ":", "sample_size", "]", "\n", "return", "sample_recently_clicked_items", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_masked_seq_values": [[1236, 1238], ["tensorflow.boolean_mask"], "methods", ["None"], ["", "", "def", "get_masked_seq_values", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "return", "tf", ".", "boolean_mask", "(", "tensor", ",", "self", ".", "item_clicked_mask", ",", "name", "=", "'masked_values'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_neg_items_click": [[1239, 1255], ["tensorflow.random.shuffle", "tensorflow.unique", "tensorflow.concat", "tensorflow.unsorted_segment_min", "tensorflow.zeros", "tensorflow.shape", "tensorflow.shape"], "methods", ["None"], ["", "def", "get_neg_items_click", "(", "self", ",", "valid_samples_session", ",", "num_neg_samples", ")", ":", "\n", "#Shuffles neg. samples for each click", "\n", "        ", "valid_samples_shuffled", "=", "tf", ".", "random", ".", "shuffle", "(", "valid_samples_session", ")", "\n", "\n", "\n", "samples_unique_vals", ",", "samples_unique_idx", "=", "tf", ".", "unique", "(", "valid_samples_shuffled", ")", "\n", "\n", "#Returning first N unique items (to avoid repetition)", "\n", "first_unique_items", "=", "tf", ".", "unsorted_segment_min", "(", "data", "=", "valid_samples_shuffled", ",", "\n", "segment_ids", "=", "samples_unique_idx", ",", "\n", "num_segments", "=", "tf", ".", "shape", "(", "samples_unique_vals", ")", "[", "0", "]", ")", "[", ":", "num_neg_samples", "]", "\n", "\n", "#Padding if necessary to keep the number of neg samples constant (ex: first batch)", "\n", "first_unique_items_padded_if_needed", "=", "tf", ".", "concat", "(", "[", "first_unique_items", ",", "tf", ".", "zeros", "(", "num_neg_samples", "-", "tf", ".", "shape", "(", "first_unique_items", ")", "[", "0", "]", ",", "tf", ".", "int64", ")", "]", ",", "axis", "=", "0", ")", "\n", "\n", "return", "first_unique_items_padded_if_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_neg_items_session": [[1257, 1269], ["tensorflow.setdiff1d", "tensorflow.map_fn", "tensorflow.cond", "tensorflow.equal", "tensorflow.constant", "tensorflow.zeros", "nar_model.NARModuleModel.get_neg_items_click"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_click"], ["", "def", "get_neg_items_session", "(", "self", ",", "session_item_ids", ",", "candidate_samples", ",", "num_neg_samples", ")", ":", "\n", "#Ignoring negative samples clicked within the session (keeps the order and repetition of candidate_samples)", "\n", "        ", "valid_samples_session", ",", "_", "=", "tf", ".", "setdiff1d", "(", "candidate_samples", ",", "session_item_ids", ",", "index_dtype", "=", "tf", ".", "int64", ")", "\n", "\n", "#Generating a random list of negative samples for each click (with no repetition)", "\n", "session_clicks_neg_items", "=", "tf", ".", "map_fn", "(", "lambda", "click_id", ":", "tf", ".", "cond", "(", "tf", ".", "equal", "(", "click_id", ",", "tf", ".", "constant", "(", "0", ",", "tf", ".", "int64", ")", ")", ",", "\n", "lambda", ":", "tf", ".", "zeros", "(", "num_neg_samples", ",", "tf", ".", "int64", ")", ",", "\n", "lambda", ":", "self", ".", "get_neg_items_click", "(", "valid_samples_session", ",", "num_neg_samples", ")", "\n", ")", "\n", ",", "session_item_ids", ")", "\n", "\n", "return", "session_clicks_neg_items", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_negative_samples": [[1271, 1280], ["tensorflow.variable_scope", "tensorflow.map_fn", "nar_model.NARModuleModel.get_neg_items_session"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_session"], ["", "def", "get_negative_samples", "(", "self", ",", "all_clicked_items", ",", "candidate_samples", ",", "num_neg_samples", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"negative_samples\"", ")", ":", "\n", "#Shuffling negative samples by session and limiting to num_neg_samples", "\n", "            ", "shuffled_neg_samples", "=", "tf", ".", "map_fn", "(", "lambda", "session_item_ids", ":", "self", ".", "get_neg_items_session", "(", "session_item_ids", ",", "\n", "candidate_samples", ",", "\n", "num_neg_samples", ")", "\n", ",", "all_clicked_items", ")", "\n", "\n", "return", "shuffled_neg_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.get_batch_negative_samples": [[1281, 1305], ["tensorflow.variable_scope", "tensorflow.reshape", "tensorflow.boolean_mask", "tensorflow.concat", "nar_model.NARModuleModel.get_negative_samples", "tensorflow.cast", "tensorflow.random.shuffle", "tensorflow.sign"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_negative_samples"], ["", "", "def", "get_batch_negative_samples", "(", "self", ",", "all_clicked_items", ",", "additional_samples", ",", "num_negative_samples", ",", "\n", "first_sampling_multiplying_factor", "=", "20", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"neg_samples_batch\"", ")", ":", "\n", "#current_batch_size, batch_max_session_length = tf.shape(item_clicked)[0], tf.shape(item_clicked)[1] ", "\n", "\n", "            ", "batch_items", "=", "tf", ".", "reshape", "(", "all_clicked_items", ",", "[", "-", "1", "]", ")", "\n", "\n", "#Removing padded (zeroed) items", "\n", "batch_items_non_zero", "=", "tf", ".", "boolean_mask", "(", "batch_items", ",", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "batch_items", ")", ",", "dtype", "=", "tf", ".", "bool", ")", ")", "\n", "\n", "#TEMP: uniform sampling -> popularity sampling", "\n", "#batch_items_unique, _ = tf.unique(batch_items_non_zero)", "\n", "batch_items_unique", "=", "batch_items_non_zero", "\n", "\n", "\n", "#Concatenating batch items with additional samples (to deal with small batches)", "\n", "candidate_neg_items", "=", "tf", ".", "concat", "(", "[", "batch_items_unique", ",", "additional_samples", "]", ",", "axis", "=", "0", ")", "\n", "\n", "#Shuffling candidates and sampling the first 20N (1000 if neg_samples=50)", "\n", "candidate_neg_items_shuffled", "=", "tf", ".", "random", ".", "shuffle", "(", "candidate_neg_items", ")", "[", ":", "(", "num_negative_samples", "*", "first_sampling_multiplying_factor", ")", "]", "\n", "\n", "batch_negative_items", "=", "self", ".", "get_negative_samples", "(", "all_clicked_items", ",", "candidate_neg_items_shuffled", ",", "num_negative_samples", ")", "\n", "\n", "return", "batch_negative_items", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.NARModuleModel.build_rnn": [[1308, 1362], ["tensorflow.variable_scope", "range", "tensorflow.contrib.rnn.MultiRNNCell", "tensorflow.nn.dynamic_rnn", "tensorflow.contrib.rnn.UGRNNCell", "tensorflow.nn.rnn_cell.DropoutWrapper", "fw_cells.append", "tensorflow.summary.histogram", "tensorflow.contrib.rnn.ResidualWrapper", "tensorflow.contrib.rnn.InputProjectionWrapper"], "methods", ["None"], ["", "", "def", "build_rnn", "(", "self", ",", "the_input", ",", "lengths", ",", "rnn_units", "=", "256", ",", "residual_connections", "=", "False", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"RNN\"", ")", ":", "\n", "            ", "fw_cells", "=", "[", "]", "\n", "#bw_cells = []", "\n", "\n", "#Hint: Use tf.contrib.rnn.InputProjectionWrapper if the number of units between layers is different", "\n", "for", "i", "in", "range", "(", "self", ".", "rnn_num_layers", ")", ":", "\n", "#cell = tf.nn.rnn_cell.GRUCell(rnn_units)  ", "\n", "#cell = tf.nn.rnn_cell.LSTMCell(rnn_units, state_is_tuple=True)        ", "\n", "                ", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "UGRNNCell", "(", "rnn_units", ")", "\n", "\n", "if", "residual_connections", ":", "\n", "                    ", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "ResidualWrapper", "(", "cell", ")", "\n", "if", "i", "==", "0", ":", "#or rnn_layer_sizes[i] != rnn_layer_sizes[i - 1]:", "\n", "#cell = tf.contrib.rnn.InputProjectionWrapper(cell, rnn_layer_sizes[i])  ", "\n", "                        ", "cell", "=", "tf", ".", "contrib", ".", "rnn", ".", "InputProjectionWrapper", "(", "cell", ",", "rnn_units", ")", "\n", "\n", "", "", "'''\n                cell = tf.contrib.rnn.AttentionCellWrapper(cell, \n                                                     attn_length=20,\n                                                     state_is_tuple=True)\n                '''", "\n", "\n", "cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "(", "cell", ",", "\n", "output_keep_prob", "=", "self", ".", "keep_prob", "\n", "#, input_keep_prob=self.keep_prob", "\n", ")", "\n", "fw_cells", ".", "append", "(", "cell", ")", "\n", "\n", "\n", "", "fw_stacked_cells", "=", "tf", ".", "contrib", ".", "rnn", ".", "MultiRNNCell", "(", "fw_cells", ",", "state_is_tuple", "=", "True", ")", "\n", "#bw_stacked_cells = tf.contrib.rnn.MultiRNNCell(bw_cells, state_is_tuple=True)", "\n", "\n", "rnn_outputs", ",", "rnn_final_hidden_state_tuples", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "fw_stacked_cells", ",", "the_input", ",", "dtype", "=", "tf", ".", "float32", ",", "sequence_length", "=", "lengths", ")", "\n", "\n", "'''            \n            outputs, states  = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=fw_stacked_cells,\n                cell_bw=bw_stacked_cells,\n                dtype=tf.float32, #tf.float64,\n                sequence_length=lengths,\n                inputs=the_input)\n\n            #output_fw, output_bw = outputs\n            #states_fw, states_bw = states\n            #last_lstm_output = output_fw[:,-1,:]\n            rnn_outputs = tf.concat(outputs, axis=2)\n            '''", "\n", "\n", "if", "self", ".", "plot_histograms", ":", "\n", "                ", "tf", ".", "summary", ".", "histogram", "(", "\"rnn/outputs\"", ",", "rnn_outputs", ")", "\n", "\n", "", "return", "rnn_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.__init__": [[1373, 1408], ["list", "nar_model.ItemsStateUpdaterHook.create_eval_metrics"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.run_gru4rec.create_eval_metrics"], ["def", "__init__", "(", "self", ",", "mode", ",", "model", ",", "eval_metrics_top_n", ",", "\n", "clicked_items_state", ",", "eval_sessions_metrics_log", ",", "\n", "sessions_negative_items_log", ",", "\n", "sessions_chameleon_recommendations_log", ",", "\n", "content_article_embeddings_matrix", ",", "\n", "articles_metadata", ",", "\n", "eval_negative_sample_relevance", ",", "\n", "eval_benchmark_classifiers", "=", "[", "]", ",", "\n", "eval_metrics_by_session_position", "=", "False", ",", "\n", "eval_cold_start", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "mode", "=", "mode", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "eval_metrics_top_n", "=", "eval_metrics_top_n", "\n", "self", ".", "eval_cold_start", "=", "eval_cold_start", "\n", "\n", "self", ".", "clicked_items_state", "=", "clicked_items_state", "\n", "self", ".", "eval_sessions_metrics_log", "=", "eval_sessions_metrics_log", "\n", "self", ".", "sessions_negative_items_log", "=", "sessions_negative_items_log", "\n", "self", ".", "sessions_chameleon_recommendations_log", "=", "sessions_chameleon_recommendations_log", "\n", "\n", "self", ".", "content_article_embeddings_matrix", "=", "content_article_embeddings_matrix", "\n", "self", ".", "articles_metadata", "=", "articles_metadata", "\n", "self", ".", "eval_negative_sample_relevance", "=", "eval_negative_sample_relevance", "\n", "self", ".", "eval_metrics_by_session_position", "=", "eval_metrics_by_session_position", "\n", "\n", "self", ".", "bench_classifiers", "=", "list", "(", "[", "clf", "[", "'recommender'", "]", "(", "self", ".", "clicked_items_state", ",", "\n", "clf", "[", "'params'", "]", ",", "\n", "ItemsStateUpdaterHook", ".", "create_eval_metrics", "(", "self", ".", "eval_metrics_top_n", ",", "\n", "self", ".", "eval_negative_sample_relevance", ",", "\n", "#False, ", "\n", "self", ".", "eval_metrics_by_session_position", ",", "\n", "self", ".", "content_article_embeddings_matrix", ",", "\n", "self", ".", "articles_metadata", ",", "\n", "self", ".", "clicked_items_state", ")", ")", "for", "clf", "in", "eval_benchmark_classifiers", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.begin": [[1410, 1431], ["tensorflow.logging.info", "nar_model.ItemsStateUpdaterHook.clicked_items_state.save_state_checkpoint", "nar_model.ItemsStateUpdaterHook.create_eval_metrics", "clf.reset_eval_metrics"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.save_state_checkpoint", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.run_gru4rec.create_eval_metrics", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.reset_eval_metrics"], ["", "def", "begin", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"Saving items state checkpoint from train\"", ")", "\n", "#Save state of items popularity and recency from train loop, to restore after evaluation finishes", "\n", "self", ".", "clicked_items_state", ".", "save_state_checkpoint", "(", ")", "\n", "\n", "#Resets streaming metrics", "\n", "self", ".", "eval_streaming_metrics_last", "=", "{", "}", "\n", "for", "clf", "in", "self", ".", "bench_classifiers", ":", "\n", "                ", "clf", ".", "reset_eval_metrics", "(", ")", "\n", "\n", "", "self", ".", "streaming_metrics", "=", "ItemsStateUpdaterHook", ".", "create_eval_metrics", "(", "self", ".", "eval_metrics_top_n", ",", "\n", "self", ".", "eval_negative_sample_relevance", ",", "\n", "self", ".", "eval_metrics_by_session_position", ",", "\n", "self", ".", "content_article_embeddings_matrix", ",", "\n", "self", ".", "articles_metadata", ",", "\n", "self", ".", "clicked_items_state", ")", "\n", "#self.metrics_by_session_pos = StreamingMetrics(topn=self.metrics_top_n)", "\n", "\n", "self", ".", "stats_logs", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.before_run": [[1434, 1471], ["tensorflow.train.SessionRunArgs", "nar_model.ItemsStateUpdaterHook.clicked_items_state.get_articles_recent_pop_norm", "nar_model.ItemsStateUpdaterHook.clicked_items_state.get_recent_clicks_buffer"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_recent_clicks_buffer"], ["", "", "def", "before_run", "(", "self", ",", "run_context", ")", ":", "\n", "        ", "fetches", "=", "{", "'clicked_items'", ":", "self", ".", "model", ".", "item_clicked", ",", "\n", "'clicked_timestamps'", ":", "self", ".", "model", ".", "event_timestamp", ",", "\n", "'next_item_labels'", ":", "self", ".", "model", ".", "next_item_label", ",", "\n", "'last_item_label'", ":", "self", ".", "model", ".", "label_last_item", ",", "\n", "'session_id'", ":", "self", ".", "model", ".", "session_id", ",", "\n", "#'session_start': self.model.session_start,", "\n", "'user_id'", ":", "self", ".", "model", ".", "user_id", ",", "\n", "}", "\n", "\n", "if", "self", ".", "eval_cold_start", "or", "(", "self", ".", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ")", ":", "\n", "            ", "fetches", "[", "'predicted_item_ids'", "]", "=", "self", ".", "model", ".", "predicted_item_ids", "\n", "fetches", "[", "'eval_batch_negative_items'", "]", "=", "self", ".", "model", ".", "batch_negative_items", "\n", "\n", "", "if", "self", ".", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "            ", "fetches", "[", "'batch_items_count'", "]", "=", "self", ".", "model", ".", "batch_items_count", "\n", "fetches", "[", "'batch_unique_items_count'", "]", "=", "self", ".", "model", ".", "batch_unique_items_count", "\n", "\n", "fetches", "[", "'hitrate_at_n'", "]", "=", "self", ".", "model", ".", "recall_at_n_update_op", "\n", "fetches", "[", "'mrr_at_n'", "]", "=", "self", ".", "model", ".", "mrr_update_op", "\n", "#fetches['ndcg_at_n'] = self.model.ndcg_at_n_mean_update_op", "\n", "\n", "fetches", "[", "'predicted_item_probs'", "]", "=", "self", ".", "model", ".", "predicted_item_probs", "\n", "\n", "", "feed_dict", "=", "{", "\n", "self", ".", "model", ".", "articles_recent_pop_norm", ":", "self", ".", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", ",", "\n", "self", ".", "model", ".", "pop_recent_items_buffer", ":", "self", ".", "clicked_items_state", ".", "get_recent_clicks_buffer", "(", ")", ",", "\n", "#Passed as placeholder (and not as a constant) to avoid been saved in checkpoints", "\n", "self", ".", "model", ".", "content_article_embeddings_matrix", ":", "self", ".", "content_article_embeddings_matrix", "\n", "}", "\n", "\n", "#Passed as placeholder (and not as a constant) to avoid been saved in checkpoints", "\n", "for", "feature_name", "in", "self", ".", "articles_metadata", ":", "\n", "            ", "feed_dict", "[", "self", ".", "model", ".", "articles_metadata", "[", "feature_name", "]", "]", "=", "self", ".", "articles_metadata", "[", "feature_name", "]", "\n", "\n", "", "return", "tf", ".", "train", ".", "SessionRunArgs", "(", "fetches", "=", "fetches", ",", "\n", "feed_dict", "=", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.evaluate_and_update_streaming_metrics_last": [[1473, 1478], ["tensorflow.logging.info", "clf.evaluate", "utils.merge_two_dicts", "clf.get_description"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.evaluate", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.merge_two_dicts", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_description"], ["", "def", "evaluate_and_update_streaming_metrics_last", "(", "self", ",", "clf", ",", "users_ids", ",", "clicked_items", ",", "next_item_labels", ",", "eval_negative_items", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "'Evaluating benchmark: {}'", ".", "format", "(", "clf", ".", "get_description", "(", ")", ")", ")", "\n", "clf_metrics", "=", "clf", ".", "evaluate", "(", "users_ids", ",", "clicked_items", ",", "next_item_labels", ",", "topk", "=", "self", ".", "eval_metrics_top_n", ",", "\n", "eval_negative_items", "=", "eval_negative_items", ")", "\n", "self", ".", "eval_streaming_metrics_last", "=", "merge_two_dicts", "(", "self", ".", "eval_streaming_metrics_last", ",", "clf_metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.update_items_cold_start_state": [[1480, 1502], ["set().union().difference", "nar_model.ItemsStateUpdaterHook.clicked_items_state.increment_current_step", "nar_model.ItemsStateUpdaterHook.clicked_items_state.update_items_first_click_step", "nar_model.ItemsStateUpdaterHook.clicked_items_state.get_cold_start_state().update_items_num_steps_before_first_rec", "set", "nar_model.ItemsStateUpdaterHook.clicked_items_state.get_current_step", "clf.get_valid_candidate_items", "clf.predict", "clf.get_cold_start_state().update_items_num_steps_before_first_rec", "set().union", "nar_model.ItemsStateUpdaterHook.clicked_items_state.get_cold_start_state", "nar_model.ItemsStateUpdaterHook.clicked_items_state.get_current_step", "set", "clf.get_cold_start_state", "set", "next_item_labels.reshape", "clicked_items.reshape"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.increment_current_step", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.update_items_first_click_step", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.ColdStartAnalysisState.update_items_num_steps_before_first_rec", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_current_step", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.predict", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.ColdStartAnalysisState.update_items_num_steps_before_first_rec", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_cold_start_state", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_current_step", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_cold_start_state"], ["", "def", "update_items_cold_start_state", "(", "self", ",", "users_ids", ",", "clicked_items", ",", "next_item_labels", ",", "eval_batch_negative_items", ",", "\n", "predicted_item_ids", ")", ":", "\n", "\n", "        ", "clicked_items_nonzero", "=", "set", "(", "clicked_items", ".", "reshape", "(", "-", "1", ")", ")", ".", "union", "(", "set", "(", "next_item_labels", ".", "reshape", "(", "-", "1", ")", ")", ")", ".", "difference", "(", "set", "(", "[", "0", "]", ")", ")", "\n", "\n", "#Increments the current training/eval step", "\n", "self", ".", "clicked_items_state", ".", "increment_current_step", "(", ")", "\n", "#Updating the step where items were clicked the first time", "\n", "self", ".", "clicked_items_state", ".", "update_items_first_click_step", "(", "clicked_items_nonzero", ")", "\n", "#Computing number of steps before the first top-n recommendation", "\n", "predicted_top_item_ids", "=", "predicted_item_ids", "[", ":", ",", ":", ",", ":", "self", ".", "eval_metrics_top_n", "]", "\n", "self", ".", "clicked_items_state", ".", "get_cold_start_state", "(", ")", ".", "update_items_num_steps_before_first_rec", "(", "predicted_top_item_ids", ",", "\n", "self", ".", "clicked_items_state", ".", "items_first_click_step", ",", "\n", "self", ".", "clicked_items_state", ".", "get_current_step", "(", ")", ")", "\n", "\n", "#For each benchmark, computes number of steps before the first top-n recommendation", "\n", "for", "clf", "in", "self", ".", "bench_classifiers", ":", "\n", "            ", "valid_candidate_items", "=", "clf", ".", "get_valid_candidate_items", "(", "next_item_labels", ",", "eval_batch_negative_items", ")", "\n", "bench_preds", "=", "clf", ".", "predict", "(", "users_ids", ",", "clicked_items", ",", "valid_items", "=", "valid_candidate_items", ",", "topk", "=", "self", ".", "eval_metrics_top_n", ")", "\n", "clf", ".", "get_cold_start_state", "(", ")", ".", "update_items_num_steps_before_first_rec", "(", "bench_preds", ",", "\n", "self", ".", "clicked_items_state", ".", "items_first_click_step", ",", "\n", "self", ".", "clicked_items_state", ".", "get_current_step", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.after_run": [[1504, 1661], ["numpy.squeeze", "numpy.concatenate", "numpy.concatenate.reshape", "numpy.max().reshape", "numpy.concatenate", "numpy.concatenate.reshape", "nar_model.ItemsStateUpdaterHook.clicked_items_state.update_items_state", "nar_model.ItemsStateUpdaterHook.clicked_items_state.update_items_coocurrences", "nar_model.ItemsStateUpdaterHook.stats_logs.append", "tensorflow.logging.info", "evaluation.update_metrics", "evaluation.compute_metrics_results", "utils.merge_two_dicts", "time.time.time", "tensorflow.logging.info", "tensorflow.logging.info", "nar_model.ItemsStateUpdaterHook.update_items_cold_start_state", "clf.train", "zip", "predicted_item_probs.round", "nar_model.ItemsStateUpdaterHook.clicked_items_state.get_articles_recent_pop_norm", "zip", "len", "nar_model.ItemsStateUpdaterHook.clicked_items_state.get_articles_recent_pop_norm", "nar_model.ItemsStateUpdaterHook.clicked_items_state.get_articles_recent_pop_norm", "nar_model.ItemsStateUpdaterHook.evaluate_and_update_streaming_metrics_last", "numpy.nonzero", "numpy.max", "numpy.nonzero", "nar_model.ItemsStateUpdaterHook.sessions_negative_items_log.append", "articles_recent_pop_norm[].round", "zip", "nar_model.ItemsStateUpdaterHook.sessions_chameleon_recommendations_log.append", "labels.tolist", "pred_item_ids.tolist", "pred_item_probs.tolist", "articles_recent_pop_norm[].round.tolist", "str", "time.time.time", "str", "list", "labels_filtered.append", "pred_item_ids_filtered.append", "pred_item_probs_filtered.append", "pred_item_norm_pops_filtered.append", "zip", "labels.tolist", "neg_items.tolist"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.update_items_state", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.update_items_coocurrences", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.update_metrics", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.compute_metrics_results", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.merge_two_dicts", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.update_items_cold_start_state", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.train", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.evaluate_and_update_streaming_metrics_last"], ["", "", "def", "after_run", "(", "self", ",", "run_context", ",", "run_values", ")", ":", "\n", "        ", "clicked_items", "=", "run_values", ".", "results", "[", "'clicked_items'", "]", "\n", "clicked_timestamps", "=", "np", ".", "squeeze", "(", "run_values", ".", "results", "[", "'clicked_timestamps'", "]", ",", "axis", "=", "-", "1", ")", "\n", "next_item_labels", "=", "run_values", ".", "results", "[", "'next_item_labels'", "]", "\n", "last_item_label", "=", "run_values", ".", "results", "[", "'last_item_label'", "]", "\n", "\n", "users_ids", "=", "run_values", ".", "results", "[", "'user_id'", "]", "\n", "sessions_ids", "=", "run_values", ".", "results", "[", "'session_id'", "]", "\n", "\n", "\n", "if", "self", ".", "eval_cold_start", "or", "(", "self", ".", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ")", ":", "\n", "            ", "predicted_item_ids", "=", "run_values", ".", "results", "[", "'predicted_item_ids'", "]", "\n", "#tf.logging.info('predicted_item_ids (shape): {}'.format(predicted_item_ids.shape))  ", "\n", "eval_batch_negative_items", "=", "run_values", ".", "results", "[", "'eval_batch_negative_items'", "]", "\n", "\n", "\n", "", "if", "self", ".", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "            ", "self", ".", "eval_streaming_metrics_last", "=", "{", "}", "\n", "#self.eval_streaming_metrics_last['hitrate_at_1'] = run_values.results['hitrate_at_1']", "\n", "self", ".", "eval_streaming_metrics_last", "[", "'hitrate_at_n'", "]", "=", "run_values", ".", "results", "[", "'hitrate_at_n'", "]", "\n", "self", ".", "eval_streaming_metrics_last", "[", "'mrr_at_n'", "]", "=", "run_values", ".", "results", "[", "'mrr_at_n'", "]", "\n", "#self.eval_streaming_metrics_last['ndcg_at_n'] = run_values.results['ndcg_at_n']", "\n", "\n", "predicted_item_probs", "=", "run_values", ".", "results", "[", "'predicted_item_probs'", "]", "\n", "\n", "if", "self", ".", "sessions_negative_items_log", "!=", "None", ":", "\n", "#Acumulating session negative items, to allow evaluation comparison", "\n", "# with benchmarks outsite the framework (e.g. Matrix Factorization) ", "\n", "                ", "for", "session_id", ",", "labels", ",", "neg_items", "in", "zip", "(", "sessions_ids", ",", "\n", "next_item_labels", ",", "\n", "eval_batch_negative_items", ")", ":", "\n", "\n", "                    ", "self", ".", "sessions_negative_items_log", ".", "append", "(", "{", "'session_id'", ":", "str", "(", "session_id", ")", ",", "#Convert numeric session_id to str because large ints are not serializable", "\n", "'negative_items'", ":", "list", "(", "[", "neg_items_click", "for", "label", ",", "neg_items_click", "in", "zip", "(", "labels", ".", "tolist", "(", ")", ",", "\n", "neg_items", ".", "tolist", "(", ")", ")", "if", "label", "!=", "0", "]", ")", "}", ")", "\n", "\n", "\n", "", "", "if", "self", ".", "sessions_chameleon_recommendations_log", "!=", "None", ":", "\n", "                ", "predicted_item_probs", "=", "run_values", ".", "results", "[", "'predicted_item_probs'", "]", "\n", "predicted_item_probs_rounded", "=", "predicted_item_probs", ".", "round", "(", "decimals", "=", "7", ")", "\n", "\n", "articles_recent_pop_norm", "=", "self", ".", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", "\n", "\n", "#Acumulating CHAMELEON predictions, labels, scores, accuracy, novelty and diversity to allow greed re-ranking approachs (e.g. MMR)", "\n", "for", "session_id", ",", "labels", ",", "pred_item_ids", ",", "pred_item_probs", "in", "zip", "(", "sessions_ids", ",", "\n", "next_item_labels", ",", "\n", "predicted_item_ids", ",", "\n", "predicted_item_probs_rounded", "\n", ")", ":", "\n", "\n", "#Reducing the precision to 5 decimals for serialization", "\n", "                    ", "pred_item_norm_pops", "=", "articles_recent_pop_norm", "[", "pred_item_ids", "]", ".", "round", "(", "decimals", "=", "7", ")", "\n", "\n", "labels_filtered", "=", "[", "]", "\n", "pred_item_ids_filtered", "=", "[", "]", "\n", "pred_item_probs_filtered", "=", "[", "]", "\n", "pred_item_norm_pops_filtered", "=", "[", "]", "\n", "\n", "for", "label", ",", "pred_item_ids_click", ",", "pred_item_probs_click", ",", "pred_item_norm_pops_click", "in", "zip", "(", "labels", ".", "tolist", "(", ")", ",", "\n", "pred_item_ids", ".", "tolist", "(", ")", ",", "\n", "pred_item_probs", ".", "tolist", "(", ")", ",", "\n", "pred_item_norm_pops", ".", "tolist", "(", ")", ")", ":", "\n", "                        ", "if", "label", "!=", "0", ":", "\n", "                            ", "labels_filtered", ".", "append", "(", "label", ")", "\n", "pred_item_ids_filtered", ".", "append", "(", "pred_item_ids_click", ")", "\n", "pred_item_probs_filtered", ".", "append", "(", "pred_item_probs_click", ")", "\n", "pred_item_norm_pops_filtered", ".", "append", "(", "pred_item_norm_pops_click", ")", "\n", "\n", "\n", "", "", "to_append", "=", "{", "'session_id'", ":", "str", "(", "session_id", ")", ",", "#Convert numeric session_id to str because large ints are not serializable", "\n", "'next_click_labels'", ":", "labels_filtered", ",", "\n", "'predicted_item_ids'", ":", "pred_item_ids_filtered", ",", "\n", "'predicted_item_probs'", ":", "pred_item_probs_filtered", ",", "\n", "'predicted_item_norm_pop'", ":", "pred_item_norm_pops_filtered", "\n", "}", "\n", "self", ".", "sessions_chameleon_recommendations_log", ".", "append", "(", "to_append", ")", "\n", "\n", "", "", "batch_stats", "=", "{", "#'eval_sampled_negative_items': eval_batch_negative_items.shape[1],", "\n", "'batch_items_count'", ":", "run_values", ".", "results", "[", "'batch_items_count'", "]", ",", "\n", "'batch_unique_items_count'", ":", "run_values", ".", "results", "[", "'batch_unique_items_count'", "]", ",", "\n", "'batch_sessions_count'", ":", "len", "(", "sessions_ids", ")", "\n", "}", "\n", "self", ".", "stats_logs", ".", "append", "(", "batch_stats", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'batch_stats: {}'", ".", "format", "(", "batch_stats", ")", ")", "\n", "\n", "preds_norm_pop", "=", "self", ".", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", "[", "predicted_item_ids", "]", "\n", "\n", "\n", "labels_norm_pop", "=", "self", ".", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", "[", "next_item_labels", "]", "\n", "\n", "#Computing metrics for this neural model", "\n", "update_metrics", "(", "predicted_item_ids", ",", "next_item_labels", ",", "labels_norm_pop", ",", "preds_norm_pop", ",", "clicked_items", ",", "\n", "self", ".", "streaming_metrics", ",", "\n", "recommender", "=", "'chameleon'", ")", "\n", "model_metrics_values", "=", "compute_metrics_results", "(", "self", ".", "streaming_metrics", ",", "\n", "recommender", "=", "'chameleon'", ")", "\n", "self", ".", "eval_streaming_metrics_last", "=", "merge_two_dicts", "(", "self", ".", "eval_streaming_metrics_last", ",", "\n", "model_metrics_values", ")", "\n", "\n", "\n", "\n", "start_eval", "=", "time", "(", ")", "\n", "#Computing metrics for Benchmark recommenders", "\n", "for", "clf", "in", "self", ".", "bench_classifiers", ":", "\n", "                ", "self", ".", "evaluate_and_update_streaming_metrics_last", "(", "clf", ",", "users_ids", ",", "\n", "clicked_items", ",", "next_item_labels", ",", "eval_batch_negative_items", ")", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Total elapsed time evaluating benchmarks: {}'", ".", "format", "(", "time", "(", ")", "-", "start_eval", ")", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Finished benchmarks evaluation'", ")", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "", "if", "self", ".", "eval_cold_start", ":", "\n", "#Updates information related to the item cold-start analysis (step of first click, # steps to the first recommendation)", "\n", "#Ps. As it predicts with the benchmarks, it is necessary to do this before training the benchmark with this batch", "\n", "            ", "self", ".", "update_items_cold_start_state", "(", "users_ids", ",", "clicked_items", ",", "next_item_labels", ",", "eval_batch_negative_items", ",", "\n", "predicted_item_ids", ")", "\n", "\n", "\n", "#Training benchmark classifier", "\n", "", "for", "clf", "in", "self", ".", "bench_classifiers", ":", "\n", "#It is required that session_ids are sorted by time (ex: first_timestamp+hash_session_id), so that", "\n", "#recommenders that trust in session_id to sort by recency work (e.g. V-SkNN)", "\n", "            ", "clf", ".", "train", "(", "users_ids", ",", "sessions_ids", ",", "clicked_items", ",", "next_item_labels", ")", "\n", "\n", "\n", "#Concatenating all clicked items in the batch (including last label)", "\n", "", "batch_clicked_items", "=", "np", ".", "concatenate", "(", "[", "clicked_items", ",", "last_item_label", "]", ",", "axis", "=", "1", ")", "\n", "#Flattening values and removing padding items (zeroes) ", "\n", "batch_clicked_items_flatten", "=", "batch_clicked_items", ".", "reshape", "(", "-", "1", ")", "\n", "batch_clicked_items_nonzero", "=", "batch_clicked_items_flatten", "[", "np", ".", "nonzero", "(", "batch_clicked_items_flatten", ")", "]", "\n", "\n", "#As timestamp of last clicks are not available for each session, assuming they are the same than previous session click", "\n", "last_timestamp_batch", "=", "np", ".", "max", "(", "clicked_timestamps", ",", "axis", "=", "1", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "batch_clicked_timestamps", "=", "np", ".", "concatenate", "(", "[", "clicked_timestamps", ",", "last_timestamp_batch", "]", ",", "axis", "=", "1", ")", "\n", "#Flattening values and removing padding items (zeroes)        ", "\n", "batch_clicked_timestamps_flatten", "=", "batch_clicked_timestamps", ".", "reshape", "(", "-", "1", ")", "\n", "batch_clicked_timestamps_nonzero", "=", "batch_clicked_timestamps_flatten", "[", "np", ".", "nonzero", "(", "batch_clicked_items_flatten", ")", "]", "\n", "\n", "#Updating items state", "\n", "self", ".", "clicked_items_state", ".", "update_items_state", "(", "batch_clicked_items_nonzero", ",", "batch_clicked_timestamps_nonzero", ")", "\n", "self", ".", "clicked_items_state", ".", "update_items_coocurrences", "(", "batch_clicked_items", ")", "\n", "\n", "\n", "'''\n        #Commenting to improve performance during training\n        #To evaluate avg. time to recommend an article since its first click\n        self.clicked_items_state.update_items_first_click_ts(clicked_items, clicked_timestamps)\n        self.clicked_items_state.update_items_delay_for_first_recommendation(predicted_item_ids, \n                                                                             clicked_timestamps, \n                                                                             topn=self.eval_metrics_top_n)\n        '''", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.add_cold_start_stats": [[1662, 1667], ["nar_model.ItemsStateUpdaterHook.clicked_items_state.cold_start_state.get_statistics", "clf.get_cold_start_state().get_statistics", "clf.get_cold_start_state", "clf.get_clf_suffix"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.ColdStartAnalysisState.get_statistics", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.ColdStartAnalysisState.get_statistics", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_cold_start_state", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix"], ["", "def", "add_cold_start_stats", "(", "self", ",", "eval_metrics", ")", ":", "\n", "        ", "PREFIX", "=", "'coldstart_'", "\n", "eval_metrics", "[", "PREFIX", "+", "'chameleon'", "]", "=", "self", ".", "clicked_items_state", ".", "cold_start_state", ".", "get_statistics", "(", ")", "\n", "for", "clf", "in", "self", ".", "bench_classifiers", ":", "\n", "            ", "eval_metrics", "[", "PREFIX", "+", "clf", ".", "get_clf_suffix", "(", ")", "]", "=", "clf", ".", "get_cold_start_state", "(", ")", ".", "get_statistics", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.end": [[1669, 1694], ["numpy.sum", "numpy.sum", "nar_model.ItemsStateUpdaterHook.eval_sessions_metrics_log.append", "tensorflow.logging.info", "tensorflow.logging.info", "nar_model.ItemsStateUpdaterHook.clicked_items_state.restore_state_checkpoint", "nar_model.ItemsStateUpdaterHook.add_cold_start_stats", "sorted", "nar_model.ItemsStateUpdaterHook.eval_streaming_metrics_last.items"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.restore_state_checkpoint", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.add_cold_start_stats"], ["", "", "def", "end", "(", "self", ",", "session", "=", "None", ")", ":", "\n", "\n", "        ", "if", "self", ".", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "#avg_neg_items = np.mean([x['eval_sampled_negative_items'] for x in self.stats_logs])", "\n", "#self.eval_streaming_metrics_last['avg_eval_sampled_neg_items'] = avg_neg_items", "\n", "\n", "            ", "clicks_count", "=", "np", ".", "sum", "(", "[", "x", "[", "'batch_items_count'", "]", "for", "x", "in", "self", ".", "stats_logs", "]", ")", "\n", "self", ".", "eval_streaming_metrics_last", "[", "'clicks_count'", "]", "=", "clicks_count", "\n", "\n", "sessions_count", "=", "np", ".", "sum", "(", "[", "x", "[", "'batch_sessions_count'", "]", "for", "x", "in", "self", ".", "stats_logs", "]", ")", "\n", "self", ".", "eval_streaming_metrics_last", "[", "'sessions_count'", "]", "=", "sessions_count", "\n", "\n", "if", "self", ".", "eval_cold_start", ":", "\n", "                ", "self", ".", "add_cold_start_stats", "(", "self", ".", "eval_streaming_metrics_last", ")", "\n", "\n", "", "self", ".", "eval_sessions_metrics_log", ".", "append", "(", "self", ".", "eval_streaming_metrics_last", ")", "\n", "eval_metrics_str", "=", "'\\n'", ".", "join", "(", "[", "\"'{}':\\t{}\"", ".", "format", "(", "metric", ",", "value", ")", "for", "metric", ",", "value", "in", "sorted", "(", "self", ".", "eval_streaming_metrics_last", ".", "items", "(", ")", ")", "]", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Evaluation metrics: [{}]\"", ".", "format", "(", "eval_metrics_str", ")", ")", "\n", "\n", "#Logs stats for time delay for the first item recommendation since its first click", "\n", "#self.clicked_items_state.log_stats_time_for_first_rec()", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"Restoring items state checkpoint from train\"", ")", "\n", "#Restoring the original state of items popularity and recency state from train loop", "\n", "self", ".", "clicked_items_state", ".", "restore_state_checkpoint", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.ItemsStateUpdaterHook.create_eval_metrics": [[1695, 1722], ["clicked_items_state.get_recent_clicks_buffer", "eval_metrics.append", "eval_metrics.append", "eval_metrics.append", "eval_metrics.append", "eval_metrics.append", "metric", "metrics.ItemCoverage", "metrics.ExpectedRankSensitiveNovelty", "metrics.ExpectedRankRelevanceSensitiveNovelty", "metrics.ContentExpectedRankRelativeSensitiveIntraListDiversity", "metrics.ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity", "eval_metrics.append", "metrics.HitRateBySessionPosition"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_recent_clicks_buffer"], ["", "", "@", "staticmethod", "\n", "def", "create_eval_metrics", "(", "top_n", ",", "\n", "eval_negative_sample_relevance", ",", "\n", "eval_metrics_by_session_position", ",", "\n", "content_article_embeddings_matrix", ",", "\n", "articles_metadata", ",", "\n", "clicked_items_state", ")", ":", "\n", "\n", "        ", "relevance_positive_sample", "=", "1.0", "\n", "#Empirical: The weight of negative samples", "\n", "relevance_negative_samples", "=", "eval_negative_sample_relevance", "\n", "\n", "recent_clicks_buffer", "=", "clicked_items_state", ".", "get_recent_clicks_buffer", "(", ")", "\n", "\n", "eval_metrics", "=", "[", "metric", "(", "topn", "=", "top_n", ")", "for", "metric", "in", "[", "HitRate", ",", "MRR", ",", "NDCG", "]", "]", "\n", "\n", "eval_metrics", ".", "append", "(", "ItemCoverage", "(", "top_n", ",", "recent_clicks_buffer", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ExpectedRankSensitiveNovelty", "(", "top_n", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ExpectedRankRelevanceSensitiveNovelty", "(", "top_n", ",", "relevance_positive_sample", ",", "relevance_negative_samples", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ContentExpectedRankRelativeSensitiveIntraListDiversity", "(", "top_n", ",", "content_article_embeddings_matrix", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity", "(", "top_n", ",", "content_article_embeddings_matrix", ",", "relevance_positive_sample", ",", "relevance_negative_samples", ")", ")", "\n", "#eval_metrics.append(CategoryExpectedIntraListDiversity(top_n, articles_metadata['category_id']))", "\n", "\n", "if", "eval_metrics_by_session_position", ":", "\n", "            ", "eval_metrics", ".", "append", "(", "HitRateBySessionPosition", "(", "top_n", ")", ")", "\n", "\n", "", "return", "eval_metrics", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.get_embedding_size": [[25, 27], ["int", "math.floor"], "function", ["None"], ["def", "get_embedding_size", "(", "unique_val_count", ",", "const_mult", "=", "8", ")", ":", "\n", "    ", "return", "int", "(", "math", ".", "floor", "(", "const_mult", "*", "unique_val_count", "**", "0.25", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.log_base": [[28, 32], ["tensorflow.log", "tensorflow.log", "tensorflow.to_float", "tensorflow.constant"], "function", ["None"], ["", "def", "log_base", "(", "x", ",", "base", ")", ":", "\n", "    ", "numerator", "=", "tf", ".", "log", "(", "tf", ".", "to_float", "(", "x", ")", ")", "\n", "denominator", "=", "tf", ".", "log", "(", "tf", ".", "constant", "(", "base", ",", "dtype", "=", "numerator", ".", "dtype", ")", ")", "\n", "return", "numerator", "/", "denominator", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.log_1p": [[33, 35], ["nar_model.log_base"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.log_base"], ["", "def", "log_1p", "(", "x", ",", "base", ")", ":", "\n", "    ", "return", "log_base", "(", "x", "+", "1", ",", "base", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.tf_ndcg_at_k": [[36, 62], ["tensorflow.nn.top_k", "nar_model.tf_ndcg_at_k._tf_dcg_at_k"], "function", ["None"], ["", "def", "tf_ndcg_at_k", "(", "r", ",", "k", ")", ":", "\n", "    ", "def", "_tf_dcg_at_k", "(", "r", ",", "k", ")", ":", "\n", "        ", "last_dim_size", "=", "tf", ".", "minimum", "(", "k", ",", "tf", ".", "shape", "(", "r", ")", "[", "-", "1", "]", ")", "\n", "\n", "input_rank", "=", "tf", ".", "rank", "(", "r", ")", "\n", "input_shape", "=", "tf", ".", "shape", "(", "r", ")", "\n", "slice_begin", "=", "tf", ".", "zeros", "(", "[", "input_rank", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "slice_size", "=", "tf", ".", "concat", "(", "[", "input_shape", "[", ":", "-", "1", "]", ",", "[", "last_dim_size", "]", "]", ",", "axis", "=", "0", ")", "\n", "r", "=", "tf", ".", "slice", "(", "tf", ".", "to_float", "(", "r", ")", ",", "\n", "begin", "=", "slice_begin", ",", "\n", "size", "=", "slice_size", ")", "\n", "\n", "last_dim_size", "=", "tf", ".", "shape", "(", "r", ")", "[", "-", "1", "]", "\n", "\n", "dcg", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "subtract", "(", "tf", ".", "pow", "(", "2.", ",", "r", ")", ",", "1", ")", "/", "log_base", "(", "tf", ".", "range", "(", "2", ",", "last_dim_size", "+", "2", ")", ",", "2.", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "dcg", "\n", "\n", "", "sorted_values", ",", "sorted_idx", "=", "tf", ".", "nn", ".", "top_k", "(", "r", ",", "k", "=", "tf", ".", "shape", "(", "r", ")", "[", "-", "1", "]", ")", "\n", "idcg", "=", "_tf_dcg_at_k", "(", "sorted_values", ",", "k", ")", "\n", "\n", "ndcg", "=", "_tf_dcg_at_k", "(", "r", ",", "k", ")", "/", "idcg", "\n", "#Filling up nans (due to zeroed IDCG) with zeros", "\n", "ndcg", "=", "tf", ".", "where", "(", "tf", ".", "is_nan", "(", "ndcg", ")", ",", "tf", ".", "zeros_like", "(", "ndcg", ")", ",", "ndcg", ")", "\n", "\n", "return", "ndcg", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.cartesian_product": [[63, 84], ["tensorflow.rank", "tensorflow.rank", "tensorflow.sparse_to_dense", "tensorflow.tile", "tensorflow.sparse_to_dense", "tensorflow.tile", "tensorflow.concat", "list", "tf.concat.set_shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.expand_dims", "tensorflow.expand_dims", "int", "int", "tf.concat.get_shape", "a.get_shape", "b.get_shape"], "function", ["None"], ["", "def", "cartesian_product", "(", "a", ",", "b", ",", "axis", ")", ":", "\n", "    ", "a_rank", "=", "tf", ".", "rank", "(", "a", ")", "\n", "a_dim", "=", "tf", ".", "shape", "(", "a", ")", "[", "axis", "]", "\n", "b_rank", "=", "tf", ".", "rank", "(", "b", ")", "\n", "b_dim", "=", "tf", ".", "shape", "(", "b", ")", "[", "axis", "]", "\n", "\n", "axis_a_repeat", "=", "tf", ".", "sparse_to_dense", "(", "sparse_indices", "=", "[", "axis", "+", "1", "]", ",", "sparse_values", "=", "[", "b_dim", "]", ",", "output_shape", "=", "[", "a_rank", "+", "1", "]", ",", "default_value", "=", "1", ")", "\n", "tile_a", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "a", ",", "axis", "+", "1", ")", ",", "axis_a_repeat", ")", "\n", "\n", "axis_b_repeat", "=", "tf", ".", "sparse_to_dense", "(", "sparse_indices", "=", "[", "axis", "]", ",", "sparse_values", "=", "[", "a_dim", "]", ",", "output_shape", "=", "[", "b_rank", "+", "1", "]", ",", "default_value", "=", "1", ")", "\n", "tile_b", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "b", ",", "axis", ")", ",", "axis_b_repeat", ")", "\n", "\n", "cart_prod", "=", "tf", ".", "concat", "(", "[", "tile_a", ",", "tile_b", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "#Defining the last dimension of resulting tensor (originally undefined)", "\n", "last_dim", "=", "int", "(", "a", ".", "get_shape", "(", ")", "[", "-", "1", "]", ")", "+", "int", "(", "b", ".", "get_shape", "(", ")", "[", "-", "1", "]", ")", "\n", "cart_prod_shape", "=", "list", "(", "cart_prod", ".", "get_shape", "(", ")", ")", "\n", "cart_prod_shape", "[", "-", "1", "]", "=", "last_dim", "\n", "cart_prod", ".", "set_shape", "(", "cart_prod_shape", ")", "\n", "\n", "return", "cart_prod", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_model.shuffle_columns": [[87, 99], ["tensorflow.constant", "tensorflow.zeros", "tensorflow.while_loop", "tensorflow.shape", "tensorflow.concat", "tf.constant.get_shape", "tensorflow.TensorShape", "tensorflow.shape", "tensorflow.expand_dims", "tensorflow.random_shuffle"], "function", ["None"], ["", "def", "shuffle_columns", "(", "x", ")", ":", "\n", "    ", "batch_size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "\n", "counter", "=", "tf", ".", "constant", "(", "0", ")", "\n", "m0", "=", "tf", ".", "zeros", "(", "shape", "=", "[", "0", ",", "tf", ".", "shape", "(", "x", ")", "[", "1", "]", "]", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "cond", "=", "lambda", "i", ",", "m", ":", "i", "<", "batch_size", "\n", "body", "=", "lambda", "i", ",", "m", ":", "[", "i", "+", "1", ",", "tf", ".", "concat", "(", "[", "m", ",", "tf", ".", "expand_dims", "(", "tf", ".", "random_shuffle", "(", "x", "[", "i", "]", ")", ",", "0", ")", "]", ",", "axis", "=", "0", ")", "]", "\n", "_", ",", "shuffled_columns", "=", "tf", ".", "while_loop", "(", "\n", "cond", ",", "body", ",", "loop_vars", "=", "[", "counter", ",", "m0", "]", ",", "\n", "shape_invariants", "=", "[", "counter", ".", "get_shape", "(", ")", ",", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", "]", ")", "\n", "\n", "return", "shuffled_columns", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.serialize": [[19, 23], ["tensorflow.gfile.Open", "pickle.dump"], "function", ["None"], ["def", "serialize", "(", "filename", ",", "obj", ")", ":", "\n", "#with open(filename, 'wb') as handle:", "\n", "    ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "'wb'", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "obj", ",", "handle", ")", "#, protocol=pickle.HIGHEST_PROTOCOL)", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.deserialize": [[24, 28], ["tensorflow.gfile.Open", "pickle.load"], "function", ["None"], ["", "", "def", "deserialize", "(", "filename", ")", ":", "\n", "#with open(filename, 'rb') as handle:", "\n", "    ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "'rb'", ")", "as", "handle", ":", "\n", "        ", "return", "pickle", ".", "load", "(", "handle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.merge_two_dicts": [[29, 36], ["None"], "function", ["None"], ["", "", "def", "merge_two_dicts", "(", "x", ",", "y", ")", ":", "\n", "#Python 2 to 3.4", "\n", "#z = x.copy()   # start with x's keys and values", "\n", "#z.update(y)    # modifies z with y's keys and values & returns None", "\n", "#return z", "\n", "#Python 3.5 or greater", "\n", "    ", "return", "{", "**", "x", ",", "**", "y", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.log_elapsed_time": [[37, 40], ["tensorflow.logging.info", "time.time"], "function", ["None"], ["", "def", "log_elapsed_time", "(", "start_time", ",", "text", "=", "''", ")", ":", "\n", "    ", "took", "=", "(", "time", "(", ")", "-", "start_time", ")", "/", "60.0", "\n", "tf", ".", "logging", ".", "info", "(", "'==== {} elapsed {:.1f} minutes'", ".", "format", "(", "text", ",", "took", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.resolve_files": [[42, 52], ["tensorflow.train.match_filenames_once", "list", "tensorflow.global_variables_initializer", "tensorflow.local_variables_initializer", "tensorflow.Session", "sess.run", "sess.run", "sorted"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run"], ["", "def", "resolve_files", "(", "regex", ")", ":", "\n", "    ", "\"\"\"Return list of files given a regex\"\"\"", "\n", "list_op", "=", "tf", ".", "train", ".", "match_filenames_once", "(", "regex", ")", "\n", "init_ops", "=", "(", "tf", ".", "global_variables_initializer", "(", ")", ",", "\n", "tf", ".", "local_variables_initializer", "(", ")", ")", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "        ", "sess", ".", "run", "(", "init_ops", ")", "\n", "files", "=", "sess", ".", "run", "(", "list_op", ")", "\n", "\n", "", "return", "list", "(", "sorted", "(", "files", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.chunks": [[53, 57], ["range", "len"], "function", ["None"], ["", "def", "chunks", "(", "l", ",", "n", ")", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from l.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "l", ")", ",", "n", ")", ":", "\n", "        ", "yield", "l", "[", "i", ":", "i", "+", "n", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.get_tf_dtype": [[59, 69], ["Exception"], "function", ["None"], ["", "", "def", "get_tf_dtype", "(", "dtype", ")", ":", "\n", "    ", "if", "dtype", "==", "'int'", ":", "\n", "        ", "tf_dtype", "=", "tf", ".", "int64", "\n", "", "elif", "dtype", "==", "'float'", ":", "\n", "        ", "tf_dtype", "=", "tf", ".", "float32", "\n", "", "elif", "dtype", "==", "'string'", "or", "dtype", "==", "'bytes'", ":", "\n", "        ", "tf_dtype", "=", "tf", ".", "string", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Invalid dtype \"{}\"'", ".", "format", "(", "dtype", ")", ")", "\n", "", "return", "tf_dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.get_pad_token": [[71, 74], ["None"], "function", ["None"], ["", "def", "get_pad_token", "(", ")", ":", "\n", "    ", "PAD_TOKEN", "=", "'<PAD>'", "\n", "return", "PAD_TOKEN", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.get_unfrequent_token": [[75, 78], ["None"], "function", ["None"], ["", "def", "get_unfrequent_token", "(", ")", ":", "\n", "    ", "UNFREQ_TOKEN", "=", "'<UNF>'", "\n", "return", "UNFREQ_TOKEN", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.get_categ_encoder_from_values": [[79, 89], ["encoder_values.extend", "list", "dict", "encoder_values.append", "encoder_values.append", "range", "zip", "utils.get_pad_token", "utils.get_unfrequent_token", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_pad_token", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_unfrequent_token"], ["", "def", "get_categ_encoder_from_values", "(", "values", ",", "include_pad_token", "=", "True", ",", "include_unfrequent_token", "=", "False", ")", ":", "\n", "    ", "encoder_values", "=", "[", "]", "\n", "if", "include_pad_token", ":", "\n", "        ", "encoder_values", ".", "append", "(", "get_pad_token", "(", ")", ")", "\n", "", "if", "include_unfrequent_token", ":", "\n", "        ", "encoder_values", ".", "append", "(", "get_unfrequent_token", "(", ")", ")", "\n", "", "encoder_values", ".", "extend", "(", "values", ")", "\n", "encoder_ids", "=", "list", "(", "range", "(", "len", "(", "encoder_values", ")", ")", ")", "\n", "encoder_dict", "=", "dict", "(", "zip", "(", "encoder_values", ",", "encoder_ids", ")", ")", "\n", "return", "encoder_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.encode_categ_feature": [[90, 95], ["utils.get_unfrequent_token"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_unfrequent_token"], ["", "def", "encode_categ_feature", "(", "value", ",", "encoder_dict", ")", ":", "\n", "    ", "if", "value", "in", "encoder_dict", ":", "\n", "        ", "return", "encoder_dict", "[", "value", "]", "\n", "", "else", ":", "\n", "        ", "return", "encoder_dict", "[", "get_unfrequent_token", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.max_n_sparse_indexes": [[97, 102], ["row_data.argsort"], "function", ["None"], ["", "", "def", "max_n_sparse_indexes", "(", "row_data", ",", "row_indices", ",", "topn", ")", ":", "\n", "    ", "i", "=", "row_data", ".", "argsort", "(", ")", "[", "-", "topn", ":", "]", "[", ":", ":", "-", "1", "]", "\n", "top_values", "=", "row_data", "[", "i", "]", "\n", "top_indices", "=", "row_indices", "[", "i", "]", "\n", "return", "top_indices", "#, top_values", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.paired_permutations": [[104, 122], ["tensorflow.constant", "tensorflow.zeros", "tensorflow.while_loop", "tensorflow.shape", "tensorflow.concat", "tf.constant.get_shape", "tensorflow.TensorShape", "tensorflow.expand_dims", "tensorflow.stack", "tensorflow.to_int32", "tensorflow.mod", "tensorflow.div"], "function", ["None"], ["", "def", "paired_permutations", "(", "x", ")", ":", "\n", "#Ensuring the vector is flatten", "\n", "#x = tf.reshape(x, [-1])    ", "\n", "    ", "size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "\n", "counter", "=", "tf", ".", "constant", "(", "0", ")", "\n", "m0", "=", "tf", ".", "zeros", "(", "shape", "=", "[", "0", ",", "2", "]", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "cond", "=", "lambda", "i", ",", "m", ":", "i", "<", "size", "*", "size", "\n", "body", "=", "lambda", "i", ",", "m", ":", "[", "i", "+", "1", ",", "tf", ".", "concat", "(", "[", "m", ",", "tf", ".", "expand_dims", "(", "tf", ".", "stack", "(", "[", "x", "[", "tf", ".", "to_int32", "(", "tf", ".", "div", "(", "i", ",", "size", ")", ")", "]", ",", "\n", "x", "[", "tf", ".", "mod", "(", "i", ",", "size", ")", "]", "]", ")", "\n", ",", "axis", "=", "0", ")", "\n", "]", ",", "axis", "=", "0", ",", "name", "=", "\"concat_rows\"", ")", "\n", "]", "\n", "_", ",", "combined_values", "=", "tf", ".", "while_loop", "(", "\n", "cond", ",", "body", ",", "\n", "loop_vars", "=", "[", "counter", ",", "m0", "]", ",", "\n", "shape_invariants", "=", "[", "counter", ".", "get_shape", "(", ")", ",", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", "]", ")", "\n", "return", "combined_values", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.get_days_diff": [[124, 128], ["None"], "function", ["None"], ["", "def", "get_days_diff", "(", "newer_timestamp", ",", "older_timestamp", ")", ":", "\n", "    ", "sec_diff", "=", "newer_timestamp", "-", "older_timestamp", "\n", "days_diff", "=", "sec_diff", "/", "60", "/", "60", "/", "24", "\n", "return", "days_diff", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.get_time_decay_factor": [[129, 136], ["utils.get_days_diff", "math.pow"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.get_days_diff"], ["", "def", "get_time_decay_factor", "(", "newer_timestamp", ",", "older_timestamp", ",", "alpha", "=", "0.5", ")", ":", "\n", "    ", "days_diff", "=", "get_days_diff", "(", "newer_timestamp", ",", "older_timestamp", ")", "\n", "denominator", "=", "math", ".", "pow", "(", "1", "+", "alpha", ",", "days_diff", ")", "\n", "if", "denominator", "!=", "0", ":", "\n", "        ", "return", "1.0", "/", "denominator", "\n", "", "else", ":", "\n", "        ", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.append_lines_to_text_file": [[137, 140], ["open", "myfile.writelines"], "function", ["None"], ["", "", "def", "append_lines_to_text_file", "(", "filename", ",", "lines", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"a\"", ")", "as", "myfile", ":", "\n", "        ", "myfile", ".", "writelines", "(", "[", "line", "+", "\"\\n\"", "for", "line", "in", "lines", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.hash_str_to_int": [[141, 143], ["int", "str", "int", "hashlib.md5().hexdigest", "hashlib.md5"], "function", ["None"], ["", "", "def", "hash_str_to_int", "(", "encoded_bytes_text", ",", "digits", ")", ":", "\n", "    ", "return", "int", "(", "str", "(", "int", "(", "hashlib", ".", "md5", "(", "encoded_bytes_text", ")", ".", "hexdigest", "(", ")", "[", ":", "8", "]", ",", "16", ")", ")", "[", ":", "digits", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.get_os_list": [[145, 168], ["None"], "function", ["None"], ["", "def", "get_os_list", "(", ")", ":", "\n", "    ", "return", "[", "'iOS'", ",", "\n", "'Android'", ",", "\n", "'Windows Phone'", ",", "\n", "'Windows Mobile'", ",", "\n", "'Windows'", ",", "\n", "'Mac OS X'", ",", "\n", "'Mac OS'", ",", "\n", "'Samsung'", ",", "\n", "'FireHbbTV'", ",", "\n", "'ATV OS X'", ",", "\n", "'tvOS'", ",", "\n", "'Chrome OS'", ",", "\n", "'Debian'", ",", "\n", "'Symbian OS'", ",", "\n", "'BlackBerry OS'", ",", "\n", "'Firefox OS'", ",", "\n", "'Android'", ",", "\n", "'Brew MP'", ",", "\n", "'Chromecast'", ",", "\n", "'webOS'", ",", "\n", "'Gentoo'", ",", "\n", "'Solaris'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.extract_os_from_user_agent": [[169, 181], ["ua_parser.user_agent_parser.ParseOS", "utils.get_os_list"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.get_os_list"], ["", "def", "extract_os_from_user_agent", "(", "user_agent", ",", "default_os", "=", "'Other'", ")", ":", "\n", "    ", "parsed_os", "=", "user_agent_parser", ".", "ParseOS", "(", "user_agent", ")", "\n", "os_family", "=", "parsed_os", "[", "'family'", "]", "\n", "if", "'Symbian'", "in", "os_family", ":", "\n", "        ", "os_family", "=", "'Symbian OS'", "\n", "", "elif", "'BlackBerry'", "in", "os_family", ":", "\n", "        ", "os_family", "=", "'BlackBerry OS'", "\n", "\n", "", "if", "os_family", "is", "None", "or", "os_family", "not", "in", "get_os_list", "(", ")", ":", "\n", "        ", "os_family", "=", "default_os", "\n", "\n", "", "return", "os_family", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.extract_domain_from_url": [[184, 191], ["domain_pattern.search", "domain_pattern.search.group"], "function", ["None"], ["def", "extract_domain_from_url", "(", "url", ")", ":", "\n", "    ", "s", "=", "domain_pattern", ".", "search", "(", "url", ")", "\n", "if", "s", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "else", ":", "\n", "        ", "domain", "=", "s", ".", "group", "(", "0", ")", "\n", "return", "domain", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.urlencode": [[193, 195], ["urllib.parse.quote"], "function", ["None"], ["", "", "def", "urlencode", "(", "str", ")", ":", "\n", "  ", "return", "urllib", ".", "parse", ".", "quote", "(", "str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.urldecode": [[197, 199], ["urllib.parse.unquote"], "function", ["None"], ["", "def", "urldecode", "(", "str", ")", ":", "\n", "  ", "return", "urllib", ".", "parse", ".", "unquote", "(", "str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.extract_local_hour_weekday": [[200, 203], ["pytz.utc.localize().astimezone", "pytz.timezone", "pytz.utc.localize().astimezone.weekday", "pytz.utc.localize", "datetime.datetime.utcfromtimestamp"], "function", ["None"], ["", "def", "extract_local_hour_weekday", "(", "timestamp_in_utc", ",", "local_tz", ")", ":", "\n", "    ", "dt", "=", "pytz", ".", "utc", ".", "localize", "(", "datetime", ".", "datetime", ".", "utcfromtimestamp", "(", "timestamp_in_utc", ")", ")", ".", "astimezone", "(", "pytz", ".", "timezone", "(", "local_tz", ")", ")", "\n", "return", "dt", ".", "hour", "+", "(", "dt", ".", "minute", "/", "60.0", ")", ",", "dt", ".", "weekday", "(", ")", "#First day is Monday", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.strip_accents": [[205, 208], ["unicodedata.normalize", "unicodedata.category"], "function", ["None"], ["", "def", "strip_accents", "(", "s", ")", ":", "\n", "    ", "return", "''", ".", "join", "(", "c", "for", "c", "in", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "s", ")", "\n", "if", "unicodedata", ".", "category", "(", "c", ")", "!=", "'Mn'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.gini_index": [[210, 231], ["np.sort.flatten", "numpy.sort", "numpy.arange", "numpy.amin", "numpy.amin", "numpy.sum", "numpy.sum"], "function", ["None"], ["", "def", "gini_index", "(", "array", ")", ":", "\n", "    ", "\"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"", "\n", "# based on bottom eq:", "\n", "# http://www.statsdirect.com/help/generatedimages/equations/equation154.svg", "\n", "# from:", "\n", "# http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm", "\n", "# All values are treated equally, arrays must be 1d:", "\n", "array", "=", "array", ".", "flatten", "(", ")", "\n", "if", "np", ".", "amin", "(", "array", ")", "<", "0", ":", "\n", "# Values cannot be negative:", "\n", "        ", "array", "-=", "np", ".", "amin", "(", "array", ")", "\n", "# Values cannot be 0:", "\n", "", "array", "+=", "0.0000001", "\n", "# Values must be sorted:", "\n", "array", "=", "np", ".", "sort", "(", "array", ")", "\n", "# Index per array element:", "\n", "index", "=", "np", ".", "arange", "(", "1", ",", "array", ".", "shape", "[", "0", "]", "+", "1", ")", "\n", "# Number of array elements:", "\n", "n", "=", "array", ".", "shape", "[", "0", "]", "\n", "# Gini coefficient:", "\n", "return", "(", "(", "np", ".", "sum", "(", "(", "2", "*", "index", "-", "n", "-", "1", ")", "*", "array", ")", ")", "/", "(", "n", "*", "np", ".", "sum", "(", "array", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.min_max_scale": [[234, 238], ["sklearn.preprocessing.MinMaxScaler", "sklearn.preprocessing.MinMaxScaler.fit_transform"], "function", ["None"], ["", "def", "min_max_scale", "(", "vector", ",", "min_max_range", "=", "(", "-", "1.0", ",", "1.0", ")", ")", ":", "\n", "    ", "scaler", "=", "MinMaxScaler", "(", "feature_range", "=", "min_max_range", ")", "\n", "norm_vector", "=", "scaler", ".", "fit_transform", "(", "vector", ")", "\n", "return", "norm_vector", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.str2bool": [[241, 250], ["isinstance", "v.lower", "v.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["", "def", "str2bool", "(", "v", ")", ":", "\n", "    ", "if", "isinstance", "(", "v", ",", "bool", ")", ":", "\n", "       ", "return", "v", "\n", "", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Boolean value expected.'", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.ColdStartAnalysisState.__init__": [[52, 55], ["dict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "items_num_steps_before_first_rec", "=", "dict", "(", ")", "\n", "self", ".", "unique_clicked_items_count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.ColdStartAnalysisState.update_items_num_steps_before_first_rec": [[56, 69], ["batch_rec_items.reshape", "set", "len", "numpy.nonzero"], "methods", ["None"], ["", "def", "update_items_num_steps_before_first_rec", "(", "self", ",", "batch_rec_items", ",", "items_first_click_step", ",", "step", ")", ":", "\n", "        ", "batch_top_rec_ids_flatten", "=", "batch_rec_items", ".", "reshape", "(", "-", "1", ")", "\n", "batch_top_rec_ids_nonzero", "=", "batch_top_rec_ids_flatten", "[", "np", ".", "nonzero", "(", "batch_top_rec_ids_flatten", ")", "]", "\n", "batch_top_rec_ids_set", "=", "set", "(", "batch_top_rec_ids_nonzero", ")", "\n", "\n", "self", ".", "unique_clicked_items_count", "=", "len", "(", "items_first_click_step", ")", "\n", "\n", "for", "item_id", "in", "batch_top_rec_ids_set", ":", "\n", "            ", "if", "item_id", "in", "items_first_click_step", "and", "item_id", "not", "in", "self", ".", "items_num_steps_before_first_rec", ":", "\n", "                ", "elapsed_steps", "=", "step", "-", "items_first_click_step", "[", "item_id", "]", "\n", "assert", "elapsed_steps", ">=", "0", "\n", "self", ".", "items_num_steps_before_first_rec", "[", "item_id", "]", "=", "elapsed_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.ColdStartAnalysisState.get_statistics": [[71, 92], ["len", "numpy.array", "list", "numpy.min", "numpy.percentile", "numpy.percentile", "numpy.percentile", "numpy.percentile", "numpy.percentile", "numpy.percentile", "numpy.percentile", "numpy.max", "numpy.mean", "numpy.std", "len", "evaluation.ColdStartAnalysisState.items_num_steps_before_first_rec.values"], "methods", ["None"], ["", "", "", "def", "get_statistics", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "items_num_steps_before_first_rec", ")", ">", "0", ":", "\n", "            ", "values", "=", "np", ".", "array", "(", "list", "(", "self", ".", "items_num_steps_before_first_rec", ".", "values", "(", ")", ")", ")", "\n", "stats", "=", "{", "'min'", ":", "np", ".", "min", "(", "values", ")", ",", "\n", "'01%'", ":", "np", ".", "percentile", "(", "values", ",", "1", ")", ",", "\n", "'10%'", ":", "np", ".", "percentile", "(", "values", ",", "10", ")", ",", "\n", "'25%'", ":", "np", ".", "percentile", "(", "values", ",", "25", ")", ",", "\n", "'50%'", ":", "np", ".", "percentile", "(", "values", ",", "50", ")", ",", "\n", "'75%'", ":", "np", ".", "percentile", "(", "values", ",", "75", ")", ",", "\n", "'90%'", ":", "np", ".", "percentile", "(", "values", ",", "90", ")", ",", "\n", "'99%'", ":", "np", ".", "percentile", "(", "values", ",", "99", ")", ",", "\n", "'max'", ":", "np", ".", "max", "(", "values", ")", ",", "\n", "'mean'", ":", "np", ".", "mean", "(", "values", ")", ",", "\n", "'std'", ":", "np", ".", "std", "(", "values", ")", ",", "\n", "'uniqueRecommendedItemsCount'", ":", "len", "(", "values", ")", ",", "\n", "'uniqueClickedItemsCount'", ":", "self", ".", "unique_clicked_items_count", "\n", "}", "\n", "", "else", ":", "\n", "            ", "stats", "=", "{", "'uniqueClickedItemsCount'", ":", "0", "}", "\n", "\n", "", "return", "stats", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.update_metrics": [[12, 27], ["metric.add", "metric.add", "metric.add", "metric.add"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add"], ["def", "update_metrics", "(", "preds", ",", "labels", ",", "labels_norm_pop", ",", "preds_norm_pop", ",", "clicked_items", ",", "streaming_metrics", ",", "recommender", "=", "''", ")", ":", "\n", "    ", "for", "metric", "in", "streaming_metrics", ":", "\n", "        ", "if", "metric", ".", "name", "==", "HitRateBySessionPosition", ".", "name", ":", "\n", "#if recommender == 'chameleon':", "\n", "                ", "metric", ".", "add", "(", "preds", ",", "labels", ",", "labels_norm_pop", ")", "\n", "", "else", ":", "\n", "            ", "if", "metric", ".", "name", "==", "ItemCoverage", ".", "name", ":", "\n", "                ", "metric", ".", "add", "(", "preds", ",", "labels", ",", "clicked_items", ")", "\n", "", "elif", "metric", ".", "name", "in", "[", "PopularityBias", ".", "name", ",", "\n", "Novelty", ".", "name", ",", "\n", "ExpectedRankSensitiveNovelty", ".", "name", ",", "\n", "ExpectedRankRelevanceSensitiveNovelty", ".", "name", "]", ":", "\n", "                ", "metric", ".", "add", "(", "preds", ",", "labels", ",", "preds_norm_pop", ")", "\n", "", "else", ":", "\n", "                ", "metric", ".", "add", "(", "preds", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.compute_metrics_results": [[28, 47], ["metric.result", "metric.result"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.result", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.result"], ["", "", "", "", "def", "compute_metrics_results", "(", "streaming_metrics", ",", "recommender", "=", "''", ")", ":", "\n", "    ", "results", "=", "{", "}", "\n", "for", "metric", "in", "streaming_metrics", ":", "\n", "        ", "if", "metric", ".", "name", "==", "HitRateBySessionPosition", ".", "name", ":", "\n", "#if recommender == 'chameleon':", "\n", "                ", "recall_by_session_pos", ",", "avg_norm_pop_by_session_pos", ",", "hitrate_total_by_session_pos", "=", "metric", ".", "result", "(", ")", "\n", "for", "key", "in", "recall_by_session_pos", ":", "\n", "                    ", "results", "[", "'{}_{}_{:02d}'", ".", "format", "(", "metric", ".", "name", ",", "recommender", ",", "key", ")", "]", "=", "recall_by_session_pos", "[", "key", "]", "\n", "if", "recommender", "==", "'chameleon'", ":", "\n", "                        ", "results", "[", "'{}_{}_{:02d}'", ".", "format", "(", "'clicks_at_pos'", ",", "recommender", ",", "key", ")", "]", "=", "hitrate_total_by_session_pos", "[", "key", "]", "\n", "results", "[", "'{}_{}_{:02d}'", ".", "format", "(", "'avg_norm_pop_by_pos'", ",", "recommender", ",", "key", ")", "]", "=", "avg_norm_pop_by_session_pos", "[", "key", "]", "\n", "", "", "", "else", ":", "\n", "            ", "result", "=", "metric", ".", "result", "(", ")", "\n", "results", "[", "'{}_{}'", ".", "format", "(", "metric", ".", "name", ",", "recommender", ")", "]", "=", "result", "\n", "\n", "\n", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.expand_single_features": [[10, 18], ["tensorflow.expand_dims", "tensorflow.convert_to_tensor"], "function", ["None"], ["def", "expand_single_features", "(", "x", ",", "features_to_expand", ")", ":", "\n", "        ", "'''\n        Hack. Because padded_batch doesn't play nice with scalres, so we expand the scalar to a vector of length 1\n        '''", "\n", "for", "feature_key", "in", "features_to_expand", ":", "\n", "            ", "x", "[", "feature_key", "]", "=", "tf", ".", "expand_dims", "(", "tf", ".", "convert_to_tensor", "(", "x", "[", "feature_key", "]", ")", ",", "0", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.deflate_single_features": [[20, 28], ["datasets.expand_to_vector_if_scalar", "tensorflow.squeeze"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.expand_to_vector_if_scalar"], ["", "def", "deflate_single_features", "(", "x", ",", "expanded_features", ")", ":", "\n", "    ", "'''\n        Undo Hack. We undo the expansion we did in expand and make sure that vector has rank 2 (adds one dimension if this batch size == 1)\n    '''", "\n", "for", "feature_key", "in", "expanded_features", ":", "\n", "        ", "x", "[", "feature_key", "]", "=", "expand_to_vector_if_scalar", "(", "tf", ".", "squeeze", "(", "x", "[", "feature_key", "]", ")", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.expand_to_vector_if_scalar": [[29, 34], ["tensorflow.cond", "tensorflow.logical_and", "tensorflow.equal", "tensorflow.equal", "tensorflow.expand_dims", "tensorflow.size", "tensorflow.constant", "tensorflow.rank", "tensorflow.constant"], "function", ["None"], ["", "def", "expand_to_vector_if_scalar", "(", "tensor", ")", ":", "\n", "    ", "return", "tf", ".", "cond", "(", "tf", ".", "logical_and", "(", "tf", ".", "equal", "(", "tf", ".", "size", "(", "tensor", ")", ",", "tf", ".", "constant", "(", "1", ")", ")", ",", "\n", "tf", ".", "equal", "(", "tf", ".", "rank", "(", "tensor", ")", ",", "tf", ".", "constant", "(", "0", ")", ")", ")", ",", "\n", "lambda", ":", "tf", ".", "expand_dims", "(", "tensor", ",", "0", ")", ",", "\n", "lambda", ":", "tensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.parse_sequence_example": [[35, 83], ["tensorflow.parse_single_sequence_example", "tensorflow.minimum", "utils.merge_two_dicts", "datasets.expand_single_features", "tensorflow.FixedLenFeature", "tensorflow.FixedLenSequenceFeature", "list", "utils.get_tf_dtype", "utils.get_tf_dtype", "features_config[].keys"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.merge_two_dicts", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.expand_single_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_tf_dtype", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_tf_dtype"], ["", "def", "parse_sequence_example", "(", "example", ",", "features_config", ",", "truncate_sequence_length", "=", "20", ")", ":", "\n", "\n", "# Define how to parse the example", "\n", "    ", "context_features", "=", "{", "}", "\n", "features_config_single", "=", "features_config", "[", "'single_features'", "]", "\n", "for", "feature_name", "in", "features_config_single", ":", "\n", "        ", "context_features", "[", "feature_name", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "\n", "dtype", "=", "get_tf_dtype", "(", "features_config_single", "[", "feature_name", "]", "[", "'dtype'", "]", ")", ")", "\n", "\n", "\n", "", "sequence_features", "=", "{", "}", "\n", "features_config_sequence", "=", "features_config", "[", "'sequence_features'", "]", "\n", "for", "feature_name", "in", "features_config_sequence", ":", "\n", "        ", "sequence_features", "[", "feature_name", "]", "=", "tf", ".", "FixedLenSequenceFeature", "(", "shape", "=", "[", "]", ",", "\n", "dtype", "=", "get_tf_dtype", "(", "features_config_sequence", "[", "feature_name", "]", "[", "'dtype'", "]", ")", ")", "\n", "\n", "", "context_parsed", ",", "sequence_parsed", "=", "tf", ".", "parse_single_sequence_example", "(", "\n", "example", ",", "\n", "sequence_features", "=", "sequence_features", ",", "\n", "context_features", "=", "context_features", ",", "\n", "example_name", "=", "\"example\"", "\n", ")", "\n", "\n", "\n", "#Truncate long sessions to a limit", "\n", "context_parsed", "[", "'session_size'", "]", "=", "tf", ".", "minimum", "(", "context_parsed", "[", "'session_size'", "]", ",", "\n", "truncate_sequence_length", ")", "\n", "for", "feature_name", "in", "sequence_parsed", ":", "\n", "        ", "sequence_parsed", "[", "feature_name", "]", "=", "sequence_parsed", "[", "feature_name", "]", "[", ":", "truncate_sequence_length", "]", "\n", "\n", "\n", "#Ignoring first click from labels", "\n", "", "sequence_parsed", "[", "'label_next_item'", "]", "=", "sequence_parsed", "[", "'item_clicked'", "]", "[", "1", ":", "]", "\n", "#Making it easy to retrieve the last label", "\n", "sequence_parsed", "[", "'label_last_item'", "]", "=", "sequence_parsed", "[", "'item_clicked'", "]", "[", "-", "1", ":", "]", "\n", "\n", "#Ignoring last clicked item from input    ", "\n", "for", "feature_key", "in", "sequence_features", ":", "\n", "        ", "if", "feature_key", "not", "in", "[", "'label_next_item'", ",", "'label_last_item'", "]", ":", "\n", "            ", "sequence_parsed", "[", "feature_key", "]", "=", "sequence_parsed", "[", "feature_key", "]", "[", ":", "-", "1", "]", "\n", "\n", "", "", "merged_features", "=", "merge_two_dicts", "(", "context_parsed", ",", "sequence_parsed", ")", "\n", "\n", "#In order the pad the dataset, I had to use this hack to expand scalars to vectors.", "\n", "merged_expanded_features", "=", "expand_single_features", "(", "merged_features", ",", "\n", "features_to_expand", "=", "list", "(", "features_config", "[", "'single_features'", "]", ".", "keys", "(", ")", ")", ")", "\n", "\n", "return", "merged_expanded_features", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.deflate_and_split_features_label": [[84, 98], ["datasets.deflate_single_features"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.deflate_single_features"], ["", "def", "deflate_and_split_features_label", "(", "x", ",", "expanded_features", ")", ":", "\n", "#Undo that hack required for padding ", "\n", "    ", "x", "=", "deflate_single_features", "(", "x", ",", "expanded_features", ")", "\n", "\n", "labels", "=", "{", "\n", "'label_next_item'", ":", "x", "[", "'label_next_item'", "]", ",", "\n", "'label_last_item'", ":", "x", "[", "'label_last_item'", "]", "\n", "}", "\n", "\n", "del", "x", "[", "'label_next_item'", "]", "\n", "del", "x", "[", "'label_last_item'", "]", "\n", "\n", "#Returning features and label separatelly", "\n", "return", "(", "x", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.make_dataset": [[100, 144], ["tensorflow.data.TFRecordDataset", "dataset.prefetch.map", "datasets.make_dataset.get_features_shapes"], "function", ["None"], ["", "def", "make_dataset", "(", "path", ",", "features_config", ",", "batch_size", "=", "128", ",", "num_map_threads", "=", "None", ",", "\n", "truncate_sequence_length", "=", "20", ")", ":", "\n", "\n", "    ", "def", "get_features_shapes", "(", "features_config", ")", ":", "\n", "        ", "features_shapes", "=", "{", "}", "\n", "\n", "for", "feature_name", "in", "features_config", "[", "'single_features'", "]", ":", "\n", "            ", "features_shapes", "[", "feature_name", "]", "=", "1", "\n", "\n", "", "for", "feature_name", "in", "features_config", "[", "'sequence_features'", "]", ":", "\n", "            ", "features_shapes", "[", "feature_name", "]", "=", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", "\n", "\n", "", "features_shapes", "[", "'label_next_item'", "]", "=", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", "\n", "features_shapes", "[", "'label_last_item'", "]", "=", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", "\n", "\n", "return", "features_shapes", "\n", "\n", "\n", "", "if", "not", "num_map_threads", ":", "\n", "        ", "num_map_threads", "=", "multiprocessing", ".", "cpu_count", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Using {} threads for parallel map'", ".", "format", "(", "num_map_threads", ")", ")", "\n", "\n", "\n", "# Read a tf record file. This makes a dataset of raw TFRecords", "\n", "", "dataset", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "path", ",", "compression_type", "=", "'GZIP'", ")", "\n", "# Apply/map the parse function to every record. Now the dataset is a bunch of dictionaries of Tensors", "\n", "dataset", "=", "dataset", ".", "map", "(", "lambda", "x", ":", "parse_sequence_example", "(", "x", ",", "features_config", ",", "\n", "truncate_sequence_length", "=", "truncate_sequence_length", ")", ",", "\n", "num_parallel_calls", "=", "num_map_threads", ")", "\n", "\n", "\n", "\n", "#Batch the dataset so that we get batch_size examples in each batch.", "\n", "#Remember each item in the dataset is a dict of tensors, we need to specify padding for each tensor seperatly    ", "\n", "features_shapes", "=", "get_features_shapes", "(", "features_config", ")", "\n", "dataset", "=", "dataset", ".", "padded_batch", "(", "batch_size", ",", "padded_shapes", "=", "features_shapes", ")", "\n", "\n", "#Splitting features and label", "\n", "expanded_features", "=", "list", "(", "features_config", "[", "'single_features'", "]", ".", "keys", "(", ")", ")", "\n", "dataset", "=", "dataset", ".", "map", "(", "lambda", "x", ":", "deflate_and_split_features_label", "(", "x", ",", "expanded_features", ")", ",", "\n", "num_parallel_calls", "=", "num_map_threads", ")", "\n", "#Pre-fetches one batch ahead", "\n", "dataset", "=", "dataset", ".", "prefetch", "(", "1", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.prepare_dataset_iterator_with_init": [[146, 163], ["datasets.make_dataset", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "tf.data.Iterator.from_structure.make_initializer"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.make_dataset"], ["", "def", "prepare_dataset_iterator_with_init", "(", "files", ",", "features_config", ",", "batch_size", "=", "128", ",", "\n", "truncate_session_length", "=", "20", ")", ":", "\n", "# Make a dataset ", "\n", "    ", "ds", "=", "make_dataset", "(", "files", ",", "features_config", ",", "batch_size", "=", "batch_size", ",", "\n", "truncate_sequence_length", "=", "truncate_session_length", ")", "\n", "\n", "# Define an abstract iterator that has the shape and type of our datasets", "\n", "iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "ds", ".", "output_types", ",", "\n", "ds", ".", "output_shapes", ")", "\n", "\n", "# This is an op that gets the next element from the iterator", "\n", "next_element", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "# These ops let us switch and reinitialize every time we finish an epoch    ", "\n", "iterator_init_op", "=", "iterator", ".", "make_initializer", "(", "ds", ")", "\n", "\n", "return", "next_element", ",", "iterator_init_op", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.datasets.prepare_dataset_iterator": [[166, 180], ["tensorflow.device", "datasets.make_dataset", "make_dataset.make_one_shot_iterator", "ds.make_one_shot_iterator.get_next"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.make_dataset"], ["", "def", "prepare_dataset_iterator", "(", "files", ",", "features_config", ",", "batch_size", "=", "128", ",", "\n", "truncate_session_length", "=", "20", ")", ":", "\n", "    ", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "# Make a dataset ", "\n", "        ", "ds", "=", "make_dataset", "(", "files", ",", "features_config", ",", "batch_size", "=", "batch_size", ",", "\n", "truncate_sequence_length", "=", "truncate_session_length", ")", "\n", "\n", "# Define an abstract iterator that has the shape and type of our datasets", "\n", "iterator", "=", "ds", ".", "make_one_shot_iterator", "(", ")", "\n", "\n", "# This is an op that gets the next element from the iterator", "\n", "next_element", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "return", "next_element", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_gcom.create_args_parser": [[13, 29], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_clicks_csv_path_regex'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the clicks CSV files.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--number_hours_to_preprocess'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'Number of hours to preprocess'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_sessions_tfrecords_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for TFRecords generated with user sessions'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_gcom.load_sessions_by_hour": [[31, 52], ["pandas.read_csv", "pd.read_csv.sort_values", "pd.read_csv.groupby().agg().reset_index", "list", "pd.read_csv.groupby().agg", "pd.read_csv.groupby"], "function", ["None"], ["", "def", "load_sessions_by_hour", "(", "clicks_file_path", ")", ":", "\n", "    ", "def", "to_list", "(", "series", ")", ":", "\n", "        ", "return", "list", "(", "series", ")", "\n", "\n", "", "clicks_hour_df", "=", "pd", ".", "read_csv", "(", "clicks_file_path", ")", "\n", "#Ensuring that sessions are chronologically ordered", "\n", "clicks_hour_df", ".", "sort_values", "(", "[", "'session_start'", ",", "'click_timestamp'", "]", ",", "inplace", "=", "True", ")", "\n", "sessions_by_hour_df", "=", "clicks_hour_df", ".", "groupby", "(", "'session_id'", ")", ".", "agg", "(", "{", "'user_id'", ":", "min", ",", "\n", "'session_start'", ":", "min", ",", "\n", "'session_size'", ":", "min", ",", "\n", "'click_article_id'", ":", "to_list", ",", "\n", "'click_timestamp'", ":", "to_list", ",", "\n", "'click_environment'", ":", "to_list", ",", "\n", "'click_deviceGroup'", ":", "to_list", ",", "\n", "'click_os'", ":", "to_list", ",", "\n", "'click_country'", ":", "to_list", ",", "\n", "'click_region'", ":", "to_list", ",", "\n", "'click_referrer_type'", ":", "to_list", ",", "\n", "}", "\n", ")", ".", "reset_index", "(", ")", "\n", "return", "sessions_by_hour_df", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_gcom.get_cicled_feature_value": [[53, 58], ["numpy.sin", "numpy.cos"], "function", ["None"], ["", "def", "get_cicled_feature_value", "(", "value", ",", "max_value", ")", ":", "\n", "    ", "value_scaled", "=", "(", "value", "+", "0.000001", ")", "/", "max_value", "\n", "value_sin", "=", "np", ".", "sin", "(", "2", "*", "np", ".", "pi", "*", "value_scaled", ")", "\n", "value_cos", "=", "np", ".", "cos", "(", "2", "*", "np", ".", "pi", "*", "value_scaled", ")", "\n", "return", "value_sin", ",", "value_cos", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_gcom.get_time_features": [[59, 74], ["utils.extract_local_hour_weekday", "nar_preprocess_gcom.get_cicled_feature_value", "session_local_hours_sin.append", "session_local_hours_cos.append", "session_local_weekdays.append", "int"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.extract_local_hour_weekday", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.get_cicled_feature_value"], ["", "def", "get_time_features", "(", "clicks_timestamps", ",", "default_timezone", "=", "'America/Sao_Paulo'", ")", ":", "\n", "    ", "session_local_hours_sin", "=", "[", "]", "\n", "session_local_hours_cos", "=", "[", "]", "\n", "session_local_weekdays", "=", "[", "]", "\n", "for", "timestamp", "in", "clicks_timestamps", ":", "\n", "#Converting timestamp to the fDefault timezone, where most clicks originate", "\n", "        ", "local_hour", ",", "local_weekday", "=", "extract_local_hour_weekday", "(", "int", "(", "timestamp", ")", "//", "1000", ",", "default_timezone", ")", "\n", "#Converting hour in two cycling features to represent the continuity of the hours", "\n", "local_hour_sin", ",", "local_hour_cos", "=", "get_cicled_feature_value", "(", "local_hour", ",", "24", ")", "\n", "session_local_hours_sin", ".", "append", "(", "local_hour_sin", ")", "\n", "session_local_hours_cos", ".", "append", "(", "local_hour_cos", ")", "\n", "local_weekday_scaled", "=", "(", "local_weekday", "+", "1", ")", "/", "7", "#First day is Monday", "\n", "session_local_weekdays", ".", "append", "(", "local_weekday_scaled", ")", "\n", "\n", "", "return", "session_local_hours_sin", ",", "session_local_hours_cos", ",", "session_local_weekdays", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_gcom.make_sequence_example": [[75, 108], ["tensorflow.train.Features", "nar_preprocess_gcom.get_time_features", "tensorflow.train.FeatureLists", "tensorflow.train.SequenceExample", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_gcom.get_time_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature"], ["", "def", "make_sequence_example", "(", "row", ")", ":", "\n", "    ", "idx", ",", "fields", "=", "row", "\n", "\n", "context_features", "=", "{", "\n", "'user_id'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "fields", "[", "'user_id'", "]", "]", ")", ")", ",", "\n", "'session_id'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "fields", "[", "'session_id'", "]", "]", ")", ")", ",", "\n", "'session_start'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "fields", "[", "'session_start'", "]", "]", ")", ")", ",", "\n", "'session_size'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "fields", "[", "'session_size'", "]", "]", ")", ")", "\n", "}", "\n", "context", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "context_features", ")", "\n", "\n", "session_local_hours_sin", ",", "session_local_hours_cos", ",", "session_local_weekdays", "=", "get_time_features", "(", "fields", "[", "\"click_timestamp\"", "]", ")", "\n", "\n", "sequence_features", "=", "{", "\n", "'event_timestamp'", ":", "make_sequential_feature", "(", "fields", "[", "\"click_timestamp\"", "]", ")", ",", "\n", "#Categorical features", "\n", "'item_clicked'", ":", "make_sequential_feature", "(", "fields", "[", "\"click_article_id\"", "]", ")", ",", "\n", "'environment'", ":", "make_sequential_feature", "(", "fields", "[", "\"click_environment\"", "]", ")", ",", "\n", "'deviceGroup'", ":", "make_sequential_feature", "(", "fields", "[", "\"click_deviceGroup\"", "]", ")", ",", "\n", "'os'", ":", "make_sequential_feature", "(", "fields", "[", "\"click_os\"", "]", ")", ",", "\n", "'country'", ":", "make_sequential_feature", "(", "fields", "[", "\"click_country\"", "]", ")", ",", "\n", "'region'", ":", "make_sequential_feature", "(", "fields", "[", "\"click_region\"", "]", ")", ",", "\n", "'referrer_type'", ":", "make_sequential_feature", "(", "fields", "[", "\"click_referrer_type\"", "]", ")", ",", "\n", "#Numerical features", "\n", "'local_hour_sin'", ":", "make_sequential_feature", "(", "session_local_hours_sin", ",", "vtype", "=", "float", ")", ",", "\n", "'local_hour_cos'", ":", "make_sequential_feature", "(", "session_local_hours_cos", ",", "vtype", "=", "float", ")", ",", "\n", "'local_weekday'", ":", "make_sequential_feature", "(", "session_local_weekdays", ",", "vtype", "=", "float", ")", ",", "\n", "}", "\n", "\n", "sequence_feature_lists", "=", "tf", ".", "train", ".", "FeatureLists", "(", "feature_list", "=", "sequence_features", ")", "\n", "\n", "return", "tf", ".", "train", ".", "SequenceExample", "(", "feature_lists", "=", "sequence_feature_lists", ",", "\n", "context", "=", "context", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_gcom.main": [[112, 138], ["nar_preprocess_gcom.create_args_parser", "create_args_parser.parse_args", "print", "sorted", "print", "enumerate", "print", "glob.glob", "nar_preprocess_gcom.load_sessions_by_hour", "tf_records_management.save_rows_to_tf_record_file", "load_sessions_by_hour.iterrows", "print", "parser.parse_args.output_sessions_tfrecords_path.replace().format", "parser.parse_args.output_sessions_tfrecords_path.replace"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_gcom.load_sessions_by_hour", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.save_rows_to_tf_record_file"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "create_args_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "print", "(", "'Loading sessions by hour'", ")", "\n", "clicks_hour_files", "=", "sorted", "(", "glob", ".", "glob", "(", "args", ".", "input_clicks_csv_path_regex", ")", ")", "\n", "\n", "print", "(", "'Exporting sessions by hour to TFRecords: {}'", ".", "format", "(", "args", ".", "output_sessions_tfrecords_path", ")", ")", "\n", "#Exporting a TFRecord for each CSV clicks file (one by hour)", "\n", "for", "hour_index", ",", "clicks_hour_file_path", "in", "enumerate", "(", "clicks_hour_files", ")", ":", "\n", "        ", "sessions_by_hour_df", "=", "load_sessions_by_hour", "(", "clicks_hour_file_path", ")", "\n", "\n", "save_rows_to_tf_record_file", "(", "sessions_by_hour_df", ".", "iterrows", "(", ")", ",", "\n", "make_sequence_example", ",", "\n", "export_filename", "=", "args", ".", "output_sessions_tfrecords_path", ".", "replace", "(", "'*'", ",", "'{0:03d}'", ")", ".", "format", "(", "hour_index", ")", ")", "\n", "\n", "\n", "if", "hour_index", "%", "10", "==", "0", ":", "\n", "            ", "print", "(", "'Exported {} TFRecord files'", ".", "format", "(", "hour_index", ")", ")", "\n", "####print('global_stats', global_stats) ", "\n", "\n", "", "if", "args", ".", "number_hours_to_preprocess", ">=", "0", "and", "hour_index", "==", "args", ".", "number_hours_to_preprocess", ":", "\n", "            ", "break", "\n", "\n", "\n", "", "", "print", "(", "'Preprocessing finalized'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.create_args_parser": [[15, 43], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_sessions_json_folder_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the folder with sessions in JSON lines file, organized by hour (exported by the Spark script - nar_preprocessing_addressa_01_dataproc.ipynb).'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_acr_metadata_embeddings_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path for a pickle with articles metadata and content embeddings, generated by ACR module.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_nar_encoders_dict_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path for a pickle with the dictionary encoders for categorical features (exported by the Spark script - nar_preprocessing_addressa_01_dataproc.ipynb)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--number_hours_to_preprocess'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'Number of hours to preprocess'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_nar_preprocessing_resources_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for a pickle with label encoders and num scalers of clicks data.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_sessions_tfrecords_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for TFRecords generated with user sessions'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_acr_module_resources": [[44, 56], ["utils.deserialize", "articles_metadata_df.set_index"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["", "def", "load_acr_module_resources", "(", "acr_module_resources_path", ")", ":", "\n", "    ", "(", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings", ")", "=", "deserialize", "(", "acr_module_resources_path", ")", "\n", "\n", "articles_metadata_df", ".", "set_index", "(", "'article_id'", ",", "inplace", "=", "False", ")", "\n", "def", "get_article_text_length", "(", "article_id", ")", ":", "\n", "        ", "text_length", "=", "articles_metadata_df", ".", "loc", "[", "article_id", "]", "[", "'text_length'", "]", "\n", "return", "text_length", "\n", "\n", "#tf.logging.info(\"Read ACR label encoders for: {}\".format(acr_label_encoders.keys()))  ", "\n", "#article_id_label_encoder = acr_label_encoders['article_id']", "\n", "", "return", "get_article_text_length", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_nar_module_resources": [[58, 65], ["utils.deserialize", "print", "utils.deserialize.keys"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["", "def", "load_nar_module_resources", "(", "nar_encoders_dict_path", ")", ":", "\n", "    ", "nar_encoders_dict", "=", "deserialize", "(", "nar_encoders_dict_path", ")", "\n", "\n", "print", "(", "\"Read NAR label encoders dict for: {}\"", ".", "format", "(", "nar_encoders_dict", ".", "keys", "(", ")", ")", ")", "\n", "\n", "return", "nar_encoders_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_sessions_json_file": [[66, 70], ["open", "json.loads"], "function", ["None"], ["", "def", "load_sessions_json_file", "(", "json_path", ")", ":", "\n", "    ", "with", "open", "(", "json_path", ",", "'r'", ")", "as", "fi", ":", "\n", "        ", "for", "line", "in", "fi", ":", "\n", "            ", "yield", "json", ".", "loads", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_sessions_hour": [[71, 79], ["os.listdir", "os.path.join", "nar_preprocess_adressa.load_sessions_json_file", "sessions.append"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_sessions_json_file"], ["", "", "", "def", "load_sessions_hour", "(", "session_hour_path", ")", ":", "\n", "    ", "sessions", "=", "[", "]", "\n", "for", "session_file", "in", "os", ".", "listdir", "(", "session_hour_path", ")", ":", "\n", "        ", "session_file_path", "=", "os", ".", "path", ".", "join", "(", "session_hour_path", ",", "session_file", ")", "\n", "sessions_hour", "=", "load_sessions_json_file", "(", "session_file_path", ")", "\n", "for", "session", "in", "sessions_hour", ":", "\n", "            ", "sessions", ".", "append", "(", "session", ")", "\n", "", "", "return", "sessions", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_sessions_hours": [[80, 91], ["sorted", "int", "os.path.join", "nar_preprocess_adressa.load_sessions_hour", "os.listdir", "os.path.isdir", "hour_folder.split", "os.path.join", "x.split"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_sessions_hour"], ["", "def", "load_sessions_hours", "(", "folder_path", ")", ":", "\n", "#Sorting hours directories (treating cases where number of digits is lower. E.x. \"session_hour=3\" < \"session_hour=20\")", "\n", "    ", "hour_folders", "=", "sorted", "(", "[", "path", "for", "path", "in", "os", ".", "listdir", "(", "folder_path", ")", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "folder_path", ",", "path", ")", ")", "]", ",", "\n", "key", "=", "lambda", "x", ":", "\"{:0>5}\"", ".", "format", "(", "x", ".", "split", "(", "'='", ")", "[", "1", "]", ")", ")", "\n", "\n", "for", "hour_folder", "in", "hour_folders", ":", "\n", "        ", "hour_index", "=", "int", "(", "hour_folder", ".", "split", "(", "'='", ")", "[", "1", "]", ")", "\n", "hour_folder_path", "=", "os", ".", "path", ".", "join", "(", "folder_path", ",", "hour_folder", ")", "\n", "sessions_hour", "=", "load_sessions_hour", "(", "hour_folder_path", ")", "\n", "yield", "(", "hour_index", ",", "sessions_hour", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.standardize_num_feature": [[109, 113], ["list", "normalizer", "min", "int"], "function", ["None"], ["def", "standardize_num_feature", "(", "feature", ",", "values", ")", ":", "\n", "    ", "scaler_config", "=", "numeric_scalers", "[", "feature", "]", "\n", "normalizer", "=", "lambda", "x", ":", "(", "min", "(", "int", "(", "x", ")", ",", "scaler_config", "[", "'valid_max'", "]", ")", "-", "scaler_config", "[", "'avg'", "]", ")", "/", "scaler_config", "[", "'stddev'", "]", "\n", "return", "list", "(", "[", "normalizer", "(", "value", ")", "for", "value", "in", "values", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.get_cicled_feature_value": [[114, 119], ["numpy.sin", "numpy.cos"], "function", ["None"], ["", "def", "get_cicled_feature_value", "(", "value", ",", "max_value", ")", ":", "\n", "    ", "value_scaled", "=", "(", "value", "+", "0.000001", ")", "/", "max_value", "\n", "value_sin", "=", "np", ".", "sin", "(", "2", "*", "np", ".", "pi", "*", "value_scaled", ")", "\n", "value_cos", "=", "np", ".", "cos", "(", "2", "*", "np", ".", "pi", "*", "value_scaled", ")", "\n", "return", "value_sin", ",", "value_cos", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.process_session_clicks_features": [[121, 196], ["set", "pandas.DataFrame().sort_values", "dict", "numpy.array", "numpy.sum", "numpy.mean", "numpy.median", "print", "sessions.append", "collections.Counter", "list", "len", "utils.gini_index", "utils.extract_local_hour_weekday", "nar_preprocess_adressa.get_cicled_feature_value", "clicked_articles_ids.append", "set.add", "pandas.DataFrame", "dict.values", "float", "len", "np.array.astype", "nar_preprocess_adressa.standardize_num_feature", "get_article_text_length_fn", "nar_preprocess_adressa.standardize_num_feature", "session[].append"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.gini_index", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.extract_local_hour_weekday", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.get_cicled_feature_value", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.standardize_num_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.standardize_num_feature"], ["", "def", "process_session_clicks_features", "(", "sessions_hour", ",", "get_article_text_length_fn", ")", ":", "\n", "    ", "sessions", "=", "[", "]", "\n", "\n", "session_count", "=", "0", "\n", "clicked_articles_ids", "=", "[", "]", "\n", "unique_clicked_articles", "=", "set", "(", ")", "\n", "#Normalizing numerical features (standardization) and creating time features", "\n", "for", "session", "in", "sessions_hour", ":", "\n", "        ", "session_count", "+=", "1", "\n", "for", "click", "in", "session", "[", "'clicks'", "]", ":", "\n", "\n", "            ", "local_hour", ",", "local_weekday", "=", "extract_local_hour_weekday", "(", "click", "[", "'timestamp'", "]", "//", "1000", ",", "\n", "\"Europe/Oslo\"", ")", "\n", "#Normalizing weekday feature", "\n", "click", "[", "'weekday'", "]", "=", "(", "local_weekday", "+", "1", "-", "3.5", ")", "/", "7", "\n", "\n", "#Transforming the hour in two \"cyclic\" features, so that the network ", "\n", "#can understand, for example, that there is one hour of difference between both 11pm to 0am and from 0am to 1am", "\n", "click", "[", "'time_hour_sin'", "]", ",", "click", "[", "'time_hour_cos'", "]", "=", "get_cicled_feature_value", "(", "local_hour", ",", "24", ")", "\n", "\n", "#Applying standardization on elapsed time", "\n", "click", "[", "'_elapsed_ms_since_last_click'", "]", "=", "standardize_num_feature", "(", "'_elapsed_ms_since_last_click'", ",", "[", "click", "[", "'_elapsed_ms_since_last_click'", "]", "]", ")", "[", "0", "]", "\n", "\n", "#If active_time_secs is not available, use the average", "\n", "if", "'active_time_secs'", "not", "in", "click", ":", "\n", "                ", "click", "[", "'active_time_secs'", "]", "=", "numeric_scalers", "[", "'active_time_secs'", "]", "[", "'avg'", "]", "\n", "#Normalizing reading time by article length (#words)", "\n", "", "click", "[", "'active_time_secs_by_word'", "]", "=", "click", "[", "'active_time_secs'", "]", "/", "get_article_text_length_fn", "(", "click", "[", "'article_id'", "]", ")", "\n", "#Applying standardization", "\n", "click", "[", "'active_time_secs_by_word'", "]", "=", "standardize_num_feature", "(", "'active_time_secs_by_word'", ",", "[", "click", "[", "'active_time_secs_by_word'", "]", "]", ")", "[", "0", "]", "\n", "#Removing unnormalized feature", "\n", "del", "click", "[", "'active_time_secs'", "]", "\n", "\n", "#Applying standardization in this feature", "\n", "#click['active_time_secs'] = standardize_num_feature('active_time_secs', [click['active_time_secs']])[0]", "\n", "\n", "#Copying click attributes as lists in the session", "\n", "for", "key", "in", "click", ":", "\n", "                ", "if", "key", "!=", "\"user_id\"", ":", "\n", "                    ", "if", "key", "not", "in", "session", ":", "\n", "                        ", "session", "[", "key", "]", "=", "[", "click", "[", "key", "]", "]", "\n", "", "else", ":", "\n", "                        ", "session", "[", "key", "]", ".", "append", "(", "click", "[", "key", "]", ")", "\n", "\n", "", "", "", "clicked_articles_ids", ".", "append", "(", "click", "[", "'article_id'", "]", ")", "\n", "unique_clicked_articles", ".", "add", "(", "click", "[", "'article_id'", "]", ")", "\n", "\n", "#Removing clicks property, as its values were copied to individual list columns", "\n", "", "del", "session", "[", "'clicks'", "]", "\n", "sessions", ".", "append", "(", "session", ")", "\n", "\n", "#Ensuring sessions within the hour are sorted by session id (time)", "\n", "", "sessions_df", "=", "pd", ".", "DataFrame", "(", "sessions", ")", ".", "sort_values", "(", "'session_id'", ")", "\n", "\n", "#Printing stats", "\n", "clicks_by_articles_counter", "=", "dict", "(", "Counter", "(", "clicked_articles_ids", ")", ")", "\n", "clicks_by_articles", "=", "np", ".", "array", "(", "list", "(", "clicks_by_articles_counter", ".", "values", "(", ")", ")", ")", "\n", "total_clicks", "=", "np", ".", "sum", "(", "clicks_by_articles", ")", "\n", "clicks_by_articles_norm", "=", "clicks_by_articles", "/", "total_clicks", "\n", "clicks_by_articles_norm_mean", "=", "np", ".", "mean", "(", "clicks_by_articles_norm", ")", "\n", "clicks_by_articles_norm_median", "=", "np", ".", "median", "(", "clicks_by_articles_norm", ")", "\n", "\n", "stats", "=", "{", "'session_count'", ":", "session_count", ",", "\n", "'clicks'", ":", "total_clicks", ",", "\n", "'clicks_by_session'", ":", "total_clicks", "/", "session_count", ",", "\n", "'unique_articles'", ":", "len", "(", "unique_clicked_articles", ")", ",", "\n", "'clicks_by_article'", ":", "float", "(", "total_clicks", ")", "/", "len", "(", "unique_clicked_articles", ")", ",", "\n", "'norm_pop_mean'", ":", "clicks_by_articles_norm_mean", ",", "\n", "'norm_pop_median'", ":", "clicks_by_articles_norm_median", ",", "\n", "'gini_index'", ":", "gini_index", "(", "clicks_by_articles", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "}", "\n", "\n", "print", "(", "\"Stats :{}\"", ".", "format", "(", "stats", ")", ")", "\n", "\n", "return", "sessions_df", ",", "stats", ",", "clicks_by_articles_counter", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.make_sequence_example": [[198, 231], ["tensorflow.train.Features", "tensorflow.train.FeatureLists", "tensorflow.train.SequenceExample", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.BytesList", "row[].encode"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature"], ["", "def", "make_sequence_example", "(", "row", ")", ":", "\n", "    ", "context_features", "=", "{", "\n", "'session_id'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'session_id'", "]", "]", ")", ")", ",", "\n", "'session_size'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'session_size'", "]", "]", ")", ")", ",", "\n", "'session_start'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'session_start'", "]", "]", ")", ")", ",", "\n", "'user_id'", ":", "tf", ".", "train", ".", "Feature", "(", "bytes_list", "=", "tf", ".", "train", ".", "BytesList", "(", "value", "=", "[", "row", "[", "'user_id'", "]", ".", "encode", "(", ")", "]", ")", ")", ",", "\n", "}", "\n", "\n", "context", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "context_features", ")", "\n", "\n", "sequence_features", "=", "{", "\n", "'event_timestamp'", ":", "make_sequential_feature", "(", "row", "[", "\"timestamp\"", "]", ")", ",", "\n", "#Categorical features", "\n", "'item_clicked'", ":", "make_sequential_feature", "(", "row", "[", "\"article_id\"", "]", ")", ",", "\n", "'city'", ":", "make_sequential_feature", "(", "row", "[", "\"city\"", "]", ")", ",", "\n", "'region'", ":", "make_sequential_feature", "(", "row", "[", "\"region\"", "]", ")", ",", "\n", "'country'", ":", "make_sequential_feature", "(", "row", "[", "\"country\"", "]", ")", ",", "\n", "'device'", ":", "make_sequential_feature", "(", "row", "[", "\"device\"", "]", ")", ",", "\n", "'os'", ":", "make_sequential_feature", "(", "row", "[", "\"os\"", "]", ")", ",", "\n", "'referrer_class'", ":", "make_sequential_feature", "(", "row", "[", "\"referrer_class\"", "]", ")", ",", "\n", "'weekday'", ":", "make_sequential_feature", "(", "row", "[", "\"weekday\"", "]", ",", "vtype", "=", "float", ")", ",", "\n", "'local_hour_sin'", ":", "make_sequential_feature", "(", "row", "[", "\"time_hour_sin\"", "]", ",", "vtype", "=", "float", ")", ",", "\n", "'local_hour_cos'", ":", "make_sequential_feature", "(", "row", "[", "\"time_hour_cos\"", "]", ",", "vtype", "=", "float", ")", ",", "\n", "'user_elapsed_ms_since_last_click'", ":", "make_sequential_feature", "(", "row", "[", "\"_elapsed_ms_since_last_click\"", "]", ",", "vtype", "=", "float", ")", ",", "\n", "'active_time_secs_by_word'", ":", "make_sequential_feature", "(", "row", "[", "\"active_time_secs_by_word\"", "]", ",", "vtype", "=", "float", ")", ",", "\n", "#To debug", "\n", "'url'", ":", "make_sequential_feature", "(", "row", "[", "\"url\"", "]", ",", "vtype", "=", "str", ")", ",", "\n", "}", "\n", "\n", "sequence_feature_lists", "=", "tf", ".", "train", ".", "FeatureLists", "(", "feature_list", "=", "sequence_features", ")", "\n", "\n", "return", "tf", ".", "train", ".", "SequenceExample", "(", "feature_lists", "=", "sequence_feature_lists", ",", "\n", "context", "=", "context", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.export_sessions_hour_to_tf_records": [[233, 240], ["output_path.replace", "print", "tf_records_management.save_rows_to_tf_record_file", "map", "len", "sessions_df.iterrows", "output_path.replace.format"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.save_rows_to_tf_record_file"], ["", "def", "export_sessions_hour_to_tf_records", "(", "hour_index", ",", "sessions_df", ",", "output_path", ")", ":", "\n", "    ", "export_file_template", "=", "output_path", ".", "replace", "(", "'*'", ",", "'{0:04d}'", ")", "\n", "\n", "print", "(", "\"Exporting hour {} (# sessions: {})\"", ".", "format", "(", "hour_index", ",", "len", "(", "sessions_df", ")", ")", ")", "\n", "save_rows_to_tf_record_file", "(", "map", "(", "lambda", "x", ":", "x", "[", "1", "]", ",", "sessions_df", ".", "iterrows", "(", ")", ")", ",", "\n", "make_sequence_example", ",", "\n", "export_filename", "=", "export_file_template", ".", "format", "(", "hour_index", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.save_nar_preprocessing_resources": [[242, 246], ["utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "save_nar_preprocessing_resources", "(", "output_path", ",", "nar_label_encoders_dict", ",", "nar_numeric_scalers", ")", ":", "\n", "    ", "to_serialize", "=", "{", "'nar_label_encoders'", ":", "nar_label_encoders_dict", ",", "\n", "'nar_numeric_scalers'", ":", "nar_numeric_scalers", "}", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.compute_total_clicks_by_article_stats": [[247, 253], ["collections.defaultdict", "hour_counters.keys"], "function", ["None"], ["", "def", "compute_total_clicks_by_article_stats", "(", "clicks_by_articles_counters", ")", ":", "\n", "    ", "result", "=", "defaultdict", "(", "int", ")", "\n", "for", "hour_counters", "in", "clicks_by_articles_counters", ":", "\n", "        ", "for", "article_key", "in", "hour_counters", ".", "keys", "(", ")", ":", "\n", "            ", "result", "[", "article_key", "]", "+=", "hour_counters", "[", "article_key", "]", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.main": [[254, 300], ["nar_preprocess_adressa.create_args_parser", "create_args_parser.parse_args", "print", "nar_preprocess_adressa.load_acr_module_resources", "print", "nar_preprocess_adressa.load_nar_module_resources", "print", "print", "nar_preprocess_adressa.load_sessions_hours", "print", "print", "nar_preprocess_adressa.save_nar_preprocessing_resources", "print", "nar_preprocess_adressa.process_session_clicks_features", "stats.append", "clicks_by_articles_counters.append", "nar_preprocess_adressa.export_sessions_hour_to_tf_records", "print"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_acr_module_resources", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_nar_module_resources", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.load_sessions_hours", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.save_nar_preprocessing_resources", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.process_session_clicks_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.nar_preprocess_adressa.export_sessions_hour_to_tf_records"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "create_args_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "\n", "print", "(", "'Loading resources generated ACR module (articles metadata)'", ")", "\n", "get_article_text_length_fn", "=", "load_acr_module_resources", "(", "args", ".", "input_acr_metadata_embeddings_path", ")", "\n", "#get_article_text_length_fn = None", "\n", "\n", "print", "(", "'Loading resources generated by the first step of NAR preprocessing (cat. features dict encoders)'", ")", "\n", "nar_encoders_dict", "=", "load_nar_module_resources", "(", "args", ".", "input_nar_encoders_dict_path", ")", "\n", "\n", "print", "(", "'Loading sessions from folder: {}'", ".", "format", "(", "args", ".", "input_sessions_json_folder_path", ")", ")", "\n", "print", "(", "'Exporting TFRecords to: {}'", ".", "format", "(", "args", ".", "output_sessions_tfrecords_path", ")", ")", "\n", "\n", "\n", "clicks_by_articles_counters", "=", "[", "]", "\n", "for", "(", "hour_index", ",", "sessions_hour", ")", "in", "load_sessions_hours", "(", "args", ".", "input_sessions_json_folder_path", ")", ":", "\n", "        ", "print", "(", "'Processing hour {}'", ".", "format", "(", "hour_index", ")", ")", "\n", "\n", "####compute_global_metrics(sessions_hour)", "\n", "\n", "\n", "sessions_hour_df", ",", "hour_stats", ",", "hour_clicks_by_articles_counter", "=", "process_session_clicks_features", "(", "sessions_hour", ",", "get_article_text_length_fn", ")", "\n", "#sessions_hour_df.to_csv('hour-{}-to-debug.csv'.format(hour_index))", "\n", "\n", "hour_stats", "[", "'_hour_index'", "]", "=", "hour_index", "\n", "stats", ".", "append", "(", "hour_stats", ")", "\n", "\n", "clicks_by_articles_counters", ".", "append", "(", "hour_clicks_by_articles_counter", ")", "\n", "\n", "export_sessions_hour_to_tf_records", "(", "hour_index", ",", "sessions_hour_df", ",", "\n", "output_path", "=", "args", ".", "output_sessions_tfrecords_path", ")", "\n", "print", "(", "''", ")", "\n", "\n", "\n", "if", "args", ".", "number_hours_to_preprocess", ">=", "0", "and", "hour_index", "==", "args", ".", "number_hours_to_preprocess", ":", "\n", "            ", "break", "\n", "\n", "", "", "print", "(", ")", "\n", "\n", "\n", "print", "(", "'Exporting Categorical Feature encoders and Numeric scalers dicts: {}'", ".", "format", "(", "args", ".", "output_nar_preprocessing_resources_path", ")", ")", "\n", "save_nar_preprocessing_resources", "(", "args", ".", "output_nar_preprocessing_resources_path", ",", "\n", "nar_encoders_dict", ",", "\n", "numeric_scalers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.create_args_parser": [[19, 59], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_articles_folder_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input Adressa contentdata folder path.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_word_embeddings_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the word2vec embeddings model (word2vec).'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--max_words_length'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "\n", "help", "=", "'Maximum tokens length of text.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_tf_records_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for generated TFRecords with news content.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_word_vocab_embeddings_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for a pickle with words vocabulary and corresponding word embeddings.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_label_encoders'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for a pickle with label encoders for categorical features.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_articles_csv_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for a CSV file with articles contents.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--articles_by_tfrecord'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "\n", "help", "=", "'Number of articles to be exported in each TFRecords file'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--vocab_most_freq_words'", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "'Most frequent words to keep in vocab'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.unique_list_if_str": [[68, 73], ["type"], "function", ["None"], ["def", "unique_list_if_str", "(", "value", ")", ":", "\n", "    ", "if", "type", "(", "value", ")", "==", "list", ":", "\n", "        ", "return", "value", "\n", "", "else", ":", "\n", "        ", "return", "[", "value", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.parse_content_general": [[74, 90], ["json.loads"], "function", ["None"], ["", "", "def", "parse_content_general", "(", "line", ")", ":", "\n", "    ", "content_raw", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "new_content", "=", "{", "}", "\n", "for", "key", "in", "content_raw", ":", "\n", "        ", "if", "key", "==", "'fields'", ":", "\n", "            ", "for", "field", "in", "content_raw", "[", "'fields'", "]", ":", "\n", "                ", "value", "=", "field", "[", "'value'", "]", "\n", "if", "field", "[", "'field'", "]", "==", "'body'", ":", "\n", "                    ", "value", "=", "' '", ".", "join", "(", "value", ")", "\n", "", "new_content", "[", "field", "[", "'field'", "]", "]", "=", "value", "\n", "", "", "else", ":", "\n", "            ", "new_content", "[", "key", "]", "=", "content_raw", "[", "key", "]", "\n", "\n", "\n", "", "", "return", "new_content", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.parse_content": [[92, 135], ["acr_preprocess_adressa.parse_content_general", "collections.defaultdict", "int", "type", "set", "dateutil.parser.parse().timestamp", "type", "acr_preprocess_adressa.unique_list_if_str", "acr_preprocess_adressa.unique_list_if_str", "acr_preprocess_adressa.unique_list_if_str", "acr_preprocess_adressa.unique_list_if_str", "acr_preprocess_adressa.unique_list_if_str", "dateutil.parser.parse"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.parse_content_general", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.unique_list_if_str", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.unique_list_if_str", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.unique_list_if_str", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.unique_list_if_str", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.unique_list_if_str"], ["", "def", "parse_content", "(", "line", ")", ":", "\n", "    ", "content_json", "=", "parse_content_general", "(", "line", ")", "\n", "content_raw", "=", "defaultdict", "(", "str", ",", "content_json", ")", "\n", "\n", "publishtime", "=", "content_raw", "[", "'publishtime'", "]", "if", "content_raw", "[", "'publishtime'", "]", "!=", "''", "else", "content_raw", "[", "'createtime'", "]", "\n", "#Converting to unix timestamp in miliseconds", "\n", "publishtime_ts", "=", "int", "(", "parse", "(", "publishtime", ")", ".", "timestamp", "(", ")", ")", "*", "1000", "\n", "\n", "author_1st", "=", "content_raw", "[", "'author'", "]", "[", "0", "]", "if", "type", "(", "content_raw", "[", "'author'", "]", ")", "==", "list", "else", "content_raw", "[", "'author'", "]", "\n", "\n", "if", "type", "(", "content_raw", "[", "'heading'", "]", ")", "==", "list", ":", "\n", "        ", "heading", "=", "set", "(", "content_raw", "[", "'heading'", "]", ")", "#Set to remove repeated phrases", "\n", "", "else", ":", "\n", "        ", "heading", "=", "[", "content_raw", "[", "'heading'", "]", "]", "\n", "\n", "", "textual_highlights", "=", "\"{} | {} | {} | {}\"", ".", "format", "(", "content_raw", "[", "'title'", "]", ",", "\n", "content_raw", "[", "'teaser'", "]", ",", "\n", "'. '", ".", "join", "(", "heading", ")", ",", "\n", "content_raw", "[", "'body'", "]", ")", ".", "replace", "(", "u'\\xad'", ",", "''", ")", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "\n", "new_content", "=", "{", "'id'", ":", "content_raw", "[", "'id'", "]", ",", "\n", "'url'", ":", "content_raw", "[", "'url'", "]", ",", "\n", "'site'", ":", "unique_list_if_str", "(", "content_raw", "[", "'og-site-name'", "]", ")", "[", "0", "]", ",", "\n", "'adressa-access'", ":", "content_raw", "[", "'adressa-access'", "]", ",", "#(free, subscriber)", "\n", "'author_1st'", ":", "author_1st", "if", "author_1st", "!=", "''", "else", "''", ",", "#3777 unique                  ", "\n", "'publishtime'", ":", "publishtime", ",", "\n", "'created_at_ts'", ":", "publishtime_ts", ",", "\n", "'text_highlights'", ":", "textual_highlights", ",", "\n", "#Extracted using NLP techniques (by Adressa)", "\n", "'concepts'", ":", "','", ".", "join", "(", "unique_list_if_str", "(", "content_raw", "[", "'kw-concept'", "]", ")", ")", ",", "#98895 unique", "\n", "'entities'", ":", "','", ".", "join", "(", "unique_list_if_str", "(", "content_raw", "[", "'kw-entity'", "]", ")", ")", ",", "#150214 unique", "\n", "'locations'", ":", "','", ".", "join", "(", "unique_list_if_str", "(", "content_raw", "[", "'kw-location'", "]", ")", ")", ",", "#5533 unique", "\n", "'persons'", ":", "','", ".", "join", "(", "unique_list_if_str", "(", "content_raw", "[", "'kw-person'", "]", ")", ")", ",", "#53535 unique", "\n", "#Categories and keywords tagged by the journalists of Adresseavisen and may be of variable quality (label)", "\n", "'category0'", ":", "content_raw", "[", "'category0'", "]", ",", "#39 unique", "\n", "'category1'", ":", "content_raw", "[", "'category1'", "]", "if", "'category1'", "in", "content_raw", "else", "''", ",", "#126 unique", "\n", "'category2'", ":", "content_raw", "[", "'category2'", "]", "if", "'category2'", "in", "content_raw", "else", "''", ",", "#75 unique", "\n", "'keywords'", ":", "content_raw", "[", "'keywords'", "]", ",", "#6489 unique", "\n", "}", "\n", "\n", "\n", "return", "new_content", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.parse_content_file": [[136, 148], ["open", "print", "line.strip", "acr_preprocess_adressa.parse_content"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.parse_content"], ["", "def", "parse_content_file", "(", "file_path", ",", "parser_fn", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "fi", ":", "\n", "        ", "try", ":", "\n", "            ", "for", "line", "in", "fi", ":", "\n", "                ", "if", "line", ".", "strip", "(", ")", "==", "'null'", ":", "\n", "                    ", "return", "None", "\n", "", "content", "=", "parser_fn", "(", "line", ")", "\n", "#Returns only the first json from content file, as others are identical, only \"score\" is different", "\n", "return", "content", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "'Error processing file \"{}\": {}'", ".", "format", "(", "file_path", ",", "e", ")", ")", "\n", "raise", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.load_contents_from_files_list": [[149, 165], ["enumerate", "print", "acr_preprocess_adressa.parse_content_file", "os.path.join", "articles.append", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.parse_content_file"], ["", "", "", "def", "load_contents_from_files_list", "(", "root_path", ",", "files_list", ")", ":", "\n", "    ", "total_contents", "=", "0", "\n", "invalid_contents_count", "=", "0", "\n", "articles", "=", "[", "]", "\n", "\n", "for", "idx", ",", "filename", "in", "enumerate", "(", "files_list", ")", ":", "\n", "        ", "file_content", "=", "parse_content_file", "(", "os", ".", "path", ".", "join", "(", "root_path", ",", "filename", ")", ",", "parse_content", ")", "\n", "if", "file_content", "==", "None", ":", "\n", "#print('File content is null: {}'.format(filename))", "\n", "            ", "invalid_contents_count", "+=", "1", "\n", "", "else", ":", "\n", "            ", "articles", ".", "append", "(", "file_content", ")", "\n", "", "total_contents", "+=", "1", "\n", "\n", "", "print", "(", "\"Processed content files: {} - Empty files: {} - Valid articles: {}\"", ".", "format", "(", "total_contents", ",", "invalid_contents_count", ",", "len", "(", "articles", ")", ")", ")", "\n", "return", "articles", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.load_contents_from_folder": [[168, 184], ["utils.chunks", "pandas.DataFrame", "pd.DataFrame.drop_duplicates", "sorted", "joblib.Parallel", "os.listdir", "joblib.delayed", "news_df[].isin", "news_df[].astype().isin", "news_df[].astype"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.chunks"], ["def", "load_contents_from_folder", "(", "path", ")", ":", "\n", "    ", "articles_files_chunks", "=", "chunks", "(", "sorted", "(", "os", ".", "listdir", "(", "path", ")", ")", ",", "FILES_BY_CHUNK", ")", "\n", "#articles_files_chunks = [list(articles_files_chunks)[0]]", "\n", "\n", "#Starting 4 processes to speed up files parsing", "\n", "articles_chunks", "=", "Parallel", "(", "n_jobs", "=", "JOBS_TO_LOAD_FILES", ")", "(", "delayed", "(", "load_contents_from_files_list", ")", "(", "path", ",", "files_list", ")", "for", "files_list", "in", "articles_files_chunks", ")", "\n", "\n", "#Merging articles in a data frame", "\n", "news_df", "=", "pd", ".", "DataFrame", "(", "[", "articles", "for", "chunk", "in", "articles_chunks", "for", "articles", "in", "chunk", "]", ")", "\n", "\n", "#Filtering out news with invalid categories or from specific sites", "\n", "news_df", "=", "news_df", "[", "(", "~", "news_df", "[", "'category0'", "]", ".", "isin", "(", "CATEGORIES_TO_IGNORE", ")", ")", "&", "(", "~", "news_df", "[", "'site'", "]", ".", "astype", "(", "str", ")", ".", "isin", "(", "SITES_TO_IGNORE", ")", ")", "]", "\n", "\n", "news_df", ".", "drop_duplicates", "(", "subset", "=", "'id'", ",", "keep", "=", "'first'", ",", "inplace", "=", "True", ")", "\n", "return", "news_df", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.flatten_list_series": [[185, 187], ["pandas.DataFrame", "series_of_lists.apply().stack().reset_index", "series_of_lists.apply().stack", "series_of_lists.apply"], "function", ["None"], ["", "def", "flatten_list_series", "(", "series_of_lists", ")", ":", "\n", "    ", "return", "pd", ".", "DataFrame", "(", "series_of_lists", ".", "apply", "(", "pd", ".", "Series", ")", ".", "stack", "(", ")", ".", "reset_index", "(", "name", "=", "'item'", ")", ")", "[", "'item'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_freq_values": [[188, 191], ["series.groupby().size", "flatten_values_counts[].sort_values().reset_index", "series.groupby", "flatten_values_counts[].sort_values"], "function", ["None"], ["", "def", "get_freq_values", "(", "series", ",", "min_freq", "=", "100", ")", ":", "\n", "    ", "flatten_values_counts", "=", "series", ".", "groupby", "(", "series", ")", ".", "size", "(", ")", "\n", "return", "flatten_values_counts", "[", "flatten_values_counts", ">=", "min_freq", "]", ".", "sort_values", "(", "ascending", "=", "False", ")", ".", "reset_index", "(", "name", "=", "'count'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_freq_values_series_of_lists": [[192, 196], ["acr_preprocess_adressa.flatten_list_series", "acr_preprocess_adressa.get_freq_values"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.flatten_list_series", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_freq_values"], ["", "def", "get_freq_values_series_of_lists", "(", "series_of_lists", ",", "min_freq", "=", "100", ")", ":", "\n", "    ", "flatten_values", "=", "flatten_list_series", "(", "series_of_lists", ")", "\n", "flatten_values_counts", "=", "get_freq_values", "(", "flatten_values", ")", "\n", "return", "flatten_values_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.include_pad_token": [[198, 200], ["numpy.hstack"], "function", ["None"], ["def", "include_pad_token", "(", "unique_values", ")", ":", "\n", "        ", "return", "np", ".", "hstack", "(", "[", "[", "PAD_TOKEN", "]", ",", "unique_values", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.include_unfrequent_token": [[202, 204], ["numpy.hstack"], "function", ["None"], ["def", "include_unfrequent_token", "(", "unique_values", ")", ":", "\n", "        ", "return", "np", ".", "hstack", "(", "[", "[", "UNFREQ_TOKEN", "]", ",", "unique_values", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_encoder_from_freq_values": [[206, 210], ["acr_preprocess_adressa.get_freq_values", "utils.get_categ_encoder_from_values", "freq_values_counts_df[].unique"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_freq_values", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_categ_encoder_from_values"], ["", "def", "get_encoder_from_freq_values", "(", "series", ",", "min_freq", "=", "100", ")", ":", "\n", "    ", "freq_values_counts_df", "=", "get_freq_values", "(", "series", ",", "min_freq", "=", "min_freq", ")", "\n", "encoder", "=", "get_categ_encoder_from_values", "(", "freq_values_counts_df", "[", "freq_values_counts_df", ".", "columns", "[", "0", "]", "]", ".", "unique", "(", ")", ",", "include_unfrequent_token", "=", "True", ")", "\n", "return", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_column": [[211, 213], ["series.apply", "utils.encode_categ_feature"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.encode_categ_feature"], ["", "def", "transform_categorical_column", "(", "series", ",", "encoder", ")", ":", "\n", "    ", "return", "series", ".", "apply", "(", "lambda", "x", ":", "encode_categ_feature", "(", "x", ",", "encoder", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column": [[214, 218], ["acr_preprocess_adressa.get_freq_values_series_of_lists", "utils.get_categ_encoder_from_values", "freq_values_counts_df[].unique"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_freq_values_series_of_lists", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_categ_encoder_from_values"], ["", "def", "get_encoder_from_freq_values_in_list_column", "(", "series", ",", "min_freq", "=", "100", ")", ":", "\n", "    ", "freq_values_counts_df", "=", "get_freq_values_series_of_lists", "(", "series", ",", "min_freq", "=", "min_freq", ")", "\n", "encoder", "=", "get_categ_encoder_from_values", "(", "freq_values_counts_df", "[", "freq_values_counts_df", ".", "columns", "[", "0", "]", "]", ".", "unique", "(", ")", ",", "include_unfrequent_token", "=", "False", ")", "\n", "return", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_list_column": [[219, 221], ["series.apply", "list"], "function", ["None"], ["", "def", "transform_categorical_list_column", "(", "series", ",", "encoder", ")", ":", "\n", "    ", "return", "series", ".", "apply", "(", "lambda", "l", ":", "list", "(", "[", "encoder", "[", "val", "]", "for", "val", "in", "l", "if", "val", "in", "encoder", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.comma_sep_values_to_list": [[222, 224], ["list", "y.strip", "value.split", "y.strip"], "function", ["None"], ["", "def", "comma_sep_values_to_list", "(", "value", ")", ":", "\n", "    ", "return", "list", "(", "[", "y", ".", "strip", "(", ")", "for", "y", "in", "value", ".", "split", "(", "','", ")", "if", "y", ".", "strip", "(", ")", "!=", "''", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_sample_weight_inv_freq": [[225, 227], ["None"], "function", ["None"], ["", "def", "get_sample_weight_inv_freq", "(", "class_value", ",", "classes_count", ",", "numerator", "=", "10.0", ")", ":", "\n", "    ", "return", "numerator", "/", "classes_count", "[", "class_value", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.process_cat_features": [[228, 300], ["utils.get_categ_encoder_from_values", "print", "acr_preprocess_adressa.transform_categorical_column", "utils.get_categ_encoder_from_values", "print", "acr_preprocess_adressa.transform_categorical_column", "sklearn.utils.class_weight.compute_class_weight", "print", "utils.get_categ_encoder_from_values", "print", "acr_preprocess_adressa.transform_categorical_column", "sklearn.utils.class_weight.compute_class_weight", "print", "acr_preprocess_adressa.get_encoder_from_freq_values", "print", "acr_preprocess_adressa.transform_categorical_column", "news_df[].apply", "news_df[].apply", "news_df[].apply", "news_df[].apply", "news_df[].apply", "acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "acr_preprocess_adressa.transform_categorical_list_column", "print", "acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "acr_preprocess_adressa.transform_categorical_list_column", "print", "acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "acr_preprocess_adressa.transform_categorical_list_column", "print", "acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "acr_preprocess_adressa.transform_categorical_list_column", "print", "acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "acr_preprocess_adressa.transform_categorical_list_column", "print", "news_df[].unique", "news_df[].unique", "len", "len", "news_df[].unique", "len", "news_df[].unique", "len", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_categ_encoder_from_values", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_categ_encoder_from_values", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_categ_encoder_from_values", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_encoder_from_freq_values", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_list_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_list_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_list_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_list_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.get_encoder_from_freq_values_in_list_column", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.transform_categorical_list_column"], ["", "def", "process_cat_features", "(", "news_df", ")", ":", "\n", "    ", "article_id_encoder", "=", "get_categ_encoder_from_values", "(", "news_df", "[", "'id'", "]", ")", "\n", "print", "(", "'Articles - unique count {}'", ".", "format", "(", "len", "(", "article_id_encoder", ")", ")", ")", "\n", "news_df", "[", "'id_encoded'", "]", "=", "transform_categorical_column", "(", "news_df", "[", "'id'", "]", ",", "article_id_encoder", ")", "\n", "\n", "category0_encoder", "=", "get_categ_encoder_from_values", "(", "news_df", "[", "'category0'", "]", ".", "unique", "(", ")", ")", "\n", "print", "(", "'Category0 - unique count {}'", ".", "format", "(", "len", "(", "category0_encoder", ")", ")", ")", "\n", "news_df", "[", "'category0_encoded'", "]", "=", "transform_categorical_column", "(", "news_df", "[", "'category0'", "]", ",", "category0_encoder", ")", "\n", "\n", "category0_class_weights", "=", "class_weight", ".", "compute_class_weight", "(", "'balanced'", ",", "classes", "=", "news_df", "[", "'category0_encoded'", "]", ".", "unique", "(", ")", ",", "y", "=", "news_df", "[", "'category0_encoded'", "]", ")", "\n", "print", "(", "'Category0 weights: {}'", ".", "format", "(", "category0_class_weights", ")", ")", "\n", "\n", "category1_encoder", "=", "get_categ_encoder_from_values", "(", "news_df", "[", "'category1'", "]", ".", "unique", "(", ")", ")", "\n", "print", "(", "'Category1 - unique count {}'", ".", "format", "(", "len", "(", "category1_encoder", ")", ")", ")", "\n", "news_df", "[", "'category1_encoded'", "]", "=", "transform_categorical_column", "(", "news_df", "[", "'category1'", "]", ",", "category1_encoder", ")", "\n", "\n", "category1_class_weights", "=", "class_weight", ".", "compute_class_weight", "(", "'balanced'", ",", "classes", "=", "news_df", "[", "'category1_encoded'", "]", ".", "unique", "(", ")", ",", "y", "=", "news_df", "[", "'category1_encoded'", "]", ")", "\n", "print", "(", "'Category1 weights: {}'", ".", "format", "(", "category1_class_weights", ")", ")", "\n", "\n", "#Including only frequent authors", "\n", "author_encoder", "=", "get_encoder_from_freq_values", "(", "news_df", "[", "'author_1st'", "]", ")", "\n", "print", "(", "'Author - freq. unique count {}'", ".", "format", "(", "len", "(", "author_encoder", ")", ")", ")", "\n", "news_df", "[", "'author_encoded'", "]", "=", "transform_categorical_column", "(", "news_df", "[", "'author_1st'", "]", ",", "author_encoder", ")", "\n", "\n", "#Converting values separated by \",\" to lists", "\n", "news_df", "[", "'keywords'", "]", "=", "news_df", "[", "'keywords'", "]", ".", "apply", "(", "comma_sep_values_to_list", ")", "\n", "news_df", "[", "'concepts'", "]", "=", "news_df", "[", "'concepts'", "]", ".", "apply", "(", "comma_sep_values_to_list", ")", "\n", "news_df", "[", "'entities'", "]", "=", "news_df", "[", "'entities'", "]", ".", "apply", "(", "comma_sep_values_to_list", ")", "\n", "news_df", "[", "'locations'", "]", "=", "news_df", "[", "'locations'", "]", ".", "apply", "(", "comma_sep_values_to_list", ")", "\n", "news_df", "[", "'persons'", "]", "=", "news_df", "[", "'persons'", "]", ".", "apply", "(", "comma_sep_values_to_list", ")", "\n", "\n", "\n", "#Processing categorical list columns (Including only frequent categories)", "\n", "\n", "keywords_encoder", "=", "get_encoder_from_freq_values_in_list_column", "(", "news_df", "[", "'keywords'", "]", ")", "\n", "news_df", "[", "'keywords_encoded'", "]", "=", "transform_categorical_list_column", "(", "news_df", "[", "'keywords'", "]", ",", "keywords_encoder", ")", "\n", "print", "(", "'Keywords - freq. unique count {}'", ".", "format", "(", "len", "(", "keywords_encoder", ")", ")", ")", "\n", "\n", "concepts_encoder", "=", "get_encoder_from_freq_values_in_list_column", "(", "news_df", "[", "'concepts'", "]", ")", "\n", "news_df", "[", "'concepts_encoded'", "]", "=", "transform_categorical_list_column", "(", "news_df", "[", "'concepts'", "]", ",", "concepts_encoder", ")", "\n", "print", "(", "'Concepts - freq. unique count {}'", ".", "format", "(", "len", "(", "concepts_encoder", ")", ")", ")", "\n", "\n", "entities_encoder", "=", "get_encoder_from_freq_values_in_list_column", "(", "news_df", "[", "'entities'", "]", ")", "\n", "news_df", "[", "'entities_encoded'", "]", "=", "transform_categorical_list_column", "(", "news_df", "[", "'entities'", "]", ",", "entities_encoder", ")", "\n", "print", "(", "'Entities - freq. unique count {}'", ".", "format", "(", "len", "(", "entities_encoder", ")", ")", ")", "\n", "\n", "locations_encoder", "=", "get_encoder_from_freq_values_in_list_column", "(", "news_df", "[", "'locations'", "]", ")", "\n", "news_df", "[", "'locations_encoded'", "]", "=", "transform_categorical_list_column", "(", "news_df", "[", "'locations'", "]", ",", "locations_encoder", ")", "\n", "print", "(", "'Locations - freq. unique count {}'", ".", "format", "(", "len", "(", "locations_encoder", ")", ")", ")", "\n", "\n", "persons_encoder", "=", "get_encoder_from_freq_values_in_list_column", "(", "news_df", "[", "'persons'", "]", ")", "\n", "news_df", "[", "'persons_encoded'", "]", "=", "transform_categorical_list_column", "(", "news_df", "[", "'persons'", "]", ",", "persons_encoder", ")", "\n", "print", "(", "'Persons - freq. unique count {}'", ".", "format", "(", "len", "(", "persons_encoder", ")", ")", ")", "\n", "\n", "\n", "cat_features_encoders", "=", "{", "'article_id'", ":", "article_id_encoder", ",", "\n", "'category0'", ":", "category0_encoder", ",", "\n", "'category1'", ":", "category1_encoder", ",", "\n", "'keywords'", ":", "keywords_encoder", ",", "\n", "'author'", ":", "author_encoder", ",", "\n", "'concepts'", ":", "concepts_encoder", ",", "\n", "'entities'", ":", "entities_encoder", ",", "\n", "'locations'", ":", "locations_encoder", ",", "\n", "'persons'", ":", "persons_encoder", ",", "\n", "}", "\n", "\n", "labels_class_weights", "=", "{", "\n", "'category0'", ":", "category0_class_weights", ",", "\n", "'category1'", ":", "category1_class_weights", ",", "\n", "}", "\n", "\n", "return", "cat_features_encoders", ",", "labels_class_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.tokenize_norwegian_article": [[302, 316], ["text.replace.replace", "nltk.tokenize.sent_tokenize", "nltk.tokenize.word_tokenize", "words_tokenized.extend", "len"], "function", ["None"], ["", "def", "tokenize_norwegian_article", "(", "text", ",", "first_sentences", "=", "12", ",", "max_words_length", "=", "1000", ")", ":", "\n", "#Removing pipes for correct sentence tokenization", "\n", "    ", "text", "=", "text", ".", "replace", "(", "'|'", ",", "'.'", ")", "\n", "words_tokenized", "=", "[", "]", "\n", "sent_count", "=", "0", "\n", "for", "sentence", "in", "nltk", ".", "tokenize", ".", "sent_tokenize", "(", "text", ",", "language", "=", "'norwegian'", ")", ":", "\n", "        ", "sent_tokenized", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "sentence", ",", "language", "=", "'norwegian'", ")", "\n", "if", "len", "(", "sent_tokenized", ")", ">=", "3", "and", "sent_tokenized", "[", "-", "1", "]", "in", "[", "'.'", ",", "'!'", ",", "'?'", ",", "';'", "]", "and", "sent_tokenized", "!=", "[", "'Saken'", ",", "'oppdateres'", ",", "'.'", "]", ":", "\n", "            ", "sent_count", "+=", "1", "\n", "words_tokenized", ".", "extend", "(", "sent_tokenized", ")", "\n", "if", "sent_count", "==", "first_sentences", ":", "\n", "                ", "break", "\n", "", "", "", "return", "words_tokenized", "[", ":", "args", ".", "max_words_length", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.save_article_cat_encoders": [[318, 321], ["utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "save_article_cat_encoders", "(", "output_path", ",", "cat_features_encoders", ",", "labels_class_weights", ")", ":", "\n", "    ", "to_serialize", "=", "(", "cat_features_encoders", ",", "labels_class_weights", ")", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.make_sequence_example": [[323, 351], ["tensorflow.train.Features", "tensorflow.train.FeatureLists", "tensorflow.train.SequenceExample", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tf_records_management.make_sequential_feature", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.BytesList", "tensorflow.train.BytesList", "row[].encode", "row[].encode"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature"], ["", "def", "make_sequence_example", "(", "row", ")", ":", "\n", "    ", "context_features", "=", "{", "\n", "'article_id'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'id_encoded'", "]", "]", ")", ")", ",", "\n", "'category0'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'category0_encoded'", "]", "]", ")", ")", ",", "\n", "'category1'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'category1_encoded'", "]", "]", ")", ")", ",", "\n", "'author'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'author_encoded'", "]", "]", ")", ")", ",", "\n", "'created_at_ts'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'created_at_ts'", "]", "]", ")", ")", ",", "\n", "'text_length'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'text_length'", "]", "]", ")", ")", ",", "\n", "#Only for debug", "\n", "'article_id_original'", ":", "tf", ".", "train", ".", "Feature", "(", "bytes_list", "=", "tf", ".", "train", ".", "BytesList", "(", "value", "=", "[", "row", "[", "'id'", "]", ".", "encode", "(", ")", "]", ")", ")", ",", "\n", "'url'", ":", "tf", ".", "train", ".", "Feature", "(", "bytes_list", "=", "tf", ".", "train", ".", "BytesList", "(", "value", "=", "[", "row", "[", "'url'", "]", ".", "encode", "(", ")", "]", ")", ")", "\n", "}", "\n", "\n", "context", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "context_features", ")", "\n", "\n", "sequence_features", "=", "{", "\n", "'text'", ":", "make_sequential_feature", "(", "row", "[", "\"text_int\"", "]", ",", "vtype", "=", "int", ")", ",", "\n", "'keywords'", ":", "make_sequential_feature", "(", "row", "[", "\"keywords_encoded\"", "]", ",", "vtype", "=", "int", ")", ",", "\n", "'concepts'", ":", "make_sequential_feature", "(", "row", "[", "\"concepts_encoded\"", "]", ",", "vtype", "=", "int", ")", ",", "\n", "'entities'", ":", "make_sequential_feature", "(", "row", "[", "\"entities_encoded\"", "]", ",", "vtype", "=", "int", ")", ",", "\n", "'locations'", ":", "make_sequential_feature", "(", "row", "[", "\"locations_encoded\"", "]", ",", "vtype", "=", "int", ")", ",", "\n", "'persons'", ":", "make_sequential_feature", "(", "row", "[", "\"persons_encoded\"", "]", ",", "vtype", "=", "int", ")", "\n", "}", "\n", "\n", "sequence_feature_lists", "=", "tf", ".", "train", ".", "FeatureLists", "(", "feature_list", "=", "sequence_features", ")", "\n", "\n", "return", "tf", ".", "train", ".", "SequenceExample", "(", "feature_lists", "=", "sequence_feature_lists", ",", "\n", "context", "=", "context", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.main": [[353, 409], ["print", "acr_preprocess_adressa.load_contents_from_folder", "print", "print", "acr_preprocess_adressa.process_cat_features", "print", "acr_preprocess_adressa.save_article_cat_encoders", "print", "load_contents_from_folder.to_csv", "print", "tokenization.tokenize_articles", "print", "tokenization.get_words_freq", "print", "word_embeddings.load_word_embeddings", "word_embeddings.process_word_embedding_for_corpus_vocab", "print", "word_embeddings.save_word_vocab_embeddings", "print", "tokenization.convert_tokens_to_int", "print", "tf_records_management.export_dataframe_to_tf_records", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_adressa.load_contents_from_folder", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.process_cat_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.save_article_cat_encoders", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_articles", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.get_words_freq", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.word_embeddings.load_word_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.word_embeddings.process_word_embedding_for_corpus_vocab", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.word_embeddings.save_word_vocab_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.convert_tokens_to_int", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.export_dataframe_to_tf_records"], ["", "def", "main", "(", ")", ":", "\n", "\n", "\n", "    ", "print", "(", "'Loading contents from folder: {}'", ".", "format", "(", "args", ".", "input_articles_folder_path", ")", ")", "\n", "news_df", "=", "load_contents_from_folder", "(", "args", ".", "input_articles_folder_path", ")", "\n", "print", "(", "'Total articles loaded: {}'", ".", "format", "(", "len", "(", "news_df", ")", ")", ")", "\n", "\n", "print", "(", "'Encoding categorical features'", ")", "\n", "cat_features_encoders", ",", "labels_class_weights", "=", "process_cat_features", "(", "news_df", ")", "\n", "\n", "print", "(", "'Exporting LabelEncoders of categorical features: {}'", ".", "format", "(", "args", ".", "output_label_encoders", ")", ")", "\n", "save_article_cat_encoders", "(", "args", ".", "output_label_encoders", ",", "cat_features_encoders", ",", "labels_class_weights", ")", "\n", "\n", "print", "(", "\"Saving news articles CSV to {}\"", ".", "format", "(", "args", ".", "output_articles_csv_path", ")", ")", "\n", "news_df", ".", "to_csv", "(", "args", ".", "output_articles_csv_path", ",", "index", "=", "False", ")", "\n", "\n", "print", "(", "'Tokenizing articles...'", ")", "\n", "tokenized_articles", "=", "tokenize_articles", "(", "news_df", "[", "'text_highlights'", "]", ".", "values", ",", "tokenization_fn", "=", "tokenize_norwegian_article", ")", "\n", "\n", "print", "(", "'Computing word frequencies...'", ")", "\n", "words_freq", "=", "get_words_freq", "(", "tokenized_articles", ")", "\n", "\n", "print", "(", "\"Loading word2vec model and extracting words of this corpus' vocabulary...\"", ")", "\n", "w2v_model", "=", "load_word_embeddings", "(", "args", ".", "input_word_embeddings_path", ",", "binary", "=", "False", ")", "\n", "word_vocab", ",", "word_embeddings_matrix", "=", "process_word_embedding_for_corpus_vocab", "(", "w2v_model", ",", "\n", "words_freq", ",", "\n", "args", ".", "vocab_most_freq_words", ")", "\n", "\n", "print", "(", "'Saving word embeddings and vocab.: {}'", ".", "format", "(", "args", ".", "output_word_vocab_embeddings_path", ")", ")", "\n", "save_word_vocab_embeddings", "(", "args", ".", "output_word_vocab_embeddings_path", ",", "\n", "word_vocab", ",", "word_embeddings_matrix", ")", "\n", "\n", "print", "(", "'Converting tokens to int numbers (according to the vocab.)...'", ")", "\n", "texts_int", ",", "texts_lengths", "=", "convert_tokens_to_int", "(", "tokenized_articles", ",", "word_vocab", ")", "\n", "news_df", "[", "'text_length'", "]", "=", "texts_lengths", "\n", "news_df", "[", "'text_int'", "]", "=", "texts_int", "\n", "\n", "data_to_export_df", "=", "news_df", "[", "[", "'id'", ",", "'url'", ",", "#For debug", "\n", "'id_encoded'", ",", "\n", "'category0_encoded'", ",", "\n", "'category1_encoded'", ",", "\n", "'keywords_encoded'", ",", "\n", "'author_encoded'", ",", "\n", "'concepts_encoded'", ",", "\n", "'entities_encoded'", ",", "\n", "'locations_encoded'", ",", "\n", "'persons_encoded'", ",", "\n", "'created_at_ts'", ",", "\n", "'text_length'", ",", "\n", "'text_int'", "]", "]", "\n", "\n", "print", "(", "'Exporting tokenized articles to TFRecords: {}'", ".", "format", "(", "args", ".", "output_tf_records_path", ")", ")", "\n", "export_dataframe_to_tf_records", "(", "data_to_export_df", ",", "\n", "make_sequence_example", ",", "\n", "output_path", "=", "args", ".", "output_tf_records_path", ",", "\n", "examples_by_file", "=", "args", ".", "articles_by_tfrecord", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_adressa.create_args_parser": [[20, 31], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_articles_csv_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the news CSV file.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_article_content_embeddings'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_adressa.load_input_csv": [[34, 41], ["pandas.read_csv", "pd.read_csv.sort_values"], "function", ["None"], ["def", "load_input_csv", "(", "path", ")", ":", "\n", "    ", "news_df", "=", "pd", ".", "read_csv", "(", "path", ",", "encoding", "=", "'utf-8'", "\n", "#,nrows=1000", "\n", ")", "\n", "#Making sure articles are sorted by there encoded id", "\n", "news_df", ".", "sort_values", "(", "'id_encoded'", ",", "inplace", "=", "True", ")", "\n", "return", "news_df", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_adressa.tokenize_norwegian_article": [[43, 57], ["text.replace.replace", "nltk.tokenize.sent_tokenize", "nltk.tokenize.word_tokenize", "words_tokenized.extend", "len"], "function", ["None"], ["", "def", "tokenize_norwegian_article", "(", "text", ",", "first_sentences", "=", "12", ",", "max_words_length", "=", "1000", ")", ":", "\n", "#Removing pipes for correct sentence tokenization", "\n", "    ", "text", "=", "text", ".", "replace", "(", "'|'", ",", "'.'", ")", "\n", "words_tokenized", "=", "[", "]", "\n", "sent_count", "=", "0", "\n", "for", "sentence", "in", "nltk", ".", "tokenize", ".", "sent_tokenize", "(", "text", ",", "language", "=", "'norwegian'", ")", ":", "\n", "        ", "sent_tokenized", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "sentence", ",", "language", "=", "'norwegian'", ")", "\n", "if", "len", "(", "sent_tokenized", ")", ">=", "3", "and", "sent_tokenized", "[", "-", "1", "]", "in", "[", "'.'", ",", "'!'", ",", "'?'", ",", "';'", "]", "and", "sent_tokenized", "!=", "[", "'Saken'", ",", "'oppdateres'", ",", "'.'", "]", ":", "\n", "            ", "sent_count", "+=", "1", "\n", "words_tokenized", ".", "extend", "(", "sent_tokenized", ")", "\n", "if", "sent_count", "==", "first_sentences", ":", "\n", "                ", "break", "\n", "", "", "", "return", "words_tokenized", "[", ":", "max_words_length", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_adressa.export_article_content_embeddings": [[59, 65], ["print", "utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "export_article_content_embeddings", "(", "content_article_embeddings", ",", "output_article_content_embeddings", ")", ":", "\n", "    ", "output_path", "=", "output_article_content_embeddings", "\n", "print", "(", "'Exporting ACR Label Encoders, Article metadata and embeddings to {}'", ".", "format", "(", "output_path", ")", ")", "\n", "#to_serialize = (acr_label_encoders, articles_metadata_df, content_article_embeddings)", "\n", "to_serialize", "=", "content_article_embeddings", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_adressa.main": [[67, 116], ["lsa_adressa.create_args_parser", "create_args_parser.parse_args", "print", "lsa_adressa.load_input_csv", "print", "print", "print", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.decomposition.TruncatedSVD", "sklearn.pipeline.make_pipeline", "sklearn.pipeline.make_pipeline.fit_transform", "print", "numpy.vstack", "numpy.mean", "numpy.vstack", "print", "lsa_adressa.export_article_content_embeddings", "news_df[].max", "len", "len", "sklearn.preprocessing.Normalizer", "len", "news_df[].tail", "pandas.isnull"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.load_input_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.export_article_content_embeddings"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "create_args_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "print", "(", "'Loading news article CSV: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "news_df", "=", "load_input_csv", "(", "args", ".", "input_articles_csv_path", ")", "\n", "print", "(", "'N. docs: {}'", ".", "format", "(", "len", "(", "news_df", ")", ")", ")", "\n", "\n", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "values", "[", "0", "]", "==", "1", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "max", "(", ")", "==", "len", "(", "news_df", ")", "\n", "assert", "len", "(", "news_df", "[", "pd", ".", "isnull", "(", "news_df", "[", "'id_encoded'", "]", ")", "]", ")", "==", "0", "\n", "\n", "\n", "print", "(", "'Tokenizing articles...'", ")", "\n", "\n", "\n", "print", "(", "'LSA = TF-IDF + SVD...'", ")", "\n", "#https://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/", "\n", "vectorizer", "=", "TfidfVectorizer", "(", "analyzer", "=", "'word'", ",", "\n", "tokenizer", "=", "lambda", "x", ":", "x", ",", "\n", "preprocessor", "=", "lambda", "x", ":", "x", ",", "\n", "token_pattern", "=", "None", ",", "\n", "stop_words", "=", "None", ",", "\n", "ngram_range", "=", "(", "1", ",", "3", ")", ",", "max_df", "=", "0.4", ",", "\n", "min_df", "=", "2", ",", "max_features", "=", "50000", ",", "\n", "norm", "=", "'l2'", ",", "use_idf", "=", "True", ",", "\n", "smooth_idf", "=", "True", ",", "sublinear_tf", "=", "False", ")", "\n", "\n", "\n", "svd", "=", "TruncatedSVD", "(", "n_components", "=", "VECTOR_SIZE", ")", "\n", "\n", "lsa", "=", "make_pipeline", "(", "vectorizer", ",", "svd", ",", "Normalizer", "(", "copy", "=", "False", ")", ")", "\n", "\n", "reduced_content", "=", "lsa", ".", "fit_transform", "(", "tokenized_articles", ")", "\n", "\n", "print", "(", "'Concatenating article content embeddings, making sure that they are sorted by the encoded article id'", ")", "\n", "article_content_embeddings", "=", "np", ".", "vstack", "(", "reduced_content", ")", "\n", "#Checking if content articles embedding size correspond to the last article_id", "\n", "assert", "article_content_embeddings", ".", "shape", "[", "0", "]", "==", "news_df", "[", "'id_encoded'", "]", ".", "tail", "(", "1", ")", ".", "values", "[", "0", "]", "\n", "\n", "embedding_for_padding_article", "=", "np", ".", "mean", "(", "article_content_embeddings", ",", "axis", "=", "0", ")", "\n", "content_article_embeddings_with_padding", "=", "np", ".", "vstack", "(", "[", "embedding_for_padding_article", ",", "article_content_embeddings", "]", ")", "\n", "del", "article_content_embeddings", "\n", "\n", "\n", "print", "(", "'Exporting article content embeddings'", ")", "\n", "del", "news_df", "\n", "export_article_content_embeddings", "(", "content_article_embeddings_with_padding", ",", "args", ".", "output_article_content_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_gcom.create_args_parser": [[19, 34], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_articles_csv_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the news CSV file.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_label_encoders_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path for a pickle with label encoders (article_id, category_id, publisher_id).'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_article_content_embeddings'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_gcom.clean_str": [[68, 95], ["re_trim.sub.replace", "re_trim.sub.lower", "re_tree_dots.sub", "re.sub", "re_remove_brackets.sub", "re_changehyphen.sub", "re_remove_html.sub", "re_transform_numbers.sub", "re_transform_url.sub", "re_transform_emails.sub", "re_quotes_1.sub", "re_quotes_2.sub", "re_quotes_3.sub", "re.sub", "re_dots.sub", "re_punctuation.sub", "re_hiphen.sub", "re_punkts.sub", "re_punkts_b.sub", "re_punkts_c.sub", "re_doublequotes_1.sub", "re_doublequotes_2.sub", "re_trim.sub", "re_trim.sub.strip"], "function", ["None"], ["def", "clean_str", "(", "string", ")", ":", "\n", "    ", "string", "=", "string", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "\"\"\"Apply all regex above to a given string.\"\"\"", "\n", "string", "=", "string", ".", "lower", "(", ")", "\n", "string", "=", "re_tree_dots", ".", "sub", "(", "'...'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\\.\\.\\.'", ",", "''", ",", "string", ")", "\n", "string", "=", "re_remove_brackets", ".", "sub", "(", "''", ",", "string", ")", "\n", "string", "=", "re_changehyphen", ".", "sub", "(", "'-'", ",", "string", ")", "\n", "string", "=", "re_remove_html", ".", "sub", "(", "' '", ",", "string", ")", "\n", "string", "=", "re_transform_numbers", ".", "sub", "(", "'0'", ",", "string", ")", "\n", "string", "=", "re_transform_url", ".", "sub", "(", "'URL'", ",", "string", ")", "\n", "string", "=", "re_transform_emails", ".", "sub", "(", "'EMAIL'", ",", "string", ")", "\n", "string", "=", "re_quotes_1", ".", "sub", "(", "r'\\1\"'", ",", "string", ")", "\n", "string", "=", "re_quotes_2", ".", "sub", "(", "r'\"\\1'", ",", "string", ")", "\n", "string", "=", "re_quotes_3", ".", "sub", "(", "'\"'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\"'", ",", "''", ",", "string", ")", "\n", "string", "=", "re_dots", ".", "sub", "(", "'.'", ",", "string", ")", "\n", "string", "=", "re_punctuation", ".", "sub", "(", "r'\\1'", ",", "string", ")", "\n", "string", "=", "re_hiphen", ".", "sub", "(", "' - '", ",", "string", ")", "\n", "string", "=", "re_punkts", ".", "sub", "(", "r'\\1 \\2 \\3'", ",", "string", ")", "\n", "string", "=", "re_punkts_b", ".", "sub", "(", "r'\\1 \\2 \\3'", ",", "string", ")", "\n", "string", "=", "re_punkts_c", ".", "sub", "(", "r'\\1 \\2'", ",", "string", ")", "\n", "string", "=", "re_doublequotes_1", ".", "sub", "(", "'\\\"'", ",", "string", ")", "\n", "string", "=", "re_doublequotes_2", ".", "sub", "(", "'\\''", ",", "string", ")", "\n", "string", "=", "re_trim", ".", "sub", "(", "' '", ",", "string", ")", "\n", "\n", "return", "string", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_gcom.clean_and_filter_first_sentences": [[98, 107], ["sent_tokenizer.tokenize", "sentences.append", "sent.count", "doc2vec_gcom.clean_str", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.clean_str"], ["def", "clean_and_filter_first_sentences", "(", "string", ",", "first_sentences", "=", "8", ")", ":", "\n", "# Tokenize sentences and remove short and malformed sentences.", "\n", "    ", "sentences", "=", "[", "]", "\n", "for", "sent", "in", "sent_tokenizer", ".", "tokenize", "(", "string", ")", ":", "\n", "        ", "if", "sent", ".", "count", "(", "' '", ")", ">=", "3", "and", "sent", "[", "-", "1", "]", "in", "[", "'.'", ",", "'!'", ",", "'?'", ",", "';'", "]", ":", "\n", "            ", "sentences", ".", "append", "(", "clean_str", "(", "sent", ")", ")", "\n", "if", "len", "(", "sentences", ")", "==", "first_sentences", ":", "\n", "                ", "break", "\n", "", "", "", "return", "' '", ".", "join", "(", "sentences", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_gcom.load_input_csv": [[110, 122], ["pandas.read_csv", "news_df[].apply", "news_df[].apply", "news_df[].apply"], "function", ["None"], ["", "def", "load_input_csv", "(", "path", ")", ":", "\n", "    ", "news_df", "=", "pd", ".", "read_csv", "(", "path", ",", "encoding", "=", "'utf-8'", "\n", "#,nrows=1000", "\n", ")", "\n", "\n", "#Concatenating all available text", "\n", "news_df", "[", "'full_text'", "]", "=", "(", "news_df", "[", "'title'", "]", ".", "apply", "(", "nan_to_str", ")", "+", "\". \"", "+", "news_df", "[", "'caption'", "]", ".", "apply", "(", "nan_to_str", ")", "+", "\". \"", "+", "news_df", "[", "'body'", "]", ".", "apply", "(", "nan_to_str", ")", "\n", ")", ".", "apply", "(", "clean_and_filter_first_sentences", ")", "\n", "\n", "return", "news_df", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_gcom.load_acr_preprocessing_assets": [[145, 150], ["utils.deserialize", "print", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["def", "load_acr_preprocessing_assets", "(", "acr_label_encoders_path", ")", ":", "\n", "    ", "acr_label_encoders", "=", "deserialize", "(", "acr_label_encoders_path", ")", "\n", "print", "(", "\"Read article id label encoder: {}\"", ".", "format", "(", "len", "(", "acr_label_encoders", "[", "'article_id'", "]", ".", "classes_", ")", ")", ")", "\n", "\n", "return", "acr_label_encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_gcom.export_article_content_embeddings": [[151, 157], ["print", "utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "export_article_content_embeddings", "(", "content_article_embeddings", ",", "output_article_content_embeddings", ")", ":", "\n", "    ", "output_path", "=", "output_article_content_embeddings", "\n", "print", "(", "'Exporting ACR Label Encoders, Article metadata and embeddings to {}'", ".", "format", "(", "output_path", ")", ")", "\n", "#to_serialize = (acr_label_encoders, articles_metadata_df, content_article_embeddings)", "\n", "to_serialize", "=", "content_article_embeddings", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_gcom.main": [[159, 250], ["doc2vec_gcom.create_args_parser", "create_args_parser.parse_args", "print", "doc2vec_gcom.load_input_csv", "print", "print", "doc2vec_gcom.load_acr_preprocessing_assets", "acr_label_encoders[].transform", "news_df.sort_values.sort_values", "news_df[].values.flatten", "print", "print", "tokenization.tokenize_articles", "gensim.models.phrases.Phrases", "gensim.models.phrases.Phraser", "gensim.models.phrases.Phrases", "gensim.models.phrases.Phraser", "print", "print", "gensim.models.doc2vec.Doc2Vec", "gensim.models.doc2vec.Doc2Vec.build_vocab", "range", "print", "numpy.vstack", "print", "doc2vec_gcom.export_article_content_embeddings", "len", "len", "len", "len", "gensim.models.doc2vec.TaggedDocument", "print", "gensim.models.doc2vec.Doc2Vec.train", "len", "news_df[].max", "enumerate", "pandas.isnull"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.load_input_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.load_acr_preprocessing_assets", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_articles", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.export_article_content_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.train"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "create_args_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "print", "(", "'Loading news article CSV: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "news_df", "=", "load_input_csv", "(", "args", ".", "input_articles_csv_path", ")", "\n", "print", "(", "'N. docs: {}'", ".", "format", "(", "len", "(", "news_df", ")", ")", ")", "\n", "\n", "print", "(", "'ACR label encoder: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "acr_label_encoders", "=", "load_acr_preprocessing_assets", "(", "args", ".", "input_label_encoders_path", ")", "\n", "news_df", "[", "'id_encoded'", "]", "=", "acr_label_encoders", "[", "'article_id'", "]", ".", "transform", "(", "news_df", "[", "'id'", "]", ")", "\n", "\n", "#Sorting results by the encoded article Id, so that the matrix coincides and checking consistency", "\n", "news_df", "=", "news_df", ".", "sort_values", "(", "'id_encoded'", ")", "\n", "ids_encoded", "=", "news_df", "[", "'id_encoded'", "]", ".", "values", ".", "flatten", "(", ")", "\n", "print", "(", "'ids_encoded.shape'", ",", "ids_encoded", ".", "shape", ")", "\n", "\n", "assert", "len", "(", "news_df", ")", "==", "len", "(", "acr_label_encoders", "[", "'article_id'", "]", ".", "classes_", ")", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "values", "[", "0", "]", "==", "0", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "max", "(", ")", "+", "1", "==", "len", "(", "news_df", ")", "\n", "assert", "len", "(", "news_df", "[", "pd", ".", "isnull", "(", "news_df", "[", "'id_encoded'", "]", ")", "]", ")", "==", "0", "\n", "del", "acr_label_encoders", "\n", "\n", "'''\n    print('Encoding categorical features')\n    article_id_encoder, category_id_encoder, domainid_encoder = process_cat_features(news_df)\n    print('Exporting LabelEncoders of categorical features: {}'.format(args.output_label_encoders))\n    save_article_cat_encoders(args.output_label_encoders, \n                              article_id_encoder, \n                              category_id_encoder, \n                              domainid_encoder)\n    '''", "\n", "\n", "print", "(", "'Tokenizing articles...'", ")", "\n", "tokenized_articles", "=", "tokenize_articles", "(", "news_df", "[", "'full_text'", "]", ")", "\n", "del", "news_df", "\n", "\n", "#print('Computing word frequencies...')", "\n", "#words_freq = get_words_freq(tokenized_articles)", "\n", "#print('Corpus vocabulary size: {}'.format(len(words_freq)))", "\n", "\n", "#Dicovering frequent bigrams", "\n", "phrases", "=", "Phrases", "(", "tokenized_articles", ")", "\n", "bigram", "=", "Phraser", "(", "phrases", ")", "\n", "tg_phrases", "=", "Phrases", "(", "bigram", "[", "tokenized_articles", "]", ")", "\n", "trigram", "=", "Phraser", "(", "tg_phrases", ")", "\n", "\n", "print", "(", "'Processing documents...'", ")", "\n", "tagged_data", "=", "[", "TaggedDocument", "(", "words", "=", "w", ",", "tags", "=", "[", "i", "]", ")", "for", "i", ",", "w", "in", "enumerate", "(", "trigram", "[", "bigram", "[", "tokenized_articles", "]", "]", ")", "]", "\n", "\n", "\n", "print", "(", "'Training doc2vec'", ")", "\n", "max_epochs", "=", "30", "\n", "vec_size", "=", "250", "\n", "alpha", "=", "0.025", "\n", "#cores = multiprocessing.cpu_count()", "\n", "#DMM (Distributed Memory Mean) - See https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-6-doc2vec-603f11832504", "\n", "model", "=", "Doc2Vec", "(", "vector_size", "=", "vec_size", ",", "\n", "alpha", "=", "alpha", ",", "\n", "min_alpha", "=", "alpha", ",", "\n", "window", "=", "5", ",", "\n", "negative", "=", "5", ",", "\n", "min_count", "=", "2", ",", "\n", "max_vocab_size", "=", "100000", ",", "\n", "dm", "=", "1", ",", "\n", "dm_mean", "=", "1", ",", "\n", "workers", "=", "6", ")", "\n", "\n", "model", ".", "build_vocab", "(", "tagged_data", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "max_epochs", ")", ":", "\n", "        ", "print", "(", "'iteration {0}'", ".", "format", "(", "epoch", ")", ")", "\n", "model", ".", "train", "(", "tagged_data", ",", "\n", "#utils.shuffle([x for x in tagged_data]),", "\n", "total_examples", "=", "model", ".", "corpus_count", ",", "\n", "epochs", "=", "1", ")", "\n", "# decrease the learning rate", "\n", "model", ".", "alpha", "-=", "0.0002", "\n", "# fix the learning rate, no decay", "\n", "model", ".", "min_alpha", "=", "model", ".", "alpha", "\n", "\n", "\n", "#print('Encoding categorical features')", "\n", "#article_id_encoder = process_cat_features(news_df)", "\n", "\n", "", "print", "(", "'Concatenating article content embeddings, making sure that they are sorted by the encoded article id'", ")", "\n", "#article_content_embeddings = np.vstack([model.docvecs[i] for i in list(news_df.index)])    ", "\n", "article_content_embeddings", "=", "np", ".", "vstack", "(", "[", "model", ".", "docvecs", "[", "i", "]", "for", "i", "in", "ids_encoded", "]", ")", "\n", "\n", "print", "(", "'Exporting article content embeddings'", ")", "\n", "export_article_content_embeddings", "(", "article_content_embeddings", ",", "args", ".", "output_article_content_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.word_embeddings.load_word_embeddings": [[7, 10], ["gensim.models.keyedvectors.KeyedVectors.load_word2vec_format"], "function", ["None"], ["def", "load_word_embeddings", "(", "path", ",", "binary", "=", "True", ")", ":", "\n", "    ", "w2v_model", "=", "KeyedVectors", ".", "load_word2vec_format", "(", "path", ",", "binary", "=", "binary", ")", "\n", "return", "w2v_model", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.word_embeddings.process_word_embedding_for_corpus_vocab": [[11, 45], ["print", "set", "print", "set", "numpy.random.seed", "numpy.random.uniform", "numpy.random.uniform", "numpy.vstack", "print", "list", "len", "map", "len", "new_embeddings_list.append", "words_freq.most_common"], "function", ["None"], ["", "def", "process_word_embedding_for_corpus_vocab", "(", "w2v_model", ",", "words_freq", ",", "\n", "keep_most_frequent_words", "=", "100000", ")", ":", "\n", "    ", "print", "(", "'Tokens vocab. from articles: {}'", ".", "format", "(", "len", "(", "words_freq", ")", ")", ")", "\n", "most_freq_words", "=", "set", "(", "list", "(", "map", "(", "lambda", "x", ":", "x", "[", "0", "]", ",", "words_freq", ".", "most_common", "(", "keep_most_frequent_words", ")", ")", ")", ")", "\n", "print", "(", "'Most common tokens vocab. from articles: {}'", ".", "format", "(", "len", "(", "most_freq_words", ")", ")", ")", "\n", "\n", "RESERVED_TOKENS_IN_VOCAB", "=", "2", "\n", "\n", "embedding_size", "=", "w2v_model", ".", "vector_size", "\n", "new_embeddings_list", "=", "[", "]", "\n", "new_vocab", "=", "{", "}", "\n", "last_token_id", "=", "RESERVED_TOKENS_IN_VOCAB", "\n", "\n", "w2v_vocab", "=", "set", "(", "w2v_model", ".", "wv", ".", "index2word", ")", "\n", "for", "word", "in", "most_freq_words", ":", "\n", "        ", "if", "word", "in", "w2v_vocab", ":", "\n", "            ", "new_vocab", "[", "word", "]", "=", "last_token_id", "\n", "last_token_id", "+=", "1", "\n", "new_embeddings_list", ".", "append", "(", "w2v_model", "[", "word", "]", ")", "\n", "\n", "\n", "#Inserting the 2 reserved tokens", "\n", "", "", "new_vocab", "[", "PAD_TOKEN", "]", "=", "0", "\n", "new_vocab", "[", "UNK_TOKEN", "]", "=", "1", "\n", "\n", "np", ".", "random", ".", "seed", "(", "10", ")", "\n", "unk_vector", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "0.04", ",", "high", "=", "0.04", ",", "size", "=", "embedding_size", ")", "\n", "pad_vector", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "0.04", ",", "high", "=", "0.04", ",", "size", "=", "embedding_size", ")", "\n", "\n", "new_embeddings_matrix", "=", "np", ".", "vstack", "(", "[", "unk_vector", ",", "pad_vector", "]", "+", "new_embeddings_list", ")", "\n", "\n", "print", "(", "'Most common tokens with word embeddings: {}'", ".", "format", "(", "new_embeddings_matrix", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n", "return", "new_vocab", ",", "new_embeddings_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.word_embeddings.save_word_vocab_embeddings": [[47, 50], ["utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "save_word_vocab_embeddings", "(", "output_path", ",", "word_vocab", ",", "word_embeddings_matrix", ")", ":", "\n", "    ", "to_serialize", "=", "(", "word_vocab", ",", "word_embeddings_matrix", ")", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.nan_to_str": [[10, 12], ["type"], "function", ["None"], ["def", "nan_to_str", "(", "value", ")", ":", "\n", "    ", "return", "''", "if", "type", "(", "value", ")", "==", "float", "else", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.get_words_freq": [[13, 16], ["nltk.FreqDist"], "function", ["None"], ["", "def", "get_words_freq", "(", "tokenized_articles", ")", ":", "\n", "    ", "words_freq", "=", "FreqDist", "(", "[", "word", "for", "article", "in", "tokenized_articles", "for", "word", "in", "article", "]", ")", "\n", "return", "words_freq", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_text": [[17, 32], ["clean_str_fn", "nltk.tokenize.word_tokenize", "tokenized_text.append", "word.lower.lower"], "function", ["None"], ["", "def", "tokenize_text", "(", "text", ",", "clean_str_fn", ",", "lower_first_word_sentence", "=", "False", ")", ":", "\n", "    ", "text", "=", "clean_str_fn", "(", "text", ")", "\n", "tokenized_text", "=", "[", "]", "\n", "new_sentence", "=", "False", "\n", "for", "word", "in", "word_tokenize", "(", "text", ")", ":", "\n", "        ", "if", "word", "in", "[", "'.'", ",", "'?'", ",", "'!'", "]", ":", "\n", "            ", "new_sentence", "=", "True", "\n", "", "else", ":", "\n", "            ", "if", "lower_first_word_sentence", "and", "new_sentence", ":", "\n", "                ", "word", "=", "word", ".", "lower", "(", ")", "\n", "new_sentence", "=", "False", "\n", "\n", "", "", "tokenized_text", ".", "append", "(", "word", ")", "\n", "\n", "", "return", "tokenized_text", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_articles": [[33, 39], ["tokenization.tokenize_text", "tokenization_fn"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_text"], ["", "def", "tokenize_articles", "(", "articles", ",", "tokenization_fn", "=", "None", ",", "clean_str_fn", "=", "lambda", "x", ":", "x", ")", ":", "\n", "    ", "if", "tokenization_fn", "==", "None", ":", "\n", "        ", "tokenized_articles", "=", "[", "tokenize_text", "(", "text", ",", "clean_str_fn", ")", "for", "text", "in", "articles", "]", "\n", "", "else", ":", "\n", "        ", "tokenized_articles", "=", "[", "tokenization_fn", "(", "text", ")", "for", "text", "in", "articles", "]", "\n", "", "return", "tokenized_articles", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.print_vocab_tokens_stats": [[40, 48], ["print", "print", "numpy.mean", "numpy.median", "numpy.max", "sum", "float", "sum", "len", "list", "filter"], "function", ["None"], ["", "def", "print_vocab_tokens_stats", "(", "tokenized_int_texts", ",", "texts_lengths", ",", "word_vocab", ")", ":", "\n", "    ", "print", "(", "'# tokens by article stats - Mean: {:.1f}, Median: {:.1f}, Max: {:.1f}'", ".", "format", "(", "\n", "np", ".", "mean", "(", "texts_lengths", ")", ",", "np", ".", "median", "(", "texts_lengths", ")", ",", "np", ".", "max", "(", "texts_lengths", ")", ")", "\n", ")", "\n", "\n", "perc_words_found_vocab", "=", "(", "sum", "(", "[", "len", "(", "list", "(", "filter", "(", "lambda", "word", ":", "word", "!=", "word_vocab", "[", "UNK_TOKEN", "]", ",", "doc", ")", ")", ")", "for", "doc", "in", "tokenized_int_texts", "]", ")", "/", "float", "(", "sum", "(", "texts_lengths", ")", ")", ")", "*", "100", "\n", "print", "(", "'{:.2f}%  tokens were found in vocabulary.'", ".", "format", "(", "perc_words_found_vocab", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.convert_tokens_to_int": [[49, 59], ["list", "numpy.array", "tokenization.print_vocab_tokens_stats", "numpy.array", "len", "tokenization.convert_tokens_to_int.token_to_int"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.print_vocab_tokens_stats"], ["", "def", "convert_tokens_to_int", "(", "tokenized_articles", ",", "word_vocab", ")", ":", "\n", "\n", "    ", "def", "token_to_int", "(", "token", ")", ":", "\n", "        ", "return", "word_vocab", "[", "token", "]", "if", "token", "in", "word_vocab", "else", "word_vocab", "[", "UNK_TOKEN", "]", "\n", "\n", "", "texts_int", "=", "list", "(", "[", "np", ".", "array", "(", "[", "token_to_int", "(", "token", ")", "for", "token", "in", "article", "]", ")", "for", "article", "in", "tokenized_articles", "]", ")", "\n", "texts_lengths", "=", "np", ".", "array", "(", "[", "len", "(", "doc", ")", "for", "doc", "in", "texts_int", "]", ")", "\n", "print_vocab_tokens_stats", "(", "texts_int", ",", "texts_lengths", ",", "word_vocab", ")", "\n", "\n", "return", "texts_int", ",", "texts_lengths", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_gcom.create_args_parser": [[19, 38], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_articles_csv_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the news CSV file.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_label_encoders_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path for a pickle with label encoders (article_id, category_id, publisher_id).'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_word_embeddings_vocab'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path for a pickle with word embeddings and its vocab dict.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_article_content_embeddings'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_gcom.clean_str": [[74, 101], ["re_trim.sub.replace", "re_trim.sub.lower", "re_tree_dots.sub", "re.sub", "re_remove_brackets.sub", "re_changehyphen.sub", "re_remove_html.sub", "re_transform_numbers.sub", "re_transform_url.sub", "re_transform_emails.sub", "re_quotes_1.sub", "re_quotes_2.sub", "re_quotes_3.sub", "re.sub", "re_dots.sub", "re_punctuation.sub", "re_hiphen.sub", "re_punkts.sub", "re_punkts_b.sub", "re_punkts_c.sub", "re_doublequotes_1.sub", "re_doublequotes_2.sub", "re_trim.sub", "re_trim.sub.strip"], "function", ["None"], ["def", "clean_str", "(", "string", ")", ":", "\n", "    ", "string", "=", "string", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "\"\"\"Apply all regex above to a given string.\"\"\"", "\n", "string", "=", "string", ".", "lower", "(", ")", "\n", "string", "=", "re_tree_dots", ".", "sub", "(", "'...'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\\.\\.\\.'", ",", "''", ",", "string", ")", "\n", "string", "=", "re_remove_brackets", ".", "sub", "(", "''", ",", "string", ")", "\n", "string", "=", "re_changehyphen", ".", "sub", "(", "'-'", ",", "string", ")", "\n", "string", "=", "re_remove_html", ".", "sub", "(", "' '", ",", "string", ")", "\n", "string", "=", "re_transform_numbers", ".", "sub", "(", "'0'", ",", "string", ")", "\n", "string", "=", "re_transform_url", ".", "sub", "(", "'URL'", ",", "string", ")", "\n", "string", "=", "re_transform_emails", ".", "sub", "(", "'EMAIL'", ",", "string", ")", "\n", "string", "=", "re_quotes_1", ".", "sub", "(", "r'\\1\"'", ",", "string", ")", "\n", "string", "=", "re_quotes_2", ".", "sub", "(", "r'\"\\1'", ",", "string", ")", "\n", "string", "=", "re_quotes_3", ".", "sub", "(", "'\"'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\"'", ",", "''", ",", "string", ")", "\n", "string", "=", "re_dots", ".", "sub", "(", "'.'", ",", "string", ")", "\n", "string", "=", "re_punctuation", ".", "sub", "(", "r'\\1'", ",", "string", ")", "\n", "string", "=", "re_hiphen", ".", "sub", "(", "' - '", ",", "string", ")", "\n", "string", "=", "re_punkts", ".", "sub", "(", "r'\\1 \\2 \\3'", ",", "string", ")", "\n", "string", "=", "re_punkts_b", ".", "sub", "(", "r'\\1 \\2 \\3'", ",", "string", ")", "\n", "string", "=", "re_punkts_c", ".", "sub", "(", "r'\\1 \\2'", ",", "string", ")", "\n", "string", "=", "re_doublequotes_1", ".", "sub", "(", "'\\\"'", ",", "string", ")", "\n", "string", "=", "re_doublequotes_2", ".", "sub", "(", "'\\''", ",", "string", ")", "\n", "string", "=", "re_trim", ".", "sub", "(", "' '", ",", "string", ")", "\n", "\n", "return", "string", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_gcom.clean_and_filter_first_sentences": [[104, 113], ["sent_tokenizer.tokenize", "sentences.append", "sent.count", "w2v_tfidf_gcom.clean_str", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.clean_str"], ["def", "clean_and_filter_first_sentences", "(", "string", ",", "first_sentences", "=", "8", ")", ":", "\n", "# Tokenize sentences and remove short and malformed sentences.", "\n", "    ", "sentences", "=", "[", "]", "\n", "for", "sent", "in", "sent_tokenizer", ".", "tokenize", "(", "string", ")", ":", "\n", "        ", "if", "sent", ".", "count", "(", "' '", ")", ">=", "3", "and", "sent", "[", "-", "1", "]", "in", "[", "'.'", ",", "'!'", ",", "'?'", ",", "';'", "]", ":", "\n", "            ", "sentences", ".", "append", "(", "clean_str", "(", "sent", ")", ")", "\n", "if", "len", "(", "sentences", ")", "==", "first_sentences", ":", "\n", "                ", "break", "\n", "", "", "", "return", "' '", ".", "join", "(", "sentences", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_gcom.load_input_csv": [[116, 128], ["pandas.read_csv", "news_df[].apply", "news_df[].apply", "news_df[].apply"], "function", ["None"], ["", "def", "load_input_csv", "(", "path", ")", ":", "\n", "    ", "news_df", "=", "pd", ".", "read_csv", "(", "path", ",", "encoding", "=", "'utf-8'", "\n", "#,nrows=1000", "\n", ")", "\n", "\n", "#Concatenating all available text", "\n", "news_df", "[", "'full_text'", "]", "=", "(", "news_df", "[", "'title'", "]", ".", "apply", "(", "nan_to_str", ")", "+", "\". \"", "+", "news_df", "[", "'caption'", "]", ".", "apply", "(", "nan_to_str", ")", "+", "\". \"", "+", "news_df", "[", "'body'", "]", ".", "apply", "(", "nan_to_str", ")", "\n", ")", ".", "apply", "(", "clean_and_filter_first_sentences", ")", "\n", "\n", "return", "news_df", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_gcom.load_acr_preprocessing_assets": [[151, 156], ["utils.deserialize", "print", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["def", "load_acr_preprocessing_assets", "(", "acr_label_encoders_path", ")", ":", "\n", "    ", "acr_label_encoders", "=", "deserialize", "(", "acr_label_encoders_path", ")", "\n", "print", "(", "\"Read article id label encoder: {}\"", ".", "format", "(", "len", "(", "acr_label_encoders", "[", "'article_id'", "]", ".", "classes_", ")", ")", ")", "\n", "\n", "return", "acr_label_encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_gcom.export_article_content_embeddings": [[157, 163], ["print", "utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "export_article_content_embeddings", "(", "content_article_embeddings", ",", "output_article_content_embeddings", ")", ":", "\n", "    ", "output_path", "=", "output_article_content_embeddings", "\n", "print", "(", "'Exporting ACR Label Encoders, Article metadata and embeddings to {}'", ".", "format", "(", "output_path", ")", ")", "\n", "#to_serialize = (acr_label_encoders, articles_metadata_df, content_article_embeddings)", "\n", "to_serialize", "=", "content_article_embeddings", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_gcom.main": [[165, 258], ["w2v_tfidf_gcom.create_args_parser", "create_args_parser.parse_args", "print", "w2v_tfidf_gcom.load_input_csv", "print", "print", "w2v_tfidf_gcom.load_acr_preprocessing_assets", "acr_label_encoders[].transform", "print", "utils.deserialize", "news_df.sort_values.sort_values", "print", "tokenization.tokenize_articles", "print", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "print", "numpy.array", "print", "tqdm.tqdm", "print", "print", "print", "numpy.vstack", "sklearn.decomposition.PCA", "sklearn.decomposition.PCA.fit_transform", "print", "print", "w2v_tfidf_gcom.export_article_content_embeddings", "len", "len", "len", "len", "sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names", "enumerate", "zip", "content_embeddings.append", "len", "news_df[].max", "vect_content.nonzero", "vect_content[].todense().tolist", "len", "print", "invalid_embeddings.append", "len", "words.append", "weights.append", "numpy.vstack().sum", "sum", "pandas.isnull", "vect_content[].todense", "numpy.vstack"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.load_input_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.load_acr_preprocessing_assets", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_articles", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.export_article_content_embeddings"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "create_args_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "print", "(", "'Loading news article CSV: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "news_df", "=", "load_input_csv", "(", "args", ".", "input_articles_csv_path", ")", "\n", "print", "(", "'N. docs: {}'", ".", "format", "(", "len", "(", "news_df", ")", ")", ")", "\n", "\n", "print", "(", "'ACR label encoder: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "acr_label_encoders", "=", "load_acr_preprocessing_assets", "(", "args", ".", "input_label_encoders_path", ")", "\n", "news_df", "[", "'id_encoded'", "]", "=", "acr_label_encoders", "[", "'article_id'", "]", ".", "transform", "(", "news_df", "[", "'id'", "]", ")", "\n", "\n", "\n", "print", "(", "'Loading word embeddings'", ")", "\n", "(", "word_vocab", ",", "word_embeddings_matrix", ")", "=", "deserialize", "(", "args", ".", "input_word_embeddings_vocab", ")", "\n", "\n", "\n", "#Sorting results by the encoded article Id, so that the matrix coincides and checking consistency", "\n", "news_df", "=", "news_df", ".", "sort_values", "(", "'id_encoded'", ")", "\n", "\n", "\n", "\n", "assert", "len", "(", "news_df", ")", "==", "len", "(", "acr_label_encoders", "[", "'article_id'", "]", ".", "classes_", ")", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "values", "[", "0", "]", "==", "0", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "max", "(", ")", "+", "1", "==", "len", "(", "news_df", ")", "\n", "assert", "len", "(", "news_df", "[", "pd", ".", "isnull", "(", "news_df", "[", "'id_encoded'", "]", ")", "]", ")", "==", "0", "\n", "\n", "del", "acr_label_encoders", "\n", "\n", "\n", "print", "(", "'Tokenizing articles...'", ")", "\n", "tokenized_articles", "=", "tokenize_articles", "(", "news_df", "[", "'full_text'", "]", ")", "\n", "del", "news_df", "\n", "\n", "\n", "print", "(", "'TF-IDF...'", ")", "\n", "#print('TF-IDF...')", "\n", "#https://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/", "\n", "vectorizer", "=", "TfidfVectorizer", "(", "analyzer", "=", "'word'", ",", "\n", "tokenizer", "=", "lambda", "x", ":", "x", ",", "\n", "preprocessor", "=", "lambda", "x", ":", "x", ",", "\n", "token_pattern", "=", "None", ",", "\n", "stop_words", "=", "None", ",", "\n", "ngram_range", "=", "(", "1", ",", "1", ")", ",", "max_df", "=", "0.4", ",", "\n", "min_df", "=", "2", ",", "max_features", "=", "50000", ",", "\n", "norm", "=", "'l2'", ",", "use_idf", "=", "True", ",", "\n", "smooth_idf", "=", "True", ",", "sublinear_tf", "=", "False", ")", "\n", "\n", "vectorized_contents", "=", "vectorizer", ".", "fit_transform", "(", "tokenized_articles", ")", "\n", "print", "(", "'vectorized_content.shape={}'", ".", "format", "(", "vectorized_contents", ".", "shape", ")", ")", "\n", "feature_names", "=", "np", ".", "array", "(", "vectorizer", ".", "get_feature_names", "(", ")", ")", "\n", "\n", "print", "(", "'Averaging word embeddings weighted by TF-IDF score'", ")", "\n", "\n", "invalid_embeddings", "=", "[", "]", "\n", "content_embeddings", "=", "[", "]", "\n", "for", "article_idx", ",", "vect_content", "in", "tqdm", "(", "enumerate", "(", "vectorized_contents", ")", ")", ":", "\n", "        ", "word_idxs", "=", "vect_content", ".", "nonzero", "(", ")", "[", "1", "]", "\n", "word_weights", "=", "vect_content", "[", "0", ",", "word_idxs", "]", ".", "todense", "(", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "word_names", "=", "feature_names", "[", "word_idxs", "]", "\n", "\n", "words", "=", "[", "]", "\n", "weights", "=", "[", "]", "\n", "for", "word", ",", "weight", "in", "zip", "(", "word_names", ",", "word_weights", ")", ":", "\n", "            ", "if", "word", "in", "word_vocab", ":", "\n", "                ", "words", ".", "append", "(", "word_embeddings_matrix", "[", "word_vocab", "[", "word", "]", "]", "*", "weight", ")", "\n", "weights", ".", "append", "(", "weight", ")", "\n", "#else:", "\n", "#    print('Word not found: {}'.format(word))", "\n", "\n", "", "", "if", "len", "(", "words", ")", ">", "0", ":", "\n", "            ", "avg_word_embedding", "=", "np", ".", "vstack", "(", "words", ")", ".", "sum", "(", "axis", "=", "0", ")", "/", "sum", "(", "weights", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'No valid words for article_idx: {}'", ".", "format", "(", "article_idx", ")", ")", "\n", "invalid_embeddings", ".", "append", "(", "article_idx", ")", "\n", "avg_word_embedding", "=", "word_embeddings_matrix", "[", "1", "]", "#Using the <UNK> token", "\n", "\n", "\n", "", "content_embeddings", ".", "append", "(", "avg_word_embedding", ")", "\n", "\n", "", "print", "(", "'Total invalid embeddings: {}'", ".", "format", "(", "len", "(", "invalid_embeddings", ")", ")", ")", "\n", "print", "(", "'Invalid embeddings: {}'", ".", "format", "(", "invalid_embeddings", ")", ")", "\n", "\n", "print", "(", "'Concatenating article content embeddings'", ")", "\n", "content_embeddings_concat", "=", "np", ".", "vstack", "(", "content_embeddings", ")", "\n", "\n", "pca", "=", "PCA", "(", "n_components", "=", "250", ")", "\n", "article_content_embeddings", "=", "pca", ".", "fit_transform", "(", "content_embeddings_concat", ")", "\n", "\n", "print", "(", "'article_content_embeddings.shape={}'", ".", "format", "(", "article_content_embeddings", ".", "shape", ")", ")", "\n", "\n", "print", "(", "'Exporting article content embeddings'", ")", "\n", "export_article_content_embeddings", "(", "article_content_embeddings", ",", "args", ".", "output_article_content_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_adressa.create_args_parser": [[20, 35], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_articles_csv_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the news CSV file.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_word_embeddings_vocab'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path for a pickle with word embeddings and its vocab dict.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_article_content_embeddings'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_adressa.load_input_csv": [[38, 45], ["pandas.read_csv", "pd.read_csv.sort_values"], "function", ["None"], ["def", "load_input_csv", "(", "path", ")", ":", "\n", "    ", "news_df", "=", "pd", ".", "read_csv", "(", "path", ",", "encoding", "=", "'utf-8'", "\n", "#,nrows=1000", "\n", ")", "\n", "#Making sure articles are sorted by there encoded id", "\n", "news_df", ".", "sort_values", "(", "'id_encoded'", ",", "inplace", "=", "True", ")", "\n", "return", "news_df", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_adressa.tokenize_norwegian_article": [[47, 61], ["text.replace.replace", "nltk.tokenize.sent_tokenize", "nltk.tokenize.word_tokenize", "words_tokenized.extend", "len"], "function", ["None"], ["", "def", "tokenize_norwegian_article", "(", "text", ",", "first_sentences", "=", "12", ",", "max_words_length", "=", "1000", ")", ":", "\n", "#Removing pipes for correct sentence tokenization", "\n", "    ", "text", "=", "text", ".", "replace", "(", "'|'", ",", "'.'", ")", "\n", "words_tokenized", "=", "[", "]", "\n", "sent_count", "=", "0", "\n", "for", "sentence", "in", "nltk", ".", "tokenize", ".", "sent_tokenize", "(", "text", ",", "language", "=", "'norwegian'", ")", ":", "\n", "        ", "sent_tokenized", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "sentence", ",", "language", "=", "'norwegian'", ")", "\n", "if", "len", "(", "sent_tokenized", ")", ">=", "3", "and", "sent_tokenized", "[", "-", "1", "]", "in", "[", "'.'", ",", "'!'", ",", "'?'", ",", "';'", "]", "and", "sent_tokenized", "!=", "[", "'Saken'", ",", "'oppdateres'", ",", "'.'", "]", ":", "\n", "            ", "sent_count", "+=", "1", "\n", "words_tokenized", ".", "extend", "(", "sent_tokenized", ")", "\n", "if", "sent_count", "==", "first_sentences", ":", "\n", "                ", "break", "\n", "", "", "", "return", "words_tokenized", "[", ":", "max_words_length", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_adressa.export_article_content_embeddings": [[63, 69], ["print", "utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "export_article_content_embeddings", "(", "content_article_embeddings", ",", "output_article_content_embeddings", ")", ":", "\n", "    ", "output_path", "=", "output_article_content_embeddings", "\n", "print", "(", "'Exporting ACR Label Encoders, Article metadata and embeddings to {}'", ".", "format", "(", "output_path", ")", ")", "\n", "#to_serialize = (acr_label_encoders, articles_metadata_df, content_article_embeddings)", "\n", "to_serialize", "=", "content_article_embeddings", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.w2v_tfidf_adressa.main": [[71, 168], ["w2v_tfidf_adressa.create_args_parser", "create_args_parser.parse_args", "print", "w2v_tfidf_adressa.load_input_csv", "print", "print", "utils.deserialize", "print", "print", "tokenization.tokenize_articles", "print", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "print", "numpy.array", "print", "tqdm.tqdm", "print", "print", "print", "numpy.vstack", "print", "numpy.hstack", "print", "sklearn.decomposition.PCA", "sklearn.decomposition.PCA.fit_transform", "print", "print", "numpy.mean", "numpy.vstack", "print", "w2v_tfidf_adressa.export_article_content_embeddings", "news_df[].max", "len", "len", "sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names", "enumerate", "zip", "content_embeddings.append", "len", "vect_content.nonzero", "vect_content[].todense().tolist", "len", "print", "invalid_embeddings.append", "len", "words.append", "weights.append", "numpy.vstack().sum", "sum", "news_df[].tail", "pandas.isnull", "vect_content[].todense", "numpy.vstack"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.load_input_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_articles", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.export_article_content_embeddings"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "create_args_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "print", "(", "'Loading news article CSV: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "news_df", "=", "load_input_csv", "(", "args", ".", "input_articles_csv_path", ")", "\n", "print", "(", "'N. docs: {}'", ".", "format", "(", "len", "(", "news_df", ")", ")", ")", "\n", "\n", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "values", "[", "0", "]", "==", "1", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "max", "(", ")", "==", "len", "(", "news_df", ")", "\n", "assert", "len", "(", "news_df", "[", "pd", ".", "isnull", "(", "news_df", "[", "'id_encoded'", "]", ")", "]", ")", "==", "0", "\n", "\n", "\n", "print", "(", "'Loading word embeddings'", ")", "\n", "(", "word_vocab", ",", "word_embeddings_matrix", ")", "=", "deserialize", "(", "args", ".", "input_word_embeddings_vocab", ")", "\n", "print", "(", "'word_embeddings_matrix'", ",", "word_embeddings_matrix", ".", "shape", ")", "\n", "\n", "\n", "\n", "print", "(", "'Tokenizing articles...'", ")", "\n", "tokenized_articles", "=", "tokenize_articles", "(", "news_df", "[", "'text_highlights'", "]", ".", "values", ",", "tokenization_fn", "=", "tokenize_norwegian_article", ")", "\n", "\n", "\n", "print", "(", "'TF-IDF...'", ")", "\n", "vectorizer", "=", "TfidfVectorizer", "(", "analyzer", "=", "'word'", ",", "\n", "tokenizer", "=", "lambda", "x", ":", "x", ",", "\n", "preprocessor", "=", "lambda", "x", ":", "x", ",", "\n", "token_pattern", "=", "None", ",", "\n", "stop_words", "=", "None", ",", "\n", "ngram_range", "=", "(", "1", ",", "1", ")", ",", "max_df", "=", "0.4", ",", "\n", "min_df", "=", "2", ",", "max_features", "=", "50000", ",", "\n", "norm", "=", "'l2'", ",", "use_idf", "=", "True", ",", "\n", "smooth_idf", "=", "True", ",", "sublinear_tf", "=", "False", ")", "\n", "\n", "vectorized_contents", "=", "vectorizer", ".", "fit_transform", "(", "tokenized_articles", ")", "\n", "print", "(", "'vectorized_content.shape={}'", ".", "format", "(", "vectorized_contents", ".", "shape", ")", ")", "\n", "feature_names", "=", "np", ".", "array", "(", "vectorizer", ".", "get_feature_names", "(", ")", ")", "\n", "\n", "print", "(", "'Averaging word embeddings weighted by TF-IDF score'", ")", "\n", "invalid_embeddings", "=", "[", "]", "\n", "content_embeddings", "=", "[", "]", "\n", "for", "article_idx", ",", "vect_content", "in", "tqdm", "(", "enumerate", "(", "vectorized_contents", ")", ")", ":", "\n", "        ", "word_idxs", "=", "vect_content", ".", "nonzero", "(", ")", "[", "1", "]", "\n", "word_weights", "=", "vect_content", "[", "0", ",", "word_idxs", "]", ".", "todense", "(", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "word_names", "=", "feature_names", "[", "word_idxs", "]", "\n", "\n", "words", "=", "[", "]", "\n", "weights", "=", "[", "]", "\n", "for", "word", ",", "weight", "in", "zip", "(", "word_names", ",", "word_weights", ")", ":", "\n", "            ", "if", "word", "in", "word_vocab", ":", "\n", "                ", "words", ".", "append", "(", "word_embeddings_matrix", "[", "word_vocab", "[", "word", "]", "]", "*", "weight", ")", "\n", "weights", ".", "append", "(", "weight", ")", "\n", "#else:", "\n", "#    print('Word not found: {}'.format(word))", "\n", "\n", "", "", "if", "len", "(", "words", ")", ">", "0", ":", "\n", "            ", "avg_word_embedding", "=", "np", ".", "vstack", "(", "words", ")", ".", "sum", "(", "axis", "=", "0", ")", "/", "sum", "(", "weights", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'No valid words for article_idx: {}'", ".", "format", "(", "article_idx", ")", ")", "\n", "invalid_embeddings", ".", "append", "(", "article_idx", ")", "\n", "avg_word_embedding", "=", "word_embeddings_matrix", "[", "1", "]", "#Using the <UNK> token", "\n", "\n", "\n", "", "content_embeddings", ".", "append", "(", "avg_word_embedding", ")", "\n", "\n", "", "print", "(", "'Total invalid embeddings: {}'", ".", "format", "(", "len", "(", "invalid_embeddings", ")", ")", ")", "\n", "print", "(", "'Invalid embeddings: {}'", ".", "format", "(", "invalid_embeddings", ")", ")", "\n", "\n", "print", "(", "'Concatenating article content embeddings'", ")", "\n", "content_embeddings_concat", "=", "np", ".", "vstack", "(", "content_embeddings", ")", "\n", "print", "(", "'content_embeddings_concat'", ",", "content_embeddings_concat", ".", "shape", ")", "\n", "\n", "#As word embeddings for Adressa have 100-dim, concatenating 3 times to get a \"300-dim\" embedding, to be able to reduce to 250", "\n", "content_embeddings_hconcat", "=", "np", ".", "hstack", "(", "[", "content_embeddings_concat", ",", "content_embeddings_concat", ",", "content_embeddings_concat", "]", ")", "\n", "print", "(", "'content_embeddings_hconcat'", ",", "content_embeddings_hconcat", ".", "shape", ")", "\n", "\n", "pca", "=", "PCA", "(", "n_components", "=", "250", ")", "\n", "article_content_embeddings", "=", "pca", ".", "fit_transform", "(", "content_embeddings_hconcat", ")", "\n", "\n", "print", "(", "'article_content_embeddings.shape={}'", ".", "format", "(", "article_content_embeddings", ".", "shape", ")", ")", "\n", "\n", "#Checking if content articles embedding size correspond to the last article_id", "\n", "assert", "article_content_embeddings", ".", "shape", "[", "0", "]", "==", "news_df", "[", "'id_encoded'", "]", ".", "tail", "(", "1", ")", ".", "values", "[", "0", "]", "\n", "\n", "\n", "\n", "print", "(", "'Concatenating article content embeddings, making sure that they are sorted by the encoded article id'", ")", "\n", "embedding_for_padding_article", "=", "np", ".", "mean", "(", "article_content_embeddings", ",", "axis", "=", "0", ")", "\n", "content_article_embeddings_with_padding", "=", "np", ".", "vstack", "(", "[", "embedding_for_padding_article", ",", "article_content_embeddings", "]", ")", "\n", "del", "article_content_embeddings", "\n", "\n", "\n", "\n", "print", "(", "'Exporting article content embeddings'", ")", "\n", "del", "news_df", "\n", "export_article_content_embeddings", "(", "content_article_embeddings_with_padding", ",", "args", ".", "output_article_content_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_gcom.create_args_parser": [[20, 35], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_articles_csv_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the news CSV file.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_label_encoders_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path for a pickle with label encoders (article_id, category_id, publisher_id).'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_article_content_embeddings'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_gcom.clean_str": [[71, 98], ["re_trim.sub.replace", "re_trim.sub.lower", "re_tree_dots.sub", "re.sub", "re_remove_brackets.sub", "re_changehyphen.sub", "re_remove_html.sub", "re_transform_numbers.sub", "re_transform_url.sub", "re_transform_emails.sub", "re_quotes_1.sub", "re_quotes_2.sub", "re_quotes_3.sub", "re.sub", "re_dots.sub", "re_punctuation.sub", "re_hiphen.sub", "re_punkts.sub", "re_punkts_b.sub", "re_punkts_c.sub", "re_doublequotes_1.sub", "re_doublequotes_2.sub", "re_trim.sub", "re_trim.sub.strip"], "function", ["None"], ["def", "clean_str", "(", "string", ")", ":", "\n", "    ", "string", "=", "string", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "\"\"\"Apply all regex above to a given string.\"\"\"", "\n", "string", "=", "string", ".", "lower", "(", ")", "\n", "string", "=", "re_tree_dots", ".", "sub", "(", "'...'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\\.\\.\\.'", ",", "''", ",", "string", ")", "\n", "string", "=", "re_remove_brackets", ".", "sub", "(", "''", ",", "string", ")", "\n", "string", "=", "re_changehyphen", ".", "sub", "(", "'-'", ",", "string", ")", "\n", "string", "=", "re_remove_html", ".", "sub", "(", "' '", ",", "string", ")", "\n", "string", "=", "re_transform_numbers", ".", "sub", "(", "'0'", ",", "string", ")", "\n", "string", "=", "re_transform_url", ".", "sub", "(", "'URL'", ",", "string", ")", "\n", "string", "=", "re_transform_emails", ".", "sub", "(", "'EMAIL'", ",", "string", ")", "\n", "string", "=", "re_quotes_1", ".", "sub", "(", "r'\\1\"'", ",", "string", ")", "\n", "string", "=", "re_quotes_2", ".", "sub", "(", "r'\"\\1'", ",", "string", ")", "\n", "string", "=", "re_quotes_3", ".", "sub", "(", "'\"'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\"'", ",", "''", ",", "string", ")", "\n", "string", "=", "re_dots", ".", "sub", "(", "'.'", ",", "string", ")", "\n", "string", "=", "re_punctuation", ".", "sub", "(", "r'\\1'", ",", "string", ")", "\n", "string", "=", "re_hiphen", ".", "sub", "(", "' - '", ",", "string", ")", "\n", "string", "=", "re_punkts", ".", "sub", "(", "r'\\1 \\2 \\3'", ",", "string", ")", "\n", "string", "=", "re_punkts_b", ".", "sub", "(", "r'\\1 \\2 \\3'", ",", "string", ")", "\n", "string", "=", "re_punkts_c", ".", "sub", "(", "r'\\1 \\2'", ",", "string", ")", "\n", "string", "=", "re_doublequotes_1", ".", "sub", "(", "'\\\"'", ",", "string", ")", "\n", "string", "=", "re_doublequotes_2", ".", "sub", "(", "'\\''", ",", "string", ")", "\n", "string", "=", "re_trim", ".", "sub", "(", "' '", ",", "string", ")", "\n", "\n", "return", "string", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_gcom.clean_and_filter_first_sentences": [[101, 110], ["sent_tokenizer.tokenize", "sentences.append", "sent.count", "lsa_gcom.clean_str", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.clean_str"], ["def", "clean_and_filter_first_sentences", "(", "string", ",", "first_sentences", "=", "8", ")", ":", "\n", "# Tokenize sentences and remove short and malformed sentences.", "\n", "    ", "sentences", "=", "[", "]", "\n", "for", "sent", "in", "sent_tokenizer", ".", "tokenize", "(", "string", ")", ":", "\n", "        ", "if", "sent", ".", "count", "(", "' '", ")", ">=", "3", "and", "sent", "[", "-", "1", "]", "in", "[", "'.'", ",", "'!'", ",", "'?'", ",", "';'", "]", ":", "\n", "            ", "sentences", ".", "append", "(", "clean_str", "(", "sent", ")", ")", "\n", "if", "len", "(", "sentences", ")", "==", "first_sentences", ":", "\n", "                ", "break", "\n", "", "", "", "return", "' '", ".", "join", "(", "sentences", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_gcom.load_input_csv": [[113, 125], ["pandas.read_csv", "news_df[].apply", "news_df[].apply", "news_df[].apply"], "function", ["None"], ["", "def", "load_input_csv", "(", "path", ")", ":", "\n", "    ", "news_df", "=", "pd", ".", "read_csv", "(", "path", ",", "encoding", "=", "'utf-8'", "\n", "#,nrows=1000", "\n", ")", "\n", "\n", "#Concatenating all available text", "\n", "news_df", "[", "'full_text'", "]", "=", "(", "news_df", "[", "'title'", "]", ".", "apply", "(", "nan_to_str", ")", "+", "\". \"", "+", "news_df", "[", "'caption'", "]", ".", "apply", "(", "nan_to_str", ")", "+", "\". \"", "+", "news_df", "[", "'body'", "]", ".", "apply", "(", "nan_to_str", ")", "\n", ")", ".", "apply", "(", "clean_and_filter_first_sentences", ")", "\n", "\n", "return", "news_df", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_gcom.load_acr_preprocessing_assets": [[127, 132], ["utils.deserialize", "print", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["", "def", "load_acr_preprocessing_assets", "(", "acr_label_encoders_path", ")", ":", "\n", "    ", "acr_label_encoders", "=", "deserialize", "(", "acr_label_encoders_path", ")", "\n", "print", "(", "\"Read article id label encoder: {}\"", ".", "format", "(", "len", "(", "acr_label_encoders", "[", "'article_id'", "]", ".", "classes_", ")", ")", ")", "\n", "\n", "return", "acr_label_encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_gcom.export_article_content_embeddings": [[133, 139], ["print", "utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "export_article_content_embeddings", "(", "content_article_embeddings", ",", "output_article_content_embeddings", ")", ":", "\n", "    ", "output_path", "=", "output_article_content_embeddings", "\n", "print", "(", "'Exporting ACR Label Encoders, Article metadata and embeddings to {}'", ".", "format", "(", "output_path", ")", ")", "\n", "#to_serialize = (acr_label_encoders, articles_metadata_df, content_article_embeddings)", "\n", "to_serialize", "=", "content_article_embeddings", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.lsa_gcom.main": [[141, 195], ["lsa_gcom.create_args_parser", "create_args_parser.parse_args", "print", "lsa_gcom.load_input_csv", "print", "print", "lsa_gcom.load_acr_preprocessing_assets", "acr_label_encoders[].transform", "news_df.sort_values.sort_values", "print", "tokenization.tokenize_articles", "print", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.decomposition.TruncatedSVD", "sklearn.pipeline.make_pipeline", "sklearn.pipeline.make_pipeline.fit_transform", "print", "numpy.vstack", "print", "lsa_gcom.export_article_content_embeddings", "len", "len", "len", "len", "sklearn.preprocessing.Normalizer", "len", "news_df[].max", "pandas.isnull"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.load_input_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.load_acr_preprocessing_assets", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_articles", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.export_article_content_embeddings"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "create_args_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "print", "(", "'Loading news article CSV: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "news_df", "=", "load_input_csv", "(", "args", ".", "input_articles_csv_path", ")", "\n", "print", "(", "'N. docs: {}'", ".", "format", "(", "len", "(", "news_df", ")", ")", ")", "\n", "\n", "print", "(", "'ACR label encoder: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "acr_label_encoders", "=", "load_acr_preprocessing_assets", "(", "args", ".", "input_label_encoders_path", ")", "\n", "news_df", "[", "'id_encoded'", "]", "=", "acr_label_encoders", "[", "'article_id'", "]", ".", "transform", "(", "news_df", "[", "'id'", "]", ")", "\n", "\n", "#Sorting results by the encoded article Id, so that the matrix coincides and checking consistency", "\n", "news_df", "=", "news_df", ".", "sort_values", "(", "'id_encoded'", ")", "\n", "\n", "assert", "len", "(", "news_df", ")", "==", "len", "(", "acr_label_encoders", "[", "'article_id'", "]", ".", "classes_", ")", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "values", "[", "0", "]", "==", "0", "\n", "assert", "news_df", "[", "'id_encoded'", "]", ".", "max", "(", ")", "+", "1", "==", "len", "(", "news_df", ")", "\n", "assert", "len", "(", "news_df", "[", "pd", ".", "isnull", "(", "news_df", "[", "'id_encoded'", "]", ")", "]", ")", "==", "0", "\n", "del", "acr_label_encoders", "\n", "\n", "\n", "\n", "print", "(", "'Tokenizing articles...'", ")", "\n", "tokenized_articles", "=", "tokenize_articles", "(", "news_df", "[", "'full_text'", "]", ")", "\n", "del", "news_df", "\n", "\n", "\n", "print", "(", "'TF-IDF + SVD...'", ")", "\n", "#print('TF-IDF...')", "\n", "#https://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/", "\n", "vectorizer", "=", "TfidfVectorizer", "(", "analyzer", "=", "'word'", ",", "\n", "tokenizer", "=", "lambda", "x", ":", "x", ",", "\n", "preprocessor", "=", "lambda", "x", ":", "x", ",", "\n", "token_pattern", "=", "None", ",", "\n", "stop_words", "=", "None", ",", "\n", "ngram_range", "=", "(", "1", ",", "3", ")", ",", "max_df", "=", "0.4", ",", "\n", "min_df", "=", "2", ",", "max_features", "=", "50000", ",", "\n", "norm", "=", "'l2'", ",", "use_idf", "=", "True", ",", "\n", "smooth_idf", "=", "True", ",", "sublinear_tf", "=", "False", ")", "\n", "\n", "\n", "svd", "=", "TruncatedSVD", "(", "n_components", "=", "VECTOR_SIZE", ")", "\n", "\n", "lsa", "=", "make_pipeline", "(", "vectorizer", ",", "svd", ",", "Normalizer", "(", "copy", "=", "False", ")", ")", "\n", "\n", "reduced_content", "=", "lsa", ".", "fit_transform", "(", "tokenized_articles", ")", "\n", "\n", "\n", "print", "(", "'Concatenating article content embeddings'", ")", "\n", "article_content_embeddings", "=", "np", ".", "vstack", "(", "reduced_content", ")", "\n", "\n", "print", "(", "'Exporting article content embeddings'", ")", "\n", "export_article_content_embeddings", "(", "article_content_embeddings", ",", "args", ".", "output_article_content_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.create_args_parser": [[16, 27], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_articles_csv_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the news CSV file.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_article_content_embeddings'", ",", "default", "=", "''", ",", "\n", "help", "=", "''", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.load_input_csv": [[28, 35], ["pandas.read_csv", "pd.read_csv.sort_values"], "function", ["None"], ["", "def", "load_input_csv", "(", "path", ")", ":", "\n", "    ", "news_df", "=", "pd", ".", "read_csv", "(", "path", ",", "encoding", "=", "'utf-8'", "\n", "#,nrows=1000", "\n", ")", "\n", "#Making sure articles are sorted by there encoded id", "\n", "news_df", ".", "sort_values", "(", "'id_encoded'", ",", "inplace", "=", "True", ")", "\n", "return", "news_df", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.tokenize_norwegian_article": [[58, 72], ["text.replace.replace", "nltk.tokenize.sent_tokenize", "nltk.tokenize.word_tokenize", "words_tokenized.extend", "len"], "function", ["None"], ["def", "tokenize_norwegian_article", "(", "text", ",", "first_sentences", "=", "12", ",", "max_words_length", "=", "1000", ")", ":", "\n", "#Removing pipes for correct sentence tokenization", "\n", "    ", "text", "=", "text", ".", "replace", "(", "'|'", ",", "'.'", ")", "\n", "words_tokenized", "=", "[", "]", "\n", "sent_count", "=", "0", "\n", "for", "sentence", "in", "nltk", ".", "tokenize", ".", "sent_tokenize", "(", "text", ",", "language", "=", "'norwegian'", ")", ":", "\n", "        ", "sent_tokenized", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "sentence", ",", "language", "=", "'norwegian'", ")", "\n", "if", "len", "(", "sent_tokenized", ")", ">=", "3", "and", "sent_tokenized", "[", "-", "1", "]", "in", "[", "'.'", ",", "'!'", ",", "'?'", ",", "';'", "]", "and", "sent_tokenized", "!=", "[", "'Saken'", ",", "'oppdateres'", ",", "'.'", "]", ":", "\n", "            ", "sent_count", "+=", "1", "\n", "words_tokenized", ".", "extend", "(", "sent_tokenized", ")", "\n", "if", "sent_count", "==", "first_sentences", ":", "\n", "                ", "break", "\n", "", "", "", "return", "words_tokenized", "[", ":", "max_words_length", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.export_article_content_embeddings": [[74, 80], ["print", "utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "export_article_content_embeddings", "(", "content_article_embeddings", ",", "output_article_content_embeddings", ")", ":", "\n", "    ", "output_path", "=", "output_article_content_embeddings", "\n", "print", "(", "'Exporting ACR Label Encoders, Article metadata and embeddings to {}'", ".", "format", "(", "output_path", ")", ")", "\n", "#to_serialize = (acr_label_encoders, articles_metadata_df, content_article_embeddings)", "\n", "to_serialize", "=", "content_article_embeddings", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.main": [[82, 156], ["doc2vec_adressa.create_args_parser", "create_args_parser.parse_args", "print", "doc2vec_adressa.load_input_csv", "print", "print", "tokenization.tokenize_articles", "print", "print", "gensim.models.doc2vec.Doc2Vec", "gensim.models.doc2vec.Doc2Vec.build_vocab", "range", "print", "numpy.vstack", "numpy.mean", "numpy.vstack", "print", "doc2vec_adressa.export_article_content_embeddings", "gensim.models.doc2vec.TaggedDocument", "print", "gensim.models.doc2vec.Doc2Vec.train", "len", "enumerate", "news_df[].tail"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.load_input_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_articles", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.doc2vec_adressa.export_article_content_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.train"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "create_args_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "print", "(", "'Loading news article CSV: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "news_df", "=", "load_input_csv", "(", "args", ".", "input_articles_csv_path", ")", "\n", "print", "(", "'N. docs: {}'", ".", "format", "(", "len", "(", "news_df", ")", ")", ")", "\n", "\n", "'''\n    print('Encoding categorical features')\n    article_id_encoder, category_id_encoder, domainid_encoder = process_cat_features(news_df)\n    print('Exporting LabelEncoders of categorical features: {}'.format(args.output_label_encoders))\n    save_article_cat_encoders(args.output_label_encoders, \n                              article_id_encoder, \n                              category_id_encoder, \n                              domainid_encoder)\n    '''", "\n", "\n", "print", "(", "'Tokenizing articles...'", ")", "\n", "tokenized_articles", "=", "tokenize_articles", "(", "news_df", "[", "'text_highlights'", "]", ".", "values", ",", "tokenization_fn", "=", "tokenize_norwegian_article", ")", "\n", "\n", "#print('Computing word frequencies...')", "\n", "#words_freq = get_words_freq(tokenized_articles)", "\n", "#print('Corpus vocabulary size: {}'.format(len(words_freq)))", "\n", "\n", "print", "(", "'Processing documents...'", ")", "\n", "tagged_data", "=", "[", "TaggedDocument", "(", "words", "=", "w", ",", "tags", "=", "[", "i", "]", ")", "for", "i", ",", "w", "in", "enumerate", "(", "tokenized_articles", ")", "]", "\n", "\n", "\n", "print", "(", "'Training doc2vec'", ")", "\n", "max_epochs", "=", "30", "\n", "vec_size", "=", "250", "\n", "alpha", "=", "0.025", "\n", "model", "=", "Doc2Vec", "(", "vector_size", "=", "vec_size", ",", "\n", "alpha", "=", "alpha", ",", "\n", "min_alpha", "=", "alpha", ",", "\n", "window", "=", "5", ",", "\n", "negative", "=", "5", ",", "\n", "min_count", "=", "2", ",", "\n", "max_vocab_size", "=", "100000", ",", "\n", "dm", "=", "1", ",", "\n", "dm_mean", "=", "1", ",", "\n", "workers", "=", "6", ")", "\n", "\n", "model", ".", "build_vocab", "(", "tagged_data", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "max_epochs", ")", ":", "\n", "        ", "print", "(", "'iteration {0}'", ".", "format", "(", "epoch", ")", ")", "\n", "model", ".", "train", "(", "tagged_data", ",", "\n", "total_examples", "=", "model", ".", "corpus_count", ",", "\n", "epochs", "=", "1", ")", "#model.iter)", "\n", "# decrease the learning rate", "\n", "model", ".", "alpha", "-=", "0.0002", "\n", "# fix the learning rate, no decay", "\n", "model", ".", "min_alpha", "=", "model", ".", "alpha", "\n", "\n", "", "del", "tokenized_articles", "\n", "\n", "\n", "#print('Encoding categorical features')", "\n", "#article_id_encoder = process_cat_features(news_df)", "\n", "\n", "print", "(", "'Concatenating article content embeddings, making sure that they are sorted by the encoded article id'", ")", "\n", "article_content_embeddings", "=", "np", ".", "vstack", "(", "[", "model", ".", "docvecs", "[", "i", "-", "1", "]", "for", "i", "in", "news_df", "[", "'id_encoded'", "]", ".", "values", "]", ")", "\n", "embedding_for_padding_article", "=", "np", ".", "mean", "(", "article_content_embeddings", ",", "axis", "=", "0", ")", "\n", "content_article_embeddings_with_padding", "=", "np", ".", "vstack", "(", "[", "embedding_for_padding_article", ",", "article_content_embeddings", "]", ")", "\n", "del", "article_content_embeddings", "\n", "\n", "#Checking if content articles embedding size correspond to the last article_id", "\n", "assert", "content_article_embeddings_with_padding", ".", "shape", "[", "0", "]", "==", "news_df", "[", "'id_encoded'", "]", ".", "tail", "(", "1", ")", ".", "values", "[", "0", "]", "+", "1", "\n", "\n", "print", "(", "'Exporting article content embeddings'", ")", "\n", "del", "news_df", "\n", "export_article_content_embeddings", "(", "content_article_embeddings_with_padding", ",", "args", ".", "output_article_content_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser": [[15, 47], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "create_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_articles_csv_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the news CSV file.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--input_word_embeddings_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Input path of the word2vec embeddings model (word2vec).'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_tf_records_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for generated TFRecords with news content.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_word_vocab_embeddings_path'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for a pickle with words vocabulary and corresponding word embeddings.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--output_label_encoders'", ",", "default", "=", "''", ",", "\n", "help", "=", "'Output path for a pickle with label encoders (article_id, category_id, publisher_id).'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--articles_by_tfrecord'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "\n", "help", "=", "'Number of articles to be exported in each TFRecords file'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'--vocab_most_freq_words'", ",", "type", "=", "int", ",", "default", "=", "100000", ",", "\n", "help", "=", "'Most frequent words to keep in vocab'", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.clean_str": [[81, 108], ["re_trim.sub.replace", "re_trim.sub.lower", "re_tree_dots.sub", "re.sub", "re_remove_brackets.sub", "re_changehyphen.sub", "re_remove_html.sub", "re_transform_numbers.sub", "re_transform_url.sub", "re_transform_emails.sub", "re_quotes_1.sub", "re_quotes_2.sub", "re_quotes_3.sub", "re.sub", "re_dots.sub", "re_punctuation.sub", "re_hiphen.sub", "re_punkts.sub", "re_punkts_b.sub", "re_punkts_c.sub", "re_doublequotes_1.sub", "re_doublequotes_2.sub", "re_trim.sub", "re_trim.sub.strip"], "function", ["None"], ["def", "clean_str", "(", "string", ")", ":", "\n", "    ", "string", "=", "string", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "\"\"\"Apply all regex above to a given string.\"\"\"", "\n", "string", "=", "string", ".", "lower", "(", ")", "\n", "string", "=", "re_tree_dots", ".", "sub", "(", "'...'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\\.\\.\\.'", ",", "''", ",", "string", ")", "\n", "string", "=", "re_remove_brackets", ".", "sub", "(", "''", ",", "string", ")", "\n", "string", "=", "re_changehyphen", ".", "sub", "(", "'-'", ",", "string", ")", "\n", "string", "=", "re_remove_html", ".", "sub", "(", "' '", ",", "string", ")", "\n", "string", "=", "re_transform_numbers", ".", "sub", "(", "'0'", ",", "string", ")", "\n", "string", "=", "re_transform_url", ".", "sub", "(", "'URL'", ",", "string", ")", "\n", "string", "=", "re_transform_emails", ".", "sub", "(", "'EMAIL'", ",", "string", ")", "\n", "string", "=", "re_quotes_1", ".", "sub", "(", "r'\\1\"'", ",", "string", ")", "\n", "string", "=", "re_quotes_2", ".", "sub", "(", "r'\"\\1'", ",", "string", ")", "\n", "string", "=", "re_quotes_3", ".", "sub", "(", "'\"'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\"'", ",", "''", ",", "string", ")", "\n", "string", "=", "re_dots", ".", "sub", "(", "'.'", ",", "string", ")", "\n", "string", "=", "re_punctuation", ".", "sub", "(", "r'\\1'", ",", "string", ")", "\n", "string", "=", "re_hiphen", ".", "sub", "(", "' - '", ",", "string", ")", "\n", "string", "=", "re_punkts", ".", "sub", "(", "r'\\1 \\2 \\3'", ",", "string", ")", "\n", "string", "=", "re_punkts_b", ".", "sub", "(", "r'\\1 \\2 \\3'", ",", "string", ")", "\n", "string", "=", "re_punkts_c", ".", "sub", "(", "r'\\1 \\2'", ",", "string", ")", "\n", "string", "=", "re_doublequotes_1", ".", "sub", "(", "'\\\"'", ",", "string", ")", "\n", "string", "=", "re_doublequotes_2", ".", "sub", "(", "'\\''", ",", "string", ")", "\n", "string", "=", "re_trim", ".", "sub", "(", "' '", ",", "string", ")", "\n", "\n", "return", "string", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.clean_and_filter_first_sentences": [[111, 120], ["sent_tokenizer.tokenize", "sentences.append", "sent.count", "acr_preprocess_gcom.clean_str", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.clean_str"], ["def", "clean_and_filter_first_sentences", "(", "string", ",", "first_sentences", "=", "8", ")", ":", "\n", "# Tokenize sentences and remove short and malformed sentences.", "\n", "    ", "sentences", "=", "[", "]", "\n", "for", "sent", "in", "sent_tokenizer", ".", "tokenize", "(", "string", ")", ":", "\n", "        ", "if", "sent", ".", "count", "(", "' '", ")", ">=", "3", "and", "sent", "[", "-", "1", "]", "in", "[", "'.'", ",", "'!'", ",", "'?'", ",", "';'", "]", ":", "\n", "            ", "sentences", ".", "append", "(", "clean_str", "(", "sent", ")", ")", "\n", "if", "len", "(", "sentences", ")", "==", "first_sentences", ":", "\n", "                ", "break", "\n", "", "", "", "return", "' '", ".", "join", "(", "sentences", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.load_input_csv": [[123, 133], ["pandas.read_csv", "news_df[].apply", "news_df[].apply", "news_df[].apply"], "function", ["None"], ["", "def", "load_input_csv", "(", "path", ")", ":", "\n", "    ", "news_df", "=", "pd", ".", "read_csv", "(", "path", ",", "encoding", "=", "'utf-8'", ")", "\n", "\n", "#Concatenating all available text", "\n", "news_df", "[", "'full_text'", "]", "=", "(", "news_df", "[", "'title'", "]", ".", "apply", "(", "nan_to_str", ")", "+", "\". \"", "+", "news_df", "[", "'caption'", "]", ".", "apply", "(", "nan_to_str", ")", "+", "\". \"", "+", "news_df", "[", "'body'", "]", ".", "apply", "(", "nan_to_str", ")", "\n", ")", ".", "apply", "(", "clean_and_filter_first_sentences", ")", "\n", "\n", "return", "news_df", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.process_cat_features": [[134, 145], ["sklearn.preprocessing.LabelEncoder", "sklearn.preprocessing.LabelEncoder.fit_transform", "sklearn.preprocessing.LabelEncoder", "sklearn.preprocessing.LabelEncoder.fit_transform", "sklearn.preprocessing.LabelEncoder", "sklearn.preprocessing.LabelEncoder.fit_transform"], "function", ["None"], ["", "def", "process_cat_features", "(", "dataframe", ")", ":", "\n", "    ", "article_id_encoder", "=", "LabelEncoder", "(", ")", "\n", "dataframe", "[", "'id_encoded'", "]", "=", "article_id_encoder", ".", "fit_transform", "(", "dataframe", "[", "'id'", "]", ")", "\n", "\n", "category_id_encoder", "=", "LabelEncoder", "(", ")", "\n", "dataframe", "[", "'categoryid_encoded'", "]", "=", "category_id_encoder", ".", "fit_transform", "(", "dataframe", "[", "'categoryid'", "]", ")", "\n", "\n", "domainid_encoder", "=", "LabelEncoder", "(", ")", "\n", "dataframe", "[", "'domainid_encoded'", "]", "=", "domainid_encoder", ".", "fit_transform", "(", "dataframe", "[", "'domainid'", "]", ")", "\n", "\n", "return", "article_id_encoder", ",", "category_id_encoder", ",", "domainid_encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.save_article_cat_encoders": [[146, 151], ["utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "save_article_cat_encoders", "(", "output_path", ",", "article_id_encoder", ",", "category_id_encoder", ",", "domainid_encoder", ")", ":", "\n", "    ", "to_serialize", "=", "{", "'article_id'", ":", "article_id_encoder", ",", "\n", "'category_id'", ":", "category_id_encoder", ",", "\n", "'publisher_id'", ":", "domainid_encoder", "}", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.make_sequence_example": [[153, 172], ["tensorflow.train.Features", "tensorflow.train.FeatureLists", "tensorflow.train.SequenceExample", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Feature", "tf_records_management.make_sequential_feature", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List", "tensorflow.train.Int64List"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature"], ["", "def", "make_sequence_example", "(", "row", ")", ":", "\n", "    ", "context_features", "=", "{", "\n", "'article_id'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'id_encoded'", "]", "]", ")", ")", ",", "\n", "'publisher_id'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'domainid_encoded'", "]", "]", ")", ")", ",", "\n", "'category_id'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'categoryid_encoded'", "]", "]", ")", ")", ",", "\n", "'created_at_ts'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'created_at_ts'", "]", "]", ")", ")", ",", "\n", "'text_length'", ":", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "row", "[", "'text_length'", "]", "]", ")", ")", "\n", "}", "\n", "\n", "context", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "context_features", ")", "\n", "\n", "sequence_features", "=", "{", "\n", "'text'", ":", "make_sequential_feature", "(", "row", "[", "\"text_int\"", "]", ",", "vtype", "=", "int", ")", "\n", "}", "\n", "\n", "sequence_feature_lists", "=", "tf", ".", "train", ".", "FeatureLists", "(", "feature_list", "=", "sequence_features", ")", "\n", "\n", "return", "tf", ".", "train", ".", "SequenceExample", "(", "feature_lists", "=", "sequence_feature_lists", ",", "\n", "context", "=", "context", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.main": [[174, 223], ["acr_preprocess_gcom.create_args_parser", "create_args_parser.parse_args", "print", "acr_preprocess_gcom.load_input_csv", "print", "acr_preprocess_gcom.process_cat_features", "print", "acr_preprocess_gcom.save_article_cat_encoders", "print", "tokenization.tokenize_articles", "print", "tokenization.get_words_freq", "print", "print", "word_embeddings.load_word_embeddings", "word_embeddings.process_word_embedding_for_corpus_vocab", "print", "word_embeddings.save_word_vocab_embeddings", "print", "tokenization.convert_tokens_to_int", "print", "tf_records_management.export_dataframe_to_tf_records", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.create_args_parser", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.load_input_csv", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.process_cat_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.acr_preprocess_gcom.save_article_cat_encoders", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.tokenize_articles", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.get_words_freq", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.word_embeddings.load_word_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.word_embeddings.process_word_embedding_for_corpus_vocab", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.word_embeddings.save_word_vocab_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.preprocessing.tokenization.convert_tokens_to_int", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.export_dataframe_to_tf_records"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "create_args_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "print", "(", "'Loading news article CSV: {}'", ".", "format", "(", "args", ".", "input_articles_csv_path", ")", ")", "\n", "news_df", "=", "load_input_csv", "(", "args", ".", "input_articles_csv_path", ")", "\n", "\n", "print", "(", "'Encoding categorical features'", ")", "\n", "article_id_encoder", ",", "category_id_encoder", ",", "domainid_encoder", "=", "process_cat_features", "(", "news_df", ")", "\n", "print", "(", "'Exporting LabelEncoders of categorical features: {}'", ".", "format", "(", "args", ".", "output_label_encoders", ")", ")", "\n", "save_article_cat_encoders", "(", "args", ".", "output_label_encoders", ",", "\n", "article_id_encoder", ",", "\n", "category_id_encoder", ",", "\n", "domainid_encoder", ")", "\n", "\n", "print", "(", "'Tokenizing articles...'", ")", "\n", "tokenized_articles", "=", "tokenize_articles", "(", "news_df", "[", "'full_text'", "]", ")", "\n", "\n", "print", "(", "'Computing word frequencies...'", ")", "\n", "words_freq", "=", "get_words_freq", "(", "tokenized_articles", ")", "\n", "print", "(", "'Corpus vocabulary size: {}'", ".", "format", "(", "len", "(", "words_freq", ")", ")", ")", "\n", "\n", "print", "(", "\"Loading word2vec model and extracting words of this corpus' vocabulary...\"", ")", "\n", "w2v_model", "=", "load_word_embeddings", "(", "args", ".", "input_word_embeddings_path", ",", "binary", "=", "False", ")", "\n", "word_vocab", ",", "word_embeddings_matrix", "=", "process_word_embedding_for_corpus_vocab", "(", "w2v_model", ",", "\n", "words_freq", ",", "\n", "args", ".", "vocab_most_freq_words", ")", "\n", "\n", "print", "(", "'Saving word embeddings and vocab.: {}'", ".", "format", "(", "args", ".", "output_word_vocab_embeddings_path", ")", ")", "\n", "save_word_vocab_embeddings", "(", "args", ".", "output_word_vocab_embeddings_path", ",", "\n", "word_vocab", ",", "word_embeddings_matrix", ")", "\n", "\n", "print", "(", "'Converting tokens to int numbers (according to the vocab.)...'", ")", "\n", "texts_int", ",", "texts_lengths", "=", "convert_tokens_to_int", "(", "tokenized_articles", ",", "word_vocab", ")", "\n", "news_df", "[", "'text_length'", "]", "=", "texts_lengths", "\n", "news_df", "[", "'text_int'", "]", "=", "texts_int", "\n", "\n", "data_to_export_df", "=", "news_df", "[", "[", "'id_encoded'", ",", "\n", "'domainid_encoded'", ",", "\n", "'categoryid_encoded'", ",", "\n", "'created_at_ts'", ",", "\n", "'text_length'", ",", "\n", "'text_int'", "]", "]", "\n", "\n", "print", "(", "'Exporting tokenized articles to TFRecords: {}'", ".", "format", "(", "args", ".", "output_tf_records_path", ")", ")", "\n", "export_dataframe_to_tf_records", "(", "data_to_export_df", ",", "\n", "make_sequence_example", ",", "\n", "output_path", "=", "args", ".", "output_tf_records_path", ",", "\n", "examples_by_file", "=", "args", ".", "articles_by_tfrecord", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.__init__": [[18, 35], ["benchmarks.BenchmarkRecommender.__init__", "getattr", "dict", "sequential_rules.SequentialRulesRecommender.get_clf_suffix", "sequential_rules.SequentialRulesRecommender.get_clf_suffix", "sequential_rules.SequentialRulesRecommender.get_clf_suffix", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix"], ["    ", "def", "__init__", "(", "self", ",", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", ":", "\n", "#super(Instructor, self).__init__(name, year) #Python 2", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", "\n", "\n", "self", ".", "max_clicks_dist", "=", "params", "[", "'max_clicks_dist'", "]", "#Max number of clicks to walk back in the session from the currently viewed item. (Default value: 10)        ", "\n", "\n", "self", ".", "dist_between_clicks_decay", "=", "params", "[", "'dist_between_clicks_decay'", "]", "#Decay function for distance between two items clicks within a session (linear, same, div, log, qudratic). (Default value: div)        ", "\n", "self", ".", "dist_between_clicks_decay_fn", "=", "getattr", "(", "self", ",", "'{}_decay'", ".", "format", "(", "self", ".", "dist_between_clicks_decay", ")", ")", "\n", "\n", "#Registering a state for this recommender which persists over TF Estimator train/eval loops", "\n", "if", "not", "self", ".", "get_clf_suffix", "(", ")", "in", "clicked_items_state", ".", "benchmarks_states", ":", "\n", "            ", "clicked_items_state", ".", "benchmarks_states", "[", "self", ".", "get_clf_suffix", "(", ")", "]", "=", "dict", "(", "{", "'rules'", ":", "defaultdict", "(", "dict", ")", "# Dict of Dict (Trained Rules)", "\n", "}", ")", "\n", "\n", "", "state", "=", "clicked_items_state", ".", "benchmarks_states", "[", "self", ".", "get_clf_suffix", "(", ")", "]", "\n", "self", ".", "rules", "=", "state", "[", "'rules'", "]", "# Dict of Dict (Trained Rules)", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.get_clf_suffix": [[36, 38], ["None"], "methods", ["None"], ["", "def", "get_clf_suffix", "(", "self", ")", ":", "\n", "        ", "return", "'sr'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.get_description": [[39, 41], ["None"], "methods", ["None"], ["", "def", "get_description", "(", "self", ")", ":", "\n", "        ", "return", "'Sequential Rules'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.get_all_sessions_clicks": [[42, 48], ["list", "list", "list", "zip", "filter", "list", "filter"], "methods", ["None"], ["", "def", "get_all_sessions_clicks", "(", "self", ",", "sessions_items", ",", "sessions_next_items", ")", ":", "\n", "        ", "sessions_all_items_but_last", "=", "list", "(", "[", "list", "(", "filter", "(", "lambda", "x", ":", "x", "!=", "0", ",", "session", ")", ")", "for", "session", "in", "sessions_items", "]", ")", "\n", "sessions_last_item_clicked", "=", "list", "(", "[", "list", "(", "filter", "(", "lambda", "x", ":", "x", "!=", "0", ",", "session", ")", ")", "[", "-", "1", "]", "for", "session", "in", "sessions_next_items", "]", ")", "\n", "sessions_all_clicks", "=", "[", "previous_items", "+", "[", "last_item", "]", "for", "previous_items", ",", "last_item", "in", "zip", "(", "sessions_all_items_but_last", ",", "sessions_last_item_clicked", ")", "]", "\n", "return", "sessions_all_clicks", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.train": [[49, 63], ["sequential_rules.SequentialRulesRecommender.get_all_sessions_clicks", "range", "len", "range", "max", "sequential_rules.SequentialRulesRecommender.dist_between_clicks_decay_fn"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.get_all_sessions_clicks"], ["", "def", "train", "(", "self", ",", "users_ids", ",", "sessions_ids", ",", "sessions_items", ",", "sessions_next_items", ")", ":", "\n", "        ", "sessions_all_items", "=", "self", ".", "get_all_sessions_clicks", "(", "sessions_items", ",", "sessions_next_items", ")", "\n", "\n", "#For each session in the batch", "\n", "for", "session_items", "in", "sessions_all_items", ":", "\n", "#For each item of the session", "\n", "            ", "for", "i", "in", "range", "(", "1", ",", "len", "(", "session_items", ")", ")", ":", "\n", "                ", "active_item", "=", "session_items", "[", "i", "]", "\n", "#For all items previously clicked in the session (limited by the max_clicks_dist)", "\n", "for", "j", "in", "range", "(", "max", "(", "0", ",", "i", "-", "self", ".", "max_clicks_dist", ")", ",", "i", ")", ":", "\n", "                    ", "past_item", "=", "session_items", "[", "j", "]", "\n", "if", "not", "active_item", "in", "self", ".", "rules", "[", "past_item", "]", ":", "\n", "                        ", "self", ".", "rules", "[", "past_item", "]", "[", "active_item", "]", "=", "0.0", "\n", "", "self", ".", "rules", "[", "past_item", "]", "[", "active_item", "]", "+=", "self", ".", "dist_between_clicks_decay_fn", "(", "i", "-", "j", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.predict": [[65, 79], ["numpy.zeros", "enumerate", "enumerate", "list", "list", "map", "sequential_rules.SequentialRulesRecommender._get_top_n_valid_items", "sorted", "sequential_rules.SequentialRulesRecommender.rules[].items"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender._get_top_n_valid_items"], ["", "", "", "", "def", "predict", "(", "self", ",", "users_ids", ",", "sessions_items", ",", "topk", "=", "5", ",", "valid_items", "=", "None", ")", ":", "\n", "        ", "session_predictions", "=", "np", ".", "zeros", "(", "dtype", "=", "np", ".", "int64", ",", "\n", "shape", "=", "[", "sessions_items", ".", "shape", "[", "0", "]", ",", "\n", "sessions_items", ".", "shape", "[", "1", "]", ",", "\n", "topk", "]", ")", "\n", "\n", "for", "row_idx", ",", "session_items", "in", "enumerate", "(", "sessions_items", ")", ":", "\n", "            ", "for", "col_idx", ",", "item", "in", "enumerate", "(", "session_items", ")", ":", "\n", "                ", "if", "item", "!=", "0", ":", "\n", "#Sorts items its score", "\n", "                    ", "preds", "=", "list", "(", "map", "(", "lambda", "y", ":", "y", "[", "0", "]", ",", "sorted", "(", "self", ".", "rules", "[", "item", "]", ".", "items", "(", ")", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", ")", ")", "\n", "session_predictions", "[", "row_idx", ",", "col_idx", "]", "=", "list", "(", "self", ".", "_get_top_n_valid_items", "(", "preds", ",", "topk", ",", "valid_items", "[", "row_idx", ",", "col_idx", "]", ")", ")", "\n", "\n", "", "", "", "return", "session_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.linear_decay": [[81, 83], ["None"], "methods", ["None"], ["", "def", "linear_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "-", "(", "0.1", "*", "i", ")", "if", "i", "<=", "100", "else", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.same_decay": [[84, 86], ["None"], "methods", ["None"], ["", "def", "same_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.div_decay": [[87, 89], ["None"], "methods", ["None"], ["", "def", "div_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "/", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.log_decay": [[90, 92], ["log10"], "methods", ["None"], ["", "def", "log_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "/", "(", "log10", "(", "i", "+", "1.7", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.sequential_rules.SequentialRulesRecommender.quadratic_decay": [[93, 95], ["None"], "methods", ["None"], ["", "def", "quadratic_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "/", "(", "i", "*", "i", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_knn.ItemKNNRecommender.__init__": [[14, 22], ["benchmarks.BenchmarkRecommender.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", ":", "\n", "#super(Instructor, self).__init__(name, year) #Python 2", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", "\n", "\n", "#Regularization. Discounts the similarity of rare items (incidental co-occurrences). ", "\n", "self", ".", "reg_lambda", "=", "params", "[", "'reg_lambda'", "]", "\n", "#Balance between normalizing with the supports of the two items. 0.5 gives cosine similarity, 1.0 gives confidence (as in association rules).", "\n", "self", ".", "alpha", "=", "params", "[", "'alpha'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_knn.ItemKNNRecommender.get_clf_suffix": [[23, 25], ["None"], "methods", ["None"], ["", "def", "get_clf_suffix", "(", "self", ")", ":", "\n", "        ", "return", "'item_knn'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_knn.ItemKNNRecommender.get_description": [[26, 28], ["None"], "methods", ["None"], ["", "def", "get_description", "(", "self", ")", ":", "\n", "        ", "return", "'Item-KNN: Most similar items sessions based on normalized cosine similarity between session co-occurence'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_knn.ItemKNNRecommender.train": [[29, 31], ["None"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "users_ids", ",", "sessions_ids", ",", "sessions_items", ",", "sessions_next_items", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_knn.ItemKNNRecommender.predict": [[32, 59], ["numpy.zeros", "item_knn.ItemKNNRecommender.clicked_items_state.get_articles_pop", "numpy.power", "enumerate", "item_knn.ItemKNNRecommender.clicked_items_state.get_items_coocurrences", "enumerate", "utils.max_n_sparse_indexes", "list", "numpy.power", "numpy.array", "item_knn.ItemKNNRecommender._get_top_n_valid_items", "len", "utils.max_n_sparse_indexes.tolist"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_pop", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_items_coocurrences", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.max_n_sparse_indexes", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender._get_top_n_valid_items"], ["", "def", "predict", "(", "self", ",", "users_ids", ",", "sessions_items", ",", "topk", "=", "5", ",", "valid_items", "=", "None", ")", ":", "\n", "        ", "session_predictions", "=", "np", ".", "zeros", "(", "dtype", "=", "np", ".", "int64", ",", "\n", "shape", "=", "[", "sessions_items", ".", "shape", "[", "0", "]", ",", "\n", "sessions_items", ".", "shape", "[", "1", "]", ",", "\n", "topk", "]", ")", "\n", "\n", "\n", "articles_support", "=", "self", ".", "clicked_items_state", ".", "get_articles_pop", "(", ")", "\n", "articles_support_norm", "=", "np", ".", "power", "(", "articles_support", "+", "self", ".", "reg_lambda", ",", "self", ".", "alpha", ")", "\n", "\n", "for", "row_idx", ",", "session_items", "in", "enumerate", "(", "sessions_items", ")", ":", "\n", "\n", "            ", "session_item_coocurrences", "=", "self", ".", "clicked_items_state", ".", "get_items_coocurrences", "(", ")", "\n", "\n", "for", "col_idx", ",", "item", "in", "enumerate", "(", "session_items", ")", ":", "\n", "                ", "if", "item", "!=", "0", ":", "\n", "                    ", "item_coocurrences", "=", "session_item_coocurrences", "[", "item", "]", "\n", "\n", "norm", "=", "articles_support_norm", "*", "np", ".", "power", "(", "articles_support", "[", "item", "]", "+", "self", ".", "reg_lambda", ",", "1.0", "-", "self", ".", "alpha", ")", "\n", "#Hack to flatten the 1-row matrix to an array", "\n", "items_similarity", "=", "np", ".", "array", "(", "item_coocurrences", "/", "norm", ")", "[", "0", "]", "\n", "\n", "sorted_items", "=", "max_n_sparse_indexes", "(", "items_similarity", "[", "item_coocurrences", ".", "indices", "]", ",", "#item_coocurrences.data, ", "\n", "item_coocurrences", ".", "indices", ",", "topn", "=", "len", "(", "item_coocurrences", ".", "indices", ")", ")", "\n", "session_predictions", "[", "row_idx", ",", "col_idx", "]", "=", "list", "(", "self", ".", "_get_top_n_valid_items", "(", "sorted_items", ".", "tolist", "(", ")", ",", "topk", ",", "valid_items", "[", "row_idx", ",", "col_idx", "]", ")", ")", "\n", "\n", "", "", "", "return", "session_predictions", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.recently_popular.RecentlyPopularRecommender.__init__": [[12, 15], ["benchmarks.BenchmarkRecommender.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", ":", "\n", "#super(Instructor, self).__init__(name, year) #Python 2", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.recently_popular.RecentlyPopularRecommender.get_clf_suffix": [[16, 18], ["None"], "methods", ["None"], ["", "def", "get_clf_suffix", "(", "self", ")", ":", "\n", "        ", "return", "'pop_recent'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.recently_popular.RecentlyPopularRecommender.get_description": [[19, 21], ["None"], "methods", ["None"], ["", "def", "get_description", "(", "self", ")", ":", "\n", "        ", "return", "'Most Popular from Recently Clicked'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.recently_popular.RecentlyPopularRecommender.get_recent_popular_item_ids": [[22, 31], ["recently_popular.RecentlyPopularRecommender.clicked_items_state.get_recent_clicks_buffer", "collections.Counter", "zip", "len", "numpy.nonzero", "collections.Counter.most_common"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_recent_clicks_buffer"], ["", "def", "get_recent_popular_item_ids", "(", "self", ")", ":", "\n", "        ", "recent_items_buffer", "=", "self", ".", "clicked_items_state", ".", "get_recent_clicks_buffer", "(", ")", "\n", "recent_items_buffer_nonzero", "=", "recent_items_buffer", "[", "np", ".", "nonzero", "(", "recent_items_buffer", ")", "]", "\n", "#Dealing with first batch, when there is no item in the buffer yet", "\n", "if", "len", "(", "recent_items_buffer_nonzero", ")", "==", "0", ":", "\n", "            ", "recent_items_buffer_nonzero", "=", "[", "0", "]", "\n", "", "item_counter", "=", "Counter", "(", "recent_items_buffer_nonzero", ")", "\n", "popular_item_ids", ",", "popular_items_count", "=", "zip", "(", "*", "item_counter", ".", "most_common", "(", ")", ")", "\n", "return", "popular_item_ids", ",", "popular_items_count", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.recently_popular.RecentlyPopularRecommender.train": [[32, 34], ["None"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "users_ids", ",", "sessions_ids", ",", "sessions_items", ",", "sessions_next_items", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.recently_popular.RecentlyPopularRecommender.predict": [[35, 49], ["recently_popular.RecentlyPopularRecommender.get_recent_popular_item_ids", "numpy.zeros", "enumerate", "enumerate", "list", "recently_popular.RecentlyPopularRecommender._get_top_n_valid_items"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.recently_popular.RecentlyPopularRecommender.get_recent_popular_item_ids", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender._get_top_n_valid_items"], ["", "def", "predict", "(", "self", ",", "users_ids", ",", "sessions_items", ",", "topk", "=", "5", ",", "valid_items", "=", "None", ")", ":", "\n", "        ", "popular_item_ids", ",", "popular_items_count", "=", "self", ".", "get_recent_popular_item_ids", "(", ")", "\n", "\n", "session_predictions", "=", "np", ".", "zeros", "(", "dtype", "=", "np", ".", "int64", ",", "\n", "shape", "=", "[", "sessions_items", ".", "shape", "[", "0", "]", ",", "\n", "sessions_items", ".", "shape", "[", "1", "]", ",", "\n", "topk", "]", ")", "\n", "\n", "for", "row_idx", ",", "session_items", "in", "enumerate", "(", "sessions_items", ")", ":", "\n", "            ", "for", "col_idx", ",", "item", "in", "enumerate", "(", "session_items", ")", ":", "\n", "                ", "if", "item", "!=", "0", ":", "\n", "                    ", "session_predictions", "[", "row_idx", ",", "col_idx", "]", "=", "list", "(", "self", ".", "_get_top_n_valid_items", "(", "popular_item_ids", ",", "topk", ",", "valid_items", "[", "row_idx", ",", "col_idx", "]", ")", ")", "\n", "\n", "", "", "", "return", "session_predictions", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.__init__": [[9, 12], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "get_recent_clicks_buffer_fn", ",", "ignore_session_items_on_sampling", "=", "True", ")", ":", "\n", "        ", "self", ".", "get_recent_clicks_buffer_fn", "=", "get_recent_clicks_buffer_fn", "\n", "self", ".", "ignore_session_items_on_sampling", "=", "ignore_session_items_on_sampling", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_sample_from_recently_clicked_items_buffer": [[13, 22], ["candidate_sampling.CandidateSamplingManager.get_recent_clicks_buffer_fn", "numpy.random.permutation", "candidate_sampling.CandidateSamplingManager.ravel", "numpy.flatnonzero"], "methods", ["None"], ["", "def", "get_sample_from_recently_clicked_items_buffer", "(", "self", ",", "sample_size", ")", ":", "\n", "        ", "pop_recent_items_buffer", "=", "self", ".", "get_recent_clicks_buffer_fn", "(", ")", "\n", "pop_recent_items_buffer_masked", "=", "pop_recent_items_buffer", ".", "ravel", "(", ")", "[", "np", ".", "flatnonzero", "(", "pop_recent_items_buffer", ")", "]", "\n", "\n", "pop_recent_items_buffer_shuffled", "=", "np", ".", "random", ".", "permutation", "(", "pop_recent_items_buffer_masked", ")", "\n", "\n", "#Samples K articles from recent clicks (popularity sampled)", "\n", "sample_recently_clicked_items", "=", "pop_recent_items_buffer_shuffled", "[", ":", "sample_size", "]", "\n", "return", "sample_recently_clicked_items", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_click": [[25, 38], ["numpy.random.permutation", "numpy.unique", "numpy.concatenate", "numpy.zeros", "numpy.argsort"], "methods", ["None"], ["", "def", "get_neg_items_click", "(", "self", ",", "valid_samples_session", ",", "num_neg_samples", ")", ":", "\n", "#Shuffles neg. samples for each click", "\n", "        ", "valid_samples_shuffled", "=", "np", ".", "random", ".", "permutation", "(", "valid_samples_session", ")", "\n", "\n", "samples_unique_vals", ",", "samples_unique_idx", "=", "np", ".", "unique", "(", "valid_samples_shuffled", ",", "return_index", "=", "True", ")", "\n", "\n", "#Returning first N unique items (to avoid repetition)", "\n", "first_unique_items", "=", "samples_unique_vals", "[", "np", ".", "argsort", "(", "samples_unique_idx", ")", "]", "[", ":", "num_neg_samples", "]", "\n", "\n", "#Padding if necessary to keep the number of neg samples constant (ex: first batch)", "\n", "first_unique_items_padded_if_needed", "=", "np", ".", "concatenate", "(", "[", "first_unique_items", ",", "np", ".", "zeros", "(", "num_neg_samples", "-", "first_unique_items", ".", "shape", "[", "0", "]", ",", "np", ".", "int64", ")", "]", ",", "axis", "=", "0", ")", "\n", "\n", "return", "first_unique_items_padded_if_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_session": [[40, 54], ["numpy.vstack", "numpy.setdiff1d", "candidate_sampling.CandidateSamplingManager.get_neg_items_click", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_click"], ["", "def", "get_neg_items_session", "(", "self", ",", "session_item_ids", ",", "candidate_samples", ",", "num_neg_samples", ")", ":", "\n", "        ", "if", "self", ".", "ignore_session_items_on_sampling", ":", "\n", "#Ignoring negative samples clicked within the session (keeps the order and repetition of candidate_samples)", "\n", "            ", "samples_for_session", "=", "np", ".", "setdiff1d", "(", "candidate_samples", ",", "session_item_ids", ",", "assume_unique", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "samples_for_session", "=", "candidate_samples", "\n", "\n", "#Generating a random list of negative samples for each click (with no repetition)", "\n", "", "session_clicks_neg_items", "=", "np", ".", "vstack", "(", "[", "self", ".", "get_neg_items_click", "(", "samples_for_session", ",", "num_neg_samples", ")", "if", "click_id", "!=", "0", "else", "np", ".", "zeros", "(", "num_neg_samples", ",", "np", ".", "int64", ")", "for", "click_id", "in", "session_item_ids", "]", ")", "\n", "\n", "return", "session_clicks_neg_items", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_negative_samples": [[56, 60], ["numpy.vstack", "numpy.expand_dims", "candidate_sampling.CandidateSamplingManager.get_neg_items_session"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_session"], ["", "def", "get_negative_samples", "(", "self", ",", "all_clicked_items", ",", "candidate_samples", ",", "num_neg_samples", ")", ":", "\n", "#Shuffling negative samples by session and limiting to num_neg_samples ", "\n", "        ", "return", "np", ".", "vstack", "(", "[", "np", ".", "expand_dims", "(", "self", ".", "get_neg_items_session", "(", "session_item_ids", ",", "candidate_samples", ",", "num_neg_samples", ")", ",", "0", ")", "for", "session_item_ids", "in", "all_clicked_items", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_batch_negative_samples_by_session": [[62, 79], ["all_clicked_items.ravel", "numpy.concatenate", "numpy.random.permutation", "candidate_sampling.CandidateSamplingManager.get_negative_samples", "numpy.flatnonzero"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_negative_samples"], ["", "def", "get_batch_negative_samples_by_session", "(", "self", ",", "all_clicked_items", ",", "additional_samples", ",", "num_negative_samples", ",", "\n", "first_sampling_multiplying_factor", "=", "20", ")", ":", "\n", "        ", "batch_items", "=", "all_clicked_items", ".", "ravel", "(", ")", "\n", "\n", "#Removing padded (zeroed) items", "\n", "batch_items_non_zero", "=", "batch_items", "[", "np", ".", "flatnonzero", "(", "batch_items", ")", "]", "\n", "\n", "#Concatenating batch items with additional samples (to deal with small batches)", "\n", "candidate_neg_items", "=", "np", ".", "concatenate", "(", "[", "batch_items_non_zero", ",", "additional_samples", "]", ",", "axis", "=", "0", ")", "\n", "\n", "#Shuffling candidates and sampling the first 20N (1000 if neg_samples=50)", "\n", "candidate_neg_items_shuffled", "=", "np", ".", "random", ".", "permutation", "(", "candidate_neg_items", ")", "\n", "candidate_neg_items_sampled", "=", "candidate_neg_items_shuffled", "[", ":", "(", "num_negative_samples", "*", "first_sampling_multiplying_factor", ")", "]", "\n", "\n", "batch_negative_items", "=", "self", ".", "get_negative_samples", "(", "all_clicked_items", ",", "candidate_neg_items_sampled", ",", "num_negative_samples", ")", "\n", "\n", "return", "batch_negative_items", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_batch_negative_samples": [[81, 92], ["candidate_sampling.CandidateSamplingManager.get_sample_from_recently_clicked_items_buffer", "candidate_sampling.CandidateSamplingManager.get_batch_negative_samples_by_session"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_sample_from_recently_clicked_items_buffer", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_batch_negative_samples_by_session"], ["", "def", "get_batch_negative_samples", "(", "self", ",", "all_clicked_items", ",", "negative_samples_by_session", ",", "negative_sample_from_buffer", ")", ":", "\n", "#Samples from recent items buffer", "\n", "        ", "negative_sample_recently_clicked_ids", "=", "self", ".", "get_sample_from_recently_clicked_items_buffer", "(", "\n", "negative_sample_from_buffer", ")", "\n", "\n", "\n", "batch_negative_items", "=", "self", ".", "get_batch_negative_samples_by_session", "(", "all_clicked_items", ",", "\n", "additional_samples", "=", "negative_sample_recently_clicked_ids", ",", "\n", "num_negative_samples", "=", "negative_samples_by_session", ")", "\n", "\n", "return", "batch_negative_items", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks_data_loader.DataLoader.__init__": [[26, 29], ["benchmarks_data_loader.DataLoader.get_session_features_config", "benchmarks_data_loader.DataLoader.init_dataset_iterator_local"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.get_session_features_config", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks_data_loader.DataLoader.init_dataset_iterator_local"], ["\t", "def", "__init__", "(", "self", ",", "dataset", ")", ":", "\n", "\t\t", "features_config", "=", "self", ".", "get_session_features_config", "(", "dataset", ")", "\n", "self", ".", "init_dataset_iterator_local", "(", "features_config", ",", "batch_size", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks_data_loader.DataLoader.get_session_features_config": [[31, 50], ["None"], "methods", ["None"], ["", "def", "get_session_features_config", "(", "self", ",", "dataset", ")", ":", "\n", "\t\t", "user_id_type", "=", "'int'", "if", "dataset", "==", "G1_DATASET", "else", "'bytes'", "\n", "session_features_config", "=", "{", "\n", "'single_features'", ":", "{", "\n", "##Control features", "\n", "'user_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "user_id_type", "}", ",", "\n", "'session_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "#'session_id': {'type': 'categorical', 'dtype': 'string'},            ", "\n", "'session_start'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'session_size'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "}", ",", "\n", "'sequence_features'", ":", "{", "\n", "#Required sequence features", "\n", "'event_timestamp'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'item_clicked'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "#, 'cardinality': 364047},           ", "\n", "}", "\n", "}", "\n", "\n", "return", "session_features_config", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks_data_loader.DataLoader.init_dataset_iterator_local": [[51, 70], ["tensorflow.device", "tensorflow.placeholder", "datasets.make_dataset", "tensorflow.data.Iterator.from_structure", "tensorflow.data.Iterator.from_structure.get_next", "tensorflow.data.Iterator.from_structure.make_initializer"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.make_dataset"], ["", "def", "init_dataset_iterator_local", "(", "self", ",", "features_config", ",", "batch_size", "=", "128", ",", "\n", "truncate_session_length", "=", "20", ")", ":", "\n", "\t    ", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "\t        ", "self", ".", "files_placeholder", "=", "tf", ".", "placeholder", "(", "tf", ".", "string", ")", "\n", "\n", "# Make a dataset ", "\n", "ds", "=", "make_dataset", "(", "self", ".", "files_placeholder", ",", "features_config", ",", "batch_size", "=", "batch_size", ",", "\n", "truncate_sequence_length", "=", "truncate_session_length", ")", "\n", "\n", "\n", "# Define an abstract iterator that has the shape and type of our datasets", "\n", "iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "ds", ".", "output_types", ",", "\n", "ds", ".", "output_shapes", ")", "\n", "\n", "# This is an op that gets the next element from the iterator", "\n", "self", ".", "next_element_op", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "# These ops let us switch and reinitialize every time we finish an epoch    ", "\n", "self", ".", "iterator_init_op", "=", "iterator", ".", "make_initializer", "(", "ds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks_data_loader.DataLoader.load_dataframe": [[72, 111], ["pandas.DataFrame", "tensorflow.Session", "sess.run", "len", "print", "print", "sess.run", "set", "zip", "data.append", "len", "set.add", "data.append"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add"], ["", "", "def", "load_dataframe", "(", "self", ",", "data_filenames", ")", ":", "\n", "\t    ", "data", "=", "[", "]", "\n", "\n", "session_cnt", "=", "0", "\n", "repeated", "=", "0", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "\t        ", "sess", ".", "run", "(", "self", ".", "iterator_init_op", ",", "feed_dict", "=", "{", "self", ".", "files_placeholder", ":", "data_filenames", "}", ")", "\n", "while", "True", ":", "\n", "\t            ", "try", ":", "\n", "#One session by batch", "\n", "\t                ", "batch_inputs", ",", "batch_labels", "=", "sess", ".", "run", "(", "self", ".", "next_element_op", ")", "\n", "\n", "item_ids_session", "=", "set", "(", ")", "\n", "\n", "session_id", "=", "batch_inputs", "[", "'session_id'", "]", "[", "0", "]", "\n", "for", "item", ",", "ts", "in", "zip", "(", "batch_inputs", "[", "'item_clicked'", "]", "[", "0", "]", ",", "batch_inputs", "[", "'event_timestamp'", "]", "[", "0", "]", ")", ":", "\n", "\t                    ", "if", "item", "in", "item_ids_session", ":", "\n", "\t                        ", "repeated", "+=", "1", "\n", "", "item_ids_session", ".", "add", "(", "item", ")", "\n", "data", ".", "append", "(", "(", "session_id", ",", "item", ",", "ts", ")", ")", "\n", "\n", "#Adding last item (label)", "\n", "", "last_item", "=", "batch_labels", "[", "'label_last_item'", "]", "[", "0", "]", "[", "0", "]", "\n", "if", "last_item", "in", "item_ids_session", ":", "\n", "\t                    ", "repeated", "+=", "1", "\n", "", "data", ".", "append", "(", "(", "session_id", ",", "last_item", ",", "ts", ")", ")", "\n", "\n", "session_cnt", "+=", "1", "\n", "#if cnt % 100 == 0:", "\n", "#    print(\"Sessions processed: {} - Clicks: {}\".format(session_cnt, len(data)))", "\n", "", "except", "tf", ".", "errors", ".", "OutOfRangeError", "as", "e", ":", "\n", "\t                ", "break", "\n", "\n", "", "", "", "if", "len", "(", "data", ")", ">", "0", ":", "\n", "\t        ", "print", "(", "\"Sessions read: {} - Clicks: {} - Repeated Clicks: {}\"", ".", "format", "(", "session_cnt", ",", "len", "(", "data", ")", ",", "repeated", ")", ")", "\n", "", "else", ":", "\n", "\t        ", "print", "(", "'WARNING: NO DATA FOUND!'", ")", "\n", "", "data_df", "=", "pd", ".", "DataFrame", "(", "data", ",", "columns", "=", "[", "'SessionId'", ",", "'ItemId'", ",", "'Time'", "]", ")", "\n", "return", "data_df", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks_data_loader.load_eval_negative_samples": [[12, 17], ["pandas.read_json", "dict"], "function", ["None"], ["def", "load_eval_negative_samples", "(", "eval_sessions_negative_samples_json_path", ")", ":", "\n", "    ", "eval_sessions_neg_samples_df", "=", "pd", ".", "read_json", "(", "eval_sessions_negative_samples_json_path", ",", "lines", "=", "True", ",", "\n", "dtype", "=", "{", "'session_id'", ":", "np", ".", "int64", "}", ")", "\n", "eval_sessions_neg_samples", "=", "dict", "(", "eval_sessions_neg_samples_df", "[", "[", "'session_id'", ",", "'negative_items'", "]", "]", ".", "values", ")", "\n", "return", "eval_sessions_neg_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling_tests.CandidateSamplingManagerTestCase.setUp": [[12, 16], ["numpy.array", "candidate_sampling.CandidateSamplingManager"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "pop_recent_items_buffer", "=", "np", ".", "array", "(", "[", "1", ",", "2", ",", "3", ",", "1", ",", "2", ",", "3", ",", "4", ",", "4", ",", "4", ",", "5", ",", "5", ",", "5", ",", "6", ",", "6", ",", "7", ",", "7", ",", "8", ",", "9", ",", "10", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "self", ".", "get_recent_clicks_buffer_fn", "=", "lambda", ":", "pop_recent_items_buffer", "\n", "self", ".", "sampling_manager", "=", "CandidateSamplingManager", "(", "self", ".", "get_recent_clicks_buffer_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling_tests.CandidateSamplingManagerTestCase.test_get_sample_from_recently_clicked_items_buffer": [[17, 21], ["candidate_sampling_tests.CandidateSamplingManagerTestCase.sampling_manager.get_sample_from_recently_clicked_items_buffer", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertNotIn"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_sample_from_recently_clicked_items_buffer"], ["", "def", "test_get_sample_from_recently_clicked_items_buffer", "(", "self", ")", ":", "\n", "        ", "sample", "=", "self", ".", "sampling_manager", ".", "get_sample_from_recently_clicked_items_buffer", "(", "5", ")", "\n", "self", ".", "assertEqual", "(", "sample", ".", "shape", ",", "(", "5", ",", ")", ",", "\"Should return 5 samples\"", ")", "\n", "self", ".", "assertNotIn", "(", "0", ",", "sample", ",", "\"Should not contain 0 values\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling_tests.CandidateSamplingManagerTestCase.test_get_neg_items_click": [[22, 27], ["candidate_sampling_tests.CandidateSamplingManagerTestCase.sampling_manager.get_neg_items_click", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "numpy.unique"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_click"], ["", "def", "test_get_neg_items_click", "(", "self", ")", ":", "\n", "        ", "valid_samples_session", "=", "[", "1", ",", "2", ",", "2", ",", "4", ",", "4", ",", "5", ",", "4", ",", "3", ",", "2", ",", "16", ",", "4", ",", "8", ",", "6", "]", "\n", "sample", "=", "self", ".", "sampling_manager", ".", "get_neg_items_click", "(", "valid_samples_session", ",", "num_neg_samples", "=", "5", ")", "\n", "self", ".", "assertEqual", "(", "sample", ".", "shape", ",", "(", "5", ",", ")", ",", "\"Should return 5 samples\"", ")", "\n", "self", ".", "assertEqual", "(", "np", ".", "unique", "(", "sample", ")", ".", "shape", ",", "(", "5", ",", ")", ",", "\"Should return 5 unique samples\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling_tests.CandidateSamplingManagerTestCase.test_get_neg_items_click_padding": [[28, 34], ["candidate_sampling_tests.CandidateSamplingManagerTestCase.sampling_manager.get_neg_items_click", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertFalse", "numpy.count_nonzero", "sample[].any"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_click"], ["", "def", "test_get_neg_items_click_padding", "(", "self", ")", ":", "\n", "        ", "valid_samples_session", "=", "[", "1", ",", "2", ",", "2", "]", "\n", "sample", "=", "self", ".", "sampling_manager", ".", "get_neg_items_click", "(", "valid_samples_session", ",", "num_neg_samples", "=", "10", ")", "\n", "self", ".", "assertEqual", "(", "sample", ".", "shape", ",", "(", "10", ",", ")", ",", "\"Should return 10 samples\"", ")", "\n", "self", ".", "assertEqual", "(", "np", ".", "count_nonzero", "(", "sample", ")", ",", "2", ",", "\"Should return 2 unique non-zeroed samples\"", ")", "\n", "self", ".", "assertFalse", "(", "sample", "[", "2", ":", "]", ".", "any", "(", ")", ",", "\"Should return 8 last items zeroed\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling_tests.CandidateSamplingManagerTestCase.test_get_neg_items_session": [[36, 46], ["candidate_sampling_tests.CandidateSamplingManagerTestCase.sampling_manager.get_neg_items_session", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertFalse", "numpy.count_nonzero", "samples[].any", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertNotIn"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_session"], ["", "def", "test_get_neg_items_session", "(", "self", ")", ":", "\n", "        ", "session_item_ids", "=", "[", "1", ",", "2", ",", "3", "]", "\n", "candidate_samples", "=", "[", "1", ",", "3", ",", "5", ",", "7", ",", "9", ",", "11", ",", "13", ",", "15", ",", "18", ",", "20", ",", "9", ",", "11", "]", "\n", "num_neg_samples", "=", "10", "\n", "samples", "=", "self", ".", "sampling_manager", ".", "get_neg_items_session", "(", "session_item_ids", ",", "candidate_samples", ",", "num_neg_samples", ")", "\n", "self", ".", "assertEqual", "(", "samples", ".", "shape", ",", "(", "3", ",", "10", ")", ",", "\"Should return 10 samples\"", ")", "\n", "self", ".", "assertEqual", "(", "np", ".", "count_nonzero", "(", "samples", "==", "0", ")", ",", "2", "*", "3", ",", "\"Should return 6 zeroed samples\"", ")", "\n", "self", ".", "assertFalse", "(", "samples", "[", ":", ",", "-", "2", ":", "]", ".", "any", "(", ")", ",", "\"Should have last 2 columns zeroed\"", ")", "\n", "for", "i", "in", "session_item_ids", ":", "\n", "            ", "self", ".", "assertNotIn", "(", "i", ",", "samples", ",", "\"Should not contain clicked items among negative samples\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling_tests.CandidateSamplingManagerTestCase.test_get_neg_items_session_not_ignore_session_items": [[48, 59], ["candidate_sampling.CandidateSamplingManager", "candidate_sampling.CandidateSamplingManager.get_neg_items_session", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertIn"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_session"], ["", "", "def", "test_get_neg_items_session_not_ignore_session_items", "(", "self", ")", ":", "\n", "        ", "session_item_ids", "=", "[", "1", ",", "2", ",", "3", "]", "\n", "candidate_samples", "=", "[", "1", ",", "3", ",", "5", ",", "7", ",", "9", ",", "2", ",", "13", ",", "15", ",", "18", ",", "20", ",", "9", "]", "\n", "num_neg_samples", "=", "10", "\n", "new_sampling_manager", "=", "CandidateSamplingManager", "(", "self", ".", "get_recent_clicks_buffer_fn", ",", "\n", "ignore_session_items_on_sampling", "=", "False", ")", "\n", "samples", "=", "new_sampling_manager", ".", "get_neg_items_session", "(", "session_item_ids", ",", "candidate_samples", ",", "num_neg_samples", ")", "\n", "self", ".", "assertEqual", "(", "samples", ".", "shape", ",", "(", "3", ",", "10", ")", ",", "\"Should return 10 samples\"", ")", "\n", "\n", "for", "i", "in", "session_item_ids", ":", "\n", "            ", "self", ".", "assertIn", "(", "i", ",", "samples", ",", "\"Should contain clicked items among candidate samples\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling_tests.CandidateSamplingManagerTestCase.test_get_negative_samples": [[61, 72], ["numpy.array", "candidate_sampling_tests.CandidateSamplingManagerTestCase.sampling_manager.get_negative_samples", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertFalse", "zip", "numpy.count_nonzero", "samples[].any", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertTrue", "len", "set().intersection().difference", "set().intersection", "set", "set", "neg_samples.ravel", "session.ravel"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_negative_samples"], ["", "", "def", "test_get_negative_samples", "(", "self", ")", ":", "\n", "        ", "sessions_item_ids", "=", "np", ".", "array", "(", "[", "[", "1", ",", "2", ",", "3", "]", ",", "\n", "[", "4", ",", "0", ",", "0", "]", "]", ")", "\n", "candidate_samples", "=", "[", "1", ",", "3", ",", "5", ",", "7", ",", "9", ",", "11", ",", "13", ",", "15", ",", "18", ",", "20", ",", "9", ",", "11", "]", "\n", "num_neg_samples", "=", "10", "\n", "samples", "=", "self", ".", "sampling_manager", ".", "get_negative_samples", "(", "sessions_item_ids", ",", "candidate_samples", ",", "num_neg_samples", ")", "\n", "self", ".", "assertEqual", "(", "samples", ".", "shape", ",", "(", "2", ",", "3", ",", "10", ")", ",", "\"Should return 2 sessions with at most 3 clicks and up to 10 neg. samples\"", ")", "\n", "self", ".", "assertEqual", "(", "np", ".", "count_nonzero", "(", "samples", "==", "0", ")", ",", "2", "*", "10", "+", "2", "*", "3", ",", "\"Should return 6 zeroed samples\"", ")", "\n", "self", ".", "assertFalse", "(", "samples", "[", "1", ",", "-", "2", ":", "]", ".", "any", "(", ")", ",", "\"Should have last 2 rows zeroed\"", ")", "\n", "for", "session", ",", "neg_samples", "in", "zip", "(", "sessions_item_ids", ",", "samples", ")", ":", "\n", "            ", "self", ".", "assertTrue", "(", "len", "(", "set", "(", "session", ".", "ravel", "(", ")", ")", ".", "intersection", "(", "set", "(", "neg_samples", ".", "ravel", "(", ")", ")", ")", ".", "difference", "(", "{", "0", "}", ")", ")", "==", "0", ",", "\"Should not contain clicked items among negative samples\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling_tests.CandidateSamplingManagerTestCase.test_get_batch_negative_samples_by_session": [[74, 87], ["numpy.array", "candidate_sampling_tests.CandidateSamplingManagerTestCase.sampling_manager.get_batch_negative_samples_by_session", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertFalse", "zip", "samples[].any", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertTrue", "len", "set().intersection().difference", "set().intersection", "set", "set", "neg_samples.ravel", "session.ravel"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_batch_negative_samples_by_session"], ["", "", "def", "test_get_batch_negative_samples_by_session", "(", "self", ")", ":", "\n", "        ", "sessions_item_ids", "=", "np", ".", "array", "(", "[", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ",", "\n", "[", "4", ",", "5", ",", "6", ",", "7", ",", "0", "]", "]", ")", "\n", "candidate_samples", "=", "[", "2", ",", "2", ",", "3", ",", "3", ",", "4", ",", "4", ",", "4", ",", "5", ",", "5", ",", "5", ",", "5", ",", "6", ",", "7", ",", "10", ",", "10", ",", "10", ",", "11", ",", "11", ",", "12", ",", "12", ",", "13", ",", "15", "]", "\n", "num_neg_samples", "=", "3", "\n", "samples", "=", "self", ".", "sampling_manager", ".", "get_batch_negative_samples_by_session", "(", "sessions_item_ids", ",", "candidate_samples", ",", "\n", "num_negative_samples", "=", "num_neg_samples", ",", "\n", "first_sampling_multiplying_factor", "=", "2", ")", "\n", "\n", "self", ".", "assertEqual", "(", "samples", ".", "shape", ",", "(", "2", ",", "5", ",", "3", ")", ",", "\"Should return 2 sessions with at most 5 clicks and up to 3 neg. samples\"", ")", "\n", "self", ".", "assertFalse", "(", "samples", "[", "1", ",", "-", "1", "]", ".", "any", "(", ")", ",", "\"Should have last session with no neg sample\"", ")", "\n", "for", "session", ",", "neg_samples", "in", "zip", "(", "sessions_item_ids", ",", "samples", ")", ":", "\n", "            ", "self", ".", "assertTrue", "(", "len", "(", "set", "(", "session", ".", "ravel", "(", ")", ")", ".", "intersection", "(", "set", "(", "neg_samples", ".", "ravel", "(", ")", ")", ")", ".", "difference", "(", "{", "0", "}", ")", ")", "==", "0", ",", "\"Should not contain clicked items among negative samples\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling_tests.CandidateSamplingManagerTestCase.test_get_batch_negative_samples": [[89, 101], ["numpy.array", "candidate_sampling_tests.CandidateSamplingManagerTestCase.sampling_manager.get_batch_negative_samples", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertEqual", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertFalse", "zip", "samples[].any", "candidate_sampling_tests.CandidateSamplingManagerTestCase.assertTrue", "len", "set().intersection().difference", "set().intersection", "set", "set", "neg_samples.ravel", "session.ravel"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_batch_negative_samples"], ["", "", "def", "test_get_batch_negative_samples", "(", "self", ")", ":", "\n", "        ", "sessions_item_ids", "=", "np", ".", "array", "(", "[", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ",", "\n", "[", "4", ",", "5", ",", "6", ",", "7", ",", "0", "]", "]", ")", "\n", "negative_samples_by_session", "=", "4", "\n", "negative_sample_from_buffer", "=", "10", "\n", "samples", "=", "self", ".", "sampling_manager", ".", "get_batch_negative_samples", "(", "sessions_item_ids", ",", "\n", "negative_samples_by_session", ",", "negative_sample_from_buffer", ")", "\n", "\n", "self", ".", "assertEqual", "(", "samples", ".", "shape", ",", "(", "2", ",", "5", ",", "4", ")", ",", "\"Should return 2 sessions with at most 5 clicks and up to 4 neg. samples\"", ")", "\n", "self", ".", "assertFalse", "(", "samples", "[", "1", ",", "-", "1", "]", ".", "any", "(", ")", ",", "\"Should have last session with no neg sample\"", ")", "\n", "for", "session", ",", "neg_samples", "in", "zip", "(", "sessions_item_ids", ",", "samples", ")", ":", "\n", "            ", "self", ".", "assertTrue", "(", "len", "(", "set", "(", "session", ".", "ravel", "(", ")", ")", ".", "intersection", "(", "set", "(", "neg_samples", ".", "ravel", "(", ")", ")", ")", ".", "difference", "(", "{", "0", "}", ")", ")", "==", "0", ",", "\"Should not contain clicked items among negative samples\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.__init__": [[32, 59], ["benchmarks.BenchmarkRecommender.__init__", "getattr", "collections.namedtuple", "dict", "session_knn.SessionBasedKNNRecommender.get_clf_suffix", "session_knn.SessionBasedKNNRecommender.get_clf_suffix", "session_knn.SessionBasedKNNRecommender.get_clf_suffix", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix"], ["    ", "def", "__init__", "(", "self", ",", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", ":", "\n", "#super(Instructor, self).__init__(name, year) #Python 2", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", "\n", "\n", "\n", "self", ".", "sessions_buffer_size", "=", "params", "[", "'sessions_buffer_size'", "]", "#Buffer size for last processed sessions", "\n", "self", ".", "candidate_sessions_sample_size", "=", "params", "[", "'candidate_sessions_sample_size'", "]", "#Number of candidate near sessions to sample", "\n", "self", ".", "sampling_strategy", "=", "params", "[", "'sampling_strategy'", "]", "#(recent,random)", "\n", "self", ".", "nearest_neighbor_session_for_scoring", "=", "params", "[", "'nearest_neighbor_session_for_scoring'", "]", "#Nearest neighbors to compute scores        ", "\n", "\n", "self", ".", "similarity", "=", "params", "[", "'similarity'", "]", "#(jaccard, cosine)", "\n", "\n", "#Decays weight of first user clicks in active session when finding neighbor sessions (same, div, linear, log, quadradic)", "\n", "self", ".", "first_session_clicks_decay", "=", "params", "[", "'first_session_clicks_decay'", "]", "\n", "self", ".", "first_session_clicks_decay_fn", "=", "getattr", "(", "self", ",", "'{}_pos_decay'", ".", "format", "(", "self", ".", "first_session_clicks_decay", ")", ")", "\n", "\n", "#Registering a state for this recommender which persists over TF Estimator train/eval loops", "\n", "if", "not", "self", ".", "get_clf_suffix", "(", ")", "in", "clicked_items_state", ".", "benchmarks_states", ":", "\n", "            ", "clicked_items_state", ".", "benchmarks_states", "[", "self", ".", "get_clf_suffix", "(", ")", "]", "=", "dict", "(", "{", "'last_sessions_buffer'", ":", "[", "]", ",", "\n", "'item_session_map'", ":", "defaultdict", "(", "set", ")", "}", ")", "\n", "\n", "", "state", "=", "clicked_items_state", ".", "benchmarks_states", "[", "self", ".", "get_clf_suffix", "(", ")", "]", "\n", "self", ".", "last_sessions_buffer", "=", "state", "[", "'last_sessions_buffer'", "]", "# Tuple: (session_id (int), item_ids (list of ints))", "\n", "self", ".", "item_session_map", "=", "state", "[", "'item_session_map'", "]", "#Dict: item -> list of session_ids", "\n", "\n", "self", ".", "SessionStruct", "=", "namedtuple", "(", "'SessionStruct'", ",", "[", "'session_id'", ",", "'item_ids'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.get_clf_suffix": [[60, 62], ["None"], "methods", ["None"], ["", "def", "get_clf_suffix", "(", "self", ")", ":", "\n", "        ", "return", "'sknn'", "if", "self", ".", "first_session_clicks_decay", "==", "'same'", "else", "'v-sknn'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.get_description": [[63, 65], ["None"], "methods", ["None"], ["", "def", "get_description", "(", "self", ")", ":", "\n", "        ", "return", "'Session-KNN: Retrieves items from similar sessions (kNN) from last sessions buffer'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.train": [[66, 72], ["numpy.hstack", "list", "session_knn.SessionBasedKNNRecommender.add_sessions_to_buffer", "set", "filter"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.add_sessions_to_buffer"], ["", "def", "train", "(", "self", ",", "users_ids", ",", "sessions_ids", ",", "sessions_items", ",", "sessions_next_items", ")", ":", "\n", "        ", "sessions_all_items", "=", "np", ".", "hstack", "(", "[", "sessions_items", ",", "sessions_next_items", "]", ")", "\n", "session_items_sets", "=", "list", "(", "[", "set", "(", "filter", "(", "lambda", "x", ":", "x", "!=", "0", ",", "session_items", ")", ")", "for", "session_items", "in", "sessions_all_items", "]", ")", "\n", "#Add sessions to buffer and removes older ones", "\n", "self", ".", "add_sessions_to_buffer", "(", "sessions_ids", ",", "session_items_sets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.predict": [[74, 94], ["numpy.zeros", "enumerate", "enumerate", "session_knn.SessionBasedKNNRecommender.find_neighbors", "session_knn.SessionBasedKNNRecommender.score_items", "list", "list", "map", "session_knn.SessionBasedKNNRecommender._get_top_n_valid_items", "sorted", "session_knn.SessionBasedKNNRecommender.items"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.find_neighbors", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.score_items", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender._get_top_n_valid_items"], ["", "def", "predict", "(", "self", ",", "users_ids", ",", "sessions_items", ",", "topk", "=", "5", ",", "valid_items", "=", "None", ")", ":", "\n", "        ", "session_predictions", "=", "np", ".", "zeros", "(", "dtype", "=", "np", ".", "int64", ",", "\n", "shape", "=", "[", "sessions_items", ".", "shape", "[", "0", "]", ",", "\n", "sessions_items", ".", "shape", "[", "1", "]", ",", "\n", "topk", "]", ")", "\n", "\n", "\n", "for", "row_idx", ",", "session_items", "in", "enumerate", "(", "sessions_items", ")", ":", "\n", "\n", "            ", "for", "col_idx", ",", "item", "in", "enumerate", "(", "session_items", ")", ":", "\n", "                ", "if", "item", "!=", "0", ":", "\n", "                    ", "partial_session_items", "=", "session_items", "[", ":", "col_idx", "+", "1", "]", "\n", "#Get a sample of k-nearest neighbor sessions  ", "\n", "#based on the session clicks up to this point                    ", "\n", "neighbor_sessions_similarity", "=", "self", ".", "find_neighbors", "(", "partial_session_items", ")", "\n", "item_scores", "=", "self", ".", "score_items", "(", "neighbor_sessions_similarity", ")", "\n", "items_ids_ranked", "=", "list", "(", "map", "(", "lambda", "y", ":", "y", "[", "0", "]", ",", "sorted", "(", "item_scores", ".", "items", "(", ")", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", ")", ")", "\n", "session_predictions", "[", "row_idx", ",", "col_idx", "]", "=", "list", "(", "self", ".", "_get_top_n_valid_items", "(", "items_ids_ranked", ",", "topk", ",", "valid_items", "[", "row_idx", ",", "col_idx", "]", ")", ")", "\n", "\n", "", "", "", "return", "session_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.add_sessions_to_buffer": [[96, 115], ["list", "session_knn.SessionBasedKNNRecommender.last_sessions_buffer.extend", "zip", "len", "session_knn.SessionBasedKNNRecommender.SessionStruct", "session_knn.SessionBasedKNNRecommender.item_session_map[].add", "session_knn.SessionBasedKNNRecommender.item_session_map[].discard", "zip"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add"], ["", "def", "add_sessions_to_buffer", "(", "self", ",", "sessions_ids", ",", "sessions_items_sets", ")", ":", "\n", "        ", "new_sessions", "=", "list", "(", "[", "self", ".", "SessionStruct", "(", "session_id", "=", "session_id", ",", "\n", "item_ids", "=", "items_set", ")", "for", "session_id", ",", "items_set", "in", "zip", "(", "sessions_ids", ",", "sessions_items_sets", ")", "]", ")", "\n", "self", ".", "last_sessions_buffer", ".", "extend", "(", "new_sessions", ")", "\n", "\n", "#Create a map from item_id to session ids where this item was clicked", "\n", "#to speed KNN search", "\n", "for", "session_id", ",", "session_items", "in", "zip", "(", "sessions_ids", ",", "sessions_items_sets", ")", ":", "\n", "            ", "for", "item_id", "in", "session_items", ":", "\n", "                ", "self", ".", "item_session_map", "[", "item_id", "]", ".", "add", "(", "session_id", ")", "\n", "\n", "#Keeping buffer on specified size", "\n", "", "", "while", "len", "(", "self", ".", "last_sessions_buffer", ")", ">", "self", ".", "sessions_buffer_size", ":", "\n", "            ", "session_idx_to_remove", "=", "0", "\n", "session_to_remove", "=", "self", ".", "last_sessions_buffer", "[", "session_idx_to_remove", "]", "\n", "for", "item_id", "in", "session_to_remove", ".", "item_ids", ":", "\n", "                ", "self", ".", "item_session_map", "[", "item_id", "]", ".", "discard", "(", "session_to_remove", ".", "session_id", ")", "\n", "", "del", "self", ".", "last_sessions_buffer", "[", "session_idx_to_remove", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.find_session_on_buffer": [[116, 134], ["len", "len"], "methods", ["None"], ["", "", "def", "find_session_on_buffer", "(", "self", ",", "session_id", ")", ":", "\n", "        ", "\"\"\"\n        Binary search (leftmost value) on the first element of list of tuples (session_id)\n        \"\"\"", "\n", "arr", "=", "self", ".", "last_sessions_buffer", "\n", "\n", "left", "=", "0", "\n", "right", "=", "len", "(", "arr", ")", "\n", "while", "left", "<", "right", ":", "\n", "            ", "mid", "=", "(", "left", "+", "right", ")", "//", "2", "\n", "if", "session_id", ">", "arr", "[", "mid", "]", ".", "session_id", ":", "\n", "                ", "left", "=", "mid", "+", "1", "\n", "", "else", ":", "\n", "                ", "right", "=", "mid", "\n", "", "", "if", "left", "!=", "len", "(", "arr", ")", "and", "arr", "[", "left", "]", ".", "session_id", "==", "session_id", ":", "\n", "            ", "return", "left", "\n", "", "else", ":", "\n", "            ", "return", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.get_session_items_from_buffer": [[135, 142], ["session_knn.SessionBasedKNNRecommender.find_session_on_buffer", "set"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.find_session_on_buffer"], ["", "", "def", "get_session_items_from_buffer", "(", "self", ",", "session_id", ")", ":", "\n", "        ", "idx", "=", "self", ".", "find_session_on_buffer", "(", "session_id", ")", "\n", "\n", "if", "idx", ">=", "0", ":", "\n", "            ", "return", "self", ".", "last_sessions_buffer", "[", "idx", "]", ".", "item_ids", "\n", "", "else", ":", "\n", "            ", "return", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.get_sessions_with_item": [[143, 156], ["None"], "methods", ["None"], ["", "", "def", "get_sessions_with_item", "(", "self", ",", "item_id", ")", ":", "\n", "        ", "'''\n        Returns all session for an item\n        \n        Parameters\n        --------\n        item: Id of the item session\n        \n        Returns \n        --------\n        out : set           \n        '''", "\n", "return", "self", ".", "item_session_map", "[", "item_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.find_neighbors": [[160, 182], ["session_knn.SessionBasedKNNRecommender.candidate_neighbor_sessions", "session_knn.SessionBasedKNNRecommender.calc_neighbor_sessions_scores", "sorted", "list", "filter"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.candidate_neighbor_sessions", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.calc_neighbor_sessions_scores"], ["", "def", "find_neighbors", "(", "self", ",", "session_items", ")", ":", "\n", "        ", "'''\n        Finds the k nearest neighbors for the given session_id and the current item input_item_id. \n        \n        Parameters\n        --------\n        session_items: set of item ids\n        input_item_id: int \n        session_id: int\n        \n        Returns \n        --------\n        out : list of tuple (session_id, similarity)           \n        '''", "\n", "candidate_neighbors_sessions", "=", "self", ".", "candidate_neighbor_sessions", "(", "session_items", ")", "\n", "candidate_neighbors_sessions_scores", "=", "self", ".", "calc_neighbor_sessions_scores", "(", "session_items", ",", "candidate_neighbors_sessions", ")", "\n", "\n", "candidate_neighbors_sessions_sorted", "=", "sorted", "(", "candidate_neighbors_sessions_scores", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "candidate_neighbors_sessions_filtered", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", "[", "1", "]", ">", "0.0", "and", "x", "[", "1", "]", "<", "1.0", ",", "candidate_neighbors_sessions_sorted", ")", ")", "\n", "nearest_neighbors", "=", "candidate_neighbors_sessions_filtered", "[", ":", "self", ".", "nearest_neighbor_session_for_scoring", "]", "\n", "\n", "return", "nearest_neighbors", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.candidate_neighbor_sessions": [[183, 214], ["list", "len", "session_knn.SessionBasedKNNRecommender.get_sessions_with_item", "sorted", "random.sample", "session_knn.SessionBasedKNNRecommender.find_session_on_buffer"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.get_sessions_with_item", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.find_session_on_buffer"], ["", "def", "candidate_neighbor_sessions", "(", "self", ",", "session_items", ")", ":", "\n", "        ", "'''\n        Find a set of session to later on find neighbors in.\n        A self.sample_size of 0 uses all sessions in which any item of the current session appears.\n        self.sampling_strategy can be performed with the options \"recent\" or \"random\".\n        \"recent\" selects the self.sample_size most recent sessions while \"random\" just choses randomly. \n        \n        Parameters\n        --------\n        sessions: set of session ids\n        \n        Returns \n        --------\n        out : set           \n        '''", "\n", "\n", "# Retrieving all sessions from buffer which contains at least one item from the current sessions", "\n", "candidate_sessions_ids", "=", "list", "(", "[", "session_id", "for", "item_id", "in", "session_items", "for", "session_id", "in", "self", ".", "get_sessions_with_item", "(", "item_id", ")", "if", "self", ".", "find_session_on_buffer", "(", "session_id", ")", "!=", "-", "1", "]", ")", "\n", "\n", "if", "self", ".", "candidate_sessions_sample_size", ">", "0", "and", "len", "(", "candidate_sessions_ids", ")", ">", "self", ".", "candidate_sessions_sample_size", ":", "\n", "\n", "            ", "if", "self", ".", "sampling_strategy", "==", "'recent'", ":", "\n", "#As session_ids are the equivalent to timestamp of the first click in the session, ordering their ids is equivalent to sort by timestamp", "\n", "                ", "candidate_sessions_ids", "=", "sorted", "(", "candidate_sessions_ids", ",", "reverse", "=", "True", ")", "[", ":", "self", ".", "candidate_sessions_sample_size", "]", "\n", "", "elif", "self", ".", "sampling_strategy", "==", "'random'", ":", "\n", "                ", "candidate_sessions_ids", "=", "random", ".", "sample", "(", "candidate_sessions_ids", ",", "self", ".", "candidate_sessions_sample_size", ")", "\n", "\n", "", "", "return", "candidate_sessions_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.score_items": [[216, 237], ["collections.defaultdict", "session_knn.SessionBasedKNNRecommender.get_session_items_from_buffer"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.get_session_items_from_buffer"], ["", "def", "score_items", "(", "self", ",", "neighbor_sessions_similarity", ")", ":", "\n", "        ", "'''\n        Compute a set of scores for all items given a set of neighbors.\n        \n        Parameters\n        --------\n        neighbors: set of session ids\n        \n        Returns \n        --------\n        out : list of tuple (item, score)           \n        '''", "\n", "# now we have the set of relevant items to make predictions", "\n", "item_scores", "=", "defaultdict", "(", "int", ")", "\n", "# iterate over the sessions", "\n", "for", "session_id", ",", "similarity", "in", "neighbor_sessions_similarity", ":", "\n", "# get the items in this session                ", "\n", "            ", "for", "item", "in", "self", ".", "get_session_items_from_buffer", "(", "session_id", ")", ":", "\n", "                ", "item_scores", "[", "item", "]", "+=", "similarity", "\n", "\n", "", "", "return", "item_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.calc_neighbor_sessions_scores": [[239, 267], ["session_knn.SessionBasedKNNRecommender.get_session_items_from_buffer", "session_knn.SessionBasedKNNRecommender.score_neighbor_sessions", "neighbors.append"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.get_session_items_from_buffer", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.score_neighbor_sessions"], ["", "def", "calc_neighbor_sessions_scores", "(", "self", ",", "session_items", ",", "sessions_ids", ")", ":", "\n", "        ", "'''\n        Calculates the configured similarity for the items in session_items and each session in sessions.\n        \n        Parameters\n        --------\n        session_items: set of item ids\n        sessions: list of session ids\n        \n        Returns \n        --------\n        out : list of tuple (session_id,similarity)           \n        '''", "\n", "\n", "\n", "#print 'nb of sessions to test ', len(sessionsToTest), ' metric: ', self.metric", "\n", "neighbors", "=", "[", "]", "\n", "cnt", "=", "0", "\n", "for", "session", "in", "sessions_ids", ":", "\n", "            ", "cnt", "+=", "1", "\n", "# get items of the session, look up the cache first ", "\n", "neighbor_session_items", "=", "self", ".", "get_session_items_from_buffer", "(", "session", ")", "\n", "\n", "neighbor_session_score", "=", "self", ".", "score_neighbor_sessions", "(", "session_items", ",", "neighbor_session_items", ")", "\n", "if", "neighbor_session_score", ">", "0", ":", "\n", "                ", "neighbors", ".", "append", "(", "(", "session", ",", "neighbor_session_score", ")", ")", "\n", "\n", "", "", "return", "neighbors", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.score_neighbor_sessions": [[268, 289], ["set", "len", "session_knn.SessionBasedKNNRecommender.decay_score", "session_knn.SessionBasedKNNRecommender.cosine_denominator", "Exception", "session_knn.SessionBasedKNNRecommender.cosine_denominator", "Exception", "session_knn.SessionBasedKNNRecommender.jaccard_denominator", "session_knn.SessionBasedKNNRecommender.jaccard_denominator"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.decay_score", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.cosine_denominator", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.cosine_denominator", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.jaccard_denominator", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.jaccard_denominator"], ["", "def", "score_neighbor_sessions", "(", "self", ",", "session_items", ",", "neighbor_session_items", ")", ":", "\n", "        ", "session_items_set", "=", "set", "(", "session_items", ")", "\n", "score", "=", "0", "\n", "if", "self", ".", "first_session_clicks_decay", "==", "'same'", ":", "\n", "            ", "intersection", "=", "len", "(", "session_items_set", "&", "neighbor_session_items", ")", "\n", "if", "self", ".", "similarity", "==", "'cosine'", ":", "\n", "                ", "score", "=", "intersection", "/", "self", ".", "cosine_denominator", "(", "session_items_set", ",", "neighbor_session_items", ")", "\n", "", "elif", "self", ".", "similarity", "==", "'jaccard'", ":", "\n", "                ", "score", "=", "intersection", "/", "self", ".", "jaccard_denominator", "(", "session_items_set", ",", "neighbor_session_items", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "\"{} is not a valid similarity (cosine, jaccard)\"", ".", "format", "(", "self", ".", "similarity", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "decay_score", "=", "self", ".", "decay_score", "(", "session_items", ",", "neighbor_session_items", ")", "\n", "\n", "if", "self", ".", "similarity", "==", "'cosine'", ":", "\n", "                ", "score", "=", "decay_score", "/", "self", ".", "cosine_denominator", "(", "session_items_set", ",", "neighbor_session_items", ")", "\n", "", "elif", "self", ".", "similarity", "==", "'jaccard'", ":", "\n", "                ", "score", "=", "decay_score", "/", "self", ".", "jaccard_denominator", "(", "session_items_set", ",", "neighbor_session_items", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "\"{} is not a valid similarity (cosine, jaccard)\"", ".", "format", "(", "self", ".", "similarity", ")", ")", "\n", "", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.decay_score": [[290, 297], ["enumerate", "reversed", "session_knn.SessionBasedKNNRecommender.first_session_clicks_decay_fn"], "methods", ["None"], ["", "def", "decay_score", "(", "self", ",", "session_items", ",", "neighbor_session_items", ")", ":", "\n", "        ", "score", "=", "0", "\n", "#Loops over the reversed list of session items, to give more weight to last clicks", "\n", "for", "position", ",", "item", "in", "enumerate", "(", "reversed", "(", "session_items", ")", ")", ":", "\n", "            ", "if", "item", "in", "neighbor_session_items", ":", "\n", "                ", "score", "+=", "self", ".", "first_session_clicks_decay_fn", "(", "position", "+", "1", ")", "\n", "", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.jaccard_denominator": [[299, 302], ["len"], "methods", ["None"], ["", "def", "jaccard_denominator", "(", "self", ",", "first", ",", "second", ")", ":", "\n", "        ", "union", "=", "len", "(", "first", "|", "second", ")", "\n", "return", "union", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.cosine_denominator": [[303, 305], ["math.sqrt", "math.sqrt", "len", "len"], "methods", ["None"], ["", "def", "cosine_denominator", "(", "self", ",", "first", ",", "second", ")", ":", "\n", "        ", "return", "sqrt", "(", "len", "(", "first", ")", ")", "*", "sqrt", "(", "len", "(", "second", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.same_pos_decay": [[306, 308], ["None"], "methods", ["None"], ["", "def", "same_pos_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.div_pos_decay": [[309, 311], ["None"], "methods", ["None"], ["", "def", "div_pos_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "/", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.linear_pos_decay": [[312, 314], ["None"], "methods", ["None"], ["", "def", "linear_pos_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "-", "(", "0.1", "*", "i", ")", "if", "i", "<=", "100", "else", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.log_pos_decay": [[315, 317], ["log10"], "methods", ["None"], ["", "def", "log_pos_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "/", "(", "log10", "(", "i", "+", "1.7", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.session_knn.SessionBasedKNNRecommender.quadratic_pos_decay": [[318, 320], ["None"], "methods", ["None"], ["", "def", "quadratic_pos_decay", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "1", "/", "(", "i", "*", "i", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.content_based.ContentBasedRecommender.__init__": [[12, 15], ["benchmarks.BenchmarkRecommender.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", ":", "\n", "#super(Instructor, self).__init__(name, year) #Python 2", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.content_based.ContentBasedRecommender.get_clf_suffix": [[16, 18], ["None"], "methods", ["None"], ["", "def", "get_clf_suffix", "(", "self", ")", ":", "\n", "        ", "return", "'cb'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.content_based.ContentBasedRecommender.get_description": [[19, 21], ["None"], "methods", ["None"], ["", "def", "get_description", "(", "self", ")", ":", "\n", "        ", "return", "'Content-Based similarity'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.content_based.ContentBasedRecommender.train": [[22, 24], ["None"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "users_ids", ",", "sessions_ids", ",", "sessions_items", ",", "sessions_next_items", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.content_based.ContentBasedRecommender.predict": [[25, 58], ["content_based.ContentBasedRecommender.clicked_items_state.get_recent_clicks_buffer", "numpy.zeros", "enumerate", "numpy.unique", "numpy.unique", "enumerate", "list", "sklearn.metrics.pairwise.cosine_similarity", "numpy.argsort", "content_based.ContentBasedRecommender._get_top_n_valid_items", "numpy.nonzero", "acr_embeddings[].reshape"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_recent_clicks_buffer", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender._get_top_n_valid_items"], ["", "def", "predict", "(", "self", ",", "users_ids", ",", "sessions_items", ",", "topk", "=", "5", ",", "valid_items", "=", "None", ")", ":", "\n", "        ", "acr_embeddings", "=", "self", ".", "eval_benchmark_params", "[", "'content_article_embeddings_matrix'", "]", "\n", "\n", "recent_items_buffer", "=", "self", ".", "clicked_items_state", ".", "get_recent_clicks_buffer", "(", ")", "\n", "if", "valid_items", "is", "None", ":", "\n", "            ", "recent_unique_item_ids", "=", "np", ".", "unique", "(", "[", "recent_items_buffer", "[", "np", ".", "nonzero", "(", "recent_items_buffer", ")", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "recent_unique_item_ids", "=", "np", ".", "unique", "(", "valid_items", ")", "\n", "\n", "", "acr_embeddings_recent_items", "=", "acr_embeddings", "[", "recent_unique_item_ids", "]", "\n", "\n", "\n", "session_predictions", "=", "np", ".", "zeros", "(", "dtype", "=", "np", ".", "int64", ",", "\n", "shape", "=", "[", "sessions_items", ".", "shape", "[", "0", "]", ",", "\n", "sessions_items", ".", "shape", "[", "1", "]", ",", "\n", "topk", "]", ")", "\n", "\n", "for", "row_idx", ",", "session_items", "in", "enumerate", "(", "sessions_items", ")", ":", "\n", "\n", "            ", "for", "col_idx", ",", "item", "in", "enumerate", "(", "session_items", ")", ":", "\n", "                ", "if", "item", "!=", "0", ":", "\n", "\n", "#Computing cosine similarity between this item and all recent items (from buffer)", "\n", "#P.s. Do not need to ignore the current item (whose similarity is always, because this item will not be among the valid items (next click + negative samples not present in the session))", "\n", "                    ", "similarities", "=", "cosine_similarity", "(", "acr_embeddings", "[", "item", "]", ".", "reshape", "(", "1", ",", "-", "1", ")", ",", "\n", "acr_embeddings_recent_items", ")", "[", "0", "]", "\n", "similar_items_sorted_idx", "=", "np", ".", "argsort", "(", "similarities", ",", "axis", "=", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "similar_items_ids", "=", "recent_unique_item_ids", "[", "similar_items_sorted_idx", "]", "\n", "\n", "session_predictions", "[", "row_idx", ",", "col_idx", "]", "=", "list", "(", "self", ".", "_get_top_n_valid_items", "(", "similar_items_ids", ",", "topk", ",", "valid_items", "[", "row_idx", ",", "col_idx", "]", ")", ")", "\n", "\n", "\n", "", "", "", "return", "session_predictions", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_cooccurrences.ItemCooccurrenceRecommender.__init__": [[13, 16], ["benchmarks.BenchmarkRecommender.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", ":", "\n", "#super(Instructor, self).__init__(name, year) #Python 2", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "clicked_items_state", ",", "params", ",", "eval_streaming_metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_cooccurrences.ItemCooccurrenceRecommender.get_clf_suffix": [[17, 19], ["None"], "methods", ["None"], ["", "def", "get_clf_suffix", "(", "self", ")", ":", "\n", "        ", "return", "'coocurrent'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_cooccurrences.ItemCooccurrenceRecommender.get_description": [[20, 22], ["None"], "methods", ["None"], ["", "def", "get_description", "(", "self", ")", ":", "\n", "        ", "return", "'Most co-ocurrent in sessions'", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_cooccurrences.ItemCooccurrenceRecommender.train": [[23, 25], ["None"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "users_ids", ",", "sessions_ids", ",", "sessions_items", ",", "sessions_next_items", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.item_cooccurrences.ItemCooccurrenceRecommender.predict": [[26, 42], ["numpy.zeros", "enumerate", "item_cooccurrences.ItemCooccurrenceRecommender.clicked_items_state.get_items_coocurrences", "enumerate", "utils.max_n_sparse_indexes", "list", "item_cooccurrences.ItemCooccurrenceRecommender._get_top_n_valid_items", "len", "utils.max_n_sparse_indexes.tolist"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_items_coocurrences", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.utils.max_n_sparse_indexes", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender._get_top_n_valid_items"], ["", "def", "predict", "(", "self", ",", "users_ids", ",", "sessions_items", ",", "topk", "=", "5", ",", "valid_items", "=", "None", ")", ":", "\n", "        ", "session_predictions", "=", "np", ".", "zeros", "(", "dtype", "=", "np", ".", "int64", ",", "\n", "shape", "=", "[", "sessions_items", ".", "shape", "[", "0", "]", ",", "\n", "sessions_items", ".", "shape", "[", "1", "]", ",", "\n", "topk", "]", ")", "\n", "\n", "for", "row_idx", ",", "session_items", "in", "enumerate", "(", "sessions_items", ")", ":", "\n", "\n", "            ", "session_item_coocurrences", "=", "self", ".", "clicked_items_state", ".", "get_items_coocurrences", "(", ")", "\n", "for", "col_idx", ",", "item", "in", "enumerate", "(", "session_items", ")", ":", "\n", "                ", "if", "item", "!=", "0", ":", "\n", "                    ", "item_coocurrences", "=", "session_item_coocurrences", "[", "item", "]", "\n", "sorted_items", "=", "max_n_sparse_indexes", "(", "item_coocurrences", ".", "data", ",", "item_coocurrences", ".", "indices", ",", "topn", "=", "len", "(", "item_coocurrences", ".", "data", ")", ")", "\n", "session_predictions", "[", "row_idx", ",", "col_idx", "]", "=", "list", "(", "self", ".", "_get_top_n_valid_items", "(", "sorted_items", ".", "tolist", "(", ")", ",", "topk", ",", "valid_items", "[", "row_idx", ",", "col_idx", "]", ")", ")", "\n", "\n", "", "", "", "return", "session_predictions", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.__init__": [[14, 18], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "clicked_items_state", ",", "eval_benchmark_params", ",", "eval_streaming_metrics", ")", ":", "\n", "        ", "self", ".", "clicked_items_state", "=", "clicked_items_state", "\n", "self", ".", "eval_benchmark_params", "=", "eval_benchmark_params", "\n", "self", ".", "streaming_metrics", "=", "eval_streaming_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix": [[19, 21], ["None"], "methods", ["None"], ["", "def", "get_clf_suffix", "(", "self", ")", ":", "\n", "        ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_description": [[22, 24], ["None"], "methods", ["None"], ["", "def", "get_description", "(", "self", ")", ":", "\n", "        ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.reset_eval_metrics": [[25, 28], ["metric.reset"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.reset"], ["", "def", "reset_eval_metrics", "(", "self", ")", ":", "\n", "        ", "for", "metric", "in", "self", ".", "streaming_metrics", ":", "\n", "            ", "metric", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.train": [[29, 31], ["None"], "methods", ["None"], ["", "", "def", "train", "(", "self", ",", "users_ids", ",", "sessions_ids", ",", "sessions_items", ",", "sessions_next_items", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.predict": [[32, 34], ["None"], "methods", ["None"], ["", "def", "predict", "(", "self", ",", "users_ids", ",", "sessions_items", ",", "topk", "=", "5", ",", "valid_items", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.evaluate": [[35, 56], ["numpy.expand_dims", "numpy.concatenate", "benchmarks.BenchmarkRecommender.predict", "evaluation.update_metrics", "evaluation.compute_metrics_results", "benchmarks.BenchmarkRecommender.clicked_items_state.get_articles_recent_pop_norm", "benchmarks.BenchmarkRecommender.clicked_items_state.get_articles_recent_pop_norm", "benchmarks.BenchmarkRecommender.get_clf_suffix", "benchmarks.BenchmarkRecommender.get_clf_suffix"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.predict", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.update_metrics", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.compute_metrics_results", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.get_clf_suffix"], ["", "def", "evaluate", "(", "self", ",", "users_ids", ",", "sessions_items", ",", "sessions_next_items", ",", "topk", "=", "5", ",", "eval_negative_items", "=", "None", ")", ":", "\n", "        ", "sessions_next_items_expanded", "=", "np", ".", "expand_dims", "(", "sessions_next_items", ",", "axis", "=", "2", ")", "\n", "\n", "\n", "eval_negative_items_expanded", "=", "eval_negative_items", "\n", "\n", "valid_items", "=", "np", ".", "concatenate", "(", "[", "sessions_next_items_expanded", ",", "eval_negative_items_expanded", "]", ",", "axis", "=", "2", ")", "\n", "\n", "preds", "=", "self", ".", "predict", "(", "users_ids", ",", "sessions_items", ",", "topk", "=", "topk", ",", "valid_items", "=", "valid_items", ")", "\n", "\n", "preds_norm_pop", "=", "self", ".", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", "[", "preds", "]", "\n", "\n", "labels_norm_pop", "=", "self", ".", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", "[", "sessions_next_items", "]", "\n", "\n", "update_metrics", "(", "preds", ",", "sessions_next_items", ",", "labels_norm_pop", ",", "preds_norm_pop", ",", "sessions_items", ",", "self", ".", "streaming_metrics", ",", "\n", "recommender", "=", "self", ".", "get_clf_suffix", "(", ")", ")", "\n", "\n", "metrics_values", "=", "compute_metrics_results", "(", "self", ".", "streaming_metrics", ",", "\n", "recommender", "=", "self", ".", "get_clf_suffix", "(", ")", ")", "\n", "\n", "return", "metrics_values", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender._get_top_n_valid_items": [[57, 69], ["range"], "methods", ["None"], ["", "def", "_get_top_n_valid_items", "(", "self", ",", "items", ",", "topk", ",", "valid_items", ")", ":", "\n", "        ", "count", "=", "0", "\n", "for", "item", "in", "items", ":", "\n", "            ", "if", "count", "==", "topk", ":", "\n", "                ", "break", "\n", "", "if", "(", "item", "in", "valid_items", ")", "or", "(", "valid_items", "is", "None", ")", ":", "\n", "                ", "count", "+=", "1", "\n", "yield", "item", "\n", "\n", "#Returning expected top k predictions with 0 (padding item), to avoid errors when attributing to vectors with fixed size", "\n", "", "", "for", "i", "in", "range", "(", "count", ",", "topk", ")", ":", "\n", "            ", "yield", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.run_sr_gnn.log_eval_metrics": [[51, 61], ["len", "test_df[].nunique", "eval_sessions_metrics_log.append", "print", "nar_utils.save_eval_benchmark_metrics_csv"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.save_eval_benchmark_metrics_csv"], ["def", "log_eval_metrics", "(", "metrics_results", ",", "eval_sessions_metrics_log", ",", "test_df", ",", "temp_folder", ")", ":", "\n", "    ", "metrics_results", "[", "'clicks_count'", "]", "=", "len", "(", "test_df", ")", "\n", "metrics_results", "[", "'sessions_count'", "]", "=", "test_df", "[", "'SessionId'", "]", ".", "nunique", "(", ")", "\n", "\n", "eval_sessions_metrics_log", ".", "append", "(", "metrics_results", ")", "\n", "\n", "print", "(", "'Exporting results to temporary folder: {}'", ".", "format", "(", "temp_folder", ")", ")", "\n", "save_eval_benchmark_metrics_csv", "(", "eval_sessions_metrics_log", ",", "temp_folder", ",", "\n", "training_hours_for_each_eval", "=", "ARGS", ".", "training_hours_for_each_eval", ",", "\n", "output_csv", "=", "EVAL_METRICS_FILE", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.run_sr_gnn.create_eval_metrics": [[63, 84], ["clicked_items_state.get_recent_clicks_buffer", "eval_metrics.append", "eval_metrics.append", "eval_metrics.append", "eval_metrics.append", "eval_metrics.append", "metric", "metrics.ItemCoverage", "metrics.ExpectedRankSensitiveNovelty", "metrics.ExpectedRankRelevanceSensitiveNovelty", "metrics.ContentExpectedRankRelativeSensitiveIntraListDiversity", "metrics.ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_recent_clicks_buffer"], ["", "def", "create_eval_metrics", "(", "top_n", ",", "\n", "eval_negative_sample_relevance", ",", "\n", "content_article_embeddings_matrix", ",", "\n", "clicked_items_state", ")", ":", "\n", "\n", "    ", "relevance_positive_sample", "=", "1.0", "\n", "#Empirical: The weight of negative samples", "\n", "relevance_negative_samples", "=", "eval_negative_sample_relevance", "\n", "\n", "recent_clicks_buffer", "=", "clicked_items_state", ".", "get_recent_clicks_buffer", "(", ")", "\n", "\n", "eval_metrics", "=", "[", "metric", "(", "topn", "=", "top_n", ")", "for", "metric", "in", "[", "HitRate", ",", "MRR", "]", "]", "\n", "\n", "#TODO: Known issue: Item coverage here is not considering as recommendable items those who were not in the train set", "\n", "eval_metrics", ".", "append", "(", "ItemCoverage", "(", "top_n", ",", "recent_clicks_buffer", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ExpectedRankSensitiveNovelty", "(", "top_n", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ExpectedRankRelevanceSensitiveNovelty", "(", "top_n", ",", "relevance_positive_sample", ",", "relevance_negative_samples", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ContentExpectedRankRelativeSensitiveIntraListDiversity", "(", "top_n", ",", "content_article_embeddings_matrix", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity", "(", "top_n", ",", "content_article_embeddings_matrix", ",", "relevance_positive_sample", ",", "relevance_negative_samples", ")", ")", "\n", "\n", "return", "eval_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.__init__": [[23, 30], ["math.sqrt"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", "=", "100", ",", "out_size", "=", "100", ",", "batch_size", "=", "100", ",", "nonhybrid", "=", "True", ")", ":", "\n", "        ", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "out_size", "=", "out_size", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "self", ".", "nonhybrid", "=", "nonhybrid", "\n", "self", ".", "stdv", "=", "1.0", "/", "math", ".", "sqrt", "(", "self", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.forward": [[31, 62], ["tensorflow.reduce_sum", "tensorflow.gather_nd", "tensorflow.gather_nd", "tensorflow.stack", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.nn.sigmoid", "tensorflow.reduce_mean", "tensorflow.trainable_variables", "tensorflow.stack", "tensorflow.stack", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.concat", "tensorflow.get_variable", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.reduce_sum", "tensorflow.matmul", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.nn.embedding_lookup", "tensorflow.reshape", "tensorflow.add_n", "tensorflow.range", "tensorflow.range", "range", "tensorflow.reshape", "tensorflow.reduce_sum", "tensorflow.reshape", "tensorflow.random_uniform_initializer", "tensorflow.reshape", "tensorflow.to_int32", "tensorflow.nn.l2_loss", "tensorflow.reshape"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid"], ["", "def", "forward", "(", "self", ",", "re_embedding", ",", "batch_size", ",", "train", "=", "True", ")", ":", "\n", "        ", "rm", "=", "tf", ".", "reduce_sum", "(", "self", ".", "mask", ",", "1", ")", "\n", "last_id", "=", "tf", ".", "gather_nd", "(", "self", ".", "alias", ",", "tf", ".", "stack", "(", "[", "tf", ".", "range", "(", "batch_size", ")", ",", "tf", ".", "to_int32", "(", "rm", ")", "-", "1", "]", ",", "axis", "=", "1", ")", ")", "\n", "last_h", "=", "tf", ".", "gather_nd", "(", "re_embedding", ",", "tf", ".", "stack", "(", "[", "tf", ".", "range", "(", "batch_size", ")", ",", "last_id", "]", ",", "axis", "=", "1", ")", ")", "\n", "seq_h", "=", "tf", ".", "stack", "(", "[", "tf", ".", "nn", ".", "embedding_lookup", "(", "re_embedding", "[", "i", "]", ",", "self", ".", "alias", "[", "i", "]", ")", "for", "i", "in", "range", "(", "batch_size", ")", "]", ",", "\n", "axis", "=", "0", ")", "#batch_size*T*d", "\n", "last", "=", "tf", ".", "matmul", "(", "last_h", ",", "self", ".", "nasr_w1", ")", "\n", "seq", "=", "tf", ".", "matmul", "(", "tf", ".", "reshape", "(", "seq_h", ",", "[", "-", "1", ",", "self", ".", "out_size", "]", ")", ",", "self", ".", "nasr_w2", ")", "\n", "last", "=", "tf", ".", "reshape", "(", "last", ",", "[", "batch_size", ",", "1", ",", "-", "1", "]", ")", "\n", "m", "=", "tf", ".", "nn", ".", "sigmoid", "(", "last", "+", "tf", ".", "reshape", "(", "seq", ",", "[", "batch_size", ",", "-", "1", ",", "self", ".", "out_size", "]", ")", "+", "self", ".", "nasr_b", ")", "\n", "coef", "=", "tf", ".", "matmul", "(", "tf", ".", "reshape", "(", "m", ",", "[", "-", "1", ",", "self", ".", "out_size", "]", ")", ",", "self", ".", "nasr_v", ",", "transpose_b", "=", "True", ")", "*", "tf", ".", "reshape", "(", "\n", "self", ".", "mask", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "b", "=", "self", ".", "embedding", "[", "1", ":", "]", "\n", "if", "not", "self", ".", "nonhybrid", ":", "\n", "            ", "ma", "=", "tf", ".", "concat", "(", "[", "tf", ".", "reduce_sum", "(", "tf", ".", "reshape", "(", "coef", ",", "[", "batch_size", ",", "-", "1", ",", "1", "]", ")", "*", "seq_h", ",", "1", ")", ",", "\n", "tf", ".", "reshape", "(", "last", ",", "[", "-", "1", ",", "self", ".", "out_size", "]", ")", "]", ",", "-", "1", ")", "\n", "self", ".", "B", "=", "tf", ".", "get_variable", "(", "'B'", ",", "[", "2", "*", "self", ".", "out_size", ",", "self", ".", "out_size", "]", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "self", ".", "stdv", ",", "self", ".", "stdv", ")", ")", "\n", "y1", "=", "tf", ".", "matmul", "(", "ma", ",", "self", ".", "B", ")", "\n", "logits", "=", "tf", ".", "matmul", "(", "y1", ",", "b", ",", "transpose_b", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "ma", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "reshape", "(", "coef", ",", "[", "batch_size", ",", "-", "1", ",", "1", "]", ")", "*", "seq_h", ",", "1", ")", "\n", "logits", "=", "tf", ".", "matmul", "(", "ma", ",", "b", ",", "transpose_b", "=", "True", ")", "\n", "", "loss", "=", "tf", ".", "reduce_mean", "(", "\n", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "labels", "=", "self", ".", "tar", "-", "1", ",", "logits", "=", "logits", ")", ")", "\n", "self", ".", "vars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "if", "train", ":", "\n", "            ", "lossL2", "=", "tf", ".", "add_n", "(", "[", "tf", ".", "nn", ".", "l2_loss", "(", "v", ")", "for", "v", "in", "self", ".", "vars", "if", "v", ".", "name", "not", "\n", "in", "[", "'bias'", ",", "'gamma'", ",", "'b'", ",", "'g'", ",", "'beta'", "]", "]", ")", "*", "self", ".", "L2", "\n", "loss", "=", "loss", "+", "lossL2", "\n", "", "return", "loss", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run": [[63, 65], ["gnn_ml_fast.Model.sess.run"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run"], ["", "def", "run", "(", "self", ",", "fetches", ",", "feed_dic", ")", ":", "\n", "        ", "return", "self", ".", "sess", ".", "run", "(", "fetches", ",", "feed_dic", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.__init__": [[68, 85], ["gnn_ml_fast.Model.__init__"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", "=", "100", ",", "out_size", "=", "100", ",", "batch_size", "=", "100", ",", "\n", "lr", "=", "0.001", ",", "l2", "=", "0.00001", ",", "step", "=", "1", ",", "lr_dc", "=", "0.1", ",", "lr_dc_step", "=", "3", ",", "nonhybrid", "=", "True", ",", "epoch_n", "=", "30", ")", ":", "\n", "        ", "super", "(", "GGNN", ",", "self", ")", ".", "__init__", "(", "hidden_size", ",", "out_size", ",", "batch_size", ",", "nonhybrid", ")", "\n", "\n", "\n", "self", ".", "L2", "=", "l2", "\n", "self", ".", "step", "=", "step", "\n", "self", ".", "lr_dc_step", "=", "lr_dc_step", "\n", "self", ".", "nonhybrid", "=", "nonhybrid", "\n", "self", ".", "lr_dc", "=", "lr_dc", "\n", "self", ".", "lr", "=", "lr", "\n", "\n", "self", ".", "epoch_n", "=", "epoch_n", "\n", "# updated while recommending", "\n", "self", ".", "session", "=", "-", "1", "\n", "self", ".", "session_items", "=", "[", "]", "\n", "self", ".", "test_idx", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.init_model": [[86, 133], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.Variable", "tensorflow.train.exponential_decay", "tensorflow.train.AdamOptimizer().minimize", "tensorflow.GPUOptions", "tensorflow.ConfigProto", "tensorflow.Session", "gnn_ml_fast.GGNN.sess.run", "tensorflow.variable_scope", "gnn_ml_fast.GGNN.forward", "tensorflow.variable_scope", "gnn_ml_fast.GGNN.forward", "tensorflow.global_variables_initializer", "tensorflow.random_uniform_initializer", "tensorflow.random_uniform_initializer", "tensorflow.random_uniform_initializer", "tensorflow.zeros_initializer", "tensorflow.random_uniform_initializer", "tensorflow.random_uniform_initializer", "tensorflow.random_uniform_initializer", "tensorflow.random_uniform_initializer", "tensorflow.random_uniform_initializer", "gnn_ml_fast.GGNN.ggnn", "gnn_ml_fast.GGNN.ggnn", "tensorflow.train.AdamOptimizer"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.forward", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.forward", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.ggnn", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.ggnn"], ["", "def", "init_model", "(", "self", ",", "trainset_size", ")", ":", "\n", "\n", "        ", "self", ".", "mask", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "alias", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int32", ")", "# \u7ed9\u7ed9\u6bcf\u4e2a\u8f93\u5165\u91cd\u65b0", "\n", "self", ".", "item", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int32", ")", "# \u91cd\u65b0\u7f16\u53f7\u7684\u5e8f\u5217\u6784\u6210\u7684\u77e9\u9635", "\n", "self", ".", "tar", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "self", ".", "nasr_w1", "=", "tf", ".", "get_variable", "(", "'nasr_w1'", ",", "[", "self", ".", "out_size", ",", "self", ".", "out_size", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "self", ".", "stdv", ",", "self", ".", "stdv", ")", ")", "\n", "self", ".", "nasr_w2", "=", "tf", ".", "get_variable", "(", "'nasr_w2'", ",", "[", "self", ".", "out_size", ",", "self", ".", "out_size", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "self", ".", "stdv", ",", "self", ".", "stdv", ")", ")", "\n", "self", ".", "nasr_v", "=", "tf", ".", "get_variable", "(", "'nasrv'", ",", "[", "1", ",", "self", ".", "out_size", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "self", ".", "stdv", ",", "self", ".", "stdv", ")", ")", "\n", "self", ".", "nasr_b", "=", "tf", ".", "get_variable", "(", "'nasr_b'", ",", "[", "self", ".", "out_size", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "\n", "self", ".", "embedding", "=", "tf", ".", "get_variable", "(", "shape", "=", "[", "self", ".", "n_nodes", ",", "self", ".", "hidden_size", "]", ",", "name", "=", "'embedding'", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "self", ".", "stdv", ",", "self", ".", "stdv", ")", ")", "\n", "self", ".", "adj_in_tr", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "self", ".", "batch_size", ",", "None", ",", "None", "]", ")", "\n", "self", ".", "adj_out_tr", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "self", ".", "batch_size", ",", "None", ",", "None", "]", ")", "\n", "self", ".", "adj_in_ts", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "self", ".", "batch_size", ",", "None", ",", "None", "]", ")", "\n", "self", ".", "adj_out_ts", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "self", ".", "batch_size", ",", "None", ",", "None", "]", ")", "\n", "\n", "self", ".", "W_in", "=", "tf", ".", "get_variable", "(", "'W_in'", ",", "shape", "=", "[", "self", ".", "out_size", ",", "self", ".", "out_size", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "self", ".", "stdv", ",", "self", ".", "stdv", ")", ")", "\n", "self", ".", "b_in", "=", "tf", ".", "get_variable", "(", "'b_in'", ",", "[", "self", ".", "out_size", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "self", ".", "stdv", ",", "self", ".", "stdv", ")", ")", "\n", "self", ".", "W_out", "=", "tf", ".", "get_variable", "(", "'W_out'", ",", "[", "self", ".", "out_size", ",", "self", ".", "out_size", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "self", ".", "stdv", ",", "self", ".", "stdv", ")", ")", "\n", "self", ".", "b_out", "=", "tf", ".", "get_variable", "(", "'b_out'", ",", "[", "self", ".", "out_size", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "\n", "initializer", "=", "tf", ".", "random_uniform_initializer", "(", "-", "self", ".", "stdv", ",", "self", ".", "stdv", ")", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "'ggnn_model'", ",", "reuse", "=", "None", ")", ":", "\n", "            ", "self", ".", "loss_train", ",", "_", "=", "self", ".", "forward", "(", "self", ".", "ggnn", "(", "self", ".", "batch_size", ",", "self", ".", "adj_in_tr", ",", "self", ".", "adj_out_tr", ")", ",", "self", ".", "batch_size", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "'ggnn_model'", ",", "reuse", "=", "True", ")", ":", "\n", "            ", "self", ".", "loss_test", ",", "self", ".", "score_test", "=", "self", ".", "forward", "(", "self", ".", "ggnn", "(", "self", ".", "batch_size", ",", "self", ".", "adj_in_ts", ",", "self", ".", "adj_out_ts", ")", ",", "self", ".", "batch_size", ",", "train", "=", "False", ")", "\n", "\n", "", "self", ".", "global_step", "=", "tf", ".", "Variable", "(", "0", ")", "\n", "decay_steps", "=", "(", "self", ".", "lr_dc_step", "*", "trainset_size", ")", "/", "self", ".", "batch_size", "\n", "self", ".", "learning_rate", "=", "tf", ".", "train", ".", "exponential_decay", "(", "self", ".", "lr", ",", "global_step", "=", "self", ".", "global_step", ",", "decay_steps", "=", "decay_steps", ",", "\n", "decay_rate", "=", "self", ".", "lr_dc", ",", "staircase", "=", "True", ")", "\n", "self", ".", "opt", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "self", ".", "learning_rate", ")", ".", "minimize", "(", "self", ".", "loss_train", ",", "global_step", "=", "self", ".", "global_step", ")", "\n", "gpu_options", "=", "tf", ".", "GPUOptions", "(", "per_process_gpu_memory_fraction", "=", "0.8", ")", "\n", "config", "=", "tf", ".", "ConfigProto", "(", "gpu_options", "=", "gpu_options", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "self", ".", "sess", "=", "tf", ".", "Session", "(", "config", "=", "config", ")", "\n", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.ggnn": [[134, 150], ["tensorflow.nn.embedding_lookup", "tensorflow.nn.rnn_cell.GRUCell", "tensorflow.reshape", "tensorflow.variable_scope", "range", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.concat", "tensorflow.nn.dynamic_rnn", "tensorflow.expand_dims", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape"], "methods", ["None"], ["", "def", "ggnn", "(", "self", ",", "batch_size", ",", "adj_in", ",", "adj_out", ")", ":", "\n", "        ", "fin_state", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "embedding", ",", "self", ".", "item", ")", "\n", "cell", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "GRUCell", "(", "self", ".", "out_size", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'gru'", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "step", ")", ":", "\n", "                ", "fin_state", "=", "tf", ".", "reshape", "(", "fin_state", ",", "[", "batch_size", ",", "-", "1", ",", "self", ".", "out_size", "]", ")", "\n", "fin_state_in", "=", "tf", ".", "reshape", "(", "tf", ".", "matmul", "(", "tf", ".", "reshape", "(", "fin_state", ",", "[", "-", "1", ",", "self", ".", "out_size", "]", ")", ",", "\n", "self", ".", "W_in", ")", "+", "self", ".", "b_in", ",", "[", "batch_size", ",", "-", "1", ",", "self", ".", "out_size", "]", ")", "\n", "fin_state_out", "=", "tf", ".", "reshape", "(", "tf", ".", "matmul", "(", "tf", ".", "reshape", "(", "fin_state", ",", "[", "-", "1", ",", "self", ".", "out_size", "]", ")", ",", "\n", "self", ".", "W_out", ")", "+", "self", ".", "b_out", ",", "[", "batch_size", ",", "-", "1", ",", "self", ".", "out_size", "]", ")", "\n", "av", "=", "tf", ".", "concat", "(", "[", "tf", ".", "matmul", "(", "adj_in", ",", "fin_state_in", ")", ",", "\n", "tf", ".", "matmul", "(", "adj_out", ",", "fin_state_out", ")", "]", ",", "axis", "=", "-", "1", ")", "\n", "state_output", ",", "fin_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "cell", ",", "tf", ".", "expand_dims", "(", "tf", ".", "reshape", "(", "av", ",", "[", "-", "1", ",", "2", "*", "self", ".", "out_size", "]", ")", ",", "axis", "=", "1", ")", ",", "\n", "initial_state", "=", "tf", ".", "reshape", "(", "fin_state", ",", "[", "-", "1", ",", "self", ".", "out_size", "]", ")", ")", "\n", "", "", "return", "tf", ".", "reshape", "(", "fin_state", ",", "[", "batch_size", ",", "-", "1", ",", "self", ".", "out_size", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.prepare_data": [[151, 160], ["utils.prepare_data", "utils.Data", "utils.Data", "len", "train_df[].unique"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.prepare_data"], ["", "def", "prepare_data", "(", "self", ",", "train_df", ",", "test_df", ",", "eval_sessions_neg_samples", ",", "method", "=", "'ggnn'", ")", ":", "\n", "        ", "self", ".", "n_nodes", "=", "len", "(", "train_df", "[", "'ItemId'", "]", ".", "unique", "(", ")", ")", "+", "1", "\n", "train_data", ",", "test_data", ",", "self", ".", "item_dict", ",", "self", ".", "reversed_item_dict", ",", "count_clicks_in_test_items_not_in_train_set", "=", "prepare_data", "(", "train_df", ",", "test_df", ",", "\n", "eval_sessions_neg_samples", "=", "eval_sessions_neg_samples", ")", "\n", "\n", "train_data", "=", "Data", "(", "train_data", ",", "sub_graph", "=", "True", ",", "method", "=", "method", ",", "shuffle", "=", "True", ")", "\n", "test_data", "=", "Data", "(", "test_data", ",", "sub_graph", "=", "True", ",", "method", "=", "method", ",", "shuffle", "=", "False", ",", "include_neg_samples", "=", "True", ")", "\n", "return", "train_data", ",", "test_data", ",", "count_clicks_in_test_items_not_in_train_set", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.fit": [[162, 184], ["tensorflow.Graph().as_default", "len", "gnn_ml_fast.GGNN.init_model", "range", "train_data.generate_batch", "zip", "print", "tensorflow.Graph", "numpy.arange", "train_data.get_slice", "gnn_ml_fast.GGNN.run", "loss_.append", "len", "numpy.mean"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.init_model", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.Data.generate_batch", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.Data.get_slice", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run"], ["", "def", "fit", "(", "self", ",", "train_data", ")", ":", "\n", "\n", "        ", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "\n", "            ", "trainset_size", "=", "len", "(", "train_data", ".", "inputs", ")", "\n", "self", ".", "init_model", "(", "trainset_size", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "self", ".", "epoch_n", ")", ":", "\n", "                ", "slices", "=", "train_data", ".", "generate_batch", "(", "self", ".", "batch_size", ")", "\n", "#print('len(slices)', len(slices))", "\n", "\n", "fetches", "=", "[", "self", ".", "opt", ",", "self", ".", "loss_train", ",", "self", ".", "global_step", "]", "\n", "loss_", "=", "[", "]", "\n", "for", "i", ",", "j", "in", "zip", "(", "slices", ",", "np", ".", "arange", "(", "len", "(", "slices", ")", ")", ")", ":", "\n", "                    ", "adj_in", ",", "adj_out", ",", "alias", ",", "item", ",", "mask", ",", "targets", "=", "train_data", ".", "get_slice", "(", "i", ")", "\n", "feed_dict", "=", "{", "self", ".", "tar", ":", "targets", ",", "self", ".", "item", ":", "item", ",", "self", ".", "adj_in_tr", ":", "adj_in", ",", "\n", "self", ".", "adj_out_tr", ":", "adj_out", ",", "self", ".", "alias", ":", "alias", ",", "self", ".", "mask", ":", "mask", "}", "\n", "_", ",", "loss", ",", "_", "=", "self", ".", "run", "(", "fetches", ",", "feed_dict", ")", "\n", "\n", "loss_", ".", "append", "(", "loss", ")", "\n", "\n", "", "print", "(", "'Epoch: {} - train loss: {}'", ".", "format", "(", "epoch", ",", "np", ".", "mean", "(", "loss_", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.evaluate": [[186, 296], ["tensorflow.Graph().as_default", "test_data.generate_batch", "zip", "numpy.mean", "evaluation.compute_metrics_results", "print", "numpy.arange", "test_data.get_slice", "gnn_ml_fast.GGNN.run", "test_loss_.append", "list", "numpy.array", "print", "numpy.vectorize", "numpy.asarray().flatten", "numpy.vectorize.", "numpy.vectorize.", "numpy.expand_dims", "evaluation.update_metrics", "clicked_items_state.update_items_state", "print", "tensorflow.Graph", "len", "numpy.concatenate", "numpy.vectorize.", "clicked_items_state.get_articles_recent_pop_norm", "clicked_items_state.get_articles_recent_pop_norm", "numpy.array", "numpy.argsort", "list", "numpy.asarray", "numpy.nonzero", "len", "numpy.array", "metric.add", "list", "zip", "filter", "zip"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.Data.generate_batch", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.compute_metrics_results", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.Data.get_slice", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.update_metrics", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.update_items_state", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add"], ["", "", "", "def", "evaluate", "(", "self", ",", "test_data", ",", "streaming_metrics", ",", "clicked_items_state", ",", "count_clicks_in_test_items_not_in_train_set", ",", "min_timestamp_testset", ")", ":", "\n", "\n", "#for m in streaming_metrics:", "\n", "#    m.reset()", "\n", "\n", "        ", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "\n", "            ", "slices", "=", "test_data", ".", "generate_batch", "(", "self", ".", "batch_size", ")", "\n", "\n", "test_loss_", "=", "[", "]", "\n", "\n", "all_scores", "=", "None", "\n", "for", "i", ",", "j", "in", "zip", "(", "slices", ",", "np", ".", "arange", "(", "len", "(", "slices", ")", ")", ")", ":", "\n", "\n", "#adj_in, adj_out, alias, item, mask, targets = self.test_data.get_slice(i)", "\n", "                ", "adj_in", ",", "adj_out", ",", "alias", ",", "item", ",", "mask", ",", "targets", ",", "negative_samples", "=", "test_data", ".", "get_slice", "(", "i", ")", "\n", "feed_dict", "=", "{", "self", ".", "tar", ":", "targets", ",", "self", ".", "item", ":", "item", ",", "self", ".", "adj_in_ts", ":", "adj_in", ",", "\n", "self", ".", "adj_out_ts", ":", "adj_out", ",", "self", ".", "alias", ":", "alias", ",", "self", ".", "mask", ":", "mask", "}", "\n", "scores", ",", "test_loss", "=", "self", ".", "run", "(", "[", "self", ".", "score_test", ",", "self", ".", "loss_test", "]", ",", "feed_dict", ")", "\n", "\n", "#print('scores',scores.shape)                    ", "\n", "\n", "test_loss_", ".", "append", "(", "test_loss", ")", "\n", "\n", "if", "all_scores", "is", "None", ":", "\n", "                    ", "all_scores", "=", "scores", "\n", "", "else", ":", "\n", "                    ", "all_scores", "=", "np", ".", "concatenate", "(", "[", "all_scores", ",", "scores", "]", ")", "\n", "\n", "\n", "\n", "#Sorting items by relevance (decreasing order) and summing 1 because item ids start at 1", "\n", "", "sorted_items_by_pred_relevance_batch", "=", "np", ".", "argsort", "(", "scores", ",", "1", ")", "[", ":", ",", ":", ":", "-", "1", "]", "+", "1", "\n", "valid_items_batch", "=", "list", "(", "[", "[", "label", "]", "+", "list", "(", "neg_samples", ")", "for", "label", ",", "neg_samples", "in", "zip", "(", "targets", ",", "negative_samples", ")", "]", ")", "\n", "\n", "\n", "sorted_valid_items", "=", "np", ".", "array", "(", "[", "list", "(", "filter", "(", "lambda", "x", ":", "x", "in", "valid_items", ",", "items", ")", ")", "for", "items", ",", "valid_items", "in", "zip", "(", "sorted_items_by_pred_relevance_batch", ",", "valid_items_batch", ")", "]", ")", "\n", "print", "(", "'sorted_valid_items'", ",", "sorted_valid_items", ".", "shape", ")", "\n", "\n", "#Processing ids for metrics calculation", "\n", "item_ids_to_original_vect", "=", "np", ".", "vectorize", "(", "lambda", "x", ":", "self", ".", "reversed_item_dict", "[", "x", "]", ")", "\n", "clicked_items", "=", "np", ".", "asarray", "(", "item", ")", ".", "flatten", "(", ")", "\n", "clicked_items", "=", "clicked_items", "[", "np", ".", "nonzero", "(", "clicked_items", ")", "]", "\n", "clicked_items_original", "=", "item_ids_to_original_vect", "(", "clicked_items", ")", "\n", "labels_original_ids", "=", "item_ids_to_original_vect", "(", "[", "targets", "]", ")", "\n", "preds_original_ids", "=", "np", ".", "expand_dims", "(", "item_ids_to_original_vect", "(", "sorted_valid_items", ")", ",", "0", ")", "\n", "labels_norm_pop", "=", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", "[", "labels_original_ids", "]", "\n", "preds_norm_pop", "=", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", "[", "preds_original_ids", "]", "\n", "\n", "update_metrics", "(", "preds_original_ids", ",", "labels_original_ids", ",", "labels_norm_pop", ",", "preds_norm_pop", ",", "\n", "clicked_items_original", ",", "streaming_metrics", ")", "\n", "\n", "'''\n                for m in streaming_metrics:\n                    m.add(np.expand_dims(sorted_valid_items, 0), [targets])\n                '''", "\n", "\n", "\n", "#Concatenating batch clicked items and labels", "\n", "#clicked_items = np.asarray(item).flatten()", "\n", "#clicked_items = clicked_items[np.nonzero(clicked_items)]", "\n", "#all_items = np.concatenate([clicked_items, targets])        ", "\n", "#print('all_items', all_items.shape)", "\n", "\n", "#As all subsequences of a session are generated, taking labels as clicked items will ignore only the first clicked item, and is a good approximation of the popularity", "\n", "#clicked_items = np.array([self.reversed_item_dict[t] for t in targets])                ", "\n", "#Using the lower timestamp from test set as baseline for truncating the last hour", "\n", "clicked_items_state", ".", "update_items_state", "(", "labels_original_ids", "[", "0", "]", ",", "np", ".", "array", "(", "[", "min_timestamp_testset", "+", "1", "]", "*", "labels_original_ids", "[", "0", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n", "\n", "", "test_loss", "=", "np", ".", "mean", "(", "test_loss_", ")", "\n", "\n", "\n", "#If there are clicks in test set items not viewed during training, as the method is not able", "\n", "#to predict those items, assume that the recommendation was not correct in metrics", "\n", "if", "count_clicks_in_test_items_not_in_train_set", ">", "0", ":", "\n", "                ", "perc_test_items_not_found", "=", "count_clicks_in_test_items_not_in_train_set", "/", "(", "len", "(", "test_data", ".", "inputs", ")", "+", "count_clicks_in_test_items_not_in_train_set", ")", "\n", "print", "(", "'{} ({}%) test set clicks in items not present in train set.'", ".", "format", "(", "count_clicks_in_test_items_not_in_train_set", ",", "perc_test_items_not_found", ")", ")", "\n", "\n", "#Include additional prediction errors (for accuracy metrics) when  next-clicked item is not available in train set", "\n", "for", "metric", "in", "streaming_metrics", ":", "\n", "                    ", "if", "metric", ".", "name", "in", "[", "HitRate", ".", "name", ",", "MRR", ".", "name", "]", ":", "\n", "                        ", "fake_targets", "=", "[", "[", "1", "]", "*", "count_clicks_in_test_items_not_in_train_set", "]", "\n", "fake_preds", "=", "np", ".", "array", "(", "[", "[", "[", "0", "]", "]", "*", "count_clicks_in_test_items_not_in_train_set", "]", ")", "\n", "metric", ".", "add", "(", "fake_preds", ",", "fake_targets", ")", "\n", "\n", "\n", "", "", "", "metric_results", "=", "compute_metrics_results", "(", "streaming_metrics", ",", "recommender", "=", "'sr-gnn'", ")", "\n", "\n", "print", "(", "'test_loss:\\t%4f\\t %s'", "%", "\n", "(", "test_loss", ",", "metric_results", ")", ")", "\n", "\n", "\n", "\n", "\n", "#self.all_scores = all_scores", "\n", "#print(\"all_scores\", self.all_scores.shape)", "\n", "\n", "'''\n            slices = test_data.generate_batch(1)\n            \n            self.predicted_item_ids = []\n            for idx in range(len(self.all_scores[0])):\n                self.predicted_item_ids.append(int(self.reversed_item_dict[idx + 1]))  # because in item_dic, indexes start from 1 (not 0)\n\n            print('self.predicted_item_ids', len(self.predicted_item_ids))\n            '''", "\n", "\n", "", "return", "metric_results", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.Data.__init__": [[58, 74], ["utils.data_masks", "numpy.asarray", "numpy.asarray", "numpy.asarray", "len", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.data_masks"], ["\n", "", "", "def", "get_tf_dtype", "(", "dtype", ")", ":", "\n", "    ", "if", "dtype", "==", "'int'", ":", "\n", "        ", "tf_dtype", "=", "tf", ".", "int64", "\n", "", "elif", "dtype", "==", "'float'", ":", "\n", "        ", "tf_dtype", "=", "tf", ".", "float32", "\n", "", "elif", "dtype", "==", "'string'", "or", "dtype", "==", "'bytes'", ":", "\n", "        ", "tf_dtype", "=", "tf", ".", "string", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Invalid dtype \"{}\"'", ".", "format", "(", "dtype", ")", ")", "\n", "", "return", "tf_dtype", "\n", "\n", "\n", "", "def", "get_pad_token", "(", ")", ":", "\n", "    ", "PAD_TOKEN", "=", "'<PAD>'", "\n", "return", "PAD_TOKEN", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.Data.generate_batch": [[75, 95], ["int", "numpy.split", "numpy.arange", "numpy.random.shuffle", "numpy.arange", "numpy.concatenate", "numpy.arange", "numpy.arange"], "methods", ["None"], ["", "def", "get_unfrequent_token", "(", ")", ":", "\n", "    ", "UNFREQ_TOKEN", "=", "'<UNF>'", "\n", "return", "UNFREQ_TOKEN", "\n", "\n", "", "def", "get_categ_encoder_from_values", "(", "values", ",", "include_pad_token", "=", "True", ",", "include_unfrequent_token", "=", "False", ")", ":", "\n", "    ", "encoder_values", "=", "[", "]", "\n", "if", "include_pad_token", ":", "\n", "        ", "encoder_values", ".", "append", "(", "get_pad_token", "(", ")", ")", "\n", "", "if", "include_unfrequent_token", ":", "\n", "        ", "encoder_values", ".", "append", "(", "get_unfrequent_token", "(", ")", ")", "\n", "", "encoder_values", ".", "extend", "(", "values", ")", "\n", "encoder_ids", "=", "list", "(", "range", "(", "len", "(", "encoder_values", ")", ")", ")", "\n", "encoder_dict", "=", "dict", "(", "zip", "(", "encoder_values", ",", "encoder_ids", ")", ")", "\n", "return", "encoder_dict", "\n", "\n", "", "def", "encode_categ_feature", "(", "value", ",", "encoder_dict", ")", ":", "\n", "    ", "if", "value", "in", "encoder_dict", ":", "\n", "        ", "return", "encoder_dict", "[", "value", "]", "\n", "", "else", ":", "\n", "        ", "return", "encoder_dict", "[", "get_unfrequent_token", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.Data.get_slice": [[96, 152], ["numpy.max", "n_node.append", "len", "numpy.unique", "items.append", "numpy.zeros", "numpy.arange", "numpy.sum", "numpy.divide", "numpy.sum", "numpy.divide", "A_in.append", "A_out.append", "alias_inputs.append", "results.append", "numpy.unique", "numpy.eye.transpose", "numpy.unique", "items.append", "numpy.eye", "numpy.arange", "A_in.append", "A_out.append", "alias_inputs.append", "results.append", "numpy.unique.tolist", "len", "numpy.where", "numpy.where", "numpy.where", "numpy.where", "numpy.unique.tolist", "len", "len", "numpy.where", "numpy.where", "numpy.where", "numpy.eye.transpose", "len", "numpy.where"], "methods", ["None"], ["\n", "", "", "def", "max_n_sparse_indexes", "(", "row_data", ",", "row_indices", ",", "topn", ")", ":", "\n", "    ", "i", "=", "row_data", ".", "argsort", "(", ")", "[", "-", "topn", ":", "]", "[", ":", ":", "-", "1", "]", "\n", "top_values", "=", "row_data", "[", "i", "]", "\n", "top_indices", "=", "row_indices", "[", "i", "]", "\n", "return", "top_indices", "#, top_values", "\n", "\n", "#Returns a tensor with 2 dimensions with all paired permutations (of size 2) of tensor x", "\n", "", "def", "paired_permutations", "(", "x", ")", ":", "\n", "#Ensuring the vector is flatten", "\n", "#x = tf.reshape(x, [-1])    ", "\n", "    ", "size", "=", "tf", ".", "shape", "(", "x", ")", "[", "0", "]", "\n", "\n", "counter", "=", "tf", ".", "constant", "(", "0", ")", "\n", "m0", "=", "tf", ".", "zeros", "(", "shape", "=", "[", "0", ",", "2", "]", ",", "dtype", "=", "x", ".", "dtype", ")", "\n", "cond", "=", "lambda", "i", ",", "m", ":", "i", "<", "size", "*", "size", "\n", "body", "=", "lambda", "i", ",", "m", ":", "[", "i", "+", "1", ",", "tf", ".", "concat", "(", "[", "m", ",", "tf", ".", "expand_dims", "(", "tf", ".", "stack", "(", "[", "x", "[", "tf", ".", "to_int32", "(", "tf", ".", "div", "(", "i", ",", "size", ")", ")", "]", ",", "\n", "x", "[", "tf", ".", "mod", "(", "i", ",", "size", ")", "]", "]", ")", "\n", ",", "axis", "=", "0", ")", "\n", "]", ",", "axis", "=", "0", ",", "name", "=", "\"concat_rows\"", ")", "\n", "]", "\n", "_", ",", "combined_values", "=", "tf", ".", "while_loop", "(", "\n", "cond", ",", "body", ",", "\n", "loop_vars", "=", "[", "counter", ",", "m0", "]", ",", "\n", "shape_invariants", "=", "[", "counter", ".", "get_shape", "(", ")", ",", "tf", ".", "TensorShape", "(", "[", "None", ",", "None", "]", ")", "]", ")", "\n", "return", "combined_values", "\n", "\n", "\n", "", "def", "get_days_diff", "(", "newer_timestamp", ",", "older_timestamp", ")", ":", "\n", "    ", "sec_diff", "=", "newer_timestamp", "-", "older_timestamp", "\n", "days_diff", "=", "sec_diff", "/", "60", "/", "60", "/", "24", "\n", "return", "days_diff", "\n", "\n", "", "def", "get_time_decay_factor", "(", "newer_timestamp", ",", "older_timestamp", ",", "alpha", "=", "0.5", ")", ":", "\n", "    ", "days_diff", "=", "get_days_diff", "(", "newer_timestamp", ",", "older_timestamp", ")", "\n", "denominator", "=", "math", ".", "pow", "(", "1", "+", "alpha", ",", "days_diff", ")", "\n", "if", "denominator", "!=", "0", ":", "\n", "        ", "return", "1.0", "/", "denominator", "\n", "", "else", ":", "\n", "        ", "return", "0.0", "\n", "\n", "", "", "def", "append_lines_to_text_file", "(", "filename", ",", "lines", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"a\"", ")", "as", "myfile", ":", "\n", "        ", "myfile", ".", "writelines", "(", "[", "line", "+", "\"\\n\"", "for", "line", "in", "lines", "]", ")", "\n", "\n", "", "", "def", "hash_str_to_int", "(", "encoded_bytes_text", ",", "digits", ")", ":", "\n", "    ", "return", "int", "(", "str", "(", "int", "(", "hashlib", ".", "md5", "(", "encoded_bytes_text", ")", ".", "hexdigest", "(", ")", "[", ":", "8", "]", ",", "16", ")", ")", "[", ":", "digits", "]", ")", "\n", "\n", "\n", "", "def", "get_os_list", "(", ")", ":", "\n", "    ", "return", "[", "'iOS'", ",", "\n", "'Android'", ",", "\n", "'Windows Phone'", ",", "\n", "'Windows Mobile'", ",", "\n", "'Windows'", ",", "\n", "'Mac OS X'", ",", "\n", "'Mac OS'", ",", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.Data.get_slice_by_session_items": [[153, 199], ["numpy.zeros", "numpy.asarray", "numpy.zeros", "range", "n_node.append", "numpy.max", "numpy.unique", "items.append", "numpy.zeros", "numpy.arange", "numpy.sum", "numpy.divide", "numpy.sum", "numpy.divide", "A_in.append", "A_out.append", "alias_inputs.append", "numpy.asarray", "len", "len", "numpy.zeros.transpose", "len", "numpy.unique", "numpy.unique.tolist", "len", "numpy.where", "numpy.where", "numpy.where", "numpy.where", "len", "numpy.where"], "methods", ["None"], ["'Samsung'", ",", "\n", "'FireHbbTV'", ",", "\n", "'ATV OS X'", ",", "\n", "'tvOS'", ",", "\n", "'Chrome OS'", ",", "\n", "'Debian'", ",", "\n", "'Symbian OS'", ",", "\n", "'BlackBerry OS'", ",", "\n", "'Firefox OS'", ",", "\n", "'Android'", ",", "\n", "'Brew MP'", ",", "\n", "'Chromecast'", ",", "\n", "'webOS'", ",", "\n", "'Gentoo'", ",", "\n", "'Solaris'", "]", "\n", "\n", "", "def", "extract_os_from_user_agent", "(", "user_agent", ",", "default_os", "=", "'Other'", ")", ":", "\n", "    ", "parsed_os", "=", "user_agent_parser", ".", "ParseOS", "(", "user_agent", ")", "\n", "os_family", "=", "parsed_os", "[", "'family'", "]", "\n", "if", "'Symbian'", "in", "os_family", ":", "\n", "        ", "os_family", "=", "'Symbian OS'", "\n", "", "elif", "'BlackBerry'", "in", "os_family", ":", "\n", "        ", "os_family", "=", "'BlackBerry OS'", "\n", "\n", "", "if", "os_family", "is", "None", "or", "os_family", "not", "in", "get_os_list", "(", ")", ":", "\n", "        ", "os_family", "=", "default_os", "\n", "\n", "", "return", "os_family", "\n", "\n", "\n", "", "domain_pattern", "=", "re", ".", "compile", "(", "r\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/\\n]+)\"", ")", "\n", "def", "extract_domain_from_url", "(", "url", ")", ":", "\n", "    ", "s", "=", "domain_pattern", ".", "search", "(", "url", ")", "\n", "if", "s", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "else", ":", "\n", "        ", "domain", "=", "s", ".", "group", "(", "0", ")", "\n", "return", "domain", "\n", "\n", "\n", "", "", "def", "urlencode", "(", "str", ")", ":", "\n", "  ", "return", "urllib", ".", "parse", ".", "quote", "(", "str", ")", "\n", "\n", "\n", "", "def", "urldecode", "(", "str", ")", ":", "\n", "  ", "return", "urllib", ".", "parse", ".", "unquote", "(", "str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.build_graph": [[16, 33], ["networkx.DiGraph", "range", "nx.DiGraph.in_edges", "nx.DiGraph.add_edge", "nx.DiGraph.in_edges", "len", "nx.DiGraph.get_edge_data", "nx.DiGraph.get_edge_data", "nx.DiGraph.add_edge", "nx.DiGraph.get_edge_data", "nx.DiGraph.get_edge_data"], "function", ["None"], ["\n", "from", "ua_parser", "import", "user_agent_parser", "\n", "\n", "def", "serialize", "(", "filename", ",", "obj", ")", ":", "\n", "#with open(filename, 'wb') as handle:", "\n", "    ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "'wb'", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "obj", ",", "handle", ")", "#, protocol=pickle.HIGHEST_PROTOCOL)", "\n", "\n", "", "", "def", "deserialize", "(", "filename", ")", ":", "\n", "#with open(filename, 'rb') as handle:", "\n", "    ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "'rb'", ")", "as", "handle", ":", "\n", "        ", "return", "pickle", ".", "load", "(", "handle", ")", "\n", "\n", "", "", "def", "merge_two_dicts", "(", "x", ",", "y", ")", ":", "\n", "#Python 2 to 3.4", "\n", "#z = x.copy()   # start with x's keys and values", "\n", "#z.update(y)    # modifies z with y's keys and values & returns None", "\n", "#return z", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.data_masks": [[35, 41], ["max", "len", "zip"], "function", ["None"], ["    ", "return", "{", "**", "x", ",", "**", "y", "}", "\n", "\n", "", "def", "log_elapsed_time", "(", "start_time", ",", "text", "=", "''", ")", ":", "\n", "    ", "took", "=", "(", "time", "(", ")", "-", "start_time", ")", "/", "60.0", "\n", "tf", ".", "logging", ".", "info", "(", "'==== {} elapsed {:.1f} minutes'", ".", "format", "(", "text", ",", "took", ")", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.split_validation": [[43, 55], ["len", "numpy.arange", "numpy.random.shuffle", "int", "numpy.round"], "function", ["None"], ["    ", "\"\"\"Return list of files given a regex\"\"\"", "\n", "list_op", "=", "tf", ".", "train", ".", "match_filenames_once", "(", "regex", ")", "\n", "init_ops", "=", "(", "tf", ".", "global_variables_initializer", "(", ")", ",", "\n", "tf", ".", "local_variables_initializer", "(", ")", ")", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "        ", "sess", ".", "run", "(", "init_ops", ")", "\n", "files", "=", "sess", ".", "run", "(", "list_op", ")", "\n", "\n", "", "return", "list", "(", "sorted", "(", "files", ")", ")", "\n", "\n", "", "def", "chunks", "(", "l", ",", "n", ")", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from l.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "l", ")", ",", "n", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.obtian_tra": [[201, 226], ["print", "len"], "function", ["None"], ["    ", "dt", "=", "pytz", ".", "utc", ".", "localize", "(", "datetime", ".", "datetime", ".", "utcfromtimestamp", "(", "timestamp_in_utc", ")", ")", ".", "astimezone", "(", "pytz", ".", "timezone", "(", "local_tz", ")", ")", "\n", "return", "dt", ".", "hour", "+", "(", "dt", ".", "minute", "/", "60.0", ")", ",", "dt", ".", "weekday", "(", ")", "#First day is Monday", "\n", "\n", "\n", "", "def", "strip_accents", "(", "s", ")", ":", "\n", "    ", "return", "''", ".", "join", "(", "c", "for", "c", "in", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "s", ")", "\n", "if", "unicodedata", ".", "category", "(", "c", ")", "!=", "'Mn'", ")", "\n", "\n", "\n", "", "def", "gini_index", "(", "array", ")", ":", "\n", "    ", "\"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"", "\n", "# based on bottom eq:", "\n", "# http://www.statsdirect.com/help/generatedimages/equations/equation154.svg", "\n", "# from:", "\n", "# http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm", "\n", "# All values are treated equally, arrays must be 1d:", "\n", "array", "=", "array", ".", "flatten", "(", ")", "\n", "if", "np", ".", "amin", "(", "array", ")", "<", "0", ":", "\n", "# Values cannot be negative:", "\n", "        ", "array", "-=", "np", ".", "amin", "(", "array", ")", "\n", "# Values cannot be 0:", "\n", "", "array", "+=", "0.0000001", "\n", "# Values must be sorted:", "\n", "array", "=", "np", ".", "sort", "(", "array", ")", "\n", "# Index per array element:", "\n", "index", "=", "np", ".", "arange", "(", "1", ",", "array", ".", "shape", "[", "0", "]", "+", "1", ")", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.obtian_tes": [[228, 244], ["len"], "function", ["None"], ["n", "=", "array", ".", "shape", "[", "0", "]", "\n", "# Gini coefficient:", "\n", "return", "(", "(", "np", ".", "sum", "(", "(", "2", "*", "index", "-", "n", "-", "1", ")", "*", "array", ")", ")", "/", "(", "n", "*", "np", ".", "sum", "(", "array", ")", ")", ")", "\n", "\n", "\n", "\n", "", "def", "min_max_scale", "(", "vector", ",", "min_max_range", "=", "(", "-", "1.0", ",", "1.0", ")", ")", ":", "\n", "    ", "scaler", "=", "MinMaxScaler", "(", "feature_range", "=", "min_max_range", ")", "\n", "norm_vector", "=", "scaler", ".", "fit_transform", "(", "vector", ")", "\n", "return", "norm_vector", "\n", "\n", "\n", "\n", "", "def", "str2bool", "(", "v", ")", ":", "\n", "    ", "if", "isinstance", "(", "v", ",", "bool", ")", ":", "\n", "       ", "return", "v", "\n", "", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.obtian_tes_with_neg_samples": [[246, 311], ["zip", "Exception", "len", "len", "list", "len", "random.sample", "list", "set().difference", "set().union", "set", "item_dict.values", "set"], "function", ["None"], ["", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Boolean value expected.'", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.process_seqs": [[312, 325], ["zip", "range", "reversed", "len", "range", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.process_seqs_with_neg_samples": [[327, 344], ["zip", "range", "reversed", "len", "range", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.prepare_data": [[346, 437], ["_collections.OrderedDict", "_collections.OrderedDict", "train.itertuples", "_collections.OrderedDict", "test.iterrows", "list", "list", "utils.obtian_tra", "utils.obtian_tes_with_neg_samples", "utils.process_seqs", "utils.process_seqs_with_neg_samples", "int", "_collections.OrderedDict.items", "_collections.OrderedDict.items"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.obtian_tra", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.obtian_tes_with_neg_samples", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.process_seqs", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.utils.process_seqs_with_neg_samples"], []], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.run_gru4rec.log_eval_metrics": [[56, 66], ["len", "test_df[].nunique", "eval_sessions_metrics_log.append", "print", "nar_utils.save_eval_benchmark_metrics_csv"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.nar_utils.save_eval_benchmark_metrics_csv"], ["def", "log_eval_metrics", "(", "metrics_results", ",", "eval_sessions_metrics_log", ",", "test_df", ",", "temp_folder", ")", ":", "\n", "    ", "metrics_results", "[", "'clicks_count'", "]", "=", "len", "(", "test_df", ")", "\n", "metrics_results", "[", "'sessions_count'", "]", "=", "test_df", "[", "'SessionId'", "]", ".", "nunique", "(", ")", "\n", "\n", "eval_sessions_metrics_log", ".", "append", "(", "metrics_results", ")", "\n", "\n", "print", "(", "'Exporting results to temporary folder: {}'", ".", "format", "(", "temp_folder", ")", ")", "\n", "save_eval_benchmark_metrics_csv", "(", "eval_sessions_metrics_log", ",", "temp_folder", ",", "\n", "training_hours_for_each_eval", "=", "ARGS", ".", "training_hours_for_each_eval", ",", "\n", "output_csv", "=", "EVAL_METRICS_FILE", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.run_gru4rec.create_eval_metrics": [[68, 89], ["clicked_items_state.get_recent_clicks_buffer", "eval_metrics.append", "eval_metrics.append", "eval_metrics.append", "eval_metrics.append", "eval_metrics.append", "metric", "metrics.ItemCoverage", "metrics.ExpectedRankSensitiveNovelty", "metrics.ExpectedRankRelevanceSensitiveNovelty", "metrics.ContentExpectedRankRelativeSensitiveIntraListDiversity", "metrics.ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_recent_clicks_buffer"], ["", "def", "create_eval_metrics", "(", "top_n", ",", "\n", "eval_negative_sample_relevance", ",", "\n", "content_article_embeddings_matrix", ",", "\n", "clicked_items_state", ")", ":", "\n", "\n", "    ", "relevance_positive_sample", "=", "1.0", "\n", "#Empirical: The weight of negative samples", "\n", "relevance_negative_samples", "=", "eval_negative_sample_relevance", "\n", "\n", "recent_clicks_buffer", "=", "clicked_items_state", ".", "get_recent_clicks_buffer", "(", ")", "\n", "\n", "eval_metrics", "=", "[", "metric", "(", "topn", "=", "top_n", ")", "for", "metric", "in", "[", "HitRate", ",", "MRR", "]", "]", "\n", "\n", "#TODO: Known issue: Item coverage here is not considering as recommendable items those who were not in the train set", "\n", "eval_metrics", ".", "append", "(", "ItemCoverage", "(", "top_n", ",", "recent_clicks_buffer", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ExpectedRankSensitiveNovelty", "(", "top_n", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ExpectedRankRelevanceSensitiveNovelty", "(", "top_n", ",", "relevance_positive_sample", ",", "relevance_negative_samples", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ContentExpectedRankRelativeSensitiveIntraListDiversity", "(", "top_n", ",", "content_article_embeddings_matrix", ")", ")", "\n", "eval_metrics", ".", "append", "(", "ContentExpectedRankRelativeRelevanceSensitiveIntraListDiversity", "(", "top_n", ",", "content_article_embeddings_matrix", ",", "relevance_positive_sample", ",", "relevance_negative_samples", ")", ")", "\n", "\n", "return", "eval_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2_evaluation.evaluate_sessions_batch_neg_samples": [[16, 182], ["len", "len", "print", "time.clock", "time.time", "test_data.sort_values", "numpy.zeros", "test_data.groupby().size().cumsum", "numpy.arange().astype", "np.arange().astype.max", "numpy.zeros", "numpy.random.seed", "print", "evaluation.compute_metrics_results", "test_data[].unique", "range", "test_data[].nunique", "test_data.groupby().size", "len", "len", "numpy.arange", "valid_mask.sum", "pr.predict_next_batch.fillna", "pr.predict_next_batch.loc[].iteritems", "numpy.expand_dims", "numpy.array", "evaluation.update_metrics", "numpy.arange", "time.clock", "time.time", "numpy.unique", "pr.predict_next_batch", "pr.predict_next_batch", "preds_items_list.append", "labels.append", "clicked_items_state.get_articles_recent_pop_norm", "clicked_items_state.get_articles_recent_pop_norm", "len", "numpy.array", "metric.add", "test_data.groupby", "numpy.array", "numpy.hstack", "pr.predict_next_batch.loc[].sort_values", "len", "numpy.in1d"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.compute_metrics_results", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.evaluation.update_metrics", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.predict_next_batch", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.predict_next_batch", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_articles_recent_pop_norm", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.metrics.CategoryExpectedIntraListDiversity.add"], ["def", "evaluate_sessions_batch_neg_samples", "(", "pr", ",", "streaming_metrics", ",", "test_data", ",", "clicked_items_state", ",", "items", "=", "None", ",", "cut_off", "=", "20", ",", "batch_size", "=", "100", ",", "\n", "count_clicks_in_test_items_not_in_train_set", "=", "0", ",", "\n", "items_inverted_dict", "=", "None", ",", "\n", "session_key", "=", "'SessionId'", ",", "item_key", "=", "'ItemId'", ",", "time_key", "=", "'Time'", ",", "session_neg_samples_key", "=", "'neg_samples'", ")", ":", "\n", "    ", "'''\n    Evaluates the GRU4Rec network wrt. recommendation accuracy measured by recall@N and MRR@N.\n\n    Parameters\n    --------\n    pr : gru4rec.GRU4Rec\n        A trained instance of the GRU4Rec network.\n    streaming_metrics : list\n        A list of metric classes providing the proper methods\n    test_data : pandas.DataFrame\n        Test data. It contains the transactions of the test set.It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n        It must have a header. Column names are arbitrary, but must correspond to the keys you use in this function.\n    items : 1D list or None\n        The list of item ID that you want to compare the score of the relevant item to. If None, all items of the training set are used. Default value is None.\n    cut-off : int\n        Cut-off value (i.e. the length of the recommendation list; N for recall@N and MRR@N). Defauld value is 20.\n    batch_size : int\n        Number of events bundled into a batch during evaluation. Speeds up evaluation. If it is set high, the memory consumption increases. Default value is 100.\n    session_key : string\n        Header of the session ID column in the input file (default: 'SessionId')\n    item_key : string\n        Header of the item ID column in the input file (default: 'ItemId')\n    time_key : string\n        Header of the timestamp column in the input file (default: 'Time')    \n    session_neg_samples_key: string\n        Header of the list column with the negative samples for the session (Sampled during NAR module training) (default: 'neg_samples')\n\n    Returns\n    --------\n    out : list of tuples\n        (metric_name, value)\n    \n    '''", "\n", "\n", "#IMPORTANT: Filtering for prediction only items present in train set, to avoid errors on GRU4REC", "\n", "#test_data = pd.merge(test_data, pd.DataFrame({'ItemIdx':pr.itemidmap.values, item_key:pr.itemidmap.index}), on=item_key, how='inner')", "\n", "#test_data = test_data.groupby(session_key).filter(lambda x: len(x) > 1)", "\n", "\n", "actions", "=", "len", "(", "test_data", ")", "\n", "sessions", "=", "len", "(", "test_data", "[", "session_key", "]", ".", "unique", "(", ")", ")", "\n", "print", "(", "'START batch eval '", ",", "actions", ",", "' actions in '", ",", "sessions", ",", "' sessions'", ")", "\n", "sc", "=", "time", ".", "clock", "(", ")", ";", "\n", "st", "=", "time", ".", "time", "(", ")", ";", "\n", "\n", "#for m in streaming_metrics:", "\n", "#    m.reset()", "\n", "\n", "pr", ".", "predict", "=", "None", "#In case someone would try to run with both items=None and not None on the same model without realizing that the predict function needs to be replaced", "\n", "test_data", ".", "sort_values", "(", "[", "session_key", ",", "time_key", ",", "item_key", "]", ",", "inplace", "=", "True", ")", "\n", "offset_sessions", "=", "np", ".", "zeros", "(", "test_data", "[", "session_key", "]", ".", "nunique", "(", ")", "+", "1", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "offset_sessions", "[", "1", ":", "]", "=", "test_data", ".", "groupby", "(", "session_key", ")", ".", "size", "(", ")", ".", "cumsum", "(", ")", "\n", "\n", "if", "len", "(", "offset_sessions", ")", "-", "1", "<", "batch_size", ":", "\n", "        ", "batch_size", "=", "len", "(", "offset_sessions", ")", "-", "1", "\n", "\n", "", "iters", "=", "np", ".", "arange", "(", "batch_size", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "\n", "maxiter", "=", "iters", ".", "max", "(", ")", "\n", "start", "=", "offset_sessions", "[", "iters", "]", "\n", "end", "=", "offset_sessions", "[", "iters", "+", "1", "]", "\n", "\n", "in_idx", "=", "np", ".", "zeros", "(", "batch_size", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "np", ".", "random", ".", "seed", "(", "42", ")", "\n", "\n", "while", "True", ":", "\n", "\n", "        ", "valid_mask", "=", "iters", ">=", "0", "\n", "if", "valid_mask", ".", "sum", "(", ")", "==", "0", ":", "\n", "            ", "break", "\n", "\n", "", "start_valid", "=", "start", "[", "valid_mask", "]", "\n", "\n", "minlen", "=", "(", "end", "[", "valid_mask", "]", "-", "start_valid", ")", ".", "min", "(", ")", "\n", "in_idx", "[", "valid_mask", "]", "=", "test_data", "[", "item_key", "]", ".", "values", "[", "start_valid", "]", "\n", "\n", "\n", "for", "i", "in", "range", "(", "minlen", "-", "1", ")", ":", "\n", "\n", "\n", "            ", "out_idx", "=", "test_data", "[", "item_key", "]", ".", "values", "[", "start_valid", "+", "i", "+", "1", "]", "\n", "neg_samples", "=", "test_data", "[", "session_neg_samples_key", "]", ".", "values", "[", "start_valid", "+", "i", "+", "1", "]", "\n", "\n", "input_item_ids", "=", "in_idx", "\n", "if", "items", "is", "not", "None", ":", "\n", "                ", "uniq_out", "=", "np", ".", "unique", "(", "np", ".", "array", "(", "out_idx", ",", "dtype", "=", "np", ".", "int32", ")", ")", "\n", "preds", "=", "pr", ".", "predict_next_batch", "(", "iters", ",", "input_item_ids", ",", "np", ".", "hstack", "(", "[", "items", ",", "uniq_out", "[", "~", "np", ".", "in1d", "(", "uniq_out", ",", "items", ")", "]", "]", ")", ",", "batch_size", ")", "\n", "", "else", ":", "\n", "                ", "preds", "=", "pr", ".", "predict_next_batch", "(", "iters", ",", "input_item_ids", ",", "None", ",", "batch_size", ")", "\n", "\n", "\n", "", "preds", ".", "fillna", "(", "0", ",", "inplace", "=", "True", ")", "\n", "in_idx", "[", "valid_mask", "]", "=", "out_idx", "\n", "\n", "\n", "preds_items_list", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "j", "=", "0", "\n", "for", "part", ",", "series", "in", "preds", ".", "loc", "[", ":", ",", "valid_mask", "]", ".", "iteritems", "(", ")", ":", "\n", "\n", "#Combining the next clicked item with a list of negative samples", "\n", "                ", "items_to_predict", "=", "[", "out_idx", "[", "j", "]", "]", "+", "neg_samples", "[", "j", "]", "\n", "\n", "preds_items", "=", "preds", ".", "loc", "[", "items_to_predict", "]", ".", "sort_values", "(", "part", ",", "ascending", "=", "False", ")", "[", "part", "]", "\n", "\n", "preds_items_list", ".", "append", "(", "preds_items", ".", "index", ".", "values", ")", "\n", "labels", ".", "append", "(", "out_idx", "[", "j", "]", ")", "\n", "\n", "j", "+=", "1", "\n", "\n", "#for m in streaming_metrics:", "\n", "#    m.add(preds_for_metrics, label_for_metrics)                    ", "\n", "\n", "#item_ids_to_original_vect = np.vectorize(lambda x: items_inverted_dict[x])", "\n", "\n", "#clicked_items = input_item_ids[np.nonzero(input_item_ids)]", "\n", "#clicked_items_original = item_ids_to_original_vect(clicked_items)", "\n", "\n", "#labels_original_ids = item_ids_to_original_vect(label_for_metrics)", "\n", "#preds_original_ids = item_ids_to_original_vect(preds_for_metrics)", "\n", "\n", "\n", "", "preds_for_metrics", "=", "np", ".", "expand_dims", "(", "preds_items_list", ",", "0", ")", "\n", "label_for_metrics", "=", "np", ".", "array", "(", "[", "labels", "]", ")", "\n", "\n", "\n", "labels_norm_pop", "=", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", "[", "label_for_metrics", "]", "\n", "preds_norm_pop", "=", "clicked_items_state", ".", "get_articles_recent_pop_norm", "(", ")", "[", "preds_for_metrics", "]", "\n", "\n", "update_metrics", "(", "preds_for_metrics", ",", "label_for_metrics", ",", "labels_norm_pop", ",", "preds_norm_pop", ",", "\n", "input_item_ids", ",", "streaming_metrics", ")", "\n", "\n", "\n", "", "start", "=", "start", "+", "minlen", "-", "1", "\n", "mask", "=", "np", ".", "arange", "(", "len", "(", "iters", ")", ")", "[", "(", "valid_mask", ")", "&", "(", "end", "-", "start", "<=", "1", ")", "]", "\n", "for", "idx", "in", "mask", ":", "\n", "            ", "maxiter", "+=", "1", "\n", "if", "maxiter", ">=", "len", "(", "offset_sessions", ")", "-", "1", ":", "\n", "                ", "iters", "[", "idx", "]", "=", "-", "1", "\n", "", "else", ":", "\n", "                ", "iters", "[", "idx", "]", "=", "maxiter", "\n", "start", "[", "idx", "]", "=", "offset_sessions", "[", "maxiter", "]", "\n", "end", "[", "idx", "]", "=", "offset_sessions", "[", "maxiter", "+", "1", "]", "\n", "\n", "", "", "", "print", "(", "'END batch eval '", ",", "(", "time", ".", "clock", "(", ")", "-", "sc", ")", ",", "'c / '", ",", "(", "time", ".", "time", "(", ")", "-", "st", ")", ",", "'s'", ")", "\n", "\n", "#If there are clicks in test set items not viewed during training, as the method is not able", "\n", "#to predict those items, assume that the recommendation was not correct in metrics", "\n", "if", "count_clicks_in_test_items_not_in_train_set", ">", "0", ":", "\n", "#perc_test_items_not_found = count_clicks_in_test_items_not_in_train_set / (len(test_data)+count_clicks_in_test_items_not_in_train_set)", "\n", "#print('{} ({}%) test set clicks in items not present in train set.'.format(count_clicks_in_test_items_not_in_train_set, perc_test_items_not_found))", "\n", "\n", "#Include additional prediction errors (for accuracy metrics) when  next-clicked item is not available in train set", "\n", "        ", "for", "metric", "in", "streaming_metrics", ":", "\n", "            ", "if", "metric", ".", "name", "in", "[", "HitRate", ".", "name", ",", "MRR", ".", "name", "]", ":", "\n", "                ", "fake_targets", "=", "[", "[", "1", "]", "*", "count_clicks_in_test_items_not_in_train_set", "]", "\n", "fake_preds", "=", "np", ".", "array", "(", "[", "[", "[", "0", "]", "]", "*", "count_clicks_in_test_items_not_in_train_set", "]", ")", "\n", "metric", ".", "add", "(", "fake_preds", ",", "fake_targets", ")", "\n", "\n", "\n", "", "", "", "metric_results", "=", "compute_metrics_results", "(", "streaming_metrics", ",", "recommender", "=", "'gru4rec'", ")", "\n", "\n", "return", "metric_results", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.__init__": [[89, 161], ["loss.startswith", "float", "numpy.ones", "numpy.fill_diagonal", "theano.shared", "final_act.startswith", "numpy.ones", "numpy.fill_diagonal", "theano.shared", "float", "final_act.startswith", "float"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "loss", ",", "final_act", ",", "hidden_act", ",", "layers", ",", "\n", "n_epochs", "=", "10", ",", "batch_size", "=", "50", ",", "dropout_p_hidden", "=", "0.5", ",", "dropout_p_embed", "=", "0.0", ",", "learning_rate", "=", "0.05", ",", "momentum", "=", "0.0", ",", "lmbd", "=", "0.0", ",", "embedding", "=", "0", ",", "n_sample", "=", "0", ",", "sample_alpha", "=", "0.75", ",", "smoothing", "=", "0", ",", "\n", "adapt", "=", "'adagrad'", ",", "decay", "=", "0.9", ",", "grad_cap", "=", "0", ",", "\n", "sigma", "=", "0", ",", "init_as_normal", "=", "False", ",", "reset_after_session", "=", "True", ",", "train_random_order", "=", "False", ",", "time_sort", "=", "True", ",", "\n", "session_key", "=", "'SessionId'", ",", "item_key", "=", "'ItemId'", ",", "time_key", "=", "'Time'", ",", "\n", "clicked_items_state", "=", "None", "\n", "#last_clicks_buffer_size=20000", "\n", ")", ":", "\n", "        ", "self", ".", "layers", "=", "layers", "\n", "self", ".", "n_epochs", "=", "n_epochs", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "dropout_p_hidden", "=", "dropout_p_hidden", "\n", "self", ".", "dropout_p_embed", "=", "dropout_p_embed", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "decay", "=", "decay", "\n", "self", ".", "momentum", "=", "momentum", "\n", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "init_as_normal", "=", "init_as_normal", "\n", "self", ".", "reset_after_session", "=", "reset_after_session", "\n", "self", ".", "session_key", "=", "session_key", "\n", "self", ".", "item_key", "=", "item_key", "\n", "self", ".", "time_key", "=", "time_key", "\n", "self", ".", "grad_cap", "=", "grad_cap", "\n", "self", ".", "train_random_order", "=", "train_random_order", "\n", "self", ".", "lmbd", "=", "lmbd", "\n", "self", ".", "embedding", "=", "embedding", "\n", "self", ".", "time_sort", "=", "time_sort", "\n", "#self.last_clicks_buffer_size = last_clicks_buffer_size", "\n", "self", ".", "clicked_items_state", "=", "clicked_items_state", "\n", "if", "adapt", "==", "'rmsprop'", ":", "self", ".", "adapt", "=", "'rmsprop'", "\n", "elif", "adapt", "==", "'adagrad'", ":", "self", ".", "adapt", "=", "'adagrad'", "\n", "elif", "adapt", "==", "'adadelta'", ":", "self", ".", "adapt", "=", "'adadelta'", "\n", "elif", "adapt", "==", "'adam'", ":", "self", ".", "adapt", "=", "'adam'", "\n", "else", ":", "self", ".", "adapt", "=", "False", "\n", "self", ".", "final_act", "=", "final_act", "\n", "if", "final_act", "==", "'linear'", ":", "self", ".", "final_activation", "=", "self", ".", "linear", "\n", "elif", "final_act", "==", "'relu'", ":", "self", ".", "final_activation", "=", "self", ".", "relu", "\n", "elif", "final_act", "==", "'softmax'", ":", "self", ".", "final_activation", "=", "self", ".", "softmax", "\n", "elif", "final_act", "==", "'tanh'", ":", "self", ".", "final_activation", "=", "self", ".", "tanh", "\n", "elif", "final_act", "==", "'softmax_logit'", ":", "self", ".", "final_activation", "=", "self", ".", "softmax_logit", "\n", "elif", "final_act", ".", "startswith", "(", "'leaky-'", ")", ":", "\n", "            ", "self", ".", "final_activation", "=", "self", ".", "leaky", "\n", "self", ".", "leak", "=", "float", "(", "final_act", "[", "6", ":", "]", ")", "\n", "", "elif", "final_act", ".", "startswith", "(", "'elu-'", ")", ":", "\n", "            ", "self", ".", "final_activation", "=", "self", ".", "elu", "\n", "self", ".", "elu_param", "=", "float", "(", "final_act", "[", "4", ":", "]", ")", "\n", "", "else", ":", "raise", "NotImplementedError", "\n", "self", ".", "loss", "=", "loss", "\n", "if", "loss", "==", "'cross-entropy'", ":", "self", ".", "loss_function", "=", "self", ".", "cross_entropy", "\n", "elif", "loss", "==", "'bpr'", ":", "self", ".", "loss_function", "=", "self", ".", "bpr", "\n", "elif", "loss", ".", "startswith", "(", "'bpr-max-'", ")", ":", "\n", "            ", "self", ".", "loss_function", "=", "self", ".", "bpr_max", "\n", "self", ".", "bpreg", "=", "float", "(", "loss", "[", "8", ":", "]", ")", "\n", "self", ".", "hack_matrix", "=", "np", ".", "ones", "(", "(", "self", ".", "batch_size", ",", "self", ".", "batch_size", "+", "n_sample", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "np", ".", "fill_diagonal", "(", "self", ".", "hack_matrix", ",", "0", ")", "\n", "self", ".", "hack_matrix", "=", "theano", ".", "shared", "(", "self", ".", "hack_matrix", ",", "borrow", "=", "True", ")", "\n", "", "elif", "loss", "==", "'top1'", ":", "self", ".", "loss_function", "=", "self", ".", "top1", "\n", "elif", "loss", "==", "'top1-max'", ":", "\n", "            ", "self", ".", "loss_function", "=", "self", ".", "top1_max", "\n", "self", ".", "hack_matrix", "=", "np", ".", "ones", "(", "(", "self", ".", "batch_size", ",", "self", ".", "batch_size", "+", "n_sample", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "np", ".", "fill_diagonal", "(", "self", ".", "hack_matrix", ",", "0", ")", "\n", "self", ".", "hack_matrix", "=", "theano", ".", "shared", "(", "self", ".", "hack_matrix", ",", "borrow", "=", "True", ")", "\n", "", "elif", "loss", "==", "'xe_logit'", ":", "self", ".", "loss_function", "=", "self", ".", "cross_entropy_logits", "\n", "else", ":", "raise", "NotImplementedError", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "if", "hidden_act", "==", "'relu'", ":", "self", ".", "hidden_activation", "=", "self", ".", "relu", "\n", "elif", "hidden_act", "==", "'tanh'", ":", "self", ".", "hidden_activation", "=", "self", ".", "tanh", "\n", "elif", "hidden_act", "==", "'linear'", ":", "self", ".", "hidden_activation", "=", "self", ".", "linear", "\n", "else", ":", "raise", "NotImplementedError", "\n", "self", ".", "n_sample", "=", "n_sample", "\n", "self", ".", "sample_alpha", "=", "sample_alpha", "\n", "self", ".", "smoothing", "=", "smoothing", "\n", "######################ACTIVATION FUNCTIONS#####################", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.linear": [[162, 164], ["None"], "methods", ["None"], ["", "def", "linear", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "X", "\n", "", "def", "tanh", "(", "self", ",", "X", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.tanh": [[164, 166], ["theano.tensor.tanh"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.tanh"], ["", "def", "tanh", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "T", ".", "tanh", "(", "X", ")", "\n", "", "def", "softmax", "(", "self", ",", "X", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.softmax": [[166, 169], ["theano.tensor.exp", "theano.tensor.exp.sum().dimshuffle", "X.max().dimshuffle", "theano.tensor.exp.sum", "X.max"], "methods", ["None"], ["", "def", "softmax", "(", "self", ",", "X", ")", ":", "\n", "        ", "e_x", "=", "T", ".", "exp", "(", "X", "-", "X", ".", "max", "(", "axis", "=", "1", ")", ".", "dimshuffle", "(", "0", ",", "'x'", ")", ")", "\n", "return", "e_x", "/", "e_x", ".", "sum", "(", "axis", "=", "1", ")", ".", "dimshuffle", "(", "0", ",", "'x'", ")", "\n", "", "def", "softmax_logit", "(", "self", ",", "X", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.softmax_logit": [[169, 172], ["X.max().dimshuffle", "theano.tensor.log", "theano.tensor.exp().sum().dimshuffle", "X.max", "theano.tensor.exp().sum", "theano.tensor.exp"], "methods", ["None"], ["", "def", "softmax_logit", "(", "self", ",", "X", ")", ":", "\n", "        ", "X", "=", "X", "-", "X", ".", "max", "(", "axis", "=", "1", ")", ".", "dimshuffle", "(", "0", ",", "'x'", ")", "\n", "return", "T", ".", "log", "(", "T", ".", "exp", "(", "X", ")", ".", "sum", "(", "axis", "=", "1", ")", ".", "dimshuffle", "(", "0", ",", "'x'", ")", ")", "-", "X", "\n", "", "def", "softmax_neg", "(", "self", ",", "X", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.softmax_neg": [[172, 179], ["hasattr", "theano.tensor.fill_diagonal", "theano.tensor.fill_diagonal.sum().dimshuffle", "theano.tensor.exp", "theano.tensor.exp", "theano.tensor.fill_diagonal.sum", "X.max().dimshuffle", "X.max().dimshuffle", "X.max", "X.max"], "methods", ["None"], ["", "def", "softmax_neg", "(", "self", ",", "X", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ",", "'hack_matrix'", ")", ":", "\n", "            ", "X", "=", "X", "*", "self", ".", "hack_matrix", "\n", "e_x", "=", "T", ".", "exp", "(", "X", "-", "X", ".", "max", "(", "axis", "=", "1", ")", ".", "dimshuffle", "(", "0", ",", "'x'", ")", ")", "*", "self", ".", "hack_matrix", "\n", "", "else", ":", "\n", "            ", "e_x", "=", "T", ".", "fill_diagonal", "(", "T", ".", "exp", "(", "X", "-", "X", ".", "max", "(", "axis", "=", "1", ")", ".", "dimshuffle", "(", "0", ",", "'x'", ")", ")", ",", "0", ")", "\n", "", "return", "e_x", "/", "e_x", ".", "sum", "(", "axis", "=", "1", ")", ".", "dimshuffle", "(", "0", ",", "'x'", ")", "\n", "", "def", "relu", "(", "self", ",", "X", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.relu": [[179, 181], ["theano.tensor.maximum"], "methods", ["None"], ["", "def", "relu", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "T", ".", "maximum", "(", "X", ",", "0", ")", "\n", "", "def", "leaky", "(", "self", ",", "X", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.leaky": [[181, 183], ["theano.tensor.switch", "theano.tensor.ge"], "methods", ["None"], ["", "def", "leaky", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "T", ".", "switch", "(", "T", ".", "ge", "(", "X", ",", "0", ")", ",", "X", ",", "self", ".", "leak", "*", "X", ")", "\n", "", "def", "elu", "(", "self", ",", "X", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.elu": [[183, 185], ["theano.tensor.switch", "theano.tensor.ge", "theano.tensor.exp"], "methods", ["None"], ["", "def", "elu", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "T", ".", "switch", "(", "T", ".", "ge", "(", "X", ",", "0", ")", ",", "X", ",", "self", ".", "elu_param", "*", "(", "T", ".", "exp", "(", "X", ")", "-", "1", ")", ")", "\n", "", "def", "sigmoid", "(", "self", ",", "X", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid": [[185, 187], ["theano.tensor.nnet.sigmoid"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid"], ["", "def", "sigmoid", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "T", ".", "nnet", ".", "sigmoid", "(", "X", ")", "\n", "#################################LOSS FUNCTIONS################################", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.cross_entropy": [[188, 194], ["theano.tensor.cast", "theano.tensor.cast", "theano.tensor.mean", "theano.tensor.mean", "theano.tensor.log", "theano.tensor.sum", "theano.tensor.log", "theano.tensor.diag", "theano.tensor.log", "theano.tensor.diag"], "methods", ["None"], ["", "def", "cross_entropy", "(", "self", ",", "yhat", ")", ":", "\n", "        ", "if", "self", ".", "smoothing", ":", "\n", "            ", "n_out", "=", "self", ".", "batch_size", "+", "self", ".", "n_sample", "\n", "return", "T", ".", "cast", "(", "T", ".", "mean", "(", "(", "1.0", "-", "(", "n_out", "/", "(", "n_out", "-", "1", ")", ")", "*", "self", ".", "smoothing", ")", "*", "(", "-", "T", ".", "log", "(", "T", ".", "diag", "(", "yhat", ")", "+", "1e-24", ")", ")", "+", "(", "self", ".", "smoothing", "/", "(", "n_out", "-", "1", ")", ")", "*", "T", ".", "sum", "(", "-", "T", ".", "log", "(", "yhat", "+", "1e-24", ")", ",", "axis", "=", "1", ")", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "", "else", ":", "\n", "            ", "return", "T", ".", "cast", "(", "T", ".", "mean", "(", "-", "T", ".", "log", "(", "T", ".", "diag", "(", "yhat", ")", "+", "1e-24", ")", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "", "", "def", "cross_entropy_logits", "(", "self", ",", "yhat", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.cross_entropy_logits": [[194, 200], ["theano.tensor.cast", "theano.tensor.cast", "theano.tensor.mean", "theano.tensor.mean", "theano.tensor.diag", "theano.tensor.diag", "theano.tensor.sum"], "methods", ["None"], ["", "", "def", "cross_entropy_logits", "(", "self", ",", "yhat", ")", ":", "\n", "        ", "if", "self", ".", "smoothing", ":", "\n", "            ", "n_out", "=", "self", ".", "batch_size", "+", "self", ".", "n_sample", "\n", "return", "T", ".", "cast", "(", "T", ".", "mean", "(", "(", "1.0", "-", "(", "n_out", "/", "(", "n_out", "-", "1", ")", ")", "*", "self", ".", "smoothing", ")", "*", "T", ".", "diag", "(", "yhat", ")", "+", "(", "self", ".", "smoothing", "/", "(", "n_out", "-", "1", ")", ")", "*", "T", ".", "sum", "(", "yhat", ",", "axis", "=", "1", ")", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "", "else", ":", "\n", "            ", "return", "T", ".", "cast", "(", "T", ".", "mean", "(", "T", ".", "diag", "(", "yhat", ")", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "", "", "def", "bpr", "(", "self", ",", "yhat", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.bpr": [[200, 202], ["theano.tensor.cast", "theano.tensor.mean", "theano.tensor.log", "theano.tensor.nnet.sigmoid", "theano.tensor.diag"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid"], ["", "", "def", "bpr", "(", "self", ",", "yhat", ")", ":", "\n", "        ", "return", "T", ".", "cast", "(", "T", ".", "mean", "(", "-", "T", ".", "log", "(", "T", ".", "nnet", ".", "sigmoid", "(", "T", ".", "diag", "(", "yhat", ")", "-", "yhat", ".", "T", ")", ")", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "", "def", "bpr_max", "(", "self", ",", "yhat", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.bpr_max": [[202, 205], ["theano.tensor.cast", "gru4rec2.GRU4Rec.softmax_neg", "theano.tensor.mean", "theano.tensor.log", "theano.tensor.sum", "theano.tensor.sum", "theano.tensor.nnet.sigmoid", "theano.tensor.diag"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.softmax_neg", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid"], ["", "def", "bpr_max", "(", "self", ",", "yhat", ")", ":", "\n", "        ", "softmax_scores", "=", "self", ".", "softmax_neg", "(", "yhat", ")", ".", "T", "\n", "return", "T", ".", "cast", "(", "T", ".", "mean", "(", "-", "T", ".", "log", "(", "T", ".", "sum", "(", "T", ".", "nnet", ".", "sigmoid", "(", "T", ".", "diag", "(", "yhat", ")", "-", "yhat", ".", "T", ")", "*", "softmax_scores", ",", "axis", "=", "0", ")", "+", "1e-24", ")", "+", "self", ".", "bpreg", "*", "T", ".", "sum", "(", "(", "yhat", ".", "T", "**", "2", ")", "*", "softmax_scores", ",", "axis", "=", "0", ")", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "", "def", "top1", "(", "self", ",", "yhat", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.top1": [[205, 208], ["theano.tensor.cast", "theano.tensor.mean", "theano.tensor.mean", "theano.tensor.nnet.sigmoid", "theano.tensor.nnet.sigmoid", "theano.tensor.nnet.sigmoid", "theano.tensor.diag", "theano.tensor.diag"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid"], ["", "def", "top1", "(", "self", ",", "yhat", ")", ":", "\n", "        ", "yhatT", "=", "yhat", ".", "T", "\n", "return", "T", ".", "cast", "(", "T", ".", "mean", "(", "T", ".", "mean", "(", "T", ".", "nnet", ".", "sigmoid", "(", "-", "T", ".", "diag", "(", "yhat", ")", "+", "yhatT", ")", "+", "T", ".", "nnet", ".", "sigmoid", "(", "yhatT", "**", "2", ")", ",", "axis", "=", "0", ")", "-", "T", ".", "nnet", ".", "sigmoid", "(", "T", ".", "diag", "(", "yhat", ")", "**", "2", ")", "/", "(", "self", ".", "batch_size", "+", "self", ".", "n_sample", ")", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "", "def", "top1_max", "(", "self", ",", "yhat", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.top1_max": [[208, 213], ["gru4rec2.GRU4Rec.softmax_neg", "theano.tensor.cast", "theano.tensor.mean", "theano.tensor.nnet.sigmoid", "theano.tensor.nnet.sigmoid", "theano.tensor.sum", "theano.tensor.diag"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.softmax_neg", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid"], ["", "def", "top1_max", "(", "self", ",", "yhat", ")", ":", "\n", "        ", "yhatT", "=", "yhat", ".", "T", "\n", "softmax_scores", "=", "self", ".", "softmax_neg", "(", "yhat", ")", "\n", "y", "=", "softmax_scores", ".", "T", "*", "(", "T", ".", "nnet", ".", "sigmoid", "(", "-", "T", ".", "diag", "(", "yhat", ")", "+", "yhatT", ")", "+", "T", ".", "nnet", ".", "sigmoid", "(", "yhatT", "**", "2", ")", ")", "\n", "return", "T", ".", "cast", "(", "T", ".", "mean", "(", "T", ".", "sum", "(", "y", ",", "axis", "=", "0", ")", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "###############################################################################", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.floatX": [[214, 216], ["numpy.asarray"], "methods", ["None"], ["", "def", "floatX", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "np", ".", "asarray", "(", "X", ",", "dtype", "=", "theano", ".", "config", ".", "floatX", ")", "\n", "", "def", "init_weights", "(", "self", ",", "shape", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_weights": [[216, 218], ["theano.shared", "gru4rec2.GRU4Rec.init_matrix"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_matrix"], ["", "def", "init_weights", "(", "self", ",", "shape", ")", ":", "\n", "        ", "return", "theano", ".", "shared", "(", "self", ".", "init_matrix", "(", "shape", ")", ",", "borrow", "=", "True", ")", "\n", "", "def", "init_matrix", "(", "self", ",", "shape", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_matrix": [[218, 225], ["numpy.sqrt", "gru4rec2.GRU4Rec.floatX", "gru4rec2.GRU4Rec.floatX", "numpy.random.randn", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.floatX", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.floatX"], ["", "def", "init_matrix", "(", "self", ",", "shape", ")", ":", "\n", "        ", "if", "self", ".", "sigma", "!=", "0", ":", "sigma", "=", "self", ".", "sigma", "\n", "else", ":", "sigma", "=", "np", ".", "sqrt", "(", "6.0", "/", "(", "shape", "[", "0", "]", "+", "shape", "[", "1", "]", ")", ")", "\n", "if", "self", ".", "init_as_normal", ":", "\n", "            ", "return", "self", ".", "floatX", "(", "np", ".", "random", ".", "randn", "(", "*", "shape", ")", "*", "sigma", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "floatX", "(", "np", ".", "random", ".", "rand", "(", "*", "shape", ")", "*", "sigma", "*", "2", "-", "sigma", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.extend_weights": [[226, 232], ["W.get_value", "W.set_value", "numpy.sqrt", "gru4rec2.GRU4Rec.floatX", "gru4rec2.GRU4Rec.floatX", "numpy.vstack", "numpy.random.randn", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.floatX", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.floatX"], ["", "", "def", "extend_weights", "(", "self", ",", "W", ",", "n_new", ")", ":", "\n", "        ", "matrix", "=", "W", ".", "get_value", "(", ")", "\n", "sigma", "=", "self", ".", "sigma", "if", "self", ".", "sigma", "!=", "0", "else", "np", ".", "sqrt", "(", "6.0", "/", "(", "matrix", ".", "shape", "[", "0", "]", "+", "matrix", ".", "shape", "[", "1", "]", "+", "n_new", ")", ")", "\n", "if", "self", ".", "init_as_normal", ":", "new_rows", "=", "self", ".", "floatX", "(", "np", ".", "random", ".", "randn", "(", "n_new", ",", "matrix", ".", "shape", "[", "1", "]", ")", "*", "sigma", ")", "\n", "else", ":", "new_rows", "=", "self", ".", "floatX", "(", "np", ".", "random", ".", "rand", "(", "n_new", ",", "matrix", ".", "shape", "[", "1", "]", ")", "*", "sigma", "*", "2", "-", "sigma", ")", "\n", "W", ".", "set_value", "(", "np", ".", "vstack", "(", "[", "matrix", ",", "new_rows", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init": [[233, 265], ["gru4rec2.GRU4Rec.init_candidate_sampling_manager", "data.sort_values", "numpy.zeros", "data.groupby().size().cumsum", "numpy.random.seed", "range", "gru4rec2.GRU4Rec.init_weights", "theano.shared", "gru4rec2.GRU4Rec.init_weights", "len", "m.append", "m.append", "m.append", "gru4rec2.GRU4Rec.Wx.append", "gru4rec2.GRU4Rec.Wh.append", "m2.append", "m2.append", "gru4rec2.GRU4Rec.Wrz.append", "gru4rec2.GRU4Rec.Bh.append", "gru4rec2.GRU4Rec.H.append", "data[].nunique", "data.groupby().size", "gru4rec2.GRU4Rec.init_matrix", "gru4rec2.GRU4Rec.init_matrix", "gru4rec2.GRU4Rec.init_matrix", "theano.shared", "gru4rec2.GRU4Rec.init_weights", "gru4rec2.GRU4Rec.init_matrix", "gru4rec2.GRU4Rec.init_matrix", "theano.shared", "theano.shared", "theano.shared", "numpy.zeros", "data.groupby", "numpy.hstack", "numpy.hstack", "numpy.zeros", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_candidate_sampling_manager", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_weights", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_weights", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_matrix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_matrix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_matrix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_weights", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_matrix", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_matrix"], ["", "def", "init", "(", "self", ",", "data", ")", ":", "\n", "#TODO: Parametrize Last Clicks buffer size", "\n", "#self.init_last_clicks_buffer()", "\n", "#Initialize negative candidate sampling manager", "\n", "        ", "self", ".", "init_candidate_sampling_manager", "(", ")", "\n", "\n", "data", ".", "sort_values", "(", "[", "self", ".", "session_key", ",", "self", ".", "time_key", "]", ",", "inplace", "=", "True", ")", "\n", "offset_sessions", "=", "np", ".", "zeros", "(", "data", "[", "self", ".", "session_key", "]", ".", "nunique", "(", ")", "+", "1", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "offset_sessions", "[", "1", ":", "]", "=", "data", ".", "groupby", "(", "self", ".", "session_key", ")", ".", "size", "(", ")", ".", "cumsum", "(", ")", "\n", "np", ".", "random", ".", "seed", "(", "42", ")", "\n", "self", ".", "Wx", ",", "self", ".", "Wh", ",", "self", ".", "Wrz", ",", "self", ".", "Bh", ",", "self", ".", "H", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "if", "self", ".", "embedding", ":", "\n", "            ", "self", ".", "E", "=", "self", ".", "init_weights", "(", "(", "self", ".", "n_items", ",", "self", ".", "embedding", ")", ")", "\n", "n_features", "=", "self", ".", "embedding", "\n", "", "else", ":", "\n", "            ", "n_features", "=", "self", ".", "n_items", "\n", "", "for", "i", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "            ", "m", "=", "[", "]", "\n", "m", ".", "append", "(", "self", ".", "init_matrix", "(", "(", "self", ".", "layers", "[", "i", "-", "1", "]", "if", "i", ">", "0", "else", "n_features", ",", "self", ".", "layers", "[", "i", "]", ")", ")", ")", "\n", "m", ".", "append", "(", "self", ".", "init_matrix", "(", "(", "self", ".", "layers", "[", "i", "-", "1", "]", "if", "i", ">", "0", "else", "n_features", ",", "self", ".", "layers", "[", "i", "]", ")", ")", ")", "\n", "m", ".", "append", "(", "self", ".", "init_matrix", "(", "(", "self", ".", "layers", "[", "i", "-", "1", "]", "if", "i", ">", "0", "else", "n_features", ",", "self", ".", "layers", "[", "i", "]", ")", ")", ")", "\n", "self", ".", "Wx", ".", "append", "(", "theano", ".", "shared", "(", "value", "=", "np", ".", "hstack", "(", "m", ")", ",", "borrow", "=", "True", ")", ")", "\n", "self", ".", "Wh", ".", "append", "(", "self", ".", "init_weights", "(", "(", "self", ".", "layers", "[", "i", "]", ",", "self", ".", "layers", "[", "i", "]", ")", ")", ")", "\n", "m2", "=", "[", "]", "\n", "m2", ".", "append", "(", "self", ".", "init_matrix", "(", "(", "self", ".", "layers", "[", "i", "]", ",", "self", ".", "layers", "[", "i", "]", ")", ")", ")", "\n", "m2", ".", "append", "(", "self", ".", "init_matrix", "(", "(", "self", ".", "layers", "[", "i", "]", ",", "self", ".", "layers", "[", "i", "]", ")", ")", ")", "\n", "self", ".", "Wrz", ".", "append", "(", "theano", ".", "shared", "(", "value", "=", "np", ".", "hstack", "(", "m2", ")", ",", "borrow", "=", "True", ")", ")", "\n", "self", ".", "Bh", ".", "append", "(", "theano", ".", "shared", "(", "value", "=", "np", ".", "zeros", "(", "(", "self", ".", "layers", "[", "i", "]", "*", "3", ",", ")", ",", "dtype", "=", "theano", ".", "config", ".", "floatX", ")", ",", "borrow", "=", "True", ")", ")", "\n", "self", ".", "H", ".", "append", "(", "theano", ".", "shared", "(", "value", "=", "np", ".", "zeros", "(", "(", "self", ".", "batch_size", ",", "self", ".", "layers", "[", "i", "]", ")", ",", "dtype", "=", "theano", ".", "config", ".", "floatX", ")", ",", "borrow", "=", "True", ")", ")", "\n", "", "self", ".", "Wy", "=", "self", ".", "init_weights", "(", "(", "self", ".", "n_items", ",", "self", ".", "layers", "[", "-", "1", "]", ")", ")", "\n", "self", ".", "By", "=", "theano", ".", "shared", "(", "value", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_items", ",", "1", ")", ",", "dtype", "=", "theano", ".", "config", ".", "floatX", ")", ",", "borrow", "=", "True", ")", "\n", "return", "offset_sessions", "\n", "", "def", "dropout", "(", "self", ",", "X", ",", "drop_p", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout": [[265, 270], ["srng.binomial"], "methods", ["None"], ["", "def", "dropout", "(", "self", ",", "X", ",", "drop_p", ")", ":", "\n", "        ", "if", "drop_p", ">", "0", ":", "\n", "            ", "retain_prob", "=", "1", "-", "drop_p", "\n", "X", "*=", "srng", ".", "binomial", "(", "X", ".", "shape", ",", "p", "=", "retain_prob", ",", "dtype", "=", "theano", ".", "config", ".", "floatX", ")", "/", "retain_prob", "\n", "", "return", "X", "\n", "", "def", "adam", "(", "self", ",", "param", ",", "grad", ",", "updates", ",", "sample_idx", "=", "None", ",", "epsilon", "=", "1e-6", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.adam": [[270, 294], ["numpy.float32", "numpy.float32", "theano.shared", "theano.shared", "theano.shared", "theano.tensor.set_subtensor", "theano.tensor.set_subtensor", "theano.tensor.set_subtensor", "param.get_value", "param.get_value", "param.get_value", "theano.tensor.sqrt"], "methods", ["None"], ["", "def", "adam", "(", "self", ",", "param", ",", "grad", ",", "updates", ",", "sample_idx", "=", "None", ",", "epsilon", "=", "1e-6", ")", ":", "\n", "        ", "v1", "=", "np", ".", "float32", "(", "self", ".", "decay", ")", "\n", "v2", "=", "np", ".", "float32", "(", "1.0", "-", "self", ".", "decay", ")", "\n", "acc", "=", "theano", ".", "shared", "(", "param", ".", "get_value", "(", "borrow", "=", "False", ")", "*", "0.", ",", "borrow", "=", "True", ")", "\n", "meang", "=", "theano", ".", "shared", "(", "param", ".", "get_value", "(", "borrow", "=", "False", ")", "*", "0.", ",", "borrow", "=", "True", ")", "\n", "countt", "=", "theano", ".", "shared", "(", "param", ".", "get_value", "(", "borrow", "=", "False", ")", "*", "0.", ",", "borrow", "=", "True", ")", "\n", "if", "sample_idx", "is", "None", ":", "\n", "            ", "acc_new", "=", "v1", "*", "acc", "+", "v2", "*", "grad", "**", "2", "\n", "meang_new", "=", "v1", "*", "meang", "+", "v2", "*", "grad", "\n", "countt_new", "=", "countt", "+", "1", "\n", "updates", "[", "acc", "]", "=", "acc_new", "\n", "updates", "[", "meang", "]", "=", "meang_new", "\n", "updates", "[", "countt", "]", "=", "countt_new", "\n", "", "else", ":", "\n", "            ", "acc_s", "=", "acc", "[", "sample_idx", "]", "\n", "meang_s", "=", "meang", "[", "sample_idx", "]", "\n", "countt_s", "=", "countt", "[", "sample_idx", "]", "\n", "acc_new", "=", "v1", "*", "acc_s", "+", "v2", "*", "grad", "**", "2", "\n", "meang_new", "=", "v1", "*", "meang_s", "+", "v2", "*", "grad", "\n", "countt_new", "=", "countt_s", "+", "1.0", "\n", "updates", "[", "acc", "]", "=", "T", ".", "set_subtensor", "(", "acc_s", ",", "acc_new", ")", "\n", "updates", "[", "meang", "]", "=", "T", ".", "set_subtensor", "(", "meang_s", ",", "meang_new", ")", "\n", "updates", "[", "countt", "]", "=", "T", ".", "set_subtensor", "(", "countt_s", ",", "countt_new", ")", "\n", "", "return", "(", "meang_new", "/", "(", "1", "-", "v1", "**", "countt_new", ")", ")", "/", "(", "T", ".", "sqrt", "(", "acc_new", "/", "(", "1", "-", "v1", "**", "countt_new", ")", ")", "+", "epsilon", ")", "\n", "", "def", "adagrad", "(", "self", ",", "param", ",", "grad", ",", "updates", ",", "sample_idx", "=", "None", ",", "epsilon", "=", "1e-6", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.adagrad": [[294, 305], ["theano.shared", "theano.tensor.cast", "theano.tensor.set_subtensor", "theano.tensor.sqrt", "param.get_value"], "methods", ["None"], ["", "def", "adagrad", "(", "self", ",", "param", ",", "grad", ",", "updates", ",", "sample_idx", "=", "None", ",", "epsilon", "=", "1e-6", ")", ":", "\n", "        ", "acc", "=", "theano", ".", "shared", "(", "param", ".", "get_value", "(", "borrow", "=", "False", ")", "*", "0.", ",", "borrow", "=", "True", ")", "\n", "if", "sample_idx", "is", "None", ":", "\n", "            ", "acc_new", "=", "acc", "+", "grad", "**", "2", "\n", "updates", "[", "acc", "]", "=", "acc_new", "\n", "", "else", ":", "\n", "            ", "acc_s", "=", "acc", "[", "sample_idx", "]", "\n", "acc_new", "=", "acc_s", "+", "grad", "**", "2", "\n", "updates", "[", "acc", "]", "=", "T", ".", "set_subtensor", "(", "acc_s", ",", "acc_new", ")", "\n", "", "gradient_scaling", "=", "T", ".", "cast", "(", "T", ".", "sqrt", "(", "acc_new", "+", "epsilon", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "return", "grad", "/", "gradient_scaling", "\n", "", "def", "adadelta", "(", "self", ",", "param", ",", "grad", ",", "updates", ",", "sample_idx", "=", "None", ",", "epsilon", "=", "1e-6", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.adadelta": [[305, 326], ["numpy.float32", "numpy.float32", "theano.shared", "theano.shared", "theano.tensor.cast", "theano.tensor.set_subtensor", "theano.tensor.set_subtensor", "theano.tensor.sqrt", "param.get_value", "param.get_value", "theano.tensor.sqrt", "theano.tensor.sqrt"], "methods", ["None"], ["", "def", "adadelta", "(", "self", ",", "param", ",", "grad", ",", "updates", ",", "sample_idx", "=", "None", ",", "epsilon", "=", "1e-6", ")", ":", "\n", "        ", "v1", "=", "np", ".", "float32", "(", "self", ".", "decay", ")", "\n", "v2", "=", "np", ".", "float32", "(", "1.0", "-", "self", ".", "decay", ")", "\n", "acc", "=", "theano", ".", "shared", "(", "param", ".", "get_value", "(", "borrow", "=", "False", ")", "*", "0.", ",", "borrow", "=", "True", ")", "\n", "upd", "=", "theano", ".", "shared", "(", "param", ".", "get_value", "(", "borrow", "=", "False", ")", "*", "0.", ",", "borrow", "=", "True", ")", "\n", "if", "sample_idx", "is", "None", ":", "\n", "            ", "acc_new", "=", "acc", "+", "grad", "**", "2", "\n", "updates", "[", "acc", "]", "=", "acc_new", "\n", "grad", "=", "T", ".", "sqrt", "(", "upd", "+", "epsilon", ")", "*", "grad", "\n", "upd_new", "=", "v1", "*", "upd", "+", "v2", "*", "grad", "**", "2", "\n", "updates", "[", "upd", "]", "=", "upd_new", "\n", "", "else", ":", "\n", "            ", "acc_s", "=", "acc", "[", "sample_idx", "]", "\n", "acc_new", "=", "acc_s", "+", "grad", "**", "2", "\n", "updates", "[", "acc", "]", "=", "T", ".", "set_subtensor", "(", "acc_s", ",", "acc_new", ")", "\n", "upd_s", "=", "upd", "[", "sample_idx", "]", "\n", "upd_new", "=", "v1", "*", "upd_s", "+", "v2", "*", "grad", "**", "2", "\n", "updates", "[", "upd", "]", "=", "T", ".", "set_subtensor", "(", "upd_s", ",", "upd_new", ")", "\n", "grad", "=", "T", ".", "sqrt", "(", "upd_s", "+", "epsilon", ")", "*", "grad", "\n", "", "gradient_scaling", "=", "T", ".", "cast", "(", "T", ".", "sqrt", "(", "acc_new", "+", "epsilon", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "return", "grad", "/", "gradient_scaling", "\n", "", "def", "rmsprop", "(", "self", ",", "param", ",", "grad", ",", "updates", ",", "sample_idx", "=", "None", ",", "epsilon", "=", "1e-6", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.rmsprop": [[326, 339], ["numpy.float32", "numpy.float32", "theano.shared", "theano.tensor.cast", "theano.tensor.set_subtensor", "theano.tensor.sqrt", "param.get_value"], "methods", ["None"], ["", "def", "rmsprop", "(", "self", ",", "param", ",", "grad", ",", "updates", ",", "sample_idx", "=", "None", ",", "epsilon", "=", "1e-6", ")", ":", "\n", "        ", "v1", "=", "np", ".", "float32", "(", "self", ".", "decay", ")", "\n", "v2", "=", "np", ".", "float32", "(", "1.0", "-", "self", ".", "decay", ")", "\n", "acc", "=", "theano", ".", "shared", "(", "param", ".", "get_value", "(", "borrow", "=", "False", ")", "*", "0.", ",", "borrow", "=", "True", ")", "\n", "if", "sample_idx", "is", "None", ":", "\n", "            ", "acc_new", "=", "v1", "*", "acc", "+", "v2", "*", "grad", "**", "2", "\n", "updates", "[", "acc", "]", "=", "acc_new", "\n", "", "else", ":", "\n", "            ", "acc_s", "=", "acc", "[", "sample_idx", "]", "\n", "acc_new", "=", "v1", "*", "acc_s", "+", "v2", "*", "grad", "**", "2", "\n", "updates", "[", "acc", "]", "=", "T", ".", "set_subtensor", "(", "acc_s", ",", "acc_new", ")", "\n", "", "gradient_scaling", "=", "T", ".", "cast", "(", "T", ".", "sqrt", "(", "acc_new", "+", "epsilon", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "return", "grad", "/", "gradient_scaling", "\n", "", "def", "RMSprop", "(", "self", ",", "cost", ",", "params", ",", "full_params", ",", "sampled_params", ",", "sidxs", ",", "epsilon", "=", "1e-6", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.RMSprop": [[339, 392], ["collections.OrderedDict", "zip", "range", "theano.tensor.grad", "theano.tensor.grad", "theano.tensor.cast", "zip", "len", "theano.tensor.sqrt", "theano.tensor.switch", "theano.shared", "theano.tensor.set_subtensor", "theano.tensor.inc_subtensor", "theano.tensor.inc_subtensor", "theano.tensor.switch", "theano.tensor.ge", "theano.shared", "gru4rec2.GRU4Rec.adagrad", "gru4rec2.GRU4Rec.rmsprop", "gru4rec2.GRU4Rec.adadelta", "gru4rec2.GRU4Rec.adam", "numpy.float32", "numpy.float32", "theano.tensor.sum", "theano.tensor.sum", "theano.tensor.ge", "gru4rec2.GRU4Rec.adagrad", "gru4rec2.GRU4Rec.rmsprop", "gru4rec2.GRU4Rec.adadelta", "gru4rec2.GRU4Rec.adam", "fullP.get_value", "p.get_value", "numpy.float32", "numpy.float32", "numpy.float32", "theano.tensor.sum", "theano.tensor.sum", "theano.tensor.sum"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.adagrad", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.rmsprop", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.adadelta", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.adam", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.adagrad", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.rmsprop", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.adadelta", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.adam"], ["", "def", "RMSprop", "(", "self", ",", "cost", ",", "params", ",", "full_params", ",", "sampled_params", ",", "sidxs", ",", "epsilon", "=", "1e-6", ")", ":", "\n", "        ", "grads", "=", "[", "T", ".", "grad", "(", "cost", "=", "cost", ",", "wrt", "=", "param", ")", "for", "param", "in", "params", "]", "\n", "sgrads", "=", "[", "T", ".", "grad", "(", "cost", "=", "cost", ",", "wrt", "=", "sparam", ")", "for", "sparam", "in", "sampled_params", "]", "\n", "updates", "=", "OrderedDict", "(", ")", "\n", "if", "self", ".", "grad_cap", ">", "0", ":", "\n", "            ", "norm", "=", "T", ".", "cast", "(", "T", ".", "sqrt", "(", "T", ".", "sum", "(", "[", "T", ".", "sum", "(", "[", "T", ".", "sum", "(", "g", "**", "2", ")", "for", "g", "in", "g_list", "]", ")", "for", "g_list", "in", "grads", "]", ")", "+", "T", ".", "sum", "(", "[", "T", ".", "sum", "(", "g", "**", "2", ")", "for", "g", "in", "sgrads", "]", ")", ")", ",", "theano", ".", "config", ".", "floatX", ")", "\n", "grads", "=", "[", "[", "T", ".", "switch", "(", "T", ".", "ge", "(", "norm", ",", "self", ".", "grad_cap", ")", ",", "g", "*", "self", ".", "grad_cap", "/", "norm", ",", "g", ")", "for", "g", "in", "g_list", "]", "for", "g_list", "in", "grads", "]", "\n", "sgrads", "=", "[", "T", ".", "switch", "(", "T", ".", "ge", "(", "norm", ",", "self", ".", "grad_cap", ")", ",", "g", "*", "self", ".", "grad_cap", "/", "norm", ",", "g", ")", "for", "g", "in", "sgrads", "]", "\n", "", "for", "p_list", ",", "g_list", "in", "zip", "(", "params", ",", "grads", ")", ":", "\n", "            ", "for", "p", ",", "g", "in", "zip", "(", "p_list", ",", "g_list", ")", ":", "\n", "                ", "if", "self", ".", "adapt", ":", "\n", "                    ", "if", "self", ".", "adapt", "==", "'adagrad'", ":", "\n", "                        ", "g", "=", "self", ".", "adagrad", "(", "p", ",", "g", ",", "updates", ")", "\n", "", "if", "self", ".", "adapt", "==", "'rmsprop'", ":", "\n", "                        ", "g", "=", "self", ".", "rmsprop", "(", "p", ",", "g", ",", "updates", ")", "\n", "", "if", "self", ".", "adapt", "==", "'adadelta'", ":", "\n", "                        ", "g", "=", "self", ".", "adadelta", "(", "p", ",", "g", ",", "updates", ")", "\n", "", "if", "self", ".", "adapt", "==", "'adam'", ":", "\n", "                        ", "g", "=", "self", ".", "adam", "(", "p", ",", "g", ",", "updates", ")", "\n", "", "", "if", "self", ".", "momentum", ">", "0", ":", "\n", "                    ", "velocity", "=", "theano", ".", "shared", "(", "p", ".", "get_value", "(", "borrow", "=", "False", ")", "*", "0.", ",", "borrow", "=", "True", ")", "\n", "velocity2", "=", "self", ".", "momentum", "*", "velocity", "-", "np", ".", "float32", "(", "self", ".", "learning_rate", ")", "*", "(", "g", "+", "self", ".", "lmbd", "*", "p", ")", "\n", "updates", "[", "velocity", "]", "=", "velocity2", "\n", "updates", "[", "p", "]", "=", "p", "+", "velocity2", "\n", "", "else", ":", "\n", "                    ", "updates", "[", "p", "]", "=", "p", "*", "np", ".", "float32", "(", "1.0", "-", "self", ".", "learning_rate", "*", "self", ".", "lmbd", ")", "-", "np", ".", "float32", "(", "self", ".", "learning_rate", ")", "*", "g", "\n", "", "", "", "for", "i", "in", "range", "(", "len", "(", "sgrads", ")", ")", ":", "\n", "            ", "g", "=", "sgrads", "[", "i", "]", "\n", "fullP", "=", "full_params", "[", "i", "]", "\n", "sample_idx", "=", "sidxs", "[", "i", "]", "\n", "sparam", "=", "sampled_params", "[", "i", "]", "\n", "if", "self", ".", "adapt", ":", "\n", "                ", "if", "self", ".", "adapt", "==", "'adagrad'", ":", "\n", "                    ", "g", "=", "self", ".", "adagrad", "(", "fullP", ",", "g", ",", "updates", ",", "sample_idx", ")", "\n", "", "if", "self", ".", "adapt", "==", "'rmsprop'", ":", "\n", "                    ", "g", "=", "self", ".", "rmsprop", "(", "fullP", ",", "g", ",", "updates", ",", "sample_idx", ")", "\n", "", "if", "self", ".", "adapt", "==", "'adadelta'", ":", "\n", "                    ", "g", "=", "self", ".", "adadelta", "(", "fullP", ",", "g", ",", "updates", ",", "sample_idx", ")", "\n", "", "if", "self", ".", "adapt", "==", "'adam'", ":", "\n", "                    ", "g", "=", "self", ".", "adam", "(", "fullP", ",", "g", ",", "updates", ",", "sample_idx", ")", "\n", "", "", "if", "self", ".", "lmbd", ">", "0", ":", "\n", "                ", "delta", "=", "np", ".", "float32", "(", "self", ".", "learning_rate", ")", "*", "(", "g", "+", "self", ".", "lmbd", "*", "sparam", ")", "\n", "", "else", ":", "\n", "                ", "delta", "=", "np", ".", "float32", "(", "self", ".", "learning_rate", ")", "*", "g", "\n", "", "if", "self", ".", "momentum", ">", "0", ":", "\n", "                ", "velocity", "=", "theano", ".", "shared", "(", "fullP", ".", "get_value", "(", "borrow", "=", "False", ")", "*", "0.", ",", "borrow", "=", "True", ")", "\n", "vs", "=", "velocity", "[", "sample_idx", "]", "\n", "velocity2", "=", "self", ".", "momentum", "*", "vs", "-", "delta", "\n", "updates", "[", "velocity", "]", "=", "T", ".", "set_subtensor", "(", "vs", ",", "velocity2", ")", "\n", "updates", "[", "fullP", "]", "=", "T", ".", "inc_subtensor", "(", "sparam", ",", "velocity2", ")", "\n", "", "else", ":", "\n", "                ", "updates", "[", "fullP", "]", "=", "T", ".", "inc_subtensor", "(", "sparam", ",", "-", "delta", ")", "\n", "", "", "return", "updates", "\n", "", "def", "model", "(", "self", ",", "X", ",", "H", ",", "Y", "=", "None", ",", "drop_p_hidden", "=", "0.0", ",", "drop_p_embed", "=", "0.0", ",", "predict", "=", "False", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.model": [[392, 432], ["range", "gru4rec2.GRU4Rec.dropout", "theano.tensor.nnet.sigmoid", "gru4rec2.GRU4Rec.hidden_activation", "gru4rec2.GRU4Rec.dropout", "len", "theano.tensor.nnet.sigmoid", "gru4rec2.GRU4Rec.hidden_activation", "gru4rec2.GRU4Rec.dropout", "H_new.append", "theano.tensor.dot", "gru4rec2.GRU4Rec.softmax", "gru4rec2.GRU4Rec.final_activation", "gru4rec2.GRU4Rec.softmax", "gru4rec2.GRU4Rec.final_activation", "theano.tensor.dot", "theano.tensor.dot", "theano.tensor.dot", "theano.tensor.dot", "theano.tensor.dot", "SBy.flatten", "theano.tensor.dot", "SBy.flatten", "theano.tensor.dot", "gru4rec2.GRU4Rec.By.flatten", "theano.tensor.dot", "gru4rec2.GRU4Rec.By.flatten"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.softmax", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.softmax"], ["", "def", "model", "(", "self", ",", "X", ",", "H", ",", "Y", "=", "None", ",", "drop_p_hidden", "=", "0.0", ",", "drop_p_embed", "=", "0.0", ",", "predict", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "embedding", ":", "\n", "            ", "Sx", "=", "self", ".", "E", "[", "X", "]", "\n", "y", "=", "self", ".", "dropout", "(", "Sx", ",", "drop_p_embed", ")", "\n", "H_new", "=", "[", "]", "\n", "start", "=", "0", "\n", "", "else", ":", "\n", "            ", "Sx", "=", "self", ".", "Wx", "[", "0", "]", "[", "X", "]", "\n", "vec", "=", "Sx", "+", "self", ".", "Bh", "[", "0", "]", "\n", "rz", "=", "T", ".", "nnet", ".", "sigmoid", "(", "vec", ".", "T", "[", "self", ".", "layers", "[", "0", "]", ":", "]", "+", "T", ".", "dot", "(", "H", "[", "0", "]", ",", "self", ".", "Wrz", "[", "0", "]", ")", ".", "T", ")", "\n", "h", "=", "self", ".", "hidden_activation", "(", "T", ".", "dot", "(", "H", "[", "0", "]", "*", "rz", "[", ":", "self", ".", "layers", "[", "0", "]", "]", ".", "T", ",", "self", ".", "Wh", "[", "0", "]", ")", "+", "vec", ".", "T", "[", ":", "self", ".", "layers", "[", "0", "]", "]", ".", "T", ")", "\n", "z", "=", "rz", "[", "self", ".", "layers", "[", "0", "]", ":", "]", ".", "T", "\n", "h", "=", "(", "1.0", "-", "z", ")", "*", "H", "[", "0", "]", "+", "z", "*", "h", "\n", "h", "=", "self", ".", "dropout", "(", "h", ",", "drop_p_hidden", ")", "\n", "H_new", "=", "[", "h", "]", "\n", "y", "=", "h", "\n", "start", "=", "1", "\n", "", "for", "i", "in", "range", "(", "start", ",", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "            ", "vec", "=", "T", ".", "dot", "(", "y", ",", "self", ".", "Wx", "[", "i", "]", ")", "+", "self", ".", "Bh", "[", "i", "]", "\n", "rz", "=", "T", ".", "nnet", ".", "sigmoid", "(", "vec", ".", "T", "[", "self", ".", "layers", "[", "i", "]", ":", "]", "+", "T", ".", "dot", "(", "H", "[", "i", "]", ",", "self", ".", "Wrz", "[", "i", "]", ")", ".", "T", ")", "\n", "h", "=", "self", ".", "hidden_activation", "(", "T", ".", "dot", "(", "H", "[", "i", "]", "*", "rz", "[", ":", "self", ".", "layers", "[", "i", "]", "]", ".", "T", ",", "self", ".", "Wh", "[", "i", "]", ")", "+", "vec", ".", "T", "[", ":", "self", ".", "layers", "[", "i", "]", "]", ".", "T", ")", "\n", "z", "=", "rz", "[", "self", ".", "layers", "[", "i", "]", ":", "]", ".", "T", "\n", "h", "=", "(", "1.0", "-", "z", ")", "*", "H", "[", "i", "]", "+", "z", "*", "h", "\n", "h", "=", "self", ".", "dropout", "(", "h", ",", "drop_p_hidden", ")", "\n", "H_new", ".", "append", "(", "h", ")", "\n", "y", "=", "h", "\n", "", "if", "Y", "is", "not", "None", ":", "\n", "            ", "Sy", "=", "self", ".", "Wy", "[", "Y", "]", "\n", "SBy", "=", "self", ".", "By", "[", "Y", "]", "\n", "if", "predict", "and", "self", ".", "final_act", "==", "'softmax_logit'", ":", "\n", "                ", "y", "=", "self", ".", "softmax", "(", "T", ".", "dot", "(", "y", ",", "Sy", ".", "T", ")", "+", "SBy", ".", "flatten", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "y", "=", "self", ".", "final_activation", "(", "T", ".", "dot", "(", "y", ",", "Sy", ".", "T", ")", "+", "SBy", ".", "flatten", "(", ")", ")", "\n", "", "return", "H_new", ",", "y", ",", "[", "Sx", ",", "Sy", ",", "SBy", "]", "\n", "", "else", ":", "\n", "            ", "if", "predict", "and", "self", ".", "final_act", "==", "'softmax_logit'", ":", "\n", "                ", "y", "=", "self", ".", "softmax", "(", "T", ".", "dot", "(", "y", ",", "self", ".", "Wy", ".", "T", ")", "+", "self", ".", "By", ".", "flatten", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "y", "=", "self", ".", "final_activation", "(", "T", ".", "dot", "(", "y", ",", "self", ".", "Wy", ".", "T", ")", "+", "self", ".", "By", ".", "flatten", "(", ")", ")", "\n", "", "return", "H_new", ",", "y", ",", "[", "Sx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.generate_neg_samples": [[433, 441], ["numpy.searchsorted", "numpy.random.choice", "sample.reshape.reshape.reshape", "numpy.random.rand"], "methods", ["None"], ["", "", "def", "generate_neg_samples", "(", "self", ",", "pop", ",", "length", ")", ":", "\n", "        ", "if", "self", ".", "sample_alpha", ":", "\n", "            ", "sample", "=", "np", ".", "searchsorted", "(", "pop", ",", "np", ".", "random", ".", "rand", "(", "self", ".", "n_sample", "*", "length", ")", ")", "\n", "", "else", ":", "\n", "            ", "sample", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "n_items", ",", "size", "=", "self", ".", "n_sample", "*", "length", ")", "\n", "", "if", "length", ">", "1", ":", "\n", "            ", "sample", "=", "sample", ".", "reshape", "(", "(", "length", ",", "self", ".", "n_sample", ")", ")", "\n", "", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init_candidate_sampling_manager": [[455, 457], ["candidate_sampling.CandidateSamplingManager", "gru4rec2.GRU4Rec.clicked_items_state.get_recent_clicks_buffer"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.get_recent_clicks_buffer"], ["def", "init_candidate_sampling_manager", "(", "self", ")", ":", "\n", "        ", "self", ".", "sampling_manager", "=", "CandidateSamplingManager", "(", "lambda", ":", "self", ".", "clicked_items_state", ".", "get_recent_clicks_buffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.generate_neg_samples_from_last_clicks_buffer": [[471, 476], ["gru4rec2.GRU4Rec.sampling_manager.get_sample_from_recently_clicked_items_buffer", "gru4rec2.GRU4Rec.sampling_manager.get_neg_items_click", "gru4rec2.GRU4Rec.item_original_to_ids_vect"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_sample_from_recently_clicked_items_buffer", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.candidate_sampling.CandidateSamplingManager.get_neg_items_click"], ["def", "generate_neg_samples_from_last_clicks_buffer", "(", "self", ")", ":", "\n", "        ", "samples_with_repetition", "=", "self", ".", "sampling_manager", ".", "get_sample_from_recently_clicked_items_buffer", "(", "self", ".", "n_sample", "*", "20", ")", "\n", "neg_items_original", "=", "self", ".", "sampling_manager", ".", "get_neg_items_click", "(", "samples_with_repetition", ",", "self", ".", "n_sample", ")", "\n", "neg_items", "=", "self", ".", "item_original_to_ids_vect", "(", "neg_items_original", ")", "\n", "return", "neg_items", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.fit": [[481, 645], ["data[].unique", "dict", "numpy.vectorize", "dict", "numpy.vectorize", "theano.tensor.ivector", "theano.tensor.ivector", "gru4rec2.GRU4Rec.model", "gru4rec2.GRU4Rec.loss_function", "gru4rec2.GRU4Rec.RMSprop", "range", "theano.function", "range", "len", "pandas.Series", "pandas.merge", "gru4rec2.GRU4Rec.init", "new_item_mask.sum", "pandas.merge", "pandas.merge.sort_values", "numpy.zeros", "pandas.merge.groupby().size().cumsum", "zip", "zip", "len", "numpy.argsort", "numpy.arange", "pandas.merge.groupby().size", "range", "numpy.arange", "numpy.arange.max", "numpy.mean", "numpy.isnan", "print", "pandas.DataFrame", "numpy.in1d", "gru4rec2.GRU4Rec.itemidmap.append", "gru4rec2.GRU4Rec.By.set_value", "print", "pandas.DataFrame", "pd.merge.groupby().size.cumsum", "pd.merge.groupby().size.sum", "print", "len", "gru4rec2.GRU4Rec.H[].set_value", "numpy.random.permutation", "gru4rec2.GRU4Rec.generate_neg_samples_from_last_clicks_buffer", "range", "print", "callback", "numpy.arange", "pandas.Series", "gru4rec2.GRU4Rec.extend_weights", "numpy.vstack", "data[].nunique", "pandas.merge.groupby().size", "[].min", "len", "pandas.merge.groupby", "print", "gru4rec2.GRU4Rec.generate_neg_samples", "print", "numpy.zeros", "theano.function.", "gru4rec2.GRU4Rec.item_ids_to_original_vect", "gru4rec2.GRU4Rec.clicked_items_state.update_items_state", "c.append", "numpy.isnan", "numpy.arange", "len", "range", "callback", "len", "numpy.hstack", "print", "len", "len", "gru4rec2.GRU4Rec.H[].get_value", "gru4rec2.GRU4Rec.H[].set_value", "str", "gru4rec2.GRU4Rec.By.get_value", "numpy.zeros", "pandas.merge.groupby", "len", "numpy.arange", "len", "pandas.merge.groupby", "gru4rec2.GRU4Rec.generate_neg_samples", "str", "len", "len"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.model", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.RMSprop", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.init", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.generate_neg_samples_from_last_clicks_buffer", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.extend_weights", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.generate_neg_samples", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.nar.clicked_items_state.ClickedItemsState.update_items_state", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.generate_neg_samples"], ["", "def", "fit", "(", "self", ",", "data", ",", "retrain", "=", "False", ",", "sample_store", "=", "10000000", ",", "callback", "=", "None", ")", ":", "\n", "        ", "'''\n        Trains the network.\n\n        Parameters\n        --------\n        data : pandas.DataFrame\n            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n        retrain : boolean\n            If False, do normal train. If True, do additional train (weigths from previous trainings are kept as the initial network) (default: False)\n        sample_store : int\n            If additional negative samples are used (n_sample > 0), the efficiency of GPU utilization can be sped up, by precomputing a large batch of negative samples (and recomputing when necessary).\n            This parameter regulizes the size of this precomputed ID set. Its value is the maximum number of int values (IDs) to be stored. Precomputed IDs are stored in the RAM.\n            For the most efficient computation, a balance must be found between storing few examples and constantly interrupting GPU computations for a short time vs. computing many examples and interrupting GPU computations for a long time (but rarely).\n\n        '''", "\n", "self", ".", "predict", "=", "None", "\n", "self", ".", "error_during_train", "=", "False", "\n", "itemids", "=", "data", "[", "self", ".", "item_key", "]", ".", "unique", "(", ")", "\n", "if", "not", "retrain", ":", "\n", "            ", "self", ".", "n_items", "=", "len", "(", "itemids", ")", "\n", "self", ".", "itemidmap", "=", "pd", ".", "Series", "(", "data", "=", "np", ".", "arange", "(", "self", ".", "n_items", ")", ",", "index", "=", "itemids", ")", "\n", "data", "=", "pd", ".", "merge", "(", "data", ",", "pd", ".", "DataFrame", "(", "{", "self", ".", "item_key", ":", "itemids", ",", "'ItemIdx'", ":", "self", ".", "itemidmap", "[", "itemids", "]", ".", "values", "}", ")", ",", "on", "=", "self", ".", "item_key", ",", "how", "=", "'inner'", ")", "\n", "offset_sessions", "=", "self", ".", "init", "(", "data", ")", "\n", "", "else", ":", "\n", "            ", "new_item_mask", "=", "~", "np", ".", "in1d", "(", "itemids", ",", "self", ".", "itemidmap", ".", "index", ")", "\n", "n_new_items", "=", "new_item_mask", ".", "sum", "(", ")", "\n", "if", "n_new_items", ":", "\n", "                ", "self", ".", "itemidmap", "=", "self", ".", "itemidmap", ".", "append", "(", "pd", ".", "Series", "(", "index", "=", "itemids", "[", "new_item_mask", "]", ",", "data", "=", "np", ".", "arange", "(", "n_new_items", ")", "+", "len", "(", "self", ".", "itemidmap", ")", ")", ")", "\n", "for", "W", "in", "[", "self", ".", "E", "if", "self", ".", "embedding", "else", "self", ".", "Wx", "[", "0", "]", ",", "self", ".", "Wy", "]", ":", "\n", "                    ", "self", ".", "extend_weights", "(", "W", ",", "n_new_items", ")", "\n", "", "self", ".", "By", ".", "set_value", "(", "np", ".", "vstack", "(", "[", "self", ".", "By", ".", "get_value", "(", ")", ",", "np", ".", "zeros", "(", "(", "n_new_items", ",", "1", ")", ",", "dtype", "=", "theano", ".", "config", ".", "floatX", ")", "]", ")", ")", "\n", "self", ".", "n_items", "+=", "n_new_items", "\n", "print", "(", "'Added {} new items. Number of items is {}.'", ".", "format", "(", "n_new_items", ",", "self", ".", "n_items", ")", ")", "\n", "", "data", "=", "pd", ".", "merge", "(", "data", ",", "pd", ".", "DataFrame", "(", "{", "self", ".", "item_key", ":", "itemids", ",", "'ItemIdx'", ":", "self", ".", "itemidmap", "[", "itemids", "]", ".", "values", "}", ")", ",", "on", "=", "self", ".", "item_key", ",", "how", "=", "'inner'", ")", "\n", "data", ".", "sort_values", "(", "[", "self", ".", "session_key", ",", "self", ".", "time_key", "]", ",", "inplace", "=", "True", ")", "\n", "offset_sessions", "=", "np", ".", "zeros", "(", "data", "[", "self", ".", "session_key", "]", ".", "nunique", "(", ")", "+", "1", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "offset_sessions", "[", "1", ":", "]", "=", "data", ".", "groupby", "(", "self", ".", "session_key", ")", ".", "size", "(", ")", ".", "cumsum", "(", ")", "\n", "\n", "#Creating and inverted dictionary (from sequential ids to original ids)", "\n", "", "self", ".", "items_dict", "=", "dict", "(", "zip", "(", "self", ".", "itemidmap", ".", "index", ",", "self", ".", "itemidmap", ".", "values", ")", ")", "\n", "#Adding to avoid error when retrieving zeroed samples", "\n", "self", ".", "items_dict", "[", "0", "]", "=", "0", "\n", "\n", "self", ".", "item_original_to_ids_vect", "=", "np", ".", "vectorize", "(", "lambda", "x", ":", "self", ".", "items_dict", "[", "x", "]", "if", "x", "in", "self", ".", "items_dict", "else", "0", ")", "\n", "\n", "self", ".", "items_inverted_dict", "=", "dict", "(", "zip", "(", "self", ".", "itemidmap", ".", "values", ",", "self", ".", "itemidmap", ".", "index", ")", ")", "\n", "self", ".", "item_ids_to_original_vect", "=", "np", ".", "vectorize", "(", "lambda", "x", ":", "self", ".", "items_inverted_dict", "[", "x", "]", ")", "\n", "\n", "X", "=", "T", ".", "ivector", "(", ")", "\n", "Y", "=", "T", ".", "ivector", "(", ")", "\n", "H_new", ",", "Y_pred", ",", "sampled_params", "=", "self", ".", "model", "(", "X", ",", "self", ".", "H", ",", "Y", ",", "self", ".", "dropout_p_hidden", ",", "self", ".", "dropout_p_embed", ")", "\n", "cost", "=", "self", ".", "loss_function", "(", "Y_pred", ")", "\n", "params", "=", "[", "self", ".", "Wx", "if", "self", ".", "embedding", "else", "self", ".", "Wx", "[", "1", ":", "]", ",", "self", ".", "Wh", ",", "self", ".", "Wrz", ",", "self", ".", "Bh", "]", "\n", "full_params", "=", "[", "self", ".", "E", "if", "self", ".", "embedding", "else", "self", ".", "Wx", "[", "0", "]", ",", "self", ".", "Wy", ",", "self", ".", "By", "]", "\n", "sidxs", "=", "[", "X", ",", "Y", ",", "Y", "]", "\n", "updates", "=", "self", ".", "RMSprop", "(", "cost", ",", "params", ",", "full_params", ",", "sampled_params", ",", "sidxs", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "H", ")", ")", ":", "\n", "            ", "updates", "[", "self", ".", "H", "[", "i", "]", "]", "=", "H_new", "[", "i", "]", "\n", "", "train_function", "=", "function", "(", "inputs", "=", "[", "X", ",", "Y", "]", ",", "outputs", "=", "cost", ",", "updates", "=", "updates", ",", "allow_input_downcast", "=", "True", ")", "\n", "base_order", "=", "np", ".", "argsort", "(", "data", ".", "groupby", "(", "self", ".", "session_key", ")", "[", "self", ".", "time_key", "]", ".", "min", "(", ")", ".", "values", ")", "if", "self", ".", "time_sort", "else", "np", ".", "arange", "(", "len", "(", "offset_sessions", ")", "-", "1", ")", "\n", "if", "self", ".", "n_sample", ":", "\n", "            ", "pop", "=", "data", ".", "groupby", "(", "'ItemId'", ")", ".", "size", "(", ")", "\n", "pop", "=", "pop", "[", "self", ".", "itemidmap", ".", "index", ".", "values", "]", ".", "values", "**", "self", ".", "sample_alpha", "\n", "pop", "=", "pop", ".", "cumsum", "(", ")", "/", "pop", ".", "sum", "(", ")", "\n", "pop", "[", "-", "1", "]", "=", "1", "\n", "if", "sample_store", ":", "\n", "                ", "generate_length", "=", "sample_store", "//", "self", ".", "n_sample", "\n", "if", "generate_length", "<=", "1", ":", "\n", "                    ", "sample_store", "=", "0", "\n", "print", "(", "'No example store was used'", ")", "\n", "", "else", ":", "\n", "                    ", "neg_samples", "=", "self", ".", "generate_neg_samples", "(", "pop", ",", "generate_length", ")", "\n", "sample_pointer", "=", "0", "\n", "print", "(", "'Created sample store with {} batches of samples'", ".", "format", "(", "generate_length", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "print", "(", "'No example store was used'", ")", "\n", "", "", "data_items", "=", "data", ".", "ItemIdx", ".", "values", "\n", "datetime_items", "=", "data", "[", "self", ".", "time_key", "]", ".", "values", "\n", "for", "epoch", "in", "range", "(", "self", ".", "n_epochs", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "                ", "self", ".", "H", "[", "i", "]", ".", "set_value", "(", "np", ".", "zeros", "(", "(", "self", ".", "batch_size", ",", "self", ".", "layers", "[", "i", "]", ")", ",", "dtype", "=", "theano", ".", "config", ".", "floatX", ")", ",", "borrow", "=", "True", ")", "\n", "", "c", "=", "[", "]", "\n", "session_idx_arr", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "offset_sessions", ")", "-", "1", ")", "if", "self", ".", "train_random_order", "else", "base_order", "\n", "iters", "=", "np", ".", "arange", "(", "self", ".", "batch_size", ")", "\n", "maxiter", "=", "iters", ".", "max", "(", ")", "\n", "start", "=", "offset_sessions", "[", "session_idx_arr", "[", "iters", "]", "]", "\n", "end", "=", "offset_sessions", "[", "session_idx_arr", "[", "iters", "]", "+", "1", "]", "\n", "finished", "=", "False", "\n", "done", "=", "0", "\n", "while", "not", "finished", ":", "\n", "                ", "minlen", "=", "(", "end", "-", "start", ")", ".", "min", "(", ")", "\n", "out_idx", "=", "data_items", "[", "start", "]", "\n", "out_datetime", "=", "datetime_items", "[", "start", "]", "\n", "\n", "#print('Generating negative samples by batch')", "\n", "sample", "=", "self", ".", "generate_neg_samples_from_last_clicks_buffer", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "minlen", "-", "1", ")", ":", "\n", "                    ", "in_idx", "=", "out_idx", "\n", "out_idx", "=", "data_items", "[", "start", "+", "i", "+", "1", "]", "\n", "\n", "in_datetime", "=", "out_datetime", "\n", "out_datetime", "=", "datetime_items", "[", "start", "+", "i", "+", "1", "]", "\n", "if", "self", ".", "n_sample", ":", "\n", "                        ", "if", "sample_store", ":", "\n", "                            ", "if", "sample_pointer", "==", "generate_length", ":", "\n", "                                ", "neg_samples", "=", "self", ".", "generate_neg_samples", "(", "pop", ",", "generate_length", ")", "\n", "sample_pointer", "=", "0", "\n", "", "sample", "=", "neg_samples", "[", "sample_pointer", "]", "\n", "sample_pointer", "+=", "1", "\n", "", "else", ":", "\n", "#sample = self.generate_neg_samples(pop, 1)", "\n", "                            ", "pass", "\n", "\n", "#print('out_idx', out_idx)", "\n", "#print('sample', sample)", "\n", "\n", "", "y", "=", "np", ".", "hstack", "(", "[", "out_idx", ",", "sample", "]", ")", "\n", "", "else", ":", "\n", "                        ", "y", "=", "out_idx", "\n", "", "cost", "=", "train_function", "(", "in_idx", ",", "y", ")", "\n", "\n", "\n", "clicked_items_original", "=", "self", ".", "item_ids_to_original_vect", "(", "in_idx", ")", "\n", "#print('in_idx', in_idx)", "\n", "#print('clicked_items_original', clicked_items_original)", "\n", "#self.update_last_clicks_items_buffer(in_idx)", "\n", "\n", "self", ".", "clicked_items_state", ".", "update_items_state", "(", "clicked_items_original", ",", "in_datetime", ")", "\n", "\n", "done", "+=", "self", ".", "batch_size", "\n", "c", ".", "append", "(", "cost", ")", "\n", "if", "np", ".", "isnan", "(", "cost", ")", ":", "\n", "                        ", "print", "(", "str", "(", "epoch", ")", "+", "': NaN error!'", ")", "\n", "self", ".", "error_during_train", "=", "True", "\n", "return", "\n", "", "", "start", "=", "start", "+", "minlen", "-", "1", "\n", "mask", "=", "np", ".", "arange", "(", "len", "(", "iters", ")", ")", "[", "(", "end", "-", "start", ")", "<=", "1", "]", "\n", "for", "idx", "in", "mask", ":", "\n", "                    ", "maxiter", "+=", "1", "\n", "if", "maxiter", ">=", "len", "(", "offset_sessions", ")", "-", "1", ":", "\n", "                        ", "finished", "=", "True", "\n", "break", "\n", "", "iters", "[", "idx", "]", "=", "maxiter", "\n", "start", "[", "idx", "]", "=", "offset_sessions", "[", "session_idx_arr", "[", "maxiter", "]", "]", "\n", "end", "[", "idx", "]", "=", "offset_sessions", "[", "session_idx_arr", "[", "maxiter", "]", "+", "1", "]", "\n", "", "if", "len", "(", "mask", ")", "and", "self", ".", "reset_after_session", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "H", ")", ")", ":", "\n", "                        ", "tmp", "=", "self", ".", "H", "[", "i", "]", ".", "get_value", "(", "borrow", "=", "True", ")", "\n", "tmp", "[", "mask", "]", "=", "0", "\n", "self", ".", "H", "[", "i", "]", ".", "set_value", "(", "tmp", ",", "borrow", "=", "True", ")", "\n", "", "", "if", "callback", "is", "not", "None", ":", "\n", "                    ", "callback", "(", "(", "epoch", "*", "len", "(", "data_items", ")", "+", "done", ")", "/", "(", "self", ".", "n_epochs", "*", "len", "(", "data_items", ")", ")", ")", "\n", "\n", "", "", "avgc", "=", "np", ".", "mean", "(", "c", ")", "\n", "if", "np", ".", "isnan", "(", "avgc", ")", ":", "\n", "                ", "print", "(", "'Epoch {}: NaN error!'", ".", "format", "(", "str", "(", "epoch", ")", ")", ")", "\n", "self", ".", "error_during_train", "=", "True", "\n", "return", "\n", "", "print", "(", "'Epoch{}\\tloss: {:.6f}'", ".", "format", "(", "epoch", ",", "avgc", ")", ")", "\n", "if", "callback", "is", "not", "None", ":", "\n", "                ", "callback", "(", "(", "epoch", "+", "1", ")", "/", "self", ".", "n_epochs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.predict_next_batch": [[646, 705], ["theano.tensor.ivector", "theano.tensor.ivector", "range", "collections.OrderedDict", "range", "numpy.arange", "len", "range", "session_ids.copy", "pandas.DataFrame", "pandas.DataFrame", "len", "gru4rec2.GRU4Rec.H[].set_value", "gru4rec2.GRU4Rec.model", "gru4rec2.GRU4Rec.model", "len", "theano.function", "theano.function", "numpy.ones", "len", "gru4rec2.GRU4Rec.H[].get_value", "gru4rec2.GRU4Rec.H[].set_value", "numpy.asarray", "numpy.asarray", "numpy.zeros", "gru4rec2.GRU4Rec.predict", "gru4rec2.GRU4Rec.predict"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.model", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.model", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.predict", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.predict"], ["", "", "", "def", "predict_next_batch", "(", "self", ",", "session_ids", ",", "input_item_ids", ",", "predict_for_item_ids", "=", "None", ",", "batch", "=", "100", ")", ":", "\n", "        ", "'''\n        Gives predicton scores for a selected set of items. Can be used in batch mode to predict for multiple independent events (i.e. events of different sessions) at once and thus speed up evaluation.\n\n        If the session ID at a given coordinate of the session_ids parameter remains the same during subsequent calls of the function, the corresponding hidden state of the network will be kept intact (i.e. that's how one can predict an item to a session).\n        If it changes, the hidden state of the network is reset to zeros.\n\n        Parameters\n        --------\n        session_ids : 1D array\n            Contains the session IDs of the events of the batch. Its length must equal to the prediction batch size (batch param).\n        input_item_ids : 1D array\n            Contains the item IDs of the events of the batch. Every item ID must be must be in the training data of the network. Its length must equal to the prediction batch size (batch param).\n        predict_for_item_ids : 1D array (optional)\n            IDs of items for which the network should give prediction scores. Every ID must be in the training set. The default value is None, which means that the network gives prediction on its every output (i.e. for all items in the training set).\n        batch : int\n            Prediction batch size.\n\n        Returns\n        --------\n        out : pandas.DataFrame\n            Prediction scores for selected items for every event of the batch.\n            Columns: events of the batch; rows: items. Rows are indexed by the item IDs.\n\n        '''", "\n", "if", "self", ".", "error_during_train", ":", "raise", "Exception", "\n", "if", "self", ".", "predict", "is", "None", "or", "self", ".", "predict_batch", "!=", "batch", ":", "\n", "            ", "X", "=", "T", ".", "ivector", "(", ")", "\n", "Y", "=", "T", ".", "ivector", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "                ", "self", ".", "H", "[", "i", "]", ".", "set_value", "(", "np", ".", "zeros", "(", "(", "batch", ",", "self", ".", "layers", "[", "i", "]", ")", ",", "dtype", "=", "theano", ".", "config", ".", "floatX", ")", ",", "borrow", "=", "True", ")", "\n", "", "if", "predict_for_item_ids", "is", "not", "None", ":", "\n", "                ", "H_new", ",", "yhat", ",", "_", "=", "self", ".", "model", "(", "X", ",", "self", ".", "H", ",", "Y", ",", "predict", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "H_new", ",", "yhat", ",", "_", "=", "self", ".", "model", "(", "X", ",", "self", ".", "H", ",", "predict", "=", "True", ")", "\n", "", "updatesH", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "H", ")", ")", ":", "\n", "                ", "updatesH", "[", "self", ".", "H", "[", "i", "]", "]", "=", "H_new", "[", "i", "]", "\n", "", "if", "predict_for_item_ids", "is", "not", "None", ":", "\n", "                ", "self", ".", "predict", "=", "function", "(", "inputs", "=", "[", "X", ",", "Y", "]", ",", "outputs", "=", "yhat", ",", "updates", "=", "updatesH", ",", "allow_input_downcast", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "predict", "=", "function", "(", "inputs", "=", "[", "X", "]", ",", "outputs", "=", "yhat", ",", "updates", "=", "updatesH", ",", "allow_input_downcast", "=", "True", ")", "\n", "", "self", ".", "current_session", "=", "np", ".", "ones", "(", "batch", ")", "*", "-", "1", "\n", "self", ".", "predict_batch", "=", "batch", "\n", "", "session_change", "=", "np", ".", "arange", "(", "batch", ")", "[", "session_ids", "!=", "self", ".", "current_session", "]", "\n", "if", "len", "(", "session_change", ")", ">", "0", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "H", ")", ")", ":", "\n", "                ", "tmp", "=", "self", ".", "H", "[", "i", "]", ".", "get_value", "(", "borrow", "=", "True", ")", "\n", "tmp", "[", "session_change", "]", "=", "0", "\n", "self", ".", "H", "[", "i", "]", ".", "set_value", "(", "tmp", ",", "borrow", "=", "True", ")", "\n", "", "self", ".", "current_session", "=", "session_ids", ".", "copy", "(", ")", "\n", "", "in_idxs", "=", "self", ".", "itemidmap", "[", "input_item_ids", "]", "\n", "if", "predict_for_item_ids", "is", "not", "None", ":", "\n", "            ", "iIdxs", "=", "self", ".", "itemidmap", "[", "predict_for_item_ids", "]", "\n", "preds", "=", "np", ".", "asarray", "(", "self", ".", "predict", "(", "in_idxs", ",", "iIdxs", ")", ")", ".", "T", "\n", "return", "pd", ".", "DataFrame", "(", "data", "=", "preds", ",", "index", "=", "predict_for_item_ids", ")", "\n", "", "else", ":", "\n", "            ", "preds", "=", "np", ".", "asarray", "(", "self", ".", "predict", "(", "in_idxs", ")", ")", ".", "T", "\n", "return", "pd", ".", "DataFrame", "(", "data", "=", "preds", ",", "index", "=", "self", ".", "itemidmap", ".", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.predict_next": [[707, 737], ["gru4rec2.GRU4Rec.predict_next_batch", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.predict_next_batch"], ["", "", "def", "predict_next", "(", "self", ",", "session_id", ",", "input_item_id", ",", "predict_for_item_ids", "=", "None", ",", "skip", "=", "False", ",", "type", "=", "'view'", ",", "timestamp", "=", "0", ")", ":", "\n", "        ", "'''\n        Gives predicton scores for a selected set of items. Can be used in batch mode to predict for multiple independent events (i.e. events of different sessions) at once and thus speed up evaluation.\n\n        If the session ID at a given coordinate of the session_ids parameter remains the same during subsequent calls of the function, the corresponding hidden state of the network will be kept intact (i.e. that's how one can predict an item to a session).\n        If it changes, the hidden state of the network is reset to zeros.\n\n        Parameters\n        --------\n        session_ids : 1D array\n            Contains the session IDs of the events of the batch. Its length must equal to the prediction batch size (batch param).\n        input_item_ids : 1D array\n            Contains the item IDs of the events of the batch. Every item ID must be must be in the training data of the network. Its length must equal to the prediction batch size (batch param).\n        predict_for_item_ids : 1D array (optional)\n            IDs of items for which the network should give prediction scores. Every ID must be in the training set. The default value is None, which means that the network gives prediction on its every output (i.e. for all items in the training set).\n        batch : int\n            Prediction batch size.\n\n        Returns\n        --------\n        out : pandas.DataFrame\n            Prediction scores for selected items for every event of the batch.\n            Columns: events of the batch; rows: items. Rows are indexed by the item IDs.\n\n        '''", "\n", "\n", "if", "not", "input_item_id", "in", "self", ".", "itemidmap", ".", "index", ":", "\n", "            ", "return", "None", "\n", "\n", "", "return", "self", ".", "predict_next_batch", "(", "np", ".", "array", "(", "[", "session_id", "]", ")", ",", "np", ".", "array", "(", "[", "input_item_id", "]", ")", ",", "predict_for_item_ids", ",", "1", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.clear": [[738, 747], ["range", "gru4rec2.GRU4Rec.Wy.set_value", "gru4rec2.GRU4Rec.By.set_value", "len", "gru4rec2.GRU4Rec.Wx[].set_value", "gru4rec2.GRU4Rec.Wh[].set_value", "gru4rec2.GRU4Rec.Wrz[].set_value", "gru4rec2.GRU4Rec.Bh[].set_value", "gru4rec2.GRU4Rec.H[].set_value"], "methods", ["None"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "for", "x", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "            ", "self", ".", "Wx", "[", "x", "]", ".", "set_value", "(", "[", "[", "]", "]", ")", "\n", "self", ".", "Wh", "[", "x", "]", ".", "set_value", "(", "[", "[", "]", "]", ")", "\n", "self", ".", "Wrz", "[", "x", "]", ".", "set_value", "(", "[", "[", "]", "]", ")", "\n", "self", ".", "Bh", "[", "x", "]", ".", "set_value", "(", "[", "]", ")", "\n", "self", ".", "H", "[", "x", "]", ".", "set_value", "(", "[", "[", "]", "]", ")", "\n", "", "self", ".", "Wy", ".", "set_value", "(", "[", "[", "]", "]", ")", "\n", "self", ".", "By", ".", "set_value", "(", "[", "[", "]", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.parse_sequence_example": [[9, 44], ["tensorflow.parse_single_sequence_example", "utils.merge_two_dicts", "acr_datasets.expand_features", "tensorflow.FixedLenFeature", "tensorflow.FixedLenSequenceFeature", "features_config[].keys", "utils.get_tf_dtype", "utils.get_tf_dtype"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.merge_two_dicts", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.expand_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_tf_dtype", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_tf_dtype"], ["def", "parse_sequence_example", "(", "example", ",", "features_config", ",", "truncate_sequence_length", "=", "300", ")", ":", "\n", "# Define how to parse the example", "\n", "    ", "'''\n    context_features = {\n        \"article_id\": tf.FixedLenFeature([], dtype=tf.int64),\n        \"publisher_id\": tf.FixedLenFeature([], dtype=tf.int64),\n        \"category_id\": tf.FixedLenFeature([], dtype=tf.int64),\n        \"created_at_ts\": tf.FixedLenFeature([], dtype=tf.int64),\n        \"text_length\": tf.FixedLenFeature([], dtype=tf.int64),\n    }\n    '''", "\n", "\n", "single_features", "=", "{", "feature_name", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "dtype", "=", "get_tf_dtype", "(", "features_config", "[", "'single_features'", "]", "[", "feature_name", "]", "[", "'dtype'", "]", ")", ")", "for", "feature_name", "in", "features_config", "[", "'single_features'", "]", "}", "\n", "\n", "sequence_features", "=", "{", "feature_name", ":", "tf", ".", "FixedLenSequenceFeature", "(", "shape", "=", "[", "]", ",", "dtype", "=", "get_tf_dtype", "(", "features_config", "[", "'sequence_features'", "]", "[", "feature_name", "]", "[", "'dtype'", "]", ")", ")", "for", "feature_name", "in", "features_config", "[", "'sequence_features'", "]", "}", "\n", "\n", "\n", "single_parsed", ",", "sequence_parsed", "=", "tf", ".", "parse_single_sequence_example", "(", "\n", "example", ",", "\n", "sequence_features", "=", "sequence_features", ",", "\n", "context_features", "=", "single_features", ",", "\n", "example_name", "=", "\"example\"", "\n", ")", "\n", "\n", "#Truncating max text size", "\n", "sequence_parsed", "[", "'text'", "]", "=", "sequence_parsed", "[", "'text'", "]", "[", ":", "truncate_sequence_length", "]", "\n", "\n", "merged_features", "=", "merge_two_dicts", "(", "single_parsed", ",", "sequence_parsed", ")", "\n", "\n", "#In order the pad the dataset, I had to use this hack to expand scalars to vectors.", "\n", "expand_features", "(", "merged_features", ",", "feature_to_expand", "=", "features_config", "[", "'single_features'", "]", ".", "keys", "(", ")", ")", "\n", "\n", "return", "merged_features", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.expand_features": [[46, 52], ["tensorflow.expand_dims", "tensorflow.convert_to_tensor"], "function", ["None"], ["", "def", "expand_features", "(", "features", ",", "feature_to_expand", ")", ":", "\n", "    ", "'''\n    Hack. Because padded_batch doesn't play nice with scalres, so we expand the scalar to a vector of length 1\n    '''", "\n", "for", "feature_key", "in", "feature_to_expand", ":", "\n", "        ", "features", "[", "feature_key", "]", "=", "tf", ".", "expand_dims", "(", "tf", ".", "convert_to_tensor", "(", "features", "[", "feature_key", "]", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.deflate_features": [[53, 59], ["tensorflow.squeeze"], "function", ["None"], ["", "", "def", "deflate_features", "(", "features", ",", "feature_to_deflate", ")", ":", "\n", "    ", "'''\n        Undo Hack. We undo the expansion we did in expand\n    '''", "\n", "for", "feature_key", "in", "feature_to_deflate", ":", "\n", "        ", "features", "[", "feature_key", "]", "=", "tf", ".", "squeeze", "(", "features", "[", "feature_key", "]", ",", "axis", "=", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.get_label_features": [[60, 64], ["None"], "function", ["None"], ["", "", "def", "get_label_features", "(", "features", ",", "features_config", ")", ":", "\n", "    ", "return", "{", "feature_name", ":", "features", "[", "feature_name", "]", "for", "feature_name", "in", "features", "if", "feature_name", "in", "features_config", "[", "'label_features'", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.deflate_and_split_features_label": [[65, 72], ["acr_datasets.deflate_features", "acr_datasets.get_label_features"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.deflate_features", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.get_label_features"], ["", "def", "deflate_and_split_features_label", "(", "features", ",", "features_config", ")", ":", "\n", "#Undo that hack required for padding ", "\n", "    ", "deflate_features", "(", "features", ",", "features_config", "[", "'single_features'", "]", ")", "\n", "labels", "=", "get_label_features", "(", "features", ",", "features_config", ")", "\n", "\n", "#Returning features and label separatelly", "\n", "return", "(", "features", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.make_dataset": [[74, 124], ["tensorflow.data.TFRecordDataset", "dataset.prefetch.map", "utils.merge_two_dicts", "dataset.prefetch.padded_batch", "dataset.prefetch.map", "dataset.prefetch.prefetch", "multiprocessing.cpu_count", "tensorflow.logging.info", "tensorflow.TensorShape", "acr_datasets.parse_sequence_example", "acr_datasets.deflate_and_split_features_label"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.merge_two_dicts", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.parse_sequence_example", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.deflate_and_split_features_label"], ["", "def", "make_dataset", "(", "path", ",", "features_config", ",", "batch_size", "=", "128", ",", "num_map_threads", "=", "None", ",", "truncate_sequence_length", "=", "300", ")", ":", "\n", "    ", "'''\n    Makes  a Tensorflow dataset that is shuffled, batched and parsed \n    You can chain all the lines here, I split them into seperate calls so I could comment easily\n    :param path: The path to a tf record file\n    :param path: The size of our batch\n    :return: a Dataset that shuffles and is padded\n    '''", "\n", "\n", "if", "not", "num_map_threads", ":", "\n", "        ", "num_map_threads", "=", "multiprocessing", ".", "cpu_count", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Using {} threads for parallel map'", ".", "format", "(", "num_map_threads", ")", ")", "\n", "\n", "\n", "# Read a tf record file. This makes a dataset of raw TFRecords", "\n", "", "dataset", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "path", ",", "compression_type", "=", "'GZIP'", ")", "\n", "# Apply/map the parse function to every record. Now the dataset is a bunch of dictionaries of Tensors", "\n", "dataset", "=", "dataset", ".", "map", "(", "lambda", "x", ":", "parse_sequence_example", "(", "x", ",", "features_config", ",", "\n", "truncate_sequence_length", "=", "truncate_sequence_length", ")", ",", "\n", "num_parallel_calls", "=", "num_map_threads", ")", "\n", "\n", "features_shapes_single", "=", "{", "key", ":", "1", "for", "key", "in", "features_config", "[", "'single_features'", "]", "}", "\n", "features_shapes_sequence", "=", "{", "key", ":", "tf", ".", "TensorShape", "(", "[", "None", "]", ")", "for", "key", "in", "features_config", "[", "'sequence_features'", "]", "}", "\n", "features_shapes", "=", "merge_two_dicts", "(", "features_shapes_single", ",", "features_shapes_sequence", ")", "\n", "\n", "#Batch the dataset so that we get batch_size examples in each batch.", "\n", "#Remember each item in the dataset is a dict of tensors, we need to specify padding for each tensor seperatly", "\n", "dataset", "=", "dataset", ".", "padded_batch", "(", "batch_size", ",", "padded_shapes", "=", "features_shapes", ")", "\n", "\n", "'''\n    #Batch the dataset so that we get batch_size examples in each batch.\n    #Remember each item in the dataset is a dict of tensors, we need to specify padding for each tensor seperatly\n    dataset = dataset.padded_batch(batch_size, padded_shapes={\n        \"article_id\": 1, #Context doesn't need any padding, its always length one\n        \"publisher_id\": 1, \n        \"category_id\": 1,\n        \"created_at_ts\": 1,\n        \"text_length\": 1,    \n        \"text\": tf.TensorShape([None]), # but the seqeunce is variable length, we pass that information to TF        \n    })\n    '''", "\n", "\n", "#Finally, we need to undo that hack from the expand function", "\n", "#dataset= dataset.map(deflate)", "\n", "#Splitting features and label", "\n", "dataset", "=", "dataset", ".", "map", "(", "lambda", "features", ":", "deflate_and_split_features_label", "(", "features", ",", "features_config", ")", ",", "\n", "num_parallel_calls", "=", "num_map_threads", ")", "\n", "#Pre-fetches rows ahead", "\n", "dataset", "=", "dataset", ".", "prefetch", "(", "buffer_size", "=", "batch_size", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.prepare_dataset_iterator_with_initializer": [[126, 142], ["acr_datasets.make_dataset", "tensorflow.data.Iterator.from_structure", "tf.data.Iterator.from_structure.get_next", "tf.data.Iterator.from_structure.make_initializer"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.make_dataset"], ["", "def", "prepare_dataset_iterator_with_initializer", "(", "files", ",", "features_config", ",", "batch_size", "=", "128", ",", "truncate_tokens_length", "=", "300", ")", ":", "\n", "# Make a dataset ", "\n", "    ", "ds", "=", "make_dataset", "(", "files", ",", "features_config", ",", "batch_size", "=", "batch_size", ",", "\n", "truncate_sequence_length", "=", "truncate_tokens_length", ")", "\n", "\n", "# Define an abstract iterator that has the shape and type of our datasets", "\n", "iterator", "=", "tf", ".", "data", ".", "Iterator", ".", "from_structure", "(", "ds", ".", "output_types", ",", "\n", "ds", ".", "output_shapes", ")", "\n", "\n", "# This is an op that gets the next element from the iterator", "\n", "next_element", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "# These ops let us switch and reinitialize every time we finish an epoch    ", "\n", "iterator_init_op", "=", "iterator", ".", "make_initializer", "(", "ds", ")", "\n", "\n", "return", "next_element", ",", "iterator_init_op", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.prepare_dataset": [[145, 165], ["tensorflow.device", "acr_datasets.make_dataset", "ds.shuffle.repeat", "ds.shuffle.make_one_shot_iterator", "ds.make_one_shot_iterator.get_next", "ds.shuffle.shuffle"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.make_dataset"], ["", "def", "prepare_dataset", "(", "files", ",", "features_config", ",", "batch_size", "=", "128", ",", "epochs", "=", "1", ",", "\n", "shuffle_dataset", "=", "True", ",", "shuffle_buffer_size", "=", "3000", ",", "\n", "truncate_tokens_length", "=", "300", ")", ":", "\n", "#Making sure that data preprocessing steps (I/O bound) are not performed on GPU", "\n", "    ", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "# Make a dataset ", "\n", "        ", "ds", "=", "make_dataset", "(", "files", ",", "features_config", ",", "batch_size", "=", "batch_size", ",", "\n", "truncate_sequence_length", "=", "truncate_tokens_length", ")", "\n", "\n", "ds", "=", "ds", ".", "repeat", "(", "epochs", ")", "\n", "if", "shuffle_dataset", ":", "\n", "            ", "ds", "=", "ds", ".", "shuffle", "(", "buffer_size", "=", "shuffle_buffer_size", ")", "\n", "\n", "# Define an abstract iterator that has the shape and type of our datasets", "\n", "", "iterator", "=", "ds", ".", "make_one_shot_iterator", "(", ")", "\n", "\n", "# This is an op that gets the next element from the iterator", "\n", "next_element", "=", "iterator", ".", "get_next", "(", ")", "\n", "\n", "return", "next_element", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.make_sequential_feature": [[12, 18], ["tensorflow.train.FeatureList", "tensorflow.train.Feature", "tensorflow.train.Feature", "tensorflow.train.Int64List", "tensorflow.train.FloatList"], "function", ["None"], ["def", "make_sequential_feature", "(", "values", ",", "vtype", "=", "int", ")", ":", "\n", "    ", "if", "vtype", "==", "int", ":", "\n", "        ", "features", "=", "[", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "[", "value", "]", ")", ")", "for", "value", "in", "values", "]", "\n", "", "elif", "vtype", "==", "float", ":", "\n", "        ", "features", "=", "[", "tf", ".", "train", ".", "Feature", "(", "float_list", "=", "tf", ".", "train", ".", "FloatList", "(", "value", "=", "[", "value", "]", ")", ")", "for", "value", "in", "values", "]", "\n", "", "elif", "vtype", "==", "str", ":", "\n", "        ", "features", "=", "[", "tf", ".", "train", ".", "Feature", "(", "bytes_list", "=", "tf", ".", "train", ".", "BytesList", "(", "value", "=", "[", "value", ".", "encode", "(", ")", "]", ")", ")", "for", "value", "in", "values", "]", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.save_rows_to_tf_record_file": [[20, 31], ["tensorflow.python.lib.io.tf_record.TFRecordOptions", "tensorflow.python.lib.io.tf_record.TFRecordWriter", "df_rows.iterrows", "tf_record.TFRecordWriter.close", "sys.stdout.flush", "make_sequence_example_fn", "tf_record.TFRecordWriter.write", "make_sequence_example_fn.SerializeToString"], "function", ["None"], ["\n", "\n", "", "def", "save_rows_to_tf_record_file", "(", "rows", ",", "make_sequence_example_fn", ",", "export_filename", ")", ":", "\n", "    ", "tf_record_options", "=", "tf_record", ".", "TFRecordOptions", "(", "tf_record", ".", "TFRecordCompressionType", ".", "GZIP", ")", "\n", "\n", "tf_writer", "=", "tf_record", ".", "TFRecordWriter", "(", "export_filename", ",", "options", "=", "tf_record_options", ")", "\n", "try", ":", "\n", "        ", "for", "row", "in", "rows", ":", "\n", "            ", "seq_example", "=", "make_sequence_example_fn", "(", "row", ")", "\n", "tf_writer", ".", "write", "(", "seq_example", ".", "SerializeToString", "(", ")", ")", "\n", "", "", "finally", ":", "\n", "        ", "tf_writer", ".", "close", "(", ")", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.export_dataframe_to_tf_records": [[32, 42], ["output_path.replace", "enumerate", "utils.chunks", "print", "tf_records_management.save_rows_to_tf_record_file", "len", "output_path.replace.format"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.chunks", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.tf_records_management.save_rows_to_tf_record_file"], ["sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "", "def", "export_dataframe_to_tf_records", "(", "dataframe", ",", "make_sequence_example_fn", ",", "output_path", ",", "\n", "examples_by_file", "=", "1000", ")", ":", "\n", "    ", "export_file_template", "=", "output_path", ".", "replace", "(", "'*'", ",", "'{0:04d}'", ")", "\n", "\n", "#Exporting rows to TF record by chunks", "\n", "for", "chunk_index", ",", "df_chunk", "in", "enumerate", "(", "chunks", "(", "dataframe", ",", "examples_by_file", ")", ")", ":", "\n", "        ", "print", "(", "\"Exporting chunk {} (length: {})\"", ".", "format", "(", "chunk_index", ",", "len", "(", "df_chunk", ")", ")", ")", "\n", "save_rows_to_tf_record_file", "(", "df_chunk", ",", "\n", "make_sequence_example_fn", ",", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.__init__": [[35, 100], ["text_feature_extractor.upper", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.constant", "tensorflow.nn.embedding_lookup", "tensorflow.logging.info", "training_task.lower", "acr_model.ACR_Model.build_graph_metadata_classification", "tensorflow.contrib.layers.xavier_initializer", "training_task.lower", "acr_model.ACR_Model.build_graph_autoencoder", "Exception", "tensorflow.nn.embedding_lookup.get_shape"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.build_graph_metadata_classification", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.build_graph_autoencoder"], ["    ", "def", "__init__", "(", "self", ",", "training_task", ",", "text_feature_extractor", ",", "features", ",", "\n", "metadata_input_features", ",", "metadata_input_feature_columns", ",", "labels", ",", "labels_features_config", ",", "mode", ",", "params", ")", ":", "\n", "        ", "self", ".", "params", "=", "params", "\n", "self", ".", "training", "=", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "text_feature_extractor", "=", "text_feature_extractor", ".", "upper", "(", ")", "\n", "self", ".", "labels_features_config", "=", "labels_features_config", "\n", "self", ".", "acr_embeddings_size", "=", "params", "[", "'acr_embeddings_size'", "]", "\n", "self", ".", "dropout_keep_prob", "=", "params", "[", "'dropout_keep_prob'", "]", "\n", "self", ".", "l2_reg_lambda", "=", "params", "[", "'l2_reg_lambda'", "]", "\n", "self", ".", "rnn_units", "=", "params", "[", "'rnn_units'", "]", "\n", "self", ".", "rnn_layers", "=", "params", "[", "'rnn_layers'", "]", "\n", "self", ".", "rnn_direction", "=", "params", "[", "'rnn_direction'", "]", "\n", "self", ".", "special_token_embedding_vector", "=", "params", "[", "'special_token_embedding_vector'", "]", "\n", "self", ".", "autoencoder_noise", "=", "params", "[", "'autoencoder_noise'", "]", "\n", "self", ".", "features", "=", "features", "\n", "self", ".", "labels", "=", "labels", "\n", "\n", "\n", "self", ".", "article_id", "=", "features", "[", "'article_id'", "]", "\n", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"main\"", ",", "initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", ":", "\n", "            ", "'''\n            input_features = []            \n            with tf.variable_scope(\"input_article_metadata\"):\n                #If there is articles metadata available for the model\n                if metadata_input_features is not None and len(metadata_input_features) > 0:\n                    metadata_features = tf.feature_column.input_layer(metadata_input_features, \n                                                                    metadata_input_feature_columns)\n\n                    tf.logging.info(\"Metadata features shape: {}\".format(metadata_features.get_shape()))\n\n                    #Creating a FC layer on top of metadata features (usually OHE categorical features) to make it dense (like the CNN output)\n                    num_metadata_inputs = max(int(int(metadata_features.get_shape()[-1]) / 4), 2)\n                    hidden_metadata = tf.layers.dense(inputs=metadata_features, units=num_metadata_inputs, \n                                             activation=tf.nn.relu,\n                                             kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg_lambda))\n\n                    tf.logging.info(\"Hidden Metadata features shape: {}\".format(hidden_metadata.get_shape()))\n\n                    input_features.append(hidden_metadata)\n                    \n\n                    #tf.summary.histogram('multi_hot', \n                    #              values=tf.reduce_sum(metadata_features[:,1:], axis=1))\n            '''", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"input_word_embeddings\"", ")", ":", "\n", "                ", "self", ".", "word_embeddings_matrix", "=", "tf", ".", "constant", "(", "self", ".", "params", "[", "'word_embeddings_matrix'", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "input_text_embeddings", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "word_embeddings_matrix", ",", "features", "[", "'text'", "]", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'input_text_layer shape: {}'", ".", "format", "(", "input_text_embeddings", ".", "get_shape", "(", ")", ")", ")", "\n", "\n", "\n", "", "if", "training_task", ".", "lower", "(", ")", "==", "'metadata_classification'", ":", "\n", "                ", "self", ".", "build_graph_metadata_classification", "(", "input_text_embeddings", ")", "\n", "\n", "\n", "", "elif", "training_task", ".", "lower", "(", ")", "==", "'autoencoder'", ":", "\n", "                ", "self", ".", "build_graph_autoencoder", "(", "input_text_embeddings", ")", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "'Training task can be only: (metadata_classification | autoencoder)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.build_graph_metadata_classification": [[103, 269], ["tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.logging.info", "tensorflow.layers.dropout", "tensorflow.layers.dense", "tensorflow.variable_scope", "tensorflow.layers.dropout", "acr_model.ACR_Model.cnn_feature_extractor", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.variable_scope", "tensorflow.constant", "tensorflow.summary.scalar", "tensorflow.losses.get_regularization_loss", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.variable_scope", "tensorflow.reverse", "acr_model.ACR_Model.build_cudnn_rnn", "tensorflow.reduce_max", "Exception", "input_features_concat.get_shape", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.variable_scope", "tensorflow.contrib.layers.optimize_loss", "tensorflow.contrib.layers.l2_regularizer", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.argmax", "tensorflow.constant", "tensorflow.constant", "tensorflow.reduce_mean", "tensorflow.summary.scalar", "tensorflow.metrics.accuracy", "tensorflow.contrib.layers.l2_regularizer", "acr_model.multi_label_predictions_binarizer", "tensorflow.gather", "tensorflow.losses.sparse_softmax_cross_entropy", "tensorflow.reduce_sum", "tensorflow.multiply", "tensorflow.logging.info", "tensorflow.reduce_mean", "tensorflow.summary.scalar", "tensorflow.train.get_global_step", "tensorflow.metrics.precision", "tensorflow.metrics.recall", "tensorflow.one_hot", "tensorflow.concat", "tensorflow.reduce_mean", "acr_model.ACR_Model.labels[].get_shape", "tensorflow.nn.sigmoid_cross_entropy_with_logits", "tensorflow.constant", "tensorflow.ones", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.dropout", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.cnn_feature_extractor", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.build_cudnn_rnn", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.multi_label_predictions_binarizer"], ["", "", "", "def", "build_graph_metadata_classification", "(", "self", ",", "input_text_embeddings", ")", ":", "\n", "        ", "labels_features_config", "=", "self", ".", "labels_features_config", "\n", "text_feature_extractor", "=", "self", ".", "text_feature_extractor", "\n", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"metadata_classification\"", ")", ":", "\n", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"textual_feature_extraction\"", ")", ":", "\n", "                ", "if", "text_feature_extractor", "==", "'CNN'", ":", "\n", "                    ", "content_features", "=", "self", ".", "cnn_feature_extractor", "(", "input_text_embeddings", ")", "\n", "\n", "", "elif", "text_feature_extractor", "in", "[", "'LSTM'", ",", "'GRU'", "]", ":", "\n", "#Reversing input feature for better performance (because usually most relevant words are in the start of the document)", "\n", "                    ", "input_text_embeddings_reversed", "=", "tf", ".", "reverse", "(", "input_text_embeddings", ",", "axis", "=", "[", "1", "]", ")", "\n", "\n", "rnn_outputs", ",", "_", ",", "_", "=", "self", ".", "build_cudnn_rnn", "(", "rnn_type", "=", "text_feature_extractor", ",", "\n", "input_text_embeddings", "=", "input_text_embeddings_reversed", ",", "\n", "text_lengths", "=", "self", ".", "features", "[", "'text_length'", "]", ",", "\n", "suffix", "=", "'rnn'", ")", "\n", "\n", "#Max pool on time (words) dimension of the output", "\n", "content_features", "=", "tf", ".", "reduce_max", "(", "rnn_outputs", ",", "axis", "=", "1", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "'Text feature extractor option invalid! Valid values are: CNN, LSTM, GRU'", ")", "\n", "\n", "\n", "#input_features.append(content_features)", "\n", "#input_features_concat = tf.concat(input_features, axis=-1)", "\n", "\n", "", "input_features_concat", "=", "content_features", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_features_concat.shape={}\"", ".", "format", "(", "input_features_concat", ".", "get_shape", "(", ")", ")", ")", "\n", "\n", "dropout_conv_layers_concat", "=", "tf", ".", "layers", ".", "dropout", "(", "inputs", "=", "input_features_concat", ",", "\n", "rate", "=", "1.0", "-", "self", ".", "dropout_keep_prob", ",", "\n", "training", "=", "self", ".", "training", ")", "\n", "\n", "fc2", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", "=", "dropout_conv_layers_concat", ",", "units", "=", "self", ".", "acr_embeddings_size", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "l2_reg_lambda", ")", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"article_content_embedding\"", ")", ":", "\n", "                    ", "hidden", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", "=", "fc2", ",", "units", "=", "self", ".", "acr_embeddings_size", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "tanh", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "l2_reg_lambda", ")", ",", "\n", "kernel_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ")", "\n", "self", ".", "article_content_embedding", "=", "hidden", "\n", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"network_output\"", ")", ":", "\n", "                ", "dropout_hidden", "=", "tf", ".", "layers", ".", "dropout", "(", "inputs", "=", "hidden", ",", "\n", "rate", "=", "1.0", "-", "self", ".", "dropout_keep_prob", ",", "\n", "training", "=", "self", ".", "training", ")", "\n", "\n", "labels_logits", "=", "{", "}", "\n", "self", ".", "labels_predictions", "=", "{", "}", "\n", "\n", "for", "label_feature_name", "in", "labels_features_config", ":", "\n", "                    ", "with", "tf", ".", "variable_scope", "(", "\"output_{}\"", ".", "format", "(", "label_feature_name", ")", ")", ":", "\n", "\n", "                        ", "label_feature", "=", "labels_features_config", "[", "label_feature_name", "]", "\n", "\n", "labels_logits", "[", "label_feature_name", "]", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", "=", "dropout_hidden", ",", "units", "=", "label_feature", "[", "'cardinality'", "]", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "l2_reg_lambda", ")", ",", "\n", "activation", "=", "None", ")", "\n", "\n", "if", "labels_features_config", "[", "label_feature_name", "]", "[", "'classification_type'", "]", "==", "'multiclass'", ":", "\n", "                            ", "self", ".", "labels_predictions", "[", "label_feature_name", "]", "=", "tf", ".", "argmax", "(", "labels_logits", "[", "label_feature_name", "]", ",", "1", ")", "\n", "\n", "", "elif", "labels_features_config", "[", "label_feature_name", "]", "[", "'classification_type'", "]", "==", "'multilabel'", ":", "\n", "#If its a multi-label head, convert labels in multi-hot representation", "\n", "                            ", "self", ".", "labels_predictions", "[", "label_feature_name", "]", "=", "multi_label_predictions_binarizer", "(", "labels_logits", "[", "label_feature_name", "]", ")", "\n", "\n", "", "", "", "", "if", "self", ".", "mode", "!=", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "\"loss\"", ")", ":", "\n", "                    ", "loss", "=", "tf", ".", "constant", "(", "0.0", ",", "tf", ".", "float32", ")", "\n", "\n", "#Creating a tensor for class weights of each label", "\n", "labels_classes_weights", "=", "{", "}", "\n", "if", "'labels_class_weights'", "in", "self", ".", "params", ":", "\n", "                        ", "for", "label_column_name", "in", "self", ".", "params", "[", "'labels_class_weights'", "]", ":", "\n", "                            ", "labels_classes_weights", "[", "label_column_name", "]", "=", "tf", ".", "constant", "(", "self", ".", "params", "[", "'labels_class_weights'", "]", "[", "label_column_name", "]", ",", "tf", ".", "float32", ")", "\n", "\n", "\n", "", "", "for", "label_feature_name", "in", "labels_features_config", ":", "\n", "                        ", "if", "labels_features_config", "[", "label_feature_name", "]", "[", "'classification_type'", "]", "==", "'multiclass'", ":", "\n", "#If the label feature have classes weights, use the weights to deal with unbalanced classes", "\n", "                            ", "weights", "=", "tf", ".", "constant", "(", "1.0", ")", "\n", "if", "label_feature_name", "in", "labels_classes_weights", ":", "\n", "                                ", "weights", "=", "tf", ".", "gather", "(", "labels_classes_weights", "[", "label_feature_name", "]", ",", "self", ".", "labels", "[", "label_feature_name", "]", ")", "\n", "\n", "", "label_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "losses", ".", "sparse_softmax_cross_entropy", "(", "logits", "=", "labels_logits", "[", "label_feature_name", "]", ",", "\n", "labels", "=", "self", ".", "labels", "[", "label_feature_name", "]", ",", "\n", "weights", "=", "weights", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'softmax_cross_entropy_loss-{}'", ".", "format", "(", "label_feature_name", ")", ",", "\n", "family", "=", "'train'", ",", "tensor", "=", "label_loss", ")", "\n", "\n", "", "elif", "labels_features_config", "[", "label_feature_name", "]", "[", "'classification_type'", "]", "==", "'multilabel'", ":", "\n", "#Tuning the label into multi-hot representation", "\n", "                            ", "self", ".", "labels", "[", "label_feature_name", "]", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "one_hot", "(", "indices", "=", "self", ".", "labels", "[", "label_feature_name", "]", ",", "\n", "depth", "=", "labels_features_config", "[", "label_feature_name", "]", "[", "'cardinality'", "]", "\n", ")", ",", "reduction_indices", "=", "1", ")", "\n", "\n", "#Forcing that label of padding value is always zero", "\n", "self", ".", "labels", "[", "label_feature_name", "]", "=", "tf", ".", "multiply", "(", "self", ".", "labels", "[", "label_feature_name", "]", ",", "\n", "tf", ".", "concat", "(", "[", "tf", ".", "constant", "(", "[", "0", "]", ",", "tf", ".", "float32", ")", ",", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "self", ".", "labels", "[", "label_feature_name", "]", ")", "[", "-", "1", "]", "-", "1", ",", "tf", ".", "float32", ")", "]", ",", "axis", "=", "0", ")", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"labels_multi_hot.shape = {}\"", ".", "format", "(", "self", ".", "labels", "[", "label_feature_name", "]", ".", "get_shape", "(", ")", ")", ")", "\n", "\n", "label_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "sigmoid_cross_entropy_with_logits", "(", "labels", "=", "self", ".", "labels", "[", "label_feature_name", "]", ",", "\n", "logits", "=", "labels_logits", "[", "label_feature_name", "]", ")", ",", "axis", "=", "1", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'sigmoid-cross_entropy_loss-{}'", ".", "format", "(", "label_feature_name", ")", ",", "\n", "family", "=", "'train'", ",", "tensor", "=", "label_loss", ")", "\n", "\n", "", "feature_weight_on_loss", "=", "labels_features_config", "[", "'feature_weight_on_loss'", "]", "if", "'feature_weight_on_loss'", "in", "labels_features_config", "else", "1.0", "\n", "loss", "+=", "feature_weight_on_loss", "*", "label_loss", "\n", "\n", "", "tf", ".", "summary", ".", "scalar", "(", "'cross_entropy_loss-total'", ",", "family", "=", "'train'", ",", "tensor", "=", "loss", ")", "\n", "\n", "reg_loss", "=", "tf", ".", "losses", ".", "get_regularization_loss", "(", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"reg_loss\"", ",", "family", "=", "'train'", ",", "tensor", "=", "reg_loss", ")", "\n", "\n", "self", ".", "total_loss", "=", "loss", "+", "reg_loss", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"total_loss\"", ",", "family", "=", "'train'", ",", "tensor", "=", "self", ".", "total_loss", ")", "\n", "\n", "", "if", "self", ".", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "                    ", "with", "tf", ".", "variable_scope", "(", "\"training\"", ")", ":", "\n", "                        ", "self", ".", "train_op", "=", "tf", ".", "contrib", ".", "layers", ".", "optimize_loss", "(", "\n", "loss", "=", "self", ".", "total_loss", ",", "\n", "optimizer", "=", "\"Adam\"", ",", "\n", "learning_rate", "=", "self", ".", "params", "[", "'learning_rate'", "]", ",", "\n", "#learning_rate_decay_fn=lambda lr, gs: tf.train.exponential_decay(params['learning_rate'], tf.train.get_global_step(), 100, 0.96, staircase=True),", "\n", "global_step", "=", "tf", ".", "train", ".", "get_global_step", "(", ")", ",", "\n", "summaries", "=", "[", "\"learning_rate\"", ",", "\"global_gradient_norm\"", "]", ")", "\n", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"eval_metrics\"", ")", ":", "\n", "\n", "                    ", "self", ".", "eval_metrics", "=", "{", "}", "\n", "\n", "for", "label_feature_name", "in", "labels_features_config", ":", "\n", "\n", "                        ", "if", "labels_features_config", "[", "label_feature_name", "]", "[", "'classification_type'", "]", "==", "'multiclass'", ":", "\n", "                            ", "accuracy", ",", "accuracy_update_op", "=", "tf", ".", "metrics", ".", "accuracy", "(", "predictions", "=", "self", ".", "labels_predictions", "[", "label_feature_name", "]", ",", "\n", "labels", "=", "self", ".", "labels", "[", "label_feature_name", "]", ",", "\n", "name", "=", "\"accuracy-{}\"", ".", "format", "(", "label_feature_name", ")", ")", "\n", "\n", "self", ".", "eval_metrics", "[", "\"accuracy-{}\"", ".", "format", "(", "label_feature_name", ")", "]", "=", "(", "accuracy", ",", "accuracy_update_op", ")", "\n", "\n", "", "elif", "labels_features_config", "[", "label_feature_name", "]", "[", "'classification_type'", "]", "==", "'multilabel'", ":", "\n", "\n", "                            ", "precision", ",", "precision_update_op", "=", "tf", ".", "metrics", ".", "precision", "(", "predictions", "=", "self", ".", "labels_predictions", "[", "label_feature_name", "]", ",", "\n", "labels", "=", "self", ".", "labels", "[", "label_feature_name", "]", ",", "\n", "name", "=", "\"precision-{}\"", ".", "format", "(", "label_feature_name", ")", ")", "\n", "\n", "self", ".", "eval_metrics", "[", "\"precision-{}\"", ".", "format", "(", "label_feature_name", ")", "]", "=", "(", "precision", ",", "precision_update_op", ")", "\n", "\n", "recall", ",", "recall_update_op", "=", "tf", ".", "metrics", ".", "recall", "(", "predictions", "=", "self", ".", "labels_predictions", "[", "label_feature_name", "]", ",", "\n", "labels", "=", "self", ".", "labels", "[", "label_feature_name", "]", ",", "\n", "name", "=", "\"recall-{}\"", ".", "format", "(", "label_feature_name", ")", ")", "\n", "\n", "self", ".", "eval_metrics", "[", "\"recall-{}\"", ".", "format", "(", "label_feature_name", ")", "]", "=", "(", "recall", ",", "recall_update_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.cnn_feature_extractor": [[272, 290], ["tensorflow.variable_scope", "map", "tensorflow.concat", "acr_model.ACR_Model.params[].split", "tensorflow.layers.conv1d", "tensorflow.reduce_max", "conv_layers.append", "tensorflow.contrib.layers.l2_regularizer"], "methods", ["None"], ["", "", "", "", "", "", "def", "cnn_feature_extractor", "(", "self", ",", "input_text_embeddings", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"CNN\"", ")", ":", "\n", "            ", "conv_layers", "=", "[", "]", "\n", "for", "kernel_sizes", "in", "map", "(", "int", ",", "self", ".", "params", "[", "'cnn_filter_sizes'", "]", ".", "split", "(", "','", ")", ")", ":", "\n", "                ", "conv", "=", "tf", ".", "layers", ".", "conv1d", "(", "\n", "inputs", "=", "input_text_embeddings", ",", "\n", "filters", "=", "self", ".", "params", "[", "'cnn_num_filters'", "]", ",", "\n", "kernel_size", "=", "kernel_sizes", ",", "\n", "padding", "=", "\"valid\"", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "self", ".", "l2_reg_lambda", ")", ")", "\n", "# Max Pooling over time (words)", "\n", "pool", "=", "tf", ".", "reduce_max", "(", "input_tensor", "=", "conv", ",", "axis", "=", "1", ")", "\n", "\n", "conv_layers", ".", "append", "(", "pool", ")", "\n", "\n", "", "conv_layers_concat", "=", "tf", ".", "concat", "(", "conv_layers", ",", "axis", "=", "-", "1", ")", "\n", "return", "conv_layers_concat", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.build_cudnn_rnn": [[293, 400], ["tensorflow.variable_scope", "tensorflow.transpose", "tensorflow.logging.info", "tensorflow.shape", "tensorflow.device", "tensorflow.transpose.get_shape", "tensorflow.variable_scope", "tensorflow.logging.info", "tensorflow.contrib.cudnn_rnn.CudnnLSTM", "tensorflow.contrib.cudnn_rnn.CudnnGRU.", "tensorflow.transpose", "acr_model.cudnn_lstm_state_to_state_tuples", "tuple", "acr_model.state_tuples_to_cudnn_lstm_state", "tensorflow.variable_scope", "tensorflow.contrib.cudnn_rnn.CudnnGRU", "tensorflow.contrib.cudnn_rnn.CudnnGRU.", "tensorflow.transpose", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.zeros_initializer", "tensorflow.concat", "tensorflow.contrib.rnn.LSTMStateTuple", "tensorflow.zeros", "tensorflow.contrib.layers.xavier_initializer", "tensorflow.zeros_initializer", "tensorflow.concat", "range", "tensorflow.zeros", "tensorflow.zeros"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.cudnn_lstm_state_to_state_tuples", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.state_tuples_to_cudnn_lstm_state"], ["", "", "def", "build_cudnn_rnn", "(", "self", ",", "rnn_type", ",", "input_text_embeddings", ",", "text_lengths", ",", "return_max_pool_over_outputs", "=", "True", ",", "\n", "cudnn_initial_state", "=", "None", ",", "suffix", "=", "''", ")", ":", "\n", "#Based on https://github.com/tensorflow/magenta/blob/master/magenta/models/shared/events_rnn_graph.py", "\n", "# and on https://github.com/mutux/ptb_lm/blob/master/lm.py", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"RNN_{}\"", ".", "format", "(", "suffix", ")", ")", ":", "\n", "            ", "batch_size", "=", "tf", ".", "shape", "(", "input_text_embeddings", ")", "[", "0", "]", "\n", "\n", "#Converting inputs from Batch-major to Time-major (requirement for CUDRNN)", "\n", "cudnn_inputs", "=", "tf", ".", "transpose", "(", "input_text_embeddings", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'cudnn_inputs={}'", ".", "format", "(", "cudnn_inputs", ".", "get_shape", "(", ")", ")", ")", "\n", "\n", "\n", "#Keep all this Ops on GPU, to avoid error when saving CudnnGRU object in the checkpoint", "\n", "with", "tf", ".", "device", "(", "'/gpu:0'", ")", ":", "\n", "\n", "                ", "if", "rnn_type", "==", "'LSTM'", ":", "\n", "                    ", "with", "tf", ".", "variable_scope", "(", "\"LSTM\"", ")", ":", "\n", "\n", "#If do not pass initial state, initializes with zeros", "\n", "                        ", "if", "cudnn_initial_state", "==", "None", ":", "\n", "                            ", "initial_state", "=", "tuple", "(", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "\n", "h", "=", "tf", ".", "zeros", "(", "[", "batch_size", ",", "self", ".", "rnn_units", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ",", "\n", "c", "=", "tf", ".", "zeros", "(", "[", "batch_size", ",", "self", ".", "rnn_units", "]", ",", "dtype", "=", "tf", ".", "float32", ")", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "rnn_layers", "*", "(", "2", "if", "self", ".", "rnn_direction", "==", "'bidirectional'", "else", "1", ")", ")", ")", "\n", "cudnn_initial_state", "=", "state_tuples_to_cudnn_lstm_state", "(", "initial_state", ")", "\n", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"cudnn_initial_state={}\"", ".", "format", "(", "cudnn_initial_state", ")", ")", "\n", "\n", "\n", "cell", "=", "tf", ".", "contrib", ".", "cudnn_rnn", ".", "CudnnLSTM", "(", "\n", "num_layers", "=", "self", ".", "rnn_layers", ",", "\n", "num_units", "=", "self", ".", "rnn_units", ",", "\n", "direction", "=", "self", ".", "rnn_direction", ",", "\n", "dropout", "=", "1.0", "-", "self", ".", "dropout_keep_prob", ",", "#Dropout is applied between each layer (no dropout is applied for a model with a single layer", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "kernel_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ",", "\n", "bias_initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ",", "\n", ")", "\n", "\n", "rnn_outputs", ",", "cudnn_final_state", "=", "cell", "(", "\n", "cudnn_inputs", ",", "\n", "initial_state", "=", "cudnn_initial_state", ",", "\n", "training", "=", "self", ".", "training", "\n", ")", "\n", "\n", "#Converting back from Time-major (requirement for CUDRNN) to Batch-major ", "\n", "rnn_outputs", "=", "tf", ".", "transpose", "(", "rnn_outputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "\n", "#Return hidden state from last RNN layer", "\n", "final_state", "=", "cudnn_lstm_state_to_state_tuples", "(", "cudnn_final_state", ")", "\n", "\n", "#Takes the last hidden state from last RNN layer", "\n", "if", "self", ".", "rnn_direction", "==", "'unidirectional'", ":", "\n", "                            ", "last_layer_state", "=", "final_state", "[", "-", "1", "]", ".", "h", "\n", "", "elif", "self", ".", "rnn_direction", "==", "'bidirectional'", ":", "\n", "#Combines last forward and backward layers", "\n", "                            ", "last_layer_state", "=", "tf", ".", "concat", "(", "[", "final_state", "[", "-", "1", "]", ".", "h", ",", "\n", "final_state", "[", "-", "2", "]", ".", "h", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "\n", "", "", "", "elif", "rnn_type", "==", "'GRU'", ":", "\n", "                    ", "with", "tf", ".", "variable_scope", "(", "\"GRU\"", ")", ":", "\n", "\n", "#If do not pass initial state, initializes with zeros", "\n", "                        ", "if", "cudnn_initial_state", "==", "None", ":", "\n", "                            ", "cudnn_initial_state", "=", "(", "tf", ".", "zeros", "(", "[", "self", ".", "rnn_layers", "*", "(", "2", "if", "self", ".", "rnn_direction", "==", "'bidirectional'", "else", "1", ")", ",", "\n", "batch_size", ",", "\n", "self", ".", "rnn_units", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", ",", "\n", ")", "\n", "\n", "#GRU", "\n", "", "cell", "=", "tf", ".", "contrib", ".", "cudnn_rnn", ".", "CudnnGRU", "(", "\n", "num_layers", "=", "self", ".", "rnn_layers", ",", "\n", "num_units", "=", "self", ".", "rnn_units", ",", "\n", "direction", "=", "self", ".", "rnn_direction", ",", "\n", "dropout", "=", "1.0", "-", "self", ".", "dropout_keep_prob", ",", "#Dropout is applied between each layer (no dropout is applied for a model with a single layer", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "kernel_initializer", "=", "tf", ".", "contrib", ".", "layers", ".", "xavier_initializer", "(", ")", ",", "\n", "bias_initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ",", "\n", ")", "\n", "\n", "rnn_outputs", ",", "cudnn_final_state", "=", "cell", "(", "\n", "cudnn_inputs", ",", "\n", "initial_state", "=", "cudnn_initial_state", ",", "\n", "training", "=", "self", ".", "training", "\n", ")", "\n", "\n", "\n", "#Converting back from Time-major (requirement for CUDRNN) to Batch-major ", "\n", "rnn_outputs", "=", "tf", ".", "transpose", "(", "rnn_outputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "#Takes the last hidden state from last RNN layer", "\n", "if", "self", ".", "rnn_direction", "==", "'unidirectional'", ":", "\n", "                            ", "last_layer_state", "=", "cudnn_final_state", "[", "0", "]", "[", "-", "1", "]", "\n", "", "elif", "self", ".", "rnn_direction", "==", "'bidirectional'", ":", "\n", "#Combines last forward and backward layers", "\n", "                            ", "last_layer_state", "=", "tf", ".", "concat", "(", "[", "cudnn_final_state", "[", "0", "]", "[", "-", "1", "]", ",", "\n", "cudnn_final_state", "[", "0", "]", "[", "-", "2", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "\n", "#Investigate: Unlike dynamic_rnn, CudnnGRU doesn't allow you to specify sequence lengths. Still, it is over an order of magnitude faster, but you will have to be careful on how you extract your outputs (e.g. if you're interested in the final hidden state of each sequence that is padded and of varying length, you will need each sequence's length).", "\n", "\n", "", "", "", "", "return", "rnn_outputs", ",", "cudnn_final_state", ",", "last_layer_state", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.build_graph_autoencoder": [[403, 532], ["tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.constant", "tensorflow.reverse", "acr_model.ACR_Model.build_cudnn_rnn", "tensorflow.layers.dense", "tensorflow.logging.info", "tensorflow.layers.dense", "tensorflow.logging.info", "tensorflow.tile", "tensorflow.concat", "acr_model.ACR_Model.build_cudnn_rnn", "tensorflow.layers.dense", "tensorflow.tile", "acr_model.cosine_sim_v2", "tensorflow.nn.top_k", "tensorflow.logging.info", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.losses.get_regularization_loss", "tensorflow.summary.scalar", "tensorflow.metrics.mean_squared_error", "tensorflow.random_normal", "tensorflow.layers.dense.get_shape", "tensorflow.layers.dense.get_shape", "sorted_word_ids.get_shape", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.shape", "tensorflow.shape", "tensorflow.cast", "tensorflow.reduce_mean", "tensorflow.square", "tensorflow.variable_scope", "tensorflow.contrib.layers.optimize_loss", "tensorflow.shape", "tensorflow.sign", "tensorflow.shape", "tensorflow.reduce_sum", "tensorflow.train.get_global_step"], "methods", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.build_cudnn_rnn", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.ACR_Model.build_cudnn_rnn", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.cosine_sim_v2"], ["", "", "def", "build_graph_autoencoder", "(", "self", ",", "input_text_embeddings", ")", ":", "\n", "        ", "'''\n        Based on https://github.com/erickrf/autoencoder\n        '''", "\n", "text_feature_extractor", "=", "self", ".", "text_feature_extractor", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"autoencoder\"", ")", ":", "\n", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"textual_feature_extraction\"", ")", ":", "\n", "\n", "                ", "input_text_embeddings_transformed", "=", "input_text_embeddings", "\n", "if", "self", ".", "autoencoder_noise", ">", "0", ":", "\n", "                    ", "input_text_embeddings_transformed", "=", "input_text_embeddings", "+", "tf", ".", "random_normal", "(", "shape", "=", "tf", ".", "shape", "(", "input_text_embeddings", ")", ",", "\n", "mean", "=", "0.0", ",", "stddev", "=", "self", ".", "autoencoder_noise", ")", "\n", "\n", "\n", "\n", "\n", "", "special_token_embedding", "=", "tf", ".", "constant", "(", "self", ".", "special_token_embedding_vector", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "#Reversing input feature for better performance (because usually most relevant words are in the start of the document)", "\n", "input_text_embeddings_reversed", "=", "tf", ".", "reverse", "(", "input_text_embeddings_transformed", ",", "axis", "=", "[", "1", "]", ")", "\n", "\n", "\n", "\n", "rnn_outputs_encoder", ",", "cudnn_final_state_encoder", ",", "last_layer_state_encoder", "=", "self", ".", "build_cudnn_rnn", "(", "\n", "rnn_type", "=", "text_feature_extractor", ",", "\n", "input_text_embeddings", "=", "input_text_embeddings_reversed", ",", "\n", "text_lengths", "=", "self", ".", "features", "[", "'text_length'", "]", ",", "suffix", "=", "'encoder'", ")", "\n", "\n", "\n", "final_state_compressed", "=", "tf", ".", "layers", ".", "dense", "(", "cudnn_final_state_encoder", "[", "0", "]", ",", "self", ".", "params", "[", "'acr_embeddings_size'", "]", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "tanh", ",", "\n", "#kernel_initializer=variance_scaling_initializer(),", "\n", "#kernel_regularizer=tf.contrib.layers.l2_regularizer(self.reg_weight_decay),", "\n", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'final_state_compressed.shape={}'", ".", "format", "(", "final_state_compressed", ".", "get_shape", "(", ")", ")", ")", "\n", "\n", "final_state_reconstructed", "=", "tf", ".", "layers", ".", "dense", "(", "final_state_compressed", ",", "self", ".", "params", "[", "'rnn_units'", "]", ",", "\n", "activation", "=", "tf", ".", "nn", ".", "tanh", ",", "\n", "#kernel_initializer=variance_scaling_initializer(),", "\n", "#kernel_regularizer=tf.contrib.layers.l2_regularizer(self.reg_weight_decay),", "\n", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'final_state_reconstructed.shape={}'", ".", "format", "(", "final_state_reconstructed", ".", "get_shape", "(", ")", ")", ")", "\n", "\n", "cudnn_final_state_reconstructed", "=", "(", "final_state_reconstructed", ",", ")", "\n", "\n", "\n", "#self.article_content_embedding = last_layer_state_encoder", "\n", "self", ".", "article_content_embedding", "=", "final_state_compressed", "[", "-", "1", "]", "\n", "\n", "\n", "\n", "\n", "\n", "#TODO: Prototype LSTM Autoencoder for reconstruction and prediction", "\n", "special_token_embedding_tiled", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "special_token_embedding", ",", "1", ")", ",", "[", "tf", ".", "shape", "(", "input_text_embeddings", ")", "[", "0", "]", ",", "1", ",", "1", "]", ")", "\n", "input_decoder", "=", "tf", ".", "concat", "(", "[", "special_token_embedding_tiled", ",", "input_text_embeddings", "[", ":", ",", ":", "-", "1", ",", ":", "]", "]", ",", "axis", "=", "1", ")", "\n", "\n", "rnn_outputs_decoder", ",", "cudnn_final_state_decoder", ",", "last_layer_state_decoder", "=", "self", ".", "build_cudnn_rnn", "(", "rnn_type", "=", "text_feature_extractor", ",", "\n", "input_text_embeddings", "=", "input_decoder", ",", "\n", "text_lengths", "=", "self", ".", "features", "[", "'text_length'", "]", ",", "suffix", "=", "'decoder'", ",", "\n", "cudnn_initial_state", "=", "cudnn_final_state_reconstructed", "\n", ")", "\n", "\n", "\n", "decoder_outputs_reconstructed", "=", "tf", ".", "layers", ".", "dense", "(", "rnn_outputs_decoder", ",", "self", ".", "params", "[", "'word_embedding_size'", "]", ",", "\n", "activation", "=", "None", ",", "\n", "#kernel_initializer=variance_scaling_initializer(),", "\n", "#kernel_regularizer=tf.contrib.layers.l2_regularizer(self.reg_weight_decay),", "\n", ")", "\n", "\n", "\n", "DEBUG_FIRST_N_WORDS", "=", "10", "\n", "\n", "\n", "#The matmul-based cosine similarity is much more efficient", "\n", "decoder_outputs_reconstructed_first_n_words", "=", "decoder_outputs_reconstructed", "[", ":", ",", ":", "DEBUG_FIRST_N_WORDS", "]", "\n", "word_embeddings_matrix_expanded", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "self", ".", "word_embeddings_matrix", ",", "0", ")", ",", "[", "tf", ".", "shape", "(", "decoder_outputs_reconstructed_first_n_words", ")", "[", "0", "]", ",", "1", ",", "1", "]", ")", "\n", "cosine_sim_matrix", "=", "cosine_sim_v2", "(", "decoder_outputs_reconstructed_first_n_words", ",", "\n", "word_embeddings_matrix_expanded", ")", "\n", "\n", "\n", "sorted_word_sims", ",", "sorted_word_ids", "=", "tf", ".", "nn", ".", "top_k", "(", "cosine_sim_matrix", ",", "k", "=", "5", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'sorted_word_ids={}'", ".", "format", "(", "sorted_word_ids", ".", "get_shape", "(", ")", ")", ")", "\n", "self", ".", "predicted_word_ids", "=", "sorted_word_ids", "\n", "\n", "\n", "\n", "if", "self", ".", "mode", "!=", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "\n", "#tf.logging.info('decoder_outputs_reconstructed: {}'.format(decoder_outputs_reconstructed))", "\n", "\n", "\n", "#mask = tf.cast(tf.sequence_mask(features['text_length']), tf.float32)", "\n", "                    ", "mask", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "tf", ".", "cast", "(", "tf", ".", "sign", "(", "self", ".", "features", "[", "'text'", "]", ")", ",", "tf", ".", "float32", ")", ",", "-", "1", ")", ",", "[", "1", ",", "1", ",", "tf", ".", "shape", "(", "input_text_embeddings", ")", "[", "2", "]", "]", ")", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'avg_valid_words'", ",", "family", "=", "'train'", ",", "tensor", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "reduce_sum", "(", "mask", ",", "axis", "=", "1", ")", ")", ")", "\n", "\n", "#Computes the MSE considering the mask", "\n", "autoencoder_reconstruction_loss_masked", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "(", "input_text_embeddings", "*", "mask", ")", "-", "(", "decoder_outputs_reconstructed", "*", "mask", ")", ")", ")", "/", "tf", ".", "reduce_sum", "(", "mask", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'autoencoder_reconstruction_loss_masked'", ",", "family", "=", "'train'", ",", "tensor", "=", "autoencoder_reconstruction_loss_masked", ")", "\n", "\n", "reg_loss", "=", "tf", ".", "losses", ".", "get_regularization_loss", "(", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"reg_loss\"", ",", "family", "=", "'train'", ",", "tensor", "=", "reg_loss", ")", "\n", "\n", "self", ".", "total_loss", "=", "autoencoder_reconstruction_loss_masked", "+", "reg_loss", "\n", "\n", "if", "self", ".", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "                        ", "with", "tf", ".", "variable_scope", "(", "\"training\"", ")", ":", "\n", "                                ", "self", ".", "train_op", "=", "tf", ".", "contrib", ".", "layers", ".", "optimize_loss", "(", "\n", "loss", "=", "self", ".", "total_loss", ",", "\n", "optimizer", "=", "\"Adam\"", ",", "\n", "learning_rate", "=", "self", ".", "params", "[", "'learning_rate'", "]", ",", "\n", "#learning_rate_decay_fn=lambda lr, gs: tf.train.exponential_decay(params['learning_rate'], tf.train.get_global_step(), 100, 0.96, staircase=True),", "\n", "global_step", "=", "tf", ".", "train", ".", "get_global_step", "(", ")", ",", "\n", "clip_gradients", "=", "5.0", ",", "\n", "summaries", "=", "[", "\"learning_rate\"", ",", "\"global_gradient_norm\"", "]", ")", "\n", "\n", "#with tf.variable_scope(\"stats\"):", "\n", "#nops = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])", "\n", "#tf.logging.info('Number of trainable parameters {}'.format(nops))", "\n", "\n", "\n", "\n", "", "", "self", ".", "eval_metrics", "=", "{", "}", "\n", "mse", ",", "mse_update_op", "=", "tf", ".", "metrics", ".", "mean_squared_error", "(", "input_text_embeddings", "*", "mask", ",", "decoder_outputs_reconstructed", "*", "mask", ")", "\n", "\n", "self", ".", "eval_metrics", "[", "\"mse\"", "]", "=", "(", "mse", ",", "mse_update_op", ")", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.multi_label_predictions_binarizer": [[6, 9], ["tensorflow.sigmoid", "tensorflow.cast", "tensorflow.greater"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.gru4rec.gru4rec2.GRU4Rec.sigmoid"], ["def", "multi_label_predictions_binarizer", "(", "predictions", ",", "threshold", "=", "0.5", ")", ":", "\n", "    ", "predictions", "=", "tf", ".", "sigmoid", "(", "predictions", ")", "\n", "return", "tf", ".", "cast", "(", "tf", ".", "greater", "(", "predictions", ",", "threshold", ")", ",", "tf", ".", "int64", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.state_tuples_to_cudnn_lstm_state": [[11, 16], ["tensorflow.stack", "tensorflow.stack"], "function", ["None"], ["", "def", "state_tuples_to_cudnn_lstm_state", "(", "lstm_state_tuples", ")", ":", "\n", "    ", "\"\"\"Convert LSTMStateTuples to CudnnLSTM format.\"\"\"", "\n", "c", "=", "tf", ".", "stack", "(", "[", "s", ".", "c", "for", "s", "in", "lstm_state_tuples", "]", ")", "\n", "h", "=", "tf", ".", "stack", "(", "[", "s", ".", "h", "for", "s", "in", "lstm_state_tuples", "]", ")", "\n", "return", "(", "c", ",", "h", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.cudnn_lstm_state_to_state_tuples": [[18, 24], ["tuple", "tensorflow.contrib.rnn.LSTMStateTuple", "zip", "tensorflow.unstack", "tensorflow.unstack"], "function", ["None"], ["", "def", "cudnn_lstm_state_to_state_tuples", "(", "cudnn_lstm_state", ")", ":", "\n", "    ", "\"\"\"Convert CudnnLSTM format to LSTMStateTuples.\"\"\"", "\n", "c", ",", "h", "=", "cudnn_lstm_state", "\n", "return", "tuple", "(", "\n", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "h", "=", "h_i", ",", "c", "=", "c_i", ")", "\n", "for", "c_i", ",", "h_i", "in", "zip", "(", "tf", ".", "unstack", "(", "c", ")", ",", "tf", ".", "unstack", "(", "h", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_model.cosine_sim_v2": [[26, 32], ["tensorflow.name_scope", "tensorflow.nn.l2_normalize", "tensorflow.nn.l2_normalize", "tensorflow.matmul"], "function", ["None"], ["", "def", "cosine_sim_v2", "(", "x1", ",", "x2", ",", "name", "=", "'Cosine_loss'", ")", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "\n", "        ", "x1_norm", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "x1", ",", "axis", "=", "-", "1", ")", "\n", "x2_norm", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "x2", ",", "axis", "=", "-", "1", ")", "\n", "num", "=", "tf", ".", "matmul", "(", "x1_norm", ",", "x2_norm", ",", "transpose_b", "=", "True", ")", "\n", "return", "num", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_gcom.get_session_features_config": [[69, 96], ["tensorflow.logging.info", "len"], "function", ["None"], ["def", "get_session_features_config", "(", "acr_label_encoders", ")", ":", "\n", "    ", "features_config", "=", "{", "\n", "'single_features'", ":", "\n", "{", "'article_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'publisher_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'category_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'created_at_ts'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'text_length'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "}", ",", "\n", "'sequence_features'", ":", "{", "\n", "'text'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "}", ",", "\n", "'label_features'", ":", "{", "\n", "'category_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'classification_type'", ":", "'multiclass'", "}", "\n", "}", "\n", "}", "\n", "\n", "#Adding cardinality to categorical features", "\n", "for", "feature_groups_key", "in", "features_config", ":", "\n", "        ", "features_group_config", "=", "features_config", "[", "feature_groups_key", "]", "\n", "for", "feature_name", "in", "features_group_config", ":", "\n", "            ", "if", "feature_name", "in", "acr_label_encoders", "and", "features_group_config", "[", "feature_name", "]", "[", "'type'", "]", "==", "'categorical'", ":", "\n", "                ", "features_group_config", "[", "feature_name", "]", "[", "'cardinality'", "]", "=", "len", "(", "acr_label_encoders", "[", "feature_name", "]", ".", "classes_", ")", "\n", "\n", "", "", "", "tf", ".", "logging", ".", "info", "(", "'Session Features: {}'", ".", "format", "(", "features_config", ")", ")", "\n", "\n", "return", "features_config", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_gcom.load_acr_preprocessing_assets": [[98, 106], ["utils.deserialize", "tensorflow.logging.info", "utils.deserialize", "tensorflow.logging.info", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["", "def", "load_acr_preprocessing_assets", "(", "acr_label_encoders_path", ",", "word_vocab_embeddings_path", ")", ":", "\n", "    ", "acr_label_encoders", "=", "deserialize", "(", "acr_label_encoders_path", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read article id label encoder: {}\"", ".", "format", "(", "len", "(", "acr_label_encoders", "[", "'article_id'", "]", ".", "classes_", ")", ")", ")", "\n", "\n", "(", "word_vocab", ",", "word_embeddings_matrix", ")", "=", "deserialize", "(", "word_vocab_embeddings_path", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read word embeddings: {}\"", ".", "format", "(", "word_embeddings_matrix", ".", "shape", ")", ")", "\n", "\n", "return", "acr_label_encoders", ",", "word_embeddings_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_gcom.acr_model_fn": [[108, 172], ["tensorflow.feature_column.indicator_column", "acr_model.ACR_Model", "tensorflow.estimator.EstimatorSpec", "tensorflow.feature_column.categorical_column_with_identity", "tensorflow.train.ProfilerHook"], "function", ["None"], ["", "def", "acr_model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "\n", "    ", "publisher_column", "=", "tf", ".", "feature_column", ".", "indicator_column", "(", "tf", ".", "feature_column", ".", "categorical_column_with_identity", "(", "\n", "key", "=", "'publisher_id'", ",", "\n", "num_buckets", "=", "params", "[", "'features_config'", "]", "[", "'single_features'", "]", "[", "'publisher_id'", "]", "[", "'cardinality'", "]", ")", ")", "\n", "metadata_feature_columns", "=", "[", "publisher_column", "]", "\n", "metadata_features", "=", "{", "'publisher_id'", ":", "features", "[", "'publisher_id'", "]", "}", "\n", "\n", "acr_model", "=", "ACR_Model", "(", "params", "[", "'training_task'", "]", ",", "params", "[", "'text_feature_extractor'", "]", ",", "features", ",", "metadata_features", ",", "metadata_feature_columns", ",", "\n", "labels", ",", "params", "[", "'features_config'", "]", "[", "'label_features'", "]", ",", "\n", "mode", ",", "params", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", "or", "\n", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ")", ":", "\n", "        ", "loss", "=", "acr_model", ".", "total_loss", "\n", "\n", "", "train_op", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "train_op", "=", "acr_model", ".", "train_op", "\n", "\n", "", "eval_metrics", "=", "{", "}", "\n", "if", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", "or", "\n", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ")", ":", "\n", "        ", "eval_metrics", "=", "acr_model", ".", "eval_metrics", "\n", "\n", "", "predictions", "=", "None", "\n", "prediction_hooks", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "        ", "predictions", "=", "{", "#Trained ACR embeddings", "\n", "'acr_embedding'", ":", "acr_model", ".", "article_content_embedding", ",", "\n", "\n", "#Additional metadata", "\n", "'article_id'", ":", "features", "[", "'article_id'", "]", ",", "\n", "'category_id'", ":", "features", "[", "'category_id'", "]", ",", "\n", "'publisher_id'", ":", "features", "[", "'publisher_id'", "]", ",", "\n", "'created_at_ts'", ":", "features", "[", "'created_at_ts'", "]", ",", "\n", "'text_length'", ":", "features", "[", "'text_length'", "]", "\n", "}", "\n", "\n", "if", "params", "[", "'training_task'", "]", "==", "'autoencoder'", ":", "\n", "            ", "predictions", "[", "'input_text'", "]", "=", "features", "[", "'text'", "]", "\n", "predictions", "[", "'predicted_word_ids'", "]", "=", "acr_model", ".", "predicted_word_ids", "\n", "", "elif", "params", "[", "'training_task'", "]", "==", "'metadata_classification'", ":", "\n", "#Saves predicted categories", "\n", "            ", "for", "feature_name", "in", "acr_model", ".", "labels_predictions", ":", "\n", "                ", "predictions", "[", "\"predictions-{}\"", ".", "format", "(", "feature_name", ")", "]", "=", "acr_model", ".", "labels_predictions", "[", "feature_name", "]", "\n", "\n", "", "", "", "training_hooks", "=", "[", "]", "\n", "if", "params", "[", "'enable_profiler_hook'", "]", ":", "\n", "        ", "profile_hook", "=", "tf", ".", "train", ".", "ProfilerHook", "(", "save_steps", "=", "100", ",", "\n", "save_secs", "=", "None", ",", "\n", "show_dataflow", "=", "True", ",", "\n", "show_memory", "=", "False", ")", "\n", "training_hooks", "=", "[", "profile_hook", "]", "\n", "\n", "\n", "\n", "", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "predictions", "=", "predictions", ",", "\n", "loss", "=", "loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "eval_metric_ops", "=", "eval_metrics", ",", "\n", "training_hooks", "=", "training_hooks", "\n", "#prediction_hooks=prediction_hooks,", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_gcom.build_acr_estimator": [[175, 214], ["tensorflow.ConfigProto", "tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator"], "function", ["None"], ["", "def", "build_acr_estimator", "(", "model_output_dir", ",", "word_embeddings_matrix", ",", "features_config", ",", "special_token_embedding_vector", ")", ":", "\n", "\n", "    ", "params", "=", "{", "'training_task'", ":", "FLAGS", ".", "training_task", ",", "\n", "'text_feature_extractor'", ":", "FLAGS", ".", "text_feature_extractor", ",", "\n", "'word_embeddings_matrix'", ":", "word_embeddings_matrix", ",", "\n", "'vocab_size'", ":", "word_embeddings_matrix", ".", "shape", "[", "0", "]", ",", "\n", "'word_embedding_size'", ":", "word_embeddings_matrix", ".", "shape", "[", "1", "]", ",", "\n", "'cnn_filter_sizes'", ":", "FLAGS", ".", "cnn_filter_sizes", ",", "\n", "'rnn_units'", ":", "FLAGS", ".", "rnn_units", ",", "\n", "'rnn_layers'", ":", "FLAGS", ".", "rnn_layers", ",", "\n", "'rnn_direction'", ":", "FLAGS", ".", "rnn_direction", ",", "\n", "'cnn_num_filters'", ":", "FLAGS", ".", "cnn_num_filters", ",", "\n", "'dropout_keep_prob'", ":", "FLAGS", ".", "dropout_keep_prob", ",", "\n", "'l2_reg_lambda'", ":", "FLAGS", ".", "l2_reg_lambda", ",", "\n", "'learning_rate'", ":", "FLAGS", ".", "learning_rate", ",", "\n", "'acr_embeddings_size'", ":", "FLAGS", ".", "acr_embeddings_size", ",", "\n", "'features_config'", ":", "features_config", ",", "\n", "'special_token_embedding_vector'", ":", "special_token_embedding_vector", ",", "\n", "'autoencoder_noise'", ":", "FLAGS", ".", "autoencoder_noise", ",", "\n", "'enable_profiler_hook'", ":", "False", "\n", "}", "\n", "\n", "session_config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ",", "\n", "log_device_placement", "=", "False", ",", "\n", "#device_count={'GPU': 0}", "\n", ")", "\n", "\n", "run_config", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "tf_random_seed", "=", "RANDOM_SEED", ",", "\n", "save_summary_steps", "=", "100", ",", "\n", "keep_checkpoint_max", "=", "1", ",", "\n", "session_config", "=", "session_config", "\n", ")", "\n", "\n", "acr_cnn_classifier", "=", "tf", ".", "estimator", ".", "Estimator", "(", "model_fn", "=", "acr_model_fn", ",", "\n", "model_dir", "=", "model_output_dir", ",", "\n", "params", "=", "params", ",", "\n", "config", "=", "run_config", ")", "\n", "\n", "return", "acr_cnn_classifier", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_gcom.export_acr_metadata_embeddings": [[216, 220], ["tensorflow.logging.info", "utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "export_acr_metadata_embeddings", "(", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings", ",", "output_path", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "'Exporting ACR Label Encoders, Article metadata and embeddings to {}'", ".", "format", "(", "output_path", ")", ")", "\n", "to_serialize", "=", "(", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings", ")", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_gcom.get_articles_metadata_embeddings": [[222, 246], ["pandas.DataFrame().sort_values", "numpy.vstack", "len", "cols_to_export.extend", "pandas.DataFrame", "cols_to_export.extend", "articles_metadata_df[].head", "articles_metadata_df[].tail", "col.startswith"], "function", ["None"], ["", "def", "get_articles_metadata_embeddings", "(", "article_metadata_with_pred_embeddings", ",", "ace_column", "=", "'acr_embedding'", ")", ":", "\n", "    ", "articles_metadata_df", "=", "pd", ".", "DataFrame", "(", "article_metadata_with_pred_embeddings", ")", ".", "sort_values", "(", "by", "=", "'article_id'", ")", "\n", "\n", "\n", "#Checking whether article ids are sorted and contiguous", "\n", "assert", "(", "articles_metadata_df", "[", "'article_id'", "]", ".", "head", "(", "1", ")", ".", "values", "[", "0", "]", "==", "0", ")", "\n", "assert", "(", "len", "(", "articles_metadata_df", ")", "==", "articles_metadata_df", "[", "'article_id'", "]", ".", "tail", "(", "1", ")", ".", "values", "[", "0", "]", "+", "1", ")", "\n", "\n", "content_article_embeddings", "=", "np", ".", "vstack", "(", "articles_metadata_df", "[", "ace_column", "]", ".", "values", ")", "\n", "\n", "\n", "cols_to_export", "=", "[", "'article_id'", ",", "'category_id'", ",", "'created_at_ts'", ",", "'publisher_id'", ",", "'text_length'", "]", "\n", "if", "FLAGS", ".", "training_task", "==", "'autoencoder'", ":", "\n", "        ", "cols_to_export", ".", "extend", "(", "[", "'input_text'", ",", "'predicted_word_ids'", "]", ")", "\n", "", "elif", "FLAGS", ".", "training_task", "==", "'metadata_classification'", ":", "\n", "        ", "cols_to_export", ".", "extend", "(", "[", "col", "for", "col", "in", "articles_metadata_df", ".", "columns", "if", "col", ".", "startswith", "(", "'predictions-'", ")", "]", ")", "\n", "\n", "\n", "#Filtering metadata columns to export", "\n", "", "articles_metadata_df", "=", "articles_metadata_df", "[", "cols_to_export", "]", "\n", "\n", "\n", "#return articles_metadata_df, (content_article_embeddings, predicted_word_ids)", "\n", "return", "articles_metadata_df", ",", "content_article_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_gcom.main": [[247, 351], ["json.loads", "task_data.get", "print", "time.time", "tensorflow.logging.info", "tensorflow.logging.info", "acr_trainer_gcom.load_acr_preprocessing_assets", "acr_trainer_gcom.get_session_features_config", "tensorflow.logging.info", "numpy.random.uniform", "acr_trainer_gcom.build_acr_estimator", "tensorflow.logging.info", "utils.resolve_files", "print", "build_acr_estimator.train", "tensorflow.logging.info", "list", "acr_trainer_gcom.get_articles_metadata_embeddings", "tensorflow.logging.info", "tensorflow.logging.info", "acr_trainer_gcom.export_acr_metadata_embeddings", "utils.log_elapsed_time", "os.environ.get", "json.loads.get", "len", "logging.getLogger", "os.path.join", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "build_acr_estimator.evaluate", "tensorflow.logging.info", "build_acr_estimator.predict", "tensorflow.logging.error", "len", "acr_datasets.prepare_dataset", "acr_datasets.prepare_dataset", "acr_datasets.prepare_dataset"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.load_acr_preprocessing_assets", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.get_session_features_config", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.build_acr_estimator", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.resolve_files", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.train", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.get_articles_metadata_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.export_acr_metadata_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.log_elapsed_time", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.evaluate", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.predict", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.prepare_dataset", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.prepare_dataset", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.prepare_dataset"], ["", "def", "main", "(", "unused_argv", ")", ":", "\n", "    ", "try", ":", "\n", "# Capture whether it will be a single training job or a hyper parameter tuning job.", "\n", "        ", "tf_config_env", "=", "json", ".", "loads", "(", "os", ".", "environ", ".", "get", "(", "'TF_CONFIG'", ",", "'{}'", ")", ")", "\n", "task_data", "=", "tf_config_env", ".", "get", "(", "'task'", ")", "or", "{", "'type'", ":", "'master'", ",", "'index'", ":", "0", "}", "\n", "trial", "=", "task_data", ".", "get", "(", "'trial'", ")", "\n", "\n", "running_on_mlengine", "=", "(", "len", "(", "tf_config_env", ")", ">", "0", ")", "\n", "print", "(", "'Running {}'", ".", "format", "(", "'on Google ML Engine'", "if", "running_on_mlengine", "else", "'on a server/machine'", ")", ")", "\n", "\n", "#Disabling duplicate logs on console when running locally", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "propagate", "=", "running_on_mlengine", "\n", "\n", "\n", "start_train", "=", "time", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Starting training job'", ")", "\n", "\n", "model_output_dir", "=", "FLAGS", ".", "model_dir", "\n", "\n", "if", "trial", "is", "not", "None", ":", "\n", "            ", "model_output_dir", "=", "os", ".", "path", ".", "join", "(", "model_output_dir", ",", "trial", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"Hyperparameter Tuning - Trial {}. model_dir = {}\"", ".", "format", "(", "trial", ",", "model_output_dir", ")", ")", "\n", "", "else", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "'Saving model outputs to {}'", ".", "format", "(", "model_output_dir", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Loading ACR preprocessing assets'", ")", "\n", "acr_label_encoders", ",", "word_embeddings_matrix", "=", "load_acr_preprocessing_assets", "(", "FLAGS", ".", "input_label_encoders_path", ",", "\n", "FLAGS", ".", "input_word_vocab_embeddings_path", ")", "\n", "\n", "features_config", "=", "get_session_features_config", "(", "acr_label_encoders", ")", "\n", "\n", "#input_tfrecords = os.path.join(FLAGS.data_dir, FLAGS.train_set_path_regex)", "\n", "input_tfrecords", "=", "FLAGS", ".", "train_set_path_regex", "\n", "tf", ".", "logging", ".", "info", "(", "'Defining input data (TFRecords): {}'", ".", "format", "(", "input_tfrecords", ")", ")", "\n", "\n", "\n", "#Creating an ambedding for a special token to initiate decoding of RNN-autoencoder", "\n", "special_token_embedding_vector", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "0.04", ",", "high", "=", "0.04", ",", "\n", "size", "=", "[", "1", ",", "word_embeddings_matrix", ".", "shape", "[", "1", "]", "]", ")", "\n", "\n", "acr_model", "=", "build_acr_estimator", "(", "model_output_dir", ",", "\n", "word_embeddings_matrix", ",", "\n", "features_config", ",", "\n", "special_token_embedding_vector", ")", "\n", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Training model'", ")", "\n", "train_files", "=", "resolve_files", "(", "input_tfrecords", ")", "\n", "#files_to_train = train_files", "\n", "#To test generalization", "\n", "files_to_train", "=", "train_files", "#[:-10]", "\n", "print", "(", "\"Training articles on TFRecords from {} to {}\"", ".", "format", "(", "files_to_train", "[", "0", "]", ",", "\n", "files_to_train", "[", "-", "1", "]", ")", ")", "\n", "acr_model", ".", "train", "(", "input_fn", "=", "lambda", ":", "prepare_dataset", "(", "files", "=", "files_to_train", ",", "\n", "features_config", "=", "features_config", ",", "\n", "batch_size", "=", "FLAGS", ".", "batch_size", ",", "\n", "epochs", "=", "FLAGS", ".", "training_epochs", ",", "\n", "shuffle_dataset", "=", "True", ",", "\n", "shuffle_buffer_size", "=", "5000", ",", "\n", "truncate_tokens_length", "=", "FLAGS", ".", "truncate_tokens_length", ")", ")", "\n", "\n", "\n", "if", "FLAGS", ".", "training_task", "==", "'metadata_classification'", ":", "\n", "#The objective is to overfitting this network, so that the ACR embedding represent well the articles content", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "'Evaluating model'", ")", "\n", "#Taking last train files as eval", "\n", "files_to_eval", "=", "train_files", "[", "-", "10", ":", "]", "\n", "eval_results", "=", "acr_model", ".", "evaluate", "(", "input_fn", "=", "lambda", ":", "prepare_dataset", "(", "files", "=", "files_to_eval", ",", "\n", "features_config", "=", "features_config", ",", "\n", "batch_size", "=", "FLAGS", ".", "batch_size", ",", "\n", "epochs", "=", "1", ",", "\n", "shuffle_dataset", "=", "False", ",", "\n", "truncate_tokens_length", "=", "FLAGS", ".", "truncate_tokens_length", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Evaluation results with TRAIN SET (objective is to overfit): {}'", ".", "format", "(", "eval_results", ")", ")", "\n", "\n", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Predicting ACR embeddings'", ")", "\n", "\n", "article_metadata_with_pred_embeddings", "=", "list", "(", "acr_model", ".", "predict", "(", "input_fn", "=", "lambda", ":", "prepare_dataset", "(", "files", "=", "train_files", ",", "\n", "features_config", "=", "features_config", ",", "\n", "batch_size", "=", "16", ",", "#Limits batch size to avoid OOM in the op to the predicted words for each word in the RNN auto-encoder (because of the vocabulary size is over 43k)    #FLAGS.batch_size, ", "\n", "epochs", "=", "1", ",", "\n", "shuffle_dataset", "=", "False", ",", "\n", "truncate_tokens_length", "=", "FLAGS", ".", "truncate_tokens_length", ")", ")", ")", "\n", "\n", "\n", "\n", "articles_metadata_df", ",", "content_article_embeddings", "=", "get_articles_metadata_embeddings", "(", "article_metadata_with_pred_embeddings", ",", "\n", "ace_column", "=", "'acr_embedding'", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Generated ACR embeddings (shape): {}'", ".", "format", "(", "content_article_embeddings", ".", "shape", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Generated ACR metadata (qtde): {}'", ".", "format", "(", "len", "(", "articles_metadata_df", ")", ")", ")", "\n", "\n", "output_path", "=", "FLAGS", ".", "output_acr_metadata_embeddings_path", "\n", "export_acr_metadata_embeddings", "(", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings", ",", "output_path", ")", "\n", "\n", "\n", "log_elapsed_time", "(", "start_train", ",", "'Finalized TRAINING'", ")", "\n", "\n", "", "except", "Exception", "as", "ex", ":", "\n", "        ", "tf", ".", "logging", ".", "error", "(", "'ERROR: {}'", ".", "format", "(", "ex", ")", ")", "\n", "raise", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize": [[6, 10], ["tensorflow.gfile.Open", "pickle.dump"], "function", ["None"], ["import", "pickle", "\n", "from", "time", "import", "time", "\n", "import", "hashlib", "\n", "import", "urllib", ".", "parse", "\n", "import", "re", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize": [[11, 15], ["tensorflow.gfile.Open", "pickle.load"], "function", ["None"], ["import", "unicodedata", "\n", "import", "pytz", "\n", "import", "datetime", "\n", "import", "numpy", "as", "np", "\n", "from", "sklearn", ".", "preprocessing", "import", "MinMaxScaler", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.merge_two_dicts": [[16, 23], ["None"], "function", ["None"], ["\n", "from", "ua_parser", "import", "user_agent_parser", "\n", "\n", "def", "serialize", "(", "filename", ",", "obj", ")", ":", "\n", "#with open(filename, 'wb') as handle:", "\n", "    ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "'wb'", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "obj", ",", "handle", ")", "#, protocol=pickle.HIGHEST_PROTOCOL)", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.log_elapsed_time": [[24, 27], ["tensorflow.logging.info", "time.time"], "function", ["None"], ["", "", "def", "deserialize", "(", "filename", ")", ":", "\n", "#with open(filename, 'rb') as handle:", "\n", "    ", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "'rb'", ")", "as", "handle", ":", "\n", "        ", "return", "pickle", ".", "load", "(", "handle", ")", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.chunks": [[28, 32], ["range", "len"], "function", ["None"], ["\n", "", "", "def", "merge_two_dicts", "(", "x", ",", "y", ")", ":", "\n", "#Python 2 to 3.4", "\n", "#z = x.copy()   # start with x's keys and values", "\n", "#z.update(y)    # modifies z with y's keys and values & returns None", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_tf_dtype": [[33, 43], ["Exception"], "function", ["None"], ["#return z", "\n", "#Python 3.5 or greater", "\n", "    ", "return", "{", "**", "x", ",", "**", "y", "}", "\n", "\n", "", "def", "log_elapsed_time", "(", "start_time", ",", "text", "=", "''", ")", ":", "\n", "    ", "took", "=", "(", "time", "(", ")", "-", "start_time", ")", "/", "60.0", "\n", "tf", ".", "logging", ".", "info", "(", "'==== {} elapsed {:.1f} minutes'", ".", "format", "(", "text", ",", "took", ")", ")", "\n", "\n", "\n", "", "def", "resolve_files", "(", "regex", ")", ":", "\n", "    ", "\"\"\"Return list of files given a regex\"\"\"", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.resolve_files": [[45, 55], ["tensorflow.train.match_filenames_once", "list", "tensorflow.global_variables_initializer", "tensorflow.local_variables_initializer", "tensorflow.Session", "sess.run", "sess.run", "sorted"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.Model.run"], ["init_ops", "=", "(", "tf", ".", "global_variables_initializer", "(", ")", ",", "\n", "tf", ".", "local_variables_initializer", "(", ")", ")", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "        ", "sess", ".", "run", "(", "init_ops", ")", "\n", "files", "=", "sess", ".", "run", "(", "list_op", ")", "\n", "\n", "", "return", "list", "(", "sorted", "(", "files", ")", ")", "\n", "\n", "", "def", "chunks", "(", "l", ",", "n", ")", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from l.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "l", ")", ",", "n", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_pad_token": [[56, 59], ["None"], "function", ["None"], ["        ", "yield", "l", "[", "i", ":", "i", "+", "n", "]", "\n", "\n", "\n", "", "", "def", "get_tf_dtype", "(", "dtype", ")", ":", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_unfrequent_token": [[60, 63], ["None"], "function", ["None"], ["    ", "if", "dtype", "==", "'int'", ":", "\n", "        ", "tf_dtype", "=", "tf", ".", "int64", "\n", "", "elif", "dtype", "==", "'float'", ":", "\n", "        ", "tf_dtype", "=", "tf", ".", "float32", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_categ_encoder_from_values": [[64, 74], ["encoder_values.extend", "list", "dict", "encoder_values.append", "encoder_values.append", "range", "zip", "utils.get_pad_token", "utils.get_unfrequent_token", "len"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_pad_token", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_unfrequent_token"], ["", "elif", "dtype", "==", "'string'", "or", "dtype", "==", "'bytes'", ":", "\n", "        ", "tf_dtype", "=", "tf", ".", "string", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Invalid dtype \"{}\"'", ".", "format", "(", "dtype", ")", ")", "\n", "", "return", "tf_dtype", "\n", "\n", "\n", "", "def", "get_pad_token", "(", ")", ":", "\n", "    ", "PAD_TOKEN", "=", "'<PAD>'", "\n", "return", "PAD_TOKEN", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.encode_categ_feature": [[75, 80], ["utils.get_unfrequent_token"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.get_unfrequent_token"], ["", "def", "get_unfrequent_token", "(", ")", ":", "\n", "    ", "UNFREQ_TOKEN", "=", "'<UNF>'", "\n", "return", "UNFREQ_TOKEN", "\n", "\n", "", "def", "get_categ_encoder_from_values", "(", "values", ",", "include_pad_token", "=", "True", ",", "include_unfrequent_token", "=", "False", ")", ":", "\n", "    ", "encoder_values", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.get_session_features_config": [[65, 101], ["tensorflow.logging.info", "len"], "function", ["None"], ["def", "get_session_features_config", "(", "acr_label_encoders", ")", ":", "\n", "    ", "features_config", "=", "{", "\n", "'single_features'", ":", "\n", "{", "'article_id'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'category0'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'category1'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'author'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'created_at_ts'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'text_length'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "}", ",", "\n", "'sequence_features'", ":", "{", "\n", "'text'", ":", "{", "'type'", ":", "'numerical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'keywords'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'concepts'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'entities'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'locations'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "'persons'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", "}", ",", "\n", "}", ",", "\n", "'label_features'", ":", "{", "\n", "'category0'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'classification_type'", ":", "'multiclass'", ",", "'feature_weight_on_loss'", ":", "1.0", "}", ",", "\n", "#'category1': {'type': 'categorical', 'dtype': 'int', 'classification_type': 'multiclass'}, #Too unbalanced", "\n", "'keywords'", ":", "{", "'type'", ":", "'categorical'", ",", "'dtype'", ":", "'int'", ",", "'classification_type'", ":", "'multilabel'", ",", "'feature_weight_on_loss'", ":", "1.0", "}", ",", "\n", "}", "\n", "}", "\n", "\n", "\n", "#Adding cardinality to categorical features", "\n", "for", "feature_groups_key", "in", "features_config", ":", "\n", "        ", "features_group_config", "=", "features_config", "[", "feature_groups_key", "]", "\n", "for", "feature_name", "in", "features_group_config", ":", "\n", "            ", "if", "feature_name", "in", "acr_label_encoders", "and", "features_group_config", "[", "feature_name", "]", "[", "'type'", "]", "==", "'categorical'", ":", "\n", "                ", "features_group_config", "[", "feature_name", "]", "[", "'cardinality'", "]", "=", "len", "(", "acr_label_encoders", "[", "feature_name", "]", ")", "\n", "\n", "", "", "", "tf", ".", "logging", ".", "info", "(", "'Session Features: {}'", ".", "format", "(", "features_config", ")", ")", "\n", "\n", "return", "features_config", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.load_acr_preprocessing_assets": [[103, 114], ["utils.deserialize", "tensorflow.logging.info", "tensorflow.logging.info", "utils.deserialize", "tensorflow.logging.info", "len", "labels_class_weights.keys"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.deserialize"], ["", "def", "load_acr_preprocessing_assets", "(", "acr_label_encoders_path", ",", "word_vocab_embeddings_path", ")", ":", "\n", "    ", "(", "acr_label_encoders", ",", "labels_class_weights", ")", "=", "deserialize", "(", "acr_label_encoders_path", ")", "\n", "article_id_encoder", "=", "acr_label_encoders", "[", "'article_id'", "]", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read article id label encoder: {}\"", ".", "format", "(", "len", "(", "acr_label_encoders", "[", "'article_id'", "]", ")", ")", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"Classes weights available for: {}\"", ".", "format", "(", "labels_class_weights", ".", "keys", "(", ")", ")", ")", "\n", "\n", "(", "word_vocab", ",", "word_embeddings_matrix", ")", "=", "deserialize", "(", "word_vocab_embeddings_path", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Read word embeddings: {}\"", ".", "format", "(", "word_embeddings_matrix", ".", "shape", ")", ")", "\n", "\n", "return", "acr_label_encoders", ",", "labels_class_weights", ",", "word_embeddings_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.create_multihot_feature": [[115, 119], ["tensorflow.feature_column.indicator_column", "tensorflow.feature_column.categorical_column_with_identity"], "function", ["None"], ["", "def", "create_multihot_feature", "(", "features", ",", "column_name", ",", "features_config", ")", ":", "\n", "    ", "column", "=", "tf", ".", "feature_column", ".", "indicator_column", "(", "tf", ".", "feature_column", ".", "categorical_column_with_identity", "(", "\n", "key", "=", "column_name", ",", "num_buckets", "=", "features_config", "[", "'sequence_features'", "]", "[", "column_name", "]", "[", "'cardinality'", "]", ")", ")", "\n", "return", "column", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.acr_model_fn": [[122, 207], ["acr_trainer_adressa.create_multihot_feature", "acr_trainer_adressa.create_multihot_feature", "acr_trainer_adressa.create_multihot_feature", "acr_trainer_adressa.create_multihot_feature", "acr_model.ACR_Model", "tensorflow.estimator.EstimatorSpec", "tensorflow.train.ProfilerHook"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.create_multihot_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.create_multihot_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.create_multihot_feature", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.create_multihot_feature"], ["def", "acr_model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "\n", "\n", "#keywords_column  = create_multihot_feature(features, 'keywords', params['features_config'])", "\n", "    ", "concepts_column", "=", "create_multihot_feature", "(", "features", ",", "'concepts'", ",", "params", "[", "'features_config'", "]", ")", "\n", "entities_column", "=", "create_multihot_feature", "(", "features", ",", "'entities'", ",", "params", "[", "'features_config'", "]", ")", "\n", "locations_column", "=", "create_multihot_feature", "(", "features", ",", "'locations'", ",", "params", "[", "'features_config'", "]", ")", "\n", "persons_column", "=", "create_multihot_feature", "(", "features", ",", "'persons'", ",", "params", "[", "'features_config'", "]", ")", "\n", "\n", "metadata_input_feature_columns", "=", "[", "concepts_column", ",", "entities_column", ",", "locations_column", ",", "persons_column", "]", "\n", "\n", "metadata_input_features", "=", "{", "'concepts'", ":", "features", "[", "'concepts'", "]", ",", "\n", "'entities'", ":", "features", "[", "'entities'", "]", ",", "\n", "'locations'", ":", "features", "[", "'locations'", "]", ",", "\n", "'persons'", ":", "features", "[", "'persons'", "]", "}", "\n", "\n", "\n", "\n", "acr_model", "=", "ACR_Model", "(", "params", "[", "'training_task'", "]", ",", "params", "[", "'text_feature_extractor'", "]", ",", "features", ",", "metadata_input_features", ",", "\n", "metadata_input_feature_columns", ",", "\n", "labels", ",", "params", "[", "'features_config'", "]", "[", "'label_features'", "]", ",", "\n", "mode", ",", "params", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", "or", "\n", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ")", ":", "\n", "        ", "loss", "=", "acr_model", ".", "total_loss", "\n", "\n", "", "train_op", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "        ", "train_op", "=", "acr_model", ".", "train_op", "\n", "\n", "", "eval_metrics", "=", "{", "}", "\n", "if", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", "or", "\n", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ")", ":", "\n", "        ", "eval_metrics", "=", "acr_model", ".", "eval_metrics", "\n", "\n", "", "predictions", "=", "None", "\n", "prediction_hooks", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "        ", "predictions", "=", "{", "#Category prediction", "\n", "#'predicted_category_id': acr_model.predictions,                       ", "\n", "#Trained ACR embeddings", "\n", "'acr_embedding'", ":", "acr_model", ".", "article_content_embedding", ",", "\n", "#Additional metadata", "\n", "'article_id'", ":", "features", "[", "'article_id'", "]", ",", "\n", "'category0'", ":", "features", "[", "'category0'", "]", ",", "\n", "'category1'", ":", "features", "[", "'category1'", "]", ",", "\n", "'author'", ":", "features", "[", "'author'", "]", ",", "\n", "'keywords'", ":", "features", "[", "'keywords'", "]", ",", "\n", "'concepts'", ":", "features", "[", "'concepts'", "]", ",", "\n", "'entities'", ":", "features", "[", "'entities'", "]", ",", "\n", "'locations'", ":", "features", "[", "'locations'", "]", ",", "\n", "'persons'", ":", "features", "[", "'persons'", "]", ",", "\n", "'created_at_ts'", ":", "features", "[", "'created_at_ts'", "]", ",", "\n", "'text_length'", ":", "features", "[", "'text_length'", "]", ",", "\n", "'input_text'", ":", "features", "[", "'text'", "]", "\n", "}", "\n", "\n", "\n", "if", "params", "[", "'training_task'", "]", "==", "'autoencoder'", ":", "\n", "#predictions['input_text'] = features['text']", "\n", "            ", "predictions", "[", "'predicted_word_ids'", "]", "=", "acr_model", ".", "predicted_word_ids", "\n", "", "elif", "params", "[", "'training_task'", "]", "==", "'metadata_classification'", ":", "\n", "#Saves predicted categories", "\n", "            ", "for", "feature_name", "in", "acr_model", ".", "labels_predictions", ":", "\n", "                ", "predictions", "[", "\"{}{}\"", ".", "format", "(", "PREDICTIONS_PREFIX", ",", "feature_name", ")", "]", "=", "acr_model", ".", "labels_predictions", "[", "feature_name", "]", "\n", "\n", "#prediction_hooks = [ACREmbeddingExtractorHook(mode, acr_model)]  ", "\n", "\n", "", "", "", "training_hooks", "=", "[", "]", "\n", "if", "params", "[", "'enable_profiler_hook'", "]", ":", "\n", "        ", "profile_hook", "=", "tf", ".", "train", ".", "ProfilerHook", "(", "save_steps", "=", "100", ",", "\n", "save_secs", "=", "None", ",", "\n", "show_dataflow", "=", "True", ",", "\n", "show_memory", "=", "False", ")", "\n", "training_hooks", "=", "[", "profile_hook", "]", "\n", "\n", "\n", "", "return", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "predictions", "=", "predictions", ",", "\n", "loss", "=", "loss", ",", "\n", "train_op", "=", "train_op", ",", "\n", "eval_metric_ops", "=", "eval_metrics", ",", "\n", "training_hooks", "=", "training_hooks", "\n", "#prediction_hooks=prediction_hooks,", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.build_acr_estimator": [[210, 248], ["tensorflow.ConfigProto", "tensorflow.estimator.RunConfig", "tensorflow.estimator.Estimator"], "function", ["None"], ["", "def", "build_acr_estimator", "(", "model_output_dir", ",", "word_embeddings_matrix", ",", "features_config", ",", "labels_class_weights", ",", "special_token_embedding_vector", ")", ":", "\n", "\n", "\n", "    ", "params", "=", "{", "'training_task'", ":", "FLAGS", ".", "training_task", ",", "\n", "'text_feature_extractor'", ":", "FLAGS", ".", "text_feature_extractor", ",", "\n", "'word_embeddings_matrix'", ":", "word_embeddings_matrix", ",", "\n", "'vocab_size'", ":", "word_embeddings_matrix", ".", "shape", "[", "0", "]", ",", "\n", "'word_embedding_size'", ":", "word_embeddings_matrix", ".", "shape", "[", "1", "]", ",", "\n", "'cnn_filter_sizes'", ":", "FLAGS", ".", "cnn_filter_sizes", ",", "\n", "'cnn_num_filters'", ":", "FLAGS", ".", "cnn_num_filters", ",", "\n", "'rnn_units'", ":", "FLAGS", ".", "rnn_units", ",", "\n", "'rnn_layers'", ":", "FLAGS", ".", "rnn_layers", ",", "\n", "'rnn_direction'", ":", "FLAGS", ".", "rnn_direction", ",", "\n", "'dropout_keep_prob'", ":", "FLAGS", ".", "dropout_keep_prob", ",", "\n", "'l2_reg_lambda'", ":", "FLAGS", ".", "l2_reg_lambda", ",", "\n", "'learning_rate'", ":", "FLAGS", ".", "learning_rate", ",", "\n", "'acr_embeddings_size'", ":", "FLAGS", ".", "acr_embeddings_size", ",", "\n", "'features_config'", ":", "features_config", ",", "\n", "'labels_class_weights'", ":", "labels_class_weights", ",", "\n", "'special_token_embedding_vector'", ":", "special_token_embedding_vector", ",", "\n", "'autoencoder_noise'", ":", "FLAGS", ".", "autoencoder_noise", ",", "\n", "'enable_profiler_hook'", ":", "False", "\n", "}", "\n", "\n", "session_config", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ")", "\n", "\n", "run_config", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "tf_random_seed", "=", "RANDOM_SEED", ",", "\n", "save_summary_steps", "=", "50", ",", "\n", "keep_checkpoint_max", "=", "1", ",", "\n", "session_config", "=", "session_config", "\n", ")", "\n", "\n", "acr_cnn_classifier", "=", "tf", ".", "estimator", ".", "Estimator", "(", "model_fn", "=", "acr_model_fn", ",", "\n", "model_dir", "=", "model_output_dir", ",", "\n", "params", "=", "params", ",", "\n", "config", "=", "run_config", ")", "\n", "\n", "return", "acr_cnn_classifier", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.export_acr_metadata_embeddings": [[250, 255], ["tensorflow.logging.info", "utils.serialize"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.serialize"], ["", "def", "export_acr_metadata_embeddings", "(", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings", ")", ":", "\n", "    ", "output_path", "=", "FLAGS", ".", "output_acr_metadata_embeddings_path", "\n", "tf", ".", "logging", ".", "info", "(", "'Exporting ACR Label Encoders, Article metadata and embeddings to {}'", ".", "format", "(", "output_path", ")", ")", "\n", "to_serialize", "=", "(", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings", ")", "\n", "serialize", "(", "output_path", ",", "to_serialize", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.get_articles_metadata_embeddings": [[257, 298], ["pandas.DataFrame().sort_values", "tensorflow.logging.info", "tensorflow.logging.info", "numpy.vstack", "numpy.mean", "numpy.vstack", "len", "articles_metadata_df[].apply", "cols_to_export.extend", "pandas.DataFrame", "cols_to_export.extend", "articles_metadata_df[].head", "articles_metadata_df[].tail", "articles_metadata_df[].head", "articles_metadata_df[].tail", "articles_metadata_df[].tail", "x.nonzero", "col.startswith"], "function", ["None"], ["", "def", "get_articles_metadata_embeddings", "(", "article_metadata_with_pred_embeddings", ")", ":", "\n", "    ", "articles_metadata_df", "=", "pd", ".", "DataFrame", "(", "article_metadata_with_pred_embeddings", ")", ".", "sort_values", "(", "by", "=", "'article_id'", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"First article id: {}\"", ".", "format", "(", "articles_metadata_df", "[", "'article_id'", "]", ".", "head", "(", "1", ")", ".", "values", "[", "0", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"Last article id: {}\"", ".", "format", "(", "articles_metadata_df", "[", "'article_id'", "]", ".", "tail", "(", "1", ")", ".", "values", "[", "0", "]", ")", ")", "\n", "\n", "#Checking whether article ids are sorted and contiguous", "\n", "assert", "(", "articles_metadata_df", "[", "'article_id'", "]", ".", "head", "(", "1", ")", ".", "values", "[", "0", "]", "==", "1", ")", "#0 is reserved for padding", "\n", "assert", "(", "len", "(", "articles_metadata_df", ")", "==", "articles_metadata_df", "[", "'article_id'", "]", ".", "tail", "(", "1", ")", ".", "values", "[", "0", "]", ")", "\n", "\n", "content_article_embeddings", "=", "np", ".", "vstack", "(", "articles_metadata_df", "[", "'acr_embedding'", "]", ".", "values", ")", "\n", "\n", "\n", "#Creating and embedding for the padding article    ", "\n", "embedding_for_padding_article", "=", "np", ".", "mean", "(", "content_article_embeddings", ",", "axis", "=", "0", ")", "\n", "content_article_embeddings_with_padding", "=", "np", ".", "vstack", "(", "[", "embedding_for_padding_article", ",", "content_article_embeddings", "]", ")", "\n", "\n", "#Checking if content articles embedding size correspond to the last article_id", "\n", "assert", "content_article_embeddings_with_padding", ".", "shape", "[", "0", "]", "==", "articles_metadata_df", "[", "'article_id'", "]", ".", "tail", "(", "1", ")", ".", "values", "[", "0", "]", "+", "1", "\n", "\n", "#Converting keywords multi-label feature from multi-hot representation back to list of keyword ids", "\n", "preds_keywords_column_name", "=", "\"{}{}\"", ".", "format", "(", "PREDICTIONS_PREFIX", ",", "\"keywords\"", ")", "\n", "if", "preds_keywords_column_name", "in", "articles_metadata_df", ".", "columns", ":", "\n", "        ", "articles_metadata_df", "[", "preds_keywords_column_name", "]", "=", "articles_metadata_df", "[", "preds_keywords_column_name", "]", ".", "apply", "(", "lambda", "x", ":", "x", ".", "nonzero", "(", ")", "[", "0", "]", ")", "\n", "\n", "\n", "", "cols_to_export", "=", "[", "'article_id'", ",", "'category0'", ",", "'category1'", ",", "\n", "'author'", ",", "'keywords'", ",", "'concepts'", ",", "'entities'", ",", "'locations'", ",", "'persons'", ",", "\n", "'created_at_ts'", ",", "'text_length'", ",", "'input_text'", "]", "\n", "if", "FLAGS", ".", "training_task", "==", "'autoencoder'", ":", "\n", "        ", "cols_to_export", ".", "extend", "(", "[", "'predicted_word_ids'", "]", ")", "\n", "", "elif", "FLAGS", ".", "training_task", "==", "'metadata_classification'", ":", "\n", "#Adding predictions columns for debug", "\n", "        ", "cols_to_export", ".", "extend", "(", "[", "col", "for", "col", "in", "articles_metadata_df", ".", "columns", "if", "col", ".", "startswith", "(", "PREDICTIONS_PREFIX", ")", "]", ")", "\n", "\n", "\n", "#Filtering metadata columns to export", "\n", "", "articles_metadata_df", "=", "articles_metadata_df", "[", "cols_to_export", "]", "\n", "\n", "return", "articles_metadata_df", ",", "content_article_embeddings_with_padding", "\n", "\n"]], "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.main": [[299, 407], ["json.loads", "task_data.get", "tensorflow.logging.info", "time.time", "tensorflow.logging.info", "tensorflow.logging.info", "acr_trainer_adressa.load_acr_preprocessing_assets", "acr_trainer_adressa.get_session_features_config", "tensorflow.logging.info", "numpy.random.uniform", "acr_trainer_adressa.build_acr_estimator", "tensorflow.logging.info", "utils.resolve_files", "tensorflow.logging.info", "build_acr_estimator.train", "tensorflow.logging.info", "build_acr_estimator.evaluate", "tensorflow.logging.info", "tensorflow.logging.info", "build_acr_estimator.predict", "acr_trainer_adressa.get_articles_metadata_embeddings", "tensorflow.logging.info", "acr_trainer_adressa.export_acr_metadata_embeddings", "utils.log_elapsed_time", "os.environ.get", "json.loads.get", "len", "logging.getLogger", "os.path.join", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.error", "acr_datasets.prepare_dataset", "acr_datasets.prepare_dataset", "acr_datasets.prepare_dataset"], "function", ["home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.load_acr_preprocessing_assets", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.get_session_features_config", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.build_acr_estimator", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.resolve_files", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.train", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.sr-gnn.gnn_ml_fast.GGNN.evaluate", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.benchmarks.benchmarks.BenchmarkRecommender.predict", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.get_articles_metadata_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_trainer_adressa.export_acr_metadata_embeddings", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.utils.log_elapsed_time", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.prepare_dataset", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.prepare_dataset", "home.repos.pwc.inspect_result.gabrielspmoreira_chameleon_recsys.acr.acr_datasets.prepare_dataset"], ["", "def", "main", "(", "unused_argv", ")", ":", "\n", "    ", "try", ":", "\n", "# Capture whether it will be a single training job or a hyper parameter tuning job.", "\n", "        ", "tf_config_env", "=", "json", ".", "loads", "(", "os", ".", "environ", ".", "get", "(", "'TF_CONFIG'", ",", "'{}'", ")", ")", "\n", "task_data", "=", "tf_config_env", ".", "get", "(", "'task'", ")", "or", "{", "'type'", ":", "'master'", ",", "'index'", ":", "0", "}", "\n", "trial", "=", "task_data", ".", "get", "(", "'trial'", ")", "\n", "\n", "running_on_mlengine", "=", "(", "len", "(", "tf_config_env", ")", ">", "0", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Running {}'", ".", "format", "(", "'on Google ML Engine'", "if", "running_on_mlengine", "else", "'on a server/machine'", ")", ")", "\n", "\n", "#Disabling duplicate logs on console when running locally", "\n", "logging", ".", "getLogger", "(", "'tensorflow'", ")", ".", "propagate", "=", "running_on_mlengine", "\n", "\n", "\n", "start_train", "=", "time", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Starting training job'", ")", "\n", "\n", "model_output_dir", "=", "FLAGS", ".", "model_dir", "\n", "\n", "if", "trial", "is", "not", "None", ":", "\n", "            ", "model_output_dir", "=", "os", ".", "path", ".", "join", "(", "model_output_dir", ",", "trial", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\n", "\"Hyperparameter Tuning - Trial {}. model_dir = {}\"", ".", "format", "(", "trial", ",", "model_output_dir", ")", ")", "\n", "", "else", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "'Saving model outputs to {}'", ".", "format", "(", "model_output_dir", ")", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "'Loading ACR preprocessing assets'", ")", "\n", "acr_label_encoders", ",", "labels_class_weights", ",", "word_embeddings_matrix", "=", "load_acr_preprocessing_assets", "(", "FLAGS", ".", "input_label_encoders_path", ",", "\n", "FLAGS", ".", "input_word_vocab_embeddings_path", ")", "\n", "\n", "features_config", "=", "get_session_features_config", "(", "acr_label_encoders", ")", "\n", "\n", "#input_tfrecords = os.path.join(FLAGS.data_dir, FLAGS.train_set_path_regex)", "\n", "input_tfrecords", "=", "FLAGS", ".", "train_set_path_regex", "\n", "tf", ".", "logging", ".", "info", "(", "'Defining input data (TFRecords): {}'", ".", "format", "(", "input_tfrecords", ")", ")", "\n", "\n", "\n", "#Creating an ambedding for a special token to initiate decoding of RNN-autoencoder", "\n", "special_token_embedding_vector", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "-", "0.04", ",", "high", "=", "0.04", ",", "\n", "size", "=", "[", "1", ",", "word_embeddings_matrix", ".", "shape", "[", "1", "]", "]", ")", "\n", "\n", "acr_model", "=", "build_acr_estimator", "(", "model_output_dir", ",", "\n", "word_embeddings_matrix", ",", "\n", "features_config", ",", "\n", "labels_class_weights", ",", "\n", "special_token_embedding_vector", ")", "\n", "\n", "\n", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Training model'", ")", "\n", "train_files", "=", "resolve_files", "(", "input_tfrecords", ")", "\n", "\n", "'''\n        shuffle(train_files)\n        test_files = train_files[-7:]\n        train_files = train_files[:-7]\n        '''", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"Training articles on TFRecords from {} to {}\"", ".", "format", "(", "train_files", "[", "0", "]", ",", "\n", "train_files", "[", "-", "1", "]", ")", ")", "\n", "acr_model", ".", "train", "(", "input_fn", "=", "lambda", ":", "prepare_dataset", "(", "files", "=", "train_files", ",", "\n", "features_config", "=", "features_config", ",", "\n", "batch_size", "=", "FLAGS", ".", "batch_size", ",", "\n", "epochs", "=", "FLAGS", ".", "training_epochs", ",", "\n", "shuffle_dataset", "=", "True", ",", "\n", "shuffle_buffer_size", "=", "10000", ")", ")", "\n", "\n", "#The objective is to overfitting this network, so that the ACR embedding represent well the articles content", "\n", "tf", ".", "logging", ".", "info", "(", "'Evaluating model - TRAIN SET'", ")", "\n", "eval_results", "=", "acr_model", ".", "evaluate", "(", "input_fn", "=", "lambda", ":", "prepare_dataset", "(", "files", "=", "train_files", ",", "\n", "features_config", "=", "features_config", ",", "\n", "batch_size", "=", "FLAGS", ".", "batch_size", ",", "\n", "epochs", "=", "1", ",", "\n", "shuffle_dataset", "=", "False", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Evaluation results with TRAIN SET (objective is to overfit): {}'", ".", "format", "(", "eval_results", ")", ")", "\n", "\n", "'''\n        tf.logging.info('Evaluating model - TEST SET')\n        eval_results = acr_model.evaluate(input_fn=lambda: prepare_dataset(files=test_files,\n                                                    features_config=features_config,\n                                                    batch_size=FLAGS.batch_size, \n                                                    epochs=1, \n                                                    shuffle_dataset=False))\n        tf.logging.info('Evaluation results with TEST SET: {}'.format(eval_results))\n        '''", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "'Predicting ACR embeddings'", ")", "\n", "\n", "article_metadata_with_pred_embeddings", "=", "acr_model", ".", "predict", "(", "input_fn", "=", "lambda", ":", "prepare_dataset", "(", "files", "=", "train_files", ",", "\n", "features_config", "=", "features_config", ",", "\n", "batch_size", "=", "FLAGS", ".", "batch_size", ",", "\n", "epochs", "=", "1", ",", "\n", "shuffle_dataset", "=", "False", ")", ")", "\n", "\n", "\n", "articles_metadata_df", ",", "content_article_embeddings", "=", "get_articles_metadata_embeddings", "(", "article_metadata_with_pred_embeddings", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Generated ACR embeddings: {}'", ".", "format", "(", "content_article_embeddings", ".", "shape", ")", ")", "\n", "\n", "export_acr_metadata_embeddings", "(", "acr_label_encoders", ",", "articles_metadata_df", ",", "content_article_embeddings", ")", "\n", "\n", "\n", "log_elapsed_time", "(", "start_train", ",", "'Finalized TRAINING'", ")", "\n", "\n", "", "except", "Exception", "as", "ex", ":", "\n", "        ", "tf", ".", "logging", ".", "error", "(", "'ERROR: {}'", ".", "format", "(", "ex", ")", ")", "\n", "raise", "\n", "\n"]]}