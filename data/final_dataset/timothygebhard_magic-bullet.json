{"home.repos.pwc.inspect_result.timothygebhard_magic-bullet.None.train_model.get_arguments": [[27, 85], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "get_arguments", "(", ")", "->", "argparse", ".", "Namespace", ":", "\n", "    ", "\"\"\"\n    Set up an ArgumentParser to get the command line arguments.\n\n    Returns:\n        A Namespace object containing all the command line arguments\n        for the script.\n    \"\"\"", "\n", "\n", "# Set up parser", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Add arguments", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Size of the mini-batches during training. '", "\n", "'Default: 64.'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Total number of training epochs. Default: 64.'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning-rate'", ",", "\n", "default", "=", "3e-4", ",", "\n", "type", "=", "float", ",", "\n", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'Initial learning rate. Default: 3e-4.'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Logging interval during training. Default: 32.'", ")", "\n", "parser", ".", "add_argument", "(", "'--resume'", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'Path to checkpoint to be used when resuming '", "\n", "'training. Default: None.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tensorboard'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "default", "=", "True", ",", "\n", "help", "=", "'Use TensorBoard to log training progress? '", "\n", "'Default: True.'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-cuda'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "default", "=", "True", ",", "\n", "help", "=", "'Train on GPU, if available? Default: True.'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "\n", "default", "=", "4", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Number of workers for DataLoaders. Default: 4.'", ")", "\n", "\n", "# Parse and return the arguments (as a Namespace object)", "\n", "arguments", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "arguments", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.None.train_model.train": [[87, 180], ["model.train", "utils.training.AverageMeter", "utils.training.AverageMeter", "enumerate", "time.time", "target.squeeze.squeeze", "optimizer.zero_grad", "model.forward().squeeze", "loss_func", "loss_func.backward", "optimizer.step", "utils.training.AverageMeter.update", "utils.training.AverageMeter.update", "data.to", "target.squeeze.to", "args.logger.add_scalar", "loss_func.item", "print", "print", "print", "print", "print", "model.forward", "time.time", "loss_func.item", "loss_func.item"], "function", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.None.train_model.train", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.step", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.training.AverageMeter.update", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.training.AverageMeter.update", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.models.FCNN.forward"], ["", "def", "train", "(", "dataloader", ":", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "\n", "model", ":", "torch", ".", "nn", ".", "Module", ",", "\n", "loss_func", ":", "Any", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "epoch", ":", "int", ",", "\n", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "\"\"\"\n    Train the given model for a single epoch using the given dataloader.\n\n    Args:\n        dataloader: The dataloader containing the training data.\n        model: Instance of the model that is being trained.\n        loss_func: A loss function to compute the error between the\n            actual and the desired output of the model.\n        optimizer: An instance of an optimizer that is used to compute\n            and perform the updates to the weights of the network.\n        epoch: The current training epoch.\n        args: Namespace object containing some global variable (e.g.,\n            command line arguments, such as the batch size)\n    \"\"\"", "\n", "\n", "# -------------------------------------------------------------------------", "\n", "# Preliminaries", "\n", "# -------------------------------------------------------------------------", "\n", "\n", "# Activate training mode", "\n", "model", ".", "train", "(", ")", "\n", "\n", "# Keep track the time to process a batch, as well as the batch losses", "\n", "batch_times", "=", "AverageMeter", "(", ")", "\n", "batch_losses", "=", "AverageMeter", "(", ")", "\n", "\n", "# -------------------------------------------------------------------------", "\n", "# Process the training dataset in mini-batches", "\n", "# -------------------------------------------------------------------------", "\n", "\n", "for", "batch_idx", ",", "(", "data", ",", "target", ")", "in", "enumerate", "(", "dataloader", ")", ":", "\n", "\n", "# Initialize start time of the batch", "\n", "        ", "batch_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Fetch data and move to device", "\n", "data", ",", "target", "=", "data", ".", "to", "(", "args", ".", "device", ")", ",", "target", ".", "to", "(", "args", ".", "device", ")", "\n", "target", "=", "target", ".", "squeeze", "(", ")", "\n", "\n", "# Clear gradients", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# Compute forward pass through model", "\n", "output", "=", "model", ".", "forward", "(", "data", ")", ".", "squeeze", "(", ")", "\n", "\n", "# Calculate the loss for the batch", "\n", "loss", "=", "loss_func", "(", "output", ",", "target", ")", "\n", "\n", "# Back-propagate the loss and update the weights", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", "closure", "=", "None", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Log information about current batch to TensorBoard", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "if", "args", ".", "tensorboard", ":", "\n", "\n", "# Compute how many examples we have processed already and log the", "\n", "# loss value for the current batch", "\n", "            ", "global_step", "=", "(", "(", "epoch", "-", "1", ")", "*", "args", ".", "n_train_batches", "+", "batch_idx", ")", "*", "args", ".", "batch_size", "\n", "args", ".", "logger", ".", "add_scalar", "(", "tag", "=", "'loss/train'", ",", "\n", "scalar_value", "=", "loss", ".", "item", "(", ")", ",", "\n", "global_step", "=", "global_step", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Additional logging to console", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "# Store the loss and processing time for the current batch", "\n", "", "batch_losses", ".", "update", "(", "loss", ".", "item", "(", ")", ")", "\n", "batch_times", ".", "update", "(", "time", ".", "time", "(", ")", "-", "batch_start", ")", "\n", "\n", "# Print information to console, if applicable", "\n", "if", "batch_idx", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "\n", "# Which fraction of batches have we already processed this epoch?", "\n", "            ", "percent", "=", "100.", "*", "batch_idx", "/", "args", ".", "n_train_batches", "\n", "\n", "# Print some information about how the training is going", "\n", "print", "(", "f'Epoch: {epoch:>3}/{args.epochs}'", ",", "end", "=", "' | '", ",", "flush", "=", "True", ")", "\n", "print", "(", "f'Batch: {batch_idx:>3}/{args.n_train_batches}'", ",", "\n", "flush", "=", "True", ",", "end", "=", "' '", ")", "\n", "print", "(", "f'({percent:>4.1f}%)'", ",", "end", "=", "' | '", ",", "flush", "=", "True", ")", "\n", "print", "(", "f'Loss: {loss.item():.6f}'", ",", "end", "=", "' | '", ",", "flush", "=", "True", ")", "\n", "print", "(", "f'Time: {batch_times.value:>6.3f}s'", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.None.train_model.validate": [[182, 270], ["model.eval", "numpy.prod", "print", "torch.no_grad", "args.logger.add_scalar", "target.squeeze.squeeze", "model.forward().squeeze", "loss_func().item", "data.to", "target.squeeze.to", "model.forward", "loss_func"], "function", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.models.FCNN.forward"], ["", "", "", "def", "validate", "(", "dataloader", ":", "torch", ".", "utils", ".", "data", ".", "DataLoader", ",", "\n", "model", ":", "torch", ".", "nn", ".", "Module", ",", "\n", "loss_func", ":", "Any", ",", "\n", "epoch", ":", "int", ",", "\n", "args", ":", "argparse", ".", "Namespace", ")", "->", "float", ":", "\n", "    ", "\"\"\"\n    At the end of each epoch, run the model on the validation dataset.\n\n    Args:\n        dataloader: The dataloader containing the validation data.\n        model: Instance of the model that is being trained.\n        loss_func: A loss function to compute the error between the\n            actual and the desired output of the model.\n        epoch: The current training epoch.\n        args: Namespace object containing some global variable (e.g.,\n            command line arguments, such as the batch size).\n\n    Returns:\n        The average loss on the validation dataset.\n    \"\"\"", "\n", "\n", "# -------------------------------------------------------------------------", "\n", "# Preliminaries", "\n", "# -------------------------------------------------------------------------", "\n", "\n", "# Activate model evaluation mode", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# Initialize validation loss as 0, because we need to sum it up over all", "\n", "# mini batches for the validation dataset", "\n", "validation_loss", "=", "0", "\n", "\n", "# Ensure the loss function uses 'sum' as the reduction method (we don't", "\n", "# want batch averages, but just one global validation average)", "\n", "reduction", "=", "loss_func", ".", "reduction", "\n", "loss_func", ".", "reduction", "=", "'sum'", "\n", "\n", "# -------------------------------------------------------------------------", "\n", "# Process the validation dataset in mini-batches", "\n", "# -------------------------------------------------------------------------", "\n", "\n", "# At test time, we do not need to compute gradients", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "# Loop in mini batches over the validation dataset", "\n", "        ", "for", "data", ",", "target", "in", "dataloader", ":", "\n", "\n", "# Fetch batch data and move to device", "\n", "            ", "data", ",", "target", "=", "data", ".", "to", "(", "args", ".", "device", ")", ",", "target", ".", "to", "(", "args", ".", "device", ")", "\n", "target", "=", "target", ".", "squeeze", "(", ")", "\n", "\n", "# Compute the forward pass through the model", "\n", "output", "=", "model", ".", "forward", "(", "data", ")", ".", "squeeze", "(", ")", "\n", "\n", "# Compute the loss for the batch", "\n", "validation_loss", "+=", "loss_func", "(", "output", ",", "target", ")", ".", "item", "(", ")", "\n", "\n", "# -------------------------------------------------------------------------", "\n", "# Compute the average validation loss", "\n", "# -------------------------------------------------------------------------", "\n", "\n", "", "", "validation_loss", "/=", "np", ".", "prod", "(", "dataloader", ".", "dataset", ".", "labels", ".", "shape", ")", "\n", "print", "(", "f'\\nAverage loss on validation set:\\t {validation_loss:.5f}\\n'", ")", "\n", "\n", "# -------------------------------------------------------------------------", "\n", "# Log stuff to TensorBoard", "\n", "# -------------------------------------------------------------------------", "\n", "\n", "if", "args", ".", "tensorboard", ":", "\n", "\n", "# Compute the current global_step (i.e., the total number examples", "\n", "# that we've seen during training so far)", "\n", "        ", "global_step", "=", "epoch", "*", "args", ".", "n_train_batches", "*", "args", ".", "batch_size", "\n", "\n", "# Log the validation loss", "\n", "args", ".", "logger", ".", "add_scalar", "(", "tag", "=", "'loss/validation'", ",", "\n", "scalar_value", "=", "validation_loss", ",", "\n", "global_step", "=", "global_step", ")", "\n", "\n", "# -------------------------------------------------------------------------", "\n", "# Postliminaries", "\n", "# -------------------------------------------------------------------------", "\n", "\n", "# Finally, restore the original reduction method of the loss function", "\n", "", "loss_func", ".", "reduction", "=", "reduction", "\n", "\n", "# Return the validation loss", "\n", "return", "validation_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.None.apply_model.get_arguments": [[29, 77], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "get_arguments", "(", ")", "->", "argparse", ".", "Namespace", ":", "\n", "    ", "\"\"\"\n    Set up an ArgumentParser to get the command line arguments.\n\n    Returns:\n        A Namespace object containing all the command line arguments\n        for the script.\n    \"\"\"", "\n", "\n", "# Set up parser", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Add arguments", "\n", "parser", ".", "add_argument", "(", "'--apply-to'", ",", "\n", "default", "=", "'testing'", ",", "\n", "type", "=", "str", ",", "\n", "metavar", "=", "'NAME'", ",", "\n", "choices", "=", "[", "'testing'", ",", "'real_events'", "]", ",", "\n", "help", "=", "'Dataset to which the trained model should be '", "\n", "'applied: either \"testing\" or \"real_events\". '", "\n", "'Default: testing.'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Size of the mini-batches in which the data set '", "\n", "'is split for applying the trained network. '", "\n", "'Default: 128.'", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint'", ",", "\n", "default", "=", "'./checkpoints/best.pth'", ",", "\n", "type", "=", "str", ",", "\n", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'Path to the checkpoint file from which to load '", "\n", "'the model. Default: ./checkpoints/best.pth.'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Number of workers for the PyTorch DataLoader. '", "\n", "'Default: 8'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-cuda'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "default", "=", "True", ",", "\n", "help", "=", "'Use GPU, if available? Default: True.'", ")", "\n", "\n", "# Parse and return the arguments (as a Namespace object)", "\n", "arguments", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "arguments", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.None.find_preimage.get_arguments": [[27, 63], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "get_arguments", "(", ")", "->", "argparse", ".", "Namespace", ":", "\n", "    ", "\"\"\"\n    Set up an ArgumentParser to get the command line arguments.\n\n    Returns:\n        A Namespace object containing all the command line arguments\n        for the script.\n    \"\"\"", "\n", "\n", "# Set up parser", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Find pre-image.'", ")", "\n", "\n", "# Add arguments", "\n", "parser", ".", "add_argument", "(", "'--constraint'", ",", "\n", "type", "=", "str", ",", "\n", "metavar", "=", "'NAME'", ",", "\n", "default", "=", "'gw_like'", ",", "\n", "help", "=", "'Type of constraint to enforce on the inputs'", "\n", "'during the optimization. '", "\n", "'Default: gw_like.'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "\n", "default", "=", "256", ",", "\n", "help", "=", "'Number of epochs for the optimization. '", "\n", "'Default: 256.'", ")", "\n", "parser", ".", "add_argument", "(", "'--index'", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "\n", "default", "=", "0", ",", "\n", "help", "=", "'Index of the sample in the noise-only part of'", "\n", "'the testing dataset. Default: 0.'", ")", "\n", "\n", "# Parse and return the arguments (as a Namespace object)", "\n", "arguments", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "arguments", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.None.find_preimage.smooth_weights": [[65, 101], ["torch.Conv1d", "torch.init.constant_", "torch.init.constant_", "nn.Conv1d.forward", "len", "weights.unsqueeze.unsqueeze", "int"], "function", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.models.FCNN.forward"], ["", "def", "smooth_weights", "(", "weights", ":", "torch", ".", "Tensor", ",", "\n", "kernel_size", ":", "int", "=", "15", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Smooth a 1D weights tensor by passing it through a 1D convolutional\n    layer with a fixed kernel of a given `kernel_size`.\n    \n    Args:\n        weights: Weights tensor to be smoothed.\n        kernel_size: Size of the kernel / rectangular window to be\n            used for smoothing.\n\n    Returns:\n        A smoothed version of the original `weights`.\n    \"\"\"", "\n", "\n", "# Ensure window_size is an odd number", "\n", "if", "kernel_size", "%", "2", "==", "0", ":", "\n", "        ", "kernel_size", "-=", "1", "\n", "\n", "# Ensure weights has the right shape, that is, add a fake", "\n", "# batch and channel dimension if necessary", "\n", "", "while", "len", "(", "weights", ".", "shape", ")", "<", "3", ":", "\n", "        ", "weights", "=", "weights", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Define convolutional layer", "\n", "", "layer", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "1", ",", "\n", "out_channels", "=", "1", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "int", "(", "kernel_size", "/", "2", ")", ")", "\n", "\n", "# Fix values of the kernel", "\n", "nn", ".", "init", ".", "constant_", "(", "layer", ".", "weight", ",", "1.0", "/", "kernel_size", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "layer", ".", "bias", ",", "0", ")", "\n", "\n", "# Apply convolution to weights tensor to smooth it", "\n", "return", "layer", ".", "forward", "(", "weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.models.FCNN.__init__": [[20, 79], ["torch.Module.__init__", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ModuleList", "torch.ModuleList", "range", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.Conv1d", "torch.Conv1d", "models.FCNN.convolutional_layers.append", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.datasets.RealEventDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "n_channels", ":", "int", "=", "512", ",", "\n", "n_convolutional_layers", ":", "int", "=", "12", ")", ":", "\n", "\n", "        ", "super", "(", "FCNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Store constructor arguments", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "self", ".", "n_channels", "=", "n_channels", "\n", "self", ".", "n_convolutional_layers", "=", "n_convolutional_layers", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Define the model's layers", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "# Define the input layer of the net that maps the number of channels", "\n", "# from 2 -> n_channels", "\n", "self", ".", "input_layer", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "2", ",", "\n", "out_channels", "=", "self", ".", "n_channels", ",", "\n", "kernel_size", "=", "1", ")", "\n", "\n", "# Define the output layer that maps the number of channels from", "\n", "# n_output_channels of the last convolutional layer -> 1", "\n", "self", ".", "output_layer", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "self", ".", "n_channels", ",", "\n", "out_channels", "=", "1", ",", "\n", "kernel_size", "=", "1", ")", "\n", "\n", "# Store convolutional layers in a ModuleList(). This is important to", "\n", "# ensure that everything works even when training with multiple GPUs.", "\n", "self", ".", "convolutional_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "# Create as many convolutional layers as requested", "\n", "for", "i", "in", "range", "(", "self", ".", "n_convolutional_layers", ")", ":", "\n", "\n", "# Create a 1D convolutional layer where the dilation increases", "\n", "# exponentially with the index of the layer", "\n", "            ", "conv_layer", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "self", ".", "n_channels", ",", "\n", "out_channels", "=", "self", ".", "n_channels", ",", "\n", "kernel_size", "=", "2", ",", "\n", "dilation", "=", "2", "**", "i", ")", "\n", "\n", "self", ".", "convolutional_layers", ".", "append", "(", "conv_layer", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Initialize the weights of the model to sensible defaults", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "# Initialize the weight and bias of the in- and output layer", "\n", "", "nn", ".", "init", ".", "kaiming_normal_", "(", "self", ".", "input_layer", ".", "weight", ")", "\n", "nn", ".", "init", ".", "kaiming_normal_", "(", "self", ".", "output_layer", ".", "weight", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "input_layer", ".", "bias", ",", "0.001", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "output_layer", ".", "bias", ",", "0.001", ")", "\n", "\n", "# Initialize all the convolutional layer in between", "\n", "for", "conv_layer", "in", "self", ".", "convolutional_layers", ":", "\n", "            ", "nn", ".", "init", ".", "kaiming_normal_", "(", "conv_layer", ".", "weight", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "conv_layer", ".", "bias", ",", "0.001", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.models.FCNN.forward": [[82, 94], ["models.FCNN.input_layer.forward", "models.FCNN.output_layer.forward", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "conv_layer.forward", "torch.relu", "torch.relu", "torch.relu", "torch.relu"], "methods", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.models.FCNN.forward", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.models.FCNN.forward", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.models.FCNN.forward"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "x", "=", "self", ".", "input_layer", ".", "forward", "(", "x", ")", "\n", "\n", "for", "conv_layer", "in", "self", ".", "convolutional_layers", ":", "\n", "            ", "x", "=", "conv_layer", ".", "forward", "(", "x", ")", "\n", "x", "=", "torch", ".", "relu", "(", "x", ")", "\n", "\n", "", "x", "=", "self", ".", "output_layer", ".", "forward", "(", "x", ")", "\n", "x", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.__init__": [[54, 97], ["checkpointing.CheckpointManager.initialize_checkpoints_directory", "isinstance", "TypeError", "isinstance", "TypeError", "TypeError", "type", "type", "type"], "methods", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.initialize_checkpoints_directory"], ["def", "__init__", "(", "self", ",", "\n", "model", ":", "torch", ".", "nn", ".", "Module", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "scheduler", ":", "Any", ",", "\n", "checkpoints_directory", ":", "str", "=", "'./checkpoints'", ",", "\n", "mode", ":", "str", "=", "'min'", ",", "\n", "step_size", ":", "int", "=", "1", ",", "\n", "last_epoch", ":", "int", "=", "-", "1", ",", "\n", "verbose", ":", "bool", "=", "True", ")", ":", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Basic sanity checks for constructor arguments", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "        ", "if", "not", "isinstance", "(", "model", ",", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'{type(model).__name__} is not a Module'", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "optimizer", ",", "torch", ".", "optim", ".", "Optimizer", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'{type(optimizer).__name__} is not an Optimizer'", ")", "\n", "\n", "", "if", "not", "scheduler", ".", "__module__", "==", "'torch.optim.lr_scheduler'", ":", "\n", "            ", "raise", "TypeError", "(", "f'{type(scheduler).__name__} is not a Scheduler'", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Store constructor arguments and initialize checkpoint directory", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "", "self", ".", "model", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "scheduler", "=", "scheduler", "\n", "self", ".", "checkpoints_directory", "=", "checkpoints_directory", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "step_size", "=", "step_size", "\n", "self", ".", "last_epoch", "=", "last_epoch", "\n", "self", ".", "verbose", "=", "verbose", "\n", "\n", "self", ".", "initialize_checkpoints_directory", "(", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Create additional class variable (best metric value so far)", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "self", ".", "best_metric", "=", "np", ".", "inf", "*", "(", "-", "1", "if", "mode", "==", "'max'", "else", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.initialize_checkpoints_directory": [[100, 108], ["pathlib.Path().mkdir", "pathlib.Path"], "methods", ["None"], ["", "def", "initialize_checkpoints_directory", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initialize the checkpoints directory (e.g., ensure it exists).\n        \"\"\"", "\n", "\n", "# Make sure the checkpoint_directory exists. If it does not exist,", "\n", "# create it (including necessary parent directories)", "\n", "Path", "(", "self", ".", "checkpoints_directory", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.step": [[111, 154], ["checkpointing.CheckpointManager.save_checkpoint", "checkpointing.CheckpointManager.save_checkpoint", "checkpointing.CheckpointManager.get_current_checkpoint", "checkpointing.CheckpointManager.get_current_checkpoint"], "methods", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.save_checkpoint", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.save_checkpoint", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.get_current_checkpoint", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.get_current_checkpoint"], ["", "def", "step", "(", "self", ",", "\n", "metric", ":", "float", ",", "\n", "epoch", ":", "int", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Save checkpoint if step size conditions meet, and update best\n        checkpoint based on `metric` and `self.mode`.\n\n        Args:\n            metric: The value of the metric that is being monitored\n                (e.g., the validation loss) for the current epoch.\n            epoch: The current epoch. If no value is provided, we'll\n                try to keep counting from the original `last_epoch`.\n        \"\"\"", "\n", "\n", "# Get the current epoch, either explicitly or by counting from the", "\n", "# initial `last_epoch` the CheckpointManager was instantiated with.", "\n", "if", "not", "epoch", ":", "\n", "            ", "epoch", "=", "self", ".", "last_epoch", "+", "1", "\n", "", "self", ".", "last_epoch", "=", "epoch", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Save best model checkpoint if applicable", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "# Check if the current model is the best we have seen so far according", "\n", "# to the provided metric and mode", "\n", "if", "(", "self", ".", "mode", "==", "\"min\"", "and", "metric", "<", "self", ".", "best_metric", ")", "or", "(", "self", ".", "mode", "==", "\"max\"", "and", "metric", ">", "self", ".", "best_metric", ")", ":", "\n", "\n", "# Update the value for the best metric", "\n", "            ", "self", ".", "best_metric", "=", "metric", "\n", "\n", "# Save the current checkpoint as the best checkpoint", "\n", "self", ".", "save_checkpoint", "(", "checkpoint", "=", "self", ".", "get_current_checkpoint", "(", ")", ",", "\n", "name", "=", "'best.pth'", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Save regular checkpoint (every `step_size` epochs)", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "", "if", "(", "self", ".", "step_size", ">", "0", ")", "and", "(", "self", ".", "last_epoch", "%", "self", ".", "step_size", "==", "0", ")", ":", "\n", "            ", "self", ".", "save_checkpoint", "(", "checkpoint", "=", "self", ".", "get_current_checkpoint", "(", ")", ",", "\n", "name", "=", "f'epoch_{epoch}.pth'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.save_checkpoint": [[157, 172], ["torch.save", "os.path.join", "print"], "methods", ["None"], ["", "", "def", "save_checkpoint", "(", "self", ",", "\n", "checkpoint", ":", "dict", ",", "\n", "name", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Save a new `checkpoint` in the `checkpoints_directory` with a\n        given `name` (usually as a *.pth file).\n        \"\"\"", "\n", "\n", "# Save the checkpoint in the pre-specified directory", "\n", "torch", ".", "save", "(", "checkpoint", ",", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoints_directory", ",", "name", ")", ")", "\n", "\n", "# Print a message if needed", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "f'Saved checkpoint: {name}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.load_checkpoint": [[175, 200], ["torch.load", "checkpointing.CheckpointManager.model.load_state_dict", "checkpointing.CheckpointManager.optimizer.load_state_dict", "checkpointing.CheckpointManager.scheduler.load_state_dict", "os.path.exists", "FileNotFoundError"], "methods", ["None"], ["", "", "def", "load_checkpoint", "(", "self", ",", "\n", "checkpoint_file_path", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Load a previously saved checkpoint into the CheckpointManager.\n\n        Args:\n            checkpoint_file_path: The path to the file containing a\n                saved checkpoint.\n        \"\"\"", "\n", "\n", "# Make sure the checkpoint we want to load exists!", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "checkpoint_file_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "f'{checkpoint_file_path} does not exist!'", ")", "\n", "\n", "# Load the checkpoint file", "\n", "", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_file_path", ")", "\n", "\n", "# Load the state dicts of the model, optimizer and LR scheduler", "\n", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optim_state_dict'", "]", ")", "\n", "self", ".", "scheduler", ".", "load_state_dict", "(", "checkpoint", "[", "'sched_state_dict'", "]", ")", "\n", "\n", "# Also load the last epoch and the best metric value", "\n", "self", ".", "last_epoch", "=", "checkpoint", "[", "'last_epoch'", "]", "\n", "self", ".", "best_metric", "=", "checkpoint", "[", "'best_metric'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.get_current_checkpoint": [[203, 230], ["isinstance", "dict", "checkpointing.CheckpointManager.model.module.state_dict", "checkpointing.CheckpointManager.model.state_dict", "checkpointing.CheckpointManager.optimizer.state_dict", "checkpointing.CheckpointManager.scheduler.state_dict"], "methods", ["None"], ["", "def", "get_current_checkpoint", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dict containing the state dict of model (taking care\n        of DataParallel case), the optimizer, the scheduler, the best\n        metric and the epoch.\n\n        Returns:\n            A checkpoint, containing the current state_dict for the\n            model, the optimizer, the LR scheduler, as well as the\n            values for the best metric and the last_epoch.\n        \"\"\"", "\n", "\n", "# Get the state dict of the model, taking care of the DataParallel", "\n", "# case (which modifies the name of the state dict)", "\n", "if", "isinstance", "(", "self", ".", "model", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "            ", "model_state_dict", "=", "self", ".", "model", ".", "module", ".", "state_dict", "(", ")", "\n", "", "else", ":", "\n", "            ", "model_state_dict", "=", "self", ".", "model", ".", "state_dict", "(", ")", "\n", "\n", "# Collect the full checkpoint in a dict an return it", "\n", "", "checkpoint", "=", "dict", "(", "model_state_dict", "=", "model_state_dict", ",", "\n", "optim_state_dict", "=", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "sched_state_dict", "=", "self", ".", "scheduler", ".", "state_dict", "(", ")", ",", "\n", "best_metric", "=", "self", ".", "best_metric", ",", "\n", "last_epoch", "=", "self", ".", "last_epoch", ")", "\n", "\n", "return", "checkpoint", "\n", "", "", ""]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.configfiles.get_config": [[17, 44], ["os.path.exists", "os.path.exists", "open", "json.load", "FileNotFoundError"], "function", ["None"], ["def", "get_config", "(", ")", "->", "dict", ":", "\n", "    ", "\"\"\"\n    Load the global configuration file, parse it as JSON, and return\n    the  contents as a Python `dict`.\n\n    Returns:\n        A Python `dict` containing the configuration file contents.\n    \"\"\"", "\n", "\n", "# Check if a local version of the config file exists, which takes", "\n", "# precedence over the default version in the repository", "\n", "if", "os", ".", "path", ".", "exists", "(", "'./CONFIG.local.json'", ")", ":", "\n", "        ", "file_path", "=", "'./CONFIG.local.json'", "\n", "\n", "# Otherwise, check of the default config file is available", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "'./CONFIG.json'", ")", ":", "\n", "        ", "file_path", "=", "'./CONFIG.json'", "\n", "\n", "# If no configuration is found, raise an error", "\n", "", "else", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "'Configuration file not found!'", ")", "\n", "\n", "# Read in and parse JSON", "\n", "", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "config_file", ":", "\n", "        ", "config", "=", "json", ".", "load", "(", "config_file", ")", "\n", "\n", "", "return", "config", "\n", "", ""]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.training.AverageMeter.__init__": [[27, 32], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "value", "=", "0", "\n", "self", ".", "average", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.training.AverageMeter.update": [[33, 40], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "\n", "value", ":", "float", ",", "\n", "steps", ":", "int", "=", "1", ")", ":", "\n", "        ", "self", ".", "value", "=", "value", "\n", "self", ".", "sum", "+=", "value", "*", "steps", "\n", "self", ".", "count", "+=", "steps", "\n", "self", ".", "average", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.training.get_lr": [[46, 59], ["None"], "function", ["None"], ["", "", "def", "get_lr", "(", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ")", "->", "float", ":", "\n", "    ", "\"\"\"\n    Get the current learning rate for a given optimizer.\n\n    Args:\n        optimizer: The optimizer whose learning rate we want to get.\n\n    Returns:\n        The learning rate of the given `optimizer`.\n    \"\"\"", "\n", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "return", "param_group", "[", "'lr'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.training.get_log_dir": [[61, 88], ["datetime.datetime.fromtimestamp", "datetime.datetime.fromtimestamp.strftime", "os.path.join", "time.time", "pathlib.Path().mkdir", "pathlib.Path"], "function", ["None"], ["", "", "def", "get_log_dir", "(", "log_base_dir", ":", "str", "=", "'./tensorboard'", ",", "\n", "ensure_exists", ":", "bool", "=", "True", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Create the path to the directory where the TensorBoard logs are\n    stored, using a \"YYYY-MM-DD_HH:MM:SS\" naming scheme for the\n    different runs.\n\n    Args:\n        log_base_dir: The base directory where the folders for\n            different runs should be created.\n        ensure_exists: Whether or not to create the directory in case\n            it does not exist yet.\n\n    Returns:\n        The path the to log directory for the current run.\n    \"\"\"", "\n", "\n", "# Create log dir path based on current timestamp", "\n", "timestamp", "=", "datetime", ".", "datetime", ".", "fromtimestamp", "(", "time", ".", "time", "(", ")", ")", "\n", "formatted_timestamp", "=", "timestamp", ".", "strftime", "(", "'%Y-%m-%d_%H:%M:%S'", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "log_base_dir", ",", "formatted_timestamp", ")", "\n", "\n", "# Make sure the directory exists", "\n", "if", "ensure_exists", ":", "\n", "        ", "Path", "(", "log_dir", ")", ".", "mkdir", "(", "exist_ok", "=", "True", ",", "parents", "=", "True", ")", "\n", "\n", "", "return", "log_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.training.update_lr": [[90, 119], ["training.get_lr", "scheduler.step", "training.get_lr", "print"], "function", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.training.get_lr", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.checkpointing.CheckpointManager.step", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.training.get_lr"], ["", "def", "update_lr", "(", "scheduler", ":", "Any", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "validation_loss", ":", "float", ")", "->", "float", ":", "\n", "    ", "\"\"\"\n    Perform a `step()` with the learning rate scheduler, and print the\n    new learning rate in case we have decreased it.\n\n    Args:\n        scheduler: Instance of a PyTorch LR scheduler class, for\n            example, ``ReduceLROnPlateau``.\n        optimizer: An instance of an optimizer that is used to compute\n            and perform the updates to the weights of the network.\n        validation_loss: The last loss obtained on the validation set.\n\n    Returns:\n        The current learning rate (LR).\n    \"\"\"", "\n", "\n", "# Get the current learning rate, then take a step with the", "\n", "# scheduler, and get the learning rate after that", "\n", "old_lr", "=", "get_lr", "(", "optimizer", ")", "\n", "scheduler", ".", "step", "(", "validation_loss", ")", "\n", "new_lr", "=", "get_lr", "(", "optimizer", ")", "\n", "\n", "# If the LR has changed, print a message to the console", "\n", "if", "new_lr", "!=", "old_lr", ":", "\n", "        ", "print", "(", "f'Reduced Learning Rate (LR): {old_lr} -> {new_lr}'", ")", "\n", "\n", "", "return", "new_lr", "\n", "", ""]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.evaluation.find_binary_peaks": [[18, 46], ["numpy.hstack", "numpy.diff", "np.hstack.astype", "numpy.where", "numpy.where", "numpy.mean", "numpy.vstack"], "function", ["None"], ["def", "find_binary_peaks", "(", "array", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"\n    Take a numpy array whose values must be all in {0, 1}, and find\n    the centers of the continuous stretches of 1s.\n\n    Example: [0, 0, 0, 0, 0, 0, 1, 0, 1, 1] -> [6.0, 8.5]\n\n    Args:\n        array: A 1D numpy array with values only in {0, 1}.\n\n    Returns:\n        A numpy array containing the centers of the continuous\n        stretches of 1s in the input `array`.\n    \"\"\"", "\n", "\n", "# Pad the input array with zeros left and right", "\n", "array", "=", "np", ".", "hstack", "(", "(", "[", "0", "]", ",", "array", ",", "[", "0", "]", ")", ")", "\n", "\n", "# Compute the differences between neighboring elements: a[n+1] - a[n]", "\n", "differences", "=", "np", ".", "diff", "(", "array", ".", "astype", "(", "int", ")", ")", "\n", "\n", "# Find the starts and ends of the intervals of ones", "\n", "starts", "=", "np", ".", "where", "(", "differences", "==", "1", ")", "[", "0", "]", "\n", "ends", "=", "np", ".", "where", "(", "differences", "==", "-", "1", ")", "[", "0", "]", "\n", "\n", "# Compute the interval centers as the means of their start and end, and", "\n", "# subtract a constant offset of 0.5 for a more intuitive interpretation", "\n", "return", "np", ".", "mean", "(", "np", ".", "vstack", "(", "(", "starts", ",", "ends", ")", ")", ",", "axis", "=", "0", ")", "-", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.evaluation.postprocess_prediction": [[48, 80], ["numpy.full", "numpy.convolve", "ValueError"], "function", ["None"], ["", "def", "postprocess_prediction", "(", "prediction", ":", "np", ".", "ndarray", ",", "\n", "threshold", ":", "float", "=", "0.5", ",", "\n", "window_size", ":", "int", "=", "256", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "\"\"\"\n    Apply post-processing steps (smoothing, thresholding) to a \"raw\"\n    network prediction.\n\n    Args:\n        prediction: A 1D numpy array with values in (0, 1) containing\n            the raw prediction of the fully convolutional neural\n            network for a given example.\n        threshold: Threshold for rounding smoothed values to 1.\n        window_size: Size of the rectangular window that is used to\n            compute a rolling average to smooth the prediction.\n\n    Returns:\n        A numpy array with the same length as the input containing the\n        smoothed and thresholded network prediction.\n    \"\"\"", "\n", "\n", "# Make sure the input has the correct dimensionality", "\n", "if", "prediction", ".", "ndim", "!=", "1", ":", "\n", "        ", "raise", "ValueError", "(", "f'Input should be 1D, but is {prediction.ndim}D!'", ")", "\n", "\n", "# Construct a rectangular window and smooth the output", "\n", "", "kernel", "=", "np", ".", "full", "(", "window_size", ",", "1.0", "/", "window_size", ")", "\n", "output", "=", "np", ".", "convolve", "(", "prediction", ",", "kernel", ",", "mode", "=", "'same'", ")", "\n", "\n", "# Round the raw outputs based on the provided threshold", "\n", "output", "=", "(", "output", ">=", "threshold", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.evaluation.get_detections_and_fp": [[82, 150], ["list", "list", "evaluation.postprocess_prediction", "evaluation.find_binary_peaks", "numpy.array", "numpy.array", "list.append", "any", "list.append", "list.append", "len", "len", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.evaluation.postprocess_prediction", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.evaluation.find_binary_peaks"], ["", "def", "get_detections_and_fp", "(", "predictions", ":", "np", ".", "ndarray", ",", "\n", "event_index", ":", "Optional", "[", "int", "]", ",", "\n", "slack_width", ":", "Optional", "[", "int", "]", ",", "\n", "window_size", ":", "int", "=", "256", ",", "\n", "threshold", ":", "float", "=", "0.5", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "\n", "np", ".", "ndarray", "]", ":", "\n", "    ", "\"\"\"\n    For every raw (i.e., not yet pre-processed) prediction in the\n    `predictions` array, check whether a possible injection was found,\n    and return the number of false positives.\n\n    Args:\n        predictions: A 2D numpy array containing (raw) predictions\n            made by the fully convolutional neural network.\n        event_index: Index of the time step which corresponds to the\n            ground truth injection time. If `None` is passed, it is\n            assumed that the samples do not contain an injection, that\n            is, every trigger is a false positive.\n        slack_width: Maximum number of time steps which the prediction\n            may be off from `event_index` to still be counted as a\n            recovered injection.\n        window_size: Size of the smoothing window that is used to\n            post-process the predictions.\n        threshold: Threshold that is used to round or \"binarize\" the\n            smoothed predictions.\n\n    Returns:\n        A tuple `(detected_flags, false_positives)` which contains for\n        every prediction a boolean flag whether or not the injection\n        was recovered (for noise-only samples, this list is empty), as\n        well as a numpy array containing the number of false positives\n        for each example.\n    \"\"\"", "\n", "\n", "# Keep track of the detections and false positives", "\n", "detected_flags", "=", "list", "(", ")", "\n", "false_positives", "=", "list", "(", ")", "\n", "\n", "# For each prediction, check if an injection was recovered, and", "\n", "# count the number of false positives produced", "\n", "for", "prediction", "in", "predictions", ":", "\n", "\n", "# Apply post-processing steps (smoothing, thresholding)", "\n", "        ", "postprocessed", "=", "postprocess_prediction", "(", "prediction", "=", "prediction", ",", "\n", "window_size", "=", "window_size", ",", "\n", "threshold", "=", "threshold", ")", "\n", "\n", "# Find all the peaks in the post-processed prediction", "\n", "all_peaks", "=", "find_binary_peaks", "(", "postprocessed", ")", "\n", "\n", "# If we are looking at a noise-only sample, every peak is a", "\n", "# false positive, and we only need to count these", "\n", "if", "event_index", "is", "None", ":", "\n", "            ", "false_positives", ".", "append", "(", "len", "(", "all_peaks", ")", ")", "\n", "continue", "\n", "\n", "# Otherwise, we need to check if the injection was recovered", "\n", "", "else", ":", "\n", "\n", "# Check if any of the peaks is within the acceptance interval", "\n", "            ", "detected", "=", "any", "(", "[", "np", ".", "abs", "(", "event_index", "-", "peak", ")", "<", "slack_width", "\n", "for", "peak", "in", "all_peaks", "]", ")", "\n", "detected_flags", ".", "append", "(", "detected", ")", "\n", "\n", "# Compute and store the number of false positives", "\n", "false_positives", ".", "append", "(", "len", "(", "all_peaks", ")", "-", "detected", ")", "\n", "\n", "", "", "return", "np", ".", "array", "(", "detected_flags", ")", ",", "np", ".", "array", "(", "false_positives", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.evaluation.get_dr_and_fpr": [[152, 192], ["sum", "sum", "numpy.mean", "dict", "sum", "len", "numpy.sum", "false_positives.values", "false_positives.values"], "function", ["None"], ["", "def", "get_dr_and_fpr", "(", "detected_flags", ":", "np", ".", "ndarray", ",", "\n", "false_positives", ":", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", ",", "\n", "sample_length", ":", "float", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "    ", "\"\"\"\n    Compute the detection ratio (DR) and the false positive rate (FPR),\n    as well as the false positive ratio, from the given trigger data\n    in `detected_flags` and `false_positives`.\n\n    Args:\n        detected_flags: A numpy array whose length matches the number\n            of examples containing an injection, and which for each\n            example specifies if the injection was recovered (value 1)\n            or not (value 0).\n        false_positives: A dictionary with keys `{'injection', 'noise'}`\n            which contains the number of false positives for each\n            example (both with and without an injection).\n        sample_length: The length (in seconds) of the original samples.\n            This is needed to compute the (average) false positive rate.\n\n    Returns:\n        A `dict` with keys `{'detection_ratio', 'false_positive_ratio',\n        'false_positive_rate'}`, which are the metrics computed from\n        the input data.\n    \"\"\"", "\n", "\n", "# Define some shortcuts for calculating our metrics", "\n", "n_samples_total", "=", "sum", "(", "[", "len", "(", "_", ")", "for", "_", "in", "false_positives", ".", "values", "(", ")", "]", ")", "\n", "n_false_positives", "=", "sum", "(", "[", "np", ".", "sum", "(", "_", ")", "for", "_", "in", "false_positives", ".", "values", "(", ")", "]", ")", "\n", "n_all_triggers", "=", "sum", "(", "detected_flags", ")", "+", "n_false_positives", "\n", "total_rec_time", "=", "sample_length", "*", "n_samples_total", "\n", "\n", "# Compute the detection ratio and the false positive ratio / rate", "\n", "detection_ratio", "=", "np", ".", "mean", "(", "detected_flags", ")", "\n", "false_positive_ratio", "=", "n_false_positives", "/", "n_all_triggers", "\n", "false_positive_rate", "=", "n_false_positives", "/", "total_rec_time", "\n", "\n", "# Return the results as a dictionary", "\n", "return", "dict", "(", "detection_ratio", "=", "detection_ratio", ",", "\n", "false_positive_ratio", "=", "false_positive_ratio", ",", "\n", "false_positive_rate", "=", "false_positive_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.evaluation.get_avg_event_time_deviation": [[194, 262], ["numpy.array", "float", "float", "evaluation.postprocess_prediction", "evaluation.find_binary_peaks", "numpy.nanmean", "numpy.nanstd", "numpy.abs", "np.array.append", "np.array.append", "min", "min"], "function", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.evaluation.postprocess_prediction", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.evaluation.find_binary_peaks"], ["", "def", "get_avg_event_time_deviation", "(", "predictions", ":", "np", ".", "ndarray", ",", "\n", "event_index", ":", "int", ",", "\n", "slack_width", ":", "int", ",", "\n", "sampling_rate", ":", "int", ",", "\n", "window_size", ":", "int", "=", "256", ",", "\n", "threshold", ":", "float", "=", "0.5", ")", "->", "Tuple", "[", "float", ",", "\n", "float", "]", ":", "\n", "    ", "\"\"\"\n    For the predictions containing an injection, find the mean and the\n    standard deviation of the difference between the predicted event\n    time and the ground truth event time.\n\n    Args:\n        predictions: A 2D numpy array containing (raw) predictions\n            made by the fully convolutional neural network for the\n            examples that do contain an injection.\n        event_index: Index of the time step which corresponds to the\n            ground truth injection time.\n        slack_width: Maximum number of time steps which the prediction\n            may be off from `event_index` to still be counted as a\n            recovered injection.\n        sampling_rate: Sampling rate of the predictions (to convert\n            between time steps and seconds).\n        window_size: Size of the smoothing window that is used to\n            post-process the predictions.\n        threshold: Threshold that is used to round or \"binarize\" the\n            smoothed predictions.\n\n    Returns:\n        A dictionary `(mean, std)` containing the mean and standard\n        deviation of the deviation of the predicted event time from\n        the ground truth event time (if the injection was recovered).\n    \"\"\"", "\n", "\n", "# Keep track of the time differences between prediction and ground truth", "\n", "deviations", "=", "[", "]", "\n", "\n", "# For each prediction, check if an injection was recovered", "\n", "for", "prediction", "in", "predictions", ":", "\n", "\n", "# Apply post-processing steps (smoothing, thresholding)", "\n", "        ", "postprocessed", "=", "postprocess_prediction", "(", "prediction", "=", "prediction", ",", "\n", "window_size", "=", "window_size", ",", "\n", "threshold", "=", "threshold", ")", "\n", "\n", "# Find all the peaks in the post-processed prediction", "\n", "all_peaks", "=", "find_binary_peaks", "(", "postprocessed", ")", "\n", "\n", "# Get the time difference to the ground truth for every peak", "\n", "differences", "=", "[", "np", ".", "abs", "(", "event_index", "-", "peak", ")", "for", "peak", "in", "all_peaks", "]", "\n", "\n", "# We only look at the peak that is closest to the ground truth", "\n", "# injection time. If this one is within the acceptance region, we", "\n", "# store its difference (in seconds); otherwise---if it is not", "\n", "# recovered---we just store NaN so that we can exclude it when", "\n", "# computing the mean and the standard deviation.", "\n", "if", "differences", "and", "min", "(", "differences", ")", "<", "slack_width", ":", "\n", "            ", "deviations", ".", "append", "(", "min", "(", "differences", ")", "/", "sampling_rate", ")", "\n", "", "else", ":", "\n", "            ", "deviations", ".", "append", "(", "np", ".", "nan", ")", "\n", "\n", "# Convert deviations to a numpy array and compute mean and std while", "\n", "# ignoring all examples where the injection as not recovered", "\n", "", "", "deviations", "=", "np", ".", "array", "(", "deviations", ")", "\n", "mean", "=", "float", "(", "np", ".", "nanmean", "(", "deviations", ")", ")", "\n", "std", "=", "float", "(", "np", ".", "nanstd", "(", "deviations", ")", ")", "\n", "\n", "return", "mean", ",", "std", "\n", "", ""]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.samplefiles.get_normalization_from_samplefile": [[20, 48], ["dict", "os.path.exists", "FileNotFoundError", "h5py.File", "float", "float", "float", "float"], "function", ["None"], ["def", "get_normalization_from_samplefile", "(", "file_path", ":", "str", ")", "->", "dict", ":", "\n", "    ", "\"\"\"\n    Extract the normalization parameters for a given HDF sample file.\n    \n    Args:\n        file_path: Path to the sample file from which to read the data.\n\n    Returns:\n        A dict {'h1_mean', 'l1_mean', 'h1_std', 'l1_std'} containing\n        the means and standard deviations of the sample file contents.\n    \"\"\"", "\n", "\n", "# Make sure that the sample file we want to read from exists", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "file_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "f'Sample file \"{file_path}\" does not exist!'", ")", "\n", "\n", "# Read the normalization parameters from the file", "\n", "", "with", "h5py", ".", "File", "(", "file_path", ",", "'r'", ")", "as", "hdf_file", ":", "\n", "        ", "h1_mean", "=", "float", "(", "hdf_file", "[", "'normalization_parameters'", "]", ".", "attrs", "[", "'h1_mean'", "]", ")", "\n", "l1_mean", "=", "float", "(", "hdf_file", "[", "'normalization_parameters'", "]", ".", "attrs", "[", "'l1_mean'", "]", ")", "\n", "h1_std", "=", "float", "(", "hdf_file", "[", "'normalization_parameters'", "]", ".", "attrs", "[", "'h1_std'", "]", ")", "\n", "l1_std", "=", "float", "(", "hdf_file", "[", "'normalization_parameters'", "]", ".", "attrs", "[", "'l1_std'", "]", ")", "\n", "\n", "# Return the values as a dictionary", "\n", "", "return", "dict", "(", "h1_mean", "=", "h1_mean", ",", "\n", "l1_mean", "=", "l1_mean", ",", "\n", "h1_std", "=", "h1_std", ",", "\n", "l1_std", "=", "l1_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.samplefiles.get_samples_from_samplefile": [[50, 222], ["os.path.exists", "FileNotFoundError", "ValueError", "h5py.File", "float", "float", "int", "int", "numpy.dstack", "numpy.swapaxes", "numpy.expand_dims().astype", "numpy.array", "numpy.array", "numpy.zeros", "numpy.array", "numpy.array", "numpy.zeros", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.expand_dims"], "function", ["None"], ["", "def", "get_samples_from_samplefile", "(", "file_path", ":", "str", ",", "\n", "normalization", ":", "dict", "=", "None", ",", "\n", "sample_type", ":", "str", "=", "'both'", ",", "\n", "label_length", ":", "float", "=", "0.2", ",", "\n", "cutoff_left", ":", "int", "=", "2047", ",", "\n", "cutoff_right", ":", "int", "=", "2048", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "\n", "np", ".", "ndarray", "]", ":", "\n", "    ", "\"\"\"\n    Read in samples from a sample file and return them as properly\n    shaped numpy arrays.\n    \n    Args:\n        file_path: Path to the sample file from which to read the data.\n        normalization: A dictionary specifying the parameters of the\n            normalization transform that will be  applied to the data.\n            This is to ensure that all samples are (approximately)\n            standard normal, which is otherwise not the  case (because\n            of PyCBC's normalization choice for the whitening).\n        sample_type: Which samples to retrieve from the sample file.\n            Options are: ['injection', 'noise', 'both'].\n        label_length: Length of the label to be generated in seconds.\n        cutoff_left: Number of time steps to cut off from the label\n            on the left.\n        cutoff_right: Number of time steps to cut off from the labels\n            on the right.\n            \n    Returns:\n        A tuple `(samples, labels)`, where `samples` is a numpy array\n        with the shape `(n_samples, 2, sampling_rate * sample_length)`.\n        It contains the samples from the sample file (both with and\n        without injection, depending on `sample_type`).\n        `labels` is also a numpy array with the shape `(n_samples, 1,\n        sampling_rate * sample_length - cutoff_left - cutoff_right)`,\n        containing the labels corresponding to the samples.\n    \"\"\"", "\n", "\n", "# -------------------------------------------------------------------------", "\n", "# Basic sanity checks", "\n", "# -------------------------------------------------------------------------", "\n", "\n", "# Make sure that the sample file we want to read from exists", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "file_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "f'Sample file \"{file_path}\" does not exist!'", ")", "\n", "\n", "", "if", "sample_type", "not", "in", "(", "'injection'", ",", "'noise'", ",", "'both'", ")", ":", "\n", "        ", "raise", "ValueError", "(", "f'Invalid value for sample_type: {sample_type}'", ")", "\n", "\n", "# -------------------------------------------------------------------------", "\n", "# Open the HDF sample file and read in the samples and labels", "\n", "# -------------------------------------------------------------------------", "\n", "\n", "", "with", "h5py", ".", "File", "(", "file_path", ",", "'r'", ")", "as", "hdf_file", ":", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Get the label location from the static_args in the sample file", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "# Get seconds_before_event and sampling_rate", "\n", "        ", "seconds_before_event", "=", "float", "(", "hdf_file", "[", "'static_arguments'", "]", ".", "attrs", "[", "'seconds_before_event'", "]", ")", "\n", "sampling_rate", "=", "float", "(", "hdf_file", "[", "'static_arguments'", "]", ".", "attrs", "[", "'target_sampling_rate'", "]", ")", "\n", "\n", "# Calculate the start and end indices for the label", "\n", "label_start", "=", "int", "(", "(", "seconds_before_event", "-", "(", "0.5", "*", "label_length", ")", ")", "*", "\n", "sampling_rate", ")", "\n", "label_end", "=", "int", "(", "(", "seconds_before_event", "+", "(", "0.5", "*", "label_length", ")", ")", "*", "\n", "sampling_rate", "+", "1", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Initialize variables so the static code checker stops complaining", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "inj_samples_h1", "=", "None", "\n", "inj_samples_l1", "=", "None", "\n", "inj_samples_label", "=", "None", "\n", "\n", "noise_samples_h1", "=", "None", "\n", "noise_samples_l1", "=", "None", "\n", "noise_samples_label", "=", "None", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Read in injection samples if requested", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "if", "sample_type", "in", "(", "'injection'", ",", "'both'", ")", ":", "\n", "\n", "# Read in samples containing an injection", "\n", "            ", "inj_samples_h1", "=", "np", ".", "array", "(", "hdf_file", "[", "'/injection_samples/h1_strain'", "]", ")", "\n", "inj_samples_l1", "=", "np", ".", "array", "(", "hdf_file", "[", "'/injection_samples/l1_strain'", "]", ")", "\n", "\n", "# Dynamically create the label", "\n", "inj_samples_label", "=", "np", ".", "zeros", "(", "shape", "=", "inj_samples_h1", ".", "shape", ")", "\n", "inj_samples_label", "[", ":", ",", "label_start", ":", "label_end", "]", "=", "1", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Read in noise samples if requested", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "", "if", "sample_type", "in", "(", "'noise'", ",", "'both'", ")", ":", "\n", "\n", "# Read in samples not containing an injection", "\n", "            ", "noise_samples_h1", "=", "np", ".", "array", "(", "hdf_file", "[", "'/noise_samples/h1_strain'", "]", ")", "\n", "noise_samples_l1", "=", "np", ".", "array", "(", "hdf_file", "[", "'/noise_samples/l1_strain'", "]", ")", "\n", "\n", "# Dynamically create the label", "\n", "noise_samples_label", "=", "np", ".", "zeros", "(", "shape", "=", "noise_samples_h1", ".", "shape", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Stack samples if necessary", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "", "if", "sample_type", "==", "'injection'", ":", "\n", "            ", "samples_h1", "=", "inj_samples_h1", "\n", "samples_l1", "=", "inj_samples_l1", "\n", "labels", "=", "inj_samples_label", "\n", "", "elif", "sample_type", "==", "'noise'", ":", "\n", "            ", "samples_h1", "=", "noise_samples_h1", "\n", "samples_l1", "=", "noise_samples_l1", "\n", "labels", "=", "noise_samples_label", "\n", "", "else", ":", "\n", "            ", "samples_h1", "=", "np", ".", "concatenate", "(", "[", "inj_samples_h1", ",", "noise_samples_h1", "]", ")", "\n", "samples_l1", "=", "np", ".", "concatenate", "(", "[", "inj_samples_l1", ",", "noise_samples_l1", "]", ")", "\n", "labels", "=", "np", ".", "concatenate", "(", "[", "inj_samples_label", ",", "noise_samples_label", "]", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Normalize the data", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "# If no normalization was explicitly given, use default values.", "\n", "# Caution: Not passing a dict for normalization means that the data", "\n", "# will not be normalized at remain at the original PyCBC outputs!", "\n", "", "if", "normalization", "is", "None", ":", "\n", "            ", "normalization", "=", "{", "'h1_mean'", ":", "0.0", ",", "'h1_std'", ":", "1.0", ",", "\n", "'l1_mean'", ":", "0.0", ",", "'l1_std'", ":", "1.0", "}", "\n", "\n", "# Apply the normalization transformation: Subtract the mean and divide", "\n", "# by the standard deviation.", "\n", "", "samples_h1", "=", "(", "samples_h1", "-", "normalization", "[", "'h1_mean'", "]", ")", "/", "normalization", "[", "'h1_std'", "]", "\n", "samples_l1", "=", "(", "samples_l1", "-", "normalization", "[", "'l1_mean'", "]", ")", "/", "normalization", "[", "'l1_std'", "]", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Stack H1 and L1 and adjust axes and dimensions", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "# Stack together H1 and L1 and adjust ordering of axes", "\n", "samples", "=", "np", ".", "dstack", "(", "[", "samples_h1", ",", "samples_l1", "]", ")", "\n", "samples", "=", "np", ".", "swapaxes", "(", "samples", ",", "1", ",", "2", ")", "\n", "\n", "# Add a dummy dimension to the labels and convert to float32", "\n", "labels", "=", "np", ".", "expand_dims", "(", "labels", ",", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Remove cutoff from labels", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "# Remove left and right cutoff from label: This is what we lose due", "\n", "# to using valid convolutions without padding. The way we choose the", "\n", "# left and right cutoff determines the alignment of the input time", "\n", "# series with its label.", "\n", "if", "cutoff_right", "!=", "0", ":", "\n", "            ", "labels", "=", "labels", "[", ":", ",", ":", ",", "cutoff_left", ":", "-", "cutoff_right", "]", "\n", "", "else", ":", "\n", "            ", "labels", "=", "labels", "[", ":", ",", ":", ",", "cutoff_left", ":", "]", "\n", "\n", "# ---------------------------------------------------------------------", "\n", "# Return samples and labels", "\n", "# ---------------------------------------------------------------------", "\n", "\n", "", "return", "samples", ",", "labels", "\n", "", "", ""]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.datasets.InjectionDataset.__init__": [[36, 61], ["configfiles.get_config", "samplefiles.get_normalization_from_samplefile", "samplefiles.get_samples_from_samplefile", "ValueError"], "methods", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.configfiles.get_config", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.samplefiles.get_normalization_from_samplefile", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.samplefiles.get_samples_from_samplefile"], ["def", "__init__", "(", "self", ",", "\n", "mode", ":", "str", ",", "\n", "sample_type", ":", "str", "=", "'both'", ")", ":", "\n", "\n", "# Basic sanity check: Must select valid mode", "\n", "        ", "if", "mode", "not", "in", "(", "\"training\"", ",", "\"validation\"", ",", "\"testing\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'mode must be one of the following:'", "\n", "'\"training\", \"validation\" or \"testing\"!'", ")", "\n", "\n", "# Load configuration file and get paths to data sets", "\n", "", "config", "=", "get_config", "(", ")", "\n", "data_paths", "=", "config", "[", "'data'", "]", "\n", "\n", "# Get the normalization parameters for the data (always from training)", "\n", "normalization", "=", "get_normalization_from_samplefile", "(", "data_paths", "[", "'training'", "]", ")", "\n", "\n", "# Load data from HDF samplefile", "\n", "self", ".", "data", ",", "self", ".", "labels", "=", "get_samples_from_samplefile", "(", "file_path", "=", "data_paths", "[", "mode", "]", ",", "\n", "normalization", "=", "normalization", ",", "\n", "sample_type", "=", "sample_type", ",", "\n", "label_length", "=", "0.2", ",", "\n", "cutoff_left", "=", "2047", ",", "\n", "cutoff_right", "=", "2048", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.datasets.InjectionDataset.__len__": [[64, 67], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.datasets.InjectionDataset.__getitem__": [[70, 77], ["torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "\n", "# Convert data to torch tensor and return", "\n", "        ", "data", "=", "torch", ".", "tensor", "(", "self", ".", "data", "[", "index", "]", ")", ".", "float", "(", ")", "\n", "label", "=", "torch", ".", "tensor", "(", "self", ".", "labels", "[", "index", "]", ")", ".", "float", "(", ")", "\n", "\n", "return", "data", ",", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.datasets.RealEventDataset.__init__": [[88, 115], ["configfiles.get_config", "samplefiles.get_normalization_from_samplefile", "numpy.vstack", "numpy.zeros", "h5py.File", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.configfiles.get_config", "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.samplefiles.get_normalization_from_samplefile"], ["def", "__init__", "(", "self", ",", "\n", "event", ":", "str", "=", "None", ")", ":", "\n", "\n", "# Load configuration file and get paths to data sets", "\n", "        ", "config", "=", "get_config", "(", ")", "\n", "file_path", "=", "config", "[", "'data'", "]", "[", "'real_events'", "]", "\n", "\n", "# Get the normalization parameters for the data (always from training)", "\n", "normalization", "=", "get_normalization_from_samplefile", "(", "config", "[", "'data'", "]", "[", "'training'", "]", ")", "\n", "\n", "# Open the HDF file and load the data for the event", "\n", "with", "h5py", ".", "File", "(", "file_path", ",", "'r'", ")", "as", "hdf_file", ":", "\n", "            ", "h1_strain", "=", "np", ".", "array", "(", "hdf_file", "[", "event", "]", "[", "'h1_strain'", "]", ")", "\n", "l1_strain", "=", "np", ".", "array", "(", "hdf_file", "[", "event", "]", "[", "'l1_strain'", "]", ")", "\n", "\n", "# Apply normalization", "\n", "", "h1_strain", "=", "(", "h1_strain", "-", "normalization", "[", "'h1_mean'", "]", ")", "/", "normalization", "[", "'h1_std'", "]", "\n", "l1_strain", "=", "(", "l1_strain", "-", "normalization", "[", "'l1_mean'", "]", ")", "/", "normalization", "[", "'l1_std'", "]", "\n", "\n", "# Construct the data", "\n", "self", ".", "data", "=", "np", ".", "vstack", "(", "(", "h1_strain", ",", "l1_strain", ")", ")", "\n", "\n", "# Construct a fake label (which we will never use)", "\n", "self", ".", "label", "=", "np", ".", "zeros", "(", "h1_strain", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.datasets.RealEventDataset.__len__": [[118, 120], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.timothygebhard_magic-bullet.utils.datasets.RealEventDataset.__getitem__": [[123, 130], ["torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "\n", "# Convert data to torch tensor and return", "\n", "        ", "data", "=", "torch", ".", "tensor", "(", "self", ".", "data", ")", ".", "float", "(", ")", "\n", "label", "=", "torch", ".", "tensor", "(", "self", ".", "label", ")", ".", "float", "(", ")", "\n", "\n", "return", "data", ",", "label", "\n", "", "", ""]]}