{"home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.main.fprint": [[16, 19], ["open", "myfile.write", "str"], "function", ["None"], ["def", "fprint", "(", "output_file", ",", "text", ")", ":", "\n", "    ", "with", "open", "(", "output_file", ",", "\"a\"", ")", "as", "myfile", ":", "\n", "        ", "myfile", ".", "write", "(", "str", "(", "text", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.main.onepass": [[20, 95], ["pool.starmap_async", "pool.starmap_async.get", "pool.starmap_async", "pool.starmap_async.get", "print", "print", "print", "sess.run", "losses_val.append", "losses_val_recon.append", "losses_val_uneq.append", "losses_val_eq.append", "len", "user_emb.tolist", "item_emb.tolist", "range", "len", "inps.append", "numpy.array().astype", "numpy.array().astype", "numpy.array", "numpy.array", "numpy.mean", "numpy.mean", "numpy.unique", "numpy.unique", "len", "len", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "len", "numpy.mean", "len", "[].append", "[].append", "bit_inspecter.bit_histogram", "bit_inspecter.bit_histogram", "numpy.sum", "int", "int", "numpy.array", "numpy.array", "np.array.astype", "np.array.astype", "min"], "function", ["home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.bit_inspecter.bit_histogram", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.bit_inspecter.bit_histogram"], ["", "", "def", "onepass", "(", "sess", ",", "eval_list", ",", "some_handle", ",", "num_samples", ",", "eval_batchsize", ",", "handle", ",", "anneal_val", ",", "batch_placeholder", ",", "is_training", ",", "force_selfmask", ",", "args", ")", ":", "\n", "    ", "losses_val", "=", "[", "]", "\n", "losses_val_recon", "=", "[", "]", "\n", "losses_val_eq", "=", "[", "]", "\n", "losses_val_uneq", "=", "[", "]", "\n", "\n", "valcounter", "=", "0", "\n", "val_user_items", "=", "{", "}", "\n", "total", "=", "num_samples", "\n", "valdone", "=", "False", "\n", "\n", "user_vectors", "=", "[", "]", "\n", "item_vectors", "=", "[", "]", "\n", "\n", "while", "not", "valdone", ":", "\n", "        ", "lossval", ",", "hamdist", ",", "userval", ",", "item_ratingval", ",", "itemsample", ",", "item_emb", ",", "user_emb", ",", "lossval_recon", ",", "lossval_uneq", ",", "lossval_eq", "=", "sess", ".", "run", "(", "eval_list", ",", "feed_dict", "=", "{", "handle", ":", "some_handle", ",", "is_training", ":", "False", ",", "\n", "anneal_val", ":", "0", ",", "\n", "batch_placeholder", ":", "min", "(", "total", ",", "eval_batchsize", ")", "}", ")", "\n", "\n", "losses_val", ".", "append", "(", "lossval", ")", "\n", "\n", "losses_val_recon", ".", "append", "(", "lossval_recon", ")", "\n", "losses_val_uneq", ".", "append", "(", "lossval_uneq", ")", "\n", "losses_val_eq", ".", "append", "(", "lossval_eq", ")", "\n", "\n", "valcounter", "+=", "1", "\n", "total", "-=", "len", "(", "userval", ")", "\n", "user_vectors", "+=", "user_emb", ".", "tolist", "(", ")", "\n", "item_vectors", "+=", "item_emb", ".", "tolist", "(", ")", "\n", "\n", "#print(hamdist)", "\n", "if", "total", "<=", "0", ":", "\n", "            ", "valdone", "=", "True", "\n", "\n", "", "for", "kk", "in", "range", "(", "len", "(", "userval", ")", ")", ":", "\n", "            ", "user", "=", "userval", "[", "kk", "]", "\n", "item_rating", "=", "item_ratingval", "[", "kk", "]", "\n", "\n", "user_item_score", "=", "-", "np", ".", "sum", "(", "user_emb", "[", "kk", "]", "*", "item_emb", "[", "kk", "]", ")", "\n", "\n", "if", "user", "not", "in", "val_user_items", ":", "\n", "                ", "val_user_items", "[", "user", "]", "=", "[", "[", "]", ",", "[", "]", "]", "\n", "\n", "", "val_user_items", "[", "user", "]", "[", "0", "]", ".", "append", "(", "int", "(", "user_item_score", ")", ")", "\n", "val_user_items", "[", "user", "]", "[", "1", "]", ".", "append", "(", "int", "(", "item_rating", ")", ")", "\n", "\n", "", "", "assert", "(", "total", "==", "0", ")", "\n", "t", "=", "0", "\n", "\n", "\n", "inps", "=", "[", "]", "\n", "for", "user", "in", "val_user_items", ":", "\n", "        ", "t", "+=", "len", "(", "val_user_items", "[", "user", "]", "[", "1", "]", ")", "\n", "inps", ".", "append", "(", "[", "val_user_items", "[", "user", "]", "[", "1", "]", ",", "val_user_items", "[", "user", "]", "[", "0", "]", "]", ")", "\n", "\n", "", "res", "=", "pool", ".", "starmap_async", "(", "ndcg_score", ",", "inps", ")", "\n", "ndcgs", "=", "res", ".", "get", "(", ")", "\n", "\n", "res_mrr", "=", "pool", ".", "starmap_async", "(", "mrr", ",", "inps", ")", "\n", "mrrs", "=", "res_mrr", ".", "get", "(", ")", "\n", "\n", "if", "not", "args", "[", "\"realvalued\"", "]", ":", "\n", "        ", "user_vectors", "=", "np", ".", "array", "(", "user_vectors", ")", ".", "astype", "(", "int", ")", "\n", "item_vectors", "=", "np", ".", "array", "(", "item_vectors", ")", ".", "astype", "(", "int", ")", "\n", "", "else", ":", "\n", "        ", "user_vectors", "=", "np", ".", "array", "(", "user_vectors", ")", "\n", "item_vectors", "=", "np", ".", "array", "(", "item_vectors", ")", "\n", "\n", "", "print", "(", "np", ".", "mean", "(", "bit_histogram", "(", "user_vectors", ",", "1", ")", ")", ",", "np", ".", "mean", "(", "bit_histogram", "(", "item_vectors", ",", "1", ")", ")", ")", "\n", "print", "(", "user_vectors", "[", "10", "]", "[", ":", "5", "]", ",", "item_vectors", "[", "10", "]", "[", ":", "5", "]", ")", "\n", "print", "(", "np", ".", "unique", "(", "user_vectors", ".", "astype", "(", "int", ")", "[", ":", "]", ")", ",", "np", ".", "unique", "(", "item_vectors", ".", "astype", "(", "int", ")", "[", ":", "]", ")", ",", "len", "(", "item_vectors", "[", "0", "]", ")", ",", "len", "(", "user_vectors", "[", "0", "]", ")", ")", "\n", "\n", "return", "np", ".", "mean", "(", "losses_val", ")", ",", "np", ".", "mean", "(", "ndcgs", ",", "0", ")", ",", "np", ".", "mean", "(", "losses_val_recon", ")", ",", "np", ".", "mean", "(", "losses_val_uneq", ")", ",", "np", ".", "mean", "(", "losses_val_eq", ")", ",", "len", "(", "ndcgs", ")", ",", "ndcgs", ",", "np", ".", "mean", "(", "mrrs", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.main.main": [[96, 300], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "vars", "print", "main.fprint", "pickle.load", "glob.glob", "glob.glob", "glob.glob", "tensorflow.reset_default_graph", "open", "args[].lower", "tensorflow.Session", "tensorflow.placeholder", "nn_helpers.generator", "nn_helpers.generator", "nn_helpers.generator", "gen_iter.get_next", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "model.Model", "model.Model._make_embedding", "model.Model._make_embedding", "model.Model._make_embedding", "model.Model.make_network", "tensorflow.Variable", "tensorflow.train.exponential_decay", "tensorflow.train.AdamOptimizer", "tf.train.AdamOptimizer.minimize", "tensorflow.global_variables_initializer", "sess.run", "sess.run", "print", "pickle.dump", "args[].lower", "time.time", "time.time", "sess.run", "times.append", "times.append", "losses_train.append", "losses_train_no_anneal.append", "open", "args[].lower", "print", "main.fprint", "sess.run", "main.onepass", "print", "main.fprint", "all_val_ndcg.append", "print", "main.fprint", "str", "args[].lower", "time.time", "time.time", "int", "numpy.mean", "numpy.mean", "numpy.mean", "sess.run", "main.onepass", "print", "sess.run", "sess.run", "datetime.datetime.now", "str", "str", "str", "numpy.mean", "numpy.mean", "numpy.mean", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.main.fprint", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.nn_helpers.generator", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.nn_helpers.generator", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.nn_helpers.generator", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._make_embedding", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._make_embedding", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._make_embedding", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model.make_network", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.main.fprint", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.main.onepass", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.main.fprint", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.main.fprint", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.main.onepass"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--batchsize\"", ",", "default", "=", "400", ",", "type", "=", "int", ")", "# 400 works well, but you can also search among {100, 200, 400, 800}", "\n", "parser", ".", "add_argument", "(", "\"--bits\"", ",", "default", "=", "32", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "default", "=", "0.001", ",", "type", "=", "float", ")", "# 0.001 works well, but you can also search among {0.005, 0.001, 0.0005}", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "default", "=", "\"ml-1m\"", ",", "type", "=", "str", ")", "# ml-1m, ml-10m, yelp16, amazon", "\n", "parser", ".", "add_argument", "(", "\"--ofile\"", ",", "default", "=", "\"../output.txt\"", ",", "type", "=", "str", ")", "# just a log file - results are saved to a file in results/", "\n", "\n", "# just keep these fixed", "\n", "parser", ".", "add_argument", "(", "\"--decay_rate\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\"--mul\"", ",", "default", "=", "0.5", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\"--anneal_val\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\"--deterministic_eval\"", ",", "default", "=", "1", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--deterministic_train\"", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--optimize_selfmask\"", ",", "default", "=", "1", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--usermask_nograd\"", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--KLweight\"", ",", "default", "=", "0.00", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\"--force_selfmask\"", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--realvalued\"", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "\n", "\n", "eval_batchsize", "=", "2000", "\n", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "savename", "=", "\"results/\"", "+", "\"_\"", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "[", "args", ".", "dataset", ",", "args", ".", "bits", ",", "args", ".", "batchsize", ",", "args", ".", "lr", ",", "args", ".", "anneal_val", ",", "args", ".", "deterministic_eval", ",", "\n", "args", ".", "deterministic_train", ",", "args", ".", "optimize_selfmask", ",", "args", ".", "usermask_nograd", ",", "args", ".", "KLweight", ",", "args", ".", "force_selfmask", ",", "args", ".", "mul", "]", "]", ")", "+", "\"_res.pkl\"", "\n", "\n", "args", ".", "realvalued", "=", "args", ".", "realvalued", ">", "0.5", "\n", "args", ".", "deterministic_eval", "=", "args", ".", "deterministic_eval", ">", "0.5", "\n", "args", ".", "usermask_nograd", "=", "args", ".", "usermask_nograd", ">", "0.5", "\n", "args", ".", "deterministic_train", "=", "args", ".", "deterministic_train", ">", "0.5", "\n", "args", ".", "optimize_selfmask", "=", "args", ".", "optimize_selfmask", ">", "0.5", "\n", "args", ".", "force_selfmask", "=", "args", ".", "force_selfmask", ">", "0.5", "\n", "\n", "args", "=", "vars", "(", "args", ")", "\n", "print", "(", "args", ")", "\n", "fprint", "(", "args", "[", "\"ofile\"", "]", ",", "args", ")", "\n", "\n", "basepath", "=", "\"../data/\"", "+", "args", "[", "\"dataset\"", "]", "+", "\"/tfrecord/\"", "\n", "dicfile", "=", "basepath", "+", "\"dict.pkl\"", "\n", "dicfile", "=", "pickle", ".", "load", "(", "open", "(", "dicfile", ",", "\"rb\"", ")", ")", "\n", "num_users", ",", "num_items", "=", "dicfile", "[", "0", "]", ",", "dicfile", "[", "1", "]", "\n", "\n", "args", "[", "\"num_users\"", "]", "=", "num_users", "\n", "args", "[", "\"num_items\"", "]", "=", "num_items", "\n", "\n", "trainfiles", "=", "glob", ".", "glob", "(", "basepath", "+", "\"*train_*tfrecord\"", ")", "\n", "valfiles", "=", "glob", ".", "glob", "(", "basepath", "+", "\"*val_*tfrecord\"", ")", "\n", "testfiles", "=", "glob", ".", "glob", "(", "basepath", "+", "\"*test_*tfrecord\"", ")", "\n", "\n", "# you can get the specific numbers below from the commeted-out code in (*) on line 196", "\n", "\n", "if", "args", "[", "\"dataset\"", "]", ".", "lower", "(", ")", "==", "\"ml-10m\"", ":", "\n", "        ", "train_samples", "=", "4201261", "\n", "val_samples", "=", "779388", "\n", "test_samples", "=", "5014822", "\n", "max_rating", "=", "10.0", "\n", "", "elif", "args", "[", "\"dataset\"", "]", ".", "lower", "(", ")", "==", "\"ml-1m\"", ":", "\n", "        ", "train_samples", "=", "420336", "\n", "val_samples", "=", "77436", "\n", "test_samples", "=", "500767", "\n", "max_rating", "=", "5.0", "\n", "", "elif", "args", "[", "\"dataset\"", "]", ".", "lower", "(", ")", "==", "\"yelp16\"", ":", "\n", "        ", "train_samples", "=", "241876", "\n", "val_samples", "=", "54245", "\n", "test_samples", "=", "306396", "\n", "max_rating", "=", "5.0", "\n", "", "elif", "args", "[", "\"dataset\"", "]", ".", "lower", "(", ")", "==", "\"amazon\"", ":", "\n", "        ", "train_samples", "=", "1896886", "\n", "val_samples", "=", "417192", "\n", "test_samples", "=", "2387890", "\n", "max_rating", "=", "5.0", "\n", "\n", "# (*)", "\n", "#print(sum(1 for _ in tf.python_io.tf_record_iterator(trainfiles[0])))", "\n", "#print(sum(1 for _ in tf.python_io.tf_record_iterator(valfiles[0])))", "\n", "#print(sum(1 for _ in tf.python_io.tf_record_iterator(testfiles[0])))", "\n", "#exit() # ...", "\n", "\n", "", "tf", ".", "reset_default_graph", "(", ")", "\n", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "\n", "\n", "        ", "handle", "=", "tf", ".", "placeholder", "(", "tf", ".", "string", ",", "shape", "=", "[", "]", ",", "name", "=", "\"handle_iterator\"", ")", "\n", "training_handle", ",", "train_iter", ",", "gen_iter", "=", "generator", "(", "sess", ",", "handle", ",", "args", "[", "\"batchsize\"", "]", ",", "trainfiles", ",", "0", ")", "\n", "val_handle", ",", "val_iter", ",", "_", "=", "generator", "(", "sess", ",", "handle", ",", "eval_batchsize", ",", "valfiles", ",", "1", ")", "\n", "test_handle", ",", "test_iter", ",", "_", "=", "generator", "(", "sess", ",", "handle", ",", "eval_batchsize", ",", "testfiles", ",", "1", ")", "\n", "\n", "sample", "=", "gen_iter", ".", "get_next", "(", ")", "\n", "user_sample", "=", "sample", "[", "0", "]", "\n", "item_sample", "=", "sample", "[", "1", "]", "\n", "item_rating", "=", "sample", "[", "4", "]", "\n", "\n", "is_training", "=", "tf", ".", "placeholder", "(", "tf", ".", "bool", ",", "name", "=", "\"is_training\"", ")", "\n", "anneal_val", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "name", "=", "\"anneal_val\"", ",", "shape", "=", "(", ")", ")", "\n", "batch_placeholder", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "name", "=", "\"batch_placeholder\"", ")", "\n", "\n", "model", "=", "Model", "(", "sample", ",", "args", ")", "\n", "\n", "item_emb_matrix", ",", "item_emb_ph", ",", "item_emb_init", "=", "model", ".", "_make_embedding", "(", "num_items", ",", "args", "[", "\"bits\"", "]", ",", "\"item_embedding\"", ")", "\n", "\n", "\n", "user_emb_matrix1", ",", "user_emb_ph1", ",", "user_emb_init1", "=", "model", ".", "_make_embedding", "(", "num_users", ",", "args", "[", "\"bits\"", "]", ",", "\"user_embedding1\"", ")", "\n", "user_mask_matrix", ",", "user_mask_ph", ",", "user_mask_init", "=", "model", ".", "_make_embedding", "(", "num_users", ",", "args", "[", "\"bits\"", "]", ",", "\"user_embedding_mask\"", ")", "\n", "\n", "\n", "loss", ",", "loss_no_anneal", ",", "scores", ",", "item_embedding", ",", "user_embedding", ",", "reconloss", ",", "rank_loss_uneq", ",", "rank_loss_eq", ",", "i1r_m", ",", "nonzerobits", "=", "model", ".", "make_network", "(", "item_emb_matrix", ",", "user_emb_matrix1", ",", "user_mask_matrix", ",", "is_training", ",", "args", ",", "max_rating", ",", "anneal_val", ",", "batch_placeholder", ")", "\n", "\n", "step", "=", "tf", ".", "Variable", "(", "0", ",", "trainable", "=", "False", ")", "\n", "lr", "=", "tf", ".", "train", ".", "exponential_decay", "(", "args", "[", "\"lr\"", "]", ",", "\n", "step", ",", "\n", "100000", ",", "\n", "args", "[", "\"decay_rate\"", "]", ",", "\n", "staircase", "=", "True", ",", "name", "=", "\"lr\"", ")", "\n", "\n", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "args", "[", "\"lr\"", "]", ",", "name", "=", "\"Adam\"", ")", "\n", "train_step", "=", "optimizer", ".", "minimize", "(", "loss", ")", "\n", "init", "=", "tf", ".", "global_variables_initializer", "(", ")", "\n", "sess", ".", "run", "(", "init", ")", "\n", "\n", "sess", ".", "run", "(", "train_iter", ".", "initializer", ")", "\n", "\n", "eval_list", "=", "[", "loss", ",", "scores", ",", "user_sample", ",", "item_rating", ",", "item_sample", ",", "item_embedding", ",", "user_embedding", ",", "reconloss", ",", "rank_loss_uneq", ",", "rank_loss_eq", "]", "\n", "counter", "=", "0", "\n", "losses_train", "=", "[", "]", "\n", "losses_train_no_anneal", "=", "[", "]", "\n", "times", "=", "[", "]", "\n", "anneal", "=", "args", "[", "\"anneal_val\"", "]", "\n", "\n", "best_val_ndcg", "=", "0", "\n", "best_val_loss", "=", "np", ".", "inf", "\n", "patience", "=", "5", "\n", "\n", "patience_counter", "=", "0", "\n", "running", "=", "True", "\n", "print", "(", "\"starting training\"", ")", "\n", "all_saves", "=", "[", "args", "]", "\n", "all_val_ndcg", "=", "[", "]", "\n", "times", "=", "[", "]", "\n", "while", "running", ":", "\n", "\n", "            ", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "lossval", ",", "loss_no_anneal_val", ",", "hamdist", ",", "_", ",", "iv", ",", "ir", ",", "nzb", "=", "sess", ".", "run", "(", "[", "loss", ",", "loss_no_anneal", ",", "scores", ",", "train_step", ",", "i1r_m", ",", "item_rating", ",", "nonzerobits", "]", ",", "feed_dict", "=", "{", "handle", ":", "training_handle", ",", "is_training", ":", "True", ",", "\n", "anneal_val", ":", "anneal", ",", "\n", "batch_placeholder", ":", "args", "[", "\"batchsize\"", "]", "}", ")", "\n", "times", ".", "append", "(", "time", ".", "time", "(", ")", "-", "t", ")", "\n", "times", "=", "times", "[", "-", "100", ":", "]", "\n", "#print(np.mean(times))", "\n", "\n", "times", ".", "append", "(", "time", ".", "time", "(", ")", "-", "start", ")", "\n", "losses_train", ".", "append", "(", "lossval", ")", "\n", "losses_train_no_anneal", ".", "append", "(", "loss_no_anneal_val", ")", "\n", "counter", "+=", "1", "\n", "\n", "anneal", "=", "anneal", "*", "0.9999", "\n", "if", "counter", "%", "int", "(", "1500", "*", "args", "[", "\"mul\"", "]", ")", "==", "0", ":", "\n", "                ", "print", "(", "\"train\"", ",", "np", ".", "mean", "(", "losses_train", ")", ",", "np", ".", "mean", "(", "losses_train_no_anneal", ")", ",", "counter", "*", "args", "[", "\"batchsize\"", "]", "/", "train_samples", ",", "np", ".", "mean", "(", "times", ")", ",", "anneal", ")", "\n", "fprint", "(", "args", "[", "\"ofile\"", "]", ",", "\" \"", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "[", "\"train\"", ",", "np", ".", "mean", "(", "losses_train", ")", ",", "np", ".", "mean", "(", "losses_train_no_anneal", ")", ",", "counter", "*", "args", "[", "\"batchsize\"", "]", "/", "train_samples", ",", "np", ".", "mean", "(", "times", ")", ",", "anneal", "]", "]", ")", ")", "\n", "losses_train", "=", "[", "]", "\n", "times", "=", "[", "]", "\n", "losses_train_no_anneal", "=", "[", "]", "\n", "\n", "sess", ".", "run", "(", "val_iter", ".", "initializer", ")", "\n", "losses_val", ",", "val_ndcg", ",", "losses_val_recon", ",", "losses_val_uneq", ",", "losses_val_eq", ",", "NN", ",", "allndcgs", ",", "val_mrr", "=", "onepass", "(", "sess", ",", "eval_list", ",", "val_handle", ",", "\n", "val_samples", ",", "eval_batchsize", ",", "\n", "handle", ",", "anneal_val", ",", "batch_placeholder", ",", "is_training", ",", "args", "[", "\"force_selfmask\"", "]", ",", "args", ")", "\n", "print", "(", "\"val ndcg@(5,10) and MRR\\t\\t\"", ",", "val_ndcg", ",", "val_mrr", ")", "#, [losses_val, losses_val_recon, losses_val_uneq, losses_val_eq], NN)", "\n", "save_val_ndcg", "=", "val_ndcg", "\n", "fprint", "(", "args", "[", "\"ofile\"", "]", ",", "\" \"", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "[", "\"val\\t\\t\"", ",", "val_ndcg", ",", "[", "losses_val", ",", "losses_val_recon", ",", "losses_val_uneq", ",", "losses_val_eq", "]", ",", "NN", "]", "]", ")", ")", "\n", "\n", "all_val_ndcg", ".", "append", "(", "best_val_ndcg", ")", "\n", "if", "val_ndcg", "[", "-", "1", "]", ">", "best_val_ndcg", ":", "# or best_val_loss > losses_val:", "\n", "                    ", "best_val_ndcg", "=", "val_ndcg", "[", "-", "1", "]", "\n", "\n", "best_val_loss", "=", "losses_val", "\n", "patience_counter", "=", "0", "\n", "", "else", ":", "\n", "                    ", "patience_counter", "+=", "1", "\n", "\n", "", "if", "patience_counter", "==", "0", ":", "\n", "                    ", "sess", ".", "run", "(", "test_iter", ".", "initializer", ")", "\n", "_", ",", "val_ndcg", ",", "losses_val_recon", ",", "losses_val_uneq", ",", "losses_val_eq", ",", "NN", ",", "_", ",", "val_mrr", "=", "onepass", "(", "sess", ",", "eval_list", ",", "test_handle", ",", "\n", "test_samples", ",", "eval_batchsize", ",", "\n", "handle", ",", "anneal_val", ",", "batch_placeholder", ",", "is_training", ",", "args", "[", "\"force_selfmask\"", "]", ",", "args", ")", "\n", "print", "(", "\"test ndcg@(5,10) and MRR\\t\\t\\t\\t\"", ",", "val_ndcg", ",", "val_mrr", ")", "#, [losses_val, losses_val_recon, losses_val_uneq, losses_val_eq], NN)", "\n", "#fprint(args[\"ofile\"], \" \".join([str(v) for v in [\"test\\t\\t\\t\\t\",  val_ndcg, [losses_val, losses_val_recon, losses_val_uneq, losses_val_eq], NN]]))", "\n", "\n", "save_item", "=", "sess", ".", "run", "(", "item_emb_matrix", ")", "\n", "save_user", "=", "sess", ".", "run", "(", "user_emb_matrix1", ")", "\n", "\n", "to_save", "=", "[", "losses_val", ",", "save_val_ndcg", ",", "allndcgs", ",", "args", ",", "all_val_ndcg", ",", "save_item", ",", "save_user", "]", "\n", "\n", "", "if", "patience_counter", ">=", "patience", ":", "\n", "                    ", "running", "=", "False", "\n", "\n", "\n", "", "print", "(", "\"patience\"", ",", "patience_counter", ",", "\"/\"", ",", "patience", ",", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", ")", "\n", "fprint", "(", "args", "[", "\"ofile\"", "]", ",", "\" \"", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "[", "\"patience\"", ",", "patience_counter", ",", "\"/\"", ",", "patience", ",", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "]", "]", ")", ")", "\n", "\n", "", "", "pickle", ".", "dump", "(", "to_save", ",", "open", "(", "savename", ",", "\"wb\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.bit_inspecter.bit_histogram": [[6, 14], ["numpy.mean"], "function", ["None"], ["def", "bit_histogram", "(", "vectors", ",", "title", ")", ":", "\n", "#plt.figure()", "\n", "\n", "    ", "bits", "=", "vectors", ".", "shape", "[", "1", "]", "\n", "\n", "avg", "=", "np", ".", "mean", "(", "vectors", ",", "0", ")", "\n", "\n", "return", "avg", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model.__init__": [[32, 35], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sample", ",", "args", ")", ":", "\n", "        ", "self", ".", "sample", "=", "sample", "\n", "self", ".", "batchsize", "=", "args", "[", "\"batchsize\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._make_embedding": [[36, 43], ["tensorflow.Variable", "tensorflow.placeholder", "tensorflow.Variable.assign", "tensorflow.random_uniform"], "methods", ["None"], ["", "def", "_make_embedding", "(", "self", ",", "vocab_size", ",", "embedding_size", ",", "name", ")", ":", "\n", "        ", "W", "=", "tf", ".", "Variable", "(", "tf", ".", "random_uniform", "(", "shape", "=", "[", "vocab_size", ",", "embedding_size", "]", ",", "minval", "=", "-", "1", ",", "maxval", "=", "1", ")", ",", "\n", "trainable", "=", "True", ",", "name", "=", "name", ")", "\n", "\n", "embedding_placeholder", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "vocab_size", ",", "embedding_size", "]", ")", "\n", "embedding_init", "=", "W", ".", "assign", "(", "embedding_placeholder", ")", "\n", "return", "(", "W", ",", "embedding_placeholder", ",", "embedding_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._extract": [[44, 59], ["tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup", "tensorflow.nn.embedding_lookup"], "methods", ["None"], ["", "def", "_extract", "(", "self", ",", "item_emb", ",", "user_embs", ",", "user_mask_emb", ")", ":", "#, user_index_embedding_matrix, user_index_embedding_ratings_matrix):", "\n", "        ", "user", ",", "i1", ",", "i2", ",", "iu", ",", "i1r", ",", "i2r", "=", "self", ".", "sample", "[", "0", "]", ",", "self", ".", "sample", "[", "1", "]", ",", "self", ".", "sample", "[", "2", "]", ",", "self", ".", "sample", "[", "3", "]", ",", "self", ".", "sample", "[", "4", "]", ",", "self", ".", "sample", "[", "5", "]", "\n", "\n", "user_mask", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "user_mask_emb", ",", "user", ")", "\n", "\n", "#user_index_values = tf.cast(tf.nn.embedding_lookup(user_index_embedding_matrix, user), tf.int32)", "\n", "#user_index_ratings_values = tf.nn.embedding_lookup(user_index_embedding_ratings_matrix, user)", "\n", "\n", "user", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "user_embs", ",", "user", ")", "#tf.concat([tf.expand_dims(tf.nn.embedding_lookup(user_emb_x, user), axis=-1) for user_emb_x in user_embs], axis=-1)", "\n", "\n", "i1", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "item_emb", ",", "i1", ")", "\n", "i2", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "item_emb", ",", "i2", ")", "\n", "i3", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "item_emb", ",", "iu", ")", "\n", "\n", "return", "user", ",", "user_mask", ",", "i1", ",", "i2", ",", "i3", ",", "i1r", ",", "i2r", "#, user_index_values, user_index_ratings_values", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._sample_gumbel": [[60, 64], ["tensorflow.random_uniform", "tensorflow.log", "tensorflow.log"], "methods", ["None"], ["", "def", "_sample_gumbel", "(", "self", ",", "shape", ",", "eps", "=", "1e-20", ")", ":", "\n", "        ", "\"\"\"Sample from Gumbel(0, 1)\"\"\"", "\n", "U", "=", "tf", ".", "random_uniform", "(", "shape", ",", "minval", "=", "0", ",", "maxval", "=", "1", ")", "\n", "return", "-", "tf", ".", "log", "(", "-", "tf", ".", "log", "(", "U", "+", "eps", ")", "+", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._gumbel_softmax_sample": [[65, 69], ["tensorflow.nn.softmax", "model.Model._sample_gumbel", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._sample_gumbel"], ["", "def", "_gumbel_softmax_sample", "(", "self", ",", "logits", ",", "temperature", ")", ":", "\n", "        ", "\"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"", "\n", "y", "=", "logits", "+", "self", ".", "_sample_gumbel", "(", "tf", ".", "shape", "(", "logits", ")", ")", "\n", "return", "tf", ".", "nn", ".", "softmax", "(", "y", "/", "temperature", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model.gumbel_softmax": [[70, 93], ["model.Model._gumbel_softmax_sample", "tensorflow.cast", "tensorflow.cond", "tensorflow.equal", "tensorflow.stop_gradient", "tensorflow.reduce_max"], "methods", ["home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._gumbel_softmax_sample"], ["", "def", "gumbel_softmax", "(", "self", ",", "logits", ",", "temperature", ",", "hard", ")", ":", "\n", "        ", "\"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n        Args:\n          logits: [batch_size, bits, n_class] unnormalized log-probs\n          temperature: non-negative scalar\n          hard: if True, take argmax, but differentiate w.r.t. soft sample y\n        Returns:\n          [batch_size, bits, n_class] sample from the Gumbel-Softmax distribution.\n          If hard=True, then the returned sample will be one-hot, otherwise it will\n          be a probabilitiy distribution that sums to 1 across classes\n        \"\"\"", "\n", "y", "=", "self", ".", "_gumbel_softmax_sample", "(", "logits", ",", "temperature", ")", "\n", "#if hard:", "\n", "\n", "# k = tf.shape(logits)[-1]", "\n", "# y_hard = tf.cast(tf.one_hot(tf.argmax(y,1),k), y.dtype)", "\n", "y_hard", "=", "tf", ".", "cast", "(", "tf", ".", "equal", "(", "y", ",", "tf", ".", "reduce_max", "(", "y", ",", "-", "1", ",", "keepdims", "=", "True", ")", ")", ",", "y", ".", "dtype", ")", "\n", "#print(tf.reduce_max(y, -1), y)", "\n", "#exit()", "\n", "y_hard", "=", "tf", ".", "stop_gradient", "(", "y_hard", "-", "y", ")", "+", "y", "\n", "\n", "y", "=", "tf", ".", "cond", "(", "hard", ",", "lambda", ":", "y_hard", ",", "lambda", ":", "y", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model.make_network": [[94, 217], ["tensorflow.python.framework.ops.RegisterGradient", "model.Model._extract", "tensorflow.sigmoid", "tensorflow.sigmoid", "tensorflow.sigmoid", "model.Model.make_network.bernoulliSample"], "methods", ["home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.Model._extract"], ["", "def", "make_network", "(", "self", ",", "item_emb", ",", "user_embs", ",", "user_mask_emb", ",", "is_training", ",", "args", ",", "max_rating", ",", "sigma_anneal", ",", "batchsize", ")", ":", "\n", "# user_index_embedding_matrix, user_index_embedding_ratings_matrix,", "\n", "\n", "#################### Bernoulli Sample #####################", "\n", "## ref code: https://r2rt.com/binary-stochastic-neurons-in-tensorflow.html", "\n", "        ", "def", "bernoulliSample", "(", "x", ")", ":", "\n", "            ", "\"\"\"\n            Uses a tensor whose values are in [0,1] to sample a tensor with values in {0, 1},\n            using the straight through estimator for the gradient.\n            E.g.,:\n            if x is 0.6, bernoulliSample(x) will be 1 with probability 0.6, and 0 otherwise,\n            and the gradient will be pass-through (identity).\n            \"\"\"", "\n", "g", "=", "tf", ".", "get_default_graph", "(", ")", "\n", "\n", "with", "ops", ".", "name_scope", "(", "\"BernoulliSample\"", ")", "as", "name", ":", "\n", "                ", "with", "g", ".", "gradient_override_map", "(", "{", "\"Ceil\"", ":", "\"Identity\"", ",", "\"Sub\"", ":", "\"BernoulliSample_ST\"", "}", ")", ":", "\n", "\n", "                    ", "if", "args", "[", "\"deterministic_train\"", "]", ":", "\n", "                        ", "train_fn", "=", "lambda", ":", "tf", ".", "minimum", "(", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "x", ")", ")", ",", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "x", ")", ")", "*", "0.5", ")", "\n", "", "else", ":", "\n", "                        ", "train_fn", "=", "lambda", ":", "tf", ".", "minimum", "(", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "x", ")", ")", ",", "tf", ".", "random_uniform", "(", "tf", ".", "shape", "(", "x", ")", ")", ")", "\n", "\n", "", "if", "args", "[", "\"deterministic_eval\"", "]", ":", "\n", "                        ", "eval_fn", "=", "lambda", ":", "tf", ".", "minimum", "(", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "x", ")", ")", ",", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "x", ")", ")", "*", "0.5", ")", "\n", "", "else", ":", "\n", "                        ", "eval_fn", "=", "lambda", ":", "tf", ".", "minimum", "(", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "x", ")", ")", ",", "tf", ".", "random_uniform", "(", "tf", ".", "shape", "(", "x", ")", ")", ")", "\n", "\n", "", "mus", "=", "tf", ".", "cond", "(", "is_training", ",", "train_fn", ",", "eval_fn", ")", "\n", "\n", "return", "tf", ".", "ceil", "(", "x", "-", "mus", ",", "name", "=", "name", ")", "\n", "\n", "", "", "", "@", "ops", ".", "RegisterGradient", "(", "\"BernoulliSample_ST\"", ")", "\n", "def", "bernoulliSample_ST", "(", "op", ",", "grad", ")", ":", "\n", "            ", "return", "[", "grad", ",", "tf", ".", "zeros", "(", "tf", ".", "shape", "(", "op", ".", "inputs", "[", "1", "]", ")", ")", "]", "\n", "\n", "###########################################################", "\n", "\n", "", "user", ",", "user_mask", ",", "i1", ",", "i2", ",", "i3", ",", "i1r", ",", "i2r", "=", "self", ".", "_extract", "(", "item_emb", ",", "user_embs", ",", "user_mask_emb", ")", "\n", "\n", "user", "=", "tf", ".", "sigmoid", "(", "user", ")", "#[:, :, 0])", "\n", "\n", "i1", "=", "tf", ".", "sigmoid", "(", "i1", ")", "\n", "i2", "=", "tf", ".", "sigmoid", "(", "i2", ")", "\n", "\n", "i1_sampling", "=", "i1", "\n", "i2_sampling", "=", "i2", "\n", "user_sampling", "=", "user", "\n", "\n", "\n", "user", "=", "bernoulliSample", "(", "user", ")", "\n", "\n", "if", "args", "[", "\"usermask_nograd\"", "]", ":", "\n", "            ", "selfmask", "=", "tf", ".", "stop_gradient", "(", "user", ")", "\n", "", "else", ":", "\n", "            ", "selfmask", "=", "user", "\n", "\n", "", "i1_org", "=", "bernoulliSample", "(", "i1", ")", "*", "(", "selfmask", "if", "args", "[", "\"optimize_selfmask\"", "]", "else", "1", ")", "\n", "i2_org", "=", "bernoulliSample", "(", "i2", ")", "*", "(", "selfmask", "if", "args", "[", "\"optimize_selfmask\"", "]", "else", "1", ")", "\n", "\n", "user_m", "=", "2", "*", "user", "-", "1", "\n", "i1_org_m", "=", "(", "2", "*", "i1_org", "-", "1", ")", "#* tf.cast(tf.abs(user) > 0.5, tf.float32) #* ((user+1)/2)", "\n", "i2_org_m", "=", "(", "2", "*", "i2_org", "-", "1", ")", "#* tf.cast(tf.abs(user) > 0.5, tf.float32) #* ((user+1)/2)", "\n", "\n", "nonzero_bits", "=", "args", "[", "\"bits\"", "]", "# tf.reduce_sum(tf.cast(user_m>0.5, tf.float32), axis=-1)", "\n", "\n", "def", "make_total_loss", "(", "i1_org", ",", "i2_org", ",", "i1r", ",", "i2r", ",", "i1_sampling", ",", "i2_sampling", ",", "anneal", ")", ":", "\n", "            ", "e0", "=", "tf", ".", "random", ".", "normal", "(", "[", "batchsize", "]", ",", "stddev", "=", "1.0", ",", "\n", "name", "=", "'normaldis0'", ")", "\n", "e1", "=", "tf", ".", "truncated_normal", "(", "[", "batchsize", ",", "args", "[", "\"bits\"", "]", "]", ",", "stddev", "=", "1.0", ",", "\n", "name", "=", "'normaldist1'", ")", "# tf.random.normal([batchsize, args[\"bits\"]])", "\n", "e2", "=", "tf", ".", "truncated_normal", "(", "[", "batchsize", ",", "args", "[", "\"bits\"", "]", "]", ",", "stddev", "=", "1.0", ",", "name", "=", "'normaldist2'", ")", "\n", "\n", "i1", "=", "i1_org", "# e1*0 + i1_org", "\n", "i2", "=", "i2_org", "#e2*0 + i2_org", "\n", "\n", "i1r", "=", "i1r", "+", "e0", "*", "anneal", "\n", "\n", "#i1r_m = 2*args[\"bits\"] * (i1r/max_rating) - args[\"bits\"]", "\n", "i1r_m", "=", "2", "*", "nonzero_bits", "*", "(", "i1r", "/", "max_rating", ")", "-", "nonzero_bits", "\n", "\n", "dot_i1", "=", "tf", ".", "reduce_sum", "(", "user_m", "*", "i1", ",", "axis", "=", "-", "1", ")", "#- tf.cond(is_training, lambda :(1.0*args[\"bits\"] - nonzero_bits), lambda :tf.reduce_sum(tf.cast(user_m<0.5, tf.float32), axis=-1))", "\n", "dot_i2", "=", "tf", ".", "reduce_sum", "(", "user_m", "*", "i2", ",", "axis", "=", "-", "1", ")", "#- tf.cond(is_training, lambda :(1.0*args[\"bits\"] - nonzero_bits), lambda :tf.reduce_sum(tf.cast(user_m<0.5, tf.float32), axis=-1))", "\n", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "math", ".", "pow", "(", "i1r_m", "-", "dot_i1", ",", "2", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# ranking loss", "\n", "different_rating", "=", "tf", ".", "cast", "(", "tf", ".", "abs", "(", "i1r", "-", "i2r", ")", ">", "0.0001", ",", "tf", ".", "float32", ")", "\n", "same_rating", "=", "tf", ".", "cast", "(", "different_rating", "<", "0.5", ",", "tf", ".", "float32", ")", "\n", "\n", "signpart", "=", "tf", ".", "cast", "(", "i1r", ">", "i2r", ",", "tf", ".", "float32", ")", "\n", "rank_loss_uneq", "=", "hinge_loss_eps", "(", "labels", "=", "signpart", ",", "logits", "=", "(", "dot_i1", "-", "dot_i2", ")", ",", "epsval", "=", "1.0", ",", "weights", "=", "different_rating", ")", "\n", "eq_dist", "=", "tf", ".", "math", ".", "pow", "(", "dot_i1", "-", "dot_i2", ",", "2", ")", "\n", "rank_loss_eq", "=", "compute_weighted_loss", "(", "eq_dist", ",", "weights", "=", "same_rating", ",", "reduction", "=", "Reduction", ".", "SUM_BY_NONZERO_WEIGHTS", ")", "\n", "\n", "# reg", "\n", "loss_kl", "=", "tf", ".", "multiply", "(", "i1_sampling", ",", "tf", ".", "math", ".", "log", "(", "tf", ".", "maximum", "(", "i1_sampling", "/", "0.5", ",", "1e-10", ")", ")", ")", "+", "tf", ".", "multiply", "(", "1", "-", "i1_sampling", ",", "tf", ".", "math", ".", "log", "(", "tf", ".", "maximum", "(", "(", "1", "-", "i1_sampling", ")", "/", "0.5", ",", "1e-10", ")", ")", ")", "\n", "\n", "loss_kl", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "reduce_sum", "(", "loss_kl", ",", "1", ")", "/", "args", "[", "\"batchsize\"", "]", ",", "axis", "=", "0", ")", "\n", "\n", "loss_kl_user", "=", "tf", ".", "multiply", "(", "user_sampling", ",", "tf", ".", "math", ".", "log", "(", "tf", ".", "maximum", "(", "user_sampling", "/", "0.5", ",", "1e-10", ")", ")", ")", "+", "tf", ".", "multiply", "(", "1", "-", "user_sampling", ",", "tf", ".", "math", ".", "log", "(", "tf", ".", "maximum", "(", "(", "1", "-", "user_sampling", ")", "/", "0.5", ",", "1e-10", ")", ")", ")", "\n", "\n", "loss_kl_user", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "reduce_sum", "(", "loss_kl_user", ",", "1", ")", "/", "args", "[", "\"batchsize\"", "]", ",", "axis", "=", "0", ")", "\n", "\n", "# combine losses", "\n", "total_loss", "=", "loss", "+", "args", "[", "\"KLweight\"", "]", "*", "(", "loss_kl", "+", "loss_kl_user", ")", "#+ (1-anneal)*(rank_loss_uneq + rank_loss_eq) # loss    loss_kl*0.1*(1-anneal)", "\n", "##total_loss = loss + (1-anneal)*(rank_loss_uneq + rank_loss_eq) # loss    loss_kl*0.1*(1-anneal)", "\n", "\n", "i1_dist", "=", "-", "dot_i1", "\n", "return", "total_loss", ",", "i1_dist", ",", "loss", ",", "rank_loss_uneq", ",", "rank_loss_eq", ",", "i1r_m", "\n", "\n", "", "total_loss", ",", "ham_dist_i1", ",", "reconloss", ",", "rank_loss_uneq", ",", "rank_loss_eq", ",", "i1r_m", "=", "make_total_loss", "(", "i1_org_m", ",", "i2_org_m", ",", "i1r", ",", "i2r", ",", "i1_sampling", ",", "i2_sampling", ",", "sigma_anneal", ")", "\n", "\n", "total", "=", "total_loss", "\n", "\n", "if", "args", "[", "\"force_selfmask\"", "]", ":", "\n", "            ", "i1_org_m", "=", "(", "i1_org_m", "+", "1", ")", "/", "2", "\n", "i1_org_m", "=", "i1_org_m", "*", "selfmask", "\n", "i1_org_m", "=", "2", "*", "i1_org_m", "-", "1", "\n", "\n", "", "return", "total", ",", "total", ",", "ham_dist_i1", ",", "i1_org_m", ",", "user_m", ",", "reconloss", ",", "rank_loss_uneq", ",", "rank_loss_eq", ",", "i1r_m", ",", "tf", ".", "reduce_sum", "(", "tf", ".", "cast", "(", "user_m", ">", "0.5", ",", "tf", ".", "float32", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.model.hinge_loss_eps": [[10, 30], ["ValueError", "ValueError", "tensorflow.python.framework.ops.name_scope", "tensorflow.python.ops.math_ops.to_float", "tensorflow.python.ops.math_ops.to_float", "math_ops.to_float.get_shape().assert_is_compatible_with", "tensorflow.python.ops.array_ops.ones_like", "tensorflow.python.ops.math_ops.subtract", "tensorflow.python.ops.nn_ops.relu", "tensorflow.losses.compute_weighted_loss", "math_ops.subtract.get_shape", "tensorflow.python.ops.array_ops.ones_like", "tensorflow.python.ops.math_ops.subtract", "math_ops.to_float.get_shape", "tensorflow.python.ops.math_ops.multiply"], "function", ["None"], ["def", "hinge_loss_eps", "(", "labels", ",", "logits", ",", "epsval", ",", "weights", "=", "1.0", ",", "scope", "=", "None", ",", "\n", "loss_collection", "=", "ops", ".", "GraphKeys", ".", "LOSSES", ",", "\n", "reduction", "=", "Reduction", ".", "SUM_BY_NONZERO_WEIGHTS", ")", ":", "\n", "  ", "if", "labels", "is", "None", ":", "\n", "    ", "raise", "ValueError", "(", "\"labels must not be None.\"", ")", "\n", "", "if", "logits", "is", "None", ":", "\n", "    ", "raise", "ValueError", "(", "\"logits must not be None.\"", ")", "\n", "", "with", "ops", ".", "name_scope", "(", "scope", ",", "\"hinge_loss\"", ",", "(", "logits", ",", "labels", ",", "weights", ")", ")", "as", "scope", ":", "\n", "    ", "logits", "=", "math_ops", ".", "to_float", "(", "logits", ")", "\n", "labels", "=", "math_ops", ".", "to_float", "(", "labels", ")", "\n", "logits", ".", "get_shape", "(", ")", ".", "assert_is_compatible_with", "(", "labels", ".", "get_shape", "(", ")", ")", "\n", "# We first need to convert binary labels to -1/1 labels (as floats).", "\n", "all_eps", "=", "array_ops", ".", "ones_like", "(", "labels", ")", "*", "epsval", "\n", "all_ones", "=", "array_ops", ".", "ones_like", "(", "labels", ")", "\n", "\n", "labels", "=", "math_ops", ".", "subtract", "(", "2", "*", "labels", ",", "all_ones", ")", "\n", "losses", "=", "nn_ops", ".", "relu", "(", "\n", "math_ops", ".", "subtract", "(", "all_eps", ",", "math_ops", ".", "multiply", "(", "labels", ",", "logits", ")", ")", ")", "\n", "return", "compute_weighted_loss", "(", "\n", "losses", ",", "weights", ",", "scope", ",", "loss_collection", ",", "reduction", "=", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.helpers.dcg_at_k": [[6, 18], ["numpy.asfarray", "ValueError", "numpy.sum", "numpy.log2", "numpy.arange"], "function", ["None"], ["def", "dcg_at_k", "(", "r", ",", "k", ",", "method", ")", ":", "\n", "    ", "r", "=", "np", ".", "asfarray", "(", "r", ")", "[", ":", "k", "]", "\n", "r", "=", "2", "**", "r", "-", "1", "\n", "if", "r", ".", "size", ":", "\n", "        ", "if", "method", "==", "0", ":", "\n", "#print(r / np.log2(np.arange(2, r.size+2) ))", "\n", "            ", "return", "r", "[", "0", "]", "+", "np", ".", "sum", "(", "r", "[", "1", ":", "]", "/", "np", ".", "log2", "(", "np", ".", "arange", "(", "2", ",", "r", ".", "size", "+", "1", ")", ")", ")", "\n", "#elif method == 1:", "\n", "#    return np.sum((r) / np.log2(np.arange(2, r.size + 2)))", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'method must be 0 or 1.'", ")", "\n", "", "", "return", "0.", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.helpers.ndcg_score": [[19, 29], ["numpy.array", "helpers._ndcg_at_k", "numpy.argsort"], "function", ["home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.helpers._ndcg_at_k"], ["", "def", "ndcg_score", "(", "labels", ",", "distances", ",", "k", "=", "10", ",", "ks", "=", "[", "5", ",", "10", "]", ")", ":", "\n", "    ", "'''\n    :param labels: ratings - higher is better\n    :param distances: hamming distances, lower is better\n    :param k:\n    :return: ndcg@k\n    '''", "\n", "labels", "=", "np", ".", "array", "(", "labels", ")", "\n", "vals", "=", "labels", "[", "np", ".", "argsort", "(", "distances", ")", "]", "\n", "return", "[", "_ndcg_at_k", "(", "vals", ",", "kk", ")", "for", "kk", "in", "ks", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.helpers._ndcg_at_k": [[30, 36], ["helpers.dcg_at_k", "sorted", "helpers.dcg_at_k"], "function", ["home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.helpers.dcg_at_k", "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.helpers.dcg_at_k"], ["", "def", "_ndcg_at_k", "(", "r", ",", "k", ",", "method", "=", "0", ")", ":", "\n", "    ", "dcg_max", "=", "dcg_at_k", "(", "sorted", "(", "r", ",", "reverse", "=", "True", ")", ",", "k", ",", "method", ")", "# revser=true, because higher ratings should be in the beginning", "\n", "if", "not", "dcg_max", ":", "\n", "        ", "return", "0.", "\n", "#print(dcg_at_k(r, k, method) , dcg_max)", "\n", "", "return", "dcg_at_k", "(", "r", ",", "k", ",", "method", ")", "/", "dcg_max", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.helpers.mrr": [[37, 49], ["numpy.array", "numpy.array", "numpy.argsort", "labels[].astype", "numpy.max", "numpy.where"], "function", ["None"], ["", "def", "mrr", "(", "labels", ",", "distances", ")", ":", "\n", "    ", "labels", "=", "np", ".", "array", "(", "labels", ")", "\n", "distances", "=", "np", ".", "array", "(", "distances", ")", "\n", "\n", "sorted_dists", "=", "np", ".", "argsort", "(", "distances", ")", "\n", "\n", "labels_ordered", "=", "labels", "[", "sorted_dists", "]", ".", "astype", "(", "int", ")", "\n", "\n", "max_rating", "=", "np", ".", "max", "(", "labels_ordered", ")", "\n", "rank", "=", "np", ".", "where", "(", "labels_ordered", "==", "max_rating", ")", "[", "0", "]", "[", "0", "]", "+", "1", "\n", "\n", "return", "1", "/", "rank", "", "", ""]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.nn_helpers.extract_fn": [[4, 35], ["tensorflow.parse_single_example", "features.keys", "tensorflow.cast", "tensorflow.cast", "tuple", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.squeeze", "tensorflow.cast"], "function", ["None"], ["def", "extract_fn", "(", "data_record", ")", ":", "\n", "    ", "'''\n    'user': tf.train.Feature(int64_list = tf.train.Int64List(value = [sample[0]])),\n    'i1': tf.train.Feature(int64_list=tf.train.Int64List(value=[sample[1]])),\n    'i2': tf.train.Feature(int64_list=tf.train.Int64List(value=[sample[2]])),\n    'i_unrated': tf.train.Feature(int64_list=tf.train.Int64List(value=[sample[3]])),\n    'i1_rating': tf.train.Feature(int64_list=tf.train.Int64List(value=[sample[4]])),\n    'i2_rating': tf.train.Feature(int64_list=tf.train.Int64List(value=[sample[5]]))\n    '''", "\n", "\n", "features", "=", "{", "\n", "'user'", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'i1'", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'i2'", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'i_unrated'", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'i1_rating'", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", ",", "\n", "'i2_rating'", ":", "tf", ".", "FixedLenFeature", "(", "[", "1", "]", ",", "tf", ".", "int64", ")", ",", "\n", "}", "\n", "\n", "sample", "=", "tf", ".", "parse_single_example", "(", "data_record", ",", "features", ")", "\n", "\n", "for", "key", "in", "features", ".", "keys", "(", ")", ":", "\n", "        ", "sample", "[", "key", "]", "=", "tf", ".", "squeeze", "(", "sample", "[", "key", "]", ",", "-", "1", ")", "\n", "\n", "", "for", "key", "in", "[", "\"user\"", ",", "\"i1\"", ",", "\"i2\"", ",", "\"i_unrated\"", "]", ":", "\n", "        ", "sample", "[", "key", "]", "=", "tf", ".", "cast", "(", "sample", "[", "key", "]", ",", "tf", ".", "int32", ")", "\n", "\n", "", "sample", "[", "\"i1_rating\"", "]", "=", "tf", ".", "cast", "(", "sample", "[", "\"i1_rating\"", "]", ",", "tf", ".", "float32", ")", "\n", "sample", "[", "\"i2_rating\"", "]", "=", "tf", ".", "cast", "(", "sample", "[", "\"i2_rating\"", "]", ",", "tf", ".", "float32", ")", "\n", "\n", "return", "tuple", "(", "[", "sample", "[", "key", "]", "for", "key", "in", "[", "\"user\"", ",", "\"i1\"", ",", "\"i2\"", ",", "\"i_unrated\"", ",", "\"i1_rating\"", ",", "\"i2_rating\"", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.casperhansen_Projected-Hamming-Dissimilarity.code.nn_helpers.generator": [[36, 58], ["tuple", "tensorflow.TensorShape", "tuple", "tensorflow.data.Dataset.from_tensor_slices", "dataset.shuffle.shuffle", "dataset.shuffle.flat_map", "dataset.shuffle.map", "dataset.shuffle.batch", "dataset.shuffle.prefetch", "dataset.shuffle.make_initializable_iterator", "tensorflow.data.Iterator.from_string_handle", "sess.run", "dataset.shuffle.repeat", "dataset.shuffle.shuffle", "dataset.make_initializable_iterator.string_handle"], "function", ["None"], ["", "def", "generator", "(", "sess", ",", "handle", ",", "batchsize", ",", "record_paths", ",", "is_test", ")", ":", "\n", "\n", "    ", "output_t", "=", "tuple", "(", "[", "tf", ".", "int32", ",", "tf", ".", "int32", ",", "tf", ".", "int32", ",", "tf", ".", "int32", ",", "tf", ".", "float32", ",", "tf", ".", "float32", "]", ")", "\n", "s", "=", "tf", ".", "TensorShape", "(", "[", "None", ",", "]", ")", "\n", "output_s", "=", "tuple", "(", "[", "s", "for", "_", "in", "output_t", "]", ")", "\n", "\n", "dataset", "=", "tf", ".", "data", ".", "Dataset", ".", "from_tensor_slices", "(", "record_paths", ")", "\n", "if", "not", "is_test", ":", "\n", "        ", "dataset", "=", "dataset", ".", "repeat", "(", ")", "\n", "", "dataset", "=", "dataset", ".", "shuffle", "(", "100", ")", "\n", "dataset", "=", "dataset", ".", "flat_map", "(", "tf", ".", "data", ".", "TFRecordDataset", ")", "\n", "dataset", "=", "dataset", ".", "map", "(", "extract_fn", ",", "num_parallel_calls", "=", "3", ")", "\n", "if", "not", "is_test", ":", "\n", "        ", "dataset", "=", "dataset", ".", "shuffle", "(", "30000", ")", "\n", "", "dataset", "=", "dataset", ".", "batch", "(", "batchsize", ")", "\n", "dataset", "=", "dataset", ".", "prefetch", "(", "10", ")", "\n", "iterator", "=", "dataset", ".", "make_initializable_iterator", "(", ")", "\n", "\n", "generic_iter", "=", "tf", ".", "data", ".", "Iterator", ".", "from_string_handle", "(", "handle", ",", "output_t", ",", "output_s", ")", "\n", "specific_handle", "=", "sess", ".", "run", "(", "iterator", ".", "string_handle", "(", ")", ")", "\n", "\n", "return", "specific_handle", ",", "iterator", ",", "generic_iter", "\n", "\n"]]}